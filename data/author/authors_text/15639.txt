Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1600?1610,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploiting Parse Structures for Native Language Identification
Sze-Meng Jojo Wong
Centre for Language Technology
Macquarie University
Sydney, Australia
sze.wong@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
mark.dras@mq.edu.au
Abstract
Attempts to profile authors according to their
characteristics extracted from textual data, in-
cluding native language, have drawn attention
in recent years, via various machine learn-
ing approaches utilising mostly lexical fea-
tures. Drawing on the idea of contrastive
analysis, which postulates that syntactic er-
rors in a text are to some extent influenced by
the native language of an author, this paper
explores the usefulness of syntactic features
for native language identification. We take
two types of parse substructure as features?
horizontal slices of trees, and the more gen-
eral feature schemas from discriminative parse
reranking?and show that using this kind of
syntactic feature results in an accuracy score
in classification of seven native languages of
around 80%, an error reduction of more than
30%.
1 Introduction
Inferring characteristics of authors from their tex-
tual data, often termed authorship profiling, has seen
a number of computational approaches proposed in
recent years. The problem is typically treated as a
classification task, where an author is classified with
respect to characteristics such as gender, age, native
language, and so on. This profile information is of-
ten of interest to marketing organisations for prod-
uct promotional reasons as well as governments or
law enforcements for crime investigation purposes.
The particular application that motivates the present
study is detection of phishing (Myers, 2007), the at-
tempt to defraud through texts that are designed to
deceive Internet users into giving away confidential
details. One class of countermeasures to phishing
consists of technical methods such as email authen-
tication; another looks at profiling of the text?s au-
thor(s) (Fette et al, 2007; Zheng et al, 2003), to
find any indications of the source of the text.
In this paper we investigate classification of a text
with respect to an author?s native language, where
this is not the language that that text is written in
(which is often the case in phishing); we refer to
this as native language identification. Initial work
by Koppel et al (2005) was followed by Tsur and
Rappoport (2007), Estival et al (2007), van Halteren
(2008), and Wong and Dras (2009). By and large,
the problem was tackled using various supervised
machine learning approaches, with mostly lexical
features over characters, words, and parts of speech,
as well as some document structure.
Syntactic features, in contrast, in particular those
that capture grammatical errors, which might po-
tentially be useful for this task, have received lit-
tle attention. Koppel et al (2005) did suggest using
syntactic errors in their work but did not investigate
them in any detail. Wong and Dras (2009) noted
the relevance of the concept of contrastive analy-
sis (Lado, 1957), which postulates that native lan-
guage constructions lead to characteristic errors in a
second language. In their experimental work, how-
ever, they used only three manual syntactic construc-
tions drawn from the literature; an ANOVA analysis
showed a detectable effect, but they did not improve
classification accuracy over purely lexical features.
In this paper, we investigate syntactic features for
native language identification that are more general
1600
than, and that do not require the manual construction
of, the above approach. Taking the trees produced
by statistical parsers, we use tree cross-sections as
features in a machine learning approach to deter-
mine which ones characterise non-native speaker er-
rors. Specifically, we look at two types of parse
tree substructure to use as features: horizontal slices
of the trees?that is, characterising parse trees as
sets of context-free grammar production rules?and
the features schemas used in discriminative parse
reranking. The goal of the present study is therefore
to investigate the influence to which syntactic fea-
tures represented by parse structures would have on
the classification task of identifying an author?s na-
tive language relative to, and in combination with,
lexical features.
The remainder of this paper is structured as fol-
lows. In Section 2, we discuss some related work on
the two key topics of this paper: primarily on com-
parable work in native language identification, and
then on how the notion of contrastive analysis can be
applicable here. We then describe the models exam-
ined in Section 3, followed by experimental setup in
Section 4. Section 5 presents results, and Section 6
discussion of those results.
2 Related Work
2.1 Native Language Identification
The earliest work on native language identification
in this classification paradigm is that of Koppel et
al. (2005), in which they deployed a machine learn-
ing approach to the task, using as features func-
tion words, character n-grams, and part-of-speech
(PoS) bi-grams, as well as some spelling mistakes.
With five different groups of English authors (of na-
tive languages Bulgarian, Czech, French, Russian,
and Spanish) selected from the first version of In-
ternational Corpus of Learner English (ICLE), they
gained a relatively high classification accuracy of
80%. Koppel et al (2005) also suggested that syn-
tactic features (syntactic errors) might be useful fea-
tures, but only investigated this idea at a shallow
level by treating rare PoS bigrams as ungrammati-
cal structures.
Tsur and Rappoport (2007) replicated the work
of Koppel et al (2005) to investigate the hypothe-
sis that the choice of words in second language writ-
ing is highly influenced by the frequency of native
language syllables ? the phonology of the native
language. Approximating this by character bi-grams
alone, they managed to achieve a classification accu-
racy of 66%.
Native language is also amongst the characteris-
tics investigated in the task of authorship profiling
by Estival et al (2007), as well as other demographic
and personality characteristics. This study used a va-
riety of lexical and document structure features. For
the native language identification classification task,
their model yielded a reasonably high accuracy of
84%, but this was over a set of only three languages
(Arabic, English and Spanish) and against a most
frequent baseline of 62.9%.
Another related work is that of van Halteren
(2008), who used the Europarl corpus of parliamen-
tary speeches. In Europarl, one original language
is transcribed, and the others translated from it; the
task was to identify the original language. On the
basis of frequency counts of word-based n-grams,
surprisingly high classification accuracies within the
range of 87-97% were achieved across six languages
(English, German, French, Dutch, Spanish, and Ital-
ian). This turns out, however, to be significantly
influenced by the use of particular phrases used by
speakers of different languages in the parliamentary
context (e.g. the way Germans typically address the
chamber).
To our knowledge, Wong and Dras (2009) is the
only work that has investigated the usefulness of
syntactic features for the task of native language
identification. They first replicated the work of
Koppel et al (2005) with the three types of lex-
ical feature, namely function words, character n-
grams, and PoS bi-grams. They then examined the
literature on contrastive analysis (see Section 2.2),
from the field of second language acquisition, and
selected three syntactic errors commonly observed
in non-native English users?subject-verb disagree-
ment, noun-number disagreement and misuse of
determiners?that had been identified as being in-
fluenced by the native language. An ANOVA anal-
ysis showed that the native language identification
constructions were identifiable; however, the over-
all classification was not improved over the lexi-
cal features by using just the three manually de-
tected syntactic errors. The best overall accuracy re-
1601
ported was 73.71%; this was on the second version
of ICLE, across seven languages (those of Koppel
et al (2005), plus the two Asian languages Chinese
and Japanese).
As a possible approach that would improve the
classification accuracy over just the three manually
detected syntactic errors, Wong and Dras (2009)
suggested deploying (but did not carry out) an idea
put forward by Gamon (2004) (citing Baayen et al
(1996)) for the related task of identifying the author
of a text: to use CFG production rules to characterise
syntactic structures used by authors.1 We note that
similar ideas have been used in the task of sentence
grammaticality judgement, which utilise parser out-
puts (both trees and by-products) as classification
features (Mutton et al, 2007; Sun et al, 2007; Fos-
ter et al, 2008; Wagner et al, 2009; Tetreault et al,
2010; Wong and Dras, 2010). We combine this idea
with one we introduce in this paper, of using dis-
criminative reranking features as a broader charac-
terisation of the parse tree.
2.2 Contrastive analysis
Contrastive analysis (Lado, 1957) was an early at-
tempt in the field of second language acquisition
to explain the kinds and source of errors that non-
native speakers make. It arose out of behaviourist
psychology, and saw language learning as an issue
of habit formation that could be inhibited by previ-
ous habits inculcated in learning the native language.
The theory was also tied to structural linguistics:
it compared the syntactic structures of the native
and second languages to find differences that might
cause learning difficulties. The Lado work postu-
lated the Contrastive Analysis Hypothesis (CAH),
claiming that ?those elements which are similar to
[the learner?s] native language will be simple for
him, and those elements that are different will be
difficult?; the consequence is that there will be more
errors made in those difficult elements.
While contrastive analysis was influential at first,
it was increasingly noticed that many errors were
1It is not entirely clear how this might work for author-
ship identification: would the Bronte? sisters, the corpus Gamon
worked with, have used a significant number of different syntac-
tic constructions from each other? In the context of native lan-
guage identification, however, constrastive analysis postulates
that this is exactly the case for the different classes.
common across all language learners regardless of
native language, which could not be explained un-
der contrastive analysis. Corder (1967) then de-
scribed an alternative, error analysis, where con-
trastive analysis-style errors were seen as only one
type of error, ?interlanguage? or ?interference? er-
rors; other types were ?intralingual? and ?develop-
mental? errors, which are not specific to the native
language (Richards, 1971).
In an overview of contrastive analysis after the
emergence of error analysis, Wardhaugh (1970)
noted that there were two interpretations of the
CAH, termed the strong and weak forms. Under the
strong form, all errors were attributed to the native
language, and clearly that was not tenable in light of
error analysis evidence. In the weak form, these dif-
ferences have an influence but are not the sole deter-
minant of language learning difficulty. Wardhaugh
noted claims at the time that the hypothesis was no
longer useful in either the strong or the weak ver-
sion: ?Such a claim is perhaps unwarranted, but a
period of quiescence is probable for CA itself?. This
appears to be the case, with the then-dominant error
analysis giving way to newer, more specialised theo-
ries of second language acquisition, such as the com-
petition model of MacWhinney and Bates (1989)
or the processability theory of Pienemann (1998).
Nevertheless, smaller studies specifically of inter-
language errors have continued to be carried out,
generally restricted in their scope to a specific gram-
matical aspect of English in which the native lan-
guage of the learners might have an influence. To
give some examples, Granger and Tyson (1996) ex-
amined the usage of connectors in English by a num-
ber of different native speakers ? French, German,
Dutch, and Chinese; Vassileva (1998) investigated
the employment of first person singular and plural
by another different set of native speakers ? Ger-
man, French, Russian, and Bulgarian; Slabakova
(2000) explored the acquisition of telicity marking
in English by Spanish and Bulgarian learners; Yang
and Huang (2004) studied the impact of the ab-
sence of grammatical tense in Chinese on the acqui-
sition of English tense-aspect system (i.e. telicity
marking); Franck et al (2002) and Vigliocco et al
(1996) specifically examined the usage of subject-
verb agreement in English by French and Spanish,
respectively. There are also a few teaching resources
1602
for English language teachers that collate such phe-
nomena, such as that of Swan and Smith (2001).
NLP techniques and a probabilistic view of na-
tive language identification now let us revisit and
make use of the weak form of the CAH. Interlan-
guage errors, as represented by differences in parse
trees, may be characteristic of the native language
of a learner; we can use the occurrence of these to
come up with a revised likelihood of the native lan-
guage. In this paper, we use machine learning in a
prediction task as our approach to this.
3 Models
This section describes the three basic models inves-
tigated: the lexical model, based on Koppel et al
(2005), as the baseline; and then the two models that
exploit syntactic information. In Section 5 we look
at the performance of each model independently and
also in combination: to combine, we just concate-
nate feature vectors.
Lexical As Wong and Dras (2009), we replicate
the features of Koppel et al (2005) to produce our
LEXICAL model. These are of three types: function
words,2 character n-grams, and PoS n-grams. We
follow Wong and Dras (2009) in resolving some un-
clear issues from Koppel et al (2005). Specifically,
we use the same list of function words, left unspec-
ified in Koppel et al (2005), that were empirically
determined by Wong and Dras (2009) to be the best
of three candidates; we used character bi-grams, as
the best performing n-grams, although this also had
been left unspecified by Koppel et al (2005); and
we used the most frequently occurring PoS bi-grams
and tri-grams, obtained by using the Brill tagger pro-
vided in NLTK (Bird et al, 2009) being trained on
the Brown corpus. In total, there are 798 features
of this class with 398 function words, 200 most fre-
quently occurring character bi-grams, and 200 most
frequently occurring PoS bi-grams. Both function
words and PoS bi-grams have feature values of bi-
nary type; while for character bi-grams, the feature
value is the relative frequency. (These types of fea-
ture value are the best performing one for each lexi-
2As with most work in authorship profiling, only function
words are used, so that the result is not tied to a particular do-
main, and no clues are obtained from different topics that dif-
ferent authors might write about.
cal feature.)
We omitted the 250 rare bi-grams used by Koppel
et al (2005), as an ablative analysis showed that they
contributed nothing to classification accuracy.
Production Rules Under this model (PROD-
RULE), we take as features horizontal slices of parse
trees, in effect treating them as sets of CFG produc-
tion rules. Feature values are binary. We look at
all possible rules as features, but also present results
for subsets of features chosen using feature selec-
tion. For each language in our dataset, we identify
the n rules most characteristic of the language using
Information Gain (IG). For m classes, we use the
formulation of Yang and Pedersen (1997):
IG(r) = ??mi=1 Pr (ci) log Pr (ci)
+Pr (r)
?m
i=1 Pr (ci|r) log Pr (ci|r)
+Pr (r?)
?m
i=1 Pr (ci|r?) log Pr (ci|r?) (1)
We also investigated simple frequencies, fre-
quency ratios, and pointwise mutual information; as
in much other work, IG performed best, so we do not
present results for the others. Bi-normal separation
(Forman, 2003), often competitive with IG, is only
suitable for binary classification.
It is worth noting that the production rules being
used here are all non-lexicalised ones, except those
lexicalised with function words and punctuation, to
avoid topic-related clues.
Reranking Features As opposed to the horizontal
parse production rules, features used for discrimina-
tive reranking are cross-sections of parse trees that
might capture other aspects of ungrammatical struc-
tures. For these we use the 13 feature schemas de-
scribed in Charniak and Johnson (2005), which were
inspired by earlier work in discriminative estimation
techniques, such as Johnson et al (1999) and Collins
(2000). Examples of these feature schemas include
tuples covering head-to-head dependencies, preter-
minals together with their closest maximal projec-
tion ancestors, and subtrees rooted in the least com-
mon ancestor.
These feature schemas are not the only possible
ones?they were empirically selected for the spe-
cific purpose of augmenting the Charniak parser.
However, much subsequent work has tended to use
1603
these same features, albeit sometimes with exten-
sions for specific purposes (e.g. Johnson and Ural
(2010) for the Berkeley parser (Petrov et al, 2006),
Ng et al (2010) for the C&C parser (Clark and Cur-
ran, 2007)). We also use this standard set, specif-
ically the set of instantiated feature schemas from
the parser from Charniak and Johnson (2005) as
trained on the Wall Street Journal (WSJ), which
gives 1,333,837 potential features.
4 Experimental Setup
4.1 Data
We use the International Corpus of Learner English
(ICLE) compiled by Granger et al (2009) for the
precise purpose of studying the English writings of
non-native English learners from diverse countries.
All the contributors to the corpus are claimed to
possess similar English proficiency levels (ranging
from intermediate to advanced learners) and are in
the same age group (all in their twenties at the time
of corpus collection.) This was also the data used by
Koppel et al (2005) and Tsur and Rappoport (2007),
although where they used the first version of the cor-
pus, we use version 2.
Briefly, the first version contains 11 sub-corpora
of English essays contributed by second-year and
third-year university students of different native lan-
guage backgrounds (mostly European and Slavic
languages) ? Bulgarian, Czech, Dutch, Finnish,
French, German, Italian, Polish, Russian, Spanish,
and Swedish; the second version has been extended
to additional 5 other native languages (including
Asian languages) ? Chinese, Japanese, Norwegian,
Turkish, and Tswana.
As per Wong and Dras (2009), we examine seven
languages, namely Bulgarian, Czech, French, Rus-
sian, Spanish, Chinese, and Japanese. For each na-
tive language, we randomly select from amongst es-
says with length of 500-1000 words. For the purpose
of the present study, we have 95 essays per native
language. For the same reason as highlighted by
Wong and Dras (2009), we intentionally use fewer
essays as compared to Koppel et al (2005)3 with a
view to reserving more data for future work. We
divide these into training sets of 70 essays per lan-
3Koppel et al (2005) took all 258 texts per language from
ICLE Version 1 and evaluated using 10-fold cross valiadation.
guage, with a held-out test set of 25 essays per
language. There are 17,718 training sentences and
6,791 testing sentences.
4.2 Parsers
We use two parsers: the Stanford parser (Klein
and Manning, 2003) and the Charniak and John-
son (henceforth C&J) parser (Charniak and Johnson,
2005). Both are widely used, and produce relatively
accurate parses: the Stanford parser gets a labelled
f-score of 85.61 on the WSJ, and the C&J 91.09.
With the Stanford parser, there are 26,284 unique
parse production rules extractable from our ICLE
training set of 490 texts, while the C&J parser pro-
duces 27,705. For reranking, we use only the C&J
parser?since the parser stores these features during
parsing, we can use them directly as classification
features. On the ICLE training data, there are 6,230
features with frequency >10, and 19,659 with fre-
quency >5.
4.3 Classifiers
For our experiments we used a maximum entropy
(MaxEnt) machine learner, MegaM4 (fifth release)
by Hal Daume? III. (We also used an SVM for com-
parison, but the results were uniformly worse, and
degraded more quickly as number of features in-
creased, so we only report the MaxEnt results here).
The classifier is tuned to obtain an optimal classifi-
cation model.
4.4 Evaluation Methodology
Given our relatively small amount of data, we use k-
fold cross-validation, choosing k = 5. While testing
for statistical significance of classification results is
often not carried out in NLP, we do so here because
the quantity of data could raise questions about the
certainty of any effect. In an encyclopedic survey of
cross-validation in machine learning contexts, Re-
faeilzadeh et al (2009) note that there is as yet no
universal standard for testing of statistical signifi-
cance; and that while more sophisticated techniques
have been proposed, none is more widely accepted
than a paired t-test over folds. We therefore use this
paired t-test over folds, as formulated of Alpaydin
4MegaM is available on http://www.cs.utah.edu/
?hal/megam/.
1604
(2004). Under this cross-validation, 5 separate train-
ing feature sets are constructed, excluding the test
fold; 3 folds are used for training, 1 fold for tuning
and 1 fold for testing.
We also use a held-out test set for comparison,
as it is well-known that cross-validation can over-
estimate prediction error (Hastie et al, 2009). We
do not carry out significance testing here?with this
held-out test set size (n = 125), two models would
have to differ by a great deal to be significant. We
only use it as a check on the effect of applying to
completely new data.
5 Results
Table 1 presents the results for the three models in-
dividually under cross-validation. The first point
to note is that PROD-RULE, under both parsers,
is a substantial improvement over LEXICAL when
(non-lexicalised) parse rules together with rules lex-
icalised with function words are used (rows marked
with * in Table 1), with the largest difference as
much as 77.75% for PROD-RULE[both]* (n = all)
versus 64.29% for LEXICAL; these differences with
respect to LEXICAL are statistically significant. (To
give an idea, the paired t-test standard error for this
largest difference is 2.52%.) In terms of error reduc-
tion, this is over 30%.
There appears to be no difference according to the
parser used, regardless of their differing accuracy on
the WSJ. Using the selection metric for PROD-RULE
without rules lexicalised with function words pro-
duces results all around those for LEXICAL; using
fewer reranking features is worse as the quality of
RERANKING declines as feature cut-offs are raised.
Another, somewhat surprising point is that the
RERANKING results are also generally around those
of LEXICAL even though like PROD-RULE they are
also using cross-sections of the parse tree. We con-
sider there might be two possible reasons for this.
The first is that the feature schemas used were orig-
inally chosen for the specific purpose of augment-
ing the performance of the Charniak parser; perhaps
others might be more appropriate here. The second
is that we selected only those instantiated feature
schemas that occurred in the WSJ, and then applied
them to ICLE. As the WSJ is filled with predomi-
nantly grammatical text, perhaps those that were not
Features MaxEnt
LEXICAL (n = 798) 64.29
PROD-RULE[Stanford] (n = 1000) 65.72
PROD-RULE[Stanford]* (n = 1000) 74.08
PROD-RULE[Stanford]* (n = all) 74.49
PROD-RULE[C&J] (n = 1000) 62.25
PROD-RULE[C&J]* (n = 1000) 71.84
PROD-RULE[C&J]* (n = all) 71.63
PROD-RULE[both] (n = 2000) 67.96
PROD-RULE[both]* (n = 2000) 74.69
PROD-RULE[both]* (n = all) 77.75
RERANKING (all features) 67.96
RERANKING (>5 counts) 66.33
RERANKING (>10 counts) 64.90
Table 1: Classification results based on 5-fold cross vali-
dation with parse rules as syntactic features (accuracy %)
Features MaxEnt
Lexical features (n = 798) 75.43
PROD-RULE[Stanford] (n = 1000) 74.29
PROD-RULE[Stanford]* (n = 1000) 79.43
PROD-RULE[Stanford]* (n = all) 78.86
PROD-RULE[C&J] (n = 1000) 73.71
PROD-RULE[C&J] (n = 1000)* 79.43
PROD-RULE[C&J] (n = all)* 80.00
PROD-RULE[both] (n = 2000) 77.71
PROD-RULE[both] (n = 2000)* 78.85
PROD-RULE[both] (n = all)* 80.00
RERANKING (all features) 77.14
RERANKING (>5 counts) 76.57
RERANKING (>10 counts) 75.43
Table 2: Classification results based on hold-out valida-
tion with parse rules as syntactic features (accuracy %)
seen on the WSJ are precisely those that might indi-
cate ungrammaticality. In contrast, the production
rules of PROD-RULE were selected only from the
ICLE training data.
Table 2 presents the results for the individual
models on the held-out test set. The results are gen-
erally higher than for cross-validation?this is not
surprising, as the texts are of the same type, but all
the training data is used (rather than the 1?1/k pro-
portion for cross-validation). Overall, the pattern is
still the same, with PROD-RULE best, then RERANK-
ING and LEXICAL broadly similar; as expected, no
differences are significant with this smaller dataset.
The gap has narrowed, but without significance test-
1605
Features MaxEnt
LEXICAL (n = 798) 64.29
LEXICAL + PROD-RULE[both] (n = 2000) 63.06
LEXICAL + PROD-RULE[both]* (n = 2000) 72.45
LEXICAL + PROD-RULE[both]* (n = all) 70.82
LEXICAL + RERANKING (n = all) 68.17
Table 3: Classification results based on 5-fold cross vali-
dation for combined models (accuracy %)
Features MaxEnt
LEXICAL (n = 798) 75.43
LEXICAL + PROD-RULE[both] (n = 2000) 80.57
LEXICAL + PROD-RULE[both]* (n = 2000) 81.14
LEXICAL + PROD-RULE[both]* (n = all) 81.71
LEXICAL + RERANKING (n = all) 76.00
Table 4: Classification results based on hold-out valida-
tion for combined models (accuracy %)
ing it is difficult to say whether this is a genuine
phenomenon. The accuracy rate for LEXICAL here
is in line with Wong and Dras (2009); and given
the smaller dataset and larger set of languages, also
broadly in line with Koppel et al (2005).
Tables 3 and 4 present results for model combina-
tions. It can be seen that the model combinations do
not produce results better than PROD-RULE alone.
Combining all features (results not presented here)
seems to degrade the overall performance even of
the MegaM: perhaps we need to derive feature vec-
tors more compactly than by feature concatenation.
6 Discussion
As illustrated in the confusion matrices (Table 5
for the PROD-RULE model, and Table 6 for the
LEXICAL model), misclassifications occur largely in
Spanish and Slavic languages, Bulgarian and Rus-
sian in particular. Unsurprisingly, Chinese is al-
most completely identified since it comes from a
entirely different language family, Sino-Tibetan, as
compared to the rest of the languages which are from
the branches of the Indo-European family (with
Japanese as the exception). Japanese and French
also appear to be easily distinguished, which could
probably be attributed to their word order or sen-
tence structure which are, to some extent, quite dif-
ferent from English. Japanese is a ?subject-object-
verb? language; and French, although having the
same word order as English, heads of phrases in
BL CZ FR RU SP CN JP
BL [14] 6 2 3 - - -
CZ 1 [20] - 3 1 - -
FR - - [25] - - - -
RU 1 4 3 [17] - - -
SP 2 1 3 1 [18] - -
CN - - - - - [24] 1
JP - - - - 1 2 [22]
Table 5: Confusion matrix based on all non-lexicalised
parse rules from both parsers on the held-out set
(BL:Bulgarian, CZ:Czech, FR:French, RU:Russian,
SP:Spanish, CN:Chinese, JP:Japanese)
BL CZ FR RU SP CN JP
BL [14] 3 2 4 2 - -
CZ 6 [16] - 2 1 - -
FR 1 - [24] - - - -
RU 3 2 3 [16] 1 - -
SP 1 2 3 1 [17] - 1
CN - - - - - [24] 1
JP - - - - 1 3 [21]
Table 6: Confusion matrix based on lexical features on
the held-out set (BL:Bulgarian, CZ:Czech, FR:French,
RU:Russian, SP:Spanish, CN:Chinese, JP:Japanese)
French typically come before modifiers as opposed
to English. Overall, the PROD-RULE model results
in fewer misclassifications compared to the LEXI-
CAL model; there are mostly only incremental im-
provements for each language, with perhaps the ex-
ception of the reduction in confusion in the Slavic
languages.
We looked at some of the data, to see what kind
of syntactic substructure is useful in classifying na-
tive language. Although using feature selection with
only 1000 features did not improve performance,
the information gain ranking does identify particu-
lar constructions as characteristic of one of the lan-
guages, and so are useful for inspection.
A phenomenon that the literature has noted as oc-
curring with Chinese speakers is that of the missing
determiner.5 This corresponds to a higher frequency
of NP rules without determiners. These rules may
be valid in other contexts, but are also used to de-
scribe ungrammatical constituents. One example is
5This does happen with native speakers of some other lan-
guages, such as Slavic ones, but not generally (from our knowl-
edge of the literature) with native speakers of others, such as
Romance ones.
1606
Rules Counts
BL CZ FR RU SP CN JP
NNP ? <R> 0 0 3 0 0 67 0
: ? - 55 51 23 39 10 9 4
PRN ? -LRB- X -RRB- 0 1 7 2 0 42 0
SYM ? * 0 1 7 3 1 42 0
: ? : 30 39 58 46 47 11 6
X ? SYM 0 2 7 4 4 42 6
NP ? NNP NNP NNS 0 3 1 0 0 31 0
S ? S : S . 36 34 53 39 41 5 9
PP ? VBG PP 9 15 16 12 13 54 13
: ? ... 16 13 39 11 24 1 3
Table 7: Top 10 rules for the Stanford parser according to Information Gain on the held-out set
(ROOT
(S
(NP
(NP (DT The) (NN development))
(PP (IN of)
(NP (NN country) (NN park))))
(VP (MD can)
(ADVP (RB directly))
(VP (VB elp)
(S
(VP (TO to)
(VP (VB alleviate)
(NP (NNS overcrowdedness)
(CC and)
(NN overpopulation))
(PP (IN in)
(NP (JJ urban)
(NN area))))))))
(. .)))
Figure 1: Parse from Chinese-speaking authors, illustrat-
ing missing determiner
(ROOT
(S
(PP (VBG According)
(PP (TO to)
(NP (NNP <R>))))
(, ,)
(NP
(NP (NN burning))
(PP (IN of)
(NP (JJ plastic)
(NN waste))))
(VP (VBZ generates)
(NP (JJ toxic)
(NNS by-products)))
(. .)))
Figure 2: Parse from Chinese-speaking authors, illustrat-
ing according to
NP ? NN NN. In Figure 1 we give the parse (from
the Stanford parser) of the sentence The develop-
ment of country park can directly elp to alleviate
overcrowdedness and overpopulation in urban area.
The phrase country park should either have a deter-
miner or be plural (in which case the appropriate rule
would be NP ? NN NNS). There is a similar phe-
nomenon with in urban area, although this is an in-
stance of the rule NP ? JJ NN.
Another production rule that occurs typically?
in fact, almost exclusively?in the texts of native
Chinese speakers is PP ? VBG PP (by the Stan-
ford parser), which almost always corresponds to the
phrase according to. In Figure 2 we give the parse
of a short sentence (According to <R>, burning of
1607
(S1
(S
(ADVP (RB Overall))
(, ,)
(NP (NNP cyber))
(VP (VBD cafeis)
(NP (DT a) (JJ good) (NN place))
(PP (IN as)
(NP (JJ recreational)
(NNP centre)))
(PP (IN with)
(NP
(NP
(DT a) (NN bundle))
(PP (IN of)
(NP (JJ up-to-dated)
(NN information))))))
(. .)))
Figure 3: Parse illustrating parser correction
plastic waste generates toxic by-products?<R>is
an in-text citation that was removed in the prepa-
ration of ICLE) that illustrates this particular con-
struction. It appears that speakers of Chinese fre-
quently use this phrase as a translation of ge?n ju`.
So in this case, what is identified is not the sort of
error that is of interest to contrastive analysis, but
just a particular construction that is characteristic of
a certain native speaker?s language, one that is per-
fectly grammatical but which is used relatively infre-
quently by others and has a slightly unusual analysis
by the parser.
We had expected to see more rules that displayed
obvious ungrammaticality, such as VP ? DT IN.
However, both parsers appear to be good at ?ig-
noring? errors, and producing relatively grammati-
cal structures (albeit ones with different frequencies
for different native languages). Figure 3 gives the
C&J parse for Overall, cyber cafeis a good place as
recreational centre with a bundle of up-to-dated in-
formation. The correction of up-to-dated rather than
up-to-date is straightforward, but the simple typo-
graphical error of running together cafe and is leads
to more complex problems for the parser. Neverthe-
less, the parser produces a solid grammatical tree,
specifically assigning the category VBD to the com-
pound cafeis. This appears to be because both the
Stanford and C&J parsers have implicit linguistic
constraints such as assumptions about heads; these
are imposed even when the text does not provide ev-
idence for them.
We also present in Table 7 the top 10 rules chosen
under the IG feature selection for the Stanford parser
on the held-out set. A number of these, and those
ranked lower, are concerned with punctuation: these
seem unlikely to be related to native language, but
perhaps rather to how students of a particular lan-
guage background are taught. Others are more typi-
cal of the sorts of example we illustrated above: PP
? VBG PP, for example, is typically connected to
the according to construction discussed in connec-
tion with Figure 2, and it can be seen that the dom-
inant frequency count there is for native Chinese
speakers (column 6 of the counts).
7 Conclusion
In this paper we have shown that, using cross-
sections of parse trees, we can improve above an al-
ready good baseline in the task of native language
identification. While we do not make any strong
claims for the Contrastive Analysis Hypothesis, the
usefulness of syntax in the context of this problem
does provide some support.
The best features arising from the classification
have been horizontal cross-sections of trees, rather
than the more general discriminative parse reranking
features that might have been expected to perform at
least as well. This relatively poorer performance by
the reranking features may be due to a number of
factors, all of which could be investigated in future
work. One is the use of feature schema instances that
did not appear in the largely grammatical WSJ; an-
other is the extension of feature schemas; and a third
is the use of a parser that does not enforce linguistic
constraints such as the Berkeley parser (Petrov et al,
2006).
Examining some of the substructures showed
some errors that were expected; other constructions
that were grammatical, but were just characteris-
tic translations of constructions that were common
in the native language; and a large number where
grammatical errors were glossed over by the parser?s
linguistic constraints, suggesting another purpose
for further work with the Berkeley parser. Overall,
the use of these led to an error reduction in over 30%
1608
in the cross-validation evaluation with significance
testing.
Acknowledgments
The authors would like to acknowledge the support
of ARC Linkage Grant LP0776267 and ARC Dis-
covery Grant DP1095443, and thank the reviewers
for useful feedback. Much gratitude is due to Mark
Johnson for his guidance on the extraction of rerank-
ing features.
References
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. MIT Press, Cambridge, MA, USA.
Harald Baayen, Hans van Halteren, and Fiona Tweedie.
1996. Outside the Cave of Shadows: Using Syntactic
Annotation to Enhance Authorship Attribution. Liter-
ary and Linguistic Computing, 11(3):121?131.
Stephen Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python: Analyzing
Text with the Natural Language Toolkit. O?Reilly Me-
dia, Inc.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180, Ann Arbor, Michigan.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML?00), Stanford, CA.
Stephen P. Corder. 1967. The significance of learners?
errors. International Review of Applied Linguistics in
Language Teaching (IRAL), 5(4):161?170.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263?272.
Ian Fette, Norman Sadeh, and Anthony Tomasic. 2007.
Learning to detect phishing emails. In Proceedings of
the 16th International World Wide Web Conference.
George Forman. 2003. An extensive empirical study of
feature selection metrics for text classification. Jour-
nal of Machine Learning Research, 3:1289?1305.
Jennifer Foster, JoachimWagner, and Josef van Genabith.
2008. Adapting a WSJ-trained parser to grammati-
cally noisy text. In Proceedings of ACL-08: HLT,
Short Papers, pages 221?224, Columbus, Ohio.
Julie Franck, Gabriella Vigliocco, and Janet Nicol. 2002.
Subject-verb agreement errors in French and English:
The role of syntactic hierarchy. Language and Cogni-
tive Processes, 17(4):371?404.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analy-
sis features. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING),
pages 611?617.
Sylviane Granger and Stephanie Tyson. 1996. Connec-
tor usage in the English essay writing of native and
non-native EFL speakers of English. World Englishes,
15(1):17?27.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses Universitaires de
Louvain, Louvian-la-Neuve.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. Springer.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Proceed-
ings of Human Language Technologies: the 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL-10), pages 665?668, Los Angeles, CA,
USA, June.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
unification-based grammars. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?99), College Park, MD.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. In Intelligence and Security Informat-
ics, volume 3495 of Lecture Notes in Computer Sci-
ence, pages 209?217. Springer-Verlag.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. University
of Michigan Press, Ann Arbor, MI, US.
Brian MacWhinney and Elizabeth Bates. 1989. The
Crosslinguistic Study of Sentence Processing. Cam-
bridge University Press, New York, NY, USA.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. GLEU: Automatic evaluation of
1609
sentence-level fluency. In Proceedings of the 45th An-
nual Meeting of the Association of Computational Lin-
guistics, pages 344?351, Prague, Czech Republic.
Steven Myers. 2007. Introduction to phishing. In
Markus Jakobsson and Steven Myers, editors, Phish-
ing and Countermeasures: Understanding the In-
creasing Problem of Electronic Identity Theft. John
Wiley & Sons, Inc., Hoboken, NJ, USA.
Dominick Ng, Matthew Honnibal, and James R. Cur-
ran. 2010. Reranking a Wide-Coverage CCG Parser.
In Proceedings of Australasian Language Technology
Association Workshop (ALTA?10), pages 90?98, Mel-
bourne, Australia.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL?06), pages
433?440, Sydney, Australia, July.
Manfred Pienemann. 1998. Language Processing and
Second Language Development: Processability The-
ory. John Benjamins, Amsterdam, The Netherlands.
Payam Refaeilzadeh, Lei Tang, and Huan Liu. 2009.
Cross-validation. In Ling Liu and M. Tamer O?zsu, ed-
itors, Encyclopedia of Database Systems, pages 532?
538. Springer, US.
Jack C. Richards. 1971. A non-contrastive approach to
error analysis. ELT Journal, 25(3):204?219.
Roumyana Slabakova. 2000. L1 transfer revisited:
the L2 acquisition of telicity marking in English by
Spanish and Bulgarian native speakers. Linguistics,
38(4):739?770.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 81?88, Prague, Czech Repub-
lic.
Michael Swan and Bernard Smith, editors. 2001.
Learner English: A teacher?s guide to interference and
other problems. Cambridge University Press, 2nd edi-
tion.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the ACL 2010
Conference Short Papers, ACLShort ?10, pages 353?
358. Association for Computational Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9?16.
Hans van Halteren. 2008. Source language markers in
EUROPARL translations. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING), pages 937?944.
Irena Vassileva. 1998. Who am I/how are we in aca-
demic writing? A contrastive analysis of authorial
presence in English, German, French, Russian and
Bulgarian. International Journal of Applied Linguis-
tics, 8(2):163?185.
Garbriella Vigliocco, Brian Butterworth, and Merrill F.
Garrett. 1996. Subject-verb agreement in Spanish
and English: Differences in the role of conceptual con-
straints. Cognition, 61(3):261?298.
JoachimWagner, Jennifer Foster, and Josef van Genabith.
2009. Judging grammaticality: Experiments in sen-
tence classification. CALICO Journal, 26(3):474?490.
Richard Wardhaugh. 1970. The Contrastive Analysis
Hypothesis. TESOL Quarterly, 4(2):123?130.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification. In
Proceedings of the Australasian Language Technology
Association Workshop 2010, pages 67?75, Melbourne,
Australia, December.
Suying Yang and Yue-Yuan Huang. 2004. The impact of
the absence of grammatical tense in L1 on the acqui-
sition of the tense-aspect system in L2. International
Review of Applied Linguistics in Language Teaching
(IRAL), 42(1):49?70.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the Fourteenth International Con-
ference on Machine Learning (ICML?97), pages 412?
420.
Rong Zheng, Yi Qin, Zan Huang, and Hsinchun Chen.
2003. Authorship analysis in cybercrime investiga-
tion. In Intelligence and Security Informatics, volume
2665 of Lecture Notes in Computer Science, pages 59?
73. Springer-Verlag.
1610
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 699?709, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Exploring Adaptor Grammars for Native Language Identification
Sze-Meng Jojo Wong Mark Dras Mark Johnson
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
{sze.wong,mark.dras,mark.johnson}@mq.edu.au
Abstract
The task of inferring the native language of
an author based on texts written in a second
language has generally been tackled as a clas-
sification problem, typically using as features
a mix of n-grams over characters and part of
speech tags (for small and fixed n) and un-
igram function words. To capture arbitrar-
ily long n-grams that syntax-based approaches
have suggested are useful, adaptor grammars
have some promise. In this work we investi-
gate their extension to identifying n-gram col-
locations of arbitrary length over a mix of PoS
tags and words, using both maxent and in-
duced syntactic language model approaches to
classification. After presenting a new, simple
baseline, we show that learned collocations
used as features in a maxent model perform
better still, but that the story is more mixed for
the syntactic language model.
1 Introduction
The task of inferring the native language of an author
based on texts written in a second language ? na-
tive language identification (NLI) ? has, since the
seminal work of Koppel et al2005), been primarily
tackled as a text classification task using supervised
machine learning techniques. Lexical features, such
as function words, character n-grams, and part-of-
speech (PoS) n-grams, have been proven to be use-
ful in NLI (Koppel et al2005; Tsur and Rappoport,
2007; Estival et al2007). The recent work of Wong
and Dras (2011), motivated by ideas from Second
Language Acquisition (SLA), has shown that syn-
tactic features ? potentially capturing syntactic er-
rors characteristic of a particular native language ?
improve performance over purely lexical ones.
PoS n-grams can be leveraged to characterise sur-
face syntactic structures: in Koppel et al2005),
for example, ungrammatical structures were approx-
imated by rare PoS bigrams. For the purpose of NLI,
small n-gram sizes like bigram or trigram might not
suffice to capture sequences that are characteristic of
a particular native language. On the other hand, an
attempt to represent these with larger n-grams would
not just lead to feature sparsity problems, but also
computational efficiency issues. Some form of fea-
ture selection should then come into play.
Adaptor grammars (Johnson, 2010), a hierarchi-
cal non-parametric extension of PCFGs (and also in-
terpretable as an extension of LDA-based topic mod-
els), hold out some promise here. In that initial
work, Johnson?s model learnt collocations of arbi-
trary length such as gradient descent and cost func-
tion, under a topic associated with machine learning.
Hardisty et al2010) applied this idea to perspective
classification, learning collocations such as pales-
tinian violence and palestinian freedom, the use of
which as features was demonstrated to help the clas-
sification of texts from the Bitter Lemons corpus as
either Palestinian or Israeli perspective.
Typically in NLI and other authorship attribu-
tion tasks, the feature sets exclude content words,
to avoid unfair cues due to potentially different do-
mains of discourse. In our context, then, what we are
interested in are ?quasi-syntactic collocations? of ei-
ther pure PoS (e.g. NN IN NN) or a mixture of PoS
with function words (e.g. NN of NN). The partic-
ular question of interest for this paper, then, is to
699
investigate whether the power of adaptor grammars
to discover collocations ? specifically, ones of ar-
bitrary length that are useful for classification ? ex-
tends to features beyond the purely lexical.
We examine two different approaches in this pa-
per. We first utilise adaptor grammars for discovery
of high performing ?quasi-syntactic collocations? of
arbitrary length as mentioned above and use them
as classification features in a conventional maximum
entropy (maxent) model for identifying the author?s
native language. In the second approach, we adopt
a grammar induction technique to learn a grammar-
based language model in a Bayesian setting. The
grammar learned can then be used to infer the most
probable native language that a given text written
in a second language is associated with. The latter
approach is actually closer to the work of Hardisty
et al2010) using adaptor grammars for perspec-
tive modeling, which inspired our general approach.
This alternative approach is also similar in nature
to the work of Bo?rschinger et al2011) in which
grounded learning of semantic parsers was reduced
to a grammatical inference task.
The structure of the paper is as follows. In Sec-
tion 2, we review the existing work of NLI as well
as the mechanics of adaptor grammars along with
their applications to classification. Section 3 details
the supervised maxent classification of NLI with
collocation (n-gram) features discovered by adaptor
grammars. The language model-based classifier is
described in Section 4. Finally, we present a dis-
cussion in Section 5 and follow with concluding re-
marks.
2 Related Work
2.1 Native Language Identification
Most of the existing research treats the task of na-
tive language identification as a form of text classi-
fication deploying supervised machine learning ap-
proaches.
The earliest notable work in this classification
paradigm is that of Koppel et al2005) using as
features function words, character n-grams, and PoS
bigrams, together with some spelling errors. Their
experiments were conducted on English essays writ-
ten by authors whose native language one of Bulgar-
ian, Czech, French, Russian, or Spanish. The cor-
pus used is the first version of International Corpus
of Learner English (ICLE). Apart from investigating
lexical features, syntactic features (errors in particu-
lar) were highlighted by Koppel et al2005) as po-
tentially useful features, but they only explored this
by characterising ungrammatical structures with rare
PoS bigrams: they chose 250 rare bigrams from the
Brown corpus.
Features for this task can include content words
or not: Koppel et al2009), in reviewing work in
the general area of authorship attribution (including
NLI), discuss the (perhaps unreasonable) advantage
that content word features can provide, and com-
ment that consequently they ?are careful . . . to dis-
tinguish results that exploit content-based features
from those that do not?. We will not be using con-
tent words as features; we therefore note only ap-
proaches to NLI that similarly do not use them.
Following Koppel et al2005), Tsur and Rap-
poport (2007) replicated their work and hypothe-
sised that word choices in second language writing
is highly influenced by the frequency of native lan-
guage syllables. They investigated this through mea-
suring classification performance with only charac-
ter bigrams as features.
Estival et al2007) tackled the broader task of
developing profiles of authors, including native lan-
guage and various other demographic and psycho-
metric author traits, across a smaller set of languages
(English, Spanish and Arabic). To this end, they de-
ployed various lexical and document structure fea-
tures.
Wong and Dras (2011), starting from the Kop-
pel et al2005) approach, explored the usefulness
of syntactic features in a broader sense in which
they characterised syntactic errors with cross sec-
tions of parse trees obtained from statistical parsers,
both horizontal slices of the parse trees in the form
of CFG production rules, and the feature schemata
used in discriminative parse reranking (Charniak
and Johnson, 2005); they also found that using the
top 200 PoS bigrams helped. Their results on the
second version of the ICLE corpus, across seven
languages (those of Koppel et alplus two Orien-
tal languages, Chinese and Japanese) demonstrated
that syntactic features of these kinds lead to signifi-
cantly better performance than the Koppel et alea-
tures alone, with a top accuracy (on 5-fold cross-
validation) of 77.75%.
700
Subsequently, Wong et al2011) explored
Bayesian topic modeling (Blei et al2003; Griffiths
and Steyvers, 2004) as a form of feature dimension-
ality reduction technique to discover coherent latent
factors (?topics?) that might capture predictive fea-
tures for individual native languages. Their topics,
rather than the typical word n-grams, consisted of
bigrams over (only) PoS. However, while there was
some evidence of topic cluster coherence, this did
not improve classification performance.
The work of the present paper differs in that it
uses Bayesian techniques to discover collocations of
arbitrary length for use in classification, over a mix
of both PoS and function words, rather than for use
as feature dimensionality reduction.
2.2 Adaptor Grammars
Adaptor Grammars are a non-parametric extension
to PCFGs that are associated with a Bayesian in-
ference procedure. Here we provide an informal
introduction to Adaptor Grammars; Johnson et al
(2007) provide a definition of Adaptor Grammars as
a hierarchy of mixtures of Dirichlet (or 2-parameter
Poisson-Dirichlet) Processes to which the reader
should turn for further details.
Adaptor Grammars can be viewed as extending
PCFGs by permitting the grammar to contain an
unbounded number of productions; they are non-
parametric in the sense that the particular produc-
tions used to analyse a corpus depends on the cor-
pus itself. Because the set of possible productions
is unbounded, they cannot be specified by simply
enumerating them, as is standard with PCFGs. In-
stead, the productions used in an adaptor gram-
mar are specified indirectly using a base grammar:
the subtrees of the base grammar?s ?adapted non-
terminals? serve as the possible productions of the
adaptor grammar (Johnson et al2007), much in
the way that subtrees function as productions in Tree
Substitution Grammars .1
Another way to view Adaptor Grammars is that
they relax the independence assumptions associated
with PCFGs. In a PCFG productions are gener-
ated independently conditioned on the parent non-
terminal, while in an Adaptor Grammar the proba-
bility of generating a subtree rooted in an adapted
1For computational efficiency reasons Adaptor Grammars
require the subtrees to completely expand to terminals. The
Fragment Grammars of O?Donnell (2011) lift this restriction.
non-terminal is roughly proportional to the number
of times it has been previously generated (a certain
amount of mass is reserved to generate ?new? sub-
trees). This means that the distribution generated by
an Adaptor Grammar ?adapts? based on the corpus
being generated.
2.2.1 Mechanics of adaptor grammars
Adaptor Grammars are specified by a PCFG G,
plus a subset of G?s non-terminals that are called
the adapted non-terminals, as well as a discount
parameter aA, where 0 ? aA < 1 and a con-
centration parameter bA, where b > ?a, for each
adapted non-terminal A. An adaptor grammar de-
fines a two-parameter Poisson-Dirichlet Process for
each adapted non-terminal A governed by the pa-
rameters aA and bA. For computational purposes it
is convenient to integrate out the Poisson-Dirichlet
Process, resulting in a predictive distribution spec-
ified by a Pitman-Yor Process (PYP). A PYP can
be understood in terms of a ?Chinese Restaurant?
metaphor in which ?customers? (observations) are
seated at ?tables?, each of which is labelled with a
sample from a ?base distribution? (Pitman and Yor,
1997).
In an Adaptor Grammar, unadapted non-terminals
expand just as they do in a PCFG; a production r ex-
panding the non-terminal is selected according to the
multinomial distribution ?r over productions speci-
fied in the grammar. Each adapted non-terminalA is
associated with its own Chinese Restaurant, where
the tables are labelled with subtrees generated by
the grammar rooted in A. In the Chinese Restau-
rant metaphor, the customers are expansions of A,
each table corresponds to a particular subtree ex-
panding A, and the PCFG specifies the base distri-
bution for each of the adapted non-terminals. An
adapted non-terminal A expands as follows. A ex-
pands to a subtree t with probability proportional to
nt, where nt is the number of times t has been pre-
viously generated. In addition, A expands using a
PCFG rule r expanding A with probability propor-
tional to (mA aA + bA) ?r, where mA is the number
of subtrees expanding A (i.e., the number of tables
in A?s restaurant). Because the underlying Pitman-
Yor Processes have a ?rich get richer? property, they
generate power-law distributions over the subtrees
for adapted non-terminals.
701
2.2.2 Adaptor grammars as LDA extension
With the ability to rewrite non-terminals to en-
tire subtrees, adaptor grammars have been used to
extend unigram-based LDA topic models (Johnson,
2010). This allows topic models to capture se-
quences of words with abitrary length rather than
just unigrams of word. It has also been shown that it
is crucial to go beyond the bag-of-words assump-
tion as topical collocations capture more meaning
information and represent more interpretable topics
(Wang et al2007).
Taking the PCFG formulation for the LDA topic
models, it can be modified such that each topic
Topici generates sequences of words by adapting
each of the Topici non-terminals (usually indicated
with an underline in an adaptor grammar). The over-
all schema for capturing topical collocations with an
adaptor grammar is as follows:
Sentence? Docj j ? 1, . . . ,m
Docj ? j j ? 1, . . . ,m
Docj ? Docj Topici i ? 1, . . . , t;
j ? 1, . . . ,m
Topici ?Words i ? 1, . . . , t
Words?Word
Words?Words Word
Word? w w ? V
There is a non-grammar-based approach to find-
ing topical collocations as demonstrated by Wang et
al. (2007). Both of these approaches learned use-
ful collocations: for instance, as mentioned in Sec-
tion 1, Johnson (2010) found collocations such gra-
dient descent and cost function associated with the
topic of machine learning; Wang et al2007) found
the topic of human receptive system comprises of
collocations such as visual cortext and motion de-
tector.
Adaptor grammars have also been deployed as a
form of feature selection in discovering useful collo-
cations for perspective classification. Hardisty et al
(2010) argued that indicators of perspectives are of-
ten beyond the length of bigrams and demonstrated
that the use of the adaptor grammar inferred n-grams
of arbitrary length as features establishes the start-
of-the-art performance for perspective classification
on the Bitter Lemons corpus, depicting two differ-
ent perspectives of Israeli and Pelestinian. We are
adopting a similar approach in this paper for classi-
fying texts with respect to the author?s native lan-
guage; but the key difference with Hardisty et al
(2010)?s approach is that our focus is on collocations
that mix PoS and lexical elements, rather than being
purely lexical.
3 Maxent Classification
In this section, we first explain the procedures taken
to set up the conventional supervised classification
task for NLI through the deployment of adaptor
grammars for discovery of ?quasi-syntactic colloca-
tions? of arbitrary length. We then present the classi-
fication results attained based on these selected sets
of n-gram features. In all of our experiments, we
investigate two sets of collocations: pure PoS and
a mixture of PoS and function words. The idea of
examining the latter set is motivated by the results
of Wong and Dras (2011) where inclusion of parse
production rules lexicalised with function words as
features had shown to improve the classification per-
formance relative to unlexicalised ones.
3.1 Experimental Setup
3.1.1 Data and evaluation
The classification experiments are conducted on
the second version of ICLE (Granger et al2009).2
Following our earlier NLI work in Wong and Dras
(2011), our data set consists of 490 texts written
in English by authors of seven different native lan-
guage groups: Bulgarian, Czech, French, Russian,
Spanish, Chinese, and Japanese. Each native lan-
guage contributes 70 out of the 490 texts. As we are
using a relative small data set, we perform k-fold
cross-validation, choosing k = 5.
3.1.2 Adaptor grammars for supervised
classification
We derive two adaptor grammars for the maxent
classification setting, where each is associated with
a different set of vocabulary (i.e. either pure PoS
or the mixture of PoS and function words). We use
2Joel Tetreault and Daniel Blanchard from ETS have pointed
out (personal communication) that there is a subtle issue with
ICLE that could have an impact on the classification perfor-
mance of NLI tasks; in particular, when character n-grams are
used as features, some special characters used in some ICLE
texts might affect performance. For our case, this should not be
of much issue since they will not appear in our collocations.
702
the grammar of Johnson (2010) as presented in Sec-
tion 2.2.2, except that the vocabulary differs: either
w ? Vpos or w ? Vpos+fw. For Vpos, there are
119 distinct PoS tags based on the Brown tagset.
Vpos+fw is extended with 398 function words as per
Wong and Dras (2011). m = 490 is the number of
documents, and t = 25 the number of topics (chosen
as the best performing one from Wong et al2011)).
Rules of the form Docj ? Docj Topici that
encode the possible topics that are associated with
a document j are given similar ? priors as used
in LDA (? = 5/t where t = 25 in our experi-
ments). Likewise, similar ? priors from LDA are
placed on the adapted rules expanding from Topici
? Words, representing the possible sequences of
words that each topic comprises (? = 0.01).3 The
inference algorithm for the adaptor grammars are
based on the Markov Chain Monte Carlo technique
made available online by Johnson (2010).4
3.1.3 Classification models with n-gram
features
Based on the two adaptor grammars inferred, the
resulting collocations (n-grams) are extracted as fea-
tures for the classification task of identifying au-
thors? native language. These n-grams found by the
adaptor grammars are only a (not necessarily proper)
subset of those n-grams that are strongly characteris-
tic of a particular native language. In principle, one
could find all strongly characteristic n-grams by enu-
merating all the possible instances of n-grams up to
a given length if the vocabulary is of a small enough
closed set, such as for PoS tags, but this is infeasi-
ble when the set is extended to PoS plus function
words. The use of adaptor grammars here can be
viewed as a form of feature selection, as in Hardisty
et al2010).
Baseline models To serve as a baseline, we take
the commonly used PoS bigrams as per the previ-
ous work of NLI (Koppel et al2005). A set of
200 PoS bigrams is selected in two ways: the 200
most frequent in the training data (as in Wong and
Dras (2011)) and the 200 with the highest informa-
tion gain (IG) values in the training data (not evalu-
3The values of ? and ? are also based on the established
values presented in Wong et al2011).
4Adaptor grammar software is available on http://web.
science.mq.edu.au/?mjohnson/Software.htm.
ated in other work).
Enumerated n-gram models Here, we enumer-
ate all the possible n-grams up to a fixed length and
select the best of these according to IG, as a general-
isation of the baseline. The first motivation for this
feature set is that, in a sense, this should give a rough
upper bound for the adaptor grammar?s PoS-alone n-
grams, as these latter should most often be a subset
of the former. The second motivation is that it gives
a robust comparison for the mixed PoS and function
word n-grams, where it is infeasible to enumerate all
of them.
ENUM-POS We enumerate all possible n-grams up
to the length of 5, and select those that actually
occur (i.e. of the
?5
i=1 119
i possible n-grams,
this is 218,042 based on the average of 5 folds).
We look at the top n-grams up to length 5 selected
by IG: the top 2,800 and the top 6,500 (for com-
parability with adaptor grammar feature sets, be-
low), as well as the top 10,000 and the top 20,000
(to study the effect of larger feature space).
Adaptor grammar n-gram models The classifi-
cation features are the two sets of selected colloca-
tions inferred by the adaptor grammars which are the
main interest of this paper.
AG-POS This first set of the adaptor grammar-
inferred features comprise of pure PoS n-grams
(i.e. Vpos). The largest length of n-gram found
is 17, but about 97% of the collocations are of
length between 2 to 5. We investigate three vari-
ants of this feature set: top 200 n-grams of all
lengths (based on IG), all n-grams of all lengths
(n = 2, 795 on average), and all n-grams up to
the length of 5 (n = 2, 710 on average).
AG-POS+FW This second set of the adaptor
grammar-inferred features are mixtures of PoS
and function words (i.e. Vpos+fw). The largest
length of n-gram found for this set is 19 and
the total number of different collocations found
is much higher. For the purpose of comparabil-
ity with the first set of adaptor grammar features,
we investigate the following five variants for this
feature set: top 200 n-grams of all lengths, all n-
grams of all lengths (n = 6, 490 on average), all
n-grams up to the length of 5 (n = 6, 417 on av-
erage), top 2,800 n-grams of all different lengths,
703
Features (n-grams) Accuracy
BASELINE-POS [top200 MOST-FREQ] 53.87
BASELINE-POS [top200 IG] 56.12
AG-POS [top200 IG] 61.02
AG-POS [all ?17-gram] (n ? 2800) 68.37
AG-POS [all ? 5-gram] (n ? 2700) 68.57
AG-POS+FW [top200 IG] 58.16
AG-POS+FW [all ?19-gram] (n ? 6500) 74.49
AG-POS+FW [all ?5-gram] (n ? 6400) 74.49
AG-POS+FW [top2800 IG ? 19-gram] 71.84
AG-POS+FW [top2800 IG ? 5-gram] 71.84
ENUM-POS [top2800 IG ? 5-gram] 69.79
ENUM-POS [top6500 IG ? 5-gram] 72.44
ENUM-POS [top10K IG ? 5-gram] 71.02
ENUM-POS [top20K IG ? 5-gram] 71.43
Table 1: Maxent classification results for individual fea-
ture sets (with 5-fold cross validation).
and top 2,800 n-grams up to the length of 5. (All
the selections are based on IG).
In our models, all feature values are of binary
type. For the classifier, we employ a maximum en-
tropy (MaxEnt) machine learner ? MegaM (fifth re-
lease) by Hal Daume? III.5
3.2 Classification results
Table 1 presents all the classification results for the
individual feature sets, along with the baselines. On
the whole, both sets of the collocations inferred by
the adaptor grammars perform better than the two
baselines. We make the following observations:
? Regarding ENUM-POS as a (rough) upper
bound, the adaptor grammar AG-POS with a
comparable number of features performs al-
most as well. However, because it is possible to
enumerate many more n-grams than are found
during the sampling process, ENUM-POS opens
up a gap over AG-POS of around 4%.
? Collocations with a mix of PoS and function
words do in fact lead to higher accuracy as
compared to those of pure PoS (except for the
top 200 n-grams); for instance, compare the
2,800 n-grams up to length 5 from the two cor-
responding sets (71.84 vs. 68.57).
? Furthermore, the adaptor grammar-inferred
collocations with mixtures of PoS and function
5MegaM software is available on http://www.cs.
utah.edu/?hal/megam/.
Features (n-grams) Accuracy
AG-POS [all ? 5-gram] & FW 72.04
ENUM-POS [top2800 ? 5-gram] & FW 73.67
AG-POS+FW & AG-POS a 75.71
AG-POS+FW & AG-POS b 74.90
AG-POS+FW & ENUM-POS [top2800] a 73.88
AG-POS+FW & ENUM-POS [top2800] b 74.69
AG-POS+FW & ENUM-POS [top10K] b 74.90
AG-POS+FW & ENUM-POS [top20K] b 75.10
Table 2: Maxent classification results for combined fea-
ture sets (with 5-fold cross validation). aFeatures from
the two sets are selected based on the overall top 3700
with highest IG; bfeatures from the two sets are just lin-
early concatenated.
words (AG-POS+FW) in general perform better
than our rough upper bound of PoS colloca-
tions, i.e. the enumerated PoS n-grams (ENUM-
POS): the overall best results of the two feature
sets are 74.49 and 72.44 respectively.
Given that the AG-POS+FW n-grams are captur-
ing different sorts of document characteristics, they
could potentially usefully be combined with the
PoS-alone features. We thus combined them with
both AG-POS and ENUM-POS feature sets, and the
classification results are presented in Table 2. We
tried two ways of integrating the feature sets: one
way is to take the overall top 2,800 of the two sets
based on IG; the other way is to just combine the two
sets of features by concatenation of feature vectors
(as indicated by a and b respectively in the result
table). For comparability purposes, we considered
only n-grams up to the length of 5. A baseline ap-
proach to this is just to add in function words as un-
igram features by feature vector concatenation, giv-
ing two further models, AG-POS [all ? 5-gram] &
FW and ENUM-POS [top2800 ? 5-gram] & FW.
Overall, the classification accuracies attained by
the combined feature sets are higher than the in-
dividual feature sets. The best performing of all
the models is achieved by combining the mixed
PoS and function word collocations with the adap-
tor grammar-inferred PoS, producing the best accu-
racy thus far of 75.71. This demonstrates that fea-
tures inferred by adaptor grammars do capture some
useful information and function words are playing
a role. The way of integrating the two feature sets
has different effects on the types of combination. As
seen in Table 2, method a works better for the com-
704
bination of the two adaptor grammar feature sets;
whereas method b works better for combining adap-
tor grammar features with enumerated n-gram fea-
tures.
Using adaptor grammar collocations also outper-
forms the alternative baseline of adding in function
words as unigrams. For instance, the best perform-
ing combined feature set of both AG-POS and AG-
POS+FW does result in higher accuracy as compared
to the two alternative baseline models, comparing
75.71 with 72.04 (and 75.71 with 73.67). This
demonstrates that our more general PoS plus func-
tion word collocations derived from adaptor gram-
mars are indeed useful, and supports the argument
of Wang et al2007) that they are a useful tech-
nique for looking into features beyond just the bag
of words.
4 Language Model-based Classification
In this section, we take a language modeling ap-
proach to native language identification; the idea
here is to adopt grammatical inference to learn
a grammar-based language model to represent the
texts written by non-English native users. The gram-
mar learned is then used to predict the most probable
native language that a document (a sentence) is as-
sociated with.
In a sense, we are using a parser-based language
model to rank the documents with respect to native
language. We draw on the work of Bo?rschinger et
al. (2011) for this section. In that work, the task
was grounded learning of a semantic parser. Train-
ing examples there consisted of natural language
strings (descriptions of a robot soccer game) and
a set of candidate meanings (actions in the robot
soccer game world) for the string; each was tagged
with a context identifier reflecting the actual action
of the game. A grammar was then induced that
would parse the examples, and was used on test data
(where the context identifier was absent) to predict
the context. We take a similar approach to devel-
oping an grammatical induction technique, although
where they used a standard LDA topic model-based
PCFG, we use an adaptor grammar. We expect that
the results will likely to be lower than for the dis-
criminative approach of Section 3. However, the
approach is of interest for a few reasons: because,
whereas the adaptor grammar plays an ancillary, fea-
ture selection role in Section 3, here the feature se-
lection is an organic part of the approach as per the
actual implementation of Hardisty et al2010); be-
cause adaptor grammars can potentially be extended
in a natural way with unlabelled data; and because,
for the purposes of this paper, it constitutes a second,
quite different way to evaluate the use of n-gram col-
locations.
4.1 Language Models
We derive two adaptor grammar-based language
models. One consists of only unigrams and bi-
grams, and the other finds n-gram collocations, in
both cases over either PoS or the mix of PoS and
function words. The assumption that we make is that
each document (each sentence) is a mixture of two
sets of topics: one is the native language-specific
topic (i.e. characteristic of the native language) and
the other is the generic topic (i.e. characteristic of
the second language ? English in our case). The
generic topic is thus shared across all languages,
and will behave quite differently from a language-
specific topic, which is not shared. In other words,
there are eight topics, representing seven native lan-
guage groups that are of interest (Bulgarian, Czech,
French, Russian, Spanish, Chinese, and Japanese)
and the second language English itself.6
Bigram models The following rule schema is
applicable to both vocabulary types of PoS and the
mixture of PoS and function words.
Root? lang langTopics
langTopics? langTopics langTopic
langTopics? langTopics nullTopic
langTopics? langTopic
langTopics? nullTopic
langTopic?Words
nullTopic?Words
Words?Word Word
Words?Word
Word? w w ? Vpos; w ? Vpos+fw
N-gram models The grammar is the same as
the above with the exception that the non-terminal
Words is now rewritten as follows in order to
6We could just induce a regular PCFG here, rather than an
adaptor grammar, by taking as terminals all pairs of PoS tags.
We use the adaptor grammar formulation for comparability.
705
capture n-gram collocations of arbitrary length.
Words?Words Word
Words?Word
It should be noted that the two grammars above
can in theory be applied to an entire document or on
individual sentences. For this present work, we work
on the sentence level as the run-time of the current
implementation of the adaptor grammars grows pro-
portional to the cube of the sentence length. For each
grammar we try both sparse and uniform Dirichlet
priors (? = {0.01, 0.1, 1.0}). The sparse priors en-
courage only a minority of the rules to be associated
with high probabilities.
4.2 Training and Evaluation
As we are using the same data set as per the pre-
vious approach, we perform 5-fold cross validation
as well. However, the training for each fold is con-
ducted with a different grammar consisting of only
the vocabulary that occur in each training fold. The
reason is that we are now having a form of super-
vised topic models where the learning process is
guided by the native languages. Hence, each of the
training sentences are prefixed with the (native) lan-
guage identifiers lang, as seen in the Root rules of
the grammar presented above.
To evaluate the grammars learned, as in
Bo?rschinger et al2011) we need to slightly modify
the grammars above by removing the language iden-
tifiers ( lang) from theRoot rules and then parse the
unlabeled sentences using a publicly available CKY
parser.7 The predicted native language is inferred
from the parse output by reading off the langTopics
that the Root is rewritten to. We take that as the
most probable native language for a particular test
sentence. At the document level, we select as the
class the language predicted for the largest number
of sentences in that document.
4.3 Parsing Results
Tables 3 and 4 present the parsing results at the sen-
tence level and the document level, respectively. On
the whole, the results at the sentence level are much
poorer as compared to those at the document level.
In light of the results of Section 3.2, it is surprising
7CKY parser by Mark Johnson is available on
http://web.science.mq.edu.au/?mjohnson/
Software.htm.
Features Accuracy
(n-grams) (? = 0.01) (? = 0.1) (? = 1.0)
AG-POS [bigrams] 26.84 27.03 26.77
AG-POS [n-grams] 25.85 25.78 25.62
AG-POS+FW [bigrams] 28.58 28.40 27.43
AG-POS+FW [n-grams] 26.64 27.64 28.75
Table 3: Language modeling-based classification results
based on parsing (at the sentence level).
Features Accuracy
(n-grams) (? = 0.01) (? = 0.1) (? = 1.0)
AG-POS [bigrams] 41.22 38.88 39.69
AG-POS [n-grams] 36.12 34.90 35.20
AG-POS+FW [bigrams] 47.45 46.94 44.64
AG-POS+FW [n-grams] 43.97 49.39 50.15
Table 4: Language modeling-based classification results
based on parsing (at the document level).
that bigram models appear to perform better than n-
gram models for both types of vocabulary, with the
exception of AG-POS+FW at the document level. In
fact, one would expect n-gram models to perform
better in general as it is a generalisation that would
contain all the potential bigrams. Nonetheless, the
language models over the mixture of PoS and func-
tion words appear to be a more suitable representa-
tive of our learner corpus as compared to those over
purely PoS, confirming the usefulness of integrated
function words for the NLI classification task.
It should also be noted that sparse priors gen-
erally appear to be more appriopriate; except that
for AG-POS+FW n-grams, uniform priors are indeed
better and resulted in the highest parsing result of
50.15. (Although all the parsing results are much
weaker as compared to the results presented in Sec-
tion 3.2, they are all higher than the majority base-
line of 14.29% i.e. 70/490).
5 Discussion
Here we take a closer look at how well each ap-
proach does in identifying the individual native lan-
guages. The confusion matrix for the best model
of two approaches are presented in Table 5 and Ta-
ble 6. Both approaches perform reasonably well for
the two Oriental languages (Chinese in particular);
this is not a major surprise, as the two languages
are not part of the language family that the rest of
the languages come from (i.e. Indo-European). Un-
der the supervised maxent classification, misclassi-
fications largely are observed in the Romance ones
(French and Spanish) as well as Russian; for the lan-
guage model-based approach, Bulgarian is identi-
706
BL CZ RU FR SP CN JP
BL [52] 5 7 4 2 - -
CZ 5 [50] 5 3 4 - 3
RU 6 8 [46] 5 1 - 4
FR 7 3 5 [43] 8 - 4
SP 7 2 4 9 [47] - 1
CN - - - - - [70] -
JP - - 2 2 1 2 [63]
Table 5: Confusion matrix based on the best performing
model under maxent setting (BL:Bulgarian, CZ:Czech,
RU:Russian, FR:French, SP:Spanish, CN:Chinese,
JP:Japanese).
BL CZ RU FR SP CN JP
BL [20] 32 9 6 - 1 2
CZ 2 [59] 3 1 - - 5
RU 3 41 [19] 2 1 - 4
FR 8 20 4 [31] 4 - 3
SP 7 27 11 12 [9] - 4
CN - 2 - 2 - [62] 4
JP - 19 1 2 - 1 [47]
Table 6: Confusion matrix based on the best
performing model under language modeling setting
(BL:Bulgarian, CZ:Czech, RU:Russian, FR:French,
SP:Spanish, CN:Chinese, JP:Japanese).
fied poorly, and Spanish moreso. However, the latter
approach appears to be better in identifying Czech.
On the whole, the maxent approach results in much
fewer misclassifications compared to its counterpart.
In fact, there is a subtle difference in the exper-
imental setting of the models derived from the two
approaches with respect to the adaptor grammar: the
number of topics. Under the maxent setting, the
number of topics t was set to 25, while we restricted
the models with the language modeling approach to
only eight topics (seven for the individual native lan-
guages and one for the common second language,
English). Looking more deeply into the topics them-
selves reveals that there appears to be at least two out
of the 25 topics (from the supervised models) asso-
ciated with n-grams that are indicative of the native
languages, taking Chinese and Japanese as examples
(see the associated topics in Table 7).8 Perhaps as-
sociating each native language with only one gener-
alised topic is not sufficient.
Furthermore, the distribution of n-grams among
the topics (i.e. subtrees of collocations derived
from the adaptor grammars) are quite different be-
tween the two approaches although the total num-
8Taking the examples from Wong et al2011) as reference,
we found similar n-grams that are indicative of Japanese and
Chinese.
Top 10 Mixture N-grams
Japanese Chinese
topic2 topic23 topic9 topic17
. . NN .
we VB PPSS VB a NN NN NN
our NNS my NN NN NN NNS
our NN CC VBN by NN
NN VBG NP . RB ,
PPSS VB PPSS think NP of NN
about NN : JJ NN
because PPSS VBD ( NN .
it . RB as VBG NN
we are PPSS ? NN NN NN NN NN NN NN
Table 7: Top mixture n-grams (collocations) for 4 out of
the 25 topics representative of Japanese and Chinese (un-
der maxent setting). N-grams of pronoun with verb are
found at the upper end of Topic2 and Topic23 reflecting
the frequent usage of Japanese; n-grams of noun are top
n-grams under Topic9 and Topic17 indicating Chinese?s
common error of determiner-noun disagreement.
ber of n-grams inferred by each approach is about
the same. For the language modeling ones, a high
number of n-grams were associated with the generic
topic nullTopic9 and each language-specific topic
langTopic has a lower number of n-grams relative
to bi-grams (Table 8) associated with it. For the
maxent models, in contrast, the majority of the top-
ics were associated with a higher number of n-grams
(Table 9). The smaller number of n-grams to be used
as features ? and the fact that their extra length
means that they will occur more sparsely in the doc-
uments ? seems to be the core of the problem.
Nonetheless, the language models inferred dis-
cover relevant n-grams that are representative of
individual native languages. For instance, the bi-
gram NN NN, which Wong and Dras (2011) claim
may reflect the error of determiner-noun disagree-
ment commonly found amongst Chinese learners,
was found under the Chinese topic at the top-2 posi-
tion with a probability of 0.052 as compared to the
other languages at the probability range of 0.0005-
0.003. Similarly, one example for Japanese, the mix-
ture bigram PPSS think, indicating frequent us-
age of pronouns within Japanese was seen under the
Japanese topic at the top-9 position with a probabil-
ity of 0.025 in relation to other languages within the
range of 0.0002-0.006: this phenomenon as char-
9This is quite plausible as there should be quite a number of
structures that are representative of native English speakers that
are shared by non-native speakers.
707
Model N-gram Frequency
Types BGTopic CZTopic FRTopic RUTopic SPTopic CNTopic JPTopic NullTopic
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
Bigrams 374 187 352 219 426 165 350 211 351 156 397 351 394 194 867 6169
N-grams 177 159 226 217 151 152 148 202 128 147 357 255 209 226 3089 7794
Table 8: Distribution of n-grams (collocations) for each topic under language modeling setting. (a) subcolumns are
for n-grams of pure PoS and (b) subcolumns are for n-grams of mixtures of PoS and function words.
N-gram Frequency
Topic1 Topic2 Topic3 Topic4 Topic5 Topic6 Topic7 Topic8 Topic9 Topic10
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
174 443 145 441 136 245 141 341 236 519 169 748 127 340 182 473 109 339 190 236
Topic11 Topic12 Topic13 Topic14 Topic15 Topic16 Topic17 Topic18 Topic19 Topic20
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
57 259 126 455 103 543 211 225 170 459 81 309 238 207 152 475 119 452 333 423
Topic21 Topic22 Topic23 Topic24 Topic25
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
245 341 168 492 194 472 201 366 195 190
Table 9: Distribution of n-grams (collocations) for each topic under maxent setting. (a) subcolumns are for n-grams
of pure PoS and (b) subcolumns are for n-grams of mixtures of PoS and function words.
Languages Excerpts from ICLE
Chinese ... the overpopulation problem in urban area ...
... The development of country park can directly ...
... when it comes to urban renewal project ...
... As developing new town in ...
... and reserve some country park as ...
Japanese ... I think many people will ...
... I think governments should not ...
... I think culture is the most significant ...
... I think the state should not ...
... I really think we must live ...
Table 10: Excerpts from ICLE illustrating the common
phenomena observed amongst Chinese and Japanese.
acteristic of Japanese speakers has also been noted
for different corpora by Ishikawa (2011). (Note that
this collocation as well as its pure PoS counterpart
PPSS VB are amongst the top n-grams discovered
under the maxent setting as seen in Table 7.) Table
10 presents some excerpts extracted from the corpus
that illustrate these two common phenomena.
To investigate further the issue associated with the
number of topics under the language modeling set-
ting, we attempted to extend the adaptor grammar
with three additional topics that represent the lan-
guage family of the seven native languages of inter-
est: Slavic, Romance, and Oriental. (The resulting
grammar is presented as below.) However, the pars-
ing result does not improve over the initial setting
with eight topics in total.
Root? lang langTopics
langTopics? langTopics langTopic
langTopics? langTopics familyTopic
langTopics? langTopics nullTopic
langTopics? langTopic
langTopics? familyTopic
langTopics? nullTopic
langTopic?Words
familyTopic?Words
nullTopic?Words
Words?Words Word
Words?Word
Word? w w ? Vpos; w ? Vpos+fw
6 Conclusion and Future Work
This paper has shown that the extension of adap-
tor grammars to discovering collocations beyond the
lexical, in particular a mix of PoS tags and function
words, can produce features useful in the NLI clas-
sification problem. More specifically, when added
to a new baseline presented in this paper, the com-
bined feature set of both types of adaptor grammar
inferred collocations produces the best result in the
context of using n-grams for NLI. The usefulness of
the collocations does vary, however, with the tech-
nique used for classification.
Future work will involve a broader exploration
of the parameter space of the adaptor grammars,
in particular the number of topics and the value
of ?; a look at other non-parametric extensions of
PCFGs, such as infinite PCFGs (Liang et al2007)
for finding a set of non-terminals permitting more
fine-grained topics; and an investigation of how the
approach can be extended to semi-supervised learn-
ing to take advantage of the vast quantity of texts
with errors available on the Web.
708
Acknowledgments
We would like to acknowledge the support of ARC
Linkage Grant LP0776267. We also thank the
anonymous reviewers for useful feedback. Much
gratitude is due to Benjamin Bo?rschinger for his
help with the language modeling implementation.
References
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alation. Journal of Machine
Learning Research, 3:993?1022.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1416?1425, Edinburgh, Scotland, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180, Ann Arbor, Michigan, June.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263?272.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses Universitaires de
Louvain, Louvian-la-Neuve.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 284?292.
Shun?ichiro Ishikawa. 2011. A New Horizon in Learner
Corpus Studies: The Aim of the ICNALE Project. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
pora and Language Technologies in Teaching, Learn-
ing and Research, pages 3?11. University of Strath-
clyde Press, Glasgow, UK.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for speci-
fying compositional nonparametric bayesian models.
In Advances in Neural Information Processing Sys-
tems 19: Proceedings of the Twentieth Annual Confer-
ence on Neural Information Processing Systems, pages
641?648.
Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and the
Structure of Proper Names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. In Intelligence and Security Informat-
ics, volume 3495 of Lecture Notes in Computer Sci-
ence, pages 209?217. Springer-Verlag.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational Methods in Authorship Attribu-
tion. Journal of the American Society for Information
Science and Technology, 60(1):9?26.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite pcfg using hierarchical
dirichlet processes. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 688?697, Prague, Czech Re-
public, June.
Timothy O?Donnell. 2011. Productivity and reuse in
language. Ph.D. thesis, Harvard University.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?900.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9?16.
Hans van Halteren. 2008. Source language markers in
EUROPARL translations. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING), pages 937?944.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings of
the 2007 Seventh IEEE International Conference on
Data Mining, ICDM ?07, pages 697?702.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, July.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic modeling for native language identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
709
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 124?133,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
NLI Shared Task 2013: MQ Submission
Shervin Malmasi Sze-Meng Jojo Wong Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
{shervin.malmasi,sze.wong,mark.dras}@mq.edu.au
Abstract
Our submission for this NLI shared task used
for the most part standard features found in re-
cent work. Our focus was instead on two other
aspects of our system: at a high level, on pos-
sible ways of constructing ensembles of multi-
ple classifiers; and at a low level, on the gran-
ularity of part-of-speech tags used as features.
We found that the choice of ensemble com-
bination method did not lead to much differ-
ence in results, although exploiting the vary-
ing behaviours of linear versus logistic regres-
sion SVM classifiers could be promising in fu-
ture work; but part-of-speech tagsets showed
noticeable differences.
We also note that the overall architecture, with
its feature set and ensemble approach, had an
accuracy of 83.1% on the test set when trained
on both the training data and development data
supplied, close to the best result of the task.
This suggests that basically throwing together
all the features of previous work will achieve
roughly the state of the art.
1 Introduction
Among the efflorescence of work on Native Lan-
guage Identification (NLI) noted by the shared task
organisers, there are two trends in recent work in
particular that we considered in building our sub-
mission. The first is the proposal and use of new
features that might have relevance to NLI: for exam-
ple, Wong and Dras (2011), motivated by the Con-
trastive Analysis Hypothesis (Lado, 1957) from the
field of Second Language Acquisition, introduced
syntactic structure as a feature; Swanson and Char-
niak (2012) introduced more complex Tree Substi-
tution (TSG) structures, learned by Bayesian infer-
ence; and Bykh and Meurers (2012) used recurring
n-grams, inspired by the variation n-gram approach
to corpus error annotation detection (Dickinson and
Meurers, 2003). Starting from the features intro-
duced in these papers and others, then, other recent
papers have compiled a comprehensive collection of
features based on the earlier work ? Tetreault et
al. (2012) is an example, combining and analysing
most of the features used in previous work. Given
the timeframe of the shared task, there seemed to be
not much mileage in trying new features that were
likely to be more peripheral to the task.
A second trend, most apparent in 2012, was the
examination of other corpora besides the Interna-
tional Corpus of Learner English used in earlier
work, and in particular the use of cross-corpus evalu-
ation (Brooke and Hirst, 2012; Tetreault et al, 2012)
to avoid topic bias in determining native language.
Possible topic bias had been a reason for avoiding
a full range of n-grams, in particular those contain-
ing content words (Koppel et al, 2009); the devel-
opment of new corpora and the analysis of the effect
of topic bias mitigated this. The consequent use of a
full range of n-grams further reinforced the view that
novel features were unlikely to be a major source of
interesting results.
We therefore concentrated on two areas: the use
of classifier ensembles, and the choice of part-of-
speech tags. With classifier ensembles, Tetreault
et al (2012) noted that these were highly useful in
their system; but while that paper had extensive fea-
124
ture descriptions, it did not discuss in detail the ap-
proach to its ensembles. We therefore decided to
examine a range of possible ensemble architectures.
With part-of-speech tags, most work has used the
Penn Treebank tagset, including those based on syn-
tactic structure. Kochmar (2011) on the other hand
used the CLAWS tagset,1 which is much richer and
more oriented to linguistic analysis than the Penn
Treebank one. Given the much larger size of the
TOEFL11 corpus used for this shared task than the
corpora used for much earlier work, data sparsity
could be less of an issue, and the tagset a viable one
for future work.
The description of our submission is therefore in
three parts. In ?2 we present the system description,
with a focus on the ensemble architectures we inves-
tigated; in ?3 we list the features we used, which are
basically those of much of the previous work; in ?4
we present results of some of the variants we tried,
particularly with respect to ensembles and tagsets;
and in ?5 we discuss some of the interesting charac-
teristics of the data we noted during the shared task.
2 System Design
Our overall approach in terms of features and clas-
sifiers used is a fairly standard one. One difference
from most approaches, but inspired by Tetreault et
al. (2012), is that we train multiple classifiers over
subsets of the features, over different feature rep-
resentations, and over different regularisation ap-
proaches; we then combine them in ensembles (Di-
etterich, 2000).
2.1 SVM Ensemble Construction
To construct our ensemble, we train individual clas-
sifiers on a single feature type (e.g. PoS n-grams),
using a specific feature value representation and
classifier. We utilise a parallel ensemble structure
where the classifiers are run on the input texts in-
dependently and their results are then fused into the
final output using a combiner.
Additionally, we also experiment with bagging
(bootstrap aggregating), a commonly used method
for ensemble generation (Breiman, 1996) to gener-
ate multiple ensembles per feature type.
1http://ucrel.lancs.ac.uk/claws/
For our classifier, we use SVMs, specifically the
LIBLINEAR SVM software package (Fan et al,
2008),2 which is well-suited to text classification
tasks with large numbers of features and large num-
bers of documents. LIBLINEAR provides both lo-
gistic regression and linear SVMs; we experiment
with both. In general, the linear classifier performs
better, but it only provides the decision output. The
logistic regression classifier on the other hand gives
probability estimates, which are required by most
of our combination methods (?2.3). We therefore
mostly use the logistic regression classifiers.
2.2 L1- and L2-regularized SVM Classifiers
In our preliminary experiments we noted that
some feature types performed better with L1-
regularization and others with L2. In this work we
generate classifiers using both methods and evaluate
their individual and combined performance.
2.3 Classifier Combination Methods
We experiment with the following decision combi-
nation methods, which have been discussed in the
machine learning literature. Polikar (2006) provides
an exposition of these rules and methods.
Plurality vote: Each classifier votes for a single
class label, the label with the highest number of
votes wins. Ties are broken arbitrarily.
Sum: All probability estimates are added together
and the label with the highest sum is picked.
Average: The mean of all scores for each class
is calculated and the label with the highest average
probability is chosen.
Median: Each label?s estimates are sorted and the
median value is selected as the final score for that
label. The label with the highest value is picked.
Product: For each class label, all of the probabil-
ity estimates are multiplied together to create the la-
bel?s final estimate. The label with the highest esti-
mate is selected. A single low score can have a big
effect on the outcome.
Highest Confidence: In this simple method, the
class label that receives the vote with the largest de-
gree of confidence is selected as the final output.
2Available at http://www.csie.ntu.edu.tw/
?cjlin/liblinear/
125
Borda Count: The confidence estimates are con-
verted to ranks and the final label selected using the
Borda count algorithm (Ho et al, 1994). In this
combination approach, broadly speaking points are
assigned to ranks, and these tallied for the overall
weight.
With the exception of the plurality vote, all of
these can be weighted. In our ensembles we also ex-
periment with weighting the output of each classifier
using its individual accuracy on the training data as
an indication of our degree of confidence in it.
2.4 Feature Representation
Most NLI studies have used two types of feature rep-
resentations: binary (presence or absence of a fea-
ture in a text) and normalized frequencies. Although
binary feature values have been used in some stud-
ies (e.g. Wong and Dras (2011)), most have used
frequency-based values.
In the course of our experiments we have ob-
served that the effect of the feature representation
varies with the feature type, size of the feature space
and the learning algorithm itself. In our current sys-
tem, then, we generate two classifiers for each fea-
ture type, one trained with frequency-based values
(raw counts scaled using the L2-norm) and the other
with binary. Our experiments assess both their indi-
vidual and joint performance.
2.5 Proficiency-level Based Classification
To utilise the proficiency level information provided
in the TOEFL11 corpus (texts are marked as either
low, medium or high proficiency), we also investi-
gate classifiers that are trained using only texts from
specific proficiencies.
Tetreault et al (2012) established that the classi-
fication accuracy of their system varied across pro-
ficiency levels, with high proficiency texts being the
hardest to classify. This is most likely due to the fact
that writers at differing skill levels commit distinct
types of errors at different rates (Ortega, 2009, for
example). If learners of different backgrounds com-
mit these errors with different distributions, these
patterns could be used by a learner to further im-
prove classification accuracy. We will use these fea-
tures in one of our experiments to investigate the
effectiveness of such proficiency-level based classi-
fiers for NLI.
3 Features
We roughly divide out feature types into lexical,
part-of-speech and syntactic. In all of the feature
types below, we perform no feature selection.
3.1 Lexical Features
As all previous work, we use function words as fea-
tures. In addition, given the attempts to control for
topic bias in the TOEFL11 corpus, we also make
use of various lexical features which have been pre-
viously avoided by researchers due to the reported
topic bias (Brooke and Hirst, 2011) in other NLI cor-
pora such as the ICLE corpus.
Function Words In contrast to content words,
function words do not have any meaning themselves,
but rather can be seen as indicating the grammat-
ical relations between other words. Examples in-
clude articles, determiners, conjunctions and auxil-
iary verbs. They have been widely used in studies of
authorship attribution as well as NLI and established
to be informative for these tasks. We use the list
of 398 common English function words from Wong
and Dras (2011). We also tested smaller sets, but ob-
served that the larger sets achieve higher accuracy.
Function Word n-grams We devised and tested a
new feature that attempts to capture patterns of func-
tion word use at the sentence level. We define func-
tion word n-grams as a type of word n-gram where
content words are skipped: they are thus a specific
subtype of skip-gram discussed by Guthrie et al
(2006). For example, the sentence We should all
start taking the bus would be reduced to we should
all the, from which we would extract the n-grams.
Character n-grams Tsur and Rappoport (2007)
demonstrated that character n-grams are a useful
feature for NLI. These n-grams can be considered
as a sub-word feature and their effectiveness is hy-
pothesized to be a result of phoneme transfer from
the writer?s L1. They can also capture orthographic
conventions of a language. Accordingly, we limit
our n-grams to a maximum size of 3 as longer se-
quences would correspond to short words and not
phonemes or syllables.
Word n-grams There has been a shift towards the
use of word-based features in several recent studies
(Brooke and Hirst, 2012; Bykh and Meurers, 2012;
126
Tetreault et al, 2012), with new corpora come into
use for NLI and researchers exploring and address-
ing the issues relating to topic bias that previously
prevented their use. Lexical choice is considered to
be a prime feature for studying language transfer ef-
fects, and researchers have found word n-grams to
be one of the strongest features for NLI. Tetreault
et al (2012) expanded on this by integrating 5-gram
language models into their system. While we did not
replicate this, we made use of word trigrams.
3.2 POS n-grams
Most studies have found that POS tag n-grams are
a very useful feature for NLI (Koppel et al, 2005;
Bykh and Meurers, 2012, for example). The tagset
provided by the Penn TreeBank is the most widely
used in these experiments, with tagging performed
by the Stanford Tagger (Toutanova et al, 2003).
We investigate the effect of tagset granularity
on classification accuracy by comparing the clas-
sification accuracy of texts tagged with the PTB
tagset against those annotated by the RASP Tagger
(Briscoe et al, 2006). The PTB POS tagset contains
36 unique tags, while the RASP system uses a subset
of the CLAWS2 tagset, consisting of 150 tags.
This is a significant size difference and we hy-
pothesize that a larger tagset could provide richer
levels of syntactically meaningful info which is
more fine-grained in distinction between syntactic
categories and contains more morpho-syntactic in-
formation such as gender, number, person, case
and tense. For example, while the PTB tagset
has four tags for pronouns (PRP, PRP$, WP,
WP$), the CLAWS tagset provides over 20 pronoun
tags (PPHO1, PPIS1, PPX2, PPY, etc.) dis-
tinguishing between person, number and grammati-
cal role. Consequently, these tags could help better
capture error patterns to be used for classification.
3.3 Syntactic Features
Adaptor grammar collocations Drawing on
Wong et al (2012), we also utilise an adaptor gram-
mar to discover arbitrary lengths of n-gram collo-
cations for the TOEFL11 corpus. We explore both
the pure part-of-speech (POS) n-grams as well as
the more promising mixtures of POS and function
words. Following a similar experimental setup as
per Wong et al (2012), we derive two adaptor gram-
mars where each is associated with a different set of
vocabulary: either pure POS or the mixture of POS
and function words. We use the grammar proposed
by Johnson (2010) for capturing topical collocations
as presented below:
Sentence ? Docj j ? 1, . . . ,m
Docj ? j j ? 1, . . . ,m
Docj ? Docj Topici i ? 1, . . . , t;
j ? 1, . . . ,m
Topici ? Words i ? 1, . . . , t
Words ? Word
Words ? Words Word
Word ? w w ? Vpos;
w ? Vpos+fw
As per Wong et al (2012), Vpos contains 119
distinct POS tags based on the Brown tagset and
Vpos+fw is extended with 398 function words used
in Wong and Dras (2011). The number of topics t
is set to 50 (instead of 25 as per Wong et al (2012))
given that the TOEFL corpus is larger than the ICLE
corpus. The inference algorithm for the adaptor
grammars are based on the Markov Chain Monte
Carlo technique made available by Johnson (2010).3
Tree Subtitution Grammar fragments In rela-
tion to the context-free grammar (CFG) rules ex-
plored in the previous NLI work of Wong and Dras
(2011), Tree Substitution Grammar (TSG) frag-
ments have been proposed by Swanson and Char-
niak (2012) as another form of syntactic features
for NLI classification tasks. Here, as an approxi-
mation to deploying the Bayesian approach to in-
duce a TSG (Post and Gildea, 2009; Swanson and
Charniak, 2012), we first parse each of the essays in
the TOEFL training corpus with the Stanford Parser
(version 2.0.4) (Klein and Manning, 2003) to obtain
the parse trees. We then extract the TSG fragments
from the parse trees using the TSG system made
available by Post and Gildea (2009).4
Stanford dependencies In Tetreault et al (2012),
Stanford dependencies were investigated as yet an-
other form of syntactic features. We follow a
similar approach: for each essay in the train-
ing corpus, we extract all the basic (rather than
3http://web.science.mq.edu.au/?mjohnson/
Software.htm
4https://github.com/mjpost/dptsg
127
the collapsed) dependencies returned by the Stan-
ford Parser (de Marneffe et al, 2006). Simi-
larly, we generate all the variations for each of
the dependencies (grammatical relations) by sub-
stituting each lemma with its corresponding PoS
tag. For instance, a grammatical relation of
det(knowledge, the) yields the following
variations: det(NN, the), det(knowledge,
DT), and det(NN, DT).
4 Experiments and Results
We report our results using 10-fold cross-validation
on the combined training and development sets, as
well as by training a model using the training and
development data and running it on the test set.
We note that for our submission, we trained only
on the training data; the results here thus differ from
the official ones.
4.1 Individual Feature Results and Analysis
We ran the classifiers generated for each feature type
to assess their performance. The results are summa-
rized in Table 1: the Train + Dev Set results were for
the system when trained on the training and develop-
ment data with 10 fold cross-validation, and the Test
Set results for the system trained on the training and
development data combined.
Character n-grams are an informative feature and
our results are very similar to those reported by pre-
vious researchers (Tsur and Rappoport, 2007). In
particular, it should be noted that the use of punc-
tuation is a very powerful feature for distinguishing
languages. Romance language speakers were most
likely to use more punctuation symbols (colons,
semicolons, ellipsis, parenthesis, etc.) and at higher
rates. Chinese, Japanese and Korean speakers were
far less likely to use punctuation.
The performance for word n-grams, TSG frag-
ments and Stanford Dependencies is very strong and
comparable to previously reported research. For the
adaptor grammar n-grams, the mixed POS/function
word version yielded best results and was included
in the ensemble.
4.2 POS-based Classification and Tagset Size
To compare the tagsets we trained individual classi-
fiers for n-grams of size 1?4 using both tagsets and
tested them. The results are shown in Table 2 and
Feature Train +
Dev Set
Test Set
Chance Baseline 9.1 9.1
Character unigram 33.99 34.70
Character bigram 51.64 49.80
Character trigram 66.43 66.70
RASP POS unigram 43.76 45.10
RASP POS bigram 58.93 61.60
RASP POS trigram 59.39 62.70
Function word unigram 51.38 54.00
Function word bigram 59.73 63.00
Word unigram 74.61 75.50
Word bigram 74.46 76.00
Word trigram 63.60 65.00
TSG Fragments 72.16 72.70
Stanford Dependencies 73.78 75.90
Adaptor Grammar
POS/FW n-grams
69.76 70.00
Table 1: Classification results for our individual features.
N PTB RASP
1 34.03 43.76
2 48.85 58.93
3 51.06 59.39
4 49.85 52.81
Table 2: Classification accuracy results for POS n-grams
of size N using both the PTB and RASP tagset. The larger
RASP tagset performed significantly better for all N.
N Accuracy
1 51.38
2 59.73
3 52.14
Table 3: Classification results for Function Word n-grams
of size N. Our proposed Function Word bigram and tri-
gram features outperform the commonly used unigrams.
128
Ensemble Train +
Dev Set
Test Set
Complete Ensemble 81.50 81.60
Only binary values 82.46 83.10
Only freq values 65.28 67.20
L1-regularized solver only 80.33 81.10
L2-regularized solver only 81.42 81.10
Bin, L1-regularized only 81.57 82.00
Bin, L2-regularized only 82.00 82.50
Table 4: Classification results for our ensembles, best re-
sult in column in bold (binary values with L1- and L2-
regularized solvers).
show that the RASP tagged data provided better per-
formance in all cases. While it is possible that these
differences could be attributed to other factors such
as tagging accuracy, we do not believe this to be the
case as the Stanford Tagger is known for its high ac-
curacy (97%). These differences are quite clear; this
finding also has implications for other syntactic fea-
tures that make use of POS tags, such as Adaptor
Grammars, Stanford Dependencies and Tree Substi-
tution Grammars.
4.3 Function Word n-grams
The classification results using our proposed Func-
tion Word n-gram feature are shown in Table 3.
They show that function word skip-grams are more
informative than the simple function word counts
that have been previously used.
4.4 Ensemble Results
Table 4 shows the results from our ensembles. The
feature types included in the ensemble are those
whose results are listed individually in Table 1. (So,
for example, we only use the RASP-tagged PoS n-
grams, not the Penn Treebank ones.) The complete
ensemble consists of four classifiers per feature type:
L1-/L2-regularized versions with both binary and
freq. values.
Bagging Our experiments with bagging did not
find any improvements in accuracy, even with larger
numbers of bootstrap samples (50 or more). Bag-
ging is said to be more suitable for unstable clas-
sifiers which have greater variability in their perfor-
mance and are more susceptible to noise in the train-
ing data (Breiman, 1996). In our experiments with
individual feature types we have found the classi-
fiers to be quite stable in their performance, across
different folds and training set sizes. This is one po-
tential reason why bagging did not yield significant
improvements.
Combiner Methods Of the methods outlined in
?2.3 we found the sum and weighted sum combiners
to be the best performing, but the weighted results
did not improve accuracy in general over their un-
weighted counterparts. Our results are reported us-
ing the unweighted sum combiner. A detailed com-
parison of the results for the combiners has been
omitted here due to time constraints; the differences
across all combination methods was roughly 1?2%.
Any new approach to ensemble combination meth-
ods would consequently want to be radically differ-
ent to expect a notable improvement in performance.
As noted at the start of this section, results here
are for the system trained on training and develop-
ment data. The best result on the test set (83.1%)
is almost 4% higher than our submission result, and
close to the highest result achieved (83.6%).
Binary & Frequency-Based Feature Values Our
results are consistent with those of Brooke and Hirst
(2012), who conclude that there is a preference
for binary feature values instead of frequency-based
ones. Including both types in the ensemble did not
improve results.
However, in other experiments on the TOEFL11
corpus we have also observed that use of frequency
information often leads to significantly better results
when using a linear SVM classifier: in fact, the lin-
ear classifier is better on all frequency feature types,
and also on some of the binary feature types. We
present results in Table 5 comparing the two. An ap-
proach using the linear SVM that provides an asso-
ciated probability score ? perhaps through bagging
? allowing it to be combined with the methods de-
scribed in ?2.3 could then perhaps boost results. All
these results were from a system using the training
data with 10 fold cross-validation.
Combining Regularisation Approaches Results
show that combining the L1- and L2-regularized
classifiers in the ensemble provided a small in-
129
Feature L2-norm scaled counts Binary
linear log. regr. linear log. regr.
Char unigram 31.60 26.23 25.68 26.36
Char bigram 51.59 41.81 41.20 45.11
Char trigram 65.78 54.97 58.30 61.76
RASP POS bigram 60.38 54.00 50.31 54.56
RASP POS trigram 58.75 53.92 55.93 58.58
Function word unigram 51.38 45.09 46.67 47.13
Function word bigram 58.95 53.22 54.97 58.53
Word unigram 70.33 55.60 69.40 72.00
Word bigram 73.90 54.25 73.65 74.93
Word trigram 63.78 52.46 64.78 64.94
Table 5: Classification results for our individual features.
crease in accuracy. Ensembles with either the L1 or
L2-regularized solver have lower accuracy than the
combined methods (row 2).
4.5 Proficiency-level Based Classification
Table 6 shows our results for training models with
texts of a given proficiency level and the accuracy on
the test set. The numbers show that in general texts
should be classified with a learner trained with texts
of a similar proficiency. They also show that not all
texts in a proficiency level are of uniform quality as
some levels perform better with data from the clos-
est neighbouring levels (e.g. Medium texts perform
best with data from all proficiencies), suggesting
that the three levels form a larger proficiency con-
tinuum where users may fall in the higher or lower
ends of a level. A larger scale with more than three
levels could help address this.
5 Discussion
5.1 Unused Experimental Features
We also experimented with some other feature types
that were not included in the final system.
CCG SuperTag n-grams In order to introduce
additional rich syntactic information into our sys-
tem, we investigated the use CCG SuperTags as fea-
ture for NLI classification. We used the C&C CCG
Train Test Acc. Train Test Acc.
Low Low 52.2 All Med 86.8
Med Low 72.1 M + H Med 85.3
High Low 40.3 L + M Med 83.8
All Low 75.2 Low High 16.1
L + M Low 76.0 Med High 68.1
Low Med 40.7 High High 65.7
Med Med 83.6 M + H High 74.7
High Med 62.1 All High 75.2
Table 6: Results for classifying the test set documents
using classifiers trained with a specific proficiency level.
Each level?s best result in bold.
Parser and SuperTagger (Curran et al, 2007) to ex-
tract SuperTag n-grams from the corpus, which were
then used as features to construct classifiers. The
best results were achieved by using n-grams of size
2?4, which achieved classification rates of around
44%. However, adding these features to our ensem-
ble did not improve the overall system accuracy. We
believe that this is because when coupled with the
other syntactic features in the system, the informa-
tion provided by the SuperTags is redundant, and
thus they were excluded from our final ensemble.
Hapax Legomena and Dis Legomena The spe-
cial word categories Hapax Legomena and Dis
legomena refer to words that appear only once and
130
twice, respectively, in a complete text. In practice,
these features are a subset of our Word Unigram
feature, where Hapax Legomena correspond to un-
igrams with an occurrence count of 1 and Hapax dis
legomena are unigrams with a count of 2.
In our experimental results we found that Ha-
pax Legomena alone provides an accuracy of 61%.
Combining the two features together yields an accu-
racy of 67%. This is an interesting finding as both
of these features alone provide an accuracy close to
the whole set of word unigrams.
5.2 Corpus Representativeness
We conducted a brief analysis of our extracted fea-
tures, looking at the most predictive ones according
to their Information Gain. Although we did not find
any obvious indicators of topic bias, we noted some
other issues of potential concern.
Chinese, Japanese and Korean speakers make ex-
cessive use of phrases such as However, First of all
and Secondly. At first glance, the usage rate of these
phrases seems unnaturally high (more than 50% of
Korean texts had a sentence beginning with How-
ever). This could perhaps be a cohort effect relat-
ing to those individually attempting this particular
TOEFL exam, rather than an L1 effect: it would
be useful to know how much variability there is in
terms of where candidates come from.
It was also noticed that many writers mention the
name of their country in their texts, and this could
potentially create a high correlation between those
words and the language class label, leading perhaps
to an artificial boosting of results. For example, the
words India, Turkey, Japan, Korea and Germany ap-
pear with high frequency in the texts of their corre-
sponding L1 speakers ? hundreds of times, in fact,
in contrast to frequencies in the single figures for
speakers of other L1s. These might also be an arte-
fact of the type of text, rather than related to the L1
as such.
5.3 Hindi vs. Telugu
We single out here this language pair because of
the high level of confusion between the two classes.
Looking at the results obtained by other teams, we
observe that this language pair provided the worst
classification accuracy for almost all teams. No
system was able to achieve an accuracy of 80%
for Hindi (something many achieved for other lan-
guages). In analysing the actual and predicted
classes for all documents classified as Hindi and
Telugu by our system, we find that generally all
of the actual Hindi and Telugu texts (96% and
99%, respectively) are within the set. Our classifier
is clearly having difficulty discriminating between
these two specific classes.
Given this, we posit that the confounding influ-
ence may have more to do with the particular style
of English that is spoken and taught within the
country, rather than the specific L1 itself. Consult-
ing other research about SLA differences in multi-
lingual countries could shed further light on this.
Analysing highly informative features provides
some clues about the influence of a common cul-
ture or national identity: in our classifier, the words
India, Indian and Hindu were highly predictive of
both Hindi and Telugu texts, but no other lan-
guages. In addition, there were terms that were
not geographically- or culturally-specific that were
strongly associated with both Hindi and Telugu:
these included hence, thus, and etc, and a much
higher rate of use of male pronouns. It has been
observed in a number of places (Sanyal, 2007, for
example) that the English spoken across India still
retains characteristics of the English that was spo-
ken during the time of the Raj and the East India
Company that have disappeared from other varities
of English, so that it can sound more formal to other
speakers, or retain traces of an archaic business cor-
respondence style; the features just noted would fit
that pattern. The effect is likely to occur regardless
of the L1.
Looking at individual language pairs in this way
could lead to incremental improvement in the overall
classification accuracy of NLI systems.
References
Leo Breiman. 1996. Bagging predictors. In Machine
Learning, pages 123?140.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceedings
of the COLING/ACL on Interactive presentation ses-
sions, COLING-ACL ?06, pages 77?80, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
131
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence. In
Proceedings of COLING 2012, pages 425?440, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c and
boxer. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 33?36, Prague, Czech Republic, June.
Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages
449?454, Genoa, Italy.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-03), pages 107?114, Budapest, Hungary.
Thomas G Dietterich. 2000. Ensemble methods in ma-
chine learning. In Multiple classifier systems, pages
1?15. Springer.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,
and Yorick Wilks. 2006. A Close Look at Skip-gram
Modelling. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC 2006), pages 1222?1225, Genoa, Italy.
Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier
systems. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 16(1):66?75.
Mark Johnson. 2010. Pcfgs, topic models, adaptor gram-
mars and learning topical collocations and the struc-
ture of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Sap-
poro, Japan. Association for Computational Linguis-
tics.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational Methods in Authorship Attribu-
tion. Journal of the American Society for Information
Science and Technology, 60(1):9?26.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. University
of Michigan Press, Ann Arbor, MI, US.
Lourdes Ortega. 2009. Understanding Second Language
Acquisition. Hodder Education, Oxford, UK.
Robi Polikar. 2006. Ensemble based systems in deci-
sion making. Circuits and Systems Magazine, IEEE,
6(3):21?45.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 45?48, Suntec, Singapore. As-
sociation for Computational Linguistics.
Jyoti Sanyal. 2007. Indlish: The Book for Every English-
Speaking Indian. Viva Books Private Limited.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In IN PRO-
CEEDINGS OF HLT-NAACL, pages 252?259.
Oren Tsur and Ari Rappoport. 2007. Using Classifier
Features for Studying the Effect of Native Language
on the Choice of Written Second Language Words.
In Proceedings of the Workshop on Cognitive Aspects
of Computational Language Acquisition, pages 9?16,
132
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
133
