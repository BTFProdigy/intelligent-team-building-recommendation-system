Finding Structural Correspondences from Bilingual Parsed Corpus 
for Corpus-b sed Translation 
Hideo Watanabe*, Sadao Kurohashi** and Eiji Aramaki** 
* IBM Researdt, Tokyo Research Laboratory 
1623-14 Shimotsuruma, Yamato, 
Kanagawa 242-8502, Japan 
watanabe@trl.ibm.co.jp 
** Graduate School of Inforlnatics, Kyoto University 
Yoshida-homnachi, Sakyo, 
Kyoto 606-8501, .JaI)an 
kuro@i.kyoto-u.ac.jp, 
aramaki@pine.kuee.kyoto-u.ac.jp 
Abstract 
In this paper, we describe a system and meth- 
ods for finding structural correspondences from the 
paired dependency structures of a source sentence 
and its translation in a target language. The sys- 
tem we have developed finds word correspondences 
first, then finds phrasal correspon(tences based on 
word correspondences. We have also developed a
GUI system with which a user can check and cor- 
rect tile correspondences retrieved by the system. 
These structural correspondences will be used as 
raw translation I)atterns in a corpus-based transla- 
tion system. 
1 Introduction 
So far, a number of methodologies and systelns 
for machine trauslation using large corpora exist. 
They include example-based at)proaches \[7, 8, 9, 
12\], pattern-based approaches \[10, 11, 14\], and sta- 
tistical approaches. For instance, example-based 
approaches use a large set of translation patterns 
each of which is a pair of parsed structures of a 
source-language fragment and its target-language 
translation fragment. Figure 1 shows an exanl- 
ple of translation by an example-based method, ill 
which translation patterns (pl) and (p2) are se- 
lected as similar to a (left hand) Japanese depen- 
dency structure, and an (right hand) English de- 
pendency structure is constructed by merging the 
target parts of these translation patterns 1.
In this kind of system, it is very important o 
collect a large set of translatiou patterns easily and 
efficiently. Previous systems, however, collect such 
translation patterns mostly manually. Therefore, 
they have problems in terms of the development 
cost. 
1Words in parenthesis at the nodes of the Japanese de- 
pendency structure are representative English translations, 
and are for explanation. 
This paper tries to provide solutions for this is- 
sue by proposing methods for finding structural 
correspondences of parsed trees of a translation 
pair. These structural correspondences are used as 
bases of translation patterns in corpus-based ap- 
proaches. 
Figure 2 shows an example of extracting struc- 
tural correspondences. In this figure, tile left tree 
is a Japanese dependency tree, the right tree is a 
dependency tree of its English translation, dotted 
arrows represent word correspondence, and a pair 
of boxes connected by a solid line represent phrasal 
correspondence. We would like to extract these 
 ,ook \ 
"4" - . .~  ...," ~.. a movie ~ 
Figure 2: An Example of Finding Structural Cor- 
respoudences 
word and phrasal correspondeuces automatically. 
In what follows, we will describe details of proce- 
dures for finding these structural correspondences. 
2 Finding Structural Correspondences 
This sectiou describes methods for finding struc- 
tural correspondences for a paired parsed trees. 
2.1 Data  St ructure  
Before going into the details of finding structural 
correspondences, we describe the data format of a 
906 
verb - -  
9a 
noun- -noun , 
n0mu--drink 
l l 0un  - -  n0un  
verb ! 
i 
,, 
t .  
"', dl lk 
 he  .__medicine 
l 
1 ~ # 
\[.-- 
I 
(p2) 
Figure 1: Translation Example by Examt)le-based ~li'anslation 
dependency structure. A det)endeney stru('ture as 
used in this pat)er is a tree consisting of nodes and 
links (or m:cs), wh('.re a node represents a content 
word, while a link rel)resents a fllnctional word or 
a relation between content words. For instance, as 
shown in Figure 2, a t)reposition "at;" is represented 
as a l ink  in l~,nglish. 
2.2 F ind ing  Word  Cor respondences  
The  tirst task for finding stru('tm:al corresI)On- 
den(:c's is to lind word (:orro, sl)ondenccs t)et;ween (;he 
nodes of a sour(:e parsed tree and the nodes of a 
t;wget parsed tree. 
Word correspondences are tkmn(1 by eonsull;ing a
source-to-target translation dictionary. Most words 
can find a unique 1;ranslation candidate in a target 
tree, but there are cases such that there are many 
translation candidates in a target parsed tree for 
a source word. Theretbre, the main task of tind- 
ing word correspondences is to determine the most 
plausible l;ranslation word mnong can(tidates. We 
call a pair of a source word and its translation 
candidate word in a target tree a word correspon- 
dence candidate denoted by WC(s,/,), where s is a 
source word and t is a target word. If 17\[TC(s,/,) ix a 
word correspondence andida.te such that there is 
rto other WC originating h'om s, then it is called 
WA word correspondence. 
The basic idea to select the most plausil)le word 
correspondence candidate ix to select a candidate 
which is near to another word correspondence whose 
source is also near to a sour(:e word in question. 
Suppose a source word s has multiple candidate 
translation target words t~ (i = 1,...,7~,), that  is, 
there are multiple 17FCs originating h'om .s'. We, 
denote these multiple word corresl)ondence candi- 
dates by WC(s, tl). For each I'VC of s, this proce- 
dure finds the neighbor WA correspondence whose 
distance to WC ix below a threshold. The distance 
between WC(sl,/,~) and WA(s.2,/,2) is defined as 
the distance between sl and .s2 plus the distmme 
between s2 and 1,2 where a distance between two 
nodes is defined as the number of nodes in the t)ath 
whoso, ends are the two nodes. Among I~VCs of 
.s for which neighbor H/A ix tound, the one with 
the smallest (listan(:(~ is chosen as the word corre- 
Sl)ondenee of s, and I/VCs whMl are not chosen 
are invalidated (or deleted). We call a word corre- 
spondence found t)y this procedure WX.  We use 
3 as t;he distance threshold of the above procedure 
currently. This procedure ix applied to all source 
nodes which have multii)le WCs. Figure 3 shows 
an example of WX word correspondence. In this 
examt)le, since the Japanese word "ki" has two En- 
glish l;ranslation word candidates "time" and "pe- 
riod," there are two WCs  (~7C 1 and WC2). The 
direct parent node "ymlryo" of "ki" has a WA cor- 
respondence (I/VA1) to "concern," and the direct 
child node "ikou" has also a WA correspondc'nee 
(WA2) to "transition." In this ease, since the dis- 
tance between I'VC2 and WA2 is smaller than the 
distan(:e between I.VC1 and WA1, I'VC~ in clmnged 
to a 1/l/X, and I~ITC1 is adandoned. 
In addition to WX correspondences, weconsider 
a special case such that given a word correspon- 
dence l'lZ(s,/,), if s has only one child node which ix 
907 
. . ,  ........ be ....... ",,, 
.. -~%omp t 
. . .WAI at / /  concern 
." time 
yuuryo ,.." 
(concern) -" 
ni ..,Wc1 same 
.... accompany 
ki..,*\[__ 
(time) .............. W_..G2 ............ 
'"-- period 
ikou ......... VVA2 of 
transition (transition) 
Figure 3: An Exmnt)le of WX Word Correst)on- 
(lence 
a leaf and t has also only one child node which ix a 
leaf, th(;n we COllStrllet a lleW word correspondence 
called 1US from these two leaf nodes. This WE 
procedure is al)plied to all word correspondences. 
Note tlmt this word correst)ondence is not to se.le, ct 
one of candidates, rather it is a new finding of word 
corre, spondence by utilizing a special structm:e. For 
instance, in Figure 3, if there is a word eorrespol> 
dence 1)etween "ki" and "period" and there is no 
word correst)ondence between "ikou" and "transi- 
tion," then I<V,g(iko'u~ transition) will be found 1)3' 
this 1)roeedure. 
These WX and WS t)rocedures are continuously 
al)plied until no new word correspondences arc t'(mnd. 
Aft;er al)l)lying the above WX and I'VS pro(:e- 
dures, there are some target words t such that t is a 
destination of a l,l/C(.s ", t) and there ix no other 1,176 , 
whose destination ix t:. In this case, the lUG(s,t)  
correspondence andidate is chosen as a valid word 
correspondence b tween s and/,,  and it; is called a 
HzZ word eorrest)ondence. 
We call a source node or a target node of a word 
correspondence an anchor node in what tbllows. 
The above t)rocedures for finding word corre- 
sI)ondences are summarized as follows: 
Find WCs by consulting translation dictionary; 
Find WAs; 
whi le  (true) { 
find WXs; 
find WSs; 
i f  no new word corresp, is found, then  break; 
} 
find WZs;  
2.3 F ind ing  Phrasa l  Co l ' res l )ondences  
The next step is to tind phrasal correspondences 
based on word eorl'eSl)ondences t'(mnd t) 3, 1)roce.- 
dures described in tim previous section. What  we 
would like to retrieve here, is a set of phrasal cor- 
respondences which (:overs all elements of a paired 
dependency trees. 
In what follows, we (:all a portion of a tree which 
consists of nodes in a 1)att~ from a node ?t I (;o all- 
oth(;r node nu which is a descen(lanl; of n:l a lin-. 
ear tree denoted by LT(v,1, n~), and we denote a 
minimal sul)tree including st)coiffed nodes hi,  ..., n.~, 
l)y T (n l , . . . ,n , ) .  For instan(:(,~ in the English tree 
structure (the right tree) in Figure 4, LT(tcch, nology ,
science) is a rectangular area covering %eclmol- 
? tg "e e ~ ogy," and SOl ,no ,, anti .T(J'acl;or, cou'ntrjl ) is a 
1)olygonal area covering "factor,""atDcl,, . . . .  t)ol- 
icy," and "country." 
The tirst step is to find a 1)air of word correst)on- 
dences W, (.~'~, t ) and ~4q(.,.~, t ~) such that .,, a.,t  
s2 constructs a linear tree LT(si ,  s2) and there is no 
anchor node in th(' 1)al;h from s~ to s2 other than .s'~ 
and .s2, where 1UI and H~ denote any tyi)e of word 
('orrest)on(lences 2 and we assmne there is a word 
corresI)ondence t)etwee, n roots of source and (;arget 
trees by defmflt. We construct a t)hrasal correspon- 
dence fi'om source nodes in LT(s , , s2)  and target 
l/o(les itl r \ ] ' ( t : l , / '2 ) ,  (l()llote(t by \];'(l~,~F'(.q'l, .";2), 5\].n(tl, t2)). 
For illstall('e~ ill F ig l l re 41~ \]"11~ \]~12~ 1)'2~ 1)3 and 
\])4 tu.'e source portions of phrasal et)rrespondences 
found in this step. 
The next stel) checks, for ea(:h 1', if all anchor 
l lo(les of wor(1 eorres1)Oll(leile(?s wllose SOUlT(;e o1 ~;al- 
get node is included in P are al,eo included in P. 
If a t)hrasal correst)ondenee satisiies this condition, 
then it is called closed, otherwise it ix called open. 
Further, nodes which are not included in the I ) in 
question are called open nodes. If a l ) ix ot)en, then 
it ix merged with other 1)hrasal correspondences 
having ol)en nodes of P so that the merged 1)hrasal 
correspondence b comes (-losed. 
Next, each P~,, is checked if there is another l)q 
which shares any nodes ottmr than anchor nodes 
with P.,,. If this is the case, these P:., and 1~ are 
lnerged into one phrasal correspondence. In Figure 
4, t)hrasal correspondences i 11 and P12 are merged 
into P1, since their source I)ortions LT (haikei, koku) 
and LT (haikci, seisaku) share "doukou" which is 
not an anchor node. 
Finally, any path whose nodes other than the 
root are not included in any 1)s but the root node 
ix included in a 1 ) is searched for. This procedure 
2Since WC is not a word correspondence (it is a candi- 
date, of word corresi)ondence), it is llOi; conside, red here. 
908 
is apl)lied I;o 1)oth source a.nd (;arget trees. A im.th 
found 1)y this 1)ro(:(xlur(~ is called an open pal, h,, m~(t 
its root no(le is called a pivot. If such an Ol)en path 
is found, it is t)rocessed as follows: l, br each 1)ivot 
node, (a) if the t)ivot is not an mmhor nod(;, then 
open lmths originating fl:om the pivot is merged 
into a 1 ) having I;he pivot, (b) if the pivot is an 
~LIlChOf l lo(lo~ {;hOll 3_ llOW t)hl'~lS~L1 c()rFos1)oII(|(~IlC( ~, iS 
created from Ol)(m 1)ai;hs originating from the m> 
thor nodes of the word (:orrcsl)on(l(:ncc. 
In Figure 4, w(: get tinally four phrasal (:orr(:- 
Sl)on(lences l~, f~, l~, an(l l~t. 
! haikei.!,,: I - ................... { -~ factor',,,l ', 
i /~0 :: a ect " 
, ,  ',,(tre, nd) i f \  ~ ~-"  l i ;  
k . '/ ;~oy, v __: 
~, koku ( seisak~'~{t - - - - .~-  - -~  
~' (C0UrlttV)_l , (p0%,~ :t' .. technology .lrltly 
~< :::>i--- -::-:~ 
= 7 :: TLI io. ; I 
(major) ~,_/_ 
- X-~ / 
' giutu"\]\] 
(technolo~ly)l' .? science 
kagaku " 
(scie, nce) . -  
P4 
/ 
t 
t /  ff 
i 
Figm:e d: An l~;xaml)le of Finding Phrasal  Corr(> 
S\])Olld(~,IIC(',S 
The above 1)ro(:edures fl)r finding l)hrasal (:orr(> 
Ht)oIIdoIICOS ~-LF(~ SlllIllIl?~riz(Kl gtS fo l lows :  
Find initial Ps; 
Mea'ge a.n Ol)Cn 1>~ with other i ' s  having 
open nodes of 1}; 
Create new Ps 1)y merging \])s 
which have more tlmn 2 (:ommon nodes; 
Find ot)en path, alld 
if the t)ivot is ml mmhor, | ;hen 
merge the path to P having the anchor, 
o therwise  create new l ) by merging 
all open t)ai,hs having l;lm pivot; 
3 Exper iments  
3.1 C, o r lms  and  D ic t ionary  
We used (l()(;lllil(~'ll|;s t'rolil White Papers on S(:i- 
en(-e and Technology (1.994 to \ ]996) pul)lished by 
the S(:ience mid Technology Agency (STA) of tim 
.\]al)mmse govcrlim(~nl;. STA lmblished th(;se White 
PaI)ers in both Jat)mmse and English. The Com- 
mmfications l{esea.rch Laboratory of" the Ministry 
of Posts and Telecommuni(:a.tion of the .\]al)mmse 
goverlmmnt supl)lied us with the l)ilingual corpus 
wtfich is already roughly aligned. We made a bilin- 
gual cortms consisting of pa.rs(;d dependency struc- 
tures by using the KNP\[2\] .\]al)mmso, 1)arser ((l(wel- 
Ol)ed by Kyoto (hfive)sity) for .Jal)anes(~ sentences 
and the ESG\[5\] English 1)arser (developed by IBM 
Watson i{e, sear(:h Center) for English s(~nl;(!nces. 
We mad(} al)oul; 500 senl;(m(:e l)airs, each of whi(:h 
11~1,'4 ;I, OIlC-I;O-OII(', 80,11|;(',11(;0 (-orresl)onden(:(~,, fl'OI\[l (,\]lO 
raw (t~tta of l;he, White l)al)crs, mid s(',l(;(;i;(xl rm> 
domly aboul; 130 s('aH;en(:c pairs for (',Xl)(Mm(;nts. 
ilow(wer, since a 1)nrser does not always \])ro(hwe 
(;orl'c(;\[; 1);~l"s(t t;re(}s~ wo (~x(:lude(1 some, ~(~ii|;(Hic(~ p;Lil's 
wlfich have severe 1)arse errors, and tinally got i\[15 
S(~,II\[;OIlC(; pairs as a, to, st s(%. 
As a trm~slation wor(1 dictionary/)etw(',(m .l at)ml(',s(; 
and English, we, tirsl; used ,l-to-l~; trmlslati()n (li(:- 
l, ionary which has mot(,' t lmn 100,000 (,ifl;l'i(;~, but 
we, fi)un(l l;}l~/{; l ller(? are som(~ word ('orr(~sl)Oll(l(~,llt;(~s 
not (:()v(ued in this di(:ti()nary. Tlmref()rG we merged 
(retries fi:om \]';-t;o-.I translatioll dictionary in order 
to get; much broad (:ov(wag(,'. The l;oDd nulnl)(}r ()f 
entries a.re now more I;ha.n \[50,000. 
3.2 Exper inmnta l  Resu l ts  
Td)le i shows l;he result of (~Xl)c, rimeni; fl)r tind- 
ing word correspond(nm(~s. A row with ALL in th(', 
l:yl)e cohmm shows Llle total  ~CClll'~lcy of WOI'(1 cor- 
r(Lqpolld('31c(~s and ol;\]l{~r rows sh()\v Llle .~iCClll'ktcy of 
each t, yt)e. It is clear that WA (:orr(~sl)Olld(~ll(;(',s 
have a very high a('cura(:y. Other word (:orresl)On-- 
do, nc(,,s also ha.ve a roJatively high ac(:ura(:y. 
Table 2 shows tim remflt of exl)erimenl,s for find~ 
ing 1)hrasal correspondences. The row with ALL in 
I;he l;yt)c cohlmn shows l;he l;ol;al accuracy of phrasal 
(:ol'r(~sl)ondo, n(:(~s found by the 1)rol)osed 1)rocedure. 
This ac(:macy level is not I)romising and it is not; 
useful for later 1)ro(:e, sses since it needs human (:he(:k- 
ing ml(l (:orrec?ion. Therefore, we sul)categoriz(~ 
each phrasal corl'eSpond(m('es, and check l;he a('- 
(:uracy for each subca.tegory. 
We consider the following sut)catcgories for 1)hrasal 
('x)rl'(}Sl)olidell(-(~s: 
? MIN ... The minimal  t)hrasal correst)ondence, 
that is, I'(1Zl'(.s'l, .s2), LT(t l ,  t2)) such that  (;herc 
909 
type 
ALL 
WA 
WX 
WS 
WZ 
1111111. nunl .  of SUCCESS 
of correct ratio found corresp. (%) corresp. 
771 745 96.63 
612 600 98.03 
131 118 90.07 
13 12 92.3 
15 15 100 
Table h Experimental Result of Word Correspon- 
dences 
are word correspondences W(s1,  t l )  and W 
(s2,t2), s2 is a direct child of St and t2 is a 
direct child of tl. 
? LTX ... P(LT(.s'I,S2),LT(tl,t2)) such that 
all nodes other titan s2 and t2 have only one 
child node. 
? LTY ... P(LT(sl,.S2), LT(tl, t2)) such that 
all nodes other than Sl, s2,1':1 and t.2 have only 
one child node. 
LTX is a special case of LTY, since Sl and tl of 
LTX must have only one child node, on the other 
hand, ones of LTY may have more than two child 
nodes. A subcategory test tbr a phrasal correspon- 
dence is done in the above order. Exmnples of these 
subcategories are shown in Fig 5. 
Tlm result of these subcategories are also shown 
in Table 2. Subcategories MIN and LTX have very 
high accuracy and this result is very promising, 
since we can avoid nmnual checking for ttmse phrasal 
correst)ondences , or we would check only these types 
of t)hrasal correspondences mmmally and discard 
other types. 
As stated earlier, since we removed only sen- 
tences with severe parsing errors from the test set, 
please note that the above mtmbers of experimental 
results are calculated for a bilingual parsed corpus 
including parsing errors. 
4 D iscuss ion  
There have been some studies on structural align- 
Inent of bilingual texts such as \[1, 4, 13, 3, 6\]. Our 
work is similar to these previous tudies at the con- 
ceptual level, but different in some aspects. \[1\] 
reported a method for extracting translation tem- 
plates by CKY parsing of bilingual sentences. This 
work is to get phrase-structure level phrasal cor- 
respondences, but our work is to get dependency- 
structure level phrasal correspondences. \[4\] pro- 
posed a method for extracting structural matclfing 
(pairs of dependency trees) by calculating matching 
similarities of two dependency structures. Their 
work focuses on tile parsing ambiguity resolution 
by calculating structural matching. Further, \[3, 6\] 
proposed structural alignnmnt of dependency struc- 
tures. Their work assuined tha.t least common an- 
cestors of each fragment of a structural correspon- 
dence are preserved, but our work does not have 
such structural restriction. \[13\] is different o oth- 
ers in that it tries to find phrasal correspondences 
by comt)aring a MT result and its manual correc- 
tion. 
In addition to these differences, the main differ- 
ence is to find classes (or categories) of phrasal cor- 
respondences which have high accuracy. In general, 
since bilingual structural alignment is very compli- 
cated and difficult task, it; is very hard to get more 
than 90% accuracy in total. If we get only such 
an accuracy rate, the result is not useful, since we 
need manual clmcks tbr the all correspondences re-
trieved. But, if we can get some classes of phrasal 
correspondence with, for instance, more than 90% 
accuracy rate, then we can reduce manual clmck- 
ing for phrasal correspondences in such classes, and 
this reduces the development cost of translation 
patterns used in later corpus-based translation pro- 
tess. As shown in the previous section, we could 
find ttmt all (:lasses of word correspondences and 
two subclasses of phrasal correspondences are more 
than 90% accurate. 
When actually using this automatically retrieved 
structural correspondence data, we must consider 
how to manually correct the incomplete parts and 
how to reuse mamlal correction data if the parser 
results are ctmnged. 
As for the tbrlner issue, we need an easy-to-use 
tool to modify correspondences to reduce the cost 
of mmmal operation. We have developed a GUI 
tool as shown in Figure 6. In this figure, the bot- 
tom half presents a pair of source and target depen- 
dency structures with word correspondences (solid 
lines) and phrasal correspondences ( equences of 
slmded circles). You can easily correct correspon- 
dences by looking at this graplfical presentation. 
As for tlm latter issue, we must develop meth- 
ods for reusing the manual correction data as much 
as possible even if tim parser outputs are changed. 
We have developed a tool for attaching phrasal 
correspondences by using existing phrasal corm- 
spondence data. This is implemented as follows: 
Each phrasal correspondence is assigned a signa- 
ture which is a pair of source and target, sentences, 
each of which tins bracketed segments which are in- 
cluded in the phrasal correspondence. For instance, 
910 
I 
tmihatu 
((~uebiomeR) 
,~10 
gijutu -,,- 
(tedr, do?.t?) 
I 
--,<Jeveloprned 
0f 
--,'.-tect'lrlOlogy 
ILl Zl.l~l.l ,~ 
\[<dime} 
go 
seityou 
(i, otldl} 
I 
ya t~.a d 0ute ki 
. . . . . .  ,.corlti nue 
*o 
- -  shob'o 
-. ~obj 
" "k 9 Kl t;dh 
/ ' / \ 
economic 
f 
kaga t,lJ 
"~. 
% 
unparallelled 
. ?tij d u - \ [ - -~e  c;h n 010 ?tl 
~<~o~l I "-)/. ' ,  ,,o  ol oy- .  x ,,0,,, 
ka nte n 
{a~laedl 
korera .,- 
science. 
# 
g 
z 
ee 
(a) MIN (b) LTX (c) LTY 
p Urp ose 
sPec I 
-,qhis 
Figure 5: Examples of Categories of Phrasal Correst)ondences 
A: 
5115511. of 
type found 
COl; l 'es i ) .  
ALL 678 
MIN 223 
LTX 17 
LTY 27 
B:  
I515151. of 
(:orrect 
co5-5"(~Sl). 
431 
215 
(~: 
SllC(;(~SS 
ratio 
~/A (%) 
63.56 
96A1 
D:  
nunL of nodes 
covered t)y A 
7248 
1234 
E: 
nunl. of nodes 
covered by B 
4278 
1194 
F: 
Sl lCCeSS 
ratio 
E/D (%) 
59.02 
96.76 
17 100 153 153 100 
20 I 74.07 253 191 75A9 
I 
Tal)le 2: lgxperinmntal Fh',sult ot' Phrasal Correst)on(len :es 
the following signature is made h)r a i)hrasal corre- 
Sl)on(lence (c) in Figure 5: 
(.~i:j) 
... \[korer~ no kanten karmlo\] kagaku \[gi- 
j u tu \ ]  ... 
... science and \[technology fl:om this 
lmrl)ose\] ... 
(/.~io) 
In the above e, xample, segments betwee, n '\[' and '\]' 
represent a phrasal correspondence. 
If new parsed dqmndency structures for a sen- 
tence pair is given, for each phrasal correspondence 
signature of the sentence pair, nodes in the struc- 
tures wtfich are inside 1)rackets of the signature are 
marked, mid if there is a minimal sul)tree consist- 
ing of only marked nodes, then a phrasal corre- 
Sl)ondence is reconstructed from the phrasal corre- 
spondence signature. By using this tool, we can 
efficiently reuse the manual efforts as much as pos- 
sible even if parsers are updated. 
5 Conc lus ion  
Ill this I)al)er, we have t)rol)osed methods for 
finding structural correspondences (word correst)on- 
dences and i)hrasal corr(;spondences) of bilingual 
parsed corpus. Further, we showed that the t)reci- 
sion of word correst)ond(mces and some catc'gories 
of t)hrasal corresl)ondences found 1)y our methods 
are highly accurate, and these correst)ondences can 
reduce the cost of trm~slation pattern accumula- 
tion. 
In addition to these results, we showed a GUI 
tool for mmmal correction and a tool for reusing 
previous correspondence data. 
As fld;ure directions, we will find more subclasses 
with high accuracy to reduce the cost for transla- 
tion pattern preparation. 
We believe that these methods and tools can ac- 
celerate the collection of a large set of translation 
patterns and the developlnent of a corlms-based 
translation system. 
911 
~rel id="28" type="P4" src="3.4,9,10,11,12.13" tgt="1,2.3,4,8,9,12" eval="T~R UE" score="O" geoeratlon='' subtype="orff' org=" con'lment='"'= 
~rel id="29" type="P5" src="1,2,3" tg~"l 0,11,12" BvaI="TRUE" score="0" generation="" subtype="org" org=" comment:="'> 
~rel ld="3O" type="P5" src="5.6.7" \[g1="5.6,7" eva~"TRUE" score="0" generation=" sublype="org" org="" cornmen~''~ 
<tel id="31" type="P5" src="7.8,9" tg~"7.F' evaI="TRUE" ecore="O" generation=" subtype="org" erg=" cerumen .t=""~. 
'~rel id="32" type="P5" src="3,4.9.10,11.12.13" tgt="1,2,3,4.e,g,12" evaI="TRUE" score="0" generation="' subtype="org" org='' comment=-"'-'. 
! 
\ 
L 
4\ 
h6 ac len ;~aad tactiilC:;Id6i' g,0tiCid;~ ; f  rn~\[,~r.c~un~les 
.. . . . . .  , . . . .  ? ~- .  
,. ? L:i : :  ? : i % 
Figure 6: An GUI tool for presenting/manipulating structural correspondences 
References  
\[1\] Kaji, H., Kids, Y., and Morimoto, Y., "Learning Trans- 
lation Templates from Bilingual Texts," Proc. of Coling 
92, pp. 672-678, I992. 
\[2\] Kurohashi, S., and Nagao, M., "A Syntactic Analy~ 
sis Method of Long Japanese Sentences based on the 
Detection of Conjunctive Structures," Computational 
Linguisties~ Voh 20, No. 4, 1994. 
\[3\] Grishman, R., "Iterative Alignment of Syntactic Struc- 
tures for a Bilingual Corpus," Proe. of 2nd Workshop 
for Very Large Corpora, pp. 57-68, 1994. 
\[4\] Matsumoto, Y., Ishimoto, H., and Utsuro, T., "Struc- 
tural Matching of Parallel Texts," Proc. of the 31st of 
ACL,  pp. 23-30, 1993. 
\[5\] MeCord, C. M., "Slot Grammars," Computational Lin- 
guistics, Voh 6, pp. 31-43, 1980. 
\[6\] Meyers, A., Yanharber, R., and Grishman, R., "Align- 
ment of Shared Forests for Bilingual Corpora," Proc. of 
the 16th of COLING, pp. 460-465, June 1996. 
\[7\] Nagao, M., "A Framework of a Mechanical Translation 
between Japanese and English by Analogy Principle," 
Elithorn, A. and Banerji, R. (eds.) : Artificial and Hu- 
man Intelligence , NATO 1984. 
\[8\] Sato, S., and Nagao, M. "Toward Memory-based Trans- 
lation," Proc. of 13th COLING, August 1990. 
\[19\] Sumita, E., Iida, II., and Kohyama, H. "'Translating 
with Examples: A New Approach to Machine 3Yanslao 
tion," Proc. of" Info Japan 90, 1990. 
\[10\] Takeda, K., "Pattern-Based Context-Free Grammars 
for Machine ~l~anslation," Proc. of 34th ACL, pp. 144-- 
15I, June 1996. 
\[11\] Takeda, K., "Pattern-Based Machine ~lYanslation," 
Proc. of 16th COLING, Vol. 2, pp. 1155-1158, August 
1996. 
\[12\] Watanabe, H. "A Similarity-Driven Transfer System," 
Proc. of the 14th COLING, Vol. 2, pp. 770.-776, 1992. 
\[13\] ~Vatanabe, H. "A Method for Extracting ~IYanslation 
Patterns from ~lS'anstation Examples," Proc. of the 5th 
Int. Conf. on Theoretical and Methodological Issues in 
Machine Translation, pp. 292-301, 1993. 
\[14\] Watanabe, H., and Takeda, K., "A Pattern-based Ma.- 
chine Translation System Extended by Example-based 
Processing," Proc. of the 36th ACL & 17th COLING, 
Vol. 2, pp. 1369o1373, 1998. 
912 
53
54
55
56
57
58
Orthographic Disambiguation Incorporating Transliterated Probability
Eiji ARAMAKI Takeshi IMAI Kengo Miyo Kazuhiko Ohe
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8655, Japan
aramaki@hcc.h.u-tokyo.ac.jp
Abstract
Orthographic variance is a fundamental
problem for many natural language process-
ing applications. The Japanese language, in
particular, contains many orthographic vari-
ants for two main reasons: (1) transliterated
words allow many possible spelling varia-
tions, and (2) many characters in Japanese
nouns can be omitted or substituted. Pre-
vious studies have mainly focused on the
former problem; in contrast, this study has
addressed both problems using the same
framework. First, we automatically col-
lected both positive examples (sets of equiv-
alent term pairs) and negative examples (sets
of inequivalent term pairs). Then, by using
both sets of examples, a support vector ma-
chine based classifier determined whether
two terms (t1 and t2) were equivalent. To
boost accuracy, we added a transliterated
probability P (t1|s)P (t2|s), which is the
probability that both terms (t1 and t2) were
transliterated from the same source term (s),
to the machine learning features. Exper-
imental results yielded high levels of ac-
curacy, demonstrating the feasibility of the
proposed approach.
1 Introduction
Spelling variations, such as ?center? and ?centre?,
which have different spellings but identical mean-
ings, are problematic for many NLP applications
including information extraction (IE), question an-
swering (QA), and machine transliteration (MT). In
Table 1: Examples of Orthographic Variants.
spaghetti Thompson operation
* ?? indicates a pronunciation. () indicates a translation.
this paper, these variations can be termed ortho-
graphic variants.
The Japanese language, in particular, contains
many orthographic variants, for two main reasons:
1. It imports many words from other languages
using transliteration, resulting in many possible
spelling variations. For example, Masuyama et
al. (2004) found at least six different spellings
for? spaghetti?in newspaper articles (Table 1
Left).
2. Many characters in Japanese nouns can be
omitted or substituted, leading to tons of in-
sertion variations (Daille et al, 1996) (Table 1
Right).
To address these problems, this study developed a
support vector machine (SVM) based classifier that
48
can determine whether two terms are equivalent. Be-
cause a SVM-based approach requires positive and
negative examples, we also developed a method to
automatically generate both examples.
Our proposed method differs from previously de-
veloped methods in two ways.
1. Previous studies have focused solely on the for-
mer problem (transliteration); our target scope
is wider. We addressed both transliteration
and character omissions/substitutions using the
same framework.
2. Most previous studies have focused on back-
transliteration (Knight and Graehl, 1998; Goto
et al, 2004), which has the goal of generating a
source word (s) for a Japanese term (t). In con-
trast, we employed a discriminative approach,
which has the goal of determining whether two
terms (t1 and t2) are equivalent. These two
goals are related. For example, if two terms (t1
and t2) were transliterated from the same word
(s), they should be orthographic variants. To
incorporate this information, we incorporated
a transliterated-probability (P (s|t1)?P (s|t2))
into the SVM features.
Although we investigated performance using
medical terms, our proposed method does not de-
pend on a target domain1.
2 Orthographic Variance in Dictionary
Entries
Before developing our methodology, we examined
problems related to orthographic variance.
First, we investigated the amount of orthographic
variance between two dictionaries? entries (DIC1
(Ito et al, 2003), totaling 69,604 entries, and DIC2
(Nanzando, 2001), totaling 27,971 entries).
Exact matches between entries only occurred for
10,577 terms (15.1% of DIC1, and 37.8% of DIC2).
From other entries, we extracted orthographic vari-
ance as follows.
STEP 1: Extracting Term Pairs with Similar
Spelling
1The domain could affect the performance, because most of
medical terms are imported from other languages, leading to
many orthographic variants.



Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 464?467,
Prague, June 2007. c?2007 Association for Computational Linguistics
UTH: Semantic Relation Classification using Physical Sizes
Eiji ARAMAKI Takeshi IMAI Kengo MIYO Kazuhiko OHE
The University of Tokyo Hospital department
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
aramaki@hcc.h.u-tokyo.ac.jp
Abstract
Although researchers have shown increas-
ing interest in extracting/classifying seman-
tic relations, most previous studies have ba-
sically relied on lexical patterns between
terms. This paper proposes a novel way to
accomplish the task: a system that captures
a physical size of an entity. Experimental
results revealed that our proposed method is
feasible and prevents the problems inherent
in other methods.
1 Introduction
Classification of semantic relations is important to
NLP as it would benefit many NLP applications,
such as machine translation and information re-
trieval.
Researchers have already proposed various
schemes. For example, Hearst (1992) manually de-
signed lexico-syntactic patterns for extracting is-a
relations. Berland and Charniak (1999) proposed a
similar method for part-whole relations. Brin (1998)
employed a bootstrapping algorithm for more spe-
cific relations (author-book relations). Kim and
Baldwin (2006) and Moldovan et al(2004) focused
on nominal relations in compound nouns. Turney
(2005) measured relation similarity between two
words. While these methods differ, they all utilize
lexical patterns between two entities.
Within this context, our goal was to utilize infor-
mation specific to an entity. Although entities con-
tain many types of information, we focused on the
physical size of an entity. Here, physical size refers
to the typical width/height of an entity. For example,
we consider book to have a physical size of 20?25
cm, and book to have a size of 10?10 m, etc.
We chose to use physical size for the following
reasons:
1. Most entities (except abstract entities) have a
physical size.
2. Several semantic relations are sensitive to phys-
ical size. For example, a content-container rela-
tion (e1 content-container e2) naturally means
that e1 has a smaller size than e2. A book is
also smaller than its container, library. A part-
whole relation has a similar constraint.
Our next problem was how to determine physi-
cal sizes. First, we used Google to conduct Web
searches using queries such as ?book (*cm x*cm)?
and ?library (*m x*m)?. Next, we extracted numeric
expressions from the search results and used the av-
erage value as the physical size.
Experimental results revealed that our proposed
approach is feasible and prevents the problems in-
herent in other methods.
2 Corpus
We used a corpus provided by SemEval2007 Task
#4 training set. This corpus consisted of 980 anno-
tated sentences (140 sentences?7 relations). Table
1 presents an example.
Although the corpus contained a large quantity of
information such as WordNet sense keys, comments,
etc., we used only the most pertinent information:
entity1 (e1), entity2 (e2), and its relation (true/false)
464
The <e1>library</e1> contained <e2>books
</e2> of guidance on the processes.
WordNet(e1) = "library\%1:14:00::",
WordNet(e2) = "book\%1:10:00::",
Content-Container(e2, e1) = "true",
Query = "the * contained books"
Table 1: An Example of Task#4 Corpus.
Figure 1: Three types of Features.
1
. For example, we extracted a triple example (li-
brary, book, true from Table 1.
3 Method
We applied support vector machine (SVM)-based
learning (Vapnik, 1999) using three types of fea-
tures: (1) basic pattern features (Section 3.1), (2) se-
lected pattern features (Section 3.2), and (3) physical
size features (Section 3.3). Figure 1 presents some
examples of these features.
3.1 Basic Pattern Features
First, the system finds lexical patterns that co-occur
with semantic relations between two entities (e1 and
e2). It does so by conducting searches using two
queries ?e1 * e2? and ?e2 * e1?. For example, two
queries, ?library * book? and ?book * library?, are
generated from Table 1.
Then, the system extracts the word (or word se-
quences) between two entities from the snippets in
the top 1,000 search results. We considered the ex-
tracted word sequences to be basic patterns. For ex-
ample, given ?...library contains the book...?, the ba-
sic pattern is ?(e1) contains the (e2)? 2.
1Our system is classified as an A4 system, and therefore
does not use WordNet or Query.
2This operation does not handle any stop-words. Therefore,
We gathered basic patterns for each relation, and
identified if each pattern had been obtained as a
SVM feature or not (1 or 0). We refer to these fea-
tures as basic pattern features.
3.2 Selected Pattern Features
Because basic pattern features are generated only
from snippets, precise co-occurrence statistics are
not available. Therefore, the system searches again
with more specific queries, such as ?library contains
the book?. However, this second search is a heavy
burden for a search engine, requiring huge numbers
of queries (# of samples ? # of basic patterns).
We thus selected the most informative n patterns
(STEP1) and conducted specific searches (# of sam-
ples ? n basic patterns)(STEP2) as follows:
STEP1: To select the most informative patterns,
we applied a decision tree (C4.5)(Quinlan,
1987) and selected the basic patterns located in
the top n branches 3.
STEP2: Then, the system searched again us-
ing the selected patterns. We considered log
weighted hits (log
10
|hits|) to be selected pat-
tern features. For example, if ?library contains
the book? produced 120,000 hits in Google, it
yields the value log
10
(12, 000) = 5.
3.3 Physical Size Features
As noted in Section 1, we theorized that an entity?s
size could be a strong clue for some semantic rela-
tions.
We estimated entity size using the following
queries:
1. ?< entity > (* cm x * cm)?,
2. ?< entity > (* x * cm)?,
3. ?< entity > (* m x * m)?,
4. ?< entity > (* x * m)?.
In these queries, < entity > indicates a slot for
each entity, such as ?book?, ?library?, etc. Then, the
system examines the search results for the numerous
expressions located in ?*? and considers the average
value to be the size.
?(e1) contains THE (e2)? and ?(e1) contains (e2)? are different
patterns.
3In the experiments in Section 4, we set n = 10.
465
Precision Recall F
?=1
PROPOSED 0.57 (=284/497) 0.60 (=284/471) 0.58
+SEL 0.56 (=281/496) 0.59 (=281/471) 0.57
+SIZE 0.53 (=269/507) 0.57 (=269/471) 0.54
BASELINE 0.53 (=259/487) 0.54 (=259/471) 0.53
Table 2: Results.
When results of size expressions were insufficient
(numbers < 10), we considered the entity to be non-
physical, i.e., to have no size.
By applying the obtained sizes, the system gener-
ates a size feature, consisting of six flags:
1. LARGE-e1: (e1?s X > e2?s X) and (e1?s Y > e2?s Y)
2. LARGE-e2: (e1?s X < e2?s X) and (e1?s Y < e2?s Y)
3. NOSIZE-e1: only e1 has no size.
4. NOSIZE-e2: only e2 has no size.
5. NOSIZE-BOTH: Both e1 and e2 have no size.
6. OTHER: Other.
4 Experiments
4.1 Experimental Set-up
To evaluate the performance of our system, we
used a SemEval-Task No#4 training set. We com-
pared the following methods using a ten-fold cross-
validation test:
1. BASELINE: with only basic pattern features.
2. +SIZE: BASELINE with size features.
3. +SEL: BASELINE with selected pattern features.
4. PROPOSED: BASELINE with both size and selected
pattern features.
For SVM learning, we used TinySVM with a lin-
ear kernel4.
4.2 Results
Table 2 presents the results. PROPOSED was the
most accurate, demonstrating the basic feasibility of
our approach.
Table 3 presents more detailed results. +SIZE
made a contribution to some relations (REL2 and
REL4). Particularly for REL4, +SIZE significantly
boosted accuracy (using McNemar tests (Gillick and
4http://chasen.org/ taku/software/TinySVM/
Figure 2: The Size of a ?Car?.
Cox, 1989); p = 0.05). However, contrary to our ex-
pectations, size features were disappointing for part-
whole relations (REL6) and content-container rela-
tions (REL7).
The reason for this was mainly the difficulty in es-
timating size. Table 4 lists the sizes of several enti-
ties, revealing some strange results, such as a library
sized 12.1 ? 8.4 cm, a house sized 53 ? 38 cm, and
a car sized 39 ? 25 cm. These sizes are unusually
small for the following reasons:
1. Some entities (e.g.?car?) rarely appear with
their size,
2. In contrast, entities such as ?toy car? or ?mini
car? frequently appear with a size.
Figure 2 presents the size distribution of ?car.?
Few instances appeared of real cars sized approxi-
mately 500 ? 400 cm, while very small cars smaller
than 100 ? 100 cm appeared frequently. Our current
method of calculating average size is ineffective un-
der this type of situation.
In the future, using physical size as a clue for de-
termining a semantic relation will require resolving
this problem.
5 Conclusion
We briefly presented a method for obtaining the size
of an entity and proposed a method for classifying
semantic relations using entity size. Experimental
results revealed that the proposed approach yielded
slightly higher performance than a baseline, demon-
strating its feasibility. If we are able to estimate en-
466
Relation PROPOSED +SEL +SIZE BASELINE
Precision 0.60 (=50/83) 0.56 (=53/93) 0.54 (=53/98) 0.50 (=53/106)
REL1 Recall 0.68 (=50/73) 0.72 (=53/73) 0.72 (=53/73) 0.72 (=53/73)
(Cause-Effect) F
?=1
0.64 0.63 0.59 0.61
Precision 0.59 (=43/72) 0.60 (=44/73) 0.56 (=45/79) 0.55 (=44/79)
REL2 Recall 0.60 (=43/71) 0.61 (=44/71) 0.63 (=45/71) 0.61 (=44/71)
(Instrument-Agency) F
?=1
0.60 0.61 0.59 0.58
Precision 0.70 (=56/80) 0.73 (=55/75) 0.65 (=54/82) 0.68 (=51/74)
REL3 Recall 0.65 (=56/85) 0.64 (=55/85) 0.63 (=54/85) 0.60 (=51/85)
(Product-Producer) F
?=1
0.67 0.68 0.64 0.64
Precision 0.41 (=23/56) 0.35 (=18/51) 0.48 (=24/49) 0.52 (=13/25)
REL4 Recall 0.42 (=23/54) 0.33 (=18/54) 0.44 (=24/54) 0.24 (=13/54)
(Origin-Entity) F
?=1
0.41 0.34 0.46 0.32
Precision 0.62 (=40/64) 0.61 (=40/65) 0.56 (=28/50) 0.56 (=29/51)
REL5 Recall 0.68 (=40/58) 0.68 (=40/58) 0.48 (=28/58) 0.50 (=29/58)
(Theme-Tool) F
?=1
0.65 0.65 0.51 0.53
Precision 0.45 (=46/101) 0.46 (=46/100) 0.41 (=49/118) 0.43 (=53/123)
REL6 Recall 0.70 (=46/65) 0.70 (=46/65) 0.75 (=49/65) 0.81 (=53/65)
(Part-Whole) F
?=1
0.55 0.55 0.53 0.56
Precision 0.63 (26/41) 0.64 (=25/39) 0.51 (=16/31) 0.55 (=16/29)
REL7 Recall 0.40 (26/65) 0.38 (=25/65) 0.24 (=16/65) 0.24 (=16/65)
(Content-Container) F
?=1
0.49 0.48 0.33 0.34
Table 3: Detailed Results.
entity # size
library 51 12.1?8.4 m
room 204 5.4?3.5 m
man 75 1.5?0.5 m
benches 33 93?42 cm
granite 68 76?48 cm
sink 34 57?25 cm
house 86 53?38 cm
books 50 46?24 cm
car 91 39?25 cm
turtles 15 38?23 cm
food 38 35?26 cm
oats 16 24?13 cm
tumor shrinkage 6 -
habitat degradation 5 -
Table 4: Some Examples of Entity Sizes.
?#? indicates the number of obtained size expressions.
?-? indicates a ?NO-SIZE? entity.
tity sizes more precisely in the future, the system
will become much more accurate.
References
Matthew Berland and Eugene Charniak. 1999. Finding parts
in very large corpora. In Proceedings of the Annual Con-
ference of the Association for Computational Linguistics
(ACL1999), pages 57?64.
Sergey Brin. 1998. Extracting patterns and relations from the
world wide web. In WebDB Workshop at 6th International
Conference on Extending Database Technology, EDBT?98,
pages 172?183.
L. Gillick and SJ Cox. 1989. Some statistical issues in the com-
parison of speech recognition algorithms. In Proceedings of
IEEE International Conference on Acoustics, Speech, and
Signal Processing, pages 532?535.
M. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of International Confer-
ence on Computational Linguistics (COLING1992), pages
539?545.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics. In Pro-
ceedings of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 491?498.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. Proceedings of HLT/NAACL-2004 Workshop on
Computational Lexical Semantics.
J.R. Quinlan. 1987. Simplifying decision trees. International
Journal of Man-Machine Studies, 27(1):221?234.
Peter D. Turney. 2005. Measuring semantic similarity by latent
relational analysis. In Proceedings of the Nineteenth Inter-
national Joint Conference on Artificial Intelligence (IJCAI-
05), pages 1136?1141.
Vladimir Vapnik. 1999. The Nature of Statistical Learning
Theory. Springer-Verlag.
467
Proceedings of the Workshop on BioNLP, pages 185?192,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
TEXT2TABLE:  
Medical Text Summarization System based on Named Entity 
Recognition and Modality Identification 
 
 
Eiji ARAMAKI Yasuhide MIURA Masatsugu TONOIKE 
The university of Tokyo Fuji Xerox Fuji Xerox 
eiji.aramaki@gmail.com Yasuhide.Miura@fujixerox.co.jp masatsugu.tonoike@fujixerox.co.jp 
 
Tomoko OHKUMA 
 
Hiroshi MASHUICHI 
 
Kazuhiko OHE 
Fuji Xerox Fuji Xerox The university of Tokyo Hospital 
ohkuma.tomoko@fujixerox.co.jp hiroshi.masuichi@fujixerox.co.jp kohe@hcc.h.u-tokyo.ac.jp 
 
 
 
Abstract 
With the rapidly growing use of electronic 
health records, the possibility of large-scale 
clinical information extraction has drawn 
much attention. It is not, however, easy to ex-
tract information because these reports are 
written in natural language. To address this 
problem, this paper presents a system that 
converts a medical text into a table structure. 
This system?s core technologies are (1) medi-
cal event recognition modules and (2) a nega-
tive event identification module that judges 
whether an event actually occurred or not. 
Regarding the latter module, this paper also 
proposes an SVM-based classifier using syn-
tactic information. Experimental results dem-
onstrate empirically that syntactic information 
can contribute to the method?s accuracy. 
1 Introduction 
The use of electronic texts in hospitals is increas-
ing rapidly everywhere. This study specifically 
examines discharge summaries, which are reports 
generated by medical personnel at the end of a pa-
tient?s hospital stay. They include massive clinical 
information about a patient?s health, such as the 
frequency of drug usage, related side-effects, and 
correlation between a disease and a patient?s ac-
tions (e.g., smoking, drinking), which enables un-
precedented large-scale research, engendering 
promising findings. 
N
A
(1
(2
(3
                                                          
evertheless, it is not easy to extract clinical in-
formation from the reports because these reports 
are written in natural language. An example of a 
discharge summary is presented in Table 1. The 
table shows records that are full of medical jargon, 
acronyms, shorthand notation, misspellings, and 
sentence fragments (Tawanda et al, 2006). 
To address this problem, this paper presents a 
proposal of a system that extracts medical events 
and date times from a text. It then converts them 
into a table structure. We designate this system 
TEXT2TABLE, which is available from a web 
site 1 . The extraction method, which achieves a 
high accuracy extraction, is based on Conditional 
Random Fields (CRFs) (Lafferty et al, 2001). 
nother problem is posed by events that do not 
actually occur, i.e., future scheduled events, events 
that are merely intended to take place, or hypo-
thetical events. As described herein, we call such 
non-actual events negative events. Negative 
events are frequently mentioned in medical re-
cords; actually, in our corpus, 12% of medical 
events are negative. Several examples of negative 
events (in italic letters) are presented below: 
 
) no headache 
) keep appointment of radiotherapy 
) .. will have intravenous fluids 
1 http://lab0.com/  
185
(4
(4'
(5
th
(6
ac
A
B
T
T
A
) .. came for radiotherapy 
) .. came for headache 
) Every week radiation therapy and chemical 
erapy are scheduled 
) Please call Dr. Smith with worsening head-
he or back pain, or any other concern. 
 
Negative events have two characteristics. First, 
various words and phrases indicate that an event is 
negative. For this study, such a word or phrase that 
makes an event negative is called a negative trig-
ger. For instance, a negation word ?no? is a nega-
tive trigger in (1). A noun ?appointment? in (2) is a 
negative trigger. Similarly, the auxiliary ?will? in 
(3) signals negation. More complex phenomena are 
presented in (4) and (4'). For instance, ?radiother-
apy? in (4) is a negative event because the therapy 
will be held in the future. In contrast, ?headache? 
in (4') is not negative because a patient actually has 
a ?headache?. These indicate that a simple rule-
based approach (such as a list of triggers) can only 
imply classification of whether an event is negative 
or not, and that information of the event category 
(e.g., a therapy or symptom) is required. 
nother characteristic is a long scope of a nega-
tive trigger. Although negative triggers are near the 
descriptive words of events in (1)?(4), there could 
alternatively be a great distance of separation, as 
portrayed in (5) and (6). In (5), a noun coordina-
tion separates a negative trigger from the event. In 
(6), the trigger ?please? renders all events in that 
sentence negative. These indicate that neighboring 
words are insufficient to determine whether an 
event is negative or not. To deal with (5), syntactic 
information is helpful because the trigger and the 
event are neighboring in the dependency structure, 
as portrayed in Fig. 2. To deal with (6), bag-of-
word (BOW) information is desired. 
ecause of the observation described above, this 
paper presents a proposal of a classifier: whether 
an event is negative or not. The proposed classifier 
uses various information, the event category, 
neighboring words, BOW, and dependent phrases. 
he point of this paper is two-fold: (1) We pro-
pose a new type of text-summarizing system 
(TEXT2TABLE) that requires a technique for a 
negative event identification. (2) We investigate 
what kind of information is helpful for negative 
event identification. 
he experiment results revealed that, in spite of 
the risk of parsing error, syntactic information can 
contribute to performance, demonstrating the fea-
sibility of the proposed approach. 
lthough experiments described in this paper are 
related to Japanese medical reports, the proposed 
method does not depend on specific languages or 
domains. 
 
Table 1: A Health Record Sample. 
BRIEF RESUME OF HOSPITAL COURSE : 57 yo with 
NSCLCa with back pain and headache . Trans-
ferred from neurosurgery for additional mgmt 
with palliative XRT to head . Pt initially 
presented with cough and hemoptysis to his 
primary MD . On CXR he was found to have a 
upper left lobe mass . He subsequently un-
derwent bronchoscopy and bx revealed non-
small cell adeno CA. STaging revealed multi-
ple bony mets including skull, spine with 
MRI revealing mild compression of vertebral 
bodies at T9, T11, T12 . T9 with encroach-
ment of spinal cord underwent urgent XRT 
with no response so he was referred to neu-
rosurgery for intervention . MRI-rt. fron-
tal, left temporal, rt cerebellar 
hemorrhagic enhancing lesions- most likely 
extensive intracranial mets? T-spine surgery 
considered second priority and plan to radi-
ate cranially immediately with steroid and 
anticonvulsant . He underwent simulation on 
3/28 to whole brain and T3-T7 fields with 
plan for rx to both sites over 2.5 weeks. 
Over the past 2 weeks he has noted frontal 
and occipital HA with left eyelid swelling, 
ptosis, and denies CP, SOB, no sig. BM in 
past 5 days, small amt of stool after sup-
pository. Neuro?He was Dilantin loaded and a 
level should be checked on 3/31 . He is to 
continue Decadron . Onc?He is to receive XRT 
on 3/31 and daily during that week . Pain 
control?Currently under control with MS con-
tin and MSIR prn. regimen . Follow HA, LBP. 
ENDO?Glucose control monitored while on de-
cadron with SSRI coverage . Will check 
HgbA1C prior to discharge . GI?Aggressive 
bowel regimen to continue at home . Pt is 
Full Code . ADDITIONAL COMMENTS: Please call 
Dr. Xellcaugh with worsening headache or 
back pain, or any other concern . Keep ap-
pointment as scheduled with XRT . Please 
check fingerstick once a day, and record, 
call MD if greater than 200 .  
 
186
 
Figure 1: Visualization result (Left), magnified (Right). 
 
 
Figure 2: Negative Triggers and Events on a Depend-
ency Structure. 
 
Table 2: Corpora and Modalities 
CORPUS MODALITY 
ACE asserted, or other 
TIMEML must, may, should, would, or 
could 
Prasad et al, 
2006 
assertion, belief, facts or eventu-
alities 
Saur? et al, 2007 certain, probable, possible, or 
other 
Inui et al, 2008 affirm, infer, doubt, hear, intend, 
ask, recommend, hypothesize, or 
other 
THIS STUDY S/O, necessity, hope, possible, 
recommend, intend  
 
Table 3: Markup Scheme (Tags and Definitions) 
Tag Definition (Examples) 
R Remedy, Medical operation 
(e.g. radiotherapy) 
T Medical test, Medical examination 
(e.g., CT, MRI) 
D Deasese, Symptom 
(e.g., Endometrial cancer, headache) 
M Medication, administration of a drug 
(e.g., Levofloxacin, Flexeril) 
A patient action 
(e.g., admitted to a hospital) 
V Other verb 
(e.g., cancer spread to ...)  
 
2 Related Works 
2.1 Previous Markup Schemes 
In the NLP field, fact identification has not been 
studied well to date. Nevertheless, similar analyses 
can be found in studies of sentence modality. 
The Automatic Content Extraction (ACE)2 in-
formation extraction program deals with event ex-
traction, by which each event is annotated with 
temporal and modal markers. 
A
S  
A
T
                                                          
 similar effort is made in the TimeML project 
(Pustejovsky et al, 2003). This project specifically 
examines temporal expressions, but several modal 
expressions are also covered. 
Prasad et al (2006) propose four factuality clas-
sifications (certain, probable...etc.) for the Penn 
Discourse TreeBank (PDTB) 3. 
aur? et al (2007) propose three modal categories
for text entailment tasks. 
mong various markup schemes, the most recent 
one is Experience Mining (Inui et al, 2008), which 
collects personal experiences from the web. They 
also distinguish whether an experience is an actual 
one or not, which is a similar problem to that con-
fronting us. 
able 2 portrays a markup scheme adopted by 
each project. Our purpose is similar to that of Ex-
perience Mining. Consequently, we fundamentally 
adopt its markup scheme. However, we modify the 
label to suit medical mannerisms. For example, 
?doubt? is modified into ?(S/O) suspicion of?. Rare 
modalities such as ?hear? are removed. 
 
2.2 Previous Algorithms 
Negation is a traditional topic in medical fields. 
Therefore, we can find many previous studies of 
the topic in the relevant literature. 
An algorithm, NegEx4 was proposed by Chap-
man et al (Chapman et al, 2001a; Chapman et al, 
2001b). It outputs an inference of whether a term is 
positive or negative. The original algorithm is 
based on a list of negation expressions. Goldin et al 
(2003) incorporate machine learning techniques 
(Na?ve Bayes and decision trees) into the algorithm. 
The extended version (ConText) was also proposed 
(Chapman et al, 2007). 
Elkin et al (2005) use a list of negation words 
and a list of negation scope-ending words to iden-
2 http://projects.ldc.upenn.edu/ace/ 
3 http://www.seas.upenn.edu/~pdtb/ 
4 http://www.dbmi.pitt.edu/chapman/NegEx.html 
187
tify negated statements and their scope. Their tech-
nique was used in The MAYO Clinic Vocabulary 
Server (MCVS)5, which encodes clinical expres-
sions into medical ontology (SNOMED-CT) and 
identifies whether the event is positive or negative. 
M
H
T
A
                                                          
utalik et al (2001) earlier developed Negfinder 
to recognize negated patterns in medical texts. 
Their system uses regular expressions to identify 
words indicating negation. Then it passes them as 
special tokens to the parser, which makes use of 
the single-token look-ahead strategy. 
uang and Lowe (2007) implemented a hybrid 
approach to automated negation detection. They 
combined regular expression matching with 
grammatical parsing: negations are classified based 
on syntactic categories. In fact, they are located in 
parse trees. Their hybrid approach can identify ne-
gated concepts in radiology reports even when they 
are located distantly from the negative term. 
he Medical Language Extraction and Encoding 
(MedLEE) system was developed as a general 
natural language processor to encode clinical doc-
uments in a structured form (Friedman et al, 
1994). Negated concepts and certainty modifiers 
are also encoded within the system. 
Veronika et al (2008) published a negation 
scope corpus6 in which both negation and uncer-
tainty are addressed. 
lthough their motivations are identical to ours, 
two important differences are apparent. (1) Previ-
ous (except for Veronika et al, 2008) methods deal 
with the two-way problem (positive or negative), 
whereas the analyses proposed herein tackle more 
fine-grained modalities. (2) Previous studies (ex-
cept for Huang et al, 2007) are based on BOW 
approaches, whereas we use syntactic information. 
3 Medical Text Summarization System: 
TEXT2TABLE 
Because the core problem of this paper is to iden-
tify negative events, this section briefly presents a 
description of the entire system, which consists of 
four steps. The detailed algorithm of negative iden-
tification is explained in Section 4. 
STEP 1: Event Identification 
First, we define the event discussed in this paper. 
We deal with events of six types, as presented in 
5 http://mayoclinproc.highwire.org/content/81/6/741.figures-
only 
6 www.inf.u-szeged.hu/rgai/bioscope 
Table 3. Two of the four are Verb Phrases (base 
VPs); the others are noun phrases (base-NPs). Be-
cause this task is similar to Named Entity Recogni-
tion (NER), we use the state-of-the art NER 
method, which is based on the IOB2 representation 
and Conditional Random Fields (CRFs). In learn-
ing, we use standard features, as shown in Table 4. 
 
Table 4: Features for Event Identification 
Lexicon 
and 
Stem 
Current target word (and its stem) and its 
surrounding words (and stem). The win-
dow size is five words (-2, -1, 0, 1, 2). 
POS Part of speech of current target word and 
its surrounding words (-2, -1, 0, 1, 2). The 
part of speech is analyzed using a POS 
tagger7. 
DIC A fragment for the target word appears in 
the medical dictionary (Ito et al, 2003).  
 
STEP 2: Normalization 
As described in Section 1, a term in a record is 
sometimes an acronym: shorthand notation. Such 
abbreviations are converted into standard notation 
through (1) date time normalization or (2) event 
normalization. 
(1) Date Time Normalization 
As for date time expressions, relative date expres-
sions are converted into YYYY/MM/DD as fol-
lows. 
  On Dec Last year ? 2007/12/XX 
  10 Dec 2008        ? 2008/12/10 
These conversions are based on heuristic rules. 
(2) Event Normalization 
Medical terms are converted into standard notation 
(dictionary entry terms) using orthographic disam-
biguation (Aramaki et al, 2008). 
STEP 3: TIME?EVENT Relation Identification 
Then, each event is tied with a date time. The cur-
rent system relies on a simple rule (i.e., an event is 
tied with the latest date time). 
STEP 4: Negative Identification 
The proposed SVM classifier distinguishes nega-
tive events from other events. The detailed algo-
rithm is described in the next section. 
4 Modality Identification Algorithm 
First, we define the negative. We classify modality 
events into eight types (Table 5). These classifica-
tions are motivated by those used in previous stud-
                                                          
7 http://chasen-legacy.sourceforge.jp/ 
188
ies (Inui et al, 2008). However, we simplify their 
scheme because several categories are rare in this 
domain. 
T
U
hese classes are not exclusive. For that reason, 
they sometimes lead to multiple class events. For 
example, given ?No chemotherapy is planned?, an 
event ?chemotherapy? belongs to two classes, 
which are ?NEGATION? and ?FUTURE?. 
Training Phase 
sing a corpus with modality annotation, we train 
a SVM classifier for each category. The training 
features come from four parts: 
(1) Current phrases: words included in a current 
event. We also regard their STEMs, POSs, and the 
current event category as features. 
(2) Surrounding phrases: words included in the 
current event phrase and its surrounding two 
phrases (p1, p2, n1, n2, as depicted in Fig. 3). The 
unit of the phrase is base-NP/VP, which is pro-
duced by the Japanese parser (Kurohashi et al, 
1994). Its window size is two in the neighboring 
phrase (p1, p2, c, n1, n2). We also deal with their 
STEMs and POSs. 
(3) Dependent phrases: words included in the 
parent phrase of the current phrase (d1 in Fig. 3), 
and grandparent phrases (d2 in Fig. 3). We also 
deal with their STEMs and POSs. 
(4) Previous Event: words (with STEMs and 
POSs) included in the previous (left side) events. 
Additionally, we deal with the previous event cate-
gory and the modality class. 
(5) Bag-of-words: all words (with STEMs and 
POSs) in the sentence. 
 
TEST Phrase 
During the test, each SVM classifier runs. 
Although this task is multiclass labeling, several 
class combinations are unnatural, such as 
FUTURE and S/O. We list up possible label com-
binations (that have at least one occurrence in the 
corpora); if such a combination appears in a text, 
we adapt a high confidence label (using a marginal 
distance). 
 
5 Experiments 
We investigate what kind of information contrib-
utes to the performance in various machine learn-
ing algorithms. 
 
Table 5: Classification of Modalities 
NEGATION An event with negation words 
such as ?not? or ?no?. 
FUTURE An event that is scheduled for 
execution in the future. 
PURPOSE An event that is planed by a doc-
tor, but its time schedule is am-
biguous (just a hope/intention).  
S/O An event (usually a disease) that 
is suspected. For example, given 
?suspected microscopic tumor in 
...?, ?microscopic tumor'' is an 
S/O event.? 
NECESSITY An event (usually a remedy or 
medical test) that is required. 
INTEND An event that is hoped for by a 
patient.  
Note that if the event is hoped by 
a doctor, we regard is a 
PURPOSE or FUTURE. For ex-
ample, given ?He hoped for 
chemical therapy?, ?chemical 
therapy? is INTEND. 
POSSIBLE An event (usually remedy) that is 
possible under the current situa-
tion. 
RECOMMEND An event (usually remedy) that is 
recommended by other doctor(s). 
 
 
5.1 Corpus and Setting 
We collected 435 Japanese discharge summaries in 
which events and the modality are annotated. For 
training, we used the CRF toolkit8 with standard 
parameters. In this experiment setting, the input is 
an event with its contexts. The output is an event 
modality class (positive of negative in two-way) 
(or more detailed modality class in nine-way). 
T
 
                                                          
he core problem addressed in this paper is mo-
dality classification. Therefore, this task setting 
assumes that all events are identified correctly. 
Table 6 presents the event identification accuracy. 
Except for the rare class V (the other verb), we got 
more than 80% F-scores. It is true that the accu-
racy is not perfect. Nevertheless, most of the re-
maining problems in this step will be solved using 
a larger corpus. 
5.2 Comparable Methods 
We conducted experiments in the 10-fold cross 
validation manner. We investigated the perform-
8 http://crfpp.sourceforge.net/ 
189
ance in various feature combinations and the fol-
lowing machine learning methods. 
 
 
Figure 3: Features 
 
Table 6: Event Identification Result. Tag precision re-
call F-score.  
 # P R F 
A (ACTION) 1,556 94.63 91.04 92.80 
V (VERB) 1,047 84.64 74.89 79.47 
D (DISEASE) 3,601 85.56 80.24 82.82 
M (MEDICINE) 1,045 86.99 81.34 84.07 
R (REMEDY) 1,699 84.50 76.36 80.22 
T (TEST) 2,077 84.74 76.68 80.51 
ALL 11,025 84.74 76.68 80.51  
 
Table 7: Various Machine Learning Method 
SVM Support Vector Machine (Vapnik, 
1999). We used TinySVM9 with a 
polynomial kernel (degree=2). 
AP Averaged Perceptron (Collins, 2002) 
PA1 Passive Aggressive I (Crammer et 
al., 2006)* 
PA2 Passive Aggressive II (Crammer et 
al., 2006)* 
CW Confidence Weighted (Dredze et al, 
2008)* 
* The online learning library10 is used for AP PA1,2 
CW . 
 
5.3 Evaluation Metrics 
We adopt evaluation of two types: 
(1) Two-way: positive or negative: 
(2) Nine-way: positive or one of eight modality 
categories. 
Recall and F-measure are investigated in both for 
evaluation precision. 
 
5.4 Results 
The results are shown in Table 8 (Two-Way) and 
in Table 9 (Nine-Way). 
Current Event Category 
The results in ID0?ID1 indicate that the current 
event category (CAT) is useful. However, events 
are sometimes misestimated in real settings. We 
                                                          
In
R
A
A
H
9 http://chasen.org/ taku/software/TinySVM/ 
10 http://code.google.com/p/oll 
must check more practical performance in the fu-
ture. 
Bag-of-words (BOW) Information 
Results in ID1?ID2 indicate that BOW is impor-
tant. 
Surrounding Phrase Contribution 
The results appearing in ID2?ID9 represent the 
contribution of each feature position. From ID3, 
ID4, and ID7 results, next phrases (n1, n2) and 
parent phrases (d1) were able to boost the accuracy. 
Despite the risk of parsing errors, parent phrases 
(d1) are helpful, which is an insight of this study. 
 contrast, we can say that the following features 
had little contribution: previous phrases (p1, p2 
from ID5 and ID6), grandparent phrases (d2 from 
ID8), and previous events (e from ID9). 
egarding p1 and p2, these modalities are rarely 
expressed in the previous parts in Japanese. 
s for d2, the grandparent phrases might be too 
removed from the target events. 
s for e, because texts in health records are frag-
mented, each event might have little relation. 
owever, the above features are also helpful in 
cases with a stronger learning algorithm. 
In fact, among ID10?ID14, the SVM-based 
classifier achieved the best accuracy with all fea-
tures (ID14). 
 
Table 8: Two-way Results 
 
? indicates the used feature. c are features from the cur-
rent phrase. p1, p2, n1, n2 are features from surrounding 
phrases. e are features from a previous event. BOW is a 
bag-of-words using features from an entire sentence. 
CAT is the category of the current event. 
 
190
Learning Methods 
Regarding the learning algorithms, all online learn-
ing methods (ID7 and ID15?17) showed lower ac-
curacies than SVM (ID11), indicating that this task 
requires heavy learning. 
 
Nine-way Results 
Table 9 presents the accuracies of each class. Fun-
damentally, we can obtain high performance in the 
frequent classes (such as NEGATION, PURPOSE, 
and S/O). In contrast, the classifier suffers from 
low frequent classes (such as FUTURE). How to 
handle such examples is a subject of future study. 
 
Table 9: Two-way Results 
 # Preci-
sion 
Re-
call 
F-
measure 
NEGATION 441 84.19 77.36 80.63 
PURPOSE 346 91.35 63.87 75.17 
S/O 242 90.74 72.39 80.53 
FUTURE 97 23.31 55.96 32.91 
POSSIBLE 36 83.33 40.55 54.55 
INTEND 32 76.66 29.35 42.44 
RECOMMEND 21 95.71 38.57 54.98 
NECESSITY 4 100 0 0  
 
4.5 Future Works 
In this section, we will discuss several remaining 
problems. First, as described, the classifier suffers 
from low frequent modality classes. To give more 
examples for such classes is an important problem. 
Our final goal is to realize precise information ex-
traction from health records. Our IE systems are 
already available at the web site (http://lab0.com). 
Comprehensive evaluation of those systems is re-
quired. 
6 Conclusions 
This paper presented a classifier that identified 
whether an event has actually occurred or not. The 
proposed SVM-based classifier uses both BOW 
information and dependency parsing results. The 
experimental results demonstrated 85.8 F-
measure% accuracy and revealed that syntactic 
information can contribute to the method?s accu-
racy. In the future, a method of handling low-
frequency events is strongly desired. 
 
 
Acknowledgments 
Part of this research is supported by Grant-in-Aid 
for Scientific Research (A) of Japan Society for the 
Promotion of Science Project Number:?20680006  
F.Y.2008-20011 and the Research Collaboration 
Project with Fuji Xerox  Co. Ltd. 
References 
Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-
ory F. Cooper, and Bruce Buchanan. 2001a. Evalua-
tion of negation phrases in narrative clinical reports. 
In Proceedings of AMIA Symp, pages 105-109. 
Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-
ory F. Cooper, and Bruce Buchanan. 2001b. A sim-
ple algorithm for identifying negated findings and 
diseases in discharge summaries. Journal of Bio-
medical Informatics, 5:301-310. 
Wendy Chapman, John Dowling and David Chu. 2007. 
ConText: An algorithm for identifying contextual 
features from clinical text. Biological, translational, 
and clinical language processing (BioNLP2007), pp. 
81?88. 
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko 
Ohe: Orthographic Disambiguation Incorporating 
Transliterated Probability International Joint Confer-
ence on Natural Language Processing (IJCNLP2008), 
pp.48-55, 2008. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey 
S. Husser, William Carruth, Larry R. Bergstrom, and 
Dietlind L. Wahner Roedler. A controlled trial of au-
tomated classification of negation from clinical notes. 
BMC Medical Informatics and Decision Making 
5:13. 
C. Friedman, P.O. Alderson, J.H. Austin, J.J. Cimino, 
and S.B. Johnson. 1994. A general natural language 
text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 
1(2):161-174. 
L. Gillick and S.J. Cox. 1989. Some statistical issues in 
the comparison of speech recognition algorithms. In 
Proceedings of IEEE International Conference on 
Acoustics, Speech, and Signal Processing, pages 532-
535. 
Ilya M. Goldin and Wendy Chapman. 2003. Learning to 
detect negation with not in medical texts. In Work-
shop at the 26th ACM SIGIR Conference. 
Yang Huang and Henry J. Lowe. 2007. A novel hybrid 
approach to automated negation detection in clinical 
radiology reports. Journal of the American Medical 
Informatics Association, 14(3):304-311. 
191
Kentaro Inui, Shuya Abe, Hiraku Morita, Megumi Egu-
chi, Asuka Sumida, Chitose Sao, Kazuo Hara, Koji 
Murakami, and Suguru Matsuyoshi. 2008. Experi-
ence mining: Building a large-scale database of per-
sonal experiences and opinions from web documents. 
In Proceedings of the 2008 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 314-
321. 
M. Ito, H. Imura, and H. Takahisa. 2003. Igaku- Shoin?s 
Medical Dictionary. Igakusyoin. 
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic 
analysis method of long Japanese sentences based on 
the detection of conjunctive structures. Computa-
tional Linguistics, 20(4). 
Pradeep G. Mutalik, Aniruddha Deshpande, and Pra-
kash M. Nadkarni. 2001. Use of general purpose ne-
gation detection to augment concept indexing of 
medical documents: A quantitative study using the 
umls. Journal of the American Medical Informatics 
Association, 8(6):598-609. 
J. Lafferty, A. McCallum, and F. Pereira: Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data, In Proceedings of the In-
ternational Conference on Machine Learning 
(ICML2001), pp.282-289, 2001. 
R. Prasad, N. Dinesh, A. Lee, A. Joshi and B. Webber: 
Annotating Attribution in the Penn Discourse Tree-
Bank, In Proceedings of the International Conference 
on Computational Linguistics and the Annual Con-
ference of the Association for Computational Lin-
guistics (COLING/ACL2006) Workshop on 
Sentiment and Subjectivity in Text, pp.31-38 (2006). 
R. Saur?, and J. Pustejovsky: Determining Modality and 
Factuality for Text Entailment, Proceedings of 
ICSC2007, pp. 509-516 (2007). 
Gaizauskas, A. Setzer, G. Katz, and D.R. Radev. 2003. 
New Directions in Question Answering: Timeml: 
Robust specification of event and temporal expres-
sions in text. AAAI Press. 
SNOMED-CT. 2002. SNOMED Clinical Terms Guide. 
College of American Pathologists.  
Sibanda Tawanda, Tian He, Peter Szolovits, and Uzuner 
Ozlem. 2006. Syntactically informed semantic cate-
gory recognizer for discharge summaries. In Proceed-
ings of the Fall Symposium of the American Medical 
Informatics Association (AMIA 2006), pages 11-15. 
Sibanda Tawanda and Uzuner Ozlem. 2006. Role of 
local context in automatic deidentification of un- 
grammatical, fragmented text. In Proceedings of the 
Human Language Technology conference and the 
North American chapter of the Association for Com-
putational Linguistics (HLT-NAACL2006), pages 
65-73. 
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The bioscope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(11). 
 
192
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 65?68,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Fast decoding and Easy Implementation:
Transliteration as Sequential Labeling
Eiji ARAMAKI
The University of Tokyo
eiji.aramaki@gmail.com
Takeshi ABEKAWWA
National Institute of Informatics
abekawa@nii.ac.jp
Abstract
Although most of previous translitera-
tion methods are based on a generative
model, this paper presents a discrimi-
native transliteration model using condi-
tional random fields. We regard charac-
ter(s) as a kind of label, which enables
us to consider a transliteration process as
a sequential labeling process. This ap-
proach has two advantages: (1) fast decod-
ing and (2) easy implementation. Experi-
mental results yielded competitive perfor-
mance, demonstrating the feasibility of the
proposed approach.
1 Introduction
To date, most transliteration methods have relied
on a generative model which resembles a statisti-
cal machine translation (SMT) model. Although
the generative approach has appealing feasibility,
it usually suffers from parameter settings, length
biases and decoding time.
We assume a transliteration process as a kind
of sequential labeling that is widely employed for
various tasks, such as Named Entity Recognition
(NER), part-of-speech (POS) labeling, and so on.
Figure 1 shows a lattice of both the transliteration
and POS labeling. As shown in that figure, both
tasks share a similar work frame: (1) an input se-
quence is decomposed into several segments; then
(2) each segments produces a label. Although the
label represents a POS in POS labeling, it repre-
sents a character (or a character sequence) in the
transliteration task.
The proposed approach entails three risks.
1. Numerous Label Variation: Although POS
requires only 10?20 labels at most, a translit-
eration process requires numerous labels. In
fact, Japanese katakana requires more than
260 labels in the following experiment (we
Figure 1: (i) Part-of-Speech Lattice and (ii)
Transliteration Lattice.
consider combinations of characters as a la-
bel). Such a huge label set might require ex-
tremely heavy calculation.
2. No Gold Standard Data: We build the gold
standard label from character alignment us-
ing GIZA++ 1. Of course, such gold standard
data contain alignment errors, which might
decrease labeling performance.
3. No Language Model: The proposed ap-
proach cannot incorporate the target language
model.
In spite of the disadvantages listed above, the
proposed method offers two strong advantages.
1. Fast Decoding: Decoding (more pre-
cisely labeling) is extremely fast (0.12?0.58
s/input). Such rapid decoding is useful for
various applications, for example, a query ex-
pansion for a search engine and so on 2.
1http://www.fjoch.com/GIZA++.html
2A fast transliteration demonstration is available at the
web site; http://akebia.hcc.h.u-tokyo.ac.jp/NEWS/
65
Figure 2: Conversion from Training set to Gold
Standard Labels
2. Easy Implementation: Because sequential
labeling is a traditional research topic, vari-
ous algorithms and tools are available. Using
them, we can easily realize various transliter-
ation systems in any language pairs.
The experimental results empirically demon-
strate that the proposed method is competitive
in several language directions (e.g. English?
Chinese).
2 Method
We developed a two-stage labeling system. First,
an input term is decomposed into several segments
(STEP1). Next, each segmentation produces sym-
bol(s) (STEP2).
2.1 STEP1: Chunking
For a given noun phrase, consisting n characters,
the system gave a label (L1...Ln) that represents
segmentations.
The segmentation is expressed as two types of
labels (label B and I), where B signifies a begin-
ning of the segmentation, and I signifies the end
of segmentation. This representation is similar to
the IOB representation, which is used in Named
Entity Recognition (NER) or chunking.
For label prediction, we used Conditional Ran-
dom Fields (CRFs), which is a state-of-the-art la-
beling algorithm. We regard a source character it-
self as a CRF feature. The window size is three
(the current character and previous/next charac-
ter).
2.2 STEP2: Symbol production
Next, the system estimates labels (T1...Tm) for
each segmentation, where m is the number of seg-
Table 1: Corpora and Sizes
Notation Language Train Test
EN-CH English?Chinese 31,961 2,896
EN-JA English?Japanese 27,993 1,489
EN-KO English?Korean 4,840 989
EN-HI English?Hindi 10,014 1,000
EN-TA English?Tamil 8,037 1,000
EN-KA English?Kannada 8,065 1,000
EN-RU English?Russian 5,977 1,000
* EN-CH is provided by (Li et al, 2004); EN-
TA, EN-KA, EN-HI and EN-RU are from (Kumaran
and Kellner, 2007); EN-JA and EN-KO are from
http://www.cjk.org/.
mentations (the number of B labels in STEP1).
The label of this step directly represents a target
language character(s). The method of building a
gold standard label is described in the next sub-
section.
Like STEP1, we use CRFs, and regard source
characters as a feature (window size=3).
2.3 Conversion from Alignment to Labels
First, character alignment is estimated using
GIZA++ as shown at the top of Fig. 2. The align-
ment direction is a target- language-to-English, as-
suming that n English characters correspond to a
target language character.
The STEP1 label is generated for each English
character. If the alignment is 1:1, we give the char-
acter aB label. If the alignment is n : 1, we assign
the first character a B label, and give the others I .
Note that we regard null alignment as a continu-
ance of the last segmentation (I).
The STEP2 label is generated for each English
segmentation (B or BI?). If a segmentation cor-
responds to two or more characters in the target
side, we regard the entire sequence as a label (see
T5 in Fig. 2).
3 Experiments
3.1 Corpus, Evaluation, and Setting
To evaluate the performance of our system,
we used a training-set and test-set provided by
NEWS3(Table 1).
We used the following six metrics (Table 2) us-
ing 10 output candidates. A white paper4 presents
the detailed definitions. For learning, we used
CRF++5 with standard parameters (f=20, c=.5).
3http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
4https://translit.i2r.a-star.edu.sg/news2009/whitepaper/
5http://crfpp.sourceforge.net/
66
Table 3: Results in Test-set
ACC MeanF MRR MAPref MAP10 MAPsys
EN?CH 0.580 0.826 0.653 0.580 0.199 0.199
EN?RU 0.531 0.912 0.635 0.531 0.219 0.219
EN?JA 0.457 0.828 0.576 0.445 0.194 0.194
EN?TA 0.365 0.884 0.504 0.360 0.172 0.172
EN?HI 0.363 0.864 0.503 0.360 0.170 0.170
EN?KA 0.324 0.856 0.438 0.315 0.148 0.148
EN?KO 0.170 0.512 0.218 0.170 0.069 0.069
Table 2: Evaluation Metrics
ACC Word Accuracy in Top 1.
MeanF
The meanF measures the fuzzy accu-
racy that is defined by the edit dis-
tance and Longest Common Subse-
quence (LCS).
MRR
Mean Reciprocal Rank. 1/MRR tells
approximately the average rank of the
correct transliteration.
MAPref
Measures the precision in the n?best
candidates tightly for each reference.
MAP10 Measures the precision in the 10-bestcandidates.
MAPsys
Measures the precision in the top Ki-
best candidates produced by the system.
3.2 Results and Discussion
Table 3 presents the performance. As shown in the
table, a significant difference was found between
languages (from low (0.17) to high (0.58)).
The high accuracy results(EN-CH or EN-RU)
are competitive with other systems (the middle
rank among the NEWS participating systems).
However, several language results (such as EN-
KO) were found to have poor performance.
We investigated the difference between high-
performance languages and the others. Table 4
shows the training/test times and the number of
labels. As shown in the table, wide divergence is
apparent in the number of labels. For example,
although EN?KO requires numerous labels (536
labels), EN?RU needs only 131 labels. This diver-
gence roughly corresponds to both training-time
and accuracy as follows: (1) EN?KO requires long
training time (11 minutes) which gave poor per-
formance (0.17 ACC), and (2) EN?RU requires
short training (only 26.3 seconds) which gave high
performance (0.53 ACC). This suggests that if the
number of labels is small, we successfully convert
transliteration into a sequential labeling task.
The test time seemed to have no relation to
Table 4: Average Test time, Training Time, and
the number of labels (label variation).
Language Test Train # of labels
EN?KO 0.436s 11m09.5s 536
EN?CH 0.201s 6m18.9s 283
EN?JA 0.247s 4m44.3s 269
EN?KA 0.190s 2m26.6s 231
EN?HI 0.302s 1m55.6s 268
EN?TA 0.124s 1m32.9s 207
EN?RU 0.580s 0m26.3s 131
* Test time is the average labeling time for an input. Training
time is the average training time for 1000 labels.
both training time and performance. To investi-
gate what gave effects on test time is a subject for
our future work.
4 Related Works
Most previous transliteration studies have re-
lied on a generative model resembling the IBM
model(Brown et al, 1993). This approach is ap-
plicable to various languages: for Japanese (Goto
et al, 2004; Knight and Graehl, 1998), Korean(Oh
and Choi, 2002; Oh and Choi, 2005; Oh and
Isahara, 2007), Arabic(Stalls and Knight, 1998;
Sherif and Kondrak, 2007), Chinese(Li et al,
2007), and Persian(Karimi et al, 2007). As de-
scribed previously, the proposed discriminative
approach differs from them.
Another perspective is that of how to repre-
sent transliteration phenomena. Methods can be
classified into three main types: (1) grapheme-
based (Li et al, 2004), (2) phoneme-based (Knight
and Graehl, 1998), and (3) combinations of these
methods (hybrid-model(Bilac and Tanaka, 2004),
and a correspondence-based model(Oh and Choi,
2002; Oh and Choi, 2005) re-ranking model (Oh
and Isahara, 2007)). Our proposed method em-
ploys a grapheme-based approach. Employing
phonemes is a challenge reserved for future stud-
ies.
Aramaki et al (2008) proposed a discrimina-
67
tive transliteration approach using Support Vector
Machines (SVMs). However, their goal, which is
to judge whether two terms come from the same
English words or not, differs from this paper goal.
5 Conclusions
This paper presents a discriminative translitera-
tion model using a sequential labeling technique.
Experimental results yielded competitive perfor-
mance, demonstrating the feasibility of the pro-
posed approach. In the future, how to incorporate
more rich information, such as language model
and phoneme, is remaining problem. We believe
this task conversion, from generation to sequential
labeling, can be useful for several practical appli-
cations.
ACKNOWLEDGMENT
Part of this research is supported by Japanese
Grant-in-Aid for Scientific Research (A) Num-
ber:20680006.
References
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and
Kazuhiko Ohe. 2008. Orthographic disambiguation
incorporating transliterated probability. In Proceed-
ings of International Joint Conference on Natural
Language Processing (IJCNLP2008), pages 48?55.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In Pro-
ceedings of The 20th International Conference on
Computational Linguistics (COLING2004), pages
597?603.
Peter F. Brown, Stephen A. Della Pietra, Vi cent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
Isao Goto, Naoto Kato, Terumasa Ehara, and Hideki
Tanaka. 2004. Back transliteration from Japanese
to English using target English context. In Proceed-
ings of The 20th International Conference on Com-
putational Linguistics (COLING2004), pages 827?
833.
Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2007. Collapsed consonant and vowel models: New
approaches for English-Persian transliteration and
back-transliteration. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL2007), pages 648?655.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In SIGIR
?07: Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 721?722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the Meeting of the Association for
Computational Linguistics (ACL2004), pages 159?
166.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, andMinghui
Dong. 2007. Semantic transliteration of per-
sonal names. In Proceedings of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL2007), pages 120?127.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proceedings of The 19th In-
ternational Conference on Computational Linguis-
tics (COLING2002), pages 758?764.
Jong-HoonOh and Key-Sun Choi. 2005. An ensemble
of grapheme and phoneme for machine translitera-
tion. In Proceedings of Second International Joint
Conference on Natural Language Processing (IJC-
NLP2005), pages 450?461.
Jong-Hoon Oh and Hitoshi Isahara. 2007. Machine
transliteration using multiple transliteration engines
and hypothesis re-ranking. In Proceedings of MT
Summit XI, pages 353?360.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics (ACL2007), pages 944?951.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in arabic text.
In Proceedings of The International Conference
on Computational Linguistics and the 36th Annual
Meeting of the Association of Computational Lin-
guistics (COLING-ACL1998) Workshop on Compu-
tational Approaches to Semitic Languages.
68
Word Selection for EBMT based on Monolingual Similarity and Translation
Confidence
Eiji Aramaki, Sadao Kurohashi, Hideki Kashioka and Hideki Tanaka
 Graduate School of Information Science and Tech. University of Tokyo
Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
aramaki, kuro@kc.t.u-tokyo.ac.jp
 ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seika, Soraku, Kyoto 619-0288, Japan
hideki.kashioka, hideki.tanaka@atr.co.jp
Abstract
We propose a method of constructing an
example-based machine translation (EBMT)
system that exploits a content-aligned bilingual
corpus. First, the sentences and phrases in the
corpus are aligned across the two languages,
and the pairs with high translation confidence
are selected and stored in the translation mem-
ory. Then, for a given input sentences, the
system searches for fitting examples based on
both the monolingual similarity and the transla-
tion confidence of the pair, and the obtained re-
sults are then combined to generate the transla-
tion. Our experiments on translation selection
showed the accuracy of 85% demonstrating the
basic feasibility of our approach.
1 Introduction
The basic idea of example-based machine translation, or
EBMT, is that translation examples similar to a part of
an input sentence are retrieved and combined to produce
a translation(Nagao, 1984). In order to make a practi-
cal MT system based on this approach, a large number
of translation examples with structural correspondences
are required. This naturally presupposes high-accuracy
parsers and well-aligned large bilingual corpora.
Over the last decade, the accuracy of the parsers im-
proved significantly. The availability of well-aligned
bilingual corpora, however, has not increased despite our
expectations. In reality, the number of bilingual cor-
pora that share the same content, such as newspapers and
broadcast news, has increased steadily. We call this type
of corpus a content-aligned corpus. With these observa-
tions, we started a research project that covered all as-
pects of constructing EBMT systems starting from using
Figure 1: Translation Example (TE).
a content-aligned corpus, i.e., a bilingual broadcast news
corpus.
First, the sentences and phrases in the corpus are
aligned across the two languages, and the pairs with high
translation confidence are selected and stored in the trans-
lation memory. Then, translation examples are retrieved
based on both the monolingual similarity and the trans-
lation confidence of the pair. Finally, these examples are
combined to generate the translation.
This paper is organized as follows. The next sec-
tion presents how to build the translation memory from
a content-aligned corpus. Section 3 describes our EBMT
system, paying special attention to the selection of trans-
lation examples. Section 4 reports experimental results
of word selection, Section 5 describes related works, and
Section 6 gives our conclusions.
* Underlined phrases and sentences have no parallel expressions in the other language.
Figure 2: NHK News Corpus.
2 Building Translation Memory
In EBMT, an input sentence can hardly be translated by
a single translation example, except when an input is ex-
tremely short or is a typical domain-dependent sentence.
Therefore, two or more translation examples are used to
translate parts of the input and are then combined to gen-
erate a whole translation. Syntactic information is useful
for composing example fragments.
In this paper, we call a structurally aligned bilingual
sentence pair a translation example or TE (Figure 1).
This section presents our method for building TEs from a
content-aligned corpus.
Since the bilingual corpus used in our project does not
contain literal translations, automatic parsing and align-
ment inevitably contain errors. Therefore, we selected
highly likely TEs to make a translation memory.
2.1 NHK News Corpus
We used a bilingual news corpus compiled by the NHK
broadcasting service (NHK News Corpus), which con-
sists of about 40,000 Japanese-English article pairs cov-
ering a five-year period. The average number of Japanese
sentences in an article is 5.2, and that of English sentence
is 7.4. Table 2 shows an example of an article pair.
As shown in Table 2, an English article is not a literal
translation of a Japanese article, although their contents
are almost parallel.
2.2 Sentence Alignment
We used a DP matching for bilingual sentence alignment,
where we allow the matching of 1-to-1, 1-to-2, 1-to-3, 2-
to-1 and 2-to-2 Japanese and English sentence pairs. This
matching covered 84% of the following evaluation set.
We selected 96 article pairs for the evaluation of sentence
and phrase alignment, and we call this the evaluation set.
We use the following score for matching, which is based
on a ratio of corresponding content words (WCR: content
Word Corresponding Ratio).
WCR  




 (1)
where 

is the number of Japanese content words in a
unit, 

is the number of English content words, and 

is the number of content words whose translation is also
in the unit, which is found by translation dictionaries?
We used the EDR electronic dictionary, EDICT,
ENAMDICT, the ANCHOR translation dictionary, and
Figure 3: Handling of Remaining Phrases.
Figure 4: WCR and Precision.
the EIJIRO translation dictionary. These dictionaries
have about two million entries in total.
On the evaluation data, the precision of the sentence
alignment (defined as follows) was 60.7%.
precision  # of correct system outputs
# of system outputs
(2)
Among types of a corresponding unit, the precision of
1-to-1 correspondence was the best, at 77.5%. Since a 1-
to-1 correspondence is suitable for the following phrase
alignment, we decided to use only the 1-to-1 correspon-
dence results.
2.3 Phrase Alignment
The 1-to-1 sentence pairs obtained in the previous sec-
tion are then aligned at phrase level by the method based
on (Aramaki et al, 2001). The method consists of the
following pre-process and two aligning steps.
Pre-process: Conversion to phrasal dependency struc-
tures.
First, the phrasal dependency structures of the sen-
tence pair are estimated. The English parser returns
a word-based phrase structure, which is merged into
a phrase sequence by the following rules and con-
verted into a dependency structure by lifting up head
phrases.
Table 1: Number of TEs.
Corpus WCR # of TEs
0.3?0.4 18290
NHK News 0.4?0.5 6975
0.5? 2314
White Paper ? 2225
SENSEVAL ? 6920
1. Function words are grouped with the following
content word.
2. Adjoining nouns are grouped into one phrase.
3. Auxiliary verbs are grouped with the following
verb.
The Japanese parser outputs the phrasal dependency
structure of an input, and that is used as is. We used
The Japanese parser KNP (Kurohashi and Nagao,
1994) and The English nl-parser (Charniak, 2000).
Step 1: Estimation of basic phrasal correspondences.
We started with the word-level alignment to get the
basic phrasal alignment. We used translation dictio-
naries for this process. The word sense ambiguity
in the dictionaries is resolved with a heuristics that
the most plausible correspondence is near other cor-
respondences.
Step 2: Expansion of phrasal correspondences.
Finally, the remaining phrases, which were not han-
dled in the step 1, are merged into a neighboring
phrase correspondence or are used to establish a new
correspondence, depending on the surrounding ex-
isting correspondences. Figure 3 shows an example
of a new correspondence established by a structural
pattern.
These procedures can detect the phrasal alignments in
a pair of sentences as shown in Figure 1.
For phrase alignment evaluation, we selected all of the
145 sentence pairs that had 1-to-1 correspondences form
the evaluation set and gave correct content word corre-
spondences to these pairs. The phrase correspondences
detected by the system were judged correct when the cor-
respondences include the manually given content word
correspondences.
Based on this criterion, the precision of phrase align-
ment was 50%. Then, we found a correlation between
the phrase alignment precision and WCR of parallel sen-
tences as shown in Figure 4. Furthermore, the precision
of sentence alignment and WCR also have a correlation.
Since their performances nearly reaches their limits when
WCR is 0.3, we decided to use parallel sentences whose
WCR is 0.3 or greater as TEs.
Figure 5: Example of Translation.
2.4 Building Translation Memory
As explained in the preceding sections, among sentence-
aligned and phrase-aligned NHK News articles, TEs with
a 1-to-1 sentence correspondence and whose WCR is 0.3
or greater are registered in the translation memory. Table
1 shows the number of TEs for each WCR range.
In addition, the Bilingual White Paper and Translation
Memory of SENSEVAL2 (Kurohashi, 2001) were also
phrase-aligned and registered in the translation memory.
Sentence alignments are already given for these corpora.
Since their parallelism are fairly high and the accuracies
of their phrase alignments are more than 70%, we utilized
all phrase-aligned sentence pairs as TEs (Table 1).
3 EBMT System
Our EBMT system translates a Japanese sentence into
English. A Japanese input sentence is parsed and trans-
formed into a phrase-based dependency structure. Then,
for each phrase, an appropriate TE is retrieved from the
translation memory that is most suitable for translating
Figure 6: Selection of a TE.
the phrase (and its neighboring phrases). Finally, the En-
glish expressions of the TEs are combined to produce the
final English translation (Figure 5).
This section describes our EBMT system, mainly the
TE selection part.
3.1 Basic Idea of TE Selection
The basic idea of TE selection is shown in Figure 6.
When a part of the input sentence and a part of the TE
source language sentence have an equal expression, the
part of the input sentence is called I and the part of the
TE source language sentence is called S. A part of the TE
target language corresponding to S is called T. The pair S
and T is called fragment of TE (FTE).
I, S and T have to meet the following conditions, as a
natural consequence of the fact that S-T is used for trans-
lating I.
1. I, S and T are each structurally connected phrases.
2. I is equal to S except for function words at the
boundaries.
3. S corresponds to T completely, that is, all phrases in
S and T are aligned.
It might be the case that for an I, two or more FTEs that
meet the above conditions exist in the translation mem-
ory. Our method takes into account the following rela-
tions among I-S-T to select the best FTE:
1. The largest pair of I and S.
2. The similarity between the surroundings of I and
these of S.
3. The confidence of alignment between S and T.
The following sections concretely present how to cal-
culate these criteria. For simplicity of explanation, we
call a set of phrasal correspondences between S and T,
EQ; that neighboring EQ, CONTEXT; that between S and
T, ALIGN (Figure 6).
3.2 Monolingual Similarity between Japanese
Expressions
The equality between I and S is a sum of the equality
score of each phrase correspondence in EQ, which is cal-
culated as follows:
EQUAL 



 





	
 

	
 (3)
where

is the number of content words in the phrase
correspondence, 
	
is the number of function words,


is the equality between content words, and 
	
is the equality between function words. 

and 
	
are given in Table 2.1
Usually, the equality score between I and S is equal to
the number of phrases in I (the number of phrase corre-
spondences in EQ), but sometimes these are slightly dif-
ferent, depending on the conjugation type and function
words.
1All constant values in Table 2 and formulas were decided
based on preliminary experiments.
On the other hand, the similarity between the surround-
ings of I and those of S is a sum of the similarity score of
each phrase correspondence in CONTEXT, which is cal-
culated as follows:
SIM 




 






 






(4)
Basically the calculation of SIM and EQUAL is the
same, except that SIM considers the relation type between
the phrase in I and its outer phrase by 

. When
the relation is the same, the influence of the surrounding
phrases must be large, so 

is set to 1.0; when the
relation is not the same, 

is set to 0.5. The rela-
tions between phases are estimated by the function word
or conjugation type of the dependent phrase.
The monolingual similarity between Japanese expres-
sions I and S is calculated as follows:



EQUAL 



SIM (5)
3.3 Translation Confidence of Japanese-to-English
Alignment
The translation confidence of phrase alignment between
S and T is the sum of the confidence score of each phrase
correspondence in ALIGN, CONF() in Table 2, and it is
weighted by the WCR of the parallel sentences.
As a final measure, the score of I-S-T is calculated as
follows:




EQUAL 



SIM






CONF

WCR (6)
3.4 Search Algorithm of FTE
For each phrase (P) in an input sentence, the most plausi-
ble FTE is retrieved by the following algorithm:
1. FTEs are retrieved from the translation memory, in
which a Japanese phrase matches P, and it is aligned
to an English phrase. (that is, these are FTEs that
meet the basic conditions for translation in Section
3.1).
2. For each FTE obtained in the previous step, it is
checked whether the surrounding phrase of P and
that of FTE are the same or similar, phrase by
phrase, and the largest I-S-T that meets the basic
conditions is detected.
Table 2: Parameters for Similarity and Confidence Calculation.
1.1 exact match
1.0 stem match


0.5  

+ 0.3 thesaurus match
0.3 POS match
0 otherwise
* 

is a similarity calculated based on NTT thesaurus(Ikehara et al, 1997) (max = 1).
1.1 exact match


1.0 stem match
0 otherwise
1.0 all content words in alignment  correspond to each other in dic
CONF() 0.8 some content words in alignment  correspond to each other in dic
0.5 otherwise
3. The score of each I-S-T is calculated, and the best
I-S-T (S-T is the FTE) is selected as the FTE for P.
As a result of detecting FTEs for phrases in the input,
two FTEs starting from the different phrase might over-
lap each other. In such a case, we employed a greedy
search algorithm that adopts the higher score FTE one
by one; therefore, each previously adopted FTE is only
partly used for translation.
On the other hand, when no FTE is obtained for an in-
put phrase, a translation dictionary is utilized (when the
phrase contains two or more content words, the longest
matching strategy is used for dictionary look-up). When
two or more possible translations are given from the dic-
tionary, the most frequent phrase/word in the NHK News
Corpus is adopted.
Figure 5 shows examples of FTEs detected by our
method.2
3.5 Generating a Target Sentence
The English expressions in the selected FTEs are com-
bined, and the English dependency structure is con-
structed. The dependency relations in FTEs are pre-
served, and the relation between the two FTEs is esti-
mated based on the relation of the input sentences. Figure
5 shows an example of a combined English dependency
structure.
When a surface expression is generated from its depen-
dency structure, its word order must be selected properly.
This can be done by preserving the word order in FTEs
and by ordering FTEs by a set of rules governing both the
dependency relation and the word-order.
The module for controlling conjugation, determiner,
and singular/plural is not yet implemented in our current
MT system.
2As the bottom example in Figure 5 shows, EBMT can eas-
ily handle head-switching translation by using an FTE that con-
tains all of the head-switching phenomena in it.
4 Experiments
For evaluation, we selected 50 sentence pairs from the
NHK News Corpus that were not used for the translation
memory. Their source (Japanese) sentences were trans-
lated by our EBMT system, and the selected FTEs were
evaluated by hand, referring to the target (English) sen-
tences.
A phrase by phrase evaluation was done to judge
whether the English expression of the selected FTE was
good or bad. The accuracy was 85.0%.
In order to investigate the effectiveness of each com-
ponent of FTE selection, we compared the following four
methods:
1. EQCONTEXTALIGN: The proposed method.
2. EQALIGN: FTE score is calculated as follows, with-
out the CONTEXT similarity:



EQUAL()



CONF()WCR (7)
3. EQCONTEXT: FTE score is calculated as follows,
without the ALIGN confidence:



EQUAL() 



SIM() (8)
4. DICONLY: Word selection is based only on dictio-
naries and frequency in the corpus.
The accuracy of each method is shown in Table 3,
and the results indicate that the proposed method, EQ-
CONTEXTALIGN, is the best, that is, using context sim-
ilarity and align confidence works effectively. Figure 7
Figure 7: Word Selection by EQCONTEXTALIGN and DICONLY.
Table 3: Experimental Results.
Good Bad Accuracy
EQCONTEXTALIGN 268 47 85.0%
(246) (35) (87.5%)
EQALIGN 254 61 80.6 %
(233) (48) (82.9%)
EQCONTEXT 234 80 74.2%
(213) (68) (75.8%)
DICONLY 232 83 73.6%
* Values in brackets indicate the accuracy only for FTEs,
excluding cases in which the dictionary was used as a
backup.
shows examples of EQCONTEXTALIGN and DICONLY.
EQCONTEXTALIGN usually selects appropriate words,
compared to DICONLY.
When there are no plausible translation examples in the
translationmemory, the system selects a low-similarity or
low-confidence FTE. However we believe this problem
will be resolved as the number of translation examples
increases, since the News Corpus is increasing day by
day.
5 Related Work
The idea of example based machine translation systems
was first proposed by (Nagao, 1984), and preliminary
systems that appeared about ten years (Sato and Na-
gao, 1990; Sadler and Vendelmans, 1990; Maruyama and
Watanabe, 1992; Furuse and Iida, 1994) showed the basic
feasibility of the idea.
Recent studies have focused on the practical aspects
of EBMT, and this technology has even been applied
to some restricted domains. The work in (Richardson
et al, 2001; Menezes and Richardson, 2001) addressed
the problem of technical manual translation in several
languages, and the work of (Imamura, 2002) dealt with
dialogues translation in the travel arrangement domain.
These works select the translation example pairs based
solely on the source language similarity. We believe this
is partly due to the high parallelism found in their cor-
pora.
Our work targets a more general corpus of wider cover-
age, i.e., the broadcast news collection. Generally avail-
able corpora like the one we use tend to be more freely
translated and suffer from lower parallelism. This com-
pelled us to use the criterion of translation confidence,
together with the criterion of monolingual similarity used
in the previous works. As we showed in this paper, this
metric succeeded in meeting our expectations.
6 Conclusion
In this paper, we described operations of the entire EBMT
process while using a content-aligned corpus, i.e., the
NHK Broadcast Corpus. In this process, one of the key
problems is how to select plausible translation examples.
We proposed a new method to select translation exam-
ples based on source language similarity and translation
confidence. In the word selection task, the performance
is highly accurate.
Acknowledgements
This work was supported in part by the 21st Century COE
program ?Information Science and Technology Strate-
gic Core? at University of Tokyo and by a contract with
the Telecommunications Advancement Organization of
Japan, entitled ?A study of speech dialogue translation
technology based on a large corpus?.
References
Eiji Aramaki, Sadao Kurohashi, Satoshi Sato, and Hideo
Watanabe. 2001. Finding translation correspondences
from parallel parsed corpus for example-based transla-
tion. In Proceedings of MT Summit VIII, pages 27?32.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In In Proceedings of NAACL 2000, pages 132?
139.
Osamu Furuse and Hitoshi Iida. 1994. Constituent
boundary parsing for example-based machine transla-
tion. In Proceedings of the 15th COLING, pages 105?
111.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentarou Ogura, and Yoshi-
fumi Oyama Yoshihiko Hayashi, editors. 1997.
Japanese Lexicon. Iwanami Publishing.
Kenji Imamura. 2002. Application of translation knowl-
edgeacquired by hierarchical phrase alignment for
pattern-based mt. In Proceedings of TMI-2002, pages
74?84.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
Sadao Kurohashi. 2001. Senseval2 Japanese translation
task. In Proceedings of SENSEVAL2, pages 37?40.
Hiroshi Maruyama and Hideo Watanabe. 1992. The
cover search algorithm for example-based translation.
In Proceedings of TMI-1992, pages 173?184.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the ACL 2001 Workshop on Data-Driven Meth-
ods in Machine Translation, pages 39?46.
Makoto Nagao. 1984. A framework of a mechanical
translation between Japanese and english by analogy
principle. In In Artificial and Human Intelligence,
pages 173?180.
Stephen D. Richardson, William B. Dolan, Arul
Menezes, and Monica Corston-Oliver. 2001. Over-
coming the customization bottleneck using example-
based mt. In Proceedings of the ACL 2001 Work-
shop onData-DrivenMethods in Machine Translation,
pages 9?16.
V. Sadler and R. Vendelmans. 1990. Pilot implementa-
tion of a bilingual knowledge bank. In Proeedings of
the 13th COLING, pages 449?451.
Satoshi Sato andMakoto Nagao. 1990. Toward memory-
based translation. InProceedings of the 13th COLING,
pages 247?252.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1568?1576,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
 
 
Twitter Catches The Flu: Detecting Influenza Epidemics using Twitter 
  Eiji ARAMAKI  Sachiko MASKAWA Mizuki MORITA The University of Tokyo The University of Tokyo National Institute of  JST PRESTO  Biomedical Innovation  Tokyo, Japan Tokyo, Japan Osaka, Japan eiji.aramaki@gmail.com  sachiko.maskawa@gmail.com morita.mizuki@gmail.com      Abstract 
With the recent rise in popularity and scale of social media, a growing need exists for systems that can extract useful information from huge amounts of data. We address the issue of detecting influenza epidemics. First, the proposed system extracts influen-za related tweets using Twitter API. Then, only tweets that mention actual influenza patients are extracted by the support vector machine (SVM) based classifier. The ex-periment results demonstrate the feasibility of the proposed approach (0.89 correlation to the gold standard). Especially at the out-break and early spread (early epidemic stage), the proposed method shows high correlation (0.97 correlation), which out-performs the state-of-the-art methods. This paper describes that Twitter texts reflect the real world, and that NLP techniques can be applied to extract only tweets that contain useful information. 
1 Introduction Twitter1, a popular micro-blogging service, has received much attention recently. It is an online network used by millions of people around the world to stay connected to their friends, family members, and co-workers through their computers and mobile telephones (Milstein et al, 2010). Nowadays, Twitter users have increased rapidly. Its community estimated as 120 million worldwide,                                                            1 http://twitter.com/ 
posts more than 5.5 million messages (tweets) eve-ry day (reported by Twitter.com in March 2011). Twitter can potentially serve as a valuable infor-mation resource for various applications. Huber-man et al (2009) analyzed the relations among friends. Boyd et al (2010) investigated commuta-tion activity. Sakaki et al (2010) addressed the detection of earthquakes. Among the numerous potential applications, this study addresses the is-sue of detecting influenza epidemics, which pre-sents two outstanding advantages over current methods.  ? Large Scale: More than a thousand messages include the word ?influenza? each day (Nov. 2008 ? Oct. 2009). Such a huge data volume dwarfs traditional surveillance resources.  ? Real-time: Twitter enables real-time and di-rect surveillance. This characteristic is ex-tremely suitable for influenza epidemic detection because early stage detection is im-portant for influenza warnings.  Although Twitter based influenza warnings poten-tially offer the advantages noted above, it might also expose inaccurate or biased information from tweets like the following (brackets []	 ? indicate the comments):  ? Headache? You might have flu. [Suspi-??cions]	 ?? The World Health Organization reports the avian influenza, or bird flu, epidemic has spread to nine Asian countries in the past few weeks. [General	 ?News] 
1568
  
? Are you coming down with influenza? [Question]  Although these tweets include mention of ?influ-enza? or ?flu?, they do not indicate that an influen-za patient is present nearby. We regard such messages (merely suspicions/questions, general news, etc.) as negative influenza tweets. We call others positive influenza tweets. In our experi-ments, 42% of all tweets that include ?influenza? are negative influenza tweets. The huge volume of such negative tweets biases the results. This paper presents a proposal of a machine-learning based classifier to filter out negative in-fluenza tweets. First, we build an annotated corpus of pairs of a tweet and positive/negative labels. Then, a support vector machine (SVM) (Cortes and Vapnik, 1995) based sentence classifier extracts only positive influenza tweets from tweets. In the experiments, the results demonstrated the high cor-relation (0.89 of the correlation), which is equal performance to that of the state-of-the-art method.  The specified research point of this study is two-fold: (1) This report describes that an SVM-based clas-sifier can filter out the negative influenza tweets (f-measure=0.76). (2) Experiments empirically demonstrate that the proposed method detects the influenza epidem-ics with high accuracy (correlation ratio=0.89): it outperforms the state-of-the-art method. 2 Influenza Epidemic Detection The detection of influenza epidemics is a national mission in every country for two reasons. (1) Anti-influenza drugs, which differ among in-fluenza types, must be prepared before the epi-demics. (2) We can only slightly predict what type of in-fluenza will spread in any given season.  This situation naturally demands the early detec-tion of influenza epidemics. This section presents a description of previous methods of influenza epi-demic detection. 2.1 Traditional Approaches Most countries have their own influenza surveil-lance organization/center: the U.S. has the Centers 
for Disease Control and Prevention (CDC)2, the E.U. has its European Influenza Surveillance Scheme (EISS), and Japan has its Infection Disease Surveillance Center (IDSC). Their surveillance systems fundamentally rely on both virology and clinical data. For example, the IDSC gathers influ-enza patient data from 5,000 clinics and releases summary reports. Such manual systems typically have a 1?2 week reporting lag. This time lag is sometimes pointed out as a major flaw. 2.2 Recent Approaches In an attempt to provide earlier influenza detection, various new approaches are proposed each year. Espino et al (2003) described a telephone triage service, a public service, to give advice to users via telephone. They investigated the number of tele-phone calls and reported a significant correlation with influenza epidemics. Magruder (2003) used the amount of over-the-counter drug sales. Because an influenza patient usually requires anti-influenza drugs, this approach is reasonable. However, in most countries, anti-influenza drugs are not available at the drug store (only hospitals provide such drugs). The state-of-the-art approach is that proposed by Ginsberg et al (2009). They used Google web search queries that correlate with an influenza epi-demic. Their approach demonstrated high accuracy (average correlation ratio of 0.97; min=0.92; max=0.99)3. Several research groups have used similar approaches. Polgreen et al (2008) used a Yahoo! query log. Hulth et al (2009) used a query log of a Switzerland web search engine.  Although the above approaches use different in-formation, they share the same approach, which is to observe patient actions directly. This approach was sufficient to obtain more numerous data than traditional services. Nevertheless, such information is unfortunately limited only to the service pro-vider. For example, web search queries are avail-able only for several companies: Google, Yahoo!, and Microsoft. This paper examines Twitter data, which are widely available. Note that Paul and Dredze (2011) also propose a similar Twitter based approach. While they focus on a word distribution, this paper                                                            2 http://www.cdc.gov/flu/weekly/ 3 Their service is available at http://www.google.org/flutrends/ (Google Flu Trend). 
1569
  
employs a sentence classification (discrimination of negative influenza tweets). 3 Influenza Corpus As described in Section 1, it is necessary to filter out negative influenza tweets to infer precise amounts of influenza epidemics. To do so, we con-structed the influenza corpus (Section 3). Then, we trained the SVM-based classifier using the corpus (Section 4). The corpus comprises pairs of sentences and a label (positive or negative). Several examples are presented in Table 1. This corpus was built using the following procedure. 3.1 Influenza Tweet  First, we collected 300 million tweets, starting from 2008 November to 2010 June, via Twitter API. Crawling results are presented in Figure 1. We extracted only influenza-related tweets using a simple word look-up of ?influenza?. This operation gave us 0.4 million tweets. We separated the data into two data groups. Training Data are 5,000 tweets sent in Novem-ber 2008. These were annotated by human annota-tors, and were then used for training. Test Data are the other data. They were used in experiments of influenza epidemics detection. Be-cause of the three dropout periods (Figure 1), the test data were separated into four periods (winter 2008, summer 2009, winter 2009, and summer 2010). 3.2 Positive?negative Annotation To each tweet in the training dataset, a human an-notator assigned one of two labels: positive or neg-ative. In this labeling procedure, we regarded a tweet that meets the following two conditions as positive data.   Condition 1 (A Tweet person or Surrounding persons have Flu): one or more people who have influenza should exist around the tweet person. Here, we regard ?around? as a distance in the same city. In cases in which the distance is unknown, we regard it as negative. Because of this annotation policy, the re-tweet type message is negative.   
 Figure 1: Twitter Data used in this Study. The data include three dropout periods because the Twitter API specifications changed in those periods. The dropout periods were removed from evaluation in the experiments (Section 5).  Table 1: Corpus (Tweets with a Positive or Nega-tive Label) Positive(+1)/	 ?Negative(-??1)	 ? Tweet	 ?+1	 ? A	 ?bad	 ?	 ?influenza	 ?is	 ?going	 ?around	 ?in	 ?our	 ?lab.	 ?+1	 ? I	 ?caught	 ?the	 ?flu.	 ?I	 ?was	 ?burning	 ?up.	 ?+1	 ? I	 ?think	 ?	 ?I'm	 ?coming	 ?down	 ?with	 ?the	 ?flu.	 ?+1	 ? It's	 ?the	 ?flu	 ?season.	 ?I	 ?had	 ?it	 ?and	 ?now	 ?he	 ?do	 ?es.	 ?+1	 ? Don't	 ?give	 ?me	 ?the	 ?flu.	 ?(Nearby	 ?people	 ?have	 ?the	 ?flu)	 ?+1	 ? My	 ?flu	 ?is	 ?worse	 ?than	 ?it	 ?was	 ?yesterday.	 ?-??1	 ? In	 ?the	 ?normal	 ?flu	 ?season,	 ?80	 ?percent	 ?of	 ?deaths	 ?occur	 ?in	 ?people	 ?over	 ?65	 ?(Simply	 ?a	 ?fact)	 ?-??1	 ? Influenza	 ?is	 ?now	 ?raging	 ?throughout	 ?Japan.	 ?(Too	 ?general.)	 ?-??1	 ? His	 ?wife	 ?also	 ?contracted	 ?the	 ?bird	 ?flu,	 ?but	 ?has	 ?recovered.	 ?(Where	 ?is	 ?his	 ?wife?)	 ?-??1	 ? You	 ?might	 ?have	 ?the	 ?flu.	 ?Has	 ?anyone	 ?around	 ?you	 ?had	 ?it?	 ?(Where	 ?are	 ?you?)	 ?-??1	 ? Bird	 ?flu	 ?damage	 ?is	 ?spreading	 ?in	 ?Japan.	 ?(Too	 ?general.)	 ??+1? indicates a positive influenza tweet. ?-1? indicates a negative influenza tweet. The case arc ?()? indicates the rea-son for the positive or negative annotation.   
 Figure 2: Feature Representation. The word boundary is detected by a morph analyzer JUMAN4.  
                                                           4 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html 
1570
  
Condition 2 (Tense/Modality): The tense should be the present tense (current) or recent past. Here, we define the ?recent past? as the prior 24 hour period (such as ?yesterday?). The sentence should be affirmative (not interrogative and not subjunc-tive). 4 Influenza Positive?negative Classifier Using the corpus (Section 3), we built a classifier that judges whether a given tweet is positive or negative. This task setting is similar to a sentence classification (such as spam e-mail filtering, senti-ment analysis, and so on). We used a popular means for sentence classification, which is based on a machine learning classifier under the bag-of-words (BOW) representation (Figure 2). The parameters were investigated in preliminary ex-periments in terms of feature window size (Section 4.1) and machine-learning methods (Section 4.2). These preliminary experiments were conducted under the ten-fold cross variation manner using the training set. 4.1 Feature (window size) Performance was dependent on the window size (the number of left/right side words). Figure 3 de-picts the performance obtained using various win-dow sizes. The best performance was scored at the BOTH=6 setting. Therefore, this window size was used for the following experiments. These results also indicated that entire sentences (BOTH=?) are unsuitable for this task. 4.2 Machine Learning Method We compared various machine-learning methods from two points of view: accuracy and time. The result, presented in Table 2, shows that SVM with a polynomial kernel showed feasibility from both viewpoints of accuracy and the training time. 5 Experiments We assessed the detection performance using actu-al influenza reports provided by the Japanese IDSC. 5.1 Comparable Methods We compared the various methods as follows:    
 
 Figure 3: Window size and Accuracy (F-measure). RIGHT shows a method used only the right context. LEFT shows a method used only the left context. BOTH represents a method using both the right and left context. The number shows the window size. ? uses all words in each context di-rection.   Classifier	 ? F-??Measure	 ? Training	 ?Time	 ?(sec)	 ?AdaBoost	 ?	 ?(Freund	 ?1996)	 ? 0.592	 ? 40.192	 ?Bagging	 ?	 ?(Breiman	 ?1996)	 ? 0.739	 ? 	 ?30.310	 ?Decision	 ?Tree	 ?(Quinlan1993)	 ? 0.698	 ? 239.446	 ?Logistic	 ?Regression	 ? 0.729	 ? 696.704	 ?Naive	 ?Bayes	 ? 0.	 ?741	 ? 7.383	 ?Nearest	 ?Neighbor	 ? 0.695	 ? 22.441	 ?Random	 ?Forest	 ?(Breiman	 ?2001)	 ? 0.729	 ? 38.683	 ?SVM	 ?(RBF	 ?kernel)	 ?	 ?(Cortes	 ?and	 ?Vapnik	 ?1995)	 ? 0.738	 ? 92.723	 ?SVM	 ?(polynomial	 ?kernel;	 ?d=2)	 ? 0.756	 ? 13.256	 ?Table 2: Machine Learning Methods and Perform-ance (F-measure and Training Time)    ? TWEET-SVM: The proposed SVM-based method (window size = 6). ? TWEET-RAW: A simple frequency-based method. This approach outputs the relative frequency of word ?influenza? appearing in Twitter. ? DRUG: The amounts of drug sales (sales of cold medicines). Statistics are provided by the Japanese Ministry of Health, Labor and Welfare. ? GOOGLE: Google flu trend detection (Japane-se version). This method uses a query log of the Google search engine (Ginsberg et al, 2009)5. 
                                                           5 http://www.google.org/flutrends/  
1571
  
5.2 Gold Standard and Test-Set For gold standard data, we used data that are de-scribed in Section 2, as reported from IDSC. The report is released once a week. Therefore, the evaluation is done on a weekly basis. We split the data into four seasons as follows: ? Season I: winter 2008, ? Season II: summer 2009, ? Season III: winter 2009, ? Season IV: summer 2010.  To investigate further detailed evaluations, we split the winters into two sub-seasons: before the peak and after the peak. We regard the peak point as the day with the highest number in that season. The statistics derived from the data are presented in Table 3.  Excessive News Period: In our experimental data, Season II and the earlier peak of Season III are special periods because news related to swine flu (H1N1 flu) is extremely hot in those seasons (Fig. 4). This paper calls them Excessive News Periods. We also investigated the results with and without the excessive news period.  
 Figure 4: A CNN news on ?swine flu? in June 2009 (Season II in our experiment). Experimental data include such excessive news peri-ods.  5.3 Evaluation Metric The evaluation metric is based on correlation (Pearson correlation) between the gold standard value and the estimated value. 
5.4 Result The results are presented in Table 4. In the non-excessive news period, the proposed method achieved the highest performance (0.890 correla-tion). This correlation is considerably higher than the query-based approach (GOOGLE), demonstrat-ing the basic feasibility of the proposed approach. However, during the excessive news periods, the proposed method suffers from an avalanche of news, generating a news bias. This phenomenon is a remaining problem to be resolved in future stud-ies.  6 Discussion 
6.1 SVM-based Negative Filtering contributes to Performance In most seasons, the proposed SVM approach (TWEET-SVM) shows higher correlation than the simple word lookup method (TWEET-RAW). The average improvement is 0.196 (max 0.56; min-0.009), which significantly boosts the correlation. This result demonstrates the basic feasibility of the proposed approach. In the future, more advantages attributable to the proposed approach can be ob-tained if the classification performance improves. 6.2 All Methods Suffer from News Bias in Excessive News Period All methods expose the poor performance that pre-vails during the excessive news period (from Sea-son II to Season III before the peak). Especially, tweet-based methods show dramatically reduced correlation, which indicates that Twitter is vulner-able to newswire bias. One reason for that vulnerability is that Twitter is a kind of communication tool by which a tweet affects other people. Consequently, the possibility exists that a few tweets related to ?flu? might spread widely, generating an explosive burst of influenza-related tweets. Future studies must ad-dress this burst phenomenon.  
1572
  
6.3 Tweets have Advantages in Early Stage Detection From practical viewpoints, the most important task is to detect influenza epidemics before the peak (early stage detection). Consequently, the correla-tion of the two seasons, Season I before the peak and Season III before the peak, presents the practi-cal performance. Figure 5 portrays detailed results of all methods. In Season I before the peak (Figure 5 Left), the proposed method (TWEET-SVM) shows the best performance among all methods. 
In Season II before the peak (Figure 5 Right), all methods including the proposed method showed poor correlation because they are included in the excessive news periods. During that season, the newswires heavily reported the swine flu twice (April 2009 and May 2009). Because of this news, we can see two peaks in Twitter-based methods (TWEET-SVM and TWEET-RAW), which indi-cates that Twitter is more sensitive to the news-wires.   
 
Table 3: Test-set Tracks and the number of data points (=weeks). The number in the bracket indicates the statistical significance level.   	 ? TWEET-RAW TWEET-SVM (Proposed Method) DRUG GOOGLE Excessive	 ?news	 ?period	 ?	 ? 0.001	 ? 0.060	 ? 0.844	 ? 0.918	 ?Non-?? excessive	 ?news	 ?period	 ? 0.831	 ? 0.890	 ? 0.308	 ? 0.847	 ?	 ?	 ? 0.683	 ? 0.816	 ? -??0.208	 ? 0.817	 ?Before	 ?peak	 ? 0.914	 ? 0.974	 ? -??0.155	 ? 0.962	 ?	 ?	 ?Season	 ?I	 ? After	 ?peak	 ? 0.952	 ? 0.955	 ? 0.557	 ? 0.959	 ?Season	 ?II	 ?	 ? -??0.009	 ? -??0.018	 ? 0.406	 ? 0.232	 ?	 ? 0.382	 ? 0.474	 ? 0.684	 ? 0.881	 ?Before	 ?peak	 ? 0.390	 ? 0.474	 ? 0.919	 ? 0.924	 ?	 ?	 ?Season	 ?III	 ? After	 ?peak	 ? 0.960	 ? 0.944	 ? 0.364	 ? 0.936	 ?Season	 ?IV	 ? 0.391	 ? 0.957	 ? 0.130	 ? 0.976	 ?Table 4: Results (Correlation Ratio). The number in bold indicates the significance correlation (p=0.05). The number with underline indicates the highest value in each season.   
All	 ?Season	 ?79	 ?weeks	 ?	 ?(0.221)	 ?Season	 ?I	 ?	 ? 2008/11/9	 ?-??	 ?2009/4/5	 ?
Season	 ?II	 ?	 ?2009/4/12	 ?-??	 ?	 ?2009/7/5	 ?
Season	 ?III	 ?	 ?2009/7/12	 ?-??	 ?2010/2/14	 ?
Season	 ?IV	 ?	 ?2010/2/21	 ?-??	 ?2010/7/4	 ?22	 ?weeks	 ?	 ?(0.423)	 ? 26	 ?	 ?weeks	 ?(0.388)	 ?Before	 ?peak	 ?2008/11/9-??2009/1/25	 ? After	 ?peak	 ?2009/2/1-??2009/4/5	 ? Before	 ?peak	 ?2009/7/12-??2009/11/29	 ? After	 ?peak	 ?2009/12/6-??2010/2/14	 ?12weeks	 ?(0.576)	 ? 10	 ?weeks	 ?(0.632)	 ?
	 ? 13	 ?weeks	 ?(0.553)	 ? 15	 ?weeks	 ?(0.514)	 ? 11	 ?weeks	 ?(0.602)	 ?
	 ? 18	 ?weeks	 ?(0.468)	 ?
Non-??excessive	 ?news	 ?period	 ?	 ?	 ? Excessive	 ?news	 ?period	 ?	 ? Non-??excessive	 ?news	 ?period	 ?	 ?
1573
  
6.4 Human Action is Sensitive before Epi-demics Figure 6 presents the distribution between the de-tected values (using GOOGLE and using TWEET-SVM) and the gold standard value (before the peak is shown by ?+?; that after the peak is shown as ?-?). Although the detected values fundamentally correlate with the gold standard, we can see differ-ent sensitivity before and after peak (The distribu-tion before peak ?+? is a higher value than after peak ?-?.).  Results show that human action, a web search (GOOGLE) and a tweet (TWEET-SVM), highly cor-responds to the real influenza before the epidemic peaks, and vice versa. More acute detection is pos-sible if we incorporate a model considering this aspect of human nature. 
7 Related Works The core technology of the proposed method is to classify whether the event is positive or negative. This task is similar to negation identification, which is a traditional topic, especially in medical fields. Therefore, we can find many previous stud-ies of the topic in the relevant literature. An algo-rithm based approach, NegEx (Chapman et al, 2001), Negfinder (Mutalik et al, 2001), and Con-Text (Chapman et al, 2007), a machine learning based approach (Elkin et al, 2005; Huang and H.J. Lowe, 2007).     
      
 Figure 5: Predicted Values in Season I (Left) and Season II (Right): the X-axis shows the date; the Y-axis shows the relative predicted value using each method.    
       Figure 6:  Patient Actions (Web Search Query and Tweet) is Sensitive before the Epidemic Peaks. Distribution between the gold standard and Detected Values (Search Engine Query (Left) and Tweet (Right)):  ?+? denotes the distribution before the peak; ?-? denotes the distribution after the peak.  
1574
  
  Previous	 ?Negation	 ?	 ?(Syntactic)	 ?
This	 ?study:	 ?Negative	 ?Influenza	 ?(Semantic)	 ?I	 ?caught	 ?a	 ?flu.	 ? Positive	 ?sentence	 ? Positive	 ?Influenza I	 ?don?t	 ?have	 ?the	 ?flu!	 ? Negative	 ?sentence	 ? Negative	 ?Influenza I	 ?have	 ?enough	 ?flu	 ?drugs.	 ? Positive	 ?sentence	 ? Negative	 ?Influenza I	 ?have	 ?not	 ?recovered	 ?from	 ?the	 ?flu. Negative	 ?sentence Positive	 ?Influenza Table 5: Our target influenza negation (semantic) and previous negation (syntactic)   Although these approaches specifically examine the syntactic negation, this study detects the nega-tive influenza, which is a specified semantic nega-tion. Table 5 presents the difference between both negations. In general, the semantic operation is difficult in general. However, this paper revealed that the domain (influenza domain) specific seman-tic operation provides reasonable results. Another aspect of this study is the target mate-rial, Twitter data, which have drawn much atten-tion. Twitter can provide suitable material for many applications such as named entity recogni-tion (NER) (Finin et al, 2010) and sentiment analysis (Barbosa and Feng, 2010). Although these studies specifically examine the fundamental NLP techniques, this study directly targets an NLP ap-plication that can contribute to our daily life. 8 Conclusion This paper proposed a new Twitter-based influenza epidemics detection method, which relies on the  Natural Language Processing (NLP). Our proposed method could successfully filter out the negative influenza tweets (f-measure=0.76), which are post-ed by the ones who did not actually catch the influ-enza. The experiments with the test data empirically demonstrate that the proposed method detects influenza epidemics with high correlation (correlation ratio=0.89), which outperforms the state-of-the-art Google method. This result shows that Twitter texts precisely reflect the real world, and that the NLP technique can extract the useful information from Twitter streams. 
 
 Figure 7:  An influenza severance system ?INFLU kun? using the proposed method is available at http://mednlp.jp/influ/.  
 Figure 8: The Timeline of Influenza Epidemics in Fukushima. While the Infection Disease Surveil-lance Center (IDSC) sometimes stops (gold stan-dard) due to the Great East Japan Earthquake, the proposed system could continue to work (Our Sys-tem).  Available Resources Corpus: The corpus of this study is provided at the http://mednlp.jp/~aramaki/KAZEMIRU/. Web System: The web service is also released at http://mednlp.jp/influ/ (Figure 7 and Figure 8).  
1575
  
References Barbosa, L. and J. Feng. 2010. Robust Sentiment Detec-tion on Twitter from Biased and Noisy Data. In Proc. 23rd Intl. Conf. on Computational Linguistics (COLING). Boyd, D., S. Golder, and G. Lotan. 2010. Tweet, tweet, retweet: Conversational aspects of retweeting on Twitter. In Proc. HICSS43. Breiman L. Random Forests. 2001. Machine learning, 45(1): 5?32. Breiman, L. Bagging predictors. 1996. Machine learn-ing, 24(2):123?140. Cortes C. and V. Vapnik. 1995. Support vector net-works. In Machine Learning, pp. 273?297. Chapman, W., W. Bridewell, P. Hanbury, G.F. Cooper, and B. Buchanan. 2001. A simple algorithm for iden-tifying negated findings and diseases in discharge summaries. Journal of Biomedical Informatics, 5:301-310. Chapman, W., J. Dowling, and D. Chu. 2007. ConText: An algorithm for identifying contextual features from clinical text. Biological, translational, and clinical language processing (BioNLP2007), pp. 81?88. Elkin, P.L., S.H. Brown, B.A. Bauer, C.S. Husser, W. Carruth, L.R. Bergstrom, and D.L. Wahner-Roedler. 2005. A controlled trial of automated classification of negation from clinical notes. BMC Medical Informat-ics and Decision Making 5:13. Espino, J., W. Hogan, and M. Wagner. 2003. Telephone triage: A timely data source for surveillance of influ-enza-like diseases. In Proc. of Annual Symposium of AMIA, pp. 215?219. Finin, T., W. Murnane, A. Karandikar, N. Keller, J. Martineau, and M. Dredze. 2010. Annotating named entities in Twitter data with crowdsourcing. In Proc. NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk (CSLDAMT '10), pp. 80-88. Freund, Y. and R. Schapire. 1996. Experiments with a new boosting algorithm. In Machine Learning Intl. Workshop, pp.148?156. Ginsberg, J., M.H. Mohebbi, R.S. Patel, and L. Bram-mer. 2009. Detecting influenza epidemics using search engine query data, Nature Vol. 457 (19). Huang, Y. and H.J. Lowe. 2007. A novel hybrid ap-proach to automated negation detection in clinical ra-diology reports. Journal of the American Medical Informatics Association, 14(3):304-311. Huberman, B. and D. R. F. Wu. 2009. Social networks that matter: Twitter under the microscope. First Monday, Vol. 14. 
Hulth, A., G. Rydevik, and A. Linde. 2009. Web Queri-es as a Source for Syndromic Surveillance. PLoS ONE 4(2). Johnson, HA., MM. Wagner, WR. Hogan, W. Chapman, RT. Olszewski, J. Dowling, and G. Barnas. 2004. Analysis of Web access logs for surveillance of in-fluenza. Stud. Health Technol. Inform. 107(Pt 2):1202-1206. Magruder, S. 2003. Evaluation of over-the-counter pharmaceutical sales as a possible early warning indi-cator of human disease. Johns Hopkins University APL Technical Digest 24:349?353. Milstein, S., A. Chowdhury, G. Hochmuth, B. Lorica, and R. Magoulas. 2008. Twitter and the micro-messaging revolution: Communication, connections, and immediacy, 140 characters at a time. O?Reilly Media. Mutalik, P.G., A. Deshpande, and P.M. Nadkarni. 2001. Use of general purpose negation detection to augment concept indexing of medical documents: A quantita-tive study using theUMLS. Journal of the American Medical Informatics Association, 8(6):598-609. Paul, MJ. and M. Dredze. 2011. You Are What You Tweet: Analyzing Twitter for Public Health. In  Proc. of the 5th International AAAI Conference on We-blogs and Social Media (ICWSM).  Polgreen, PM., Y. Chen, D.M. Pennock, and F.D. Nel-son. 2008. Using Internet Searches for Influenza Sur-veillance, Clinical Infectious Diseases Vol. 47 (11) pp. 1443-1448. Quinlan. J. 1993. C4. 5: programs for machine learning. Morgan Kaufmann. Sakaki, T., M. Okazaki, and Y. Matsuo. 2010. Earth-quake shakes Twitter users: real-time event detection by social sensors, in Proc. of Conf. on World Wide Web (WWW).      
1576
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 75?83,
Beijing, August 2010
Adverse?Effect Relations Extraction from  
Massive Clinical Records 
Yasuhide Miura a, Eiji Aramaki b, Tomoko Ohkuma a, Masatsugu Tonoike a,  
Daigo Sugihara a, Hiroshi Masuichi a and Kazuhiko Ohe c 
     a  Fuji Xerox Co., Ltd.
       b Center for Knowledge Structuring, University of Tokyo
         c University of Tokyo Hospital
yasuhide.miura@fujixerox.co.jp, eiji.aramaki@gmail.com, 
{ohkuma.tomoko,masatsugu.tonoike,daigo.sugihara, 
hiroshi.masuichi}@fujixerox.co.jp,  
kohe@hcc.h.u-tokyo.ac.jp 
 
Abstract 
The rapid spread of electronic health 
records raised an interest to large-scale 
information extraction from clinical 
texts. Considering such a background, 
we are developing a method that can 
extract adverse drug event and effect 
(adverse?effect) relations from massive 
clinical records. Adverse?effect rela-
tions share some features with relations 
proposed in previous relation extrac-
tion studies, but they also have unique 
characteristics. Adverse?effect rela-
tions are usually uncertain. Not even 
medical experts can usually determine 
whether a symptom that arises after a 
medication represents an adverse?
effect relation or not. We propose a 
method to extract adverse?effect rela-
tions using a machine-learning tech-
nique with dependency features. We 
performed experiments to extract ad-
verse?effect relations from 2,577 clini-
cal texts, and obtained F1-score of 
37.54 with an optimal parameters and 
F1-score of 34.90 with automatically 
tuned parameters. The results also 
show that dependency features increase 
the extraction F1-score by 3.59.  
1 Introduction  
The widespread use of electronic health rec-
ords (EHR) made clinical texts to be stored as 
computer processable data. EHRs contain im-
portant information about patients? health. 
However, extracting clinical information from 
EHRs is not easy because they are likely to be 
written in a natural language. 
We are working on a task to extract adverse 
drug event and effect relations from clinical 
records. Usually, the association between a 
drug and its adverse?effect relation is investi-
gated using numerous human resources, cost-
ing much time and money. The motivation of 
our task comes from this situation. An example 
of the task is presented in Figure 1. We defined 
an adverse?effect relation as a relation that 
holds between a drug entity and a symptom 
entity. The sentence illustrates the occurrence 
of the adverse?effect hepatic disorder by the 
Singulair medication.  
 
Figure 1. Example of an adverse?effect relation. 
A hepatic disorder found was suspected drug-induced and the Singulair was stopped.
adverse?effect relation
symptom drug
75
A salient characteristic of adverse?effect re-
lations is that they are usually uncertain. The 
sentence in the example states that the hepatic 
disorder is suspected drug-induced, which 
means the hepatic disorder is likely to present 
an adverse?effect relation. Figure 2 presents an 
example in which an adverse?effect relation is 
suspected, but words to indicate the suspicion 
are not stated. The two effects of the drug??the 
recovery of HbA1c and the appearance of the 
edema??are expressed merely as observation 
results in this sentence. The recovery of 
HbA1c is an expected effect of the drug and 
the appearance of the edema probably repre-
sents an adverse?effect case. The uncertain 
nature of adverse?effect relations often engen-
ders the statement of an adverse?effect rela-
tion as an observed fact. A sentence includ-
ing an adverse?effect relation occasionally be-
comes long to list all observations that ap-
peared after administration of a medication. 
Whether an interpretation that expresses an 
adverse?effect relation, such as drug-induced 
or suspected to be an adverse?effect, exists in a 
clinical record or not depends on a person who 
writes it. However, an adverse?effect relation 
is associated with an undesired effect of a 
medication. Its appearance would engender an 
extra action (e.g. stopped in the first example) 
or lead to an extra indication (e.g. but ? ap-
peared in the second example). Proper han-
dling of this extra information is likely to boost 
the extraction accuracy. 
The challenge of this study is to capture re-
lations with various certainties. To establish 
this goal, we used a dependency structure for 
the adverse?effect relation extraction method. 
Adverse?effect statements are assumed to 
share a dependency structure to a certain 
degree. For example, if we obtain the depend-
ency structures as shown in Figure 3, then we 
can easily determine that the structures are 
similar. Of course, obtaining such perfect pars-
ing results is not always possible. A statistical 
syntactic parser is known to perform badly if a 
text to be parsed belongs to a domain which 
differs from a domain on which the parser is 
trained (Gildea, 2001). A statistical parser will 
likely output incomplete results in these texts 
and will likely have a negative effect on rela-
tion extraction methods which depend on it. 
The specified research topic of this study is to 
investigate whether incomplete dependency 
structures are effective and how they behave in 
the extraction of uncertain relations.  
Figure 2. The example of an adverse-effect relation where the suspicion is not stated. 
Figure 3. The example of a similarity within dependency structures. 
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
A suspected drug-induced hepatic disorder found and the Singulair was stopped.
conjunct
nominal subject nominal subject
nominal subject nominal subject
conjunct
was
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
adverse-effect relation
drug symptom
76
2 Related Works 
Various studies have been done to extract se-
mantic information from texts. SemEval-2007 
Task:04 (Girju et al, 2007) is a task to extract 
semantic relations between nominals. The task 
includes ?Cause?Effect? relation extraction, 
which shares some similarity with a task that 
will be presented herein. Saeger et al (2008) 
presented a method to extract potential trou-
bles or obstacles related to the use of a given 
object. This relation can be interpreted as a 
more general relation of the adverse?effect 
relation. The protein?protein interaction (PPI) 
annotation extraction task of BioCreative II 
(Krallinger et al, 2008) is a task to extract PPI 
from PubMed abstracts. BioNLP?09 Shared 
Task on Event Extraction (Kim et al, 2009) is 
a task to extract bio-molecular events (bio-
events) from the GENIA event corpus.  
Similar characteristics to those of the ad-
verse?effect relation are described in previous 
reports in the bio-medical domain. Friedman et 
al. (1994) describes the certainty in findings of 
clinical radiology. Certainty is also known in 
scientific papers of biomedical domains as 
speculation (Light et al, 2004). Vincze et al 
(2008) are producing a freely available corpus 
including annotations of uncertainty along with 
its scope. 
Dependency structure feature which we uti-
lized to extract adverse?effect relations are 
widely used in relation extraction tasks. We 
present previous works which used syntac-
tic/dependency information as a feature of a 
statistical method. Beamer et al (2007), Giuli-
ano et al (2007), and Hendrickx et al (2007) 
all used syntactic information with machine 
learning techniques in SemEval-2007 Task:04 
and achieved good performance. Riedel et al 
(2009) used dependency path features with a 
statistical relational learning method in Bi-
oNLP?09 Shared Task on Event Extraction and 
achieved the best performance in the event en-
richment subtask. Miyao et al (2008) com-
pared syntactic information of various statisti-
cal parsers on PPI. 
3 Corpus  
We produced an annotated corpus of adverse?
effect relations to develop and test an adverse?
effect relation extraction method. This section 
presents a description of details of the corpus. 
3.1 Texts Comprising the Corpus 
We used a discharge summary among various 
documents in a hospital as the source data of 
the task. The discharge summary is a docu-
ment created by a doctor or another medical 
expert at the conclusion of a hospital stay. 
Medications performed during a stay are writ-
ten in discharge summaries. If adverse?effect 
relations were observed during the stay, they 
are likely to be expressed in free text. Texts 
written in discharge summaries tend to be writ-
ten more roughly than texts in newspaper arti-
cles or scientific papers. For example, the 
amounts of medications are often written in a 
name-value list as shown below: 
?When admitted to the hospital, Artist 6 mg1x, 
Diovan 70 mg1x, Norvasac 5 mg1x and BP 
was 145/83, but after dialysis, BP showed a 
decreasing tendency and in 5/14 Norvasac was 
reduced to 2.5 mg1x.? 
3.2 Why Adverse?Effect Relation Extrac-
tion from Discharge Summaries is 
Important 
In many countries, adverse?effects are investi-
gated through multiple phases of clinical trials, 
but unexpected adverse?effects occur in actual 
medications. One reason why this occurs is 
that drugs are often used in combination with 
others in actual medications. Clinical trials 
usually target single drug use. For that reason, 
the combinatory uses of drugs occasionally 
engender unknown effects. This situation natu-
rally motivates automatic adverse?effect rela-
tion extraction from actual patient records.  
  
77
 3.3 Corpus Size 
We collected 3,012 discharge summaries1 writ-
ten in Japanese from all departments of a hos-
pital. To reduce a cost to survey the occurrence 
of adverse?effects in the summaries, we first 
split the summaries into two sets: SET-A, 
which contains keywords related to adverse?
effects and SET-B, which do not contain the 
keywords. The keywords we used were ?stop, 
change, adverse effect?, and they were chosen 
based on a heuristic. The keyword filtering 
resulted to SET-A with 435 summaries and 
SET-B with 2,577 summaries. Regarding SET-
A, we randomly sampled 275 summaries and 
four annotators annotated adverse?effect in-
formation to these summaries to create the ad-
verse?effect relation corpus. For SET-B, the 
four annotators checked the small portion of 
the summaries. Cases of ambiguity were re-
solved through discussion, and even suspicious 
adverse?effect relations were annotated in the 
corpus as positive data. The overview of the 
summary selection is presented in Figure 4.  
                                                 
1 All private information was removed from them. 
The definition of private information was referred 
from the HIPAA guidelines. 
3.4 Quantities of Adverse?Effects in Clin-
ical Texts 
55.6% (=158/275) of the summaries in SET-A 
contained adverse?effects. 11.3% (=6/53) of 
the summaries in SET-B contained adverse?
effects. Since the ratio of SET-A:SET-B is 
14.4:85.6, we estimated that about 17.7%  
(=0.556?0.144+0.113?0.856) of the summar-
ies contain adverse?effects. Even considering 
that a summary may only include suspected 
adverse?effects, we think that discharge sum-
maries are a valuable resource to explore ad-
verse?effects. 
3.5 Annotated Information 
We annotated information of two kinds to the 
corpus: term information and relation infor-
mation. 
(1) Term Annotation  
Term annotation includes two tags: a tag to 
express a drug and a tag to express a drug ef-
fect. Table 1 presents the definition. In the 
corpus, 2,739 drugs and 12,391 effects were 
annotated. 
(2) Relation Annotation  
Adverse?effect relations are annotated as the 
?relation? attribute of the term tags. We repre-
sent the effect of a drug as a relation between a 
drug tag and a symptom tag. Table 2 presents 
Table 2. Annotation examples. 
Figure 4. The overview of the summary 
selection. 
Table 1. Markup scheme. 
The expression of a disease or 
symptom: e.g. endometrial cancer, 
headache. This tag covers not only a 
noun phrase but also a verb phrase 
such as ?<symptom>feels a pain in 
front of the head</symptom>?.
symptom
The expression of an administrated 
drug: e.g. Levofloxacin, Flexeril. 
drug
Definition and Examplestag
<drug relation=?1?>ACTOS(30)</drug> brought 
both <symptom relation=?1?>headache<symptom> 
and <symptom relation=?1?>insomnia</symptom>.
<drug relation=?1?>Ridora</drug> resumed 
because it is associated with an <symptom 
relation=?1?>eczematous rash</symptom>.
* If a drug has two or more adverse-effects, 
symptoms take a same relation ID.
3,012 
discharge 
summaries
435
summaries
w/ keywords
2,577
summaries
w/o keywords
275
summaries
53
summaries
153
summaries
w/ adverse?
effects
122
summaries
w/o adverse?
effects
6
summaries
w/ adverse?
effects
47
summaries
w/o adverse?
effects
YES NO
Contain keywords?
Random samplingRandom sampling
Contain adverse?
effects?
Contain adverse?
effects?
YES YESNO NO
SET-A (annotated corpus) SET-B
78
several examples, wherein ?relation=1? de-
notes the ID of a adverse?effect relation. In the 
corpus, 236 relations were annotated.  
4 Extraction Method 
We present a simple adverse?effect relation 
extraction method. We extract drug?symptom 
pairs from the corpus and discriminate them 
using a machine-learning technique. Features 
based on morphological analysis and depend-
ency analysis are used in discrimination. This 
approach is similar to the PPI extraction ap-
proach of Miyao et al (2008), in which we 
binary classify pairs whether they are in ad-
verse?effect relations or not. A pattern-based 
semi-supervised approach like Saeger et al 
(2008), or more generally Espresso (Pantel and 
Pennacchiotti, 2006), can also be taken, but we 
chose a pair classification approach to avoid 
the effect of seed patterns. To capture a view 
of an adverseness of a drug, a statistic of ad-
verse?effect relations is important. We do not 
want to favor certain patterns and chose a pair 
classification approach to equally treat every 
relation. Extraction steps of our method are as 
presented below. 
STEP 1: Pair Extraction   
All combinations of drug?symptom pairs that 
appear in a same sentence are extracted. Pairs 
<drug relation=?1?>Lasix</drug> for 
<symptom>hyperpiesia</symptom> has 
been suspended due to the appearance of 
a <symptom relation=?1?>headache
</symptom>.
headacheLasixpositive
hyperpiesiaLasixnegative
symptomdruglabel
ID Feature Definition and Examples
1 Character Distance The number of characters between members of a pair.
2 Morpheme Distance The number of morpheme between members of a pair.
3 Pair Order Order in which a drug and a symptom appear in a text; 
?drug?symptom? or ?symptom?drug?.
4 Symptom Type The type of symptom: ?disease name?, ?medical test name?, 
or ?medical test value?. 
5 Morpheme Chain Base?forms of morphemes that appear between a pair.
6 Dependency Chain Base?forms of morphemes included in the minimal 
dependency path of a pair.
7 Case Frame Chain Verb, case frame, and object triples that appear between a 
pair: e.g. ?examine? ??de?(case particle) ? ?inhalation?, 
?begin? ??wo?(case particle) ??medication?.
8 Case Frame 
Dependency Chain
Verb, case frame, and object triples included in the minimal 
dependency path of a pair.
Figure 6. Dependency chain example. 
 
Figure 5. Pair extraction example. 
hyperpiesia no-PP
for no-PP
Lasix wo-PP
headache no-PP 
appear niyori-PP
suspend ta-AUX
Lasix, wo-PP, headache, no-PP, 
appear, niyori-PP, suspend, ta-AUX
minimal path
Table 3. Features used in adverse-effect extraction. 
79
with the same relation ID become positive 
samples; pairs with different relation IDs be-
come negative samples. Figure 5 shows exam-
ples of positive and negative samples.  
STEP 2: Feature Extraction  
Features presented in Table 3 are extracted. 
The text in the corpus is in Japanese. Some 
features assume widely known characteristics 
of Japanese. For example, the dependency fea-
ture allows a phrase to depend on only one 
phrase that appears after a dependent phrase. 
Figure 6 portrays an example of a dependency 
chain feature. In the example, most terms were 
translated into English, excluding postpositions 
(PP) and auxiliaries (AUX), which are ex-
pressed in italic. To reduce the negative effect 
of feature sparsity, features which appeared in 
more than three summaries are used for fea-
tures with respective IDs 5?8. 
STEP 3: Machine Learning  
The support vector machine (SVM) (Vapnik, 
1995) is trained using positive/negative labels 
and features extracted in prior steps. In testing,                                          
an unlabeled pair is given a positive or nega-
tive label with the trained SVM.  
5 Experiment 
We performed two experiments to evaluate the 
extraction method. 
5.1 Experiment 1 
Experiment 1 aimed to observe the effects of 
the presented features. Five combinations of 
the features were evaluated with a five-fold 
cross validation assuming that an optimal pa-
rameter combination was obtained. The exper-
iment conditions are described below: 
A. Data  
7,690 drug?symptom pairs were extracted 
from the corpus.  Manually annotated infor-
mation was used to identify drugs and symp-
toms. Within 7,690 pairs, 149 pairs failed to 
extract the dependency chain feature. We re-
moved these 149 pairs and used the remaining 
7,541 pairs in the experiment. The 7,541 pairs 
consisted of 367 positive samples and 7,174 
negative samples.  
B. Feature Combinations  
We tested the five combinations of features in 
the experiment. Manually annotated infor-
mation was used for the symptom type feature. 
Features related to morphemes are obtained by 
processing sentences with a Japanese mor-
phology analyzer (JUMAN2 ver. 6.0). Features 
related to dependency and case are obtained by 
processing sentences using a Japanese depend-
ency parser (KNP ver. 3.0; Kurohashi and Na-
gao, 1994).  
C. Evaluations  
We evaluated the extraction method with all 
combinations of SVM parameters in certain 
                                                 
2 http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman-e.html 
E
D
C
B
A
ID
35.45
35.01
34.39
33.30
26.72
Precision
41.05
40.67
43.06
42.43
46.21
Recall
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=3.0, log(g)=-5.0, p=0.10
Parameters
37.181,2,3,4,5,6,7,8
36.781,2,3,4,5,6,8
37.541,2,3,4,5,6,7
36.641,2,3,4,5,6
33.051,2,3,4,5
F1-scoreFeature 
Combination
Table 4. Best F1-scores and their parameters. 
Figure 7. Precision?recall distribution. 
80
ranges. We used LIBSVM3 ver. 2.89 as an im-
plementation of SVM. The radial basis func-
tion (RBF) was used as the kernel function of 
SVM. The probability estimates option of 
LIBSVM was used to obtain the confidence 
value of discrimination.  
The gamma parameter of the RBF kernel 
was chosen from the range of [2-20, 20]. The C 
parameter of SVM was chosen from the range 
of [2-10, 210]. The SVM was trained and tested 
on 441 combinations of gamma and C. In test-
ing, the probability threshold parameter p be-
tween [0.05, 0.95] was also chosen, and the F1-
scores of all combination of gamma, C, and p 
were calculated with five-fold cross validation. 
The best F1-scores and their parameter values 
for each combination of features (optimal F1-
scores in this setting) are portrayed in Table 4. 
The precision?recall distribution of F1-scores 
with feature combination C is presented in 
Figure 7.  
5.2 Experiment 2 
Experiment 2 aimed to observe the perfor-
mance of our extraction method when SVM 
parameters were automatically tuned. In this 
experiment, we performed two cross valida-
tions: a cross validation to tune SVM parame-
ters and another cross validation to evaluate 
the extraction method. The experiment condi-
tions are described below:  
A. Data 
The same data as Experiment 1 were used. 
B. Feature Combination  
Feature combination C, which performed best 
in Experiment 1, was used.  
C. Evaluation  
                                                 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Two five-fold cross validations were per-
formed. The first cross validation divided the 
data to 5 sets (A, B, C, D, and E) each consist-
ing of development set and test set with the 
ratio of 4:1.  The second cross validation train 
and test all combination of SVM parameters (C, 
gamma, and p) in certain ranges and decide the 
optimal parameter combination(s) for  the de-
velopment sets of A, B, C, D, and E. The se-
cond cross validation denotes the execution of 
Experiment 1 for each development set.  For 
each optimal parameter combination of A, B, 
C, D, and E, the corresponding development 
set was trained and the trained model was test-
ed on the corresponding test set. The average 
F1-score on five test sets marked 34.90, which 
is 2.64 lower than the F1-score of Experiment 1 
with the same feature combination. 
6 Discussion 
The result of the experiment reveals the effec-
tiveness of the dependency chain feature and 
the case-frame chain feature. This section pre-
sents a description of the effects of several fea-
tures in detail. The section also mentions re-
maining problems in our extraction method.  
6.1 Effects of the Dependency Chain Fea-
ture and Case-frame Features  
A. Dependency Chain Feature  
The dependency chain features improved the 
F1-score by 3.59 (the F1-score difference be-
tween feature combination A and B). This in-
crease was obtained using 260 improved pairs 
and 127 deproved pairs. Improved pairs con-
Figure 8. Relation between the number of 
pairs and the morpheme distance. 
Figure 9. Number of dependency errors 
in the improved pairs sentences. 
25
93
23
sentence
with no error
sentence
with 1?3
errors
sentence
with 4 or
more errors
0
10
20
30
40
50
distance less 
than 40
distance larger than
or equal to 40
fre
qu
en
cy
improved 
deproved
81
tribute to the increase of a F1-score. Deproved 
pairs have the opposite effect. 
We observed that improved pairs tend to 
have longer morpheme distance compared to 
deproved pairs. Figure 8 shows the relation 
between the number of pairs and the mor-
pheme distance of improved pairs and de-
proved pairs. The ratio between the improved 
pairs and the deproved pairs is 11:1 when the 
distance is greater than 40.  In contrast, the 
ratio is 2:1 when the distance is smaller than 
40. This observation suggests that adverse?
effect relations share dependency structures to 
a certain degree.  
We also observed that in improved pairs, 
dependency errors tended to be low. Figure 9 
presents the manually counted number of de-
pendency errors in the 141 sentences in which 
the 260 improved pairs exist: 65.96 % of the 
sentences included 1?3 errors. The result sug-
gests that the dependency structure is effective 
even if it includes small errors.  
B. Case-frame Features  
The effect of the case-frame dependency chain 
feature differed with the effect of the depend-
ency chain feature. The case-frame chain fea-
ture improved the F1-score by 0.90 (the F1-
score difference between feature combination 
B and C), but the case-frame dependency chain 
feature decreased the F1-score by 0.36 (the F1-
score difference between feature combination 
C and E). One reason for the negative effect of 
the case-frame dependency feature might be 
feature sparsity, but no clear evidence of it has 
been found.  
6.2 Remaining Problems 
A. Imbalanced Data  
The adverse?effect relation pairs we used in 
the experiment were not balanced. Low values 
of optimal probability threshold parameter p 
suggest the degree of imbalance. We are con-
sidering introduction of some kind of method-
ology to reduce negative samples or to use a 
machine learning method that can accommo-
date imbalanced data well.  
B. Use of Medical Resources  
The extraction method we propose uses no 
medical resources. Girju et al (2007) indicate 
the effect of WordNet senses in the classifica-
tion of a semantic relation between nominals. 
Krallinger et al (2008) report that top scoring 
teams in the interaction pair subtask used so-
phisticated interactor protein normalization 
strategies. If medical terms in texts can be 
mapped to a medical terminology or ontology, 
it would likely improve the extraction accuracy.  
C. Fully Automated Extraction 
In the experiments, we used the manually 
annotated information to extract pairs and fea-
tures. This setting is, of course, not real if we 
consider a situation to extract adverse?effect 
relations from massive clinical records, but we 
chose it to focus on the relation extraction 
problem. We performed an event recognition 
experiment (Aramaki et al, 2009) and 
achieved F1-score of about 80. We assume that 
drug expressions and symptom expressions to 
be automatically recognized in a similar accu-
racy.  
We are planning to perform a fully automat-
ed adverse?effect relations extraction from a 
larger set of clinical texts to see the perfor-
mance of our method on a raw corpus. The 
extraction F1-score will likely to decrease, but 
we intend to observe the other aspect of the 
extraction, like the overall tendency of extract-
ed relations.  
7 Conclusion 
We presented a method to extract adverse?
effect relations from texts. One important 
characteristic of adverse?effect relations is that 
they are uncertain in most cases. We per-
formed experiments to extract adverse?effect 
relations from 2,577 clinical texts, and ob-
tained F1-score of 37.54 with optimal SVM 
parameters and F1-score of 34.90 with auto-
matically tuned SVM parameters. Results also 
show that dependency features increase the 
extraction F1-score by 3.59. We observed that 
an increased F1-score was obtained using the 
improvement of adverse?effects with long 
morpheme distance, which suggests that ad-
verse?effect relations share dependency struc-
tures to a certain degree. We also observed that 
the increase of the F1-score was obtained with 
dependency structures that include small errors, 
which suggests that the dependency structure 
is effective even if it includes small errors. 
  
82
References 
Aramaki, Eiji, Yasuhide Miura, Masatsugu Tonoike, 
Tomoko Ohkuma, Hiroshi Masuichi, and 
Kazuhiko Ohe. 2009. TEXT2TABLE: Medical 
Text Summarization System Based on Named 
Entity Recognition and Modality Identification. 
In Proceedings of the BioNLP 2009 Workshop, 
pages 185-192. 
Beamer, Brandon, Suma Bhat, Brant Chee, Andrew 
Fister, Alla Rozovskaya, and Roxana Girju. 
2007. UIUC: A Knowledge-rich Approach to 
Identifying Semantic Relations between Nomi-
nals. In Proceedings of Fourth International 
Workshop on Semantic Evaluations, pages 386-
389. 
Friedman, Carol, Philip O. Alderson, John H. M. 
Austin, James J. Cimino, and Stephen B. John-
son. 1994. A General Natural-language Text 
Processor for Clinical Radiology. Journal of the 
American Medical Informatics Association, 1(2), 
pages 161-174. 
Gildea, Daniel. 2001. Corpus Variation and Parser 
Performance. In Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1-9. 
Girju, Roxana, Preslav Nakov, Vivi Nastase,  Stan 
Szpakowicz, Peter Turney, and Deniz Yuret.  
2007. SemEval-2007 task 04: Classification of 
Semantic Relations between Nominals. In Pro-
ceedings of Fourth International Workshop on 
Semantic Evaluations, pages 13-18. 
Giuliano, Claudio, Alberto Lavelli, Daniele Pighin, 
and Lorenza Romano. 2007. FBK-IRST: Kernel 
Methods for Semantic Relation Extraction. In 
Proceedings of the 4th International Workshop 
on Semantic Evaluations, pages 141-144.  
Hendrickx , Iris, Roser Morante, Caroline Sporleder, 
and Antal van den Bosch. 2007. ILK: Machine 
learning of semantic relations with shallow fea-
tures and almost no data. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions, 187-190. 
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, and Jun?ichi Tsujii. 2009. 
Overview of  BioNLP?09 Shared Task on Event 
Extraction. In Proceedings of the BioNLP 2009 
Workshop Companion Volume for Shared Task, 
pages 1-9. 
Krallinger, Martin, Florian Leitner, Carlos  
Rodriguez-Penagos, and Alfonso Valencia. 2008.  
Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Ge-
nome Biology 2008, 9(Suppl 2):S4. 
Kurohashi, Sadao and Makoto Nagao. 1994. KN 
Parser : Japanese Dependency/Case Structure 
Analyzer. In Proceedings of The International 
Workshop on Sharable Natural Language Re-
sources, pages 22-28. Software available at 
http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/knp-e.html. 
Light, Marc, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Pro-
ceedings of HLT/NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, On-
tologies and Databases, pages 17-24. 
Miyao, Yusuke, Rune S?tre, Kenji Sagae, Takuya 
Matsuzaki, and Jun'ichi Tsujii. 2008. Task-
oriented Evaluation of Syntactic Parsers and 
Their Representations. In Proceedings of the 
46th Annual Meeting of the Association for 
Computational Linguistics: Human Language 
Technologies, pages 46-54. 
Pantel, Patrick and Marco Pennacchiotti. 2006. Es-
presso: Leveraging Generic Patterns for Auto-
matically Harvesting Semantic Relations. In 
Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 113-120. 
Riedel, Sebastian, Hong-Woo Chun, Toshihisa 
Takagi, and Jun'ichi Tsujii. 2009. A Markov 
Logic Approach to Bio-Molecular Event Extrac-
tion. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 
41-49. 
Saeger, Stijn De, Kentaro Torisawa, and Jun?ichi 
Kazama. 2008. Looking for Trouble. In Proceed-
ings of the 22nd International Conference on 
Computational Linguistics, pages 185-192. 
Vapnik, Vladimir N.. 1995. The Nature of Statisti-
cal Learning Theory. Springer-Verlag New York, 
Inc.. 
Vincze, Veronika, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008.  The Bio-
Scope corpus: biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioin-
formatics 2008, 9(Suppl 11):S9.  
 
83
