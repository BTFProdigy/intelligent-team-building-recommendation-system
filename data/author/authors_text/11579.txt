Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 81?90,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Active Learning by Labeling Features
Gregory Druck
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
gdruck@cs.umass.edu
Burr Settles
Dept. of Biostatistics &
Medical Informatics
Dept. of Computer Sciences
University of Wisconsin
Madison, WI 53706
bsettles@cs.wisc.edu
Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Methods that learn from prior informa-
tion about input features such as general-
ized expectation (GE) have been used to
train accurate models with very little ef-
fort. In this paper, we propose an ac-
tive learning approach in which the ma-
chine solicits ?labels? on features rather
than instances. In both simulated and real
user experiments on two sequence label-
ing tasks we show that our active learning
method outperforms passive learning with
features as well as traditional active learn-
ing with instances. Preliminary experi-
ments suggest that novel interfaces which
intelligently solicit labels on multiple fea-
tures facilitate more efficient annotation.
1 Introduction
The application of machine learning to new prob-
lems is slowed by the need for labeled training
data. When output variables are structured, an-
notation can be particularly difficult and time-
consuming. For example, when training a condi-
tional random field (Lafferty et al, 2001) to ex-
tract fields such as rent, contact, features, and utilities
from apartment classifieds, labeling 22 instances
(2,540 tokens) provides only 66.1% accuracy.
1
Recent work has used unlabeled data and lim-
ited prior information about input features to boot-
strap accurate structured output models. For ex-
ample, both Haghighi and Klein (2006) and Mann
and McCallum (2008) have demonstrated results
better than 66.1% on the apartments task de-
scribed above using only a list of 33 highly dis-
criminative features and the labels they indicate.
However, these methods have only been applied
in scenarios in which the user supplies such prior
knowledge before learning begins.
1
Averaged over 10 randomly selected sets of 22 instances.
In traditional active learning (Settles, 2009), the
machine queries the user for only the labels of in-
stances that would be most helpful to the machine.
This paper proposes an active learning approach in
which the user provides ?labels? for input features,
rather than instances. A labeled input feature de-
notes that a particular input feature, for example
the word call, is highly indicative of a particular
label, such as contact. Table 1 provides an excerpt
of a feature active learning session.
In this paper, we advocate using generalized
expectation (GE) criteria (Mann and McCallum,
2008) for learning with labeled features. We pro-
vide an alternate treatment of the GE objective
function used by Mann and McCallum (2008) and
a novel speedup to the gradient computation. We
then provide a pool-based feature active learning
algorithm that includes an option to skip queries,
for cases in which a feature has no clear label.
We propose and evaluate feature query selection
algorithms that aim to reduce model uncertainty,
and compare to several baselines. We evaluate
our method using both real and simulated user ex-
periments on two sequence labeling tasks. Com-
pared to previous approaches (Raghavan and Al-
lan, 2007), our method can be used for both classi-
fication and structured tasks, and the feature query
selection methods we propose perform better.
We use experiments with simulated labelers on
real data to extensively compare feature query se-
lection algorithms and evaluate on multiple ran-
dom splits. To make these simulations more re-
alistic, the effort required to perform different la-
beling actions is estimated from additional exper-
iments with real users. The results show that ac-
tive learning with features outperforms both pas-
sive learning with features and traditional active
learning with instances.
In the user experiments, each annotator actively
labels instances, actively labels features one at a
time, and actively labels batches of features orga-
81
accuracy 46.5? 60.5
feature label
PHONE* contact
call contact
deposit rent
month rent
pets restrict.
lease rent
appointment contact
parking features
EMAIL* contact
information contact
accuracy 60.5? 67.1
feature label
water utilities
close neighbor.
garbage utilities
included utilities
features
shopping neighbor.
bart neighbor.
downtown neighbor.
TIME* contact
bath size
Table 1: Two iterations of feature active learning.
Each table shows the features labeled, and the re-
sulting change in accuracy. Note that the word in-
cluded was labeled as both utilities and features, and
that ? denotes a regular expression feature.
nized using a ?grid? interface. The results support
the findings of the simulated experiments and pro-
vide evidence that the ?grid? interface can facili-
tate more efficient annotation.
2 Conditional Random Fields
In this section we describe the underlying proba-
bilistic model for all methods in this paper. We
focus on sequence labeling, though the described
methods could be applied to other structured out-
put or classification tasks. We model the proba-
bility of the label sequence y ? Y
n
conditioned
on the input sequence x ? X
n
, p(y|x; ?) using
first-order linear-chain conditional random fields
(CRFs) (Lafferty et al, 2001). This probability is
p(y|x; ?) =
1
Z
x
exp
(
?
i
?
j
?
j
f
j
(y
i
, y
i+1
,x, i)
)
,
where Z
x
is the partition function and feature
functions f
j
consider the entire input sequence
and at most two consecutive output variables.
The most probable output sequence and transition
marginal distributions can be computed using vari-
ants of Viterbi and forward-backward.
Provided a training data distribution p?, we es-
timate CRF parameters by maximizing the condi-
tional log likelihood of the training data.
L(?) = E
p?(x,y)
[log p(y|x; ?)]
We use numerical optimization to maximize L(?),
which requires the gradient of L(?) with respect
to the parameters. It can be shown that the par-
tial derivative with respect to parameter j is equal
to the difference between the empirical expecta-
tion of F
j
and the model expectation of F
j
, where
F
j
(y,x) =
?
i
f
j
(y
i
, y
i+1
,x, i).
?
??
j
L(?) = E
p?(x,y)
[F
j
(y,x)]
? E
p?(x)
[E
p(y|x;?)
[F
j
(y,x)]].
We also include a zero-mean variance ?
2
= 10
Gaussian prior on parameters in all experiments.
2
2.1 Learning with missing labels
The training set may contain partially labeled se-
quences. Let z denote missing labels. We esti-
mate parameters with this data by maximizing the
marginal log-likelihood of the observed labels.
L
MML
(?) = E
p?(x,y)
[log
?
z
p(y, z|x; ?)]
We refer to this training method as maximum
marginal likelihood (MML); it has also been ex-
plored by Quattoni et al (2007).
The gradient of L
MML
(?) can also be written
as the difference of two expectations. The first is
an expectation over the empirical distribution of x
and y, and the model distribution of z. The second
is a double expectation over the empirical distribu-
tion of x and the model distribution of y and z.
?
??
j
L
MML
(?) = E
p?(x,y)
[E
p(z|y,x;?)
[F
j
(y, z,x)]]
? E
p?(x)
[E
p(y,z|x;?)
[F
j
(y, z,x)]].
We train models using L
MML
(?) with expected
gradient (Salakhutdinov et al, 2003).
To additionally leverage unlabeled data, we
compare with entropy regularization (ER). ER
adds a term to the objective function that en-
courages confident predictions on unlabeled data.
Training of linear-chain CRFs with ER is de-
scribed by Jiao et al (2006).
3 Generalized Expectation Criteria
In this section, we give a brief overview of gen-
eralized expectation criteria (GE) (Mann and Mc-
Callum, 2008; Druck et al, 2008) and explain how
we can use GE to learn CRF parameters with esti-
mates of feature expectations and unlabeled data.
GE criteria are terms in a parameter estimation
objective function that express preferences on the
2
10 is a default value that works well in many settings.
82
value of a model expectation of some function.
Given a score function S, an empirical distribution
p?(x), a model distribution p(y|x; ?), and a con-
straint function G
k
(x,y), the value of a GE crite-
rion is G(?) = S(E
p?(x)
[E
p(y|x;?)
[G
k
(x,y)]]).
GE provides a flexible framework for parameter
estimation because each of these elements can take
an arbitrary form. The most important difference
between GE and other parameter estimation meth-
ods is that it does not require a one-to-one cor-
respondence between constraint functions G
k
and
model feature functions. We leverage this flexi-
bility to estimate parameters of feature-rich CRFs
with a very small set of expectation constraints.
Constraint functions G
k
can be normalized so
that the sum of the expectations of a set of func-
tions is 1. In this case, S may measure the di-
vergence between the expectation of the constraint
function and a target expectation
?
G
k
.
G(?) =
?
G
k
log(E[G
k
(x,y)]), (1)
where E[G
k
(x,y)] = E
p?(x)
[E
p(y|x;?)
[G
k
(x,y)]].
It can be shown that the partial derivative of
G(?) with respect to parameter j is proportional to
the predicted covariance between the model fea-
ture function F
j
and the constraint function G
k
.
3
?
??
j
G(?) =
?
G
k
E[G
k
(x,y)]
? (2)
(
E
p?(x)
[
E
p(y|x;?)
[F
j
(x,y)G
k
(x,y)]
? E
p(y|x;?)
[F
j
(x,y)]E
p(y|x;?)
[G
k
(x,y)]
]
)
The partial derivative shows that GE learns pa-
rameter values for model feature functions based
on their predicted covariance with the constraint
functions. GE can thus be interpreted as a boot-
strapping method that uses the limited training sig-
nal to learn about parameters for related model
feature functions.
3.1 Learning with feature-label distributions
Mann and McCallum (2008) apply GE to a linear-
chain, first-order CRF. In this section we provide
an alternate treatment that arrives at the same ob-
jective function from the general form described
in the previous section.
Often, feature functions in a first-order linear-
chain CRF f are binary, and are the conjunction
3
If we use squared error for S, the partial derivative is the
covariance multiplied by 2(
?
G
k
? E[G
k
(x,y)]).
of an observational test q(x, i) and a label pair test
1
{y
i
=y
?
,y
i+1
=y
??
}
.
4
f(y
i
, y
i+1
,x, i) = 1
{y
i
=y
?
,y
i+1
=y
??
}
q(x, i)
The constraint functions G
k
we use here decom-
pose and operate similarly, except that they only
include a test for a single label. Single label con-
straints are easier for users to estimate and make
GE training more efficient. Label transition struc-
ture can be learned automatically from single la-
bel constraints through the covariance-based pa-
rameter update of Equation 2. For convenience,
we can write G
yk
to denote the constraint func-
tion that combines observation test k with a test
for label y. We also add a normalization constant
C
k
= E
p?(x)
[
?
i
q
k
(x, i)],
G
yk
(x,y) =
?
i
1
C
k
1
{y
i
=y}
q
k
(x, i)
Under this construction the expectation of G
yk
is
the predicted conditional probability that the label
at some arbitrary position i is y when the observa-
tional test at i succeeds, p?(y
i
=y|q
k
(x, i)=1; ?).
If we have a set of constraint functions {G
yk
:
y ? Y}, and we use the score function in Equa-
tion 1, then the GE objective function specifies the
minimization of the KL divergence between the
model and target distributions over labels condi-
tioned on the success of the observational test. In
general the objective function will consist of many
such KL divergence penalties.
Computing the first term of the covariance in
Equation 2 requires a marginal distribution over
three labels, two of which will be consecutive, but
the other of which could appear anywhere in the
sequence. We can compute this marginal using
the algorithm of Mann and McCallum (2008). As
previously described, this algorithm is O(n|Y|
3
)
for a sequence of length n. However, we make
the following novel observation: we do not need
to compute the extra lattices for feature label pairs
with
?
G
yk
= 0, since this makes Equation 2 equal
to zero. In Mann and McCallum (2008), probabil-
ities were smoothed so that ?
y
?
G
yk
> 0. If we
assume that only a small number of labels m have
non-zero probability, then the time complexity of
the gradient computation is O(nm|Y|
2
). In this
paper typically 1 ?m? 4, while |Y| is 11 or 13.
4
We this notation for an indicator function that returns 1
if the condition in braces is satisfied, and 0 otherwise.
83
In experiments in this paper, using this optimiza-
tion does not significantly affect final accuracy.
We use numerical optimization to estimate
model parameters. In general GE objective func-
tions are not convex. Consequently, we initial-
ize 0th-order CRF parameters using a sliding win-
dow logistic regression model trained with GE.
We also include a Gaussian prior on parameters
with ?
2
= 10 in the objective function.
3.2 Learning with labeled features
The training procedure described above requires
a set of observational tests or input features with
target distributions over labels. Estimating a dis-
tribution could be a difficult task for an annotator.
Consequently, we abstract away from specifying
a distribution by allowing the user to assign labels
to features (c.f. Haghighi and Klein (2006) , Druck
et al (2008)). For example, we say that the word
feature call has label contact. A label for a feature
simply indicates that the feature is a good indicator
of the label. Note that features can have multiple
labels, as does included in the active learning ses-
sion shown in Table 1. We convert an input feature
with a set of labels L into a distribution by assign-
ing probability 1/|L| for each l ? L and probabil-
ity 0 for each l /? L. By assigning 0 probability to
labels l /? L, we can use the speed-up described in
the previous section.
3.3 Related Work
Other proposed learning methods use labeled fea-
tures to label unlabeled data. The resulting
partially-labeled corpus can be used to train a CRF
by maximizing MML. Similarly, prototype-driven
learning (PDL) (Haghighi and Klein, 2006) opti-
mizes the joint marginal likelihood of data labeled
with prototype input features for each label. Ad-
ditional features that indicate similarity to the pro-
totypes help the model to generalize. In a previ-
ous comparison between GE and PDL (Mann and
McCallum, 2008), GE outperformed PDL without
the extra similarity features, whose construction
may be problem-specific. GE also performed bet-
ter when supplied accurate label distributions.
Additionally, both MML and PDL do not natu-
rally generalize to learning with features that have
multiple labels or distributions over labels, as in
these scenarios labeling the unlabeled data is not
straightforward. In this paper, we attempt to ad-
dress this problem using a simple heuristic: when
there are multiple choices for a token?s label, sam-
ple a label. In Section 5 we use this heuristic with
MML, but in general obtain poor results.
Raghavan and Allan (2007) also propose sev-
eral methods for learning with labeled features,
but in a previous comparison GE gave better re-
sults (Druck et al, 2008). Additionally, the gen-
eralization of these methods to structured output
spaces is not straightforward. Chang et al (2007)
present an algorithm for learning with constraints,
but this method requires users to set weights by
hand. We plan to explore the use of the recently
developed related methods of Bellare et al (2009),
Grac?a et al (2008), and Liang et al (2009) in fu-
ture work. Druck et al (2008) provide a survey
of other related methods for learning with labeled
input features.
4 Active Learning by Labeling Features
Feature active learning, presented in Algorithm 1,
is a pool-based active learning algorithm (Lewis
and Gale, 1994) (with a pool of features rather
than instances). The novel components of the
algorithm are an option to skip a query and the
notion that skipping and labeling have different
costs. The option to skip is important when us-
ing feature queries because a user may not know
how to label some features. In each iteration the
model is retrained using the train procedure, which
takes as input a set of labeled features C and un-
labeled data distribution p?. For the reasons de-
scribed in Section 3.3, we advocate using GE for
the train procedure. Then, while the iteration cost
c is less than the maximum cost c
max
, the feature
query q that maximizes the query selection met-
ric ? is selected. The accept function determines
whether the labeler will label q. If q is labeled, it
is added to the set of labeled features C, and the
label cost c
label
is added to c. Otherwise, the skip
cost c
skip
is added to c. This process continues for
N iterations.
4.1 Feature query selection methods
In this section we propose feature query selection
methods ?. Queries with a higher scores are con-
sidered better candidates. Note again that by fea-
tures we mean observational tests q
k
(x, i). It is
also important to note these are not feature selec-
tion methods since we are determining the features
for which supervisory feedback will be most help-
ful to the model, rather than determining which
features will be part of the model.
84
Algorithm 1 Feature Active Learning
Input: empirical distribution p?, initial feature constraints
C, label cost c
label
, skip cost c
skip
, max cost per iteration
c
max
, max iterations N
Output: model parameters ?
for i = 1 to N do
? = train(p?, C)
c = 0
while c < c
max
do
q = argmax
q
k
?(q
k
)
if accept(q) then
C = C ? label(q)
c = c+ c
label
else
c = c+ c
skip
end if
end while
end for
? = train(p?, C)
We propose to select queries that provide the
largest reduction in model uncertainty. We notate
possible responses to a query q
k
as g?. The Ex-
pected Information Gain (EIG) of a query is the
expectation of the reduction in model uncertainty
over all possible responses. Mathematically, IG is
?
EIG
(q
k
) = E
p(g?|q
k
;?)
[E
p?(x)
[H(p(y|x; ?)?
H(p(y|x; ?
g?
)]],
where ?
g?
are the new model parameters if the re-
sponse to q
k
is g?. Unfortunately, this method is
computationally intractable. Re-estimating ?
g?
will
typically involve retraining the model, and do-
ing this for each possible query-response pair is
prohibitively expensive for structured output mod-
els. Computing the expectation over possible re-
sponses is also difficult, as in this paper users may
provide a set of labels for a query, and more gen-
erally g? could be a distribution over labels.
Instead, we propose a tractable strategy for re-
ducing model uncertainty, motivated by traditional
uncertainty sampling (Lewis and Gale, 1994). We
assume that when a user responds to a query, the
reduction in uncertainty will be equal to the To-
tal Uncertainty (TU), the sum of the marginal en-
tropies at the positions where the feature occurs.
?
TU
(q
k
) =
?
i
?
j
q
k
(x
i
, j)H(p(y
j
|x
i
; ?))
Total uncertainty, however, is highly biased to-
wards selecting frequent features. A mean un-
certainty variant, normalized by the feature?s
count, would tend to choose very infrequent fea-
tures. Consequently we propose a tradeoff be-
tween the two extremes, called weighted uncer-
tainty (WU), that scales the mean uncertainty by
the log count of the feature in the corpus.
?
WU
(q
k
) = log(C
k
)
?
TU
(q
k
)
C
k
.
Finally, we also suggest an uncertainty-based met-
ric called diverse uncertainty (DU) that encour-
ages diversity among queries by multiplying TU
by the mean dissimilarity between the feature and
previously labeled features. For sequence labeling
tasks, we can measure the relatedness of features
using distributional similarity.
5
?
DU
(q
k
) = ?
TU
(q
k
)
1
|C|
?
j?C
1?sim(q
k
, q
j
)
We contrast the notion of uncertainty described
above with another type of uncertainty: the en-
tropy of the predicted label distribution for the fea-
ture, or expectation uncertainty (EU). As above
we also multiply by the log feature count.
?
EU
(q
k
) = log(C
k
)H(p?(y
i
= y|q
k
(x, i)=1; ?))
EU is flawed because it will have a large value for
non-discriminative features.
The methods described above require the model
to be retrained between iterations. To verify that
this is necessary, we compare against query selec-
tion methods that only consider the previously la-
beled features. First, we consider a feature query
selection method called coverage (cov) that aims
to select features that are dissimilar from existing
labeled features, increasing the labeled features?
?coverage? of the feature space. In order to com-
pensate for choosing very infrequent features, we
multiply by the log count of the feature.
?
cov
(q
k
) = log(C
k
)
1
|C|
?
j?C
1? sim(q
k
, q
j
)
Motivated by the feature query selection method
of Tandem Learning (Raghavan and Allan, 2007)
(see Section 4.2 for further discussion), we con-
sider a feature selection metric similarity (sim)
that is the maximum similarity to a labeled fea-
ture, weighted by the log count of the feature.
?
sim
(q
k
) = log(C
k
)max
j?C
sim(q
k
, q
j
)
5
sim(q
k
, q
j
) returns the cosine similarity between context
vectors of words occurring in a window of ?3.
85
Features similar to those already labeled are likely
to be discriminative, and therefore likely to be la-
beled (rather than skipped). However, insufficient
diversity may also result in an inaccurate model,
suggesting that coverage should select more use-
ful queries than similarity.
Finally, we compare with several passive base-
lines. Random (rand) assigns scores to features
randomly. Frequency (freq) scores input features
using their frequency in the training data.
?
freq
(q
k
) =
?
i
?
j
q
k
(x
i
, j)
Top LDA (LDA) selects the top words from 50
topics learned from the unlabeled data using la-
tent Dirichlet alocation (LDA) (Blei et al, 2003).
More specifically, the words w generated by each
topic t are ranked using the conditional probability
p(w|t). The word feature is assigned its maximum
rank across all topics.
?
LDA
(q
k
) = max
t
rank
LDA
(q
k
, t)
This method will select useful features if the top-
ics discovered are relevant to the task. A similar
heuristic was used by Druck et al (2008).
4.2 Related Work
Tandem Learning (Raghavan and Allan, 2007) is
an algorithm that combines feature and instance
active learning for classification. The algorithm it-
eratively queries the user first for instance labels,
then for feature labels. Feature queries are selected
according to their co-occurrence with important
model features and previously labeled features. As
noted in Section 3.3, GE is preferable to the meth-
ods Tandem Learning uses to learn with labeled
features. We address the mixing of feature and in-
stance queries in Section 4.3.
In order to better understand differences in fea-
ture query selection methodology, we proposed a
feature query selection method motivated
6
by the
method used in Tandem Learning in Section 4.1.
However, this method performs poorly in the ex-
periments in Section 5.
Liang et al (2009) simultaneously developed
a method for learning with and actively selecting
6
The query selection method of Raghavan and Allan
(2007) requires a stack that is modified between queries
within each iteration. Here query scores are only updated
after each iteration of labeling.
measurements, or target expectations with associ-
ated noise. The measurement selection method
proposed by Liang et al (2009) is based on
Bayesian experimental design and is similar to
the expected information gain method described
above. Consequently this method is likely to be
intractable for real applications. Note that Liang
et al (2009) only use this method in synthetic ex-
periments, and instead use a method similar to to-
tal uncertainty for experiments in part-of-speech
tagging. Unlike the experiments presented in this
paper, Liang et al (2009) conduct only simulated
active learning experiments and do not consider
skipping queries.
Sindhwani (Sindhwani et al, 2009) simultane-
ously developed an active learning method that
queries for both instance and feature labels that
are then used in a graph-based learning algorithm.
They find that querying certain features outper-
forms querying uncertain features, but this is likely
because their query selection method is similar
to the expectation uncertainty method described
above, and consequently non-discriminative fea-
tures may be queried often (see also the discus-
sion in Section 4.1). It is also not clear how this
graph-based training method would generalize to
structured output spaces.
4.3 Expectation Constraint Active Learning
Throughout this paper, we have focussed on label-
ing input features. However, the proposed meth-
ods generalize to queries for expectation estimates
of arbitrary functions, for example queries for the
label distributions for input features, labels for in-
stances (using a function that is non-zero only for
a particular instance), partial labels for instances,
and class priors. The uncertainty-based query se-
lection methods described in Section 4.1 apply
naturally to these new query types. Importantly
this framework would allow principled mixing of
different query types, instead of alternating be-
tween them as in Tandem Learning (Raghavan and
Allan, 2007). When mixing queries, it will be
important to use different costs for different an-
notation types (Vijayanarasimhan and Grauman,
2008), and estimate the probability of obtaining a
useful response to a query. We plan to pursue these
directions in future work. This idea was also pro-
posed by Liang et al (2009), but no experiments
with mixed active learning were presented.
86
5 Simulated User Experiments
In this section we experiment with an automated
oracle labeler. When presented an instance query,
the oracle simply provides the true labels. When
presented a feature query, the oracle first decides
whether to skip the query. We have found that
users are more likely to label features that are rel-
evant for only a few labels. Therefore, the oracle
labels a feature if the entropy of its per occurrence
label expectation, H(p?(y
i
= y|q
k
(x, i) = 1; ?)) ?
0.7. The oracle then labels the feature using a
heuristic: label the feature with the label whose
expectation is highest, as well as any label whose
expectation is at least half as large.
We estimate the effort of different labeling ac-
tions with preliminary experiments in which we
observe users labeling data for ten minutes. Users
took an average of 4 seconds to label a feature, 2
seconds to skip a feature, and 0.7 seconds to la-
bel a token. We setup experiments such that each
iteration simulates one minute of labeling by set-
ting c
max
= 60, c
skip
= 2 and c
label
= 4. For
instance active learning, we use Algorithm 1 but
without the skip option, and set c
label
= 0.7. We
use N = 10 iterations, so the entire experiment
simulates 10 minutes of annotation time. For ef-
ficiency, we consider the 500 most frequent unla-
beled features in each iteration. To start, ten ran-
domly selected seed labeled features are provided.
We use random (rand) selection, uncertainty
sampling (US) (using sequence entropy, normal-
ized by sequence length) and information den-
sity (ID) (Settles and Craven, 2008) to select in-
stance queries. We use Entropy Regularization
(ER) (Jiao et al, 2006) to leverage unlabeled in-
stances.
7
We weight the ER term by choosing the
best
8
weight in {10
?3
, 10
?2
, 10
?1
, 1, 10} multi-
plied by
#labeled
#unlabeled
for each data set and query se-
lection method. Seed instances are provided such
that the simulated labeling time is equivalent to la-
beling 10 features.
We evaluate on two sequence labeling tasks.
The apartments task involves segmenting 300
apartment classified ads into 11 fields including
features, rent, neighborhood, and contact. We use
the same feature processing as Haghighi and Klein
(2006), with the addition of context features in a
window of ?3. The cora references task is to ex-
tract 13 BibTeX fields such as author and booktitle
7
Results using self-training instead of ER are similar.
8
As measured by test accuracy, giving ER an advantage.
method apartments cora
mean final mean final
ER rand 48.1 53.6 75.9 81.1
ER US 51.7 57.9 76.0 83.2
ER ID 51.4 56.9 75.9 83.1
MML rand 47.7 51.2 58.6 64.6
MML WU 57.6 60.8 61.0 66.2
GE rand 59.0 64.8
?
77.6 83.7
GE freq 66.5
?
71.6
?
68.6 79.8
GE LDA 65.7
?
71.4
?
74.9 85.0
GE cov 68.2
??
72.6
?
73.5 83.3
GE sim 57.8 65.9
?
67.1 79.2
GE EU 66.5
?
71.6
?
68.6 79.8
GE TU 70.1
??
73.6
??
76.9 88.2
??
GE WU 71.6
??
74.6
??
80.3
??
88.1
??
GE DU 70.5
??
74.4
??
78.4
?
87.5
??
Table 2: Mean and final token accuracy results.
A
?
or
?
denotes that a GE method significantly
outperforms all non-GE or passive GE methods,
respectively. Bold entries significantly outperform
all others. Methods in italics are passive.
from 500 research paper references. We use a stan-
dard set of word, regular expressions, and lexicon
features, as well as context features in a window
of ?3. All results are averaged over ten random
80:20 splits of the data.
5.1 Results
Table 2 presents mean (across all iterations) and
final token accuracy results. On the apartments
task, GE methods greatly outperform MML
9
and
ER methods. Each uncertainty-based GE method
also outperforms all passive GE methods. On the
cora task, only GE with weighted uncertainty sig-
nificantly outperforms ER and passive GE meth-
ods in terms of mean accuracy, but all uncertainty-
based GE methods provide higher final accuracy.
This suggests that on the cora task, active GE
methods are performing better in later iterations.
Figure 1, which compares the learning curves of
the best performing methods of each type, shows
this phenomenon. Further analysis reveals that the
uncertainty-based methods are choosing frequent
features that are more likely to be skipped than
those selected randomly in early iterations.
We next compare with the results of related
methods published elsewhere. We cannot make
claims about statistical significance, but the results
9
Only the best MML results are shown.
87
illustrate the competitiveness of our method. The
74.6% final accuracy on apartments is higher than
any result obtained by Haghighi and Klein (2006)
(the highest is 74.1%), higher than the supervised
HMM results reported by Grenager et al (2005)
(74.4%), and matches the results of Mann and Mc-
Callum (2008) with GE with more accurate sam-
pled label distributions and 10 labeled examples.
Chang et al (2007) only obtain better results than
88.2% on cora when using 300 labeled examples
(two hours of estimated annotation time), 5000 ad-
ditional unlabeled examples, and extra test time in-
ference constraints. Note that obtaining these re-
sults required only 10 simulated minutes of anno-
tation time, and that GE methods are provided no
information about the label transition matrix.
6 User Experiments
Another advantage of feature queries is that fea-
ture names are concise enough to be browsed,
rather than considered individually. This allows
the design of improved interfaces that can further
increase the speed of feature active learning. We
built a prototype interface that allows the user to
quickly browse many candidate features. The fea-
tures are split into groups of five features each.
Each group contains features that are related, as
measured by distributional similarity. The features
within each group are sorted according to the ac-
tive learning metric. This interface, displayed in
Figure 3, may be useful because features in the
same group are likely to have the same label.
We conduct three types of experiments. First, a
user labels instances selected by information den-
sity, and models are trained using ER. The in-
stance labeling interface allows the user to label
tokens quickly by extending the current selection
one token at a time and only requiring a single
keystroke to label an entire segment. Second,
the user labels features presented one-at-a-time by
weighted uncertainty, and models are trained us-
ing GE. To aid the user in understanding the func-
tion of the feature quickly, we provide several ex-
amples of the feature occurring in context and the
model?s current predicted label distribution for the
feature. Finally, the user labels features organized
using the grid interface described in the previous
paragraph. Weighted uncertainty is used to sort
feature queries within each group, and GE is used
to train models. Each iteration of labeling lasts
two minutes, and there are five iterations. Retrain-
ing with ER between iterations takes an average
of 5 minutes on cora and 3 minutes on apart-
ments. With GE, the retraining times are on av-
erage 6 minutes on cora and 4 minutes on apart-
ments. Consequently, even when viewed with to-
tal time, rather than annotation time, feature active
learning is beneficial. While waiting for models to
retrain, users can perform other tasks.
Figure 2 displays the results. User 1 labeled
apartments data, while Users 2 and 3 labeled cora
data. User 1 was able to obtain much better results
with feature labeling than with instance labeling,
but performed slightly worse with the grid inter-
face than with the serial interface. User 1 com-
mented that they found the label definitions for
apartments to be imprecise, so the other experi-
ments were conducted on the cora data. User 2
obtained better results with feature labeling than
instance labeling, and obtained higher mean ac-
curacy with the grid interface. User 3 was much
better at labeling features than instances, and per-
formed especially well using the grid interface.
7 Conclusion
We proposed an active learning approach in which
features, rather than instances, are labeled. We
presented an algorithm for active learning with
features and several feature query selection meth-
ods that approximate the expected reduction in
model uncertainty of a feature query. In simu-
lated experiments, active learning with features
outperformed passive learning with features, and
uncertainty-based feature query selection outper-
formed other baseline methods. In both simulated
and real user experiments, active learning with
features outperformed passive and active learning
with instances. Finally, we proposed a new label-
ing interface that leverages the conciseness of fea-
ture queries. User experiments suggested that this
grid interface can improve labeling efficiency.
Acknowledgments
We thank Kedar Bellare for helpful discussions and Gau-
rav Chandalia for providing code. This work was supported
in part by the Center for Intelligent Information Retrieval
and the Central Intelligence Agency, the National Security
Agency and National Science Foundation under NSF grant
#IIS-0326249. The second author was supported by a grant
from National Human Genome Research Institute. Any opin-
ions, findings and conclusions or recommendations are the
authors? and do not necessarily reflect those of the sponsor.
88
2 4 6 8 1035
4045
5055
6065
7075
80
simulated annotation time (minutes)
token
 accu
racy
apartments
 
 
ER + uncertaintyMML + weighted uncertaintyGE + frequencyGE + weighted uncertainty 2 4 6 8 1045
5055
6065
7075
8085
90
simulated annotation time (minutes)
token
 accu
racy
cora
 
 
ER + uncertaintyMML + weighted uncertaintyGE + randomGE + weighted uncertainty
Figure 1: Token accuracy vs. time for best performing ER, MML, passive GE, and active GE methods.
2 4 6 8 10510
1520
2530
3540
4550
5560
65
annotation time (minutes)
token 
accura
cy
user 1 ? apartments
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid) 2 4 6 8 1030
3540
4550
5560
6570
annotation time (minutes)
token 
accura
cy
user 2 ? cora
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid) 2 4 6 8 103540
4550
5560
6570
7580
85
annotation time (minutes)
token 
accura
cy
user 3 ? cora
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid)
Figure 2: User experiments with instance labeling and feature labeling with the serial and grid interfaces.
Figure 3: Grid feature labeling interface. Boxes on the left contain groups of features that appear in
similar contexts. Features in the same group often receive the same label. On the right, the model?s
current expectation and occurrences of the selected feature in context are displayed.
89
References
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning with
expectation constraints. In UAI.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation.
Journal of Machine Learning Research, 3:2003.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL, pages 280?287.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In SIGIR.
Joao Grac?a, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20. MIT Press.
Trond Grenager, Dan Klein, and Christopher D. Man-
ning. 2005. Unsupervised learning of field segmen-
tation models for information extraction. In ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In HTL-NAACL.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In ACL, pages
209?216.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
David D. Lewis and William A. Gale. 1994. A sequen-
tial algorithm for training text classifiers. In SIGIR,
pages 3?12, New York, NY, USA. Springer-Verlag
New York, Inc.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In ICML.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In ACL.
A. Quattoni, S. Wang, L.-P Morency, M. Collins, and
T. Darrell. 2007. Hidden conditional random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 29:1848?1852, October.
Hema Raghavan and James Allan. 2007. An interac-
tive algorithm for asking and incorporating feature
feedback into support vector machines. In SIGIR,
pages 79?86.
Ruslan Salakhutdinov, Sam Roweis, and Zoubin
Ghahramani. 2003. Optimization with em and
expectation-conjugate-gradient. In ICML, pages
672?679.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP.
Burr Settles. 2009. Active learning literature survey.
Technical Report 1648, University of Wisconsin -
Madison.
Vikas Sindhwani, Prem Melville, and Richard D.
Lawrence. 2009. Uncertainty sampling and trans-
ductive experimental design for active dual supervi-
sion. In ICML.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2008. Multi-level active prediction of useful image
annotations for recognition. In NIPS.
90
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 360?368,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-supervised Learning of Dependency Parsers
using Generalized Expectation Criteria
Gregory Druck
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
gdruck@cs.umass.edu
Gideon Mann
Google, Inc.
76 9th Ave.
New York, NY 10011
gideon.mann@gmail.com
Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
In this paper, we propose a novel method
for semi-supervised learning of non-
projective log-linear dependency parsers
using directly expressed linguistic prior
knowledge (e.g. a noun?s parent is often a
verb). Model parameters are estimated us-
ing a generalized expectation (GE) objec-
tive function that penalizes the mismatch
between model predictions and linguistic
expectation constraints. In a comparison
with two prominent ?unsupervised? learn-
ing methods that require indirect biasing
toward the correct syntactic structure, we
show that GE can attain better accuracy
with as few as 20 intuitive constraints. We
also present positive experimental results
on longer sentences in multiple languages.
1 Introduction
Early approaches to parsing assumed a grammar
provided by human experts (Quirk et al, 1985).
Later approaches avoided grammar writing by
learning the grammar from sentences explicitly
annotated with their syntactic structure (Black et
al., 1992). While such supervised approaches have
yielded accurate parsers (Charniak, 2001), the
syntactic annotation of corpora such as the Penn
Treebank is extremely costly, and consequently
there are few treebanks of comparable size.
As a result, there has been recent interest in
unsupervised parsing. However, in order to at-
tain reasonable accuracy, these methods have to
be carefully biased towards the desired syntac-
tic structure. This weak supervision has been
encoded using priors and initializations (Klein
and Manning, 2004; Smith, 2006), specialized
models (Klein and Manning, 2004; Seginer,
2007; Bod, 2006), and implicit negative evi-
dence (Smith, 2006). These indirect methods for
leveraging prior knowledge can be cumbersome
and unintuitive for a non-machine-learning expert.
This paper proposes a method for directly guid-
ing the learning of dependency parsers with nat-
urally encoded linguistic insights. Generalized
expectation (GE) (Mann and McCallum, 2008;
Druck et al, 2008) is a recently proposed frame-
work for incorporating prior knowledge into the
learning of conditional random fields (CRFs) (Laf-
ferty et al, 2001). GE criteria express a preference
on the value of a model expectation. For example,
we know that ?in English, when a determiner is di-
rectly to the left of a noun, the noun is usually the
parent of the determiner?. With GE we may add
a term to the objective function that encourages a
feature-rich CRF to match this expectation on un-
labeled data, and in the process learn about related
features. In this paper we use a non-projective de-
pendency tree CRF (Smith and Smith, 2007).
While a complete exploration of linguistic prior
knowledge for dependency parsing is beyond the
scope of this paper, we provide several promis-
ing demonstrations of the proposed method. On
the English WSJ10 data set, GE training outper-
forms two prominent unsupervised methods using
only 20 constraints either elicited from a human
or provided by an ?oracle? simulating a human.
We also present experiments on longer sentences
in Dutch, Spanish, and Turkish in which we obtain
accuracy comparable to supervised learning with
tens to hundreds of complete parsed sentences.
2 Related Work
This work is closely related to the prototype-
driven grammar induction method of Haghighi
and Klein (2006), which uses prototype phrases
to guide the EM algorithm in learning a PCFG.
Direct comparison with this method is not possi-
ble because we are interested in dependency syn-
tax rather than phrase structure syntax. However,
the approach we advocate has several significant
360
advantages. GE is more general than prototype-
driven learning because GE constraints can be un-
certain. Additionally prototype-driven grammar
induction needs to be used in conjunction with
other unsupervised methods (distributional simi-
larity and CCM (Klein and Manning, 2004)) to
attain reasonable accuracy, and is only evaluated
on length 10 or less sentences with no lexical in-
formation. In contrast, GE uses only the provided
constraints and unparsed sentences, and is used to
train a feature-rich discriminative model.
Conventional semi-supervised learning requires
parsed sentences. Kate and Mooney (2007) and
McClosky et al (2006) both use modified forms
of self-training to bootstrap parsers from limited
labeled data. Wang et al (2008) combine a struc-
tured loss on parsed sentences with a least squares
loss on unlabeled sentences. Koo et al (2008) use
a large unlabeled corpus to estimate cluster fea-
tures which help the parser generalize with fewer
examples. Smith and Eisner (2007) apply entropy
regularization to dependency parsing. The above
methods can be applied to small seed corpora, but
McDonald1 has criticized such methods as work-
ing from an unrealistic premise, as a significant
amount of the effort required to build a treebank
comes in the first 100 sentences (both because of
the time it takes to create an appropriate rubric and
to train annotators).
There are also a number of methods for unsu-
pervised learning of dependency parsers. Klein
and Manning (2004) use a carefully initialized and
structured generative model (DMV) in conjunc-
tion with the EM algorithm to get the first positive
results on unsupervised dependency parsing. As
empirical evidence of the sensitivity of DMV to
initialization, Smith (2006) (pg. 37) uses three dif-
ferent initializations, and only one, the method of
Klein and Manning (2004), gives accuracy higher
than 31% on the WSJ10 corpus (see Section 5).
This initialization encodes the prior knowledge
that long distance attachments are unlikely.
Smith and Eisner (2005) develop contrastive
estimation (CE), in which the model is encour-
aged to move probability mass away from im-
plicit negative examples defined using a care-
fully chosen neighborhood function. For instance,
Smith (2006) (pg. 82) uses eight different neigh-
borhood functions to estimate parameters for the
DMV model. The best performing neighborhood
1R. McDonald, personal communication, 2007
function DEL1ORTRANS1 provides accuracy of
57.6% on WSJ10 (see Section 5). Another neigh-
borhood, DEL1ORTRANS2, provides accuracy of
51.2%. The remaining six neighborhood func-
tions provide accuracy below 50%. This demon-
strates that constructing an appropriate neighbor-
hood function can be delicate and challenging.
Smith and Eisner (2006) propose structural an-
nealing (SA), in which a strong bias for local de-
pendency attachments is enforced early in learn-
ing, and then gradually relaxed. This method is
sensitive to the annealing schedule. Smith (2006)
(pg. 136) use 10 annealing schedules in conjunc-
tion with three initializers. The best performing
combination attains accuracy of 66.7% on WSJ10,
but the worst attains accuracy of 32.5%.
Finally, Seginer (2007) and Bod (2006) ap-
proach unsupervised parsing by constructing
novel syntactic models. The development and tun-
ing of the above methods constitute the encoding
of prior domain knowledge about the desired syn-
tactic structure. In contrast, our framework pro-
vides a straightforward and explicit method for in-
corporating prior knowledge.
Ganchev et al (2009) propose a related method
that uses posterior constrained EM to learn a pro-
jective target language parser using only a source
language parser and word alignments.
3 Generalized Expectation Criteria
Generalized expectation criteria (Mann and Mc-
Callum, 2008; Druck et al, 2008) are terms in
a parameter estimation objective function that ex-
press a preference on the value of a model expec-
tation. Let x represent input variables (i.e. a sen-
tence) and y represent output variables (i.e. a parse
tree). A generalized expectation term G(?) is de-
fined by a constraint function G(y,x) that returns
a non-negative real value given input and output
variables, an empirical distribution p?(x) over in-
put variables (i.e. unlabeled data), a model distri-
bution p?(y|x), and a score function S:
G(?) = S(Ep?(x)[Ep?(y|x)[G(y,x)]]).
In this paper, we use a score function that is the
squared difference of the model expectation of G
and some target expectation G?:
Ssq = ?(G?? Ep?(x)[Ep?(y|x)[G(y,x)]])
2 (1)
We can incorporate prior knowledge into the train-
ing of p?(y|x) by specifying the from of the con-
straint function G and the target expectation G?.
361
Importantly, G does not need to match a particular
feature in the underlying model.
The complete objective function2 includes mul-
tiple GE terms and a prior on parameters3, p(?)
O(?;D) = p(?) +
?
G
G(?)
GE has been applied to logistic regression mod-
els (Mann and McCallum, 2007; Druck et al,
2008) and linear chain CRFs (Mann and McCal-
lum, 2008). In the following sections we apply
GE to non-projective CRF dependency parsing.
3.1 GE in General CRFs
We first consider an arbitrarily structured condi-
tional random field (Lafferty et al, 2001) p?(y|x).
We describe the CRF for non-projective depen-
dency parsing in Section 3.2. The probability of
an output y conditioned on an input x is
p?(y|x) =
1
Zx
exp
(?
j
?jFj(y,x)
)
,
where Fj are feature functions over the cliques
of the graphical model and Z(x) is a normaliz-
ing constant that ensures p?(y|x) sums to 1. We
are interested in the expectation of constraint func-
tion G(x,y) under this model. We abbreviate this
model expectation as:
G? = Ep?(x)[Ep?(y|x)[G(y,x)]]
It can be shown that partial derivative of G(?) us-
ing Ssq4 with respect to model parameter ?j is
?
??j
G(?) = 2(G??G?) (2)
(
Ep?(x)
[
Ep?(y|x) [G(y,x)Fj(y,x)]
?Ep?(y|x) [G(y,x)]Ep?(y|x) [Fj(y,x)]
])
.
Equation 2 has an intuitive interpretation. The first
term (on the first line) is the difference between the
model and target expectations. The second term
2In general, the objective function could also include the
likelihood of available labeled data, but throughout this paper
we assume we have no parsed sentences.
3Throughout this paper we use a Gaussian prior on pa-
rameters with ?2 = 10.
4In previous work, S was the KL-divergence from the tar-
get expectation. The partial derivative of the KL divergence
score function includes the same covariance term as above
but substitutes a different multiplicative term: G?/G?.
(the rest of the equation) is the predicted covari-
ance between the constraint function G and the
model feature function Fj . Therefore, if the con-
straint is not satisfied, GE updates parameters for
features that the model predicts are related to the
constraint function.
If there are constraint functions G for all model
feature functions Fj , and the target expectations
G? are estimated from labeled data, then the glob-
ally optimal parameter setting under the GE objec-
tive function is equivalent to the maximum likeli-
hood solution. However, GE does not require such
a one-to-one correspondence between constraint
functions and model feature functions. This al-
lows bootstrapping of feature-rich models with a
small number of prior expectation constraints.
3.2 Non-Projective Dependency Tree CRFs
We now define a CRF p?(y|x) for unlabeled, non-
projective5 dependency parsing. The tree y is rep-
resented as a vector of the same length as the sen-
tence, where yi is the index of the parent of word
i. The probability of a tree y given sentence x is
p?(y|x) =
1
Zx
exp
( n?
i=1
?
j
?jfj(xi, xyi ,x)
)
,
where fj are edge-factored feature functions that
consider the child input (word, tag, or other fea-
ture), the parent input, and the rest of the sen-
tence. This factorization implies that dependency
decisions are independent conditioned on the in-
put sentence x if y is a tree. ComputingZx and the
edge expectations needed for partial derivatives re-
quires summing over all possible trees for x.
By relating the sum of the scores of all possible
trees to counting the number of spanning trees in a
graph, it can be shown that Zx is the determinant
of the Kirchoff matrixK, which is constructed us-
ing the scores of possible edges. (McDonald and
Satta, 2007; Smith and Smith, 2007). Computing
the determinant takes O(n3) time, where n is the
length of the sentence. To compute the marginal
probability of a particular edge k ? i (i.e. yi=k),
the score of any edge k? ? i such that k? 6= k is
set to 0. The determinant of the resulting modi-
fied Kirchoff matrix Kk?i is then the sum of the
scores of all trees that include the edge k ? i. The
5Note that we could instead define a CRF for projective
dependency parse trees and use a variant of the inside outside
algorithm for inference. We choose non-projective because it
is the more general case.
362
marginal p(yi=k|x; ?) can be computed by divid-
ing this score by Zx (McDonald and Satta, 2007).
Computing all edge expectations with this algo-
rithm takes O(n5) time. Smith and Smith (2007)
describe a more efficient algorithm that can com-
pute all edge expectations in O(n3) time using the
inverse of the Kirchoff matrix K?1.
3.3 GE for Non-Projective Dependency Tree
CRFs
While in general constraint functions G may
consider multiple edges, in this paper we use
edge-factored constraint functions. In this case
Ep?(y|x)[G(y,x)]Ep?(y|x)[Fj(y,x)], the second
term of the covariance in Equation 2, can be
computed using the edge marginal distributions
p?(yi|x). The first term of the covariance
Ep?(y|x)[G(y,x)Fj(y,x)] is more difficult to
compute because it requires the marginal proba-
bility of two edges p?(yi, yi? |x). It is important to
note that the model p? is still edge-factored.
The sum of the scores of all trees that contain
edges k ? i and k? ? i? can be computed by set-
ting the scores of edges j ? i such that j 6= k and
j? ? i? such that j? 6= k? to 0, and computing the
determinant of the resulting modified Kirchoff ma-
trixKk?i,k??i? . There areO(n4) pairs of possible
edges, and the determinant computation takes time
O(n3), so this naive algorithm takes O(n7) time.
An improved algorithm computes, for each pos-
sible edge k ? i, a modified Kirchoff matrix
Kk?i that requires the presence of that edge.
Then, the method of Smith and Smith (2007) can
be used to compute the probability of every pos-
sible edge conditioned on the presence of k ? i,
p?(yi? =k?|yi = k,x), using K
?1
k?i. Multiplying
this probability by p?(yi=k|x) yields the desired
two edge marginal. Because this algorithm pulls
the O(n3) matrix operation out of the inner loop
over edges, the run time is reduced to O(n5).
If it were possible to perform only one O(n3)
matrix operation per sentence, then the gradient
computation would take onlyO(n4) time, the time
required to consider all pairs of edges. Unfortu-
nately, there is no straightforward generalization
of the method of Smith and Smith (2007) to the
two edge marginal problem. Specifically, Laplace
expansion generalizes to second-order matrix mi-
nors, but it is not clear how to compute second-
order cofactors from the inverse Kirchoff matrix
alone (c.f. (Smith and Smith, 2007)).
Consequently, we also propose an approxima-
tion that can be used to speed up GE training at
the expense of a less accurate covariance compu-
tation. We consider different cases of the edges
k ? i, and k? ? i?.
? p?(yi=k, yi?=k?|x)=0 when i=i? and k 6=k?
(different parent for the same word), or when
i=k? and k=i? (cycle), because these pairs of
edges break the tree constraint.
? p?(yi=k, yi? =k?|x)=p?(yi=k|x) when i=
i?, k=k?.
? p?(yi=k, yi? =k?|x)?p?(yi=k|x)p?(yi? =
k?|x) when i 6= i? and i 6= k? or i? 6= k
(different words, do not create a cycle). This
approximation assumes that pairs of edges
that do not fall into one of the above cases
are conditionally independent given x. This
is not true because there are partial trees in
which k ? i and k? ? i? can appear sepa-
rately, but not together (for example if i = k?
and the partial tree contains i? ? k).
Using this approximation, the covariance for one
sentence is approximately equal to
n?
i
Ep?(yi|x)[fj(xi, xyi ,x)g(xi, xyi ,x)]
?
n?
i
Ep?(yi|x)[fj(xi, xyi ,x)]Ep?(yi|x)[g(xi, xyi ,x)]
?
n?
i,k
p?(yi=k|x)p?(yk=i|x)fj(xi, xk,x)g(xk, xi,x).
Intuitively, the first and second terms compute a
covariance over possible parents for a single word,
and the third term accounts for cycles. Computing
the above takes O(n3) time, the time required to
compute single edge marginals. In this paper, we
use the O(n5) exact method, though we find that
the accuracy attained by approximate training is
usually within 5% of the exact method.
If G is not edge-factored, then we need to com-
pute a marginal over three or more edges, making
exact training intractable. An appealing alterna-
tive to a similar approximation to the above would
use loopy belief propagation to efficiently approx-
imate the marginals (Smith and Eisner, 2008).
In this paper g is binary and normalized by its
total count in the corpus. The expectation of g is
then the probability that it indicates a true edge.
363
4 Linguistic Prior Knowledge
Training parsers using GE with the aid of linguists
is an exciting direction for future work. In this pa-
per, we use constraints derived from several basic
types of linguistic knowledge.
One simple form of linguistic knowledge is the
set of possible parent tags for a given child tag.
This type of constraint was used in the devel-
opment of a rule-based dependency parser (De-
busmann et al, 2004). Additional information
can be obtained from small grammar fragments.
Haghighi and Klein (2006) provide a list of proto-
type phrase structure rules that can be augmented
with dependencies and used to define constraints
involving parent and child tags, surrounding or
interposing tags, direction, and distance. Finally
there are well known hypotheses about the direc-
tion and distance of attachments that can be used
to define constraints. Eisner and Smith (2005) use
the fact that short attachments are more common
to improve unsupervised parsing accuracy.
4.1 ?Oracle? constraints
For some experiments that follow we use ?ora-
cle? constraints that are estimated from labeled
data. This involves choosing feature templates
(motivated by the linguistic knowledge described
above) and estimating target expectations. Oracle
methods used in this paper consider three simple
statistics of candidate constraint functions: count
c?(g), edge count c?edge(g), and edge probability
p?(edge|g). Let D be the labeled corpus.
c?(g) =
?
x?D
?
i
?
j
g(xi, xj ,x)
c?edge(g) =
?
(x,y)?D
?
i
g(xi, xyi ,x)
p?(edge|g) =
c?edge(g)
c?(g)
Constraint functions are selected according to
some combination of the above statistics. In
some cases we additionally prune the candidate
set by considering only certain templates. To
compute the target expectation, we simply use
bin(p?(edge|g)), where bin returns the closest
value in the set {0, 0.1, 0.25, 0.5, 0.75, 1}. This
can be viewed as specifying that g is very indica-
tive of edge, somewhat indicative of edge, etc.
5 Experimental Comparison with
Unsupervised Learning
In this section we compare GE training with meth-
ods for unsupervised parsing. We use the WSJ10
corpus (as processed by Smith (2006)), which is
comprised of English sentences of ten words or
fewer (after stripping punctuation) from the WSJ
portion of the Penn Treebank. As in previous work
sentences contain only part-of-speech tags.
We compare GE and supervised training of an
edge-factored CRF with unsupervised learning of
a DMV model (Klein and Manning, 2004) using
EM and contrastive estimation (CE) (Smith and
Eisner, 2005). We also report the accuracy of an
attach-right baseline6. Finally, we report the ac-
curacy of a constraint baseline that assigns a score
to each possible edge that is the sum of the target
expectations for all constraints on that edge. Pos-
sible edges without constraints receive a score of
0. These scores are used as input to the maximum
spanning tree algorithm, which returns the best
tree. Note that this is a strong baseline because it
can handle uncertain constraints, and the tree con-
straint imposed by the MST algorithm helps infor-
mation propagate across edges.
We note that there are considerable differences
between the DMV and CRF models. The DMV
model is more expressive than the CRF because
it can model the arity of a head as well as sib-
ling relationships. Because these features consider
multiple edges, including them in the CRF model
would make exact inference intractable (McDon-
ald and Satta, 2007). However, the CRF may con-
sider the distance between head and child, whereas
DMV does not model distance. The CRF also
models non-projective trees, which when evaluat-
ing on English is likely a disadvantage.
Consequently, we experiment with two sets of
features for the CRF model. The first, restricted
set includes features that consider the head and
child tags of the dependency conjoined with the
direction of the attachment, (parent-POS,child-
POS,direction). With this feature set, the CRF
model is less expressive than DMV. The sec-
ond full set includes standard features for edge-
factored dependency parsers (McDonald et al,
2005), though still unlexicalized. The CRF can-
not consider valency even with the full feature set,
but this is balanced by the ability to use distance.
6The reported accuracies with the DMV model and the
attach-right baseline are taken from (Smith, 2006).
364
feature ex. feature ex.
MD? VB 1.00 NNS? VBD 0.75
POS? NN 0.75 PRP? VBD 0.75
JJ? NNS 0.75 VBD? TO 1.00
NNP? POS 0.75 VBD? VBN 0.75
ROOT?MD 0.75 NNS? VBP 0.75
ROOT? VBD 1.00 PRP? VBP 0.75
ROOT? VBP 0.75 VBP? VBN 0.75
ROOT? VBZ 0.75 PRP? VBZ 0.75
TO? VB 1.00 NN? VBZ 0.75
VBN? IN 0.75 VBZ? VBN 0.75
Table 1: 20 constraints that give 61.3% accuracy
on WSJ10. Tags are grouped according to heads,
and are in the order they appear in the sentence,
with the arrow pointing from head to modifier.
We generate constraints in two ways. First,
we use oracle constraints of the form (parent-
POS,child-POS,direction) such that c?(g) ? 200.
We choose constraints in descending order of
p?(edge|g). The first 20 constraints selected using
this method are displayed in Table 1.
Although the reader can verify that the con-
straints in Table 1 are reasonable, we addition-
ally experiment with human-provided constraints.
We use the prototype phrase-structure constraints
provided by Haghighi and Klein (2006), and
with the aid of head-finding rules, extract 14
(parent-pos,child-pos,direction) constraints.7 We
then estimated target expectations for these con-
straints using our prior knowledge, without look-
ing at the training data. We also created a second
constraint set with an additional six constraints for
tag pairs that were previously underrepresented.
5.1 Results
We present results varying the number of con-
straints in Figures 1 and 2. Figure 1 compares
supervised and GE training of the CRF model, as
well as the feature constraint baseline. First we
note that GE training using the full feature set sub-
stantially outperforms the restricted feature set,
despite the fact that the same set of constraints
is used for both experiments. This result demon-
strates GE?s ability to learn about related but non-
constrained features. GE training also outper-
forms the baseline8.
We compare GE training of the CRF model
7Because the CFG rules in (Haghighi and Klein, 2006)
are ?flattened? and in some cases do not generate appropriate
dependency constraints, we only used a subset.
8The baseline eventually matches the accuracy of the re-
stricted CRF but this is understandable because GE?s ability
to bootstrap is greatly reduced with the restricted feature set.
with unsupervised learning of the DMV model
in Figure 29. Despite the fact that the restricted
CRF is less expressive than DMV, GE training of
this model outperforms EM with 30 constraints
and CE with 50 constraints. GE training of the
full CRF outperforms EM with 10 constraints and
CE with 20 constraints (those displayed in Ta-
ble 1). GE training of the full CRF with the set of
14 constraints from (Haghighi and Klein, 2006),
gives accuracy of 53.8%, which is above the inter-
polated oracle constraints curve (43.5% accuracy
with 10 constraints, 61.3% accuracy with 20 con-
straints). With the 6 additional constraints, we ob-
tain accuracy of 57.7% and match CE.
Recall that CE, EM, and the DMV model in-
corporate prior knowledge indirectly, and that the
reported results are heavily-tuned ideal cases (see
Section 2). In contrast, GE provides a method to
directly encode intuitive linguistic insights.
Finally, note that structural annealing (Smith
and Eisner, 2006) provides 66.7% accuracy on
WSJ10 when choosing the best performing an-
nealing schedule (Smith, 2006). As noted in Sec-
tion 2 other annealing schedules provide accuracy
as low as 32.5%. GE training of the full CRF at-
tains accuracy of 67.0% with 30 constraints.
6 Experimental Comparison with
Supervised Training on Long
Sentences
Unsupervised parsing methods are typically eval-
uated on short sentences, as in Section 5. In this
section we show that GE can be used to train
parsers for longer sentences that provide compa-
rable accuracy to supervised training with tens to
hundreds of parsed sentences.
We use the standard train/test splits of the
Spanish, Dutch, and Turkish data from the 2006
CoNLL Shared Task. We also use standard
edge-factored feature templates (McDonald et al,
2005)10. We experiment with versions of the dat-
9Klein and Manning (2004) report 43.2% accuracy for
DMV with EM on WSJ10. When jointly modeling con-
stituency and dependencies, Klein and Manning (2004) re-
port accuracy of 47.5%. Seginer (2007) and Bod (2006) pro-
pose unsupervised phrase structure parsing methods that give
better unlabeled F-scores than DMV with EM, but they do
not report directed dependency accuracy.
10Typical feature processing uses only supported features,
or those features that occur on at least one true edge in the
training data. Because we assume that the data is unlabeled,
we instead use features on all possible edges. This generates
tens of millions features, so we prune those features that oc-
cur fewer than 10 total times, as in (Smith and Eisner, 2007).
365
10 20 30 40 50 6010
20
30
40
50
60
70
80
90
number of constraints
accu
racy
 
 
constraint baselineCRF restricted supervisedCRF supervisedCRF restricted GECRF GECRF GE human
Figure 1: Comparison of the constraint baseline and
both GE and supervised training of the restricted and
full CRF. Note that supervised training uses 5,301
parsed sentences. GE with human provided con-
straints closely matches the oracle results.
10 20 30 40 50 6010
20
30
40
50
60
70
80
number of constraints
accu
racy
 
 
attach right baselineDMV EMDMV CECRF restricted GECRF GECRF GE human
Figure 2: Comparison of GE training of the re-
stricted and full CRFs with unsupervised learning of
DMV. GE training of the full CRF outperforms CE
with just 20 constraints. GE also matches CE with
20 human provided constraints.
sets in which we remove sentences that are longer
than 20 words and 60 words.
For these experiments, we use an oracle
constraint selection method motivated by the
linguistic prior knowledge described in Section 4.
The first set of constraints specify the most
frequent head tag, attachment direction, and
distance combinations for each child tag. Specif-
ically, we select oracle constraints of the type
(parent-CPOS,child-CPOS,direction,distance)11.
We add constraints for every g such that
c?edge(g) > 100 for max length 60 data sets, and
c?edge(g)>10 times for max length 20 data sets.
In some cases, the possible parent constraints
described above will not be enough to provide
high accuracy, because they do not consider other
tags in the sentence (McDonald et al, 2005).
Consequently, we experiment with adding an
additional 25 sequence constraints (for what are
often called ?between? and ?surrounding? fea-
tures). The oracle feature selection method aims to
choose such constraints that help to reduce uncer-
tainty in the possible parents constraint set. Con-
sequently, we consider sequence features gs with
p?(edge|gs=1) ? 0.75, and whose corresponding
(parent-CPOS,child-CPOS,direction,distance)
constraint g, has edge probability p?(edge|g) ?
0.25. Among these candidates, we sort by
c?(gs=1), and select the top 25.
We compare with the constraint baseline de-
scribed in Section 5. Additionally, we report
11For these experiments we use coarse-grained part-of-
speech tags in constraints.
the number of parsed sentences required for su-
pervised CRF training (averaged over 5 random
splits) to match the accuracy of GE training using
the possible parents + sequence constraint set.
The results are provided in Table 2. We first
observe that GE always beats the baseline, espe-
cially on parent decisions for which there are no
constraints (not reported in Table 2, but for exam-
ple 53.8% vs. 20.5% on Turkish 20). Second, we
note that accuracy is always improved by adding
sequence constraints. Importantly, we observe
that GE gives comparable performance to super-
vised training with tens or hundreds of parsed sen-
tences. These parsed sentences provide a tremen-
dous amount of information to the model, as for
example in 20 Spanish length ? 60 sentences, a
total of 1,630,466 features are observed, 330,856
of them unique. In contrast, the constraint-based
methods are provided at most a few hundred con-
straints. When comparing the human costs of
parsing sentences and specifying constraints, re-
member that parsing sentences requires the devel-
opment of detailed annotation guidelines, which
can be extremely time-consuming (see also the
discussion is Section 2).
Finally, we experiment with iteratively
adding constraints. We sort constraints with
c?(g) > 50 by p?(edge|g), and ensure that 50%
are (parent-CPOS,child-CPOS,direction,distance)
constraints and 50% are sequence constraints.
For lack of space, we only show the results for
Spanish 60. In Figure 3, we see that GE beats
the baseline more soundly than above, and that
366
possible parent constraints + sequence constraints complete trees
baseline GE baseline GE
dutch 20 69.5 70.7 69.8 71.8 80-160
dutch 60 66.5 69.3 66.7 69.8 40-80
spanish 20 70.0 73.2 71.2 75.8 40-80
spanish 60 62.1 66.2 62.7 66.9 20-40
turkish 20 66.3 71.8 67.1 72.9 80-160
turkish 60 62.1 65.5 62.3 66.6 20-40
Table 2: Experiments on Dutch, Spanish, and Turkish with maximum sentence lengths of 20 and 60. Observe that GE
outperforms the baseline, adding sequence constraints improves accuracy, and accuracy with GE training is comparable to
supervised training with tens to hundreds of parsed sentences.
parent tag true predicted
det. 0.005 0.005
adv. 0.018 0.013
conj. 0.012 0.001
pron. 0.011 0.009
verb 0.355 0.405
adj. 0.067 0.075
punc. 0.031 0.013
noun 0.276 0.272
prep. 0.181 0.165
direction true predicted
right 0.621 0.598
left 0.339 0.362
distance true predicted
1 0.495 0.564
2 0.194 0.206
3 0.066 0.050
4 0.042 0.037
5 0.028 0.031
6-10 0.069 0.033
> 10 0.066 0.039
feature (distance) false pos. occ.
verb? punc. (>10) 1183
noun? prep. (1) 1139
adj. ? prep. (1) 855
verb? verb (6-10) 756
verb? verb (>10) 569
noun? punc. (1) 512
verb? punc. (2) 509
prep. ? punc. (1) 476
verb? punc. (4) 427
verb? prep. (1) 422
Table 3: Error analysis for GE training with possible parent + sequence constraints on Spanish 60 data. On the left, the
predicted and true distribution over parent coarse part-of-speech tags. In the middle, the predicted and true distributions over
attachment directions and distances. On the right, common features on false positive edges.
100 200 300 400 500 600 700 8002530
3540
4550
5560
6570
75
number of constraints
accura
cy
Spanish (maximum length 60)
 
 
constraint baselineGE
Figure 3: Comparing GE training of a CRF and constraint
baseline while increasing the number of oracle constraints.
adding constraints continues to increase accuracy.
7 Error Analysis
In this section, we analyze the errors of the model
learned with the possible parent + sequence con-
straints on the Spanish 60 data. In Table 3, we
present four types of analysis. First, we present
the predicted and true distributions over coarse-
grained parent part of speech tags. We can see
that verb is being predicted as a parent tag more
often then it should be, while most other tags are
predicted less often than they should be. Next, we
show the predicted and true distributions over at-
tachment direction and distance. From this we see
that the model is often incorrectly predicting left
attachments, and is predicting too many short at-
tachments. Finally, we show the most common
parent-child tag with direction and distance fea-
tures that occur on false positive edges. From this
table, we see that many errors concern the attach-
ments of punctuation. The second line indicates a
prepositional phrase attachment ambiguity.
This analysis could also be performed by a lin-
guist by looking at predicted trees for selected sen-
tences. Once errors are identified, GE constraints
could be added to address these problems.
8 Conclusions
In this paper, we developed a novel method for
the semi-supervised learning of a non-projective
CRF dependency parser that directly uses linguis-
tic prior knowledge as a training signal. It is our
hope that this method will permit more effective
leveraging of linguistic insight and resources and
enable the construction of parsers in languages and
domains where treebanks are not available.
Acknowledgments
We thank Ryan McDonald, Keith Hall, John Hale, Xiaoyun
Wu, and David Smith for helpful discussions. This work
was completed in part while Gregory Druck was an intern
at Google. This work was supported in part by the Center
for Intelligent Information Retrieval, The Central Intelligence
Agency, the National Security Agency and National Science
Foundation under NSF grant #IIS-0326249, and by the De-
fense Advanced Research Projects Agency (DARPA) under
Contract No. FA8750-07-D-0185/0004. Any opinions, find-
ings and conclusions or recommendations expressed in this
material are the author?s and do not necessarily reflect those
of the sponsor.
367
References
E. Black, J. Lafferty, and S. Roukos. 1992. Development and
evaluation of a broad-coverage probabilistic grammar of
english language computer manuals. In ACL, pages 185?
192.
Rens Bod. 2006. An all-subtrees approach to unsupervised
parsing. In ACL, pages 865?872.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL.
R. Debusmann, D. Duchier, A. Koller, M. Kuhlmann,
G. Smolka, and S. Thater. 2004. A relational syntax-
semantics interface based on dependency grammar. In
COLING.
G. Druck, G. S. Mann, and A. McCallum. 2008. Learning
from labeled features using generalized expectation crite-
ria. In SIGIR.
J. Eisner and N.A. Smith. 2005. Parsing with soft and hard
constraints on dependency length. In IWPT.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext projec-
tion constraints. In ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven grammar
induction. In COLING.
R. J. Kate and R. J. Mooney. 2007. Semi-supervised learning
for semantic parsing using support vector machines. In
HLT-NAACL (Short Papers).
D. Klein and C. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
G. Mann and A. McCallum. 2007. Simple, robust, scal-
able semi-supervised learning via expectation regulariza-
tion. In ICML.
G. Mann and A. McCallum. 2008. Generalized expectation
criteria for semi-supervised learning of conditional ran-
dom fields. In ACL.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In HLT-NAACL.
Ryan McDonald and Giorgio Satta. 2007. On the complex-
ity of non-projective data-driven dependency parsing. In
Proc. of IWPT, pages 121?132.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL, pages 91?98.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman.
Yoav Seginer. 2007. Fast unsupervised incremental parsing.
In ACL, pages 384?391, Prague, Czech Republic.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data. In
ACL, pages 354?362.
Noah A. Smith and Jason Eisner. 2006. Annealing struc-
tural bias in multilingual weighted grammar induction. In
COLING-ACL, pages 569?576.
David A. Smith and Jason Eisner. 2007. Bootstrapping
feature-rich dependency parsers with entropic priors. In
EMNLP-CoNLL, pages 667?677.
David A. Smith and Jason Eisner. 2008. Dependency parsing
by belief propagation. In EMNLP.
David A. Smith and Noah A. Smith. 2007. Probabilistic
models of nonprojective dependency trees. In EMNLP-
CoNLL, pages 132?140.
Noah A. Smith. 2006. Novel Estimation Methods for Un-
supervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2008.
Semi-supervised convex training for dependency parsing.
In ACL, pages 532?540.
368
Rich Prior Knowledge in 
Learning for NLP
Gregory Druck, Kuzman Ganchev, Jo?o Gra?a
Why Incorporate Prior Knowledge?
have: unlabeled data
option: hire
linguist
annotators
Why Incorporate Prior Knowledge?
have: unlabeled data
option: hire
linguist
annotators
This approach does not 
scale to every task and 
domain of interest.
However, we already 
know a lot about most 
problems of interest.
Example: Document Classification 
?
Prior Knowledge: 
?
labeled features: information about the label 
distribution when word w is present
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
Documents
Labels
newsgroups classification
baseball Mac politics
...
hit Apple senate
...
Braves Macintosh taxes
...
runs Powerbook liberal
...
sentiment polarity
positive negative
memorable terrible
perfect boring
exciting mess
Example: Information Extraction
?
Prior Knowledge: 
?
labeled features: 
?
the word ACM should be labeled either journal or 
booktitle most of the time
?
non-Markov (long-range) dependencies:
?
each reference has at most one segment of each type
W. H. Enright. Improving the efficiency of matrix operations 
in the numerical solution of stiff ordinary differential 
equations. ACM Trans. Math. Softw., 4(2), 127-136, June 1978.
extraction from 
research papers:
Example: Part-of-speech Induction
?
Prior Knowledge: 
?
linguistic knowledge: each sentence should have a verb
?
posterior sparsity: the total number of different POS tags 
assigned to each word type should be small
Tags
A career with the European 
institutions must become more 
attractive. Too many young, new...
Text
Example: Dependency Grammar Induction
?
Prior Knowledge: 
?
linguistic rules: nouns are usually dependents of verbs
?
noisy labeled data: target language parses should be 
similar to aligned parses in a resource-rich source language 
Example: Word Alignment
?
Prior Knowledge: 
?
Bijectivity: alignment should be mostly one-to-one
?
Symmetry: source?target and target?source 
alignments should agree
A career with the European institutions must become more attractive. 
Uma carreira nas institui??es europeias t?m de se tornar mais atractiva. 
This Tutorial
In general, how can we leverage such knowledge 
and an unannotated corpus during learning?
Notation & Models
input variables (documents, sentences):
structured output variables (parses, sequences):
unstructured output variables (labels):
input / output variables for entire corpus: 
probabilistic model parameters:
generative models:
discriminative models:
model feature function:  
p ?(y|x)
p?(x,y)
x
y
?
f(x , y)
X Y
y
Learning Scenarios
?
Unsupervised: 
?
unlabeled data + prior knowledge
?
Lightly Supervised: 
?
unlabeled data + ?informative? prior knowledge
?
i.e. provides specific information about labels 
?
Semi-Supervised: 
?
labeled data + unlabeled data + prior knowledge
Running Example #1:
Document Classification
?
model: Maximum Entropy Classifier (Logistic Regression) 
?
setting: lightly supervised; no labeled data
?
prior knowledge: 
?
labeled features: information about the label 
distribution when word w is present
?
label is often hockey or baseball when game is present
p?(y|x) =
1
Z(x)
exp(? ? f(x, y))
Running Example #2:
Word Alignment
?
model: first-order Hidden Markov Model (HMM)
?
setting: unsupervised
?
prior knowledge: 
?
Bijectivity: alignment should be mostly one-to-one
1 1
2 3
we know
the way
sabemos       el       camino      null
1 2 3 0
p?(y,x) = p?(y0)
N?
i=1
p?(yi|yi?1)p?( x i|yi)
Problem
?
This output does not agree with prior knowledge!
?
 six target words align to source word animada
?
 five source words do not align with any target word 
gameconvivialvery,animatedanwasit
cordialmuyyanimadamaneraunadejugaban
model
data output
x
1
x
2
x
3
y
1
y
2
y
3
+
Limited Approach: Labeling Data
limitation: Often unclear how to do conversion
?
Example #1: often (not always) game ? {hockey,baseball} 
?
Example #2: alignment should be mostly one-to-one
prior 
knowledge
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
approach: Convert prior knowledge to labeled data.
Prototypes (+ cluster features):   
?
 [Haghighi & Klein 06]
Others: 
?
[Raghavan & Allan 07]       
?
[Schapire et al 02]
Limited Approach: Bayesian Approach
approach: Encode prior knowledge with a prior on parameters.
limitation: Our prior knowledge is not about parameters! 
Parameters are difficult to interpret; hard to get desired effect.
?
Example #1: often (not always) game ? {hockey,baseball}
?
Example #2: alignment should be mostly one-to-one
natural: ?   should be small (or sparse)??
( informative prior )
possible: ?    should be close to   ??i ??i
p (?)
specifying 
x
1
x
2
x
3
y
1
y
2
y
3
??
[Dayanik et al 06]
[Johnson 07], among many others
Limited Approach: Augmenting Model
limitation: can be difficult to get desired effect
?
Example #1: often (not always) game ? {hockey,baseball}
limitation: may make exact inference intractable
?
Example #2: Bijectivity makes inference #P-complete
x
1
x
2
x
3
y
1
y
2
y
3
z
1
approach: Encode prior knowledge with 
additional variables and dependencies.
This Tutorial
develop:
?
a language for directly encoding prior knowledge
?
methods for learning with knowledge in this language
?
( approximations to modeling this language directly )
?
(loosely) these methods perform mappings for us: 
?
encoded prior knowledge            parameters
?
encoded prior knowledge            labeling 
?
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
?
?
A Language for Encoding Prior Knowledge
Our prior knowledge is about distributions over latent 
output variables. (output variables are interpretable)
Specifically, we know some properties of this distribution:
?
Example #1: often (not always) game?{hockey,baseball}
Formulation: know about the expectations of some 
functions under distribution over latent output variables
Constraint Features
?
constraint feature function: 
?
Example #1: 
?
for document x, returns a vector with a 1 in the lth 
position if y is the lth label and the word w is in x
?
Example #2: 
?
returns a vector with mth value = number of target 
words in sentence x that align with source word m
?(x , y )
?w(x, y) = 1 (y = l )1( w ? x)
?(x,y) =
N?
i=1
1(yi = m)
Expectations of Constraint Features
?
Example #1:  Corpus expectation: 
?
vector with expected distribution over labels for 
documents that contain w (     is the count of w)
?
Example #2:  Per-example expectation: 
?
vector with mth value = expected number of target 
words that align with source word m 
Ep? [?(X,Y)] =
1
c w
?
x
?
y
p?(y|x)?w(x, y)
E p ? [?(x,y)] =
?
y
p?(y|x)?(x,y)
c w
Expressing Preferences
?
express preferences using target values: 
?
Example #1:                           
?
label distribution for game is close to [40% 40% 20%]
?
Example #2:                           
?
expected number of target words that align with each 
source word is at most one
??
E p ? [? w ( X , Y )] ? ??
E p ? [?(x , y)] ? ??
Preview: Labeled Features
User Experiments [Druck et al 08]
0 100 200 300 400 500 600 700 8000.4
0.5
0.6
0.7
0.8
0.9
1
labeling time in seconds
tes
ting
 ac
cur
acy
 
 
GEER
~2 minutes, 100 
features labeled 
(or skipped): 
82% accuracy
~15 minutes, 100 
documents labeled 
(or skipped):
78% accuracy
PC vs. Mac
complete set of 
labeled features
PC Mac
dos mac
ibm apple
hp quadra
dx
targets set with 
simple heuristic: 
majority label gets 
90% of mass
Preview:  Word Alignment
[Gra?a et al 10]
60
68.75
77.5
86.25
95
En-Pt Pt-En En-Es Es-En
HMM HMM + Bijectivity Constraint
Overview of the Frameworks
Running Example
Model Family: conditional exponential models
                   are model features
p?(Y|X) =
exp( ? ? f(X,Y))
Z(X)
Z(X) =
?
Y
e x p( ? ? f(X , Y))
f( X , Y )
                Choosing parameters
Model Family: conditional exponential models
Objective: maximize observed data likelihood
Note: Frameworks also suitable for 
generative models (no labeled data necessary)   
?
p?(Y|X) =
exp( ? ? f(X,Y))
Z(X)
max
?
log p?( Y L | X L ) + log p(?)
d e f
= L (?; D L )
Visual Example: Maximum Likelihood
Model:                                      
Objective:
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L )? 0. 1???
2
2
p(Y|X) =
?
i
exp( y i x i ? ?)
Z ( x i)
A language for prior information
The expectations of user-defined constraint 
features             are close to some value 
?( X , Y ) ??
E [?( X , Y )] ? ??
Running Example:
Want to ensure that 25% of unlabeled 
documents are about politics
?
constraint features
 
?
preferred expected value
?
Expectation w.r.t. unlabeled data
?(x , y) =
?
1 if y is ?politics?
0 otherwise
?? = 0 . 25
Constraint-Driven Learning
Motivation: Hard EM algorithm with preferences
Hard EM: 
Constraint Driven Learning:
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E-Step: set ?Y = argmax
Y
log p ?( Y | X )?penalty( Y )
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E-Step: set ?Y = argmax
Y
log p ?( Y | X )
M. Chang, L. Ratinov, D. Roth (2007).
Constraint-Driven Learning
Motivation: Hard EM algorithm with preferences
Constraint Driven Learning:
?
penalties encode similar information as 
* more on this later *
?
E-Step can be hard; use beam search
E-Step: set ?Y = argmax
Y
log p ?( Y | X )?penalty( Y )
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E [ ? ] ? ??
Visual Example: Constraint Driven Learning
    where     are ?imagined? labels and
?Y
-
+
oo
o
o o
o o o
?[ ?Y ] = count(+ , ?Y )
max
?,Y?
log p?( Y L| X L)? 0. 1???
2
2 s.t. ?(
?Y ) = 2
Posterior Regularization
Motivation: EM algorithm with sane posteriors
EM:
Constrained EM:
E-Step: set q ( Y ) = argmin
q
D KL ( q ( Y )||p?( Y | X ))
M-Step: set ? = argmax
?
E q ( Y ) [ p ?( Y | X )]
E-Step: set q ( Y ) = argmin
q ?Q
D KL ( q ( Y )||p?(y|x))
M-Step: set ? = argmax
?
E q ( Y )[ p ?( Y | X )]
J. Gra?a, K. Ganchev, B. Taskar (2007).
Posterior Regularization
Motivation: EM algorithm with sane posteriors
Idea:                  provide constraints
Objective:
E [ ? ] ? ??
define Q : set of q such that E q [?] ? ??
m ax
?
L(? ; D L )?D KL (Q || p?( Y | X ))
run EM-like procedure but use proposal q ? Q
where
D KL is Kullback-Leibler divergence
X = D U are the input variables for unlabeled corpus
Y is label for entire unlabeled corpus
Posterior Regularization
Hard constraints:
Soft constraints:
max
?
L(?; D L) ? min
q ?Q
D KL ( q ( Y )|| p ?( Y | X ))
Q =
?
q ( Y ) :
?
?
? E q [?( Y )] = ??
?
?
?
2
2
? ?
?
max
?
L(?;D L ) ? min
q
?
D KL (q( Y )|| p?( Y | X )) +
?
?
?
? E q [?( Y )] = ??
?
?
?
2
2
?
Visual Example: Posterior Regularization 
 where: 
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L )? 0. 1???
2
2 ? D KL ( Q|| p?)
D KL ( Q|| p?) = min
q
D KL ( q ||p?) s.t. E q [?] = 2
Generalized Expectation Constraints
Motivation: augment log-likelihood with cost for ?bad? 
posteriors.
Objective:
where
                                                                    is short-hand
Optimization: gradient descent on    
max
?
L (?; D L)?
?
?
? E p ? ( Y | X ) [?] ? ??
?
?
?
?
E p ? ( Y | X )
[?] = E p ? ( Y | X ) [?( X , Y )]
=
?
Y
p?( Y | X )?( X , Y )
?
G. Mann, A. McCallum (2007). 
A visual comparison of the frameworks
Objective: Generalized Expectation Constraints
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L ) ? 0. 1???
2
2 ? 500?E p ? [?] ? 2?
2
2
Types of constraints
Constraint Driven Learning: Penalized Viterbi
?
Easy if                           decompose as the model.
                 and
?
Otherwise:
?
Beam search
?
Integer linear program 
p ( Y | X ) =
?
c
p c (y c | X )
argmax
Y
log p?( Y | X ) ? ??( X , Y )? ????
??( X , Y )? ????
??( X , Y )? ???? =
?
c
? c ( X , y c )
Types of constraints
Posterior Regularization: KL projection
?
Usually easy if               decompose as the model:
     and
?
Otherwise: Sample (e.g. K. Bellare, G. Druck, and A. McCallum, 2009)
?( Y , X )
p ( Y | X ) =
?
c
p c (y c | X )
q ( Y | X ) =
?
c
q c (y c | X )
?( X , Y ) =
?
c
? c ( X , y c )
?
min
q
D KL ( q ||p?) s.t. ?E q [?] ? ???? ? ?
Types of constraints
Generalized Expectation Constraints: Direct gradient
?
Usually easy if:
?
decomposes as the model
?
Can compute                * more on this later *
?
Unstructured
?
Sequence, Grammar (semiring trick)
?
Otherwise: sample or approximate the gradient.
?( Y , X )
max
?
L (?; D L)?
?
?
? E p ? ( Y | X ) [?] ? ??
?
?
?
?
?( X , Y ) =
?
c
? c ( X , y c )
E [ ?? f ]
A Bayesian View: Measurements
Objective: mode of    given observations
X L ? X
Y L Y
?( X , Y )
b
Figure 4.1: The model used by Liang et al [2009], using our notation. We have separated
treatment of the labeled data (XL,YL) from treatment of the unlabeled data X.
and produce some value ?(X,Y), which is never observed directly. Instead, we observe
some noisy version b ? ?(X,Y). The measured values b are distributed according to
some noise model pN(b|?(X,Y)). Liang et al [2009] note that the optimization is convex
for log-concave noise and use box noise in their experiments, giving b uniform probability
in some range near ?(X,Y).
In the Bayesian setting, the model parameters ? as well as the observed measurement
values b are random variables. Liang et al [2009] use the mode of p(?|XL,YL,X,b) as a
point estimate for ?:
argmax
?
p(?|XL,YL,X,b) = argmax
?
?
Y
p(?,Y,b|X,XL,YL), (4.6)
with equality because p(?|XL,YL,X,b) ? p(?,b|XL,YL,X) =
?
Y p(?,Y,b|X,XL,YL). Liang et al [2009] focus on computing p(?,Y,b|X,XL,YL).
They define their model for this quantity as follows:
p(?,Y,b|X,XL,YL) = p(?|XL,YL) p?(Y|X) pN(b|?(X,Y)) (4.7)
where the Y and X are particular instantiations of the random variables in the entire unla-
beled corpusX. Equation 4.7 is a product of three terms: a prior on ?, the model probability
p?(Y|X), and a noise model pN(b|?). The noise model is the probability that we observe
a value, b, of the measurement features ?, given that its actual value was ?(X,Y). The
idea is that we model errors in the estimation of the posterior probabilities as noise in the
measurement process. Liang et al [2009] use a uniform distribution over ?(X,Y) ? ?,
which they call ?box noise?. Under this model, observing b farther than ? from ?(X,Y)
has zero probability. In log space, the exact MAP objective, becomes:
max
?
L(?) + logEp?(Y|X)
?
pN(b|?(X,Y))
?
. (4.8)
31
max
?
l og p(?) +
?
( x , y ) ? D L
l og p?( y |x) = L (? ; D L )
?
P. Liang, M. Jordan, D. Klein (2009)
Objective: mode of    given observations
A Bayesian View: Measurements
X L ? X
Y L Y
?( X , Y )
b
Figure 4.1: The model used by Liang et l. [2009], using our notation. We have separated
treatment of the labeled data (XL,YL) from treatment of the unlabeled data X.
and produce some value ?(X,Y), which is never observed directly. Instead, we observe
some noisy version b ? ?(X,Y). The measured values b are distributed according to
some noise model pN(b|?(X,Y)). Liang et al [2009] note that the optimization is convex
for log-concave noise and use box noise in their experiments, giving b uniform probability
in some range near ?(X,Y).
In the Bayesian setting, the model parameters ? as well as the observed measurement
values b are random variables. Liang et al [2009] use the mode of p(?|XL,YL,X,b) as a
point estimate for ?:
argmax
?
p(?|XL,YL,X,b) = argmax
?
?
Y
p(?,Y,b|X,XL,YL), (4.6)
with equality because p(?|XL,YL,X,b) ? p(?,b|XL,YL,X) =
?
Y p(?,Y,b|X,XL,YL). Liang et al [2009] focus on computing p(?,Y,b|X,XL,YL).
They define their model for this quantity as follows:
p(?,Y,b|X,XL,YL) = p(?|XL,YL) p?(Y|X) pN(b|?(X,Y)) (4.7)
where the Y and X are particular instantiations of the random variables in the entire unla-
beled corpusX. Equation 4.7 is a product of three terms: a prior on ?, the model probability
p?(Y|X), and a noise model pN(b|?). The noise model is the probability that we observe
a value, b, of the measurement features ?, given that its actual value was ?(X,Y). The
idea is that we model errors in the estimation of the posterior probabilities as noise in the
measurement process. Liang et al [2009] use a uniform distribution over ?(X,Y) ? ?,
which they call ?box noise?. Under this model, observing b farther than ? from ?(X,Y)
has zero probability. In log space, the exact MAP objective, becomes:
max
?
L(?) + logEp?(Y|X)
?
pN(b|?(X,Y))
?
. (4.8)
31
max
?
L (?;D L ) + log E p ? ( Y | X )
?
p(??|?( X , Y ))
?
?
What's wrong with this picture?
Objective: mode of    given observations
Example: Exactly 25% of articles are ?politics?
What is the probability exactly 25% of the articles are 
labeled ``politics''?
How do we optimize this with respect to  ?
max
?
L (? ; D L ) + log E p ?(Y | X)
?
p(??|?( X , Y ))
?
?
?
p(??|?( X , Y )) = 1
?
?? = ?( X , Y )
?
E p ?(Y | X)
?
1 (?? = ?(X , Y))
?
What's wrong with this picture?
Example: Compute prob:  25% of docs are ?politics?.
    Naively:
      in this case we can use a DP, but if 
there are many constraints, that doesn?t 
work.
Easier: What is the expected number of ?politics? articles?
Article p(?politics?)
1 0.2
2 0.4
3 0.1
4 0.6
0 . 2 + 0 . 4 + 0 . 1 + 0 . 6
0 . 2 ? (1 ? 0 . 4) ? (1 ? 0 . 1) ? (1 ? 0 . 6)
+ . . . +
+(1 ? 0 . 2) ? (1 ? 0 . 4) ? (1 ? 0 . 1) ? 0 . 6
Probabilities and Expectations
difficult to compute expectations of arbitrary functions but...
Usually:             decomposes as a sum
e.g. 25% of articles are ?politics?
Idea: approximate 
?( X , Y )
?(X , Y) =
?
instances
?( x , y )
E p ?(Y | X)
?
p
?
?? | ?( X , Y )
??
? p
?
?? | E p ?(Y | X) [?( X , Y )]
?
Probabilities and Expectations
Approximation:
Objective:
Example:                      is Gaussian     
                                              is 
so for appropriate                           this is identical to GE!
E p ? ( Y | X )
?
p
?
?? | ?
??
? p
?
?? | E p ? ( Y | X ) [ ? ]
?
max
?
L (?;D L ) + log p
?
?? | E p ?(Y | X) [?]
?
l og p
?
?? | E [?]
?
?
p
?
?? | E [ ? ]
?
l og p
?
?? | E [ ? ]
?
?
?
?
? E [ ? ] ? ??
?
?
?
2
2
Optimizing GE objective
GE Objective:
?
Gradient involves covariance
this can be hard because
and the usual dynamic programs (inside outside, forward 
backward) can?t compute this.
C ov( ? , f) = E [ ?? f ] ? E [ ? ] ? E [ f ]
E [?? f ] =
?
Y
p ( Y )?( Y )? f ( Y )
O GE = max
?
L (? ; D L )?
?
?
? E p ?(Y | X) [ ?( X , Y )] ? ??
?
?
?
?
Optimizing GE Objective
Maintaining both     and      in the DP is expensive
* Semiring trick can help for some problems *
x1 x2 x3 x3
y1 y2 y3 y4
E [?? f ] =
?
Y
p ( Y )?( Y )? f ( Y )
?( Y )? f ( Y ) =
?
?
i
?(yi)
?
?
?
?
?
j
f (y j )
?
?
yi y j
   E.g. if inference is a hypergraph problem.
A Variational Approximation
GE Objective:
?
Can be hard to compute                   in gradient. 
Idea: use variational approximation
* Note: this is the PR objective *
q ( Y ) ? p?( Y | X )
max? , q (Y) L(?; D L )?D KL
?
q ( Y ) || p?( Y | X )
?
?
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
C ov( ? , f )
O GE = max
?
L (? ; D L )?
?
?
???? E p ?(Y|X) [ ?( X , Y )]
?
?
?
?
Approximating with the mode
PR Objective: 
sometimes minimizing the KL is hard.  
Idea: use hard assignment                               :
?
                                    becomes 
?
                                 becomes 
?
use EM-like procedure to optimize
Constraint Driven Learning Objective:
max? , q ( Y ) L(?; D L )?D KL
?
q ( Y ) || p?( Y | X )
?
?
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
q ( Y ) ? 1 ( Y = ?Y )
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
l og p ( ?Y )D KL
?
q ( Y ) || p?( Y | X )
?
l og p(?? | ?( X , ?Y ))
m ax
?, ?Y
L (? ; D L) + log p ?(
?Y ) + log p (??|?( X , ?Y ))
Visual Summary
Measurements
Generalized
Expectation
Distribution
Matching
Posterior
Regularization
Coupled Semi-
Supervised
Learning
Constraint
Driven
Learning
variational approximation;
Jensen?s inequality
variational
approximation
MAP
approximation
MAP
approximation
logE[ p N (??|?)] ? log p N (??|E[?])
Applications
?
Unstructured problems:
?
Document Classification
?
Sequence problems:
?
Information Extraction
?
Pos-Induction 
?
Word Alignment
?
Tree problems:
?
Grammar Induction
Document Classification
?
Model: Max. Entropy Classifier (Logistic Regression)
?
Challenge: What if we have no labeled data?
?
cannot use standard unsupervised learning:
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
Documents
Labels
p ?(y|x) =
exp(? ? f(x, y))
?
y
exp(? ? f(x, y))
?
y
p?( y | x )= 1
Labeled Features
?
often we can still provide some light supervision
?
prior knowledge: labeled features
?
formally: have an estimate of the distribution over labels 
for documents that contain word w:
?? w
newsgroups classification
baseball Mac politics
...
hit Apple senate
...
Braves Macintosh taxes
...
runs Powerbook liberal
...
sentiment polarity
positive negative
memorable terrible
perfect boring
exciting mess
Leveraging Labeled Features with GE
[Mann & McCallum 07], [Druck et al 08]
?
constraint feature: 
?
for a document x, returns a vector with a 1 in the lth 
position if y is the lth label and the word w is in x
?
expectation: label distribution for docs that contain w
?
GE penalty: KL divergence from target distribution
? w (x, y) = 1 (y = l )1( w ? x)
1
c w
?
x
E p? ( y | x ) [ ?w(x, y)]
D KL
?
??w||
1
c w
?
x
Ep?(y | x)[?w(x , y )]
?
User Experiments with Labeled Features
[Druck et al 08]
0 100 200 300 400 500 600 700 8000.4
0.5
0.6
0.7
0.8
0.9
1
labeling time in seconds
tes
ting
 ac
cur
acy
 
 
GEER
~2 minutes, 100 
features labeled 
(or skipped): 
82% accuracy
~15 minutes, 100 
documents labeled 
(or skipped):
78% accuracy
PC vs. Mac
complete set of 
labeled features
PC Mac
dos mac
ibm apple
hp quadra
dx
targets set with 
simple heuristic: 
majority label gets 
90% of mass
Experiments with Labeled Features
[Druck et al 08]
60
65
70
75
80
sentiment (50) webkb (100) newsgroups (500)
GE (model contains only labeled features)
GE (model also contains unlabeled features)
15x
3.5x
6.5x
learning about ?unlabeled features? through 
covariance improves generalization
estimated speed-up over 
labeling documents
Information Extraction: Example Tasks
?
citation extraction: 
?
apartment listing extraction: 
Detached single family house. 3 bedrooms 1 1/2 baths.  Almost 
1000 square feet in living area. 1 car garage. New pergo floor 
and tile kitchen floor. New interior/exterior paint. Close to 
shopping mall and bus stop. Near 101/280. Available July 1, 
2004. If you are interested, email for more details.
Cousot, P. and Cousot, R. 1978. Static determination of 
dynamic properties of recursive procedures. In Proceedings of 
the IFIP Conference on Programming Concepts, E. Neuhold, 
Ed. North-Holland Pub. Co., 237-277.
Information Extraction: Markov Models
?
models for sequence labeling based IE
?
Hidden Markov Model (HMM):  
?
Conditional Random Field (CRF):  
p?(y,x) = p?(y0)
N?
i=1
p?(yi|yi?1)p?( x i|yi)
p?(y|x) =
1
Z(x)
exp(
N?
i =1
? ? f (x, yi? 1 , yi))
expectation:
label distribution when q is true
model: Linear Chain CRF
note: Semiring trick makes GE 
O(L
2
) instead of O(L
3
) as in 
[Mann & McCallum 08]
Information Extraction: Labeled Features
[Mann & McCallum 08], [Liang et al 09]
ROOMMATES respectful
CONTACT *phone*
FEATURES laundry
apartments example 
labeled features:
1
c q
?
x
?
i
E p?(yi | x )[?q(x, yi, i )]
constraint features:
vector with a 1 in the lth 
position if y is the lth label 
and predicate q is true (i.e. w 
is present at i)
? q (x, yi , i) = 1 (y i = l)q(x, i)
Information Extraction: Labeled Features
[Haghighi & Klein 06], [Mann & McCallum 08], [Liang et al 09]
apartment listing extraction
Prototype
GE (KL)
Measurements/PR
650
700
750
800
850
0 labeled 10 labeled 100 labeled
supervised CRF (100) [MM08]
?
accurate with constraints alone 
?
outperform fully supervised with 
constraints and labeled data
Limitations of Markov Models
?
predicted: 
?
prediction has two author and two title segments:
?
error #1: Neuhold, Ed. should be editor
?
error #2: North-Holland Pub. Co., should be 
publisher
?
A Markov model cannot represent that at most one segment 
of each type appears in each reference.
Cousot, P. and Cousot, R. 1978. Static determination of 
dynamic properties of recursive procedures. In Proceedings of 
the IFIP Conference on Programming Concepts, E. Neuhold, 
Ed. North-Holland Pub. Co., 237-277.
Long-Range Constraints
[Chang et al 07] [Bellare et al 09]
?
?Each field is a contiguous sequence of tokens and appears 
at most once in a citation.?
?
constraint feature: counts the number of segments of 
each type
?
constrained to be ? 1 using PR or CODL
?
additional constraints: 10 labeled features such as:
?
pages?pages 
?
proc.?booktitle
Long-Range Constraints
[Chang et al 07] [Bellare et al 09]
constraints improve both 
CRF (PR) and HMM (CODL)
50
60
70
80
90
5 labeled 20 labeled
CRF CRF + PR
HMM HMM + CODL
citation model method description
[Mann et al 07] MaxEnt GE
constraints on 
label marginals
[Druck et al 09] CRF GE
actively labeled 
features
[Bellare & 
McCallum 09] 
alignment 
CRF
GE labeled features
[Singh et al 10] 
semi-Markov 
CRF
PR labeled gazetteers 
[Druck et al 10] HMM PR
constraints derived 
from labeled data
Other Applications in 
Information Extraction
Pos Induction
Low Tag Ambiguity
[Gra?a et al 09] 
JJ
VB
NN
car
object
romantic
offensive
being
E[degree] = 1.5E[degree] = 10000  0 2 4 6 8 10  0  200  400  600  800  1000 1200 1400 1600 1800L 1L ! rank of word by L1L!SupervisedHMMDistribution of word ambiguity
N V ADJ Prep ADV
0.9 0.1 0 0 0
0.7 0.1 0.1 0 0.1
0.1 0.3 0 0.6 0
0.3 0.6 0 0 0.1
0.3 0.7 0 0 0
?
Pick a particular word type: run
?
Stack all occurrences
?
Calculate posterior probability 
?
Take the maximum for each tag
?
Sum the maxes
a run into town.
of the mile run.
run gold.
run errands.
run for mayor.
Sum
1
Sum
1
1
1
1
0.9 0.7 0.1 0.6 0.2
Max
Sum
2.5
Measuring Tag Ambiguity
[Gra?a et al 09] 
?wti :Word type w  has hidden state t at occurrence i
m i n
cwt
E q ( y ) [?wti] ? c wt
?1 / ?? =
?
w t
c w t
Tag Sparsity
[Gra?a et al 09] 
0
1.25
2.5
3.75
5
En Pt Es
A
m
b
i
g
u
i
t
y
 
d
i
f
f
e
r
e
n
c
e
HMM L1LMax
Average ambiguity 
difference 0 2 4 6 8 10  0  200  400  600  800  1000 1200 1400 1600 1800L 1L ! rank of word by L1L!SupervisedHMMHMM+SpDistribution of word ambiguity
Results
[Gra?a et al 09] 
50
57.5
65
72.5
80
En Pt Bg Es Dk Tr
HMM HMM+Sp
3.8
6.7
7.4
7.6 9.6
3.8
6.5 % Average Improvement
Word Alignments
[Gra?a et al 10] 
?
Bijectivity constraints:
?
Each word should align to at most one other word
?
Symmetry constraints:
?
Directional models should agree
Bijectivity Constraints
[Gra?a et al 10]
Bijective Constraints
0 1 2 3 4 5 6 7 8
0 ? ? ? ? ? ? ? ? ? jugaban ? ? 1
1 ? ? ? ? ? ? ? ? ? de ? ? 1
2 ? ? ? ? ? ? ? ? ? una ? ? 1
3 ? ? ? ? ? ? ? ? ? manera ? ? 1
4 ? ? ? ? ? ? ? ? ? animada ? ? 1
5 ? ? ? ? ? ? ? ? ? y ? ? 1
6 ? ? ? ? ? ? ? ? ? muy ? ? 1
7 ? ? ? ? ? ? ? ? ? cordial ? ? 1
8 ? ? ? ? ? ? ? ? ?. ? ? 1
it was an animated
, very convivial
game
.
50 / 74
Bijective Constraints - After projection
0 1 2 3 4 5 6 7 8
0 ? ? ? ? ? ? ? ? ? jugaban ? ? 1
1 ? ? ? ? ? ? ? ? ? de ? ? 1
2 ? ? ? ? ? ? ? ? ? una ? ? 1
3 ? ? ? ? ? ? ? ? ? manera ? ? 1
4 ? ? ? ? ? ? ? ? ? animada ? ? 1
5 ? ? ? ? ? ? ? ? ? y ? ? 1
6 ? ? ? ? ? ? ? ? ? muy ? ? 1
7 ? ? ? ? ? ? ? ? ? cordial ? ? 1
8 ? ? ? ? ? ? ? ? ? . ? ? 1
it was an animated
, very convivial
game
.
51 / 74Feature: 
Constraint: 
?(x,y) =
N?
i=1
1(yi = m)
E q [?(x , y)] ? 1
Symmetry Constraints
[Gra?a et al 10]
Feature:
Constraint: 
Sym metric - Original posteriors
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
p ? t
q
??
p ? t
??
p ? t
55 / 74
Eq [ ?(x , y)] = 0
?(x, y ) =
?
??
??
+ 1 y ?
??
y and
??
y i = j
? 1 y ?
??
y and
??
y j = i
0 otherwise
??
p ?( y | )
??
p ?( y )
Symmetry Constraints
[Gra?a et al 10]
Before projection: After projection:
Symmetric - After projection
E-Step qs(z) = arg min
q(z)? Q s
KL [qs(z) || p?t (z | xs)]
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
0 1 2 3 4
0 ? ? ? ? ? no
??q (z)1
? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
0 ? ? ? ? ? no
??q (z)1
? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
M-Step Does not change
56 / 74
??
p ?( y |x)
??
p ?( y |x
?
q ( y )
?
q ( y )
Results
[Gra?a et al 10]
 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMHMM 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMHMM
Evolution with data size 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM  50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM
? Specially useful for low data situations
6 1 / 74
Evolution with data size 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM  50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM
? Specially useful for low data situations
6 1 / 74
Results
[Gra?a et al 10]
 60 65 70
 75 80 85
 90 95
En-Pt Pt-En Pt-Fr Fr-Pt En-Es Es-En Es-Fr Fr-Es Pt-Es Es-Pt En-Fr Fr-EnLanguagesHMM70.5 67.5
73.0 77.6 75.7 74.9 80.9 84.0 82.4 79.8 76.3 78.3
B-HMM
85.0 74.4 71.3
86.3 88.4 87.2 87.2 86.5 82.5 90.1 90.8 91.6
S-HMM
86.2 85.0 82.4 87.9 82.7 84.6 89.1 88.9 84.6 91.8 93.4 9
4.6
Dependency Parsing
DMV Model
[Gra?a et al 04]
Dependency model with valence
(Klein and Manning, ACL 2004)
x
y
Regularization
N
creates
V
sparse
ADJ
grammars
N
p?(x, y) = ?root(V )
? ?stop(nostop|V ,right,false) ? ?child(N|V ,right)
? ?stop(stop|V ,right,true) ? ?stop(nostop|V ,left,false) ? ?child(N|V ,left)
. . .
3/9
Dependency Parsing
?
Transfer annotations from another language
?
[Ganchev et al 09]
?
Constrain the number of child/parent 
relations
?
[Gillenwater et al 11]
?
Use linguistic rules
?
[Druck et al 09] [Naseem et al 10]
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
?
Use information from a resource rich 
language
?
Make the annotation transfer robust
?
Preserve n % of the edges
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
E q [?(x,y)] =
1
| C
x
|
?
y? C x
q (y|x)
E q [ ?(x,y)] ? b
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
66
67
68
69
70
ES BG
DMV PR-Transfer
Dependency Parsing
Posterior Sparsity
[Gra?a et al 10]
?
ML learns very ambiguous grammars
?
all productions have some probability
?
constrain the number of possible 
productions
Dependency Parsing
Posterior Sparsity
[Gillenwater et al 11]
Measuring ambiguity on distributions over trees
N
?
N
V
?
N
AD
J
?
N
N
?
V
V
?
V
AD
J
?
V
N
?
AD
J
V
?
AD
J
AD
J
?
AD
J
SparsityN isV workingV
0.40.6 0 1 0
SparsityN isV workingV
0.4 0.6 .4 .6 0
UseV goodADJ grammarsN
0.70.3 0 .7 .3
UseV goodADJ grammarsN
0.40.6 .4 .6 0
max ?
sum = 3.3 ? 0 1 .3 .4 .6 0 .4 .6 0
7/9
Dependency Parsing
Posterior Sparsity
[Gillenwater et al 11]
GILLENWATER, GANCHEV, GRA?A, PEREIRA, TASKAR
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
1.00
1.00 1.000.49
0.51
1.00
0.57
0.43
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
1.00 0.83 0.75 0.990.92
0.35
0.48
Figure 14: Posterior edge probabilities for an example sentence from the Spanish test corpus. Top
is Gold, middle is EM, and bottom is PR.
since then it does not have to pay the cost of assigning a parent with a new tag to cover each noun
that does not come with a determiner.
Table 4 contrasts the most frequent types of errors EM, SDP, and PR make on several test sets
where PR does well. The ?acc? column is accuracy and the ?errs? column is the absolute number
of errors of the key type. Accuracy for the key ?parent POS truth/guess? child POS? is computed
as a function of the true relation. So, if the key is pt /p g ? c , then accuracy is:
acc =
# of pt ? c in Viterbi parses
# of pt ? c in gold parses
. (25)
In the following subsections we provide some analysis of the results from Table 4.
7.1 English Corrections
Considering English first, there are several notable differences between EM and PR errors. Similar
to the example for Spanish, the direction of the noun-determiner relation is corrected by PR. This is
reflected by the VB/DT? NN key, the NN/VBZ? DT key, the NN/IN? DT key, the IN/DT?
NN key, the NN/VBD? DT key, the NN/VBP? DT key, and the NN/VB? DT key, which for
EM and SDP have accuracy 0. PR corrects these errors.
A second correction PR makes is reflected in the VB/TO? VB key. One explanation for the
reason PR is able to correctly identify VBs as the parents of other VBs instead of mistakenly making
TO the parent of VBs is that ?VB CC VB? is a frequently occurring sequence. For example, ?build
and hold? and ?panic and bail? are two instances of the ?VB CC VB? pattern from the test corpus.
Presented with such scenarios, where there is no TO present to be the parent of VB, PR chooses the
first VB as the parent of the second. It maintains this preference for making the first VB a parent of
the second when encountered with ?VB TO VB? sequences, such as ?used to eliminate?, because it
would have to pay an additional penalty to make TO the parent of the second VB. In this manner,
PR corrects the VB/TO? VB key error of EM and SDP.
26
Gold:
DVM:
DMV+Sparsity:
Dependency Parsing
Posterior Sparsity
[Gillenwater t al. 11]
0
17.5
35
52.5
70
English Bulgarian Portuguese Checz Spanish German
DMV DMV+Sparsity
Dependency Parsing
Linguistic Rules
[Naseem et al 10]
Using Universal Linguistic Knowledge to Guide Grammar Induction
Tahira Naseem, Harr Chen, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{ tahira, harr, regina} @csail.mit.edu
Mark Johnson
Department of Computing
Macquarie University
mark.johnson@mq.edu.au
Abstract
We present an approach to grammar induc-
tion that utilizes syntactic universals to im-
prove dependency parsing across a range of
languages. Our method uses a single set
of manually-specified language-independent
rules that identify syntactic dependencies be-
tween pairs of syntactic categories that com-
monly occur across languages. During infer-
ence of the probabilistic model, we use pos-
terior expectation constraints to require that a
minimum proportion of the dependencies we
infer be instances of these rules. We also auto-
matically refine the syntactic categories given
in our coarsely tagged input. Across six lan-
guages our approach outperforms state-of-the-
art unsupervised methods by a significant mar-
gin.1
1 Introduction
Despite surface differences, human languages ex-
hibit striking similarities in many fundamental as-
pects of syntactic structure. These structural corre-
spondences, referred to as syntactic universals, have
been extensively studied in linguistics (Baker, 2001;
Carnie, 2002; White, 2003; Newmeyer, 2005) and
underlie many approaches in multilingual parsing.
In fact, much recent work has demonstrated that
learning cross-lingual correspondences from cor-
pus data greatly reduces the ambiguity inherent in
syntactic analysis (Kuhn, 2004; Burkett and Klein,
2008; Cohen and Smith, 2009a; Snyder et al, 2009;
Berg-Kirkpatrick and Klein, 2010).
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/dependency/
Root? Auxiliary Noun? Adjective
Root? Verb Noun? Article
Verb? Noun Noun? Noun
Verb? Pronoun Noun? Numeral
Verb? Adverb Preposition? Noun
Verb? Verb Adjective? Adverb
Auxiliary? Verb
Table 1: The manually-specified universal dependency
rules used in our experiments. These rules specify head-
dependent relationships between coarse (i.e., unsplit)
syntactic categories. An explanation of the ruleset is pro-
vided in Section 5.
In this paper, we present an alternative gram-
mar induction approach that exploits these struc-
tural correspondences by declaratively encoding a
small set of universal dependency rules. As input
to the model, we assume a corpus annotated with
coarse syntactic categories (i.e., high-level part-of-
speech tags) and a set of universal rules defined over
these categories, such as those in Table 1. These
rules incorporate the definitional properties of syn-
tactic categories in terms of their interdependencies
and thus are universal across languages. They can
potentially help disambiguate structural ambiguities
that are difficult to learn from data alone ? for
example, our rules prefer analyses in which verbs
are dependents of auxiliaries, even though analyz-
ing auxiliaries as dependents of verbs is also consis-
tent with the data. Leveraging these universal rules
has the potential to improve parsing performance
for a large number of human languages; this is par-
ticularly relevant to the processing of low-resource
Small set of 
universal rules
= 1 if edge in rule set
E q [ ?(x,y)] ? b
?(x , y)
Dependency Parsing
Linguistic Rules
[Nas em et al 10]
0
20
40
60
80
English Danish Portuguese Slovene Spanish Swedish
DMV DMV+Rules
Dependency Parsing:
Applications using Other Models
?
Tree CRF
?
[Druck et al 09]
?
MST Parser
?
[Ganchev et al 09]
Other Applications
?
Multi view learning:
?
[Ganchev et al 08]
?
Relation extraction:
?
[Chen et al 11]
Implementation Tips and Tricks
Off-the-Shelf Tools: MALLET
http://mallet.cs.umass.edu
?
off-the-shelf support for labeled features
?
models: MaxEnt Classifier, Linear Chain CRF (one and two 
label constraints)
?
methods: GE and PR
?
constraints on label distributions for input features
?
GE penalties:  KL divergence,     (+ soft inequalities)
?
PR penalties:     (+ soft inequalities)
?
in development: Tree CRF,      and other penalties
?
2
2
?
2
2
?1
Off-the-Shelf Tools: MALLET
http://mallet.cs.umass.edu
?
import data in SVMLight-like or CoNLL03-like formats
?
import constraints in a simple text format:
?
easily specify method options (i.e. SimpleTagger):
positive interesting:2 film:1 ...
negative tired:1 sequel:1 ...
positive best:1 recommend:2 ...
U.N.       NNP  B-NP  B-ORG 
official   NN   I-NP  O 
heads      VBZ  B-VP  O 
tired negative:0.8 positive:0.2
best positive:0.9 negative:0.1
U.N. B-ORG:0.7,0.9
B-VP O:0.95,
java cc.mallet.fst.semi_supervised.tui.SemiSupSimpleTagger \
--train true --test lab --loss l2 --learning ge \
unlabeled.txt test.txt constraints.txt
New GE Constraints: MALLET
http://mallet.cs.umass.edu
?
Java Interfaces for implementing new GE constraints
?
covariance computation implemented (MaxEnt, CRF)
?
primarily need to write methods to:
?
restriction: constraints must factor with model
?
restriction: GE objective must be differentiable
compute constraint features and expectations
compute GE objective value
compute GE objective gradient (but not covariance)
New PR Constraints: MALLET
http://mallet.cs.umass.edu
?
Java Interfaces for implementing new PR constraints
?
inference algorithms implemented (MaxEnt, CRF)
?
primarily need to write methods for E-step (projection):
?
restriction: constraints must factor with model
compute constraint features and expectations
compute scores under q for E-step
compute objective function for E-step
compute gradient for E-step
GE Implementation Advice
?
computing covariance (required for gradient): 
?
trick: compute cov. of composite constraint feature
?
example:     penalty: 
?
result: only need to store vectors of size            in 
computation, rather than covariance matrix
?
trick: efficient gradient computation in hypergraphs
?
use semiring algorithms of [Li & Eisner 09] 
?
result: same time complexity as supervised (w. both)
? c (x , y) =
?
?
2( ??? E [?])?(x , y)?22
d i m( f )
GE Implementation Advice
?
parameter regularization: 
?
    regularization encourages bootstrapping by penalizing 
very large parameter values:
?
optimization: non-convex
?
usually L-BFGS still preferable (use ?restart trick?)
?
zero initialization usually works well
?
other init: supervised, MaxEnt, GE in simpler model
?
2
2
>
Off-the-Shelf Tools: PR Toolkit
http://code.google.com/p/pr-toolkit/
?
off-the-shelf support for PR
?
models:  
?
MaxEnt Classifier, HMM,DMV
?
applications:  
?
Word Alignment, Pos Induction, Grammar Induction
?
constraints: posterior sparsity, bijectivity, agreement
?
No command line mode
?
Smaller support base
PR Implementation example:
Word Alignment - Bijectivity
?
Learning: EM, PR
?
void eStep(counts, lattices);
?
void mStep(counts);
?
lattice constraint.project(lattice);
?
Model: HMM
?
lattice computePosteriors(lattice);
?
void addCount(lattice, counts);
?
void updateParameters(counts);
?
Constraints: Bijectivity
?
lattice project(lattice);
PR Implementation example:
EM
class EM {
 model;
 	
void em(n){
 lattices= model.getLattices();
 counts = model.counts();	 	 	
 for(i=0; i< n; i++) {	 	 	
	 eStep(counts, lattices);
	 mStep(counts);
 }
}
	
void eStep(counts, lattices) {	
	 counts.clear();
	 for(l : lattices)  {		 	
	  model.computePosterior(l);
	  model.addCount(l,counts);	
	 }
}	
void mStep(counts) {
	 model.updateParameters(counts);
}
......
}
PR Implementation example:
PR
class PR {
 model;
 constraint;
	
void em(n){
 lattices= model.getLattices();
 counts = model.counts();	 	 	
 for(i=0; i< n; i++) {	 	 	
	 eStep(counts, lattices);
	 mStep(counts);
 }
}
	
void eStep(counts, lattices) {	
	 counts.clear();
	 for(l : lattices){	 	 	
	  model.computePosterior(l);
    constraint.project(l);
	  model.addCount(l,counts);	
	 }
}	
void mStep(counts) {
	 model.updateParameters(counts);
}
......
}
PR Implementation example:
HMM
class HMM {
 obsProb, transProbs,initProbs;
	
lattice computerPosteriors(lattice){
 ?Run forward backward?
}
	
void addCount(lattice,counts){
 ?Add posteriors to count table?
}
void updateParams(counts){
 ?Normalize counts?
 ?Copy counts to params table?
}
void getCounts(){
 ?return copy of params structures?
}
void getLattices(){
 ?return structure of all lattices 
in the corpus?
}
......
}
PR Implementation example:
Bijective constraints
?
Constraint: returns a vector with mth value = number of 
target words in sentence x that align with source word m
?(x,y) =
N?
i=1
1(yi = m) Q = { q : E q [?(x,y)] ? 1}
?
Primal: Hard
D KL ( Q| p?) = arg min
q
D KL ( q |p?)
?
Dual: Easy
arg max
?? 0
? b T ? ?? l og Z (?)? ||?|| 2
Z (?) =
?
y
p?( y |x) exp(?? ? ?(x , y ))
PR Implementation example:
Bijective Constraints
class BijectiveConstraints {
model;
lattice project(lattice){
 obj = BijectiveObj(model,lattice);
 Optimizer.optimize(obj);
}
	
}
class BijectiveObj {
  lattice;
  
void updateModel(newLambda){
 lattice_ = lattice*exp(newLambda);
 computerPosteriors(lattice)
}
double getObj(){
  obj = -dot(lambda,b);
  obj -= lattice.likelihood;
  obj -= l2Norm(lambda);
}
double[] getGrad(){
 grad = lattice.posteriors - b;
 grad -= norm(lambda);
 return grad;
}
Other Software Packages
?
Learning Based Java:  
?
http://cogcomp.cs.illinois.edu/page/software_view/11
?
support for Constraint-Driven Learning
?
Factorie:   
?
http://code.google.com/p/factorie/
?
support for GE and PR in development
Rich Prior Knowledge in Learning for Natural
Language Processing
Bibliography
For a more up-to-date bibliography as well as additional information about
these methods, point your browser to: http://sideinfo.wikkii.com/
1 Constraint-Driven Learning
Constraint driven learning (CoDL) was first introduced in Chang et al [2007],
and has been used also in Chang et al [2008]. A further paper on the topic is
in submission [Chang et al, 2010].
2 Generalized Expectation
Generalized Expectation (GE) constraints were first introduced by Mann and
McCallum [2007] 1 and were used to incorporate prior knowledge about the label
distribution into semi-supervised classification. GE constraints have also been
used to leverage ?labeled features? in document classification [Druck et al, 2008]
and information extraction [Mann and McCallum, 2008, Druck et al, 2009b,
Bellare and McCallum, 2009], and to incorporate linguistic prior knowledge
into dependency grammar induction [Druck et al, 2009a].
3 Posterior Regularization
The most clearly written overview of Posterior Regularization (PR) is Ganchev
et al [2010]. PR was first introduced in Graca et al [2008], and has been
applied to dependency grammar induction [Ganchev et al, 2009, Gillenwater
et al, 2009, 2011, Naseem et al, 2010], part of speech induction [Grac?a et al,
2009a], multi-view learning [Ganchev et al, 2008], word alignment [Graca et al,
2008, Ganchev et al, 2009, Grac?a et al, 2009b], and cross-lingual semantic
alignment [Platt et al, 2010]. The framework was independently discovered
by Bellare et al [2009] as an approximation to GE constraints, under the name
Alternating Projections, and used under that name also by Singh et al [2010]
and Druck and McCallum [2010] for information extraction. The framework
was also independently discovered by Liang et al [2009] as an approximation to
1In Mann and McCallum [2007] the method was called Expectation Regularization.
a Bayesian model motivated by modeling prior information as measurements,
and applied to information extraction.
4 Closely related frameworks
Quadrianto et al [2009] introduce a distribution matching framework very
closely related to GE constraints, with the idea that the model should pre-
dict the same feature expectations on labeled and undlabeled data for a set of
features, formalized as a kernel.
Carlson et al [2010] introduce a framework for semi-supervised learning
based on constraints, and trained with an iterative update algorithm very similar
to CoDL, but introducing only confident constraints as the algorithm progresses.
Gupta and Sarawagi [2011] introduce a framework for agreement that is
closely related to the PR-based work in Ganchev et al [2008], with a slightly
different objective and a different training algorithm.
References
K. Bellare, G. Druck, and A. McCallum. Alternating projections for learning
with expectation constraints. In Proc. UAI, 2009.
Kedar Bellare and Andrew McCallum. Generalized expectation criteria for boot-
strapping extractors using record-text alignment. In EMNLP, pages 131?140,
2009.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka Jr.,
and Tom M. Mitchell. Coupled Semi-Supervised Learning for Information
Extraction. In Proceedings of the Third ACM International Conference on
Web Search and Data Mining (WSDM), 2010.
M. Chang, L. Ratinov, and D. Roth. Guiding semi-supervision with constraint-
driven learning. In Proc. ACL, 2007.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. Structured learning with con-
strained conditional models. 2010. In submission.
M.W. Chang, L. Ratinov, N. Rizzolo, and D. Roth. Learning and inference
with constraints. In Proceedings of the National Conference on Artificial
Intelligence (AAAI). AAAI, 2008.
G. Druck, G. Mann, and A. McCallum. Learning from labeled features using
generalized expectation criteria. In Proc. SIGIR, 2008.
G. Druck, G. Mann, and A. McCallum. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Proc. ACL-IJCNLP, 2009a.
Gregory Druck and Andrew McCallum. High-performance semi-supervised
learning using discriminatively constrained generative models. In Proceedings
of the International Conference on Machine Learning (ICML 2010), pages
319?326, 2010.
Gregory Druck, Burr Settles, and Andrew McCallum. Active learning by label-
ing features. In EMNLP, pages 81?90, 2009b.
K. Ganchev, J. Grac?a, J. Blitzer, and B. Taskar. Multi-view learning over
structured and non-identical outputs. In Proc. UAI, 2008.
K. Ganchev, J. Gillenwater, and B. Taskar. Dependency grammar induction via
bitext projection constraints. In Proc. ACL-IJCNLP, 2009.
Kuzman Ganchev, Joo Graa, Jennifer Gillenwater, and Ben Taskar. Posterior
sparsity in unsupervised dependency parsing. Journal of Machine Learn-
ing Research, 11:2001?2049, July 2010. URL http://jmlr.csail.mit.edu/
papers/v11/ganchev10a.html.
Jennifer Gillenwater, Kuzman Ganchev, Joo Graa, Ben Taskar, and Fernando
Pereira. Sparsity in grammar induction. In NIPS Workshop on Grammar
Induction, Representation of Language and Language Learning, 2009.
Jennifer Gillenwater, Kuzman Ganchev, Joo Graa, Fernando Pereira, and Ben
Taskar. Posterior sparsity in unsupervised dependency parsing. Journal of
Machine Learning Research, 12:455?490, February 2011. URL http://jmlr.
csail.mit.edu/papers/v12/gillenwater11a.html.
Joao Graca, Kuzman Ganchev, and Ben Taskar. Expectation maximization
and posterior constraints. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing Systems 20, pages 569?
576. MIT Press, Cambridge, MA, 2008.
J. Grac?a, K. Ganchev, F. Pereira, and B. Taskar. Parameter vs. posterior
sparisty in latent variable models. In Proc. NIPS, 2009a.
J. Grac?a, K. Ganchev, and B. Taskar. Postcat - posterior constrained alignment
toolkit. In The Third Machine Translation Marathon, 2009b.
Rahul Gupta and Sunita Sarawagi. Joint training for open-domain extraction
on the web: exploiting overlap when supervision is limited. In Proceedings of
the Fourth ACM International Conference on Web Search and Data Mining
(WSDM), 2011.
P. Liang, M. I. Jordan, and D. Klein. Learning from measurements in exponen-
tial families. In Proc. ICML, 2009.
G. S. Mann and A. McCallum. Simple, robust, scalable semi-supervised learning
via expectation regularization. In Proc. ICML, 2007.
G. S. Mann and A. McCallum. Generalized expectation criteria for semi-
supervised learning of conditional random fields. In Proc. ACL, 2008.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. Using uni-
versal linguistic knowledge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing,
pages 1234?1244, Cambridge, MA, October 2010. Association for Computa-
tional Linguistics. URL http://www.aclweb.org/anthology/D10-1120.
John Platt, Kristina Toutanova, and Wen-tau Yih. Translingual document rep-
resentations from discriminative projections. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language Processing, pages 251?261,
Cambridge, MA, October 2010. Association for Computational Linguistics.
URL http://www.aclweb.org/anthology/D10-1025.
Novi Quadrianto, James Petterson, and Alex Smola. Distribution matching for
transduction. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams,
and A. Culotta, editors, Advances in Neural Information Processing Systems
22, pages 1500?1508. MIT Press, 2009.
Sameer Singh, Dustin Hillard, and Chris Leggetter. Minimally-supervised ex-
traction of entities from text advertisements. In Human Language Tech-
nologies: The 2010 Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages 73?81, Los Angeles,
California, June 2010. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/N10-1009.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 545?553,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Spice it Up? Mining Refinements to Online
Instructions from User Generated Content
Gregory Druck
Yahoo! Research
gdruck@gmail.com
Bo Pang
Yahoo! Research
bopang42@gmail.com
Abstract
There are a growing number of popular web
sites where users submit and review instruc-
tions for completing tasks as varied as build-
ing a table and baking a pie. In addition to pro-
viding their subjective evaluation, reviewers
often provide actionable refinements. These
refinements clarify, correct, improve, or pro-
vide alternatives to the original instructions.
However, identifying and reading all relevant
reviews is a daunting task for a user. In this
paper, we propose a generative model that
jointly identifies user-proposed refinements in
instruction reviews at multiple granularities,
and aligns them to the appropriate steps in the
original instructions. Labeled data is not read-
ily available for these tasks, so we focus on
the unsupervised setting. In experiments in the
recipe domain, our model provides 90.1% F1
for predicting refinements at the review level,
and 77.0% F1 for predicting refinement seg-
ments within reviews.
1 Introduction
People turn to the web to seek advice on a wide
variety of subjects. An analysis of web search
queries posed as questions revealed that ?how to?
questions are the most popular (Pang and Kumar,
2011). People consult online resources to answer
technical questions like ?how to put music on my
ipod,? and to find instructions for tasks like tying
a tie and cooking Thanksgiving dinner. Not sur-
prisingly, there are many Web sites dedicated to
providing instructions. For instance, on the pop-
ular DIY site instructables.com (?share what you
make?), users post instructions for making a wide
variety of objects ranging from bed frames to ?The
Stirling Engine, absorb energy from candles, coffee,
and more!1? There are also sites like allrecipes.com
that are dedicated to a specific domain. On these
community-based instruction sites, instructions are
posted and reviewed by users. For instance, the
aforementioned ?Stirling engine? has received over
350 reviews on instructables.com.
While user-generated instructions greatly increase
the variety of instructions available online, they
are not necessarily foolproof, or appropriate for all
users. For instance, in the case of recipes, a user
missing a certain ingredient at home might wonder
whether it can be safely omitted; a user who wants
to get a slightly different flavor might want to find
out what substitutions can be used to achieve that ef-
fect. Reviews posted by other users provide a great
resource for mining such information. In recipe re-
views, users often offer their customized version of
the recipe by describing changes they made: e.g., ?I
halved the salt? or ?I used honey instead of sugar.?
In addition, they may clarify portions of the instruc-
tions that are too concise for a novice to follow, or
describe changes to the cooking method that result
in a better dish. We refer to such actionable infor-
mation as a refinement.
Refinements can be quite prevalent in instruction
reviews. In a random sample of recipe reviews
from allrecipes.com, we found that 57.8% contain
refinements of the original recipe. However, sift-
ing through all reviews for refinements is a daunting
1http://www.instructables.com/id/
The-Sterling-Engine-absorb-energy-from-candles-c
545
task for a user. Instead, we would like to automat-
ically identify refinements in reviews, summarize
them, and either create an annotated version of the
instructions that reflects the collective experience of
the community, or, more ambitiously, revise the in-
structions directly.
In this paper, we take first steps toward these goals
by addressing the following tasks: (1) identifying re-
views that contain refinements, (2) identifying text
segments within reviews that describe refinements,
and (3) aligning these refinement segments to steps
in the instructions being reviewed (Figure 1 provides
an example). Solving these tasks provides a foun-
dation for downstream summarization and seman-
tic analysis, and also suggests intermediate applica-
tions. For example, we can use review classifica-
tion to filter or rank reviews as they are presented to
future users, since reviews that contain refinements
are more informative than a review which only says
?Great recipe, thanks for posting!?
To the best of our knowledge, no previous work
has explored this aspect of user-generated text.
While review mining has been studied extensively,
we differ from previous work in that instead of fo-
cusing on evaluative information, we focus action-
able information in the reviews. (See Section 2 for a
more detailed discussion.)
There is no existing labeled data for the tasks of
interest, and we would like the methods we develop
to be easily applied in multiple domains. Motivated
by this, we propose a generative model for solving
these tasks jointly without labeled data. Interest-
ingly, we find that jointly modeling refinements at
both the review and segment level is beneficial. We
created a new recipe data set, and manually labeled
a random sample to evaluate our model and several
baselines. We obtain 90.1% F1 for predicting refine-
ments at the review level, and 77.0% F1 for predict-
ing refinement segments within reviews.
2 Related Work
At first glance, the task of identifying refinements
appears similar to subjectivity detection (see (Pang
and Lee, 2008) for a survey). However, note that an
objective sentence is not necessarily a refinement:
e.g., ?I took the cake to work?; and a subjective sen-
tence can still contain a refinement: e.g., ?I reduced
the sugar and it came out perfectly.?
Our end goal is similar to review summarization.
However, previous work on review summarization
(Hu and Liu, 2004; Popescu and Etzioni, 2005; Titov
and McDonald, 2008) in product or service domains
focused on summarizing evaluative information ?
more specifically, identifying ratable aspects (e.g.,
?food? and ?service? for restaurants) and summariz-
ing the overall sentiment polarity for each aspect. In
contrast, we are interested in extracting a subset of
the non-evaluative information. Rather than ratable
aspects that are common across the entire domain
(e.g., ?ingredient?, ?cooking method?), we are in-
terested in actionable information that is related and
specific to the subject of the review.
Note that while our end goal is to summa-
rize objective information, it is still very differ-
ent from standard multi-document summarization
(Radev et al, 2002) of news articles. Apart from
differences in the quantity and the nature of the in-
put, we aim to summarize a distribution over what
should or can be changed, rather than produce a con-
sensus using different accounts of an event. In terms
of modeling approaches, in the context of extractive
summarization, Barzilay and Lee (2004) model con-
tent structure (i.e., the order in which topics appear)
in documents. We also model document structure,
but we do so to help identify refinement segments.
We share with previous work on predicting re-
view quality or helpfulness an interest in identify-
ing ?informative? text. Early work tried to exploit
the intuition that a helpful review is one that com-
ments on product details. However, incorporating
product-aspect-mention count (Kim et al, 2006) or
similarity between the review and product specifi-
cation (Zhang and Varadarajan, 2006) as features
did not seem to improve the performance when the
task was predicting the percentage of helpfulness
votes. Instead of using the helpfulness votes, Liu
et al (2007) manually annotated reviews with qual-
ity judgements, where a best review was defined as
one that contains complete and detailed comments.
Our notion of informativeness differs from previ-
ous work. We do not seek reviews that contain de-
tailed evaluative information; instead, we seek re-
views that contain detailed actionable information.
Furthermore, we are not expecting any single review
to be comprehensive; rather, we seek to extract a
546
collection of refinements representing the collective
wisdom of the community.
To the best of our knowledge, there is little pre-
vious work on mining user-generated data for ac-
tionable information. However, there has been in-
creasing interest in language grounding. In partic-
ular, recent work has studied learning to act in an
external environment by following textual instruc-
tions (Branavan et al, 2009, 2010, 2011; Vogel and
Jurafsky, 2010). This line of research is complemen-
tary to our work. While we do not utilize extensive
linguistic knowledge to analyze actionable informa-
tion, we view this is an interesting future direction.
We propose a generative model that makes pre-
dictions at both the review and review segment level.
Recent work uses a discriminative model with a sim-
ilar structure to perform sentence-level sentiment
analysis with review-level supervision (Ta?ckstro?m
and McDonald, 2011). However, sentiment polarity
labels at the review level are easily obtained. In con-
trast, refinement labels are not naturally available,
motivating the use of unsupervised learning. Note
that the model of Ta?ckstro?m and McDonald (2011)
cannot be used in a fully unsupervised setting.
3 Refinements
In this section, we define refinements more pre-
cisely. We use recipes as our running example, but
our problem formulation and models are not specific
to this domain.
A refinement is a piece of text containing action-
able information that is not entailed by the original
instructions, but can be used to modify or expand the
original instructions. A refinement could propose an
alternative method or an improvement (e.g., ?I re-
placed half of the shortening with butter?, ?Let the
shrimp sit in 1/2 marinade for 3 hours?), as well as
provide clarification (?definitely use THIN cut pork
chops, otherwise your panko will burn before your
chops are cooked?).
Furthermore, we distinguish between a verified
refinement (what the user actually did) and a hy-
pothetical refinement (?next time I think I will try
evaporated milk?). In domains similar to recipes,
where instructions may be carried out repeatedly,
there exist refinements in both forms. Since instruc-
tions should, in principle, contain information that
has been well tested, in this work, we consider only
the former as our target class. In a small percent-
age of reviews we observed ?failed attempts? where
a user did not follow a certain step and regretted the
diversion. In this work, we do not consider them to
be refinements. We refer to text that does not contain
refinements as background.
Finally, we note that the presence of a past tense
verb does not imply a refinement (e.g., ?Everyone
loved this dish?, ?I got many compliments?). In fact,
not all text segments that describe an action are re-
finements (e.g., ?I took the cake to work?, ?I fol-
lowed the instructions to a T?).
4 Models
In this section we describe our models. To iden-
tify refinements without labeled data, we propose
a generative model of reviews (or more gener-
ally documents) with latent variables. We assume
that each review x is divided into segments, x =
(x1, . . . ,xT ). Each segment is a sub-sentence-level
text span. We assume that the segmentation is ob-
served, and hence it is not modeled. The segmenta-
tion procedure we use is described in Section 5.1.
While we focus on the unsupervised setting, note
that the model can also be used in a semi-supervised
setting. In particular, coarse (review-level) labels
can be used to guide the induction of fine-grained
latent structure (segment labels, alignments).
4.1 Identifying Refinements
We start by directly modeling refinements at the seg-
ment level. Our first intuition is that refinement and
background segments can often be identified by lex-
ical differences. Based on this intuition, we can ig-
nore document structure and generate the segments
with a segment-level mixture of multinomials (S-
Mix). In general we could use n multinomials to
represent refinements and m multinomials to repre-
sent background text, but in this paper we simply use
n=m= 1. Therefore, unsupervised learning in S-
Mix can be viewed as clustering the segments with
two latent states. As is standard practice in unsu-
pervised learning, we subsequently map these latent
states onto the labels of interest: r and b, for refine-
ment and background, respectively. Note, however,
that this model ignores potential sequential depen-
547
dencies among segments. A segment following a re-
finement segment in a review may be more likely to
be a refinement than background, for example.
To incorporate this intuition, we could instead
generate reviews with a HMM (Rabiner, 1989) over
segments (S-HMM) with two latent states. Let zi
be the latent label variable for the ith segment. The
joint probability of a review and segment labeling is
p(x, z;?) =
T?
j=1
p(zj |zj?1;?)p(xj |zj ;?), (1)
where p(zj |zj?1;?) are multinomial transition dis-
tributions, allowing the model to learn that p(zj =
r|zj?1 = r;?) > p(zj = b|zj?1 = r;?) as moti-
vated above, and p(xj |zj ;?) are multinomial emis-
sion distributions. Note that all words in a segment
are generated independently conditioned on zj .
While S-HMM models sequential dependencies,
note that it imposes the same transition probabili-
ties on each review. In a manually labeled random
sample of recipe reviews, we find that refinement
segments tend to be clustered together in certain re-
views (?bursty?), rather than uniformly distributed
across all reviews. Specifically, while we estimate
that 23% of all segments are refinements, 42% of
reviews do not contain any refinements. In reviews
that contain a refinement, 34% of segments are re-
finements. S-HMM cannot model this phenomenon.
Consequently, we extend S-HMM to include a la-
tent label variable y for each review that takes val-
ues yes (contains refinement) and no (does not con-
tain refinement). The extended model is a mixture
of HMMs (RS-MixHMM) where y is the mixture
component.
p(x, y, z;?) = p(y;?)p(x, z|y;?) (2)
The two HMMs p(x, z | y=yes;?) and p(x, z | y=
no;?) can learn different transition multinomials
and consequently different distributions over z for
different y. On the other hand, we do not believe
the textual content of the background segments in a
y = yes review should be different from those in
a y = no review. Thus, the emission distributions
are shared between the two HMMs, p(xj |zj , y;?) =
p(xj |zj ;?).
Note that the definition of y imposes additional
constraints on RS-MixHMM: 1) reviews with y=no
cannot contain refinement segments, and 2) reviews
with y = yes must contain at least one refinement
segment. We enforce constraint (1) by disallow-
ing refinement segments zj = r when y = no:
p(zj = r|zj?1, y = no;?) = 0. Therefore, with
one background label, only the all background la-
bel sequence has non-zero probability when y=no.
Enforcing constraint (2) is more challenging, as the
y = yes HMM must assign zero probability when
all segments are background, but permit background
segments when refinement segments are present.
To enforce constraint (2), we ?rewire? the HMM
structure for y = yes so that a path that does not
go through the refinement state r is impossible. We
first expand the state representation by replacing b
with two states that encode whether or not the first
r has been encountered yet: bnot?yet encodes that
all previous states in the path have also been back-
ground; bok encodes that at least one refinement state
has been encountered2. We prohibit paths from end-
ing with bnot?yet by augmenting RS-MixHMM with
a special final state f , and fixing p(zT+1 = f |zT =
bnot?yet, y = yes;?) = 0. Furthermore, to enforce
the correct semantics of each state, paths cannot start
with bok, p(z1 = bok|y = yes;?) = 0, and transi-
tions from bnot?yet to bok, bok to bnot?yet, and r to
bnot?yet are prohibited.
Note that RS-MixHMM also generalizes to the
case where there are multiple refinement (n>1) and
background (m > 1) labels. Let Zr be the set of
refinement labels, and Zb be the set of background
labels. The transition structure is analogous to the
n= m= 1 case, but statements involving r are ap-
plied for each z ? Zr, and statements involving b are
applied for each z ? Zb. For example, the y = yes
HMM contains 2|Zb| background states.
In summary, the generative process of RS-
MixHMM involves first selecting whether the re-
view will contain a refinement. If the answer is yes,
a sequence of background segments and at least one
refinement segment are generated using the y= yes
HMM. If the answer is no, only background seg-
ments are generated. Interestingly, by enforcing
constraints (1) and (2), we break the label symme-
try that necessitates mapping latent states onto labels
2In this paper, the two background states share emission
multinomials, p(xj |zj = bnot?yet;?) = p(xj |zj = bok;?),
though this is not required.
548
when using S-Mix and S-HMM. Indeed, in the ex-
periments we present in Section 5.3, mapping is not
necessary for RS-MixHMM.
Note that the relationship between document-
level labels and segment-level labels that we model
is related to the multiple-instance setting (Dietterich
et al, 1997) in the machine learning literature. In
multiple-instance learning (MIL), rather than having
explicit labels at the instance (e.g., segment) level,
labels are given for bags of instances (e.g., docu-
ments). In the binary case, a bag is negative only
if all of its instances are negative. While we share
this problem formulation, work on MIL has mostly
focussed on supervised learning settings, and thus
it is not directly applicable to our unsupervised set-
ting. Foulds and Smyth (2011) propose a generative
model for MIL in which the generation of the bag
label y is conditioned on the instance labels z. As a
result of this setup, their model reduces to our S-Mix
baseline in a fully unsupervised setting.
Finally, although we motivated including the
review-level latent variable y as a way to improve
segment-level prediction of z, note that predictions
of y are useful in and of themselves. They provide
some notion of review usefulness and can be used to
filter reviews for search and browsing. They addi-
tionally give us a way to measure whether a set of
instructions is often modified or performed as speci-
fied. Finally, if we want to provide supervision, it is
much easier to annotate whether a review contains a
refinement than to annotate each segment.
4.2 Alignment with the Instructions
In addition to the review x, we also observe the set of
instructions s being discussed. Often a review will
reference specific parts of the instructions. We as-
sume that each set of instructions is segmented into
steps, s = (s1, . . . , sS). We augment our model
with latent alignment variables a = (a1, . . . , aT ),
where aj = ` denotes that the jth review segment is
referring to the `th step of s. We also define a special
NULL instruction step. An alignment to NULL sig-
nifies that the segment does not refer to a specific in-
struction step. Note that this encoding assumes that
each review segment refers to at most one instruction
step. Alignment predictions could facilitate further
analysis of how refinements affect the instructions,
as well as aid in summarization and visualization of
refinements.
The joint probability under the augmented model,
which we refer to as RSA-MixHMM, is
p(a,x, y, z|s;?) = p(y;?)p(a,x, z|y, s;?) (3)
p(a,x, z|y, s;?) =
T?
j=1
p(aj , zj |aj?1, zj?1, y, s;?)
? p(xj |aj , zj , s;?).
Note that the instructions s are assumed to be ob-
served and hence are not generated by the model.
RSA-MixHMM can be viewed as a mixture of
HMMs where each state encodes both a segment la-
bel zj and an alignment variable aj . Encoding an
alignment problem as a sequence labeling problem
was first proposed by Vogel et al (1996). Note that
RSA-MixHMM uses a similar expanded state rep-
resentation and transition structure as RS-MixHMM
to encode the semantics of y.
In our current model, the transition probability de-
composes into the product of independent label tran-
sition and alignment transition probabilities
p(aj , zj |aj?1, zj?1, y, s;?) =p(aj |aj?1, y, s;?)
? p(zj |zj?1, y, s;?),
and p(aj |aj?1, y, s;?) = p(aj |y, s;?) simply en-
codes the probability that segments align to a (non-
NULL) instruction step given y. This allows the
model to learn, for example, that reviews that con-
tain refinements refer to the instructions more often.
Intuitively, a segment and the step it refers to
should be lexically similar. Consequently, RSA-
MixHMM generates segments using a mixture of the
multinomial distribution for the segment label zj and
the (fixed) multinomial distribution3 for the step saj .
In this paper, we do not model the mixture proba-
bility and simply assume that all overlapping words
are generated by the instruction step. When aj =
NULL, only the segment label multinomial is used.
Finally, we disallow an alignment to a non-NULL
step if no words overlap: p(xj |aj , zj , s;?) = 0.
4.3 Inference and Parameter Estimation
Because our model is tree-structured, we can
efficiently compute exact marginal distributions
3Stopwords are removed from the instruction step.
549
over latent variables using the sum-product algo-
rithm (Koller and Friedman, 2009). Similarly, to
find maximum probability assignments, we use the
max-product algorithm.
At training time we observe a set of re-
views and corresponding instructions, D =
{(x1, s1), . . . , (xN , sN )}. The other variables, y, z,
and a, are latent. For all models, we estimate param-
eters to maximize the marginal likelihood of the ob-
served reviews. For example, for RSA-MixHMM,
we estimate parameters using
argmax
?
N?
i=1
log
?
a,z,y
p(a,xi, y, z|si;?).
This problem cannot be solved analytically, so we
use the Expectation Maximization (EM) algorithm.
5 Experiments
5.1 Data
In this paper, we use recipes and reviews from
allrecipes.com, an active community where we es-
timate that the mean number of reviews per recipe is
54.2. We randomly selected 22,437 reviews for our
data set. Of these, we randomly selected a subset
of 550 reviews and determined whether or not each
contains a refinement, using the definition provided
in Section 3. In total, 318 of the 550 (57.8%) con-
tain a refinement. We then randomly selected 119 of
the 550 and labeled the individual segments. Of the
712 segments in the selected reviews, 165 (23.2%)
are refinements and 547 are background.
We now define our review segmentation scheme.
Most prior work on modeling latent document sub-
structure uses sentence-level labels (Barzilay and
Lee, 2004; Ta?ckstro?m and McDonald, 2011). In
the recipe data, we find that sentences often con-
tain both refinement and background segments: ?[I
used a slow cooker with this recipe and] [it turned
out great!]? Additionally, we find that sentences of-
ten contain several distinct refinements: ?[I set them
on top and around the pork and] [tossed in a can
of undrained french cut green beans and] [cooked
everything on high for about 3 hours].? To make re-
finements easier to identify, and to facilitate down-
stream processing, we allow sub-sentence segments.
Our segmentation procedure leverages a phrase
structure parser. In this paper we use the Stanford
Parser4. Based on a quick manual inspection, do-
main shift and ungrammatical sentences do cause
a significant degradation in parsing accuracy when
compared to in-domain data. However, this is ac-
ceptable because we only use the parser for segmen-
tation. We first parse the entire review, and subse-
quently iterate through the tokens, adding a segment
break when any of the following conditions is met:
? sentence break (determined by the parser)
? token is a coordinating conjunction (CC) with
parent other than NP, PP, ADJP
? token is a comma (,) with parent other than NP,
PP, ADJP
? token is a colon (:)
The resulting segmentations are fixed during learn-
ing. In future work we could extend our model to
additionally identify segment boundaries.
5.2 Experimental Setup
We first describe the methods we evaluate. For com-
parison, we provide results with a baseline that ran-
domly guesses according to the class distribution for
each task. We also evaluate a Review-level model:
? R-Mix: A review-level mixture of multinomi-
als with two latent states.
Note that this is similar to clustering at the review
level, except that class priors are estimated. R-Mix
does not provide segment labels, though they can be
obtained by labeling all segments with the review
label.
We also evaluate the two Segment-level models
described in Section 4.1 (with two latent states):
? S-Mix: A segment-level mixture model.
? S-HMM: A segment-level HMM (Eq. 1).
These models do not provide review labels. To ob-
tain them, we assign y = yes if any segment is la-
beled as a refinement, and y=no otherwise.
Finally, we evaluate three versions of our model
(Review + Segment and Review + Segment +
4http://nlp.stanford.edu/software/lex-parser.shtml
550
Alignment) with one refinement segment label and
one background segment label5:
? RS-MixHMM: A mixture of HMMs (Eq. 2)
with constraints (1) and (2) (see Section 4).
? RS-MixMix: A variant of RS-MixHMM with-
out sequential dependencies.
? RSA-MixHMM: The full model that also in-
corporates alignment (Eq. 3).
Segment multinomials are initialized with a small
amount of random noise to break the initial symme-
try. RSA-MixHMM segment multinomials are in-
stead initialized to the RS-MixHMM solution. We
apply add-0.01 smoothing to the emission multino-
mials and add-1 smoothing to the transition multi-
nomials in the M-step. We estimate parameters with
21,887 unlabeled reviews by running EM until the
relative percentage decrease in the marginal likeli-
hood is ? 10?4 (typically 10-20 iterations).
The models are evaluated on refinement F1 and
accuracy for both review and segment predictions
using the annotated data described in Section 5.1.
For R-Mix and the segment (S-) models, we select
the 1:1 mapping of latent states to labels that maxi-
mizes F1. For RSA-MixHMM and the RS- models
this was not necessary (see Section 4.1).
5.3 Results
Table 1 displays the results. R-Mix fails to ac-
curately distinguish refinement and background re-
views. The words that best discriminate the two
discovered review classes are ?savory ingredients?
(chicken, pepper, meat, garlic, soup) and ?bak-
ing/dessert ingredients? (chocolate, cake, pie, these,
flour). In other words, reviews naturally cluster by
topics rather than whether they contain refinements.
The segment models (S-) substantially outper-
form R-Mix on all metrics, demonstrating the ben-
efit of segment-level modeling and our segmenta-
tion scheme. However, S-HMM fails to model
the ?burstiness? of refinement segments (see Sec-
tion 4.1). It predicts that 76.2% of reviews con-
tain refinements, and additionally that 40.9% of seg-
ments contain refinements, whereas the true values
5Attempts at modeling refinement and background sub-
types by increasing the number of latent states failed to sub-
stantially improve the results.
are 57.8% and 23.2%, respectively. As a result, these
models provide high recall but low precision.
In comparison, our models, which model the re-
view labels6 y, yield more accurate refinement pre-
dictions. They provide statistically significant im-
provements in review and segment F1, as well as
accuracy, over the baseline models. RS-MixHMM
predicts that 62.9% of reviews contain refinements
and 28.2% of segments contain refinements, values
that are much closer to the ground truth. The re-
finement emission distributions for S-HMM and RS-
MixHMM are fairly similar, but the probabilities of
several key terms like added, used, and instead are
higher with RS-MixHMM.
The review F1 results demonstrate that our mod-
els are able to very accurately distinguish refinement
reviews from background reviews. As motivated in
Section 4.1, there are several applications that can
benefit from review-level predictions directly. Addi-
tionally, note that review labeling is not a trivial task.
We trained a supervised logistic regression model
with bag-of-words and length features (for both the
number of segments and the number of words) using
10-fold cross validation on the labeled dataset. This
supervised model yields mean review F1 of 78.4,
11.7 F1 points below the best unsupervised result7.
Augmenting RS-MixMix with sequential depen-
dencies, yielding RS-MixHMM, provides a mod-
erate (though not statistically significant) improve-
ment in segment F1. RS-MixHMM learns that re-
finement reviews typically begin and end with back-
ground segments, and that refinement segments tend
to appear in succession.
RSA-MixHMM additionally learns that segments
in refinement reviews are more likely to align to non-
NULL recipe steps. It also encourages the segment
multinomials to focus modeling effort on words that
appear only in the reviews. As a result, in addition to
yielding alignments, RSA-MixHMM provides small
improvements over RS-MixHMM (though they are
not statistically significant).
6We note that enforcing the constraint that a refinement re-
view must contain at least one refinement segment using the
method in Section 4.1 provides a statistically significant signif-
icant improvement in review F1 of 4.0 for RS-MixHMM.
7Note that we do not consider this performance to be the
upper-bound of supervised approaches; clearly, supervised ap-
proaches could benefit from additional labeled data. However,
labeled data is relatively expensive to obtain for this task.
551
Model
review (57.8% refinement) segment (23.2% refinement)
acc prec rec F1 acc prec rec F1
random baseline 51.2? 57.8 57.8 57.8? 64.4? 23.2 23.2 23.2?
R-Mix 61.5? 69.1 60.4 64.4? 55.8? 27.9 57.6 37.6?
S-Mix 77.5? 72.4 98.7 83.5? 80.6? 54.7 95.2 69.5?
S-HMM 79.8? 74.7 98.4 84.9? 80.3? 54.3 95.8 69.3?
RS-MixMix 87.1 85.4 93.7 89.4 86.4 65.6 86.7 74.7
RS-MixHMM 87.3 85.6 93.7 89.5 87.9 69.7 84.8 76.5
RSA-MixHMM 88.2 87.1 93.4 90.1 88.5 71.7 83.0 77.0
Table 1: Unsupervised experiments comparing models for review and segment refinement identification on the recipe
data. Bold indicates the best result, and a ? next to an accuracy or F1 value indicates that the improvements obtained
by RS-MixMix, RS-MixHMM, and RSA-MixHMM are significant (p = 0.05 according to a bootstrap test).
[ I loved these muffins! ] [ I used walnuts inside 
the batter and ] [ used whole wheat flour only 
as well as flaxseed instead of wheat germ. ] 
[ They turned out great! ] [ I couldn't stop eating 
them. ] [ I've made several batches of these 
muffins and all have been great. ] [ I make tiny 
alterations each time usually. ] [ These muffins 
are great with pears as well. ] [ I think golden 
raisins are much better than regular also! ]
1. Preheat oven to 375 degrees F (190 degrees C).
2. Lightly oil 18 muffin cups, or coat with nonstick 
cooking spray.
3. In a medium bowl, whisk together eggs, egg whites, 
apple butter, oil and vanilla.
4. In a large bowl, stir together flours, sugar, cinnamon, 
baking powder, baking soda and salt.
5. Stir in carrots, apples and raisins.
6. Stir in apple butter mixture until just moistened.
7. Spoon the batter into the prepared muffin cups, filling 
them about 3/4 full.
8. In a small bowl, combine walnuts and wheat germ; 
sprinkle over the muffin tops.
9. Bake at 375 degrees F (190 degrees C) for 15 to 20 
minutes, or until the tops are golden and spring back 
when lightly pressed.
Figure 1: Example output (best viewed in color). Bold segments in the review (left) are those predicted to be refine-
ments. Red indicates an incorrect segment label, according to our gold labels. Alignments to recipe steps (right) are
indicated with colors and arrows. Segments without colors and arrows align to the NULL recipe step (see Section 4.2).
We provide an example alignment in Figure 1.
Annotating ground truth alignments is challenging
and time-consuming due to ambiguity, and we feel
that the alignments are best evaluated via a down-
stream task. Therefore, we leave thorough evalua-
tion of the quality of the alignments to future work.
6 Conclusion and Future Work
In this paper, we developed unsupervised meth-
ods based on generative models for mining refine-
ments to online instructions from reviews. The pro-
posed models leverage lexical differences in refine-
ment and background segments. By augmenting the
base models with additional structure (review labels,
alignments), we obtained more accurate predictions.
However, to further improve accuracy, more lin-
guistic knowledge and structure will need to be in-
corporated. The current models provide many false
positives in the more subtle cases, when some words
that typically indicate a refinement are present, but
the text does not describe a refinement according to
the definition in Section 3. Examples include hypo-
thetical refinements (?next time I will substitute...?)
and discussion of the recipe without modification (?I
found it strange to... but it worked ...?, ?I love bal-
samic vinegar and herbs?, ?they baked up nicely?).
Other future directions include improving the
alignment model, for example by allowing words in
the instruction step to be ?translated? into words in
the review segment. Though we focussed on recipes,
the models we proposed are general, and could be
applied to other domains. We also plan to consider
this task in other settings such as online forums, and
develop methods for summarizing refinements.
Acknowledgments
We thank Andrei Broder and the anonymous reviewers
for helpful discussions and comments.
552
References
Regina Barzilay and Lillian Lee. Catching the drift:
Probabilistic content models, with applications to
generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120, 2004.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. Reinforcement learning for
mapping instructions to actions. In Proceedings
of the Association for Computational Linguistics
(ACL), 2009.
S.R.K Branavan, Luke Zettlemoyer, and Regina
Barzilay. Reading between the lines: Learning
to map high-level instructions to commands. In
Proceedings of the Association for Computational
Linguistics (ACL), 2010.
S.R.K. Branavan, David Silver, and Regina Barzilay.
Learning to win by reading manuals in a monte-
carlo framework. In Proceedings of the Associa-
tion for Computational Linguistics (ACL), 2011.
Thomas G. Dietterich, Richard H. Lathrop, and
Toma?s Lozano-Pe?rez. Solving the multiple in-
stance problem with axis-parallel rectangles. Ar-
tificial Intelligence, 89(1 - 2):31 ? 71, 1997.
J. R. Foulds and P. Smyth. Multi-instance mixture
models and semi-supervised learning. In SIAM
International Conference on Data Mining, 2011.
Minqing Hu and Bing Liu. Mining and summa-
rizing customer reviews. In Proceedings of the
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 168?177,
2004.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and
Marco Pennacchiotti. Automatically assessing re-
view helpfulness. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 423?430, 2006.
D. Koller and N. Friedman. Probabilistic Graphical
Models: Principles and Techniques. MIT Press,
2009.
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou
Huang, and Ming Zhou. Low-quality product
review detection in opinion summarization. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL), pages 334?342, 2007.
Bo Pang and Ravi Kumar. Search in the lost sense
of query: Question formulation in web search
queries and its temporal changes. In Proceedings
of the Association for Computational Linguistics
(ACL), 2011.
Bo Pang and Lillian Lee. Opinion mining and sen-
timent analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135, 2008.
Ana-Maria Popescu and Oren Etzioni. Extract-
ing product features and opinions from reviews.
In Proceedings of the Human Language Tech-
nology Conference and the Conference on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP), 2005.
Lawrence Rabiner. A tutorial on hidden markov
models and selected applications in speech recog-
nition. Proceedings of the IEEE, 77(2):257?286,
1989.
Dragomir R. Radev, Eduard Hovy, and Kathleen
McKeown. Introduction to the special issue on
summarization. Computational Linguistics, 28
(4):399?408, 2002. ISSN 0891-2017.
Oscar Ta?ckstro?m and Ryan McDonald. Discovering
fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the
33rd European conference on Advances in infor-
mation retrieval, ECIR?11, pages 368?374, 2011.
Ivan Titov and Ryan McDonald. A joint model of
text and aspect ratings for sentiment summariza-
tion. In Proceedings of the Association for Com-
putational Linguistics (ACL), 2008.
Adam Vogel and Daniel Jurafsky. Learning to fol-
low navigational directions. In Proceedings of the
Association for Computational Linguistics (ACL),
2010.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics - Volume 2, COL-
ING ?96, pages 836?841, 1996.
Zhu Zhang and Balaji Varadarajan. Utility scoring
of product reviews. In Proceedings of the ACM
SIGIR Conference on Information and Knowledge
Management (CIKM), pages 51?57, 2006.
553
