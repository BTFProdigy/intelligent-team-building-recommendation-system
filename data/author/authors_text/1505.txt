Proceedings of the ACL 2007 Demo and Poster Sessions, pages 45?48,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic enrichment of journal articles using chemical named entity
recognition
Colin R. Batchelor
Royal Society of Chemistry
Thomas Graham House
Milton Road
Cambridge
UK CB4 0WF
batchelorc@rsc.org
Peter T. Corbett
Unilever Centre for Molecular Science Informatics
University Chemical Laboratory
Lensfield Road
Cambridge
UK CB2 1EW
ptc24@cam.ac.uk
Abstract
We describe the semantic enrichment of journal
articles with chemical structures and biomedi-
cal ontology terms using Oscar, a program for
chemical named entity recognition (NER). We
describe how Oscar works and how it can been
adapted for general NER. We discuss its imple-
mentation in a real publishing workflow and pos-
sible applications for enriched articles.
1 Introduction
The volume of chemical literature published has ex-
ploded over the past few years. The crossover between
chemistry and molecular biology, disciplines which of-
ten study similar systems with contrasting techniques and
describe their results in different languages, has also in-
creased. Readers need to be able to navigate the literature
more effectively, and also to understand unfamiliar termi-
nology and its context. One relatively unexplored method
for this is semantic enrichment. Substructure and simi-
larity searching for chemical compounds is a particularly
exciting prospect.
Enrichment of the bibliographic data in an article with
hyperlinked citations is now commonplace. However,
the actual scientific content has remained largely unen-
hanced, this falling to secondary services and experimen-
tal websites such as GoPubMed (Delfs et al, 2005) or
EBIMed (Rebholz-Schuhmann et al, 2007). There are
a few examples of semantic enrichment on small (a few
dozen articles per year) journals such as Nature Chemi-
cal Biology being an example, but for a larger journal it
is impractical to do this entirely by hand.
This paper concentrates on implementing semantic
enrichment of journal articles as part of a publishing
workflow, specifically chemical structures and biomedi-
cal terms. In the Motivation section, we introduce Oscar
as a system for chemical NER and recognition of ontol-
ogy terms. In the Implementation section we will discuss
how Oscar works and how to set up ontologies for use
with Oscar, specifically GO. In the Case study section we
describe how the output of Oscar can be fed into a pub-
lishing workflow. Finally we discuss some outstanding
ambiguity problems in chemical NER. We also compare
the system to EBIMed (Rebholz-Schuhmann et al, 2007)
throughout.
2 Motivation
There are three routes for getting hold of chemical
structures from chemical text?from chemical compound
names, from author-supplied files containing connection
tables, and from images. The preferred representation
of chemical structures is as diagrams, often annotated
with curly arrows to illustrate the mechanisms of chem-
ical reactions. The structures in these diagrams are typ-
ically given numbers, which then appear in the text in
bold face. However, because text-processing is more ad-
vanced in this regard than image-processing, we shall
concentrate on NER, which is performed with a sys-
tem called Oscar. A preliminary overview of the sys-
tem was presented by Corbett and Murray-Rust (2006).
Oscar is open source and can be downloaded from
http://oscar3-chem.sourceforge.net/
As a first step in representing biomedical content, we
identify Gene Ontology (GO) terms in full text.1 (The
Gene Ontology Consortium, 2000) We have chosen a rel-
atively simple starting point in order to gain experience
in implementing useful semantic markup in a publishing
workflow without a substantial word-sense disambigua-
tion effort. GO terms are largely compositional (Mungall,
2004), hence incomplete matches will still be useful, and
that there is generally a low level of semantic ambiguity.
For example, there are only 133 single-word GO terms,
which significantly reduces the chance of polysemy for
the 20000 or so others. In contrast, gene and protein
1We also use other OBO ontologies, specifically those for
nucleic acid sequences (SO) and cell type (CL).
45
(.*) activity$ ? (\1)
(.*) formation$ ? ?
(.*) synthesis$ ? ?
ribonuclease ? RNAse
? ribonuclease
?alpha- (etc.) ? ?- (etc.)
? alpha- (etc.)
pluralize nouns
stopwords ? ?
Table 1: Example rules from ?Lucinda?, used for generat-
ing recogniser input from OBO files
names are generally short, non-compositional and often
polysemous with ordinary English words such as Cat or
Rat.
3 Implementation
Oscar is intended to be a component in larger workflows,
such as the Sciborg system (Copestake et al, 2006). It
is a shallow named-entity recogniser and does not per-
form deeper parsing. Hence there is no analysis of the
text above the level of the term, with the exception of
acronym matching, which is dealt with below, and some
treatment of the boldface chemical compound numbers
where they appear in section headings. It is optimized
for chemical NER, but can be extended to handle general
term recognition. The EBIMed system, in contrast, is a
pipeline, and lemmatizes words as part of a larger work-
flow.
To identify plurals and other variants of non-chemical
NEs we have a ruleset, nicknamed Lucinda, outlined in
Table 1, for generating the input for the recogniser from
external data. We use the plain-text OBO 1.2 format,
which is the definitive format for the dissemination of the
OBO ontologies.
We strive to keep this ruleset as small as possible, with
the exception of determining plurals and a few other reg-
ular variants. The reason for keeping plurals outside the
ontology is that plurals in ordinary text and in ontologies
can have quite different meanings.
There is also a short stopword list applied at this stage,
which is different from Oscar?s internal stopword han-
dling, described below.
3.1 Named entity recognition and resolution
Oscar has a recogniser to identify chemical names and
ontology terms, and a resolver which matches NEs to on-
tology IDs or chemical structures. The recogniser classi-
fies NEs according to the scheme in Corbett et al (2007).
The classes which are relevant here are CM, which iden-
tifies a chemical compound, either because it appears in
Oscar?s chemical dictionary, which also contains struc-
2 5 8 5 \s
4 5 8 0 \s
X
1
6
2
6 2 2 \s X 1 6
3X 1 6 4
Figure 1: Cartoon of part of the recogniser. The mapping
between this automaton and example GO terms is given
in Table 2.
GO term Regex pair
bud neck 2585\s4580\s
2585\s4580\sX162
bud neck polarisome 2585\s4580\s622\s
2585\s4580\s622\sX163
polarisome 622\s
622\sX164
Table 2: Mapping in Fig. 1. The regexes are purely il-
lustrative. IDs 162, 163 and 164 map on to GO:0005935,
GO:0031560 and GO:0000133 respectively.
tures and InChIs,2 or according to Oscar?s n-gram model,
regular expressions and other heuristics and ASE, a sin-
gle word ending in ?-ase? or ?-ases? and representing an
enzyme type. We add the class ONT to these, to cover
terms found in ontologies that do not belong in the other
classes, and STOP, which is the class of stopwords.
We sketch the recogniser in Fig. 1. To build the recog-
niser: Each term in the input data is tokenized and the
tokens converted into a sequence of digits followed by a
space. These new tokens are concatenated and converted
into a pair of regular expressions. One of these expres-
sions has X followed by a term ID appended to it. These
regex?regex pairs are converted into finite automata, the
union of which is determinized. The resulting DFA is ex-
amined for accept states. For each accept state for which
a transition to X is also present, the sequences of digits
after the X is used to build a mapping of accept states to
ontology IDs (Table 2).
To apply the recogniser: The input text is tokenized,
and for each token a set of representations is calculated
which map to sequences of digits as above. We then make
an empty set of DFA instances (a pointer to the DFA,
2An InChI is a canonical identifier for a chemical com-
pound. http://www.iupac.org/inchi/
46
which state it?s in and which tokens it has matched so
far), and for each token, add a new DFA instance for each
DFA, and for each representation of the token, clone the
DFA instance. If it does not accept the digit-sequence
representation of the token, throw it away. If it is in an
accept state, note which tokens it has matched, and if the
accept state maps to an ontology ID (ontID), we have an
NE which can be annotated with the ontID.
Take all of the potential NEs. For all NEs that have the
same sequence of tokens, share all of the ontIDs. Assign
its class according to a priority list where STOP comes
first and CM precedes ASE and ONT. For the system in
Fig. 1, the phrase ?bud neck polarisome? matches three
IDs. We choose the longest?leftmost sequence. If the
resolver generates an InChI for an NE, we look up this
InChI in ChEBI (de Matos et al, 2006), a biochemical
ontology, and take the ontology ID. This has the effect
of aligning ChEBI with other databases and systematic
nomenclature.
3.2 Gene Ontology
In working out how to mine the literature for GO terms,
we have taken our lead from the domain experts, the GO
editors and the curators of the Gene Ontology Annotation
(GOA) database.
The Functional Curation task in the first BioCreative
exercise (Blaschke et al, 2005) is the closest we have
found to a systematic evaluation of GO term identifica-
tion. The brief was to assign GO annotations to human
proteins and recover supporting text. The GOA curators
evaluated the results (Camon et al, 2005) and list some
common mistakes in the methods used to identify GO
terms. These include annotating to obsolete terms, pre-
dicting GO terms on too tenuous a link with the original
text, for example in one case the phrase ?pH value? was
annotated to ?pH domain binding? (GO:0042731), diffi-
culties with word order, and choosing too much support-
ing text, for example an entire first paragraph of text.
So at the suggestion of the GO editors, Oscar works on
exact matches to term names (as preprocessed above) and
their exact (within the OBO syntax) synonyms.
The most relevant GO terms to chemistry concern en-
zymes, which are proteins that catalyse chemical pro-
cesses. Typically their names are multiword expressions
ending in ?-ase?. The enzyme A B Xase will often be
represented by GO terms ?A B Xase activity?, a descrip-
tion of what the enzyme does, and ?A B Xase complex?,
a cellular component which consists of two or more pro-
tein subunits. In general the bare phrase ?A B Xase? will
refer to the activity, so the ruleset in Table 1 deletes the
word ?activity? from the GO term.
We shall briefly compare our method with the algo-
rithms in EBIMed and GoPubMed. The EBIMed algo-
rithm for GO term identification is very similar to ours,
except for the point about lemmatization listed above, and
its explicit variation of character case, which is handled
in Oscar by its case normalization algorithm. In contrast,
the algorithm in GoPubMed works by matching short
?seed? terms and then expanding them. This copes with
cases such as ?protein threonine/tyrosine kinase activity?
(GO:0030296) where the full term is unlikely to be found
in ordinary text; the words ?protein? and ?activity? are
generally omitted. However, the approach in (Delfs et
al., 2005) cannot be applied blindly; the authors claim for
example that ?biosynthesis? can be ignored without com-
promising the reader?s understanding. In chemistry jour-
nal articles most mentions of a chemical compound will
not refer to how it is formed in nature; they will refer to
the compound itself, its analogues or other processes. In
fact, our ruleset in Table 1 explicitly disallows GO term
synonyms ending in ? synthesis? or ? formation? since
they do not necessarily represent biological processes. It
is also not clear from Delfs et al (2005) how robust the
algorithm is to the sort of errors identified by Camon et
al. (2005).
4 Case study
The problem is to take a journal article, apply meaningful
and useful annotations, connect them to stable resources,
allow technical editors to check and add further annota-
tions, and disseminate the article in enriched form.
Most chemical publishers use XML as a stable format
for maintaining their documents for at least some stages
of the publication process. The Sciborg project (Copes-
take et al, 2006) and the Royal Society of Chemistry
(RSC) use SciXML (Rupp et al, 2006) and RSC XML
respectively. For the overall Sciborg workflow, standoff
annotation is used to store the different sets of annota-
tions. For the purposes of this paper, however, we make
use of the inline output of Oscar, which is SciXML with
<ne> elements for the annotations.
Not all of the RSC XML need be mined for NEs;
much of it is bibliographic markup which can confuse
parsers. Only the useful parts are converted into SciXML
and passed to Oscar, where they are annotated. These
SciXML annotations are then pasted back into the RSC
XML, where they can be checked by technical editors.
In running text, NEs are annotated with an ID local
to the XML file, which refers to <compound> and
<annotation> elements in a block at the end, which
contain chemical structure information and ontology IDs.
This is a lightweight compromise between pure standoff
and pure inline annotation.
We find useful annotations by aggressive threshold-
ing. The only classes which survive are ONTs, and those
CMs which have a chemical structure found by the re-
solver. This enables the chemical NER part of Oscar
to be tuned for high recall even as part of a publishing
47
workflow. Only CMs which correspond to an unambigu-
ous molecule or molecular ion are treated as a chemical
compound; everything else is referred to an appropriate
ontology. We use the InChI as a stable representation for
chemical structure, and the curated OBO ontologies for
biomedical terms.
The role of technical editors is to remove faulty anno-
tations, add new compounds to the chemical dictionary,
based on chemical structures supplied by authors, sug-
gest new GO terms to the ontology curators, and extend
the stopword lists of both Oscar and Lucinda as appropri-
ate. At present (May 2007), this happens after publication
of articles on the web, but is intended to become part of
the routine editing process in the course of 2007.
This enriched XML can then be converted into HTML
and RSS by means of XSL stylesheets and database
lookups, as in the RSC?s Project Prospect.3 The imme-
diate benefits of this work are increased readability of ar-
ticles for readers and extensive cross-linking with other
articles that have been enhanced in the same way. Fu-
ture developments could easily involve structure-based
searching, ontology-based search of journal articles, and
finding correlations between biological processes and
small molecule structures.
5 Ambiguity in chemical NER
One important omission is disambiguating the exact ref-
erent of a chemical name, which is not always clear with-
out context. For example ?the pyridine 6?, is a class de-
scription, but the phrase ?the pyridine molecule? refers to
a particular compound. ChEBI, which contains an ontol-
ogy of molecular structure, uses plurals to indicate chem-
ical classes, for example ?benzenes?, which is often, but
not always, what ?benzenes? means in text. Currently
Oscar does not distinguish between singular and plural.
Amino acids and saccharides are particularly trouble-
some on account of homochirality. Unless otherwise
specified, ?histidine? and ?ribose? specify the molecules
with the chirality found in nature, or to be precise,
L-histidine and D-ribose respectively. What is even
worse is that ?histidine? seldom refers to the independent
molecule; it usually means the histidine residue, part of a
larger entity.
6 Acknowledgements
We thank Dietrich Rebholz-Schuhmann for useful dis-
cussions. CRB thanks Jane Lomax, Jen Clark, Amelia
Ireland and Midori Harris for extensive cooperation and
help, and Richard Kidd, Neil Hunter and Jeff White at
the RSC. PTC thanks Ann Copestake and Peter Murray-
Rust for supervision. This work was funded by EPSRC
(EP/C010035/1).
3http://www.projectprospect.org/
References
Christian Blaschke, Eduardo Andres Leon, Martin
Krallinger and Alfonso Valencia. 2005. Evaluation
of BioCreAtIvE assessment of task 2 BMC Bioinfor-
matics 6(Suppl 1):S16
Evelyn B. Camon, Daniel G. Barrell, Emily C. Dimmer,
Vivian Lee, Michele Magrane, John Maslen, David
Binns and Rolf Apweiler. 2005. An evaluation of GO
annotation retrieval for BioCreAtIvE and GOA BMC
Bioinformatics 6(Suppl 1):S17
Ann Copestake, Peter Corbett, Peter Murray-Rust, C. J.
Rupp, Advaith Siddharthan, Simone Teufel and Ben
Waldron. 2006. An Architecture for Language Tech-
nology for Processing Scientific Texts. In Proceedings
of the 4th UK E-Science All Hands Meeting. Notting-
ham, UK.
Peter Corbett, Colin Batchelor and Simone Teufel. 2007.
Annotation of Chemical Named Entities. In Proceed-
ings of BioNLP in ACL (BioNLP?07).
Peter T. Corbett and Peter Murray-Rust. 2006. High-
throughput identification of chemistry in life science
texts. LNCS, 4216:107?118.
P. de Matos, M. Ennis, M. Darsow, M. Guedj, K. Degt-
yarenko, and R. Apweiler. 2006. ChEBI - Chemical
Entities of Biological Interest Nucleic Acids Research,
Database Summary Paper 646.
The Gene Ontology Consortium. 2000. Gene Ontology:
Tool for the Unification of Biology Nature Genetics,
25:25?29.
Ralph Delfs, Andreas Doms, Alexander Kozlenkov and
Michael Schroeder. 2004. GoPubMed: Exploring
PubMed with the GeneOntology. Proceedings of Ger-
man Bioinformatics Conference, 169?178.
Christopher J. Mungall. 2004. Obol: integrating lan-
guage and meaning in bio-ontologies. Comparative
and Functional Genomics, 5:509?520.
Dietrich Rebholz-Schuhmann, Harald Kirsch, Miguel
Arregui, Sylvain Gaudan, Mark Riethoven and Peter
Stoehr. 2007. EBIMed?text crunching to gather
facts for proteins from Medline. Bioinformatics,
23(2):e237?e244.
C. J. Rupp, Ann Copestake, Simone Teufel and Benjamin
Waldron. 2006. Flexible Interfaces in the Application
of Language Technology to an eScience Corpus. In
Proceedings of the 4th UK E-Science All Hands Meet-
ing. Nottingham, UK.
48
BioNLP 2007: Biological, translational, and clinical language processing, pages 57?64,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotation of Chemical Named Entities
Peter Corbett
Cambridge University
Chemical Laboratory
Lensfield Road
Cambridge
UK CB2 1EW
ptc24@cam.ac.uk
Colin Batchelor
Royal Society of Chemistry
Thomas Graham House
Milton Road
Cambridge
UK CB4 0WF
batchelorc@rsc.org
Simone Teufel
Natural Language and
Information Processing Group
Computer Laboratory
University of Cambridge
UK CB3 0FD
sht25@cam.ac.uk
Abstract
We describe the annotation of chemical
named entities in scientific text. A set of an-
notation guidelines defines 5 types of named
entities, and provides instructions for the
resolution of special cases. A corpus of full-
text chemistry papers was annotated, with an
inter-annotator agreement
 
score of 93%.
An investigation of named entity recogni-
tion using LingPipe suggests that
 
scores
of 63% are possible without customisation,
and scores of 74% are possible with the ad-
dition of custom tokenisation and the use of
dictionaries.
1 Introduction
Recent efforts in applying natural language pro-
cessing to natural science texts have focused on
the recognition of genes and proteins in biomedi-
cal text. These large biomolecules are?mostly?
conveniently described as sequences of subunits,
strings written in alphabets of 4 or 20 letters. Ad-
vances in sequencing techniques have lead to a boom
in genomics and proteomics, with a concomitant
need for natural language processing techniques to
analyse the texts in which they are discussed.
However, proteins and nucleic acids provide only
a part of the biochemical picture. Smaller chemical
species, which are better described atom-by-atom,
play their roles too, both in terms of their inter-
actions with large biomolecules like proteins, and
in the more general biomedical context. A num-
ber of resources exist to provide chemical infor-
mation to the biological community. For example,
the National Center For Biotechnology Information
(NCBI) has added the chemical database PubChem1
to its collections of bioinformatics data, and the on-
tology ChEBI (Chemical Entities of Biological In-
terest) (de Matos et al, 2006) has been added to the
Open Biological Ontologies (OBO) family.
Small-molecule chemistry also plays a role in
biomedical natural language processing. PubMed
has included abstracts from medicinal chemistry
journals for a long time, and is increasingly carry-
ing other chemistry journals too. Both the GENIA
corpus (Kim et al, 2003) and the BioIE cytochrome
P450 corpus (Kulick et al, 2004) come with named
entity annotations that include a proportion of chem-
icals, and at least a few abstracts that are recognis-
able as chemistry abstracts.
Chemical named entity recognition enables a
number of applications. Linking chemical names to
chemical structures, by a mixture of database lookup
and the parsing of systematic nomenclature, allows
the creation of semantically enhanced articles, with
benefits for readers. An example of this is shown in
the Project Prospect2 annotations by the Royal So-
ciety of Chemistry (RSC). Linking chemical NER
to chemical information retrieval techniques allows
corpora to be searched for chemicals with similar
structures to a query molecule, or chemicals that
contain a particular structural motif (Corbett and
Murray-Rust, 2006). With information extraction
techniques, chemicals could be linked to their prop-
erties, applications and reactions, and with tradi-
tional gene/protein NLP techniques, it could be pos-
1http://pubchem.ncbi.nlm.nih.gov/
2http://www.projectprospect.org/
57
sible to discover new links between chemical data
and bioinformatics data.
A few chemical named entity recognition (Cor-
bett and Murray-Rust, 2006; Townsend et al, 2005;
Vasserman, 2004; Kemp and Lynch, 1998; Sun et
al., 2007) or classification (Wilbur et al, 1999) sys-
tems have been published. A plugin for the GATE
system3 will also recognise a limited range of chem-
ical entities. Other named entity recognition or
classification systems (Narayanaswamy et al, 2003;
Torii et al, 2004; Torii and Vijay-Shanker, 2002;
Spasic and Ananiadou, 2004) sometimes include
chemicals as well as genes, proteins and other bio-
logical entities. However, due to differences in cor-
pora and the scope of the task, it is difficult to com-
pare them. There has been no chemical equivalent
of the JNLPBA (Kim et al, 2004) or BioCreAtIvE
(Yeh et al, 2005) evaluations. Therefore, a corpus
and a task definition are required.
To find an upper bound on the levels of perfor-
mance that are available for the task, it is necessary
to study the inter-annotator agreement for the man-
ual annotation of the texts. In particular, it is useful
to see to what extent the guidelines can be applied by
those not involved in their development. Producing
guidelines that enable a highly consistent annotation
may raise the quality of the results of any machine-
learning techniques that use training data applied to
the guidelines, and producing guidelines that cover
a broad range of subdomains is also important (Din-
gare et al, 2005).
2 Annotation Guidelines
We have prepared a set of guidelines for the an-
notation of the names of chemical compounds and
related entities in scientific papers. These guide-
lines grew out of work on PubMed abstracts, and
have since been developed with reference to organic
chemistry journals, and later a range of journals en-
compassing the whole of chemistry.
Our annotation guidelines focus on the chemicals
themselves; we believe that these represent the ma-
jor source of rare words in chemistry papers, and
are of the greatest interest to end-users. Further-
more, many chemical names are formed systemat-
ically or semi-systematically, and can be interpreted
3http://www.gate.ac.uk/
without resorting to dictionaries and databases. As
well as chemical names themselves, we also con-
sider other words or phrases that are formed from
chemical names.
The various types are summarised in Table 1.
Type Description Example
CM chemical compound citric acid
RN chemical reaction 1,3-dimethylation
CJ chemical adjective pyrazolic
ASE enzyme methylase
CPR chemical prefix 1,3-
Table 1: Named entity types
The logic behind the classes is best explained with
an example drawn from the corpus described in the
next section:
In addition, we have found in previous
studies that the Zn  

?Tris system is also
capable of efficiently hydrolyzing other

-
lactams, such as clavulanic acid, which
is a typical mechanism-based inhibitor of
active-site serine

-lactamases (clavulanic
acid is also a fairly good substrate of the
zinc-

-lactamase from B. fragilis).
Here, ?clavulanic acid? is a specific chemical com-
pound (a CM), referred to by a trivial (unsystem-
atic) name, and ?  -lactams? is a class of chemi-
cal compounds (also a CM), defined by a particu-
lar structural motif. ?Zn  

?Tris? is another CM (a
complex rather than a molecule), and despite be-
ing named in an ad hoc manner, the name is com-
positional and it is reasonably clear to a trained
chemist what it is. ?Serine? (another CM) can be
used to refer to an amino acid as a whole compound,
but in this case refers to it as a part of a larger
biomolecule. The word ?hydrolyzing? (an RN) de-
notes a reaction involving the chemical ?water?. ?

-
lactamases? (an ASE) denotes a class of enzymes
that process

-lactams, and ?zinc-

-lactamase? (an-
other ASE) denotes a  -lactamase that uses zinc.
By our guidelines, the terms ?mechanism-based in-
hibitor? or ?substrate? are not annotated, as they de-
note a chemical role, rather than giving information
about the structure or composition of the chemicals.
58
The full guidelines occupy 31 pages (including a
quick reference section), and contain 93 rules. Al-
most all of these have examples, and many have sev-
eral examples.
A few distinctions need to be explained here. The
classes RN, CJ and ASE do not include all reactions,
adjectives or enzymes, but only those that entail
specific chemicals or classes of chemicals?usually
by being formed by the modification of a chemical
name?for example, ?

-lactamases? in the example
above is formed from the name of a class of chem-
icals. Words derived from Greek and Latin words
for ?water?, such as ?aqueous? and ?hydrolysis?, are
included when making these annotations.
The class CPR consists of prefixes, more often
found in systematic chemical names, giving details
of the geometry of molecules, that are attached to
normal English words. For example, the chemi-
cal 1,2-diiodopentane is a 1,2-disubstituted pentane,
and the ?1,2-? forms the CPR in ?1,2-disubstituted?.
Although these contructions sometimes occur as in-
fixes within chemical names, we have only seen
these used as prefixes outside of them. We believe
that identifying these prefixes will be useful in the
adaptation of lexicalised parsers to chemical text.
The annotation task includes a small amount of
word sense disambiguation. Although most chemi-
cal names do not have non-chemical homonyms, a
few do. Chemical elements, and element symbols,
give particular problems. Examples of this include
?lead?, ?In? (indium), ?As? (arsenic), ?Be? (beryl-
lium), ?No? (nobelium) and ?K? (potassium?this is
confusable with Kelvin). These are only annotated
when they occur in their chemical sense.
2.1 Related Work
We know of two publicly available corpora that also
include chemicals in their named-entity markup. In
both of these, there are significant differences to
many aspects of the annotation. In general, our
guidelines tend to give more importance to concepts
regarding chemical structure, and less importance to
biological role, than the other corpora do.
The GENIA corpus (Kim et al, 2003) in-
cludes several different classes for chemi-
cals. Our class CM roughly corresponds to
the union of GENIA?s atom, inorganic,
other organic compound, nucleotide
and amino acid monomer classes, and also
parts of lipid and carbohydrate (we ex-
clude macromolecules such as lipoproteins and
lipopolysaccharides). Occasionally terms that
match our class RN are included as other name.
Our CM class also includes chemical names
that occur within enzyme or other protein names
(e.g. ?inosine-5   -monophosphate? in ?inosine-5   -
monophosphate dehydrogenase?) whereas the
GENIA corpus (which allows nesting) typically
does not. The GENIA corpus also sometimes
includes qualifiers in terms, giving ?intracellular
calcium? where we would only annotate ?calcium?,
and also includes some role/application terms such
as ?antioxidant? and ?reactive intermediate?.
The BioIE P450 corpus (Kulick et al, 2004), by
contrast, includes chemicals, proteins and other sub-
stances such as foodstuffs in a single category called
?substance?. Again, role terms such as ?inhibitor? are
included, and may be merged with chemical names
to make entities such as ?fentanyl metabolites? (we
would only mark up ?fentanyl?). Fragments of
chemicals such as ?methyl group? are not marked up;
in our annotations, the ?methyl? is marked up.
The BioIE corpus was produced with extensive
guidelines; in the GENIA corpus, much more was
left to the judgement of the annotators. These lead
to inconsistencies, such as whether to annotate ?an-
tioxidant? (our guidelines treat this as a biological
role, and do not mark it up). We are unaware of an
inter-annotator agreement study for either corpus.
Both of these corpora include other classes of
named entities, and additional information such as
sentence boundaries.
3 Inter-annotator Agreement
3.1 Related Work
We are unaware of any studies of inter-annotator
agreement with regards to chemicals. However, a
few studies of gene/protein inter-annotator agree-
ment do exist. Demetriou and Gaizauskas (2003)
report an
 
score of 89% between two domain ex-
perts for a task involving various aspects of protein
science. Morgan et al (2004) report an   score of
87% between a domain expert and a systems devel-
oper for D. melanogaster gene names. Vlachos and
Gasperin (2006) produced a revised version of the
59
guidelines for the task, and were able to achieve an
 
score of 91%, and a kappa of 0.905, between a
computational linguist and a domain expert.
3.2 Subjects
Three subjects took part in the study. Subject A
was a chemist and the main author of the guidelines.
Subject B was another chemist, highly involved in
the development of the guidelines. Subject C was a
PhD student with a chemistry degree. His involve-
ment in the development of guidelines was limited to
proof-reading an early version of the guidelines. C
was trained by A, by being given half an hour?s train-
ing, a test paper to annotate (which satisfied A that C
understood the general principles of the guidelines),
and a short debriefing session before being given the
papers to annotate.
3.3 Materials
The study was performed on 14 papers (full pa-
pers and communications only, not review articles
or other secondary publications) published by the
Royal Society of Chemistry. These were taken from
the journal issues from January 2004 (excluding a
themed issue of one of the journals). One paper was
randomly selected to represent each of the 14 jour-
nals that carried suitable papers. These 14 papers
represent a diverse sample of topics, covering areas
of organic, inorganic, physical, analytical and com-
putational chemistry, and also areas where chemistry
overlaps with biology, environmental science, mate-
rials and mineral science, and education.
From these papers, we collected the title, section
headings, abstract and paragraphs, and discarded the
rest. To maximise the value of annotator effort, we
also automatically discarded the experimental sec-
tions, by looking for headers such as ?Experimen-
tal?. This policy can be justified thus: In chemistry
papers, a section titled ?Results and Discussion? car-
ries enough information about the experiments per-
formed to follow the argument of the paper, whereas
the experimental section carries precise details of the
protocols that are usually only of interest to people
intending to replicate or adapt the experiments per-
formed. It is increasingly common for chemistry pa-
pers not to contain an experimental section in the
paper proper, but to include one in the supporting
online information. Furthermore, experimental sec-
tions are often quite long and tedious to annotate,
and previous studies have shown that named-entity
recognition is easier on experimental sections too
(Townsend et al, 2005).
A few experimental sections (or parts thereof)
were not automatically detected, and instead were
removed by hand.
3.4 Procedure
The papers were hand-annotated using our in-house
annotation software. This software displays the text
so as to preserve aspects of the style of the text such
as subscripts and superscripts, and allows the anno-
tators to freely select spans of text with character-
level precision?the text was not tokenised prior to
annotation. Spans were not allowed to overlap or to
nest. Each selected span was assigned to exactly one
of the five available classes.
During annotation the subjects were allowed to
refer to the guidelines (explained in the previous sec-
tion), to reference sources such as PubChem and
Wikipedia, and to use their domain knowledge as
chemists. They were not allowed to confer with
anyone over the annotation, nor to refer to texts an-
notated during development of the guidelines. The
training of subject C by A was completed prior to A
annotating the papers involved in the exercise.
3.5 Evaluation Methodology
Inter-annotator agreement was measured pairwise,
using the
 
score. To calculate this, all of the ex-
act matches were found and counted, and all of the
entities annotated by one annotator but not the other
(and vice versa) were counted. For an exact match,
the left boundary, right boundary and type of the an-
notation had to match entirely. Thus, if one anno-
tator had annotated ?hexane?ethyl acetate? as a sin-
gle entity, and the other had annotated it as ?hexane?
and ?ethyl acetate?, then that would count as three
cases of disagreement and no cases of agreement.
We use the
 
score as it is a standard measure in the
domain?however, as a measure it has weaknesses
which will be discussed in the next subsection.
Given the character-level nature of the annotation
task, and that the papers were not tokenised, the task
cannot sensibly be cast as a classification problem,
and so we have not calculated any kappa scores.
60
Overall results were calculated using two meth-
ods. The first method was to calculate the total lev-
els of agreement and disagreement across the whole
corpus, and to calculate a total   score based on that.
The second method was to calculate   scores for in-
dividual papers (removing a single paper that con-
tained two named entities?neither of which were
spotted by subject B?as an outlier), and to calculate
an unweighted mean, standard deviation and 95%
confidence intervals based on those scores.
3.6 Results and Discussion
Subjects   (corpus)   (average) std. dev.
A?B 92.8% 92.9%   3.4% 6.2%
A?C 90.0% 91.4%   3.1% 5.7%
B?C 86.1% 87.6%   3.1% 5.7%
Table 2: Inter-annotator agreement results.   values
are 95% confidence intervals.
The results of the analysis are shown in Table 2.
The whole-corpus
 
scores suggest that high levels
of agreement (93%) are possible. This is equivalent
to or better than quoted values for biomedical inter-
annotator agreement. However, the poorer agree-
ments involving C would suggest that some of this is
due to some extra information being communicated
during the development of the guidelines.
A closer analysis shows that this is not the case. A
single paper, containing a large number of entities, is
notable as a major source of disagreement between
A and C, and B and C, but not A and B. Looking
at the annotations themselves, the paper contained
many repetitions of the difficult entity ?Zn  

?Tris?,
and also of similar entities. If the offending paper is
removed from consideration, the agreement between
A and C exceeds the agreement between A and B.
This analysis is confirmed using the per-paper  
scores. Two-tailed, pairwise t-tests (excluding the
outlier paper) showed that the difference in mean  
scores between the A?B and A?C agreements was
not statistically significant at the 0.05 significance
level; however, the differences between B?C and A?
B, and between B?C and A?C were.
A breakdown of the inter-annotator agreements
by type is shown in Table 3. CM and RN, at least,
seem to be reliably annotated. The other classes are
less easy to assess, due to their rarity, both in terms
Type
 
Number
CM 93% 2751
RN 94% 79
CJ 56% 20
ASE 96% 25
CPR 77% 10
Table 3: Inter-annotator agreement, by type.  
scores are corpus totals, between Subjects A and C.
The number is the number of entities of that class
found by Subject A.
of their total occurrence in the corpus and the num-
ber of papers that contain them.
We speculate that the poorer B?C agreement may
be due to differing error rates in the annotation. In
many cases, it was clear from the corpus that errors
were made due to failing to spot relevant entities, or
by failing to look up difficult cases in the guidelines.
Although it is not possible to make a formal analy-
sis of this, we suspect that A made fewer errors, due
to a greater familiarity with the task and the guide-
lines. This is supported by the results, as more er-
rors would be involved in the B?C comparison than
in comparisons involving A, leading to higher levels
of disagreement.
We have also examined the types of disagree-
ments made. There were very few cases where two
annotators had annotated an entity with the same
start and end point, but a different type; there were
2 cases of this between A and C, and 3 cases in each
of the other two comparisons. All of these were con-
fusions between CM and CJ.
In the A?B comparison, there were 415 entities
that were annotated by either A or B that did not
have a corresponding exact match. 183 (44%) of
those were simple cases where the two annotators
did not agree as to whether the entity should be
marked up or not (i.e. the other annotator had not
placed any entity wholly or partially within that
span). For example, some annotators failed to spot
instances of ?water?, or disagreed over whether ?fat?
(as a synonym for ?lipid?) was to be marked up.
The remainder of those disagreements are due
to disagreements of class, of where the boundaries
should be, of how many entities there should be in
a given span, and combinations of the above. In all
61
of these cases, the fact that the annotators produce at
least one entity each for a given case means that dis-
agreements of this type are penalised harshly, and
therefore are given disproportionate weight. How-
ever, it is also likely that disagreements over whether
to mark an entity up are more likely to represent a
simple mistake than a disagreement over how to in-
terpret the guidelines; it is easy to miss an entity that
should be marked up when scanning the text.
A particularly interesting class of disagreement
concerns whether a span of text should be anno-
tated as one entity or two. For example, ?Zn  

?Tris?
could be marked up as a single entity, or as ?Zn  

?
and ?Tris?. We looked for cases where one annota-
tor had a single entity, the left edge of which cor-
responded to the left edge of an entity annotated by
the other annotator, and the right edge corresponded
to the right edge of a different entity. We found 43
cases of this. As in each of these cases, at least three
entities are involved, this pattern accounts for at least
30% of the inter-annotator disagreement. Only 17 of
these cases contained whitespace?in the rest of the
cases, hyphens, dashes or slashes were involved.
4 Analysis of the Corpus
To generate a larger corpus, a further two batches of
papers were selected and preprocessed in the manner
described for the inter-annotator agreement study
and annotated by Subject A. These were combined
with the annotations made by Subject A during the
agreement study, to produce a corpus of 42 papers.
Type Entities Papers
CM 6865 94.1% 42 100%
RN 288 4.0% 23 55%
CJ 60 0.8% 20 48%
ASE 31 0.4% 5 12%
CPR 53 0.7% 9 21%
Table 4: Occurrence of entities in the corpus, and
numbers of papers containing at least one entity of a
type.
From Table 4 it is clear that CM is by far the most
common type of named entity in the corpus. Obser-
vation of the corpus shows that RN is common in
certain genres of paper (for example organic synthe-
sis papers), and generally absent from other genres.
ASE, too, is a specialised category, and did not occur
much in this corpus.
A closer examination of CM showed more than
90% of these to contain no whitespace. However,
this is not to say that there are not significant num-
bers of multi-token entities. The difficulty of to-
kenising the corpus is illustrated by the fact that
1114 CM entities contained hyphens or dashes, and
388 CM entities were adjacent to hyphens or dashes
in the corpus. This means that any named entity
recogniser will have to have a specialised tokeniser,
or be good at handling multi-token entities.
Tokenising the CM entities on whitespace and
normalising their case revealed 1579 distinct
words?of these, 1364 only occurred in one paper.
There were 4301 occurrences of these words (out of
a total of 7626). Whereas the difficulties found in
gene/protein NER with complex multiword entities
and polysemous words are less likely to be a prob-
lem here, the problems with tokenisation and large
numbers of unknown words remain just as pressing.
As with biomedical text (Yeh et al, 2005), cases
of conjunctive and disjunctive nomenclature, such
as ?benzoic and thiophenic acids? and ?bromo- or
chlorobenzene? exist in the corpus. However, these
only accounted for 27 CM entities.
5 Named-Entity Recognition
To establish some baseline measures of perfor-
mance, we applied the named-entity modules from
the toolkit LingPipe,4 which has been success-
fully applied to NER of D. melanogaster genes
(e.g. by Vlachos and Gasperin (2006)). Ling-
Pipe uses a first-order HMM, using an enriched
tagset that marks not only the positions of the
named entities, but the tokens in front of and
behind them. Two different strategies are em-
ployed for handling unknown tokens. The
first (the TokenShapeChunker) replaces un-
known or rare tokens with a morphologically-
based classification. The second, newer module
(the CharLmHmmChunker) estimates the prob-
ability of an observed word given a tag us-
ing language models based on character-level   -
grams. The LingPipe developers suggest that the
TokenShapeChunker typically outperforms the
4http://www.alias-i.com/lingpipe/
62
CharLmHmmChunker. However, the more so-
phisticated handling of unknown words by the
CharLmHmmChunker suggests that it might be a
good fit to the domain.
As well as examining the performance of Ling-
Pipe out of the box, we were also able to make some
customisations. We have a custom tokeniser, con-
taining several adaptations to chemical text. For ex-
ample, our tokeniser will only remove brackets from
the front and back of tokens, and only if that would
not cause the brackets within the token to become
unbalanced. For example, no brackets would be re-
moved from ?(R)-acetoin?. Likewise, it will only
tokenise on a hyphen if the hyphen is surrounded
by two lower-case letters on either side (and if the
letters to the left are not common prehyphen com-
ponents of chemical names), or if the string to the
right has been seen in the training data to be hy-
phenated with a chemical name (e.g. ?derived? in
?benzene-derived?). By contrast, the default Ling-
Pipe tokeniser is much more aggressive, and will to-
kenise on hyphens and brackets wherever they occur.
The CharLmHmmChunker?s language models
can also be fed dictionaries as additional training
data?we have experimented with using a list of
chemical names derived from ChEBI (de Matos et
al., 2006), and a list of chemical elements. We have
also made an extension to LingPipe?s token classi-
fier, which adds classification based on chemically-
relevant suffixes (e.g. -yl, -ate, -ic, -ase, -lysis), and
membership in the aforementioned chemical lists, or
in a standard English dictionary.
We analysed the performance of the different
LingPipe configurations by 3-fold cross-validation,
using the 42-paper corpus described in the previous
section. In each fold, 28 whole papers were used as
training data, holding out the other 14 as test data.
The results are shown in Table 5.
From Table 5, we can see that the character   -
gram language models offer clear advantages over
the older techniques, especially when coupled to a
custom tokeniser (which gives a boost to   of over
7%), and trained with additional chemical names.
The usefulness of character-based   -grams has also
been demonstrated elsewhere (Wilbur et al, 1999;
Vasserman, 2004; Townsend et al, 2005). Their use
here in an HMM is particularly apt, as it allows the
token-internal features in the language model to be
Configuration
    
TokenShape 67.0% 52.9% 59.1%
+  71.2% 62.3% 66.5%
+  67.4% 52.5% 59.0%
+  +  73.3% 62.5% 67.4%
CharLm 62.7% 63.4% 63.1%
+  59.8% 68.8% 64.0%
+  71.1% 70.0% 70.5%
+  +  75.3% 73.5% 74.4%
Table 5: LingPipe performance using different con-
figurations.  = custom token classifier,  = chemical
name lists,  = custom tokeniser
combined with the token context.
The impact of custom tokenisation upon
the older TokenShapeChunker is less dra-
matic. It is possible that tokens that contain
hyphens, brackets and other special characters are
more likely to be unknown or rare tokens?the
TokenShapeChunker has previously been
reported to make most of its mistakes on these
(Vlachos and Gasperin, 2006), so tokenising them
is likely to make less of an impact. It is also
possible that chemical names are more distinctive
as a string of subtokens rather than as one large
token?this may offset the loss in accuracy from
getting the start and end positions wrong. The
CharLmHmmChunker already has a mecha-
nism for spotting distinctive substrings such as
?N,N?-? and ?-3-?, and so the case for having long,
well-formed tokens becomes much less equivocal.
It is also notable that improvements in tokenisa-
tion are synergistic with other improvements?the
advantage of using the CharLmHmmChunker is
much more apparent when the custom tokeniser is
used, as is the advantage of using word lists as addi-
tional training data. It is notable that for the unmod-
ified TokenShapeChunker, using the custom to-
keniser actually harms performance.
6 Conclusion
We have produced annotation guidelines that enable
the annotation of chemicals and related entities in
scientific texts in a highly consistent manner. We
have annotated a corpus using these guidelines, an
analysis of which, and the results of using an off-
63
the-shelf NER toolkit, show that finding good ap-
proaches to tokenisation and the handling of un-
known words is critical in the recognition of these
entities. The corpus and guidelines are available by
contacting the first author.
7 Acknowledgements
We thank Ann Copestake and Peter Murray-Rust
for supervision, Andreas Vlachos and Advaith Sid-
dharthan for valuable discussions, and David Jessop
for annotation. We thank the RSC for providing the
papers, and the UK eScience Programme and EP-
SRC (EP/C010035/1) for funding.
References
Peter T. Corbett and Peter Murray-Rust. 2006. High-
Throughput Identification of Chemistry in Life Sci-
ence Texts. CompLife, LNBI 4216:107?118.
P. de Matos, M. Ennis, M. Darsow, M. Guedj, K. Degt-
yarenko and R. Apweiler. 2006. ChEBI ? Chemi-
cal Entities of Biological Interest. Nucleic Acids Res,
Database Summary Paper 646.
George Demetriou and Rob Gaizauskas. 2003. Cor-
pus resources for development and evaluation of a bi-
ological text mining system. Proceedings of the Third
Meeting of the Special Interest Group on Text Mining,
Brisbane, Australia, July.
Shipra Dingare, Malvina Nissim, Jenny Finkel, Christo-
pher Manning and Claire Grover. 2005. A system for
identifying named entities in biomedical text: how re-
sults from two evaluations reflect on both the system
and the evaluations. Comparative and Functional Ge-
nomics, 6(1-2),77-85.
Nick Kemp and Michael Lynch. 1998. Extraction of In-
formation from the Text of Chemical Patents. 1. Iden-
tification of Specific Chemical Names. J. Chem. Inf.
Comput. Sci., 38:544-551.
J.-D. Kim, T. Ohta, Y. Tateisi and J. Tsujii. 2003. GE-
NIA corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl 1):i180-i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi and Nigel Collier. 2004. Introduction
to the Bio-Entity Recognition Task at JNLPBA. Pro-
ceedings of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications, 70-75.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein and
Lyle Ungar. 2004. Integrated Annotation for Biomed-
ical Information Extraction HLT/NAACL BioLINK
workshop, 61-68.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. Journal of Biomedical
Informatics, 37(6):396-410.
Meenakshi Narayanaswamy, K. E. Ravikumar and K.
Vijay-Shanker. 2003. A Biological Named Entity
Recogniser. Pac. Symp. Biocomput., 427-438.
Irena Spasic and Sophia Ananiadou. 2004. Using
Automatically Learnt Verb Selectional Preferences
for Classification of Biomedical Terms. Journal of
Biomedical Informatics, 37(6):483-497.
Bingjun Sun, Qingzhao Tan, Prasenjit Mitra and C. Lee
Giles. 2007. Extraction and Search of Chemical For-
mulae in Text Documents on the Web. The 16th In-
ternational World Wide Web Conference (WWW?07),
251-259.
Manabu Torii and K. Vijay-Shanker. 2002. Using Unla-
beled MEDLINE Abstracts for Biological Named En-
tity Classification. Genome Informatics, 13:567-568.
Manabu Torii, Sachin Kamboj and K. Vijay-Shanker.
2004. Using name-internal and contextual features to
classify biological terms. Journal of Biomedical Infor-
matics, 37:498-511.
Joe A. Townsend, Ann A. Copestake, Peter Murray-Rust,
Simone H. Teufel and Christopher A. Waudby. 2005.
Language Technology for Processing Chemistry Pub-
lications. Proceedings of the fourth UK e-Science All
Hands Meeting, 247-253.
Alexander Vasserman. 2004. Identifying Chemical
Names in Biomedical Text: An Investigation of the
Substring Co-occurence Based Approaches. Pro-
ceedings of the Student Research Workshop at HLT-
NAACL. 7-12.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and Evaluating Named Entity Recognition
in the Biomedical Domain. Proceedings of BioNLP in
HLT-NAACL. 138-145.
W. John Wilbur, George F. Hazard, Jr., Guy Divita,
James G. Mork, Alan R. Aronson and Allen C.
Browne. 1999. Analysis of Biomedical Text for
Chemical Names: A Comparison of Three Methods.
Proc. AMIA Symp. 176-180.
Alexander Yeh, Alexander Morgan, Marc Colosimo and
Lynette Hirschman. 2005. BioCreAtIvE Task IA:
gene mention finding evaluation. BMC Bioinformat-
ics 6(Suppl I):S2.
64
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 54?62,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Cascaded Classifiers for Confidence-Based Chemical Named Entity
Recognition
Peter Corbett
Unilever Centre For
Molecular Science Informatics
Chemical Laboratory University Of Cambridge
CB2 1EW, UK
ptc24@cam.ac.uk
Ann Copestake
Computer Laboratory
University Of Cambridge
CB3 0FD, UK
aac10@cl.cam.ac.uk
Abstract
Chemical named entities represent an impor-
tant facet of biomedical text. We have de-
veloped a system to use character-based n-
grams, Maximum Entropy Markov Models
and rescoring to recognise chemical names
and other such entities, and to make confi-
dence estimates for the extracted entities. An
adjustable threshold allows the system to be
tuned to high precision or high recall. At a
threshold set for balanced precision and recall,
we were able to extract named entities at an
F score of 80.7% from chemistry papers and
83.2% from PubMed abstracts. Furthermore,
we were able to achieve 57.6% and 60.3% re-
call at 95% precision, and 58.9% and 49.1%
precision at 90% recall. These results show
that chemical named entities can be extracted
with good performance, and that the proper-
ties of the extraction can be tuned to suit the
demands of the task.
1 Introduction
Systems for the recognition of biomedical named
entities have traditionally worked on a ?first-best?
approach, where all of the entities recognised have
equal status, and precision and recall are given
roughly equal importance. This does not reflect that
fact that precision is of greater importance for some
applications, and recall is the key for others. Fur-
thermore, knowing the confidence1 with which the
1In this paper, we use ?confidence? to refer to a system?s
estimate of the probability that a potential named entity is a cor-
rect named entity.
system has assigned the named entities is likely to
be useful in a range of different applications.
Named entities of relevance to biomedical sci-
ence include not only genes and proteins but also
other chemical substances which can be of inter-
est as drugs, metabolites, nutrients, enzyme cofac-
tors, experimental reagents and in many other roles.
We have recently investigated the issue of chemical
named entities (Corbett et al, 2007), by compiling a
set of manual annotation guidelines, demonstrating
93% interannotator agreement and manually anno-
tating a set of 42 chemistry papers. In this paper we
demonstrate a named entity recogniser that assigns
a confidence score to each named entity, allowing it
to be tuned for high precision or recall.
Our review of the methods of chemical named
entity recognition showed a consistent theme: the
use of character-based n-Grams to identify chemi-
cal names via their constituent substrings (Wilbur et
al., 1999; Vasserman, 2004; Townsend et al, 2005).
This can be a powerful technique, due to systematic
and semisystematic chemical names and additional
conventions in drug names. However this technique
does not cover all aspects of chemical nomenclature.
Much current named entity work uses approaches
which combine the structured prediction abilities
of HMMs and their derivatives with techniques
which enable the use of large, diverse feature sets
such as maximum entropy (also known as logis-
tic regression). Maximum Entropy Markov Mod-
els, (MEMMs) (McCallum et al, 2000) provide a
relatively simple framework for this. MEMMs do
have a theoretical weakness, namely the ?label bias?
problem (Lafferty et al, 2001), which has been ad-
54
dressed with the development of Conditional Ran-
dom Fields (CRFs). CRFs are now a mainstay of
the field, being used in a high proportion of entries
in the latest BioCreative evaluation (Krallinger and
Hirschman, 2007). However, despite the label bias
problem, MEMMs still attract interest due to practi-
cal advantages such as shorter training cycles.
The framework of HMMs and their successors of-
fers three modes of operation; first-best, n-best and
confidence-based. In first-best NER, the Viterbi al-
gorithm is used to identify a single sequence of la-
bels for the target sentence. In n-best operation,
the n best sequences for the sentence are identi-
fied, along with their probabilities, for example by
coupling the Viterbi algorithm with A* search. In
confidence-based operation, potential entities (with
a probability above a threshold) are identified di-
rectly, without directly seeking a single optimal la-
belling for the entire sentence. This is done by
examining the probability of the label transitions
within the entity, and the forward and backward
probabilities at the start and end of the entity. This
mode has been termed the Constrained Forward-
Backward algorithm (Culotta andMcCallum, 2004).
Where a single unambiguous non-overlapping la-
belling is required, it can be obtained by identify-
ing cases where the entities overlap, and discarding
those with lower probabilities.
Confidence-based extraction has two main advan-
tages. First, it enables the balance between precision
and recall to be controlled by varying the probability
threshold. Second, confidence-based NER avoids
over-commitment in systems where it is used as a
preprocessor, since multiple overlapping options can
be used as input to later components.
The optimum balance between recall and preci-
sion depends on the application of the NER and on
the other components in the system. High precision
is useful in search even when recall is low when
there is a large degree of redundancy in the informa-
tion in the original documents. High precision NER
may also be useful in contexts such as the extraction
of seed terms for clustering algorithms. Balanced
precision/recall is often appropriate for search, al-
though in principle it is desirable to be able to shift
the balance if there are too many/too few results.
Balanced precision/recall is also generally assumed
for use in strictly pipelined systems, when a single
set of consistent NER results is to be passed on to
subsequent processing. Contexts where high recall
is appropriate include those where a search is being
carried out where there is little redundancy (cf Car-
penter 2007) or where the NER system is being used
with other components which can filter the results.
One use of our NER system is within a language
processing architecture (Copestake et al, 2006) that
systematically allows for ambiguity by treating the
input/output of each component as a lattice (repre-
sented in terms of standoff annotation on an orig-
inal XML document). This system exploits rela-
tively deep parsing, which is not fully robust to NER
errors but which can exploit complex syntactic in-
formation to select between candidate NER results.
NER preprocessing is especially important in the
context of chemistry terms which utilise punctuation
characters (e.g., ?2,4-dinitrotoluene?, ?2,4- and 2,6-
dinitrotoluene?) since failure to identify these will
lead to tokenisation errors in the parser. Such errors
frequently cause complete parse failure, or highly
inaccurate analyses. In our approach, the NER re-
sults contribute edges to a lattice which can (option-
ally) be treated as tokens by the parser. The NER
results may compete with analyses provided by the
main parser lexicon. In this context, some NER er-
rors are unimportant: e.g., the parser is not sensitive
to all the distinctions between types of named entity.
In other cases, the parser will filter the NER results.
Hence it makes sense to emphasise recall over pre-
cision. We also hypothesise that we will be able to
incorporate the NER confidence scores as features
in the parse ranking model.
Another example of the use of high-recall NER in
an integrated system is shown in the editing work-
flows used by the Royal Society of Chemistry in
their Project Prospect system (Batchelor and Cor-
bett, 2007), where chemical named entity recogni-
tion is used to produce semantically-enriched jour-
nal articles. In this situation, high recall is desirable,
as false positives can be removed in two ways; by
removing entities where a chemical structure cannot
be assigned, and by having them checked by a tech-
nical editor. False negatives are harder to correct.
The use of confidence-based recognition has been
demonstrated with CRFs in the domain of contact
details (Culotta and McCallum, 2004), and using
HMMs in the domain of gene annotation (Carpen-
55
ter, 2007). In the latter case, the LingPipe toolkit
was used in the BioCreative 2 evaluation without
significant adaptation. Although only 54% preci-
sion was achieved at 60% recall (the best systems
were achieving precision and recall scores in the
high eighties), the system was capable of 99.99%
recall with 7% precision, and 95% recall with 18%
precision, indicating that very high recall could be
obtained in this difficult domain.
Another potential use of confidence-based NER
is the potential to rescore named entities. In this
approach, the NER system is run, generating a set
of named entities. Information obtained about these
entities throughout the document (or corpus) that
they occur in can then be used in further classi-
fiers. We are not aware of examples of rescoring
being applied to confidence-based NER, but there
are precedents using other modes of operations. For
example, Krishnan and Manning (2006) describe a
system where a first-best CRF is used to analyse a
corpus, the results of which are then used to gener-
ate additional features to use in a second first-best
CRF. Similarly, Yoshida and Tsujii (2007) use an n-
best MEMM to generate multiple analyses for a sen-
tence, and re-rank the analyses based on information
extracted from neighbouring sentences.
Therefore, to explore the potential of these tech-
niques, we have produced a chemical NER system
that uses a MEMM for confidence-based extraction
of named entities, with an emphasis on the use of
character-level n-Grams, and a rescoring system.
2 Corpus
Previously, we have produced a set of annotation
guidelines for chemical named entities, and used
them to annotate a set of 42 chemistry papers (Cor-
bett et al, 2007). Inter-annotator agreement was
tested on 14 of these, and found to be 93%. The an-
notation guidelines specified five classes of named
entity, which are detailed in Table 1. The annotation
was performed on untokenised text.
To test the applicability of the method to a
different corpus, we retrieved 500 PubMed ab-
stracts and titles, and annotated them using the
same methods. The abstracts were acquired us-
ing the query metabolism[Mesh] AND drug
AND hasabstract. This produced a diverse set
of abstracts spanning a wide range of subject ar-
eas, but which contain a higher proportion of rele-
vant terms than PubMed overall. 445 out of 500 ab-
stracts contained at least one named entity, whereas
249 contained at least ten. Notably, the ASE class
was more common in the PubMed corpus than in
the chemistry papers, reflecting the important of en-
zymes to biological and medical topics.
In this study, we have left out the named entity
type CPR, as it is rare (<1%) and causes difficulties
with tokenisation. This entity type covers cases such
as the ?1,3-? in ?1,3-disubstituted?, and as such re-
quires the ?1,3-? to be a separate token or token se-
quence. However, we have found that recognition
of the other four classes is improved if words such
as ?1,3-disubstituted? are kept together as single to-
kens. Therefore it makes sense to treat the recogni-
tion of CPR as an essentially separate problem - a
problem that will not be addressed here.
Type Description Example nCh nPM
CM compound citric acid 6865 4494
RN reaction methylation 288 401
CJ adjective pyrazolic 60 87
ASE enzyme demethylase 31 181
CPR prefix 1,3- 53 21
Table 1: Named Entity types. nCh = number in Chem-
istry corpus, nPM = number in PubMed corpus.
3 Methods
Our system is quite complex, and as such we have
made the source code available (see below). The fol-
lowing gives an outline of the system:
3.1 External Resources
Chemical names were extracted from the chem-
ical ontology ChEBI (Degtyarenko et al, 2008),
and a standard English word list was taken from
/usr/share/dict/words on a Linux system2.
A list of chemical element names and symbols was
also compiled. To overcome the shortage of enti-
ties of type ASE, a list of words from enzyme names
2This dictionary was chosen as it contains inflectional forms
of English words. Our system does not perform stemming,
partly because suffixes are often good cues as to whether a word
is chemical or not.
56
ending in ?-ase? was extracted from the Gene Ontol-
ogy (GO), and hand sorted into words of type ASE,
and words not of type ASE.
3.2 Overview of operation
The text is tokenised before processing; this is
done using the tokeniser described in our previous
work (Corbett et al, 2007), which is adapted to
chemical text.
Our system uses three groups of classifiers to
recognise chemical names. The first classifier?the
?preclassifier??uses character-level n-grams to esti-
mate the probabilities of whether tokens are chemi-
cal or not. The output of this classification is com-
bined with information from the suffix of the word,
and is used to provide features for the MEMM.
The second group of classifiers constitute the
MEMM proper. Named entities are represented us-
ing an BIO-encoding, and methods analogous to
other confidence-based taggers (Culotta and McCal-
lum, 2004; Carpenter, 2007) are used to estimate
the conditional probability of tag sequences corre-
sponding to named entities. The result of this is
a list of potential named entities, with start posi-
tions, end positions, types and probabilities, where
all of the probabilities are above a threshold value.
A small set of hand-written filtering rules is used to
remove obvious absurdities, such as named entities
ending in the word ?the?, and simple violations of
the annotation guidelines, such as named entities of
type ASE that contain whitespace. These filtering
rules make very little difference at recall values up
to about 80%?however, we have found that they are
useful for improving precision at very high recall.
The third group of classifiers?one per entity
type?implement a rescoring system. After all of
the potential entities from a document have been
generated, a set of features is generated for each en-
tity. These features are derived from the probabili-
ties of other entities that share the same text string
as the entity, from probabilities of potential syn-
onyms found via acronym matching and other pro-
cesses, and most importantly, from the pre-rescoring
probability of the entities themselves. In essence,
the rescoring process performs Bayesian reasoning
by adjusting the raw probabilities from the previ-
ous stage up or down based on nonlocal information
within the document.
3.3 Overview of training
A form of training conceptually similar to cross-
validation is used to train the three layers of clas-
sifiers. To train the overall system, the set of docu-
ments used for training is split into three. Two thirds
are used to train a MEMM, which is then used to
generate training data for the rescorer using the held-
out last third. This process is repeated another two
times, holding out a different third of the training
data each time. Finally, the rescorer is trained using
all of the training data generated by this procedure,
and the final version of the MEMM is generated us-
ing all of the training data. This procedure ensures
that both the MEMM and the rescorer are able to
make use of all of the training data, and also that
the rescorer is trained to work with the output of a
MEMM that has not been trained on the documents
that it is to rescore.
A similar procedure is used when training the
MEMM itself. The available set of documents to use
as training data is divided into half. One half is used
to train the preclassifier and build its associated dic-
tionaries, which are then used to generate features
for the MEMM on the other half of the data. The
roles of each half are then reversed, and the same
process is applied. Finally, the MEMM is trained
using all of the generated features, and a new pre-
classifier is trained using all of the available training
data.
It should be noted that the dictionaries extracted
during the training of the preclassifier are also used
directly in the MEMM.
3.4 The character n-gram based preclassifier
During the training of the preclassifier, sets of to-
kens are extracted from the hand-annotated train-
ing data. A heuristic is used to classify these
into ?word tokens??those that match the regex
.*[a-z][a-z].*, and ?nonword tokens??those
that do not (this class includes many acronyms and
chemical formulae). The n-gram analysis is only
performed upon ?word tokens?.
The token sets that are compiled are chemi-
cal word tokens (those that only appear inside
named entities), nonchemical word tokens (those
that do not appear in entities), chemical nonword
tokens, nonchemical nonword tokens and ambigu-
57
ous tokens?those that occur both inside and out-
side of named entities. A few other minor sets are
collected to deal with tokens related to such proper
noun-containing entities as ?Diels?Alder reaction?.
Some of this data is combined with external dic-
tionaries to train the preclassifier, which works us-
ing 4-grams of characters and modified Kneser-Ney
smoothing, as described by Townsend et al (2005).
The set of ?chemical word tokens? is used as a set of
positive examples, along with tokens extracted from
ChEBI, a list of element names and symbols, and
the ASE tokens extracted from the GO. The negative
examples used are the extracted ?nonchemical word
tokens?, the non-ASE tokens from the GO and to-
kens taken from the English dictionary?except for
those that were listed as positive examples. This gets
around the problem that the English dictionary con-
tains the names of all of the elements and a number
of simple compounds such as ?ethanol?.
During operation, n-gram analysis is used to cal-
culate a score for each word token, of the form:
ln(P (token|chem)) ? ln(P (token|nonchem))
If this score is above zero, the preclassifier clas-
sifies the token as chemical and gives it a tentative
type, based on its suffix. This can be considered to
be a ?first draft? of its named entity type. For exam-
ple tokens ending in ?-ation? are given the type RN,
whereas those ending in ?-ene? are given type CM.
3.5 The MEMM
The MEMM is a first-order MEMM, in that it has a
separate maximum-entropy model for each possible
preceeding tag. No information about the tag se-
quence was included directly in the feature set. We
use the OpenNLP MaxEnt classifier3 for maximum-
entropy classification.
The feature set for the MEMM is divided into
three types of features; type 1 (which apply to the
token itself), type 2 (which can apply to the token it-
self, the previous token and the next token) and type
3 (which can act as type 2 features, and which can
also form bigrams with other type 3 features).
An example type 1 feature would be 4G=ceti,
indicating that the 4-gram ceti had been found
in the token. An example type 2 feature would be
3http://maxent.sourceforge.net/
c-1:w=in, indicating that the previous token was
?in?. An example bigram constructed from type 3
features would be bg:0:1:ct=CJ w=acid, in-
dicating that the preclassifier had classified the token
as being of type CJ, and having a score above zero,
and that the next token was ?acid?.
Type 1 features include 1, 2, 3 and 4-grams of
characters found within the token, whether the to-
ken appeared in any of the word lists, and features to
represent the probability and type given by the pre-
classifier for that token. Type 2 features include the
token itself with any terminal letter ?s? removed, the
token converted to lowercase (if it matched the regex
.*[a-z][a-z].*), and a three-character suffix
taken from the token. The token itself was usually
used as a type 2 feature, unless it unless it was short
(less than four characters), or had been found to be
an ambiguous token during preclassifier training, in
which case it was type 3. Other type 3 features in-
clude a word shape feature, and tentative type of the
token if the preclassifier had classed it as chemical.
A few other features were used to cover a few spe-
cial cases, and were found to yield a slight improve-
ment during development.
After generating the features, a feature selection
based on log-likelihood ratios is used to remove the
least informative features, with a threshold set to re-
move about half of them. This was found during
development to have only a very small beneficial ef-
fect on the performance of the classifier, but it did
make training faster and produced smaller models.
This largely removed rare features which were only
found on a few non-chemical tokens.
3.6 The rescorer
The rescoring system works by constructing four
maximum entropy classifiers, one for each entity
type. The output of these classifiers is a probabil-
ity of whether or not a potential named entity really
is a correct named entity. The generation of features
is done on a per-document basis.
The key features in the rescorer represent the
probability of the potential entity as estimated by
the MEMM. The raw probability p is converted to
the logit score
l = ln(p) ? ln(1 ? p)
This mirrors the way probabilities are represented
58
within maximum entropy (aka logistic regression)
classifiers. If l is positive, int(min(15.0, l) ? 50)
instances 4 of the feature conf+ are generated, and
a corresponding technique is used if l is negative.
Before generating further features, it is necessary
to find entities that are ?blocked??entities that over-
lap with other entities of higher confidence. For ex-
ample, consider ?ethyl acetate?, which might give
rise to the named entity ?ethyl acetate? with 98%
confidence, and also ?ethyl? with 1% confidence and
?acetate? with 1% confidence. In this case, ?ethyl?
and ?acetate? would be blocked by ?ethyl acetate?.
Further features are generated by collecting to-
gether all of the unblocked5 potential entities of a
type that share the same string, calculating the max-
imum and average probability, and calculating the
difference between the p and those quantities.
Some acronym and abbreviation handling is also
performed. The system looks for named entities that
are surrounded by brackets. For each of these, a list
of features is generated that is then given to every
other entity of the same string. If there is a potential
entity to the left of the bracketed potential abbre-
viation, then features are generated to represent the
probability of that potential entity, and how well the
string form of that entity matches the potential ab-
breviation. If no potential entity is found to match
with, then features are generated to represent how
well the potential abbreviation matches the tokens
to the left of it. By this method, the rescorer can
gather information about whether a potential abbre-
viation stands for a named entity, something other
than a named entity?or whether it is not an abbre-
viation at all, and use that information to help score
all occurrences of that abbreviation in the document.
4 Evaluation
The systems were evaluated by 3-fold cross-
validation methodology, whereby the data was split
into three equal folds (in the case of the chemistry
4We found that 15.0 was a good threshold by experimenta-
tion on development data: papers annotated during trial runs of
the annotation process.
5Doing this without regards for blocking causes problems.
In a document containing both ?ethyl acetate? and ?ethyl
group?, it would be detrimental to allow the low confidence
for the ?ethyl? in ?ethyl acetate? to lower the confidence of the
?ethyl? in ?ethyl group?.
papers, each fold consists of one paper per journal.
For the PubMed abstracts, each fold consists of one
third of the total abstracts). For each fold, the system
was trained on the other two folds and then evaluated
on that fold, and the results were pooled.
The direct output from the system is a list of
putative named entities with start positions, end
positions, types and confidence scores. This list
was sorted in order of confidence?most confident
first?and each entity was classified as a true posi-
tive or a false positive according to whether an ex-
act match (start position, end position and type all
matched perfectly) could be found in the annotated
corpus. Also, the number of entities in the annotated
corpus was recorded.
Precision/recall curves were plotted from these
lists by selecting the first n elements, and calculat-
ing precision and recall taking all of the elements in
this sublist as true or false positives, and all the enti-
ties in the corpus that were not in the sublist as false
negatives. The value of n was gradually increased,
recording the scores at each point. The area under
the curve (treating precision as zero at recall values
higher than the highest reported) was used to calcu-
late mean average precision (MAP). Finally, F were
generated by selecting all of the entities with a con-
fidence score of 0.3 or higher.
0.0 0.2 0.4 0.6 0.8 1.00
.0
0.
2
0.
4
0.
6
0.
8
1.
0
Recall
Pr
ec
is
io
n
Full System
No Rescorer
No Preclassifier
No n?Grams
Customised LingPipe HMM
Pure LingPipe HMM
Figure 1: Evaluation on chemistry papers.
The results of this evaluation on the corpus of
59
chemistry papers is show in Figure 1. The full sys-
tem achieves 57.6% recall at 95% precision, 58.9%
precision at 90% recall, and 78.7% precision and
82.9% recall (F = 80.7%) at a confidence threshold
of 0.3. Also shown are the results of successively
eliminating parts of the system. ?No Rescorer? re-
moves the rescorer. In ?No Preclassifier?, the pre-
classifier is disabled, and all of the dictionaries ex-
tracted during the training of the preclassifier are
also disabled. Finally, in ?No n-Grams?, the 1-, 2-
, 3- and 4-grams used directly by the MEMM are
also disabled, showing the results of using a sys-
tem where no character-level n-grams are used at all.
These modifications apply successively?for exam-
ple, in the ?No n-Grams? case the rescorer and pre-
classifier are also disabled. These results validate the
the cascade of classifiers, and underline the impor-
tance of character-level n-grams in chemical NER.
We also show comparisons to an HMM-based
approach, based on LingPipe 3.4.0.6 This is es-
sentially the same system as described by Corbett
et al (2007), but operating in a confidence-based
mode. The HMMs used make use of character-level
n-Grams, but do not allow the use of the rich fea-
ture set used by the MEMM. The line ?Customised
LingPipe HMM? shows the system using the cus-
tom tokenisation and ChEBI-derived dictionary used
in the MEMM system, whereas the ?Pure LingPipe
HMM? shows the system used with the default to-
keniser and no external dictionaries. In the region
where precision is roughly equal to recall (mimick-
ing the operation of a first-best system), the fact that
the MEMM-based system outperforms an HMM is
no surprise. However, it is gratifying that a clear
advantage can be seen throughout the whole recall
range studied (0-97%), indicating that the training
processes for the MEMM are not excessively at-
tuned to the first-best decision boundary. This in-
creased accuracy comes at a price in the speed of
development, training and execution.
It is notable that we were not able to achieve ex-
tremes of recall at tolerable levels of precision us-
ing any of the systems, whereas it was possible for
LingPipe to achieve 99.99% recall at 7% precision in
the BioCreative 2006 evaluation. There are a num-
ber of potential reasons for this. The first is that the
6http://alias-i.com/lingpipe/
tokeniser used in all systems apart from the ?Pure
LingPipe HMM? system tries in general to make
as few token boundaries as possible; this leads to
some cases where the boundaries of the entities to
be recognised in the test paper occur in the middle
of tokens, thus making those entities unrecognisable
whatever the threshold. However this does not ap-
pear to be the whole problem. Other factors that may
have had an influence include the more generous
method of evaluation at BioCreative 2006, (where
several allowable alternatives were given for diffi-
cult named entities), and the greater quantity and di-
versity (sentences selected from a large number of
different texts, rather than a relatively small number
of whole full papers) of training data. Finally, there
might be some important difference between chem-
ical names and gene names.
0.0 0.2 0.4 0.6 0.8 1.00
.0
0.
2
0.
4
0.
6
0.
8
1.
0
Recall
Pr
ec
is
io
n
Full System
No Rescorer
No Preclassifier
No n?Grams
Customised LingPipe HMM
Pure LingPipe HMM
Figure 2: Evaluation on PubMed abstracts.
Figure 2 shows the results of running the sys-
tem on the set of annotated PubMed abstracts. The
full system achieves 60.3% recall at 95% precision,
49.1% precision at 90% recall, and 85.0% preci-
sion and 81.6% recall (F = 83.2%) at a confidence
threshold of 0.3. In PubMed abstracts, it is common
to define ad-hoc abbreviations for chemicals within
an abstract (e.g., the abstract might say ?dexametha-
sone (DEX)?, and then use ?DEX? and not ?dexam-
ethasone? throughout the rest of the abstract). The
rescorer provides a good place to resolve these ab-
60
breviations, and thus has a much larger effect than
in the case of chemistry papers where these ad hoc
abbreviations are less common. It is also notable
that the maximum recall is lower in this case. One
system?the ?Pure LingPipe HMM?, which uses a
different, more aggressive tokeniser from the other
systems?has a clear advantage in terms of maxi-
mum recall, showing that overcautious tokenisation
limits the recall of the other systems.
In some cases the systems studied behave
strangely, having ?spikes? of lowered precision at
very low recall, indicating that the systems can occa-
sionally be overconfident, and assign very high con-
fidence scores to incorrect named entities.
Corpus System MAP F
Chemistry Full 87.1% 80.8%
Chemistry No Rescorer 86.8% 81.0%
Chemistry No Preclassifier 82.7% 74.8%
Chemistry No n-Grams 79.2% 72.2%
Chemistry Custom LingPipe 75.9% 74.6%
Chemistry Pure LingPipe 66.9% 63.2%
Chemistry No Overlaps 82.9% 80.8%
PubMed Full 86.1% 83.2%
PubMed No Rescorer 83.3% 79.1%
PubMed No Preclassifier 81.4% 73.4%
PubMed No n-Grams 77.6% 70.6%
PubMed Custom LingPipe 78.6% 75.6%
PubMed Pure LingPipe 71.9% 66.1%
Table 2: F scores (at confidence threshold of 0.3) and
Mean Average Precision (MAP) values for Figs. 1-3.
Neither corpus contains enough data for the re-
sults to reach a plateau?using additional training
data is likely to give improvements in performance.
The ?No Overlaps? line in Figure 3 shows the ef-
fect of removing ?blocked? named entities (as de-
fined in section 3.6) prior to rescoring. This sim-
ulates a situation where an unambiguous inline an-
notation is required?for example a situation where
a paper is displayed with the named entities being
highlighted. This condition makes little difference
at low to medium recall, but it sets an effective max-
imum recall of 90%. The remaining 10% of cases
presumably consist of situations where the recog-
niser is finding an entity in the right part of the text,
but making boundary or type errors.
0.0 0.2 0.4 0.6 0.8 1.00
.0
0.
2
0.
4
0.
6
0.
8
1.
0
Recall
Pr
ec
is
io
n
Full System
No Overlaps
Figure 3: Evaluation on chemistry papers, showing ef-
fects of disallowing overlapping entities.
5 Conclusion
We have demonstrated that MEMMs can be adapted
to recognise chemical named entities, and that the
balance between precision and recall can be tuned
effectively, at least in the range of 0 - 95% recall.
The MEMM system is available as part of the OS-
CAR3 chemical named entity recognition system. 7
Acknowledgements
PTC thanks Peter Murray-Rust for supervision. We
thank the Royal Society of Chemistry for provid-
ing the papers, and the EPSRC (EP/C010035/1) for
funding. We thank the reviewers for their helpful
suggestions and regret that we did not have the time
or space to address all of the issues raised.
References
Colin Batchelor and Peter Corbett. 2007. Semantic en-
richment of journal articles using chemical named en-
tity recognition Proceedings of the ACL 2007 Demo
and Poster Sessions, pp 45-48. Prague, Czech Repub-
lic.
Bob Carpenter. 2007. LingPipe for 99.99% Recall of
Gene Mentions Proceedings of the Second BioCre-
ative Challenge Evaluation Workshop, 307-309.
7https://sourceforge.net/projects/oscar3-chem
61
Ann Copestake, Peter Corbett, Peter Murray-Rust, C. J.
Rupp, Advaith Siddharthan, Simone Teufel and Ben
Waldron. 2006. An Architecture for Language Tech-
nology for Processing Scientific Texts. Proceedings of
the 4th UK E-Science All Hands Meeting, Nottingham,
UK.
Peter Corbett, Colin Batchelor and Simone Teufel. 2007.
Annotation of Chemical Named Entities BioNLP
2007: Biological, translational, and clinical language
processing, pp 57-64. Prague, Czech Republic.
Aron Culotta and Andrew McCallum 2004. Confidence
Estimation for Information Extraction Proceedings of
Human Language Technology Conference and North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pp 109-112. Boston,
MA.
Kirill Degtyarenko, Paula de Matos, Marcus Ennis, Janna
Hastings, Martin Zbinden, AlanMcNaught, Rafael Al-
cantara, Michael Darsow, Mickael Guedj and Michael
Ashburner. 2008. ChEBI: a database and ontology for
chemical entities of biological interest. Nucleic Acids
Res, Vol. 36, Database issue D344-D350.
The Gene Ontology Consortium 2000. Gene Ontology:
tool for the unification of biology. Nature Genetics,
Vol. 25, 26-29.
Martin Krallinger and Lynette Hirschman, editors. 2007.
Proceedings of the Second BioCreative Challenge
Evaluation Workshop.
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, 1121-1128.
Sindey, Australia.
John Lafferty, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the Eighteenth International Conference
on Machine Learning, 282-289.
Andrew McCallum, Dayne Freitag and Fernando Pereira.
2000. Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation Proceedings of
the Seventeenth International Conference on Machine
Learning, 591-598. San Fransisco, CA.
Joe A. Townsend, Ann Copestake, Peter Murray-Rust, Si-
mone H. Teufel and Christopher A. Waudby. 2005.
Language Technology for Processing Chemistry Pub-
lications Proceedings of the fourth UK e-Science All
Hands Meeting, 247-253. Nottingham, UK.
Alexander Vasserman 2004 Identifying Chemical
Names in Biomedial Text: An Investigation of the
Substring Co-occurence Based Approaches Proceed-
ings of the Student Research Workshop at HLT-NAACL
W. John Wilbur, George F. Hazard, Jr., Guy Divita,
James G. Mork, Alan R. Aronson and Allen C.
Browne. 1999 Analysis of Biomedical Text for Chem-
ical Names: A Comparison of Three Methods Proc.
AMIA Symp. 176-180.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for Biomedical Named-Entity Recognition BioNLP
2007: Biological, translational, and clinical language
processing, pp 57-64. Prague, Czech Republic.
62
