Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 620?629, Prague, June 2007. c?2007 Association for Computational Linguistics
Recovery of Empty Nodes in Parse Structures
Denis Filimonov1
1University of Maryland
College Park, MD 20742
den@cs.umd.edu
Mary P. Harper1,2
2Purdue University
West Lafayette, IN 47907
mharper@casl.umd.edu
Abstract
In this paper, we describe a new algorithm
for recovering WH-trace empty nodes. Our
approach combines a set of hand-written
patterns together with a probabilistic model.
Because the patterns heavily utilize regu-
lar expressions, the pertinent tree structures
are covered using a limited number of pat-
terns. The probabilistic model is essen-
tially a probabilistic context-free grammar
(PCFG) approach with the patterns acting as
the terminals in production rules. We eval-
uate the algorithm?s performance on gold
trees and parser output using three differ-
ent metrics. Our method compares favorably
with state-of-the-art algorithms that recover
WH-traces.
1 Introduction
In this paper, we describe a new algorithm for re-
covering WH-trace empty nodes in gold parse trees
in the Penn Treebank and, more importantly, in
automatically generated parses. This problem has
only been investigated by a handful of researchers
and yet it is important for a variety of applications,
e.g., mapping parse trees to logical representations
and structured representations for language mod-
eling. For example, SuperARV language models
(LMs) (Wang and Harper, 2002; Wang et al, 2003),
which tightly integrate lexical features and syntactic
constraints, have been found to significantly reduce
word error in English speech recognition tasks. In
order to generate SuperARV LM training, a state-of-
the-art parser is used to parse training material and
then a rule-based transformer converts the parses to
the SuperARV representation. The transformer is
quite accurate when operating on treebank parses;
however, trees produced by the parser lack one im-
portant type of information ? gaps, particularly WH-
traces, which are important for more accurate ex-
traction of the SuperARVs.
Approaches applied to the problem of empty
node recovery fall into three categories. Dienes
and Dubey (2003) recover empty nodes as a pre-
processing step and pass strings with gaps to their
parser. Their performance was comparable to
(Johnson, 2002); however, they did not evaluate
the impact of the gaps on parser performance.
Collins (1999) directly incorporated wh-traces into
his Model 3 parser, but he did not evaluate gap in-
sertion accuracy directly. Most of the research be-
longs to the third category, i.e., post-processing of
parser output. Johnson (2002) used corpus-induced
patterns to insert gaps into both gold standard trees
and parser output. Campbell (2004) developed a
set of linguistically motivated hand-written rules for
gap insertion. Machine learning methods were em-
ployed by (Higgins, 2003; Levy and Manning, 2004;
Gabbard et al, 2006).
In this paper, we develop a probabilistic model
that uses a set of patterns and tree matching to guide
the insertion of WH-traces. We only insert traces of
non-null WH-phrases, as they are most relevant for
our goals. Our effort differs from the previous ap-
proaches in that we have developed an algorithm for
the insertion of gaps that combines a small set of ex-
pressive patterns with a probabilistic grammar-based
model.
620
2 The Model
We have developed a set of tree-matching patterns
that are applied to propagate a gap down a path in
a parse tree. Pattern examples appear in Figure 1.
Each pattern is designed to match a subtree (a root
and one or more levels below that root) and used to
guide the propagation of the trace into one or more
nodes at the terminal level of the pattern (indicated
using directed edges). Since tree-matching patterns
are applied in a top-down fashion, multiple patterns
can match the same subtree and allow alternative
ways to propagate a gap. Hence, we have developed
a probabilistic model to select among the alterna-
tive paths. We have created 24 patterns for WHNP
traces, 16 for WHADVP, 18 for WHPP, and 11 for
WHADJP.
Figure 1: Examples of tree-matching patterns
Before describing our model, we first introduce
some notation.
? TNij is a tree dominating the string of words be-
tween positions i and j with N being the label of
the root. We assume there are no unary chains like
N?X? ...?Y ?N (which could be collapsed to
a single node N ) in the tree, so that TNij uniquely
describes the subtree.
? A gap location gab,Ncd is represented as a tuple
(gaptype, ancstr(a, b,N), c, d), where gaptype
is the type of the gap, (e.g., whnp for a WHNP
trace), ancstr(a, b,N) is the gap?s nearest ances-
tor, with a and b being its span and N being its
label, and c and d indicating where the gap can
be inserted. Note that a gap?s location is specified
precisely when c = d. If the gap is yet to be in-
serted into its final location but will be inserted
somewhere inside ancstr(a, b,N), then we set
c = a and d = b.
? ancstr(a, b,N) in the tuple for gab,Nxy is the tree
TNab .
? p(gab,Nxy |gaptype, TNij ) is the probability that a
gap of gaptype is located between x and y, with a
and b being the span of its ancestor, and i ? a ?
x ? y ? b ? j.
Given this notation, our model is tasked to identify
the best location for the gap in a parse tree among
the alternatives, i.e.,
argmax
x,a,b,N
Pr(gab,Nxx |T, gaptype)
where gab,Nxx represents a gap location in a tree, and
T = TNij is the subtree of the parse tree whose
root node is the nearest ancestor node dominating
the WH-phrase, excluding the WH-node itself, and
gaptype is the type of the gap. In order to simplify
the notation, we will omit the root labels N in TNij
and gab,Nxy , implying that they match where appropri-
ate.
To guide this model, we utilize tree-matching pat-
terns (see Figure 1), which are formally defined as
functions:
ptrn : T ? G ? ? ? {none}
where T is the space of parse trees, G is the space
of gap types, and ? is the space of gaps gabcd ,
and none is a special value representing failure to
match1. The application of a pattern is defined as:
app(ptrn, ?, gaptype) = ptrn(?, gaptype), where
? ? T and gaptype ? G. We define application of
patterns as follows:
app(ptrn, Tij , gaptype) ? gabxy : i ? a ? x < y ? b ? j
app(ptrn, Tij , gaptype) ? gabxx : i ? a ? x ? b ? j
app(ptrn, Tij , gaptype) ? none
Because patterns are uniquely associated with spe-
cific gap types, we will omit gaptype to simplify the
notation. Application is a function defined for every
pair (ptrn, Tij) with fixed gaptype. Patterns are ap-
plied to the root of Tij , not to an arbitrary subtree.
Consider an example of pattern application shown
in Figure 2. The tree contains a relative clause such
that the WHNP-phrase that was moved from some
location inside the subtree of its sister node S.
2
viewers
3
will
4
tune
5
in
6
to
7
see
8
1Modeling conjunction requires an alternative definition for
patterns: ptrn : T ? G ? Powerset(?) ? {none}. For the
sake of simplicity, we ignore conjunctions in the following dis-
cussion, except for in the few places where it matters, since this
has little impact on the development of our model.
621
Figure 2: A pattern application example
Now suppose there is a pattern P1 that matches
the tree T28 indicating that the gap is some-
where in its subtree T38 (will tune in to see), i.e.,
app(P1, T28) ? g3838 . The process of applying pat-
terns continues until the pattern P4 proposes an ex-
act location for the gap: app(P4, T78) = g7888 .
Figure 3: Another pattern application example
Suppose that, in addition to the pattern applica-
tions shown in Figure 2, there is one more, namely:
app(P5, T48) ? g4866 . The sequence of patterns
P1, P2, P5 proposes an alternative grammatically
plausible location for the gap, as shown in Figure
3. Notice that the combination of the two sequences
produces a tree of patterns, as shown in Figure 4,
and this pattern tree covers much of the structure of
the T28 subtree.
2.1 Tree Classes
The number of unique subtrees that contain WH-
phrases is essentially infinite; hence, modeling them
directly is infeasible. However, trees with varying
details, e.g., optional adverbials, often can be char-
P1
P2
P3C
D P4,$
E
A
B
F
P5,$
Figure 4: Pattern tree
acterized by the same tree of patterns. Hence, we
can represent the space of trees by utilizing a rela-
tively small set of classes of trees that are determined
by their tree of pattern applications.
Let ? be the set of all patterns. We define the set
of patterns matching tree Tij as follows:
M(Tij) = {P | P ? ? ? app(P, Tij) 6= none}
To enable recursive application:
app(ptrn, gabxy) =
{ app(ptrn, Tab) if x < y
none if x = y
A Pattern Chain PC is a sequence of pairs
of patterns and sets of pattern sets, terminated by
$, i.e., ( p1M1 ,
p2
M2 , ...
pn
Mn , $), where ?i pi ? Mi ?
?. Mi = M(Tab), where Tab is the result of
consequent application of the first i ? 1 patterns:
app(pi?1, app(pi?2, ..., app(p1, T??))) = gabxy, and
where T?? is the subtree we started with, (T28 in the
example above). We define the application of a pat-
tern chain PC = ( p1M1 ,
p2
M2 , ...
pn
Mn , $) to a tree Tij
as:
app(PC, Tij) = app(pn, ...app(p2, app(p1, Tij)))
It is important to also define a function to map
a tree to the set of pattern chains applicable to a
particular tree. The pseudocode for this function
called FindPCs appears in Figure 52. When ap-
plied to Tij , this function returns the set of all pat-
tern chains, applications of which would result in
concrete gap locations. The algorithm is guaranteed
to terminate as long as trees are of finite depth and
each pattern moves the gap location down at least
one level in the tree at each iteration. Using this
function, we define Tree Class (TC) of a tree Tij
as TC(Tij) = FindPCs(Tij).
2list ? element means ?append element to list?.
622
function FindPCs?(Tij , PC, allPCs) {
Mij ? {P | P ? ? ? app(P, Tij) 6= none}
forall P ? Mij
gabxy ? app(P, Tij)
PC ? PC ? PMij
if x = y then // gabxy is a concrete location
allPCs ? allPCs ? {PC ? $}
else
allPCs ? FindPCs?(Tab, PC, allPCs)
return allPCs }
function FindPCs(Tij) { return FindPCs?(Tij , [ ], ?) }
Figure 5: Pseudocode for FindPCs
In the case of a conjunction, the function Find-
PCs is slightly more complex. Recall that in this
case app(P, Tij) produces a set of gaps or none. The
pseudocode for this case appears in Figure 6.
2.2 A Gap Automaton
The set of pattern chains constructed by the function
FindPCs can be represented as a pattern tree with
patterns being the edges. For example, the pattern
tree in Figure 4 corresponds to the tree displayed in
Figures 2 and 3.
This pattern tree captures the history of gap prop-
agations beginning at A. Assuming at that point only
pattern P1 is applicable, subtree B is produced. If P2
yields subtree C, and at that point patterns P3 and
P5 can be applied, this yields subtree D and exact
location F (which is expressed by the termination
symbol $), respectively. Finally, pattern P4 matches
subtree D and proposes exact gap location E. It is
important to note that this pattern tree can be thought
of as an automaton, with A,B,C,D,E, and F be-
ing the states and the pattern applications being the
transitions.
Now, let us assign meaning of the states
A,B,C, and D to be the set of matching patterns,
i.e., A = {P1}, B = {P2}, C = {P3, P5}, D = {P4}, and
E = F = ?. Given this representation, the pattern
chains for the insertion of the gaps in our example
would be as follows:
({P1}) P1? ({P2}) P2? ({P3, P5}) P3? ({P4}) P4,$?? (?)
({P1}) P1? ({P2}) P2? ({P3, P5}) P5,$?? (?)
With this representation, we can create a regular
grammar using patterns as the terminals and their
function CrossProd(PC1, PC2) {
prod ? ?
forall pci ? PC1
forall pcj ? PC2 : prod ? prod?{pci?pcj}
return prod }
function FindPCs(Tij) {
Mij ? {P | P ? ? ? app(P, Tij) 6= none}
newPCs ? ?
forall P ? Mij
PCs ? {[ ]}
forall gabxy ? app(P, Tij)
if x = y then
forall pc ? PCs : pc ? pc ? $
else
PCs ? CrossProd(PCs,FindPCs(Tab))
forall pc ? PCs : pc ? PMij ? pc
newPCs ? newPCs ? PCs
return newPCs }
The set app(P, Tij) must be ordered, so that
branches of conjunction are concatenated in a well de-
fined order.
Figure 6: Pseudocode for FindPCs in the case of
conjunction
powerset as the non-terminals (adding a few more
details like the start symbol) and production rules
such as {P2} ? P2 {P3, P5}. However, for our exam-
ple the chain of patterns applied P1, P2, P3, P4, $ could
generate a pattern tree that is incompatible with the
original tree. For example:
({P1}) P1? ({P2}) P2? ({P3, P5}) P3? ({P3, P4}) P4,$?? (?)
which might correspond to something like ?that
viewers will tune in to expect to see.? Note that this
pattern chain belongs to a different tree class, which
incidentally would have inserted the gap at a differ-
ent location (VP see gap).
To overcome this problem we add additional con-
straints to the grammar to ensure that all parses the
grammar generates belong to the same tree class.
One way to do this is to include the start state of
a transition as an element of the terminal, e.g., P2{P2} ,
P3
{P3,P5} . That is, we extend the terminals to include
the left-hand side of the productions they are emitted
from, e.g.,
{P2} ? P2{P2} {P3, P5}
623
{P3, P5} ? P3{P3, P5} {P4}
and the sequence of terminals becomes:
P1
{P1}
P2
{P2}
P3
{P3,P5}
P4
{P4} $.
Note that the grammar is unambiguous. For such
a grammar, the question ?what is the probability of a
parse tree given a string and grammar? doesn?t make
sense; however, the question ?what is the probability
of a string given the grammar? is still valid, and this
is essentially what we require to develop a genera-
tive model for gap insertion.
2.3 The Pattern Grammar
Let us define the pattern grammar more rigorously.
Let ? be the set of patterns, and ?? ? ? be the set
of terminal patterns3. Let pset(P ) be the set of all
subsets of patterns which include the pattern P , i.e.,
pset(P ) = {? ? {P} | ? ? powerset(?)}
? Let T = { Ppset(P ) | P ? ?}
?{$} be the set of
terminals, where $ is a special symbol4.
? Let N = {S}? powerset(?) be the set of non-
terminals with S being the start symbol.
? Let P be the set of productions, defined as the
union of the following sets:
1. {S ? ? | ? ? powerset(?)}.
2. {? ? P? ? | P ? ???? , ? ? pset(P ) and ? ?powerset(?)}. These are nonterminal transi-
tions, note that they emit only non-terminal pat-
terns.
3. {? ? P? $ | P ? ?? and ? ? pset(P )}. These
are the terminal transitions, they emit a termi-
nal pattern and the symbol $.
4. {? ? P? ?1 . . . ?n | P ? ? ? ?? , ? ?pset(P ) and ?i?[1..n] ?i ? powerset(?)}.
This rule models conjunction with n branches.
2.4 Our Gap Model
Given the grammar defined in the previous subsec-
tion, we will define a probabilistic model for gap in-
sertion. Recall that our goal is to find:
argmax
x,a,b
Pr(gabxx|T )
Just like the probability of a sentence is obtained by
summing up the probabilities of its parses, the prob-
ability of the gap being at gabxx is the sum of proba-
bilities of all pattern chains that yield gabxx.
3Patterns that generate exact position for a gap.
4Symbol $ helps to separate branches in strings with con-
junction.
Pr(gabxx|T ) =
?
pci??
Pr(pci|T )
where ? = {pc | app(pc, T ) = gabxx}. Note that
pci ? TC(T ) by definition.
For our model, we use two approximations. First,
we collapse a tree T into its Tree Class TC(T ), ef-
fectively ignoring details irrelevant to gap insertion:
Pr(pci|T ) ? Pr(pci|TC(T ))
Figure 7: A pattern tree with the pattern chain
ABDGM marked using bold lines
Consider the pattern tree shown in Figure 7. The
probability of the pattern chain ABDGM given the
pattern tree can be computed as:
Pr(ABDGM |TC(T )) = Pr(ABDGM,TC(T ))Pr(TC(T ))
= NR(ABDGM,TC(T ))
NR(TC(T ))
where NR(TC(T )) is the number of occurrences
of the tree class TC(T ) in the training corpus and
NR(ABDGM,TC(T )) is the number cases when
the pattern chain ABDGM leads to a correct gap in
trees corresponding to the tree class TC(T ). For
many tree classes, NR(TC(T )) may be a small
number or even zero, thus this direct approach can-
not be applied to the estimation of Pr(pci|TC(T )).
Further approximation is required to tackle the spar-
sity issue.
In the following discussion, XY will denote
an edge (pattern) between vertices X and Y in
624
the pattern tree shown in Figure 7. Note that
Pr(ABDGM |TC(T )) can be represented as:
Pr(AB|TC(T ), A)? Pr(BD|TC(T ), AB)?
?Pr(DG|TC(T ), ABD)? Pr(GM |TC(T ), ABDG)
We make an independence assumption, specifi-
cally, that Pr(BD|TC(T ), AB) depends only on
states B, D, and the edge between them, not on
the whole pattern tree or the edges above B, i.e.,
Pr(BD|TC(T ), AB) ? Pr(BD,D|B). Note that
this probability is equivalent to the probability of a
production Pr(B BD? D) of a PCFG.
Recall that the meaning assigned to a state
in pattern grammar in Section 2.2 is the set of
patterns matching at that state. Thus, accord-
ing to that semantics, only the edges displayed
bold in Figure 8 are involved in computation of
Pr(B BD? D). Written in the style we used for
our grammar, the production is {BD,BE,BF} ?
BD
{BD,BE,BF}{DG,DH}.
Figure 8: The context considered for estimation of
the probability of transition from B to D
Pattern trees are fairly shallow (partly because
many patterns cover several layers in a parse tree
as can be seen in Figures 1 and 2); therefore, the
context associated with a production covers a good
part of a pattern tree. Another important observa-
tion is that the local configuration of a node, which
is described by the set of matching patterns, is the
most relevant to the decision of where the gap is to
be propagated5. This is the reason why the states are
represented this way.
Formally, the second approximation we make is
5We have evaluated a model that only uses
Pr(BD|{BD,BE,BF}) for the probability of taking
BD and found it performs only slightly worse than the model
presented here.
as follows:
Pr(pci|TC(T )) ? Pr(pci|G)
where G is a PCFG model based on the grammar
described above.
Pr(pci|G) =
?
prodj?P(pci)
Pr(prodj |G)
where P(pci) is the parse of the pattern chain pci
which is a string of terminals of G. Combining the
formulae:
Pr(gabxx|T ) ?
?
pci??
Pr(pci|G)
Finally, since Pr(TC(T )|G) is a constant for T ,
argmax
x,a,b
Pr(gabxx|T ) ? argmaxx,a,b
?
pci??
Pr(pci|G)
To handle conjunction, we must express the fact
that pattern chains yield sets of gaps. Thus, the goal
becomes:
argmax
(x1,a1,b1),...,(xn,an,bn)
Pr({ga1b1x1x1 , . . . , ganbnxnxn}|T )
Pr({ga1b1x1x1 , . . . , ganbnxnxn}|T ) =
?
pci??
Pr(pci|T )
where ? = {pc | app(pc, T ) =
{ga1b1x1x1 , . . . , ganbnxnxn}}. The remaining equations
are unaffected.
2.5 Smoothing
Even for the relatively small number of patterns,
the number of non-terminals in the grammar can
potentially be large (2|?|). This does not happen
in practice since most patterns are mutually exclu-
sive. Nonetheless, productions, unseen in the train-
ing data, do occur and their probabilities have to be
estimated. Rewriting the probability of a transition
Pr(A ? aA B) as P(A, a,B), we use the following in-
terpolation:
P?(A, a,B) = ?1P(A, a,B) + ?2P(A, a)
+?3P(A,B) + ?4P(a,B) + ?5P(a)
We estimate the parameters on the held out data
(section 24 of WSJ) using a hill-climbing algorithm.
625
3 Evaluation
3.1 Setup
We compare our algorithm under a variety of condi-
tions to the work of (Johnson, 2002) and (Gabbard
et al, 2006). We selected these two approaches be-
cause of their availability6. In addition, (Gabbard et
al., 2006) provides state-of-the-art results. Since we
only model the insertion of WH-traces, all metrics
include co-indexation with the correct WH phrases
identified by their type and word span.
We evaluate on three metrics. The first metric,
which was introduced by Johnson (2002), has been
widely reported by researchers investigating gap in-
sertion. A gap is scored as correct only when it has
the correct type and string position. The metric has
the shortcoming that it does not require correct at-
tachment into the tree.
The second metric, which was developed by
Campbell (2004), scores a gap as correct only when
it has the correct gap type and its mother node has
the correct nonterminal label and word span. As
Campbell points out, this metric does not restrict the
position of the gap among its siblings, which in most
cases is desirable; however, in some cases (e.g., dou-
ble object constructions), it does not correctly detect
errors in object order. This metric is also adversely
affected by incorrect attachments of optional con-
stituents, such as PPs, due to the span requirement.
To overcome the latter issue with Campbell?s met-
ric, we propose to use a third metric that evaluates
gaps with respect to correctness of their lexical head,
type of the mother node, and the type of the co-
indexed wh-phrase. This metric differs from that
used by Levy and Manning (2004) in that it counts
only the dependencies involving gaps, and so it rep-
resents performance of the gap insertion algorithm
more directly.
We evaluate gap insertion on gold trees from sec-
tion 23 of the Wall Street Journal Penn Treebank
(WSJ) and parse trees automatically produced using
the Charniak (2000) and Bikel (2004) parsers. These
parsers were trained using sections 00 through 22 of
the WSJ with section 24 as the development set.
Because our algorithm inserts only traces of non-
empty WH phrases, to fairly compare to Johnson?s
and Gabbard?s performance on WH-traces alone, we
6Johnson?s source code is publicly available, and Ryan Gab-
bard kindly provided us with output trees produced by his sys-
tem.
remove the other gap types from both the gold trees
and the output of their algorithms. Note that Gab-
bard et al?s algorithm requires the use of function
tags, which are produced using a modified version
of the Bikel parser (Gabbard et al, 2006) and a sep-
arate software tool (Blaheta, 2003) for the Charniak
parser output.
For our algorithm, we do not utilize function tags,
but we automatically replace the tags of auxiliary
verbs in tensed constructions with AUX prior to in-
serting gaps using tree surgeon (Levy and Andrew,
2006). We found that Johnson?s algorithm more
accurately inserts gaps when operating on auxified
trees, and so we evaluate his algorithm using these
modified trees.
In order to assess robustness of our algorithm, we
evaluate it on a corpus of a different genre ? Broad-
cast News Penn Treebank (BN), and compare the re-
sult with Johnson?s and Gabbard?s algorithms. The
BN corpus uses a modified version of annotation
guidelines, with some of the modifications affecting
gap placement.
Treebank 2 guidelines (WSJ style):
(SBAR (WHNP-2 (WP whom))
(S (NP-SBJ (PRP they))
(VP (VBD called)
(S (NP-SBJ (-NONE- *T*-2))
(NP-PRD (NNS exploiters))))))
Treebank 2a guidelines (BN style):
(SBAR-NOM (WHNP-1 (WP what))
(S (NP-SBJ (PRP they))
(VP (VBP call)
(NP-2 (-NONE- *T*-1))
(S-CLR (NP-SBJ (-NONE- *PRO*-2))
(NP-PRD (DT an) (NN epidemic))))))
Since our algorithms were trained on WSJ, we ap-
ply tree transformations to the BN corpus to convert
these trees to WSJ style. We also auxify the trees as
described previously.
3.2 Results
Table 1 presents gap insertion F measure for John-
son?s (2002) (denoted J), Gabbard?s (2006) (denoted
G), and our (denoted Pres) algorithms on section 23
gold trees, as well as on parses generated by the
Charniak and Bikel parsers. In addition to WHNP
and WHADVP results that are reported in the liter-
ature, we also present results for WHPP gaps even
though there is a small number of them in section
23 (i.e., 22 gaps total). Since there are only 3 non-
empty WHADJP phrases in section 23, we omit
them in our evaluation.
626
Gold Trees Charniak Parser Bikel Parser
Metric J G Pres J G Pres J G Pres
WHNP Johnson 94.8 90.7 97.9 89.8 86.3 91.5 90.2 86.8 92.6
Campbell 94.8 97.0 99.1 81.9 83.8 83.5 80.7 81.5 82.2
Head dep 94.8 97.0 99.1 88.8 90.6 91.0 89.1 91.4 92.3
WHADVP Johnson 75.5 91.4 96.5 61.4 78.0 80.0 61.0 77.9 77.2
Campbell 74.5 89.1 95.0 61.4 71.7 78.4 60.0 71.5 74.8
Head dep 75.5 89.8 95.8 64.4 78.0 84.7 63.0 77.1 80.3
WHPP Johnson 58.1 N/R 72.7 35.7 N/R 55.0 42.9 N/R 53.7
Campbell 51.6 N/R 86.4 28.6 N/R 60.0 35.7 N/R 63.4
Head dep 51.6 N/R 86.4 35.7 N/R 70.0 35.7 N/R 73.2
Table 1: F1 performance on section 23 of WSJ (N/R indicates not reported)
Compared to Johnson?s and Gabbard?s algorithm,
our algorithm significantly reduces the error on
gold trees (table 1). Operating on automatically
parsed trees, our system compares favorably on
all WH traces, using all metrics, except for two
instances: Gabbard?s algorithm has better perfor-
mance on WHNP, using Cambpell?s metric and trees
generated by the Charniak parser by 0.3% and on
WHADVP, using Johnson?s metric and trees pro-
duces by the Bikel parser by 0.7%. However, we
believe that the dependency metric is more appropri-
ate for evaluation on automatically parsed trees be-
cause it enforces the most important aspects of tree
structure for evaluating gap insertion. The relatively
poor performance of Johnson?s and our algorithms
on WHPP gaps compared that on WHADVP gaps
is probably due, at least in part, to the significantly
smaller number of WHPP gaps in the training corpus
and the relatively wider range of possible attachment
sites for the prepositional phrases.
Table 2 displays how well the algorithms trained
on WSJ perform on BN. A large number of the er-
rors are due to FRAGs which are far more com-
mon in the speech corpus than in WSJ. WHPP and
WHADJP, although more rare than the other types,
are presented for reference.
3.3 Error Analysis
It is clear from the contrast between the results based
on gold standard trees and the automatically pro-
duced parses in Table 1 that parse error is a major
source of error. Parse error impacts all of the met-
rics, but the patterns of errors are different. For WH-
NPs, Campbell?s metric is lower than the other two
across all three algorithms, suggesting that this met-
ric is adversely affected by factors that do not im-
pact the other metrics (most likely the span of the
gap?s mother node). For WHADVPs, the metrics
show a similar degradation due to parse error across
the board. We are reluctant to draw conclusions for
the metrics on WHPPs; however, it should be noted
that the position of the PP should be less critical for
evaluating these gaps than their correct attachment,
suggesting that the head dependency metric would
more accurately reflect the performance of the sys-
tem for these gaps.
Campbell?s metric has an interesting property: in
parse trees, we can compute the upper bound on re-
call by simply checking whether the correct WH-
phrase and gap?s mother node exist in the parse tree.
We present recall results and upper bounds in Table
3. Clearly the algorithms are performing close to the
upper bound for WHNPs when we take into account
the impact of parse errors on this metric. Clearly
there is room for improvement for the WHPPs.
Metric J G Pres
WHNP Johnson 88.0 90.3 92.0
Campbell 88.2 94.0 95.3
Head dep 88.3 94.0 95.3
WHADVP Johnson 76.4 92.0 94.3
Campbell 76.3 88.2 92.4
Head dep 76.3 88.5 92.5
WHPP Johnson 56.6 N/R 75.7
Campbell 60.4 N/R 91.9
Head dep 60.4 N/R 91.9
WHADJP Johnson N/R N/R 89.8
Campbell N/R N/R 85.7
Head dep N/R N/R 85.7
Table 2: F1 performance on gold trees of BN
In addition to parser errors, which naturally have
the most profound impact on the performance, we
found the following sources of errors to have impact
on our results:
? Annotation errors and inconsistency in PTB,
which impact not only the training of our system,
but also its evaluation.
627
Charniak Parser J G Pres UB
WHNP 81.9 82.8 83.5 84.0
WHADVP 61.4 71.7 78.4 81.1
WHPP 28.6 N/R 60.0 86.4
Bikel Parser J G Pres UB
WHNP 77.0 80.5 81.5 82.0
WHADVP 47.2 70.1 74.8 78.0
WHPP 22.7 N/R 59.1 81.8
Table 3: Recall on trees produced by the Charniak
and Bikel parsers and their upper bounds (UB)
1. There are some POS labeling errors that con-
fuse our patterns, e.g.,
(SBAR (WHNP-3 (IN that))
(S (NP-SBJ (NNP Canada))
(VP (NNS exports)
(NP (-NONE- *T*-3))
(PP ...))))
2. Some WHADVPs have gaps attached in the
wrong places or do not have gaps at all, e.g.,
(SBAR (WHADVP (WRB when))
(S (NP (PRP he))
(VP (VBD arrived)
(PP (IN at)
(NP ...))
(ADVP (NP (CD two)
(NNS days))
(JJ later)))))
3. PTB annotation guidelines leave it to annota-
tors to decide whether the gap should be at-
tached at the conjunction level or inside its
branches (Bies et al, 1995) leading to incon-
sistency in attachment decisions for adverbial
gaps.
? Lack of coverage: Even though the patterns we
use are very expressive, due to their small number
some rare cases are left uncovered.
? Model errors: Sometimes despite one of the appli-
cable pattern chains proposes the correct gap, the
probabilistic model chooses otherwise. We be-
lieve that a lexicalized model can eliminate most
of these errors.
4 Conclusions and Future Work
The main contribution of this paper is the de-
velopment of a generative probabilistic model for
gap insertion that operates on subtree structures.
Our model achieves state-of-the-art performance,
demonstrating results very close to the upper bound
on WHNP using Campbell?s metric. Performance
for WHADVPs and especially WHPPs, however,
has room for improvement.
We believe that lexicalizing the model by adding
information about lexical heads of the gaps may re-
solve some of the errors. For example:
(SBAR (WHADVP-3 (WRB when))
(S (NP (NNP Congress))
(VP (VBD wanted)
(S (VP (TO to)
(VP (VB know) ...)))
(ADVP (-NONE- *T*-3)))))
(SBAR (WHADVP-1 (WRB when))
(S (NP (PRP it))
(VP (AUX is)
(VP (VBN expected)
(S (VP (TO to)
(VP (VB deliver) ...
(ADVP (-NONE- *T*-1)))))))))
These sentences have very similar structure, with
two potential places to insert gaps (ignoring re-
ordering with siblings). The current model inserts
the gaps as follows: when Congress (VP wanted (S
to know) gap) and when it is (VP expected (S to
deliver) gap), making an error in the second case
(partly due to the bias towards shorter pattern chains,
typical for a PCFG). However, deliver is more likely
to take a temporal modifier than know.
In future work, we will investigate methods for
adding lexical information to our model in order to
improve the performance on WHADVPs and WH-
PPs. In addition, we will investigate methods for
automatically inferring patterns from a treebank cor-
pus to support fast porting of our approach to other
languages with treebanks.
5 Acknowledgements
We would like to thank Ryan Gabbard for provid-
ing us output from his algorithm for evaluation. We
would also like to thank the anonymous reviewers
for invaluable comments. This material is based
upon work supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of DARPA.
References
A. Bies, M. Ferguson, K. Katz, and R. MacIntyre. 1995.
Bracketing guidelines for treebank II style Penn Tree-
bank project. Technical report.
D. M. Bikel. 2004. On the Parameter Space of Gen-
628
erative Lexicalized Statistical Parsing Models. Ph.D.
thesis, University of Pennsylvania.
D. Blaheta. 2003. Function Tagging. Ph.D. thesis,
Brown University.
R. Campbell. 2004. Using linguistic principles to re-
cover empty categories. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics.
M. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
P. Dienes and A. Dubey. 2003. Antecedent recovery:
Experiments with a trace tagger. In Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing.
R. Gabbard, S. Kulick, and M. Marcus. 2006. Fully pars-
ing the Penn Treebank. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
D. Higgins. 2003. A machine-learning approach to the
identification of WH gaps. In Proceedings of the An-
nual Meeting of the European Chapter of the Associa-
tion for Computational Linguistics.
M. Johnson. 2002. A simple pattern-matching algorithm
for recovering empty nodes and their antecedents. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
R. Levy and G Andrew. 2006. Tregex and Tsurgeon:
Tools for querying and manipulating tree data struc-
tures. In Proceedings of LREC.
R. Levy and C. Manning. 2004. Deep dependencies
from context-free statistical parsers: Correcting the
surface dependency approximation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
W. Wang and M. P. Harper. 2002. The SuperARV lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources in language
modeling. In Proceedings of the Empirical Methods
in Natural Language Processing.
W. Wang, M. P. Harper, and A. Stolcke. 2003. The ro-
bustness of an almost-parsing language model given
errorful training data. In Proceedings of the IEEE In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing.
629
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1114?1123,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Joint Language Model With Fine-grain Syntactic Tags
Denis Filimonov1
1Laboratory for Computational Linguistics
and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
den@cs.umd.edu
Mary Harper1,2
2Human Language Technology
Center of Excellence
Johns Hopkins University
mharper@umiacs.umd.edu
Abstract
We present a scalable joint language
model designed to utilize fine-grain syn-
tactic tags. We discuss challenges such
a design faces and describe our solutions
that scale well to large tagsets and cor-
pora. We advocate the use of relatively
simple tags that do not require deep lin-
guistic knowledge of the language but pro-
vide more structural information than POS
tags and can be derived from automati-
cally generated parse trees ? a combina-
tion of properties that allows easy adop-
tion of this model for new languages. We
propose two fine-grain tagsets and evalu-
ate our model using these tags, as well as
POS tags and SuperARV tags in a speech
recognition task and discuss future direc-
tions.
1 Introduction
In a number of language processing tasks, particu-
larly automatic speech recognition (ASR) and ma-
chine translation (MT), there is the problem of se-
lecting the best sequence of words from multiple
hypotheses. This problem stems from the noisy
channel approach to these applications. The noisy
channel model states that the observed data, e.g.,
the acoustic signal, is the result of some input
translated by some unknown stochastic process.
Then the problem of finding the best sequence of
words given the acoustic input, not approachable
directly, is transformed into two separate models:
argmax
w
n
1
p(w
n
1
|A) = argmax
w
n
1
p(A|w
n
1
) ? p(w
n
1
)
(1)
where A is the acoustic signal and wn
1
is a se-
quence of n words. p(A|wn
1
) is called an acoustic
model and p(wn
1
) is the language model1.
Typically, these applications use language mod-
els that compute the probability of a sequence in a
generative way:
p(w
n
1
) =
n
?
i=1
p(w
i
|w
i?1
1
)
Approximation is required to keep the parameter
space tractable. Most commonly the context is re-
duced to just a few immediately preceding words.
This type of model is called an ngram model:
p(w
i
|w
i?1
1
) ? p(w
i
|w
i?1
i?n+1
)
Even with limited context, the parameter space can
be quite sparse and requires sophisticated tech-
niques for reliable probability estimation (Chen
and Goodman, 1996). While the ngram models
perform fairly well, they are only capable of cap-
turing very shallow knowledge of the language.
There is extensive literature on a variety of
methods that have been used to imbue models
with syntactic and semantic information in differ-
ent ways. These methods can be broadly catego-
rized into two types:
? The first method uses surface words within
its context, sometimes organizing them into
deterministic classes. Models of this type in-
clude: (Brown et al, 1992; Zitouni, 2007),
which use semantic word clustering, and
(Bahl et al, 1990), which uses variable-
length context.
? The other method adds stochastic variables
to express the ambiguous nature of surface
words2. To obtain the probability of the next
1Real applications use argmax
w
n
1
p(A|w
n
1
)?p(w
n
1
)
?
?n
?
instead of Eq. 1, where ? and ? are set to optimize a heldout
set.
2These variables have to be predicted by the model.
1114
word we need to sum over all assignments of
the stochastic variables, as in Eq. 2.
p(w
i
|w
i?1
1
) =
?
t
1
...t
i
p(w
i
t
i
|w
i?1
1
t
i?1
1
) (2)
=
?
t
1
...t
i
p(w
i
t
i
|w
i?1
1
t
i?1
1
)p(w
i?1
1
t
i?1
1
)
?
t
1
...t
i?1
p(w
i?1
1
t
i?1
1
)
Models of this type, which we call joint
models since they essentially predict joint
events of words and some random vari-
able(s), include (Chelba and Jelinek, 2000)
which used POS tags in combination with
?parser instructions? for constructing a full
parse tree in a left-to-right manner; (Wang
et al, 2003) used SuperARVs (complex tu-
ples of dependency information) without re-
solving the dependencies, thus called almost
parsing; (Niesler and Woodland, 1996; Hee-
man, 1999) utilize part of speech (POS) tags.
Note that some models reduce the context by
making the following approximation:
p(w
i
t
i
|w
i?1
1
t
i?1
1
) ? p(w
i
|t
i
)?p(t
i
|t
i?1
1
) (3)
thus, transforming the problem into a stan-
dard HMM application. However, these
models perform poorly and have only been
able to improve over the ngram model when
interpolated with it (Niesler and Woodland,
1996).
Although joint models have the potential
to better express variability in word usage
through the introduction of additional latent
variables, they do not necessarily perform
better because the increased dimensionality
of the context substantially increases the al-
ready complex problem of parameter estima-
tion. The complexity of the space also makes
computation of the probability a challenge
because of space and time constraints. This
makes the choice of the random variables a
matter of utmost importance.
The model presented in this paper has some el-
ements borrowed from prior work, notably (Hee-
man, 1999; Xu and Jelinek, 2004), while others
are novel.
1.1 Paper Outline
The message we aim to deliver in this paper can
be summarized in two theses:
? Use fine-grain syntactic tags in a joint LM.
We propose a joint language model that can
be used with a variety of tagsets. In Section
2, we describe those that we used in our ex-
periments. Rather than tailoring our model to
these tagsets, we aim for flexibility and pro-
pose an information theoretic framework for
quick evaluation for tagsets, thus simplifying
the creation of new tagsets. We show that
our model with fine-grain tagsets outperform
the coarser POS model, as well as the ngram
baseline, in Section 5.
? Address the challenges that arise in a joint
language model with fine-grain tags. While
the idea of using joint language modeling is
not novel (Chelba and Jelinek, 2000; Hee-
man, 1999), nor is the idea of using fine-grain
tags (Bangalore, 1996; Wang et al, 2003),
none of prior papers focus on the issues that
arise from the combination of joint language
modeling with fine-grain tags, both in terms
of reliable parameter estimation and scalabil-
ity in the face of the increased computational
complexity. We dedicate Sections 3 and 4 to
this problem.
In Section 6, we summarize conclusions and lay
out directions for future work.
2 Structural Information
As we have mentioned, the selection of the ran-
dom variable in Eq. 2 is extremely important for
the performance of the model. On one hand, we
would like for this variable to provide maximum
information. On the other hand, as the number of
parameters grow, we must address reliable param-
eter estimation in the face of sparsity, as well as
increased computational complexity. In the fol-
lowing section we will compare the use of Super-
ARVs, POS tags, and other structural tags derived
from parse trees.
2.1 POS Tags
Part-of-speech tags can be easily obtained for
unannotated data using off-the-shelf POS taggers
or PCFG parsers. However, the amount of infor-
mation these tags typically provide is very limited,
1115
Figure 1: A parse tree example
e.g., while it is helpful to know whether fly is a
verb or a noun, knowing that you is a personal pro-
noun does not carry the information whether it is
a subject or an object (given the Penn Tree Bank
tagset), which would certainly help to predict the
following word.
2.2 SuperARV
The SuperARV essentially organizes information
concerning one consistent set of dependency links
for a word that can be directly derived from its
syntactic parse. SuperARVs encode lexical in-
formation as well as syntactic and semantic con-
straints in a uniform representation that is much
more fine-grained than POS. It is a four-tuple
(C;F ;R+;D), where C is the lexical category
of the word, F is a vector of lexical features for
the word, R+ is a set of governor and need labels
that indicate the function of the word in the sen-
tence and the types of words it needs, and D rep-
resents the relative position of the word and its de-
pendents. We refer the reader to the literature for
further details on SuperARVs (Wang and Harper,
2002; Wang et al, 2003).
SuperARVs can be produced from parse trees
by applying deterministic rules. In this work we
use SuperARVs as individual tags and do not clus-
ter them based of their structure. While Super-
ARVs are very attractive for language modeling,
developing such a rich set of annotations for a new
language would require a large amount of human
effort.
We propose two other types of tags which have
not been applied to this task, although similar in-
formation has been used in parsing.
2.3 Modifee Tag
This tag is a combination of the word?s POS
tag and the POS tag of its governor role. We
designed it to resemble dependency parse struc-
ture. For example, the sentence in Figure 1 would
be tagged: the/DT-NN black/JJ-NN cat/NN-VBD
sat/VBD-root. Henceforth, we will refer to this
kind of tag as head.
2.4 Parent Constituent
This tag is a combination of the word?s POS tag
with its immediate parent in the parse tree, along
with the POS tag?s relative position among its sib-
lings. We refer to this type of tags as parent. The
example in Figure 1 will be tagged: the/DT-NP-
start black/JJ-NP-mid cat/NN-NP-end sat/VB-VP-
single. This tagset is designed to represent con-
stituency information.
Note that the head and parent tagsets are more
language-independent (all they require is a tree-
bank) than the SuperARVs which, not only uti-
lized the treebank, but were explicitly designed by
a linguist for English only.
2.5 Information Theoretic Comparison of
Tags
As we have mentioned in Section 1, the choice of
the tagset is very important to the performance of
the model. There are two conflicting intuitions for
tags: on one hand they should be specific enough
to be helpful in the language model?s task; on the
other hand, they should be easy for the LM to pre-
dict.
Of course, in order to argue which tags are more
suitable, we need some quantifiable metrics. We
propose an information theoretic approach:
? To quantify how hard it is to predict a tag, we
compute the conditional entropy:
H
p
(t
i
|w
i
) = H
p
(t
i
w
i
)?H
p
(w
i
)
=
?
w
i
t
i
p(t
i
w
i
) log p(t
i
|w
i
)
? To measure how helpful a tagset is in the LM
task, we compute the reduction of the condi-
tional cross entropy:
H
p?,q
(w
i
|w
i?1
t
i?1
) ?H
p?,q
(w
i
|w
i?1
) =
?
?
w
i
i?1
t
i?1
p?(w
i
i?1
t
i?1
) log q(w
i
|w
i?1
t
i?1
)
+
?
w
i
i?1
p?(w
i
i?1
) log q(w
i
|w
i?1
)
= ?
?
w
i
i?1
t
i?1
p?(w
i
i?1
t
i?1
) log
q(w
i
|w
i?1
t
i?1
)
q(w
i
|w
i?1
)
1116
Note that in this case we use conditional
cross entropy because conditional entropy
has the tendency to overfit the data as we se-
lect more and more fine-grain tags. Indeed,
H
p
(w
i
|w
i?1
t
i?1
) can be reduced to zero if
the tags are specific enough, which would
never happen in reality. This is not a prob-
lem for the former metric because the con-
text there, w
i
, is fixed. For this metric, we
use a smoothed distribution p? computed on
the training set3 and the test distribution q.
 
B
it
s
0
0.5
1
1.5
2
2.5
3
Tags
POS SuperARV parent head
Figure 2: Changes in entropy for different tagsets
The results of these measurements are presented
in Figure 2. POS tags, albeit easy to predict, pro-
vide very little additional information about the
following word, and therefore we would not ex-
pect them to perform very well. The parent tagset
seems to perform somewhat better than Super-
ARVs ? it provides 0.13 bits more information
while being only 0.09 bits harder to predict based
on the word. The head tagset is interesting: it pro-
vides 0.2 bits more information about the follow-
ing word (which would correspond to 15% per-
plexity reduction if we had perfect tags), but on
the other hand the model is less likely to predict
these tags accurately.
This approach is only a crude estimate (it uses
only unigram and bigram context) but it is very
useful for designing tagsets, e.g., for a new lan-
guage, because it allows us to assess relative per-
formance of tagsets without having to train a full
model.
3We used one-count smoothing (Chen and Goodman,
1996).
3 Language Model Structure
The size and sparsity of the parameter space of the
joint model necessitate the use of dimensionality
reduction measures in order to make the model
computationally tractable and to allow for accu-
rate estimation of the model?s parameters. We also
want the model to be able to easily accommodate
additional sources of information such as morpho-
logical features, prosody, etc. In the rest of this
section, we discuss avenues we have taken to ad-
dress these problems.
3.1 Decision Tree Clustering
Binary decision tree clustering has been shown to
be effective for reducing the parameter space in
language modeling (Bahl et al, 1990; Heeman,
1999) and other language processing applications,
e.g., (Magerman, 1994). Like any clustering algo-
rithm, it can be represented by a function H that
maps the space of histories to a set of equivalence
classes.
p(w
i
t
i
|w
i?1
i?n+1
t
i?1
i?n+1
) ? p(w
i
t
i
|H(w
i?1
i?n+1
t
i?1
i?n+1
))
(4)
While the tree construction algorithm is fairly
standard ? to recursively select binary questions
about the history optimizing some function ? there
are important decisions to make in terms of which
questions to ask and which function to optimize.
In the remainder of this section, we discuss the de-
cisions we made regarding these issues.
3.2 Factors
The Factored Language Model (FLM) (Bilmes
and Kirchhoff, 2003) offers a convenient view of
the input data: it represents every word in a sen-
tence as a tuple of factors. This allows us to extend
the language model with additional parameters. In
an FLM, however, all factors have to be determin-
istically computed in a joint model; whereas, we
need to distinguish between the factors that are
given or computed and the factors that the model
must predict stochastically. We call these types
of factors overt and hidden, respectively. Exam-
ples of overt factors include surface words, mor-
phological features such as suffixes, case informa-
tion when available, etc., and the hidden factors
are POS, SuperARVs, or other tags.
Henceforth, we will use word to represent the
set of overt factors and tag to represent the set of
hidden factors.
1117
3.3 Hidden Factors Tree
Similarly to (Heeman, 1999), we construct a bi-
nary tree where each tag is a leaf; we will refer
to this tree as the Hidden Factors Tree (HFT). We
use Minimum Discriminative Information (MDI)
algorithm (Zitouni, 2007) to build the tree. The
HFT represents a hierarchical clustering of the tag
space. One of the reasons for doing this is to allow
questions about subsets of tags rather than individ-
ual tags alone4.
Unlike (Heeman, 1999), where the tree of tags
was only used to create questions, this representa-
tion of the tag space is, in addition, a key feature
of our decoding optimizations, which we discuss
in Section 4.
3.4 Questions
The context space is partitioned by means of bi-
nary questions. We use different types of ques-
tions for hidden and overt factors.
? Questions about surface words are con-
structed using the Exchange algorithm (Mar-
tin et al, 1998). This algorithm takes the set
of words that appear at a certain position in
the training data associated with the current
node in the history tree and divides the set
into two complementary subsets greedily op-
timizing some target function (we use the av-
erage entropy of the marginalized word dis-
tribution, the same as for question selection).
Note that since the algorithm only operates
on the words that appear in the training data,
we need to do something more to account for
the unseen words. Thus, to represent this type
of question, we create the history tree struc-
ture depicted in Fig. 4.
For other overt factors with smaller vocabu-
laries, such as suffixes, we use equality ques-
tions.
? As we mentioned in Section 3.3, we use the
Hidden Factors Tree to create questions about
hidden factors. Note that every node in a bi-
nary tree can be represented by a binary path
from the root with all nodes under an inner
node sharing the same prefix. Thus, a ques-
tion about whether a tag belongs to a subset
4Trying all possible subsets of tags is not feasible since
there are 2|T | of them. The tree allows us to reduce the num-
ber to O(T ) of the most meaningful (as per the clustering
algorithm) subsets.
Figure 3: Recursive smoothing: p?
n
= ?
n
p
n
+
(1? ?
n
)p?
n
?
of tags dominated by a node can be expressed
as whether the tag?s path matches the binary
prefix.
3.5 Optimization Criterion and Stopping
Rule
To select questions we use the average entropy of
the marginalized word distribution. We found that
this criterion significantly outperforms the entropy
of the distribution of joint events. This is proba-
bly due to the increased sparsity of the joint distri-
bution and the fact that our ultimate metrics, i.e.,
WER and word perplexity, involve only words.
3.6 Distribution Representation
In a cluster H
x
, we factor the joint distribution as
follows:
p(w
i
t
i
|H
x
) = p(w
i
|H
x
) ? p(t
i
|w
i
, H
x
)
where p(t
i
|w
i
, H
x
) is represented in the form of
an HFT, in which each leaf has the probability of a
tag and each internal node contains the sum of the
probabilities of the tags it dominates. This repre-
sentation is designed to assist the decoding process
described in Section 4.
3.7 Smoothing
In order to estimate probability distributions at the
leaves of the history tree, we use the following re-
cursive formula:
p?
n
(w
i
t
i
) = ?
n
p
n
(w
i
t
i
) + (1? ?
n
)p?
n
?
(w
i
t
i
) (5)
where n? is the n-th node?s parent, p
n
(w
i
t
i
) is
the distribution at node n (see Figure 3). The
1118
root of the tree is interpolated with the distribu-
tion p
unif
(w
i
t
i
) =
1
|V |
p
ML
(t
i
|w
i
)
5
. To estimate
interpolation parameters ?
n
, we use the EM algo-
rithm described in (Magerman, 1994); however,
rather than setting aside a separate development
set of optimizing ?
n
, we use 4-fold cross valida-
tion and take the geometric mean of the resulting
coefficients6. We chose this approach because a
small development set often does not overlap with
the training set for low-count nodes, leading the
EM algorithm to set ?
n
= 0 for those nodes.
Let us consider one leaf of the history tree in
isolation. Its context can be represented by the
path to the root, i.e., the sequence of questions and
answers q
1
, . . . q
(n
?
)
?q
n
? (with q
1
being the answer
to the topmost question):
p?
n
(w
i
t
i
) = p?(w
i
t
i
|q
1
. . . q
(n
?
)
?q
n
?
)
Represented this way, Eq. 5 is a variant of Jelinek-
Mercer smoothing:
p?(w
i
t
i
|q
1
. . . q
n
?
) = ?
n
p(w
i
t
i
|q
1
. . . q
n
?
) +
(1? ?
n
)p?(w
i
t
i
|q
1
. . . q
(n
?
)
?)
For backoff nodes (see Fig. 4), we use a lower
order model7 interpolated with the distribution at
the backoff node?s grandparent (see node A in Fig.
4):
p?
B
(w
i
t
i
|w
i?1
i?n+1
t
i?1
i?n+1
) =
?
A
p?
bo
(w
i
t
i
|w
i?1
i?n+2
t
i?1
i?n+2
) + (1 ? ?
A
)p?
A
(w
i
t
i
)
How to compute ?
A
is an open question. For this
study, we use a simple heuristic based on obser-
vation that the further node A is from the root
the more reliable the distribution p?
A
(w
i
t
i
) is, and
hence ?
A
is lower. The formula we use is as fol-
lows:
?
A
=
1
?
1 + distanceToRoot(A)
5We use this distribution rather than uniform joint distri-
bution 1
|V ||T |
because we do not want to allow word-tag pairs
that have never been observed. The idea is similar to (Thede
and Harper, 1999).
6To avoid a large number of zeros due to the product, we
set a minimum for ? to be 10?7.
7The lower order model is constructed by the same algo-
rithm, although with smaller context. Note that the lower or-
der model can back off on words or tags, or both. In this paper
we backoff both on words and tags, i.e., p(w
i
t
i
|w
i?1
i?2
t
i?1
i?2
)
backs off to p(w
i
t
i
|w
i?1
t
i?1
), which in turn backs off to the
unigram p(w
i
t
i
).
Figure 4: A fragment of the decision tree with a
backoff node. S ? ?S is the set of words observed
in the training data at the node A. To account for
unseen words, we add the backoff node B.
4 Decoding
As in HMM decoding, in order to compute prob-
abilities for i-th step, we need to sum over |T |n?1
possible combinations of tags in the history, where
T is the set of tags and n is the order of the
model. With |T | predictions for the i-th step, we
have O(|T |n) computational complexity per word.
Straightforward computation of these probabili-
ties is problematic even for a trigram model with
POS tags, i.e., n = 3, |T | ? 40. A standard ap-
proach to limit computational requirements is to
use beam search where only N most likely paths
are retained. However, with fine-grain tags where
|T | ? 1, 500, a tractable beam size would only
cover a small fraction of the whole space, leading
to search errors such as pruning good paths.
Note that we have a history clustering function
(Eq. 4) represented by the decision tree, and we
should be able to exploit this clustering to elimi-
nate unnecessary computations involving equiva-
lent histories. Note that words in the history are
known exactly, thus we can create a projection of
the clustering function H in Eq. 4 to the plane
w
i?1
i?n+1
= const, i.e., where words in the context
are fixed to be whatever is observed in the history:
H(w
i?1
i?n+1
t
i?1
i?n+1
) ?
?
H
w
i?1
i?n+1
=const
(t
i?1
i?n+1
)
(6)
The number of distinct clusters in the projection
?
H depends on the decision tree configuration and
can vary greatly for different words wi?1
i?n+1
in the
history, but generally it is relatively small:
|
?
H
w
i?1
i?n+1
=const
(t
i?1
i?n+1
)| ? |T
n?1
| (7)
1119
Figure 5: Questions about hidden factors split
states (see Figure 6) in the decoding lattice rep-
resented by HFTs.
thus, the number of probabilities that we need to
compute is | ?H
w
i?1
i?n+1
=const
| ? |T |.
Our decoding algorithm works similarly to
HMM decoding with the exception that the set of
hidden states is not predetermined. Let us illus-
trate how it works in the case of a bigram model.
Recall that the set of tags T is represented as a
binary tree (HFT) and the only type of questions
about tags is about matching a binary prefix in the
HFT. Such a question dissects the HFT into two
parts as depicted in Figure 5. The cost of this op-
eration is O(log |T |).
We represent states in the decoding lattice as
shown in the Figure 6, where pS
in
is the probability
of reaching the state S:
p
S
in
=
?
S
?
?IN
S
?
?
p
S
?
in
p(w
i?2
|H
S
?
)
?
t?T
S
?
p(t|w
i?2
H
S
?
)
?
?
where IN
S
is the set of incoming links to the
state S from the previous time index, and T
S
? is
the set of tags generated from the state S? repre-
sented as a fragment of the HFT. Note, that since
we maintain the property that the probability as-
signed to an inner node of the HFT is the sum
of probabilities of the tags it dominates, the sum
?
t?T
S
?
p(t|w
i?2
H
S
?
) is located at the root of T
S
? ,
and therefore this is an O(1) operation.
Now given the state S at time i ? 1, in order to
generate tag predictions for i-th word, we apply
questions from the history clustering tree, start-
ing from the top. Questions about overt factors
Figure 6: A state S in the decoding lattice. pS
in
is
the probability of reaching the state S through the
set of links IN
S
. The probabilities of generating
the tags p(t
i?1
|w
i?1
, H
s
), (t
i?1
? T
S
) are repre-
sented in the form of the HFT.
always follow either a true or false branch, implic-
itly computing the projection in Eq. 6. Questions
about hidden factors, can split the state S into two
states S
true
and S
false
, each retaining a part of T
S
as shown in the Figure 5.
The process continues until each fragment of
each state at the time i ? 1 reaches the bottom of
the history tree, at which point new states for time
i are generated from the clusters associated with
leaves. The states at i? 1 that generate the cluster
H
?
S
become the incoming links to the state ?S.
Higher order models work similarly, except that
at each time we consider a state S at time i ? 1
along with one of its incoming links (to some
depth according to the size of the context).
5 Experimental Setup
To evaluate the impact of fine-grain tags on lan-
guage modeling, we trained our model with five
settings: In the first model, questions were re-
stricted to be about overt factors only, thus making
it a tree-based word model. In the second model,
we used POS tags. To evaluate the effect of fine-
grain tags, we train two models: head and parent
described in Section 2.3 and Section 2.4 respec-
tively. Since our joint model can be used with
any kind of tags, we also trained it with Super-
ARV tags (Wang et al, 2003). The SuperARVs
were created from the same parse trees that were
used to produce POS and fine-grain tags. All our
models, including SuperARV, use trigram context.
We include standard trigram, four-gram, and five-
1120
gram models for reference. The ngram models
were trained using SRILM toolkit with interpo-
lated modified Kneser-Ney smoothing.
We evaluate our model with an nbest rescoring
task using 100-best lists from the DARPA WSJ?93
and WSJ?92 20k open vocabulary data sets. The
details on the acoustic model used to produce the
nbest lists can be found in (Wang and Harper,
2002). Since the data sets are small, we com-
bined the 93et and 93dt sets for evaluation and
used 92et for the optimization8. We transformed
the nbest lists to match PTB tokenization, namely
separating possessives from nouns, n?t from auxil-
iary verbs in contractions, as well as contractions
from personal pronouns.
All language models were trained on the NYT
1994-1995 section of the English Gigaword cor-
pus (approximately 70M words). Since the New
York Times covers a wider range of topics than
the Wall Street Journal, we eliminated the most ir-
relevant stories based on their trigram coverage by
sections 00-22 of WSJ. We also eliminated sen-
tences over 120 words, because the parser?s per-
formance drops significantly on long sentences.
After parsing the corpus, we deleted sentences that
were assigned a very low probability by the parser.
Overall we removed only a few percent of the data;
however, we believe that such a rigorous approach
to data cleaning is important for building discrim-
inating models.
Parse trees were produced by an extended ver-
sion of the Berkeley parser (Huang and Harper,
2009). We trained the parser on a combination of
the BN and WSJ treebanks, preprocessed to make
them more consistent with each other. We also
modified the trees for the speech recognition task
by replacing numbers and abbreviations with their
verbalized forms. We pre-processed the NYT cor-
pus in the same way, and parsed it. After that, we
removed punctuation and downcased words. For
the ngram model, we used text processed in the
same way.
In head and parent models, tag vocabularies
contain approximately 1,500 tags each, while the
SuperARV model has approximately 1,400 dis-
tinct SuperARVs, most of which represent verbs
(1,200).
In these experiments we did not use overt fac-
tors other than the surface word because they split
8We optimized the LM weight and computed WER with
scripts in the SRILM and NIST SCTK toolkits.
Models WER
trigram (baseline) 17.5
four-gram 17.7
five-gram 17.8
Word Tree 17.3
POS Tags 17.0
Head Tags 16.8
Parent Tags 16.7
SuperARV 16.9
Table 1: WER results, optimized on 92et set, eval-
uated on combined 93et and 93dt set. The Oracle
WER is 9.5%.
<unk>, effectively changing the vocabulary thus
making perplexity incomparable to models with-
out these factors, without improving WER notice-
ably. However, we do plan to use more overt
factors in Machine Translation experiments where
a language model faces a wider range of OOV
phenomena, such as abbreviations, foreign words,
numbers, dates, time, etc.
Table 1 summarizes performance of the LMs on
the rescoring task. The parent tags model outper-
forms the trigram baseline model by 0.8% WER.
Note that four- and five-gram models fail to out-
perform the trigram baseline. We believe this is
due to the sparsity as well as relatively short sen-
tences in the test set (16 words on average).
Interestingly, whereas the improvement of the
POS model over the baseline is not statistically
significant (p < 0.10)9, the fine-grain models out-
perform the baseline much more reliably: p <
0.03 (SuperARV) and p < 0.007 (parent).
We present perplexity evaluations in Table 2.
The perplexity was computed on Section 23 of
WSJ PTB, preprocessed as the rest of the data we
used. The head model has the lowest perplexity
outperforming the baseline by 9%. Note, it even
outperforms the five-gram model, although by a
small 2% margin.
Although the improvements by the fine-grain
tagsets over POS are not significant (due to the
small size of the test set), the reductions in per-
plexity suggest that the improvements are not ran-
dom.
9For statistical significance, we used SCTK implementa-
tion of the mapsswe test.
1121
Models PPL
trigram (baseline) 162
four-gram 152
five-gram 150
Word Tree 160
POS Tags 154
Head Tags 147
Parent Tags 150
SuperARV 150
Table 2: Perplexity results on Section 23 WSJ
PTB
6 Conclusion and Future Work
In this paper, we presented a joint language mod-
eling framework. Unlike any prior work known
to us, it was not tailored for any specific tag set,
rather it was designed to accommodate any set
of tags, especially large sets (? 1, 000), which
present challenges one does not encounter with
smaller tag sets, such at POS tags. We discussed
these challenges and our solutions to them. Some
of the solutions proposed are novel, particularly
the decoding algorithm.
We also proposed two simple fine-grain tagsets,
which, when applied in language modeling, per-
form comparably to highly sophisticated tag sets
(SuperARV). We would like to stress that, while
our fine-grain tags did not significantly outperform
SuperARVs, the former use much less linguistic
knowledge and can be automatically induced for
any language with a treebank.
Because a joint language model inherently pre-
dicts hidden events (tags), it can also be used to
generate the best sequence of those events, i.e.,
tagging. We evaluated our model in the POS tag-
ging task and observed similar results: the fine-
grain models outperform the POS model, while
both outperform the state-of-the-art HMM POS
taggers. We refer to (Filimonov and Harper, 2009)
for details on these experiments.
We plan to investigate how parser accuracy and
data selection strategies, e.g., based on parser con-
fidence scores, impact the performance of our
model. We also plan on evaluating the model?s
performance on other genres of speech, as well as
in other tasks such as Machine Translation. We
are also working on scaling our model further to
accommodate amounts of data typical for mod-
ern large-scale ngram models. Finally, we plan to
apply the technique to other languages with tree-
banks, such as Chinese and Arabic.
We intend to release the source code of our
model within several months of this publication.
7 Acknowledgments
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023 and NSF IIS-0703859. Any opinions,
findings and/or recommendations expressed in this
paper are those of the authors and do not necessar-
ily reflect the views of the funding agencies or the
institutions where the work was completed.
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical
language model for natural language speech recog-
nition. Readings in speech recognition, pages 507?
514.
Srinivas Bangalore. 1996. ?Almost parsing? technique
for language modeling. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, volume 2, pages 1173?1176.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff.
In Proceedings of HLT/NACCL, 2003, pages 4?6.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jennifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling for speech recognition.
CoRR.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310?318, Morristown, NJ, USA. Association
for Computational Linguistics.
Denis Filimonov and Mary Harper. 2009. Measuring
tagging performance of a joint language model. In
Proceedings of the Interspeech 2009.
Peter A. Heeman. 1999. POS tags and decision trees
for language modeling. In In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 129?137.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP
2009.
1122
David M. Magerman. 1994. Natural language pars-
ing as statistical pattern recognition. Ph.D. thesis,
Stanford, CA, USA.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 1253?1256.
Thomas R. Niesler and Phil C. Woodland. 1996.
A variable-length category-based n-gram language
model. Proceedings of the IEEE International Con-
ference on Acoustics, Speech, and Signal Process-
ing, 1:164?167 vol. 1, May.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden markov model for part-of-speech tag-
ging. In Proceedings of the 37th Annual Meeting of
the ACL, pages 175?182.
Wen Wang and Mary P. Harper. 2002. The SuperARV
language model: investigating the effectiveness of
tightly integrating multiple knowledge sources. In
EMNLP ?02: Proceedings of the ACL-02 conference
on Empirical methods in natural language process-
ing, pages 238?247, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Wen Wang, Mary P. Harper, and Andreas Stolcke.
2003. The robustness of an almost-parsing language
model given errorful training data. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing.
Peng Xu and Frederick Jelinek. 2004. Random forests
in language modeling. In in Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model un-
seen events in speech recognition. Computer Speech
& Language, 21(1):88?104.
1123
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691?699,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Syntactic Decision Tree LMs: Random Selection or Intelligent Design?
Denis Filimonov??
?Human Language Technology
Center of Excellence
Johns Hopkins University
den@cs.umd.edu
Mary Harper?
?Department of Computer Science
University of Maryland, College Park
mharper@umd.edu
Abstract
Decision trees have been applied to a vari-
ety of NLP tasks, including language mod-
eling, for their ability to handle a variety of
attributes and sparse context space. More-
over, forests (collections of decision trees)
have been shown to substantially outperform
individual decision trees. In this work, we in-
vestigate methods for combining trees in a for-
est, as well as methods for diversifying trees
for the task of syntactic language modeling.
We show that our tree interpolation technique
outperforms the standard method used in the
literature, and that, on this particular task, re-
stricting tree contexts in a principled way pro-
duces smaller and better forests, with the best
achieving an 8% relative reduction in Word
Error Rate over an n-gram baseline.
1 Introduction
Language Models (LMs) are an essential part of
NLP applications that require selection of the most
fluent word sequence among multiple hypotheses.
The most prominent applications include Automatic
Speech Recognition (ASR) and Machine Transla-
tion (MT).
Statistical LMs formulate the problem as the
computation of the model?s probability to gener-
ate the word sequence w1, w2, . . . , wm (denoted as
wm1 ), assuming that higher probability corresponds
to more fluent hypotheses. LMs are often repre-
sented in the following generative form:
p(wm1 ) =
m?
i=1
p(wi|wi?11 )
Note the context space for this function, wi?11 is ar-
bitrarily long, necessitating some independence as-
sumption, which usually consists of reducing the rel-
evant context to n?1 immediately preceding tokens:
p(wi|wi?11 ) ? p(wi|wi?1i?n+1) (1)
These distributions are typically estimated from ob-
served counts of n-grams wii?n+1 in the training
data. The context space is still far too large1; there-
fore, the models are recursively smoothed using
lower order distributions. For instance, in a widely
used n-gram LM, the probabilities are estimated as
follows:
p?(wi|wi?1i?n+1) = ?(wi|wi?1i?n+1) + (2)
?(wi?1i?n+1) ? p?(wi|wi?1i?n+2)
where ? is a discounted probability2.
Note that this type of model is a simple Markov
chain lacking any notion of syntax. It is widely
accepted that languages do have some structure.
Moreover, it has been shown that incorporating syn-
tax into a language model can improve its perfor-
mance (Bangalore, 1996; Heeman, 1998; Chelba
and Jelinek, 2000; Filimonov and Harper, 2009). A
straightforward way of incorporating syntax into a
language model is by assigning a tag to each word
and modeling them jointly; then to obtain the proba-
1O(|V |n?1) in n-gram model with typical order n =
3 . . . 5, and a vocabulary size of |V | = 104 . . . 106.
2We refer the reader to (Chen and Goodman, 1996) for a
survey of the discounting methods for n-gram models.
691
bility of a word sequence, the tags must be marginal-
ized out:
p(wm1 ) =
?
t1...tm
p(wm1 tm1 ) =
?
t1...tm
m?
i=1
p(witi|wi?11 ti?11 )
An independence assumption similar to Eq. 1 can be
made:
p(witi|wi?11 ti?11 ) ? p(witi|wi?1i?n+1ti?1i?n+1) (3)
A primary goal of our research is to build strong
syntactic language models and provide effective
methods for constructing them to the research com-
munity. Note that the tags in the context of the joint
model in Eq. 3 exacerbate the already sparse prob-
lem in Eq. 1, which makes the probability estima-
tion particularly challenging. We utilize decision
trees for joint syntactic language models to clus-
ter context because of their strengths (reliance on
information theoretic metrics to cluster context in
the face of extreme sparsity and the ability to in-
corporate attributes of different types3), and at the
same time, unlike log-linear models (Rosenfeld et
al., 1994), computationally expensive probability
normalization does not have to be postponed until
runtime.
In Section 2, we describe the details of the syntac-
tic decision tree LM. Construction of a single-tree
model is difficult due to the inevitable greediness
of the tree construction process and its tendency to
overfit the data. This problem is often addressed by
interpolating with lower order decision trees. In Sec-
tion 3, we point out the inappropriateness of backoff
methods borrowed from n-gram models for decision
tree LMs and briefly describe a generalized interpo-
lation for such models. The generalized interpola-
tion method allows the addition of any number of
trees to the model, and thus raises the question: what
is the best way to create diverse decision trees so that
their combination results in a stronger model, while
at the same time keeping the total number of trees in
the model relatively low for computational practical-
ity. In Section 4, we explore and evaluate a variety
3For example, morphological features can be very helpful
for modeling highly inflectional languages (Bilmes and Kirch-
hoff, 2003).
of methods for creating different trees. To support
our findings, we evaluate several of the models on
an ASR rescoring task in Section 5. Finally, we dis-
cuss our findings in Section 6.
2 Joint Syntactic Decision Tree LM
A decision tree provides us with a clustering func-
tion ?(wi?1i?n+1ti?1i?n+1) ? {?1, . . . ,?N}, where N
is the number of clusters, and clusters ?k are disjoint
subsets of the context space. The probability estima-
tion for a joint decision tree model is approximated
as follows:
p(witi|wi?1i?n+1ti?1i?n+1) ? p(witi|?(wi?1i?n+1ti?1i?n+1))(4)
In the remainder of this section, we briefly describe
the techniques that we use to construct such a deci-
sion tree ? and to estimate the probability distribu-
tion for the joint model in Eq. 4.
2.1 Decision Tree Construction
We use recursive partitioning to grow decision trees.
In this approach, a number of alternative binary
splits of the training data associated with a node are
evaluated using some metric, the best split is chosen,
checked against a stopping rule (which aims at pre-
venting overfitting to the training data and usually
involves a heldout set), and then the two partitions
become the child nodes if the stopping rule does not
apply. Then the algorithm proceeds recursively into
the newly constructed leaves.
Binary splits are often referred to as questions
about the context because a binary partition can
be represented by a binary function that decides
whether an element of context space belongs to one
partition or the other. We utilize univariate questions
where each question partitions the context on one
attribute, e.g., wi?2 or ti?1. The questions about
words and tags are constructed differently:
? The questions q about the words are in the form
q(x) ? wi+x ? S, where x is an integer be-
tween ?n + 1 and ?1, and S ? V is a subset
of the word vocabulary V . To construct the set
S, we take the set of words So observed at the
offset x in the training data associated with the
692
current node and split it into two complemen-
tary subsets S ? S? = So using the Exchange
algorithm (Martin et al, 1998). Because the
algorithm is greedy and depends on the initial-
ization, we construct 4 questions per word po-
sition using different random initializations of
the Exchange algorithm.
Since we need to account for words that were
not observed in the training data, we utilize
the structure depicted in Figure 1. To estimate
the probability at the backoff node (B in Fig-
ure 1), we can either use the probability from its
grandparent nodeA or estimate it using a lower
order tree (see Section 3), or combine the two.
We have observed no noticeable difference be-
tween these methods, which suggests that only
a small fraction of probability is estimated from
these nodes; therefore, for simplicity, we use
the probability estimated at the backoff node?s
grandparent.
? To create questions about tags we create a hi-
erarchical clustering of all tags in the form of
a binary tree. This is done beforehand, using
the Minimum Discriminating Information al-
gorithm (Zitouni, 2007) with the entire train-
ing data set. In this tree, each leaf is an in-
dividual tag and each internal node is associ-
ated with the subset of tags that the node dom-
inates. Questions about tags are constructed in
the form q(x, k) ? ti+x ? Tk, where k is a
node in the tag tree and Tk is the subset of tags
associated with that node. The rationale behind
constructing tag questions in this form is that
it enables a more efficient decoding algorithm
than standard HMM decoding (Filimonov and
Harper, 2009).
Questions are evaluated in two steps. First the
context attribute x is selected using a metric simi-
lar to information gain ratio proposed by (Quinlan,
1986):
M = 1? H(wi)?H(wi|x)H(x) = 1?
I(x;wi)
H(x)
where x is one of the context attributes, e.g., wi?2
or ti?1. Then, among the questions about attribute
wi?2??S
Backoff leaf
yes
yesno
no
A
B
wi?2?S
Figure 1: A fragment of the decision tree with a backoff
node. S ? S? is the set of words observed in the training
data at the node A. To account for unseen words, we add
the backoff node B.
x, we select the question that maximizes the entropy
reduction.
Instead of dedicating an explicit heldout set for
the stopping criterion, we utilize a technique simi-
lar to cross validation: the training data set is par-
titioned into four folds, and the best question is re-
quired to reduce entropy on each of the folds.
Note that the tree induction algorithm can also be
used to construct trees without tags:
p(wi|wi?1i?n+1) ? p(wi|?(wi?1i?n+1))
We refer to this model as the word-tree model. By
comparing syntactic and word-tree models, we are
able to separate the effects of decision tree modeling
and syntactic information on language modeling by
comparing both models to an n-gram baseline.
2.2 In-tree Smoothing
A decision tree offers a hierarchy of clusterings that
can be exploited for smoothing. We can interpo-
late the observed distributions at leaves recursively
with their parents, as in (Bahl et al, 1990; Heeman,
1998):
p?k(witi) = ?kpML(witi) + (1? ?k)p?k?(witi) (5)
where pML is the observed distribution at node k
and k? is the parent of k. The coefficients ?k are
estimated using an EM algorithm.
We can also combine p(witi|?(wi?1i?n+1ti?1i?n+1))
with lower order decision trees, i.e.,
693
p(witi|?(wi?1i?n+2ti?1i?n+2)), and so on up until
p(witi) which is a one-node tree (essentially a
unigram model). Although superficially similar to
backoff in n-gram models, lower order decision
trees differ substantially from lower order n-gram
models and require different interpolation methods.
In the next section, we discuss this difference and
present a generalized interpolation that is more
suitable for combining decision tree models.
3 Interpolation with Backoff Tree Models
In this section, for simplicity of presentation, we fo-
cus on the equations for word models, but the same
equations apply equally to joint models (Eq. 3) with
trivial transformations.
3.1 Backoff Property
Let us rewrite the interpolation Eq. 2 in a more
generic way:
p?(wi|wi?11 ) = ?n(wi|?n(wi?11 )) + (6)
?(?n(wi?11 )) ? p?(wi|BOn?1(wi?11 ))
where, ?n is a discounted distribution, ?n is a clus-
tering function of order n, and ?(?n(wi?11 )) is the
backoff weight chosen to normalize the distribution.
BOn?1 is the backoff clustering function of order
n ? 1, representing a reduction of context size. In
the case of an n-gram model, ?n(wi?11 ) is the set
of word sequences where the last n ? 1 words are
wi?1i?n+1. Similarly, BOn?1(wi?11 ) is the set of se-
quences ending with wi?1i?n+2. In the case of a de-
cision tree model, the same backoff function is typ-
ically used, but the clustering function can be arbi-
trary.
The intuition behind Eq. 6 is that the backoff con-
text BOn?1(wi?11 ) allows for a more robust (but
less informed) probability estimation than the con-
text cluster ?n(wi?11 ). More precisely:
?wi?11 ,W : W ? ?n(w
i?1
1 )?W ? BOn?1(wi?11 )
(7)
that is, every word sequence W that belongs to a
context cluster ?n(wi?11 ), belongs to the same back-
off cluster BOn?1(wi?11 ) (hence has the same back-
off distribution). For n-gram models, Property 7
?nBOn?2 Backofk leyaslknol Asol ? cl??ac?kal?eeoyockl?? c?
?A?l?A??eel?ya?yk?l Ak? e?o? ???l?A?aeel?ya?yk?l??a?Ako?
?ckofkl ?A? ?ackofkl ?A?
Figure 2: Backoff Property
trivially holds since BOn?1(wi?11 ) and ?n(wi?11 )
are defined as sets of sequences ending with wi?1i?n+2
andwi?1i?n+1, with the former clearly being a superset
of the latter. However, when ? can be arbitrary, e.g.,
a decision tree, the property is not necessarily satis-
fied. Figure 2 illustrates cases when the Property 7
is satisfied (a) and violated (b).
Let us consider what happens when we have
two context sequences W and W ? that belong to
the same cluster ?n(W ) = ?n(W ?) but differ-
ent backoff clusters BOn?1(W ) 6= BOn?1(W ?).
For example: suppose we have ?(wi?2wi?1) =
({on}, {may,june}) and two corresponding backoff
clusters: BO? = ({may}) and BO?? = ({june}).
Following on, the word may is likely to be a month
rather than a modal verb, although the latter is
more frequent and will dominate in BO?. There-
fore we have much less faith in p?(wi|BO?) than in
p?(wi|BO??) and would like a much smaller weight
? assigned to BO?. However this would not be pos-
sible in the backoff scheme in Eq. 6, thus we will
have to settle on a compromise value of ?, resulting
in suboptimal performance.
Hence arbitrary clustering (an advantage of deci-
sion trees) leads to a violation of Property 7, which
is likely to produce a degradation in performance if
backoff interpolation Eq. 6 is used.
3.2 Generalized Interpolation
Recursive linear interpolation similar to Jelinek-
Mercer smoothing for n-gram models (Jelinek and
Mercer, 1980) has been applied to decision tree
models:
694
p?n(wi|wi?1i?n+1) = ?n(?n) ? pn(wi|?n) + (8)
(1? ?n(?n)) ? p?n?1(wi|wi?1i?n+2)
where ?n ? ?n(wi?1i?n+1), and ?n(?n) ? [0, 1] are
assigned to each cluster and are optimized on a held-
out set using EM. pn(wi|?n) is the probability dis-
tribution at the cluster ?n in the tree of order n. This
interpolation method is particularly useful as, un-
like count-based discounting methods (e.g., Kneser-
Ney), it can be applied to already smoothed distribu-
tions pn.
In (Filimonov and Harper, 2011), we observed
that because of the violation of Property 7 in deci-
sion tree models, the interpolation method of Eq. 8
is not appropriate for such models. Instead we pro-
posed the following generalized form of linear inter-
polation:
p?n(wi|wi?1i?n+1) =
?n
m=1 ?m(?m) ? pm(wi|?m)?n
m=1 ?m(?m)
(9)
Note that the recursive interpolation of Eq. 8 can
be represented in this form with the additional con-
straint?nm=1 ?m(?m) = 1, which is not required in
the generalized interpolation of Eq. 9; thus, the gen-
eralized interpolation, albeit having the same num-
ber of parameters, has more degrees of freedom. We
also showed that the recursive interpolation Eq. 8 is
a special case of Eq. 9 that occurs when the Prop-
erty 7 holds.
4 From Backoff Trees to Forest
Note that, in Eq. 9, individual trees do not have ex-
plicit higher-lower order relations, they are treated
as a collection of trees, i.e., as a forest. Naturally,
to benefit from the forest model, its trees must differ
in some way. Different trees can be created based
on differences in the training data, differences in the
tree growing algorithm, or some non-determinism in
the way the trees are constructed.
(Xu, 2005) used randomization techniques to pro-
duce a large forest of decision trees that were com-
bined as follows:
p(wi|wi?1i?n+1) =
1
M
M?
m=1
pm(wi|wi?1i?n+1) (10)
whereM is the number of decision trees in the forest
(he proposed M = 100) and pm is the m-th tree
model4. Note that this type of interpolation assumes
that each tree model is ?equal? a priori and therefore
is only appropriate when the tree models are grown
in the same way (particularly, using the same order
of context). Note that Eq. 10 is a special case of
Eq. 9 when all parameters ? are equal.
(Xu, 2005) showed that, although each individual
tree is a fairly weak model, their combination out-
performs the n-gram baseline substantially. How-
ever, we find this approach impractical for online
application of any sizable model: In our experi-
ments, fourgram trees have approximately 1.8 mil-
lion leaves and the tree structure itself (without prob-
abilities) occupies nearly 200MB of disk space af-
ter compression. It would be infeasible to apply a
model consisting of more than a handful of such
trees without distributed computing of some sort.
Therefore, we pose the following question: If we
can afford to have only a handful of trees in the
model, what would be best approach to construct
those trees?
In the remainder of this section, we will describe
the experimental setup, discuss and evaluate differ-
ent ways of building decision tree forests for lan-
guage modeling, and compare combination methods
based on Eq. 9 and Eq. 10 (when Eq. 10 is applica-
ble).
4.1 Experimental Setup
To train our models we use 35M words of WSJ
94-96 from LDC2008T13. The text was converted
into speech-like form, namely numbers and abbrevi-
ations were verbalized, text was downcased, punctu-
ation was removed, and contractions and possessives
were joined with the previous word (i.e., they ?ll be-
comes they?ll). For the syntactic modeling, we used
tags comprised of the POS tags of the word and it?s
head. Parsing of the text for tag extraction occurred
after verbalization of numbers and abbreviations but
4Note that (Xu, 2005) used lower order models to estimate
pm.
695
before any further processing; we used a latent vari-
able PCFG parser as in (Huang and Harper, 2009).
For reference, we include an n-gram model with
modified interpolated KN discounting. All mod-
els use the same vocabulary of approximately 50k
words.
Perplexity numbers reported in Tables 1, 2, 3,
and 4 are computed on WSJ section 23 (tokenized
in the same way)5.
In Table 1, we show results reported in (Filimonov
and Harper, 2011), which we use as the baseline for
further experiments. We constructed two sets of de-
cision trees (a joint syntactic model and a word-tree
model) as described in Section 2. Each set was com-
prised of a fourgram tree with backoff trigram, bi-
gram, and unigram trees. We combined these trees
using either Eq. 8 or Eq. 9. The ? parameters in
Eq. 8 were estimated using EM by maximizing like-
lihood of a heldout set (we utilized 4-way cross-
validation); whereas, the parameters in Eq. 9 were
estimated using L-BFGS because the denominator
in Eq. 9 makes the maximization step problematic.
4.2 Random Forest
(Xu, 2005) evaluated a variety of randomization
techniques that can be used to build trees. He used
a word-only model, with questions constructed us-
ing the Exchange algorithm, similar to our model.
He tried two methods of randomization: selecting
the positions in the history for question construction
by a Bernoulli trials6, and random initialization of
the Exchange algorithm. He found that when the
Exchange algorithm was initialized randomly, the
Bernoulli trial parameter did not matter; however,
when the Exchange algorithm was initialized deter-
ministically; lower values for the Bernoulli trial pa-
rameter r yielded better overall forest performance.
We implemented a similar method, namely, initial-
izing the Exchange algorithm randomly and using
r = 0.1 for Bernoulli trials7.
There is a key difference between the two ran-
5This section was not used for training the parser or for the
LM training.
6In this method, positions in the history are ignored with
probability 1? r, where r is the Bernoulli trials parameter.
7Note that because in the joint model, the question about
tags are deterministic, we use a lower value of r than (Xu, 2005)
to increase randomness.
domization methods. Since we do not have an a
priori preference for choosing initializations for the
Exchange algorithm, by using random initializations
it is hoped that due to the greedy nature of the al-
gorithm, the constructed trees, while being ?unde-
graded,?8 will be sufficiently different so that their
combination improves over an individual tree. By
introducing Bernoulli trials, on the other hand, there
is a choice to purposely degrade the quality of in-
dividual trees in the hope that additional diversity
would enable their combination to compensate for
the loss of quality in individual trees.
Another way of introducing randomness to the
tree construction without apparent degradation of in-
dividual tree quality is through varying the data, e.g.,
using different folds of the training data (see Sec-
tion 2.1).
Let us take a closer look at the effect of differ-
ent types of randomization on individual trees and
their combinations. In the first set of experiments,
we compare the performance of a single undegraded
fourgram tree9 with forests of fourgram trees grown
randomly with Bernoulli trials. Having only same-
order trees in a forest allows us to apply interpola-
tion of Eq. 10 (used in (Xu, 2005)) and compare
with the interpolation method presented in Eq. 9. By
comparing forests of different sizes with the baseline
from Table 1, we are able to evaluate the effect of
randomization in decision tree growing and assess
the importance of the lower order trees.
The results are shown in Table 2. Note that, while
an undegraded syntactic tree is better than the word
tree, the situation is reversed when the trees are
grown randomly. This can be explained by the fact
that the joint model has a much higher dimensional-
ity of the context space, and therefore is much more
sensitive to the clustering method.
As we increase the number of random trees in the
forest, the perplexity decreases as expected, with the
interpolation method of Eq. 9 showing improvement
of a few percentile points over Eq. 10. Note that
in the case of the word-tree model, it takes 4 ran-
dom decision trees to reach the performance of a sin-
gle undegraded tree, while in the joint model, even
8Here and henceforth, by ?undegraded? we mean ?accord-
ing to the algorithm described in Section 2.?
9Since each tree has a smooth distribution based on Eq. 5,
lower order trees are not strictly required.
696
Eq. 8 Eq. 9 (generalized)
order n-gram word-tree syntactic word-tree syntactic
2-gram 261.0 257.8 214.3 258.1 214.6
3-gram 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%)
4-gram 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%)
Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of
perplexity relative to the lower order model of the same type.
word-tree syntactic
Eq. 10 Eq. 9 Eq. 10 Eq. 9
1 ? undgr 204.9 189.1
1 ? rnd 250.2 289.9
2 ? rnd 229.5 221.5 244.6 240.9
3 ? rnd 227.5 214.5 226.2 220.0
4 ? rnd 219.5 205.0 219.5 212.2
5 ? rnd 200.9 184.1 216.5 209.0
baseline N/A 155.7 N/A 147.1
Table 2: Perplexity numbers obtained using fourgram
trees only. Note that ?undgr? and ?rnd? denote unde-
graded and randomly grown trees with Bernoulli trials,
respectively, and the number indicates the number of
trees in the forest. Also ?baseline? refers to the fourgram
models with lower order trees (from Table 1, Eq. 9).
5 trees are much worse than a single decision tree
constructed without randomization. Finally, com-
pare the performance of single undegraded fourgram
trees in Table 2 with fourgram models in Table 1,
which are constructed with lower order trees: both
word-tree and joint models in Table 1 have over
20% lower perplexity compared to the correspond-
ing models consisting of a single fourgram tree.
In Table 3, we evaluate forests of fourgram trees
produced using randomizations without degrading
the tree construction algorithm. That is, we use ran-
dom initializations of the Exchange algorithm and,
additionally, variations in the training data fold. All
forests in this table use the interpolation method
of Eq. 9. Note that, while these perplexity num-
bers are substantially better than trees produced with
Bernoulli trials in Table 2, they are still significantly
worse than the baseline model from Table 1.
These results suggest that, while it is beneficial
to combine different decision trees, we should in-
troduce differences to the tree construction process
word-tree syntactic
# trees Exchng. +data Exchng. +data
1 204.9 189.1
2 185.9 186.5 174.5 173.7
3 179.5 179.9 168.8 167.2
4 176.2 176.4 165.1 164.0
5 173.7 172.0 163.0 162.0
baseline 155.7 147.1
Table 3: Perplexity numbers obtained using fourgram
trees produced using random initialization of the Ex-
change algorithm (Exchng. columns) and, additionally,
variations in training data folds (+data columns). Note
that ?baseline? refers to the fourgram models with lower
order trees (from Table 1). All models use the interpola-
tion method of Eq. 9.
without degrading the trees when introducing ran-
domness, especially for joint models. In addition,
lower order trees seem to play an important role for
high quality model combination.
4.3 Context-Restricted Forest
As we have mentioned above, combining higher and
lower order decision trees produces much better re-
sults. A lower order decision tree is grown from
a lower order context space, i.e., the context space
where we purposely ignore some attributes. Note
that in this case, rather than randomly ignoring con-
texts via Bernoulli trials at every node in the decision
tree, we discard some context attributes upfront in
a principled manner (i.e., most distant context) and
then grow the decision tree without degradation.
Since the joint model, having more context at-
tributes, affords a larger variety of different contexts,
we use this model in the remaining experiments.
In Table 4, we present the perplexity numbers for
our standard model with additional trees. We de-
note context-restricted trees by their Markovian or-
697
Model size PPL
1w1t + 2w2t + 3w3t + 4w4t (*) 294MB 147.1
(*) + 4w3t + 3w2t 579MB 143.5
(*) + 4w3t + 3w4t 587MB 144.9
(*) + 4w3t + 3w4t + 3w2t + 2w3t 699MB 140.7
(*) + 1 ? bernoulli-rnd 464MB 149.7
(*) + 2 ? bernoulli-rnd 632MB 150.4
(*) + 3 ? bernoulli-rnd 804MB 151.1
(*) + 1 ? data-rnd 484MB 147.0
(*) + 2 ? data-rnd 673MB 145.0
(*) + 3 ? data-rnd 864MB 145.2
Table 4: Perplexity results using the standard syntactic
model with additional trees. ?bernoulli-rnd? and ?data-
rnd? indicate fourgram trees randomized using Bernoulli
trials and varying training data, respectively. The second
column shows the combined size of decision trees in the
forest.
ders (words w and tags t independently), so 3w2t
indicates a decision tree implementing the probabil-
ity function: p(witi|wi?1wi?2ti?1). The fourgram
joint model presented in Table 1 has four trees and
is labeled with the formula ?1w1t + 2w2t + 3w3t +
4w4t? in Table 4. The randomly grown trees (de-
noted ?bernoulli-rnd?) are grown utilizing the full
context 4w4t using the methods described in Sec-
tion 4.2. All models utilize the generalized interpo-
lation method described in Section 3.2.
As can be seen in Table 4, adding undegraded
trees consistently improves the performance of an
already strong baseline, while adding random trees
only increases the perplexity because their quality
is worse than undegraded trees?. Trees produced
by data randomization (denoted ?data-rnd?) also im-
prove the performance of the model; however, the
improvement is not greater than that of additional
lower order trees, which are considerably smaller in
size.
5 ASR Rescoring Results
In order to verify that the improvements in perplex-
ity that we observe in Tables 1 and 4 are sufficient
for an impact on a task, we measure Word Error
Rate (WER) of our models on an Automatic Speech
Recognition (ASR) rescoring task using the Wall
Street Journal corpus (WSJ) for evaluation. The test
set consists of 4,088 utterances of WSJ0. We opti-
Model PPL WER
n-gram 161.7 7.81%
1w1t + 2w2t + 3w3t + 4w4t (Eq.8) 156.5 7.57%
1w1t + 2w2t + 3w3t + 4w4t (*) 147.1 7.32%
(*) + 4w3t + 3w4t + 3w2t + 2w3t 140.7 7.20%
Table 5: Perplexity and WER results. Note that the last
two rows are syntactic models using the interpolation
method of Eq. 9.
mized the weights for the combination of acoustic
and language model scores on a separate develop-
ment set comprised of 1,243 utterances from Hub2
5k closed vocabulary and the WSJ1 5k open vocab-
ulary sets.
The ASR system used to produce lattices is based
on the 2007 IBM Speech transcription system for the
GALE Distillation Go/No-go Evaluation (Chen et
al., 2006). The acoustic models are state-of-the-art
discriminatively trained models which are trained on
Broadcast News (BN) Hub4 acoustic training data.
Lattices were produced using a trigram LM trained
on the same data as the models we evaluate, then
1,000 best unique hypotheses were extracted from
the lattices. WER of the 1-best hypothesis on the
test set is 8.07% and the oracle WER is 3.54%.
In Table 5, we present WER results along with
the corresponding perplexity numbers from Ta-
bles 1 and 4 for our lowest perplexity syntactic
model, as well as the baselines (modified KN n-gram
model and standard decision tree models using in-
terpolation methods of Eq. 8 and Eq. 9). The in-
terpolation method of Eq. 9 substantially improves
performance over the interpolation method of Eq. 8,
reducing WER by 0.25% absolute (p < 10?5).
Adding four trees utilizing context restricted in dif-
ferent ways further reduces WER by 0.12%, which
is also a statistically significant (p < 0.025) im-
provement over the baseline models labeled (*). Al-
together, the improvements over the n-gram baseline
add up to 0.61% absolute (8% relative) WER reduc-
tion.
6 Conclusion
In this paper, we investigate various aspects of com-
bining multiple decision trees in a single language
model. We observe that the generalized interpola-
698
tion (Eq. 9) for decision tree models proposed in
(Filimonov and Harper, 2011) is in fact a forest in-
terpolation method rather than a backoff interpola-
tion because, in Eq. 9, models do not have explicit
higher-lower order relation as they do in backoff in-
terpolation (Eq. 6). Thus, in this paper we investi-
gate the question of how to construct decision trees
so that their combination results in improved per-
formance (under the assumption that computational
tractability allows only a handful of decision trees
in a forest). We compare various techniques for
producing forests of trees and observe that methods
that diversify trees by introducing random degrada-
tion of the tree construction algorithm perform more
poorly (especially with joint models) than methods
in which the trees are constructed without degrada-
tion and with variability being introduced via param-
eters that are inherently arbitrary (e.g., training data
fold differences or initializations of greedy search
algorithms). Additionally, we observe that simply
restricting the context used to construct trees in dif-
ferent ways, not only produces smaller trees (be-
cause of the context reduction), but the resulting
variations in trees also produce forests that are at
least as good as forests of larger trees.
7 Acknowledgments
We would like to thank Ariya Rastrow for providing
word lattices for the ASR rescoring experiments.
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical lan-
guage model for natural language speech recognition.
Readings in speech recognition, pages 507?514.
Srinivas Bangalore. 1996. ?Almost parsing? technique
for language modeling. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 1173?1176.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proceedings of HLT/NAACL, pages 4?6.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling for speech recognition. CoRR.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proceedings of the 34th annual meeting on As-
sociation for Computational Linguistics, pages 310?
318.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
Denis Filimonov and Mary Harper. 2009. A joint lan-
guage model with fine-grain syntactic tags. In Pro-
ceedings of the EMNLP 2009.
Denis Filimonov and Mary Harper. 2011. Generalized
interpolation in decision tree LM. In Proceedings of
the 49st Annual Meeting of the Association for Com-
putational Linguistics.
Peter Heeman. 1998. POS tagging versus classes in lan-
guage modeling. In Sixth Workshop on Very Large
Corpora.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP 2009.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering. In
Speech Communication, pages 1253?1256.
J. R. Quinlan. 1986. Induction of decision trees. Ma-
chine Learning, 1(1):81?106.
Ronald Rosenfeld, Jaime Carbonell, and Alexander Rud-
nicky. 1994. Adaptive statistical language modeling:
A maximum entropy approach. Technical report.
Peng Xu. 2005. Random Forests and Data Sparseness
Problem in Language Modeling. Ph.D. thesis, Balti-
more, Maryland, April.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model unseen
events in speech recognition. Computer Speech &
Language, 21(1):88?104.
699
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 216?224,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Contextual Information Improves OOV Detection in Speech
Carolina Parada, Mark Dredze
HLTCOE
Johns Hopkins University
3400 North Charles Street,
Baltimore MD 21210, USA
carolinap@jhu.edu
mdredze@cs.jhu.edu
Denis Filimonov
HLTCOE
University of Maryland,
College Park, MD 20742 USA
den@cs.umd.edu
Frederick Jelinek
HLTCOE
Johns Hopkins University
3400 North Charles Street,
Baltimore MD 21210, USA
jelinek@jhu.edu
Abstract
Out-of-vocabulary (OOV) words represent an
important source of error in large vocabulary
continuous speech recognition (LVCSR) sys-
tems. These words cause recognition failures,
which propagate through pipeline systems im-
pacting the performance of downstream ap-
plications. The detection of OOV regions in
the output of a LVCSR system is typically ad-
dressed as a binary classification task, where
each region is independently classified using
local information. In this paper, we show that
jointly predicting OOV regions, and includ-
ing contextual information from each region,
leads to substantial improvement in OOV de-
tection. Compared to the state-of-the-art, we
reduce the missed OOV rate from 42.6% to
28.4% at 10% false alarm rate.
1 Introduction
Even with a vocabulary of one hundred thou-
sand words, a large vocabulary continuous speech
recognition (LVCSR) system encounters out-of-
vocabulary (OOV) words, especially in new do-
mains or genres. New words often include named
entities, foreign words, rare and invented words.
Since these words were not seen during training, the
LVCSR system has no way to recognize them.
OOV words are an important source of error in
LVCSR systems for three reasons. First, OOVs can
never be recognized by the LVCSR system, even if
repeated. Second, OOV words contribute to recog-
nition errors in surrounding words, which propagate
into to later processing stages (translation, under-
standing, document retrieval, etc.). Third, OOVs
are often information-rich nouns ? mis-recognized
OOVs can have a greater impact on the understand-
ing of the transcript than other words.
One solution is to simply increase the LVCSR
system?s vocabulary, but there are always new
words. Additionally, increasing the vocabulary size
without limit can sometimes produce higher word
error rates (WER), leading to a tradeoff between
recognition accuracy of frequent and rare words.
A more effective solution is to detect the presence
of OOVs directly. Once identified, OOVs can be
flagged for annotation and addition to the system?s
vocabulary, or OOV segments can be transcribed
with a phone recognizer, creating an open vocabu-
lary LVCSR system. Identified OOVs prevent error
propagation in the application pipeline.
In the literature, there are two basic approaches
to OOV detection: 1) filler models, which explicitly
represent OOVs using a filler, sub-word, or generic
word model (Bazzi, 2002; Schaaf, 2001; Bisani and
Ney, 2005; Klakow et al, 1999; Wang, 2009); and
2) confidence estimation models, which use differ-
ent confidence scores to find unreliable regions and
label them as OOV (Lin et al, 2007; Burget et al,
2008; Sun et al, 2001; Wessel et al, 2001).
Recently, Rastrow et al (2009a) presented an ap-
proach that combined confidence estimation models
and filler models to improve state-of-the-art results
for OOV detection. This approach and other confi-
dence based systems (Hazen and Bazzi, 2001; Lin
et al, 2007), treat OOV detection as a binary clas-
sification task; each region is independently classi-
fied using local information as IV or OOV. This
work moves beyond this independence assumption
216
that considers regions independently for OOV de-
tection. We treat OOV detection as a sequence la-
beling problem and add features based on the local
lexical context of each region as well as global fea-
tures from a language model using the entire utter-
ance. Our results show that such information im-
proves OOV detection and we obtain large reduc-
tions in error compared to the best previously re-
ported results. Furthermore, our approach can be
combined with any confidence based system.
We begin by reviewing the current state-of-the-art
results for OOV detection. After describing our ex-
perimental setup, we generalize the framework to a
sequence labeling problem, which includes features
from the local context, lexical context, and entire ut-
terance. Each stage yields additional improvements
over the baseline system. We conclude with a review
of related work.
2 Maximum Entropy OOV Detection
Our baseline system is the Maximum Entropy model
with features from filler and confidence estimation
models proposed by Rastrow et al (2009a). Based
on filler models, this approach models OOVs by
constructing a hybrid system which combines words
and sub-word units. Sub-word units, or fragments,
are variable length phone sequences selected using
statistical methods (Siohan and Bacchiani, 2005).
The vocabulary contains a word and a fragment lex-
icon; fragments are used to represent OOVs in the
language model text. Language model training text
is obtained by replacing low frequency words (as-
sumed OOVs) by their fragment representation. Pro-
nunciations for OOVs are obtained using grapheme
to phoneme models (Chen, 2003).
This approach also includes properties from con-
fidence estimation systems. Using a hybrid LVCSR
system, they obtain confusion networks (Mangu et
al., 1999), compact representations of the recog-
nizer?s most likely hypotheses. For an utterance,
the confusion network is composed of a sequence
of confused regions, indicating the set of most likely
word/sub-word hypotheses uttered and their poste-
rior probabilities1 in a specific time interval.
1P (wi|A): posterior probability of word i given the acous-
tics, which includes the language model and acoustic model
scores, as described in (Mangu et al, 1999).
Figure 1 depicts a confusion network decoded by
the hybrid system for a section of an utterance in our
test-set. Below the network we present the reference
transcription. In this example, two OOVs were ut-
tered: ?slobodan? and ?milosevic? and decoded as
four and three in-vocabulary words, respectively. A
confused region (also called ?bin?) corresponds to
a set of competing hypothesis between two nodes.
The goal is to correctly label each of the ?bins? as
OOV or IV. Note the presence of both fragments
(e.g. s l ow, l aa s) and words in some of the
hypothesis bins.
For any bin of the confusion network, Rastrow et
al. combine features from that region using a binary
Maximum Entropy classifier (White et al, 2007).
Their most effective features were:
Fragment-Posterior =
?
f?tj
p(f |tj)
Word-Entropy = ?
?
w?tj
p(w|tj) log p(w|tj)
tj is the current bin in the confusion network and f
is a fragment in the hybrid dictionary.
We obtained confusion networks for a standard
word based system and the hybrid system described
above. We re-implemented the above features, ob-
taining nearly identical results to Rastrow et al us-
ing Mallet?s MaxEnt classifier (McCallum, 2002). 2
All real-valued features were normalized and quan-
tized using the uniform-occupancy partitioning de-
scribed in White et al (2007).3 The MaxEnt model
is regularized using a Gaussian prior (?2 = 100),
but we found results generally insensitive to ?.
3 Experimental Setup
Before we introduce and evaluate our context ap-
proach, we establish an experimental setup. We used
the dataset constructed by Can et al (2009) to eval-
uate Spoken Term Detection (STD) of OOVs; we
refer to this corpus as OOVCORP. The corpus con-
tains 100 hours of transcribed Broadcast News En-
glish speech emphasizing OOVs. There are 1290
unique OOVs in the corpus, which were selected
with a minimum of 5 acoustic instances per word.
2Small differences are due to a change in MaxEnt library.
3All experiments use 50 partitions with a minimum of 100
training values per partition.
217
Figure 1: Example confusion network from the hybrid system with OOV regions and BIO encoding. Hypothesis are
ordered by decreasing value of posterior probability. Best hypothesis is the concatenation of the top word/fragments
in each bin. We omit posterior probabilities due to spacing.
Common English words were filtered out to ob-
tain meaningful OOVs: e.g. NATALIE, PUTIN,
QAEDA, HOLLOWAY. Since the corpus was de-
signed for STD, short OOVs (less than 4 phones)
were explicitly excluded. This resulted in roughly
24K (2%) OOV tokens.
For a LVCSR system we used the IBM Speech
Recognition Toolkit (Soltau et al, 2005)4 with
acoustic models trained on 300 hours of HUB4 data
(Fiscus et al, 1998) and excluded utterances con-
taining OOV words as marked in OOVCORP. The lan-
guage model was trained on 400M words from var-
ious text sources with a 83K word vocabulary. The
LVCSR system?s WER on the standard RT04 BN
test set was 19.4%. Excluded utterances were di-
vided into 5 hours of training and 95 hours of test
data for the OOV detector. Both train and test sets
have a 2% OOV rate. We used this split for all exper-
iments. Note that the OOV training set is different
from the LVCSR training set.
In addition to a word-based LVCSR system, we
use a hybrid LVCSR system, combining word and
sub-word (fragments) units. Combined word/sub-
word systems have improved OOV Spoken Term
Detection performance (Mamou et al, 2007; Parada
et al, 2009), better phone error rates, especially in
OOV regions (Rastrow et al, 2009b), and state-of-
the-art performance for OOV detection. Our hybrid
system?s lexicon has 83K words and 20K fragments
derived using Rastrow et al (2009a). The 1290 ex-
cluded words are OOVs to both the word and hybrid
4We use the IBM system with speaker adaptive training
based on maximum likelihood with no discriminative training.
systems.
Note that our experiments use a different dataset
than Rastrow et. al., but we have a larger vocabu-
lary (83K vs 20K), which is closer to most modern
LVCSR system vocabularies; the resulting OOVs
are more challenging but more realistic.
3.1 Evaluation
Confusion networks are obtained from both the
word and hybrid LVCSR systems. In order to eval-
uate the performance of the OOV detector, we align
the reference transcript to the audio. The LVCSR
transcript is compared to the reference transcript at
the confused region level, so each confused region
is tagged as either OOV or IV. The OOV detector
assigns a score/probability for IV/OOV to each of
these regions.
Previous research reported OOV detection accu-
racy on all test data. However, once an OOV word
has been observed in the training data for the OOV
detector, even if it never appeared in the LVCSR
training data, it is no longer truly OOV. The fea-
tures used in previous approaches did not necessar-
ily provide an advantage on observed versus unob-
served OOVs, but our features do yield an advan-
tage. Therefore, in the sections that follow we re-
port unobserved OOV accuracy: OOV words that
do not appear in either the OOV detector?s or the
LVCSR?s training data. While this penalizes our re-
sults, it is a more informative metric of true system
performance.
We present results using standard detection error
tradeoff (DET) curves (Martin et al, 1997). DET
218
curves measure tradeoffs between misses and false
alarms and can be used to determine the optimal op-
erating point of a system. The x-axis varies the false
alarm rate (false positive) and the y-axis varies the
miss (false negative) rate; lower curves are better.
4 From MaxEnt to CRFs
As a classification algorithm, Maximum Entropy as-
signs a label to each region independently. However,
OOV words tend to be recognized as two or more IV
words, hence OOV regions tend to co-occur. In the
example of Figure 1, the OOV word ?slobodan? was
recognized as four IV words: ?slow vote i mean?.
This suggests that sequence models, which jointly
assign all labels in a sequence, may be more appro-
priate. Therefore, we begin incorporating context by
moving from classification to sequence models.
MaxEnt classification models the target label as
p(yi|xi), where yi is a discrete variable representing
the ith label (?IV? or ?OOV?) and xi is a feature
vector representing information for position i. The
conditional distribution for yi takes the form
p(yi|xi) =
1
Z(xi)
exp(
K?
k=1
?kfk(yi,xi)) ,
Z(xi) is a normalization term and f(yi,xi) is a vec-
tor ofK features, such as those defined in Section 2.
The model is trained discriminatively: parameters ?
are chosen to maximize conditional data likelihood.
Conditional Random Fields (CRF) (Lafferty et
al., 2001) generalize MaxEnt models to sequence
tasks. While having the same model structure as
Hidden Markov Models (HMMs), CRFs are trained
discriminatively and can use large numbers of corre-
lated features. Their primary advantage over Max-
Ent models is their ability to find an optimal labeling
for the entire sequence rather than greedy local deci-
sions. CRFs have been used successfully used in nu-
merous text processing tasks and while less popular
in speech, still applied successfully, such as sentence
boundary detection (Liu et al, 2005).
A CRF models the entire label sequence y as:
p(y|x) =
1
Z(x)
exp(?F (y,x)) ,
where F (y,x) is a global feature vector for input
sequence x and label sequence y and Z(x) is a nor-
malization term.5
5 Context for OOV Detection
We begin by including a minimal amount of local
context in making OOV decisions: the predicted la-
bels for adjacent confused regions (bins). This infor-
mation helps when OOV bins occur in close proxim-
ity, such as successive OOV bins. This is indeed the
case: in the OOV detector training data only 48% of
OOV sequences contained a single bin; sequences
were of length 2 (40%), 3 (9%) and 4 (2%). We
found similar results in the test data. Therefore, we
expect that even a minimal amount of context based
on the labels of adjacent bins will help.
A natural way of incorporating contextual infor-
mation is through a CRF, which introduces depen-
dencies between each label and its neighbors. If a
neighboring bin is likely an OOV, it increases the
chance that the current bin is OOV.
In sequence models, another technique for cap-
turing contextual dependence is the label encoding
scheme. In information extraction, where sequences
of adjacent tokens are likely to receive the same
tag, the beginning of each sequence receives a dif-
ferent tag from words that continue the sequence.
For example, the first token in a person name is
labeled B-PER and all subsequent tokens are la-
beled I-PER. This is commonly referred to as BIO
encoding (beginning, inside, outside). We applied
this encoding technique to our task, labeling bins
as either IV (in vocabulary), B-OOV (begin OOV)
and I-OOV (inside OOV), as illustrated in Figure 1.
This encoding allows the algorithm to identify fea-
tures which might be more indicative of the begin-
ning of an OOV sequence. We found that this en-
coding achieved a superior performance to a simple
IV/OOV encoding. We therefore utilize the BIO en-
coding in all CRF experiments.
Another means of introducing context is through
the order of the CRF model. A first order model
(n = 1) adds dependencies only between neighbor-
ing labels, whereas an n order model creates depen-
dencies between labels up to a distance of n posi-
tions. Higher order models capture length of label
5CRF experiments used the CRF++ package
http://crfpp.sourceforge.net/
219
regions (up to length n). We experiment with both
a first order and a second order CRF. Higher order
models did not provide any improvements.
In order to establish a comparative baseline, we
first present results using the same features from
the system described in Section 2 (Word-Entropy
and Fragment-Posterior). All real-valued features
were normalized and quantized using the uniform-
occupancy partitioning described in White et al
(2007).6 Quantization of real valued features is stan-
dard for log-linear models as it allows the model to
take advantage of non-linear characteristics of fea-
ture values and is better handled by the regulariza-
tion term. As in White et. al. we found it improved
performance.
Figure 2 depicts DET curves for OOV detection
for the MaxEnt baseline and first and second order
CRFs with BIO encoding on unobserved OOVs in
the test data. We generated predictions at different
false alarm rates by varying a probability threshold.
For MaxEnt we used the predicted label probability
and for CRFs the marginal probability of each bin?s
label. While the first order CRF achieves nearly
identical performance to the MaxEnt baseline, the
second order CRF shows a clear improvement. The
second order model has a 5% absolute improvement
at 10% false alarm rate, despite using the identi-
cal features as the MaxEnt baseline. Even a small
amount of context as expressed through local label-
ing decisions improves OOV detection.
The quantization of the features yields quan-
tized prediction scores, resulting in the non-smooth
curves for the MaxEnt and 1st order CRF results.
However, when using a second order CRF the OOV
score varies more smoothly since more features
(context labels) are considered in the prediction of
the current label.
6 Local Lexical Context
A popular approach in sequence tagging, such as in-
formation extraction or part of speech tagging, is to
include features based on local lexical content and
context. In detecting a name, both the lexical form
?John? and the preceding lexical context ?Mr.? pro-
vide clues that ?John? is a name. While we do not
6All experiments use 50 partitions with a minimum of 100
training values per partition.
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
MaxEnt (Baseline)CRF (First Order)CRF (Second Order)
Figure 2: DET curves for OOV detection using a Max-
imum Entropy (MaxEnt) classifier and contextual infor-
mation using a 1st order and 2nd order CRF. All models
use the same baseline features (Section 2).
know the actual lexical items in the speech sequence,
the speech recognizer output can be used as a best
guess. In the example of Figure 1, the words ?for-
mer president? are good indicators that the following
word is either the word ?of? or a name, and hence a
potential OOV. Combining this lexical context with
hypothesized words can help label the subsequent
regions as OOVs (note that none of the hypothesized
words in the third bin are ?of?, names, or nouns).
Words from the LVCSR decoding of the sentence
are used in the CRF OOV detector. For each bin in
the confusion network, we select the word with the
highest probability (best hypothesis). We then add
the best hypothesis word as a feature of the form:
current word=X. These features capture how the
LVCSR system incorrectly recognizes OOV words.
However, since detection is measured on unobserved
OOVs, these features alone may not help.
Instead, we turn to lexical context, which includes
correctly recognized IV words. We evaluate the fol-
lowing sets of features derived from lexical context:
? Current bin?s best hypothesis. (Current-Word)
? Unigrams and bigrams from the best hypoth-
esis in a window of 5 words around current
bin. This feature ignores the best hypothesis in
the current bin, i.e., word[-2],word[-1]
is included, but word[-1],word[0] is not.
(Context-Bigrams)
220
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
CRF (Second Order)+Current-Word+Context-Bigrams+Current-Trigrams+All-Words+All-Words-Stemmed
Figure 3: A second order CRF (Section 5) and additional
features including including word identities from current
and neighboring bins (Section 6).
? Unigrams, bigrams, and trigrams in a window
of 5 words around and including current bin.
(Current-Trigrams)
? All of the above features. (All-Words)
? All above features and their stems.7 (All-
Words-Stemmed)
We added these features to the second order CRF
with BIO encoding and baseline features (Figure 3).
As expected, the current words did not improve per-
formance on unobserved OOVs. When the current
words are combined with the lexical context and
their lemmas, they give a significant boost in perfor-
mance: a 4.2% absolute improvement at 10% false
alarm rate over the previous CRF system, and 9.3%
over the MaxEnt baseline. Interestingly, only com-
bining context and current word gives a substantial
gain. This indicates that OOVs tend to occur with
certain distributional characteristics that are inde-
pendent of the OOV word uttered (since we consider
only unobserved OOVs), perhaps because OOVs
tend to be named entities, foreign words, or rare
nouns. The importance of distributional features is
well known for named entity recognition and part
of speech tagging (Pereira et al, 1993). Other fea-
tures such as sub-strings or baseline features (Word-
7To obtain stemmed words, we use the CPAN package:
http://search.cpan.org/~snowhare/Lingua-Stem-0.83.
Entropy, Fragment-Posterior) from neighboring bins
did not provide further improvement.
7 Global Utterance Context
We now include features that incorporate informa-
tion from the entire utterance. The probability of an
utterance as computed by a language model is of-
ten used as a measure of fluency of the utterance.
We also observe that OOV words tend to take very
specific syntactic roles (more than half of them are
proper nouns), which means the surrounding context
will have predictive lexical and syntactic properties.
Therefore, we use a syntactic language model.
7.1 Language Models
We evaluated both a standard trigram language
model and a syntactic language model (Filimonov
and Harper, 2009a). The syntactic model estimates
the joint probability of the word and its syntactic tag
based on the preceding words and tags. The proba-
bility of an utterance wn1 of length n is computed by
summing over all latent syntactic tag assignments:
p(utt) = p(wn1 ) =
?
t1...tn
n?
i?1
p(wi, ti|w
i?1
1 , t
i?1
1 )
(1)
where wi and ti are the word and tag at posi-
tion i, and wi?11 and t
i?1
1 are sequences of words
and tags of length i ? 1 starting a position 1.
The model is restricted to a trigram context, i.e.,
p(wi, ti|w
i?1
i?2, t
i?1
i?2); experiments that increased the
order yielded no improvement.
We trained the language model on 130 million
words from Hub4 CSR 1996 (Garofolo et al, 1996).
The corpus was parsed using a modified Berkeley
parser (Huang and Harper, 2009) and tags extracted
from parse trees incorporated the word?s POS, the
label of its immediate parent, and the relative posi-
tion of the word among its siblings. 8 The parser
required separated contractions and possessives, but
we recombined those words after parsing to match
the LVCSR tokenization, merging their tags. Since
we are considering OOV detection, the language
model was restricted to LVCSR system?s vocabu-
lary.
8The parent tagset of Filimonov and Harper (2009a).
221
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
All-Words-Lemmas+3gram-LM+Syntactic-LM+Syntactic-LM+Tags
Figure 4: Features from a language model added to the
best CRF from Section 6 (All-Words-Stemmed).
We also used the standard trigram LM for refer-
ence. It was trained on the same data and with the
same vocabulary using the SRILM toolkit. We used
interpolated modified KN discounting.
7.2 Language Model Features
We designed features based on the entire utterance
using the language model to measure how the utter-
ance is effected by the current token: whether the
utterance is more likely given the recognized word
or some OOV word.
Likelihood-ratio = log
p(utt)
p(utt|wi = unknown)
Norm-LM-score =
log p(utt)
length(utt)
where p(utt) represents the probability of the ut-
terance using the best path hypothesis word of the
LVCSR system, and p(utt|wi = unknown) is the
probability of the entire utterance with the current
word in the LVCSR output replaced by the token
<unk>, used to represent OOVs. Intuitively, when
an OOV word is recognized as an IV word, the flu-
ency of the utterance is disrupted, especially if the
IV is a function word. The Likelihood-ratio is de-
signed to show whether the utterance is more fluent
(more likely) if the current word is a misrecognized
OOV. 9 The second feature (Norm-LM-score) is the
9Note that in the standard n-gram LM the feature reduces to
log
Qi+n?1
k=i p(wk|w
k?1
k?n+1)
Qi+n?1
k=i p(wk|w
k?1
k?n+1,wi=unknown)
, i.e., only n n-grams actu-
0 5 10 15 20 25 30 35 40P(FA)0
10
20
30
40
50
60
70
80
P(M
iss)
MaxEnt (Baseline)CRF All FeaturesCRF All Features (Unobserved)CRF All Features (Observed)
Figure 5: A CRF with all context features compared to
the state-of-the-art MaxEnt baseline. Results for the CRF
are shown for unobserved, observed and both OOVs.
normalized likelihood of the utterance. An unlikely
utterance biases the system to predicting OOVs.
We evaluated a CRF with these features and
all lexical context features (Section 6) using both
the trigram model and the joint syntactic language
model (Figure 4). Each model improved perfor-
mance, but the syntactic model provided the largest
improvement. At 10% false alarm rate it yields a
4% absolute improvement with respect to the pre-
vious best result (All-Words-Stemmed) and 13.3%
over the MaxEnt baseline. Higher order language
models did not improve.
7.3 Additional Syntactic Features
We explored other syntactic features; the most ef-
fective was the 5-tag window of POS tags of the
best hypothesis.10 The additive improvement of this
feature is depicted in Figure 4 labeled ?+Syntactic-
LM+Tags.? With this feature, we achieve a small ad-
ditional gain. We tried other syntactic features with-
out added benefit, such as the most likely POS tag
for <unk>in the utterance.
ally contribute. However, in the syntactic LM, the entire utter-
ance is affected by the change of one word through the latent
states (tags) (Eq. 1), thus making it a truly global feature.
10The POS tags were generated by the same syntactic LM
(see Section 7.1) as described in (Filimonov and Harper,
2009b). In this case, POS tags include merged tags, i.e., the vo-
cabulary word fred?s may be tagged as NNP-POS or NNP-VBZ.
222
8 Final System
Figure 5 summarizes all of the context features in a
single second order BIO encoded CRF. Results are
shown for state-of-the-art MaxEnt (Rastrow et al,
2009a) as well as for the CRF on unobserved, ob-
served and combined OOVs. For unobserved OOVs
our final system achieves a 14.2% absolute improve-
ment at 10% FA rate. The absolute improvement
on all OOVs was 23.7%. This result includes ob-
served OOVs: words that are OOV for the LVCSR
but are encountered in the OOV detector?s training
data. MaxEnt achieved similar performance for ob-
served and unobserved OOVs so we only include a
single combined result.
Note that the MaxEnt curve flattens at 26% false
alarms, while the CRF continues to decrease. The
elbow in the MaxEnt curve corresponds to the prob-
ability threshold at which no other labeled OOV re-
gion has a non-zero OOV score (regions with zero
entropy and no fragments). In this case, the CRF
model can still rely on the context to predict a non-
zero OOV score. This helps applications where
misses are more heavily penalized than false alarms.
9 Related Work
Most approaches to OOV detection in speech can
be categorized as filler models or confidence esti-
mation models. Filler models vary in three dimen-
sions: 1) The type of filler units used: variable-
length phoneme units (as the baseline system) vs
joint letter sound sub-words; 2) Method used to de-
rive units: data-driven (Bazzi and Glass, 2001) or
linguistically motivated (Choueiter, 2009); 3) The
method for incorporating the LVCSR system: hi-
erarchical (Bazzi, 2002) or flat models (Bisani and
Ney, 2005). Our approach can be integrated with
any of these systems.
We have shown that combining the presence of
sub-word units with other measures of confidence
can provided significant improvements, and other
proposed local confidence measures could be in-
cluded in our system as well. Lin et al (2007)
uses joint word/phone lattice alignments and clas-
sifies high local miss-alignment regions as OOVs.
Hazen and Bazzi (2001) combines filler models with
word confidence scores, such as the minimum nor-
malized log-likelihood acoustic model score for a
word and, the fraction of the N-best utterance hy-
potheses in which a hypothesized word appears.
Limited contextual information has been pre-
viously exploited (although maintaining indepen-
dence assumptions on the labels). Burget et al
(2008) used a neural-network (NN) phone-posterior
estimator as a feature for OOV detection. The
network is fed with posterior probabilities from
weakly-constrained (phonetic-based) and strongly-
constrained (word-based) recognizers. Their sys-
tem estimates frame-based scores, and interestingly,
they report large improvements when using tempo-
ral context in the NN input. This context is quite lim-
ited; it refers to posterior scores from one frame on
each side. Other features are considered and com-
bined using a MaxEnt model. They attribute this
gain to sampling from neighboring phonemes. Sun
et al (2001) combines a filler-based model with a
confidence approach by using several acoustic fea-
tures along with context based features, such as
whether the next word is a filler, acoustic confidence
features for next word, number of fillers, etc.
None of these approaches consider OOV detec-
tion as a sequence labeling problem. The work of
Liu et al (2005) is most similar to the approach pre-
sented here, but applies a CRF to sentence boundary
detection.
10 Conclusion and Future Work
We have presented a novel and effective approach to
improve OOV detection in the output confusion net-
works of a LVCSR system. Local and global con-
textual information is integrated with sub-word pos-
terior probabilities obtained from a hybrid LVCSR
system in a CRF to detect OOV regions effectively.
At a 10% FA rate, we reduce the missed OOV rate
from 42.6% to 28.4%, a 33.3% relative error reduc-
tion. Our future work will focus on additional fea-
tures from the recognizer aside from the single best-
hypothesis, as well as other applications of contex-
tual sequence prediction to speech tasks.
Acknowledgments
The authors thank Ariya Rastrow for providing the
baseline system code, Abhinav Sethy and Bhuvana
Ramabhadran for providing the data used in the ex-
periments and for many insightful discussions.
223
References
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Eurospeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
M. Bisani and H. Ney. 2005. Open vocabulary speech
recognition with flag hybrid models. In INTER-
SPEECH.
L. Burget, P. Schwarz, P. Matejka, M. Hannemann,
A. Rastrow, C. White, S. Khudanpur, H. Hermansky,
and J. Cernocky. 2008. Combination of strongly and
weakly constrained recognizers for reliable detection
of OOVS. In ICASSP.
Dogan Can, Erica Cooper, Abhinav Sethy, Chris White,
Bhuvana Ramabhadran, and Murat Saraclar. 2009.
Effect of pronounciations on OOV queries in spoken
term detection. ICASSP.
Stanley F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
G. Choueiter. 2009. Linguistically-motivated sub-
word modeling with applications to speech recogni-
tion. Ph.D. thesis, Massachusetts Institute of Technol-
ogy.
Denis Filimonov and Mary Harper. 2009a. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Denis Filimonov and Mary Harper. 2009b. Measuring
tagging performance of a joint language model. In
Proceedings of the Interspeech 2009.
Jonathan Fiscus, John Garofolo, Mark Przybocki,
William Fisher, and David Pallett, 1998. 1997 En-
glish Broadcast News Speech (HUB4). Linguistic
Data Consortium, Philadelphia.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Timothy J. Hazen and Issam Bazzi. 2001. A comparison
and combination of methods for OOV word detection
and word confidence scoring. In Proceedings of the
International Conference on Acoustics.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In EMNLP.
Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999.
OOV-detection in large vocabulary system using au-
tomatically defined word-fragments as fillers. In Eu-
rospeech.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Interna-
tional Conference on Machine Learning (ICML).
Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007.
OOV detection by joint word/phone lattice alignment.
In ASRU, pages 478?483, Dec.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Jonathan Mamou, Bhuvana Ramabhadran, and Olivier
Siohan. 2007. Vocabulary independent spoken term
detection. In SIGIR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words. In Eurospeech.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocky. 1997. The DET curve in assessment of
detection task performance. In Eurospeech.
Andrew McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.
umass.edu.
Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009. Query-by-example spoken term detec-
tion for OOV terms. In ASRU.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In ACL.
Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramabhad-
ran. 2009a. A new method for OOV detection using
hybrid word/fragment system. ICASSP.
Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,
and Fred Jelinek. 2009b. Towards using hybrid,
word, and fragment units for vocabulary independent
LVCSR systems. INTERSPEECH.
T. Schaaf. 2001. Detection of OOV words using gen-
eralized word models and a semantic class language
model. In Eurospeech.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary-
independent audio search using path-based graph in-
dexing. In INTERSPEECH.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The IBM 2004 conversational
telephony system for rich transcription. In ICASSP.
H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using
word confidence measure for OOV words detection in
a spontaneous spoken dialog system. In Eurospeech.
Stanley Wang. 2009. Using graphone models in au-
tomatic speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001.
Confidence measures for large vocabulary continuous
speech recognition. IEEE Transactions on Speech and
Audio Processing, 9(3).
Christopher White, Jasha Droppo, Alex Acero, and Ju-
lian Odell. 2007. Maximum entropy confidence esti-
mation for speech recognition. In ICASSP.
224
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 620?624,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Generalized Interpolation in Decision Tree LM
Denis Filimonov??
?Human Language Technology
Center of Excellence
Johns Hopkins University
den@cs.umd.edu
Mary Harper?
?Department of Computer Science
University of Maryland, College Park
mharper@umd.edu
Abstract
In the face of sparsity, statistical models are
often interpolated with lower order (backoff)
models, particularly in Language Modeling.
In this paper, we argue that there is a rela-
tion between the higher order and the backoff
model that must be satisfied in order for the
interpolation to be effective. We show that in
n-gram models, the relation is trivially held,
but in models that allow arbitrary clustering
of context (such as decision tree models), this
relation is generally not satisfied. Based on
this insight, we also propose a generalization
of linear interpolation which significantly im-
proves the performance of a decision tree lan-
guage model.
1 Introduction
A prominent use case for Language Models (LMs)
in NLP applications such as Automatic Speech
Recognition (ASR) and Machine Translation (MT)
is selection of the most fluent word sequence among
multiple hypotheses. Statistical LMs formulate the
problem as the computation of the model?s proba-
bility to generate the word sequencew1w2 . . . wm ?
wm1 , assuming that higher probability corresponds to
more fluent hypotheses. LMs are often represented
in the following generative form:
p(wm1 ) =
m?
i=1
p(wi|w
i?1
1 )
In the following discussion, we will refer to the func-
tion p(wi|w
i?1
1 ) as a language model.
Note the context space for this function, wi?11
is arbitrarily long, necessitating some independence
assumption, which usually consists of reducing the
relevant context to n? 1 immediately preceding to-
kens:
p(wi|w
i?1
1 ) ? p(wi|w
i?1
i?n+1)
These distributions are typically estimated from ob-
served counts of n-grams wii?n+1 in the training
data. The context space is still far too large; there-
fore, the models are recursively smoothed using
lower order distributions. For instance, in a widely
used n-gram LM, the probabilities are estimated as
follows:
p?(wi|w
i?1
i?n+1) = ?(wi|w
i?1
i?n+1) + (1)
?(wi?1i?n+1) ? p?(wi|w
i?1
i?n+2)
where ? is a discounted probability1.
In addition to n-gram models, there are many
other ways to estimate probability distributions
p(wi|w
i?1
i?n+1); in this work, we are particularly in-
terested in models involving decision trees (DTs).
As in n-gram models, DT models also often uti-
lize interpolation with lower order models; however,
there are issues concerning the interpolation which
arise from the fact that decision trees permit arbi-
trary clustering of context, and these issues are the
main subject of this paper.
1We refer the reader to (Chen and Goodman, 1999) for a
survey of the discounting methods for n-gram models.
620
2 Decision Trees
The vast context space in a language model man-
dates the use of context clustering in some form. In
n-gram models, the clustering can be represented as
a k-ary decision tree of depth n ? 1, where k is the
size of the vocabulary. Note that this is a very con-
strained form of a decision tree, and is probably sub-
optimal. Indeed, it is likely that some of the clusters
predict very similar distributions of words, and the
model would benefit from merging them. Therefore,
it is reasonable to believe that arbitrary (i.e., uncon-
strained) context clustering such as a decision tree
should be able to outperform the n-gram model.
A decision tree provides us with a clustering func-
tion ?(wi?1i?n+1) ? {?
1, . . . ,?N}, where N is the
number of clusters (leaves in the DT), and clusters
?k are disjoint subsets of the context space; the
probability estimation is approximated as follows:
p(wi|w
i?1
i?n+1) ? p(wi|?(w
i?1
i?n+1)) (2)
Methods of DT construction and probability estima-
tion used in this work are based on (Filimonov and
Harper, 2009); therefore, we refer the reader to that
paper for details.
Another advantage of using decision trees is the
ease of adding parameters such as syntactic tags:
p(wm1 ) =
X
t1...tm
p(wm1 t
m
1 ) =
X
t1...tm
mY
i=1
p(witi|w
i?1
1 t
i?1
1 )
?
X
t1...tm
mY
i=1
p(witi|?(w
i?1
i?n+1t
i?1
i?n+1)) (3)
In this case, the decision tree would cluster the con-
text space wi?1i?n+1t
i?1
i?n+1 based on information the-
oretic metrics, without utilizing heuristics for which
order the context attributes are to be backed off (cf.
Eq. 1). In subsequent discussion, we will write
equations for word models (Eq. 2), but they are
equally applicable to joint models (Eq. 3) with trivial
transformations.
3 Backoff Property
Let us rewrite the interpolation Eq. 1 in a more
generic way:
p?(wi|w
i?1
1 ) = ?n(wi|?n(w
i?1
1 )) + (4)
?(?n(w
i?1
1 )) ? p?(wi|BOn?1(w
i?1
1 ))
where, ?n is a discounted distribution, ?n is a clus-
tering function of order n, and ?(?n(w
i?1
1 )) is the
backoff weight chosen to normalize the distribution.
BOn?1 is the backoff clustering function of order
n ? 1, representing a reduction of context size. In
the case of an n-gram model, ?n(w
i?1
1 ) is the set
of word sequences where the last n ? 1 words are
wi?1i?n+1, similarly, BOn?1(w
i?1
1 ) is the set of se-
quences ending with wi?1i?n+2. In the case of a de-
cision tree model, the same backoff function is typ-
ically used, but the clustering function can be arbi-
trary.
The intuition behind Eq. 4 is that the backoff con-
text BOn?1(w
i?1
1 ) allows for more robust (but less
informed) probability estimation than the context
cluster ?n(w
i?1
1 ). More precisely:
?wi?11 ,W
: W ? ?n(w
i?1
1 )?W ? BOn?1(w
i?1
1 )
(5)
that is, every word sequence W that belongs to a
context cluster ?n(w
i?1
1 ), belongs to the same back-
off cluster BOn?1(w
i?1
1 ) (hence has the same back-
off distribution). For n-gram models, Property 5
trivially holds since BOn?1(w
i?1
1 ) and ?n(w
i?1
1 )
are defined as sets of sequences ending with wi?1i?n+2
and wi?1i?n+1 with the former clearly being a superset
of the latter. However, when ? can be arbitrary, e.g.,
a decision tree, that is not necessarily so.
Let us consider what happens when we have
two context sequences W and W ? that belong to
the same cluster ?n(W ) = ?n(W ?) but differ-
ent backoff clusters BOn?1(W ) 6= BOn?1(W ?).
For example: suppose we have ?(wi?2wi?1) =
({on}, {may,june}) and two corresponding backoff
clusters: BO? = ({may}) and BO?? = ({june}).
Following on, the word may is likely to be a month
rather than a modal verb, although the latter is
more frequent and will dominate in BO?. There-
fore we have much less faith in p?(wi|BO?) than in
p?(wi|BO??) and would like a much smaller weight ?
assigned to BO?, but it is not possible in the back-
off scheme in Eq. 4, thus we will have to settle on a
compromise value of ?, resulting in suboptimal per-
formance.
We would expect this effect to be more pro-
nounced in higher order models, because viola-
621
tions of Property 5 are less frequent in lower or-
der models. Indeed, in a 2-gram model, the
property is never violated since its backoff, un-
igram, contains the entire context in one clus-
ter. The 3-gram example above, ?(wi?2wi?1) =
({on}, {may,june}), although illustrative, is not
likely to occur because may in wi?1 position will
likely be split from june very early on, since it is
very informative about the following word. How-
ever, in a 4-gram model, ?(wi?3wi?2wi?1) =
({on}, {may,june}, {<unk>}) is quite plausible.
Thus, arbitrary clustering (an advantage of DTs)
leads to violation of Property 5, which, we argue,
may lead to a degradation of performance if back-
off interpolation Eq. 4 is used. In the next section,
we generalize the interpolation scheme which, as we
show in Section 6, allows us to find a better solution
in the face of the violation of Property 5.
4 Linear Interpolation
We use linear interpolation as the baseline, rep-
resented recursively, which is similar to Jelinek-
Mercer smoothing for n-gram models (Jelinek and
Mercer, 1980):
p?n(wi|w
i?1
i?n+1) = ?n(?n) ? pn(wi|?n) + (6)
(1? ?n(?n)) ? p?n?1(wi|w
i?1
i?n+2)
where ?n ? ?n(w
i?1
i?n+1), and ?n(?n) ? [0, 1] are
assigned to each cluster and are optimized on a held-
out set using EM. pn(wi|?n) is the probability dis-
tribution at the cluster ?n in the tree of order n. This
interpolation method is particularly useful as, un-
like count-based discounting methods (e.g., Kneser-
Ney), it can be applied to already smooth distribu-
tions pn2.
5 Generalized Interpolation
We can unwind the recursion in Eq. 6 and make sub-
stitutions:
?n(?n) ? ??n(?n)
(1? ?n(?n)) ? ?n?1(?n?1) ? ??n?1(?n?1)
...
2In decision trees, the distribution at a cluster (leaf) is often
recursively interpolated with its parent node, e.g. (Bahl et al,
1990; Heeman, 1999; Filimonov and Harper, 2009).
p?n(wi|w
i?1
i?n+1) =
n?
m=1
??m(?m) ? pm(wi|?m) (7)
n?
m=1
??m(?m) = 1
Note that in this parameterization, the weight as-
signed to pn?1(wi|?n?1) is limited by (1??n(?n)),
i.e., the weight assigned to the higher order model.
Ideally we should be able to assign a different set
of interpolation weights for every eligible combina-
tion of clusters ?n, ?n?1, . . . , ?1. However, not only
is the number of such combinations extremely large,
but many of them will not be observed in the train-
ing data, making parameter estimation cumbersome.
Therefore, we propose the following parameteriza-
tion for the interpolation of decision tree models:
p?n(wi|w
i?1
i?n+1) =
?n
m=1 ?m(?m) ? pm(wi|?m)?n
m=1 ?m(?m)
(8)
Note that this parameterization has the same num-
ber of parameters as in Eq. 7 (one per cluster in ev-
ery tree), but the number of degrees of freedom is
larger because the the parameters are not constrained
to sum to 1, hence the denominator.
In Eq. 8, there is no explicit distinction between
higher order and backoff models. Indeed, it ac-
knowledges that lower order models are not backoff
models when Property 5 is not satisfied. However,
it can be shown that Eq. 8 reduces to Eq. 6 if Prop-
erty 5 holds. Therefore, the new parameterization
can be thought of as a generalization of linear inter-
polation. Indeed, suppose we have the parameteri-
zation in Eq. 8 and Property 5. Let us transform this
parameterization into Eq. 7 by induction. We define:
?m ?
m?
k=1
?k ; ?m = ?m + ?m?1
where, due to space limitation, we redefine ?m ?
?m(?m) and ?m ? ?m(?m); ?m ? ?m(w
i?1
1 ),
i.e., the cluster of model order m, to which the se-
quence wi?11 belongs. The lowest order distribution
p1 is not interpolated with anything, hence:
?1p?1(wi|?1) = ?1p1(wi|?1)
Now the induction step. From Property 5, it follows
that ?m ? ?m?1, thus, for all sequences in ?wn1 ?
622
n-gram DT: Eq. 6 (baseline) DT: Eq. 8 (generalized)
order Jelinek-Mercer Mod KN word-tree syntactic word-tree syntactic
2-gram 270.2 261.0 257.8 214.3 258.1 214.6
3-gram 186.5 (31.0%) 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%)
4-gram 177.1 (5.0%) 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%)
Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of per-
plexity relative to the lower order model of the same type. ?Word-tree? and ?syntactic? refer to DT models estimated
using words only (Eq. 2) and words and tags jointly (Eq. 3).
?m, we have the same distribution:
?mpm(wi|?m) + ?m?1p?m?1(wi|?m?1) =
= ?m
(
?m
?m
pm(wi|?m) +
?m?1
?m
p?m?1(wi|?m?1)
)
= ?m
(
??mpm(wi|?m) + (1? ??m)p?m?1(wi|?m?1)
)
= ?mp?m(wi|?m) ; ??m ?
?m
?m
Note that the last transformation is because ?m ?
?m?1; had it not been the case, p?m would depend on
the combination of ?m and ?m?1 and require multi-
ple parameters to be represented on its entire domain
wn1 ? ?m. After n iterations, we have:
n?
m=1
?m(?m)pm(wi|?m) = ?np?n(wi|?n); (cf. Eq. 8)
Thus, we have constructed p?n(wi|?n) using the
same recursive representation as in Eq. 6, which
proves that the standard linear interpolation is a spe-
cial case of the new interpolation scheme, which oc-
curs when the backoff Property 5 holds.
6 Results and Discussion
Models are trained on 35M words of WSJ 94-96
from LDC2008T13. The text was converted into
speech-like form, namely numbers and abbrevia-
tions were verbalized, text was downcased, punc-
tuation was removed, and contractions and posses-
sives were joined with the previous word (i.e., they
?ll becomes they?ll). For syntactic modeling, we
used tags comprised of POS tags of the word and its
head, as in (Filimonov and Harper, 2009). Parsing
of the text for tag extraction occurred after verbal-
ization of numbers and abbreviations but before any
further processing; we used an appropriately trained
latent variable PCFG parser (Huang and Harper,
2009). For reference, we include n-gram models
with Jelinek-Mercer and modified interpolated KN
discounting. All models use the same vocabulary of
approximately 50k words.
We implemented four decision tree models3: two
using the interpolation method of (Eq. 6) and two
based on the generalized interpolation (Eq. 8). Pa-
rameters ? were estimated using the L-BFGS to
minimize the entropy on a heldout set. In order to
eliminate the influence of all factors other than the
interpolation, we used the same decision trees. The
perplexity results on WSJ section 23 are presented in
Table 1. As we have predicted, the effect of the new
interpolation becomes apparent at the 4-gram order,
when Property 5 is most frequently violated. Note
that we observe similar patterns for both word-tree
and syntactic models, with syntactic models outper-
forming their word-tree counterparts.
We believe that (Xu and Jelinek, 2004) also suf-
fers from violation of Property 5, however, since
they use a heuristic method4 to set backoff weights,
it is difficult to ascertain the extent.
7 Conclusion
The main contribution of this paper is the insight
that in the standard recursive backoff there is an im-
plied relation between the backoff and the higher or-
der models, which is essential for adequate perfor-
mance. When this relation is not satisfied other in-
terpolation methods should be employed; hence, we
propose a generalization of linear interpolation that
significantly outperforms the standard form in such
a scenario.
3We refer the reader to (Filimonov and Harper, 2009) for
details on the tree construction algorithm.
4The higher order model was discounted according to KN
discounting, while the lower order model could be either a lower
order DT (forest) model, or a standard n-gram model, with the
former performing slightly better.
623
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical lan-
guage model for natural language speech recognition.
Readings in speech recognition, pages 507?514.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech & Language, 13(4):359?393.
Denis Filimonov and Mary Harper. 2009. A joint lan-
guage model with fine-grain syntactic tags. In Pro-
ceedings of the EMNLP.
Peter A. Heeman. 1999. POS tags and decision trees
for language modeling. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
129?137.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP 2009.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
Peng Xu and Frederick Jelinek. 2004. Random forests in
language modeling. In Proceedings of the EMNLP.
624
