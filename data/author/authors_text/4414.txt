Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 53?61,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Phrasetable Smoothing for Statistical Machine Translation
George Foster and Roland Kuhn and Howard Johnson
National Research Council Canada
Ottawa, Ontario, Canada
firstname.lastname@nrc.gc.ca
Abstract
We discuss different strategies for smooth-
ing the phrasetable in Statistical MT, and
give results over a range of translation set-
tings. We show that any type of smooth-
ing is a better idea than the relative-
frequency estimates that are often used.
The best smoothing techniques yield con-
sistent gains of approximately 1% (abso-
lute) according to the BLEU metric.
1 Introduction
Smoothing is an important technique in statistical
NLP, used to deal with perennial data sparseness
and empirical distributions that overfit the training
corpus. Surprisingly, however, it is rarely men-
tioned in statistical Machine Translation. In par-
ticular, state-of-the-art phrase-based SMT relies
on a phrasetable?a large set of ngram pairs over
the source and target languages, along with their
translation probabilities. This table, which may
contain tens of millions of entries, and phrases of
up to ten words or more, is an excellent candidate
for smoothing. Yet very few publications describe
phrasetable smoothing techniques in detail.
In this paper, we provide the first system-
atic study of smoothing methods for phrase-based
SMT. Although we introduce a few new ideas,
most methods described here were devised by oth-
ers; the main purpose of this paper is not to in-
vent new methods, but to compare methods. In
experiments over many language pairs, we show
that smoothing yields small but consistent gains in
translation performance. We feel that this paper
only scratches the surface: many other combina-
tions of phrasetable smoothing techniques remain
to be tested.
We define a phrasetable as a set of source
phrases (ngrams) s? and their translations t?, along
with associated translation probabilities p(s?|t?) and
p(t?|s?). These conditional distributions are derived
from the joint frequencies c(s?, t?) of source/target
phrase pairs observed in a word-aligned parallel
corpus.
Traditionally, maximum-likelihood estimation
from relative frequencies is used to obtain con-
ditional probabilities (Koehn et al, 2003), eg,
p(s?|t?) = c(s?, t?)/?s? c(s?, t?) (since the estimation
problems for p(s?|t?) and p(t?|s?) are symmetrical,
we will usually refer only to p(s?|t?) for brevity).
The most obvious example of the overfitting this
causes can be seen in phrase pairs whose con-
stituent phrases occur only once in the corpus.
These are assigned conditional probabilities of 1,
higher than the estimated probabilities of pairs for
which much more evidence exists, in the typical
case where the latter have constituents that co-
occur occasionally with other phrases. During de-
coding, overlapping phrase pairs are in direct com-
petition, so estimation biases such as this one in
favour of infrequent pairs have the potential to sig-
nificantly degrade translation quality.
An excellent discussion of smoothing tech-
niques developed for ngram language models
(LMs) may be found in (Chen and Goodman,
1998; Goodman, 2001). Phrasetable smoothing
differs from ngram LM smoothing in the follow-
ing ways:
? Probabilities of individual unseen events are
not important. Because the decoder only
proposes phrase translations that are in the
phrasetable (ie, that have non-zero count), it
never requires estimates for pairs s?, t? having
53
c(s?, t?) = 0.1 However, probability mass is
reserved for the set of unseen translations,
implying that probability mass is subtracted
from the seen translations.
? There is no obvious lower-order distribution
for backoff. One of the most important tech-
niques in ngram LM smoothing is to com-
bine estimates made using the previous n? 1
words with those using only the previous n?i
words, for i = 2 . . . n. This relies on the
fact that closer words are more informative,
which has no direct analog in phrasetable
smoothing.
? The predicted objects are word sequences
(in another language). This contrasts to LM
smoothing where they are single words, and
are thus less amenable to decomposition for
smoothing purposes.
We propose various ways of dealing with these
special features of the phrasetable smoothing
problem, and give evaluations of their perfor-
mance within a phrase-based SMT system.
The paper is structured as follows: section 2
gives a brief description of our phrase-based SMT
system; section 3 presents the smoothing tech-
niques used; section 4 reviews previous work; sec-
tion 5 gives experimental results; and section 6
concludes and discusses future work.
2 Phrase-based Statistical MT
Given a source sentence s, our phrase-based SMT
system tries to find the target sentence t? that is
the most likely translation of s. To make search
more efficient, we use the Viterbi approximation
and seek the most likely combination of t and its
alignment a with s, rather than just the most likely
t:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-
get phrases such that t = t?1 . . . t?K ; s?k are source
phrases such that s = s?j1 . . . s?jK ; and s?k is the
translation of the kth target phrase t?k.
1This is a first approximation; exceptions occur when dif-
ferent phrasetables are used in parallel, and when rules are
used to translate certain classes of entities.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[
?
i
?ifi(s, t,a)
]
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al, 2001) on a development corpus. The
features used in this study are: the length of t;
a single-parameter distortion penalty on phrase
reordering in a, as described in (Koehn et al,
2003); phrase translation model probabilities; and
trigram language model probabilities log p(t), us-
ing Kneser-Ney smoothing as implemented in the
SRILM toolkit (Stolcke, 2002).
Phrase translation model probabilities are fea-
tures of the form:
log p(s|t,a) ?
K
?
k=1
log p(s?k|t?k)
ie, we assume that the phrases s?k specified by a
are conditionally independent, and depend only on
their aligned phrases t?k. The ?forward? phrase
probabilities p(t?|s?) are not used as features, but
only as a filter on the set of possible translations:
for each source phrase s? that matches some ngram
in s, only the 30 top-ranked translations t? accord-
ing to p(t?|s?) are retained.
To derive the joint counts c(s?, t?) from which
p(s?|t?) and p(t?|s?) are estimated, we use the phrase
induction algorithm described in (Koehn et al,
2003), with symmetrized word alignments gener-
ated using IBM model 2 (Brown et al, 1993).
3 Smoothing Techniques
Smoothing involves some recipe for modifying
conditional distributions away from pure relative-
frequency estimates made from joint counts, in or-
der to compensate for data sparsity. In the spirit of
((Hastie et al, 2001), figure 2.11, pg. 38) smooth-
ing can be seen as a way of combining the relative-
frequency estimate, which is a model with high
complexity, high variance, and low bias, with an-
other model with lower complexity, lower vari-
ance, and high bias, in the hope of obtaining bet-
ter performance on new data. There are two main
ingredients in all such recipes: some probability
distribution that is smoother than relative frequen-
cies (ie, that has fewer parameters and is thus less
54
complex) and some technique for combining that
distribution with relative frequency estimates. We
will now discuss both these choices: the distribu-
tion for carrying out smoothing and the combina-
tion technique. In this discussion, we use p?() to
denote relative frequency distributions.
Choice of Smoothing Distribution
One can distinguish between two approaches to
smoothing phrase tables. Black-box techniques do
not look inside phrases but instead treat them as
atomic objects: that is, both the s? and the t? in the
expression p(s?|t?) are treated as units about which
nothing is known except their counts. In contrast,
glass-box methods break phrases down into their
component words.
The black-box approach, which is the sim-
pler of the two, has received little attention in
the SMT literature. An interesting aspect of
this approach is that it allows one to implement
phrasetable smoothing techniques that are analo-
gous to LM smoothing techniques, by treating the
problem of estimating p(s?|t?) as if it were the prob-
lem of estimating a bigram conditional probabil-
ity. In this paper, we give experimental results
for phrasetable smoothing techniques analogous
to Good-Turing, Fixed-Discount, Kneser-Ney, and
Modified Kneser-Ney LM smoothing.
Glass-box methods for phrasetable smoothing
have been described by other authors: see sec-
tion 3.3. These authors decompose p(s?|t?) into a
set of lexical distributions p(s|t?) by making inde-
pendence assumptions about the words s in s?. The
other possibility, which is similar in spirit to ngram
LM lower-order estimates, is to combine estimates
made by replacing words in t? with wildcards, as
proposed in section 3.4.
Choice of Combination Technique
Although we explored a variety of black-box and
glass-box smoothing distributions, we only tried
two combination techniques: linear interpolation,
which we used for black-box smoothing, and log-
linear interpolation, which we used for glass-box
smoothing.
For black-box smoothing, we could have used a
backoff scheme or an interpolation scheme. Back-
off schemes have the form:
p(s?|t?) =
{
ph(s?|t?), c(s?, t?) ? ?
pb(s?|t?), else
where ph(s?|t?) is a higher-order distribution,
pb(s?|t?) is a smooth backoff distribution, and ? is
a threshold above which counts are considered re-
liable. Typically, ? = 1 and ph(s?|t?) is version of
p?(s?|t?) modified to reserve some probability mass
for unseen events.
Interpolation schemes have the general form:
p(s?|t?) = ?(s?, t?)p?(s?|t?) + ?(s?, t?)pb(s?|t?), (1)
where ? and ? are combining coefficients. As
noted in (Chen and Goodman, 1998), a key
difference between interpolation and backoff is
that the former approach uses information from
the smoothing distribution to modify p?(s?|t?) for
higher-frequency events, whereas the latter uses
it only for low-frequency events (most often 0-
frequency events). Since for phrasetable smooth-
ing, better prediction of unseen (zero-count)
events has no direct impact?only seen events are
represented in the phrasetable, and thus hypoth-
esized during decoding?interpolation seemed a
more suitable approach.
For combining relative-frequency estimates
with glass-box smoothing distributions, we em-
ployed loglinear interpolation. This is the tradi-
tional approach for glass-box smoothing (Koehn
et al, 2003; Zens and Ney, 2004). To illustrate the
difference between linear and loglinear interpola-
tion, consider combining two Bernoulli distribu-
tions p1(x) and p2(x) using each method:
plinear(x) = ?p1(x) + (1? ?)p2(x)
ploglin(x) =
p1(x)?p2(x)
p1(x)?p2(x) + q1(x)?q2(x)
where qi(x) = 1 ? pi(x). Setting p2(x) = 0.5
to simulate uniform smoothing gives ploglin(x) =
p1(x)?/(p1(x)? + q1(x)?). This is actually less
smooth than the original distribution p1(x): it pre-
serves extreme values 0 and 1, and makes inter-
mediate values more extreme. On the other hand,
plinear(x) = ?p1(x) + (1 ? ?)/2, which has the
opposite properties: it moderates extreme values
and tends to preserve intermediate values.
An advantage of loglinear interpolation is that
we can tune loglinear weights so as to maximize
the true objective function, for instance BLEU; re-
call that our translation model is itself loglinear,
with weights set to minimize errors. In fact, a lim-
itation of the experiments described in this paper
is that the loglinear weights for the glass-box tech-
niques were optimized for BLEU using Och?s al-
gorithm (Och, 2003), while the linear weights for
55
black-box techniques were set heuristically. Ob-
viously, this gives the glass-box techniques an ad-
vantage when the different smoothing techniques
are compared using BLEU! Implementing an al-
gorithm for optimizing linear weights according to
BLEU is high on our list of priorities.
The preceding discussion implicitly assumes a
single set of counts c(s?, t?) from which conditional
distributions are derived. But, as phrases of differ-
ent lengths are likely to have different statistical
properties, it might be worthwhile to break down
the global phrasetable into separate phrasetables
for each value of |t?| for the purposes of smooth-
ing. Any similar strategy that does not split up
{s?|c(s?, t?) > 0} for any fixed t? can be applied to
any smoothing scheme. This is another idea we
are eager to try soon.
We now describe the individual smoothing
schemes we have implemented. Four of them
are black-box techniques: Good-Turing and three
fixed-discount techniques (fixed-discount inter-
polated with unigram distribution, Kneser-Ney
fixed-discount, and modified Kneser-Ney fixed-
discount). Two of them are glass-box techniques:
Zens-Ney ?noisy-or? and Koehn-Och-Marcu IBM
smoothing. Our experiments tested not only these
individual schemes, but also some loglinear com-
binations of a black-box technique with a glass-
box technique.
3.1 Good-Turing
Good-Turing smoothing is a well-known tech-
nique (Church and Gale, 1991) in which observed
counts c are modified according to the formula:
cg = (c + 1)nc+1/nc (2)
where cg is a modified count value used to replace
c in subsequent relative-frequency estimates, and
nc is the number of events having count c. An
intuitive motivation for this formula is that it ap-
proximates relative-frequency estimates made by
successively leaving out each event in the corpus,
and then averaging the results (Na?das, 1985).
A practical difficulty in implementing Good-
Turing smoothing is that the nc are noisy for large
c. For instance, there may be only one phrase
pair that occurs exactly c = 347, 623 times in a
large corpus, and no pair that occurs c = 347, 624
times, leading to cg(347, 623) = 0, clearly not
what is intended. Our solution to this problem
is based on the technique described in (Church
and Gale, 1991). We first take the log of the ob-
served (c, nc) values, and then use a linear least
squares fit to log nc as a function of log c. To en-
sure that the result stays close to the reliable values
of nc for large c, error terms are weighted by c, ie:
c(log nc? log n?c)2, where n?c are the fitted values.
Our implementation pools all counts c(s?, t?) to-
gether to obtain n?c (we have not yet tried separate
counts based on length of t? as discussed above). It
follows directly from (2) that the total count mass
assigned to unseen phrase pairs is cg(0)n0 = n1,
which we approximate by n?1. This mass is dis-
tributed among contexts t? in proportion to c(t?),
giving final estimates:
p(s?|t?) = cg(s?, t?)?
s cg(s?, t?) + p(t?)n?1
,
where p(t?) = c(t?)/?t? c(t?).
3.2 Fixed-Discount Methods
Fixed-discount methods subtract a fixed discount
D from all non-zero counts, and distribute the re-
sulting probability mass according to a smoothing
distribution (Kneser and Ney, 1995). We use an
interpolated version of fixed-discount proposed by
(Chen and Goodman, 1998) rather than the origi-
nal backoff version. For phrase pairs with non-
zero counts, this distribution has the general form:
p(s?|t?) = c(s?, t?)?D?
s? c(s?, t?)
+ ?(t?)pb(s?|t?), (3)
where pb(s?|t?) is the smoothing distribution. Nor-
malization constraints fix the value of ?(t?):
?(t?) = D n1+(?, t?)/
?
s?
c(s?, t?),
where n1+(?, t?) is the number of phrases s? for
which c(s?, t?) > 0.
We experimented with two choices for the
smoothing distribution pb(s?|t?). The first is a plain
unigram p(s?), and the second is the Kneser-Ney
lower-order distribution:
pb(s?) = n1+(s?, ?)/
?
s?
n1+(s?, ?),
ie, the proportion of unique target phrases that s? is
associated with, where n1+(s?, ?) is defined anal-
ogously to n1+(?, t?). Intuitively, the idea is that
source phrases that co-occur with many different
56
target phrases are more likely to appear in new
contexts.
For both unigram and Kneser-Ney smoothing
distributions, we used a discounting coefficient de-
rived by (Ney et al, 1994) on the basis of a leave-
one-out analysis: D = n1/(n1 + 2n2). For the
Kneser-Ney smoothing distribution, we also tested
the ?Modified Kneser-Ney? extension suggested
in (Chen and Goodman, 1998), in which specific
coefficients Dc are used for small count values
c up to a maximum of three (ie D3 is used for
c ? 3). For c = 2 and c = 3, we used formu-
las given in that paper.
3.3 Lexical Decomposition
The two glass-box techniques that we considered
involve decomposing source phrases with inde-
pendence assumptions. The simplest approach as-
sumes that all source words are conditionally in-
dependent, so that:
p(s?|t?) =
J?
?
j=1
p(sj|t?)
We implemented two variants for p(sj|t?) that
are described in previous work. (Zens and Ney,
2004) describe a ?noisy-or? combination:
p(sj |t?) = 1? p(s?j |t?)
? 1?
I?
?
i=1
(1? p(sj |ti))
where s?j is the probability that sj is not in the
translation of t?, and p(sj|ti) is a lexical proba-
bility. (Zens and Ney, 2004) obtain p(sj|ti) from
smoothed relative-frequency estimates in a word-
aligned corpus. Our implementation simply uses
IBM1 probabilities, which obviate further smooth-
ing.
The noisy-or combination stipulates that sj
should not appear in s? if it is not the translation
of any of the words in t?. The complement of this,
proposed in (Koehn et al, 2005), to say that sj
should appear in s? if it is the translation of at least
one of the words in t?:
p(sj|t?) =
?
i?Aj
p(sj |ti)/|Aj |
where Aj is a set of likely alignment connections
for sj . In our implementation of this method,
we assumed that Aj = {1, . . . , I?}, ie the set of
all connections, and used IBM1 probabilities for
p(s|t).
3.4 Lower-Order Combinations
We mentioned earlier that LM ngrams have a
naturally-ordered sequence of smoothing distribu-
tions, obtained by successively dropping the last
word in the context. For phrasetable smoothing,
because no word in t? is a priori less informative
than any others, there is no exact parallel to this
technique. However, it is clear that estimates made
by replacing particular target (conditioning) words
with wildcards will be smoother than the original
relative frequencies. A simple scheme for combin-
ing them is just to average:
p(s?|t?) =
?
i=I?
c?i (s?, t?)
?
s? c?i (s?, t?)
/I?
where:
c?i (s?, t?) =
?
ti
c(s?, t1 . . . ti . . . tI?).
One might also consider progressively replacing
the least informative remaining word in the target
phrase (using tf-idf or a similar measure).
The same idea could be applied in reverse, by
replacing particular source (conditioned) words
with wildcards. We have not yet implemented
this new glass-box smoothing technique, but it has
considerable appeal. The idea is similar in spirit to
Collins? backoff method for prepositional phrase
attachment (Collins and Brooks, 1995).
4 Related Work
As mentioned previously, (Chen and Goodman,
1998) give a comprehensive survey and evalua-
tion of smoothing techniques for language mod-
eling. As also mentioned previously, there is
relatively little published work on smoothing for
statistical MT. For the IBM models, alignment
probabilities need to be smoothed for combina-
tions of sentence lengths and positions not encoun-
tered in training data (Garc??a-Varea et al, 1998).
Moore (2004) has found that smoothing to cor-
rect overestimated IBM1 lexical probabilities for
rare words can improve word-alignment perfor-
mance. Langlais (2005) reports negative results
for synonym-based smoothing of IBM2 lexical
probabilities prior to extracting phrases for phrase-
based SMT.
For phrase-based SMT, the use of smoothing to
avoid zero probabilities during phrase induction is
reported in (Marcu and Wong, 2002), but no de-
tails are given. As described above, (Zens and
57
Ney, 2004) and (Koehn et al, 2005) use two dif-
ferent variants of glass-box smoothing (which they
call ?lexical smoothing?) over the phrasetable, and
combine the resulting estimates with pure relative-
frequency ones in a loglinear model. Finally, (Cet-
tollo et al, 2005) describes the use of Witten-Bell
smoothing (a black-box technique) for phrasetable
counts, but does not give a comparison to other
methods. As Witten-Bell is reported by (Chen and
Goodman, 1998) to be significantly worse than
Kneser-Ney smoothing, we have not yet tested this
method.
5 Experiments
We carried out experiments in two different set-
tings: broad-coverage ones across six European
language pairs using selected smoothing tech-
niques and relatively small training corpora; and
Chinese to English experiments using all im-
plemented smoothing techniques and large train-
ing corpora. For the black-box techniques,
the smoothed phrase table replaced the original
relative-frequency (RF) phrase table. For the
glass-box techniques, a phrase table (either the
original RF phrase table or its replacement after
black-box smoothing) was interpolated in loglin-
ear fashion with the smoothing glass-box distribu-
tion, with weights set to maximize BLEU on a de-
velopment corpus.
To estimate the significance of the results across
different methods, we used 1000-fold pairwise
bootstrap resampling at the 95% confidence level.
5.1 Broad-Coverage Experiments
In order to measure the benefit of phrasetable
smoothing for relatively small corpora, we used
the data made available for the WMT06 shared
task (WMT, 2006). This exercise is conducted
openly with access to all needed resources and
is thus ideal for benchmarking statistical phrase-
based translation systems on a number of language
pairs.
The WMT06 corpus is based on sentences ex-
tracted from the proceedings of the European Par-
liament. Separate sentence-aligned parallel cor-
pora of about 700,000 sentences (about 150MB)
are provided for the three language pairs hav-
ing one of French, Spanish and German with En-
glish. SRILM language models based on the same
source are also provided for each of the four lan-
guages. We used the provided 2000-sentence dev-
sets for tuning loglinear parameters, and tested on
the 3064-sentence test sets.
Results are shown in table 1 for relative-
frequency (RF), Good-Turing (GT), Kneser-Ney
with 1 (KN1) and 3 (KN3) discount coefficients;
and loglinear combinations of both RF and KN3
phrasetables with Zens-Ney-IBM1 (ZN-IBM1)
smoothed phrasetables (these combinations are
denoted RF+ZN-IBM1 and KN3+ZN-IBM1).
It is apparent from table 1 that any kind of
phrase table smoothing is better than using none;
the minimum improvement is 0.45 BLEU, and
the difference between RF and all other meth-
ods is statistically significant. Also, Kneser-
Ney smoothing gives a statistically significant im-
provement over GT smoothing, with a minimum
gain of 0.30 BLEU. Using more discounting co-
efficients does not appear to help. Smoothing
relative frequencies with an additional Zens-Ney
phrasetable gives about the same gain as Kneser-
Ney smoothing on its own. However, combining
Kneser-Ney with Zens-Ney gives a clear gain over
any other method (statistically significant for all
language pairs except en?es and en?de) demon-
strating that these approaches are complementary.
5.2 Chinese-English Experiments
To test the effects of smoothing with larger
corpora, we ran a set of experiments for
Chinese-English translation using the corpora
distributed for the NIST MT05 evaluation
(www.nist.gov/speech/tests/mt). These are sum-
marized in table 2. Due to the large size of
the out-of-domain UN corpus, we trained one
phrasetable on it, and another on all other parallel
corpora (smoothing was applied to both). We also
used a subset of the English Gigaword corpus to
augment the LM training material.
corpus use sentences
non-UN phrasetable1 + LM 3,164,180
UN phrasetable2 + LM 4,979,345
Gigaword LM 11,681,852
multi-p3 dev 993
eval-04 test 1788
Table 2: Chinese-English Corpora
Table 3 contains results for the Chinese-English
experiments, including fixed-discount with uni-
gram smoothing (FDU), and Koehn-Och-Marcu
smoothing with the IBM1 model (KOM-IBM1)
58
smoothing method fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
RF 25.35 27.25 20.46 27.20 27.18 14.60
GT 25.95 28.07 21.06 27.85 27.96 15.05
KN1 26.83 28.66 21.36 28.62 28.71 15.42
KN3 26.84 28.69 21.53 28.64 28.70 15.40
RF+ZN-IBM1 26.84 28.63 21.32 28.84 28.45 15.44
KN3+ZN-IBM1 27.25 29.30 21.77 29.00 28.86 15.49
Table 1: Broad-coverage results
as described in section 3.3. As with the
broad-coverage experiments, all of the black-box
smoothing techniques do significantly better than
the RF baseline. However, GT appears to work
better in the large-corpus setting: it is statistically
indistinguishable from KN3, and both these meth-
ods are significantly better than all other fixed-
discount variants, among which there is little dif-
ference.
Not surprisingly, the two glass-box methods,
ZN-IBM1 and KOM-IBM1, do poorly when used
on their own. However, in combination with an-
other phrasetable, they yield the best results, ob-
tained by RF+ZN-IBM1 and GT+KOM-IBM1,
which are statistically indistinguishable. In con-
strast to the situation in the broad-coverage set-
ting, these are not significantly better than the
best black-box method (GT) on its own, although
RF+ZN-IBM1 is better than all other glass-box
combinations.
smoothing method BLEU score
RF 29.85
GT 30.66
FDU 30.23
KN1 30.29
KN2 30.13
KN3 30.54
ZN-IBM1 29.55
KOM-IBM1 28.09
RF+ZN-IBM1 30.95
RF+KOM-IBM1 30.10
GT+ZN-IBM1 30.45
GT+KOM-IBM1 30.81
KN3+ZN-IBM1 30.66
Table 3: Chinese-English Results
A striking difference between the broad-
coverage setting and the Chinese-English setting
is that in the former it appears to be beneficial
to apply KN3 smoothing to the phrasetable that
gets combined with the best glass-box phrasetable
(ZN), whereas in the latter setting it does not. To
test whether this was due to corpus size (as the
broad-coverage corpora are around 10% of those
for Chinese-English), we calculated Chinese-
English learning curves for the RF+ZN-IBM1 and
KN3-ZN-IBM1 methods, shown in figure 1. The
results are somewhat inconclusive: although the
KN3+ZN-IBM1 curve is perhaps slightly flatter,
the most obvious characteristic is that this method
appears to be highly sensitive to the particular cor-
pus sample used.
 0.25
 0.255
 0.26
 0.265
 0.27
 0.275
 0.28
 0.285
 0.29
 0.295
 0.3
 0  10  20  30  40  50  60  70  80
B
L
E
U
proportion of corpus
Learning curves for smoothing methods
RF+ZN-IBM1
KN3+ZN-IBM1
Figure 1: Learning curves for two glass-box com-
binations.
6 Conclusion and Future Work
We tested different phrasetable smoothing tech-
niques in two different translation settings: Eu-
ropean language pairs with relatively small cor-
pora, and Chinese to English translation with large
corpora. The smoothing techniques fall into two
59
categories: black-box methods that work only on
phrase-pair counts; and glass-box methods that de-
compose phrase probabilities into lexical proba-
bilities. In our implementation, black-box tech-
niques use linear interpolation to combine relative
frequency estimates with smoothing distributions,
while glass-box techniques are combined in log-
linear fashion with either relative-frequencies or
black-box estimates.
All smoothing techniques tested gave statisti-
cally significant gains over pure relative-frequency
estimates. In the small-corpus setting, the best
technique is a loglinear combination of Kneser-
Ney count smoothing with Zens-Ney glass-box
smoothing; this yields an average gain of 1.6
BLEU points over relative frequencies. In the
large-corpus setting, the best technique is a log-
linear combination of relative-frequency estimates
with Zens-Ney smoothing, with a gain of 1.1
BLEU points. Of the two glass-box smoothing
methods tested, Zens-Ney appears to have a slight
advantage over Koehn-Och-Marcu. Of the black-
box methods tested, Kneser-Ney is clearly bet-
ter for small corpora, but is equivalent to Good-
Turing for larger corpora.
The paper describes several smoothing alterna-
tives which we intend to test in future work:
? Linear versus loglinear combinations (in our
current work, these coincide with the black-
box versus glass-box distinction, making it
impossible to draw conclusions).
? Lower-order distributions as described in sec-
tion 3.4.
? Separate count-smoothing bins based on
phrase length.
7 Acknowledgements
The authors would like to thank their colleague
Michel Simard for stimulating discussions. The
first author would like to thank all his colleagues
for encouraging him to taste a delicacy that was
new to him (shredded paper with maple syrup).
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-
0023. Any opinions, findings and conclusions or
recommendations expressed in this material are
those of the author(s) and do not necessarily re-
flect the views of the Defense Advanced Research
Projects Agency (DARPA).
References
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993. The
mathematics of Machine Translation: Parameter es-
timation. Computational Linguistics, 19(2):263?
312, June.
M. Cettollo, M. Federico, N. Bertoldi, R. Cattoni, and
B. Chen. 2005. A look inside the ITC-irst SMT
system. In Proceedings of MT Summit X, Phuket,
Thailand, September. International Association for
Machine Translation.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University.
K. Church and W. Gale. 1991. A comparison of the
enhanced Good-Turing and deleted estimation meth-
ods for estimating probabilities of English bigrams.
Computer speech and language, 5(1):19?54.
M. Collins and J. Brooks. 1995. Prepositional phrase
attachment through a backed-off model. In Proceed-
ings of the 3rd ACL Workshop on Very Large Cor-
pora (WVLC), Cambridge, Massachusetts.
Ismael Garc??a-Varea, Francisco Casacuberta, and Her-
mann Ney. 1998. An iterative, DP-based search al-
gorithm for statistical machine translation. In Pro-
ceedings of the 5th International Conference on Spo-
ken Language Processing (ICSLP) 1998, volume 4,
pages 1135?1138, Sydney, Australia, December.
Joshua Goodman. 2001. A bit of progress in language
modeling. Computer Speech and Language.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2001. The Elements of Statistical Learning.
Springer.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP) 1995,
pages 181?184, Detroit, Michigan. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Ed-
uard Hovy, editor, Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 127?133, Edmonton, Alberta,
Canada, May. NAACL.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, D. Talbot, and M. White. 2005. Ed-
inburgh system description for the 2005 NIST MT
evaluation. In Proceedings of Machine Translation
Evaluation Workshop.
Philippe Langlais, Guihong Cao, and Fabrizio Gotti.
2005. RALI: SMT shared task system description.
60
In Proceedings of the 2nd ACL workshop on Build-
ing and Using Parallel Texts, pages 137?140, Uni-
versity of Michigan, Ann Arbor, June.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Philadelphia, PA.
Robert C. Moore. 2004. Improving IBM word-
alignment model 1. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Barcelona, July.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 10:1?38.
Arthur Na?das. 1985. On Turing?s formula for
word probabilities. IEEE Transactions on Acous-
tics, Speech and Signal Processing (ASSP), ASSP-
33(6):1415?1417, December.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Meeting of the Association for
Computational Linguistics (ACL), Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Re-
port RC22176, IBM, September.
Andreas Stolcke. 2002. SRILM - an extensi-
ble language modeling toolkit. In Proceedings of
the 7th International Conference on Spoken Lan-
guage Processing (ICSLP) 2002, Denver, Colorado,
September.
WMT. 2006. The NAACL Workshop on Statistical
Machine Translation (www.statmt.org/wmt06), New
York, June.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proceedings of Human Language Technol-
ogy Conference / North American Chapter of the
ACL, Boston, May.
61
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 913?920
Manchester, August 2008
 
	
	ffProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 967?975, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Translation Quality by Discarding Most of the Phrasetable
J Howard Johnson and Joel Martin
Interactive Information Group
National Research Council Canada
Ottawa, Ontario, Canada
firstname.lastname@nrc.gc.ca
George Foster and Roland Kuhn
Interactive Language Technologies Group
National Research Council Canada
Gatineau, Que?bec, Canada
firstname.lastname@nrc.gc.ca
Abstract
It is possible to reduce the bulk of phrase-
tables for Statistical Machine Translation us-
ing a technique based on the significance
testing of phrase pair co-occurrence in the
parallel corpus. The savings can be quite
substantial (up to 90%) and cause no reduc-
tion in BLEU score. In some cases, an im-
provement in BLEU is obtained at the same
time although the effect is less pronounced
if state-of-the-art phrasetable smoothing is
employed.
1 Introduction
An important part of the process of Statistical Ma-
chine Translation (SMT) involves inferring a large
table of phrase pairs that are translations of each
other from a large corpus of aligned sentences.
These phrase pairs together with estimates of con-
ditional probabilities and useful feature weights,
called collectively a phrasetable, are used to match
a source sentence to produce candidate translations.
The choice of the best translation is made based
on the combination of the probabilities and feature
weights, and much discussion has been made of how
to make the estimates of probabilites, how to smooth
these estimates, and what features are most useful
for discriminating among the translations.
However, a cursory glance at phrasetables pro-
duced often suggests that many of the translations
are wrong or will never be used in any translation.
On the other hand, most obvious ways of reducing
the bulk usually lead to a reduction in translation
quality as measured by BLEU score. This has led to
an impression that these pairs must contribute some-
thing in the grand scheme of things and, certainly,
more data is better than less.
Nonetheless, this bulk comes at a cost. Large ta-
bles lead to large data structures that require more
resources and more time to process and, more im-
portantly, effort directed in handling large tables
could likely be more usefully employed in more fea-
tures or more sophisticated search.
In this paper, we show that it is possible to prune
phrasetables using a straightforward approach based
on significance testing, that this approach does not
adversely affect the quality of translation as mea-
sured by BLEU score, and that savings in terms of
number of discarded phrase pairs can be quite sub-
stantial. Even more surprising, pruning can actu-
ally raise the BLEU score although this phenomenon
is less prominent if state of the art smoothing of
phrasetable probabilities is employed.
Section 2 reviews the basic ideas of Statistical
Machine Translation as well as those of testing sig-
nificance of associations in two by two contingency
tables departing from independence. From this, a
filtering algorithm will be described that keeps only
phrase pairs that pass a significance test. Section 3
outlines a number of experiments that demonstrate
the phenomenon and measure its magnitude. Sec-
tion 4 presents the results of these experiments. The
paper concludes with a summary of what has been
learned and a discussion of continuing work that
builds on these ideas.
967
2 Background Theory
2.1 Our Approach to Statistical Machine
Translation
We define a phrasetable as a set of source phrases (n-
grams) s? and their translations (m-grams) t?, along
with associated translation probabilities p(s?|t?) and
p(t?|s?). These conditional distributions are derived
from the joint frequencies c(s?, t?) of source / tar-
get n,m-grams observed in a word-aligned parallel
corpus. These joint counts are estimated using the
phrase induction algorithm described in (Koehn et
al., 2003), with symmetrized word alignments gen-
erated using IBM model 2 (Brown et al, 1993).
Phrases are limited to 8 tokens in length (n,m ? 8).
Given a source sentence s, our phrase-based SMT
system tries to find the target sentence t? that is the
most likely translation of s. To make search more
efficient, we use the Viterbi approximation and seek
the most likely combination of t and its alignment a
with s, rather than just the most likely t:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-
get phrases such that t = t?1...t?K ; s?k are source
phrases such that s = s?j1 ...s?jK ; and s?k is the trans-
lation of the kth target phrase t?k.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[
?
i
?ifi(s, t,a)
]
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al , 2001) on a development corpus. The
features used are: the length of t; a single-parameter
distortion penalty on phrase reordering in a, as de-
scribed in (Koehn et al, 2003); phrase translation
model probabilities; and 4-gram language model
probabilities log p(t), using Kneser-Ney smooth-
ing as implemented in the SRILM toolkit (Stolcke,
2002).
Phrase translation model probabilities are features
of the form:
log p(s|t,a) ?
K?
k=1
log p(s?k|t?k)
i.e., we assume that the phrases s?k specified by a are
conditionally independent, and depend only on their
aligned phrases t?k.
The ?forward? phrase probabilities p(t?|s?) are not
used as features, but only as a filter on the set of
possible translations: for each source phrase s? that
matches some ngram in s, only the 30 top-ranked
translations t? according to p(t?|s?) are retained. One
of the reviewers has pointed out correctly that tak-
ing only the top 30 translations will interact with the
subject under study; however, this pruning technique
has been used as a way of controlling the width of
our beam search and rebalancing search parameters
would have complicated this study and taken it away
from our standard practice.
The phrase translation model probabilities are
smoothed according to one of several techniques as
described in (Foster et al, 2006) and identified in the
discussion below.
2.2 Significance testing using two by two
contingency tables
Each phrase pair can be thought of as am n,m-gram
(s?, t?) where s? is an n-gram from the source side of
the corpus and t? is an m-gram from the target side
of the corpus.
We then define: C(s?, t?) as the number of parallel
sentences that contain one or more occurrences of
s? on the source side and t? on the target side; C(s?)
the number of parallel sentences that contain one or
more occurrences of s? on the source side; and C(t?)
the number of parallel sentences that contain one or
more occurrences of t? on the target side. Together
with N , the number of parallel sentences, we have
enough information to draw up a two by two contin-
gency table representing the unconditional relation-
ship between s? and t?. This table is shown in Table
1.
A standard statistical technique used to assess the
importance of an association represented by a con-
tingency table involves calculating the probability
that the observed table or one that is more extreme
could occur by chance assuming a model of inde-
pendence. This is called a significance test. Intro-
ductory statistics texts describe one such test called
the Chi-squared test.
There are other tests that more accurately apply
to our small tables with only two rows and columns.
968
Table 1: Two by two contingency table for s? and t?
C(s?, t?) C(s?)? C(s?, t?) C(s?)
C(t?)? C(s?, t?) N ? C(s?)? C(t?) + C(s?, t?) N ? C(s?)
C(t?) N ? C(t?) N
In particular, Fisher?s exact test calculates probabil-
ity of the observed table using the hypergeometric
distibution.
ph(C(s?, t?)) =
(
C(s?)
C(s?, t?)
)(
N ? C(s?)
C(t?)? C(s?, t?)
)
(
N
C(t?)
)
The p-value associated with our observed table is
then calculated by summing probabilities for tables
that have a larger C(s?, t?)).
p-value(C(s?, t?)) =
??
k=C(s?,t?)
ph(k)
This probability is interpreted as the probability
of observing by chance an association that is at least
as strong as the given one and hence its significance.
Agresti (1996) provides an excellent introduction to
this topic and the general ideas of significance test-
ing in contingency tables.
Fisher?s exact test of significance is considered a
gold standard since it represents the precise proba-
bilities under realistic assumptions. Tests such as the
Chi-squared test or the log-likelihood-ratio test (yet
another approximate test of significance) depend on
asymptotic assumptions that are often not valid for
small counts.
Note that the count C(s?, t?) can be larger or
smaller than c(s?, t?) discussed above. In most cases,
it will be larger, because it counts all co-occurrences
of s? with t? rather than just those that respect the
word alignment. It can be smaller though because
multiple co-occurrences can occur within a single
aligned sentence pair and be counted multiple times
in c(s?, t?). On the other hand, C(s?, t?) will not count
all of the possible ways that an n,m-grammatch can
occur within a single sentence pair; it will count the
match only once per sentence pair in which it occurs.
Moore (2004) discusses the use of signifi-
cance testing of word associations using the log-
likelihood-ratio test and Fisher?s exact test. He
shows that Fisher?s exact test is often a practical
method if a number of techniques are followed:
1. approximating the logarithms of factorials us-
ing commonly available numerical approxima-
tions to the log gamma function,
2. using a well-known recurrence for the hyperge-
ometic distribution,
3. noting that few terms usually need to be
summed, and
4. observing that convergence is usually rapid.
2.3 Significance pruning
The idea behind significance pruning of phrasetables
is that not all of the phrase pairs in a phrasetable are
equally supported by the data and that many of the
weakly supported pairs could be removed because:
1. the chance of them occurring again might be
low, and
2. their occurrence in the given corpus may be the
result of an artifact (a combination of effects
where several estimates artificially compensate
for one another). This concept is usually re-
ferred to as overfit since the model fits aspects
of the training data that do not lead to improved
prediction.
Phrase pairs that cannot stand on their own by
demonstrating a certain level of significance are sus-
pect and removing them from the phrasetable may
969
be beneficial in terms of reducing the size of data
structures. This will be shown to be the case in rather
general terms.
Note that this pruning may and quite often will
remove all of the candidate translations for a source
phrase. This might seem to be a bad idea but it must
be remembered that deleting longer phrases will al-
low combinations of shorter phrases to be used and
these might have more and better translations from
the corpus. Here is part of the intuition about how
phrasetable smoothing may interact with phrasetable
pruning: both are discouraging longer but infrequent
phrases from the corpus in favour of combinations of
more frequent, shorter phrases.
Because the probabilities involved below will be
so incredibly tiny, we will work instead with the neg-
ative of the natural logs of the probabilities. Thus
instead of selecting phrase pairs with a p-value less
than exp(?20), we will select phrase pairs with a
negative-log-p-value greater than 20. This has the
advantage of working with ordinary-sized numbers
and the happy convention that bigger means more
pruning.
2.4 C(s?, t?) = 1, 1-1-1 Tables and the ?
Threshold
An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and
each of the component phrases occurs exactly once
in its side of the parallel corpus.
These phrase pairs will be referred to as 1-1-1
phrase pairs and the corresponding tables will be
called 1-1-1 contingency tables because C(s?) = 1,
C(t?) = 1, and C(s?, t?) = 1.
Moore (2004) comments that the p-value for these
tables under Fisher?s exact test is 1/N . Since we are
using thresholds of the negative logarithm of the p-
value, the value ? = log(N) is a useful threshold to
consider.
In particular, ? +  (where  is an appropriately
small positive number) is the smallest threshold that
results in none of the 1-1-1 phrase pairs being in-
cluded. Similarly, ? ?  is the largest threshold that
results in all of the 1-1-1 phrase pairs being included.
Because 1-1-1 phrase pairs can make up a large part
of the phrase table, this is important observation for
its own sake.
Since the contingency table with C(s?, t?) = 1 hav-
ing the greatest significance (lowest p-value) is the
1-1-1 table, using the threshold of ?+  can be used
to exclude all of the phrase pairs occurring exactly
once (C(s?, t?) = 1).
The common strategy of deleting all of the 1-
count phrase pairs is very similar in effect to the use
of the ? +  threshold.
3 Experiments
3.1 WMT06
The corpora used for most of these experiments are
publicly available and have been used for a num-
ber of comparative studies (Workshop on Statisti-
cal Machine Translation, 2006). Provided as part of
the materials for the shared task are parallel corpora
for French?English, Spanish?English, and German?
English as well as language models for English,
French, Spanish, and German. These are all based
on the Europarl resources (Europarl, 2003).
The only change made to these corpora was to
convert them to lowercase and to Unicode UTF-8.
Phrasetables were produced by symmetrizing IBM2
conditional probabilities as described above.
The phrasetables were then used as a list of
n,m-grams for which counts C(s?, t?), C(s?), and
C(t?) were obtained. Negative-log-p-values under
Fisher?s exact test were computed for each of the
phrase pairs in the phrasetable and the entry was
censored if the negative-log-p-value for the test was
below the pruning threshold. The entries that are
kept are ones that are highly significant.
A number of combinations involving many differ-
ent pruning thresholds were considered: no pruning,
10, ??, ?+, 15, 20, 25, 50, 100, and 1000. In ad-
dition, a number of different phrasetable smoothing
algorithms were used: no smoothing, Good-Turing
smoothing, Kneser-Ney 3 parameter smoothing and
the loglinear mixture involving two features called
Zens-Ney (Foster et al, 2006).
3.2 Chinese
To test the effects of significance pruning on larger
corpora, a series of experiments was run on a much
larger corpus based on that distributed for MT06
Chinese?English (NIST MT, 2006). Since the ob-
jective was to assess how the method scaled we used
our preferred phrasetable smoothing technique of
970
1000100101
BLEU by Pruning Threshold
no smoothing
3
3
333 3
3
3
3
GT (+1)
+ +
+++ +
+
+
+
KN3 (+2)
2 2222 2
2
2
2
ZN (+3)
? ???? ?
?
?
?
107
106
105
1000100101
Phrasetable Size by Pruning Threshold
size3 3
333
3
3
3
3
107106105
BLEU by Phrasetable Size
no smoothing
3
3
3333
3
3
3
GT (+1)
++
++++
+
+
+
KN3 (+2)
222222
2
2
2
ZN (+3)
??????
?
?
?
Figure 1: WMT06: Results for French ?? English.
[to separate the curves, graphs for smoothed meth-
ods are shifted by +1, +2, or +3 BLEU points]
Table 2: Corpus Sizes and ? Values
number of
parallel sentences ?
WMT06: fr?? en 688,031 13.4415892
WMT06: es?? en 730,740 13.501813
WMT06: de?? en 751,088 13.5292781
Chinese?English: best 3,164,228 14.9674197
Chinese?English: UN-v2 4,979,345 15.4208089
Zens-Ney and separated our corpus into two phrase-
tables, one based on the UN corpus and the other
based on the best of the remaining parallel corpora
available to us.
Different pruning thresholds were considered: no
pruning, 14, 16, 18, 20, and 25. In addition, another
more aggressive method of pruning was attempted.
Moore points out, correctly, that phrase pairs that oc-
cur in only one sentence pair, (C(s?, t?) = 1 ), are less
reliable and might require more special treatment.
These are all pruned automatically at thresholds of
16 and above but not at threshold of 14. A spe-
cial series of runs was done for threshold 14 with all
of these singletons removed to see whether at these
thresholds it was the significance level or the prun-
ing of phrase pairs with (C(s?, t?) = 1 ) that was more
important. This is identified as 14? in the results.
4 Results
The results of the experiments are described in Ta-
bles 2 through 6.
Table 2 presents the sizes of the various parallel
corpora showing the number of parallel sentences,
N , for each of the experiments, together with the ?
thresholds (? = log(N)).
Table 3 shows the sizes of the phrasetables that
result from the various pruning thresholds described
for the WMT06 data. It is clear that this is extremely
aggressive pruning at the given levels.
Table 4 shows the corresponding phrasetable sizes
for the large corpus Chinese?English data. The
pruning is not as aggressive as for the WMT06 data
but still quite sizeable.
Tables 5 and 6 show the main results for the
WMT06 and the Chinese?English large corpus ex-
periments. To make these results more graphic, Fig-
ure 1 shows the French ?? English data from the
WMT06 results in the form of three graphs. Note
971
Table 3: WMT06: Distinct phrase pairs by pruning threshold
threshold fr?? en es?? en de?? en
none 9,314,165 100% 11,591,013 100% 6,954,243 100%
10 7,999,081 85.9% 10,212,019 88.1% 5,849,593 84.1%
??  6,014,294 64.6% 7,865,072 67.9% 4,357,620 62.7%
? +  1,435,576 15.4% 1,592,655 13.7% 1,163,296 16.7%
15 1,377,375 14.8% 1,533,610 13.2% 1,115,559 16.0%
20 1,152,780 12.4% 1,291,113 11.1% 928,855 13.4%
25 905,201 9.7% 1,000,264 8.6% 732,230 10.5%
50 446,757 4.8% 481,737 4.2% 365,118 5.3%
100 235,132 2.5% 251,999 2.2% 189,655 2.7%
1000 22,873 0.2% 24,070 0.2% 16,467 0.2%
Table 4: Chinese?English: Distinct phrase pairs by pruning threshold
threshold best UN-v2
none 18,858,589 100% 20,228,273 100%
14 7,666,063 40.7% 13,276,885 65.6%
16 4,280,845 22.7% 7,691,660 38.0%
18 4,084,167 21.7% 7,434,939 36.8%
20 3,887,397 20.6% 7,145,827 35.3%
25 3,403,674 18.0% 6,316,795 31.2%
also pruning C(s?, t?) = 1
14? 4,477,920 23.7% 7,917,062 39.1%
that an artificial separation of 1 BLEU point has
been introduced into these graphs to separate them.
Without this, they lie on top of each other and hide
the essential point. In compensation, the scale for
the BLEU co-ordinate has been removed.
These results are summarized in the following
subsections.
4.1 BLEU as a function of threshold
In tables 5 and 6, the largest BLEU score for each
set of runs has been marked in bold font. In addition,
to highlight that there are many near ties for largest
BLEU, all BLEU scores that are within 0.1 of the
best are also marked in bold.
When this is done it becomes clear that pruning
at a level of 20 for the WMT06 runs would not re-
duce BLEU in most cases and in many cases would
actually increase it. A pruning threshold of 20 cor-
responds to discarding roughly 90% of the phrase-
table.
For the Chinese?English large corpus runs, a level
of 16 seems to be about the best with a small in-
crease in BLEU and a 60% ? 70% reduction in the
size of the phrasetable.
4.2 BLEU as a function of depth of pruning
Another view of this can be taken from Tables 5
and 6. The fraction of the phrasetable retained is
a more or less simple function of pruning threshold
as shown in Tables 3 and 4. By including the per-
centages in Tables 5 and 6, we can see that BLEU
goes up as the fraction approaches between 20% and
30%.
This seems to be a relatively stable observation
across the experiments. It is also easily explained by
its strong relationship to pruning threshold.
4.3 Large corpora
Table 6 shows that this is not just a small corpus phe-
nomenon. There is a sizeable benefit both in phrase-
table reduction and a modest improvement to BLEU
even in this case.
4.4 Is this just the same as phrasetable
smoothing?
One question that occurred early on was whether this
improvement in BLEU is somehow related to the
improvement in BLEU that occurs with phrasetable
smoothing.
972
It appears that the answer is, in the main, yes, al-
though there is definitely something else going on.
It is true that the benefit in terms of BLEU is less-
ened for better types of phrasetable smoothing but
the benefit in terms of the reduction in bulk holds. It
is reassuring to see that no harm to BLEU is done by
removing even 80% of the phrasetable.
4.5 Comment about C(s?, t?) = 1
Another question that came up is the role of phrase
pairs that occur only once: C(s?, t?) = 1. In particu-
lar as discussed above, the most significant of these
are the 1-1-1 phrase pairs whose components also
only occur once: C(s?) = 1, and C(t?) = 1. These
phrase pairs are amazingly frequent in the phrase-
tables and are pruned in all of the experiments ex-
cept when pruning threshold is equal to 14.
The Chinese?English large corpus experiments
give us a good opportunity to show that significance
level seems to be more an issue than the case that
C(s?, t?) = 1.
Note that we could have kept the phrase pairs
whose marginal counts were greater than one but
most of these are of lower significance and likely
are pruned already by the threshold. The given con-
figuration was considered the most likely to yield a
benefit and its poor performance led to the whole
idea being put aside.
5 Conclusions and Continuing Work
To sum up, the main conclusions are five in number:
1. Phrasetables produced by the standard Diag-
Andmethod (Koehn et al, 2003) can be aggres-
sively pruned using significance pruning with-
out worsening BLEU.
2. If phrasetable smoothing is not done, the BLEU
score will improve under aggressive signifi-
cance pruning.
3. If phrasetable smoothing is done, the improve-
ment is small or negligible but there is still no
loss on aggressive pruning.
4. The preservation of BLEU score in the pres-
ence of large-scale pruning is a strong effect in
small and moderate size phrasetables, but oc-
curs also in much larger phrasetables.
5. In larger phrasetables based on larger corpora,
the percentage of the table that can be dis-
carded appears to decrease. This is plausible
since a similar effect (a decrease in the benefit
of smoothing) has been noted with phrasetable
smoothing (Foster et al, 2006). Together these
results suggest that, for these corpus sizes, the
increase in the number of strongly supported
phrase pairs is greater than the increase in the
number of poorly supported pairs, which agrees
with intuition.
Although there may be other approaches to prun-
ing that achieve a similar effect, the use of Fisher?s
exact test is mathematically and conceptually one of
the simplest since it asks a question separately for
each phrase pair: ?Considering this phase pair in
isolation of any other analysis on the corpus, could it
have occurred plausibly by purely random processes
inherent in the corpus construction?? If the answer
is ?Yes?, then it is hard to argue that the phrase pair
is an association of general applicability from the
evidence in this corpus alone.
Note that the removal of 1-count phrase pairs is
subsumed by significance pruning with a threshold
greater than ? and many of the other simple ap-
proaches (from an implementation point of view)
are more difficult to justify as simply as the above
significance test. Nonetheless, there remains work
to do in determining if computationally simpler ap-
proaches do as well. Moore?s work suggests that
log-likelihood-ratio would be a cheaper and accurate
enough alternative, for example.
We will now return to the interaction of the se-
lection in our beam search of the top 30 candidates
based on forward conditional probabilities. This will
affect our results but most likely in the following
manner:
1. For very small thresholds, the beam will be-
come much wider and the search will take
much longer. In order to allow the experiments
to complete in a reasonable time, other means
will need to be employed to reduce the choices.
This reduction will also interact with the sig-
nificance pruning but in a less understandable
manner.
2. For large thresholds, there will not be 30
973
choices and so there will be no effect.
3. For intermediate thresholds, the extra prun-
ing might reduce BLEU score but by a small
amount because most of the best choices are
included in the search.
Using thresholds that remove most of the phrase-
table would no doubt qualify as large thresholds so
the question is addressing the true shape of the curve
for smaller thresholds and not at the expected operat-
ing levels. Nonetheless, this is a subject for further
study, especially as we consider alternatives to our
?filter 30? approach for managing beam width.
There are a number of important ways that this
work can and will be continued. The code base for
taking a list of n,m-grams and computing the re-
quired frequencies for signifance evaluation can be
applied to related problems. For example, skip-n-
grams (n-grams that allow for gaps of fixed or vari-
able size) may be studied better using this approach
leading to insight about methods that weakly ap-
proximate patterns.
The original goal of this work was to better un-
derstand the character of phrasetables, and it re-
mains a useful diagnostic technique. It will hope-
fully lead to more understanding of what it takes
to make a good phrasetable especially for languages
that require morphological analysis or segmentation
to produce good tables using standard methods.
The negative-log-p-value promises to be a useful
feature and we are currently evaluating its merits.
6 Acknowledgement
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).?
References
Alan Agresti. 1996. An Introduction to Categorical Data
Analysis. Wiley.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra and Robert L. Mercer. 1993. The Mathemat-
ics of Statistical Machine Translation: Parameter es-
timation. Computational Linguistics, 19(2):263?312,
June.
Philipp Koehn 2003. Europarl: A Mul-
tilingual Corpus for Evaluation of Ma-
chine Translation. Unpublished draft. see
http://www.iccs.inf.ed.ac.uk/?pkoehn
/publications/europarl.pdf
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Machine
Translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, Sydney, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing (ICASSP) 1995, pages
181?184, Detroit, Michigan. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Eduard
Hovy, editor, Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 127?133, Edmonton, Alberta, Canada, May.
NAACL.
Robert C. Moore. 2004. On Log-Likelihood-Ratios and
the Significance of Rare Events. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, Barcelona, Spain.
NIST. 2006. NIST MT Benchmark Test. see
http://www.nist.gov/speech/tests/mt/
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Computa-
tional Linguistics(ACL), Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Report
RC22176, IBM, September.
NAACL Workshop on Statistical Machine Translation.
2006. see http://www.statmt.org/wmt06/
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP) 2002, Denver, Colorado, September.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of Human Language Technology Conference
/ North American Chapter of the ACL, Boston, May.
974
Table 5: WMT06 Results: BLEU by type of smoothing and pruning threshold
threshold phrasetable % fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
relative frequency: no smoothing
none 100% 25.39 27.26 20.74 27.29 27.17 14.71
10 84?88% 25.97 27.81 21.08 27.82 27.71 15.09
??  63?68% 26.32 28.00 21.27 28.11 28.09 15.19
? +  14?17% 26.34 28.27 21.22 28.16 28.08 15.24
15 13?15% 26.36 28.50 21.14 28.20 28.18 15.29
20 11?13% 26.51 28.45 21.36 28.28 28.06 15.28
25 8?10% 26.50 28.38 21.28 28.32 27.97 15.25
50 4?5% 26.26 27.88 20.87 28.05 27.90 15.08
100 2% 25.66 27.07 20.07 27.38 27.11 14.66
1000 0.2% 20.49 21.66 15.23 22.51 22.31 11.36
Good-Turing
none 100% 25.96 28.14 21.17 27.84 27.95 15.13
10 84?88% 26.33 28.33 21.38 28.18 28.27 15.22
??  63?68% 26.54 28.63 21.50 28.36 28.39 15.31
? +  14?17% 26.24 28.49 21.15 28.22 28.16 15.28
15 13?15% 26.48 28.03 21.21 28.27 28.21 15.31
20 11?13% 26.65 28.45 21.41 28.36 28.14 15.25
25 8?10% 26.54 28.56 21.31 28.35 28.04 15.28
50 4?5% 26.26 27.78 20.94 28.07 27.95 15.08
100 2% 25.70 27.07 20.12 27.41 27.13 14.66
1000 0.2% 20.49 21.66 15.52 22.53 22.31 11.37
Kneser-Ney (3 parameter)
none 100% 26.89 28.70 21.78 28.64 28.71 15.50
10 84?88% 26.79 28.78 21.71 28.63 28.41 15.35
15 13?15% 26.49 28.69 21.34 28.60 28.57 15.52
20 11?13% 26.73 28.67 21.54 28.56 28.44 15.41
25 8?10% 26.84 28.70 21.29 28.54 28.21 15.42
50 4?5% 26.44 28.16 20.93 28.17 28.05 15.17
100 2% 25.72 27.27 20.11 27.50 27.26 14.58
1000 0.2% 20.48 21.70 15.28 22.58 22.36 11.33
Zens-Ney
none 100% 26.87 29.07 21.55 28.75 28.54 15.50
10 84?88% 26.81 29.00 21.65 28.72 28.52 15.54
15 13?15% 26.92 28.67 21.74 28.79 28.32 15.44
20 11?13% 26.93 28.47 21.72 28.69 28.42 15.45
25 8?10% 26.85 28.79 21.58 28.59 28.27 15.37
50 4?5% 26.51 27.96 20.96 28.30 27.96 15.27
100 2% 25.82 27.34 20.02 27.57 27.30 14.51
1000 0.2% 20.50 21.76 15.46 22.68 22.33 11.56
Table 6: Chinese Results: BLEU by pruning threshold
threshold phrasetable % nist04 nist05 nist06-GALE nist06-NIST
Zens-Ney Smoothing applied to all phrasetables
none 100% 32.14 30.69 13.06 27.97
14 40?65% 32.66 31.14 13.11 28.35
16 22?38% 32.73 30.97 13.14 28.00
18 21?36% 31.56 30.45 12.49 27.03
20 20?35% 32.00 30.73 12.50 27.33
25 18?31% 30.54 29.58 11.68 26.12
also pruning C(s?, t?) = 1
14? 23?39% 32.08 30.99 12.75 27.66
975
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 25?32,
New York, June 2006. c?2006 Association for Computational Linguistics
Segment Choice Models: Feature-Rich Models for Global  
Distortion in Statistical Machine Translation 
 
 
Roland Kuhn, Denis Yuen, Michel Simard, Patrick Paul,  
George Foster, Eric Joanis, and Howard Johnson 
 
Institute for Information Technology, National Research Council of Canada 
Gatineau, Qu?bec, CANADA  
Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, 
Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com  
 
 
 
 
Abstract 
This paper presents a new approach to 
distortion (phrase reordering) in phrase-
based machine translation (MT). Distor-
tion is modeled as a sequence of choices 
during translation. The approach yields 
trainable, probabilistic distortion models 
that are global: they assign a probability 
to each possible phrase reordering. These 
?segment choice? models (SCMs) can be 
trained on ?segment-aligned? sentence 
pairs; they can be applied during decoding 
or rescoring. The approach yields a metric 
called ?distortion perplexity? (?disperp?) 
for comparing SCMs offline on test data, 
analogous to perplexity for language 
models. A decision-tree-based SCM is 
tested on Chinese-to-English translation, 
and outperforms a baseline distortion 
penalty approach at the 99% confidence 
level. 
1 Introduction: Defining SCMs  
The work presented here was done in the context 
of phrase-based MT (Koehn et al, 2003; Och and 
Ney, 2004). Distortion in phrase-based MT occurs 
when the order of phrases in the source-language 
sentence changes during translation, so the order of 
corresponding phrases in the target-language trans-
lation is different. Some MT systems allow arbi-
trary reordering of phrases, but impose a distortion 
penalty proportional to the difference between the 
new and the original phrase order (Koehn, 2004). 
Some interesting recent research focuses on reor-
dering within a narrow window of phrases (Kumar 
and Byrne, 2005; Tillmann and Zhang, 2005; Till-
mann, 2004). The (Tillmann, 2004) paper intro-
duced lexical features for distortion modeling. A 
recent paper (Collins et al, 2005) shows that major 
gains can be obtained by constructing a parse tree 
for the source sentence and then applying hand-
crafted reordering rules to rewrite the source in 
target-language-like word order prior to MT.  
 
Our model assumes that the source sentence is 
completely segmented prior to distortion. This 
simplifying assumption requires generation of hy-
potheses about the segmentation of the complete 
source sentence during decoding. The model also 
assumes that each translation hypothesis grows in a 
predetermined order. E.g., Koehn?s decoder 
(Koehn 2004) builds each new hypothesis by add-
ing phrases to it left-to-right (order is deterministic 
for the target hypothesis). Our model doesn?t re-
quire this order of operation ? it would support 
right-to-left or inwards-outwards hypothesis con-
struction ? but it does require a predictable order. 
 
One can keep track of how segments in the 
source sentence have been rearranged during de-
coding for a given hypothesis, using what we call a 
?distorted source-language hypothesis? (DSH). A 
similar concept appears in (Collins et al, 2005) 
(this paper?s preoccupations strongly resemble 
25
ours, though our method is completely different: 
we don?t parse the source, and use only automati-
cally generated rules). Figure 1 shows an example 
of a DSH for German-to-English translation (case 
information is removed). Here, German ?ich habe 
das buch gelesen .? is translated into English ?i 
have read the book .? The DSH shows the distor-
tion of the German segments into an English-like 
word order that occurred during translation (we 
tend to use the word ?segment? rather than the 
more linguistically-charged  ?phrase?). 
Figure 1. Example of German-to-English DSH 
From the DSH, one can reconstruct the series of 
segment choices. In Figure 1 - given a left-to-right 
decoder - ?[ich]? was chosen from five candidates 
to be the leftmost segment in the DSH. Next, 
?[habe]? was chosen from four remaining candi-
dates, ?[gelesen]? from three candidates, and ?[das 
buch]? from two candidates. Finally, the decoder 
was forced to choose ?[.]?.   
 
Segment Choice Models (SCMs) assign 
probabilities to segment choices made as the DSH 
is constructed. The available choices at a given 
time are called the ?Remaining Segments? (RS). 
Consider a valid (though stupid) SCM that assigns 
equal probabilities to all segments in the RS. This 
uniform SCM assigns a probability of 1/5! to the 
DSH in Figure 1: the probability of choosing 
?[ich]? from among 5 RS was 1/5, then the 
probability of ?[habe]? among 4 RS was  1/4 , etc. 
The uniform SCM would be of little use to an MT 
system. In the next two sections we describe some 
more informative SCMs, define the ?distortion 
perplexity? (?disperp?) metric for comparing 
SCMs offline on a test corpus, and show how to 
construct this corpus.  
2 Disperp and Distortion Corpora 
2.1 Defining Disperp 
The ultimate reason for choosing one SCM over 
another will be the performance of an MT system 
containing it, as measured by a metric like BLEU 
(Papineni et al, 2002). However, training and 
testing a large-scale MT system for each new SCM 
would be costly. Also, the distortion component?s 
effect on the total score is muffled by other 
components (e.g., the phrase translation and target 
language models). Can we devise a quick 
standalone metric for comparing SCMs? 
 
There is an offline metric for statistical language 
models: perplexity (Jelinek, 1990). By analogy, the 
higher the overall probability a given SCM assigns 
to a test corpus of representative distorted sentence 
hypotheses (DSHs), the better the quality of the 
SCM. To define distortion perplexity (?disperp?), 
let PrM(dk) = the probability an SCM M assigns to 
a DSH for sentence k, dk. If T is a test corpus 
comprising numerous DSHs, the probability of the 
corpus according to M is PrM(T) =   k PrM(dk).  
Let S(T) = total number of segments in T. Then 
disperp(M,T) = PrM(T)-1/S(T). This gives the mean 
number of choices model M allows; the lower the 
disperp for corpus T, the better M is as a model for 
T (a model X that predicts segment choice in T 
perfectly would have disperp(X,T) = 1.0).  
2.2 Some Simple A Priori SCMs 
The uniform SCM assigns to the DSH dk that has 
S(dk) segments the probability 1/[S(dk)!] . We call 
this Model A. Let?s define some other illustrative 
SCMs. Fig. 2 shows a sentence that has 7 segments 
with 10 words (numbered 0-9 by original order). 
Three segments in the source have been used; the 
decoder has a choice of four RS. Which of the RS 
has the highest probability of being chosen? Per-
haps [2 3], because it is the leftmost RS: the ?left-
most? predictor. Or, the last phrase in the DSH will 
be followed by the phrase that originally followed 
it, [8 9]: the ?following? predictor. Or, perhaps 
positions in the source and target should be close, 
so since the next DSH position to be filled is 4, 
phrase [4] should be favoured: the ?parallel? pre-
dictor. 
 
 
Figure 2. Segment choice prediction example 
Model B will be based on the ?leftmost? predic-
tor, giving the leftmost segment in the RS twice the 
probability of the other segments, and giving the 
Original German:   [ich] [habe] [das buch] [gelesen]    [.] 
DSH for German:  [ich] [habe]  [gelesen]    [das buch] [.] 
(English:                [i]     [have]   [read]        [the book] [.]) 
original:  [0 1] [2 3] [4] [5] [6] [7] [8 9] 
DSH:  [0 1] [5] [7],   RS:  [2 3], [4], [6], [8 9] 
26
others uniform probabilities. Model C will be 
based on the ?following? predictor, doubling the 
probability for the segment in the RS whose first 
word was the closest to the last word in the DSH, 
and otherwise assigning uniform probabilities. Fi-
nally, Model D combines ?leftmost? and ?follow-
ing?: where the leftmost and following segments 
are different, both are assigned double the uniform 
probability; if they are the same segment, that 
segment has four times the uniform probability. Of 
course, the factor of 2.0 in these models is arbi-
trary. For Figure 2, probabilities would be: 
? Model A: PrA([2 3])= PrA([4])= PrA([6])= 
PrA([8 9]) = 1/4; 
? Model B: PrB ([2 3])= 2/5, PrB([4])= 
PrB([6])= PrB([8 9]) = 1/5; 
? Model C: PrC ([2 3])= PrC ([4])= PrC([6]) 
= 1/5, PrC([8 9]) = 2/5; 
? Model D: PrD ([2 3]) = PrD([8 9]) = 1/3, 
PrD([4])= PrD([6]) = 1/6.  
 
Finally, let?s define an SCM derived from the 
distortion penalty used by systems based on the 
?following? predictor, as in (Koehn, 2004). Let ai = 
start position of source phrase translated into ith 
target phrase, bi -1= end position of source phrase 
that?s translated into (i-1)th target phrase. Then 
distortion penalty d(ai, bi-1) =   ?ai? bi-1 -1?; the total 
distortion is the product of the phrase distortion 
penalties. This penalty is applied as a kind of non-
normalized probability in the decoder. The value of 
   for given (source, target) languages is optimized 
on development data. 
To turn this penalty into an SCM, penalties are 
normalized into probabilities, at each decoding 
stage; we call the result Model P (for ?penalty?). 
Model P with    = 1.0 is the same as uniform 
Model A. In disperp experiments, Model P with    
optimized on held-out data performs better than 
Models A-D (see Figure 5), suggesting that dis-
perp is a realistic measure.  
Models A-D are models whose parameters were 
all defined a priori; Model P has one trainable pa-
rameter,  . Next, let?s explore distortion models 
with several trainable parameters.  
2.3 Constructing a Distortion Corpus 
To compare SCMs using disperp and to train 
complex SCMs, we need a corpus of representative 
examples of DSHs. There are several ways of ob-
taining such a corpus. For the experiments de-
scribed here, the MT system was first trained on a 
bilingual sentence-aligned corpus. Then, the sys-
tem was run in a second pass over its own training 
corpus, using its phrase table with the standard dis-
tortion penalty to obtain a best-fit phrase alignment 
between each (source, target) sentence pair. Each 
such alignment yields a DSH whose segments are 
aligned with their original positions in the source; 
we call such a source-DSH alignment a ?segment 
alignment?. We now use a leave-one-out procedure 
to ensure that information derived from a given 
sentence pair is not used to segment-align that sen-
tence pair. In our initial experiments we didn?t do 
this, with the result that the segment-aligned cor-
pus underrepresented the case where words or N-
grams not in the phrase table are seen in the source 
sentence during decoding.  
3 A Trainable Decision Tree SCM 
Almost any machine learning technique could be 
used to create a trainable SCM. We implemented 
one based on decision trees (DTs), not because 
DTs necessarily yield the best results but for soft-
ware engineering reasons: DTs are a quick way to 
explore a variety of features, and are easily inter-
preted when grown (so that examining them can 
suggest further features). We grew N DTs, each 
defined by the number of choices available at a 
given moment. The highest-numbered DT has a 
?+? to show it handles N+1 or more choices. E.g., 
if we set N=4, we grow a ?2-choice?, a ?3-choice?, 
a ?4-choice?, and a ?5+-choice tree?. The 2-choice 
tree handles cases where there are 2 segments in 
the RS, assigning a probability to each; the 3-
choice tree handles cases where there are 3 seg-
ments in the RS, etc. The 5+-choice tree is differ-
ent from the others: it handles cases where there 
are 5 segments in the RS to choose from, and 
cases where there are more than 5. The value of N 
is arbitrary; e.g., for N=8, the trees go from ?2-
choice? up to ?9+-choice?.  
Suppose a left-to-right decoder with an N=4 
SCM is translating a sentence with seven phrases. 
Initially, when the DSH is empty, the 5+-choice 
tree assigns probabilities to each of these seven. It 
27
will use the 5+-choice tree twice more, to assign 
probabilities to six RS, then to five. To extend the 
hypothesis, it will then use the 4-choice tree, the 3-
choice tree, and finally the 2-choice tree. Disperps 
for this SCM are calculated on test corpus DSHs in 
the same left-to-right way, using the tree for the 
number of choices in the RS to find the probability 
of each segment choice. 
Segments need labels, so the N-choice DT can 
assign probabilities to the N segments in the RS. 
We currently use a ?following? labeling scheme. 
Let X be the original source position of the last 
word put into the DSH, plus 1. In Figure 2, this 
was word 7, so X=8. In our scheme, the RS seg-
ment whose first word is closest to X is labeled 
?A?; the second-closest segment is labeled ?B?, 
etc. Thus, segments are labeled in order of the 
(Koehn, 2004) penalty; the ?A? segment gets the 
lowest penalty. Ties between segments on the right 
and the left of X are broken by first labeling the 
right segment. In Figure 2, the labels for the RS 
are ?A? = [8 9], ?B? = [6], ?C? = [4], ?D? = [2 3].  
 
 
 
 
 
 
 
 
Figure 3. Some question types for choice DTs 
Figure 3 shows the main types of questions used 
for tree-growing, comprising position questions 
and word-based questions. Position questions 
pertain to location, length, and ordering of seg-
ments. Some position questions ask about the dis-
tance between the first word of a segment and the 
?following? position X: e.g., if the answer to 
?pos(A)-pos(X)=0?? is yes, then segment A comes 
immediately after the last DSH segment in the 
source, and is thus highly likely to be chosen. 
There are also questions relating to the ?leftmost? 
and ?parallel? predictors (above, sec. 2.2). The 
fseg() and bseg() functions count segments in the 
RS from left to right and right to left respectively, 
allowing, e.g., the question whether a given seg-
ment is the second last segment in the RS. The 
only word-based questions currently implemented 
ask whether a given word is contained in a given 
segment (or anywhere in the DSH, or anywhere in 
the RS). This type could be made richer by allow-
ing questions about the position of a given word in 
a given segment, questions about syntax, etc.  
Figure 4 shows an example of a 5+-choice DT. 
The ?+? in its name indicates that it will handle 
cases where there are 5 or more segments in the 
RS. The counts stored in the leaves of this DT rep-
resent the number of training data items that ended 
up there; the counts are used to estimate probabili-
ties. Some smoothing will be done to avoid zero 
probabilities, e.g., for class C in node 3.  
 
Figure 4. Example of a 5+-choice tree 
For ?+? DTs, the label closest to the end of the 
alphabet (?E? in Figure 4) stands for a class that 
can include more than one segment. E.g., if this 
5+-choice DT is used to estimate probabilities for a 
7-segment RS, the segment closest to X is labeled 
?A?, the second closest ?B?, the third closest ?C?, 
and the fourth closest ?D?. That leaves 3 segments, 
all labeled ?E?. The DT shown yields probability 
Pr(E) that one of these three will be chosen. Cur-
rently, we apply a uniform distribution within this 
?furthest from X? class, so the probability of any 
one of the three ?E? segments is estimated as 
Pr(E)/3.  
To train the DTs, we generate data items from 
the second-pass DSH corpus. Each DSH generates 
several data items. E.g., moving across a seven-
segment DSH from left to right, there is an exam-
ple of the seven-choice case, then one of the six-
choice case, etc. Thus, this DSH provides three 
items for training the 5+-choice DT and one item 
     pos(A)-pos(X)<0? 
A:27 B:23 C:20 D:11 E:19  
        today    DSH? 
A:10 B:8 C:10 D:6 E:5 
A:8 B:6 C:0 D:2 E:4 A:2 B:2 C:10 D:4 E:1 
A:17 B:15 C:10 D:5 E:14 
yes no 
yes no 
1. 
3. 
2. 5. 
4. 
1. Position Questions 
Segment Length Questions 
E.g., ?lgth(DSH)<5??, ?lgth(B)=2??, ?lgth(RS)<6??, etc.  
Questions about Original Position 
Let pos(seg) = index of seg?s first word in source sentence 
E.g., ?pos(A)=9??, ?pos(C) <17??, etc.  
Questions With X (?following? word position)  
E.g., ?pos(X)=9??, ?pos(C) ? pos(X) <0??, etc.  
Segment Order Questions  
Let fseg = segment # (forward), bseg = segment # (back-
ward) 
E.g., ?fseg(D) = 1??, ?bseg(A) <5??, etc.  
2. Word-Based Questions  
E.g., ?and   DSH??, ?November   B??, etc.  
28
each for training the 4-choice, 3-choice, and 2-
choice DTs. The DT training method was based on 
Gelfand-Ravishankar-Delp expansion-pruning 
(Gelfand et al, 1991), for DTs whose nodes con-
tain probability distributions (Lazarid?s et al, 
1996).  
4 Disperp Experiments 
We carried out SCM disperp experiments for the 
English-Chinese task, in both directions. That is, 
we trained and tested models both for the distortion 
of English into Chinese-like phrase order, and the 
distortion of Chinese into English-like phrase or-
der. For reasons of space, details about the ?dis-
torted English? experiments won?t be given here. 
Training and development data for the distorted 
Chinese experiments were taken from the NIST 
2005 release of the FBIS corpus of Xinhua news 
stories. The training corpus comprised 62,000 
FBIS segment alignments, and the development 
?dev? corpus comprised a disjoint set of 2,306 
segment alignments from the same FBIS corpus. 
All disperp results are obtained by testing on ?dev? 
corpus. 
 
Distorted Chinese: Models A-D, P, & a four-DT 
Model
1
2
3
4
5
6
7
8
50
0
10
00
20
00
40
00
80
00
16
00
0
32
00
0
62
00
0
# training alignments (log scale)
Di
sp
er
p 
o
n
 
"
de
v
"
 
Model A
Model B
Model C
Model D
Model P (alpha =
0.77)
Four DTs: pos +
100-wd qns
 
Figure 5. Several SCMs for distorted Chinese 
Figure 5 shows disperp results for the models 
described earlier. The y axis begins at 1.0 (mini-
mum value of disperp). The x axis shows number 
of alignments (DSHs) used to train DTs, on a log 
scale. Models A-D are fixed in advance; Model P?s 
single parameter    was optimized once on the en-
tire training set of 62K FBIS alignments (to 0.77) 
rather than separately for each amount of training 
data. Model P, the normalized version of  Koehn?s 
distortion penalty, is superior to Models A-D, and 
the DT-based SCM is superior to Model P.  
The Figure 5 DT-based SCM had four trees (2-
choice, 3-choice, 4-choice, and 5+-choice) with 
position-based and word-based questions. The 
word-based questions involved only the 100 most 
frequent Chinese words in the training corpus. The 
system?s disperp drops from 3.1 to 2.8 as the num-
ber of alignments goes from 500 to 62K. 
Figure 6 examines the effect of allowing word-
based questions. These questions provide a signifi-
cant disperp improvement, which grows with the 
amount of training data. 
Distorted Chinese: effect of allowing word qns 
(four- DT models)
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
50
0
10
00
20
00
40
00
80
00
16
00
0
32
00
0
62
00
0
# training alignments (log scale) 
Di
sp
er
p 
o
n
 
"
de
v
"
 
Four DTs: pos qns
only
Four DTs: pos +
100-wd qns
 
Figure 6. Do word-based questions help? 
In the ?four-DT? results above, examples with 
five or more segments are handled by the same 
?5+-choice? tree. Increasing the number of trees 
allows finer modeling of multi-segment cases 
while spreading the training data more thinly. 
Thus, the optimal number of trees depends on the 
amount of training data. Fixing this amount to 32K 
alignments, we varied the number of trees. Figure 
7 shows that this parameter has a significant im-
pact on disperp, and that questions based on the 
most frequent 100 Chinese words help perform-
ance for any number of trees.  
29
Distorted Chinese: Disperp vs. # of trees (all 
trees grown on 32K alignments)
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3 4 5 6 7 8 9 10 11 12 13 14
# of trees
Di
sp
er
p 
o
n
 
"
de
v"
 
pos qns only
pos + 100-wd qns
 
Figure 7. Varying the number of DTs  
In Figure 8 the number of the most frequent 
Chinese words for questions is varied (for a 13-DT 
system trained on 32K alignments). Most of the 
improvement came from the 8 most frequent 
words, especially from the most frequent, the 
comma ?,?. This behaviour seems to be specific to 
Chinese. In our ?distorted English? experiments, 
questions about the 8 most frequent words also 
gave a significant improvement, but each of the 8 
words had a fairly equal share in the improvement. 
Distorted Chinese: Disperp vs. #words (all trees 
grown on 32K alignments)
2.58
2.6
2.62
2.64
2.66
2.68
2.7
2.72
0 2 8 32 12
8
51
2
# words tried for qns (log scale)
Di
sp
er
p 
o
n
 
"
de
v
"
Performance of 13-
DT system
 
Figure 8. Varying #words (13-DT system) 
Finally, we grew the DT system used for the MT 
experiments: one with 13 trees and questions about 
the 25 most frequent Chinese words, grown on 
88K alignments. Its disperp on the ?dev? used for 
the MT experiments (a different ?dev? from the 
one above ? see Sec. 5.2) was 2.42 vs. 3.48 for the 
baseline Model P system: a 30% drop.  
5 Machine Translation Experiments 
5.1 SCMs for Decoding 
SCMs assume that the source sentence is fully 
segmented throughout decoding. Thus, the system 
must guess the segmentation for the unconsumed 
part of the source (?remaining source?: RS). For 
the results below, we used a simple heuristic: RS is 
broken into one-word segments. In future, we will 
apply a more realistic segmentation model to RS 
(or modify DT training to reflect accurately RS 
treatment during decoding).  
5.2 Chinese-to-English MT Experiments  
The training corpus for the MT system?s phrase 
tables consists of all parallel text available for the 
NIST MT05 Chinese-English evaluation, except 
the Xinhua corpora and part 3 of LDC's ?Multiple-
Translation Chinese Corpus? (MTCCp3). The Eng-
lish language model was trained on the same cor-
pora, plus 250M words from Gigaword. The DT-
based SCM was trained and tuned on a subset of 
this same training corpus (above). The dev corpus 
for optimizing component weights is MTCCp3. 
The experimental results below were obtained by 
testing on the evaluation set for MTeval NIST04.  
Phrase tables were learned from the training cor-
pus using the ?diag-and? method (Koehn et al, 
2003), and using IBM model 2 to produce initial 
word alignments (these authors found this worked 
as well as IBM4). Phrase probabilities were based 
on unsmoothed relative frequencies. The model 
used by the decoder was a log-linear combination 
of a phrase translation model (only in the 
P(source|target) direction), trigram language 
model, word penalty (lexical weighting), an op-
tional segmentation model (in the form of a phrase 
penalty) and distortion model. Weights on the 
components were assigned using the (Och, 2003) 
method for max-BLEU training on the develop-
ment set. The decoder uses a dynamic-
programming beam-search, like the one in (Koehn, 
2004). Future-cost estimates for all distortion mod-
els are assigned using the baseline penalty model. 
5.3 Decoding Results 
30
29,40
29,60
29,80
30,00
30,20
30,40
30,60
30,80
31,00
31,20
no PP PP no PP PP
DP DT
BL
EU
 
sc
o
re
1x beam
4x beam
 
Figure 9. BLEU on NIST04 (95% conf. = ?0.7) 
Figure 9 shows experimental results. The ?DP? 
systems use the distortion penalty in (Koehn, 2004) 
with    optimized on ?dev?, while ?DT? systems 
use the DT-based SCM. ?1x? is the default beam 
width, while ?4x? is a wider beam (our notation 
reflects decoding time, so ?4x? takes four times as 
long as ?1x?). ?PP? denotes presence of the phrase 
penalty component. The advantage of DTs as 
measured by difference between the score of the 
best DT system and the best DP system is 0.75 
BLEU at 1x and 0.5 BLEU at 4x. With a 95% 
bootstrap confidence interval of ?0.7 BLEU (based 
on 1000-fold resampling), the resolution of these 
results is too coarse to draw firm conclusions. 
Thus, we carried out another 1000-fold bootstrap 
resampling test on NIST04, this time for pairwise 
system comparison. Table 1 shows results for 
BLEU comparisons between the systems with the 
default (1x) beam. The entries show how often the 
A system (columns) had a better score than the B 
system (rows), in 1000 observations.  
   A    
vs. B   
DP,  
no PP 
DP, PP DT,  
no PP 
DT, PP 
DP,  
no PP 
x 2.95% 99.45% 99.55% 
DP, PP 97.05% x 99.95% 99.95% 
DT,  
no PP 
0.55% 0.05% x 65.68% 
DT, PP 0.45% 0.05% 34.32% x 
Table 1. Pairwise comparison for 1x systems 
The table shows that both DT-based 1x systems 
performed better than either of the DP systems 
more than 99% of the time (underlined results). 
Though not shown in the table, the same was true 
with 4x beam search. The DT 1x system with a 
phrase penalty had a higher score than the DT 1x 
system without one about 66% of the time. 
6 Summary and Discussion 
In this paper, we presented a new class of probabil-
istic model for distortion, based on the choices 
made during translation. Unlike some recent dis-
tortion models (Kumar and Byrne, 2005; Tillmann 
and Zhang, 2005; Tillmann, 2004) these Segment 
Choice Models (SCMs) allow phrases to be moved 
globally, between any positions in the sentence. 
They also lend themselves to quick offline com-
parison by means of a new metric called disperp. 
We developed a decision-tree (DT) based SCM 
whose parameters were optimized on a ?dev? cor-
pus via disperp. Two variants of the DT system 
were experimentally compared with two systems 
with a distortion penalty on a Chinese-to-English 
task. In pairwise bootstrap comparisons, the sys-
tems with DT-based distortion outperformed the 
penalty-based systems more than 99% of the time. 
The computational cost of training the DTs on 
large quantities of data is comparable to that of 
training phrase tables on the same data - large but 
manageable ? and increases linearly with the 
amount of training data. However, currently there 
is a major problem with DT training: the low pro-
portion of Chinese-English sentence pairs that can 
be fully segment-aligned and thus be used for DT 
training (about 27%). This may result in selection 
bias that impairs performance. We plan to imple-
ment an alignment algorithm with smoothed phrase 
tables (Johnson et al 2006) to achieve segment 
alignment on 100% of the training data. 
Decoding time with the DT-based distortion 
model is roughly proportional to the square of the 
number of tokens in the source sentence. Thus, 
long sentences pose a challenge, particularly dur-
ing the weight optimization step. In experiments on 
other language pairs reported elsewhere (Johnson 
et al 2006), we applied a heuristic: DT training 
and decoding involved source sentences with 60 or 
fewer tokens, while longer sentences were handled 
with the distortion penalty. A more principled ap-
31
proach would be to divide long source sentences 
into chunks not exceeding 60 or so tokens, within 
each of which reordering is allowed, but which 
cannot themselves be reordered.  
The experiments above used a segmentation 
model that was a count of the number of source 
segments (sometimes called ?phrase penalty?), but 
we are currently exploring more sophisticated 
models. Once we have found the best segmentation 
model, we will improve the system?s current na?ve 
single-word segmentation of the remaining source 
sentence during decoding, and construct a more 
accurate future cost function for beam search. An-
other obvious system improvement would be to 
incorporate more advanced word-based features in 
the DTs, such as questions about word classes 
(Tillmann and Zhang 2005, Tillmann 2004).  
We also plan to apply SCMs to rescoring N-best 
lists from the decoder. For rescoring, one could 
apply several SCMs, some with assumptions dif-
fering from those of the decoder. E.g., one could 
apply right-to-left SCMs, or ?distorted target? 
SCMs which assume a target hypothesis generated 
the source sentence, instead of vice versa.  
Finally, we are contemplating an entirely differ-
ent approach to DT-based SCMs for decoding. In 
this approach, only one DT would be used, with 
only two output classes that could be called ?C? 
and ?N?. The input to such a tree would be a par-
ticular segment in the remaining source sentence, 
with contextual information (e.g., the sequence of 
segments already chosen). The DT would estimate 
the probability Pr(C) that the specified segment is 
?chosen? and the probability Pr(N) that it is ?not 
chosen?. This would eliminate the need to guess 
the segmentation of the remaining source sentence.  
References  
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. ?The Mathematics of Statistical Machine 
Translation: Parameter Estimation?. Computational 
Linguistics, 19(2), pp. 263-311.  
 
M. Collins, P. Koehn, and I. Ku   erov?. 2005. ?Clause 
Restructuring for Statistical Machine Translation?. 
Proc. ACL, Ann Arbor, USA, pp. 531-540. 
 
S. Gelfand, C. Ravishankar, and E. Delp. 1991. ?An 
Iterative Growing and Pruning Algorithm for Clas-
sification Tree Design?. IEEE Trans. Patt. Analy. 
Mach. Int. (IEEE PAMI), V. 13, no. 2, pp. 163-174.  
 
F. Jelinek. 1990. ?Self-Organized Language Modeling 
for Speech Recognition? in Readings in Speech 
Recognition (ed. A. Waibel and K. Lee, publ. Mor-
gan Kaufmann), pp. 450-506.  
 
H. Johnson, F. Sadat, G. Foster, R. Kuhn, M. Simard, E. 
Joanis, and S. Larkin. 2006. ?PORTAGE: with 
Smoothed Phrase Tables and Segment Choice Mod-
els?.  Submitted to NAACL 2006 Workshop on Statis-
tical Machine Translation, New York City. 
P. Koehn. 2004. ?Pharaoh: a Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Mod-
els?. Assoc. Machine Trans. Americas (AMTA04). 
 
P. Koehn, F.-J. Och and D. Marcu. 2003. ?Statistical 
Phrase-Based Translation?. Proc. Human Lang. 
Tech. Conf. N. Am. Chapt. Assoc. Comp. Ling. 
(NAACL03), pp. 127-133.  
 
S. Kumar and W. Byrne. 2005. ?Local Phrase Reorder-
ing Models for Statistical Machine Translation?. 
HLT/EMNLP, pp. 161-168, Vancouver, Canada.  
 
A. Lazarid?s, Y. Normandin, and R. Kuhn. 1996. ?Im-
proving Decision Trees for Acoustic Modeling?. 
Int. Conf. Spoken Lang. Proc. (ICSLP96), V. 2, pp. 
1053-1056, Philadelphia, Pennsylvania, USA. 
 
F. Och and H. Ney. 2004. ?The Alignment Template 
Approach to Statistical Machine Translation?. 
Comp. Linguistics, V. 30, Issue 4, pp. 417-449.  
 
Franz Josef Och. 2003. ?Minimum Error Rate Training 
for Statistical Machine  Translation?. Proc. ACL, 
Sapporo, Japan. 
 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
?BLEU: A method for automatic evaluation of ma-
chine translation?. Proc. ACL, pp. 311-318. 
 
C. Tillmann and T. Zhang. 2005. ?A Localized Predic-
tion Model for Statistical Machine Translation?. 
Proc. ACL.  
 
C. Tillmann. 2004. ?A Block Orientation Model for 
Statistical Machine Translation?. HLT/NAACL. 
 
S. Vogel, H. Ney, and C. Tillmann. 1996. ?HMM-Based 
Word Alignment in Statistical Translation?. 
COLING, pp. 836-841. 
32
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129?132,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
PORTAGE: A Phrase-based Machine Translation System 
 
Fatiha Sadat+, Howard Johnson++, Akakpo Agbago+, George Foster+,               
Roland Kuhn+, Joel Martin++ and Aaron Tikuisis?
 
+ NRC Institute for Information 
Technology 
101 St-Jean-Bosco Street  
Gatineau, QC K1A 0R6, Canada 
++ NRC Institute for Information 
Technology 
1200 Montreal Road  
Ottawa, ON K1A 0R6, Canada 
?University of Waterloo 
200 University Avenue W., 
Waterloo, Ontario, Canada 
 
firstname.lastname@cnrc-nrc.gc.ca aptikuis@uwaterloo.ca  
 
Abstract 
This paper describes the participation of 
the Portage team at NRC Canada in the 
shared task1 of ACL 2005 Workshop on 
Building and Using Parallel Texts. We dis-
cuss Portage, a statistical phrase-based 
machine translation system, and present 
experimental results on the four language 
pairs of the shared task. First, we focus on 
the French-English task using multiple re-
sources and techniques. Then we describe 
our contribution on the Finnish-English, 
Spanish-English and German-English lan-
guage pairs using the provided data for the 
shared task.  
1 Introduction 
The rapid growth of the Internet has led to a rapid 
growth in the need for information exchange among 
different languages. Machine Translation (MT) and 
related technologies have become essential to the 
information flow between speakers of different lan-
guages on the Internet. Statistical Machine Transla-
tion (SMT), a data-driven approach to producing 
translation systems, is becoming a practical solution 
to the longstanding goal of cheap natural language 
processing.  
In this paper, we describe Portage, a statistical 
phrase-based machine translation system, which we 
evaluated on all different language pairs that were 
provided for the shared task.  As Portage is a very 
                                                           
1 http://www.statmt.org/wpt05/mt-shared-task/ 
new system, our main goal in participating in the 
workshop was to test it out on different language 
pairs, and to establish baseline performance for the 
purpose of comparison against other systems and 
against future improvements.  To do this, we used a 
fairly standard configuration for phrase-based SMT, 
described in the next section. 
Of the language pairs in the shared task, French-
English is particularly interesting to us in light of 
Canada?s demographics and policy of official bilin-
gualism. We therefore divided our participation into 
two parts: one stream for French-English and an-
other for Finnish-, German-, and Spanish-English. 
For the French-English stream, we tested the use of 
additional data resources along with hand-coded 
rules for translating numbers and dates. For the 
other streams, we used only the provided resources 
in a purely statistical framework (although we also 
investigated several automatic methods of coping 
with Finnish morphology). 
The remainder of the paper is organized as fol-
lows. Section 2 describes the architecture of the 
Portage system, including its hand-coded rules for 
French-English.  Experimental results for the four 
pairs of languages are reported in Section 3. Section 
4 concludes and gives pointers to future work. 
2 Portage  
Portage operates in three main phases: preprocess-
ing of raw data into tokens, with translation sugges-
tions for some words or phrases generated by rules; 
decoding to produce one or more translation hy-
potheses; and error-driven rescoring to choose the 
best final hypothesis. (A fourth postprocessing 
phase was not needed for the shared task.) 
129
2.1 Preprocessing 
Preprocessing is a necessary first step in order to 
convert raw texts in both source and target lan-
guages into a format suitable for both model train-
ing and decoding (Foster et al, 2003).  For the 
supplied Europarl corpora, we relied on the existing 
segmentation and tokenization, except for French, 
which we manipulated slightly to bring into line 
with our existing conventions (e.g., converting l ? 
an  into l? an).  For the Hansard corpus used to 
supplement our French-English resources (de-
scribed in section 3 below), we used our own 
alignment based on Moore?s algorithm (Moore, 
2002), segmentation, and tokenization procedures. 
Languages with rich morphology are often prob-
lematic for statistical machine translation because 
the available data lacks instances of all possible 
forms of a word to efficiently train a translation sys-
tem. In a language like German, new words can be 
formed by compounding (writing two or more 
words together without a space or a hyphen in be-
tween). Segmentation is a crucial step in preproc-
essing languages such as German and Finnish texts.
In addition to these simple operations, we also 
developed a rule-based component to detect num-
bers and dates in the source text and identify their 
translation in the target text. This component was 
developed on the Hansard corpus, and applied to the 
French-English texts (i.e. Europarl and Hansard), on 
the development data in both languages, and on the 
test data. 
2.2 Decoding 
Decoding is the central phase in SMT, involving a 
search for the hypotheses t that have highest prob-
abilities of being translations of the current source 
sentence s according to a model for P(t|s). Our 
model for P(t|s) is a log-linear combination of four 
main components: one or more trigram language 
models, one or more phrase translation models, a 
distortion model, and a word-length feature. The 
trigram language model is implemented in the 
SRILM toolkit (Stolcke, 2002). The phrase-based 
translation model is similar to the one described in 
(Koehn, 2004), and relies on symmetrized IBM 
model 2 word-alignments for phrase pair induction. 
The distortion model is also very similar to 
Koehn?s, with the exception of a final cost to ac-
count for sentence endings.  
s
To set weights on the components of the log-
linear model, we implemented Och?s algorithm 
(Och, 2003).  This essentially involves generating, 
in an iterative process, a set of nbest translation hy-
potheses that are representative of the entire search 
space for a given set of source sentences. Once this 
is accomplished, a variant of Powell?s algorithm is 
used to find weights that optimize BLEU score 
(Papineni et al 2002) over these hypotheses, com-
pared to reference translations. Unfortunately, our 
implementation of this algorithm converged only 
very slowly to a satisfactory final nbest list, so we 
used two different ad hoc strategies for setting 
weights: choosing the best values encountered dur-
ing
, with the exception of a 
ch as the ability to decode either 
w ards.  
 transla-
 
rent language pairs of the 
sha d t
hared t
- 
 the iterations of Och?s algorithm (French-
English), and a grid search (all other languages).  
To perform the actual translation, we used our 
decoder, Canoe, which implements a dynamic-
programming beam search algorithm based on that 
of Pharaoh (Koehn, 2004). Canoe is input-output 
compatible with Pharaoh
few extensions su
back ards or forw
2.3 Rescoring 
To improve raw output from Canoe, we used a 
rescoring strategy: have Canoe generate a list of 
nbest translations rather than just one, then reorder 
the list using a model trained with Och?s method to 
optimize BLEU score. This is identical to the final 
pass of the algorithm described in the previous sec-
tion, except for the use of a more powerful log-
linear model than would have been feasible to use 
inside the decoder. In addition to the four basic fea-
tures of the initial model, our rescoring model in-
cluded IBM2 model probabilities in both directions 
(i.e., P(s|t) and P(t|s)); and an IBM1-based feature 
designed to detect whether any words in one lan-
guage seemed to be left without satisfactory
tions in the other language. This missing-word
feature was also applied in both directions. 
3 Experiments on the Shared Task 
We conducted experiments and evaluations on 
Portage using the diffe
re ask. The training data was provided for the 
ask as follows:  
Training data of 688,031 sentences in 
French and English. A similarly sized cor-
130
pus is provided for Finnish, Spanish and 
German with matched English translations. 
orpus was used to generate both 
lan
e translations into English, was 
 
 Portage for a comparative study ex-
ploiting and combining different resources and 
tec
 
3. arl corpus 
4. 
rd corpora as training data and 
 
t  mod est  
p icipation at th h-English tas 9.53. 
od D  Decoding+Rescoring
- Development test data of 2,000 sentences in 
the four languages.  
In addition to the provided data, a set of 
6,056,014 sentences extracted from Hansard corpus, 
the official record of Canada?s parliamentary de-
bates, was used in both French and English lan-
guages. This c
guage and translation models for use in decoding 
and rescoring. 
The development test data was split into two 
parts: The first part that includes 1,000 sentences in 
each language with reference translations into Eng-
lish served in the optimization of weights for both 
the decoding and rescoring models. In this study, 
number of n-best lists was set to 1,000. The second 
part, which includes 1,000 sentences in each lan-
guage with referenc
used in the evaluation of the performance of the
translation models. 
3.1 Experiments on the French-English Task 
Our goal for this language pair was to conduct ex-
periments on
hniques:  
1. Method E is based on the Europarl corpus 
as training data, 
2. Method E-H is based on both Europarl and 
Hansard corpora as training data, 
Method E-p is based on the Europ
as training data and parsing numbers and 
dates in the preprocessing phase, 
Method E-H-p is based on both Europarl 
and Hansa
parsing numbers and date in the preprocess-
ing phase. 
Results are shown in Table 1 for the French-
English task. The first column of Table 1 indicates 
the method, the second column gives results for 
decoding with Canoe only, and the third column for 
decoding and rescoring with Canoe. For comparison 
between the four methods, there was an improve-
ment in terms of BLEU scores when using two lan-
guage models and two translation models generated 
from Europarl and Hansard corpora; however, pars-
ing numbers and dates had a negative impact on the
ranslation els. The b  BLEU score for our
art e Frenc k was 2
Meth ecoding
E 27.71 29.22 
E-H 28.71 29.53 
E-p 26.45 28.21 
E-H-p 28.29 28.56 
Ta
ed 
f 
of increased trade within North 
merica but also functions as a good counterpoint 
for French-English. 
 
ble 1. BLEU scores for the French-English test 
sentences  
 
A noteworthy feature of these results is that the 
improvement given by the out-of-domain Hansard 
corpus was very slight. Although we suspect that 
somewhat better performance could have been 
achieved by better weight optimization, this result 
clearly underscores the importance of matching 
training and test domains. A related point is that our 
number and date translation rules actually caused a 
performance drop due to the fact that they were op-
timized for typographical conventions prevalent in 
Hansard, which are quite different from those used 
in Europarl. 
Our best result ranked third in the shared 
WPT05 French-English task , with a difference of 
0.74 in terms of BLEU score from the first rank
participant, and a difference of 0.67 in terms o
BLEU score from the second ranked participant. 
3.2 Experiments on other Pairs of Languages 
The WPT05 workshop provides a good opportunity 
to achieve our benchmarking goals with corpora 
that provide challenging difficulties. German and 
Finnish are languages that make considerable use of 
compounding. Finnish, in addition, has a particu-
larly complex morphology that is organized on 
principles that are quite different from any in Eng-
lish. This results in much longer word forms each of 
which occurs very infrequently. 
Our original intent was to propose a number of pos-
sible statistical approaches to analyzing and split-
ting these word forms and improving our results. 
Since none of these yielded results as good as the 
baseline, we will continue this work until we under-
stand what is really needed. We also care very 
much about translating between French and English 
in Canada and plan to spend a lot of extra effort on 
difficulties that occur in this case. Translation be-
tween Spanish and English is also becoming more 
mportant as a result i
A
131
Language Pair Decoding+Rescoring
Finnish-English 20.95 
German-English 23.21 
Spanish English 29.08 
Ta
and 1.56 in 
m ores, respectively, compared to 
the first ranked participant.   
l 
ation, greater use of morphological 
R
Fr
Meeting of the Association for Computational 
Fr
Statistical Machine Transla-
Ge
id 
Ke
e 
Ki
al Meeting of the Association for Com-
M
ne Trans-
Oc
 of the 40th Annual Meet-
Fr
 Proceedings of 
Ph parl: A multilingual corpusfor 
 P
ation 
Models. In Proceedings of the Association for Ma-
chine Translation in the Americas AMTA 2004. 
ble 2 BLEU scores for the Finnish-English, Ger-
man-English and Spanish-English test sentences  
 
To establish our baseline, the only preprocessing 
we did was lowercasing (using the provided tokeni-
zation). Canoe was run without any special settings, 
although weights for distortion, word penalty, lan-
guage model, and translation model were optimized 
using a grid search, as described above. Rescoring 
was also done, and usually resulted in at least an 
extra BLEU point.  
Our final results are shown in Table 2. Ranks at 
the shared WPT05 Finnish-, German-, and Spanish-
English tasks were assigned as second, third and 
fourth, with differences of 1.06, 1.87 
ter s of BLEU sc
4 Conclusion 
We have reported on our participation in the shared 
task of the ACL 2005 Workshop on Building and 
Using Parallel Texts, conducting evaluations of 
Portage, our statistical machine translation system, 
on all four language pairs. Our best BLEU scores 
for the French-, Finnish-, German-, and Spanish-
English at this stage were 29.5, 20.95, 23.21 and 
29.08, respectively. In total, eleven teams took part 
at the shared task and most of them submitted re-
sults for all pairs of languages.  Our results distin-
guished the NRC team at the third, second, third 
and fourth ranks with slight differences with the 
first ranked participants. 
A major goal of this work was to evaluate Port-
age at its first stage of implementation on different 
pairs of languages. This evaluation has served to 
identify some problems with our system in the areas 
of weight optimization and number and date rules. 
It has also indicated the limits of using out-of-
domain corpora, and the difficulty of morphologi-
cally complex languages like Finnish. 
Current and planned future work includes the 
exploitation of comparable corpora for statistica
machine transl
knowledge, and better features for nbest rescoring. 
eferences 
Andreas Stolcke. 2002. SRILM - an Extensible Language 
Modeling Toolkit. In ICSLP-2002, 901-904. 
anz Josef Och, Hermann Ney. 2000. Improved Statisti-
cal Alignment Models. In Proceedings of the 38th An-
nual 
Linguistics, Hong Kong, China, October 2000, 440-
447. 
anz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, Dragomir Radev. 2004. A Smor-
gasbord of Features for 
tion. In Proceeding of the HLT/NAACL 2004, 
Boston, MA, May 2004. 
orge Foster, Simona Gandrabur, Philippe Langlais, 
Pierre Plamondon, Graham Russell and Michel Si-
mard. 2003. Statistical Machine Translation: Rap
Development with Limited Resources. In Proceedings 
of MT Summit IX 2003, New Orleans, September.  
vin Knight, Ishwar Chander, Matthew Haines, Va-
sileios Hatzivassiloglou, Eduard Hovy, Masayo Iida, 
Steve K. Luk, Richard Whitney, and Kenji Yamada. 
1995. Filling Knowledge Gaps in a Broad-Coverag
MT System. In Proceedings of the International Joint 
Conference on Artificial Intelligence (IJCAI), 1995. 
shore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings of 
the 40th Annu
putational Linguistics ACL, Philadelphia, July 2002, 
pp. 311-318. 
oore, Robert. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Machine Transla-
tion: From Research to Real Users (Proceedings of the 
5th Conference of the Association for Machi
lation in the Americas, Tiburon, California), Springer-
Verlag, Heidelberg, Germany, pp. 135-244. 
h, F. J. and H. Ney. 2002. Discriminative Training 
and Maximum Entropy Models for Statistical Machine 
Translation. In Proceedings
ing of the Association for Computational Linguistics, 
Philadelphia, pp. 295?302. 
anz Josef Och, 2003. Minimum Error Rate Training 
for Statistical Machine Translation. In
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, July. 
ilipp Koehn. 2002. Euro
evaluation of machine translation. Ms., University of 
Southern California. 
hilipp Koehn. 2004. Pharaoh: a Beam Search Decoder 
for Phrase-based Statistical Machine Transl
132
Proceedings of the Workshop on Statistical Machine Translation, pages 134?137,
New York City, June 2006. c?2006 Association for Computational Linguistics
PORTAGE: with Smoothed Phrase Tables
and Segment Choice Models
Howard Johnson
National Research Council
Institute for Information Technology
Interactive Information
1200 Montreal Road
Ottawa, ON, Canada K1A 0R6
Howard.Johnson@cnrc-nrc.gc.ca
Fatiha Sadat, George Foster, Roland Kuhn,
Michel Simard, Eric Joanis and Samuel Larkin
National Research Council
Institute for Information Technology
Interactive Language Technologies
101 St-Jean-Bosco Street
Gatineau, QC, Canada K1A 0R6
firstname.lastname@cnrc-nrc.gc.ca
Abstract
Improvements to Portage and its partici-
pation in the shared task of NAACL 2006
Workshop on Statistical Machine Trans-
lation are described. Promising ideas in
phrase table smoothing and global dis-
tortion using feature-rich models are dis-
cussed as well as numerous improvements
in the software base.
1 Introduction
The statistical machine translation system Portage is
participating in the NAACL 2006 Workshop on Sta-
tistical Machine Translation. This is a good opportu-
nity to do benchmarking against a publicly available
data set and explore the benefits of a number of re-
cently added features.
Section 2 describes the changes that have been
made to Portage in the past year that affect the par-
ticipation in the 2006 shared task. Section 3 outlines
the methods employed for this task and extensions
of it. In Section 4 the results are summarized in tab-
ular form. Following these, there is a conclusions
section that highlights what can be gleaned of value
from these results.
2 Portage
Because this is the second participation of Portage in
such a shared task, a description of the base system
can be found elsewhere (Sadat et al 2005). Briefly,
Portage is a research vehicle and development pro-
totype system exploiting the state-of-the-art in sta-
tistical machine translation (SMT). It uses a custom
built decoder followed by a rescoring module that
adjusts weights based on a number of features de-
fined on the source sentence. We will devote space
to discussing changes made since the 2005 shared
task.
2.1 Phrase-Table Smoothing
Phrase-based SMT relies on conditional distribu-
tions p(s|t) and p(t|s) that are derived from the joint
frequencies c(s, t) of source/target phrase pairs ob-
served in an aligned parallel corpus. Traditionally,
relative-frequency estimation is used to derive con-
ditional distributions, ie p(s|t) = c(s, t)/
?
s c(s, t).
However, relative-frequency estimation has the
well-known problem of favouring rare events. For
instance, any phrase pair whose constituents occur
only once in the corpus will be assigned a probabil-
ity of 1, almost certainly higher than the probabili-
ties of pairs for which much more evidence exists.
During translation, rare pairs can directly compete
with overlapping frequent pairs, so overestimating
their probabilities can significantly degrade perfor-
mance.
To address this problem, we implemented two
simple smoothing strategies. The first is based on
the Good-Turing technique as described in (Church
and Gale, 1991). This replaces each observed joint
frequency c with cg = (c + 1)nc+1/nc, where nc
is the number of distinct pairs with frequency c
(smoothed for large c). It also assigns a total count
mass of n1 to unseen pairs, which we distributed
in proportion to the frequency of each conditioning
134
phrase. The resulting estimates are:
pg(s|t) =
cg(s, t)
?
s cg(s, t) + p(t)n1
,
where p(t) = c(t)/
?
t c(t). The estimates for
pg(t|s) are analogous.
The second strategy is Kneser-Ney smoothing
(Kneser and Ney, 1995), using the interpolated vari-
ant described in (Chen and Goodman., 1998):1
pk(s|t) =
c(s, t) ? D + D n1+(?, t) pk(s)
?
s c(s, t)
where D = n1/(n1 + 2n2), n1+(?, t) is the num-
ber of distinct phrases s with which t co-occurs, and
pk(s) = n1+(s, ?)/
?
s n1+(s, ?), with n1+(s, ?)
analogous to n1+(?, t).
Our approach to phrase-table smoothing contrasts
to previous work (Zens and Ney, 2004) in which
smoothed phrase probabilities are constructed from
word-pair probabilities and combined in a log-linear
model with an unsmoothed phrase-table. We believe
the two approaches are complementary, so a combi-
nation of both would be worth exploring in future
work.
2.2 Feature-Rich DT-based distortion
In a recent paper (Kuhn et al 2006), we presented a
new class of probabilistic ?Segment ChoiceModels?
(SCMs) for distortion in phrase-based systems. In
some situations, SCMs will assign a better distortion
score to a drastic reordering of the source sentence
than to no reordering; in this, SCMs differ from the
conventional penalty-based distortion, which always
favours less rather than more distortion.
We developed a particular kind of SCM based on
decision trees (DTs) containing both questions of a
positional type (e.g., questions about the distance
of a given phrase from the beginning of the source
sentence or from the previously translated phrase)
and word-based questions (e.g., questions about the
presence or absence of given words in a specified
phrase).
The DTs are grown on a corpus consisting of
segment-aligned bilingual sentence pairs. This
1As for Good-Turing smoothing, this formula applies only
to pairs s, t for which c(s, t) > 0, since these are the only ones
considered by the decoder.
segment-aligned corpus is obtained by training a
phrase translation model on a large bilingual cor-
pus and then using it (in conjunction with a distor-
tion penalty) to carry out alignments between the
phrases in the source-language sentence and those
in the corresponding target-language sentence in a
second bilingual corpus. Typically, the first corpus
(on which the phrase translation model is trained) is
the same as the second corpus (on which alignment
is carried out). To avoid overfitting, the alignment
algorithm is leave-one-out: statistics derived from
a particular sentence pair are not used to align that
sentence pair.
Note that the experiments reported in (Kuhn et
al, 2006) focused on translation of Chinese into En-
glish. The interest of the experiments reported here
onWMT data was to see if the feature-rich DT-based
distortion model could be useful for MT between
other language pairs.
3 Application to the Shared Task: Methods
3.1 Restricted Resource Exercise
The first exercise that was done is to replicate the
conditions of 2005 as closely as possible to see the
effects of one year of research and development.
The second exercise was to replicate all three of
these translation exercises using the 2006 language
model, and to do the three exercises of translat-
ing out of English into French, Spanish, and Ger-
man. This was our baseline for other studies. A
third exercise involved modifying the generation
of the phrase-table to incorporate our Good-Turing
smoothing. All six language pairs were re-processed
with these phrase-tables. The improvement in the
results on the devtest set were compelling. This be-
came the baseline for further work. A fourth ex-
ercise involved replacing penalty-based distortion
modelling with the feature-rich decision-tree based
distortion modelling described above. A fifth ex-
ercise involved the use of a Kneser-Ney phrase-
table smoothing algorithm as an alternative to Good-
Turing.
For all of these exercises, 1-best results after de-
coding were calculated as well as rescoring on 1000-
best lists of results using 12 feature functions (13
in the case of decision-tree based distortion mod-
elling). The results submitted for the shared task
135
were the results of the third and fourth exercises
where rescoring had been applied.
3.2 Open Resource Exercise
Our goal in this exercise was to conduct a com-
parative study using additional training data for the
French-English shared task. Results of WPT 2005
showed an improvement of at least 0.3 BLEU point
when exploiting different resources for the French-
English pair of languages. In addition to the training
resources used in WPT 2005 for the French-English
task, i.e. Europarl and Hansard, we used a bilingual
dictionary, Le Grand Dictionnaire Terminologique
(GDT) 2 to train translation models and the English
side of the UN parallel corpus (LDC2004E13) to
train an English language model. Integrating termi-
nological lexicons into a statistical machine transla-
tion engine is not a straightforward operation, since
we cannot expect them to come with attached prob-
abilities. The approach we took consists on view-
ing all translation candidates of each source term or
phrase as equiprobable (Sadat et al 2006).
In total, the data used in this second part of our
contribution to WMT 2006 is described as follows:
(1) A set of 688,031 sentences in French and En-
glish extracted from the Europarl parallel corpus (2)
A set of 6,056,014 sentences in French and English
extracted from the Hansard parallel corpus, the offi-
cial record of Canada?s parliamentary debates. (3) A
set of 701,709 sentences in French and English ex-
tracted from the bilingual dictionary GDT. (4) Lan-
guage models were trained on the French and En-
glish parts of the Europarl and Hansard. We used
the provided Europarl corpus while omitting data
from Q4/2000 (October-December), since it is re-
served for development and test data. (5) An addi-
tional English language model was trained on 128
million words of the UN Parallel corpus.
For the supplied Europarl corpora, we relied on
the existing segmentation and tokenization, except
for French, which we manipulated slightly to bring
into line with our existing conventions (e.g., convert-
ing l ? an into l? an, aujourd ? hui into aujourd?hui).
For the Hansard corpus used to supplement our
French-English resources, we used our own align-
ment based on Moore?s algorithm, segmentation,
2http://www.granddictionnaire.com/
and tokenization procedures. English preprocessing
simply included lower-casing, separating punctua-
tion from words and splitting off ?s.
4 Results
The results are shown in Table 1. The numbers
shown are BLEU scores. The MC rows correspond
to the multi-corpora results described in the open re-
source exercise section above. All other rows are
from the restricted resource exercise.
The devtest results are the scores computed be-
fore the shared-task submission and were used to
drive the choice of direction of the research. The
test results were computed after the shared-task sub-
mission and serve for validation of the conclusions.
We believe that our use of multiple training cor-
pora as well as our re-tokenization for French and
an enhanced language model resulted in our overall
success in the English-French translation track. The
results for the in-domain test data puts our group at
the top of the ranking table drawn by the organizers
(first on Adequacy and fluency and third on BLEU
scores).
5 Conclusion
Benchmarking with same language model and pa-
rameters as WPT05 reproduces the results with a
tiny improvement. The larger language model used
in 2006 for English yields about half a BLEU. Good-
Turing phrase table smoothing yields roughly half
a BLEU point. Kneser-Ney phrase table smooth-
ing yields between a third and half a BLEU point
more than Good-Turing. Decision tree based distor-
tion yields a small improvement for the devtest set
when rescoring was not used but failed to show im-
provement on the test set.
In summary, the results from phrase-table
smoothing are extremely encouraging. On the other
hand, the feature-rich decision tree distortion mod-
elling requires additional work before it provides a
good pay-back. Fortunately we have some encour-
aging avenues under investigation. Clearly there is
more work needed for both of these areas.
Acknowledgements
We wish to thank Aaron Tikuisis and Denis Yuen
for important contributions to the Portage code base
136
Table 1: Restricted and open resource results
fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
devtest: with rescoring
WPT05 29.32 29.08 23.21
LM-2005 29.30 29.21 23.41
LM-2006 29.88 29.54 23.94 30.43 28.81 17.33
GT-PTS 30.35 29.84 24.60 30.89 29.54 17.62
GT-PTS+DT-dist 30.09 29.44 24.62 31.06 29.46 17.84
KN-PTS 30.55 30.12 24.66 31.28 29.90 17.78
MC WPT05 29.63
MC 30.09 31.30
MC+GT-PTS 30.75 31.37
devtest: 1-best after decoding
LM-2006 28.59 28.45 23.22 29.22 28.30 16.94
GT-PTS 29.23 28.91 23.67 30.07 28.86 17.32
GT-PTS+DT-dist 29.48 29.07 23.50 30.22 29.46 17.42
KN-PTS 29.77 29.76 23.27 30.73 29.62 17.78
MC WPT05 28.71
MC 29.63 31.01
MC+GT-PTS 29.90 31.22
test: with rescoring
LM-2006 26.64 28.43 21.33 28.06 28.01 15.19
GT-PTS 27.19 28.95 21.91 28.60 28.83 15.38
GT-PTS+DT-dist 26.84 28.56 21.84 28.56 28.59 15.45
KN-PTS 27.40 29.07 21.98 28.96 29.06 15.64
MC 26.95 29.12
MC+GT-PTS 27.10 29.46
test: 1-best after decoding
LM-2006 25.35 27.25 20.46 27.20 27.18 14.60
GT-PTS 25.95 28.07 21.06 27.85 27.96 15.05
GT-PTS+DT-dist 25.86 28.04 20.74 27.85 27.97 14.92
KN-PTS 26.83 28.66 21.36 28.62 28.71 15.42
MC 26.70 28.74
MC+GT-PTS 26.81 29.03
and the OQLF (Office Que?be?cois de la Langue
Franc?aise) for permission to use the GDT.
References
S. F. Chen and J. T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
K. Church and W. Gale. 1991. A comparison of the en-
hanced Good-Turing and deleted estimation methods
for estimating probabilities of English bigrams. Com-
puter speech and language, 5(1):19?54.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP) 1995, pages 181?184, Detroit, Michi-
gan. IEEE.
R. Kuhn, D. Yuen, M. Simard, G. Foster, P. Paul, E. Joa-
nis and J. H. Johnson. 2006. Segment Choice Models:
Feature-Rich Models for Global Distortion in Statisti-
cal Machine Translation (accepted for publication in
HLT-NAACL conference, to be held June 2006).
F. Sadat, J. H. Johnson, A. Agbago, G. Foster, R. Kuhn,
J. Martin and A. Tikuisis. 2005. PORTAGE: A
Phrase-based Machine Translation System In Proc.
ACL 2005 Workshop on building and using parallel
texts. Ann Arbor, Michigan.
F. Sadat, G. Foster and R. Kuhn. 2006. Syste`me de tra-
duction automatique statistique combinant diffe?rentes
ressources. In Proc. TALN 2006 (Traitement Automa-
tique des Langues Naturelles). Leuven, Belgium, April
10-13, 2006.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conference / North American
Chapter of the ACL, Boston, May.
137
Proceedings of the Second Workshop on Statistical Machine Translation, pages 17?24,
Prague, June 2007. c?2007 Association for Computational Linguistics
Integration of an Arabic Transliteration Module into a Statistical 
Machine Translation System
Mehdi M. Kashani+, Eric Joanis++, Roland Kuhn++, George Foster++, Fred Popowich+
+ School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby, BC V5A 1S6, Canada
mmostafa@sfu.ca
 popowich@sfu.ca
++ NRC Institute for Information Technology
101 St-Jean-Bosco Street 
Gatineau, QC K1A 0R6, Canada
firstname.lastname@cnrc-nrc.gc.ca
Abstract
We provide an in-depth analysis of the in-
tegration of an Arabic-to-English translit-
eration system into a general-purpose 
phrase-based statistical machine translation 
system. We study the integration from dif-
ferent aspects and evaluate the improve-
ment that can be attributed to the integra-
tion using the BLEU metric. Our experi-
ments show that a transliteration module 
can help significantly in the situation where 
the test data is rich with previously unseen 
named entities. We obtain 70% and 53% of 
the theoretical maximum improvement we 
could achieve, as measured by an oracle on 
development and test sets respectively for 
OOV words (out of vocabulary source 
words not appearing in the phrase table).
1 Introduction
Transliteration is the practice of transcribing a 
word or text written in one writing system into an-
other writing system. The most frequent candidates 
for transliteration are person names, locations, or-
ganizations and imported words. The lack of a 
fully comprehensive bilingual dictionary including 
the entries for all named entities (NEs) renders the 
task of transliteration necessary for certain natural 
language processing applications dealing with 
named entities. Two applications where translitera-
tion can be particularly useful are machine transla-
tion (MT) and cross lingual information retrieval. 
While transliteration itself is a relatively well-
studied problem, its effect on the aforementioned 
applications is still under investigation.
Transliteration as a self-contained task has its 
own challenges, but applying it to a real applica-
tion introduces new challenges. In this paper we 
analyze the efficacy of integrating a transliteration 
module into a real MT system and evaluate the 
performance.
When working on a limited domain, given a suf-
ficiently large amount of training data, almost all 
of the words in the unseen data (in the same do-
main) will have appeared in the training corpus. 
But this argument does not hold for NEs, because 
no matter how big the training corpus is, there will 
always be unseen names of people and locations. 
Current MT systems either leave such unknown 
names as they are in the final target text or remove 
them in order to obtain a better evaluation score. 
None of these methods can give the reader who is 
not familiar with the source language any informa-
tion about those out-of-vocabulary (OOV) words, 
especially when the source and target languages 
use different scripts. If these words are not names, 
one can usually guess what they are, by using the 
partial information of other parts of speech. But, in 
the case of names, there is no way to determine the 
individual or location the sentence is talking about. 
So, to improve the usability of a translation, it is 
particularly important to handle NEs well.
The importance of NEs is not yet reflected in the 
evaluation methods used in the MT community, 
the most common of which is the BLEU metric. 
BLEU (Papineni et al 2002) was devised to pro-
vide automatic evaluation of MT output. In this 
metric n-gram similarity of the MT output is com-
puted with one or more references made by human 
17
translators. BLEU does not distinguish between 
different words and gives equal weight to all. In 
this paper, we base our evaluation on the BLEU 
metric and show that using transliteration has im-
pact on it (and in some cases significant impact). 
However, we believe that such integration is more 
important for practical uses of MT than BLEU in-
dicates.
Other than improving readability and raising the 
BLEU score, another advantage of using a translit-
eration system is that having the right translation 
for a name helps the language model select a better 
ordering for other words. For example, our phrase 
table1 does not have any entry for ?????? (Dulles) 
and when running MT system on the plain Arabic 
text we get
and this trip was cancelled [?] by the american 
authorities responsible for security at the airport 
???? .
We ran our MT system twice, once by suggest-
ing ?dallas? and another time ?dulles? as English 
equivalents for ?????? and the decoder generated 
the following sentences, respectively:
and this trip was cancelled [?] by the american 
authorities responsible for security at the airport 
at dallas .
and this trip was cancelled [?] by the american 
authorities responsible for security at dulles air-
port .2
Every statistical MT (SMT) system assigns a 
probability distribution to the words that are seen 
in its parallel training data, including proper names. 
The richer the training data, the higher the chance 
for a given name in the test data to be found in the 
translation tables. In other words, an MT system 
with a relatively rich phrase table is able to trans-
late many of the common names in the test data, 
with all the remaining words being rare and foreign. 
So unlike a self-contained transliteration module, 
which typically deals with a mix of ?easy? and 
                                                
1 A table where the conditional probabilities of target 
phrases given source phrases (and vice versa) is kept.
2 Note that the language model can be trained on more 
text, and hence can know more NEs than the translation 
model does.
?hard? names, the primary use for a transliteration 
module embedded in an SMT system will be to 
deal with the ?hard? names left over after the 
phrase tables have provided translations for the 
?easy? ones. That means that when measuring the 
performance improvements caused by embedding 
a transliteration module in an MT system, one 
must keep in mind that such improvements are dif-
ficult to attain: they are won mainly by correctly 
transliterating ?hard? names. 
Another issue with OOV words is that some of 
them remained untranslated due to misspellings in 
the source text. For example, we encountered 
??????? (?Hthearow?) instead of ??????? 
(?Heathrow?) or ??????? (?Brezer?) instead of 
??????? (?Bremer?) in our development test set. 
Also, evaluation by BLEU (or a similar auto-
matic metric) is problematic. Almost all of the MT 
evaluations use one or more reference translations 
as the gold standard and, using some metrics, they 
give a score to the MT output. The problem with 
NEs is that they usually have more than a single 
equivalent in the target language (especially if they 
don't originally come from the target language) 
which may or may not have been captured in the 
gold standard. So even if the transliteration module 
comes up with a correct interpretation of a name it 
might not receive credit as far as the limited num-
ber of correct names in the references are con-
cerned.
Our first impression was that having more inter-
pretations for a name in the references would raise 
the transliteration module?s chance to generate at 
least one of them, hence improving the perform-
ance. But, in practice, when references do not 
agree on a name?s transliteration that is the sign of 
an ambiguity. In these cases, the transliteration 
module often suggests a correct transliteration that 
the decoder outputs correctly, but which fails to 
receive credit from the BLEU metric because this 
transliteration is not found in the references. As an 
example, for the name ?????????, four references 
came up with four different interpretations: 
swerios, swiriyus, severius, sweires. A quick query 
in Google showed us another four acceptable in-
terpretations (severios, sewerios, sweirios, saw-
erios).
Machine transliteration has been an active re-
search field for quite a while (Al-Onaizan and 
Knight, 2002; AbdulJaleel and Larkey, 2003; Kle-
mentiev and Roth, 2006; Sproat et al 2006) but to 
18
our knowledge there is little published work on 
evaluating transliteration within a real MT system.
The closest work to ours is described in (Hassan 
and Sorensen, 2005) where they have a list of 
names in Arabic and feed this list as the input text 
to their MT system. They evaluate their system in 
three different cases: as a word-based NE transla-
tion, phrase-based NE translation and in presence 
of a transliteration module. Then, they report the 
BLEU score on the final output. Since their text is 
comprised of only NEs, the BLEU increase is quite 
high. Combining all three models, they get a 24.9 
BLEU point increase over the na?ve baseline. The 
difference they report between their best method 
without transliteration and the one including trans-
literation is 8.12 BLEU points for person names 
(their best increase).
In section 2, we introduce different methods for 
incorporating a transliteration module into an MT 
system and justify our choice. In section 3, the 
transliteration module is briefly introduced and we 
explain how we prepared its output for use by the 
MT system. In section 4, an evaluation of the inte-
gration is provided. Finally, section 5 concludes 
the paper.
2 Our Approach
Before going into details of our approach, an 
overview of Portage (Sadat et al 2005), the 
machine translation system that we used for our 
experiments and some of its properties should be 
provided.
Portage is a statistical phrase-based SMT system 
similar to Pharaoh (Koehn et al 2003).  Given a 
source sentence, it tries to find the target sentence 
that maximizes the joint probability of a target sen-
tence and a phrase alignment according to a loglin-
ear model. Features in the loglinear model consist 
of a phrase-based translation model with relative-
frequency and lexical probability estimates; a 4-
gram language model using Kneser-Ney smooth-
ing, trained with the SRILM toolkit; a single-
parameter distortion penalty on phrase reordering; 
and a word-length penalty. Weights on the loglin-
ear features are set using Och's algorithm (Och, 
2003) to maximize the system's BLEU score on a 
development corpus. To generate phrase pairs from 
a parallel corpus, we use the "diag-and" phrase 
induction algorithm described in (Koehn et al 
2003), with symmetrized word alignments gener-
ated using IBM model 2 (Brown et al 1993).
Portage allows the use of SGML-like markup 
for arbitrary entities within the input text. The 
markup can be used to specify translations 
provided by external sources for the entities, such 
as rule-based translations of numbers and dates, or 
a transliteration module for OOVs in our work. 
Many SMT systems have this capability, so 
although the details given here pertain to Portage, 
the techniques described can be used in many 
different SMT systems.
As an example, suppose we already have two 
different transliterations with their probabilities for 
the Arabic name ??????. We can replace every 
occurrence of the ?????? in the Arabic input text 
with the following:
<NAME target="mohammed|mohamed"
prob=".7|.3"> ???? </NAME>
By running Portage on this marked up text, the 
decoder chooses between entries in its own phrase 
table and the marked-up text. One thing that is 
important for our task is that if the entry cannot be 
found in Portage?s phrase tables, it is guaranteed 
that one of the candidates inside the markup will 
be chosen. Even if none of the candidates exist in 
the language model, the decoder still picks one of 
them, because the system assigns a small arbitrary 
probability (we typically use e-18) as unigram 
probability of each unseen word.
We considered four different methods for 
incorporating the transliteration module into the 
MT system. The first and second methods need an 
NE tagger and the other two do not require any 
external tools.
Method 1: use an NE tagger to extract the 
names in the Arabic input text. Then, run the 
transliteration module on them and assign 
probabilities to top candidates. Use the markup 
capability of Portage and replace each name in the 
Arabic text with the SGML-like tag including 
different probabilities for different candidates. 
Feed the marked-up text to Portage to translate.
Method 2: similar to method 1 but instead of 
using the marked-up text, a new phrase table, only 
containing entries for the names in the Arabic input 
text is built and added to Portage?s existing phrase 
tables. A weight is given to this phrase table and 
19
then the decoder uses this phrase table as well as 
its own phrase tables to decide which translation to 
choose when encountering the names in the 
text.  The main difference between methods 1 and 
2 is that in our system, method 2 allows for a bleu-
optimal weight to be learned for the NE phrase 
table, whereas the weight on the rules for method 1 
has to be set by hand.
Method 3: run Portage on the plain Arabic text. 
Extract all untranslated Arabic OOVs and run the 
transliteration module on them. Replace them with 
the top candidate.
Method 4: run Portage on the plain Arabic text. 
Extract all untranslated Arabic OOVs and run the 
transliteration module on them. Replace them with 
SGML-like tags including different probabilities 
for different candidates, as described previously. 
Feed the marked-up text to Portage to translate.
The first two methods need a powerful NE 
tagger with a high recall value. We computed the 
recall value on the development set OOVs using 
two different NE taggers, Tagger A and Tagger B 
(each from a different research group). Taggers A 
and B showed a recall of 33% and 53% respec-
tively, both being low for our purposes. Another 
issue with these two methods is that for many of 
the names the transliteration module will compete 
with the internal phrase table. Our observations 
show that if a name exists in the phrase table, it is 
likely to be translated correctly. In general, 
observed parallel data (i.e. training data) should be 
a more reliable source of information than 
transliteration, encouraging us to use transliteration 
most appropriately as a ?back-off? method. In a 
few cases, the Arabic name is ambiguous with a 
common word and is mistakenly translated as such. 
For example, ????? ??? ???? is an Arabic name that 
should be transliterated as ?Hani Abu Nahl? but 
since ????? also means ?solve?, the MT system 
outputs ?Hani Abu Solve?. The advantage of the 
first two methods is that they can deal with such 
cases. But considering the noise in the NE 
detectors, handling them increases the risk of 
losing already correct translations of other names.
The third method is simple and easy to use but 
not optimal: it does not take advantage of the 
decoder?s internal features (notably the language 
models) and only picks up the highest scoring 
candidate from the transliteration module.
The fourth method only deals with those words 
that the MT system was unable to deal with and 
had to leave untranslated in the final text. 
Therefore whatever suggestions the transliteration 
module makes do not need to compete with the 
internal phrase tables, which is good because we 
expect the phrase tables to be a more reliable 
source of information. It is guaranteed that the 
translation quality will be improved (in the worst 
case, a bad transliteration is still more informative 
than the original word in Arabic script). Moreover, 
unlike the third method, we take advantage of all 
internal decoder features on the second pass. We 
adopt the fourth method for our experiment. The 
following example better illustrates how this 
approach works:
Example: Suppose we have the following sentence 
in the Arabic input text: 
???? ???? ????? ????? ???????.
Portage is run on the Arabic plain text and yields 
the following output:
blair accepts ????? report in full .
The Arabic word ??????? (Hutton) is extracted and 
fed to the transliteration module. The 
transliteration module comes up with some English 
candidates, each with different probabilities as 
estimated by the HMM. They are rescaled (as will 
be explained in section 3) and the following 
markup text will be generated to replace the 
untranslated ??????? in the first plain Arabic 
sentence:
<NAME target="hoton|hutton|authon" 
prob="0.1|0.00028|4.64e-05">?????</NAME> 
Portage is then run on this newly marked up text 
(second pass). From now on, with the additional 
guidance of the language models, it is the 
decoder?s task to decide between different markup 
suggestions. For the above example, the following 
output will be generated:
blair accepts hutton report in full .
20
3 Transliteration System
In this section we provide a brief overview of the 
embedded transliteration system we used for our 
experiment. For the full description refer to 
(Kashani et al 2007).
3.1 Three Phase Transliteration
The transliteration module follows the noisy 
channel framework. The adapted spelling-based 
generative model is similar to (Al-Onaizan and 
Knight, 2002). It consists of three consecutive 
phases, the first two using HMMs and the Viterbi 
algorithm, and the third using a number of 
monolingual dictionaries to match the close entries 
or to filter out some invalid candidates from the 
first two phases.
Since in Arabic, the diacritics are usually 
omitted in writing, a name like ?????? (Mohamed) 
would have an equivalent like ?mhmd? if we only 
take into account the written letters. To address 
this issue, we run Viterbi in two different passes 
(each called a phase), using HMMs trained on data 
prepared in different ways.
In phase 1, the system tries to find the best 
transliterations of the written word, without caring 
about what the hidden diacritics would be (in our 
example, mhmd).
In phase 2, given the Arabic input and the output 
candidates from phase 1, the system fills in the 
possible blanks in between using the character-
based language model (yielding ?mohamed? as a 
possible output, among others).
To prepare the character-level translation model 
for both phases we adopted an approach similar to 
(AbdulJaleel and Larkey, 2003).
In phase 3, the Google unigram model 
(LDC2006T13 from the LDC catalog) is first used 
to filter out the noise (i.e. those candidates that do 
not exist in the Google unigram are removed from 
the candidate list). Then a combination of some 
monolingual dictionaries of person names is used 
to find close matches between their entries and the 
HMM output candidates based on the Levenshtein 
distance metric.
3.2 Task-specific Changes to the Module
Due to the nature of the task at hand and by 
observing the development test set and its 
references, the following major changes became 
necessary:
Removing Part of Phase Three: By observing the 
OOV words in the development test set, we 
realized that having the monolingual dictionary in 
the pipeline and using the Levensthtein distance as 
a metric for adding the closest dictionary entries to 
the final output, does not help much, mainly 
because OOVs are rarely in the dictionary. So, the 
dictionary part not only slows down the execution 
but would also add noise to the final output (by 
adding some entries that probably are not the 
desired outputs). However, we kept the Google 
unigram filtering in the pipeline.
Rescaling HMM Probabilities: Although the 
transliteration module outputs HMM probability 
score for each candidate, and the MT system also 
uses probability scores, in practice the translitera-
tion scores have to be adjusted.  For example, if 
three consecutive candidates have log probabilities 
-40, -42 and -50, the decoder should be given val-
ues with similar differences in scale, comparable 
with the typical differences in its internal features 
(eg. Language Models). Knowing that the entries 
in the internal features usually have exponential 
differences, we adopted the following conversion 
formula:
p'i = 0.1*(pi/pmax)?
Equation 1
where pi = 10(output of HMM for candidate i) and max is the 
best candidate.
We rescale the HMM probability so that the top 
candidate is (arbitrarily) given a probability of p'max
= 0.1.  It immediately follows that the rescaled 
score would be 0.1 * pi / pmax.  Since the decoder
combines its models in a log-linear fashion, we 
apply an exponent ? to the HMM probabilities be-
fore scaling them, as way to control the weight of 
those probabilities in decoding.  This yields equa-
tion 1.  Ideally, we would like the weight ? to be 
optimized the same way other decoder weights are 
optimized, but our decoder does not support this 
yet, so for this work we arbitrarily set the weight to 
? = 0.2, which seems to work well. For the above 
example, the distribution would be 0.1, 0.039 and 0.001.
21
Prefix Detachment: Arabic is a morphologically 
rich language. Even after performing tokenization, 
some words still remain untokenized. If the 
composite word is frequent, there is a chance that it 
exists in the phrase table but many times it does 
not, especially if the main part of that word is a 
named entity. We did not want to delve into the 
details of morphology: we only considered two 
frequent prefixes: ??? (?va? meaning ?and?) and 
???? (?al? determiner in Arabic). If a word starts 
with either of these two prefixes, we detach them 
and run the transliteration module once on the 
detached name and a second time on the whole 
word. The output candidates are merged 
automatically based on their scores, and the 
decoder decides which one to choose.
Keeping the Top 5 HMM Candidates: The 
transliteration module uses the Google unigram 
model to filter out the candidate words that do not 
appear above a certain threshold (200 times) on the 
Internet. This helps eliminate hundreds of 
unwanted sequences of letters. But, we decided to 
keep top-5 candidates on the output list, even if 
they are rejected by the Google unigram model 
because sometimes the transliteration module is
unable to suggest the correct equivalent or in other 
cases the OOV should actually be translated rather 
than transliterated 3 . In these cases, the closest 
literal transliteration will still provide the end user 
more information about the entity than the word in 
Arabic script would.
4 Evaluation
Although there are metrics that directly address NE 
translation performance4, we chose to use BLEU 
because our purpose is to assess NE translation 
within MT, and BLEU is currently the standard 
metric for MT.
                                                
3 This would happen especially for ancient names or 
some names that underwent sophisticated morphologi-
cal transformations (For example, Abraham in English 
and ??????? (Ibrahim) in Arabic).
4 NIST?s NE translation task 
(http://www.nist.gov/speech/tests/ace/index.htm) is an 
example.
4.1 Training Data
We used the data made available for the 2006 
NIST Machine Translation Evaluation. Our bilin-
gual training corpus consisted of 4M sentence pairs
drawn mostly from newswire and UN domains. 
We trained one language model on the English half 
of this corpus (137M running words), and another 
on the English Gigaword corpus (2.3G running 
words). For tuning feature weights, we used LDC's 
"multiple translation part 1" corpus, which contains 
1,043 sentence pairs. 
4.2 Test Data
We used the NIST MT04 evaluation set and the 
NIST MT05 evaluation set as our development and 
blind test sets. The development test set consists of 
1353 sentences, 233 of which contain OOVs. 
Among them 100 sentences have OOVs that are 
actually named entities. The blind test set consists 
of 1056 sentences, 189 of them having OOVs and 
131 of them having OOV named entities. The 
number of sentences for each experiment is 
summarized in table 1.
Whole Text OOV 
Sentences
OOV-NE 
Sentences
Dev test set 1353 233 100
Blind test set 1056 189 131
Table 1: Distribution of sentences in test sets.
4.3 Results
As the baseline, we ran the Portage without the 
transliteration module on development and blind 
test sets. The second column of table 2 shows 
baseline BLEU scores. We applied method 4 as 
outlined in section 2 and computed the BLEU 
score, also in order to compare the results we 
implemented method 3 on the same test sets. The 
BLEU scores obtained from methods 3 and 4 are 
shown in columns 3 and 4 of table 2.
baseline Method 3 Method 4 Oracle
Dev 44.67 44.71 44.83 44.90
Blind 48.56 48.62 48.80 49.01
Table 2: BLEU score on different test sets.
Considering the fact that only a small portion of 
the test set has out-of-vocabulary named entities, 
22
we computed the BLEU score on two different 
sub-portions of the test set: first, on the sentences 
with OOVs; second, only on the sentences 
containing OOV named entities. The BLEU 
increase on different portions of the test set is 
shown in table 3.
baseline Method 4
Dev OOV sentences 39.17 40.02
OOV-NE Sentences 44.56 46.31
blind OOV sentences 43.93 45.07
OOV-NE Sentences 42.32 44.87
Table 3: BLEU score on different 
portions of the test sets.
To set an upper bound on how much applying 
any transliteration module can contribute to the 
overall results, we developed an oracle-like 
dictionary for the OOVs in the test sets, which was 
then used to create a markup Arabic text. By 
feeding this markup input to the MT system we 
obtained the result shown in column 5 of table 2. 
This is the performance our system would achieve 
if it had perfect accuracy in transliteration, 
including correctly guessing what errors the human 
translators made in the references.  Method 4 
achieves 70% of this maximum gain on dev, and 
53% on blind.
5 Conclusion
This paper has described the integration of a trans-
literation module into a state-of-the-art statistical 
machine translation (SMT) system for the Arabic 
to English task. The final version of the translitera-
tion module operates in three phases. First, it gen-
erates English letter sequences corresponding to 
the Arabic letter sequence; for the typical case 
where the Arabic omits diacritics, this often means 
that the English letter sequence is incomplete (e.g., 
vowels are often missing). In the next phase, the 
module tries to guess the missing English letters. 
In the third phase, the module uses a huge collec-
tion of English unigrams to filter out improbable or 
impossible English words and names. We de-
scribed four possible methods for integrating this
module in an SMT system. Two of these methods 
require NE taggers of higher quality than those 
available to us, and were not explored experimen-
tally. Method 3 inserts the top-scoring candidate 
from the transliteration module in the translation 
wherever there was an Arabic OOV in the source. 
Method 4 outputs multiple candidates from the
transliteration module, each with a score; the SMT 
system combines these scores with language model 
scores to decide which candidate will be chosen. In 
our experiments, Method 4 consistently outper-
formed Model 3. Note that although we used 
BLEU as the metric for all experiments in this pa-
per, BLEU greatly understates the importance of
accurate transliteration for many practical SMT 
applications.
References
Nasreen AbdulJaleel and Leah S. Larkey, 2003. Statisti-
cal Transliteration for English-Arabic Cross Lan-
guage Information Retrieval, Proceedings of the 
Twelfth International Conference on Information and 
Knowledge Management, New Orleans, LA
Yaser Al-Onaizan and Kevin Knight, 2002. Machine 
Transliteration of Names in Arabic Text, Proceedings 
of the ACL Workshop on Computational Approaches 
to Semitic Languages 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer, 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation, Computational Linguistics
Hany Hassan and Jeffrey Sorensen, 2005. An Integrated 
Approach for Arabic-English Named Entity Transla-
tion, Proceedings of the ACL Workshop on Compu-
tational Approaches to Semitic Languages (ACL), 
University of Michigan, Ann Arbor
Mehdi M. Kashani, Fred Popowich, and Anoop Sarkar, 
2007. Automatic Transliteration of Proper Nouns 
from Arabic to English, Proceedings of the Second 
Workshop on Computational Approaches to Arabic 
Script-based Languages
Alexandre Klementiev and Dan Roth, 2006. Named 
Entity Transliteration and Discovery from Multilin-
gual Comparable Corpora, COLING-ACL, Sidney, 
Australia
Philipp Koehn, Franz Josef Och, and Daniel Marcu, 
2003. Statistical Phrase-based Translation, In Pro-
ceedings of HLT-NAACL, Edmonton, Canada
Franz Josef Och, 2003. Minimum Error Rate Training 
for Statistical Machine Translation, In Proceedings 
of the 41th Annual Meeting of the Association for 
Computation Linguistics, Sapporo
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu, 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
23
of the 40th Annual Conference of the Association for 
Computational Linguistics (ACL), Philadelphia, PA
Fatiha Sadat, Howard Johnson, Akakpo Agbago, 
George Foster, Roland Kuhn, Aaron Tikuisis, 2005. 
Portage: A Phrase-base Machine Translation System.
In Proceedings of the ACL Workshop on Building 
and Using Parallel Texts, Ann Arbor, Michigan
Richard Sproat, Tao Tao, and ChengXiang Zhai, 2006, 
Named Entity Transliteration with Comparable Cor-
pora, COLING-ACL, Sidney, Australia
24
Proceedings of the Second Workshop on Statistical Machine Translation, pages 128?135,
Prague, June 2007. c?2007 Association for Computational Linguistics
Mixture-Model Adaptation for SMT
George Foster and Roland Kuhn
National Research Council Canada
first.last@nrc.gc.ca
Abstract
We describe a mixture-model approach to
adapting a Statistical Machine Translation
System for new domains, using weights that
depend on text distances to mixture compo-
nents. We investigate a number of variants
on this approach, including cross-domain
versus dynamic adaptation; linear versus
loglinear mixtures; language and transla-
tion model adaptation; different methods of
assigning weights; and granularity of the
source unit being adapted to. The best
methods achieve gains of approximately one
BLEU percentage point over a state-of-the
art non-adapted baseline system.
1 Introduction
Language varies significantly across different gen-
res, topics, styles, etc. This affects empirical mod-
els: a model trained on a corpus of car-repair manu-
als, for instance, will not be well suited to an appli-
cation in the field of tourism. Ideally, models should
be trained on text that is representative of the area
in which they will be used, but such text is not al-
ways available. This is especially the case for bilin-
gual applications, because parallel training corpora
are relatively rare and tend to be drawn from spe-
cific domains such as parliamentary proceedings.
In this paper we address the problem of adapting
a statistical machine translation system by adjust-
ing its parameters based on some information about
a test domain. We assume two basic settings. In
cross-domain adaptation, a small sample of parallel
in-domain text is available, and it is used to optimize
for translating future texts drawn from the same do-
main. In dynamic adaptation, no domain informa-
tion is available ahead of time, and adaptation is
based on the current source text under translation.
Approaches developed for the two settings can be
complementary: an in-domain development corpus
can be used to make broad adjustments, which can
then be fine tuned for individual source texts.
Our method is based on the classical technique
of mixture modeling (Hastie et al, 2001). This
involves dividing the training corpus into different
components, training a model on each part, then
weighting each model appropriately for the current
context. Mixture modeling is a simple framework
that encompasses many different variants, as de-
scribed below. It is naturally fairly low dimensional,
because as the number of sub-models increases, the
amount of text available to train each, and therefore
its reliability, decreases. This makes it suitable for
discriminative SMT training, which is still a chal-
lenge for large parameter sets (Tillmann and Zhang,
2006; Liang et al, 2006).
Techniques for assigning mixture weights depend
on the setting. In cross-domain adaptation, knowl-
edge of both source and target texts in the in-domain
sample can be used to optimize weights directly. In
dynamic adaptation, training poses a problem be-
cause no reference text is available. Our solution
is to construct a multi-domain development sample
for learning parameter settings that are intended to
generalize to new domains (ones not represented in
the sample). We do not learn mixture weights di-
rectly with this method, because there is little hope
128
that these would be well suited to new domains. In-
stead we attempt to learn how weights should be set
as a function of distance. To our knowledge, this ap-
proach to dynamic adaptation for SMT is novel, and
it is one of the main contributions of the paper.
A second contribution is a fairly broad investiga-
tion of the large space of alternatives defined by the
mixture-modeling framework, using a simple genre-
based corpus decomposition. We experimented with
the following choices: cross-domain versus dynamic
adaptation; linear versus loglinear mixtures; lan-
guage and translation model adaptation; various text
distance metrics; different ways of converting dis-
tance metrics into weights; and granularity of the
source unit being adapted to.
The remainder of the paper is structured follows:
section 2 briefly describes our phrase-based SMT
system; section 3 describes mixture-model adapta-
tion; section 4 gives experimental results; section 5
summarizes previous work; and section 6 concludes.
2 Phrase-based Statistical MT
Our baseline is a standard phrase-based SMT sys-
tem (Koehn et al, 2003). Given a source sentence s,
this tries to find the target sentence t? that is the most
likely translation of s, using the Viterbi approxima-
tion:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where alignment a = (s?1, t?1, j1), ..., (s?K , t?K , jK);
t?k are target phrases such that t = t?1 . . . t?K ; s?k are
source phrases such that s = s?j1 . . . s?jK ; and s?k is
the translation of the kth target phrase t?k.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[?
i
?ifi(s, t,a)
]
(1)
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al, 2001) on a development corpus. The
features used in this study are: the length of
t; a single-parameter distortion penalty on phrase
reordering in a, as described in (Koehn et al,
2003); phrase translation model probabilities; and
4-gram language model probabilities log p(t), us-
ing Kneser-Ney smoothing as implemented in the
SRILM toolkit.
Phrase translation model probabilities are features
of the form: log p(s|t,a) ? ?Kk=1 log p(s?k|t?k).
We use two different estimates for the conditional
probabilities p(t?|s?) and p(s?|t?): relative frequencies
and ?lexical? probabilities as described in (Zens and
Ney, 2004). In both cases, the ?forward? phrase
probabilities p(t?|s?) are not used as features, but only
as a filter on the set of possible translations: for each
source phrase s? that matches some ngram in s, only
the 30 top-ranked translations t? according to p(t?|s?)
are retained.
To derive the joint counts c(s?, t?) from which
p(s?|t?) and p(t?|s?) are estimated, we use the phrase in-
duction algorithm described in (Koehn et al, 2003),
with symmetrized word alignments generated using
IBM model 2 (Brown et al, 1993).
3 Mixture-Model Adaptation
Our approach to mixture-model adaptation can be
summarized by the following general algorithm:
1. Split the corpus into different components, ac-
cording to some criterion.
2. Train a model on each corpus component.
3. Weight each model according to its fit with the
test domain:
? For cross-domain adaptation, set param-
eters using a development corpus drawn
from the test domain, and use for all fu-
ture documents.
? For dynamic adaptation, set global param-
eters using a development corpus drawn
from several different domains. Set mix-
ture weights as a function of the distances
from corpus components to the current
source text.
4. Combine weighted component models into a
single global model, and use it to translate as
described in the previous section.
We now describe each aspect of this algorithm in
more detail.
129
3.1 Corpus Decomposition
We partition the corpus into different genres, defined
as being roughly identical to corpus source. This is
the simplest way to exploit heterogeneous training
material for adaptation. An alternative, which we
have not explored, would be to cluster the corpus
automatically according to topic.
3.2 Component Models
We adapt both language and translation model fea-
tures within the overall loglinear combination (1).
To train translation models on each corpus com-
ponent, we used a global IBM2 model for word
alignment (in order to avoid degradation in align-
ment quality due to smaller training corpora), then
extracted component-specific relative frequencies
for phrase pairs. Lexical probabilities were also de-
rived from the global IBM2 model, and were not
adapted.
The procedure for training component-specific
language models on the target halves of each cor-
pus component is identical to the procedure for the
global model described in section 2. In addition to
the component models, we also used a large static
global model.
3.3 Combining Framework
The most commonly-used framework for mixture
models is a linear one:
p(x|h) =
?
c
?cpc(x|h) (2)
where p(x|h) is either a language or translation
model; pc(x|h) is a model trained on component c,
and ?c is the corresponding weight. An alternative,
suggested by the form of the global model, is a log-
linear combination:
p(x|h) =
?
c
pc(x|h)?c
where we write ?c to emphasize that in this case
the mixing parameters are global weights, like the
weights on the other features within the loglinear
model. This is in contrast to linear mixing, where the
combined model p(x|h) receives a loglinear weight,
but the weights on the components do not partici-
pate in the global loglinear combination. One conse-
quence is that it is more difficult to set linear weights
using standard minimum-error training techniques,
which assume only a ?flat? loglinear model.
3.4 Distance Metrics
We used four standard distance metrics to cap-
ture the relation between the current source or tar-
get text q and each corpus component.1 All are
monolingual?they are applied only to source text
or only to target text.
The tf/idf metric commonly used in information
retrieval is defined as cos(vc,vq), where vc and
vq are vectors derived from component c and doc-
ument q, each consisting of elements of the form:
?p?(w) log p?doc(w), where p?(w) is the relative fre-
quency of word w within the component or docu-
ment, and pdoc(w) is the proportion of components
it appears in.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is a technique for implicitly capturing the
semantic properties of texts, based on the use of
Singular Value Decomposition to produce a rank-
reduced approximation of an original matrix of word
and document frequencies. We applied this tech-
nique to all documents in the training corpus (as op-
posed to components), reduced the rank to 100, then
calculated the projections of the component and doc-
ument vectors described in the previous paragraph
into the reduced space.
Perplexity (Jelinek, 1997) is a standard way of
evaluating the quality of a language model on a test
text. We define a perplexity-based distance metric
pc(q)1/|q|, where pc(q) is the probability assigned to
q by an ngram language model trained on compo-
nent c.
The final distance metric, which we call EM, is
based on expressing the probability of q as a word-
level mixture model: p(q) = ?|q|i=1
?
c dcpc(wi|hi),
where q = w1 . . . w|q|, and pc(w|h) is the ngram
probability of w following word sequence h in com-
ponent c. It is straighforward to use the EM algo-
rithm to find the set of weights d?c,?c that maxi-
mizes the likelihood of q. The weight d?c is defined
as the distance to component c. For all experiments
described below, we used a probability difference
threshold of 0.001 as the EM convergence criterion.
1Although we refer to these metrics as distances, most are
in fact proximities, and we use the convention throughout that
higher values mean closer.
130
3.5 Learning Adaptive Parameters
Our focus in this paper is on adaptation via mixture
weights. However, we note that the usual loglinear
parameter tuning described in section 2 can also be
considered adaptation in the cross-domain setting,
because learned preferences for word penalty, rel-
ative LM/TM weighting, etc, will reflect the target
domain. This is not the case for dynamic adapta-
tion, where, in the absence of an in-domain devel-
opment corpus, the only information we can hope to
glean are the weights on adapted models compared
to other features of the system.
The method used for adapting mixture weights
depends on both the combining framework (loglin-
ear versus linear), and the adaptive setting (cross-
domain versus dynamic), as described below.
3.5.1 Setting Loglinear Mixture Weights
When using a loglinear combining framework as
described in section 3.3, mixture weights are set
in the same way as the other loglinear parameters
when performing cross-domain adaptation. Loglin-
ear mixture models were not used for dynamic adap-
tation.
3.5.2 Setting Linear Mixture Weights
For both adaptive settings, linear mixture weights
were set as a function of the distance metrics de-
scribed in section 3.4. Given a set of metrics
{D1, . . . , Dm}, let di,c be the distance from the cur-
rent text to component c according to metric Di. A
simple approach to weighting is to choose a single
metric Di, and set the weights in (2) to be propor-
tional to the corresponding distances:
?c = di,c/
?
c?
di,c? . (3)
Because different distance metrics may capture
complementary information, and because optimal
weights might be a non-linear function of distance,
we also experimented with a linear combination of
metrics transformed using a sigmoid function:
?c =
m?
i=1
?i
1 + exp(ai(bi ? di,c)) (4)
where ?i reflects the relative predictive power of Di,
and the sigmoid parametes ai and bi can be set to
selectively suppress contributions from components
that are far away. Here we assume that ?i absorbs
a normalization constant, so that the ?c?s sum to 1.
In this approach, there are three parameters per dis-
tance metric to learn: ?i, ai, and bi. In general, these
parameters are also specific to the particular model
being adapted, ie the LM or the TM.
To optimize these parameters, we fixed global
loglinear weights at values obtained with Och?s al-
gorithm using representative adapted models based
on a single distance metric in (3), then used the
Downhill Simplex algorithm (Press et al, 2002) to
maximize BLEU score on the development corpus.
For tractability, we followed standard practice with
this technique and considered only monotonic align-
ments when decoding (Zens and Ney, 2004).
The two approaches just described avoid condi-
tioning ?c explicitly on c. This is necessary for
dynamic adaptation, since any genre preferences
learned from the development corpus cannot be ex-
pected to generalize. However, it is not necessary
for cross-domain adaptation, where the genre of the
development corpus is assumed to represent the test
domain. Therefore, we also experimented with us-
ing Downhill Simplex optimization to directly learn
the set of linear weights ?c that yield maximum
BLEU score on the development corpus.
A final variant on setting linear mixture weights is
a hybrid between cross-domain and dynamic adap-
tation. In this approach, both the global loglinear
weights and, if they are being used, the mixture pa-
rameters ?i, ai, bi are set to characterize the test do-
main as in cross-domain adaptation. When trans-
lating, however, distances to the current source text
are used in (3) or (4) instead of distances to the in-
domain development corpus. This obviously limits
the metrics used to ones that depend only on source
text.
4 Experiments
All experiments were run on the NIST MT evalua-
tion 2006 Chinese data set. Table 1 summarizes the
corpora used. The training corpus was divided into
seven components according to genre; in all cases
these were identical to LDC corpora, with the excep-
tion of the Newswire component, which was amal-
gamated from several smaller corpora. The target
131
genre for cross-domain adaptation was newswire,
for which high-quality training material is avail-
able. The cross-domain development set NIST04-
nw is the newswire subset of the NIST 2004 evalu-
ation set, and the dynamic adaptation development
set NIST04-mix is a balanced mixed-genre subset of
NIST 2004. The NIST 2005 evaluation set was used
for testing cross-domain adaptation, and the NIST
2006 evaluation set (both the ?GALE? and ?NIST?
parts) was used to test dynamic adaptation.
Because different development corpora are used
for cross-domain and dynamic adaptation, we
trained one static baseline model for each of these
adaptation settings, on the corresponding develop-
ment set.
All results given in this section are BLEU scores.
role corpus genres sent
train FBIS04 nw 182k
HK Hans proceedings 1,375k
HK Laws legal 475k
HK News press release 740k
Newswire nw 26k
Sinorama news mag 366k
UN proceedings 4,979k
dev NIST04-nw nw 901
NIST04-mix nw, sp, ed 889
test NIST05 nw 1,082
NIST06-GALE nw, ng, bn, bc 2,276
NIST06-NIST nw, ng, bn 1,664
Table 1: Corpora. In the genres column: nw =
newswire, sp = speeches, ed = editorial, ng = news-
group, bn = broadcast news, and bc = broadcast con-
versation.
4.1 Linear versus Loglinear Combination
Table 2 shows a comparison between linear and
loglinear mixing frameworks, with uniform weights
used in the linear mixture. Both types of mixture
model are better than the baseline, but the linear
mixture is slightly better than the loglinear mix-
ture. This is quite surprising, because these results
are on the development set: the loglinear model
tunes its component weights on this set, whereas
the linear model only adjusts global LM and TM
weights. We speculated that this may have been due
to non-smooth component models, and tried various
smoothing schemes, including Kneser-Ney phrase
table smoothing similar to that described in (Foster
et al, 2006), and binary features to indicate phrase-
pair presence within different components. None
helped, however, and we conclude that the problem
is most likely that Och?s algorithm is unable to find
a good maximimum in this setting. Due to this re-
sult, all experiments we describe below involve lin-
ear mixtures only.
combination adapted model
LM TM LM+TM
baseline 30.2 30.2 30.2
loglinear mixture 30.9 31.2 31.4
uniform linear mixture 31.2 31.1 31.8
Table 2: Linear versus loglinear combinations on
NIST04-nw.
4.2 Distance Metrics for Weighting
Table 3 compares the performance of all distance
metrics described in section 3.4 when used on their
own as defined in (3). The difference between them
is fairly small, but appears to be consistent across
LM and TM adaptation and (for the LM metrics)
across source and target side matching. In general,
LM metrics seem to have a slight advantage over the
vector space metrics, with EM being the best overall.
We focus on this metric for most of the experiments
that follow.
metric source text target text
LM TM LM TM
tf/idf 31.3 31.3 31.1 31.1
LSA 31.5 31.6
perplexity 31.6 31.3 31.7 31.5
EM 31.7 31.6 32.1 31.3
Table 3: Distance metrics for linear combination on
the NIST04-nw development set. (Entries in the top
right corner are missing due to lack of time.)
Table 4 shows the performance of the parame-
terized weighting function described by (4), with
source-side EM and LSA metrics as inputs. This
is compared to direct weight optimization, as both
these techniques use Downhill Simplex for param-
eter tuning. Unfortunately, neither is able to beat
132
the performance of the normalized source-side EM
metric on its own (reproduced on the first line from
table 3). In additional tests we verified that this also
holds for the test corpus. We speculate that this dis-
appointing result is due to compromises made in or-
der to run Downhill Simplex efficiently, including
holding global weights fixed, using only a single
starting point, and running with monotone decoding.
weighting LM TM
EM-src, direct 31.7 31.6
EM-src + LSA-src, parameterized 31.0 30.0
direct optimization 31.7 30.2
Table 4: Weighting techniques for linear combina-
tion on the NIST04-nw development set.
4.3 Cross-Domain versus Dynamic Adaptation
Table 5 shows results for cross-domain adaptation,
using the source-side EM metric for linear weight-
ing. Both LM and TM adaptation are effective, with
test-set improvements of approximately 1 BLEU
point over the baseline for LM adaptation and some-
what less for TM adaptation. Performance also im-
proves on the NIST06 out-of-domain test set (al-
though this set includes a newswire portion as well).
However, combined LM and TM adaptation is not
better than LM adaptation on its own, indicating that
the individual adapted models may be capturing the
same information.
model dev test
nist04- nist05 nist06-
nw nist
baseline 30.2 30.3 26.5
EM-src LM 31.7 31.2 27.8
EM-src TM 31.6 30.9 27.3
EM-src LM+TM 32.5 31.2 27.7
Table 5: Cross-Domain adaptation results.
Table 6 contains results for dynamic adaptation,
using the source-side EM metric for linear weight-
ing. In this setting, TM adaptation is much less
effective, not significantly better than the baseline;
performance of combined LM and TM adaptation
is also lower. However, LM adaptation improves
over the baseline by up to a BLEU point. The per-
formance of cross domain adaptation (reproduced
from table 5 on the second line) is slightly better for
the in-domain test set (NIST05), but worse than dy-
namic adaptation on the two mixed-domain sets.
model dev test
nist04- nist05 nist06- nist06-
mix nist gale
baseline 31.9 30.4 27.6 12.9
cross LM n/a 31.2 27.8 12.5
LM 32.8 30.8 28.6 13.4
TM 32.4 30.7 27.6 12.8
LM+TM 33.4 30.8 28.5 13.0
Table 6: Dynamic adaptation results, using src-side
EM distances.
model NIST05
baseline 30.3
cross EM-src LM 31.2
cross EM-src TM 30.9
hybrid EM-src LM 30.9
hybrid EM-src TM 30.7
Table 7: Hybrid adaptation results.
Table 7 shows results for the hybrid approach de-
scribed at the end of section 3.5.2: global weights
are learned on NIST04-nw, but linear weights are
derived dynamically from the current test file. Per-
formance drops slightly compared to pure cross-
domain adaptation, indicating that it may be impor-
tant to have a good fit between global and mixture
weights.
4.4 Source Granularity
The results of the final experiment, to determine the
effects of source granularity on dynamic adaptation,
are shown in table 8. Source-side EM distances are
applied to the whole test set, to genres within the set,
and to each document individually. Global weights
were tuned specifically for each of these conditions.
There appears to be little difference among these ap-
proaches, although genre-based adaptation perhaps
has a slight advantage.
133
granularity dev test
nist04- nist05 nist06- nist06-
mix nist gale
baseline 31.9 30.4 27.6 12.9
file 32.4 30.8 28.6 13.4
genre 32.5 31.1 28.9 13.2
document 32.9 30.9 28.6 13.4
Table 8: The effects of source granularity on dy-
namic adaptation.
5 Related Work
Mixture modeling is a standard technique in ma-
chine learning (Hastie et al, 2001). It has been
widely used to adapt language models for speech
recognition and other applications, for instance us-
ing cross-domain topic mixtures, (Iyer and Osten-
dorf, 1999), dynamic topic mixtures (Kneser and
Steinbiss, 1993), hierachical mixtures (Florian and
Yarowsky, 1999), and cache mixtures (Kuhn and De
Mori, 1990).
Most previous work on adaptive SMT focuses on
the use of IR techniques to identify a relevant sub-
set of the training corpus from which an adapted
model can be learned. Byrne et al(2003) use co-
sine distance from the current source document to
find relevant parallel texts for training an adapted
translation model, with background information for
smoothing alignments. Hildebrand et al(1995) de-
scribe a similar approach, but apply it at the sentence
level, and use it for language model as well as trans-
lation model adaptation. They rely on a perplexity
heuristic to determine an optimal size for the rele-
vant subset. Zhao et al(2004) apply a slightly differ-
ent sentence-level strategy to language model adap-
tation, first generating an nbest list with a baseline
system, then finding similar sentences in a monolin-
gual target-language corpus. This approach has the
advantage of not limiting LM adaptation to a parallel
corpus, but the disadvantage of requiring two trans-
lation passes (one to generate the nbest lists, and an-
other to translate with the adapted model).
Ueffing (2006) describes a self-training approach
that also uses a two-pass algorithm. A baseline sys-
tem generates translations that, after confidence fil-
tering, are used to construct a parallel corpus based
on the test set. Standard phrase-extraction tech-
niques are then applied to extract an adapted phrase
table from the system?s own output.
Finally, Zhang et al(2006) cluster the parallel
training corpus using an algorithm that heuristically
minimizes the average entropy of source-side and
target-side language models over a fixed number of
clusters. Each source sentence is then decoded us-
ing the language model trained on the cluster that
assigns highest likelihood to that sentence.
The work we present here is complementary
to both the IR approaches and Ueffing?s method
because it provides a way of exploiting a pre-
established corpus division. This has the potential
to allow sentences having little surface similarity to
the current source text to contribute statistics that
may be relevant to its translation, for instance by
raising the probability of rare but pertinent words.
Our work can also be seen as extending all previous
approaches in that it assigns weights to components
depending on their degree of relevance, rather than
assuming a binary distinction between relevant and
non-relevant components.
6 Conclusion and Future Work
We have investigated a number of approaches to
mixture-based adaptation using genres for Chi-
nese to English translation. The most successful
is to weight component models in proportion to
maximum-likelihood (EM) weights for the current
text given an ngram language model mixture trained
on corpus components. This resulted in gains of
around one BLEU point. A more sophisticated ap-
proach that attempts to transform and combine mul-
tiple distance metrics did not yield positive results,
probably due to an unsucessful optmization proce-
dure.
Other conclusions are: linear mixtures are more
tractable than loglinear ones; LM-based metrics are
better than VS-based ones; LM adaptation works
well, and adding an adapted TM yields no improve-
ment; cross-domain adaptation is optimal, but dy-
namic adaptation is a good fallback strategy; and
source granularity at the genre level is better than
the document or test-set level.
In future work, we plan to improve the optimiza-
tion procedure for parameterized weight functions.
We will also look at bilingual metrics for cross-
134
domain adaptation, and investigate better combina-
tions of cross-domain and dynamic adaptation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathematics
of Machine Translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?312, June.
W. Byrne, S. Khudanpur, W. Kim, S. Kumar, P. Pecina,
P. Virga, P. Xu, and D. Yarowsky. 2003. The JHU
2003 Chinese-English Machine Translation System.
In MT Summit IX, New Orleans, September.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. JASIS, 41(6):391?407.
Radu Florian and David Yarowsky. 1999. Dynamic non-
local language modeling via hierarchical topic-based
adaptation. In ACL 1999, pages 167?174, College
Park, Maryland, June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In EMNLP 2006, Sydney, Australia.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2001. The Elements of Statistical Learning. Springer.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 1995. Adaptation of the transla-
tion model for statistical machine translation based on
information retrieval. In EAMT 1995, Budapest, May.
R. Iyer and M. Ostendorf. 1999. Modeling long dis-
tance dependence in language: Topic mixtures vs. dy-
namic cache models. In IEEE Trans on Speech and
Language Processing, 1999.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Reinhard Kneser and Volker Steinbiss. 1993. On the
dynamic adaptation of stochastic language models.
In ICASSP 1993, pages 586?589, Minneapolis, Min-
nesota. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
2003, pages 127?133.
Roland Kuhn and Renato De Mori. 1990. A cache-based
natural language model for speech recognition. IEEE
Trans on PAMI, 12(6):570?583, June.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL 2006
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL 2003, Sapporo,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Report
RC22176, IBM, September.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge,
UK.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
ACL 2006.
Nicola Ueffing. 2006. Self-training for machine trans-
lation. In NIPS 2006 Workshop on MLIA, Whistler,
B.C., December.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT/NAACL 2004, Boston, May.
R. Zhang, H. Yamamoto, M. Paul, H. Okuma, K. Yasuda,
Y. Lepage, E. Denoual, D. Mochihashi, A. Finch, and
E. Sumita. 2006. The NiCT-ATR statistical machine
translation system for the IWSLT 2006 evaluation. In
IWSLT 2006.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In COLING
2004, Geneva, August.
135
Proceedings of the Second Workshop on Statistical Machine Translation, pages 203?206,
Prague, June 2007. c?2007 Association for Computational Linguistics
Rule-based Translation With Statistical Phrase-based Post-editing
Michel Simard, Nicola Ueffing, Pierre Isabelle and Roland Kuhn
Interactive Language Technologies Group
National Research Council of Canada
Gatineau, Canada, K1A 0R6
firstname.lastname@nrc-cnrc.gc.ca
Abstract
This article describes a machine translation
system based on an automatic post-editing
strategy: initially translate the input text into
the target-language using a rule-based MT
system, then automatically post-edit the out-
put using a statistical phrase-based system.
An implementation of this approach based
on the SYSTRAN and PORTAGE MT sys-
tems was used in the shared task of the Sec-
ond Workshop on Statistical Machine Trans-
lation. Experimental results on the test data
of the previous campaign are presented.
1 Introduction
Simard et al (2007) have recently shown how a sta-
tistical phrase-based machine translation system can
be used as an automatic post-editing (APE) layer,
on top of a rule-based machine translation system.
The motivation for their work is the repetitive nature
of the errors typically made by rule-based systems.
Given appropriate training material, a statistical MT
system can be trained to correct these systematic er-
rors, therefore reducing the post-editing effort. The
statistical system views the output of the rule-based
system as the source language, and reference hu-
man translations as the target language. Because the
training material for the APE layer will typically be
domain-specific, this process can be viewed as a way
of automatically adapting a rule-based system to a
specific application domain.
This approach has been shown experimentally
to produce large improvements in performance not
only over the baseline rule-based system that it cor-
rects, but also over a similar statistical phrase-based
MT system used in standalone mode, i.e. translating
the ?real? source text directly: Simard et al report a
reduction in post-editing effort of up to a third when
compared to the input rule-based translation, and as
much as 5 BLEU points improvement over the direct
SMT approach.
These impressive results, however, were obtained
in a very specific and somewhat unusual context:
the training and test corpora were extracted from
a collection of manually post-edited machine trans-
lations. The two corpora (one English-to-French,
one French-to-English) each contained three paral-
lel ?views? of the same data: 1) the source language
text, 2) a machine translation of that text into the
target language, as produced by a commercial rule-
based MT system, and 3) the final target-language
version of the text, produced by manually post-
editing the machine translation. Furthermore, the
corpus was very small, at least by SMT standards:
500K words of source-language data in the French-
to-English direction, 350K words in the English-to-
French. Because of this, the authors were left with
two important questions: 1) how would the results
scale up to much larger quantities of training data?
and 2) are the results related to the dependent nature
of the translations, i.e. is the automatic post-editing
approach still effective when the machine and hu-
man translations are produced independently of one
another?
With these two questions in mind, we partici-
pated in the shared task of the Second Workshop
on Statistical Machine Translation with an auto-
matic post-editing strategy: initially translate the in-
put text into the target-language using a rule-based
system, namely SYSTRAN, and automatically post-
edit the output using a statistical phrase-based sys-
tem, namely PORTAGE. We describe our system in
more detail in Section 2, and present some experi-
mental results in Section 3.
203
2 System description
Our system is composed of two main components:
a rule-based MT system, which handles the initial
translation into the target language, and a statistical
phrase-based post-editing system, which performs
domain-specific corrections and adaptations to the
output. We describe each component separately be-
low.
2.1 Rule-based Translation
The initial source-to-target language translation is
performed using the SYSTRAN machine translation
system, version 6. A detailed overview of SYS-
TRAN systems can be found in Dugast et al (2007).
For this shared task, we used the French-to-English
and English-to-French configurations of the system.
Although it is possible to provide the system with
specialized lexica, we did not rely on this feature,
and used the system in its basic ?out-of-the-box?
configuration.
2.2 Statistical Phrase-based Post-Editing
The output of the rule-based MT system described
above is fed into a post-editing layer that performs
domain-specific corrections and adaptation. This
operation is conceptually not very different from a
?target-to-target? translation; for this task, we used
the PORTAGE system, a state-of-the-art statistical
phrase-based machine translation system developed
at the National Research Council of Canada (NRC).1
A general description of PORTAGE can be found in
(Sadat et al, 2005).
For our participation in this shared task, we de-
cided to configure and train the PORTAGE system
for post-editing in a manner as much as possible
similar to the corresponding translation system, the
details of which can be found in (Ueffing et al,
2007). The main features of this configuration are:
? The use of two distinct phrase tables, contain-
ing phrase pairs extracted from the Europarl
and the News Commentary training corpora re-
spectively.
? Multiple phrase-probability feature functions
in the log-linear models, including a joint prob-
1A version of PORTAGE is made available by the NRC to
Canadian universities for research and education purposes.
ability estimate, a standard frequency-based
conditional probability estimate, and variants
thereof based on different smoothing methods
(Foster et al, 2006).
? A 4-gram language model trained on the com-
bined Europarl and News Commentary target-
language corpora.
? A 3-gram adapted language model: this is
trained on a mini-corpus of test-relevant target-
language sentences, extracted from the training
material using standard information retrieval
techniques.
? A 5-gram truecasing model, trained on the
combined Europarl and News Commentary
target-language corpora.
2.3 Training data
Ideally, the training material for the post-editing
layer of our system should consist in a corpus of
text in two parallel versions: on the one hand, raw
machine translation output, and on the other hand,
manually post-edited versions of these translations.
This is the type of data that was used in the initial
study of Simard et al (2007).
Unfortunately, this sort of training data is seldom
available. Instead, we propose using training ma-
terial derived directly from standard, source-target
parallel corpora. The idea is to translate the source
portion of the parallel corpus into the target lan-
guage, using the rule-based MT component. The
post-editing component can then be trained using
this translation as ?source? training material, and the
existing target portion of the parallel corpus as ?tar-
get? training material. Note how this sort of data
is subtly different from the data used by Simard et
al.: there, the ?target? text was dependent on the
?source?, in the sense that it was produced by manu-
ally post-editing the machine translation; here, the
two can be said to be independent, in the sense
that both ?source? and ?target? were produced inde-
pendently by man and machine (but from the same
?real? source, of course). It was one of the initial
motivations of the current work to verify to what ex-
tent the performance of the APE approach is affected
by using two different translations (human and ma-
204
en ? fr fr ? en
Europarl (>32M words/language)
SYSTRAN 23.06 20.11
PORTAGE 31.01 30.90
SYSTRAN+PORTAGE 31.11 30.61
News Commentary (1M words/language)
SYSTRAN 24.41 18.09
PORTAGE 25.98 25.17
SYSTRAN+PORTAGE 28.80 26.79
Table 1: System performances on WMT-06 test. All
figures are single-reference BLEU scores, computed
on truecased, detokenized translations.
chine) instead of two versions of the same transla-
tion (raw MT versus post-edited MT).
We concentrated our efforts on the English-
French language pair. For each translation direc-
tion, we prepared two systems: one for the Eu-
roparl domain, and one for the News Commentary
domain. The two systems have almost identical
configurations (phrase tables, log-linear model fea-
tures, etc.); the only differences between the two
are the adapted language model, which is computed
based on the specific text to be translated and the
parameters of the log-linear models, which are opti-
mized using domain-specific development sets. For
the Europarl domain system, we used the dev2006
and devtest2006 data sets, while for the News Com-
mentary, we used the nc-dev2007. Typically, the
optimization procedure will give higher weights to
Europarl-trained phrase tables for the Europarl do-
main systems, and inversely for the News Commen-
tary domain systems.
3 Experimental Results
We computed BLEU scores for all four systems on
the 2006 test data (test2006 for the Europarl do-
main and nc-devtest2007 for the News Commen-
tary). The results are presented in Table 1. As points
of comparison, we also give the scores obtained by
the SYSTRAN systems on their own (i.e. without a
post-editing layer), and by the PORTAGE MT sys-
tems on their own (i.e. translating directly source
into target).
The first observation is that, as was the case
in the Simard et al study, post-editing (SYS-
TRAN+PORTAGE lines) very significantly in-
creases the BLEU scores of the rule-based system
(SYSTRAN lines). This increase is more spectacu-
lar in the Europarl domain and when translating into
English, but it is visible for all four systems.
For the News Commentary domain, the APE
strategy (SYSTRAN+PORTAGE lines) clearly out-
performs the direct SMT strategy (PORTAGE lines):
translating into English, the gain exceeds 1.5 BLEU
points, while for French, it is close to 3 BLEU
points. In contrast, for the Europarl domain, both ap-
proaches display similar performances. Let us recall
that the News Commentary corpus contains less than
50K sentence pairs, totalling a little over one mil-
lion words in each language. With close to 1.3 mil-
lion sentence pairs, the Europarl corpus is almost 30
times larger. Our results therefore appear to confirm
one of the conjectures of the Simard et al study:
that APE is better suited for domains with limited
quantities of available training data. To better un-
derstand this behavior, we trained series of APE and
SMT systems on the Europarl data, using increas-
ing amounts of training data. The resulting learning
curves are presented in Figure 1.2
As observed in the Simard et al study, while both
the SMT and APE systems improve quite steadily
with more data (note the logarithmic scale), SMT
appears to improve more rapidly than APE. How-
ever, there doesn?t seem to be a clear ?crossover?
point, as initially conjectured by Simard et al In-
stead, SMT eventually catches up with APE (any-
where between 100K and 1M sentence pairs), be-
yond which point both approaches appear to be more
or less equivalent. Again, one impressive feature
of the APE strategy is how little data is actually re-
quired to improve upon the rule-based system upon
which it is built: around 5000 sentence pairs for
English-to-French, and 2000 for French-to-English.
4 Conclusions
We have presented a combination MT system based
on a post-editing strategy, in which a statistical
phrase-based system corrects the output of a rule-
based translation system. Experiments confirm the
2The systems used for this experiment are simplified ver-
sions of those described in Section 2, using only one phrase
table, a trigram language model and no rescoring; furthermore,
they were optimized and tested on short sentences only.
205
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 1  10  100  1000
BL
EU
 s
co
re
Training sentences (x 1000)
English to French
SYSTRAN
PORTAGE
SYSTRAN + PORTAGE
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 1  10  100  1000
BL
EU
 s
co
re
Training sentences (x 1000)
French to English
SYSTRAN
PORTAGE
SYSTRAN + PORTAGE
Figure 1: BLEU scores on Europarl data under increasing amounts of training data for PORTAGE SMT
alone and SYSTRAN MT with PORTAGE APE.
conclusions of earlier studies: not only can phrase-
based post-editing significantly improve the out-
put of a rule-based MT system (in terms of BLEU
score), but when training data is scarce, it also out-
performs a direct phrase-based MT strategy. Fur-
thermore, our results indicate that the training data
for the post-editing component does not need to be
manually post-edited translations, it can be gener-
ated from standard parallel corpora. Finally, our ex-
periments show that while post-editing is most effec-
tive when little training data is available, it remains
competitive with phrase-based translation even with
much larger amounts of data.
This work opens the door to a number of lines of
investigation. For example, it was mentioned earlier
that phrase-based APE could be seen as a form of au-
tomatic domain-adaptation for rule-based methods.
One thing we would like to verify is how this ap-
proach compares to the standard ?lexical customiza-
tion? method proposed by most rule-based MT ven-
dors. Also, in the experiments reported here, we
have used identical configurations for the APE and
direct SMT systems. However, it might be possible
to modify the phrase-based system so as to better
adapt it to the APE task. For example, it could be
useful for the APE layer to ?look? at the real source-
language text, in addition to the MT output it is post-
editing. Finally, we have so far considered the front-
end rule-based system as a ?black box?. But in the
end, the real question is: Which part of the rule-
based processing is really making things easier for
the phrase-based post-editing layer? Answering this
question will likely require diving into the internals
of the rule-based component. These are all direc-
tions that we are currently pursuing.
Acknowledgements
This work was done as part of a collaboration with
SYSTRAN S.A. Many thanks go to Jean Senellart,
Jens Stephan, Dimitris Sabatakakis and all those
people behind the scene at SYSTRAN.
References
L. Dugast, J. Senellart, and P. Koehn. 2007. StatisticalPost-Edition on SYSTRAN Rule-Based TranslationSystem. In Proceedings of the Second Workshop OnStatistical Machine Translation, Prague, Czech Re-
public.
G. Foster, R. Kuhn, and H. Johnson. 2006. PhrasetableSmoothing for Statistical Machine Translation. InProceedings of EMNLP 2006, pages 53?61, Sydney,
Australia.
F. Sadat, H. Johnson, A. Agbago, G. Foster, R. Kuhn,
J. Martin, and A. Tikuisis. 2005. PORTAGE: APhrase-Based Machine Translation System. In Pro-ceedings of the ACL Workshop on Building and UsingParallel Texts, pages 129?132, Ann Arbor, USA.
M. Simard, C. Goutte, and P. Isabelle. 2007. Sta-
tistical Phrase-Based Post-Editing. In Human Lan-guage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Proceedings of the Main Con-ference, pages 508?515, Rochester, USA.
N. Ueffing, M. Simard, S. Larkin, and H. Johnson. 2007.NRC?s PORTAGE system for WMT 2007. In Pro-ceedings of the Second Workshop On Statistical Ma-chine Translation, Prague, Czech Republic.
206
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 242?249,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Stabilizing Minimum Error Rate Training
George Foster and Roland Kuhn
National Research Council Canada
first.last@nrc.gc.ca
Abstract
The most commonly used method for
training feature weights in statistical ma-
chine translation (SMT) systems is Och?s
minimum error rate training (MERT) pro-
cedure. A well-known problemwith Och?s
procedure is that it tends to be sensitive
to small changes in the system, particu-
larly when the number of features is large.
In this paper, we quantify the stability
of Och?s procedure by supplying different
random seeds to a core component of the
procedure (Powell?s algorithm). We show
that for systems with many features, there
is extensive variation in outcomes, both on
the development data and on the test data.
We analyze the causes of this variation and
propose modifications to the MERT proce-
dure that improve stability while helping
performance on test data.
1 Introduction
Most recent approaches in SMT, eg (Koehn et al,
2003; Chiang, 2005), use a log-linear model to
combine probabilistic features. Minimum Error-
Rate Training (MERT) aims to find the set of log-
linear weights that yields the best translation per-
formance on a development corpus according to
some metric such as BLEU. This is an essen-
tial step in SMT training that can significantly
improve performance on a test corpus compared
to setting weights by hand. MERT is a difficult
problem, however, because calculating BLEU as a
function of log-linear weights requires decoding,
which is an expensive operation. Moreover, be-
cause this function is not differentiable, efficient
gradient-based optimization algorithms cannot be
used.
Och?s procedure is the most widely-used ver-
sion of MERT for SMT (Och, 2003). To reduce
computational cost, it relies on the key technique
of optimizing weights over n-best lists of transla-
tion hypotheses rather than over all possible hy-
potheses. This allows the most probable hypoth-
esis under a given set of weights?and the corre-
sponding BLEU score?to be found by enumer-
ating n-best entries rather than decoding. Some
variant on Powell?s algorithm (Press et al, 2002)
is typically used to maximize BLEU in this set-
ting. The n-best lists are constructed by alternat-
ing decoding and BLEU maximization operations:
decoding adds new hypotheses to the current lists,
then BLEU is maximized over the lists to find new
best weights for the subsequent decoding step, etc.
This process continues until no new hypotheses
are found.
Och?s procedure works well in practice, usually
converging after 10?20 calls to the decoder, far
fewer than would be required to maximize BLEU
directly with a general-purpose optimization algo-
rithm. However, it tends to be sensitive to small
changes in the system, particularly for large fea-
ture sets. This is a well-known problem with
Och?s procedure (Och et al, 2004). It makes it
difficult to assess the contribution of features, be-
cause the measured gain in performance due to a
new feature can depend heavily on the setting of
some apparently unrelated parameter such as the
size of n-best list used. Features with the poten-
tial for statistically significant gains may be re-
jected because Och?s procedure failed to find good
weights for them.
In this paper we attempt to quantify the stabil-
ity of Och?s procedure under different conditions
by measuring the variation in test-set scores across
different random seeds used with Powell?s algo-
rithm. We show that there is extensive variation
for large feature sets, and that it is due to two main
factors: the occasional failure of Och?s procedure
to find a good maximum on the development set,
and the failure of some maxima to generalize to
242
the test set. We analyze the causes of each of these
problems, and propose solutions for improving the
stability of the overall procedure.
2 Previous Work
One possible approach to estimating log-linear
weights on features is to dispense with the n-best
lists employed by Och?s procedure and, instead,
to optimize weights by directly accessing the de-
coder. The disadvantage of this approach is that
far more iterations of decoding of the full devel-
opment set are required. In (Zens and Ney, 2004)
the downhill simplex method is used to estimate
the weights; around 200 iterations are required for
convergence to occur. However, each iteration is
unusually fast, because only monotone decoding
is permitted (i.e., the order of phrases in the tar-
get language mirrors that in the source language).
Similarly, Cettolo and Federico (2004) apply the
simplex method to optimize weights directly using
the decoder. In their experiments on NIST 2003
Chinese-English data, they found about 100 iter-
ations of decoding were required. Although they
obtained consistent and stable performance gains
for MT, these were inferior to the gains yielded
by Och?s procedure in (Och, 2003). Taking Och?s
MERT procedure as a baseline, (Zens et al, 2007)
experiment with different training criteria for SMT
and obtain the best results for a criterion they call
?expected BLEU score?.
Moore and Quirk (2008) share the goal under-
lying our own research: improving, rather than
replacing, Och?s MERT procedure. They focus
on the step in the procedure where the set of fea-
ture weights optimizing BLEU (or some other MT
metric) for an n-best list is estimated. Typically,
several different starting points are tried for this
set of weights; often, one of the starting points is
the best set of weights found for the previous set
of n-best hypotheses. The other starting points are
often chosen randomly. In this paper, Moore and
Quirk look at the best way of generating the ran-
dom starting points; they find that starting points
generated by a random walk from previous max-
ima are superior to those generated from a uni-
form distribution. The criterion used throughout
the paper to judge the performance of MERT is the
BLEU score on the development test set (rather
than, for instance, the variance of that score, or
the BLEU score on held-out test data). Another
contribution of the paper is ingenious methods for
pruning the set of n-best hypotheses at each itera-
tion.
Cer et al(2008) also aim at improving Och?s
MERT. They focus on the search for the best set
of weights for an n-best list that follows choice
of a starting point. They propose a modified ver-
sion of Powell?s in which ?diagonal? directions
are chosen at random. They also modify the ob-
jective function used by Powell?s to reflect the
width of the optima found. They are able to show
that their modified version of MERT outperforms
both a version using Powell?s, and a more heuris-
tic search algorithm devised by Philipp Koehn
that they call Koehn Coordinate Descent, as mea-
sured on the development set and two test data
sets. (Duh and Kirchhoff, 2008) ingeniously uses
MERT as a weak learner in a boosting algorithm
that is applied to the n-best reranking task, with
good results (a gain of about 0.8 BLEU on the test
set).
Recently, some interesting work has been done
on what might be considered a generalization of
Och?s procedure (Macherey et al, 2008). In this
generalization, candidate hypotheses in each iter-
ation of the procedure are represented as lattices,
rather than as n-best lists. This makes it possi-
ble for a far greater proportion of the search space
to be represented: a graph density of 40 arcs per
phrase was used, which corresponds to an n-best
size of more than two octillion (2 ? 1027) entries.
Experimental results for three NIST 2008 tasks
were very encouraging: though BLEU scores for
the lattice variant of Och?s procedure did not typ-
ically exceed those for the n-best variant on de-
velopment data, on test data the lattice variant out-
performed the n-best approach by between 0.6 and
2.5 BLEU points. The convergence behaviour of
the lattice variant was also much smoother than
that of the n-best variant. It would be interesting
to apply some of the insights of the current paper
to the lattice variant of Och?s procedure.
3 Och?s MERT Procedure
Och?s procedure works as follows. First the de-
coder is run using an initial set of weights to gen-
erate n best translations (usually around 100) for
each source sentence. These are added to exist-
ing n-best lists (initially empty). Next, Powell?s
algorithm is used to find the weights that maxi-
mize BLEU score when used to choose the best
hypotheses from the n-best lists. These weights
243
are plugged back into the decoder, and the pro-
cess repeats, nominally until the n-best lists stop
growing, but often in practice until some criterion
of convergence such as minimum weight change
is attained. The weights that give the best BLEU
score when used with the decoder are output.
The point of this procedure is to bypass di-
rect search for the weights that result in maxi-
mum BLEU score, which would involve decoding
using many different sets of weights in order to
find which ones gave the best translations. Och?s
procedure typically runs the decoder only 10?20
times, which is probably at least one order of mag-
nitude fewer than a direct approach. The main
trick is to build up n-best lists that are represen-
tative of the search space, in the sense that a given
set of weights will give approximately the same
BLEU score when used to choose the best hy-
potheses from the n-best lists as it would when de-
coding. By iterating, the algorithm avoids weights
that give good scores on the n-best lists but bad
ones with the decoder, since the bad hypotheses
that are scored highly by such weights will get
added to the n-best lists, thereby preventing the
choice of these weights in future iterations. Unfor-
tunately, there is no corresponding guarantee that
weights which give good scores with the decoder
but bad ones on the nbest lists will get chosen.
Finding the set of weights that maximizes
BLEU score over n-best lists is a relatively easy
problem because candidate weight sets can be
evaluated in time proportional to n (simply cal-
culate the score of each hypothesis according to
the current weight set, then measure BLEU on the
highest scoring hypothesis for each source sen-
tence). Powell?s algorithm basically loops over
each feature in turn, setting its weight to an op-
timum value before moving on.1 Och?s linemax
algorithm is used to perform this optimization effi-
ciently and exactly. However this does not guaran-
tee that Powell?s algorithm will find a global max-
imum, and so Powell?s is typically run with many
different randomly-chosen initial weights in order
to try to find a good maximum.
4 Experimental Setup
The experiments described here were carried out
with a standard phrase-based SMT system (Koehn
1It can also choose to optimize linear combinations of
weights in order to avoid ridges that are not aligned with the
original coordinates, which can be done just as easily.
corpus num sents num Chinese toks
dev1 1506 38,312
dev2 2080 55,159
nist04 1788 53,446
nist06 1664 41,798
Table 1: Development and test corpora.
et al, 2003) employing a log-linear combination
of feature functions. HMM and IBM2 models
were used to perform separate word alignments,
which were symmetrized by the usual ?diag-and?
algorithm prior to phrase extraction. Decoding
used beam search with the cube pruning algorithm
(Huang and Chiang, 2007).
We used two separate log-linear models for
MERT:
? large: 16 phrase-table features, 2 4-gram lan-
guage model features, 1 distortion feature,
and 1 word-count feature (20 features in to-
tal).
? small: 2 phrase-table features, 1 4-gram lan-
guage model feature, 1 distortion feature, and
1 word-count feature (5 features in total).
The phrase-table features for the large model were
derived as follows. Globally-trained HMM and
IBM2 models were each used to extract phrases
from UN and non-UN portions of the training cor-
pora (see below). This produced four separate
phrase tables, each of which was used to generate
both relative-frequency and ?lexical? conditional
phrase-pair probabilities in both directions (target
given source and vice versa). The two language
model features in the large log-linear model were
trained on the UN and non-UN corpora. Phrase-
table features for the small model were derived by
taking the union of the four individual tables, sum-
ming joint counts, then calculating relative fre-
quencies.
All experiments were run using the Chi-
nese/English data made available for NIST?s 2008
MT evaluation. This included approximately 5M
sentence pairs of data from the UN corpus, and
approximatel 4M sentence pairs of other mate-
rial. The English Gigaword corpus was not used
for language model training. Two separate devel-
opment corpora were derived from a mix of the
NIST 2005 evaluation set and some webtext drawn
from the training material (disjoint from the train-
ing set used). The evaluation sets for NIST 2004
244
cfg nist04 nist06
avg ? S avg ? S
S1 31.17 1.09 0.28 26.95 0.90 0.27
S2 31.44 0.22 0.07 27.38 0.71 0.19
L1 33.03 1.09 0.37 29.22 0.97 0.34
L2 33.37 1.49 0.49 29.61 2.14 0.66
Table 2: Test-set BLEU score variation with 10
different random seeds, for small (S) and large (L)
models on dev sets 1 and 2. The avg column gives
the average BLEU score over the 10 runs; ? gives
the difference between the maximum and mini-
mum scores, and S is the standard deviation.
and NIST 2005 corpora were used for testing. Ta-
ble 1 summarizes the sizes of the devtest corpora,
all of which have four reference translations.
5 Measuring the Stability of Och?s
Algorithm
To gauge the response of Och?s algorithm to small
changes in system configuration, we varied the
seed value for initializing the random number gen-
erator used to produce random starting points for
Powell?s algorithm. For each of 10 different seed
values, Och?s algorithm was run for a maximum of
30 iterations2 using 100-best lists. Table 2 shows
the results for the two different log-linear models
described in the previous section.
The two development sets exhibit a similar pat-
tern: the small models appear to be somewhat
more stable, but all models show considerable
variation in test-set BLEU scores. For the large
models, the average difference between best and
worst BLEU scores is almost 1.5% absolute, with
an average standard deviation of almost 0.5%.
Differences of as little as 0.35% are significant at
a 95% confidence level according to paired boot-
strap resampling tests on this data, so these varia-
tions are much too large to be ignored.
The variation in table 2 might result from Och?s
algorithm failing to maximize development-set
BLEU properly on certain runs. Alternatively, it
could be finding different maxima that vary in the
extent to which they generalize to the test sets.
Both of these factors appear to play a role. The
ranges of BLEU scores on the two development
corpora with the large models are 0.86 and 1.3 re-
spectively; the corresponding standard deviations
2Sufficient for effective convergence in all cases we
tested.
dev nist04 nist06 inter
? r ? r ?
dev1 0.18 0.42 -0.27 0.07 0.73
dev2 0.55 0.60 0.73 0.85 0.94
Table 3: Pearson (?) and Spearman rank (r) cor-
relation between dev-set and test-set BLEU scores
for the large log-linear model. The final column
shows nist04/nist06 correlation.
are 0.27 and 0.38. Different runs clearly have sig-
nificantly different degrees of success in maximiz-
ing BLEU.
To test whether the variation in development-
set BLEU scores accounts completely for the vari-
ation in test-set scores, we measured the correla-
tion between them. The results in table 3 show
that this varies considerably across the two de-
velopment and test corpora. Although the rank
correlation is always positive and is in some
cases quite high, there are many examples where
higher development-set scores lead to lower test-
set scores. Interestingly, the correlation between
the two test-set scores (shown in the last column of
the table) is much higher than that between the de-
velopment and test sets. Since the test sets are not
particularly similar to each other, this suggests that
some sets of log-linear weights are in fact overfit-
ting the development corpus.
5.1 Bootstrapping with Random Seeds
The results above indicate that the stability prob-
lems with Och?s MERT can be quite severe, es-
pecially when tuning weights for a fairly large
number of features. However, they also consti-
tute a baseline solution to these problems: run
MERT some number of times with different ran-
dom seeds, then choose the run that achieves the
highest BLEU score on a test set. Since test-
set scores are highly correlated, these weights are
likely to generalize well to new data. Applying
this procedure using the nist04 corpus to choose
weights yields a BLEU increase of 0.69 on nist06
compared to the average value over the 10 runs in
table 2; operating in the reverse direction gives an
increase of 0.37 on nist04.3
3These increases are averages over the increases on each
development set. This comparison is not strictly fair to the
baseline single-MERT procedure, since it relies on a test set
for model selection (using the development set would have
yielded gains of 0.25 for nist06 and 0.27 for nist04). How-
ever, it is fairly typical to select models (involving different
feature sets, etc) using a test set, for later evaluation on a
245
 29
 29.2
 29.4
 29.6
 29.8
 30
 30.2
 30.4
 30.6
 1  2  3  4  5  6  7  8  9  10
number of runs
NIST06 BLEU scores versus number of random runs
dev2
dev1
Figure 1: Results on the nist06 test corpus, using
nist04 to choose best weights from varying num-
bers of MERT runs, averaged over 1000 random
draws. The error bars indicate the magnitude of
the standard deviation.
An obvious drawback to this technique is that
it requires the expensive MERT procedure to be
run many times. To measure the potential gain
from using fewer runs, and to estimate the stability
of the procedure, we used a bootstrap simulation.
For each development set and each n from 1 to 10,
we randomly drew 1000 sets of n runs from the
data used for table 2, then recorded the behaviour
of the nist06 scores that corresponded to the best
nist04 score. The results are plotted in figure 1.
There is no obvious optimal point on the curves,
although 7 runs would be required to reduce the
standard deviation on dev2 (the set with the higher
variance) below 0.35. In the following sections
we evaluate some alternatives that are less com-
putationally expensive. The large model setting is
assumed throughout.
6 Improving Maximization
In this section we address the problem of improv-
ing the maximization procedure over the devel-
opment corpus. In general, we expect that being
able to consistently find higher maxima will lead
to lower variance in test-set scores. Previous work,
eg (Moore and Quirk, 2008; Cer et al, 2008), has
focused on improving the performance of Powell?s
algorithm. The degree to which this is effective de-
pends on how good an approximation the current
n-best lists are to the true search space. As illus-
second, blind, test set. A multi-MERT strategy could be nat-
urally incorporated into such a regime, and seems unlikely to
give rise to substantial bias.
A B C
Figure 2: True objective function (bold curve)
compared to n-best approximation (light curve).
Och?s algorithm can correct for false maxima like
B by adding hypotheses to n-best lists, but may
not find the true global maximum (C), converging
to local peaks like A instead.
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
BL
EU
iter
BLEU scores versus Och iteration
best run
worst run
Figure 3: Development-set BLEU scores after
each Och iteration for two different training runs
on the dev2 corpus.
trated in figure 2, it is possible for the true space to
contain maxima that are absent from the approxi-
mate (n-best) space. Figure 3 gives some evidence
that this happens in practice. It shows the evolu-
tion of decoder BLEU scores with iteration for the
best and worst runs for dev2. Although the worst
run explores a somewhat promising area at itera-
tion 7, it converges soon afterwards in a region that
gives lower true BLEU scores. This is not due to
a failure of Powell?s algorithm, since the scores on
the n-best lists rise monotonically in this range.
We explored various simple strategies for avoid-
ing the kind of local-maximum behaviour exhib-
ited in figure 3. These are orthogonal to improve-
ments to Powell?s algorithm, which was used in
its standard form. Our baseline implementation of
Och?s algorithm calls Powell?s three times start-
ing with each of the three best weight sets from
the previous iteration, then a certain number of
times with randomly-generated weights. The to-
tal number of Powell?s calls is determined by an
algorithm that tries to minimize the probability of
246
a new starting point producing a better maximum.4
The first strategy was simply to re-seed the ran-
dom number generator (based on a given global
seed value) for each iteration of Och?s algorithm.
Our implementation had previously re-used the
same ?random? starting points for Powell?s across
different Och iterations. This is arguably justifi-
able on the grounds that the function to be opti-
mized is different each time.
The second strategy was motivated by the ob-
servation that after the first several iterations of
Och?s algorithm, the starting point that leads to
the best Powell?s result is nearly always one of
the three previous best weight sets rather than a
randomly-generated set. To encourage the algo-
rithm to consider other alternatives, we used the
three best results from all previous Och?s itera-
tions. That is, on iteration n, Powell?s is started
with the three best results from iteration n?1, then
the three best from n?2, and so forth. If more than
3(n ? 1) points are required by the stopping al-
gorithm described above, then they are generated
randomly.
The final strategy is more explicitly aimed at
forcing the algorithm to cover a broader por-
tion of the search space. Rather than choosing
the maximum-BLEU results from Powell?s algo-
rithm for the subsequent decoding step, we choose
weight vectors that yield high BLEU scores and
are dissimilar from previous decoding weights.
Formally:
?? = argmax
??P
w rbleu(?) + (1? w) rdist(?),
where P is the set of all weight vectors returned
by Powell?s on the current iteration, rbleu(?) is
??s BLEU score divided by the highest score for
any vector in P , and rdist(?) is ??s distance to
previous weights divided by the largest distance
for any vector in P . Distance to previous weights
is measured by taking the minimum L2 distance
from ? to any of the decoding weight vectors used
during the previous m Och iterations.
Intuitively, the weight w that controls the im-
portance of BLEU score relative to novelty should
increase gradually as Och?s algorithm progresses
in order to focus the search on the best maxi-
4Whenever a new maximum is encountered, at least the
current number of new starting points must be tried before
stopping, with a minimum of 10 points in total. Experiments
where the total number of starts was fixed at 30 did not pro-
duce significantly different results.
mum found (roughly similar to simulated anneal-
ing search). To accomplish this, w is defined as:
w = 1? a/(iter + b),
where b ? 0 and a ? b + 1 are parameters that
control w?s decay, and iter is the current Och iter-
ation.
Each of the three strategies outlined above was
run using 10 random seeds with both development
corpora. The weight selection strategy was run
with two different sets of values for the a and b
parameters: a = 1, b = 1 and a = 5, b = 9. Each
assigns equal weight to BLEU score and novelty
on the first iteration, but under the first parameter-
ization the weight on novelty decays more swiftly,
to 0.03 by the final iteration compared to to 0.13.
The results are shown in table 4. The best strat-
egy overall appears to be a combination of all three
techniques outlined above. Under the a = 5,
b = 9, m = 3 parametrization for the final (weight
selection) strategy, this improves the development
set scores by an average of approximately 0.4%
BLEU compared to the baseline, while signifi-
cantly reducing the variation across different runs.
Performance of weight selection appears to be
quite insensitive to its parameters: there is no sig-
nificant difference between the a = 1, b = 1 and
a = 5, b = 9 settings. It is possible that further
tuning of these parameters would yield better re-
sults, but this is an expensive procedure; we were
also wary of overfitting. A good fallback is the
first two strategies, which together achieve results
that are almost equivalent to the final gains due to
weight selection.
7 Generalization
As demonstrated in section 5, better performance
on the development set does not necessarily lead
to better performance on the test set: two weight
vectors that give approximately the same dev-set
BLEU score can give very different test-set scores.
We investigated several vectors with this charac-
teristic from the experiments described above, but
were unable to find any intrinsic property that was
a good predictor of test-set performance, perhaps
due to the fact that the weights are scale invari-
ant. We also tried averaging BLEU over boot-
strapped samples of the development corpora, but
this was also not convincingly correlated with test-
set BLEU.
247
strategy dev avg ? S
baseline 1 22.64 0.87 0.27
2 19.11 1.31 0.38
re-seed 1 22.87 0.65 0.21
2 19.37 0.60 0.17
+history 1 22.99 0.43 0.15
2 19.44 0.35 0.11
+sel 1,1,3 1 23.12 0.59 0.19
2 19.53 0.38 0.13
+sel 5,9,3 1 23.11 0.42 0.13
2 19.46 0.44 0.14
Table 4: Performance of various strategies for im-
proving maximization on the dev corpora: base-
line is the baseline used in section 5; re-seed is
random generator re-seeding; history is accumu-
lation of previous best weights as starting point;
and sel a,b,m is the final, weight selection, strat-
egy described in section 6, parameterized by a, b,
and m. Strategies are applied cumulatively, as in-
dicated by the + signs.
An alternate approach was inspired by the reg-
ularization method described in (Cer et al, 2008).
In essence, this uses the average BLEU score from
the points close to a given maximum as a surro-
gate for the BLEU at the maximum, in order to
penalize maxima that are ?narrow? and therefore
more likely to be spurious. While Cer et aluse this
technique while maximizing along a single dimen-
sion within Powell?s algorithm, we apply it over
all dimensions with the vectors output from Pow-
ell?s. Each individual weight is perturbed accord-
ing to a normal distribution (with variance 1e-03),
then the resulting vector is used to calculate BLEU
over the n-best lists. The average score over 10
such perturbed vectors is used to calculate rbleu
in the weight-selection method from the previous
section.
The results from regularized weight selection
are compared to standard weight selection and to
the baseline MERT algorithm in table 5. Regu-
larization appears to have very little effect on the
weight selection approach. This does not neces-
sarily contradict the results of Cer et al since it is
applied in a very different setting. The standard
weight selection technique (in combination with
the re-seeding and history accumulation strate-
gies) gives a systematic improvement in average
test-set BLEU score over the baseline, although it
does not substantially reduce variance.
strategy dev test avg ? S
baseline 1 04 33.03 1.09 0.37
06 29.22 0.97 0.34
2 04 33.37 1.49 0.49
06 29.61 2.14 0.66
(+) sel 5,9,3 1 04 33.43 1.23 0.41
06 29.62 0.98 0.31
2 04 33.95 1.03 0.37
06 30.32 0.88 0.30
+ reg 10 1 04 33.36 1.45 0.49
06 29.56 1.25 0.39
2 04 33.81 0.94 0.28
06 30.17 1.21 0.35
Table 5: Performance of various MERT tech-
niques on the test corpora. (+) sel 5,9,3 is the same
configuration as +sel 5,9,3 in table 4; + reg 10
uses regularized BLEU within this procedure.
8 Conclusion
In this paper, we have investigated the stability
of Och?s MERT algorithm using different random
seeds within Powell?s algorithm to simulate the
effect of small changes to a system. We found
that test-set BLEU scores can vary by 1 percent
or more across 10 runs of Och?s algorithm with
different random seeds. Using a bootstrap analy-
sis, we demonstrate that an effective, though ex-
pensive, way to stabilize MERT would be to run it
many times (at least 7), then choose the weights
that give best results on a held-out corpus. We
propose less expensive simple strategies for avoid-
ing local maxima that systematically improve test-
set BLEU scores averaged over 10 MERT runs, as
well as reducing their variance in some cases. An
attempt to improve on these strategies by regular-
izing BLEU was not effective.
In future work, we plan to integrate improved
variants on Powell?s algorithm, which are orthog-
onal to the investigations reported here.
9 Acknowlegement
This material is partly based upon work supported
by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023. Any opinions, findings and conclu-
sions or recommendations expressed in this ma-
terial are those of the authors and do not neces-
sarily reflect the views of the Defense Advanced
Research Projects Agency (DARPA).
248
References
Daniel Cer, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Regularization and search for minimum
error rate training. In Proceedings of the ACL Work-
shop on Statistical Machine Translation, Columbus,
June. WMT.
Mauro Cettolo and Marcello Federico. 2004. Min-
imum error training of log-linear translation mod-
els. In International Workshop on Spoken Language
Translation, Kyoto, September.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Ann Ar-
bor, Michigan, July.
Kevin Duh and Katrin Kirchhoff. 2008. Beyond log-
linear models: Boosted minimum error rate training
for n-best re-ranking. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Columbus, Ohio, June.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics
(ACL), Prague, Czech Republic, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Ed-
uard Hovy, editor, Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 127?133, Edmonton, Alberta,
Canada, May. NAACL.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine transla-
tion. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Honolulu.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statisti-
cal machine translation. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING) 2008, Manchester, August.
Franz Josef Och, Daniel Gildea, and Sanjeev Khudan-
pur et al 2004. Final report of johns hopkins
2003 summer workshop on syntax for statistical ma-
chine translation (revised version). Technical report,
February 25.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Meeting of the Association for
Computational Linguistics (ACL), Sapporo, July.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
Recipes in C++. Cambridge University Press, Cam-
bridge, UK.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proceedings of Human Language Technol-
ogy Conference / North American Chapter of the
ACL, Boston, May.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Prague, Czech Re-
public.
249
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608?616,
Beijing, August 2010
Phrase Clustering for Smoothing TM Probabilities ? or, How to 
Extract Paraphrases from Phrase Tables  
1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford 
1National Research Council of Canada 
2University of Waterloo 
1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com 
 
Abstract 
This paper describes how to cluster to-
gether the phrases of a phrase-based sta-
tistical machine translation (SMT) sys-
tem, using information in the phrase table 
itself. The clustering is symmetric and 
recursive: it is applied both to source-
language and target-language phrases, 
and the clustering in one language helps 
determine the clustering in the other. The 
phrase clusters have many possible uses. 
This paper looks at one of these uses: 
smoothing the conditional translation 
model (TM) probabilities employed by 
the SMT system. We incorporated 
phrase-cluster-derived probability esti-
mates into a baseline loglinear feature 
combination that included relative fre-
quency and lexically-weighted condition-
al probability estimates. In Chinese-
English (C-E) and French-English (F-E) 
learning curve experiments, we obtained 
a gain over the baseline in 29 of 30 tests, 
with a maximum gain of 0.55 BLEU 
points (though most gains were fairly 
small). The largest gains came with me-
dium (200-400K sentence pairs) rather 
than with small (less than 100K sentence 
pairs) amounts of training data, contrary 
to what one would expect from the pa-
raphrasing literature. We have only be-
gun to explore the original smoothing 
approach described here.  
1 Introduction and Related Work 
The source-language and target-language ?phras-
es? employed by many statistical machine trans-
lation (SMT) systems are anomalous: they are 
arbitrary sequences of contiguous words ex-
tracted by complex heuristics from a bilingual 
corpus, satisfying no formal linguistic criteria. 
Nevertheless, phrase-based systems perform bet-
ter than word-based systems (Koehn 2010, pp. 
127-129). In this paper, we look at what happens 
when we cluster together these anomalous but 
useful entities.  
Here, we apply phrase clustering to obtain bet-
ter estimates for ?backward? probability P(s|t) 
and ?forward? probability P(t|s), where s is a 
source-language phrase, t is a target-language 
phrase, and phrase pair (s,t) was seen at least 
once in training data. The current work is thus 
related to work on smoothing P(s|t) and P(t|s) ? 
see (Foster et al, 2006). The relative frequency 
estimates for P(s|t) and P(t|s) are  
ttstsPRF /#),(#)|( = and stsstPRF /#),(#)|( = , 
where #(s,t) denotes the number of times phrase 
pair (s,t) was observed, etc. These estimates are 
typically smoothed with ?lexical? estimates 
found by breaking phrases s and t into words. 
We adopt a different idea, that of smoothing 
PRF(s|t) and PRF(t|s) with estimates obtained from 
phrases that have similar meanings to s and t. In 
our experiments, the two methods were com-
bined, yielding an improvement over lexical 
smoothing alone ? this indicates they provide 
complementary information. E.g., lexical esti-
mates don?t work well for non-compositional 
phrases like ?kick the bucket? - our method 
might cluster this phrase with ?die? and ?expire? 
and thus provide better smoothing. The research 
that comes closest to ours is the work of 
Schwenk et al (2007) on continuous space N-
gram models, where a neural network is em-
ployed to smooth translation probabilities. How-
ever, both Schwenk et al?s smoothing technique 
608
and the system to which it is applied are quite 
different from ours. 
Phrase clustering is also somewhat related to 
work on paraphrases for SMT. Key papers in this 
area include (Bannard and Callison-Burch, 2005), 
which pioneered the extraction of paraphrases 
from bilingual parallel corpora, (Callison-Burch 
et al, 2006) which showed that paraphrase gen-
eration could improve SMT performance, (Calli-
son-Burch, 2008) and (Zhao et al, 2008) which 
showed how to improve the quality of paraphras-
es, and (Marton et al, 2009) which derived pa-
raphrases from monolingual data using distribu-
tional information. Paraphrases typically help 
SMT systems trained on under 100K sentence 
pairs the most.  
The phrase clustering algorithm in this paper 
outputs groups of source-language and target-
language phrases with similar meanings: paraph-
rases. However, previous work on paraphrases 
for SMT has aimed at finding translations for 
source-language phrases in the system?s input 
that weren?t seen during system training. Our 
approach is completely useless in this situation: 
it only generates new information for target or 
source phrases that are already in the system?s 
phrase table. Thus, we find paraphrases for many 
of the source and target phrases that are in the 
phrase table, while the work cited above looks 
for paraphrases of source phrases that are not in 
the phrase table.  
Our work also differs from most work on pa-
raphrases in that information is extracted not 
from sources outside the SMT system (e.g., pivot 
languages or thesauri) but from the system?s 
phrase table. In this respect if no other, it is simi-
lar to Chiang?s classic work on hierarchical 
phrase-based systems (Chiang, 2005), though 
Chiang was mining a very different type of in-
formation from phrase tables. 
Because of all these differences between work 
on paraphrasing and the phrase clustering ap-
proach, both in terms of the input information 
and where they are best applied, we did not expe-
rimentally compare the two approaches.     
2 Deriving Conditional Probabilities 
from Phrase Clusters 
Given phrase clusters in the source and target 
languages, how would one derive estimates for 
conditional probabilities P(s|t) and P(t|s)? We 
assume that the clustering is ?hard?: each source 
phrase s belongs to exactly one cluster C(s), and 
each target phrase t belongs to exactly one 
cluster C(t). Some of these clusters will contain 
singleton phrases, and others will contain more 
than one phrase. Let ?#? denote the total number 
of observations in the training data associated 
with a phrase or phrase cluster. E.g., suppose the 
English cluster CS contains the three phrases 
?red?, ?dark red?, and ?burgundy?, with 50, 25, 
and 10 observations in the training data 
respectively ? then #(CS) = 85. Also, let #(CS,CT) 
be the number of co-occurrences in the training 
data of source-language cluster CS and target-
language cluster CT.  
The phrase-cluster-based probabilities PPC are: 
)(#
))(),((#
)(#
)(#
))(|)(())(|()|(
tC
tCsC
sC
s
tCsCPsCsPtsPPC
?=
?=
  (1) 
and 
)(#
))(),((#
)(#
)(#
))(|)(())(|()|(
sC
tCsC
tC
t
sCtCPtCtPstPPC
?=
?=
   (2) 
Note that the PPC will often be non-zero where 
the corresponding relative frequency estimates 
PRF were zero (the opposite can?t happen). Also, 
the PPC will be most useful where the phrase be-
ing conditioned on was seldom seen in the train-
ing data. If t was seen 1,000 times during train-
ing, the PRF(s|t) are reliable and don?t need 
smoothing; but if t was seen 6 times,  PPC(s|t) 
may yield valuable extra information. The same 
kind of argument applies to estimation of P(t|s). 
3 Clustering Phrases 
We used only information ?native? to phrase 
tables to cluster phrases. Two types of similarity 
metric between phrases or phrase clusters were 
employed: count-based metrics and edit-based 
metrics. The former are based on phrase co-
occurrence counts; the latter are based on the 
word sequences that make up the phrases. Each 
has its advantages. Count-based metrics can de-
duce from the similar translations of two phrases 
that they have similar meanings, despite dissimi-
larity between the two word sequences ? e.g., 
they can deduce that ?red? and ?burgundy? be-
long in the same cluster. However, these metrics 
are unreliable when total counts are low, since 
phrase co-occurrences are determined by a noisy 
alignment process. Edit-based metrics are inde-
pendent of how often phrases were observed. 
However, sometimes they can be fooled by 
phrases that have similar word sequences but 
different meanings (e.g., ?the dog bit the man? 
609
and ?the man bit the dog?, or ?walk on the 
beach? and ?don?t walk on the beach?). In our 
experiments, we used a combination of count-
based and edit-based metrics to cluster phrases 
(by simply multiplying the metrics together). 
However, we invested most of our effort in per-
fecting the count-based component: our edit-
based metric was fairly na?ve.  
If we rely mainly on count-based similarity 
between phrases to cluster them, and this kind of 
similarity is most reliable when phrases have 
high counts, yet we need phrase-cluster-based 
estimates most for phrases with low counts, 
aren?t we carrying out clustering on the phrases 
that need it least? Our hope was that there is a 
class of phrases with intermediate counts (e.g., 
with 3-15 observations in the training data) that 
can be clustered reliably, but still benefit from 
phrase-cluster-based probability estimates.  
3.1 Count-based clustering: overview  
Figure 1 shows count-based phrase clustering. 
One first arbitrarily picks a language (either 
source or target) and then clusters together some 
of the phrases in that language. One then switch-
es to the other language and clusters phrases in 
that language, then switches back to the first one, 
and so on until enough clustering has taken place.  
Each phrase or phrase cluster is represented by 
the vector of its co-occurrence counts. To calcu-
late the similarity between two phrase clusters, 
one first normalizes their count vectors. At the 
top of Figure 1, source phrase s1 occurred 9 
times: 7 times aligned with target phrase t1, 2 
times aligned with t4. For source similarity com-
putation, the entry for (s1,t1) is normalized to 7/9 
= 0.78 and the entry for (s1,t4) is normalized to 
2/9 = 0.22 (these normalized values are shown in 
brackets and italics after the counts).  
The two most similar normalized vectors at 
the top of Figure 1 are those associated with 
phrases s1 and s2. These phrases are merged by 
adding corresponding counts, yielding a new 
vector associated with the new phrase cluster {s1, 
s2}. In real life, one would now do more source-
language clustering on the source language side; 
in this example, we immediately proceed to tar-
get-language clustering (carried out in target lan-
guage space). Note that the target similarity cal-
culations are affected by the previous source 
clustering (because s1 and s2 are now 
represented by the same coordinate, t3 and t4 are 
now closer than they were in the initial table). In 
this manner, we can iterate back and forth be-
tween the two languages. The final output is a 
table of joint phrase cluster counts, which is used 
to estimate the PPC (see previous section).   
3.2 Count-based clustering: details 
Count-based similarity is computed as follows:   
1. Phrase alignment is a noisy process, so 
we first apply a transformation analogous 
to tf-idf in information retrieval (Salton 
and McGill, 1986) to phrase cluster 
 
Figure 1: Example of phrase clustering 
 
610
counts. For source similarity computation, 
each co-occurrence count #(CS,CT) be-
tween source cluster CS and target cluster 
CT is multiplied by a factor that reflects 
the information content of CT. Let 
#diff(CS) be number of clusters on the 
source side, and let #[CT>0] for a par-
ticular target cluster CT be the number of 
source clusters CS that co-occur with CT. 
Then let 
])0[/#)(log(#),(#),('# >?= TSTSTS CCdiffCCCC .   
Similarly, for target similarity computa-
tion, let 
])0[/#)(log(#),(#),('# >?= STTSTS CCdiffCCCC .   
E.g., in source similarity computation, if 
CT co-occurs with all source clusters, its 
contribution will be set to zero (because 
it carries little information).  
2. We normalize by dividing each vector of 
tf-idf counts ),('# TS CC  by the total num-
ber of observations in the vector. 
3. We compute the similarity between each 
pair of tf-idf vectors using either the co-
sine measure (Salton and McGill, 1986) 
or one of a family of probabilistic metrics 
described below.  
4. We cluster together the most similar vec-
tors; this involves summing the unmodi-
fied counts #(CS,CT) of the vectors (i.e., 
the tf-idf transformation is only applied 
for the purposes of similarity calculation 
and is not retained).  
Now, we?ll describe the probabilistic metrics 
we considered. For a count vector of dimension 
D, u
 
= (u1, u2, ?, uD), define a function 
)/log(...)/log()( 11 ?? ?++?= i iDDi i uuuuuuI u . 
I(u) is a measure of how well the data in u are 
modeled by the normalized vector (u1/?iui,  ?, 
uD/?iui).  Thus, when two count vectors u and v 
are merged (by adding them) we have the follow-
ing measure of the loss in modeling accuracy:  
 
Probability Loss (PL): 
 )()()(),( vuvuvu +?+= IIIPL .   (3) 
 
However, if we choose merges with the lowest 
PL, we will usually merge only vectors with 
small counts. We are more interested in the aver-
age impact of a merge, so we define 
 
Average Probability Loss (APL):  
  )/())()()((),( ?? ++?+= i ii i vuIIIAPL vuvuvu . (4) 
In our initial experiments, APL worked better 
than PL. However, APL had a strange side-effect. 
Most of the phrase clusters it induced made intui-
tive sense, but there were typically three or four 
clusters with large numbers of observations on 
both language sides that grouped together phras-
es with wildly disparate meanings. Why does 
APL induce these ?monster clusters?? 
Consider two count vectors u and v. If ?iui is 
very big and ?ivi is small, then I(u) and I(u + v) 
will be very similar, and APL will be approx-
imately I(v) /[?iui + ?ivi ] which will be close to 
zero. Thus, the decision will probably be made to 
merge u and v, even if they have quite different 
semantics. The resulting cluster, whose counts 
are represented by u + v, is now even bigger and 
even more likely to swallow up other small count 
vectors in the next rounds of merging: it becomes 
a kind of black hole.  
To deal with this problem, we devised another 
metric. Let 
)/log(...)/log()|( 11 ?? ?++?= i iDDi i vvuvvuI vu . 
This is a measure of how well the counts in v 
predict the distribution of counts in u. Then let  
 
Maximum Average Probability Loss (MAPL):  
))|()(,)|()(max(),( ??
+?+?
=
i ii i
v
II
u
IIMAPL vuvvvuuuvu
 .(5) 
 
The first term inside the maximum indicates the 
average probability loss for an observation in u 
when it is modeled by u+v instead of u; similarly, 
the second term indicates the average probability 
loss for an observation in v. If we merge vector 
pairs with the lowest values of MAPL, we will 
never merge vectors in a way that will cause a 
large loss to either of the two parents.  
In practice, we found that all these metrics 
worked better when multiplied by the Dice coef-
ficient based distance. For u and v, this is 
||||
||21),(
vu
vu
vu
+
??
?=Dice , where ?|u|? means 
the number of non-zero count entries in u, and 
?| vu ? |? is the number of count entries that are 
non-zero in u and v. 
3.3 Edit-based similarity 
In most of our experiments, count-based metrics 
were combined with edit-based metrics; we put 
little effort into optimizing the edit metrics. Let 
MCWS stand for ?maximum common word se-
quence?. For phrases p1 and p2, we define  
611
)()(
)),((21),(
21
21
21 plenplen
ppMCWSlenppEdit
+
?
?= .        (6) 
where len() returns the number of  words. This 
metric doesn?t take word identities into account; 
in future work, we may weight differences in-
volving content words more heavily.  
We also defined an edit-based metric for dis-
tance between phrase clusters. Let cluster 1 have 
phrases ?red? (10); ?burgundy? (5); ?resembling 
scarlet? (2) and cluster 2 have ?dark burgundy? 
(7); ?scarlet? (3) (number of observations in 
brackets). What is the edit distance between clus-
ters 1 and 2? We defined the distance as that be-
tween the two phrases with the most observa-
tions in each cluster. Thus, distance between 
clusters 1 and 2 would be Edit(?red?, ?dark bur-
gundy?)=1.0. Other definitions are possible.  
3.4 Examples of phrase clusters 
Figure 2 shows an English phrase cluster learned 
during C-E experiments by a metric combining 
count-based and edit-based information. Each 
phrase is followed by its count in brackets; we 
don?t show phrases with low counts. Since our 
edit distance sees words as atoms (it doesn?t 
know about morphology), the phrases containing 
?emancipating? were clustered with phrases con-
taining ?emancipation? based on count informa-
tion, rather than because of the common stem.  
Figure 3 shows part of a French phrase cluster 
learned during F-E experiments by the same 
mixed metric. The surface forms are quite varied, 
but most of the phrases mean ?to assure or to 
guarantee that something will happen?. An inter-
esting exception is ?pas faire? ? it means not to 
do something (?pas? is negative). This illustrates 
why we need a better edit distance that heavily 
weights negative words.  
 
emancipating (247), emancipate 
(167), emancipate our (73), emanci-
pating thinking (67), emancipate 
our minds (46), further emancipate 
(45), emancipate the (38), emanci-
pate the mind (38), emancipating 
minds (33), emancipate their (32), 
emancipate their minds (27), eman-
cipating our minds (24), emancipat-
ing our (21), emancipate our mind 
(21), further emancipate our (19), 
emancipate our thinking (14), fur-
ther emancipate their (11), emanci-
pating the minds (9), emancipate 
thinking (8), unfettering (8) ...  
 
Figure 2: partial English phrase cluster 
 
garantir que (64), assurer que 
(46), veiller ? ce que (27), afin 
de garantir (24), faire en sorte 
(19), de garantir que (16), afin de 
garantir que (14), faire des (14), 
de veiller ? ce (14), s' assurer 
que (13), de veiller ? ce que (13), 
pour garantir que (13), de faire en 
sorte (8), de faire en sorte que 
(7), ? garantir que (6), pas faire 
(5), de veiller (5)? 
 
Figure 3:  partial French phrase cluster 
4 Experiments  
We carried out experiments on a standard one-
pass phrase-based SMT system with a phrase 
table derived from merged counts of symme-
trized IBM2 and HMM alignments; the system 
has both lexicalized and distance-based distor-
tion components (there is a 7-word distortion 
limit) and employs cube pruning (Huang and 
Chiang, 2007). The baseline is a loglinear feature 
combination that includes language models, the 
distortion components, relative frequency esti-
mators PRF(s|t) and PRF(t|s) and lexical weight 
estimators PLW(s|t) and PLW(t|s). The PLW() com-
ponents are based on (Zens and Ney, 2004); Fos-
ter et al (2006) found this to be the most effec-
tive lexical smoothing technique. The phrase-
cluster-based components PPC(s|t) and PPC(t|s) 
are incorporated as additional loglinear feature 
functions. Weights on feature functions are 
found by lattice MERT (Macherey et al, 2008).  
4.1 Data 
We evaluated our method on C-E and F-E tasks. 
For each pair, we carried out experiments on 
training corpora of different sizes. C-E data were 
from the NIST1 2009 evaluation; all the allowed 
bilingual corpora except the UN corpus, Hong 
Kong Hansard and Hong Kong Law corpus were 
used to estimate the translation model. For C-E, 
we trained two 5-gram language models: the first 
on the English side of the parallel data, and the 
second on the English Gigaword corpus. 
Our C-E development set is made up mainly 
of data from the NIST 2005 test set; it also in-
cludes some balanced-genre web-text from the 
NIST training material. Evaluation was per-
formed on the NIST 2006 and 2008 test sets. 
Table 1 gives figures for training, development 
and test corpora for C-E tasks; |S| is the number 
of sentences, and |W| is the number of words. 
There are four references for dev and test sets. 
                                               
1
 http://www.nist.gov/speech/tests/mt 
612
   Chi Eng 
All parallel 
Train 
|S| 3.3M 
|W| 68.2M 66.5M 
Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics for Chinese-to-English tasks. 
 
 
   Fre Eng 
Train Europarl |S| 1.6M 
|W| 51.3M 46.6M 
Dev 2008 |S| 2,051 
Test 2009 |S| 2,525 
2010 |S| 2,489 
GigaFrEn |S| - 22.5M 
 
Table 2: Statistics for French-to-English tasks. 
 
 
Lang (#sent) C-E (3.3M) F-E (1.6M) 
  #count-1  #other  #count-1  #other 
 
 
Src 
Before 
clustering 
11.3M 5.7M 28.1M 21.2M 
After  
clustering 
11.3M 5.3M 28.1M 19.3M 
#clustered 0 0.4M 0 1.9M 
 
 
Tgt 
Before 
clustering 
11.9M 6.0M 25.6M 20.4M 
After  
clustering 
11.9M 5.6M 25.6M 18.5M 
#clustered 0 0.4M 0 1.9M 
 
Table 3: # phrase classes before & after clustering. 
 
For F-E tasks, we used WMT 20102 F-E track 
data sets. Parallel Europarl data are used for 
training; WMT Newstest 2008 set is the dev set, 
and WMT Newstest 2009 and 2010 are the test 
sets. One reference is provided for each source 
input sentence. Two language models are used in 
this task: one is the English side of the parallel 
data, and the second is the English side of the 
GigaFrEn corpus. Table 2 summarizes the train-
ing, development and test corpora for F-E tasks. 
4.2 Amount of clustering and metric 
For both C-E and E-F, we assumed that phrases 
seen only once in training data couldn?t be clus-
tered reliably, so we prevented these ?count 1? 
phrases from participating in clustering. The key 
                                               
2
 http://www.statmt.org/wmt10/ 
clustering parameter is the number of merge op-
erations per iteration, given as a percentage of 
the number of potential same-language phrase 
pairs satisfying a simple criterion (some overlap 
in translations to the other language). Prelimi-
nary tests involving the FBIS corpus (about 8% 
of the C-E data) caused us to set this parameter at 
5%. For C-E, we first clustered Chinese with this 
5% value, then English with the same amount. 
For F-E, we first clustered French, then English, 
using 5% in both cases.  
Table 3 shows the results. Only 2-4% of the 
total phrases in each language end up in a cluster 
(that?s 6.5-9% of eligible phrases, i.e., of phrases 
that aren?t ?count 1?). However, about 20-25% 
of translation probabilities are smoothed for both 
language pairs. Based on these preliminary tests, 
we decided to use MAPLDiceEdit ??  
( DMAPLEdit ? ) as our metric (though 
CosineEdit ?  was a close runner-up).  
4.3 Results and discussion 
Our evaluation metric is IBM BLEU (Papineni et 
al., 2002), which performs case-insensitive 
matching of n-grams up to n = 4. Our first expe-
riment evaluated the effects of the phrase cluster-
ing features given various amounts of training 
data. Figure 4 gives the BLEU score improve-
ments for the two language pairs, with results for 
each pair averaged over two test sets (training 
data size shown as #sentences). The improve-
ment is largest for medium amounts of training 
data. Since the F-E training data has more words 
per sentence than C-E, the two peaks would have 
been closer together if we?d put #words on the x 
axis: improvements for both tasks peak around 6-
8 M English words. For more details, refer to 
Table 4 and Table 5. The biggest improvement 
is 0.55 BLEU for the NIST06 test. More impor-
tantly, cluster features yield gains in 29 of 30 
experiments. Surprisingly, a reviewer asked if 
we?d done significance tests on the individual 
results shown in Tables 4 and 5. Most likely, 
many of these individual results are insignificant, 
but so what? Based on the tables, the probability 
of the null hypothesis that our method has no 
effect is equivalent to that of tossing a fair coin 
30 times and getting 29 heads (if we adopt an 
independence approximation).  
In the research on paraphrases cited earlier, 
paraphrases tend to be most helpful for small 
amounts of training data. By contrast, our 
approach seems to be most helpful for medium 
amounts of training data (200-400K sentence 
613
pairs). We attribute this to the properties of 
count-based clustering. When there is little 
training data, clustering is unreliable; when there 
is much data, clustering is reliable but unneeded, 
because most relative frequencies are well-
estimated. In between, phrase cluster probability 
estimates are both reliable and useful. 
 
 
 
Figure 4: Average BLEU improvement for C-E and 
F-E tasks (each averaged over two tests) vs. #training 
sent. 
 
Finally, we carried out experiments to see if 
some of our earlier decisions were correct. Were 
we right to use DMAPL instead of cosine as the 
count-based component of our metric? Experi-
ments with DMAPLEdit ?  vs. 
CosineEdit ? on 400K C-E (tested on NIST06 
and NIST08) and on 200K F-E (tested on News-
test2009 and 2010) showed a tiny advantage for 
DMAPLEdit ? of about 0.06 BLEU. So we 
probably didn?t make the wrong decision here 
(though it doesn?t matter much). Were we right 
to include the Edit component? Carrying out ana-
logous experiments with DMAPLEdit ? vs. 
DMAPL, we found that dropping Edit caused a 
loss of 0.1-0.2 BLEU for all four test sets. Here 
again, we made the right decision.  
In a final experiment, we allowed ?count 1? 
phrases to participate in clustering (using 
DMAPLEdit ? ). The resulting C-E system had 
somewhat more clustered phrases than the pre-
vious one (for both Chinese and English, about 
3.5% of phrases were in clusters compared to 
2.5% in the previous system). To our surprise, 
this led to a slight improvement in BLEU: the 
400K C-E system now yielded 30.25 on NIST06 
(up 0.09) and 23.88 on NIST08 (up 0.13). The F-
E system where ?count 1? clustering is allowed 
also had more phrases in clusters than the system 
where it?s prohibited (the former has just under 
10% of French and English phrases in clusters vs. 
 
Data size 
Nist06 Nist08 
Baseline +phrase-clustering Improv. Baseline +phrase-clustering Improv. 
25K 21.66 21.88 0.22 15.80 15.99 0.19 
50K 23.23 23.43 0.20 17.69 17.84 0.15 
100K 25.83 26.24 0.41 20.08 20.27 0.19 
200K 27.80 28.26 0.46 21.28 21.58 0.30 
400K 29.61 30.16 0.55 23.37 23.75 0.38 
800K 30.87 31.17 0.30 24.41 24.65 0.24 
1.6M 32.94 33.10 0.16 25.61 25.72 0.11 
3.3M 33.59 33.64 0.05 26.84 26.85 0.01 
 
Table 4: BLEU(%) scores for C-E with the various training corpora, including baseline results, results for with 
phrase clustering, and the absolute improvements. Corpus size is measured in sentences. 
 
 
Data size 
Newstest2009 Newstest2010 
Baseline +phrase-clustering Improv. Baseline +phrase-clustering Improv. 
25K 20.21 20.37 0.16 20.54 20.73 0.19 
50K 21.25 21.44 0.19 21.95 22.11 0.16 
100K 22.56 22.86 0.30 23.44 23.69 0.25 
200K 23.67 24.02 0.35 24.31 24.71 0.40 
400K 24.36 24.50 0.14 25.28 25.46 0.18 
800K 24.92 24.97 0.05 25.80 25.90 0.10 
1.6M 25.47 25.47 0.00 26.35 26.37 0.02 
 
Table 5: BLEU(%) scores for F-E with the various training corpora, including baseline results without phrase 
clustering feature, results for phrase clustering, and the absolute improvements. 
 
614
4% for the latter). For F-E, the 200K system al-
lowing ?count 1? clustering again yielded a 
slightly higher BLEU: 24.07 on Newstest2009 
and 24.76 on Newstest2010 (up 0.05 in both cas-
es). Thus, our decision not to allow ?count 1? 
phrases to participate in clustering in the Table 4 
and 5 experiments appears to have been a mis-
take. We suspect we can greatly improve han-
dling of ?count 1? phrases ? e.g., by weighting 
the Edit component of the similarity metric more 
heavily when assigning these phrases to clusters.  
5 Conclusion and Future Work 
We have shown that source-language and target-
language phrases in the phrase table can be clus-
tered, and that these clusters can be used to 
smooth ?forward? and ?backward? estimates 
P(t|s) and P(s|t), yielding modest but consistent 
BLEU gains over a baseline that included lexical 
smoothing. Though our experiments were done 
on a phrase-based system, this method could also 
be applied to hierarchical phrase-based SMT and 
syntactic SMT systems. There are several possi-
bilities for future work based on new applica-
tions for phrase clusters: 
? In the experiments above, we used 
phrase clusters to smooth P(t|s) and P(s|t) 
when the pair (s,t) was observed in train-
ing data. However, the phrase clusters 
often give non-zero probabilities for P(t|s) 
and P(s|t) when s and t were both in the 
training data, but didn?t co-occur. We 
could allow the decoder to consider such 
?invented? phrase pairs (s,t).  
? Phrase clusters could be used to con-
struct target language models (LMs) in 
which the basic unit is a phrase cluster 
rather than a word. For instance, a tri-
cluster model would estimate the proba-
bility of phrase p at time i as a function 
of its phrase cluster, Ci(p), and the two 
preceding phrase clusters Ci-1 and Ci-2: 
)|())(|()( 21 ???= iiii CCCfCfP ppp
.  
? Lexicalized distortion models could be 
modified so as to condition distortion 
events on phrase clusters.  
? We could build SMT grammars in which 
the terminals are phrases and the parents 
of terminals are phrase clusters.  
The phrase clustering algorithm described 
above could be improved in several ways: 
? In the above, the edit distance between 
phrases and between phrase clusters was 
crudely defined. If we improve edit dis-
tance, it will have an especially large 
impact on ?count 1? phrases, for which 
count-based metrics are unreliable and 
which are a large proportion of all phras-
es. The edit distance between two phras-
es weighted all words equally: preferably, 
weights for word substitution, insertion, 
or deletion would be learned from purely 
count-derived phrase clusters (content 
words and negative words might have 
heavier weights than other words). The 
edit distance between two phrase clusters 
was defined as the edit distance between 
the phrases with the most observations in 
each cluster. E.g., distance to the phrase 
cluster in Figure 2 is defined as the 
phrase edit distance to ?emancipating?. 
Instead, one could allow a cluster to be 
characterized by (e.g.) up to three phras-
es, and let distance between two clusters 
be the minimum or average pairwise edit 
distance between these characteristic 
phrases.  
? To cluster phrases, we only used infor-
mation derived from phrase tables. In fu-
ture, we could also use the kind of in-
formation used in work on paraphrases, 
such as the context surrounding phrases 
in monolingual corpora, entries in the-
sauri, and information from pivot lan-
guages. 
? The phrase clustering above was ?hard?: 
each phrase in either language belongs to 
exactly one cluster. We could modify 
our algorithms to carry out ?soft? clus-
tering. For instance, we could interpolate 
the probabilities associated with a phrase 
with probabilities from its neighbours.  
? Clustering is a primitive way of finding 
latent structure in the table of joint 
phrase counts. One could apply principal 
component analysis or a related algo-
rithm to this table. 
References 
C. Bannard and C. Callison-Burch. ?Paraphrasing 
with Bilingual Parallel Corpora?. Proc. ACL, pp. 
597-604, Ann Arbor, USA, June 2005.  
C. Callison-Burch, P. Koehn, and M. Osborne. ?Im-
proved Statistical Machine Translation Using Pa-
raphrases?. Proc. HLT/NAACL, pp. 17-24, New 
York City, USA, June 2006.  
615
C. Callison-Burch. ?Syntactic Constraints on Paraph-
rases Extracted from Parallel Corpora?. Proc. 
EMNLP, pp. 196-205, Honolulu, USA, October 
2008.  
D. Chiang. ?A hierarchical phrase-based model for 
statistical machine translation?. Proc. ACL, pp. 
263-270, Ann Arbor, USA, June 2005.  
G. Foster, R. Kuhn, and H. Johnson. ?Phrasetable 
smoothing for statistical machine translation?. 
Proc. EMNLP, pp. 53-61, Sydney, Australia, July 
2006.  
L. Huang and D. Chiang. ?Forest Rescoring: Faster 
Decoding with Integrated Language Models?. 
Proc. ACL, pp.  144-151, Prague, Czech Republic, 
June 2007.  
P. Koehn. 2010. Statistical Machine Translation. 
Cambridge University Press, Cambridge, UK.  
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 
?Lattice-based Minimum Error Rate Training for 
Statistical Machine Translation?. Proc. EMNLP, 
pp. 725-734, Honolulu, USA, October 2008.  
Y. Marton, C. Callison-Burch, and Philip Resnik. 
?Improved Statistical Machine Translation Using 
Monolingually-Derived Paraphrases?. Proc. 
EMNLP, pp. 381-390, Singapore, August 2009.  
K. Papineni, S. Roukos, T. Ward, and W. Zhu. ?Bleu: 
a method for automatic evaluation of machine 
translation?. Proc. ACL, pp. 311?318, Philadel-
phia, July 2002.  
G. Salton and M. McGill. 1986. Introduction to Mod-
ern Information Retrieval.  McGraw-Hill Inc., New 
York, USA. 
H. Schwenk, M. Costa-juss?, and J. Fonollosa. 
?Smooth Bilingual N-gram Translation?. Proc. 
Joint EMNLP/CoNLL, pp. 430-438, Prague, Czech 
Republic, June 2007.  
R.  Zens and H. Ney. ?Improvements in phrase-based 
statistical machine translation?. Proc. ACL/HLT, 
pp. 257-264, Boston, USA, May 2004. 
S. Zhao, H. Wang, T. Liu, and S. Li. ?Pivot Approach 
for Extracting Paraphrase Patterns from Bilingual 
Corpora?. Proc. ACL/HLT, pp. 780-788, Colum-
bus, USA, June 2008.  
 
616
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451?459,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Crown in Right of Canada.
Discriminative Instance Weighting for Domain Adaptation
in Statistical Machine Translation
George Foster and Cyril Goutte and Roland Kuhn
National Research Council Canada
283 Alexandre-Tache? Blvd
Gatineau, QC J8X 3X7
first.last@nrc.gc.ca
Abstract
We describe a new approach to SMT adapta-
tion that weights out-of-domain phrase pairs
according to their relevance to the target do-
main, determined by both how similar to it
they appear to be, and whether they belong to
general language or not. This extends previ-
ous work on discriminative weighting by us-
ing a finer granularity, focusing on the prop-
erties of instances rather than corpus com-
ponents, and using a simpler training proce-
dure. We incorporate instance weighting into
a mixture-model framework, and find that it
yields consistent improvements over a wide
range of baselines.
1 Introduction
Domain adaptation is a common concern when op-
timizing empirical NLP applications. Even when
there is training data available in the domain of inter-
est, there is often additional data from other domains
that could in principle be used to improve perfor-
mance. Realizing gains in practice can be challeng-
ing, however, particularly when the target domain is
distant from the background data. For developers
of Statistical Machine Translation (SMT) systems,
an additional complication is the heterogeneous na-
ture of SMT components (word-alignment model,
language model, translation model, etc.), which pre-
cludes a single universal approach to adaptation.
In this paper we study the problem of us-
ing a parallel corpus from a background domain
(OUT) to improve performance on a target do-
main (IN) for which a smaller amount of parallel
training material?though adequate for reasonable
performance?is also available. This is a standard
adaptation problem for SMT. It is difficult when IN
and OUT are dissimilar, as they are in the cases we
study. For simplicity, we assume that OUT is ho-
mogeneous. The techniques we develop can be ex-
tended in a relatively straightforward manner to the
more general case when OUT consists of multiple
sub-domains.
There is a fairly large body of work on SMT
adaptation. We introduce several new ideas. First,
we aim to explicitly characterize examples from
OUT as belonging to general language or not. Pre-
vious approaches have tried to find examples that
are similar to the target domain. This is less ef-
fective in our setting, where IN and OUT are dis-
parate. The idea of distinguishing between general
and domain-specific examples is due to Daume? and
Marcu (2006), who used a maximum-entropy model
with latent variables to capture the degree of speci-
ficity. Daume? (2007) applies a related idea in a
simpler way, by splitting features into general and
domain-specific versions. This highly effective ap-
proach is not directly applicable to the multinomial
models used for core SMT components, which have
no natural method for combining split features, so
we rely on an instance-weighting approach (Jiang
and Zhai, 2007) to downweight domain-specific ex-
amples in OUT. Within this framework, we use fea-
tures intended to capture degree of generality, in-
cluding the output from an SVM classifier that uses
the intersection between IN and OUT as positive ex-
amples.
Our second contribution is to apply instance
451
weighting at the level of phrase pairs. Sentence
pairs are the natural instances for SMT, but sen-
tences often contain a mix of domain-specific and
general language. For instance, the sentence Sim-
ilar improvements in haemoglobin levels were re-
ported in the scientific literature for other epoetins
would likely be considered domain-specific despite
the presence of general phrases like were reported
in. Phrase-level granularity distinguishes our work
from previous work by Matsoukas et al(2009), who
weight sentences according to sub-corpus and genre
membership.
Finally, we make some improvements to baseline
approaches. We train linear mixture models for con-
ditional phrase pair probabilities over IN and OUT
so as to maximize the likelihood of an empirical
joint phrase-pair distribution extracted from a de-
velopment set. This is a simple and effective alter-
native to setting weights discriminatively to maxi-
mize a metric such as BLEU. A similar maximum-
likelihood approach was used by Foster and Kuhn
(2007), but for language models only. For compar-
ison to information-retrieval inspired baselines, eg
(Lu? et al, 2007), we select sentences from OUT
using language model perplexities from IN. This
is a straightforward technique that is arguably bet-
ter suited to the adaptation task than the standard
method of treating representative IN sentences as
queries, then pooling the match results.
The paper is structured as follows. Section 2 de-
scribes our baseline techniques for SMT adaptation,
and section 3 describes the instance-weighting ap-
proach. Experiments are presented in section 4. Sec-
tion 5 covers relevant previous work on SMT adap-
tation, and section 6 concludes.
2 Baseline SMT Adaptation Techniques
Standard SMT systems have a hierarchical param-
eter structure: top-level log-linear weights are used
to combine a small set of complex features, inter-
preted as log probabilities, many of which have their
own internal parameters and objectives. The top-
level weights are trained to maximize a metric such
as BLEU on a small development set of approxi-
mately 1000 sentence pairs. Thus, provided at least
this amount of IN data is available?as it is in our
setting?adapting these weights is straightforward.
We focus here instead on adapting the two most im-
portant features: the language model (LM), which
estimates the probability p(w|h) of a target word w
following an ngram h; and the translation models
(TM) p(s|t) and p(t|s), which give the probability
of source phrase s translating to target phrase t, and
vice versa. We do not adapt the alignment procedure
for generating the phrase table from which the TM
distributions are derived.
2.1 Simple Baselines
The natural baseline approach is to concatenate data
from IN and OUT. Its success depends on the two
domains being relatively close, and on the OUT cor-
pus not being so large as to overwhelm the contribu-
tion of IN.
When OUT is large and distinct, its contribution
can be controlled by training separate IN and OUT
models, and weighting their combination. An easy
way to achieve this is to put the domain-specific
LMs and TMs into the top-level log-linear model
and learn optimal weights with MERT (Och, 2003).
This has the potential drawback of increasing the
number of features, which can make MERT less sta-
ble (Foster and Kuhn, 2009).
2.2 Linear Combinations
Apart fromMERT difficulties, a conceptual problem
with log-linear combination is that it multiplies fea-
ture probabilities, essentially forcing different fea-
tures to agree on high-scoring candidates. This is
appropriate in cases where it is sanctioned by Bayes?
law, such as multiplying LM and TM probabilities,
but for adaptation a more suitable framework is of-
ten a mixture model in which each event may be
generated from some domain. This leads to a linear
combination of domain-specific probabilities, with
weights in [0, 1], normalized to sum to 1.
Linear weights are difficult to incorporate into the
standard MERT procedure because they are ?hid-
den? within a top-level probability that represents
the linear combination.1 Following previous work
(Foster and Kuhn, 2007), we circumvent this prob-
lem by choosing weights to optimize corpus log-
likelihood, which is roughly speaking the training
criterion used by the LM and TM themselves.
1This precludes the use of exact line-maximization within
Powell?s algorithm (Och, 2003), for instance.
452
For the LM, adaptive weights are set as follows:
?? = argmax
?
?
w,h
p?(w, h) log
?
i
?ipi(w|h), (1)
where ? is a weight vector containing an element ?i
for each domain (just IN and OUT in our case), pi
are the corresponding domain-specific models, and
p?(w, h) is an empirical distribution from a target-
language training corpus?we used the IN dev set
for this.
It is not immediately obvious how to formulate an
equivalent to equation (1) for an adapted TM, be-
cause there is no well-defined objective for learning
TMs from parallel corpora. This has led previous
workers to adopt ad hoc linear weighting schemes
(Finch and Sumita, 2008; Foster and Kuhn, 2007;
Lu? et al, 2007). However, we note that the final con-
ditional estimates p(s|t) from a given phrase table
maximize the likelihood of joint empirical phrase
pair counts over a word-aligned corpus. This sug-
gests a direct parallel to (1):
?? = argmax
?
?
s,t
p?(s, t) log
?
i
?ipi(s|t), (2)
where p?(s, t) is a joint empirical distribution ex-
tracted from the IN dev set using the standard pro-
cedure.2
An alternative form of linear combination is a
maximum a posteriori (MAP) combination (Bacchi-
ani et al, 2004). For the TM, this is:
p(s|t) = cI(s, t) + ? po(s|t)
cI(t) + ?
, (3)
where cI(s, t) is the count in the IN phrase table of
pair (s, t), po(s|t) is its probability under the OUT
TM, and cI(t) =
?
s? cI(s?, t). This is motivated by
taking ? po(s|t) to be the parameters of a Dirich-
let prior on phrase probabilities, then maximizing
posterior estimates p(s|t) given the IN corpus. In-
tuitively, it places more weight on OUT when less
evidence from IN is available. To set ?, we used the
same criterion as for ?, over a dev corpus:
?? = argmax
?
?
s,t
p?(s, t) log cI(s, t) + ? po(s|t)
cI(t) + ?
.
2Using non-adapted IBM models trained on all available IN
and OUT data.
TheMAP combination was used for TM probabil-
ities only, in part due to a technical difficulty in for-
mulating coherent counts when using standard LM
smoothing techniques (Kneser and Ney, 1995).3
2.3 Sentence Selection
Motivated by information retrieval, a number of
approaches choose ?relevant? sentence pairs from
OUT by matching individual source sentences from
IN (Hildebrand et al, 2005; Lu? et al, 2007), or
individual target hypotheses (Zhao et al, 2004).
The matching sentence pairs are then added to the
IN corpus, and the system is re-trained. Although
matching is done at the sentence level, this informa-
tion is subsequently discarded when all matches are
pooled.
To approximate these baselines, we implemented
a very simple sentence selection algorithm in which
parallel sentence pairs from OUT are ranked by the
perplexity of their target half according to the IN lan-
guage model. The number of top-ranked pairs to re-
tain is chosen to optimize dev-set BLEU score.
3 Instance Weighting
The sentence-selection approach is crude in that it
imposes a binary distinction between useful and
non-useful parts of OUT. Matsoukas et al(2009)
generalize it by learning weights on sentence pairs
that are used when estimating relative-frequency
phrase-pair probabilities. The weight on each sen-
tence is a value in [0, 1] computed by a perceptron
with Boolean features that indicate collection and
genre membership.
We extend the Matsoukas et alapproach in sev-
eral ways. First, we learn weights on individual
phrase pairs rather than sentences. Intuitively, as
suggested by the example in the introduction, this
is the right granularity to capture domain effects.
Second, rather than relying on a division of the cor-
pus into manually-assigned portions, we use features
intended to capture the usefulness of each phrase
pair. Finally, we incorporate the instance-weighting
model into a general linear combination, and learn
weights and mixing parameters simultaneously.
3Bacchiani et al(2004) solve this problem by reconstitut-
ing joint counts from smoothed conditional estimates and un-
smoothed marginals, but this seems somewhat unsatisfactory.
453
3.1 Model
The overall adapted TM is a combination of the
form:
p(s|t) = ?t pI(s|t) + (1? ?t) po(s|t), (4)
where pI(s|t) is derived from the IN corpus us-
ing relative-frequency estimates, and po(s|t) is an
instance-weighted model derived from the OUT cor-
pus. This combination generalizes (2) and (3): we
use either ?t = ? to obtain a fixed-weight linear
combination, or ?t = cI(t)/(cI(t) + ?) to obtain a
MAP combination.
We model po(s|t) using a MAP criterion over
weighted phrase-pair counts:
po(s|t) =
c?(s, t) + ?u(s|t)?
s? c?(s?, t) + ?
(5)
where c?(s, t) is a modified count for pair (s, t)
in OUT, u(s|t) is a prior distribution, and ? is a
prior weight. The original OUT counts co(s, t) are
weighted by a logistic function w?(s, t):
c?(s, t) = co(s, t) w?(s, t) (6)
= co(s, t) [1 + exp(?
?
i
?ifi(s, t))]?1,
where each fi(s, t) is a feature intended to charac-
terize the usefulness of (s, t), weighted by ?i.
The mixing parameters and feature weights (col-
lectively ?) are optimized simultaneously using dev-
set maximum likelihood as before:
?? = argmax
?
?
s,t
p?(s, t) log p(s|t;?). (7)
This is a somewhat less direct objective than used
by Matsoukas et al who make an iterative approxi-
mation to expected TER. However, it is robust, effi-
cient, and easy to implement.4
To perform the maximization in (7), we used
the popular L-BFGS algorithm (Liu and Nocedal,
1989), which requires gradient information. Drop-
ping the conditioning on ? for brevity, and let-
ting c??(s, t) = c?(s, t) + ?u(s|t), and c??(t) =
4Note that the probabilities in (7) need only be evaluated
over the support of p?(s, t), which is quite small when this dis-
tribution is derived from a dev set. Maximizing (7) is thus much
faster than a typical MERT run.
?
s? c??(s?, t):
? log p(s|t)
??t
= kt
[
pI(s|t)
p(s|t) ?
po(s|t)
p(s|t)
]
? log p(s|t)
??
= 1? ?t
p(s|t)
[
u(s|t)
c??(t)
?
c??(s, t)
c??(t)2
]
? log p(s|t)
??i
= 1? ?t
p(s|t)
[
c??i(s, t)
c??(t)
?
c??(s, t)c??i(t)
c??(t)2
]
where:
kt =
{
1 fixed weight
?cI(t)/(cI(t) + ?)2 MAP
c??i(s, t) = fi(s, t)(1? w?(s, t))c?(s, t)
and:
c??i(t) =
?
s?
c??i(s
?, t).
3.2 Interpretation and Variants
To motivate weighting joint OUT counts as in (6),
we begin with the ?ideal? objective for setting
multinomial phrase probabilities ? = {p(s|t),?st},
which is the likelihood with respect to the true IN
distribution pI?(s, t). Jiang and Zhai (2007) sug-
gest the following derivation, making use of the true
OUT distribution po?(s, t):
?? = argmax
?
?
s,t
pI?(s, t) log p?(s|t) (8)
= argmax
?
?
s,t
pI?(s, t)
po?(s, t)
po?(s, t) log p?(s|t)
? argmax
?
?
s,t
pI?(s, t)
po?(s, t)
co(s, t) log p?(s|t),
where co(s, t) are the counts from OUT, as in (6).
This has solutions:
p??(s|t) =
pI?(s, t)
po?(s, t)
co(s, t)/
?
s?
pI?(s?, t)
po?(s?, t)
co(s?, t),
and from the similarity to (5), assuming ? = 0, we
see that w?(s, t) can be interpreted as approximat-
ing pI?(s, t)/po?(s, t). The logistic function, whose
outputs are in [0, 1], forces pI?(s, t) ? po?(s, t). This
is not unreasonable given the application to phrase
pairs fromOUT, but it suggests that an interesting al-
ternative might be to use a plain log-linear weighting
454
function exp(?i ?ifi(s, t)), with outputs in [0,?].
We have not yet tried this.
An alternate approximation to (8) would be to let
w?(s, t) directly approximate pI?(s, t). With the ad-
ditional assumption that (s, t) can be restricted to the
support of co(s, t), this is equivalent to a ?flat? alter-
native to (6) in which each non-zero co(s, t) is set to
one. This variant is tested in the experiments below.
A final alternate approach would be to combine
weighted joint frequencies rather than conditional
estimates, ie: cI(s, t) + w?(s, t)co(, s, t), suitably
normalized.5 Such an approach could be simulated
by a MAP-style combination in which separate ?(t)
values were maintained for each t. This would make
the model more powerful, but at the cost of having
to learn to downweight OUT separately for each t,
which we suspect would require more training data
for reliable performance. We have not explored this
strategy.
3.3 Simple Features
We used 22 features for the logistic weighting
model, divided into two groups: one intended to re-
flect the degree to which a phrase pair belongs to
general language, and one intended to capture simi-
larity to the IN domain.
The 14 general-language features embody
straightforward cues: frequency, ?centrality? as
reflected in model scores, and lack of burstiness.
They are:
? total number of tokens in the phrase pair (1);
? OUT corpus frequency (1);
? OUT-corpus frequencies of rarest source and
target words (2);
? perplexities for OUT IBM1 models, in both di-
rections (2);
? average and minimum source and target word
?document frequencies? in the OUT corpus,
using successive 100-line pseudo-documents6
(4); and
5We are grateful to an anonymous reviewer for pointing this
out.
6One of our experimental settings lacks document bound-
aries, and we used this approximation in both settings for con-
sistency.
? average and minimum source and target word
values from the OUT corpus of the following
statistic, intended to reflect degree of burstiness
(higher values indicate less bursty behaviour):
g/(L ? L/(l + 1) + (), where g is the sum
over all sentences containing the word of the
distance (number of sentences) to the nearest
sentence that also contains the word, L is the
total number of sentences, l is the number of
sentences that contain the word, and ( is a small
constant (4).
The 8 similarity-to-IN features are based on word
frequencies and scores from various models trained
on the IN corpus:
? 1gram and 2gram source and target perplexities
according to the IN LM (4);7
? source and target OOV counts with respect to
IN (2); and
? perplexities for IN IBM1 models, in both direc-
tions (2).
To avoid numerical problems, each feature was
normalized by subtracting its mean and dividing by
its standard deviation.
3.4 SVM Feature
In addition to using the simple features directly, we
also trained an SVM classifier with these features
to distinguish between IN and OUT phrase pairs.
Phrase tables were extracted from the IN and OUT
training corpora (not the dev as was used for instance
weighting models), and phrase pairs in the intersec-
tion of the IN and OUT phrase tables were used as
positive examples, with two alternate definitions of
negative examples:
1. Pairs from OUT that are not in IN, but whose
source phrase is.
2. Pairs from OUT that are not in IN, but whose
source phrase is, and where the intersection of
IN and OUT translations for that source phrase
is empty.
7In the case of the Chinese experiments below, source LMs
were trained using text segmented with the LDC segmenter, as
were the other Chinese models in our system.
455
The classifier trained using the 2nd definition had
higher accuracy on a development set. We used it to
score all phrase pairs in the OUT table, in order to
provide a feature for the instance-weighting model.
4 Experiments
4.1 Corpora and System
We carried out translation experiments in two dif-
ferent settings. The first setting uses the Euro-
pean Medicines Agency (EMEA) corpus (Tiede-
mann, 2009) as IN, and the Europarl (EP) cor-
pus (www.statmt.org/europarl) as OUT,
for English/French translation in both directions.
The dev and test sets were randomly chosen from
the EMEA corpus. Figure 1 shows sample sentences
from these domains, which are widely divergent.
The second setting uses the news-related sub-
corpora for the NIST09 MT Chinese to English
evaluation8 as IN, and the remaining NIST paral-
lel Chinese/English corpora (UN, Hong Kong Laws,
and Hong Kong Hansard) as OUT. The dev cor-
pus was taken from the NIST05 evaluation set, aug-
mented with some randomly-selected material re-
served from the training set. The NIST06 and
NIST08 evaluation sets were used for testing. (Thus
the domain of the dev and test corpora matches IN.)
Compared to the EMEA/EP setting, the two do-
mains in the NIST setting are less homogeneous and
more similar to each other; there is also considerably
more IN text available.
The corpora for both settings are summarized in
table 1.
corpus sentence pairs
Europarl 1,328,360
EMEA train 11,770
EMEA dev 1,533
EMEA test 1,522
NIST OUT 6,677,729
NIST IN train 2,103,827
NIST IN dev 1,894
NIST06 test 1,664
NIST08 test 1,357
Table 1: Corpora
8www.itl.nist.gov/iad/mig//tests/mt/2009
The reference medicine for Silapo is
EPREX/ERYPO, which contains epoetin alfa.
Le me?dicament de re?fe?rence de Silapo est
EPREX/ERYPO, qui contient de l?e?poe?tine alfa.
?
I would also like to point out to commissioner Liika-
nen that it is not easy to take a matter to a national
court.
Je voudrais pre?ciser, a` l?adresse du commissaire
Liikanen, qu?il n?est pas aise? de recourir aux tri-
bunaux nationaux.
Figure 1: Sentence pairs from EMEA (top) and Europarl
text.
We used a standard one-pass phrase-based sys-
tem (Koehn et al, 2003), with the following fea-
tures: relative-frequency TM probabilities in both
directions; a 4-gram LM with Kneser-Ney smooth-
ing; word-displacement distortion model; and word
count. Feature weights were set using Och?s MERT
algorithm (Och, 2003). The corpus was word-
aligned using both HMM and IBM2 models, and the
phrase table was the union of phrases extracted from
these separate alignments, with a length limit of 7.
It was filtered to retain the top 30 translations for
each source phrase using the TM part of the current
log-linear model.
4.2 Results
Table 2 shows results for both settings and all meth-
ods described in sections 2 and 3. The 1st block
contains the simple baselines from section 2.1. The
natural baseline (baseline) outperforms the pure IN
system only for EMEA/EP fren. Log-linear combi-
nation (loglin) improves on this in all cases, and also
beats the pure IN system.
The 2nd block contains the IR system, which was
tuned by selecting text in multiples of the size of the
EMEA training corpus, according to dev set perfor-
mance. This significantly underperforms log-linear
combination.
The 3rd block contains the mixture baselines. The
linear LM (lin lm), TM (lin tm) and MAP TM (map
tm) used with non-adapted counterparts perform in
all cases slightly worse than the log-linear combi-
nation, which adapts both LM and TM components.
However, when the linear LM is combined with a
456
method EMEA/EP NIST
fren enfr nst06 nst08
in 32.77 31.98 27.65 21.65
out 20.42 17.41 19.85 15.71
baseline 33.61 31.15 26.93 21.01
loglin 35.94 32.62 28.09 21.85
ir 33.75 31.91 ?? ??
lin lm 35.61 31.55 28.02 21.68
lin tm 35.32 32.52 27.16 21.32
map tm 35.15 31.99 27.20 21.17
lm+lin tm 36.42 33.49 27.83 22.03
lm+map tm 36.28 33.31 28.05 22.11
iw all 36.55 33.73 28.74 22.28
iw all map 37.01 33.90 30.04 23.76
iw all flat 36.50 33.42 28.31 22.13
iw gen map 36.98 33.75 29.81 23.56
iw sim map 36.82 33.68 29.66 23.53
iw svm map 36.79 33.67 ?? ??
Table 2: Results, for EMEA/EP translation into English
(fren) and French (enfr); and for NIST Chinese to En-
glish translation with NIST06 and NIST08 evaluation
sets. Numbers are BLEU scores.
linear TM (lm+lin tm) or MAP TM (lm+map TM),
the results are much better than a log-linear com-
bination for the EMEA setting, and on a par for
NIST. This is consistent with the nature of these two
settings: log-linear combination, which effectively
takes the intersection of IN and OUT, does relatively
better on NIST, where the domains are broader and
closer together. Somewhat surprisingly, there do not
appear to be large systematic differences between
linear and MAP combinations.
The 4th block contains instance-weighting mod-
els trained on all features, used within a MAP TM
combination, and with a linear LM mixture. The
iw all map variant uses a non-0 ? weight on a uni-
form prior in po(s|t), and outperforms a version
with ? = 0 (iw all) and the ?flattened? variant de-
scribed in section 3.2. Clearly, retaining the origi-
nal frequencies is important for good performance,
and globally smoothing the final weighted frequen-
cies is crucial. This best instance-weighting model
beats the equivalant model without instance weights
by between 0.6 BLEU and 1.8 BLEU, and beats the
log-linear baseline by a large margin.
The final block in table 2 shows models trained
on feature subsets and on the SVM feature described
in 3.4. The general-language features have a slight
advantage over the similarity features, and both are
better than the SVM feature.
5 Related Work
We have already mentioned the closely related work
by Matsoukas et al(2009) on discriminative cor-
pus weighting, and Jiang and Zhai (2007) on (non-
discriminative) instance weighting. It is difficult to
directly compare the Matsoukas et alresults with
ours, since our out-of-domain corpus is homoge-
neous; given heterogeneous training data, however,
it would be trivial to include Matsoukas-style iden-
tity features in our instance-weighting model. Al-
though these authors report better gains than ours,
they are with respect to a non-adapted baseline. Fi-
nally, we note that Jiang?s instance-weighting frame-
work is broader than we have presented above, en-
compassing among other possibilities the use of un-
labelled IN data, which is applicable to SMT settings
where source-only IN corpora are available.
It is also worth pointing out a connection with
Daume??s (2007) work that splits each feature into
domain-specific and general copies. At first glance,
this seems only peripherally related to our work,
since the specific/general distinction is made for fea-
tures rather than instances. However, for multino-
mial models like our LMs and TMs, there is a one to
one correspondence between instances and features,
eg the correspondence between a phrase pair (s, t)
and its conditional multinomial probability p(s|t).
As mentioned above, it is not obvious how to ap-
ply Daume??s approach to multinomials, which do
not have a mechanism for combining split features.
Recent work by Finkel and Manning (2009) which
re-casts Daume??s approach in a hierarchical MAP
framework may be applicable to this problem.
Moving beyond directly related work, major
themes in SMT adaptation include the IR (Hilde-
brand et al, 2005; Lu? et al, 2007; Zhao et al,
2004) and mixture (Finch and Sumita, 2008; Fos-
ter and Kuhn, 2007; Koehn and Schroeder, 2007; Lu?
et al, 2007) approaches for LMs and TMs described
above, as well as methods for exploiting monolin-
gual in-domain text, typically by translating it auto-
matically and then performing self training (Bertoldi
457
and Federico, 2009; Ueffing et al, 2007; Schwenk
and Senellart, 2009). There has also been some
work on adapting the word alignment model prior to
phrase extraction (Civera and Juan, 2007; Wu et al,
2005), and on dynamically choosing a dev set (Xu
et al, 2007). Other work includes transferring latent
topic distributions from source to target language for
LM adaptation, (Tam et al, 2007) and adapting fea-
tures at the sentence level to different categories of
sentence (Finch and Sumita, 2008).
6 Conclusion
In this paper we have proposed an approach for
instance-weighting phrase pairs in an out-of-domain
corpus in order to improve in-domain performance.
Each out-of-domain phrase pair is characterized by
a set of simple features intended to reflect how use-
ful it will be. The features are weighted within a
logistic model to give an overall weight that is ap-
plied to the phrase pair?s frequency prior to making
MAP-smoothed relative-frequency estimates (dif-
ferent weights are learned for each conditioning
direction). These estimates are in turn combined
linearly with relative-frequency estimates from an
in-domain phrase table. Mixing, smoothing, and
instance-feature weights are learned at the same time
using an efficient maximum-likelihood procedure
that relies on only a small in-domain development
corpus.
We obtained positive results using a very sim-
ple phrase-based system in two different adaptation
settings: using English/French Europarl to improve
a performance on a small, specialized medical do-
main; and using non-news portions of the NIST09
training material to improve performance on the
news-related corpora. In both cases, the instance-
weighting approach improved over a wide range of
baselines, giving gains of over 2 BLEU points over
the best non-adapted baseline, and gains of between
0.6 and 1.8 over an equivalent mixture model (with
an identical training procedure but without instance
weighting).
In future work we plan to try this approach with
more competitive SMT systems, and to extend in-
stance weighting to other standard SMT components
such as the LM, lexical phrase weights, and lexical-
ized distortion. We will also directly compare with
a baseline similar to the Matsoukas et alapproach in
order to measure the benefit from weighting phrase
pairs (or ngrams) rather than full sentences. Finally,
we intend to explore more sophisticated instance-
weighting features for capturing the degree of gen-
erality of phrase pairs.
References
ACL. 2007. Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL),
Prague, Czech Republic, June.
Michel Bacchiani, Brian Roark, and Murat Saraclar.
2004. Language model adaptation with MAP esti-
mation and the perceptron algorithm. In NAACL04
(NAA, 2004).
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In WMT09 (WMT, 2009).
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in Statistical Machine Translation with mixture mod-
elling. In WMT07 (WMT, 2007).
Hal Daume? III and Daniel Marcu. 2006. Domain Adap-
tation for Statistical Classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In ACL-07 (ACL, 2007).
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine translation.
In Proceedings of the ACL Workshop on Statistical
Machine Translation, Columbus, June. WMT.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian domain adaptation. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL), Boulder, June.
NAACL.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In WMT07 (WMT, 2007).
George Foster and Roland Kuhn. 2009. Stabilizing min-
imum error rate training. In WMT09 (WMT, 2009).
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th EAMT
Conference, Budapest, May.
Jing Jiang and ChengXiang Zhai. 2007. Instance
Weighting for Domain Adaptation in NLP. In ACL-
07 (ACL, 2007).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
458
Speech, and Signal Processing (ICASSP) 1995, pages
181?184, Detroit, Michigan. IEEE.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL), pages 127?133,
Edmonton, May. NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. In Proceedings
of the 2007 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Prague, Czech
Republic.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
NAACL. 2004. Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Boston, May.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Sapporo, July. ACL.
Holger Schwenk and Jean Senellart. 2009. Translation
model adaptation for an arabic/french news translation
system by lightly-supervised training. In Proceedings
of MT Summit XII, Ottawa, Canada, September. Inter-
national Association for Machine Translation.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-LSA Based LM Adaptation for Spoken Lan-
guage Translation. In ACL-07 (ACL, 2007).
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In ACL-07 (ACL, 2007).
WMT. 2007. Proceedings of the ACL Workshop on Sta-
tistical Machine Translation, Prague, June.
WMT. 2009. Proceedings of the 4th Workshop on Statis-
tical Machine Translation, Athens, March.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005.
Alignment model adaptation for domain-specific word
alignment. In Proceedings of the 43th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Ann Arbor, Michigan, July. ACL.
Jia Xu, Yonggang Deng, Yuqing Gao, and Hermann Ney.
2007. Domain dependent statistical machine transla-
tion. In MT Summit XI, Copenhagen, September.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING) 2004, Geneva, August.
459
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 631?642, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Enlarging Paraphrase Collections through Generalization and Instantiation
Atsushi Fujita
Future University Hakodate
116-2 Kameda-nakano-cho,
Hakodate, Hokkaido, 041-8655, Japan
fujita@fun.ac.jp
Pierre Isabelle Roland Kuhn
National Research Council Canada
283 Alexandre-Tache? Boulevard,
Gatineau, QC, J8X 3X7, Canada
{Pierre.Isabelle, Roland.Kuhn}@nrc.ca
Abstract
This paper presents a paraphrase acquisition
method that uncovers and exploits generali-
ties underlying paraphrases: paraphrase pat-
terns are first induced and then used to col-
lect novel instances. Unlike existing methods,
ours uses both bilingual parallel and monolin-
gual corpora. While the former are regarded as
a source of high-quality seed paraphrases, the
latter are searched for paraphrases that match
patterns learned from the seed paraphrases.
We show how one can use monolingual cor-
pora, which are far more numerous and larger
than bilingual corpora, to obtain paraphrases
that rival in quality those derived directly from
bilingual corpora. In our experiments, the
number of paraphrase pairs obtained in this
way from monolingual corpora was a large
multiple of the number of seed paraphrases.
Human evaluation through a paraphrase sub-
stitution test demonstrated that the newly ac-
quired paraphrase pairs are of reasonable qual-
ity. Remaining noise can be further reduced
by filtering seed paraphrases.
1 Introduction
Paraphrases are semantically equivalent expressions
in the same language. Because ?equivalence? is the
most fundamental semantic relationship, techniques
for generating and recognizing paraphrases play an
important role in a wide range of natural language
processing tasks (Madnani and Dorr, 2010).
In the last decade, automatic acquisition of knowl-
edge about paraphrases from corpora has been draw-
ing the attention of many researchers. Typically, the
acquired knowledge is simply represented as pairs of
semantically equivalent sub-sentential expressions
as in (1).
(1) a. look like ? resemble
b. control system ? controller
The challenge in acquiring paraphrases is to ensure
good coverage of the targeted classes of paraphrases
along with a low proportion of incorrect pairs. How-
ever, no matter what type of resource has been used,
it has proven difficult to acquire paraphrase pairs
with both high recall and high precision.
Among various types of corpora, monolingual
corpora can be considered the best source for high-
coverage paraphrase acquisition, because there is
far more monolingual than bilingual text avail-
able. Most methods that exploit monolingual cor-
pora rely on the Distributional Hypothesis (Harris,
1968): expressions that appear in similar contexts
are expected to have similar meaning. However,
if one uses purely distributional criteria, it is dif-
ficult to distinguish real paraphrases from pairs of
expressions that are related in other ways, such as
antonyms and cousin words.
In contrast, since the work in (Bannard and
Callison-Burch, 2005), bilingual parallel corpora
have been acknowledged as a good source of high-
quality paraphrases: paraphrases are obtained by
putting together expressions that receive the same
translation in the other language (pivot language).
Because translation expresses a specific meaning
more directly than context in the aforementioned ap-
proach, pairs of expressions acquired in this manner
tend to be correct paraphrases. However, the cov-
erage problem remains: there is much less bilingual
parallel than monolingual text available.
Our objective in this paper is to obtain para-
phrases that have high quality (like those extracted
from bilingual parallel corpora via pivoting) but can
be generated in large quantity (like those extracted
631
from monolingual corpora via contextual similar-
ity). To achieve this, we propose a method that ex-
ploits general patterns underlying paraphrases and
uses both bilingual parallel and monolingual sources
of information. Given a relatively high-quality set of
paraphrases obtained from a bilingual parallel cor-
pus, a set of paraphrase patterns is first induced.
Then, appropriate instances of such patterns, i.e.,
potential paraphrases, are harvested from a mono-
lingual corpus.
After reviewing existing methods in Section 2,
our method is presented in Section 3. Section 4
describes our experiments in acquiring paraphrases
and presents statistics summarizing the coverage of
our method. Section 5 describes a human evaluation
of the quality of the acquired paraphrases. Finally,
Section 6 concludes this paper.
2 Literature on Paraphrase Acquisition
This section summarizes existing corpus-based
methods for paraphrase acquisition, following the
classification in (Hashimoto et al2011): similarity-
based and alignment-based methods.
2.1 Similarity-based Methods
Techniques that use monolingual (non-parallel) cor-
pora mostly rely on the Distributional Hypothesis
(Harris, 1968). Because a large quantity of mono-
lingual data is available for many languages, a large
number of paraphrase candidates can be acquired
(Lin and Pantel, 2001; Pas?ca and Dienes, 2005; Bha-
gat and Ravichandran, 2008, etc.). The recipes pro-
posed so far are based on three main ingredients, i.e.,
features used for representing context of target ex-
pression (contextual features), criteria for weighting
and filtering features, and aggregation functions.
A drawback of relying only on contextual simi-
larity is that it tends to give high scores to semanti-
cally related but non-equivalent expressions, such as
antonyms and cousin words. To enhance the preci-
sion of the results, filtering mechanisms need to be
introduced (Marton et al2011).
2.2 Alignment-based Methods
Pairs of expressions that get translated to the same
expression in a different language can be regarded as
paraphrases. On the basis of this hypothesis, Barzi-
lay and McKeown (2001) and Pang et al2003)
created monolingual parallel corpora from multiple
human translations of the same source. Then, they
extracted corresponding parts of such parallel sen-
tences as sub-sentential paraphrases.
Leveraging recent advances in statistical ma-
chine translation (SMT), Bannard and Callison-
Burch (2005) proposed a method for acquiring sub-
sentential paraphrases from bilingual parallel cor-
pora. As in SMT, a translation table is first built on
the basis of alignments between expressions, such as
words, phrases, and subtrees, across a parallel sen-
tence pair. Then, pairs of expressions (e1, e2) in the
same language that are aligned with the same ex-
pressions in the other language (pivot language) are
extracted as paraphrases. The likelihood of e2 being
a paraphrase of e1 is given by
p(e2|e1) =
?
f?Tr(e1,e2)
p(e2|f)p(f |e1), (1)
where Tr(e1, e2) stands for the set of shared trans-
lations of e1 and e2. Each factor p(e|f) and p(f |e)
is estimated from the number of times e and f are
aligned and the number of occurrences of each ex-
pression in each language. Kok and Brockett (2010)
showed how one can discover paraphrases that do
not share any translation in one language by travers-
ing a graph created from multiple translation tables,
each corresponding to a bilingual parallel corpus.
This approach, however, suffers from a cover-
age problem, because both monolingual parallel and
bilingual parallel corpora tend to be significantly
smaller than monolingual non-parallel corpora. The
acquired pairs of expressions include some non-
paraphrases as well. Many of these come from er-
roneous alignments, which are particularly frequent
when the given corpus is small.
Monolingual comparable corpora have also been
exploited as sources of paraphrases using alignment-
based methods. For instance, multiple news arti-
cles covering the same event (Shinyama et al2002;
Barzilay and Lee, 2003; Dolan et al2004; Wubben
et al2009) have been used. Such corpora have
also been created manually through crowdsourcing
(Chen and Dolan, 2011). However, the availabil-
ity of monolingual comparable corpora is very lim-
ited for most languages; thus, approaches relying
on these corpora have typically produced only very
632
small collections of paraphrases. Hashimoto et al
(2011) found a way around this limitation by collect-
ing sentences that constitute explicit definitions of
particular words or phrases from monolingual non-
parallel Web documents, pairing sentences that de-
fine the same noun phrase, and then finding corre-
sponding phrases in each sentence pair. One limita-
tion of this approach is that it requires a considerable
amount of labeled data for both the corpus construc-
tion and the paraphrase extraction steps.
2.3 Summary
Existing methods have investigated one of the fol-
lowing four types of corpora as their principal re-
source1: monolingual non-parallel corpora, mono-
lingual parallel corpora, monolingual comparable
corpora, and bilingual parallel corpora. No matter
what type of resource has been used, however, it
has proven difficult to acquire paraphrases with both
high recall and precision, with the possible excep-
tion of the method in (Hashimoto et al2011) which
requires large amounts of labeled data.
3 Proposed Method
While most existing methods deal with expressions
only at the surface level, ours exploits generalities
underlying paraphrases to achieve better coverage
while retaining high precision. Furthermore, unlike
existing methods, ours uses both bilingual parallel
and monolingual non-parallel corpora as sources for
acquiring paraphrases.
The process is illustrated in Figure 1. First, a
set of high-quality seed paraphrases, PSeed , is ac-
quired from bilingual parallel corpora by using an
alignment-based method. Then, our method collects
further paraphrases through the following two steps.
Generalization (Step 2): Paraphrase patterns are
learned from the seed paraphrases, PSeed .
Instantiation (Step 3): A novel set of paraphrase
pairs, PHvst , is finally harvested from mono-
lingual non-parallel corpora using the learned
patterns; each newly acquired paraphrase pair
is assessed by contextual similarity.
1Chan et al2011) used monolingual corpora only for re-
ranking paraphrases obtained from bilingual parallel corpora.
To the best of our knowledge, bilingual comparable corpora
have never been used as sources for acquiring paraphrases.
Monolingual Non-parallel Corpus
Step 1. Seed Paraphrase Acquisition
Step 2. Paraphrase Pattern Induction
Step 3. Paraphrase Instance Acquisition
?health issue? ? ?health problem? ?look like? ? ?resemble? ?regional issue? ? ?regional problem? 
?health issue? ? ?probl?me de sant?? ?health problem? ? ?probl?me de sant?? ?look like? ? ?ressemble? ?regional issue? ? ?probl?me r?gional? ?regional problem? ? ?probl?me r?gional? ?resemble? ? ?ressemble? 
?X issue? ? ?X problem?;         {food, regional, ...}
?backlog issue? ? ?backlog problem? ?communal issue? ? ?communal problem? ?phishing issue? ? ?phishing problem? ?spatial issue? ? ?spatial problem?
Translation Table
PSeed: Seed Paraphrases
Paraphrase Patterns
PHvst: Novel Paraphrases
Bilingual Parallel Corpus
Figure 1: Process of paraphrase acquisition.
The set PSeed acquired early in the process can be
pooled with the set PHvst harvested in the last stage
of the process.
3.1 Step 1. Seed Paraphrase Acquisition
The goal of the first step is to obtain a set of high-
quality paraphrase pairs, PSeed .
For this purpose, alignment-based methods with
bilingual or monolingual parallel corpora are prefer-
able to similarity-based methods applied to non-
parallel corpora. Among various options, in this pa-
per, we start from the standard technique proposed
by Bannard and Callison-Burch (2005) with bilin-
gual parallel corpora (see also Section 2.2). In par-
ticular, we assume the phrase-based SMT frame-
work (Koehn et al2003). Then, we purify the re-
sults with several filtering methods.
The phrase pair extraction process of phrase-
based SMT systems aims at high recall for increased
robustness of the translation process. As a result,
a naive application of the paraphrase acquisition
method produces pairs of expressions that are not
exact paraphrases. For instance, the algorithm ex-
plained in Koehn (2009, p.134) extracts both ?dass?
and ?, dass? as counterparts of ?that? from the sen-
tence pair. To reduce that kind of noise, we apply
some filtering techniques to the candidate translation
pairs. First, statistically unreliable translation pairs
(Johnson et al2007) are filtered out. Then, we also
filter out phrases made up entirely of stop words (in-
cluding punctuation marks), both in the language of
interest and in the pivot language.
Let PRaw be the initial set of paraphrase pairs ex-
tracted from the sanitized translation table. We first
633
lp: control apparatus
rp: control devicep(rp|lp)
.172
rp: control system
.032
rp: the control device
.015
rp: control device of the.005
rp: controlling device.004
rp: control system of
.003
rp: a control system for an
.001
rp: a controlling device
.001
Figure 2: RHS-filtering for ?control apparatus?.
rp: control device
lp: controller p(lp|rp)
.153
lp: control apparatus
.135
lp: the control apparatus
.010
lp: control apparatus of .008
lp: controlling unit .004
lp: control equipment
.002
lp: controller for a
.001
lp: to the control apparatus
.001
Figure 3: LHS-filtering for ?control device?.
discard pairs whose difference comprises only stop
words, such as ?the schools? ? ?schools and?. We
also remove pairs containing only singular-plural
differences, such as ?family unit? ? ?family units?.
Depending on the language of interest, other types of
morphological variants, such as those shown in (2),
may also be ignored.
(2) a. ?europe?enne? ? ?europe?en?
(Gender in French)
b. ?guten Lo?sungen? ? ?gute Lo?sungen?
(Case in German)
We further filter out less reliable pairs, such as
those shown with dotted lines in Figures 2 and 3.
This is carried out by comparing the right-hand side
(RHS) phrases of each left-hand side (LHS) phrase,
and vice versa2. Given a set of paraphrase pairs,
RHS phrases corresponding to the same LHS phrase
lp are compared. A RHS phrase rp is not licensed iff
lp has another RHS phrase rp? (?= rp) which satis-
fies the following two conditions (see also Figure 2).
? rp? is a word sub-sequence of rp
? rp? is a more likely paraphrase than rp,
i.e., p(rp ?|lp) > p(rp|lp)
LHS phrases for each RHS phrase rp are also com-
pared in a similar manner, i.e., a LHS phrase lp is
not qualified as a legitimate source of rp iff rp has
another LHS phrase lp? (?= lp) which satisfies the
following conditions (see also Figure 3).
? lp? is a word sub-sequence of lp
? lp? is a more likely source than lp,
i.e., p(lp ?|rp) > p(lp|rp)
The two directions of filtering are separately applied
and the intersection of their results is retained.
2cf. Denkowski and Lavie (2011); they only compared each
RHS phrase to its corresponding LHS phrase.
Candidate pairs are finally filtered on the basis
of their reliability score. Traditionally, a threshold
(thp) on the conditional probability given by Eq. (1)
is used (Du et al2010; Max, 2010; Denkowski
and Lavie, 2011, etc.). Furthermore, we also re-
quire that LHS and RHS phrases exceed a thresh-
old (ths ) on their contextual similarity in a mono-
lingual corpus. This paper neither proposes a spe-
cific recipe nor makes a comprehensive comparison
of existing recipes for computing contextual simi-
larity, although one particular recipe is used in our
experiments (see Section 4.1).
3.2 Step 2. Paraphrase Pattern Induction
From a set of seed paraphrases, PSeed , paraphrase
patterns are induced. For instance, from paraphrases
in (3), we induce paraphrase patterns in (4).
(3) a. ?restraint system? ? ?restraint apparatus?
b. ?movement against racism?
? ?anti-racism movement?
c. ?middle eastern countries?
? ?countries in the middle east?
(4) a. ?X system? ? ?X apparatus?
b. ?X against Y ? ? ?anti-Y X?
c. ?X eastern Y ? ? ?Y in the X east?
Word pairs of LHS and RHS phrases will be re-
placed with variable slots iff they are fully identi-
cal or singular-plural variants. Note that stop words
are retained. While a deeper level of lexical cor-
respondences, such as ?eastern? and ?east? in (3c)
and ?system? and ?apparatus? in (3a), could be cap-
tured, this would require the use of rich language
resources, thereby making the method less portable
to resource-poor languages.
634
Note that our aim is to automatically capture gen-
eral paraphrase patterns of the kind that have some-
times been manually described (Jacquemin, 1999;
Fujita et al2007). This is different from ap-
proaches that attach variable slots to paraphrases for
calculating their similarity (Lin and Pantel, 2001;
Szpektor and Dagan, 2008) or for constraining
the context in which they are regarded legitimate
(Callison-Burch, 2008; Zhao et al2009).
3.3 Step 3. Paraphrase Instance Acquisition
Given a set of paraphrase patterns, such as those
shown in (4), a set of novel instances, i.e., novel
paraphrases, PHvst , will now be harvested from
monolingual non-parallel corpora. In other words,
a set of appropriate slot-fillers will be extracted.
First, expressions that match both elements of
the pattern, except stop words, are collected from
a given monolingual corpus. Pattern matching alone
may generate inappropriate pairs, so we then assess
the legitimacy of each collected slot-filler.
Let LHS (w) and RHS (w) be the expressions
generated by instantiating the k variable slots in
LHS and RHS phrases of the pattern with a k-tuple
of slot-fillers w (= w1, . . . , wk), respectively. We
estimate how likelyRHS (w) is to be a paraphrase of
LHS (w) based on the contextual similarity between
them using a monolingual corpus; a pair of phrases
is discarded if they are used in substantially dissim-
ilar contexts. We use the same recipe and threshold
value for ths with Step 1 in our experiments.
Contextual similarity of antonyms and cousin
words can also be high, as they are often used in sim-
ilar contexts. However, this is not a problem in our
framework, because semantic equivalence between
LHS (w) and RHS (w) is almost entirely guaran-
teed as a result of the way the corresponding patterns
were learned from a bilingual parallel corpus.
3.4 Characteristics
In terms of coverage, PHvst is expected to be greatly
larger than PSeed , although it will not cover to-
tally different pairs of paraphrases, such as those
shown in (1). On the other hand, the quality of
PHvst depends on that of PSeed . Unlike in the pure
similarity-based method, PHvst is constrained by the
paraphrase patterns derived from the set of high-
quality paraphrases, PSeed , and will therefore gen-
erally exclude the kind of semantically similar but
non-equivalent pairs that contextual similarity alone
tends to extract alongside real paraphrases.
As mentioned in Section 3.1, other types of meth-
ods can be used for obtaining high-quality seed
paraphrases, PSeed . For instance, the supervised
method proposed by Hashimoto et al2011) uses
the existence of shared words as a feature to deter-
mine whether the given pair of expressions are para-
phrases, and thereby extracts many pairs sharing the
same words. Thus, their output has a high potential
to be used as an alternative seed for our method.
Another advantage of our method is that it does
not require any labeled data, unlike the super-
vised methods proposed by Zhao et al2009) and
Hashimoto et al2011).
4 Quantitative Impact
4.1 Experimental Settings
Two different sets of corpora were used as data
sources; in both settings, we acquired English para-
phrases.
Europarl: The English-French version of the Eu-
roparl Parallel Corpus3 consisting of 1.8M sen-
tence pairs (51M words in English and 56M
words in French) was used as a bilingual par-
allel corpus, while its English side and the En-
glish side of the 109 French-English corpus4
consisting of 23.8M sentences (649M words)
were used as monolingual data.
Patent: The Japanese-English Patent Translation
data (Fujii et al2010) consisting of 3.2M sen-
tence pairs (122M morphemes in Japanese and
106Mwords in English) was used as a bilingual
parallel corpus, while its English side and the
30.0M sentences (626M words) from the 2007
chapter of NTCIR unaligned patent documents
were used as monolingual data.
To study the behavior of our method for different
amounts of bilingual parallel data, we carried out
learning curve experiments.
We used our in-house tokenizer for segmentation
of English and French sentences and MeCab5 for
Japanese sentences.
3http://statmt.org/europarl/, release 6
4http://statmt.org/wmt10/training-giga-fren.tar
5http://mecab.sourceforge.net/, version 0.98
635
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# of words in the English side of bilingual corpus
PRawPRaw (thp=0.01)PSeed (thp=?, ths=?)PSeed (thp=0.01, ths=?)
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# of words in the English side of bilingual corpus
PRawPRaw (thp=0.01)PSeed (thp=?, ths=?)PSeed (thp=0.01, ths=?)
Figure 4: # of paraphrase pairs in PSeed (left: Europarl, right: Patent).
Stop word lists for sanitizing translation pairs and
paraphrase pairs were manually compiled: we enu-
merated 442 English words, 193 French words, and
149 Japanese morphemes, respectively.
From a bilingual parallel corpus, a translation ta-
ble was created by our in-house phrase-based SMT
system, PORTAGE (Sadat et al2005). Phrase
alignments of each sentence pair were identified by
the heuristic ?grow-diag-final?6 with a maximum
phrase length 8. The resulting translation pairs were
then filtered with the significance pruning technique
of (Johnson et al2007), using ? + ? as threshold.
As contextual features for computing similarity
of each paraphrase pair, all of the 1- to 4-grams of
words adjacent to each occurrence of a phrase were
counted. This is a compromise between less expen-
sive but noisier approaches, such as bag-of-words,
and more accurate but more expensive approaches
that incorporate syntactic features (Lin and Pantel,
2001; Shinyama et al2002; Pang et al2003;
Szpektor and Dagan, 2008). Contextual similarity is
finally measured by taking cosine between two fea-
ture vectors.
4.2 Statistics on Acquired Paraphrases
Seed Paraphrases (PSeed )
Figure 4 shows the number of paraphrase pairs
PSeed obtained from the bilingual parallel corpora.
The general trend is simply that the larger the cor-
pus is, the more paraphrases are acquired.
Given the initial set of paraphrases, PRaw (???),
our filtering techniques (?
2
?) discarded a large por-
tion (63-75% in Europarl and 43-64% in Patent) of
them. Pairs with zero similarity were also filtered
out, i.e., ths = ?. This suggests that many incorrect
6http://statmt.org/moses/?n=FactoredTraining.AlignWords
and/or relatively useless pairs, such as those shown
in Figures 2 and 3, had originally been acquired.
Lines with ??? show the results based on a
widely-used threshold value on the conditional prob-
ability in Eq. (1), i.e., thp = 0.01 (Du et al2010;
Max, 2010; Denkowski and Lavie, 2011, etc.). The
percentage of paraphrase pairs thereby discarded
varied greatly depending on the corpus size (17-78%
in Europarl and 31-82% in Patent), suggesting that
the threshold value should be determined depending
on the given corpus. In the following experiment,
however, we conform to the convention thp = 0.01
(???) to ensure the quality of PSeed that we will be
using for inducing paraphrase patterns, even though
this results in discarding some less frequent but cor-
rect paraphrase pairs, such as ?control apparatus?
? ?controlling device? in Figure 2.
Paraphrase Patterns
Figures 5 and 6 show the number of paraphrase
patterns that our method induced and their cover-
age against PSeed , respectively. Due to their rather
rigid form, the patterns covered no more than 15%
of PSeed in Europarl. In contrast, a higher propor-
tion of PSeed in Patent was generalized into patterns.
We speculate it is because the patent domain con-
tains many expressions, including technical terms,
that have similar variations of constructions.
The acquired patterns were mostly one-variable
patterns: 88-93% and 80-91% of total patterns for
different variants of the Europarl and Patent set-
tings, respectively. Given that there are far more
one-variable patterns than other types, and that one-
variable patterns are the simplest type, we hence-
forth focus on them. More complex patterns, includ-
ing two-variable patterns (7-11% and 8-17% in each
setting), will be investigated in our future work.
636
102
103
104
105
106
106 107 108
# o
f pa
rap
hra
se 
pat
tern
s
# of words in the English side of bilingual corpus
All (Patent)1-var (Patent)All (Europarl)1-var (Europarl)
Figure 5: # of paraphrase patterns.
 0
 10
 20
 30
 40
106 107 108
Co
ver
age
 [%]
# of words in the English side of bilingual corpus
All (Patent)1-var (Patent)All (Europarl)1-var (Europarl)
Figure 6: Coverage of the paraphrase patterns.
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# o
f un
iqu
e L
HS
 ph
ras
es
# of words in the English side of bilingual corpus
Pair in PHvstLHS in PHvstPair in PSeedLHS in PSeed
18.1M
1.22M
103
104
105
106
107
108
106 107 108
# o
f pa
rap
hra
se 
pai
rs
# o
f un
iqu
e L
HS
 ph
ras
es
# of words in the English side of bilingual corpus
Pair in PHvstLHS in PHvstPair in PSeedLHS in PSeed
28.7M
1.41M
Figure 7: # of paraphrase pairs and unique LHS phrases in PSeed and PHvst (left: Europarl, right: Patent).
Novel Paraphrases (PHvst )
Using the paraphrase patterns, novel paraphrase
pairs, PHvst , were harvested from the monolingual
non-parallel corpora. In this experiment, we only
retained one-variable patterns and regarded only sin-
gle words as slot-fillers for them. Nevertheless, we
managed to acquire a large number of paraphrase
pairs as depicted in Figure 7, where pairs having
zero similarity were excluded. For instance, when
the full size of bilingual parallel corpus in Patent was
used, we acquired 1.41M pairs of seed paraphrases,
PSeed , and 28.7M pairs of novel paraphrases, PHvst .
In other words, our method expanded PSeed by about
21 times. The number of unique LHS phrases that
PHvst covers was also significantly larger than that
of PSeed .
Figure 8 highlights the remarkably large ratio of
PHvst to PSeed in terms of the number of paraphrase
pairs and the number of unique LHS phrases. The
smaller the bilingual corpus is, the higher the ratio
is, except when there is only a very small amount of
Europarl data. This demonstrates that our method is
quite powerful, given a minimum amount of data.
Another striking difference between PSeed and
PHvst is the average number of RHS phrases per
unique LHS phrase, i.e., their relative yield. As
displayed in Figure 9, the yield for PHvst increased
rapidly with the scaling up of the bilingual cor-
pus, while that of PSeed only grew slowly. The
alignment-based method with bilingual corpora can-
not produce very many RHS phrases per unique
LHS phrase due to its reliance on conditional prob-
ability and the surface level processing. In con-
trast, our method does not limit the number of RHS
phrases: each RHS phrase is separately assessed by
its similarity to the corresponding LHS phrase. One
limitation of our method is that it cannot achieve
high yield for PHvst whenever only a small num-
ber of paraphrase patterns can be extracted from the
bilingual corpus (see also Figure 5).
Both the ratio of PHvst to PSeed and the relative
yield could probably be increased by scaling up the
monolingual corpus. For instance, in the patent do-
main, monolingual documents 10 times larger than
the one used in the above experiments are avail-
able at the NTCIR project7. It would be interesting
to compare the relative gains brought by in-domain
versus general-purpose corpora.
7http://ntcir.nii.ac.jp/PatentMT-2/
637
 0
 20
 40
 60
 80
106 107 108
Ra
tio 
of P
Hv
st t
o P
Se
ed
# of words in the English side of bilingual corpus
LHS (Patent)Pair (Patent)LHS (Europarl)Pair (Europarl)
Figure 8: Ratio of PHvst to PSeed .
 1
 2
 3
 4
 5
106 107 108
Av
g. #
 of 
RH
S p
hra
ses
# of words in the English side of bilingual corpus
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl)
Figure 9: Average # of RHS phrases per LHS phrase.
103
104
105
106
107
108
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
# o
f pa
rap
hra
se 
pai
rs
Probability threshold thp
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl) 103
104
105
106
107
108
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
# o
f pa
rap
hra
se 
pai
rs
Similarity threshold ths
PHvst (Patent)PSeed (Patent)PHvst (Europarl)PSeed (Europarl)
Figure 10: # of acquired paraphrase pairs against threshold values.
(left: probability-based (0.01 ? thp ? 0.9, ths = ?), right: similarity-based (? ? ths ? 0.9, thp = 0.01))
Finally, we investigated how the number of para-
phrase pairs varies depending on the values for the
two thresholds, i.e., thp on the conditional probabil-
ity and ths on the contextual similarity, respectively.
Figure 10 shows the results when the full sizes of
bilingual corpora are used. When the pairs were fil-
tered only with thp , the number of paraphrase pairs
in PHvst decreased more slowly than that of PSeed
according to the increase of the threshold value. This
is a benefit from our generalization and instantiation
method. The same paraphrase pattern is often in-
duced from more than one paraphrase pair in PSeed .
Thus, as long as at least one of them has a proba-
bility higher than the given threshold value, corre-
sponding novel paraphrases can be harvested.
On the other hand, as a results of assessing each
individual paraphrase pair by the contextual similar-
ity, many pairs in PHvst , which are supposed to be
incorrect instances of their corresponding pattern,
are filtered out by a larger threshold value for ths .
In contrast, many pairs in PSeed have a relatively
high similarity, e.g., 40% of all pairs have similarity
higher than 0.4. This indicates the quality of PSeed
is highly guaranteed by the shared translations.
5 Human Evaluation of Quality
To confirm that the quality of PHvst is sufficiently
high, we carried out a substitution test.
First, by substituting sub-sentential paraphrases
to existing sentences in a given test corpus, pairs
of slightly different sentences were automatically
generated. For instance, by applying ?looks like?
? ?resembles? to (5), (6) was generated.
(5) The roof looks like a prehistoric lizard?s spine.
(6) The roof resembles a prehistoric lizard?s spine.
Human evaluators were then asked to score each
pair of an original sentence and a paraphrased sen-
tence with the following two 5-point scale grades
proposed by Callison-Burch (2008):
Grammaticality: whether the paraphrased sen-
tence is grammatical (1: horrible, 5: perfect)
Meaning: whether the meaning of the original sen-
tence is properly retained by the paraphrased
sentence (1: totally different, 5: equivalent)
To make results more consistent and reduce the
human labor, evaluators were asked to rate at the
same time several paraphrases for the same source
phrase. For instance, given a source sentence (5), the
638
evaluators might be given the following sentences in
addition to a paraphrased sentence (6).
(7) The roof seems like a prehistoric lizard?s spine.
(8) The roofwould look like a prehistoric lizard?s spine.
In this experiment, we showed five paraphrases
per source phrase, assuming that evaluators would
get confused if too large a number of paraphrase
candidates were presented at the same time.
5.1 Data for Evaluation
As in previous work (Callison-Burch, 2008; Chan
et al2011), we evaluated paraphrases acquired
from the Europarl corpus on news sentences. Para-
phrase examples were automatically generated from
the English part ofWMT 2008-2011 ?newstest? data
(10,050 unique sentences) by applying the union of
PSeed and PHvst of the Europarl setting (19.3M para-
phrases for 5.95M phrases).
On the other hand, paraphrases acquired from
patent documents are much more difficult to eval-
uate due to the following reasons. First, they may
be too domain-specific to be of any use in general
areas such as news sentences. However, conduct-
ing an in-domain evaluation would be difficult with-
out enrolling domain experts. We expect that para-
phrases from a domain can be used safely in that
domain. Nevertheless, deciding under what circum-
stances they can be used safely in another domain is
an interesting research question.
To reduce the human labor for the evaluation, sen-
tences were restricted to those with moderate length:
10-30 words, which are expected to provide suf-
ficient but succinct context. To propose multiple
paraphrase candidates at the same time, we also re-
stricted phrases to be paraphrased (LHS phrases) to
those having at least five paraphrases including ones
from PHvst . This resulted in 60,421 paraphrases for
988 phrase tokens (353 unique phrases).
Finally, we randomly sampled 80 unique phrase
tokens and five unique paraphrases for each phrase
token (400 examples in total), and asked six people
having a high level of English proficiency to evalu-
ate them. Inter-evaluator agreement was calculated
from five different pairs of evaluators, each judging
the same 10 examples. The remaining 350 exam-
ples were divided into six chunks of slightly unequal
length, with each chunk being judged by one of the
six evaluators.
5-point Binary
n G M G M Both
PSeed 55 4.60 4.35 0.85 0.93 0.78
PHvst 295 4.22 3.35 0.74 0.67 0.55
Total 350 4.28 3.50 0.76 0.71 0.58
Table 1: Avg. score and precision of binary classification.
5.2 Results
Table 1 shows the average of the original 5-point
scale scores and the percentage of examples that
are judged correct based on a binary judgment
(Callison-Burch, 2008): an example is considered to
be correct iff the grammaticality score is 4 or above
and/or the meaning score is 3 or above. Paraphrases
based on PSeed achieved a quite high performance
in both grammaticality (?G?) and meaning (?M?) in
part because of the effectiveness of our filtering tech-
niques. The performance of paraphrases drawn from
PHvst was reasonably high and similar to the scores
0.68 for grammaticality, 0.61 for meaning, and 0.55
for both, of the best model reported in (Callison-
Burch, 2008), although it was inferior to PSeed .
Despite the fact that all of our evaluators had a
high-level command of English, the agreement was
not very high. This was true even when the col-
lected scores were mapped into binary classes. In
this case, the ? values (Cohen, 1960) for each crite-
rion were 0.45 and 0.45, respectively, which indicate
the agreement was ?fair?. To obtain a better ? value,
the criteria for grading will need to be improved.
However, we think that was not too low either8.
The most promising way for improving the qual-
ity of PHvst is to ensure that paraphrase patterns
cover only legitimate paraphrases. We investigated
this by filtering the manually scored paraphrase ex-
amples with two thresholds for cleaning seed para-
phrases PSeed : thp on the conditional probability es-
timated using the bilingual parallel corpus and ths
on the contextual similarity in the monolingual non-
parallel corpus. Figure 11 shows the average score
of the examples whose corresponding paraphrase is
obtainable with the given threshold values. Note that
the points in the figure with higher threshold values
are less reliable than the others, because filtering re-
duces the number of the manually scored examples
8Note that Callison-Burch (2008) might possibly underesti-
mate the chance agreement and overestimate the ? values, be-
cause the distribution of human scores would not be uniform.
639
 3
 3.5
 4
 4.5
 5
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
g. s
cor
e
Probability threshold thp
Grammaticality (PSeed)Grammaticality (PHvst)Meaning (PSeed)Meaning (PHvst)
 3
 3.5
 4
 4.5
 5
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Av
g. s
cor
e
Similarity threshold ths
Grammaticality (PSeed)Grammaticality (PHvst)Meaning (PSeed)Meaning (PHvst)
Figure 11: Average score of paraphrase examples against threshold values.
(left: probability-based (0.01 ? thp ? 0.9, ths = ?), right: similarity-based (? ? ths ? 0.9, thp = 0.01))
The points with higher threshold values are less reliable than the others,
because filtering reduces the number of the manually scored examples used to calculate scores.
used to calculate scores. Nevertheless, it indicates
that better filtering of PSeed with higher threshold
values is likely to produce a better-quality set of
paraphrases PHvst . For instance, an inappropriate
paraphrase pattern (9a) was excluded with thp = 0.1
or ths = 0.1, while correct ones (9b) and (9c) re-
mained even when a large threshold value is used.
(9) a. ?X years? ? ?turn X?
b. ?X supplied? ? ?X provided?
c. ?main X? ? ?most significant X?
Kendall?s correlation coefficient ?B (Kendall,
1938) between the contextual similarity and each of
the human scores were 0.24 for grammaticality and
0.21 for meaning, respectively. Although they are ri-
valing the best results reported in (Chan et al2011),
i.e., 0.24 and 0.21, similarity metrics should be fur-
ther investigated to realize a more accurate filtering.
6 Conclusion
In this paper, we exploited general patterns under-
lying paraphrases to acquire automatically a large
number of high-quality paraphrase pairs using both
bilingual parallel and monolingual non-parallel cor-
pora. Experiments using two sets of corpora demon-
strated that our method is able to leverage informa-
tion in a relatively small bilingual parallel corpus
to exploit large amounts of information in a rela-
tively large monolingual non-parallel corpus. Hu-
man evaluation through a paraphrase substitution
test revealed that the acquired paraphrases are gen-
erally of reasonable quality. Our original objective
was to extract from monolingual corpora a large
quantity of paraphrases whose quality is as high as
that of paraphrases from bilingual parallel corpora.
We have met the quantity part of the objective, and
have come close to meeting the quality part.
There are three main directions for our future
work. First, we intend to carry out in-depth anal-
yses of the proposed method. For instance, while
we showed that the performance of phrase substi-
tution could be improved by removing noisy seed
paraphrases, this also strongly affected the quan-
tity. We will therefore investigate similarity metrics
in our future work. Other interesting questions re-
lated to the work presented here are, as mentioned in
Section 4.2, exploitation of patterns with more than
one variable, learning curve experiments with dif-
ferent amounts of monolingual data, and compari-
son of in-domain and general-purpose monolingual
corpora. Second, we have an interest in exploiting
sophisticated paraphrase patterns; for instance, by
inducing patterns hierarchically (recursively) and in-
corporating lexical resources such as those exempli-
fied in (4). Finally, the developed paraphrase col-
lection will be attested through applications, such
as sentence compression (Cohn and Lapata, 2008;
Ganitkevitch et al2011) and machine translation
(Callison-Burch et al2006; Marton et al2009).
Acknowledgments
We are deeply grateful to our colleagues at National
Research Council Canada, especially George Foster,
Eric Joanis, and Samuel Larkin, for their technical
support. The first author is currently a JSPS (the
Japan Society for the Promotion of Science) Post-
doctoral Fellow for Research Abroad.
640
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 597?604.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 50?57.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Hu-
man Language Technology Conference and the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 16?23.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 161?170.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 17?24.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
196?205.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van-
Durme. 2011. Reranking bilingually extracted para-
phrases using monolingual distributional similarity. In
Proceedings of the Workshop on Geometrical Models
of Natual Language Semantics (GEMS), pages 33?42.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 190?200.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING), pages 137?144.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalua-
tion of machine translation systems. In Proceedings of
the 6th Workshop on Statistical Machine Translation
(WMT), pages 85?91.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th International Conference
on Computational Linguistics (COLING), pages 350?
356.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
translation using source language paraphrase lattices.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 420?429.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the patent
translation task at the NTCIR-8 workshop. In Pro-
ceedings of NTCIR-8 Workshop Meeting, pages 371?
376.
Atsushi Fujita, Shuhei Kato, Naoki Kato, and Satoshi
Sato. 2007. A compositional approach toward dy-
namic phrasal thesaurus. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing (WTEP), pages 151?158.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel cor-
pora for text-to-text generation. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1168?1179.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. John Wiley & Sons.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jun?ichi Kazama, and Sadao Kurohashi. 2011. Ex-
tracting paraphrases from definition sentences on the
Web. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1087?1097.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 341?348.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the 2007 Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975.
Maurice Kendall. 1938. A new measure of rank correla-
tion. Biometrika, 30(1-2):81?93.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Con-
ference and the North American Chapter of the Asso-
641
ciation for Computational Linguistics (HLT-NAACL),
pages 48?54.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Proceedings of Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT), pages 145?
153.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Nitin Madnani and Bonnie J. Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved statistical machine translation using
monolingually-derived paraphrases. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 381?390.
Yuval Marton, Ahmed El Kholy, and Nizar Habash.
2011. Filtering antonymous, trend-contrasting, and
polarity-dissimilar distributional paraphrases for im-
proving statistical machine translation. In Proceedings
of the 6th Workshop on Statistical Machine Translation
(WMT), pages 237?249.
Aure?lien Max. 2010. Example-based paraphrasing for
improved phrase-based statistical machine translation.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 656?666.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the Web.
In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 119?130.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of the 2003 Human Language Technol-
ogy Conference and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 102?109.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Roland Kuhn, Joel Martin, and Aaron Tikuisis.
2005. PORTAGE: A phrase-based machine transla-
tion system. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 129?132.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of the 2002
Human Language Technology Conference (HLT).
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING). 849-856.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation, pages 122?125.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting paraphrase patterns from bilin-
gual parallel corpora. Natural Language Engineering,
15(4):503?526.
642
Proceedings of NAACL-HLT 2013, pages 938?946,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Adaptation of Reordering Models for Statistical Machine Translation
Boxing Chen, George Foster and Roland Kuhn
National Research Council Canada
first.last@nrc-cnrc.gc.ca
Abstract
Previous research on domain adaptation (DA)
for statistical machine translation (SMT) has
mainly focused on the translation model (TM)
and the language model (LM). To the best of
our knowledge, there is no previous work on
reordering model (RM) adaptation for phrase-
based SMT. In this paper, we demonstrate
that mixture model adaptation of a lexical-
ized RM can significantly improve SMT per-
formance, even when the system already con-
tains a domain-adapted TM and LM. We find
that, surprisingly, different training corpora
can vary widely in their reordering character-
istics for particular phrase pairs. Furthermore,
particular training corpora may be highly suit-
able for training the TM or the LM, but unsuit-
able for training the RM, or vice versa, so mix-
ture weights for these models should be esti-
mated separately. An additional contribution
of the paper is to propose two improvements
to mixture model adaptation: smoothing the
in-domain sample, and weighting instances
by document frequency. Applied to mixture
RMs in our experiments, these techniques (es-
pecially smoothing) yield significant perfor-
mance improvements.
1 Introduction
A phrase-based statistical machine translation
(SMT) system typically has three main components:
a translation model (TM) that contains information
about how to translate word sequences (phrases)
from the source language to the target language,
a language model (LM) that contains information
about probable word sequences in the target lan-
guage, and a reordering model (RM) that indicates
how the order of words in the source sentence is
likely to influence the order of words in the target
sentence. The TM and the RM are trained on parallel
data, and the LM is trained on target-language data.
Usage of language and therefore the best translation
practice differs widely across genres, topics, and di-
alects, and even depends on a particular author?s or
publication?s style; the word ?domain? is often used
to indicate a particular combination of all these fac-
tors. Unless there is a perfect match between the
training data domain and the (test) domain in which
the SMT system will be used, one can often get bet-
ter performance by adapting the system to the test
domain.
In offline domain adaptation, the system is pro-
vided with a sample of translated sentences from
the test domain prior to deployment. In a popular
variant of offline adaptation, linear mixture model
adaptation, each training corpus is used to gener-
ate a separate model component that forms part of
a linear combination, and the sample is used to as-
sign a weight to each component (Foster and Kuhn,
2007). If the sample resembles some of the corpora
more than others, those corpora will receive higher
weights in the combination.
Previous research on domain adaptation for SMT
has focused on the TM and the LM. Such research
is easily motivated: translations across domains are
unreliable. For example, the Chinese translation
of the English word ?mouse? would most likely be
?laoshu ??? if the topic is the animal; if the topic
is computer hardware, its translation would most
938
likely be ?shubiao???. However, when the trans-
lation is for people in Taiwan, even when the topic
is computer hardware, its translation would more
likely be ?huashu ???. It is intuitively obvious
why TM and LM adaptation would be helpful here.
By contrast, it is not at all obvious that RM model
adaptation will improve SMT performace. One
would expect reordering behaviour to be characteris-
tic of a particular language pair, but not of particular
domains. At most, one might think that reordering
is lexicalized?perhaps, (for instance) in translating
from Chinese to English, or from Arabic to English,
there are certain words whose English translations
tend to undergo long-distance movement from their
original positions, while others stay close to their
original positions. However, one would not expect
a particular Chinese adverb or a particular Arabic
noun to undergo long-distance movement when be-
ing translated into English in one domain, but not in
others. Nevertheless, that is what we observe: see
section 5 below.
This paper shows that RM adaptation improves
the performance of our phrase-based SMT system.
In our implementation, the RM is adapted by means
of a linear mixture model, but it is likely that other
forms of RM adaptation would also work. We ob-
tain even more effective RM adaptation by smooth-
ing the in-domain sample and by weighting orienta-
tion counts by the document frequency of the phrase
pair. Both improvements could be applied to the TM
or the LM as well, though we have not done so.
Finally, the paper analyzes reordering to see why
RM adaptation works. There seem to be two fac-
tors at work. First, the reordering behaviour of
words and phrases often differs dramatically from
one bilingual corpus to another. Second, there are
corpora (for instance, comparable corpora and bilin-
gual lexicons) which may contain very valuable in-
formation for the TM, but which are poor sources
of RM information; RM adaptation downweights in-
formation from these corpora significantly, and thus
improves the overall quality of the RM.
2 Reordering Model
In early SMT systems, such as (Koehn, 2004),
changes in word order when a sentence is trans-
lated were modeled by means of a penalty that is in-
curred when the decoder chooses, as the next source
phrase to be translated, a phrase that does not imme-
diately follow the previously translated source sen-
tence. Thus, the system penalizes deviations from
monotone order, with the magnitude of the penalty
being proportional to distance in the source sentence
between the end of the previously translated source
phrase and the start of the newly chosen source
phrase.
Many SMT systems, including our own, still use
this distance-based penalty as a feature. However,
starting with (Tillmann and Zhang, 2005; Koehn
et al, 2005), a more sophisticated type of reorder-
ing model has often been adopted as well, and has
yielded consistent performance gains. This type of
RM typically identifies three possible orientations
for a newly chosen source phrase: monotone (M),
swap (S), and discontinuous (D). The M orientation
occurs when the newly chosen phrase is immedi-
ately to the right of the previously translated phrase
in the source sentence, the S orientation occurs when
the new phrase is immediately to the left of the pre-
vious phrase, and the D orientation covers all other
cases.1 This type of RM is lexicalized: the estimated
probabilities of M, S and D depend on the source-
language and target-language words in both the pre-
vious phrase pair and the newly chosen one.
Galley and Manning (2008) proposed a ?hierar-
chical? lexicalized RM in which the orientation (M,
S, or D) is determined not by individual phrase pairs,
but by blocks. A block is the largest contiguous se-
quence of phrase pairs that satisfies the phrase pair
consistency requirement of having no external links.
Thus, classification of the orientation of a newly
chosen phrase as M, S, or D is carried out as if the
decoder always chose the longest possible source
phrase in the past, and will choose the longest pos-
sible source phrase in the future.
The RM used in this paper is hierarchical and lex-
icalized. For a given phrase pair (f , e), we estimate
the probabilities that it will be in an M, S, or D ori-
entation o with respect to the previous phrase pair
and the following phrase pair (two separate distri-
butions). Orientation counts c(o, f, e) are obtained
from a word-aligned corpus using the method de-
1Some researchers have distinguished between left and right
versions of the D orientation, but this 4-orientation scheme has
not yielded significant gains over the 3-orientation one.
939
scribed in (Cherry et al, 2012), and corresponding
probabilities p(o|f, e) are estimated using recursive
MAP smoothing:
p(o|f, e) =
c(o, f, e) + ?f p(o|f) + ?e p(o|e)
c(f, e) + ?f + ?e
p(o|f) =
c(o, f) + ?g p(o)
c(f) + ?g
p(o) =
c(o) + ?u/3
c(?) + ?u
, (1)
where p(o|e) is defined analogously to p(o|f), and
the four smoothing parameters ?e, ?f , ?g, and ?u
are set to values that minimize the perplexity of the
resulting model on held-out data.
During decoding, orientations with respect to the
previous context are obtained from a shift-reduce
parser, and orientations with respect to following
context are approximated using the coverage vector
(Cherry et al, 2012).
3 RM Adaptation
3.1 Linear mixture model
Following previous work (Foster and Kuhn, 2007;
Foster et al, 2010), we adopt the linear mixture
model technique for RM adaptation. This technique
trains separate models for each training corpus, then
learns weights for each of the models and combines
the weighted component models into a single model.
If we have N sub-corpora, the global reordering
model probabilities p(o|f, e) are computed as in (2):
p(o|f, e) =
N?
i=1
?i pi(o|f, e) (2)
where pi(o|f, e) is the reordering model trained on
sub-corpus i, and ?i is its weight.
Following (Foster et al, 2010), we use the EM
algorithm to learn the weights that maximize the
probability of phrase-pair orientations in the devel-
opment set (in-domain data):
?? = argmax
?
?
o,f,e
p?(o, f, e) log
N?
i=1
?i pi(o|f, e)
(3)
where p?(o, f, e) is the empirical distribution of
counts in the dev set (proportional to c(o, f, e)). Two
separate sets of mixing weights are learned: one for
the distribution with respect to the previous phrase
pair, and one for the next phrase pair.
3.2 Development set smoothing
In Equation 3, p?(o, f, e) is extracted from the in-
domain development set. Since dev sets for SMT
systems are typically small (1,000-3,000 sentences),
we apply smoothing to this RM. We first obtain
a smoothed conditional distribution p(o|f, e) using
the MAP technique described above, then multiply
by the empirical marginal p?(e, f) to obtain a final
smoothed joint distribution p(o, f, e).
There is nothing about this idea that limits it to
the RM: smoothing could be applied to the statistics
in the dev that are used to estimate a mixture TM
or LM, in order to mitigate over-fitting. However,
we note that, compared to the TM, the over-fitting
problem is likely to be more acute for the RM, since
it splits counts for each phrase pair into three cate-
gories.
3.3 Document-frequency weighting
Mixture models, like the RM in this paper, depend
on the existence of multiple training corpora, with
each sub-corpus nominally representing a domain.
A recent paper suggests that some phrase pairs be-
long to general language, while others are domain-
specific (Foster et al, 2010). If a phrase pair exists
in all training corpora, it probably belongs to general
language; on the other hand, if it appears in only
one or two training corpora, it is more likely to be
domain-specific.
We were interested in seeing whether information
about domain-specificity could improve the estima-
tion of mixture RM weights. The intuition is that
phrase pairs that belong to general language should
contribute more to determining sub-corpus weights,
since they are the ones whose reordering behaviour
is most likely to shift with domain. To capture this
intuition, we multiplied the empirical distribution in
(3) by the following factor, inspired by the standard
document-frequency formula:
D(f, e) = log(DF (f, e) +K), (4)
where DF (f, e) is the number of sub-corpora
that (f, e) appears in, and K is an empirically-
determined smoothing term.
940
corpus # segs # en tok % genres
fbis 250K 10.5M 3.7 nw
financial 90K 2.5M 0.9 financial
gale bc 79K 1.3M 0.5 bc
gale bn 75K 1.8M 0.6 bn ng
gale nw 25K 696K 0.2 nw
gale wl 24K 596K 0.2 wl
hkh 1.3M 39.5M 14.0 Hansard
hkl 400K 9.3M 3.3 legal
hkn 702K 16.6M 5.9 nw
isi 558K 18.0M 6.4 nw
lex&ne 1.3M 2.0M 0.7 lexicon
others nw 146K 5.2M 1.8 nw
sinorama 282K 10.0M 3.5 nw
un 5.0M 164M 58.2 un
TOTAL 10.1M 283M 100.0 (all)
devtest
tune 1,506 161K nw wl
NIST06 1,664 189K nw bn ng
NIST08 1,357 164K nw wl
Table 1: NIST Chinese-English data. In the gen-
res column: nw=newswire, bc=broadcast conversa-
tion, bn=broadcast news, wl=weblog, ng=newsgroup,
un=United Nations proceedings.
4 Experiments
4.1 Data setting
We carried out experiments in two different settings,
both involving data from NIST Open MT 2012.2
The first setting uses data from the Chinese to En-
glish constrained track, comprising 283M English
tokens. We manually identified 14 sub-corpora on
the basis of genres and origins. Table 1 summarizes
the statistics and genres of all the training corpora
and the development and test sets; for the training
corpora, we show their size in number of words as
a percentage of all training data. Most training cor-
pora consist of parallel sentence pairs. The isi and
lex&ne corpora are exceptions: the former is ex-
tracted from comparable data, while the latter is a
lexicon that includes many named entities. The de-
velopment set (tune) was taken from the NIST 2005
evaluation set, augmented with some web-genre ma-
terial reserved from other NIST corpora.
2http://www.nist.gov/itl/iad/mig/openmt12.cfm
corpus # segs # en toks % genres
gale bc 57K 1.6M 3.3 bc
gale bn 45K 1.2M 2.5 bn
gale ng 21K 491K 1.0 ng
gale nw 17K 659K 1.4 nw
gale wl 24K 590K 1.2 wl
isi 1,124K 34.7M 72.6 nw
other nw 224K 8.7M 18.2 nw
TOTAL 1,512K 47.8M 100.0 (all)
devtest
NIST06 1,664 202K nw wl
NIST08 1,360 205K nw wl
NIST09 1,313 187K nw wl
Table 2: NIST Arabic-English data. In the gen-
res column: nw=newswire, bc=broadcast conversation,
bn=broadcase news, ng=newsgroup, wl=weblog.
The second setting uses NIST 2012 Arabic to En-
glish data, but excluding the UN data. There are
about 47.8 million English running words in these
training data. We manually grouped the training data
into 7 groups according to genre and origin. Ta-
ble 2 summarizes the statistics and genres of all the
training corpora and the development and test sets.
Note that for this language pair, the comparable isi
data represent a large proportion of the training data:
72% of the English words. We use the evaluation
sets from NIST 2006, 2008, and 2009 as our devel-
opment set and two test sets, respectively.
4.2 System
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et al,
2007). The corpus was word-aligned using IBM2,
HMM, and IBM4 models, and the phrase table was
the union of phrase pairs extracted from these sepa-
rate alignments, with a length limit of 7. The trans-
lation model was smoothed in both directions with
KN smoothing (Chen et al, 2011). The DF smooth-
ing term K in equation 4 was set to 0.1 using held-
out optimization. We use the hierarchical lexical-
ized RM described above, with a distortion limit of
7. Other features include lexical weighting in both
directions, word count, a distance-based RM, a 4-
gram LM trained on the target side of the parallel
data, and a 6-gram English Gigaword LM. The sys-
941
system Chinese Arabic
baseline 31.7 46.8
baseline+loglin 29.6 45.9
RMA 31.8 47.7**
RMA+DF 32.2* 47.9**
RMA+dev smoothing 32.3* 48.3**
RMA+dev smoothing+DF 32.8** 48.2**
Table 3: Results for variants of RM adaptation.
system Chinese Arabic
LM+TM adaptation 33.2 47.7
+RMA+dev-smoothing+DF 33.5 48.4**
Table 4: RM adaptation improves over a baseline con-
taining adapted LMs and TMs.
tem was tuned with batch lattice MIRA (Cherry and
Foster, 2012).
4.3 Results
For our main baseline, we simply concatenate all
training data. We also tried augmenting this with
separate log-linear features corresponding to sub-
corpus-specific RMs. Our metric is case-insensitvie
IBM BLEU-4 (Papineni et al, 2002); we report
BLEU scores averaged across both test sets. Follow-
ing (Koehn, 2004), we use the bootstrap-resampling
test to do significance testing. In tables 3 to 5, *
and ** denote significant gains over the baseline at
p < 0.05 and p < 0.01 levels, respectively.
Table 3 shows that reordering model adaptation
helps in both data settings. Adding either document-
frequency weighting (equation 4) or dev-set smooth-
ing makes the improvement significant in both set-
tings. Using both techniques together yields highly
significant improvements.
Our second experiment measures the improve-
ment from RM adaptation over a baseline that
includes adapted LMs and TMs. We use the
same technique?linear mixtures with EM-tuned
weights?to adapt these models. Table 4 shows that
adapting the RM gives gains over this strong base-
line for both language pairs; improvements are sig-
nificant in the case of Arabic to English.
The third experiment breaks down the gains in the
last line of table 4 by individual adapted model. As
shown in table 5, RM adaptation yielded the largest
system Chinese Arabic
baseline 31.7 46.8
LM adaptation 32.1* 47.0
TM adaptation 33.0** 47.5**
RM adaptation 32.8** 48.2**
Table 5: Comparison of LM, TM, and RM adaptation.
improvement on Arabic, while TM adaptation did
best on Chinese. Surprisingly, both methods sig-
nificantly outperformed LM adaptation, which only
achieved significant gains over the baseline for Chi-
nese.
5 Analysis
Why does RM adaptation work? Intuitively, one
would think that reordering behaviour for a given
phrase pair should not be much affected by domain,
making RM adaptation pointless. That is probably
why (as far as we know) no-one has tried it before.
In this section, we describe three factors that account
for at least part of the observed gains.
5.1 Weighting by corpus quality
One answer to the above question is that some cor-
pora are better for training RMs than others. Fur-
thermore, corpora that are good for training the LM
or TM are not necessarily good for training the RM,
and vice versa. Tables 6 and 7 illustrate this. These
list the weights assigned to various sub-corpora for
LM, TM, and RM mixture models.
The weights assigned to the isi sub-corpus in par-
ticular exhibit a striking pattern. These are high in
the LM mixtures, moderate in the TM mixtures, and
very low in the RM mixtures. When one considers
that isi contains 72.6% of the English words in the
Arabic training data (see table 2), its weight of 0.01
in the RM mixture is remarkable.
On reflection, it makes sense that EM would as-
sign weights in the order it does. The isi corpus
consists of comparable data: sentence pairs whose
source- and target-language sides are similar, but of-
ten not mutual translations. These are a valuable
source of in-domain n-grams for the LM; a some-
what noisy source of in-domain phrase pairs for the
TM; and an unreliable source of re-ordering patterns
for the RM. Figure 1 shows this. Although the two
942
LM TM RM
isi (0.23) un (0.29) un (0.21)
gale nw (0.11) fbis (0.15) gale nw (0.13)
un (0.11) hkh (0.10) lex&ne (0.12)
sino. (0.09) gale nw (0.09) hkh (0.08)
fbis (0.08) gale bn (0.07) fbis (0.08)
fin. (0.07) oth nw (0.06) gale bn (0.08)
oth nw (0.07) sino. (0.06) gale wl (0.06)
gale bn (0.07) isi (0.05) gale bc (0.06)
gale wl (0.06) hkn (0.04) hkn (0.04)
hkh (0.06) fin. (0.04) fin. (0.04)
hkn (0.03) gale bc (0.03) oth nw (0.03)
gale bc (0.02) gale wl (0.02) hkl (0.03)
lex&ne (0.00) lex&ne (0.00) isi (0.01)
hkl (0.00) hkl (0.00) sino. (0.01)
Table 6: Chinese-English sub-corpora for LM, TM, and
RM mixture models, ordered by mixture weight.
LM TM RM
isi (0.41) isi (0.35) gale bc (0.21)
oth nw (0.19) oth nw (0.29) gale ng (0.20)
gale ng (0.15) gale bc (0.10) gale nw (0.20)
gale wl (0.09) gale ng (0.08) oth nw (0.13)
gale nw (0.07) gale bn (0.07) gale ng (0.12)
gale bc (0.05) gale nw (0.07) gale wb (0.11)
gale bn (0.02) gale wl (0.05) isi (0.01)
Table 7: Arabic-English sub-corpora for LM, TM, and
RM mixture models, ordered by mixture weight.
sides of the comparable data are similar, they give
the misleading impression that the phrases labeled
1, 2, 3 in the Chinese source should be reordered as
2, 3, 1 in English. We show a reference translation
of the Chinese source (not found in the comparable
data) that reorders the phrases as 1, 3, 2.
Thus, RM adaptation allows the RM to learn that
certain corpora whose reordering information is of
lower quality corpora should have lower weights.
The optimal weights for corpora inside an RM may
be different from the optimal weights inside a TM or
LM.
5.2 Weighting by domain match
So is this all that RM adaptation does: downweight
poor-quality data? We believe there is more to
RM adaptation than that. Specifically, even if one
 REF: The American list of goods that would incur tariffs in retaliation would certainly not be accepted by the Chinese government.  SRC: ??(1) ? ?? ??? ??(2) ?? ?? ? ??  ?(3)?  TGT: And the Chinese(2) side would certainly not accept(3)  the unreasonable demands put forward by the Americans(1) concerning the protection of intellectual property rights .  
Figure 1: Example of sentence pair from comparable
data; underlined words with the same number are trans-
lations of each other
Corpus M S D Count
fbis 0.50 0.07 0.43 685
financial 0.32 0.28 0.41 65
gale bc 0.60 0.10 0.31 50
gale bn 0.47 0.15 0.37 109
gale nw 0.51 0.05 0.44 326
gale wl 0.42 0.26 0.32 52
hkh 0.29 0.23 0.48 130
hkl 0.28 0.16 0.56 263
hkn 0.30 0.27 0.43 241
isi 0.24 0.16 0.60 240
lex&ne 0.94 0.03 0.02 1
others nw 0.29 0.16 0.55 23
sinorama 0.44 0.07 0.49 110
un 0.37 0.10 0.53 15
dev 0.46 0.24 0.31 11
Table 8: Orientation frequencies for the phrase pair ??
? immediately?, with respect to the previous phrase.
considers only high-quality data for training RMs
(ignoring comparable data, etc.) one sees differ-
ences in reordering behaviour between different do-
mains. This isn?t just because of differences in word
frequencies between domains, because we observe
domain-dependent differences in reordering for the
same phrase pair. Two examples are given below:
one Chinese-English, one Arabic-English.
Table 8 shows reordering data for the phrase
pair ??? immediately? in various corpora. No-
tice the strong difference in behaviour between the
three Hong Kong corpora?hkh, hkl and hkn?and
some of the other corpora, for instance fbis. In the
943
Corpus M S D Count
gale bc 0.50 0.27 0.22 233
gale bn 0.56 0.21 0.23 226
gale ng 0.51 0.13 0.37 295
gale nw 0.47 0.20 0.33 167
gale wl 0.56 0.18 0.26 127
isi 0.50 0.06 0.44 5502
other nw 0.50 0.16 0.34 1450
dev 0.75 0.12 0.13 52
Table 9: Orientation frequencies for the phrase pair
?work AlEml? with respect to the previous phrase.
Hong Kong corpora, immediately is much less likely
(probability of around 0.3) to be associated with a
monotone (M) orientation than it is in fbis (proba-
bility of 0.5). This phrase pair is relatively frequent
in both corpora, so this disparity seems too great to
be due to chance.
Table 9 shows reordering behaviour for the phrase
pair ?work AlEml?3 across different sub-corpora.
As in the Chinese example, there appear to be sig-
nificant differences in reordering patterns for cer-
tain corpora. For instance, gale bc swaps this well-
attested phrase pair twice as often (probability of
0.27) as gale ng (probability of 0.13).
For Chinese, it is possible that dialect plays a role
in reordering behaviour. In theory, Mandarin Chi-
nese is a single language which is quite different,
especially in spoken form, from other languages of
China such as Cantonese, Hokkien, Shanghainese,
and so on. In practice, many speakers of Mandarin
may be unconsciously influenced by other languages
that they speak, or by other languages that they don?t
speak but that have an influence over people they in-
teract with frequently. Word order can be affected
by this: the Mandarin of Mainland China, Hong
Kong and Taiwan sometimes has slightly different
word order. Hong Kong Mandarin can be somewhat
influenced by Cantonese, and Taiwan Mandarin by
Hokkien. For instance, if a verb is modified by an
adverb in Mandarin, the standard word order is ?ad-
verb verb?. However, since in Cantonese, ?verb ad-
verb? is a more common word order, speakers and
writers of Mandarin in Hong Kong may adopt the
3We represent the Arabic word AlEml in its Buckwalter
transliteration.
  	
 

 	 

Figure 2: An example of different word ordering in Man-
darin from different area.
?verb adverb? order in that language as well. Figure
2 shows how a different word order in the Mandarin
source affects reordering when translating into En-
glish. Perhaps in situations where different training
corpora represent different dialects, RM adaptation
involves an element of dialect adaptation. We are ea-
ger to test this hypothesis for Arabic?different di-
alects of Arabic are much more different from each
other than dialects of Mandarin, and reordering is
often one of the differences?but we do not have ac-
cess to Arabic training, dev, and test data in which
the dialects are clearly separated.
It is possible that RM adaptation also has an el-
ement of genre adaptation. We have not yet been
able to confirm or refute this hypothesis. However,
whatever is causing the corpus-dependent reorder-
ing patterns for particular phrase pairs shown in the
two tables above, it is clear that they may explain
the performance improvements we observe for RM
adaptation in our experiments.
5.3 Penalizing highly-specific phrase pairs
In section 3.3 we described our strategy for giving
general (high document-frequency) phrase pairs that
occur in the dev set more influence in determining
mixing weights. An artifact of our implementation
applies a similar strategy to the probability estimates
for all phrase pairs in the model. This is that 0 prob-
abilities are assigned to all orientations whenever a
phrase pair is absent from a particular sub-corpus.
Thus, for example, a pair (f, e) that occurs only
in sub-corpus iwill receive a probability p(o|f, e) =
?i pi(o|f, e) in the mixture model (equation 2).
Since ?i ? 1, this amounts to a penalty on pairs
that occur in few sub-corpora, especially ones with
low mixture weights.
The resulting mixture model is deficient (non-
944
normalized), but easy to fix by backing off to a
global distribution such as p(o) in equation 1. How-
ever, we found that this ?fix? caused large drops in
performance, for instance from the Arabic BLEU
score of 48.3 reported in table 3 to 46.0. We there-
fore retained the original strategy, which can be seen
as a form of instance weighting. Moreover, it is one
that is particularly effective in the RM, since, com-
pared to a similar strategy in the TM (which we also
employ), it applies to whole phrase pairs and results
in much larger penalties.
6 Related work
Domain adaptation is an active topic in the NLP re-
search community. Its application to SMT systems
has recently received considerable attention. Previ-
ous work on SMT adaptation has mainly focused
on translation model (TM) and language model
(LM) adaptation. Approaches that have been tried
for SMT model adaptation include mixture models,
transductive learning, data selection, data weighting,
and phrase sense disambiguation.
Research on mixture models has considered both
linear and log-linear mixtures. Both were studied
in (Foster and Kuhn, 2007), which concluded that
the best approach was to combine sub-models of
the same type (for instance, several different TMs
or several different LMs) linearly, while combining
models of different types (for instance, a mixture
TM with a mixture LM) log-linearly. (Koehn and
Schroeder, 2007), instead, opted for combining the
sub-models directly in the SMT log-linear frame-
work.
In transductive learning, an MT system trained on
general domain data is used to translate in-domain
monolingual data. The resulting bilingual sentence
pairs are then used as additional training data (Ueff-
ing et al, 2007; Chen et al, 2008; Schwenk, 2008;
Bertoldi and Federico, 2009).
Data selection approaches (Zhao et al, 2004; Lu?
et al, 2007; Moore and Lewis, 2010; Axelrod et
al., 2011) search for bilingual sentence pairs that are
similar to the in-domain ?dev? data, then add them
to the training data. The selection criteria are typi-
cally related to the TM, though the newly found data
will be used for training not only the TM but also the
LM and RM.
Data weighting approaches (Matsoukas et al,
2009; Foster et al, 2010; Huang and Xiang, 2010;
Phillips and Brown, 2011; Sennrich, 2012) use a
rich feature set to decide on weights for the train-
ing data, at the sentence or phrase pair level. For
instance, a sentence from a corpus whose domain is
far from that of the dev set would typically receive
a low weight, but sentences in this corpus that ap-
pear to be of a general nature might receive higher
weights.
The 2012 JHU workshop on Domain Adapta-
tion for MT 4 proposed phrase sense disambiguation
(PSD) for translation model adaptation. In this ap-
proach, the context of a phrase helps the system to
find the appropriate translation.
All of the above work focuses on either TM or
LM domain adaptation.
7 Conclusions
In this paper, we adapt the lexicalized reordering
model (RM) of an SMT system to the domain in
which the system will operate using a mixture model
approach. Domain adaptation of translation mod-
els (TMs) and language models (LMs) has become
common for SMT systems, but to our knowledge
this is the first attempt in the literature to adapt the
RM. Our experiments demonstrate that RM adap-
tation can significantly improve translation quality,
even when the system already has TM and LM adap-
tation. We also experimented with two modifica-
tions to linear mixture model adaptation: dev set
smoothing and weighting orientation counts with
document frequency of phrase pairs. Both ideas
are potentially applicable to TM and LM adaptation.
Dev set smoothing, in particular, seems to improve
the performance of RM adaptation significantly. Fi-
nally, we investigate why RM adaptation helps SMT
performance. Three factors seem to be important:
downweighting information from corpora that are
less suitable for modeling reordering (such as com-
parable corpora), dialect/genre effects, and implicit
instance weighting.
4http://www.clsp.jhu.edu/workshops/archive/ws-
12/groups/dasmt
945
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In EMNLP 2011.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the 4th
Workshop on Statistical Machine Translation, Athens,
March. WMT.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for smt self-
enhancement. In ACL 2008.
Boxing Chen, Roland Kuhn, George Foster, and Howard
Johnson. 2011. Unpacking and transforming feature
functions: New ways to smooth phrase tables. In MT
Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In WMT 2012.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the ACL Work-
shop on Statistical Machine Translation, Prague, June.
WMT.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Boston.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model. In
EMNLP 2008, pages 848?856, Hawaii, October.
Fei Huang and Bing Xiang. 2010. Feature-rich discrimi-
native phrase rescoring for SMT. In COLING 2010.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, D. Talbot, and M. White. 2005. Edin-
burgh system description for the 2005 NIST MT eval-
uation. In Proceedings of Machine Translation Evalu-
ation Workshop.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL 2007, Demon-
stration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the 6th Conference of the As-
sociation for Machine Translation in the Americas,
Georgetown University, Washington D.C., October.
Springer-Verlag.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. In Proceedings
of the 2007 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Prague, Czech
Republic.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL
2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311?318, Philadel-
phia, July. ACL.
Aaron B. Phillips and Ralf D. Brown. 2011. Training
machine translation with a second-order taylor approx-
imation of weighted translation instances. In MT Sum-
mit 2011.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT 2008.
Rico Sennrich. 2012. Mixture-modeling with unsuper-
vised clusters for domain adaptation in statistical ma-
chine translation. In EACL 2012.
Christoph Tillmann and Tong Zhang. 2005. A localized
prediction model for statistical machine translation. In
Proceedings of the 43th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Ann Ar-
bor, Michigan, July. ACL.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Prague, Czech Republic, June. ACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING) 2004, Geneva, August.
946
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 834?843,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bilingual Sense Similarity for Statistical Machine Translation 
 
 
Boxing Chen, George Foster and Roland Kuhn 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca 
 
  
 
Abstract 
 
This paper proposes new algorithms to com-
pute the sense similarity between two units 
(words, phrases, rules, etc.) from parallel cor-
pora. The sense similarity scores are computed 
by using the vector space model.  We then ap-
ply the algorithms to statistical machine trans-
lation by computing the sense similarity be-
tween the source and target side of translation 
rule pairs. Similarity scores are used as addi-
tional features of the translation model to im-
prove translation performance. Significant im-
provements are obtained over a state-of-the-art 
hierarchical phrase-based machine translation 
system. 
1 Introduction 
The sense of a term can generally be inferred 
from its context. The underlying idea is that a 
term is characterized by the contexts it co-occurs 
with. This is also well known as the Distribu-
tional Hypothesis (Harris, 1954): terms occurring 
in similar contexts tend to have similar mean-
ings. There has been a lot of work to compute the 
sense similarity between terms based on their 
distribution in a corpus, such as (Hindle, 1990; 
Lund and Burgess, 1996; Landauer and Dumais, 
1997; Lin, 1998; Turney, 2001; Pantel and Lin, 
2002; Pado and Lapata, 2007).  
In the work just cited, a common procedure is 
followed. Given two terms to be compared, one 
first extracts various features for each term from 
their contexts in a corpus and forms a vector 
space model (VSM); then, one computes their 
similarity by using similarity functions. The fea-
tures include words within a surface window of a 
fixed size (Lund and Burgess, 1996), grammati-
cal dependencies (Lin, 1998; Pantel and Lin 
2002; Pado and Lapata, 2007), etc.  The similari-
ty function which has been most widely used is 
cosine distance (Salton and McGill, 1983); other 
similarity functions include Euclidean distance, 
City Block distance (Bullinaria and Levy; 2007), 
and Dice and Jaccard coefficients (Frakes and 
Baeza-Yates, 1992), etc. Measures of monolin-
gual sense similarity have been widely used in 
many applications, such as synonym recognizing 
(Landauer and Dumais, 1997), word clustering 
(Pantel and Lin 2002), word sense disambigua-
tion (Yuret and Yatbaz 2009), etc. 
Use of the vector space model to compute  
sense similarity has also been adapted to the mul-
tilingual condition,  based on the assumption that 
two terms with similar meanings often occur in 
comparable contexts across languages. Fung 
(1998) and Rapp (1999) adopted VSM for the 
application of extracting translation pairs from 
comparable or even unrelated corpora. The vec-
tors in different languages are first mapped to a 
common space using an initial bilingual dictio-
nary, and then compared. 
However, there is no previous work that uses 
the VSM to compute sense similarity for terms 
from parallel corpora. The sense similarities, i.e. 
the translation probabilities in a translation mod-
el, for units from parallel corpora are mainly 
based on the co-occurrence counts of the two 
units. Therefore, questions emerge: how good is 
the sense similarity computed via VSM for two 
units from parallel corpora? Is it useful for multi-
lingual applications, such as statistical machine 
translation (SMT)? 
In this paper, we try to answer these questions, 
focusing on sense similarity applied to the SMT 
task. For this task, translation rules are heuristi-
cally extracted from automatically word-aligned 
sentence pairs. Due to noise in the training cor-
pus or wrong word alignment, the source and 
target sides of some rules are not semantically 
equivalent, as can be seen from the following 
834
real examples which are taken from the rule table 
built on our training data (Section 5.1): 
?? ? X ?? ||| one of X (*) 
?? ? X ?? ||| one of X in the world    
?? ?? ||| many citizens 
?? ?? ||| many hong kong residents (*) 
The source and target sides of the rules with (*) 
at the end are not semantically equivalent; it 
seems likely that measuring the semantic similar-
ity from their context between the source and 
target sides of rules might be helpful to machine 
translation. 
In this work, we first propose new algorithms 
to compute the sense similarity between two 
units (unit here includes word, phrase, rule, etc.) 
in different languages by using their contexts. 
Second, we use the sense similarities between the 
source and target sides of a translation rule to 
improve statistical machine translation perfor-
mance.  
This work attempts to measure directly the 
sense similarity for units from different languag-
es by comparing their contexts1. Our contribution 
includes proposing new bilingual sense similarity 
algorithms and applying them to machine trans-
lation. 
We chose a hierarchical phrase-based SMT 
system as our baseline; thus, the units involved 
in computation of sense similarities are hierar-
chical rules. 
2 Hierarchical phrase-based MT system 
The hierarchical phrase-based translation method 
(Chiang, 2005; Chiang, 2007) is a formal syntax-
based translation modeling method; its transla-
tion model is a weighted synchronous context 
free grammar (SCFG). No explicit linguistic syn-
tactic information appears in the model. An 
SCFG rule has the following form: 
~,,???X
 
where X is a non-terminal symbol shared by all 
the rules; each rule has at most two non-
terminals. ?  (? ) is a source (target) string con-
sisting of terminal and non-terminal symbols. ~  
defines a one-to-one correspondence between 
non-terminals in ?  and ? . 
                                               
1
 There has been a lot of work (more details in Section 7) on 
applying word sense disambiguation (WSD) techniques in 
SMT for translation selection. However, WSD techniques 
for SMT do so indirectly, using source-side context to help 
select a particular translation for a source rule. 
 source target 
Ini. phr. ? ?? ? ?? he attended the meeting 
Rule 1 
Context 1 
? ?? ? X1 
?? 
he attended X1 
the, meeting 
Rule 2 
Context 2 
?? 
?, ??, ? 
the meeting 
he, attended 
Rule 3 
Context 3 
? X1?? 
??, ? 
he X1 the meeting 
attended 
Rule 4 
Context 4 
?? ? 
?,?? 
attended 
he, the, meeting 
 
Figure 1: example of hierarchical rule pairs and their 
context features. 
 
Rule frequencies are counted during rule ex-
traction over word-aligned sentence pairs, and 
they are normalized to estimate features on rules. 
Following (Chiang, 2005; Chiang, 2007), 4 fea-
tures are computed for each rule: 
? )|( ??P  and )|( ??P  are direct and in-
verse rule-based conditional probabilities; 
? )|( ??wP  and )|( ??wP are direct and in-
verse lexical weights (Koehn et al, 2003). 
Empirically, this method has yielded better 
performance on language pairs such as Chinese-
English than the phrase-based method because it 
permits phrases with gaps; it generalizes the 
normal phrase-based models in a way that allows 
long-distance reordering (Chiang, 2005; Chiang, 
2007). We use the Joshua implementation of the 
method for decoding (Li et al, 2009). 
3 Bag-of-Words Vector Space Model 
To compute the sense similarity via VSM, we 
follow the previous work (Lin, 1998) and 
represent the source and target side of a rule by 
feature vectors. In our work, each feature corres-
ponds to a context word which co-occurs with 
the translation rule. 
3.1 Context Features 
In the hierarchical phrase-based translation me-
thod, the translation rules are extracted by ab-
stracting some words from an initial phrase pair 
(Chiang, 2005). Consider a rule with non-
terminals on the source and target side; for a giv-
en instance of the rule (a particular phrase pair in 
the training corpus), the context will be the 
words instantiating the non-terminals. In turn, the 
context for the sub-phrases that instantiate the 
non-terminals will be the words in the remainder 
of the phrase pair. For example in Figure 1, if we 
835
have an initial phrase pair ? ?? ? ?? ||| he 
attended the meeting, and we extract four rules 
from this initial phrase: ? ?? ? X1 ||| he at-
tended X1, ?? ||| the meeting, ? X1?? ||| he 
X1 the meeting, and?? ? ||| attended. There-
fore, the and meeting are context features of tar-
get pattern he attended X1; he and attended are 
the context features of the meeting; attended is 
the context feature of he X1 the meeting;  also he, 
the and meeting are the context feature of at-
tended (in each case, there are also source-side 
context features).  
3.2 Bag-of-Words Model 
For each side of a translation rule pair, its context 
words are all collected from the training data, 
and two ?bags-of-words? which consist of col-
lections of source and target context words co-
occurring with the rule?s source and target sides 
are created. 
},...,,{
},...,,{
21
21
Je
If
eeeB
fffB
=
=
                        (1) 
where )1( Iifi ??  are source context words 
which co-occur with the source side of rule ? , 
and )1( Jje j ??  are target context words 
which co-occur with the target side of rule ? . 
Therefore, we can represent source and target 
sides of the rule by vectors fv
v
  and ev
v
 as in Eq-
uation (2): 
},...,,{
},...,,{
21
21
J
I
eeee
ffff
wwwv
wwwv
=
=
v
v
                     (2) 
where 
ifw  and jew are values for each source 
and target context feature; normally, these values 
are based on the counts of the words in the cor-
responding bags.  
3.3 Feature Weighting Schemes 
We use pointwise mutual information (Church et 
al., 1990) to compute the feature values. Let c 
( fBc ? or eBc ?  ) be a context word and 
),( crF  be the frequency count of a rule r (?  or 
? ) co-occurring with the context word c. The 
pointwise mutual information ),( crMI  is de-
fined as: 
N
cF
N
rF
N
crF
crMIcrw )(log)(log
),(log
),(),(
?
==
          (3) 
where N is the total frequency counts of all rules 
and their context words. Since we are using this 
value as a weight, following (Turney, 2001), we 
drop log, N and )(rF . Thus (3) simplifies to:  
)(
),(),(
cF
crF
crw =
                     (4) 
It can be seen as an estimate of )|( crP , the em-
pirical probability of observing r given c. 
A problem with )|( crP  is that it is biased 
towards infrequent words/features. We therefore 
smooth ),( crw  with add-k smoothing: 
kRcF
kcrF
kcrF
kcrF
crw R
i
i
+
+
=
+
+
=
?
=
)(
),(
)),((
),(),(
1
  (5) 
where k is a tunable global smoothing constant, 
and R is the number of rules. 
4 Similarity Functions 
There are many possibilities for calculating simi-
larities between bags-of-words in different lan-
guages. We consider IBM model 1 probabilities 
and cosine distance similarity functions. 
4.1 IBM Model 1 Probabilities 
For the IBM model 1 similarity function, we take 
the geometric mean of symmetrized conditional 
IBM model 1 (Brown et al, 1993) bag probabili-
ties, as in Equation (6). 
))|()|((),( feef BBPBBPsqrtsim ?=??       (6) 
To compute )|( ef BBP , IBM model 1 as-
sumes that all source words are conditionally 
independent, so that: 
 ?
=
=
I
i
eief BfpBBP
1
)|()|(                (7) 
To compute, we use a ?Noisy-OR? combina-
tion which has shown better performance than 
standard IBM model 1 probability, as described 
in (Zens and Ney, 2004): 
)|(1)|( eiei BfpBfp ?=                       (8) 
?
=
???
J
j
jiei efpBfp
1
))|(1(1)|(          (9) 
where )|( ei Bfp  is the probability that if  is not 
in the translation of eB , and  is the IBM model 1 
probability. 
4.2 Vector Space Mapping 
A common way to calculate semantic similarity 
is by vector space cosine distance; we will also 
836
use this similarity function in our algorithm. 
However, the two vectors in Equation (2) cannot 
be directly compared because the axes of their 
spaces represent different words in different lan-
guages, and also their dimensions I and J are not 
assured to be the same. Therefore, we need to 
first map a vector into the space of the other vec-
tor, so that the similarity can be calculated. Fung 
(1998) and Rapp (1999) map the vector one-
dimension-to-one-dimension (a context word is a 
dimension in each vector space) from one lan-
guage to another language via an initial bilingual 
dictionary. We follow (Zhao et al, 2004) to do 
vector space mapping.  
Our goal is ? given a source pattern ? to dis-
tinguish between the senses of its associated tar-
get patterns. Therefore, we map all vectors in 
target language into the vector space in the 
source language. What we want is a representa-
tion 
av
v
 in the source language space of the target 
vector 
ev
v
. To get 
av
v
, we can let ifaw , the weight 
of the ith source feature, be a linear combination 
over target features. That is to say, given a 
source feature weight for fi, each target feature 
weight is linked to it with some probability. So 
that we can calculate a transformed vector from 
the target vectors by calculating weights if
aw  us-
ing a translation lexicon: 
?
=
=
J
j
eji
f
a j
i wefw
1
)|Pr(                    (10) 
where )|( ji efp  is a lexical probability (we use 
IBM model 1 probability). Now the source vec-
tor and the mapped vector av
v
 have the same di-
mensions as shown in (11): 
},...,,{
},...,,{
21
21
I
I
f
a
f
a
f
aa
ffff
wwwv
wwwv
=
=
v
v
                   (11) 
4.3 Na?ve Cosine Distance Similarity 
The standard cosine distance is defined as the 
inner product of the two vectors fv
v
 and av
v
 nor-
malized by their norms. Based on Equation (10) 
and (11), it is easy to derive the similarity as fol-
lows: 
)()(
)|Pr(
||||),cos(),(
1
2
1
2
1 1
??
??
==
= =
=
?
?
==
I
i
f
a
I
I
f
I
i
J
j
ejif
af
af
af
i
i
ji
wsqrtwsqrt
wefw
vv
vv
vvsim vv
vv
vv??
         (12) 
where I and J are the number of the words in 
source and target bag-of-words; 
ifw  and jew are 
values of source and target features; ifaw  is the 
transformed weight mapped from all target fea-
tures to the source dimension at word fi. 
4.4 Improved Similarity Function 
To incorporate more information than the origi-
nal similarity functions ? IBM model 1 proba-
bilities in Equation (6) and na?ve cosine distance 
similarity function in Equation (12) ? we refine 
the similarity function and propose a new algo-
rithm.  
As shown in Figure 2, suppose that we have a 
rule pair ),( ?? . fullfC  and fulleC  are the contexts 
extracted according to the definition in section 3 
from the full training data for ?
 
and for ? , re-
spectively. coocfC and cooceC  are the contexts for 
?
 
  and ?   when ?
 
and ? co-occur. Obviously, 
they satisfy the constraints: fullf
cooc
f CC ?  and  
full
e
cooc
e CC ? .  Therefore, the original similarity 
functions are to compare the two context vectors 
built on full training data directly, as shown in 
Equation (13). 
),(),( fullefullf CCsimsim =??             (13) 
Then, we propose a new similarity function as 
follows: 
321 ),(),(),(
),(
???
??
cooc
e
full
e
cooc
e
cooc
f
cooc
f
full
f CCsimCCsimCCsim
sim
??
=
(14) 
where the parameters i? (i=1,2,3) can be tuned 
via minimal error rate training (MERT) (Och, 
2003). 
 
 
 
 
 
 
 
 
 
 
Figure 2: contexts for rule ?
 
  and ? . 
 
A unit?s sense is defined by all its contexts in 
the whole training data; it may have a lot of dif-
ferent senses in the whole training data. Howev-
er, when it is linked with another unit in the other 
language, its sense pool is constrained and is just 
?  
?  
full
fC  coocfC  
   
full
eC  cooceC  
837
a subset of the whole sense set. ),( coocffullf CCsim  
is the metric which evaluates the similarity be-
tween the whole sense pool of ?  and the sense 
pool when ?  co-occurs with ? ; 
),( coocefulle CCsim  is the analogous similarity me-
tric for ? . They range from 0 to 1. These two 
metrics both evaluate the similarity for two vec-
tors in the same language, so using cosine dis-
tance to compute the similarity is straightfor-
ward. And we can set a relatively large size for 
the vector, since it is not necessary to do vector 
mapping as the vectors are in the same language. 
),( coocecoocf CCsim  computes the similarity between 
the context vectors when ?
 
and ? co-occur. We 
may compute ),( coocecoocf CCsim by using IBM 
model 1 probability and cosine distance similari-
ty functions as Equation (6) and (12). Therefore, 
on top of the degree of bilingual semantic simi-
larity between a source and a target translation 
unit, we have also incorporated the monolingual 
semantic similarity between all occurrences of a 
source or target unit, and that unit?s occurrence 
as part of the given rule, into the sense similarity 
measure. 
5 Experiments 
We evaluate the algorithm of bilingual sense si-
milarity via machine translation. The sense simi-
larity scores are used as feature functions in the 
translation model. 
5.1 Data 
We evaluated with different language pairs: Chi-
nese-to-English, and German-to-English. For 
Chinese-to-English tasks, we carried out the ex-
periments in two data conditions. The first one is 
the large data condition, based on training data 
for the NIST 2  2009 evaluation Chinese-to-
English track. In particular, all the allowed bilin-
gual corpora except the UN corpus and Hong 
Kong Hansard corpus have been used for esti-
mating the translation model. The second one is 
the small data condition where only the FBIS3 
corpus is used to train the translation model. We 
trained two language models: the first one is a 4-
gram LM which is estimated on the target side of 
the texts used in the large data condition. The 
second LM is a 5-gram LM trained on the so-
                                               
2
 http://www.nist.gov/speech/tests/mt 
3
 LDC2003E14 
called English Gigaword corpus. Both language 
models are used for both tasks. 
We carried out experiments for translating 
Chinese to English. We use the same develop-
ment and test sets for the two data conditions. 
We first created a development set which used 
mainly data from the NIST 2005 test set, and 
also some balanced-genre web-text from the 
NIST training material. Evaluation was per-
formed on the NIST 2006 and 2008 test sets. Ta-
ble 1 gives figures for training, development and 
test corpora; |S| is the number of the sentences, 
and |W| is the number of running words. Four 
references are provided for all dev and test sets. 
 
   Chi Eng 
 
Parallel 
Train 
Large 
Data 
|S| 3,322K 
|W| 64.2M 62.6M 
Small 
Data 
|S| 245K 
|W| 9.0M 10.5M 
   Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics of training, dev, and test sets for 
Chinese-to-English task. 
 
For German-to-English tasks, we used WMT 
20064 data sets. The parallel training data con-
tains 21 million target words; both the dev set 
and test set contain 2000 sentences; one refer-
ence is provided for each source input sentence. 
Only the target-language half of the parallel 
training data are used to train the language model 
in this task.  
5.2 Results 
For the baseline, we train the translation model 
by following (Chiang, 2005; Chiang, 2007) and 
our decoder is Joshua5, an open-source hierar-
chical phrase-based machine translation system 
written in Java. Our evaluation metric is IBM 
BLEU (Papineni et al, 2002), which performs 
case-insensitive matching of n-grams up to n = 4. 
Following (Koehn, 2004), we use the bootstrap-
resampling test to do significance testing. 
By observing the results on dev set in the addi-
tional experiments, we first set the smoothing 
constant k in Equation (5) to 0.5. 
Then, we need to set the sizes of the vectors to 
balance the computing time and translation accu-
                                               
4
 http://www.statmt.org/wmt06/ 
5
 http://www.cs.jhu.edu/~ccb/joshua/index.html 
838
racy, i.e., we keep only the top N context words 
with the highest feature value for each side of a 
rule 6 . In the following, we use ?Alg1? to 
represent the original similarity functions which 
compare the two context vectors built on full 
training data, as in Equation (13); while we use 
?Alg2? to represent the improved similarity as in 
Equation (14). ?IBM? represents IBM model 1 
probabilities, and ?COS? represents cosine dis-
tance similarity function. 
After carrying out a series of additional expe-
riments on the small data condition and observ-
ing the results on the dev set, we set the size of 
the vector to 500 for Alg1; while for Alg2, we 
set the sizes of fullfC  and fulleC N1 to 1000, and the 
sizes of coocfC  and cooceC N2 to 100.  
The sizes of the vectors in Alg2 are set in the 
following process: first, we set N2 to 500 and let 
N1  range from 500 to 3,000, we observed that the 
dev set got best performance when N1 was 1000; 
then we set N1 to 1000 and let N1 range from 50 
to 1000, we got best performance when N1 =100. 
We use this setting as the default setting in all 
remaining experiments. 
 
Algorithm NIST?06 NIST?08 
Baseline 27.4 21.2 
Alg1 IBM 27.8* 21.5 
Alg1 COS 27.8* 21.5 
Alg2 IBM 27.9* 21.6* 
Alg2 COS 28.1** 21.7* 
 
Table 2: Results (BLEU%) of small data Chinese-to-
English NIST task. Alg1 represents the original simi-
larity functions as in Equation (13); while Alg2 
represents the improved similarity as in Equation 
(14). IBM represents IBM model 1 probability, and 
COS represents cosine distance similarity function. * 
or ** means result is significantly better than the 
baseline (p < 0.05 or p < 0.01, respectively). 
 
 Ch-En De-En 
Algorithm NIST?06 NIST?08 Test?06 
Baseline 31.0 23.8 26.9 
Alg2 IBM 31.5* 24.5** 27.2* 
Alg2 COS 31.6** 24.5** 27.3* 
 
Table 3: Results (BLEU%) of large data Chinese-to-
English NIST task and German-to-English WMT 
task. 
                                               
6
 We have also conducted additional experiments by remov-
ing the stop words from the context vectors; however, we 
did not observe any consistent improvement. So we filter 
the context vectors by only considering the feature values. 
Table 2 compares the performance of Alg1 
and Alg2 on the Chinese-to-English small data 
condition. Both Alg1 and Alg2 improved the 
performance over the baseline, and Alg2 ob-
tained slight and consistent improvements over 
Alg1. The improved similarity function Alg2 
makes it possible to incorporate monolingual 
semantic similarity on top of the bilingual se-
mantic similarity, thus it may improve the accu-
racy of the similarity estimate. Alg2 significantly 
improved the performance over the baseline. The 
Alg2 cosine similarity function got 0.7 BLEU-
score (p<0.01) improvement over the baseline 
for NIST 2006 test set, and a 0.5 BLEU-score 
(p<0.05) for NIST 2008 test set. 
Table 3 reports the performance of Alg2 on 
Chinese-to-English NIST large data condition 
and German-to-English WMT task. We can see 
that IBM model 1 and cosine distance similarity 
function both obtained significant improvement 
on all test sets of the two tasks. The two similari-
ty functions obtained comparable results. 
6 Analysis and Discussion 
6.1 Effect of Single Features 
In Alg2, the similarity score consists of three 
parts as in Equation (14): ),( coocffullf CCsim , 
),( coocefulle CCsim , and ),( coocecoocf CCsim ; where  
),( coocecoocf CCsim  could be computed by IBM mod-
el 1 probabilities ),( coocecoocfIBM CCsim  or cosine dis-
tance similarity function ),( coocecoocfCOS CCsim . 
Therefore, our first study is to determine which 
one of the above four features has the most im-
pact on the result. Table 4 shows the results ob-
tained by using each of the 4 features. First, we 
can see that ),( coocecoocfIBM CCsim  always gives a 
better improvement than ),( coocecoocfCOS CCsim . This 
is because  ),( coocecoocfIBM CCsim  scores are more 
diverse than the latter when the number of con-
text features is small (there are many rules that 
have only a few contexts.) For an extreme exam-
ple, suppose that there is only one context word 
in each vector of source and target context fea-
tures, and the translation probability of the two 
context words is not 0. In this case, 
),( coocecoocfIBM CCsim   reflects the translation proba-
bility of the context word pair, while 
),( coocecoocfCOS CCsim  is always 1.  
   Second, ),( coocffullf CCsim  and ),( coocefulle CCsim   
also give some improvements even when used 
839
independently. For a possible explanation, con-
sider the following example. The Chinese word 
?? ? can translate to ?red?, ?communist?, or 
?hong? (the transliteration of ?, when it is used 
in a person?s name).  Since these translations are 
likely to be associated with very different source 
contexts, each will have a low ),( coocffullf CCsim  
score.  Another Chinese word ?? may translate 
into synonymous words, such as ?brook?, 
?stream?, and ?rivulet?, each of which will have 
a high  ),( coocffullf CCsim  score. Clearly, ? is a 
more ?dangerous? word than??, since choos-
ing the wrong translation for it would be a bad 
mistake. But if the two words have similar trans-
lation distributions, the system cannot distinguish 
between them. The monolingual similarity scores 
give it the ability to avoid ?dangerous? words, 
and choose alternatives (such as larger phrase 
translations) when available. 
Third, the similarity function of Alg2 consis-
tently achieved further improvement by incorpo-
rating the monolingual similarities computed for 
the source and target side. This confirms the ef-
fectiveness of our algorithm. 
 
 CE_LD CE_SD 
testset (NIST) ?06 ?08 ?06 ?08 
Baseline 31.0 23.8 27.4 21.2 
),( coocffullf CCsim  31.1 24.3 27.5 21.3 
),( coocefulle CCsim  31.1 23.9 27.9 21.5 
),( coocecoocfIBM CCsim  31.4 24.3 27.9 21.5 
),( coocecoocfCOS CCsim  31.2 23.9 27.7 21.4 
Alg2 IBM 31.5 24.5 27.9 21.6 
Alg2 COS 31.6 24.5 28.1 21.7 
 
Table 4: Results (BLEU%) of Chinese-to-English 
large data (CE_LD) and small data (CE_SD) NIST 
task by applying one feature. 
6.2 Effect of Combining the Two Similari-
ties 
We then combine the two similarity scores by 
using both of them as features to see if we could 
obtain further improvement. In practice, we use 
the four features in Table 4 together.  
Table 5 reports the results on the small data 
condition. We observed further improvement on 
dev set, but failed to get the same improvements 
on test sets or even lost performance. Since the 
IBM+COS configuration has one extra feature, it 
is possible that it overfits the dev set. 
 
Algorithm Dev NIST?06 NIST?08 
Baseline 20.2 27.4 21.2 
Alg2 IBM 20.5 27.9 21.6 
Alg2 COS 20.6 28.1 21.7 
Alg2 IBM+COS 20.8 27.9 21.5 
 
Table 5: Results (BLEU%) for combination of two 
similarity scores. Further improvement was only ob-
tained on dev set but not on test sets. 
6.3 Comparison with Simple Contextual 
Features 
Now, we try to answer the question: can the si-
milarity features computed by the function in 
Equation (14) be replaced with some other sim-
ple features? We did additional experiments on 
small data Chinese-to-English task to test the 
following features: (15) and (16) represent the 
sum of the counts of the context words in Cfull, 
while (17) represents the proportion of words in 
the context of ?  that appeared in the context of 
the rule ( ?? , ); similarly, (18) is related to the 
properties of the words in the context of ? . 
? ?= fullfi Cf if fFN ),()( ??              (15) 
? ?= fullej Ce je eFN ),()( ??                (16) 
)(
),(
),(
?
?
??
f
Cf i
f N
fF
E
cooc
fi? ?
=           (17) 
)(
),(
),( ?
?
??
e
Ce j
e N
eF
E
cooc
ej? ?
=           (18)   
where ),( ifF ?  and ),( jeF ?  are the frequency 
counts of rule ?  or ?   co-occurring with the 
context word if  or je   respectively. 
 
Feature Dev NIST?06 NIST?08 
Baseline 20.2 27.4 21.2 
+Nf 20.5 27.6 21.4 
+Ne 20.5 27.5 21.3 
+Ef 20.4 27.5 21.2 
+Ee 20.4 27.3 21.2 
+Nf+Ne 20.5 27.5 21.3 
 
Table 6: Results (BLEU%) of using simple features 
based on context on small data NIST task. Some im-
provements are obtained on dev set, but there was no 
significant effect on the test sets. 
 
Table 6 shows results obtained by adding the 
above features to the system for the small data 
840
condition. Although all these features have ob-
tained some improvements on dev set, there was 
no significant effect on the test sets. This means 
simple features based on context, such as the 
sum of the counts of the context features, are not 
as helpful as the sense similarity computed by 
Equation (14). 
6.4 Null Context Feature 
There are two cases where no context word can 
be extracted according to the definition of con-
text in Section 3.1. The first case is when a rule 
pair is always a full sentence-pair in the training 
data. The second case is when for some rule 
pairs, either their source or target contexts are 
out of the span limit of the initial phrase, so that 
we cannot extract contexts for those rule-pairs. 
For Chinese-to-English NIST task, there are 
about 1% of the rules that do not have contexts; 
for German-to-English task, this number is about 
0.4%. We assign a uniform number as their bi-
lingual sense similarity score, and this number is 
tuned through MERT. We call it the null context 
feature. It is included in all the results reported 
from Table 2 to Table 6. In Table 7, we show the 
weight of the null context feature tuned by run-
ning MERT in the experiments reported in Sec-
tion 5.2. We can learn that penalties always dis-
courage using those rules which have no context 
to be extracted.  
 
 
Alg. 
Task 
CE_SD CE_LD DE 
Alg2 IBM -0.09 -0.37 -0.15 
Alg2 COS -0.59 -0.42 -0.36 
 
Table 7: Weight learned for employing the null con-
text feature. CE_SD, CE_LD and DE are Chinese-to-
English small data task, large data task and German-
to-English task respectively. 
6.5 Discussion 
Our aim in this paper is to characterize the se-
mantic similarity of bilingual hierarchical rules. 
We can make several observations concerning 
our features: 
1) Rules that are largely syntactic in nature, 
such as ? X ||| the X of, will have very diffuse 
?meanings? and therefore lower similarity 
scores. It could be that the gains we obtained 
come simply from biasing the system against 
such rules. However, the results in table 6 show 
that this is unlikely to be the case: features that 
just count context words help very little. 
2) In addition to bilingual similarity, Alg2 re-
lies on the degree of monolingual similarity be-
tween the sense of a source or target unit within a 
rule, and the sense of the unit in general. This has 
a bias in favor of less ambiguous rules, i.e. rules 
involving only units with closely related mean-
ings. Although this bias is helpful on its own, 
possibly due to the mechanism we outline in sec-
tion 6.1, it appears to have a synergistic effect 
when used along with the bilingual similarity 
feature. 
3) Finally, we note that many of the features 
we use for capturing similarity, such as the con-
text ?the, of? for instantiations of X in the unit 
the X of, are arguably more syntactic than seman-
tic. Thus, like other ?semantic? approaches, ours 
can be seen as blending syntactic and semantic 
information. 
7 Related Work 
There has been extensive work on incorporating 
semantics into SMT. Key papers by Carpuat and 
Wu (2007) and Chan et al(2007) showed that 
word-sense disambiguation (WSD) techniques 
relying on source-language context can be effec-
tive in selecting translations in phrase-based and 
hierarchical SMT. More recent work has aimed 
at incorporating richer disambiguating features 
into the SMT log-linear model (Gimpel and 
Smith, 2008; Chiang et al 2009); predicting co-
herent sets of target words rather than individual 
phrase translations (Bangalore et al 2009; Maus-
er et al 2009); and selecting applicable rules in 
hierarchical (He et al 2008) and syntactic (Liu et 
al, 2008) translation, relying on source as well as 
target context. Work by Wu and Fung (2009) 
breaks new ground in attempting to match se-
mantic roles derived from a semantic parser 
across source and target languages. 
Our work is different from all the above ap-
proaches in that we attempt to discriminate 
among hierarchical rules based on: 1) the degree 
of bilingual semantic similarity between source 
and target translation units; and 2) the monolin-
gual semantic similarity between occurrences of 
source or target units as part of the given rule, 
and in general. In another words, WSD explicitly 
tries to choose a translation given the current 
source context, while our work rates rule pairs 
independent of the current context. 
8 Conclusions and Future Work 
In this paper, we have proposed an approach that 
uses the vector space model to compute the sense 
841
similarity for terms from parallel corpora and 
applied it to statistical machine translation. We 
saw that the bilingual sense similarity computed 
by our algorithm led to significant improve-
ments. Therefore, we can answer the questions 
proposed in Section 1. We have shown that the 
sense similarity computed between units from 
parallel corpora by means of our algorithm is 
helpful for at least one multilingual application: 
statistical machine translation. 
Finally, although we described and evaluated 
bilingual sense similarity algorithms applied to a 
hierarchical phrase-based system, this method is 
also suitable for syntax-based MT systems and 
phrase-based MT systems. The only difference is 
the definition of the context. For a syntax-based 
system, the context of a rule could be defined 
similarly to the way it was defined in the work 
described above. For a phrase-based system, the 
context of a phrase could be defined as its sur-
rounding words in a given size window. In our 
future work, we may try this algorithm on syn-
tax-based MT systems and phrase-based MT sys-
tems with different context features. It would 
also be possible to use this technique during 
training of an SMT system ? for instance, to im-
prove the bilingual word alignment or reduce the 
training data noise. 
References  
S. Bangalore, S. Kanthak, and P. Haffner. 2009. Sta-
tistical Machine Translation through Global Lexi-
cal Selection and Sentence Reconstruction. In: 
Goutte et al(ed.), Learning Machine Translation. 
MIT Press. 
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & 
R. L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2) 263-312. 
J. Bullinaria and J. Levy. 2007. Extracting semantic 
representations from word co-occurrence statistics: 
A computational study. Behavior Research Me-
thods, 39 (3), 510?526. 
M. Carpuat and D. Wu. 2007. Improving Statistical 
Machine Translation using Word Sense Disambig-
uation. In:  Proceedings of EMNLP, Prague. 
M. Carpuat. 2009. One Translation per Discourse. In:  
Proceedings of NAACL HLT Workshop on Se-
mantic Evaluations, Boulder, CO. 
Y. Chan, H. Ng and D. Chiang. 2007. Word Sense 
Disambiguation Improves Statistical Machine 
Translation. In:  Proceedings of ACL, Prague. 
D. Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In: Proceedings 
of ACL, pp. 263?270. 
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics. 33(2):201?228. 
D. Chiang, W. Wang and K. Knight. 2009. 11,001 
new features for statistical machine translation. In: 
Proc. NAACL HLT, pp. 218?226. 
K. W. Church and P. Hanks. 1990. Word association 
norms, mutual information, and lexicography. 
Computational Linguistics, 16(1):22?29. 
W. B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structure and Algo-
rithms. Prentice Hall. 
P. Fung. 1998. A statistical view on bilingual lexicon 
extraction: From parallel corpora to non-parallel 
corpora. In: Proceedings of AMTA, pp. 1?17. Oct. 
Langhorne, PA, USA. 
J. Gimenez and L. Marquez. 2009. Discriminative 
Phrase Selection for SMT. In: Goutte et al(ed.), 
Learning Machine Translation. MIT Press. 
K. Gimpel and N. A. Smith. 2008. Rich Source-Side 
Context for Statistical Machine Translation. In: 
Proceedings of WMT, Columbus, OH. 
Z. Harris. 1954. Distributional structure. Word, 
10(23): 146-162. 
Z. He, Q. Liu, and S. Lin. 2008. Improving Statistical 
Machine Translation using Lexicalized Rule Selec-
tion. In: Proceedings of COLING, Manchester, 
UK. 
D. Hindle. 1990. Noun classification from predicate-
argument structures. In: Proceedings of ACL. pp. 
268-275. Pittsburgh, PA. 
P. Koehn, F. Och, D. Marcu. 2003. Statistical Phrase-
Based Translation. In: Proceedings of HLT-
NAACL. pp. 127-133, Edmonton, Canada 
P.  Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In:  Proceedings of 
EMNLP, pp. 388?395. July, Barcelona, Spain. 
T. Landauer and S. T. Dumais. 1997. A solution to 
Plato?s problem: The Latent Semantic Analysis 
theory of the acquisition, induction, and representa-
tion of knowledge. Psychological Review. 104:211-
240. 
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. 
Khudanpur, L. Schwartz, W. Thornton, J. Weese 
and O. Zaidan, 2009. Joshua: An Open Source 
Toolkit for Parsing-based Machine Translation. In: 
Proceedings of the WMT.  March. Athens, Greece. 
D. Lin. 1998. Automatic retrieval and clustering of 
similar words. In: Proceedings of COLING/ACL-
98. pp. 768-774. Montreal, Canada.  
842
Q. Liu, Z. He, Y. Liu and S. Lin. 2008. Maximum 
Entropy based Rule Selection Model for Syntax-
based Statistical Machine Translation. In: Proceed-
ings of EMNLP, Honolulu, Hawaii. 
K. Lund, and C. Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28 (2), 203?208. 
A. Mauser, S. Hasan and H. Ney. 2009. Extending 
Statistical Machine Translation with Discrimina-
tive and Trigger-Based Lexicon Models. In: Pro-
ceedings of EMNLP, Singapore. 
F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL. 
Sapporo, Japan. 
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational 
Linguistics, 33 (2), 161?199. 
P. Pantel and D. Lin. 2002. Discovering word senses 
from text. In: Proceedings of ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, 
pp. 613?619. Edmonton, Canada. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, pp. 311?
318. July. Philadelphia, PA, USA. 
R. Rapp. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
Corpora. In: Proceedings of ACL, pp. 519?526. 
June. Maryland. 
G. Salton and M. J. McGill. 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill, New 
York. 
P. Turney. 2001. Mining the Web for synonyms: 
PMI-IR versus LSA on TOEFL. In: Proceedings of 
the Twelfth European Conference on Machine 
Learning, pp. 491?502, Berlin, Germany.  
D. Wu and P. Fung. 2009. Semantic Roles for SMT: 
A Hybrid Two-Pass Model. In: Proceedings of 
NAACL/HLT, Boulder, CO. 
D. Yuret and M. A. Yatbaz. 2009. The Noisy Channel 
Model for Unsupervised Word Sense Disambigua-
tion. In: Computational Linguistics. Vol. 1(1) 1-18. 
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In: Proceed-
ings of NAACL-HLT. Boston, MA. 
B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004. 
Phrase pair rescoring with term weighting for sta-
tistical machine translation. In Proceedings of 
EMNLP, pp. 206?213. July. Barcelona, Spain. 
 
843
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930?939,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
PORT:  a Precision-Order-Recall MT Evaluation Metric for Tuning 
 
 
Boxing Chen, Roland Kuhn and Samuel Larkin 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
{Boxing.Chen, Roland.Kuhn, Samuel.Larkin}@nrc.ca 
 
  
Abstract 
Many machine translation (MT) evaluation 
metrics have been shown to correlate better 
with human judgment than BLEU. In 
principle, tuning on these metrics should 
yield better systems than tuning on BLEU. 
However, due to issues such as speed, 
requirements for linguistic resources, and 
optimization difficulty, they have not been 
widely adopted for tuning. This paper 
presents PORT 1 , a new MT  evaluation 
metric which combines precision, recall 
and an ordering metric and which is 
primarily designed for tuning MT systems. 
PORT does not require external resources 
and is quick to compute. It has a better 
correlation with human judgment than 
BLEU. We compare PORT-tuned MT 
systems to BLEU-tuned baselines in five 
experimental conditions involving four 
language pairs. PORT tuning achieves 
consistently better performance than BLEU 
tuning, according to four automated 
metrics (including BLEU) and to human 
evaluation: in comparisons of outputs from 
300 source sentences, human judges 
preferred the PORT-tuned output 45.3% of 
the time (vs. 32.7% BLEU tuning  
preferences and 22.0% ties).  
1 Introduction 
Automatic evaluation metrics for machine 
translation (MT) quality are a key part of building 
statistical MT (SMT) systems. They play two 
                                                           
1
 PORT: Precision-Order-Recall Tunable metric. 
roles: to allow rapid (though sometimes inaccurate) 
comparisons between different systems or between 
different versions of the same system, and to 
perform tuning of parameter values during system 
training. The latter has become important since the 
invention of minimum error rate training (MERT) 
(Och, 2003) and related tuning methods. These 
methods perform repeated decoding runs with 
different system parameter values, which are tuned 
to optimize the value of the evaluation metric over 
a development set with reference translations. 
MT evaluation metrics fall into three groups:  
? BLEU (Papineni et al, 2002), NIST 
(Doddington, 2002), WER, PER, TER 
(Snover et al, 2006), and LRscore (Birch and 
Osborne, 2011) do not use external linguistic 
information; they are fast to compute (except 
TER).  
? METEOR (Banerjee and Lavie, 2005), 
METEOR-NEXT (Denkowski and Lavie 
2010), TER-Plus (Snover et al, 2009), 
MaxSim (Chan and Ng, 2008), TESLA (Liu 
et al, 2010), AMBER (Chen and Kuhn, 2011) 
and MTeRater (Parton et al, 2011) exploit 
some limited linguistic resources, such as 
synonym dictionaries, part-of-speech tagging, 
paraphrasing tables or word root lists.  
? More sophisticated metrics such as RTE 
(Pado et al, 2009), DCU-LFG (He et al, 
2010) and MEANT (Lo and Wu, 2011) use 
higher level syntactic or semantic analysis to 
score translations. 
Among these metrics, BLEU is the most widely 
used for both evaluation and tuning. Many of the 
metrics correlate better with human judgments of 
translation quality than BLEU, as shown in recent 
WMT Evaluation Task reports (Callison-Burch et 
930
al., 2010; Callison-Burch et al, 2011). However, 
BLEU remains the de facto standard tuning metric, 
for two reasons. First, there is no evidence that any 
other tuning metric yields better MT systems. Cer 
et al (2010) showed that BLEU tuning is more 
robust than tuning with other metrics (METEOR, 
TER, etc.), as gauged by both automatic and 
human evaluation. Second, though a tuning metric 
should correlate strongly with human judgment, 
MERT (and similar algorithms) invoke the chosen 
metric so often that it must be computed quickly.  
Liu et al (2011) claimed that TESLA tuning 
performed better than BLEU tuning according to 
human judgment. However, in the WMT 2011 
?tunable metrics? shared pilot task, this did not 
hold (Callison-Burch et al, 2011). In (Birch and 
Osborne, 2011), humans preferred the output from 
LRscore-tuned systems 52.5% of the time, versus 
BLEU-tuned system outputs 43.9% of the time. 
In this work, our goal is to devise a metric that, 
like BLEU, is computationally cheap and 
language-independent, but that yields better MT 
systems than BLEU when used for tuning. We 
tried out different combinations of statistics before 
settling on the final definition of our metric.  The 
final version, PORT, combines precision, recall, 
strict brevity penalty (Chiang et al, 2008) and 
strict redundancy penalty (Chen and Kuhn, 2011) 
in a quadratic mean expression. This expression is 
then further combined with a new measure of word 
ordering, v, designed to reflect long-distance as 
well as short-distance word reordering (BLEU only 
reflects short-distance reordering). In a later 
section, 3.3, we describe experiments that vary 
parts of the definition of PORT.  
Results given below show that PORT correlates 
better with human judgments of translation quality 
than BLEU does, and sometimes outperforms 
METEOR in this respect, based on data from 
WMT (2008-2010). However, since PORT is 
designed for tuning, the most important results are 
those showing that PORT tuning yields systems 
with better translations than those produced by 
BLEU tuning ? both as determined by automatic 
metrics (including BLEU), and according to 
human judgment, as applied to five data conditions 
involving four language pairs. 
2 BLEU and PORT 
First, define n-gram precision p(n) and recall r(n): 
)(grams-n#
)(grams-n#)(
T
RT
np ?=                 (1) 
)(grams-n#
)(grams-n#)(
R
RT
nr
?
=              (2) 
where T = translation, R = reference. Both BLEU 
and PORT are defined on the document-level, i.e. 
T and R are whole texts. If there are multiple 
references, we use closest reference length for each 
translation hypothesis to compute the numbers of 
the reference n-grams. 
2.1 BLEU 
BLEU is composed of precision Pg(N) and brevity 
penalty BP: 
BPNPBLEU g ?= )(                 (3)  
where Pg(N) is the geometric average of n-gram 
precisions 
NN
n
g npNP
1
1
)()( ???
?
???
?
= ?
=
               (4) 
The BLEU brevity penalty punishes the score if 
the translation length len(T) is shorter than the 
reference length len(R); it is: ( ))(/)(1,0.1min TlenRleneBP ?=         (5) 
2.2 PORT 
PORT has five components: precision, recall, strict 
brevity penalty (Chiang et al, 2008), strict 
redundancy penalty (Chen and Kuhn, 2011) and an 
ordering measure v. The design of PORT is based 
on exhaustive experiments on a development data 
set. We do not have room here to give a rationale 
for all the choices we made when we designed 
PORT. However, a later section (3.3) reconsiders 
some of these design decisions.  
2.2.1 Precision and Recall 
The average precision and average recall used in 
PORT (unlike those used in BLEU) are the 
arithmetic average of n-gram precisions Pa(N) and 
recalls Ra(N): 
?
=
=
N
n
a npN
NP
1
)(1)(                 (6) 
?
=
=
N
n
a nrN
NR
1
)(1)(                   (7) 
931
We use two penalties to avoid too long or too 
short MT outputs. The first, the strict brevity 
penalty (SBP), is proposed in (Chiang et al, 2008). 
Let ti be the translation of input sentence i, and let 
ri be its reference. Set 
???
?
???
?
?= ?
?
i ii
i i
rt
r
SBP |}||,min{|
||
1exp         (8) 
The second is the strict redundancy penalty (SRP), 
proposed in (Chen and Kuhn, 2011): 
???
?
???
?
?= ?
?
i i
i ii
r
rt
SRP ||
|}||,max{|
1exp         (9) 
To combine precision and recall, we tried four 
averaging methods: arithmetic (A), geometric (G), 
harmonic (H), and quadratic (Q) mean. If all of the 
values to be averaged are positive, the order is 
maxQAGHmin ????? , with equality 
holding if and only if all the values being averaged 
are equal. We chose the quadratic mean to 
combine precision and recall, as follows: 
2
))(())(()(
22 SRPNRSBPNPNQmean aa ?+?=   (10)  
2.2.2 Ordering Measure 
Word ordering measures for MT compare two 
permutations of the original source-language word 
sequence: the permutation represented by the 
sequence of corresponding words in the MT 
output, and the permutation in the reference. 
Several ordering measures have been integrated 
into MT evaluation metrics recently. Birch and 
Osborne (2011) use either Hamming Distance or 
Kendall?s ? Distance (Kendall, 1938) in their 
metric LRscore, thus obtaining two versions of 
LRscore. Similarly, Isozaki et al (2011) adopt 
either Kendall?s ? Distance or Spearman?s ? 
(Spearman, 1904) distance in their metrics.  
Our measure, v, is different from all of these. 
We use word alignment to compute the two 
permutations (LRscore also uses word alignment). 
The word alignment between the source input and 
reference is computed using GIZA++ (Och and 
Ney, 2003) beforehand with the default settings, 
then is refined with the heuristic grow-diag-final-
and; the word alignment between the source input 
and the translation is generated by the decoder with 
the help of word alignment inside each phrase pair. 
PORT uses permutations. These encode one-to-
one relations but not one-to-many, many-to-one, 
many-to-many or null relations, all of which can 
occur in word alignments. We constrain the 
forbidden types of relation to become one-to-one, 
as in (Birch and Osborne, 2011). Thus, in a one-to-
many alignment, the single source word is forced 
to align with the first target word; in a many-to-one 
alignment, monotone order is assumed for the 
target words; and source words originally aligned 
to null are aligned to the target word position just 
after the previous source word?s target position.  
After the normalization above, suppose we have 
two permutations for the same source n-word 
input. E.g., let P1 = reference, P2 = hypothesis: 
P1: 11p  
2
1p  
3
1p  
4
1p  ? 
ip1  ? 
np1  
 P2: 12p  
2
2p  
3
2p  
4
2p  ? 
ip2  ? 
np2
 
Here, each jip is an integer denoting position in the 
original source (e.g., 11p = 7 means that the first 
word in P1 is the 7th source word). 
The ordering metric v is computed from two 
distance measures. The first is absolute 
permutation distance: 
?
=
?=
n
i
ii ppPPDIST
1
21211 ||),(               (11) 
Let       
2/)1(
),(1 2111 +?= nn
PPDIST
?                     (12)                  
v1 ranges from 0 to 1; a larger value means more 
similarity between the two permutations. This 
metric is similar to Spearman?s ? (Spearman, 
1904). However, we have found that ? punishes 
long-distance reorderings too heavily. For instance, 
1? is more tolerant than ? of the movement of 
?recently? in this example:  
Ref: Recently, I visited Paris 
Hyp: I visited Paris recently  
Inspired by HMM word alignment (Vogel et al, 
1996), our second distance measure is based on 
jump width. This punishes a sequence of words 
that moves a long distance with its internal order 
conserved, only once rather than on every word. In 
the following, only two groups of words have 
moved, so the jump width punishment is light: 
Ref: In the winter of 2010, I visited Paris 
Hyp: I visited Paris in the winter of 2010  
So the second distance measure is 
932
?
=
??
???=
n
i
iiii ppppPPDIST
1
1
22
1
11212 |)()(|),(   (13) 
where we set 001 =p  and 0
0
2 =p . Let 
1
),(1 2 2122
?
?=
n
PPDIST
v                     (14) 
As with v1, v2 is also from 0 to 1, and larger values 
indicate more similar permutations. The ordering 
measure vs is the harmonic mean of v1 and v2:  
)/1/1/(2 21 vvvs +=
 
.                     (15) 
 vs in (15) is computed at segment level. For 
multiple references, we compute vs for each, and 
then choose the biggest one as the segment level 
ordering similarity. We compute document level 
ordering with a weighted arithmetic mean:  
?
?
=
=
?
= l
s s
l
s ss
Rlen
Rlenv
v
1
1
)(
)(
                    (16) 
where l is the number of segments of the 
document, and len(R) is the length of the reference. 
2.2.3 Combined Metric 
Finally, Qmean(N) (Eq. (10) and the word ordering 
measure v are combined in a harmonic mean: 
?vNQmeanPORT /1)(/1
2
+
=           (17) 
Here ?  is a free parameter that is tuned on held-
out data. As it increases, the importance of the 
ordering measure v goes up. For our experiments, 
we tuned ?  on Chinese-English data, setting it to 
0.25 and keeping this value for the other language 
pairs. The use of v means that unlike BLEU, PORT 
requires word alignment information. 
 
3 Experiments 
3.1 PORT as an Evaluation Metric 
We studied PORT as an evaluation metric on 
WMT data; test sets include WMT 2008, WMT 
2009, and WMT 2010 all-to-English, plus 2009, 
2010 English-to-all submissions. The languages 
?all? (?xx? in Table 1) include French, Spanish, 
German and Czech. Table 1 summarizes the test 
set statistics. In order to compute the v part of 
PORT, we require source-target word alignments 
for the references and MT outputs. These aren?t 
included in WMT data, so we compute them with 
GIZA++. 
We used Spearman?s rank correlation coefficient 
? to measure correlation of the metric with system-
level human judgments of translation. The human 
judgment score is based on the ?Rank? only, i.e., 
how often the translations of the system were rated 
as better than those from other systems (Callison-
Burch et al, 2008). Thus, BLEU, METEOR, and 
PORT were evaluated on how well their rankings 
correlated with the human ones. For the segment 
level, we follow (Callison-Burch et al, 2010) in 
using Kendall?s rank correlation coefficient ?.  
As shown in Table 2, we compared PORT with 
smoothed BLEU (mteval-v13a), and METEOR 
v1.0. Both BLEU and PORT perform matching of 
n-grams up to n = 4. 
 
Set Year Lang. #system #sent-pair 
Test1 2008 xx-en 43 7,804 
Test2 2009 xx-en 45 15,087 
Test3 2009 en-xx 40 14,563 
Test4 2010 xx-en 53 15,964 
Test5 2010 en-xx 32 18,508 
Table 1: Statistics of the WMT dev and test sets. 
 
 
 
Metric 
Into-En Out-of-En 
sys.  seg. sys.  seg. 
BLEU 0.792 0.215 0.777 0.240 
METEOR 0.834 0.231 0.835 0.225 
PORT 0.801 0.236 0.804 0.242 
Table 2: Correlations with human judgment on WMT 
 
PORT achieved the best segment level 
correlation with human judgment on both the ?into 
English? and ?out of English? tasks. At the system 
level, PORT is better than BLEU, but not as good 
as METEOR.  This is because we designed PORT 
to carry out tuning; we did not optimize its 
performance as an evaluation metric, but rather, to 
optimize system tuning performance. There are 
some other possible reasons why PORT did not 
outperform METEOR v1.0 at system level. Most 
WMT submissions involve language pairs with 
similar word order, so the ordering factor v in 
PORT won?t play a big role. Also, v depends on 
source-target word alignments for reference and 
test sets. These alignments were performed by 
GIZA++ models trained on the test data only.  
933
3.2 PORT as a Metric for Tuning 
3.2.1 Experimental details 
The first set of experiments to study PORT as a 
tuning metric involved Chinese-to-English (zh-en); 
there were two data conditions. The first is the 
small data condition where FBIS2 is used to train 
the translation and reordering models. It contains 
10.5M target word tokens. We trained two 
language models (LMs), which were combined 
loglinearly. The first is a 4-gram LM which is 
estimated on the target side of the texts used in the 
large data condition (below). The second is a 5-
gram LM estimated on English Gigaword.  
The large data condition uses training data from 
NIST3 2009 (Chinese-English track). All allowed 
bilingual corpora except UN, Hong Kong Laws and 
Hong Kong Hansard were used to train the 
translation model and reordering models. There are 
about 62.6M target word tokens. The same two 
LMs are used for large data as for small data, and 
the same development (?dev?) and test sets are also 
used. The dev set comprised mainly data from the 
NIST 2005 test set, and also some balanced-genre 
web-text from NIST. Evaluation was performed on 
NIST 2006 and 2008. Four references were 
provided for all dev and test sets. 
The third data condition is a French-to-English 
(fr-en). The parallel training data is from Canadian 
Hansard data, containing 59.3M word tokens. We 
used two LMs in loglinear combination: a 4-gram 
LM trained on the target side of the parallel 
training data, and the English Gigaword 5-gram 
LM. The dev set has 1992 sentences; the two test 
sets have 2140 and 2164 sentences respectively. 
There is one reference for all dev and test sets.  
The fourth and fifth conditions involve German-
-English Europarl data. This parallel corpus 
contains 48.5M German tokens and 50.8M English 
tokens. We translate both German-to-English (de-
en) and English-to-German (en-de). The two 
conditions both use an LM trained on the target 
side of the parallel training data, and de-en also 
uses the English Gigaword 5-gram LM. News test 
2008 set is used as dev set; News test 2009, 2010, 
2011 are used as test sets. One reference is 
provided for all dev and test sets. 
                                                           
2
 LDC2003E14 
3
 http://www.nist.gov/speech/tests/mt 
All experiments were carried out with ?  in Eq. 
(17) set to 0.25, and involved only lowercase 
European-language text. They were performed 
with MOSES (Koehn et al, 2007), whose decoder 
includes lexicalized reordering, translation models, 
language models, and word and phrase penalties.  
Tuning was done with n-best MERT, which is 
available in MOSES. In all tuning experiments, 
both BLEU and PORT performed lower case 
matching of n-grams up to n = 4. We also 
conducted experiments with tuning on a version of 
BLEU that incorporates SBP (Chiang et al, 2008) 
as a baseline. The results of original IBM BLEU 
and BLEU with SBP were tied; to save space, we 
only report results for original IBM BLEU here. 
3.2.2 Comparisons with automatic metrics 
First, let us see if BLEU-tuning and PORT-tuning 
yield systems with different translations for the 
same input. The first row of Table 3 shows the 
percentage of identical sentence outputs for the 
two tuning types on test data. The second row 
shows the similarity of the two outputs at word-
level (as measured by 1-TER): e.g., for the two zh-
en tasks, the two tuning types give systems whose 
outputs are about 25-30% different at the word 
level. By contrast, only about 10% of output words 
for fr-en differ for BLEU vs. PORT tuning.  
 
 zh-en 
small 
zh-en 
large 
fr-en 
Hans 
de-en 
WMT 
en-de 
WMT 
Same sent.  17.7% 13.5% 56.6% 23.7% 26.1% 
1-TER 74.2 70.9 91.6 87.1 86.6 
Table 3: Similarity of BLEU-tuned and PORT-tuned 
system outputs on test data. 
 
 
Task 
 
Tune 
Evaluation metrics (%) 
BLEU MTR 1-TER PORT 
zh-en 
small 
BLEU 
PORT 
26.8  
27.2* 
55.2 
55.7 
38.0 
38.0 
49.7 
50.0 
zh-en 
large 
BLEU 
PORT 
29.9  
30.3*  
58.4 
59.0 
41.2 
42.0 
53.0 
53.2 
fr-en 
Hans 
BLEU 
PORT 
38.8  
38.8  
69.8 
69.6 
54.2 
54.6 
57.1 
57.1 
de-en 
WMT 
BLEU 
PORT 
20.1  
20.3 
55.6 
56.0 
38.4 
38.4 
39.6 
39.7 
en-de 
WMT 
BLEU 
PORT 
13.6 
13.6 
43.3 
43.3 
30.1 
30.7 
31.7 
31.7 
Table 4: Automatic evaluation scores on test data. 
 * indicates the results are significantly better than the 
baseline (p<0.05). 
 
934
Table 4 shows translation quality for BLEU- and 
PORT-tuned systems, as assessed by automatic 
metrics. We employed BLEU4, METEOR (v1.0), 
TER (v0.7.25), and the new metric PORT. In the 
table, TER scores are presented as 1-TER to ensure 
that for all metrics, higher scores mean higher 
quality. All scores are averages over the relevant 
test sets. There are twenty comparisons in the 
table. Among these, there is one case (French-
English assessed with METEOR) where BLEU 
outperforms PORT, there are seven ties, and there 
are twelve cases where PORT is better. Table 3 
shows that fr-en outputs are very similar for both 
tuning types, so the fr-en results are perhaps less 
informative than the others. Overall, PORT tuning 
has a striking advantage over BLEU tuning.  
Both (Liu et al, 2011) and (Cer et al, 2011) 
showed that with MERT, if you want the best 
possible score for a system?s translations according 
to metric M, then you should tune with M. This 
doesn?t appear to be true when PORT and BLEU 
tuning are compared in Table 4. For the two 
Chinese-to-English tasks in the table, PORT tuning 
yields a better BLEU score than BLEU tuning, 
with significance at p < 0.05. We are currently 
investigating why PORT tuning gives higher 
BLEU scores than BLEU tuning for Chinese-
English and German-English. In internal tests we 
have found no systematic difference in dev-set 
BLEUs, so we speculate that PORT?s emphasis on 
reordering yields models that generalize better for 
these two language pairs. 
3.2.3 Human Evaluation 
We conducted a human evaluation on outputs from 
BLEU- and PORT-tuned systems. The examples 
are randomly picked from all ?to-English? 
conditions shown in Tables 3 & 4 (i.e., all 
conditions except English-to-German).  
We performed pairwise comparison of the 
translations produced by the system types as in 
(Callison-Burch et al, 2010; Callison-Burch et al, 
2011). First, we eliminated examples where the 
reference had fewer than 10 words or more than 50 
words, or where outputs of the BLEU-tuned and 
PORT-tuned systems were identical. The 
evaluators (colleagues not involved with this 
paper) objected to comparing two bad translations, 
so we then selected for human evaluation only 
translations that had high sentence-level (1-TER) 
scores. To be fair to both metrics, for each 
condition, we took the union of examples whose 
BLEU-tuned output was in the top n% of BLEU 
outputs and those whose PORT-tuned output was 
in the top n% of PORT outputs (based on (1-
TER)). The value of n varied by condition: we 
chose the top 20% of zh-en small, top 20% of en-
de, top 50% of fr-en and top 40% of zh-en large. 
We then randomly picked 450 of these examples to 
form the manual evaluation set. This set was split 
into 15 subsets, each containing 30 sentences. The 
first subset was used as a common set; each of the 
other 14 subsets was put in a separate file, to which 
the common set is added.  Each of the 14 
evaluators received one of these files, containing 
60 examples (30 unique examples and 30 examples 
shared with the other evaluators). Within each 
example, BLEU-tuned and PORT-tuned outputs 
were presented in random order. 
After receiving the 14 annotated files, we 
computed Fleiss?s Kappa (Fleiss, 1971) on the 
common set to measure inter-annotator agreement, 
all? . Then, we excluded annotators one at a time 
to compute i? (Kappa score without i-th annotator, 
i.e., from the other 13). Finally, we filtered out the 
files from the 4 annotators whose answers were 
most different from everybody else?s: i.e., 
annotators with the biggest iall ?? ?  values. 
This left 10 files from 10 evaluators. We threw 
away the common set in each file, leaving 300 
pairwise comparisons. Table 5 shows that the 
evaluators preferred the output from the PORT-
tuned system 136 times, the output from the 
BLEU-tuned one 98 times, and had no preference 
the other 66 times. This indicates that there is a 
human preference for outputs from the PORT-
tuned system over those from the BLEU-tuned 
system at the p<0.01 significance level (in cases 
where people prefer one of them). 
PORT tuning seems to have a bigger advantage 
over BLEU tuning when the translation task is 
hard. Of the Table 5 language pairs, the one where 
PORT tuning helps most has the lowest BLEU in 
Table 4 (German-English); the one where it helps 
least in Table 5 has the highest BLEU in Table 4 
(French-English). (Table 5 does not prove BLEU is 
superior to PORT for French-English tuning: 
statistically, the difference between 14 and 17 here 
is a tie). Maybe by picking examples for each 
condition that were the easiest for the system to 
translate (to make human evaluation easier), we 
935
mildly biased the results in Table 5 against PORT 
tuning. Another possible factor is reordering. 
PORT differs from BLEU partly in modeling long-
distance reordering more accurately; English and 
French have similar word order, but the other two 
language pairs don?t. The results in section 3.3 
(below) for Qmean, a version of PORT without 
word ordering factor v, suggest v may be defined 
suboptimally for French-English.  
 
 PORT win BLEU win equal total 
zh-en 
small 
19 
38.8% 
18 
36.7% 
12 
24.5% 
49 
zh-en 
large 
69 
45.7% 
46 
30.5% 
36 
23.8% 
151 
fr-en 
Hans 
14 
32.6% 
17 
39.5% 
12 
27.9% 
43 
de-en 
WMT 
34 
59.7% 
17 
29.8% 
6 
10.5% 
57 
All 136 
45.3% 
98 
32.7% 
66 
22.0% 
300 
Table 5: Human preference for outputs from PORT-
tuned vs. BLEU-tuned system. 
3.2.4 Computation time  
A good tuning metric should run very fast; this is 
one of the advantages of BLEU. Table 6 shows the 
time required to score the 100-best hypotheses for 
the dev set for each data condition during MERT 
for BLEU and PORT in similar implementations. 
The average time of each iteration, including 
model loading, decoding, scoring and running 
MERT4, is in brackets. PORT takes roughly 1.5 ? 
2.5 as long to compute as BLEU, which is 
reasonable for a tuning metric.  
 
 zh-en 
small 
zh-en 
large 
fr-en 
Hans 
de-en 
WMT 
en-de 
WMT 
BLEU 3 (13)  3 (17) 2 (19) 2 (20) 2 (11) 
PORT 5 (21) 5 (24) 4 (28) 5 (28) 4 (15) 
Table 6: Time to score 100-best hypotheses (average 
time per iteration) in minutes.  
3.2.5 Robustness to word alignment errors 
PORT, unlike BLEU, depends on word 
alignments. How does quality of word alignment 
between source and reference affect PORT tuning? 
We created a dev set from Chinese Tree Bank 
                                                           
4
 Our experiments are run on a cluster. The average time for 
an iteration includes queuing, and the speed of each node is 
slightly different, so bracketed times are only for reference. 
(CTB) hand-aligned data. It contains 588 sentences 
(13K target words), with one reference. We also 
ran GIZA++ to obtain its automatic word 
alignment, computed on CTB and FBIS.  The AER 
of the GIZA++ word alignment on CTB is 0.32.  
In Table 7, CTB is the dev set. The table shows 
tuning with BLEU, PORT with human word 
alignment (PORT + HWA), and PORT with 
GIZA++ word alignment (PORT + GWA); the 
condition is zh-en small. Despite the AER of 0.32 
for automatic word alignment, PORT tuning works 
about as well with this alignment as for the gold 
standard CTB one. (The BLEU baseline in Table 7 
differs from the Table 4 BLEU baseline because 
the dev sets differ).  
 
Tune BLEU MTR 1-TER PORT 
BLEU 25.1 53.7 36.4 47.8 
PORT + HWA 25.3 54.4 37.0 48.2 
PORT + GWA 25.3 54.6 36.4 48.1 
Table 7: PORT tuning - human & GIZA++ alignment 
 
Task Tune BLEU MTR 1-TER PORT 
zh-en 
small 
BLEU 
PORT 
Qmean 
26.8 
27.2 
26.8 
55.2 
55.7 
55.3 
38.0 
38.0 
38.2 
49.7 
50.0 
49.8 
zh-en 
large 
BLEU 
PORT 
Qmean 
29.9 
30.3 
30.2 
58.4 
59.0 
58.5 
41.2 
42.0 
41.8 
53.0 
53.2 
53.1 
fr-en 
Hans 
BLEU 
PORT 
Qmean 
38.8 
38.8 
38.8 
69.8 
69.6 
69.8 
54.2 
54.6 
54.6 
57.1 
57.1 
57.1 
de-en 
WMT 
BLEU 
PORT 
Qmean 
20.1 
20.3 
20.3 
55.6 
56.0 
56.3 
38.4 
38.4 
38.1 
39.6 
39.7 
39.7 
en-de 
WMT 
BLEU 
PORT 
Qmean 
13.6 
13.6 
13.6 
43.3 
43.3 
43.4 
30.1 
30.7 
30.3 
31.7 
31.7 
31.7 
Table 8: Impact of ordering measure v on PORT 
3.3 Analysis 
Now, we look at the details of PORT to see which 
of them are the most important. We do not have 
space here to describe all the details we studied, 
but we can describe some of them. E.g., does the 
ordering measure v help tuning performance? To 
answer this, we introduce an intermediate metric. 
This is Qmean as in Eq. (10): PORT without the 
ordering measure. Table 8 compares tuning with 
BLEU, PORT, and Qmean.  PORT outperforms 
Qmean on seven of the eight automatic scores 
shown for small and large Chinese-English. 
936
However, for the European language pairs, PORT 
and Qmean seem to be tied. This may be because 
we optimized ?  in Eq. (18) for Chinese-English, 
making the influence of word ordering measure v 
in PORT too strong for the European pairs, which 
have similar word order.  
Measure v seems to help Chinese-English 
tuning. What would results be on that language 
pair if we were to replace v in PORT with another 
ordering measure? Table 9 gives a partial answer, 
with Spearman?s ? and Kendall?s ? replacing v 
with ? or ? in PORT for the zh-en small condition 
(CTB with human word alignment is the dev set). 
The original definition of PORT seems preferable. 
 
Tune BLEU METEOR 1-TER 
BLEU 25.1 53.7 36.4 
PORT(v) 25.3 54.4 37.0 
PORT(?) 25.1 54.2 36.3 
PORT(?) 25.1 54.0 36.0 
Table 9: Comparison of the ordering measure: replacing 
? with ? or ? in PORT. 
 
 
Task 
 
Tune 
ordering measures 
? ? v 
NIST06 BLEU 
PORT 
0.979 
0.979 
0.926 
0.928 
0.915 
0.917 
NIST08 BLEU 
PORT 
0.980 
0.981 
0.926 
0.929 
0.916 
0.918 
CTB BLEU 
PORT 
0.973 
0.975 
0.860 
0.866 
0.847 
0.853 
Table 10: Ordering scores (?, ? and v) for test sets NIST 
2006, 2008 and CTB. 
 
A related question is how much word ordering 
improvement we obtained from tuning with PORT. 
We evaluate Chinese-English word ordering with 
three measures: Spearman?s ?, Kendall?s ? distance  
as applied to two permutations (see section 2.2.2) 
and our own measure v. Table 10 shows the effects 
of BLEU and PORT tuning on these three 
measures, for three test sets in the zh-en large 
condition. Reference alignments for CTB were 
created by humans, while the NIST06 and NIST08 
reference alignments were produced with GIZA++. 
A large value of ?, ?, or v implies outputs have 
ordering similar to that in the reference. From the 
table, we see that the PORT-tuned system yielded 
better word order than the BLEU-tuned system in 
all nine combinations of test sets and ordering 
measures. The advantage of PORT tuning is 
particularly noticeable on the most reliable test set: 
the hand-aligned CTB data.  
What is the impact of the strict redundancy 
penalty on PORT? Note that in Table 8, even 
though Qmean has no ordering measure, it 
outperforms BLEU. Table 11 shows the BLEU 
brevity penalty (BP) and (number of matching 1- 
& 4- grams)/(number of total 1- & 4- grams) for 
the translations. The BLEU-tuned and Qmean-
tuned systems generate similar numbers of 
matching n-grams, but Qmean-tuned systems 
produce fewer n-grams (thus, shorter translations). 
E.g., for zh-en small, the BLEU-tuned system 
produced 44,677 1-grams (words), while the 
Qmean-trained system one produced 43,555 1-
grams; both have about 32,000 1-grams matching 
the references. Thus, the Qmean translations have 
higher precision. We believe this is because of the 
strict redundancy penalty in Qmean. As usual, 
French-English is the outlier: the two outputs here 
are typically so similar that BLEU and Qmean 
tuning yield very similar n-gram statistics. 
 
Task Tune 1-gram 4-gram BP 
zh-en 
small 
BLEU 
Qmean 
32055/44677 
31996/43555 
4603/39716 
4617/38595 
0.967 
0.962 
zh-en 
large 
BLEU 
Qmean 
34583/45370 
34369/44229 
5954/40410 
5987/39271 
0.972 
0.959 
fr-en 
Hans 
BLEU 
Qmean 
28141/40525 
28167/40798 
8654/34224 
8695/34495 
0.983 
0.990 
de-en 
WMT 
BLEU 
Qmean 
42380/75428 
42173/72403 
5151/66425 
5203/63401 
1.000 
0.968 
en-de 
WMT 
BLEU 
Qmean 
30326/62367 
30343/62092 
2261/54812 
2298/54537 
1.000 
0.997 
Table 11: #matching-ngram/#total-ngram and BP score  
4 Conclusions 
In this paper, we have proposed a new tuning 
metric for SMT systems.  PORT incorporates 
precision, recall, strict brevity penalty and strict 
redundancy penalty, plus a new word ordering 
measure v.  As an evaluation metric, PORT 
performed better than BLEU at the system level 
and the segment level, and it was competitive with 
or slightly superior to METEOR at the segment 
level. Most important, our results show that PORT-
tuned MT systems yield better translations  than  
BLEU-tuned systems on several language pairs, 
according both to automatic metrics and human 
evaluations. In future work, we plan to tune the 
free parameter ? for each language pair. 
937
References 
S. Banerjee and A. Lavie. 2005. METEOR: An 
automatic metric for MT evaluation with improved 
correlation with human judgments. In Proceedings of 
ACL Workshop on Intrinsic & Extrinsic Evaluation 
Measures for Machine Translation and/or 
Summarization. 
A. Birch and M. Osborne. 2011. Reordering Metrics for 
MT. In Proceedings of ACL.  
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz and 
J. Schroeder. 2008. Further Meta-Evaluation of 
Machine Translation. In Proceedings of WMT. 
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. 
Re-evaluating the role of BLEU in machine 
translation research. In Proceedings of EACL. 
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, M. 
Przybocki and O. Zaidan. 2010. Findings of the 2010 
Joint Workshop on Statistical Machine Translation 
and Metrics for Machine Translation. In Proceedings 
of WMT. 
C. Callison-Burch, P. Koehn, C. Monz and O. Zaidan. 
2011. Findings of the 2011 Workshop on Statistical 
Machine Translation. In Proceedings of WMT. 
D. Cer, D. Jurafsky and C. Manning. 2010. The Best 
Lexical Metric for Phrase-Based Statistical MT 
System Optimization. In Proceedings of NAACL. 
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maximum 
similarity metric for machine translation evaluation. 
In Proceedings of ACL. 
B. Chen and R. Kuhn. 2011. AMBER: A Modified 
BLEU, Enhanced Ranking Metric. In: Proceedings of 
WMT. Edinburgh, UK. July. 
D. Chiang, S. DeNeefe, Y. S. Chan, and H. T. Ng. 2008. 
Decomposability of translation metrics for improved 
evaluation and efficient algorithms. In Proceedings of 
EMNLP, pages 610?619. 
M. Denkowski and A. Lavie. 2010. Meteor-next and the 
meteor paraphrase tables: Improved evaluation 
support for five target languages. In Proceedings of 
the Joint Fifth Workshop on SMT and 
MetricsMATR, pages 314?317. 
G. Doddington. 2002. Automatic evaluation of machine 
translation quality using n-gram co-occurrence 
statistics. In Proceedings of HLT. 
J. L. Fleiss. 1971. Measuring nominal scale agreement 
among many raters. In Psychological Bulletin, Vol. 
76, No. 5 pp. 378?382. 
Y. He, J. Du, A. Way and J. van Genabith. 2010. The 
DCU dependency-based metric in WMT-
MetricsMATR 2010. In Proceedings of the Joint 
Fifth Workshop on Statistical Machine Translation 
and MetricsMATR, pages 324?328.  
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada. 
2010. Automatic Evaluation of Translation Quality 
for Distant Language Pairs. In Proceedings of 
EMNLP.  
M. Kendall. 1938. A New Measure of Rank Correlation. 
In Biometrika, 30 (1?2), pp. 81?89. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, 
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. 
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proceedings of ACL, 
pp. 177-180, Prague, Czech Republic. 
A. Lavie and M. J. Denkowski. 2009. The METEOR 
metric for automatic evaluation of machine 
translation. Machine Translation, 23. 
C. Liu, D. Dahlmeier, and H. T. Ng. 2010. TESLA: 
Translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the 
Joint Fifth Workshop on Statistical Machine 
Translation and MetricsMATR, pages 329?334. 
C. Liu, D. Dahlmeier, and H. T. Ng. 2011. Better 
evaluation metrics lead to better machine translation. 
In Proceedings of EMNLP. 
C. Lo and D. Wu. 2011. MEANT: An inexpensive, 
high-accuracy, semi-automatic metric for evaluating 
translation utility based on semantic roles. In 
Proceedings of ACL. 
F. J. Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
ACL-2003. Sapporo, Japan. 
F. J. Och and H. Ney. 2003. A Systematic Comparison 
of Various Statistical Alignment Models. In 
Computational Linguistics, 29, pp. 19?51. 
S. Pado, M. Galley, D. Jurafsky, and C.D. Manning. 
2009. Robust machine translation evaluation with 
entailment features. In Proceedings of ACL-IJCNLP. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
BLEU: a method for automatic evaluation of 
machine translation. In Proceedings of ACL. 
K. Parton, J. Tetreault, N. Madnani and M. Chodorow. 
2011.  E-rating Machine Translation. In Proceedings 
of WMT. 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A Study of Translation Edit Rate 
938
with Targeted Human Annotation. In Proceedings of 
Association for Machine Translation in the Americas. 
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 
2009. Fluency, Adequacy, or HTER? Exploring 
Different Human Judgments with a Tunable MT 
Metric. In Proceedings of the Fourth Workshop on 
Statistical Machine Translation, Athens, Greece. 
C. Spearman. 1904. The proof and measurement of 
association between two things. In American Journal 
of Psychology, 15, pp. 72?101. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based 
word alignment in statistical translation. In 
Proceedings of COLING. 
939
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285?1293,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Vector Space Model for Adaptation in Statistical Machine Translation
Boxing Chen, Roland Kuhn and George Foster
National Research Council Canada
first.last@nrc-cnrc.gc.ca
Abstract
This paper proposes a new approach to
domain adaptation in statistical machine
translation (SMT) based on a vector space
model (VSM). The general idea is first to
create a vector profile for the in-domain
development (?dev?) set. This profile
might, for instance, be a vector with a di-
mensionality equal to the number of train-
ing subcorpora; each entry in the vector re-
flects the contribution of a particular sub-
corpus to all the phrase pairs that can be
extracted from the dev set. Then, for
each phrase pair extracted from the train-
ing data, we create a vector with features
defined in the same way, and calculate its
similarity score with the vector represent-
ing the dev set. Thus, we obtain a de-
coding feature whose value represents the
phrase pair?s closeness to the dev. This is
a simple, computationally cheap form of
instance weighting for phrase pairs. Ex-
periments on large scale NIST evaluation
data show improvements over strong base-
lines: +1.8 BLEU on Arabic to English
and +1.4 BLEU on Chinese to English
over a non-adapted baseline, and signifi-
cant improvements in most circumstances
over baselines with linear mixture model
adaptation. An informal analysis suggests
that VSM adaptation may help in making
a good choice among words with the same
meaning, on the basis of style and genre.
1 Introduction
The translation models of a statistical machine
translation (SMT) system are trained on parallel
data. Usage of language and therefore the best
translation practice differs widely across genres,
topics, and dialects, and even depends on a partic-
ular author?s or publication?s style; the word ?do-
main? is often used to indicate a particular combi-
nation of all these factors. Unless there is a per-
fect match between the training data domain and
the (test) domain in which the SMT system will
be used, one can often get better performance by
adapting the system to the test domain.
Domain adaptation is an active topic in the nat-
ural language processing (NLP) research commu-
nity. Its application to SMT systems has recently
received considerable attention. Approaches that
have been tried for SMT model adaptation include
mixture models, transductive learning, data selec-
tion, instance weighting, and phrase sense disam-
biguation, etc.
Research on mixture models has considered
both linear and log-linear mixtures. Both were
studied in (Foster and Kuhn, 2007), which con-
cluded that the best approach was to combine sub-
models of the same type (for instance, several
different TMs or several different LMs) linearly,
while combining models of different types (for in-
stance, a mixture TM with a mixture LM) log-
linearly. (Koehn and Schroeder, 2007), instead,
opted for combining the sub-models directly in the
SMT log-linear framework.
In transductive learning, an MT system trained
on general domain data is used to translate in-
domain monolingual data. The resulting bilingual
sentence pairs are then used as additional train-
ing data (Ueffing et al, 2007; Chen et al, 2008;
Schwenk, 2008; Bertoldi and Federico, 2009).
Data selection approaches (Zhao et al, 2004;
Hildebrand et al, 2005; Lu? et al, 2007; Moore
and Lewis, 2010; Axelrod et al, 2011) search for
bilingual sentence pairs that are similar to the in-
domain ?dev? data, then add them to the training
data.
Instance weighting approaches (Matsoukas et
al., 2009; Foster et al, 2010; Huang and Xiang,
2010; Phillips and Brown, 2011; Sennrich, 2012)1285
typically use a rich feature set to decide on weights
for the training data, at the sentence or phrase pair
level. For example, a sentence from a subcorpus
whose domain is far from that of the dev set would
typically receive a low weight, but sentences in
this subcorpus that appear to be of a general na-
ture might receive higher weights.
The 2012 JHU workshop on Domain Adapta-
tion for MT 1 proposed phrase sense disambigua-
tion (PSD) for translation model adaptation. In
this approach, the context of a phrase helps the
system to find the appropriate translation.
In this paper, we propose a new instance weight-
ing approach to domain adaptation based on a vec-
tor space model (VSM). As in (Foster et al, 2010),
this approach works at the level of phrase pairs.
However, the VSM approach is simpler and more
straightforward. Instead of using word-based fea-
tures and a computationally expensive training
procedure, we capture the distributional properties
of each phrase pair directly, representing it as a
vector in a space which also contains a representa-
tion of the dev set. The similarity between a given
phrase pair?s vector and the dev set vector be-
comes a feature for the decoder. It rewards phrase
pairs that are in some sense closer to those found
in the dev set, and punishes the rest. In initial ex-
periments, we tried three different similarity func-
tions: Bhattacharyya coefficient, Jensen-Shannon
divergency, and cosine measure. They all enabled
VSM adaptation to beat the non-adaptive baseline,
but Bhattacharyya similarity worked best, so we
adopted it for the remaining experiments.
The vector space used by VSM adaptation can
be defined in various ways. In the experiments
described below, we chose a definition that mea-
sures the contribution (to counts of a given phrase
pair, or to counts of all phrase pairs in the dev
set) of each training subcorpus. Thus, the vari-
ant of VSM adaptation tested here bears a super-
ficial resemblance to domain adaptation based on
mixture models for TMs, as in (Foster and Kuhn,
2007), in that both approaches rely on information
about the subcorpora from which the data origi-
nate. However, a key difference is that in this pa-
per we explicitly capture each phrase pair?s dis-
tribution across subcorpora, and compare it to the
aggregated distribution of phrase pairs in the dev
set. In mixture models, a phrase pair?s distribu-
1http://www.clsp.jhu.edu/workshops/archive/ws-
12/groups/dasmt
tion across subcorpora is captured only implicitly,
by probabilities that reflect the prevalence of the
pair within each subcorpus. Thus, VSM adapta-
tion occurs at a much finer granularity than mix-
ture model adaptation. More fundamentally, there
is nothing about the VSM idea that obliges us to
define the vector space in terms of subcorpora.
For instance, we could cluster the words in the
source language into S clusters, and the words in
the target language into T clusters. Then, treat-
ing the dev set and each phrase pair as a pair of
bags of words (a source bag and a target bag) one
could represent each as a vector of dimension S +
T, with entries calculated from the counts associ-
ated with the S + T clusters (in a way similar to
that described for phrase pairs below). The (dev,
phrase pair) similarity would then be independent
of the subcorpora. One can think of several other
ways of defining the vector space that might yield
even better results than those reported here. Thus,
VSM adaptation is not limited to the variant of it
that we tested in our experiments.
2 Vector space model adaptation
Vector space models (VSMs) have been widely
applied in many information retrieval and natural
language processing applications. For instance, to
compute the sense similarity between terms, many
researchers extract features for each term from its
context in a corpus, define a VSM and then ap-
ply similarity functions (Hindle, 1990; Lund and
Burgess, 1996; Lin, 1998; Turney, 2001).
In our experiments, we exploited the fact that
the training data come from a set of subcorpora.
For instance, the Chinese-English training data are
made up of 14 subcorpora (see section 3 below).
Suppose we have C subcorpora. The domain vec-
tor for a phrase-pair (f, e) is defined as
V (f, e) =< w1(f, e), ...wi(f, e), ..., wC(f, e) >,
(1)
where wi(f, e) is a standard tf ? idf weight, i.e.
wi(f, e) = tfi (f, e) ? idf (f, e) . (2)
To avoid a bias towards longer corpora, we nor-
malize the raw joint count ci(f, e) in the corpus
si by dividing by the maximum raw count of any
phrase pair extracted in the corpus si. Let1286
tfi (f, e) =
ci (f, e)
max {ci (fj , ek) , (fj , ek) ? si}
.
(3)
The idf (f, e) is the inverse document fre-
quency: a measure of whether the phrase-pair
(f, e) is common or rare across all subcorpora. We
use the standard formula:
idf (f, e) = log
( C
df (f, e) + ?
)
, (4)
where df(f, e) is the number of subcorpora that
(f, e) appears in, and ? is an empirically deter-
mined smoothing term.
For the in-domain dev set, we first run word
alignment and phrases extracting in the usual way
for the dev set, then sum the distribution of each
phrase pair (fj , ek) extracted from the dev data
across subcorpora to represent its domain informa-
tion. The dev vector is thus
V (dev) =< w1(dev), . . . , wC(dev) >, (5)
where
wi(dev) =
j=J?
j=0
k=K?
k=0
cdev (fj , ek)wi(fj , ek) (6)
J,K are the total numbers of source/target
phrases extracted from the dev data respectively.
cdev (fj , ek) is the joint count of phrase pair fj , ek
found in the dev set.
The vector can also be built with other features
of the phrase pair. For instance, we could replace
the raw joint count ci(f, e) in Equation 3 with the
raw marginal count of phrase pairs (f, e). There-
fore, even within the variant of VSM adaptation
we focus on in this paper, where the definition of
the vector space is based on the existence of sub-
corpora, one could utilize other definitions of the
vectors of the similarity function than those we uti-
lized in our experiments.
2.1 Vector similarity functions
VSM uses the similarity score between the vec-
tor representing the in-domain dev set and the vec-
tor representing each phrase pair as a decoder fea-
ture. There are many similarity functions we could
have employed for this purpose (Cha, 2007). We
tested three commonly-used functions: the Bhat-
tacharyya coefficient (BC) (Bhattacharyya, 1943;
Kazama et al, 2010), the Jensen-Shannon diver-
gence (JSD), and the cosine measure. According
to (Cha, 2007), these belong to three different fam-
ilies of similarity functions: the Fidelity family,
the Shannon?s entropy family, and the inner Prod-
uct family respectively. It was BC similarity that
yielded the best performance, and that we ended
up using in subsequent experiments.
To map the BC score onto a range from 0 to
1, we first normalize each weight in the vector by
dividing it by the sum of the weights. Thus, we get
the probability distribution of a phrase pair or the
phrase pairs in the dev data across all subcorpora:
pi(f, e) =
wi(f, e)?j=C
j=1 wj(f, e)
(7)
pi(dev) =
wi(dev)?j=C
j=1 wj(dev)
(8)
To further improve the similarity score, we ap-
ply absolute discounting smoothing when calcu-
lating the probability distributions pi(f, e). We
subtract a discounting value ? from the non-zero
pi(f, e), and equally allocate the remaining proba-
bility mass to the zero probabilities. We carry out
the same smoothing for the probability distribu-
tions pi(dev). The smoothing constant ? is deter-
mined empirically on held-out data.
The Bhattacharyya coefficient (BC) is defined
as follows:
BC(dev; f, e) =
i=C?
i=0
?
pi(dev) ? pi(f, e) (9)
The other two similarity functions we also
tested are JSD and cosine (Cos). They are defined
as follows:
JSD(dev; f, e) = (10)
1
2[
i=C?
i=1
pi(dev) log
2pi(dev)
pi(dev) + pi(f, e)
+
i=C?
i=1
pi(f, e) log
2pi(f, e)
pi(dev) + pi(f, e)
]
Cos(dev; f, e) =
?
i pi(dev) ? pi (f, e)??
i p2i (dev)
??
i p2i (f, e)
(11)1287
corpus # segs # en tok % genres
fbis 250K 10.5M 3.7 nw
financial 90K 2.5M 0.9 fin
gale bc 79K 1.3M 0.5 bc
gale bn 75K 1.8M 0.6 bn ng
gale nw 25K 696K 0.2 nw
gale wl 24K 596K 0.2 wl
hkh 1.3M 39.5M 14.0 hans
hkl 400K 9.3M 3.3 legal
hkn 702K 16.6M 5.9 nw
isi 558K 18.0M 6.4 nw
lex&ne 1.3M 2.0M 0.7 lex
other nw 146K 5.2M 1.8 nw
sinorama 282K 10.0M 3.5 nw
un 5.0M 164M 58.2 un
TOTAL 10.1M 283M 100.0 (all)
devtest
tune 1,506 161K nw wl
NIST06 1,664 189K nw bng
NIST08 1,357 164K nw wl
Table 1: NIST Chinese-English data. In the
genres column: nw=newswire, bc=broadcast
conversation, bn=broadcast news, wl=weblog,
ng=newsgroup, un=UN proc., bng = bn & ng.
3 Experiments
3.1 Data setting
We carried out experiments in two different set-
tings, both involving data from NIST Open MT
2012.2 The first setting is based on data from
the Chinese to English constrained track, compris-
ing about 283 million English running words. We
manually grouped the training data into 14 corpora
according to genre and origin. Table 1 summa-
rizes information about the training, development
and test sets; we show the sizes of the training sub-
corpora in number of words as a percentage of all
training data. Most training subcorpora consist of
parallel sentence pairs. The isi and lex&ne cor-
pora are exceptions: the former is extracted from
comparable data, while the latter is a lexicon that
includes many named entities. The development
set (tune) was taken from the NIST 2005 evalua-
tion set, augmented with some web-genre material
reserved from other NIST corpora.
The second setting uses NIST 2012 Arabic to
English data, but excludes the UN data. There are
about 47.8 million English running words in these
2http://www.nist.gov/itl/iad/mig/openmt12.cfm
corpus # segs # en toks % gen
gale bc 57K 1.6M 3.3 bc
gale bn 45K 1.2M 2.5 bn
gale ng 21K 491K 1.0 ng
gale nw 17K 659K 1.4 nw
gale wl 24K 590K 1.2 wl
isi 1,124K 34.7M 72.6 nw
other nw 224K 8.7M 18.2 nw
TOTAL 1,512K 47.8M 100.0 (all)
devtest
NIST06 1,664 202K nwl
NIST08 1,360 205K nwl
NIST09 1,313 187K nwl
Table 2: NIST Arabic-English data. In the gen
(genres) column: nw=newswire, bc=broadcast
conversation, bn=broadcast news, ng=newsgroup,
wl=weblog, nwl = nw & wl.
training data. We manually grouped the training
data into 7 groups according to genre and origin.
Table 2 summarizes information about the train-
ing, development and test sets. Note that for this
language pair, the comparable isi data represent a
large proportion of the training data: 72% of the
English words. We use the evaluation sets from
NIST 2006, 2008, and 2009 as our development
set and two test sets, respectively.
3.2 System
Experiments were carried out with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007). Each corpus was word-aligned using
IBM2, HMM, and IBM4 models, and the phrase
table was the union of phrase pairs extracted from
these separate alignments, with a length limit of
7. The translation model (TM) was smoothed in
both directions with KN smoothing (Chen et al,
2011). We use the hierarchical lexicalized reorder-
ing model (RM) (Galley and Manning, 2008), with
a distortion limit of 7. Other features include lex-
ical weighting in both directions, word count, a
distance-based RM, a 4-gram LM trained on the
target side of the parallel data, and a 6-gram En-
glish Gigaword LM. The system was tuned with
batch lattice MIRA (Cherry and Foster, 2012).
3.3 Results
For the baseline, we simply concatenate all train-
ing data. We have also compared our approach
to two widely used TM domain adaptation ap-1288
proaches. One is the log-linear combination
of TMs trained on each subcorpus (Koehn and
Schroeder, 2007), with weights of each model
tuned under minimal error rate training using
MIRA. The other is a linear combination of TMs
trained on each subcorpus, with the weights of
each model learned with an EM algorithm to max-
imize the likelihood of joint empirical phrase pair
counts for in-domain dev data. For details, refer to
(Foster and Kuhn, 2007).
The value of ? and ? (see Eq 4 and Section 2.1)
are determined by the performance on the dev
set of the Arabic-to-English system. For both
Arabic-to-English and Chinese-to-English exper-
iment, these values obtained on Arabic dev were
used to obtain the results below: ? was set to 8,
and ? was set to 0.01. (Later, we ran an exper-
iment on Chinese-to-English with ? and ? tuned
specifically for that language pair, but the perfor-
mance for the Chinese-English system only im-
proved by a tiny, insignificant amount).
Our metric is case-insensitive IBM BLEU (Pa-
pineni et al, 2002), which performs matching of
n-grams up to n = 4; we report BLEU scores av-
eraged across both test sets NIST06 and NIST08
for Chinese; NIST08 and NIST09 for Arabic.
Following (Koehn, 2004), we use the bootstrap-
resampling test to do significance testing. In ta-
bles 3 to 5, * and ** denote significant gains over
the baseline at p < 0.05 and p < 0.01 levels, re-
spectively.
We first compare the performance of differ-
ent similarity functions: cosine (COS), Jensen-
Shannon divergence (JSD) and Bhattacharyya co-
efficient (BC). The results are shown in Table 3.
All three functions obtained improvements. Both
COS and BC yield statistically significant im-
provements over the baseline, with BC performing
better than COS by a further statistically signifi-
cant margin. The Bhattacharyya coefficient is ex-
plicitly designed to measure the overlap between
the probability distributions of two statistical sam-
ples or populations, which is precisely what we are
trying to do here: we are trying to reward phrase
pairs whose distribution is similar to that of the
dev set. Thus, its superior performance in these
experiments is not unexpected.
In the next set of experiments, we compared
VSM adaptation using the BC similarity function
with the baseline which concatenates all training
data and with log-linear and linear TM mixtures
system Chinese Arabic
baseline 31.7 46.8
COS 32.3* 47.8**
JSD 32.1 47.1
BC 33.0** 48.4**
Table 3: Comparison of different similarity func-
tions. * and ** denote significant gains over the
baseline at p < 0.05 and p < 0.01 levels, respec-
tively.
system Chinese Arabic
baseline 31.7 46.8
loglinear tm 28.4 44.5
linear tm 32.7** 47.5**
vsm, BC 33.0** 48.4**
Table 4: Results for variants of adaptation.
whose components are based on subcorpora. Ta-
ble 4 shows that log-linear combination performs
worse than the baseline: the tuning algorithm
failed to optimize the log-linear combination even
on dev set. For Chinese, the BLEU score of the
dev set on the baseline system is 27.3, while on
the log-linear combination system, it is 24.0; for
Arabic, the BLEU score of the dev set on the base-
line system is 46.8, while on the log-linear com-
bination system, it is 45.4. We also tried adding
the global model to the loglinear combination and
it didn?t improve over the baseline for either lan-
guage pair. Linear mixture was significantly better
than the baseline at the p < 0.01 level for both lan-
guage pairs. Since our approach, VSM, performed
better than the linear mixture for both pairs, it is of
course also significantly better than the baseline at
the p < 0.01 level.
This raises the question: is VSM performance
significantly better than that of a linear mixture of
TMs? The answer (not shown in the table) is that
for Arabic to English, VSM performance is bet-
ter than linear mixture at the p < 0.01 level. For
Chinese to English, the argument for the superi-
ority of VSM over linear mixture is less convinc-
ing: there is significance at the p < 0.05 for one
of the two test sets (NIST06) but not for the other
(NIST08). At any rate, these results establish that
VSM adaptation is clearly superior to linear mix-
ture TM adaptation, for one of the two language
pairs.
In Table 4, the VSM results are based on the1289
system Chinese Arabic
baseline 31.7 46.8
linear tm 32.7** 47.5**
vsm, joint 33.0** 48.4**
vsm, src-marginal 32.2* 47.3*
vsm, tgt-marginal 32.6** 47.6**
vsm, src+tgt (2 feat.) 32.7** 48.2**
vsm, joint+src (2 feat.) 32.9** 48.4**
vsm, joint+tgt (2 feat.) 32.9** 48.4**
vsm, joint+src+tgt (3 feat.) 33.1** 48.6**
Table 5: Results for adaptation based on joint or
maginal counts.
vector of the joint counts of the phrase pair. In
the next experiment, we replace the joint counts
with the source or target marginal counts. In Ta-
ble 5, we first show the results based on source
and target marginal counts, then the results of us-
ing feature sets drawn from three decoder VSM
features: a joint count feature, a source marginal
count feature, and a target marginal count fea-
ture. For instance, the last row shows the results
when all three features are used (with their weights
tuned by MIRA). It looks as though the source and
target marginal counts contain useful information.
The best performance is obtained by combining all
three sources of information. The 3-feature ver-
sion of VSM yields +1.8 BLEU over the baseline
for Arabic to English, and +1.4 BLEU for Chinese
to English.
When we compared two sets of results in Ta-
ble 4, the joint count version of VSM and lin-
ear mixture of TMs, we found that for Arabic to
English, VSM performance is better than linear
mixture at the p < 0.01 level; the Chinese to
English significance test was inconclusive (VSM
found to be superior to linear mixture at p < 0.05
for NIST06 but not for NIST08). We now have
somewhat better results for the 3-feature version
of VSM shown in Table 5. How do these new re-
sults affect the VSM vs. linear mixture compari-
son? Naturally, the conclusions for Arabic don?t
change. For Chinese, 3-feature VSM is now su-
perior to linear mixture at p < 0.01 on NIST06
test set, but 3-feature VSM still doesn?t have a sta-
tistically significant edge over linear mixture on
NIST08 test set. A fair summary would be that 3-
feature VSM adaptation is decisively superior to
linear mixture adaptation for Arabic to English,
and highly competitive with linear mixture adap-
tation for Chinese to English.
Our last set of experiments examined the ques-
tion: when added to a system that already has
some form of linear mixture model adaptation,
does VSM improve performance? In (Foster and
Kuhn, 2007), two kinds of linear mixture were de-
scribed: linear mixture of language models (LMs),
and linear mixture of translation models (TMs).
Some of the results reported above involved lin-
ear TM mixtures, but none of them involved lin-
ear LM mixtures. Table 6 shows the results of
different combinations of VSM and mixture mod-
els. * and ** denote significant gains over the row
no vsm at p < 0.05 and p < 0.01 levels, re-
spectively. This means that in the table, the base-
line within each box containing three results is the
topmost result in the box. For instance, with an
initial Chinese system that employs linear mixture
LM adaptation (lin-lm) and has a BLEU of 32.1,
adding 1-feature VSM adaptation (+vsm, joint)
improves performance to 33.1 (improvement sig-
nificant at p < 0.01), while adding 3-feature VSM
instead (+vsm, 3 feat.) improves performance to
33.2 (also significant at p < 0.01). For Arabic, in-
cluding either form of VSM adaptation always im-
proves performance with significance at p < 0.01,
even over a system including both linear TM and
linear LM adaptation. For Chinese, adding VSM
still always yields an improvement, but the im-
provement is not significant if linear TM adapta-
tion is already in the system. These results show
that combining VSM adaptation and either or both
kinds of linear mixture adaptation never hurts per-
formance, and often improves it by a significant
amount.
3.4 Informal Data Analysis
To get an intuition for how VSM adaptation im-
proves BLEU scores, we compared outputs from
the baseline and VSM-adapted system (?vsm,
joint? in Table 5) on the Chinese test data. We
focused on examples where the two systems had
translated the same source-language (Chinese)
phrase s differently, and where the target-language
(English) translation of s chosen by the VSM-
adapted system, tV , had a higher Bhattacharyya
score for similarity with the dev set than did the
phrase that was chosen by the baseline system, tB .
Thus, we ignored differences in the two transla-
tions that might have been due to the secondary
effects of VSM adaptation (such as a different tar-1290
no-lin-adap lin-lm lin-tm lin-lm+lin-tm
no vsm 31.7 32.1 32.7 33.1
Chinese +vsm, joint 33.0** 33.1** 33.0 33.3
+vsm, 3 feat. 33.1** 33.2** 33.1 33.4
no vsm 46.8 47.0 47.5 47.7
Arabic +vsm, joint 48.4** 48.7** 48.6** 48.8**
+vsm, 3 feat. 48.6** 48.8** 48.7** 48.9**
Table 6: Results of combining VSM and linear mixture adaptation. ?lin-lm? is linear language model
adaptation, ?lin-tm? is linear translation model adaptation. * and ** denote significant gains over the row
?no vsm? at p < 0.05 and p < 0.01 levels, respectively.
get phrase being preferred by the language model
in the VSM-adapted system from the one preferred
in the baseline system because of a Bhattacharyya-
mediated change in the phrase preceding it).
An interesting pattern soon emerged: the VSM-
adapted system seems to be better than the base-
line at choosing among synonyms in a way that is
appropriate to the genre or style of a text. For in-
stance, where the text to be translated is from an
informal genre such as weblog, the VSM-adapted
system will often pick an informal word where the
baseline picks a formal word with the same or sim-
ilar meaning, and vice versa where the text to be
translated is from a more formal genre. To our
surprise, we saw few examples where the VSM-
adapted system did a better job than the baseline of
choosing between two words with different mean-
ing, but we saw many examples where the VSM-
adapted system did a better job than the baseline
of choosing between two words that both have the
same meaning according to considerations of style
and genre.
Two examples are shown in Table 7. In the
first example, the first two lines show that VSM
finds that the Chinese-English phrase pair (??,
assaulted) has a Bhattacharyya (BC) similarity of
0.556163 to the dev set, while the phrase pair (?
?, beat) has a BC similarity of 0.780787 to the
dev. In this situation, the VSM-adapted system
thus prefers ?beat? to ?assaulted? as a translation
for ??. The next four lines show the source
sentence (SRC), the reference (REF), the baseline
output (BSL), and the output of the VSM-adapted
system. Note that the result of VSM adaptation is
that the rather formal word ?assaulted? is replaced
by its informal near-synonym ?beat? in the trans-
lation of an informal weblog text.
?apprehend? might be preferable to ?arrest? in
a legal text. However, it looks as though the
VSM-adapted system has learned from the dev
that among synonyms, those more characteristic
of news stories than of legal texts should be cho-
sen: it therefore picks ?arrest? over its synonym
?apprehend?.
What follows is a partial list of pairs of phrases
(all single words) from our system?s outputs,
where the baseline chose the first member of a pair
and the VSM-adapted system chose the second
member of the pair to translate the same Chinese
phrase into English (because the second word
yields a better BC score for the dev set we used).
It will be seen that nearly all of the pairs involve
synonyms or near-synonyms rather than words
with radically different senses (one exception
below is ?center? vs ?heart?). Instead, the differ-
ences between the two words tend to be related to
genre or style: gunmen-mobsters, champion-star,
updated-latest, caricatures-cartoons, spill-leakage,
hiv-aids, inkling-clues, behaviour-actions, deceit-
trick, brazen-shameless, aristocratic-noble,
circumvent-avoid, attack-criticized, descent-born,
hasten-quickly, precipice-cliff, center-heart,
blessing-approval, imminent-approaching,
stormed-rushed, etc.
4 Conclusions and future work
This paper proposed a new approach to domain
adaptation in statistical machine translation, based
on vector space models (VSMs). This approach
measures the similarity between a vector repre-
senting a particular phrase pair in the phrase ta-
ble and a vector representing the dev set, yield-
ing a feature associated with that phrase pair that
will be used by the decoder. The approach is
simple, easy to implement, and computationally
cheap. For the two language pairs we looked
at, it provided a large performance improvement
over a non-adaptive baseline, and also compared1291
1 phrase ??? assaulted (0.556163)
pairs ??? beat (0.780787)
SRC ...???????????...
REF ... those local ruffians and hooligans who beat up villagers ...
BSL ... those who assaulted the villagers land hooligans ...
VSM ... hooligans who beat the villagers ...
2 phrase ??? apprehend (0.286533)
pairs ??? arrest (0.603342)
SRC ... ?????????????
REF ... catch the killers and bring them to justice .
BSL ... apprehend the perpetrators and bring them to justice .
VSM ... arrest the perpetrators and bring them to justice .
Table 7: Examples show that VSM chooses translations according to considerations of style and genre.
favourably with linear mixture adaptation tech-
niques.
Furthermore, VSM adaptation can be exploited
in a number of different ways, which we have only
begun to explore. In our experiments, we based
the vector space on subcorpora defined by the na-
ture of the training data. This was done purely
out of convenience: there are many, many ways to
define a vector space in this situation. An obvi-
ous and appealing one, which we intend to try in
future, is a vector space based on a bag-of-words
topic model. A feature derived from this topic-
related vector space might complement some fea-
tures derived from the subcorpora which we ex-
plored in the experiments above, and which seem
to exploit information related to genre and style.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In EMNLP 2011.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th Workshop on Statistical Machine Translation,
Athens, March. WMT.
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bulletin of the Calcutta
Mathematical Society, 35:99?109.
Sung-Hyuk Cha. 2007. Comprehensive survey on dis-
tance/similarity measures between probability den-
sity functions. International Journal of Mathe-
matical Models ind Methods in Applied Sciences,
1(4):300?307.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for smt self-
enhancement. In ACL 2008.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In MT Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
ACL Workshop on Statistical Machine Translation,
Prague, June. WMT.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Boston.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model.
In EMNLP 2008, pages 848?856, Hawaii, October.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
EAMT Conference, Budapest, May.
Donald Hindle. 1990. Noun classification from predi-
cate.argument structures. In Proceedings of the 28th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 268?275, Pittsburgh,
PA, June. ACL.
Fei Huang and Bing Xiang. 2010. Feature-rich dis-
criminative phrase rescoring for SMT. In COLING
2010.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A1292
bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 247?256, Uppsala, Swe-
den, July. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL 2007,
Demonstration Session.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Barcelona, Spain.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768?774, Montreal, Quebec, Canada.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Pro-
ceedings of the 2007 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
Prague, Czech Republic.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods Instru-
ments and Computers, 28(2):203?208.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL
2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, July. ACL.
Aaron B. Phillips and Ralf D. Brown. 2011. Train-
ing machine translation with a second-order taylor
approximation of weighted translation instances. In
MT Summit 2011.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT 2008.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In EACL 2012.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Twelfth European
Conference on Machine Learning, page 491?502,
Berlin, Germany.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Prague, Czech Republic, June.
ACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING) 2004, Geneva, Au-
gust.
1293
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 11?16,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Fast Consensus Hypothesis Regeneration for Machine Translation 
 
Boxing Chen, George Foster and Roland Kuhn 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca 
 
  
 
Abstract 
This paper presents a fast consensus hy-
pothesis regeneration approach for ma-
chine translation. It combines the advan-
tages of feature-based fast consensus de-
coding and hypothesis regeneration.  Our 
approach is more efficient than previous 
work on hypothesis regeneration, and it 
explores a wider search space than con-
sensus decoding, resulting in improved 
performance.  Experimental results show 
consistent improvements across language 
pairs, and an improvement of up to 0.72 
BLEU is obtained over a competitive 
single-pass baseline on the Chinese-to-
English NIST task. 
1 Introduction 
State-of-the-art statistical machine translation 
(SMT) systems are often described as a two-pass 
process. In the first pass, decoding algorithms are 
applied to generate either a translation N-best list 
or a translation forest.  Then in the second pass, 
various re-ranking algorithms are adopted to 
compute the final translation. The re-ranking al-
gorithms include rescoring (Och et al, 2004) and 
Minimum Bayes-Risk (MBR) decoding (Kumar 
and Byrne, 2004; Zhang and Gildea, 2008; 
Tromble et al, 2008). Rescoring uses more so-
phisticated additional feature functions to score 
the hypotheses. MBR decoding directly incorpo-
rates the evaluation metrics (i.e., loss function), 
into the decision criterion, so it is effective in 
tuning the MT performance for a specific loss 
function. In particular, sentence-level BLEU loss 
function gives gains on BLEU (Kumar and 
Byrne, 2004).  
The na?ve MBR algorithm computes the loss 
function between every pair of k hypotheses, 
needing O(k2) comparisons. Therefore, only 
small number k is applicable. Very recently, De-
Nero et al (2009) proposed a fast consensus de-
coding (FCD) algorithm in which the similarity 
scores are computed based on the feature expec-
tations over the translation N-best list or transla-
tion forest. It is equivalent to MBR decoding 
when using a linear similarity function, such as 
unigram precision.  
Re-ranking approaches improve performance 
on an N-best list whose contents are fixed. A   
complementary strategy is to augment the con-
tents of an N-best list in order to broaden the 
search space. Chen et al(2008) have proposed a 
three-pass SMT process, in which a hypothesis 
regeneration pass is added between the decoding 
and rescoring passes. New hypotheses are gener-
ated based on the original N-best hypotheses 
through n-gram expansion, confusion-network 
decoding or re-decoding. All three hypothesis 
regeneration methods obtained decent and com-
parable improvements in conjunction with the 
same rescoring model. However, since the final 
translation candidates in this approach are pro-
duced from different methods, local feature func-
tions (such as translation models and reordering 
models) of each hypothesis are not directly com-
parable and rescoring must exploit rich global 
feature functions to compensate for the loss of 
local feature functions. Thus this approach is de-
pendent on the use of computationally expensive 
features for rescoring, which makes it inefficient.  
In this paper, we propose a fast consensus hy-
pothesis regeneration method that combines the 
advantages of feature-based fast consensus de-
coding and hypothesis regeneration. That is, we 
integrate the feature-based similarity/loss func-
tion based on evaluation metrics such as BLEU 
score into the hypothesis regeneration procedure 
to score the partial hypotheses in the beam search 
and compute the final translations. Thus, our ap-
proach is more efficient than the original three-
pass hypothesis regeneration. Moreover, our ap-
proach explores more search space than consen-
11
sus decoding, giving it an advantage over the 
latter. 
In particular, we extend linear corpus BLEU 
(Tromble et al, 2008) to n-gram expectation-
based linear BLEU, then further extend the n-
gram expectation computed on full-length hypo-
theses to n-gram expectation computed on fixed-
length partial hypotheses. Finally, we extend the 
hypothesis regeneration with forward n-gram 
expansion to bidirectional n-gram expansion in-
cluding both the forward and backward n-gram 
expansion. Experimental results show consistent 
improvements over the baseline across language 
pairs, and up to 0.72 BLEU points are obtained 
from a competitive baseline on the Chinese-to-
English NIST task. 
2 Fast Consensus Hypothesis Regenera-
tion 
Since the three hypothesis regeneration methods 
with n-gram expansion, confusion network de-
coding and re-decoding produce very similar per-
formance (Chen et al, 2008), we consider only 
n-gram expansion method in this paper. N-gram 
expansion can (almost) fully exploit the search 
space of target strings which can be generated by 
an n-gram language model trained on the N-best 
hypotheses (Chen et al, 2007). 
2.1 Hypothesis regeneration with bidirec-
tional n-gram expansion 
N-gram expansion (Chen et al, 2007) works as 
follows: firstly, train an n-gram language model 
based on the translation N-best list or translation 
forest; secondly, expand each partial hypothesis 
by appending a word via overlapped (n-1)-grams 
until the partial hypothesis reaches the sentence 
ending symbol. In each expanding step, the par-
tial hypotheses are pruned through a beam-search 
algorithm with scoring functions. 
Duchateau et al (2001) shows that the back-
ward language model contains information com-
plementary to the information in the forward 
language model. Hence, on top of the forward n-
gram expansion used in (Chen et al, 2008), we 
further introduce backward n-gram expansion to 
the hypothesis regeneration procedure. Backward 
n-gram expansion involves letting the partial hy-
potheses start from the last words that appeared 
in the translation N-best list and having the ex-
pansion go from right to left. 
Figure 1 gives an example of backward n-
gram expansion. The second row shows bi-grams 
which are extracted from the original hypotheses 
in the first row. The third row shows how a par-
tial hypothesis is expanded via backward n-gram 
expansion method. The fourth row lists some 
new hypotheses generated by backward n-gram 
expansion which do not exist in the original hy-
pothesis list. 
 
 
original 
 hypotheses 
about weeks' work . 
one week's work 
about one week's 
about a week work 
about one week work 
bi-grams about weeks', weeks' work, ?, 
about one, ?,  week work. 
backward 
n-gram 
 expansion 
partial hyp.    week's work 
n-gram one week's  
new partial hyp. one week's work 
 
 
new 
 hypotheses 
about one week's work 
about week's work 
one weeks' work . 
one week's work . 
one week's work . 
 
Figure 1: Example of original hypotheses; bi-grams 
collected from them; backward expanding a partial 
hypothesis via an overlapped n-1-gram; and new hy-
potheses generated through backward n-gram expan-
sion. 
2.2 Feature-based scoring functions 
To speed up the search, the partial hypotheses 
are pruned via beam-search in each expanding 
step. Therefore, the scoring functions applied 
with the beam-search algorithm are very impor-
tant. In (Chen et al, 2008), more than 10 addi-
tional global features are computed to rank the 
partial hypothesis list, and this is not an efficient 
way. In this paper, we propose to directly incor-
porate the evaluation metrics such as BLEU 
score to rank the candidates. The scoring func-
tions of this work are derived from the method of 
lattice Minimum Bayes-risk (MBR) decoding 
(Tromble et al, 2008) and fast consensus decod-
ing (DeNero et al, 2009), which were originally 
inspired from N-best MBR decoding (Kumar and 
Byrne, 2004). 
From a set of translation candidates E, MBR 
decoding chooses the translation that has the 
least expected loss with respect to other candi-
dates. Given a hypothesis set E, under the proba-
bility model )|( feP , MBR computes the trans-
lation e~  as follows: 
 
12
)|(),(minarg~ fePeeLe
EeEe
??= ?
???
        (1) 
 
where f is the source sentence, ),( eeL ?  is the loss 
function of two translations e and e? . 
Suppose that we are interested in maximizing 
the BLEU score (Papineni et al, 2002) to optim-
ize the translation performance. The loss func-
tion is defined as ),(1),( eeBLEUeeL ??=? ,  
then the MBR objective can be re-written as 
 
)|(),(maxarg~ fePeeBLEUe
EeEe
??= ?
???
         (2) 
 
E represents the space of the translations. For 
N-best MBR decoding, this space is the N-best 
list produced by a baseline decoder (Kumar and 
Byrne, 2004). For lattice MBR decoding, this 
space is the set of candidates encoded in the lat-
tice (Tromble et al, 2008). Here, with hypothesis 
regeneration, this space includes: 1) the transla-
tions produced by the baseline decoder either in 
an N-best list or encoded in a translation lattice, 
and 2) the translations created by hypothesis re-
generation. 
However, BLEU score is not linear with the 
length of the hypothesis, which makes the scor-
ing process for each expanding step of hypothe-
sis regeneration very slow. To further speed up 
the beam search procedure, we use an extension 
of a linear function of a Taylor approximation to 
the logarithm of corpus BLEU which was devel-
oped by (Tromble et al, 2008).  The original 
BLEU score of two hypotheses e and e? are 
computed as follows. 
 
)),(log(
4
1
exp(),(),(
4
1
?
=
???=?
n
n
eePeeeeBLEU ?    (3) 
 
where ),( eePn ?  is the precision of n-grams in the 
hypothesis e given e? and  ),( ee ??  is a brevity 
penalty. Let |e| denote the length of e. The corpus 
log-BLEU gain is defined as follows: 
 
)),(log(
4
1)||
||1,0min()),(log(
4
1
?
=
?+
?
?=?
n
n eeP
e
e
eeBLEU  (4) 
 
Therefore, the first-order Taylor approxima-
tion to the logarithm of corpus BLEU is shown 
in Equation (5). 
 
?
=
??+=?
4
1
0 ),(4
1||),(
n
nn
eeceeeG ??                    (5) 
where ),( eecn ? are the counts of the matched n-
grams and 
n?  ( 40 ?? n ) are constant weights 
estimated with held-out data.  
Suppose we have computed the expected n-
gram counts from the N-best list or translation 
forest. Then we may extend linear corpus BLEU 
in (5) to n-gram expectation-based linear corpus 
BLEU to score the partial hypotheses h. That is 
 
? ?
= ?
??+=
4
1
0 ),()],'([4
1||)',(
n Tt
nnn
n
thtecEhehG ???       (6) 
 
where ),( thn?  are n-gram indicator functions that 
equal 1 if n-gram t  appears in h  and 0 other-
wise; )],'([ tecE n  ( 41 ?? n ) are the real-valued 
n-gram expectations. Different from lattice MBR 
decoding, n-gram expectations in this work are 
computed over the original translation N-best list 
or translation forest; 
nT  ( 41 ?? n ) are the sets of 
n-grams collected from translation N-best list or 
translation forest. Then we make a further exten-
sion: the expectations of the n-gram counts for 
each expanding step are computed over the par-
tial translations. The lengths of all partial hypo-
theses are the same in each n-gram expanding 
step. For instance, in the 5th n-gram expanding 
step, the lengths of all the partial hypotheses are 
5 words. Therefore, we use n-gram count expec-
tations computed over partial original transla-
tions that only contain the first 5 words. The rea-
son is that this solution contains more informa-
tion about word orderings, since some n-grams 
appear more than others at the beginning of the 
translations while they may appear with the same 
or even lower frequencies than others in the full 
translations.  
Once the expanding process of hypothesis re-
generation is finished, we use a more precise 
BLEU metric to score all the translation candi-
dates. We extend BLEU score in (3) to n-gram 
expectation-based BLEU. That is: 
 
?
?
?
?
?
?
?
?
?
?
+???
?
???
?
?=
=
? ?
?
=
?
?
4
1 ),(
)]),'([),,(min(
log
4
1
||
|]'[|1,0minexp
)',()(
n
Tt
n
Tt
nn
n
n
thc
tecEthc
h
eE
ehBLEUhScore
                                                        (7) 
 
where ),( thcn  is the count of  n-gram t in the 
hypothesis h. The step of choosing the final 
translation is the same as fast consensus decod-
ing (DeNero et al, 2009): first we compute n-
13
gram feature expectations, and then we choose 
the translation that is most similar to the others 
via expected similarity according to feature-
based BLEU score as shown in (7). The differ-
ence is the space of translations: the space of fast 
consensus decoding is the same as MBR decod-
ing, while the space of hypothesis regeneration is 
enlarged by the new translations produced via n-
gram expansion. 
2.3 Fast consensus hypothesis regeneration 
We first generate two new hypothesis lists via 
forward and backward n-gram expansion using 
the scoring function in Equation (6). Then we 
choose a final translation using the scoring func-
tion in Equation (7) from the union of the origi-
nal hypotheses and newly generated hypotheses. 
The original hypotheses are from the N-best list 
or extracted from the translation forest. The new 
hypotheses are generated by forward or back-
ward n-gram expansion or are the union of both 
two new hypothesis lists (this is called ?bi-
directional n-gram expansion?). 
3 Experimental Results 
We carried out experiments based on translation 
N-best lists generated by a state-of-the-art 
phrase-based statistical machine translation sys-
tem, similar to (Koehn et al, 2007). In detail, the 
phrase table is derived from merged counts of 
symmetrized IBM2 and HMM alignments; the 
system has both lexicalized and distance-based 
distortion components (there is a 7-word distor-
tion limit) and employs cube pruning (Huang and 
Chiang, 2007). The baseline is a log-linear fea-
ture combination that includes language models, 
the distortion components, translation model, 
phrase and word penalties. Weights on feature 
functions are found by lattice MERT (Macherey 
et al, 2008). 
3.1 Data 
We evaluated with different language pairs: Chi-
nese-to-English, and German-to-English. Chi-
nese-to-English tasks are based on training data 
for the NIST 1  2009 evaluation Chinese-to-
English track. All the allowed bilingual corpora 
have been used for estimating the translation 
model. We trained two language models: the first 
one is a 5-gram LM which is estimated on the 
target side of the parallel data. The second is a 5-
                                               
1
 http://www.nist.gov/speech/tests/mt 
gram LM trained on the so-called English Giga-
word corpus. 
 
   Chi Eng 
Parallel 
Train 
Large 
Data 
|S| 10.1M 
|W| 270.0M 279.1M 
   Dev |S| 1,506 1,506?4 
Test NIST06 |S| 1,664 1,664?4 
NIST08 |S| 1,357 1,357?4 
Gigaword |S| - 11.7M 
 
Table 1: Statistics of training, dev, and test sets for 
Chinese-to-English task. 
 
We carried out experiments for translating 
Chinese to English. We first created a develop-
ment set which used mainly data from the NIST 
2005 test set, and also some balanced-genre web-
text from the NIST training material. Evaluation 
was performed on the NIST 2006 and 2008 test 
sets. Table 1 gives figures for training, develop-
ment and test corpora; |S| is the number of the 
sentences, and |W| is the size of running words. 
Four references are provided for all dev and test 
sets. 
For German-to-English tasks, we used WMT 
20062 data sets. The parallel training data con-
tains about 1 million sentence pairs and includes 
21 million target words; both the dev set and test 
set contain 2000 sentences; one reference is pro-
vided for each source input sentence. Only the 
target-language half of the parallel training data 
are used to train the language model in this task. 
3.2 Results 
Our evaluation metric is IBM BLEU (Papineni et 
al., 2002), which performs case-insensitive 
matching of n-grams up to n = 4.  
Our first experiment was carried out over 
1000-best lists on Chinese-to-English task. For 
comparison, we also conducted experiments with 
rescoring (two-pass) and three-pass hypothesis 
regeneration with only forward n-gram expan-
sion as proposed in (Chen et al, 2008). In the 
?rescoring? and ?three-pass? systems, we used 
the same rescoring model. There are 21 rescoring 
features in total, mainly translation lexicon 
scores from IBM and HMM models, posterior 
probabilities for words, n-grams, and sentence 
length, and language models, etc. For a complete 
description, please refer to (Ueffing et al, 2007). 
The results in BLEU-4 are reported in Table 2. 
 
                                               
2
 http://www.statmt.org/wmt06/ 
14
testset NIST?06 NIST?08 
baseline 35.70 28.60 
rescoring 36.01 28.97 
three-pass 35.98 28.99 
FCD 36.00 29.10 
Fwd. 36.13 29.19 
Bwd. 36.11 29.20 
Bid. 36.20 29.28 
 
Table 2: Translation performances in BLEU-4(%) 
over 1000-best lists for Chinese-to-English task: ?res-
coring? represents the results of rescoring; ?three-
pass?, three-pass hypothesis regeneration with for-
ward n-gram expansion; ?FCD?, fast consensus de-
coding; ?Fwd?, the results of hypothesis regeneration 
with forward n-gram expansion; ?Bwd?, backward n-
gram expansion; and ?Bid?, bi-directional n-gram 
expansion. 
 
Firstly, rescoring improved performance over 
the baseline by 0.3-0.4 BLEU point. Three-pass 
hypothesis regeneration with only forward n-
gram expansion (?three-pass? in Table 2) ob-
tained almost the same improvements as rescor-
ing. Three-pass hypothesis regeneration exploits 
more hypotheses than rescoring, while rescoring 
involves more scoring feature functions than the 
former. They reached a balance in this experi-
ment. Then, fast consensus decoding (?FCD? in 
Table 2) obtains 0.3-0.5 BLEU point improve-
ments over the baseline. Both forward and back-
ward n-gram expansion (?Fwd.? and ?Bwd.? in 
Table 2) improved about 0.1 BLEU point over 
the results of consensus decoding. Fast consen-
sus hypothesis regeneration (Fwd. and Bwd. in 
Table 2) got better improvements than three-pass 
hypothesis regeneration (?three-pass? in Table 2) 
by 0.1-0.2 BLEU point. Finally, combining hy-
pothesis lists from forward and backward n-gram 
expansion (?Bid.? in Table 2), further slight 
gains were obtained. 
 
testset Average time 
three-pass 3h 54m 
Fwd. 25m 
Bwd. 28m 
Bid. 40m 
 
Table 3: Average processing time of NIST?06 and 
NIST?08 test sets used in different systems. Times 
include n-best list regeneration and re-ranking. 
 
Moreover, fast consensus hypothesis regenera-
tion is much faster than the three-pass one, be-
cause the former only needs to compute one fea-
ture, while the latter needs to compute more than 
20 additional features. In this experiment, the 
former is about 10 times faster than the latter in 
terms of processing time, as shown in Table 3. 
 
In our second experiment, we set the size of 
N-best list N equal to 10,000 for both Chinese-to-
English and German-to-English tasks. The re-
sults are reported in Table 4. The same trend as 
in the first experiment can also be observed in 
this experiment. It is worth noticing that enlarg-
ing the size of the N-best list from 1000 to 
10,000 did not change the performance signifi-
cantly. Bi-directional n-gram expansion obtained 
improvements of 0.24 BLEU-score for WMT 
2006 de-en test set; 0.55 for NIST 2006 test set; 
and 0.72 for NIST 2008 test set over the base-
line. 
 
Lang. ch-en de-en 
testset NIST?06 NIST?08 Test2006 
baseline 35.70 28.60 26.92 
FCD 36.03 29.08 27.03 
Fwd. 36.16 29.25 27.11 
Bwd. 36.17 29.22 27.12 
Bid. 36.25 29.32 27.16 
 
Table 4: Translation performances in BLEU-4 (%) 
over 10K-best lists. 
 
We then tested the effect of the extension ac-
cording to which the expectations over n-gram 
counts are computed on partial hypotheses rather 
than whole candidate translations as described in 
Section 2.2. As shown in Table 5, we got tiny 
improvements on both test sets by computing the 
expectations over n-gram counts on partial hypo-
theses. 
 
testset NIST?06 NIST?08 
full 36.11 29.14 
partial 36.13 29.19 
 
Table 5: Translation performances in BLEU-4 (%) 
over 1000-best lists for Chinese-to-English task: 
?full? represents expectations over n-gram counts that 
are computed on whole hypotheses; ?partial? 
represents expectations over n-gram counts that are 
computed on partial hypotheses. 
3.3 Discussion  
To speed up the search, the partial hypotheses in 
each expanding step are pruned. When pruning is 
applied, forward and backward n-gram expan-
sion would generate different new hypothesis 
lists. Let us look back at the example in Figure 1.  
15
Given 5 original hypotheses in Figure 1, if we set 
the beam size equal to 5 (the size of the original 
hypotheses), the forward and backward n-gram 
expansion generated different new hypothesis 
lists, as shown in Figure 2. 
 
forward backward 
one week's work . 
about week's work 
one week's work . 
about one week's work 
 
Figure 2: Different new hypothesis lists generated by 
forward and backward n-gram expansion. 
 
For bi-directional n-gram expansion, the cho-
sen translation for a source sentence comes from 
the decoder 94% of the time for WMT 2006 test 
set, 90% for NIST test sets; it comes from for-
ward n-gram expansion 2% of the time for WMT 
2006 test set, 4% for NIST test sets; it comes 
from backward n-gram expansion 4% of the time 
for WMT 2006 test set, 6% for NIST test sets. 
This proves bidirectional n-gram expansion is a 
good way of enlarging the search space. 
4 Conclusions and Future Work 
We have proposed a fast consensus hypothesis 
regeneration approach for machine translation. It 
combines the advantages of feature-based con-
sensus decoding and hypothesis regeneration. 
This approach is more efficient than previous 
work on hypothesis regeneration, and it explores 
a wider search space than consensus decoding, 
resulting in improved performance.  Experiments 
showed consistent improvements across lan-
guage pairs. 
Instead of N-best lists, translation lattices or 
forests have been shown to be effective for MBR 
decoding (Zhang and Gildea, 2008; Tromble et 
al., 2008), and DeNero et al (2009) showed how 
to compute expectations of n-grams from a trans-
lation forest. Therefore, our future work may 
involve hypothesis regeneration using an n-gram 
language model trained on the translation forest. 
References  
B. Chen, M. Federico and M. Cettolo. 2007. Better N-
best Translations through Generative n-gram Lan-
guage Models. In: Proceedings of MT Summit XI. 
Copenhagen, Denmark. September. 
B. Chen, M. Zhang, A. Aw, and H. Li. 2008. Regene-
rating Hypotheses for Statistical Machine Transla-
tion. In: Proceedings of COLING. pp105-112. 
Manchester, UK, August. 
J. DeNero, D. Chiang and K. Knight. 2009. Fast Con-
sensus Decoding over Translation Forests. In: Pro-
ceedings of ACL. Singapore, August. 
J. Duchateau, K. Demuynck, and P. Wambacq. 2001. 
Confidence scoring based on backward language 
models. In: Proceedings of ICASSP 2001. Salt 
Lake City, Utah, USA, May. 
L. Huang and D. Chiang. 2007. Forest Rescoring: 
Faster Decoding with Integrated Language Models. 
In: Proceedings of ACL. pp. 144-151, Prague, 
Czech Republic, June.  
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In: Proceedings of 
ACL. pp. 177-180, Prague, Czech Republic. 
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk 
decoding for statistical machine translation. In: 
Proceedings of NAACL. Boston, MA, May. 
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 
2008. Lattice-based Minimum Error Rate Training 
for Statistical Machine Translation. In: Proceed-
ings of EMNLP. pp. 725-734, Honolulu, USA, 
October. 
F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL. 
Sapporo, Japan. July. 
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. 
Eng, V. Jain, Z. Jin, and D. Radev. 2004. A Smor-
gasbord of Features for Statistical Machine Trans-
lation. In: Proceedings of NAACL. Boston. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
BLEU: A method for automatic evaluation of ma-
chine translation. In: Proceedings of the ACL 2002. 
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 
2008. Lattice minimum Bayes-risk decoding for 
statistical machine translation. In: Proceedings of 
EMNLP. Hawaii, US. October. 
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.  
2007. NRC?s Portage system for WMT 2007. In: 
Proceedings of ACL Workshop on SMT. Prague, 
Czech Republic, June. 
H. Zhang and D. Gildea. 2008. Efficient multipass 
decoding for synchronous context free grammars. 
In: Proceedings of ACL. Columbus, US. June. 
16
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127?132,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Lessons from NRC?s Portage System at WMT 2010 
 
 
Samuel Larkin, Boxing Chen, George Foster, Ulrich Germann, Eric Joanis, 
Howard Johnson, and Roland Kuhn  
National Research Council of Canada (NRC) 
Gatineau, Qu?bec, Canada. 
Firstname.Lastname@cnrc-nrc.gc.ca 
 
  
 
Abstract 
 
NRC?s Portage system participated in the Eng-
lish-French (E-F) and French-English (F-E) 
translation tasks of the ACL WMT 2010 eval-
uation. The most notable improvement over 
earlier versions of Portage is an efficient im-
plementation of lattice MERT. While Portage 
has typically performed well in Chinese to 
English MT evaluations, most recently in the 
NIST09 evaluation, our participation in WMT 
2010 revealed some interesting differences be-
tween Chinese-English and E-F/F-E transla-
tion, and alerted us to certain weak spots in 
our system. Most of this paper discusses the 
problems we found in our system and ways of 
fixing them. We learned several lessons that 
we think will be of general interest.  
1 Introduction 
Portage, the statistical machine translation sys-
tem of the National Research Council of Canada 
(NRC), is a two-pass phrase-based system. The 
translation tasks to which it is most often applied 
are Chinese to English, English to French (hen-
ceforth ?E-F?), and French to English (hence-
forth ?F-E?): in recent years we worked on Chi-
nese-English translation for the GALE project 
and for NIST evaluations, and English and 
French are Canada?s two official languages. In 
WMT 2010, Portage scored 28.5 BLEU (un-
cased) for F-E, but only 27.0 BLEU (uncased) 
for E-F. For both language pairs, Portage tru-
ecasing caused a loss of 1.4 BLEU; other WMT 
systems typically lost around 1.0 BLEU after 
truecasing. In Canada, about 80% of translations 
between English and French are from English to 
French, so we would have preferred better results 
for that direction. This paper first describes the 
version of Portage that participated in WMT 
2010. It then analyzes problems with the system 
and describes the solutions we found for some of 
them.  
2 Portage system description 
2.1 Core engine and training data 
The NRC system uses a standard two-pass 
phrase-based approach. Major features in the 
first-pass loglinear model include phrase tables 
derived from symmetrized IBM2 alignments and 
symmetrized HMM alignments, a distance-based 
distortion model, a lexicalized distortion model, 
and language models (LMs) that can be either 
static or else dynamic mixtures. Each phrase ta-
ble used was a merged one, created by separately 
training an IBM2-based and an HMM-based 
joint count table on the same data and then add-
ing the counts. Each includes relative frequency 
estimates and lexical estimates (based on Zens 
and Ney, 2004) of forward and backward condi-
tional probabilities. The lexicalized distortion 
probabilities are also obtained by adding IBM2 
and HMM counts. They involve 6 features (mo-
notone, swap and discontinuous features for fol-
lowing and preceding phrase) and are condi-
tioned on phrase pairs in a model similar to that 
of Moses (Koehn et al, 2005); a MAP-based 
backoff smoothing scheme is used to combat 
data sparseness when estimating these probabili-
ties. Dynamic mixture LMs are linear mixtures 
of ngram models trained on parallel sub-corpora 
with weights set to minimize perplexity of the 
current source text as described in (Foster and 
Kuhn, 2007); henceforth, we?ll call them ?dy-
namic LMs?.  
Decoding uses the cube-pruning algorithm of 
(Huang and Chiang, 2007) with a 7-word distor-
tion limit. Contrary to the usual implementation 
of distortion limits, we allow a new phrase to end 
127
more than 7 words past the first non-covered 
word, as long as the new phrase starts within 7 
words from the first non-covered word. Notwith-
standing the distortion limit, contiguous phrases 
can always be swapped. Out-of-vocabulary 
(OOV) source words are passed through un-
changed to the target. Loglinear weights are 
tuned with Och's max-BLEU algorithm over lat-
tices (Macherey et al, 2008); more details about 
lattice MERT are given in the next section. The 
second pass rescores 1000-best lists produced by 
the first pass, with additional features including 
various LM and IBM-model probabilities; ngram, 
length, and reordering posterior probabilities and 
frequencies; and quote and parenthesis mismatch 
indicators. To improve the quality of the maxima 
found by MERT when using large sets of partial-
ly-overlapping rescoring features, we use greedy 
feature selection, first expanding from a baseline 
set, then pruning. 
We restricted our training data to data that was 
directly available through the workshop's web-
site; we didn?t use the LDC resources mentioned 
on the website (e.g., French Gigaword, English 
Gigaword). Below, ?mono? refers to all mono-
lingual data (Europarl, news-commentary, and 
shuffle); ?mono? English is roughly three times 
bigger than ?mono? French (50.6 M lines in 
?mono? English, 17.7 M lines in ?mono? French). 
?Domain? refers to all WMT parallel training 
data except GigaFrEn (i.e., Europarl, news-
commentary, and UN).   
2.2 Preprocessing and postprocessing 
We used our own English and French pre- and 
post-processing tools, rather than those available 
from the WMT web site. For training, all English 
and French text is tokenized with a language-
specific tokenizer and then mapped to lowercase. 
Truecasing uses an HMM approach, with lexical 
probabilities derived from ?mono? and transition 
probabilities from a 3-gram LM trained on tru-
ecase ?mono?. A subsequent rule-based pass ca-
pitalizes sentence-initial words. A final detokeni-
zation step undoes the tokenization. 
2.3 System configurations for WMT 2010 
In the weeks preceding the evaluation, we tried 
several ways of arranging the resources available 
to us. We picked the configurations that gave the 
highest BLEU scores on WMT2009 Newstest. 
We found that tuning with lattice MERT rather 
than N-best MERT allowed us to employ more 
parameters and obtain better results.  
E-F system components: 
1. Phrase table trained on ?domain?;  
2. Phrase table trained on GigaFrEn;  
3. Lexicalized distortion model trained on 
?domain?;  
4. Distance-based distortion model; 
5. 5-gram French LM trained on ?mono?;  
6. 4-gram LM trained on French half of 
GigaFrEn;  
7. Dynamic LM composed of 4 LMs, each 
trained on the French half of a parallel 
corpus (5-gram LM trained on ?domain?, 
4-gram LM on GigaFrEn, 5-gram LM on 
news-commentary and 5-gram LM on 
UN). 
 
The F-E system is a mirror image of the E-F sys-
tem.  
3 Details of lattice MERT (LMERT) 
Our system?s implementation of LMERT (Ma-
cherey et al, 2008) is the most notable recent 
change in our system. As more and more features 
are included in the loglinear model, especially if 
they are correlated, N-best MERT (Och, 2003) 
shows more and more instability, because of 
convergence to local optima (Foster and Kuhn, 
2009). We had been looking for methods that 
promise more stability and better convergence. 
LMERT seemed to fit the bill. It optimizes over 
the complete lattice of candidate translations af-
ter a decoding run. This avoids some of the prob-
lems of N-best lists, which lack variety, leading 
to poor local optima and the need for many de-
coder runs. 
Though the algorithm is straightforward and is 
highly parallelizable, attention must be paid to 
space and time resource issues during implemen-
tation. Lattices output by our decoder were large 
and needed to be shrunk dramatically for the al-
gorithm to function well. Fortunately, this could 
be achieved via the finite state equivalence algo-
rithm for minimizing deterministic finite state 
machines. The second helpful idea was to sepa-
rate out the features that were a function of the 
phrase associated with an arc (e.g., translation 
length and translation model probability fea-
tures). These features could then be stored in a 
smaller phrase-feature table. Features associated 
with language or distortion models could be han-
dled in a larger transition-feature table. 
The above ideas, plus careful coding of data 
structures, brought the memory footprint down 
sufficiently to allow us to use complete lattices 
from the decoder and optimize over the complete 
128
development set for NIST09 Chinese-English. 
However, combining lattices between decoder 
runs again resulted in excessive memory re-
quirements. We achieved acceptable perfor-
mance by searching only the lattice from the lat-
est decoder run; perhaps information from earlier 
runs, though critical for convergence in N-best 
MERT, isn?t as important for LMERT.  
Until a reviewer suggested it, we had not 
thought of pruning lattices to a specified graph 
density as a solution for our memory problems. 
This is referred to in a single sentence in (Ma-
cherey et al, 2008), which does not specify its 
implementation or its impact on performance, 
and is an option of OpenFst (we didn?t use 
OpenFst). We will certainly experiment with lat-
tice pruning in future.  
Powell's algorithm (PA), which is at the core 
of MERT, has good convergence when features 
are mostly independent and do not depart much 
from a simple coordinate search; it can run into 
problems when there are many correlated fea-
tures (as with multiple translation and language 
models). Figure 1 shows the kind of case where 
PA works well. The contours of the function be-
ing optimized are relatively smooth, facilitating 
learning of new search directions from gradients. 
Figure 2 shows a more difficult case: there is 
a single optimum, but noise dominates and PA 
has difficulty finding new directions. Search of-
ten iterates over the original co-ordinates, miss-
ing optima that are nearby but in directions not 
discoverable from local gradients. Probes in ran-
dom directions can do better than iteration over 
the same directions (this is similar to the method 
proposed for N-best MERT by Cer et al, 2008). 
Each 1-dimensional MERT optimization is exact, 
so if our probe stabs a region with better scores, 
it will be discovered. Figures 1 and 2 only hint 
at the problem: in reality, 2-dimensional search 
isn?t a problem. The difficulties occur as the di-
mension grows: in high dimensions, it is more 
important to get good directions and they are 
harder to find. 
For WMT 2010, we crafted a compromise 
with the best properties of PA, yet alowing for a 
more aggressive search in more directions. We 
start with PA. As long as PA is adding new di-
rection vectors, it is continued. When PA stops 
adding new directions, random rotation (ortho-
gonal transformation) of the coordinates is per-
formed and PA is restarted in the new space. PA 
almost always fails to introduce new directions 
within the new coordinates, then fails again, so 
another set of random coordinates is chosen. This 
process repeats until convergence. In future 
work, we will look at incorporating random res-
tarts into the algorithm as additional insurance 
against premature convergence.  
Our LMERT implementation has room for 
improvement: it may still run into over-fitting 
problems with many correlated features. Howev-
er, during preparation for the evaluation, we no-
ticed that LMERT converged better than N-best 
MERT, allowing models with more features and 
higher BLEU to be chosen.  
After the WMT submission, we discovered 
that our LMERT implementation had a bug; our 
submission was tuned with this buggy LMERT. 
Comparison between our E-F submission tuned 
with N-best MERT and the same system tuned 
with bug-fixed LMERT shows BLEU gains of 
+1.5-3.5 for LMERT (on dev, WMT2009, and 
WMT2010, with no rescoring). However, N-best 
MERT performed very poorly in this particular 
case; we usually obtain a gain due to LMERT of 
+0.2-1.0 (e.g., for the submitted F-E system).  
 
 
Figure 1: Convergence for PA (Smooth Feature 
Space)  
 
 
Figure 2: Convergence for PA with Random Rotation 
(Rough Feature Space) 
129
4 Problems and Solutions 
4.1 Fixing LMERT  
Just after the evaluation, we noticed a discrepan-
cy for E-F between BLEU scores computed dur-
ing LMERT optimization and scores from the 1-
best list immediately after decoding. Our 
LMERT code had a bug that garbled any ac-
cented word in the version of the French refer-
ence in memory; previous LMERT experiments 
had English as target language, so the bug hadn?t 
showed up. The bug didn?t affect characters in 
the 7-bit ASCII set, such as English ones, only 
accented characters. Words in candidate transla-
tions were not garbled, so correct translations 
with accents received a lower BLEU score than 
they should have. As Table 1 shows, this bug 
cost us about 0.5 BLEU for WMT 2010 E-F after 
rescoring (according to NRC?s internal version 
of BLEU, which differs slightly from WMT?s 
BLEU). Despite this bug, the system tuned with 
buggy LMERT (and submitted) was still better 
than the best system we obtained with N-best 
MERT. The bug didn?t affect F-E scores.  
 
 Dev WMT2009 WMT2010 
LMERT (bug) 25.26 26.85 27.55 
LMERT 
 (no bug) 
25.43 26.89 28.07 
 
Table 1: LMERT bug fix (E-F BLEU after rescoring) 
4.2 Fixing odd translations 
After the evaluation, we carefully studied the 
system outputs on the WMT 2010 test data, par-
ticularly for E-F. Apart from truecasing errors, 
we noticed two kinds of bad behaviour: transla-
tions of proper names and apparent passthrough 
of English words to the French side.  
Examples of E-F translations of proper names 
from our WMT 2010 submission (each from a 
different sentence): 
 
Mr. Onderka ? M. Roman, Luk?? Marvan ? G. 
Luk??, Janey ? The, Janette Tozer ? Janette, 
Aysel Tugluk ? joints tugluk, Tawa Hallae ? 
Ottawa, Oleson ?  production,  Alcobendas ?  ; 
 
When the LMERT bug was fixed, some but 
not all of these bad translations were corrected 
(e.g., 3 of the 8 examples above were corrected). 
Our system passes OOV words through un-
changed. Thus, the names above aren?t OOVs, 
but words that occur rarely in the training data, 
and for which bad alignments have a dispropor-
tionate effect. We realized that when a source 
word begins with a capital, that may be a signal 
that it should be passed through. We thus de-
signed a passthrough feature function that applies 
to all capitalized forms not at the start of a sen-
tence (and also to forms at the sentence start if 
they?re capitalized elsewhere). Sequences of one 
or more capitalized forms are grouped into a 
phrase suggestion (e.g., Barack Obama ? bar-
rack obama) which competes with phrase table 
entries and is assigned a weight by MERT. 
The passthrough feature function yields a tiny 
improvement over the E-F system with the bug-
fixed LMERT on the dev corpus (WMT2008): 
+0.06 BLEU (without rescoring). It yields a larg-
er improvement on our test corpus: +0.27 BLEU 
(without rescoring). Furthermore, it corrects all 
the examples from the WMT 2010 test shown 
above (after the LMERT bug fix 5 of the 8 ex-
amples above still had problems, but when the 
passthrough function is incorporated all of them 
go away). Though the BLEU gain is small, we 
are happy to have almost eradicated this type of 
error, which human beings find very annoying.  
The opposite type of error is apparent pass-
through. For instance, ?we?re? appeared 12 times 
in the WMT 2010 test data, and was translated 6 
times into French as ?we?re? - even though better 
translations had higher forward probabilities. The 
source of the problem is the backward probabili-
ty P(E=?we?re?|F=?we?re?), which is 1.0; the 
backward probabilities for valid French transla-
tions of ?we?re? are lower. Because of the high 
probability P(E=?we?re?|F=?we?re?) within the 
loglinear combination, the decoder often chooses 
?we?re? as the French translation of ?we?re?. 
The (E=?we?re?, F=?we?re?) pair in WMT 
2010 phrase tables arose from two sentence pairs 
where the ?French? translation of an English sen-
tence is a copy of that English sentence. In both, 
the original English sentence contains ?we?re?. 
Naturally, the English words on the ?French? 
side are word-aligned with their identical twins 
on the English side. Generally, if the training 
data has sentence pairs where the ?French? sen-
tence contains words from the English sentence, 
those words will get high backward probabilities 
of being translated as themselves. This problem 
may not show up as an apparent passthrough; 
instead, it may cause MERT to lower the weight 
of the backward probability component, thus 
hurting performance.  
We estimated English contamination of the 
French side of the parallel training data by ma-
130
nually inspecting a random sample of ?French? 
sentences containing common English function 
words. Manual inspection is needed for accurate 
estimation: a legitimate French sentence might 
contain mostly English words if, e.g., it is short 
and cites the title of an English work (this 
wouldn?t count as contamination). The degree of 
contamination is roughly 0.05% for Europarl, 
0.5% for news-commentary, 0.5% for UN, and 
1% for GigaFrEn (in these corpora the French is 
also contaminated by other languages, particular-
ly German). Foreign contamination of English 
for these corpora appears to be much less fre-
quent.  
Contamination can take strange forms. We ex-
pected to see English sentences copied over in-
tact to the French side, and we did, but we did 
not expect to see so many ?French? sentences 
that interleaved short English word sequences 
with short French word sequences, apparently 
because text with an English and a French col-
umn had been copied by taking lines from alter-
nate columns. We found many of these inter-
leaved ?French? sentences, and found some of 
them in exactly this form on the Web (i.e., the 
corruption didn?t occur during WMT data collec-
tion). The details may not matter: whenever the 
?French? training sentence contains words from 
its English twin, there can be serious damage via 
backward probabilities. 
To test this hypothesis, we filtered all parallel 
and monolingual training data for the E-F system 
with a language guessing tool called text_cat 
(Cavnar and Trenkle, 1994). From parallel data, 
we filtered out sentence pairs whose French side 
had a high probability of not being French; from 
LM training data, sentences with a high non-
French probability. We set the filtering level by 
inspecting the guesser?s assessment of news-
commentary sentences, choosing a rather aggres-
sive level that eliminated 0.7% of news-
commentary sentence pairs. We used the same 
level to filter Europarl (0.8% of sentence pairs 
removed), UN (3.4%), GigaFrEn (4.7%), and 
?mono? (4.3% of sentences).  
 
 Dev WMT2009 WMT2010 
Baseline 25.23 26.47 27.72 
Filtered 25.45 26.66 27.98 
 
Table 2: Data filtering (E-F BLEU, no rescoring) 
 
Table 2 shows the results: a small but consis-
tent gain (about +0.2 BLEU without rescoring). 
We have not yet confirmed the hypothesis that 
copies of source-language words in the paired 
target sentence within training data can damage 
system performance via backward probabilities.  
4.3 Fixing problems with LM training   
Post-evaluation, we realized that our arrange-
ment of the training data for the LMs for both 
language directions was flawed. The grouping 
together of disparate corpora in ?mono? and 
?domain? didn?t allow higher-quality, truly in-
domain corpora to be weighted more heavily 
(e.g., the news corpora should have higher 
weights than Europarl, but they are lumped to-
gether in ?mono?). There are also potentially 
harmful overlaps between LMs (e.g., GigaFrEn 
is used both inside and outside the dynamic LM).  
We trained a new set of French LMs for the E-
F system, which replaced all the French LMs 
(#5-7) described in section 2.3 in the E-F system: 
1. 5-gram LM trained on news-commentary 
and shuffle;  
2. Dynamic LM based on 4 5-gram LMs 
trained on French side of parallel data 
(LM trained on GigaFrEn, LM on UN, 
LM on Europarl, and LM on news-
commentary). 
We did not apply the passthrough function or 
language filtering (section 4.2) to any of the 
training data for any component (LMs, TMs, dis-
tortion models) of this system; we did use the 
bug-fixed version of LMERT (section 4.1). 
The experiments with these new French LMs 
for the E-F system yielded a small decrease of 
NRC BLEU on dev (-0.15) and small increases 
on WMT Newstest 2009 and Newstest 2010 
(+0.2 and +0.4 respectively without rescoring). 
We didn?t do F-E experiments of this type.  
4.4 Pooling improvements   
The improvements above were (individual un-
cased E-F BLEU gains without rescoring in 
brackets): LMERT bug fix (about +0.5); pass-
through feature function (+0.1-0.3); language 
filtering for French (+0.2). There was also a 
small gain on test data by rearranging E-F LM 
training data, though the loss on ?dev? suggests 
this may be a statistical fluctuation. We built 
these four improvements into the evaluation E-F 
system, along with quote normalization: in all 
training and test data, diverse single quotes were 
mapped onto the ascii single quote, and diverse 
double quotes were mapped onto the ascii double 
quote. The average result on WMT2009 and 
WMT2010 was +1.7 BLEU points compared to 
the original system, so there may be synergy be-
131
tween the improvements. The original system 
had gained +0.3 from rescoring, while the final 
improved system only gained +0.1 from rescor-
ing: a post-evaluation rescored gain of +1.5.  
An experiment in which we dropped lexica-
lized distortion from the improved system 
showed that this component yields about +0.2 
BLEU. Much earlier, when we were still training 
systems with N-best MERT, incorporation of the 
6-feature lexicalized distortion often caused 
scores to go down (by as much as 2.8 BLEU). 
This illustrates how LMERT can make incorpo-
ration of many more features worthwhile.  
4.5 Fixing truecasing  
Our truecaser doesn?t work as well as truecasers 
of other WMT groups: we lost 1.4 BLEU by tru-
ecasing in both language directions, while others 
lost 1.0 or less. To improve our truecaser, we 
tried: 1. Training it on all relevant data and 2. 
Collecting 3-gram case-pattern statistics instead 
of unigrams. Neither of these helped significant-
ly. One way of improving the truecaser would be 
to let case information from source words influ-
ence the case of the corresponding target words. 
Alternatively, one of the reviewers stated that 
several labs involved in WMT have no separate 
truecaser and simply train on truecase text. We 
had previously tried this approach for NIST Chi-
nese-English and discarded it because of its poor 
performance. We are currently re-trying it on 
WMT data; if it works better than having a sepa-
rate truecaser, this was yet another area where 
lessons from Chinese-English were misleading. 
5 Lessons  
LMERT is an improvement over N-best MERT. 
The submitted system was one for which N-best 
MERT happened to work very badly, so we got 
ridiculously large gains of +1.5-3.5 BLEU for 
non-buggy LMERT over N-best MERT. These 
results are outliers: in experiments with similar 
configurations, we typically get +0.2-1.0 for 
LMERT over N-best MERT. Post-evaluation, 
four minor improvements ? a case-based pass-
through function, language filtering, LM rear-
rangement, and quote normalization ? collective-
ly gave a nice improvement. Nothing we tried 
helped truecaser performance significantly, 
though we have some ideas on how to proceed. 
We learned some lessons from WMT 2010. 
Always test your system on the relevant lan-
guage pair. Our original version of LMERT was 
developed on Chinese-English and worked well 
there, but had a bug that surfaced only when the 
target language had accents.  
European language pairs are more porous to 
information than Chinese-English. Our WMT 
system reflected design decisions for Chinese-
English, and thus didn?t exploit case information 
in the source: it passed through OOVs to the tar-
get, but didn?t pass through upper-case words 
that are likely to be proper nouns.  
It is beneficial to remove foreign-language 
contamination from the training data.  
When entering an evaluation one hasn?t parti-
cipated in for several years, always read system 
papers from the previous year. Some of the 
WMT 2008 system papers mention passthrough 
of some non-OOVs, filtering out of noisy train-
ing data, and using the case of a source word to 
predict the case of the corresponding target word. 
References  
William Cavnar and John Trenkle. 1994. N-Gram-
Based Text Categorization. Proc. Symposium on 
Document Analysis and Information Retrieval, 
UNLV Publications/Reprographics, pp. 161-175. 
Daniel Cer, Daniel Jurafsky, and Christopher D. 
Manning. 2008. Regularization and search for min-
imum error rate training. Proc. Workshop on 
SMT, pp. 26-34. 
George Foster and Roland Kuhn. 2009. Stabilizing 
Minimum Error Rate Training. Proc. Workshop 
on SMT, pp. 242-249. 
George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. Proc. Workshop on 
SMT, pp. 128-135. 
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language 
Models.  Proc. ACL, pp.  144-151. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Transcription Evalua-
tion. MT Eval. Workshop. 
Wolfgang Macherey, Franz Josef Och, Ignacio Thay-
er, and Jakob Uszkoreit. 2008. Lattice-based Min-
imum Error Rate Training for Statistical Machine-
Translation. Conf. EMNLP, pp. 725-734. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation.  Proc. ACL, 
pp. 160-167.  
Richard Zens and Hermann Ney. 2004. Improvements 
in Phrase-Based Statistical Machine Translation. 
Proc. HLT/NAACL, pp. 257-264. 
132
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 71?77,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
AMBER: A Modified BLEU, Enhanced Ranking Metric 
 
Boxing Chen and Roland Kuhn 
National Research Council of Canada, Gatineau, Qu?bec, Canada 
First.Last@nrc.gc.ca 
 
  
  
Abstract 
This paper proposes a new automatic ma-
chine translation evaluation metric: 
AMBER, which is based on the metric 
BLEU but incorporates recall, extra penal-
ties, and some text processing variants. 
There is very little linguistic information in 
AMBER. We evaluate its system-level cor-
relation and sentence-level consistency 
scores with human rankings from the 
WMT shared evaluation task; AMBER 
achieves state-of-the-art performance. 
1 Introduction 
Automatic evaluation metrics for machine transla-
tion (MT) quality play a critical role in the devel-
opment of statistical MT systems. Several metrics 
have been proposed in recent years.  Metrics such 
as BLEU (Papineni et al, 2002), NIST (Dodding-
ton, 2002), WER, PER, and TER (Snover et al, 
2006) do not use any linguistic information - they 
only apply surface matching. METEOR (Banerjee 
and Lavie, 2005), METEOR-NEXT (Denkowski 
and Lavie 2010), TER-Plus (Snover et al, 2009), 
MaxSim (Chan and Ng, 2008), and TESLA (Liu et 
al., 2010) exploit some limited linguistic resources, 
such as synonym dictionaries, part-of-speech tag-
ging or paraphrasing tables. More sophisticated 
metrics such as RTE (Pado et al, 2009) and DCU-
LFG (He et al, 2010) use higher level syntactic or 
semantic analysis to score translations. 
Though several of these metrics have shown bet-
ter correlation with human judgment than BLEU, 
BLEU is still the de facto standard evaluation me-
tric. This is probably due to the following facts: 
1. BLEU is language independent (except for 
word segmentation decisions).  
2. BLEU can be computed quickly. This is im-
portant when choosing a metric to tune an 
MT system. 
3. BLEU seems to be the best tuning metric 
from a quality point of view - i.e., models 
trained using BLEU obtain the highest 
scores from humans and even from other 
metrics (Cer et al, 2010). 
When we developed our own metric, we decided 
to make it a modified version of BLEU whose 
rankings of translations would (ideally) correlate 
even more highly with human rankings. Thus, our 
metric is called AMBER: ?A Modified Bleu, En-
hanced Ranking? metric. Some of the AMBER 
variants use an information source with a mild lin-
guistic flavour ? morphological knowledge about 
suffixes, roots and prefixes ? but otherwise, the 
metric is based entirely on surface comparisons.  
2 AMBER 
Like BLEU, AMBER is composed of two parts: a 
score and a penalty. 
 
penaltyscoreAMBER ?=                 (1)  
 
To address weaknesses of BLEU described in 
the literature (Callison-Burch et al, 2006; Lavie 
and Denkowski, 2009), we use more sophisticated 
formulae to compute the score and penalty. 
2.1 Enhancing the score 
First, we enrich the score part with geometric av-
erage of n-gram precisions (AvgP), F-measure de-
rived from the arithmetic averages of precision and 
recall (Fmean), and arithmetic average of F-
measure of precision and recall for each n-gram 
(AvgF). Let us define n-gram precision and recall 
as follows: 
71
)(#
)(#)(
Tngrams
RTngrams
np ?=               (2) 
  )(#
)(#)(
Rngrams
RTngrams
nr
?
=               (3) 
where T = translation, R = reference.  
Then the geometric average of n-gram preci-
sions AvgP, which is also the score part of the 
BLEU metric, is defined as: 
NN
n
npNAvgP
1
1
)()( ???
?
???
?
= ?
=
             (4) 
 
The arithmetic averages for n-gram precision 
and recall are: 
?
=
=
N
n
np
N
NP
1
)(1)(                       (5) 
?
=
=
M
n
nr
M
MR
1
)(1)(                      (6) 
 The F-measure that is derived from P(N) and 
R(M), (Fmean), is given by: 
)()1()(
)()(),,(
MRNP
MRNPMNFmean
??
?
?+
=     (7) 
 
The arithmetic average of F-measure of preci-
sion and recall for each n-gram (AvgF) is given by: 
?
=
?+
=
N
n nrnp
nrnp
N
NAvgF
1 )()1()(
)()(1),(
??
?    (8) 
The score is the weighted average of the three 
values: AvgP, Fmean, and AvgF. 
),()1(
),,(
)()(
21
2
1
???
??
?
NAvgF
MNFmean
NAvgPNscore
???+
?+
?=
  
 (9) 
The free parameters N, M,? , 1?  and 2?  were  
manually tuned on a dev set.  
2.2 Various penalties 
Instead of the original brevity penalty, we experi-
mented with a product of various penalties: 
?
=
=
P
i
w
i
ipenpenalty
1
                  (10) 
where wi is the weight of each penalty peni.  
Strict brevity penalty (SBP): (Chiang et al, 
2008) proposed this penalty. Let ti be the transla-
tion of input sentence i, and let ri be its reference 
(or if there is more than one, the reference whose 
length in words || ir  is closest to length || it ). Set 
???
?
???
?
?= ?
?
i ii
i i
rt
r
SBP |}||,min{|
||
1exp     (11) 
Strict redundancy penalty (SRP): long sen-
tences are preferred by recall. Since we rely on 
both recall and precision to compute the score, it is 
necessary to punish the sentences that are too long.  
???
?
???
?
?= ?
?
i i
i ii
r
rt
SRP ||
|}||,max{|
1exp      (12) 
Character-based strict brevity penalty 
(CSBP) and Character-based strict redundancy 
penalty (CSRP) are defined similarly. The only 
difference with the above two penalties is that 
here, length is measured in characters. 
Chunk penalty (CKP): the same penalty as in 
METEOR: 
?
? ???
?
???
?
??= )(#
#1
wordmatches
chunksCKP       (13) 
? and ?  are free parameters. We do not compute 
the word alignment between the translation and 
reference; therefore, the number of chunks is com-
puted as )(#)(## wordmatchesbigrammatcheschunks ?= . 
For example, in the following two-sentence trans-
lation (references not shown), let ?mi? stand for a 
matched word, ?x? stand for zero, one or more 
unmatched words:  
S1: m1 m2 x m3 m4 m5 x m6  
S2: m7 x m8 m9 x m10 m11 m12 x m13 
If we consider only unigrams and bigrams, there 
are 13 matched words and 6 matched bigrams (m1 
m2, m3 m4, m4 m5, m8 m9, m10 m11, m11 m12), so there 
are 13-6=7 chunks (m1 m2, m3 m4 m5, m6, m7, m8 m9, 
m10 m11 m12, m13).  
Continuity penalty (CTP): if all matched 
words are continuous, then 
segmentRTgramsn
RTngrams
#)()1(#
)(#
???
?
 equals 1.  
Example: 
S3: m1 m2 m3 m4 m5m6  
S4: m7 m8 m9m10 m11 m12 m13 
There are 13 matched unigrams, and 11 matched 
bi-grams; we get 11/(13-2)=1. Therefore, a conti-
nuity penalty is computed as: 
72
 ???
?
???
?
???
?
?
?= ?
=
N
n segmentRTgramsn
RTngrams
N
CTP
2 #)()1(#
)(#
1
1
exp (14) 
 
Short word difference penalty (SWDP): a 
good translation should have roughly the same 
number of stop words as the reference. To make 
AMBER more portable across all Indo-European 
languages, we use short words (those with fewer 
than 4 characters) to approximate the stop words.   
))(#
||
exp(
runigram
baSWDP ??=           (15) 
where a and b are the number of short words in the 
translation and reference respectively. 
Long word difference penalty (LWDP): is de-
fined similarly to SWDP.  
))(#
||
exp(
runigram
dcLWDP ??=           (15) 
where c and d  are the number of long words (those 
longer than 3 characters) in the translation and ref-
erence respectively. 
Normalized Spearman?s correlation penalty 
(NSCP): we adopt this from (Isozaki et al, 2010). 
This penalty evaluates similarity in word order be-
tween the translation and reference. We first de-
termine word correspondences between the 
translation and reference; then, we rank words by 
their position in the sentences. Finally, we compute 
Spearman?s correlation between the ranks of the n 
words common to the translation and reference. 
)1()1(1
2
?+
?=
?
nnn
d
i i?                (16) 
where di indicates the distance between the ranks 
of the i-th element. For example: 
T: Bob reading book likes
 
R: Bob likes reading book
 
 
The rank vector of the reference is [1, 2, 3, 4], 
while the translation rank vector is [1, 3, 4, 2]. The 
Spearman?s correlation score between these two 
vectors is 
)14(4)14(
)42()34()23(01
222
???+
?+?+?+
?
=0.90. 
In order to avoid negative values, we normalized 
the correlation score, obtaining the penalty NSCP: 
2)1( /?NSCP +=                     (17) 
Normalized Kendall?s correlation penalty 
(NKCP):  this is adopted from (Birch and Os-
borne, 2010) and (Isozaki et al, 2010). In the pre-
vious example, where the rank vector of the 
translation is [1, 3, 4, 2], there are 624 =C  pairs of 
integers. There are 4 increasing pairs: (1,3), (1,4), 
(1,2) and (3,4). Kendall?s correlation is defined by:  
1
#
#2 ??=
pairsall
pairsasingincre
?         (18) 
Therefore, Kendall?s correlation for the transla-
tion ?Bob reading book likes? is 16/42 ?? =0.33. 
Again, to avoid negative values, we normalized 
the coefficient score, obtaining the penalty NKCP: 
2)1( /NKCP ?+=                     (19) 
2.3 Term weighting 
The original BLEU metric weights all n-grams 
equally; however, different n-grams have different 
amounts of information. We experimented with 
applying tf-idf to weight each n-gram according to 
its information value. 
2.4 Four matching strategies 
In the original BLEU metric, there is only one 
matching strategy: n-gram matching. In AMBER, 
we provide four matching strategies (the best 
AMBER variant used three of these): 
1. N-gram matching: involved in computing 
precision and recall. 
2. Fixed-gap n-gram: the size of the gap be-
tween words ?word1 [] word2? is fixed; 
involved in computing precision only. 
3. Flexible-gap n-gram:  the size of the gap 
between words ?word1 * word2? is flexi-
ble; involved in computing precision only. 
4. Skip n-gram: as used ROUGE (Lin, 2004); 
involved in computing precision only. 
2.5 Input preprocessing 
The AMBER score can be computed with different 
types of preprocessing. When using more than one 
type, we computed the final score as an average 
over runs, one run per type (our default AMBER 
variant used three of the preprocessing types): 
?
=
=
T
t
tAMBER
T
AMBERFinal
1
)(1_
 
We provide 8 types of possible text input: 
0. Original - true-cased and untokenized. 
73
1. Normalized - tokenized and lower-cased.  
(All variants 2-7 below also tokenized and 
lower-cased.)  
2. ?Stemmed? - each word only keeps its first 
4 letters. 
3. ?Suffixed? - each word only keeps its last 
4 letters. 
4. Split type 1 - each longer-than-4-letter 
word is segmented into two sub-words, 
with one being the first 4 letters and the 
other the last 2 letters. If the word has 5 
letters, the 4th letter appears twice: e.g., 
?gangs? becomes ?gang? + ?gs?. If the 
word has more than 6 letters, the middle 
part is thrown away 
5. Split type 2 - each word is segmented into 
fixed-length (4-letter) sub-word sequences, 
starting from the left.  
6. Split type 3 - each word is segmented into 
prefix, root, and suffix. The list of English 
prefixes, roots, and suffixes used to split 
the word is from the Internet1; it is used to 
split words from all languages. Linguistic 
knowledge is applied here (but not in any 
other aspect of AMBER).  
7. Long words only - small words (those with 
fewer than 4 letters) are removed. 
3 Experiments 
3.1 Experimental data 
We evaluated AMBER on WMT data, using WMT 
2008 all-to-English submissions as the dev set. 
Test sets include WMT 2009 all-to-English, WMT 
2010 all-to-English and 2010 English-to-all sub-
missions. Table 1 summarizes the dev and test set 
statistics. 
Set Dev Test1 Test2 Test3 
Year 2008 2009 2010 2010 
Lang. xx-en xx-en xx-en en-xx 
#system 43 39 53 32 
#sent-pair 7,861 13,912 14,212 13,165 
Table 1: statistics of the dev and test sets. 
                                                           
1http://en.wikipedia.org/wiki/List_of_Greek_and_Latin_roots_
in_English 
3.2 Default settings 
Before evaluation, we manually tuned all free pa-
rameters on the dev set to maximize the system-
level correlation with human judgments and de-
cided on the following default settings for 
AMBER:   
1. The parameters in the formula  
),()1(
),,(
)()(
21
2
1
???
??
?
NAvgF
MNFmean
NAvgPNscore
???+
?+
?=
 
are set as  N=4, M=1, ? =0.9, 1? = 0.3 
and 2? = 0.5.  
2. All penalties are applied; the manually set 
penalty weights are shown in Table 2. 
3. We took the average of runs over input text 
types 1, 4, and 6 (i.e. normalized text, 
split type 1 and split type 3).  
4. In Chunk penalty (CKP), 3=? , and 
? =0.1. 
5. By default, tf-idf is not applied.  
6. We used three matching strategies: n-gram, 
fixed-gap n-gram, and flexible-gap n-
gram; they are equally weighted. 
 
Name of penalty Weight value 
SBP 0.30 
SRP 0.10 
CSBP 0.15 
CSRP 0.05 
SWDP 0.10 
LWDP 0.20 
CKP 1.00 
CTP 0.80 
NSCP 0.50 
NKCP 2.00 
Table 2: Weight of each penalty 
3.3 Evaluation metrics 
We used Spearman?s rank correlation coefficient to 
measure the correlation of AMBER with the hu-
man judgments of translation at the system level. 
The human judgment score we used is based on the 
?Rank? only, i.e., how often the translations of the 
system were rated as better than the translations 
from other systems (Callison-Burch et al, 2008). 
Thus, AMBER and the other metrics were eva-
luated on how well their rankings correlated with 
74
the human ones. For the sentence level, we use 
consistency rate, i.e., how consistent the ranking of 
sentence pairs is with the human judgments. 
3.4 Results 
All test results shown in this section are averaged 
over all three tests described in 3.1. First, we com-
pare AMBER with two of the most widely used 
metrics: original IBM BLEU and METEOR v1.0. 
Table 3 gives the results; it shows both the version 
of AMBER with basic preprocessing, AMBER(1) 
(with tokenization and lowercasing) and the default 
version used as baseline for most of our experi-
ments (AMBER(1,4,6)). Both versions of AMBER 
perform better than BLEU and METEOR on both 
system and sentence levels. 
 
Metric  
 Dev     3 tests average   ? tests 
BLEU_ibm 
(baseline) 
sys 
sent 
0.68            0.72               N/A 
0.37            0.40               N/A 
METEOR 
     v1.0 
sys 
sent 
0.80            0.80              +0.08 
0.58            0.56              +0.17 
AMBER(1) 
(basic preproc.) 
sys 
sent 
0.83            0.83              +0.11 
0.61            0.58              +0.19 
AMBER(1,4,6) 
(default)  
sys 
sent 
0.84            0.86              +0.14 
0.62            0.60              +0.20 
 
 Table 3: Results of AMBER vs BLEU and METEOR 
 
Second, as shown in Table 4, we evaluated the 
impact of different types of preprocessing, and 
some combinations of preprocessing (we do one 
run of evaluation for each type and average the 
results). From this table, we can see that splitting 
words into sub-words improves both system- and 
sentence-level correlation. Recall that input 6 pre-
processing splits words according to a list of Eng-
lish prefixes, roots, and suffixes: AMBER(4,6) is 
the best variant. Although test 3 results, for target 
languages other than English, are not broken out 
separately in this table,  they are as follows: input 1 
yielded 0.8345  system-level correlation and 
0.5848 sentence-level consistency, but input 6 
yielded 0.8766 (+0.04 gain) and 0.5990 (+0.01) 
respectively. Thus, surprisingly, splitting non-
English words up according to English morpholo-
gy helps performance, perhaps because French, 
Spanish, German, and even Czech share some 
word roots with English. However, as indicated by 
the underlined results, if one wishes to avoid the 
use of any linguistic information, AMBER(4) per-
forms almost as well as AMBER(4,6). The default 
setting, AMBER(1,4,6), doesn?t perform quite as 
well as AMBER(4,6) or AMBER(4), but is quite 
reasonable.  
Varying the preprocessing seems to have more 
impact than varying the other parameters we expe-
rimented with.  In Table 5, ?none+tf-idf? means 
we do one run without tf-idf and one run for ?tf-idf 
only?, and then average the scores. Here, applying 
tf-idf seems to benefit performance slightly. 
 
Input  
 Dev     3 tests average     ? tests 
0  
(baseline) 
sys 
sent 
0.84            0.79                 N/A 
0.59            0.58                 N/A 
1 sys 
sent 
0.83            0.83               +0.04 
0.61            0.58               +0.00 
2 sys 
sent 
0.83            0.84               +0.05 
0.61            0.59               +0.01 
3 sys 
sent 
0.83            0.84               +0.05 
0.61            0.58               +0.00 
4 sys 
sent 
0.84            0.87               +0.08 
0.62            0.60               +0.01 
5 sys 
sent 
0.82            0.86               +0.07 
0.61            0.56               +0.01 
6 sys 
sent 
0.83            0.88               +0.09 
0.62            0.60               +0.02 
7 sys 
sent 
0.34            0.56               -0.23 
0.58            0.53               -0.05 
1,4 sys 
sent 
0.84            0.85               +0.07 
0.62            0.60               +0.01 
4,6 sys 
sent 
0.83            0.88               +0.09 
0.62            0.60               +0.02 
1,4,6 sys 
sent 
0.84            0.86               +0.07 
0.62            0.60               +0.02 
 
Table 4: Varying AMBER preprocessing (best  
linguistic = bold, best non-ling. = underline) 
 
tf-idf  
  Dev     3 tests average    ? tests 
none 
(baseline) 
sys 
sent 
0.84             0.86                N/A 
0.62             0.60                N/A 
tf-idf 
only 
sys 
sent 
0.81             0.88              +0.02 
0.62             0.61              +0.01 
none+tf-
idf 
sys 
sent 
0.82             0.87              +0.01 
0.62             0.61              +0.01 
 
Table 5: Effect of tf-idf on AMBER(1,4,6) 
 
Table 6 shows what happens if you disable one 
penalty at a time (leaving the weights of the other 
penalties at their original values). The biggest sys-
tem-level performance degradation occurs when 
LWDP is dropped, so this seems to be the most 
75
useful penalty. On the other hand, dropping CKP, 
CSRP, and SRP may actually improve perfor-
mance. Firm conclusions would require retuning of 
weights each time a penalty is dropped; this is fu-
ture work.  
 
Penalties  
  Dev     3 tests average    ? tests 
All 
(baseline) 
sys 
sent 
0.84            0.86               N/A 
0.62            0.60               N/A 
-SBP sys 
sent 
0.82            0.84               -0.02 
0.62            0.60               -0.00 
-SRP sys 
sent 
0.83            0.88              +0.01 
0.62            0.60              +0.00 
-CSBP sys 
sent 
0.84            0.85               -0.01 
0.62            0.60              +0.00 
-CSRP sys 
sent 
0.83            0.87              +0.01 
0.62            0.60               -0.00 
-SWDP sys 
sent 
0.84            0.86               -0.00 
0.62            0.60              +0.00 
-LWDP sys 
sent 
0.83            0.83               -0.03 
0.62            0.60               -0.00 
-CTP sys 
sent 
0.82            0.84               -0.02 
0.62            0.60               -0.00 
-CKP sys 
sent 
0.83            0.87              +0.01 
0.62            0.60               -0.00 
-NSCP sys 
sent 
0.83            0.86               -0.00 
0.62            0.60              +0.00 
-NKCP sys 
sent 
0.82            0.85               -0.01 
0.62            0.60              +0.00 
 
Table 6: Dropping penalties from AMBER(1,4,6) ? 
biggest drops on test in bold 
 
Matching  
 Dev     3 tests avg     ? tests 
n-gram + fxd-
gap+ flx-gap 
(default) 
sys 
sent 
0.84             0.86         N/A 
0.62             0.60         N/A 
n-gram sys 
sent 
0.84             0.86         -0.00 
0.62             0.60         -0.00 
fxd-gap+ 
 n-gram 
sys 
sent 
0.84             0.86         -0.00 
0.62             0.60         -0.00 
flx-gap+ 
 n-gram 
sys 
sent 
0.83             0.86         -0.00 
0.62             0.60         -0.00 
skip+ 
 n-gram 
sys 
sent 
0.83             0.85         -0.01 
0.62             0.60         -0.00 
All four 
matchings 
sys 
sent 
0.83             0.86         -0.01 
0.62             0.60          0.00 
Table 7: Varying matching strategy for AMBER(1,4,6) 
 
Finally, we evaluated the effect of the matching 
strategy. According to the results shown in Table 
7, our default strategy, which uses three of the four 
types of matching (n-grams, fixed-gap n-grams, 
and flexible-gap n-grams) is close to optimal;  the 
use of skip n-grams (either by itself or in combina-
tion) may hurt performance at both system and 
sentence levels.  
4 Conclusion 
This paper describes AMBER, a new machine 
translation metric that is a modification of the 
widely used BLEU metric. We used more sophisti-
cated formulae to compute the score, we developed 
several new penalties to match the human judg-
ment, we tried different preprocessing types, we 
tried tf-idf, and we tried four n-gram matching 
strategies. The choice of preprocessing type 
seemed to have the biggest impact on performance. 
AMBER(4,6) had the best performance of any va-
riant we tried. However, it has the disadvantage of 
using some light linguistic knowledge about Eng-
lish morphology (which, oddly, seems to be help-
ful for other languages too). A purist may prefer 
AMBER(1,4) or AMBER(4), which use no linguis-
tic information and still match human judgment 
much more closely than either BLEU or 
METEOR. These variants of AMBER share 
BLEU?s virtues: they are language-independent 
and can be computed quickly. 
Of course, AMBER could incorporate more lin-
guistic information: e.g., we could use linguistical-
ly defined stop word lists in the SWDP and LWDP 
penalties, or use synonyms or paraphrasing in the 
n-gram matching.  
AMBER can be thought of as a weighted com-
bination of dozens of computationally cheap fea-
tures based on word surface forms for evaluating 
MT quality. This paper has shown that combining 
such features can be a very effective strategy for 
attaining better correlation with human judgment. 
Here, the weights on the features were manually 
tuned; in future work, we plan to learn weights on 
features automatically. We also plan to redesign 
AMBER so that it becomes a metric that is highly 
suitable for tuning SMT systems. 
References 
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of ACL 
Workshop on Intrinsic & Extrinsic Evaluation Meas-
ures for Machine Translation and/or Summarization. 
76
A. Birch and M. Osborne. 2010. LRscore for evaluating 
lexical and reordering quality in MT. In Proceedings 
of the Joint Fifth Workshop on Statistical Machine 
Translation and MetricsMATR, pages 302?307.  
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz and 
J. Schroeder. 2008. Further Meta-Evaluation of Ma-
chine Translation. In Proceedings of WMT. 
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. 
Re-evaluating the role of BLEU in machine transla-
tion research. In Proceedings of EACL. 
D. Cer, D. Jurafsky and C. Manning. 2010. The Best 
Lexical Metric for Phrase-Based Statistical MT Sys-
tem Optimization. In Proceedings of NAACL. 
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maximum 
similarity metric for machine translation evaluation. 
In Proceedings of ACL. 
D. Chiang, S. DeNeefe, Y. S. Chan, and H. T. Ng. 2008. 
Decomposability of translation metrics for improved 
evaluation and efficient algorithms. In Proceedings 
of EMNLP, pages 610?619. 
M. Denkowski and A. Lavie. 2010. Meteor-next and the 
meteor paraphrase tables: Improved evaluation sup-
port for five target languages. In Proceedings of the 
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 314?317. 
George Doddington. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of HLT. 
Y. He, J. Du, A. Way, and J. van Genabith. 2010. The 
DCU dependency-based metric in WMT-
MetricsMATR 2010. In Proceedings of the Joint 
Fifth Workshop on Statistical Machine Translation 
and MetricsMATR, pages 324?328.  
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada. 
2010. Automatic Evaluation of Translation Quality 
for Distant Language Pairs. In Proceedings of 
EMNLP.  
A. Lavie and M. J. Denkowski. 2009. The METEOR 
metric for automatic evaluation of machine transla-
tion. Machine Translation, 23. 
C.-Y. Lin. 2004. ROUGE: a Package for Automatic 
Evaluation of Summaries. In Proceedings of the 
Workshop on Text Summarization Branches Out 
(WAS 2004), Barcelona, Spain.  
C. Liu, D. Dahlmeier, and H. T. Ng. 2010. Tesla: Trans-
lation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the 
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 329?334. 
S. Pado, M. Galley, D. Jurafsky, and C.D. Manning. 
2009. Robust machine translation evaluation with en-
tailment features. In Proceedings of ACL-IJCNLP. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL. 
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. 
Makhoul. 2006. A Study of Translation Edit Rate 
with Targeted Human Annotation. In Proceedings of 
Association for Machine Translation in the Americas. 
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 
2009. Fluency, Adequacy, or HTER? Exploring Dif-
ferent Human Judgments with a Tunable MT Metric. 
In Proceedings of the Fourth Workshop on Statistical 
Machine Translation, Athens, Greece. 
77
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 59?63,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Improving AMBER, an MT Evaluation Metric 
 
Boxing Chen, Roland Kuhn and George Foster 
 
National Research Council Canada 
283 Alexandre-Tach? Boulevard, Gatineau (Qu?bec), Canada J8X 3X7 
 
{Boxing.Chen, Roland.Kuhn, George.Foster}@nrc.ca 
 
  
Abstract 
A recent paper described a new machine 
translation evaluation metric, AMBER. This 
paper describes two changes to AMBER. The 
first one is incorporation of a new ordering 
penalty; the second one is the use of the 
downhill simplex algorithm to tune the 
weights for the components of AMBER. We 
tested the impact of the two changes, using 
data from the WMT metrics task. Each of the 
changes by itself improved the performance of 
AMBER, and the two together yielded even 
greater improvement, which in some cases 
was more than additive. The new version of 
AMBER clearly outperforms BLEU in terms 
of correlation with human judgment.  
1 Introduction 
AMBER is a machine translation evaluation metric 
first described in (Chen and Kuhn, 2011). It is de-
signed to have the advantages of BLEU (Papineni 
et al, 2002), such as nearly complete language 
independence and rapid computability, while at-
taining even higher correlation with human judg-
ment. According to the paper just cited: ?It can be 
thought of as a weighted combination of dozens of 
computationally cheap features based on word sur-
face forms for evaluating MT quality?. Many re-
cently defined machine translation metrics seek to 
exploit deeper sources of knowledge than are 
available to BLEU, such as external lexical and 
syntactic resources. Unlike these and like BLEU, 
AMBER relies entirely on matching surface forms 
in tokens in the hypothesis and reference, thus sac-
rificing depth of knowledge for simplicity and 
speed.  
In this paper, we describe two improvements to 
AMBER. The first is a new ordering penalty called 
?v? developed in (Chen et al, 2012). The second 
remedies a weakness in the 2011 version of 
AMBER  by carrying out automatic, rather than 
manual, tuning of this metric?s free parameters; we 
now use the simplex algorithm to do the tuning. 
2 AMBER 
AMBER is the product of a score and a penalty, as 
in Equation (1); in this, it resembles BLEU. How-
ever, both the score part and the penalty part are 
more sophisticated than in BLEU. The score part 
(Equation 2) is enriched by incorporating the 
weighted average of n-gram precisions (AvgP), the 
F-measure derived from the arithmetic averages of 
precision and recall (Fmean), and the arithmetic 
average of F-measure of precision and recall for 
each n-gram (AvgF). The penalty part is a 
weighted product of several different penalties 
(Equation 3). Our original AMBER paper (Chen 
and Kuhn, 2011) describes the ten penalties used at 
that time; two of these penalties, the normalized 
Spearman?s correlation penalty and the normalized 
Kendall?s correlation penalty, model word reorder-
ing.  
 
penaltyscoreAMBER ?=                 (1)  
AvgF
FmeanAvgPscore
???+
?+?=
)1( 21
21
??
??
  
      (2) 
?
=
=
P
i
w
i
ipenpenalty
1
                           (3) 
where 1?  and 2?  are weights of each score com-
ponent; wi is the weight of each penalty peni. 
59
In addition to the more complex score and pen-
alty factors, AMBER differs from BLEU in two 
other ways: 
? Not only fixed n-grams, but three different 
kinds of flexible n-grams, are used in com-
puting scores and penalties.  
? The AMBER score can be computed with 
different types of text preprocessing, i.e. 
different combinations of several text pre-
processing techniques: lowercasing, to-
kenization, stemming, word splitting, etc. 8 
types were tried in (Chen and Kuhn, 2011). 
When using more than one type, the final 
score is computed as an average over runs, 
one run per type. In the experiments re-
ported below, we averaged over two types 
of preprocessing. 
3 Improvements to AMBER 
3.1   Ordering penalty v 
We use a simple matching algorithm (Isozaki et 
al., 2010) to do 1-1 word alignment between the 
hypothesis and the reference.  
After word alignment, represent the reference by 
a list of normalized positions of those of its words 
that were aligned with words in the hypothesis, and 
represent the hypothesis by a list of positions for 
the corresponding words in the reference. For both 
lists, unaligned words are ignored. E.g., let P1 = 
reference, P2 = hypothesis: 
P1: 11p  
2
1p  
3
1p  
4
1p  ? 
ip1  ? 
np1  
 P2: 12p  
2
2p  
3
2p  
4
2p  ? 
ip2  ? 
np2
 
Suppose we have 
Ref: in the winter of 2010 , I visited Paris 
Hyp: I visited Paris in 2010 ?s winter 
Then we obtain 
P1: 1 2 3 4 5 6  (the 2nd word ?the?, 4th 
word ?of? and 6th word ?,? in the reference 
are not aligned to any word in the 
hypothesis. Thus, their positions are not in 
P1, so the positions of the matching words 
?in winter 2010 I visited Paris? are nor-
malized to 1 2 3 4 5 6) 
P2: 4 5 6 1 3 2 (the word ??s? was 
unaligned).  
The ordering metric v is computed from two 
distance measures. The first is absolute 
permutation distance: 
?
=
?=
n
i
ii ppPPDIST
1
21211 ||),(               (4) 
Let       
2/)1(
),(1 2111 +?= nn
PPDIST
?                     (5)                  
v1 ranges from 0 to 1; a larger value means more 
similarity between the two permutations. This 
metric is similar to Spearman?s ? (Spearman, 
1904). However, we have found that ? punishes 
long-distance reordering too heavily. For instance, 
1?
 
is more tolerant than ? of the movement of 
?recently? in this example:  
Ref: Recently , I visited Paris 
Hyp: I visited Paris recently  
P1: 1 2 3 4 
P2: 2 3 4 1 
Its 2.0-1 1)4(16
)9116(1
?==
?
+++? ; however, its  
4.0-1 1)/24(4 3111 == + +++1v . 
Inspired by HMM word alignment (Vogel et al, 
1996), our second distance measure is based on 
jump width. This punishes only once a sequence of 
words that moves a long distance with the internal 
word order conserved, rather than on every word. 
In the following, only two groups of words have 
moved, so the jump width punishment is light: 
Ref: In the winter of 2010, I visited Paris 
Hyp: I visited Paris in the winter of 2010  
The second distance measure is 
?
=
??
???=
n
i
iiii ppppPPDIST
1
1
22
1
11212 |)()(|),(   (6) 
where we set 001 =p  and 0
0
2 =p . Let 
1
),(1 2 2122
?
?=
n
PPDIST
v                     (7) 
As with v1, v2 is also from 0 to 1, and larger values 
indicate more similar permutations. The ordering 
measure vs is the harmonic mean of v1 and v2 (Chen 
et al, 2012):  
)11(2 21 /v/v/vs +=
 
.                     (8) 
 In (Chen et al, 2012) we found this to be slightly 
more effective than the geometric mean. vs in (8) is 
computed at segment level. We compute document 
level ordering vD with a weighted arithmetic mean:  
60
?
?
=
=
?
= l
s s
l
s ss
D
Rlen
Rlenv
v
1
1
)(
)(
                    (9) 
where l is the number of segments of the 
document, and len(R) is the length of the reference 
after text preprocessing. vs is the segment-level 
ordering penalty. 
Recall that the penalty part of AMBER is the 
weighted product of several component penalties. 
In the original version of AMBER, there were 10 
component penalties. In the new version, v is in-
corporated as an additional, 11th weighted penalty 
in (3). Thus, the new version of AMBER incorpo-
rates three reordering penalties: Spearman?s 
correlation, Kendall?s correlation, and v. Note that 
v is also incorporated in a tuning metric we recent-
ly devised (Chen et al, 2012).   
3.2   Automatic tuning 
In (Chen and Kuhn, 2011), we manually set the 17 
free parameters of AMBER (see section 3.2 of that 
paper). In the experiments reported below, we 
tuned the 18 free parameters ? the original 17 plus 
the ordering metric v described in the previous sec-
tion - automatically, using the downhill simplex 
method of (Nelder and Mead, 1965) as described 
in (Press et al, 2002). This is a multidimensional 
optimization technique inspired by geometrical 
considerations that has shown good performance in 
a variety of applications.  
4 Experiments 
The experiments are carried out on WMT metric 
task data: specifically, the WMT 2008, WMT 
2009, WMT 2010, WMT 2011 all-to-English, and 
English-to-all submissions. The languages ?all? 
(?xx? in Table 1) include French, Spanish, German 
and Czech. Table 1 summarizes the statistics for 
these data sets. 
 
Set Year Lang. #system #sent-pair 
Test1 2008 xx-En 43 7,804 
Test2 2009 xx-En 45 15,087 
Test3 2009 en-Ex 40 14,563 
Test4 2010 xx-En 53 15,964 
Test5 2010 en-xx 32 18,508 
Test6 2011 xx-En 78 16,120 
Test7 2011 en-xx 94 23,209 
 
Table 1: Statistics of the WMT dev and test sets. 
 
We used 2008 and 2011 data as dev sets, 2009 
and 2010 data as test sets. Spearman?s rank 
correlation coefficient ? was employed to measure 
correlation of the metric with system-level human 
judgments of translation. The human judgment 
score was based on the ?Rank? only, i.e., how 
often the translations of the system were rated as 
better than those from other systems (Callison-
Burch et al, 2008). Thus, BLEU and the new ver-
sion of AMBER were evaluated on how well their 
rankings correlated with the human ones. For the 
segment level, we followed (Callison-Burch et al, 
2010) in using Kendall?s rank correlation 
coefficient ?. 
In what follows, ?AMBER1? will denote a vari-
ant of AMBER as described in (Chen and Kuhn, 
2011). Specifically, it is the variant AMBER(1,4) ? 
that is, the variant in which results are averaged 
over two runs with the following preprocessing: 
1. A run with tokenization and lower-casing 
2. A run in which tokenization and lower-
casing are followed by the word splitting. 
Each word with more than 4 letters is seg-
mented into two sub-words, with one being 
the first 4 letters and the other the last 2 let-
ters. If the word has 5 letters, the 4th letter 
appears twice: e.g., ?gangs? becomes 
?gang? + ?gs?. If the word has more than 6 
letters, the middle part is thrown away.  
The second run above requires some explana-
tion. Recall that in AMBER, we wish to avoid use 
of external resources such as stemmers and mor-
phological analyzers, and we aim at maximal lan-
guage independence. Here, we are doing a kind of 
?poor man?s morphological analysis?. The first 
four letters of a word are an approximation of its 
stem, and the last two letters typically carry at least 
some information about number, gender, case, etc. 
Some information is lost, but on the other hand, 
when we use the metric for a new language (or at 
least, a new Indo-European language) we know 
that it will extract at least some of the information 
hidden inside morphologically complex words. 
The results shown in Tables 2-4 compare the 
correlation of variants of AMBER with human 
judgment; Table 5 compares the best version of 
AMBER (AMBER2) with BLEU. For instance, to 
calculate segment-level correlations using 
61
Kendall?s ?, we carried out 33,071 paired compari-
sons for out-of-English and 31,051 paired compar-
isons for into-English. The resulting ? was 
calculated per system, then averaged for each con-
dition (out-of-English and into-English) to obtain 
one out-of-English value and one into-English val-
ue. 
First, we compared the performance of 
AMBER1 with a version of AMBER1 that in-
cludes the new reordering penalty v, at the system 
and segment levels. The results are shown in Table 
2. The greatest impact of v is on ?out of English? at 
the segment level, but none of the results are par-
ticularly impressive.  
 
 AMBER1 +v Change 
Into-En 
System 
0.860 0.862 0.002 
(+0.2%) 
Into-En 
Segment 
0.178 0.180 0.002 
 (+1.1%) 
Out-of-En 
System 
0.637 0.637 0 
 (0%) 
Out-of-En 
Segment 
0.167 0.170 0.003 
(+1.8%) 
 
Table 2: Correlation with human judgment for 
AMBER1 vs. (AMBER1 including v). 
 
Second, we compared the performance of manu-
ally tuned AMBER1 with AMBER1 whose param-
eters were tuned by the simplex method. The 
tuning was run four times on the dev set, once for 
each possible combination of into/out-of English 
and system/segment level. Table 3 shows the re-
sults on the test set. This change had a greater im-
pact, especially on the segment level. 
 
 AMBER1 +Simplex Change 
Into-En 
 System 
0.860 0.862 0.002 
(+0.2%) 
Into-En 
Segment 
0.178 0.184 0.006  
(+3.4%) 
Out-of-En 
 System 
0.637 0.637 0 
 (0%) 
Out-of-En  
Segment 
0.167 0.182 0.015 
(+9.0%) 
 
Table 3: Correlation with human judgment for 
AMBER1 vs. simplex-tuned AMBER1. 
 
Then, we compared the performance of 
AMBER1 with AMBER1 that contains v and that 
has been tuned by the simplex method. We will 
denote the new version of AMBER containing 
both changes ?AMBER2?. It will be seen from 
Table 4 that AMBER2 is a major improvement 
over AMBER1 at the segment level. In the case of 
?into English? at the segment level, the impact of 
the two changes seems to have been synergistic: 
adding together the percentage improvements due 
to v and simplex from Tables 2 and 3, one would 
have expected an improvement of 4.5% for both 
changes together, but the actual improvement was 
6.2%. Furthermore, there was no improvement at 
the system level for ?out of English? when either 
change was tried separately, but there was a small 
improvement when the two changes were com-
bined.  
 
 AMBER1 AMBER2 Change 
Into-En 
System 
0.860 0.870 0.010 
(+1.2%) 
Into-En 
Segment 
0.178 0.189 0.011 
(+6.2%) 
Out-of-En 
System 
0.637 0.642 0.005 
(+0.8%) 
Out-of-En 
Segment 
0.167 0.184 0.017 
(+10.2%) 
 
Table 4: Correlation with human judgment for 
AMBER1 vs. AMBER2. 
 
Of course, the most important question is: does 
the new version of AMBER (AMBER2) perform 
better than BLEU? Table 5 answers this question 
(the version of BLEU used here was smoothed 
BLEU (mteval-v13a)). There is a clear advantage 
for AMBER2 over BLEU at both the system and 
segment levels, for both ?into English? and ?out of 
English?.  
 
 BLEU AMBER2 Change 
Into-En 
System 
0.773 0.870 0.097 
(+12.5%) 
Into-En 
Segment 
0.154 0.189 0.035 
(+22.7%) 
Out-of-En 
System 
0.574 0.642 0.068 
(+11.8%) 
Out-of-En 
Segment 
0.149 0.184 0.035 
(+23.5%) 
 
Table 5: Correlation with human judgment for 
 BLEU vs. AMBER2. 
 
62
5 Conclusion 
We have made two changes to AMBER, a metric 
described in (Chen and Kuhn, 2011). In our exper-
iments, the new version of AMBER was shown to 
be an improvement on the original version in terms 
of correlation with human judgment. Furthermore, 
it outperformed BLEU by about 12% at the system 
level and about 23% at the segment level.  
A good evaluation metric is not necessarily a 
good tuning metric, and vice versa. In parallel with 
our work on AMBER for evaluation, we have also 
been exploring a machine translation tuning metric 
called PORT (Chen et al, 2012). AMBER and 
PORT differ in many details, but they share the 
same underlying philosophy: to exploit surface 
similarities between hypothesis and references 
even more thoroughly than BLEU does, rather than 
to invoke external resources with richer linguistic 
knowledge. So far, the results for PORT have been 
just as encouraging as the ones for AMBER re-
ported here.  
Reference 
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, M. 
Przybocki and O. Zaidan. 2010. Findings of the 2010 
Joint Workshop on Statistical Machine Translation 
and Metrics for Machine Translation. In Proceedings 
of WMT. 
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz and 
J. Schroeder. 2008. Further Meta-Evaluation of Ma-
chine Translation. In Proceedings of WMT. 
B. Chen, R. Kuhn, and S. Larkin. 2012. PORT:  a Preci-
sion-Order-Recall MT Evaluation Metric for Tuning. 
Accepted for publication in Proceedings of ACL. 
B. Chen and R. Kuhn. 2011. AMBER: a Modified 
BLEU, Enhanced Ranking Metric. In Proceedings of 
the Sixth Workshop on Statistical Machine Transla-
tion, Edinburgh, Scotland.  
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada. 
2010. Automatic Evaluation of Translation Quality 
for Distant Language Pairs. In Proceedings of 
EMNLP.  
J. Nelder and R. Mead. 1965. A simplex method for 
function minimization. Computer Journal V. 7, pages 
308?313. 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL. 
W. Press, S. Teukolsky, W. Vetterling and B. Flannery. 
2002. Numerical Recipes in C++. Cambridge Uni-
versity Press, Cambridge, UK.  
C. Spearman. 1904. The proof and measurement of as-
sociation between two things. In American Journal of 
Psychology, V. 15, pages 72?101. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based 
word alignment in statistical translation. In Proceed-
ings of COLING. 
63
