The FrameNet Data and Software
Collin F. Baker
International Computer Science Institute
Berkeley, California, USA
collinb@icsi.berkeley.edu
Hiroaki Sato
Senshu University
Kawasaki, Japan
hiroaki@ics.senshu-u.ac.jp
Abstract
The FrameNet project has developed a
lexical knowledge base providing a unique
level of detail as to the the possible syn-
tactic realizations of the specific seman-
tic roles evoked by each predicator, for
roughly 7,000 lexical units, on the ba-
sis of annotating more than 100,000 ex-
ample sentences extracted from corpora.
An interim version of the FrameNet data
was released in October, 2002 and is be-
ing widely used. A new, more portable
version of the FrameNet software is also
being made available to researchers else-
where, including the Spanish FrameNet
project.
This demo and poster will briefly ex-
plain the principles of Frame Semantics
and demonstrate the new unified tools for
lexicon building and annotation and also
FrameSQL, a search tool for finding pat-
terns in annotated sentences. We will dis-
cuss the content and format of the data re-
leases and how the software and data can
be used by other NLP researchers.
1 Introduction
FrameNet1 (Fontenelle, 2003; Fillmore, 2002;
Baker et al, 1998) is a lexicographic research
project which aims to produce a lexicon contain-
ing very detailed information about the relation be-
1http://framenet.ICSI.berkeley.edu/ framenet
tween the semantics and the syntax of predicators,
including verbs, nouns and adjectives, for a substan-
tial subset of English.
The basic unit of analysis is the semantic frame,
defined as a type of event or state and the partici-
pants and ?props? associated with it, which we call
frame elements (FEs).2 Frames range from highly
abstract to quite specific. An example of an abstract
frame would be the Replacement frame, with FEs
such as OLD and NEW as in the sentence Pat re-
placed [Old the curtains] [New with wooden blinds].
One sense of the verb replace is associated with
the Replacement frame, thus constituting one lexical
unit (LU), the basic unit of the FrameNet lexicon.
An example of a more specific frame is Ap-
ply heat, with FEs such as COOK, FOOD, MEDIUM,
and DURATION. as in Boil [Food the rice] [Duration
for 3 minutes] [Medium in water], then drain.3 LUs
in Apply heat include char, fry, grill, and mi-
crowave, etc.
In our daily work, we define a frame and its
FEs, make lists of words that evoke the frame (its
LUs), extract example sentences containing these
LUs from corpora, and semi-automatically annotate
the parts of the sentences which are the realizations
of these FEs, including marking the phrase type (PT)
and grammatical function (GF). We can then auto-
matically create a report which constitutes a lexical
entry for this LU, detailing all the possible ways in
which these FEs can be syntactically realized. The
2In similar approaches, these have been referred to as
schemas or scenarios, with their associated roles or slots.
3In this sentence, as in most examples of boil in recipes,
the COOK is constructionally null-instantiated, because of the
imperative.
annotated sentences and lexical entries for approxi-
mately 7,000 LUs will be available on the FN web-
site and the data will be released by the end of Au-
gust in several formats.
2 Frame Semantics and FrameNet II
2.1 Frame Semantics in Theory and Practice
The development of the theory of Frame Semantics
began more than 25 years ago (Fillmore, 1976; Fill-
more, 1977), but since 1997, thanks to two NSF
grants4, we have been able to apply it in a serious
way to building a lexicon which we intend to be
both usable by human beings and machine-tractable,
so that it can serve as a lexical database for NLP,
computational lexical semantics, etc. In FrameNet
II, all the data, including the definitions of frames,
FEs, and LUs and all of the sentences and the an-
notation associated with them is stored in one rela-
tional database implemented in MySQL (Baker et
al., 2003; Fillmore et al, 2001).
The FrameNet public website contains an index
by frame and an index by LU which links to both
the lexical entry and the full annotation for each LU.
The frame-to-frame relations which are now being
entered in the database will be visible on the website
soon.
2.2 FrameNet II Data Release 1.0
The HTML version of the data consists of all the
files on the web site, so that users can set up a local
copy and browse it with any web browser. It is fairly
compact, less than 100 Mb in all.
The plain XML version of the data consists of the
following files:
frames.xml This file contains the descriptions of all
the 450 frames and their FEs, totaling more
than 3,000. Each frame also includes informa-
tion as to frame-to-frame relations.
luNNN.xml There is one such file per LU (roughly
7500) which contain the example sentences and
annotation (if any) for each LU.
4We are grateful to the National Science Foundation for
funding the project through two grants, IRI #9618838 and
ITR/HCI #0086132. We refer to these two three-year stages
in the life of the project as FrameNet I and FrameNet II.
relations.xml A file containing information about
frame-to-frame and FE-to-FE relations and
meta-relations between them.
We intend to have a version of the XML that
includes RDF of the DAML+OIL flavor, so that
the FN frames and FEs can be related to existing
ontologies and Semantic Web-aware applications
can access FN data using a standard methodology.
Narayanan has created such a version for the FN I
data, and a new version reflecting the more complex
FN II data is under construction (Narayanan et al,
2002).
3 The FrameNet Software Suite
3.1 The FrameNet Desktop tools
The FN software used for frame definition and an-
notation has been fundamentally rewritten since the
demo at the LREC conference last summer (Fill-
more et al, 2002a). The two major changes are (1)
combining the frame editing tools and the annotation
tools into a single GUI, making the interface more
intuitive and (2) moving to a client-server model.
In the previous version, each client accessed the
database directly, which made it very difficult to
avoid collisions between users, and meant that each
client was large, containing a lot of the logic of the
application, MySQL-specific queries, etc. In the
new version, the basic modules are now the MySQL
database, an application server, and one or more
client processes. This has a number of advantages:
(1) All the database calls are made by the server,
making it much easier to avoid conflicts between
users. (2) The application server contains nearly all
the logic, meaning that the clients are ?thin? pro-
cesses, concerned mainly with the GUI. (3) The sep-
aration into client and server makes it easier to set up
remote access to the FN database. (4) The increased
overhead caused by the more complex architecture
is at least offset by the ability to cache frequently-
requested data on the server, making access much
faster.
The public FrameNet web pages contain static
versions of several reports drawn from the database,
notably, the lexical entry report, displaying all the
valences of each LU. The working environment for
the staff includes dynamic versions of these reports
and several others, all written as java applets. Par-
tially shared code makes these reports accessible
within the desktop package as well.
3.2 API, Library, and Utilities
We are currently working on defining a FN API
and writing libraries for accessing the database from
other programs. We plan to distribute a command-
line utility as a demonstration of this API.
4 FrameSQL and Kernel Dependency
Graphs
4.1 Searching with FrameSQL
Prof. Hiroaki Sato of Senshu University has written
a web-based tool which allows users to search ex-
isting FN annotations in a variety of ways. The tool
also makes conveniently available several other elec-
tronic resources such as WordNet, and other on-line
dictionaries. It is especially useful for doing conven-
tional lexicography.
4.2 Kernel Dependency Graphs
The major product of the project is the lexical
database of frame descriptions and annotated sen-
tences; although these clearly are potentially very
useful in many sorts of NLP task, FrameNet (at
least in its present phase) remains primarily lexi-
cographic. Nevertheless, as a an intermediate step
toward applications such as automatic text summa-
rization, we have recently begun studying kernel
dependency graphs (KDGs), which provide a sort
of automatic summarization of annotated sentences.
KDGs consist of
  the predicator (verb, noun, or adjective),
  the lexical heads of its dependents
  the ?marking? on the dependents (prepositions,
complementizers, etc. if any), and
  the FEs of the dependents.
To take a simple example, (1-a), which is anno-
tated for the target chained in the Attaching frame,
could be represented as the KDG in (1-b).
(1) a. [Agent Four activists] chained [Item
themselves] [Goal to an oil drilling rig
being towed to the Barents Sea] [Time in
early August].
b.
<KDG frame="Attaching" LU="chain.v">
<Agent>activists</Agent>
<Item>themselves</Item>
<Goal>to:oil\_drilling\_rig</Goal>
<Time>in:August</Time>
</KDG>
The situation can be complicated by the pres-
ence of higher control verbs and ?transparent? nouns
which bring about a mismatch between the semantic
head and the syntactic head of an FE (Fillmore et al,
2002b), as in (2), which should have the same KDG
as (1-a).
(2) [Agent Four activists] planned to chain [Item
themselves] [Goal to the bottom of an oil
drilling rig being towed to the Barents Sea]
[Time in early August].
5 Layered Annotation and Frame
Semantic Parsing
A large majority of FEs are annotated with a triplet
of labels, one for the FE name, one for the phrase
type and one for the grammatical function of the
constituent with regard to the target. But the FN
software allows more than three layers of annotation
for a single target, for situations such as when one
FE contains another (e.g. in [Agent You] ?re hurting
[Body part [Victim my] arms]).
In addition, the FN software allows us to annotate
more than one target in a sentence. A full represen-
tation of the meaning of a sentence can be built up
by composing the semantics of the frames evoked by
the major predicators.
6 Applications and Related Projects
In addition to the original lexicographic goal, a pre-
liminary version of our frame descriptions and the
set of more than 100,000 annotated sentences have
been released to more than 80 research groups in
more than 15 countries. The FN data is being used
for a variety of purposes, some of which we had
foreseen and others which we had not; these in-
clude uses as teaching materials for lexical seman-
tics classes, as a basis for developing multi-lingual
lexica, as an interlingua for machine translation, and
as training data for NLP systems that perform ques-
tion answering, information retrieval (Mohit and
Narayanan, 2003), and automatic semantic parsing
(Gildea and Jurafsky, 2002).
A number of scholars have expressed interest in
building FrameNets for other languages. Of these,
three have already begun work: In Spain, a team
from several universities, led by Prof. Carlos Subi-
rats of U A Barcelona, is building using their own
extraction software and the FrameNet desktop tools
to build a Spanish FrameNet (Subirats and Petruck,
forthcoming 2003) http://www.gemini.es/SFN. In
Saarbru?cken, Germany, work is proceeding on hand-
annotating a parsed corpus with FrameNet FE labels
(Erk et al, ). And in Japan, researchers from Keio
University and University of Tokyo are building a
Japanese FrameNet in the domains of motion and
communication, using a large newspaper corpus.
7 Contents of the Demo
We will demonstrate how the software can be used to
create a frame, create a frame element, create a lexi-
cal unit , define a set of rules for extracting example
sentences (and, optionally, marking FEs on them),
open an existing LU and annotate sentences, mark
an LU as finished, create a frame-to-frame relation,
and attach a semantic type to an FE or an LU.
We will demonstrate the reports available on the
internal web pages. We will show the complex
searches against the FrameNet data that can be run
using FrameSQL, including displaying the result-
ing sentences as KDGs. We will demonstrate how
frames can be composed to represent the meaning
of sentences using a (manual) frame semantic pars-
ing of a newspaper crime report as an example.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In ACL, ed-
itor, COLING-ACL ?98: Proceedings of the Confer-
ence, held at the University of Montre?al, pages 86?90.
Association for Computational Linguistics.
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
2003. The structure of the FrameNet database. Inter-
national Journal of Lexicography.
K. Erk, A. Kowalski, and M. Pinkal. A corpus re-
source for lexical semantics. Submitted. Available
at http://www.coli.uni-sb.de/ erk/ OnlinePapers/ Lex-
Proj.ps.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Benjamin Tsou and Olivia
Kwong, editors, Proceedings of the 15th Pacific Asia
Conference on Language, Information and Computa-
tion, Hong Kong.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002a. The FrameNet database and software tools. In
Proceedings of the Third International Conference on
Languag Resources and Evaluation, volume IV, Las
Palmas. LREC.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002b. Seeing arguments through transparent struc-
tures. In Proceedings of the Third International Con-
ference on Languag Resources and Evaluation, vol-
ume III, Las Palmas. LREC.
Charles J. Fillmore. 1976. Frame semantics and the na-
ture of language. In Annals of the New York Academy
of Sciences: Conference on the Origin and Develop-
ment of Language and Speech, volume 280, pages 20?
32.
Charles J. Fillmore. 1977. Scenes-and-frames seman-
tics. In Antonio Zampolli, editor, Linguistic Struc-
tures Processing, number 59 in Fundamental Studies
in Computer Science. North Holland Publishing.
Charles J. Fillmore. 2002. Linking sense to syntax in
FrameNet. In Proceedings of 19th International Con-
ference on Computational Linguistics, Taipei. COL-
ING.
Thierry Fontenelle, editor. 2003. International Journal
of Lexicography. Oxford University Press. (Special
issue devoted to FrameNet.).
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Behrang Mohit and Srinivas Narayanan. 2003. Seman-
tic extraction with wide-coverage lexical resources. In
Proceedings of the Human Language Technology Con-
ference (HLT-NAACL), Edmonton, Canada.
Srinivas Narayanan, Charles J. Fillmore, Collin F. Baker,
and Miriam R.L. Petruck. 2002. FrameNet meets the
semantic web: A DAML+OIL frame representation.
In Proceedings of the 18th National Conference on Ar-
tificial Intelligence, Edmonotn, Alberta. AAAI.
Carlos Subirats and Miriam R. L. Petruck. forthcoming
2003. The Spanish FrameNet project. In Proceedings
of the Seventeenth International Congress of Linguists,
Prague.
Putting FrameNet Data into the ISO Linguistic Annotation Framework
Srinivas Narayanan Miriam R. L. Petruck Collin F. Baker Charles J. Fillmore
 
snarayan, miriamp, collinb, fillmore@icsi.berkeley.edu
International Computer Science Institute
1947 Center St., Berkeley, California
1 Abstract
This paper describes FrameNet (Lowe et al, 1997; Baker
et al, 1998; Fillmore et al, 2002), an online lexical re-
source for English based on the principles of frame se-
mantics (Fillmore, 1977a; Fillmore, 1982; Fillmore and
Atkins, 1992), and considers the FrameNet database in
reference to the proposed ISO model for linguistic an-
notation of language resources (ISO TC37 SC4 )(ISO,
2002; Ide and Romary, 2001b). We provide a data cat-
egory specification for frame semantics and FrameNet
annotations in an RDF-based language. More specifi-
cally, we provide a DAML+OIL markup for lexical units,
defined as a relation between a lemma and a semantic
frame, and frame-to-frame relations, namely Inheritance
and Subframes. The paper includes simple examples of
FrameNet annotated sentences in an XML/RDF format
that references the project-specific data category specifi-
cation.
2 Frame Semantics and the FrameNet
Project
FrameNet?s goal is to provide, for a significant portion
of the vocabulary of contemporary English, a body of
semantically and syntactically annotated sentences from
which reliable information can be reported on the va-
lences or combinatorial possibilities of each item in-
cluded.
A semantic frame is a script-like structure of infer-
ences, which are linked to the meanings of linguistic
units (lexical items). Each frame identifies a set of
frame elements (FEs), which are frame-specific seman-
tic roles (participants, props, phases of a state of affairs).
Our description of each lexical item identifies the frames
which underlie a given meaning and the ways in which
the FEs are realized in structures headed by the word.
The FrameNet database documents the range of semantic
and syntactic combinatory possibilities (valences) of each
word in each of its senses, through manual annotation of
example sentences and automatic summarization of the
resulting annotations. FrameNet I focused on governors,
meaning that for the most part, annotation was done in re-
spect to verbs; in FrameNet II, we have been annotating
in respect to governed words as well.1 This paper will
explain the theory behind FrameNet, briefly discuss the
annotation process, and then describe how the FrameNet
data can be represented in RDF, using DAML+OIL, so
that researchers on the semantic web can use the data.
2.0.1 Frame Semantic Background
In Frame Semantics (Fillmore, 1976; Fillmore, 1977b;
Fillmore and Atkins, 1992; Petruck, 1996), a linguistic
unit, in our case, a word (in just one of its senses), evokes
a particular frame. An ?evoked? frame is the structure of
knowledge required for the understanding of a given lexi-
cal or phrasal item. The frames in question can be simple
? small static scenes or states of affairs, simple patterns
of contrast, relations between entities and the roles they
serve ? or possibly quite complex event types that pro-
vide the background for words that profile one or more of
their phases or participants.
For example, the word bartender evokes a scene of ser-
vice in a setting where alcoholic beverages are consumed,
and profiles the person whose role is to prepare and serve
these beverages. In a sentence like The bartender asked
for my ID, it is the individual who occupies that role that
we understand as making the request, and the request for
identification is understood against the set of assumptions
and practices of that frame.
1The National Science Foundation has provided funding for
FrameNet through two grants, IRI #9618838 ?Tools for Lex-
icon Building? (1997-2000, PI Charles Fillmore, Co-PI Dan
Jurafsky) and ITS/HCI #0086132 ?FrameNet++: An On-Line
Lexical Semantic Resource and its Application to Speech and
Language Technology? (PI Charles Fillmore, Co-PIs Dan Ju-
rafsky, Srini Narayanan, and Mark Gawron). We refer to the
two phases of the project as FrameNet I and FrameNet II.
2.0.2 Replacement: An Example Frame
A schematic description of the REPLACEMENT frame
will include an AGENT effecting a change in the relation-
ship between a PLACE (which can be a role, a function,
a location, a job, a status, etc.) and a THEME. For ex-
ample, in the sentence Sal replaced his cap on his bald
head, Sal fills the role of AGENT, his cap instantiates
the FE THEME, and on his bald head is the PLACE. The
words defined in terms of this frame include exchange.v,
interchange.v, replace.v, replacement.n, substitute.v, sub-
stitution.n, succeed.v, supplant.v, swap.v, switch.v, and
trade.v.
The REPLACEMENT frame involves states of affairs
and transitions between them such that other situations
are covered: an ?old theme?, which we refer to as OLD,
starts out at the PLACE and ends up not at the PLACE,
while a ?new theme?, which we call NEW, starts out not
at the PLACE and ends up at the PLACE (as in Factory
owners replaced workers by machines).
Syntactically, the role of AGENT can be expressed by
a simple NP (e.g. Margot switched her gaze to the floor,
a conjoined NP (e.g. Margot and her admirer exchanged
glances), or two separate constituents, an NP and a PP
(e.g. Margot exchanged glances with her admirer). Sim-
ilarly, PLACE may be expressed as one PP or two. Com-
pare Ginny switched the phone between hands and Ginny
switched the phone from one hand to the other. And, if
OLD and NEW are of the same type, they can be expressed
as a single FE (e.g. The photographer switched lenses).
2.1 The FrameNet Process
Using attested instances of contemporary English,
FrameNet documents the manner in which frame ele-
ments (for given words in given meanings) are grammat-
ically instantiated in English sentences and organizes and
exhibits the results of such findings in a systematic way.
For example, in causative uses of the words, an expres-
sion about replacing NP with NP takes the direct object
as the OLD and the oblique object as the NEW (e.g. Nancy
replaced her desktop computer with a laptop), whereas
substituting NP for NP does it the other way around (e.g.
Nancy substituted a laptop for her desktop computer).
A commitment to basing such generalizations on attes-
tations from a large corpus, however, has revealed that in
both UK and US English, the verb substitute also partic-
ipates in the valence pattern found with replace, i.e. we
find examples of substituting the OLD with the NEW (e.g.
Nancy subsitituted a laptop with her desktop computer).
In their daily work, FrameNet staff members record the
variety of combinatorial patterns found in the corpus for
each word in the FrameNet lexicon, present the results
as the valences of the words, create software capable of
deriving from the annotations as much other information
as possible about the words, and add manually only that
information which cannot ? or cannot easily ? be derived
automatically from the corpus or from the set of anno-
tated examples.
2.2 Frame-to-Frame Relations
The FrameNet database records information about sev-
eral different kinds of semantic relations, consisting
mostly of frame-to-frame relations which indicate seman-
tic relationships between collections of concepts. The
two that we consider here are Inheritance and Sub-
frames.
2.2.1 Inheritance
Frame Inheritance is a relationship by which a sin-
gle frame can be seen as an elaboration of one or more
other parent frames, with bindings between the inherited
semantic roles. In such cases, all of the frame elements,
subframes, and semantic types of the parent have equal
or more specific correspondents in the child frame. Con-
sider for example, the CHANGE OF LEADERSHIP frame,
which characterizes the appointment of a new leader or
removal from office of an old one, and whose FEs in-
clude: SELECTOR, the being or entity that brings about
the change in leadership (in the case of a democratic pro-
cess, the electorate); OLD LEADER, the person removed
from office; OLD ORDER, the political order that existed
before the change; NEW LEADER, the person appointed
to office; and ROLE, the position occupied by the new or
old leader. Some of the words that belong to this frame
describe the successful removal from office of a leader
(e.g. overthrow, oust, depose), others only the attempt
(e.g. uprising, rebellion). This frame inherits from the
more abstract REPLACEMENT frame described above,
with the following FEs further specified in the child: OLD
and NEW are narrowed to humans beings or political en-
tities, i.e. OLD LEADER and NEW LEADER, respectively;
and PLACE is an (abstract) position of political power, i.e.
ROLE.
2.2.2 Subframes
The other type of relation between frames which is cur-
rently represented in the FN database is between a com-
plex frame and several simpler frames which constitute
it. We call this relationship Subframes. In such cases,
frame elements of the complex frame may be identified
(mapped) to the frame elements of the subparts, although
not all frame elements of one need have any relation to
the other. Also, the ordering and other temporal rela-
tionships of the subframes can be specified using binary
precedence relations. To illustrate, consider the complex
CRIMINAL PROCESS frame, defined as follows: A Sus-
pect is arrested by an AUTHORITY on certain CHARGES,
then is arraigned as a DEFENDANT. If at any time the
DEFENDANT pleads guilty, then the DEFENDANT is sen-
tenced, otherwise the DEFENDANT first goes to trial. If
the VERDICT after the trial is guilty, then the DEFEN-
DANT is sentenced. In the end, the DEFENDANT is ei-
ther released or is given a SENTENCE by a JUDGE at
the sentencing. For each step in the process, there is a
separate frame in the database, including ARREST, AR-
RAIGNMENT, TRIAL, SENTENCING, and so on. Each of
these frames is related to the CRIMINAL PROCESS frame
via the SubFrame relation in the frame editor. Moreover,
subframes (of the same complex frame) are related to
each other through their ordering.
We have recognized the need to deal with other types
of relations among frames, and, so far, have identified
two, SeeAlso, and Using. Currently, many Using rela-
tions are indicated in the FrameNet database.
2.3 The FrameNet Product
The FrameNet database contains descriptions of more
than 7,000 lexical units based on more than 130,000 an-
notated sentences. This information is available for a
wide range of natural language processing applications,
including question answering, machine translation, and
information extraction.
The FN database can be seen both as a dictionary and
a thesaurus. As a dictionary, each lexical unit (LU)
(lemma in a given sense) is provided with (1) the name of
its frame, (2) a definition, (3) a valence description which
summarizes the attested combinatorial possibilities, and
(4) access to annotated examples. The FN database can
also be seen as a thesaurus, associating groups of lexical
units in frames and associating frames with each other
(see below). The FrameNet database differs from existing
lexical resources in the specificity of the frames and se-
mantic roles it defines, the information it provides about
relations between frames, and the degree of detail pro-
vided on the possible syntactic realizations of semantic
roles for each LU.
While Ide, et al, (2002)(Ide et al, 2002) offers a rep-
resentation scheme for dictionaries and other lexical data,
the kind of information in the FrameNet database is not
expressed in the same level of depth in any existing print
dictionary or computational lexical resource. For in-
stance, while WordNet describes semantic relations be-
tween words, it does not recognize conceptual schemas,
i.e. frames, that mediate in these relations, and therefore
does not have the means to link arguments of predicating
words with the semantic roles they express. FrameNet
also differs from WordNet in showing semantic relations
across parts of speech, and in providing contextual infor-
mation enriched with semantics (beyond the ?Someone
s something? format of WordNet argument-structure
representations). Thus, the complex relational structure
inherent in the FrameNet frame element and frame-to-
frame relations exercises and potentially extends the ISO
TC37 SC4 standard (ISO, 2002). The rest of this paper
describes our encoding of the FrameNet database in an
RDF-based environment.
3 A Data Category Specification for Frame
Semantics in RDF
The World Wide Web (WWW) contains a large amount
of information which is expanding at a rapid rate. Most
of that information is currently being represented using
the Hypertext Markup Language (HTML), which is de-
signed to allow web developers to display information in
a way that is accessible to humans for viewing via web
browsers. While HTML allows us to visualize the infor-
mation on the web, it doesn?t provide much capability to
describe the information in ways that facilitate the use
of software programs to find or interpret it. The World
Wide Web Consortium (W3C) has developed the Exten-
sible Markup Language (XML) which allows informa-
tion to be more accurately described using tags. As an
example, the word crawl on a web site might represent
an offline search process (as in web crawling) or an ex-
position of a type of animate motion. The use of XML to
provide metadata markup, such as for crawl, makes the
meaning of the word unambiguous. However, XML has
a limited capability to describe the relationships (schemas
or ontologies) with respect to objects. The use of ontolo-
gies provides a very powerful way to describe objects and
their relationships to other objects. The DAML language
was developed as an extension to XML and the Resource
Description Framework (RDF). The latest release of the
language (DAML+OIL) (http://www.daml.org) provides
a rich set of constructs with which to create ontologies
and to markup information so that it is machine readable
and understandable.
Framenet-1 has been translated into DAML+OIL.
We developed an automatic translator from FrameNet
to DAML+OIL which is being updated to reflect
FrameNet2 data. With periodic updates as the FrameNet
data increases, we expect it to become useful for var-
ious applications on the Semantic Web. DAML+OIL
is written in RDF (http://www.w3.org/TR/daml+oil-
walkthru/#RDF1), i.e., DAML+OIL markup is
a specific kind of RDF markup. RDF, in turn,
is written in XML, using XML Namespaces
(http://www.w3.org/TR/daml+oil-walkthru/#XMLNS),
and URIs. Thus, our framenet declaration begins with an
RDF start tag including several namespace declarations
of the form:
<?Xml version=?1.0? encoding=?ISO-8859-1??>
<!DOCTYPE uridef[
<!ENTITY rdf
"http://www.w3.org/1999/02/22-rdf-syntax-ns">
<!ENTITY rdfs
"http://www.w3.org/2000/01/rdf-schema">
<!ENTITY xsd
"http://www.w3.org/2000/10/XMLSchema">
<!ENTITY daml
"http://www.daml.org/2001/03/daml+oil">
<!ENTITY daml
"http://www.daml.org/services/daml-s/0.9/process">
]>
<rdf:RDF
xmlns:rdf = "&rdf;#"
xmlns:rdfs = "&rdfs;#"
xmlns:xsd = "&xsd;#"
xmlns:daml = "&daml;#"
xmlns:CYC = "&cyc;#"
>
So in this document, the rdf: prefix should be un-
derstood as referring to things drawn from the names-
pace called http://www.w3.org/1999/02/22-rdf-syntax-
ns#. This is a conventional RDF declaration appear-
ing verbatim at the beginning of almost every rdf doc-
ument. The second and third declarations make simi-
lar statements about the RDF Schema and XML Schema
datatype namespaces. The fourth declaration says that in
this document, elements prefixed with daml: should be
understood as referring to things drawn from the names-
pace called http://www.w3.org/2001/03/daml+oil#. This
again is a conventional DAML+OIL declaration. We
use the XML entity model to use shortcuts with re-
ferring to the URIs.2 The other DAML+OIL on-
tologies used in the FrameNet description include
the DAML-S (http://www.daml.org/services) service
ontologies, the OpenCYC DAML ontology (http://
www.cyc.com/2002/04/08/cyc.daml), and the SRI time
ontology (http:// www.ai.sri.com/ daml/ontologies/ sri-
basic/1-0/Time.daml) which is currently being re-
vised with the new DAML+OIL time ontology effort.
http://www.icsi.berkeley.edu/ snarayan/frame-2.daml has
a complete namespace and imported ontology list.
The most general object of interest is a frame. We de-
fine the FRAME class as a daml:class We then define a
bunch of bookkeeping properties on the FRAME class. An
example of the name property is shown below.
<daml:Class rdf:ID="Frame">
<rdfs:comment> The most general class </rdfs:comment>
</daml:Class>
<daml:ObjectProperty rdf:ID="Name">
<rdfs:domain rdf:resource="#Frame"/>
<rdfs:range rdf:resource="&rdf-schema;#Literal"/>
</daml:ObjectProperty>
In FrameNet, the basic relation between a word
(Lemma) and a frame is the Lexical Unit (LU). The do-
main of the Lexical Unit is a Lemma or word and its range
is a Frame. An LU is defined in DAML as a property.
<daml:ObjectProperty rdf:ID= "LU">
<rdfs:domain rdf:resource="#Lexeme"/>
<rdfs:range rdf:resource="#Frame"/>
</daml:ObjectProperty>
2Note that all URIs are globally scoped, so without this the
entire path has to be specified.
Roles are relations defined on frames ranging over the
specific type of the filler. We use daml:objectProperty
to define the roles of a frame. The domain of a role is
its frame. We leave the type of the filler unrestricted at
this level, allowing specific roles to specialize this fur-
ther. Note that we use the daml:samePropertyAs relation
to specify synonyms. The fragment below specifies that
Frame Element, Role, and FE are synonyms.
<daml:ObjectProperty rdf:ID= "role">
<rdfs:domain rdf:resource="#Frame"/>
<rdfs:range rdf:resource="&daml;#Thing"/>
</daml:ObjectProperty>
<daml:ObjectProperty rdf:ID="frameElement">
<daml:samePropertyAs rdf:resource="#role"/>
</daml:ObjectProperty>
<daml:ObjectProperty rdf:ID="FE">
<daml:samePropertyAs rdf:resource="#role"/>
</daml:ObjectProperty>
We use the various constructs daml:maxCardinality,
daml:minCardinality, daml:cardinalityQ, etc. from
DAML to specify cardinality restrictions on the fillers of
a role property. The markup fragment below shows the
specification of a single valued role.
<daml:ObjectProperty rdf:ID= "singleValuedRole">
<rdfs:domain rdf:resource="#Frame"/>
<rdfs:range>
<rdfs:subClassOf>
<daml:Restriction daml:maxCardinality="1">
<daml:onProperty rdf:resource="#Role"/>
</daml:Restriction>
</rdfs:subClassOf>
</daml:Class>
The relation between frames (such as ARREST) and
CRIMINAL PROCESS is often captured by a set of bind-
ings between frame elements (such as the arrested person
is the same individual as the person charged who is the
same individual as the defendant in a criminal process).
To capture such bindings, we introduce a special relation
called bindingRelation whose domain and range are roles
(either from the same or different frames).
<daml:ObjectProperty rdf:ID="bindingRelation">
<rdfs:domain rdf:resource="#Role"/>
<rdfs:range rdf:resource="#Role"/>
</daml:ObjectProperty>
By far the most important binding relation is the iden-
tification of roles (i.e. they refer to the same value (ob-
ject)). This can be specified through the relation identify
which is a subProperty of bindingRelation. Note that in
order to do this, we have to extend the DAML+OIL lan-
guage which does not allow properties to be defined over
other properties. We use the DAML-S ontology primitive
daml-s:sameValuesAs to specify the identify relations.
<daml:ObjectProperty rdf:ID="identify">
<rdfs:subPropertyOf rdf:resource="#bindingRelation"/>
<rdfs:domain rdf:resource="#Role"/>
<daml-s:sameValuesAs rdf:resource="#rdfs:range"/>
</daml:ObjectProperty>
In FrameNet, a frame may inherit (A ISA B) from
other frames or be composed of a set of subframes
(which are frames themselves). For instance, the frame
CRIMINAL PROCESS has subframes that correspond to
various stages (ARREST, ARRAIGNMENT, CHARGE,
etc.). Subframe relations are represented using the
daml:objectProperty.3
<daml:ObjectProperty rdf:ID="subFrameOf">
<rdfs:domain rdf:resource="#Frame"/>
<rdfs:range rdf:resource="#Frame"/>
</daml:ObjectProperty>
A central relation between subframes is one of tem-
poral ordering. We use precedes (in the sense of imme-
diately precedes)) to encode this relation between sub-
frames.
<daml:ObjectProperty rdf:ID="precedes">
<rdfs:domain rdf:resource="#subFrame"/>
<rdfs:range rdf:resource="#subFrame"/>
</daml:ObjectProperty>
We can define a property temporalOrdering that is the
transitive version of precedes.
daml:TransitiveProperty rdf:ID="TemporalOrdering">
<rdfs:label>TemporalOrdering</rdfs:label>
</daml:TransitiveProperty>
Note that the temporalOrdering property only says it is
transitive, not that it is a transitive version of precedes.
DAML+OIL does not currently allow us to express this
relation. (see http://www.daml.org/2001/03/daml+oil-
walkthru#properties).
Frame Elements may also inherit from each other. We
use the rdfs:subPropertyOf to specify this dependences.
For example, the following markup in DAML+OIL spec-
ifies that the role (Frame Element) MOTHER inherits from
the role (Frame Element) PARENT. Note we can add fur-
ther restrictions to the new role. For instance, we may
want to restrict the filler of the MOTHER to be female (as
opposed to animal for PARENT).
<daml:ObjectProperty rdf:ID="mother">
<rdfs:subPropertyOf rdf:resource="#parent"/>
<rdfs:range rdf:resource="#Female"/>
</daml:ObjectProperty>
With these basic frame primitives defined, we are ready
to look at an example using the Criminal Process frames.
3.1 An Example: The Criminal Process Frame
The basic frame is the CRIMINAL PROCESS Frame. It is
a type of background frame. CP is used as a shorthand
for this frame.
3The subFrameOf relation has a direct translation to a richer
semantic representation that is able to model and reason about
complex processes (such as buying, selling, reserving tickets)
and services on the web. While the details of the representation
are outside the scope of the this paper, the interested reader can
look at (Narayanan and McIlraith, 2002) for an exposition of
the markup language and its operational semantics.
<daml:Class rdf:ID="CriminalProcess">
<rdfs:subClassOf rdf:resource="#Frame"/>
</daml:Class>
<daml:Class rdf:ID="CP">
<daml:sameClassAs rdf:resource="#CriminalProcess"/>
</daml:Class>
The CRIMINALPROCESS frame has a set of associated
roles. These roles include that of COURT, DEFENDANT,
PROSECUTION, DEFENSE, JURY, and CHARGES. Each
of these roles may have a filler with a specific seman-
tic type restriction. FrameNet does not specify the world
knowledge and ontology required to reason about Frame
Element filler types. We believe that one of the possible
advantages in encoding FrameNet data in DAML+OIL is
that as and when ontologies become available on the web
(uch as OpenCYC), we can link to them for this purpose.
In the example fragment below we use the CYC Court-
Judicial collection to specify the type of the COURT and
the CYC Lawyer definition to specify the type restric-
tion on the frame element DEFENSE. For illustrative pur-
poses, the DAML+OIL markup below shows the use of
a different ontology (from CYC) to restrict the defendant
to be of type PERSON as defined in the example ontol-
ogy. This restriction uses the DAML+OIL example from
http://www.daml.org/2001/03/daml+oil-ex)
<daml:ObjectProperty rdf:ID="court">
<rdfs:subPropertyOf rdf:resource="#FE"/>
<rdfs:domain rdf:resource="#CriminalProcess"/>
<rdfs:range rdf:resource="&CYC;#Court-Judicial"/>
</daml:ObjectProperty>
<daml:ObjectProperty rdf:ID="defense">
<rdfs:subPropertyOf rdf:resource="#FE"/>
<rdfs:domain rdf:resource="#CriminalProcess"/>
<rdfs:range rdf:resource="&CYC;#Lawyer"/>
</daml:ObjectProperty>
<daml:ObjectProperty rdf:ID="defendant">
<rdfs:subPropertyOf rdf:resource="#FE"/>
<rdfs:domain rdf:resource="#CriminalProcess"/>
<rdfs:range rdf:resource="&daml-ex;Person"/>
</daml:ObjectProperty>
The set of binding relations involves a set of role
identification statements that specify that a role of a
frame (subframe) has the same value (bound to the
same object) as the role of a subframe (frame). We
could specify these constraints either a) as anonymous
subclass restrictions on the criminal process class (see
http://www.daml.org/2001/03/daml+oil-ex for examples)
or b) we could name each individual constraint (and
thus obtain a handle onto that property). We chose the
later method in our DAML+OIL encoding of FrameNet
to allow users/programs to query any specific con-
straint (or modify it). Note also that the use of the
dotting notation (A.b) to specify paths through sim-
ple and complex frames and is not fully supported
in DAML+OIL (see http://www.daml.org/services/daml-
s/2001/10/rationale.html and also (Narayanan and McIl-
raith, 2002) for more info).
<daml:ObjectProperty rdf:ID="prosecutionConstraint">
<rdfs:subPropertyOf rdf:resource="#identify"/>
<rdfs:domain rdf:resource="#CP.prosecution"/>
<rdfs:range rdf:resource="#Trial.prosecution"/>
</daml:ObjectProperty>
<daml:ObjectProperty rdf:ID="defendantConstraint">
<rdfs:subPropertyOf rdf:resource="#identify"/>
<rdfs:domain rdf:resource="#CP.defendant"/>
<rdfs:range rdf:resource="#Arrest.suspect"/>
</daml:ObjectProperty>
Subframes of the CRIMINALPROCESS frame are de-
fined by their type (LexicalFrame or a Background-
Frame). For example, ARREST and ARRAIGNMENT
are Lexical Frames while TRIAL is a BackgroundFrame
(all are subframes of CRIMINALPROCESS. We sub-
type the subFrameOf property to specify the individ-
ual subframe relations (shown below for the relation sub-
frameOf(Criminal Process, Arraignment)).
<daml:Class rdf:ID="Arrest">
<rdfs:comment> A subframe </rdfs:comment>
<rdfs:subClassOf rdf:resource="#LexicalFrame"/>
</daml:Class>
<daml:Class rdf:ID="Arraignment">
<rdfs:comment> A subframe </rdfs:comment>
<rdfs:subClassOf rdf:resource="#LexicalFrame"/>
</daml:Class>
<daml:Class rdf:ID="Trial">
<rdfs:comment> A subframe </rdfs:comment>
<rdfs:subClassOf rdf:resource="#BackgroundFrame"/>
</daml:Class>
<daml:ObjectProperty rdf:ID="arraignSubFrame">
<rdfs:subPropertyOf rdf:resource="#subFrameOf"/>
<rdfs:domain rdf:resource="#CP"/>
<rdfs:range rdf:resource="#Arraignment"/>
</daml:ObjectProperty>
To specify the the relation precedes(Arrest, Arraign-
ment) we restrict the property precedes within (the do-
main of) the ARREST frame to have as one of its range
values the frame (class) ARRAIGNMENT. This is done
using the property restriction feature with DAML+OIL
as follows.
<daml:Class rdf:about="#Arrest">
<rdfs:subClassOf>
<daml:Restriction>
<daml:onProperty rdf:resource="#precedes"/>
<daml:hasClass rdf:resource="#Arraignment"/>
</daml:Restriction>
</rdfs:subClassOf>
</daml:Class>
With this markup of the ontology, we can create anno-
tation instances for examples with targets that belong to
the CRIMINALPROCESS (or its associated) frames.
At the current stage, we have converted all of
FrameNet 1 data (annotations and frame descriptions)
to DAML+OIL. The translator has also been updated to
handle the more complex semantic relations (both frame
and frame element based) in FrameNet 2. We plan to
release both the XML and the RDF-based DAML+OIL
versions of all FrameNet 2 releases.
4 Examples of Annotated Sentences
4.1 Basic Annotation of Verb Arguments and
Complements as Triplets
Consider the following sentence, which is annotated for
the target nab, a verb in the ARREST frame; the frame
elements represented are the arresting AUTHORITIES, the
SUSPECT and the TIME when the event took place:
[ Authorities Police] nabbed [ Suspect the
man], who was out on licence from prison,
[ Time when he returned home].
The phrase who was out on licence from prison pro-
vides additional information about the SUSPECT, but it is
not syntactically an argument or complement of the tar-
get verb nab, nor semantically an element of the ARREST
frame, so it is not annotated.
How do we intend to represent this in XML conform-
ing to the proposed standards? The header of the file will
refer to the FrameNet Data Category specification dis-
cussed in the last section, but hereafter we will omit the
domain name space specifications and use a more human-
readable style of XML. The conversion to the full ISO
style should be straightforward.
1 <?xml version="1.0" encoding="UTF-8"?>
2 [DOCTYPE definitions like those shown in
the preceding section go here ]
3 <lexunit-annotation name="nab" frame="Arrest" pos="V">
4 <definition>COD: catch (someone) doing something
wrong. </definition>
5 <subcorpus name="V-001-all">
The entity <lexunit-annotation>, which com-
prises the rest of the file includes attributes giving the
name of the lexical unit (nab), the name of the frame
(ARREST), and the part of speech of the lemma (verb).
The first included element is a definition of the lemma
within the frame, seen on line 4.
The entities contained within the lexunit-annotation
are called subcorpora; each represents a particular syn-
tactic pattern, combination of collocates, etc. In the case
of nab, there are so few instances of the word that we
have lumped them all into one subcorpus as indicated by
the subcorpus name ?all? on line 5. It might seem logi-
cal that the entities within the subcorpus should be sen-
tences, but in fact, we recognize the possibility that one
sentence might be annotated several times, for several tar-
gets. There might even be several instances of the same
target lemma in the same sentence in the same frame
(e.g. The FBI nabbed Jones in NYC, while the Moun-
ties nabbed Smith in Toronto), each with its own set of
FEs. Therefore, the next smaller entity is the annotation
set (line 6).
The annotation set4, shown below, consists of the
<sentence>, which contains only the <text> of the
sentence, and a set of layers, each consisting of a set of
labels. Each label has attributes start and end, giving the
stating and ending position in the text to which it is ap-
plied. This sentence is typical of the basic FrameNet an-
notation style, in that there are three main layers, one for
frame elements (?FE?, line 8), one for the phrase type
(PT) of each FE (line 22), and one for the grammatical
function (GF) of each FE (line 15). In each case, there
are three coextensive labels; thus the word Police, in text
positions 0-5 expresses the FE AUTHORITIES (line 10),
has the phrase type ?NP? (line 24) and is the subject of the
verb nab, which we refer to as external argument ?Ext?
(line 17). The other two frame elements are shown by
similar triplets, SUSPECT-NP-Obj and TIME-Swh-Comp,
the latter meaning a complement of the verb consisting of
a clause (S-node) introduced by a WH-relative.
6 <annotationSet status="MANUAL">
7 <layers>
8 <layer name="FE">
9 <labels>
10 <label name="Authorities" start="0"
end="5" />
11 <label name="Suspect" start="14" end="20" />
12 <label name="Time" start="61" end="81" />
13 </labels>
14 </layer>
15 <layer name="GF">
16 <labels>
17 <label name="Ext" start="0" end="5" />
18 <label name="Obj" start="14" end="20" />
19 <label name="Comp" start="61" end="81" />
20 </labels>
21 </layer>
22 <layer name="PT">
23 <labels>
24 <label name="NP" start="0" end="5" />
25 <label name="NP" start="14" end="20" />
26 <label name="Swh" start="61" end="81" />
27 </labels>
28 </layer>
29 <layer name="Sent" />
30 <layer name="Other" />
31 <layer name="Target">
32 <labels>
33 <label name="Target" start="7" end="12" />
34 </labels>
35 </layer>
36 <layer name="Verb" />
37 </layers>
38 <sentence aPos="34400709">
39 <text>Police nabbed the man, who was out on
licence from prison, when he returned home.
</text>
40 </sentence>
41 </annotationSet>
4The XML shown here is somewhat simplified from the rep-
resentation being distributed by FrameNet, which includes at-
tributes on each label giving an ID number, the date and time
of creation, the name of the annotator, etc. In these examples,
we use several XML tags without defining them. Without go-
ing into unnecessary detail, we note here that they can be de-
fined in the DCS and the Dialect specification as described in
(Ide and Romary, 2001a). We are also using a condensed no-
tation with multiple attributes on entities for reasons of space,
although proper RDF requires that they be split out.
There are three other layers shown in the example,
none of which contain labels, called Sentence, Verb, and
Other. The layer Target contains the single label Target;
the fact that nab is the target word is indicated in the same
way as the information about FEs.
Note that this XML format is ?standoff? annotation in
the sense that the labels refer to text locations by charac-
ter positions (allowing any number of labels on various
layers, overlapping labels, etc.), but that the text and the
annotations appear in the same document. This is con-
trary to the general sense of the ISO standard, which uses
indirect pointers to an entirely separate document con-
taining the primary data. The indirect approach has cer-
tain advantages, and where the primary data is audio or
video, is virtually unavoidable. But in the case of the
current FrameNet data, where the annotations all apply
to individual sentences, there seem to be some advan-
tages, at least for human readers, of having the text of
the sentence and the annotation contained within a fairly
low-level XML entity, allowing the reader to glance back
and forth between them.5 In formulating standards for
linguistic annotation, it might be wise to take these ad-
vantages and disadvantages into consideration; perhaps
either situation might be allowable under the standard.
4.2 Other Types of Annotation
As the basic unit of annotation is the label, which can be
applied to anything ranging from a single character to an
entire sentence, and there are no a priori constraints on
labels overlapping, a great variety of information can be
represented in this way. We will not be able to demon-
strate all the possibilities here, but we will give a some
representative examples.
In FrameNet, event nouns are annotated in the same
frame (and hence with the same FEs) as the correspond-
ing verbs; the main differences are that the syntactic pat-
terns for the FEs of nouns are more varied, and (with
rare exceptions), no FEs of nouns are required to be ex-
pressed. Consider the noun arrest, also in the ARREST
frame, in the sentence:
Two witnesses have come forward with infor-
mation that could lead to [ Suspect the killer ?s]
arrest .
In this case the SUSPECT is expressed as a possessive (the
killer?s; it could equally well have been in a PP headed by
of (the arrest of the killer).
<annotationSet status="MANUAL">
5The location of the sentences in the original corpora is still
recoverable from the aPos attribute, which gives the absolute
position from which the sentence was abstracted. The name of
the corpus is given in another attribute which has been omitted
in the example.
<layers>
<layer name="FE">
<labels>
<label name="Suspect" start="68" end="80" />
</labels>
</layer>
<layer name="GF">
<labels>
<label name="Gen" start="68" end="80" />
</labels>
</layer>
<layer name="PT">
<labels>
<label name="Poss" start="68" end="80" />
</labels>
</layer>
<layer name="Sent" />
<layer name="Other" />
<layer name="Target">
<labels>
<label name="Target" start="82" end="87" />
</labels>
</layer>
<layer name="Noun" />
</layers>
<sentence aPos="102536044">
<text>Two witnesses have come forward with
information that could lead to the killer?s arrest.
</text>
</sentence>
</annotationSet>
In addition to marking the FE SUSPECT from ARREST,
we could also annotate the same sentence again in the
CAUSATION frame with the target lead, which would cre-
ate an annotation set listed under the the LU lead to:
Two witnesses have come forward with [ Cause
information that] could lead [ Effect to the
killer?s arrest].
The same sentence would be annotated in two differ-
ent frames, and the semantics of the two frames could
(in theory) be combined compositionally to get the se-
mantics of the phrase information that could lead to the
killer?s arrest. Similar processes of annotating in multi-
ple frames with targets come forward (and possibly wit-
ness as well) should yield a full semantics of the sentence.
6
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In ACL, ed-
itor, COLING-ACL ?98: Proceedings of the Confer-
ence, held at the University of Montre?al, pages 86?90.
Association for Computational Linguistics.
Charles J. Fillmore and B.T.S. Atkins. 1992. Towards
a frame-based lexicon: The semantics of RISK and its
6The qualification ?in theory? is included because the
present phase of the FrameNet project is not undertaking to im-
plement a system of semantic composition; we are just trying to
annotate enough examples in enough frames to provide a basis
for semantic parsing (in this context, automatic FE recognition)
and composition of annotation sets.
neighbors. In Adrienne Lehrer and Eva Feder Kittay,
editors, Frames, Fields and Contrasts. Lawrence Erl-
baum Associates.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. The framenet database and software tools. In
Proceedings of the Third International Conference on
Languag Resources and Evaluation, volume IV, Las
Palmas. LREC.
Charles J. Fillmore. 1976. Frame semantics and the na-
ture of language. In Annals of the New York Academy
of Sciences: Conference on the Origin and Develop-
ment of Language and Speech, volume 280, pages 20?
32.
Charles J. Fillmore. 1977a. The need for a frame seman-
tics in linguistics. In Hans Karlgren, editor, Statistical
Methods in Linguistics. Scriptor.
Charles J. Fillmore. 1977b. Scenes-and-frames seman-
tics. In Antonio Zampolli, editor, Linguistic Struc-
tures Processing, number 59 in Fundamental Studies
in Computer Science. North Holland Publishing.
Charles J. Fillmore. 1982. Frame semantics. In Lin-
guistics in the Morning Calm, pages 111?137. Hanshin
Publishing Co., Seoul, South Korea.
Nancy Ide and Laurent Romary. 2001a. A common
framework for syntactic annotation. In Proceedings of
ACL 2001, pages 298?305, Toulouse. ACL.
Nancy Ide and Laurent Romary. 2001b. Standards for
language resources. In Proceedings of the IRCS Work-
shop on Linguistic Databases, pages 141?149, Phi-
lapdelphia. IRCS.
Nancy Ide, Adam Kilgarriff, and Laurent Romary. 2002.
A formal model of dictionary structure and con-
tent. In Proceedings of Euralex 2000, pages 113?126,
Stuttgart. EURALEX.
ISO. 2002. Iso tc 37-4 n029: Linguistic annotation
framework. Internet. http:// www.tc37sc4.org/ docu-
ment.htm.
John B. Lowe, Collin F. Baker, and Charles J. Fillmore.
1997. A frame-semantic approach to semantic anno-
tation. In Marc Light, editor, Tagging Text with Lexi-
cal Semantics: Why, What and How? Special Interest
Group on the Lexicon, Association for Computational
Linguistics.
Srini Narayanan and Sheila McIlraith. 2002. Simula-
tion, verification and automated composition of web
services. In Proc. Eleventh International World Wide
Web Conference (WWW2002), May.
Miriam R. L. Petruck. 1996. Frame semantics. In
Jef Verschueren, Jan-Ola stman, Jan Blommaert, and
Chris Bulcaen, editors, Handbook of Pragmatics. John
Benjamins.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 99?104,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval?07 Task 19: Frame Semantic Structure Extraction
Collin Baker, Michael Ellsworth
International Computer Science Institute
Berkeley, California
{collinb,infinity}
@icsi.berkeley.edu
Katrin Erk
Computer Science Dept.
University of Texas
Austin
katrin.erk@mail.utexas.edu
Abstract
This task consists of recognizing words
and phrases that evoke semantic frames as
defined in the FrameNet project (http:
//framenet.icsi.berkeley.edu),
and their semantic dependents, which are
usually, but not always, their syntactic
dependents (including subjects). The train-
ing data was FN annotated sentences. In
testing, participants automatically annotated
three previously unseen texts to match gold
standard (human) annotation, including pre-
dicting previously unseen frames and roles.
Precision and recall were measured both for
matching of labels of frames and FEs and
for matching of semantic dependency trees
based on the annotation.
1 Introduction
The task of labeling frame-evoking words with ap-
propriate frames is similar to WSD, while the task of
assigning frame elements is called Semantic Role
Labeling (SRL), and has been the subject of several
shared tasks at ACL and CoNLL. For example, in
the sentence ?Matilde said, ?I rarely eat rutabaga,??
said evokes the Statement frame, and eat evokes
the Ingestion frame. The role of SPEAKER in the
Statement frame is filled by Matilda, and the role
of MESSAGE, by the whole quotation. In the Inges-
tion frame, I is the INGESTOR and rutabaga fills the
INGESTIBLES role. Since the ingestion event is con-
tained within the MESSAGE of the Statement event,
we can represent the fact that the message conveyed
was about ingestion, just by annotating the sentence
with respect to these two frames.
After training on FN annotations, the participants?
systems labeled three new texts automatically. The
evaluation measured precision and recall for frames
and frame elements, with partial credit for incorrect
but closely related frames. Two types of evaluation
were carried out: Label matching evaluation, in
which the participant?s labeled data was compared
directly with the gold standard labeled data, and Se-
mantic dependency evaluation, in which both the
gold standard and the submitted data were first con-
verted to semantic dependency graphs in XML for-
mat, and then these graphs were compared.
There are three points that make this task harder
and more interesting than earlier SRL tasks: (1)
while previous tasks focused on role assignment, the
current task also comprises the identification of the
appropriate FrameNet frame, similar to WSD, (2)
the task comprises not only the labeling of individ-
ual predicates and their arguments, but also the inte-
gration of all labels into an overall semantic depen-
dency graph, a partial semantic representation of
the overall sentence meaning based on frames and
roles, and (3) the test data includes occurrences of
frames that are not seen in the training data. For
these cases, participant systems have to identify the
closest known frame. This is a very realistic sce-
nario, encouraging the development of robust sys-
tems showing graceful degradation in the face of un-
known events.
99
2 Frame semantics and FrameNet
The basic concept of Frame Semantics is that many
words are best understood as part of a group of
terms that are related to a particular type of event
and the participants and ?props? involved in it (Fill-
more, 1976; Fillmore, 1982). The classes of events
are the semantic frames of the lexical units (LUs)
that evoke them, and the roles associated with the
event are referred to as frame elements (FEs). The
same type of analysis applies not only to events but
also to relations and states; the frame-evoking ex-
pressions may be single words or multi-word ex-
pressions, which may be of any syntactic category.
Note that these FE names are quite frame-specific;
generalizations over them are expressed via explicit
FE-FE relations.
The Berkeley FrameNet project (hereafter FN)
(Fillmore et al, 2003) is creating a computer- and
human-readable lexical resource for English, based
on the theory of frame semantics and supported by
corpus evidence. The current release (1.3) of the
FrameNet data, which has been freely available for
instructional and research purposes since the fall
of 2006, includes roughly 780 frames with roughly
10,000 word senses (lexical units). It also contains
roughly 150,000 annotation sets, of which 139,000
are lexicographic examples, with each sentence an-
notated for a single predicator. The remainder are
from full-text annotation in which each sentence is
annotated for all predicators; 1,700 sentences are an-
notated in the full-text portion of the database, ac-
counting for roughly 11,700 annotation sets, or 6.8
predicators (=annotation sets) per sentence. Nearly
all of the frames are connected into a single graph
by frame-to-frame relations, almost all of which
have associated FE-to-FE relations (Fillmore et al,
2004a)
2.1 Frame Semantics of texts
The ultimate goal is to represent the lexical se-
mantics of all the sentences in a text, based on
the relations between predicators and their depen-
dents, including both phrases and clauses, which
may, in turn, include other predicators; although this
has been a long-standing goal of FN (Fillmore and
Baker, 2001), automatic means of doing this are only
now becoming available.
Consider a sentence from one of the testing texts:
(1) This geography is important in understanding
Dublin.
In the frame semantic analysis of this sentence,
there are two predicators which FN has analyzed:
important and understanding, as well as one which
we have not yet analyzed, geography. In addition,
Dublin is recognized by the NER system as a loca-
tion. In the gold standard annotation, we have the
annotation shown in (2) for the Importance frame,
evoked by the target important, and the annotation
shown in (3) for the Grasp frame, evoked by under-
standing.
(2) [FACTOR This geography] [COP is] IMPOR-TANT [UNDERTAKING in understanding Dublin].[INTERESTED PARTY INI](3) This geography is important in UNDER-
STANDING [PHENOMENON Dublin]. [COGNIZERCNI]
The definitions of the two frames begin like this:
Importance: A FACTOR affects the outcome of an
UNDERTAKING, which can be a goal-oriented activ-
ity or the maintenance of a desirable state, the work
in a FIELD, or something portrayed as affecting an
INTERESTED PARTY. . .
Grasp: A COGNIZER possesses knowledge about
the workings, significance, or meaning of an idea or
object, which we call PHENOMENON, and is able to
make predictions about the behavior or occurrence
of the PHENOMENON. . .
Using these definitions and the labels, and the fact
that the target and FEs of one frame are subsumed
by an FE of the other, we can compose the mean-
ings of the two frames to produce a detailed para-
phrase of the meaning of the sentence: Something
denoted by this geography is a factor which affects
the outcome of the undertaking of understanding the
location called ?Dublin? by any interested party. We
have not dealt with geography as a frame-evoking
expression, although we would eventually like to.
(The preposition in serves only as a marker of the
frame element UNDERTAKING.)
In (2), the INTERESTED PARTY is not a label on
any part of the text; rather, it is marked INI, for ?in-
definite null instantiation?, meaning that it is con-
ceptually required as part of the frame definition,
absent from the sentence, and not recoverable from
the context as being a particular individual?meaning
100
that this geography is important for anyone in gen-
eral?s understanding of Dublin. In (3), the COG-
NIZER is ?constructionally null instantiated?, as the
gerund understanding licenses omission of its sub-
ject. The marking of null instantiations is important
in handling text coherence and was part of the gold
standard, but as far as we know, none of the partici-
pants attempted it, and it was ignored in the evalua-
tion.
Note that we have collapsed the two null instan-
tiated FEs, the INTERESTED PARTY of the impor-
tance frame and the COGNIZER in the Grasp frame,
since they are not constrained to be distinct.
2.2 Semantic dependency graphs
Since the role fillers are dependents (broadly speak-
ing) of the predicators, the full FrameNet annotation
of a sentence is roughly equivalent to a dependency
parse, in which some of the arcs are labeled with role
names; and a dependency graph can be derived algo-
rithmically from FrameNet annotation; an early ver-
sion of this was proposed by (Fillmore et al, 2004b)
Fig. 1 shows the semantic dependency graph de-
rived from sentence (1); this graphical representa-
tion was derived from a semantic dependency XML
file (see Sec. 5). It shows that the top frame in this
sentence is evoked by the word important, although
the syntactic head is the copula is (here given the
more general label ?Support?). The labels on the
arcs are either the names of frame elements or indi-
cations of which of the daughter nodes are seman-
tic heads, which is important in some versions of
the evaluation. The labels on nodes are either frame
names (also colored gray), syntactic phrases types
(e.g. NP), or the names of certain other syntactic
?connectors?, in this case, Marker and Support.
3 Definition of the task
3.1 Training data
The major part of the training data for the task con-
sisted of the current data release from FrameNet
(Release 1.3), described in Sec.2 This was supple-
mented by additional training data made available
through SemEval to participants in this task. In ad-
dition to updated versions of some of the full-text an-
notation from Release 1.3, three files from the ANC
were included: from Slate.com, ?Stephanopoulos
Importance:
important 
Marker: in 
Undertaking
 NP 
Factor
Grasp:
understanding 
SemHead
This geography
Head
NE:location:
Dublin 
DenotedFE: location
Phenomenon
 <s> 
Supp: is 
Head
.
SemHead
Figure 1: Sample Semantic Dependency Graph
Crimes? and ?Entrepreneur as Madonna?, and from
the Berlitz travel guides, ?History of Jerusalem?.
3.2 Testing data
The testing data was made up of three texts, none
of which had been seen before; the gold standard
consisted of manual annotations (by the FrameNet
team) of these texts for all frame evoking expres-
sions and the fillers of the associated frame ele-
ments. All annotation of the testing data was care-
fully reviewed by the FN staff to insure its cor-
rectness. Since most of the texts annotated in the
FN database are from the NTI website (www.nti.
org), we decided to take two of the three test-
ing texts from there also. One, ?China Overview?,
was very similar to other annotated texts such
as ?Taiwan Introduction?, ?Russia Overview?, etc.
available in Release 1.3. The other NTI text,
?Work Advances?, while in the same domain, was
shorter and closer to newspaper style than the rest
of the NTI texts. Finally, the ?Introduction to
101
Sents NEs Frames
Tokens Types
Work 14 31 174 77
China 39 90 405 125
Dublin 67 86 480 165
Totals 120 207 1059 272
Table 1: Summary of Testing Data
Dublin?, taken from the American National Cor-
pus (ANC, www.americannationalcorpus.
org) Berlitz travel guides, is of quite a different
genre, although the ?History of Jerusalem? text in
the training data was somewhat similar. Table 1
gives some statistics on the three testing files. To
give a flavor of the texts, here are two sentences;
frame evoking words are in boldface:
From ?Work Advances?: ?The Iranians are now
willing to accept the installation of cameras only
outside the cascade halls, which will not enable the
IAEA to monitor the entire uranium enrichment
process,? the diplomat said.
From ?Introduction to Dublin?: And in this
city, where literature and theater have historically
dominated the scene, visual arts are finally com-
ing into their own with the new Museum of Modern
Art and the many galleries that display the work of
modern Irish artists.
4 Participants
A number of groups downloaded the training or test-
ing data, but in the end, only three groups submitted
results: the UTD-SRL group and the LTH group,
who submitted full results, and the CLR group who
submitted results for frames only. It should also be
noted that the LTH group had the testing data for
longer than the 10 days allowed by the rules of the
exercise, which means that the results of the two
teams are not exactly comparable. Also, the results
from the CLR group were initially formatted slightly
differently from the gold standard with regard to
character spacing; a later reformatting allowed their
results to be scored with the other groups?.
The LTH system used only SVM classifiers, while
the UTD-SRL system used a combination of SVM
and ME classifiers, determined experimentally. The
CLR system did not use classifiers, but hand-written
symbolic rules. Please consult the separate system
papers for details about the features used.
5 Evaluation
The labels-only matching was similar to previous
shared tasks, but the dependency structure evalua-
tion deserves further explanation: The XML seman-
tic dependency structure was produced by a program
called fttosem, implemented in Perl, which goes
sentence by sentence through a FrameNet full-text
XML file, taking LU, FE, and other labels and using
them to structure a syntactically unparsed piece of a
sentence into a syntactic-semantic tree. Two basic
principles allow us to produce this tree: (1) LUs are
the sole syntactic head of a phrase whose semantics
is expressed by their frame and (2) each label span
is interpreted as the boundaries of a syntactic phrase,
so that when a larger label span subsumes a smaller
one, the larger span can be interpreted as a the higher
node in a hierarchical tree. There are a fair num-
ber of complications, largely involving identifying
mismatches between syntactic and semantic headed-
ness. Some of these (support verbs, copulas, mod-
ifiers, transparent nouns, relative clauses) are anno-
tated in the data with their own labels, while oth-
ers (syntactic markers, e.g. prepositions, and auxil-
iary verbs) must be identified using simple syntactic
heuristics and part-of-speech tags.
For this evaluation, a non-frame node counts as
matching provided that it includes the head of the
gold standard, whether or not non-head children of
that node are included. For frame nodes, the partici-
pants got full credit if the frame of the node matched
the gold standard.
5.1 Partial credit for related frames
One of the problems inherent in testing against un-
seen data is that it will inevitably contain lexical
units that have not previously been annotated in
FrameNet, so that systems which do not generalize
well cannot get them right. In principle, the deci-
sion as to what frame to add a new LU to should be
helped by the same criteria that are used to assign
polysemous lemmas to existing frames. However,
in practice this assignment is difficult, precisely be-
cause, unlike WSD, there is no assumption that all
the senses of each lemma are defined in advance; if
102
the system can?t be sure that a new use of a lemma
is in one of the frames listed for that lemma, then
it must consider all the 800+ frames as possibili-
ties. This amounts to the automatic induction of
fine-grained semantic similarity from corpus data, a
notoriously difficult problem (Stevenson and Joanis,
2003; Schulte im Walde, 2003).
For LUs which clearly do not fit into any exist-
ing frames, the problem is still more difficult. In the
course of creating the gold standard annotation of
the three testing texts, the FN team created almost 40
new frames. We cannot ask that participants hit upon
the new frame name, but the new frames are not cre-
ated in a vacuum; as mentioned above, they are al-
most always added to the existing structure of frame-
to-frame relations; this allows us to give credit for
assignment to frames which are not the precise one
in the gold standard, but are close in terms of frame-
to-frame relations. Whenever participants? proposed
frames were wrong but connected to the right frame
by frame relations, partial credit was given, decreas-
ing by 20% for each link in the frame-frame relation
graph between the proposed frame and the gold stan-
dard. For FEs, each frame element had to match the
gold standard frame element and contain at least the
same head word in order to gain full credit; again,
partial credit was given for frame elements related
via FE-to-FE relations.
6 Results
Text Group Recall Prec. F1
Dublin UTD-SRL 0.4188 0.7716 0.5430
China UTD-SRL 0.5498 0.8009 0.6520
Work UTD-SRL 0.5251 0.8382 0.6457
Dublin LTH 0.5184 0.7156 0.6012
China LTH 0.6261 0.7731 0.6918
Work LTH 0.6606 0.8642 0.7488
Dublin CLR 0.3984 0.6469 0.4931
China CLR 0.4621 0.6302 0.5332
Work CLR 0.5054 0.7452 0.6023
Table 2: Frame Recognition only
The strictness of the requirement of exact bound-
ary matching (which depends on an accurate syntac-
tic parse) is compounded by the cascading effect of
semantic classification errors, as seen by comparing
Text Group Recall Prec. F1
Label matching only
Dublin UTD-SRL 0.27699 0.55663 0.36991
China UTD-SRL 0.31639 0.51715 0.39260
Work UTD-SRL 0.31098 0.62408 0.41511
Dublin LTH 0.36536 0.55065 0.43926
China LTH 0.39370 0.54958 0.45876
Work LTH 0.41521 0.61069 0.49433
Semantic dependency matching
Dublin UTD-SRL 0.26238 0.53432 0.35194
China UTD-SRL 0.31489 0.53145 0.39546
Work UTD-SRL 0.30641 0.61842 0.40978
Dublin LTH 0.36345 0.54857 0.43722
China LTH 0.40995 0.57410 0.47833
Work LTH 0.45970 0.67352 0.54644
Table 3: Results for combined Frame and FE recog-
nition
the F-scores in Table 3 with those in Table 2. The
difficulty of the task is reflected in the F-scores of
around 35% for the most difficult text in the most
difficult condition, but participants still managed to
reach F-scores as high as 75% for the more limited
task of Frame Identification (Table 2), which more
closely matches traditional Senseval tasks, despite
the lack of a full sense inventory. The difficulty
posed by having such an unconstrained task led to
understandably low recall scores in all participants
(between 25 and 50%). The systems submitted by
the teams differed in their sensitivity to differences
in the texts: UTD-SRL?s system varied by around
10% across texts, while LTH?s varied by 15%.
There are some rather encouraging results also.
The participants rather consistently performed bet-
ter with our more complex, but also more useful and
realistic scoring, including partial credit and grad-
ing on semantic dependency rather than exact span
match (compare the top and bottom halves of Table
3). The participants all performed relatively well on
the frame-recognition task, with precision scores av-
eraging 63% and topping 85%.
7 Discussion
The testing data for this task turned out to be espe-
cially challenging with regard to new frames, since,
in an effort to annotate especially thoroughly, almost
103
40 new frames were created in the process of an-
notating these three specific passages. One result
of this was that the test passages had more unseen
frames than a random unseen passage, which prob-
ably lowered the recall on frames. It appears that
this was not entirely compensated by giving partial
credit for related frames.
This task is a more advanced and realistic version
of the Automatic Semantic Role Labeling task of
Senseval-3 (Litkowski, 2004). Unlike that task, the
testing data was previously unseen, participants had
to determine the correct frames as a first step, and
participants also had to determine FE boundaries,
which were given in the Senseval-3.
A crucial difference from similar approaches,
such as SRL with PropBank roles (Pradhan et al,
2004) is that by identifying relations as part of a
frame, you have identified a gestalt of relations that
enables far more inference, and sentences from the
same passage that use other words from the same
frame will be easier to link together. Thus, the
FN SRL results are translatable fairly directly into
formal representations which can be used for rea-
soning, question answering, etc. (Scheffczyk et
al., 2006; Frank and Semecky, 2004; Sinha and
Narayanan, 2005).
Despite the problems with recall, the participants
have expressed a determination to work to improve
these results, and the FN staff are eager to collabo-
rate in this effort. A project is now underway at ICSI
to speed up frame and LU definition, and another to
speed up the training of SRL systems is just begin-
ning, so the prospects for improvement seem good.
This material is based in part upon work sup-
ported by the National Science Foundation under
Grant No. IIS-0535297.
References
Charles J. Fillmore and Collin F. Baker. 2001. Framesemantics for text understanding. In Proceedingsof WordNet and Other Lexical Resources Workshop,Pittsburgh, June. NAACL.
Charles J. Fillmore, Christopher R. Johnson, andMiriam R.L Petruck. 2003. Background to FrameNet.International Journal of Lexicography, 16.3:235?250.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2004a. FrameNet as a ?Net?. In Proceedings ofLREC, volume 4, pages 1091?1094, Lisbon. ELRA.
Charles J. Fillmore, Josef Ruppenhofer, and Collin F.Baker. 2004b. FrameNet and representing the linkbetween semantic and syntactic relations. In Chu-
ren Huang and Winfried Lenders, editors, Frontiersin Linguistics, volume I of Language and LinguisitcsMonograph Series B, pages 19?59. Inst. of Linguistics,
Acadmia Sinica, Taipei.
Charles J. Fillmore. 1976. Frame semantics and the na-ture of language. Annals of the New York Academy ofSciences, 280:20?32.
Charles J. Fillmore. 1982. Frame semantics. In Lin-guistics in the Morning Calm, pages 111?137. Han-shin Publishing Co., Seoul, South Korea.
Anette Frank and Jiri Semecky. 2004. Corpus-basedinduction of an LFG syntax-semantics interface for
frame semantic processing. In Proceedings of the 5thInternational Workshop on Linguistically InterpretedCorpora (LINC 2004), Geneva, Switzerland.
Ken Litkowski. 2004. Senseval-3 task: Automatic label-
ing of semantic roles. In Rada Mihalcea and Phil Ed-monds, editors, Senseval-3: Third International Work-shop on the Evaluation of Systems for the SemanticAnalysis of Text, pages 9?12, Barcelona, Spain, July.Association for Computational Linguistics.
Sameer S. Pradhan, Wayne H. Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages
233?240, Boston, Massachusetts, USA, May 2 - May7. Association for Computational Linguistics.
Jan Scheffczyk, Collin F. Baker, and Srini Narayanan.2006. Ontology-based reasoning about lexical re-
sources. In Alessandro Oltramari, editor, Proceedingsof ONTOLEX 2006, pages 1?8, Genoa. LREC.
Sabine Schulte im Walde. 2003. Experiments on thechoice of features for learning verb classes. In Pro-ceedings of the 10th Conference of the EACL (EACL-03).
Steve Sinha and Srini Narayanan. 2005. Model basedanswer selection. In Proceedings of the Workshop onTextual Inference, 18th National Conference on Artifi-cial Intelligence, PA, Pittsburgh. AAAI.
Suzanne Stevenson and Eric Joanis. 2003. Semi-supervised verb class discovery using noisy features.
In Proceedings of the 7th Conference on Natural Lan-guage Learning (CoNLL-03), pages 71?78.
104
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106?111,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse
Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{josefr,csporled}@coli.uni-sb.de
Roser Morante
CNTS
University of Antwerp
Roser.Morante@ua.ac.be
Collin Baker
ICSI
Berkeley, CA 94704
collin@icsi.berkeley.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
In this paper, we describe the SemEval-2010
shared task on ?Linking Events and Their Par-
ticipants in Discourse?. This task is a variant
of the classical semantic role labelling task.
The novel aspect is that we focus on linking
local semantic argument structures across sen-
tence boundaries. Specifically, the task aims at
linking locally uninstantiated roles to their co-
referents in the wider discourse context (if such
co-referents exist). This task is potentially ben-
eficial for a number of NLP applications and
we hope that it will not only attract researchers
from the semantic role labelling community
but also from co-reference resolution and infor-
mation extraction.
1 Introduction
Semantic role labelling (SRL) has been defined as
a sentence-level natural-language processing task in
which semantic roles are assigned to the syntactic
arguments of a predicate (Gildea and Jurafsky, 2002).
Semantic roles describe the function of the partici-
pants in an event. Identifying the semantic roles of
the predicates in a text allows knowing who did what
to whom when where how, etc.
SRL has attracted much attention in recent years,
as witnessed by several shared tasks in Sense-
val/SemEval (Ma`rquez et al, 2007; Litkowski, 2004;
Baker et al, 2007; Diab et al, 2007), and CoNLL
(Carreras and Ma`rquez, 2004; Carreras and Ma`rquez,
2005; Surdeanu et al, 2008). The state-of-the-art
in semantic role labelling has now advanced so
much that a number of studies have shown that au-
tomatically inferred semantic argument structures
can lead to tangible performance gains in NLP ap-
plications such as information extraction (Surdeanu
et al, 2003), question answering (Shen and Lapata,
2007) or recognising textual entailment (Burchardt
and Frank, 2006).
However, semantic role labelling as it is currently
defined also misses a lot of information that would
be beneficial for NLP applications that deal with
text understanding (in the broadest sense), such as
information extraction, summarisation, or question
answering. The reason for this is that SRL has tra-
ditionally been viewed as a sentence-internal task.
Hence, relations between different local semantic ar-
gument structures are disregarded and this leads to a
loss of important semantic information.
This view of SRL as a sentence-internal task is
partly due to the fact that large-scale manual anno-
tation projects such as FrameNet1 and PropBank2
typically present their annotations lexicographically
by lemma rather than by source text. Furthermore,
in the case of FrameNet, the annotation effort did
not start out with the goal of exhaustive corpus an-
notation but instead focused on isolated instances of
the target words sampled from a very large corpus,
which did not allow for a view of the data as ?full-text
annotation?.
It is clear that there is an interplay between local
argument structure and the surrounding discourse
(Fillmore, 1977). In early work, Palmer et al (1986)
discussed filling null complements from context by
using knowledge about individual predicates and ten-
1http://framenet.icsi.berkeley.edu/
2http://verbs.colorado.edu/?mpalmer/
projects/ace.html
106
dencies of referential chaining across sentences. But
so far there have been few attempts to find links
between argument structures across clause and sen-
tence boundaries explicitly on the basis of semantic
relations between the predicates involved. Two no-
table exceptions are Fillmore and Baker (2001) and
Burchardt et al (2005). Fillmore and Baker (2001)
analyse a short newspaper article and discuss how
frame semantics could benefit discourse processing
but without making concrete suggestions of how to
model this. Burchardt et al (2005) provide a detailed
analysis of the links between the local semantic argu-
ment structures in a short text; however their system
is not fully implemented either.
In the shared task, we intend to make a first step
towards taking SRL beyond the domain of individual
sentences by linking local semantic argument struc-
tures to the wider discourse context. In particular, we
address the problem of finding fillers for roles which
are neither instantiated as direct dependents of our
target predicates nor displaced through long-distance
dependency or coinstantatiation constructions. Of-
ten a referent for an uninstantiated role can be found
in the wider context, i.e. in preceding or following
sentences. An example is given in (1), where the
CHARGES role (ARG2 in PropBank) of cleared is left
empty but can be linked to murder in the previous
sentence.
(1) In a lengthy court case the defendant was
tried for murder. In the end, he was cleared.
Another very rich example is provided by (2),
where, for instance, the experiencer and the object of
jealousy are not overtly expressed as syntactic depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.
NIs are also very frequent in clinical reports.
For example, in (3) the EXPERIENCER role of
?cough?, ?tachypnea?, and ?breathing? can be linked
to ?twenty-two month old?. Text mining systems in
the biomedical domain focus on extracting relations
between biomedical entities and information about
patients. It is important that these systems extract
information as accurately as possible. Thus, finding
co-referents for NIs is also very relevant for improv-
ing results on mining relations in biomedical texts.
(3) Twenty-two month old with history of recur-
rent right middle lobe infiltrate. Increased
cough, tachypnea, and work of breathing.
In the following sections we describe the task in
more detail. We start by providing some background
on null instantiations (Section 2). Section 3 gives an
overview of the task, followed by a description of
how we intend to create the data (Section 4). Sec-
tion 5 provides a short description of how null in-
stantiations could be resolved automatically given
the provided data. Finally, Section 6 discusses the
evaluation measures and we wrap up in Section 7.
2 Background on Null Instantiation
The theory of null complementation used here is the
one adopted by FrameNet, which derives from the
work of Fillmore (1986).3 Briefly, omissions of core
arguments of predicates are categorised along two
dimensions, the licensor and the interpretation they
receive. The idea of a licensor refers to the fact that
either a particular lexical item or a particular gram-
matical construction must be present for the omission
of a frame element (FE) to occur. For instance, the
omission of the agent in (4) is licensed by the passive
construction.
(4) No doubt, mistakes were made 0Protagonist.
The omission is a constructional omission because
it can apply to any predicate with an appropriate
semantics that allows it to combine with the passive
construction. On the other hand, the omission in (5)
is lexically specific: the verb arrive allows the Goal
to be unspecified but the verb reach, also a member
of the Arriving frame, does not.
(5) We arrived 0Goal at 8pm.
The above two examples also illustrate the second
major dimension of variation. Whereas, in (4) the
protagonist making the mistake is only existentially
bound within the discourse (instance of indefinite null
3Palmer et al?s (1986) treatment of uninstantiated ?essential
roles? is very similar (see also Palmer (1990)).
107
instantiation, INI), the Goal location in (5) is an entity
that must be accessible to speaker and hearer from
the discourse or its context (definite null instantiation,
DNI). Finally note that the licensing construction or
lexical item fully and reliably determines the interpre-
tation. Missing by-phrases always have an indefinite
interpretation and whenever arrive omits the Goal
lexically, the Goal has to be interpreted as definite,
as it is in (5).
The import of this classification to the task here
is that we will concentrate on cases of DNI whether
they are licensed lexically or constructionally.
3 Task Description
We plan to run the task in the following two modes:
Full Task For the full task we supply a test set in
which the target words are marked and labelled with
the correct sense (i.e. frame).4 The participants then
have to:
1. find the overt semantic arguments of the target
(role recognition)
2. label them with the correct role (role labelling)
3. recognize definite null instantiations and find
links to antecedents in the wider context (NI
linking)
NIs only In the second mode, participants will be
supplied with a test set which is annotated with gold
standard local semantic argument structure.5 The
task is then restricted to recognizing that a core role
is missing, ascertaining that it must have a definite
interpretation and finding a filler for it (i.e., sub-task
3 from the full task).
The full task and the null instantiation linking task
will be evaluated separately. By setting up a SRL
task, we expect to attract participants from the es-
tablished SRL community. Furthermore, by allow-
ing participants to only address the second task, we
4We supply the correct sense to ensure that all systems use
the same role inventory for each target (i.e., the role inventory
associated with the gold standard sense). This makes it easier
to evaluate the systems consistently with respect to role assign-
ments and null instantiation linking, which is our main focus.
5The training set is identical for both set-ups and will contain
the full annotation, i.e., frames, semantic roles and their fillers,
and referents of null instantiations in the wider context (see
Section 4 for details).
hope to also attract researchers from areas such as co-
reference resolution or information extraction who do
not want to implement a complete SRL system. We
also plan to provide the data with both FrameNet and
PropBank style annotations to encourage researchers
from both areas to take part.
4 Data
The data will come from one of Arthur Conan
Doyle?s fiction works. We chose fiction rather than
news because we believe that fiction texts with
a linear narrative generally contain more context-
resolvable null instantiations. They also tend to be
longer and have a simpler structure than news texts
which typically revisit the same facts repeatedly at
different levels of detail (in the so-called ?inverted
pyramid? structure) and which mix event reports with
commentary and evaluation, thus sequencing mate-
rial that is understood as running in parallel. Fiction
texts should lend themselves more readily to a first at-
tempt at integrating discourse structure into semantic
role labeling. We chose Conan Doyle?s work because
most of his books are not subject to copyright restric-
tions anymore, which allows us to freely release the
annotated data.
We plan to make the data sets available with both
FrameNet and PropBank semantic argument anno-
tation, so that participants can choose which frame-
work they want to work in. The annotations will
originally be made using FrameNet-style and will
later be mapped semi-automatically to PropBank an-
notations. The data set for the FrameNet version of
the task will be built at Saarland University, in close
co-operation with the FrameNet team in Berkeley.
We aim for the same density of annotation as is ex-
hibited by FrameNet?s existing full-text annotation6
and are currently investigating whether the semantic
argument annotation can be done semi-automatically,
e.g., by starting the annotation with a run of the Shal-
maneser role labeller (Erk and Pado?, 2006), whose
output is then corrected and expanded manually. To
ensure a high annotation quality, at least part of the
data will be annotated by two annotators and then
manually adjudicated. We also provide detailed an-
notation guidelines (largely following the FrameNet
6http://framenet.icsi.berkeley.edu/
index.php?option=com_wrapper&Itemid=84
108
guidelines) and any open questions are discussed in
a weekly annotation meeting.
For the annotation of null instantiations and their
links to the surrounding discourse we have to create
new guidelines as this is a novel annotation task. We
will adopt ideas from the annotation of co-reference
information, linking locally unrealised roles to all
mentions of the referents in the surrounding dis-
course, where available. We will mark only identity
relations but not part-whole or bridging relations be-
tween referents. The set of unrealised roles under
consideration includes only the core arguments but
not adjuncts (peripheral or extra-thematic roles in
FrameNet?s terminology). Possible antecedents are
not restricted to noun phrases but include all con-
stituents that can be (local) role fillers for some pred-
icate plus complete sentences (which can sometimes
fill roles such as MESSAGE).
The data-set for PropBank will be created by map-
ping the FrameNet annotations onto PropBank and
NomBank labels. For verbal targets, we use the Sem-
link7 mappings. For nominal targets, there is no
existing hand-checked mapping between FrameNet
and NomBank but we will explore a way of build-
ing a FrameNet - NomBank mapping at least for
eventive nouns indirectly with the help of Semlink.
This would take advantage of the fact that PropBank
verbs and eventive NomBank nouns both have a map-
ping to VerbNet classes, which are referenced also by
Semlink. Time permitting, non-eventive nouns could
be mapped manually. For FrameNet targets of other
parts of speech, in particular adjectives and prepo-
sitions, no equivalent PropBank-style counterparts
will be available. The result of the automatic map-
pings will be partly hand-checked. The annotations
resolving null instantiations need no adjustment.
We intend to annotate at least two data sets of
around 4,000 words. One set for testing and one for
training. Because we realise that the training set will
not be large enough to train a semantic role labelling
system on it, we permit the participants to boost the
training data for the SRL task by making use of the
existing FrameNet and PropBank corpora.8
7http://verbs.colorado.edu/semlink/
8This may require some genre adaption but we believe this is
feasible.
5 Resolving Null Instantiations
We conceive of null instantiation resolution as a three
step problem. First, one needs to determine whether a
core role is missing. This involves looking up which
core roles are overtly expressed and which are not.
In the second step, one needs to determine what
licenses an omission and what its interpretation is.
To do this, one can use rules and heuristics based on
various syntactic and lexical facts of English. As an
example of a relevant syntactic fact, consider that sub-
jects in English can only be omitted when licensed by
a construction. One such construction is the impera-
tive (e.g. Please, sit down). Since this construction
also specifies that the missing referent must be the
addressee of the speaker of the imperative, it is clear
what referent one has to try to find.
As for using lexical knowledge, consider omis-
sions of the Goods FE of the verb steal in the Theft
frame. FrameNet annotation shows that whenever
the Goods FE of steal is missing it is interpreted in-
definitely, suggesting that a new instance of the FE
being missing should have the same interpretation.
More evidence to the same effect can be derived us-
ing Ruppenhofer?s (2004) observation that the inter-
pretation of a lexically licensed omission is definite
if the overt instances of the FE have mostly definite
form (i.e. have definite determiners such as that, the ,
this), and indefinite if they are mostly indefinite (i.e.
have bare or indefinite determiners such as a(n) or
some). The morphology of overt instances of an FE
could be inspected in the FrameNet data, or if the
predicate has only one sense or a very dominant one,
then the frequencies could even be estimated from
unannotated corpora.
The third step is linking definite omissions to ref-
erents in the context. This linking problem could be
modelled as a co-reference resolution task. While
the work of Palmer et al (1986) relied on special
lexicons, one might instead want to learn information
about the semantic content of different role fillers
and then assess for each of the potential referents in
the discourse context whether their semantic content
is close enough to the expected content of the null
instantiated role.
Information about the likely fillers of a role can
be obtained from annotated data sets (e.g., FrameNet
or PropBank). For instance, typical fillers of the
109
CHARGES role of clear might be murder, accusa-
tions, allegations, fraud etc. The semantic content of
the role could then be represented in a vector space
model, using additional unannotated data to build
meaning vectors for the attested role fillers. Meaning
vectors for potential role fillers in the context of the
null instantiation could be built in a similar fashion.
The likelihood of a potential filler filling the target
role can then be modelled as the distance between the
meaning vector of the filler and the role in the vec-
tor space model (see Pado? et al (2008) for a similar
approach for semi-automatic SRL).
We envisage that the manually annotated null in-
stantiated data can be used to learn additionally
heuristics for the filler resolution task, such as in-
formation about the average distance between a null
instantiation and its most recent co-referent.
6 Evaluation
As mentioned above we allow participants to address
either the full role recognition and labelling task plus
the linking of null instantiations or to make use of
the gold standard semantic argument structure and
look only at the null instantiations. We also permit
systems to perform either FrameNet or PropBank
style SRL. Hence, systems can be entered for four
subtasks which will be evaluated separately:
? full task, FrameNet
? null instantiations, FrameNet
? full task, PropBank
? null instantiations, PropBank
The focus for the proposed task is on the null in-
stantiation linking, however, for completeness, we
also evaluate the standard SRL task. For role recogni-
tion and labelling we use a standard evaluation set-up,
i.e., for role recognition we will evaluate the accuracy
with respect to the manually created gold standard,
for role labelling we will evaluate precision, recall,
and F-Score.
The null instantiation linkings are evaluated
slightly differently. In the gold standard, we will iden-
tify referents for null instantiations in the discourse
context. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system should be given
credit if the null instantiation is linked to any of these
expressions. To achieve this we create equivalence
sets for the referents of null instantiations. If the null
instantiation is linked to any item in the equivalence
set, the link is counted as a true positive. We can then
define NI linking precision as the number of all true
positive links divided by the number of links made by
a system, and NI linking recall as the number of true
positive links divided by the number of links between
a null instantiation and its equivalence set in the gold
standard. NI linking F-Score is then the harmonic
mean between NI linking precision and recall.
Since it may sometimes be difficult to determine
the correct extend of the filler of an NI, we score
an automatic annotation as correct if it includes the
head of the gold standard filler in the predicted filler.
However, in order to not favour systems which link
NIs to excessively large spans of text to maximise the
likelihood of linking to a correct referent, we intro-
duce a second evaluation measure, which computes
the overlap (Dice coefficient) between the words in
the predicted filler (P) of a null instantiation and the
words in the gold standard one (G):
NI linking overlap = 2|P ?G||P |+ |G| (6)
Example (7) illustrates this point. The verb won in
the second sentence evokes the Finish competition
frame whose COMPETITION role is null instantiated.
From the context it is clear that the competition role
is semantically filled by their first TV debate (head:
debate) and last night?s debate (head: debate) in
the previous sentences. These two expressions make
up the equivalence set for the COMPETITION role in
the last sentence. Any system that would predict a
linkage to a filler that covers the head of either of
these two expressions would score a true positive for
this NI. However, a system that linked to last night?s
debate would have an NI linking overlap of 1 (i.e.,
2*3/(3+3)) while a system linking the whole second
sentence Last night?s debate was eagerly anticipated
to the NI would have an NI linking overlap of 0.67
(i.e., 2*3/(6+3))
(7) US presidential rivals Republican John
McCain and Democrat Barack Obama have
yesterday evening attacked each other over
110
foreign policy and the economy, in [their
first TV debate]Competition. [Last night?s
debate]Competition was eagerly anticipated.
Two national flash polls suggest that
[Obama]Competitor wonFinish competition
0Competition.
7 Conclusion
In this paper, we described the SemEval-2010 shared
task on ?Linking Events and Their Participants in
Discourse?. With this task, we intend to take a first
step towards viewing semantic role labelling not as a
sentence internal problem but as a task which should
really take the discourse context into account. Specif-
ically, we focus on finding referents for roles which
are null instantiated in the local context. This is po-
tentially useful for various NLP applications. We
believe that the task is timely and interesting for a
number of researchers not only from the semantic
role labelling community but also from fields such as
co-reference resolution or information extraction.
While our task focuses specifically on finding links
between null instantiated roles and the discourse con-
text, we hope that in setting it up, we can stimulate re-
search on the interaction between discourse structure
and semantic argument structure in general. Possible
future editions of the task could then focus on addi-
tional connections between local semantic argument
structures (e.g., linking argument structures that refer
to the same event).
8 Acknowledgements
Josef Ruppenhofer and Caroline Sporleder are supported
by the German Research Foundation DFG (under grant
PI 154/9-3 and the Cluster of Excellence Multimodal
Computing and Interaction (MMCI), respectively). Roser
Morante?s research is funded by the GOA project BIO-
GRAPH of the University of Antwerp.
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.
A. Burchardt and A. Frank. 2006. Approximating textual
entailment with LFG and framenet frames. In Pro-
ceedings of the Second Recognising Textual Entailment
Workshop.
A. Burchardt, A. Frank, and M. Pinkal. 2005. Building
text meaning representations from contextually related
frames ? A case study. In Proceedings of IWCS-6.
X. Carreras and Ll. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL-04, pages 89?97.
X. Carreras and Ll. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-05, pages 152?164.
M. Diab, M. Alkhalifa, S. ElKateb, C. Fellbaum, A. Man-
souri, and M. Palmer. 2007. SemEval-2007 Task 18:
Arabic semantic labeling. In Proc. of SemEval-07.
K. Erk and S. Pado?. 2006. Shalmaneser - a flexible
toolbox for semantic role assignment. In Proceedings
of LREC-06.
C.J. Fillmore and C.F. Baker. 2001. Frame semantics for
text understanding. In Proc. of the NAACL-01 Work-
shop on WordNet and Other Lexical Resources.
C.J. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In Antonio Zampolli,
editor, Fundamental Studies in Computer Science, No.
59, pages 55?88. North Holland Publishing.
C.J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual Meet-
ing of the Berkeley Liguistics Society.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?
288.
K. Litkowski. 2004. SENSEVAL-3 Task: Automatic
labeling of semantic roles. In Proc. of SENSEVAL-3.
L. Ma`rquez, L. Villarejo, M. A. Mart?`, and M. Taule`. 2007.
SemEval-2007 Task 09: Multilevel semantic annotation
of Catalan and Spanish. In Proceedings of SemEval-07.
S. Pado?, M. Pennacchiotti, and C. Sporleder. 2008. Se-
mantic role assignment for event nominalisations by
leveraging verbal data. In Proceedings of Coling-2008.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, and J. Dowding. 1986. Recovering
implicit information. In Proceedings of ACL-1986.
M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.
J. Ruppenhofer. 2004. The interaction of valence and
information structure. Ph.d., University of California,
Berkeley, CA.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In Proc. of EMNLP-07.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate arguments structures for infor-
mation extraction. In Proceedings of ACL-2003.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proceedings of CoNLL-2008, pages 159?177.
111
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 125?129,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
WordNet and FrameNet as Complementary Resources for Annotation
Collin F. Baker
International Computer Science Institute
1947 Center St., Berkeley, California 94704
collinb@icsi.berkeley.edu
Christiane Fellbaum
Princeton University
Princeton, NJ 08540-5233
fellbaum@princeton.edu
Abstract
WordNet and FrameNet are widely used lexi-
cal resources, but they are very different from
each other and are often used in completely
different ways in NLP. In a case study in which
a short passage is annotated in both frame-
works, we show how the synsets and defini-
tions of WordNet and the syntagmatic infor-
mation from FrameNet can complement each
other, forming a more complete representa-
tion of the lexical semantic of a text than ei-
ther could alone. Close comparisons between
them also suggest ways in which they can be
brought into alignment.
1 Background and motivation
FrameNet and WordNet are two lexical databases that
are widely used for NLP, often in conjunction. Because
of their complementary designs they are obvious candi-
dates for alignment, and an exploratory research project
within the larger context of the semantic annotation of
the the American national Corpus is currently under-
way. We give specific illustrative examples of annota-
tions against both resources, highlighting their different
contributions towards a rich semantic analysis.
WordNet (WN):1 (Fellbaum, 1998), is a large elec-
tronic lexical database of English. Originally con-
ceived as a full-scale model of human semantic orga-
nization, it was quickly embraced by the Natural Lan-
guage Processing (NLP) community, a development
that guided its subsequent growth and design. Word-
Net has become the lexical database of choice for NLP
and has been incorporated into other language tools,
including VerbNet (Kipper et al, 2000) and OntoNotes
(Hovy et al, 2006). Numerous on-line dictionaries, in-
cluding Google?s ?define? function, rely significantly
on WordNet.
WordNet?s coverage is sometimes criticized as be-
ing too fine-grained for automatic processing, though
its inventory is not larger than that of a standard col-
legiate dictionary. But the present limitation of auto-
matic WSD cannot be entirely blamed on existing sys-
tems; for example, Fellbaum and Grabowski (1997)
1http://wordnet.princeton.edu
have shown that humans, too, have difficulties identi-
fying context-appropriate dictionary senses. One an-
swer is clearly that meanings do not exist outside con-
texts. Furthermore, although WN does contain ?sen-
tence frames? such as ?Somebody ?-s something?
for a transitive verb with a human agent, it provides
little syntagmatic information, except for what can
be gleaned from the example sentences. WordNet?s
great strength is its extensive coverage, with more than
117,000 synonym sets (synsets), each with a definition
and relations to other synsets covering almost all the
general vocabulary of English.
FrameNet (FN):2 (Fontenelle, 2003) is a lexical
resource organized not around words per se, but se-
mantic frames (Fillmore, 1976): characterizations of
events, relations, and states which are the conceptual
basis for understanding groups of word senses, called
lexical units (LUs). Frames are distinguished by the
set of roles involved, known as frame elements (FEs).
Much of the information in the FrameNet lexicon is
derived by annotating corpus sentences; for each LU,
groups of sentences are extracted from a corpus, sen-
tences which collectively exemplify all of the lexico-
graphically relevant syntactic patterns in which the LU
occurs. A few examples of each pattern are annotated;
annotators not only mark the target word which evokes
the frame in the mind of the hearer, but also mark
those phrases which are syntactically related to the tar-
get word and express its frame elements. FrameNet is
much smaller than WordNet, covering roughly 11,000
LUs, but contains very rich syntagmatic information
about the combinatorial possibilities of each LU.
Given these two lexical resources with different
strengths, it seems clear that combining WN and FN
annotation will produce a more complete semantic rep-
resentation of the meaning of a text than either could
alone. What follows is intended as an example of how
they can usefully be combined.
2 Case Study: Aegean History
The text chosen for this study is a paragraph from the
American National Corpus3 (Ide et al, 2002), from the
Berlitz travel guide to Greece, discussing the history of
2http://framenet.icsi.berkeley.edu
3http://www.americannationalcorpus.org
125
Greece, specifically the Aegean islands after the fall of
Byzantium to the Crusaders. Although brief, its three
sentences provide ample material to demonstrate some
of the subtlety of both WN and FN annotation:
(1) While Byzantine land was being divided, there
was no one in control of the seas, so pirates raided
towns on many of the islands. (2) To counter this, the
populations moved from their homes on the coast and
built settlements inland, out of sight of the raiding par-
ties. (3) This created a pattern seen today throughout
the Aegean of a small port (skala) which serves an in-
land settlement or chora, making it easier to protect the
island from attack.
Below, we present three tables containing the anno-
tation of both the WordNet synsets for each open class
(content) word in the text4 and the FrameNet frames
and the fillers of the frame elements in each sentence.
We also provide brief notes on some interesting fea-
tures of the semantics of each sentence.
2.1 Discussion of Sentence 1, shown in Table 1 on
page 4 :
(2) Information about what the land was separated into
is not given in the sentence nor clear from the context,
so the PARTS FE has been annotated as ?indefinite null
instantiated? (INI). Clearly this is an intentional action,
but because the verb is passive, the agent can be (and
is) omitted, so the AGENT FE is marked as ?construc-
tionally null instantiated? (CNI).5
(4) In addition to FEs and their phrase types and
grammatical functions, FrameNet annotates a limited
set of syntactic facts: here, in is annotated as at ?sup-
port preposition?, allowing control to function as an ad-
jectival, and was as a copula, allowing no one to fill the
External syntactic position of in control.
(5) Since FN is based on semantic frames, annota-
tion of nouns is largely limited to those which express
events (e.g. destruction), relations (brother), or states
(height). For the most part, nouns denoting artifacts
and natural kinds evoke relatively uninteresting frames,
and hence relatively few of them have been included
in FN. However, there are three such instances in this
sentence, seas, islands (9), and towns (12); In all three
cases, the frame-evoking noun also denotes the filler of
the FE LOCALE.
(6) At the top level of organization, so evokes
the Causation frame. Actually, it is misleading to
simply annotate control of the seas in the frames
Be in control and Natural features; here, we regard
seas as metonymic for ?ship traffic on the seas?, but
neither the FN annotation nor the WN definition indi-
cates this.
(7) The noun pirates evokes the very rich frame of
4Note that for reasons of space, many WN examples have
been omitted.
5In fact, the previous sentence describes the sack of Con-
stantinople by the Crusaders, so they can be inferred to be the
dividers of the lands, as well.
Piracy, and also denotes the filler of the FE PERPE-
TRATOR, but that is the only FE filled in in that frame.
Instead, pirates actually fills the ASSAILANT FE of the
Attack frame, (8); the main idea is about the raids, not
the piratical acts on the seas that the same people have a
habit of committing. Note that the WN definition takes
the view that raiding coastal towns is a typical part of
piracy.
(10) Political locales roughly corresponds to
?Geopolitical entity? in named entity recognition.
Despite the relatively fine level of detail of the anno-
tations, there are still many important semantic features
of the sentence not represented in FrameNet or Word-
Net. For example, there is no treatment of negation cum
quantification, no representation of the fact that there
was no one in control should mean that Be in control
is not happening.
2.2 Discussion of Sentence 2, shown in Table 2 on
page 5:
The two highest level predicates in this sentence are
moved (2) and built (6), in the frames Motion and
Building respectively; since they are conjoined, the
phrase to counter this fills the FE PURPOSE in both
frames. 6 In (2) the GOAL FE of the Motion is marked
as definite null instantiation (DNI), because, although
it is not expressed in the VP headed by moved, it is
recoverable from context (i.e. the second VP).
(4) Note that FN puts this sense of home in the Build-
ings frame7, but WN has a less specific definition. (6)
Coast is a Relational natural feature because it is de-
fined in relation to another natural feature; a coast
has to be the coast of some land mass, although here
the land mass is DNI. (9) Inland both evokes a Loca-
tive relation and denotes the GROUND FE. (10) FN and
WN agree on a sense of sight denoting the range of vi-
sion. (11) WN?s example sentence for raid is precisely
about pirates.
2.3 Discussion of Sentence 3 shown in Table 3 on
page 5:
(2) The concept of ?pattern? is very slippery?the ar-
rangement of port and inland settlement is both spa-
tial and temporal in terms of building practices over
centuries. (3) This sense of see can refer to the area
in which something is seen, the time, or the condi-
tions under which it can be seen; these are subsumed
by the FE STATE. (4) Today expresses a Tempo-
ral collocation and denotes the LANDMARK. (Repe-
titions of the words settlement and island have been
omitted.) The interrelation among (7), (10), (11) and
(12) is rather complex: the arrangement in which the
port serves the settlement has the making easier as a
result. The arrangement is also the CAUSE FE of mak-
ing. Easier in the Difficulty frame requires an EX-
6This is a peripheral FE, common to all frames which
inherit from the Intentionally act frame.
7Not to be confused with the Building frame, in (7).
126
PERIENCER FE which is not specified here (thus INI)
and an ACTIVITY FE, to protect. The FE PROTEC-
TION (which can be a person, a thing, or an activity) is
marked CNI, because it is the external argument of the
infinitive.
3 Towards an alignment of WordNet and
FrameNet
We hope these examples have shown that finding re-
lated WN and FN senses can contribute to text under-
standing. Fellbaum and Baker (2008) discuss the re-
spective strengths and weaknesses of WN and FN as
well as their complementary advantages that could be
fruitfully exploited aligning the two resources. Work
of this type is actually underway; researchers are semi-
automatically annotating selected lemmas in the Amer-
ican National Corpus with both FN frames and WN
senses. The lemmas are chosen so as to reflect the part
of speech distribution in text and to represent a spec-
trum of frequency and polysemy. A preliminary group
of instances are manually tagged by trained annotators,
and then the teams working on WN and FN annota-
tion discuss and resolve discrepancies among the tag-
gers before the remaining tokens are annotated.
Three cases sum up the annotation and alignment
process:
(1) In the very unlikely case that a synset and a frame
contain exactly the same set of lexemes, their corre-
spondence is simply recorded.
(2) In the more common case in which all the words
in a synset are a subset of those in the frame, or all the
words in a frame are a subset of those in the synset, this
fact is also recorded.
(3) In case two synsets are subsets of the LUs of one
frame, we will record this and note that it as a possible
candidate for collapsing the synsets, respectively.
FN and WN are two comprehensive but comple-
mentary lexical resources. Both WN?s paradigmatic
and FN?s syntagmatic approach to lexical semantics are
needed for a rich representation of word meaning in
context. We have demonstrated how text can be an-
notated against both resources to provide the founda-
tion for deep language understanding and, as an im-
portant by-product, help to align the word senses of
these widely-used resources. Of course, these ex-
amples were manually annotated, but automatic sys-
tems for word-sense disambiguation (largely based on
WordNet) and FrameNet role labeling (Johansson and
Nugues, 2007; Coppola et al, 2008) are improving
rapidly. The project just described is intended to pro-
vide more gold-standard annotation (both WN and FN)
to help train automatic systems for both WN and FN
annotation, which are clearly related tasks e.g. (Prad-
han et al, 2007; Erk, 2005).
Acknowledgment
We gratefully acknowledge support from the National
Science Foundation (#IIS-0705199) for the work re-
ported here.
References
Bonaventura Coppola, Alessandro Moschitti, Sara
Tonelli, and Giuseppe Riccardi. 2008. Automatic
framenet-based annotation of conversational speech.
In Proceedings of IEEE-SLT 2008, pages 73?76,
Goa, India, December.
Katrin Erk. 2005. Frame assignment as word sense
disambiguation. In Proceedings of IWCS 6, Tilburg.
Christiane Fellbaum and Collin F. Baker. 2008. Can
WordNet and FrameNet be made ?interoperable??
In Jonathan Webster, Nancy Ide, and Alex Chengyu
Fang, editors, Proceedings of The First International
Conference on Global Interoperability for Language
Resources, pages 67?74, Hong Kong. City Univer-
sity.
Christiane Fellbaum and J. Grabowski. 1997. Anal-
ysis of a hand-tagging task. In Proceedings of
the ACL/Siglex workshop. Association for Compu-
tational Linguistics.
Christane Fellbaum, editor. 1998. WordNet. An
electronic lexical database. MIT Press, Cam-
bridge/Mass.
Charles J. Fillmore. 1976. Frame semantics and the
nature of language. Annals of the New York Academy
of Sciences, 280:20?32.
Thierry Fontenelle, editor. 2003. International Jour-
nal of Lexicography?Special Issue on FrameNet,
volume 16. Oxford University Press.
Eduard H. Hovy, Mitch Marcus, Martha Palmer,
Sameer Pradhan, Lance Ramshaw, and Ralph-
Weischedel. 2006. OntoNotes: The 90% solution.
In Proceedings of HLT-NAACL 2006, New York.
Nancy Ide, Randi Reppen, and Keith Suderman. 2002.
The American National Corpus: More than the
web can provide. In Proceedings of the Third
Language Resources and Evaluation Conference
(LREC), pages 839?44, Las Palmas, Canary Islands,
Spain.
Richard Johansson and Pierre Nugues. 2007. LTH:
Semantic structure extraction using nonprojective
dependency trees. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 227?230, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon. In
Seventeenth National Conference on Artificial Intel-
ligence, Austin, TX. AAAI-2000.
127
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87?92,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
1. Frame: Political locales: [CONTAINER POSSESSOR
Byzantine] [LOCALE LAND]
WN: (adj) Byzantine (of or relating to or characteristic
of the Byzantine Empire or the ancient city of Byzan-
tium) (n) domain, demesne, land (territory over which
rule or control is exercised) ?his domain extended into
Europe?; ?he made it the law of the land?
2. Frame: Separating: [WHOLE Byzantine land] was
being DIVIDED [AGENT CNI] [PARTS INI]
WN: (v) divide, split, split up, separate, dissever, carve
up (separate into parts or portions) ?divide the cake into
three equal parts?; ?The British carved up the Ottoman
Empire after World War I?)
3. Frame: Existence: [TIME While Byzantine land was
being divided], THERE WAS [ENTITY no one in con-
trol of the seas]
4. Frame: Be in control: there [was COPULA]
[CONTROLLING ENTITY no one] [in SUPPORT] CON-
TROL [DEPENDENT ENTITY of the seas]
WN: (n) control (power to direct or determine) ?under
control?)
5. Frame: Natural features: [LOCALE SEAS]
WN: (n) sea (a division of an ocean or a large body of
salt water partially enclosed by land)
6. Frame: Causation:
[CAUSE While Byzantine land was being divided, there
was no one in control of the seas], SO [EFFECT pirates
raided towns on many of the islands]
7. Frame: Piracy: [PERPETRATOR PIRATES]
WN: (n) pirate, buccaneer, sea robber, sea rover (some-
one who robs at sea or plunders the land from the sea
without having a commission from any sovereign na-
tion)
8. Frame: Attack: [ASSAILANT pirates] RAIDED
[VICTIM towns on many of the islands]
WN: (v) foray into, raid (enter someone else?s territory
and take spoils) ?The pirates raided the coastal villages
regularly?)
9. Frame: Political locales: [LOCALE TOWNS]
[RELATIVE LOCATION on many of the islands].
WN: (n) town (an urban area with a fixed boundary that
is smaller than a city)
10. Frame: Locative relation: [FIGURE towns] ON
[GROUND many of the islands]
11. Frame: Quantity: [QUANTITY MANY]
[INDIVIDUALS of the islands]
12. Frame: Natural features: [LOCALE ISLANDS]
WN: (n) island (a land mass (smaller than a continent)
that is surrounded by water)
Table 1: FN/WN Annotation of sentence 1
128
1. Frame: Thwarting: To COUNTER [ACTION this],
[PREVENTING CAUSE the populations moved . . . raiding
parties]
WN:(v) anticipate, foresee, forestall, counter (act in
advance of; deal with ahead of time)
2. Frame: Aggregate: [AGGREGATE POPULATIONS]
WN: (n) population (the people who inhabit a terri-
tory or state) ?the population seemed to be well fed and
clothed?
3. Frame: Motion:
[PURPOSE To counter this], [THEME the populations]
MOVED [SOURCE from their homes on the coast]
[GOAL DNI]
WN: (v) move (change residence, affiliation, or place
of employment)
4. Frame: Buildings: [BUILDING HOMES] [PLACE on
the coast]
WN: (n) home, place (where you live at a particular
time) ?deliver the package to my home?
5. Frame: Locative relation: [FIGURE their homes] ON
[GROUND the coast]
6. Frame: Relational natural features:
[FOCAL FEATURE COAST] [RELATIVE LOCATION
DNI]
WN: (n) seashore, coast, seacoast, sea-coast (the shore
of a sea or ocean)
7. Frame: Building:
[PURPOSE To counter this], [AGENT the populations]
. . . BUILT [CREATED ENTITY settlements] [PLACE in-
land], [PLACE out of sight of the raiding parties].
WN: (v) construct, build, make (make by combining
materials and parts)
8. Frame: Locale by use: [LOCALE SETTLE-
MENTS]
WN: (n) village, small town, settlement (a community
of people smaller than a town)
9. Frame: Locative relation: built [FIGURE settle-
ments] [GROUND INLAND]
WN: (adv) inland (towards or into the interior of a re-
gion) ?the town is five miles inland?
10. Frame: Range: . . . out of [DISTANCE SIGHT]
[PARTICIPANT of the raiding parties]
WN: (n) sight, ken (the range of vision) ?out of sight of
land?
11. Frame: Attack: RAIDING [ASSAILANT parties]
WN: (v) foray into, raid (enter someone else?s territory
and take spoils) ?The pirates raided the coastal villages
regularly?
12. Frame: Aggregate: [AGGREGATEPROPERTY raid-
ing] [AGGREGATE PARTIES]
WN: (n) party, company (a band of people associated
temporarily in some activity) ?they organized a party to
search for food?
Table 2: FN/WN Annotation of sentence 2
1. Frame: Creating:
[CAUSE This] CREATED [CREATED ENTITY a pattern
seen today . . . from attack].
WN: (v) create (bring into existence) ?He created a new
movement in painting?
2. Frame: Pattern: PATTERN [DESCRIPTOR seen to-
day throughout the Aegean] [ENTITIES of a small port
(skala) which serves an inland settlement or chora]
WN: (n) practice, pattern (a customary way of opera-
tion or behavior) ?they changed their dietary pattern?
3. Frame: Perception experience: [PHENOMENON a
pattern] SEEN [TIME today] [STATE throughout the
Aegean] [PHENOMENON of a small port . . . from attack].
[PERCEIVER PASSIVE CNI]
WN: (v) witness, find, see (perceive or be contempora-
neous with) ?You?ll see a lot of cheating in this school?
4. Frame: Temporal collocation: [TRAJECTOR EVENT
a pattern seen] [LANDMARK EVENT TODAY]
[TRAJECTOR EVENT throughout the Aegean. . . attack]
WN: (n) today (the present time or age) ?the world
of today? (n) Aegean, Aegean Sea (an arm of the
Mediterranean between Greece and Turkey. . . )
5. Frame: Dimension: [DIMENSION SMALL] [OBJECT
port]
WN: (adj) small, little (limited or below average in
number or quantity or magnitude or extent)
6. Frame: Locale by use: [DESCRIPTOR small]
[LOCALE PORT]
WN: (n) port (a place (seaport or airport) where people
and merchandise can enter or leave a country)
7. Frame: Assistance: [HELPER a small port (skala)]
[HELPER which] SERVES [BENEFITED PARTY an in-
land settlement or chora], [RESULT making it easier to
protect the island from attack]
WN: (v) service, serve (be used by; as of a utility) ?The
sewage plant served the neighboring communities?
8. Frame: Locative relation: [GROUND INLAND]
[FIGURE settlement]
10. Frame: causation: [CAUSE a small port (skala)
which serves an inland settlement or chora], MAK-
ING it [EFFECT easier to protect the island from attack.]
[AFFECTED DNI]
WN: chora: not in WordNet (v) make, get (give certain
properties to something) ?This invention will make you
a millionaire?
11. Frame: Difficulty: EASIER [ACTIVITY to protect
the island from attack]. [EXPERIENCER INI]
WN: (adj) easy (posing no difficulty; requiring little ef-
fort) ?an easy job?; ?an easy victory?
12. Frame: Protecting: [PROTECTION CNI] PROTECT
[ASSET the island] [DANGER from attack]
WN: (v) protect (shield from danger, injury, destruc-
tion, or damage) ?Weatherbeater protects your roof
from the rain?
14. Frame: Attack: from ATTACK. [ASSAILANT DNI]
WN: (n) attack, onslaught, onset, onrush ((military) an
offensive against an enemy (using weapons)) ?the at-
tack began at dawn?
Table 3: FN/WN Annotation of sentence 3
129
Proceedings of the ACL 2010 Conference Short Papers, pages 68?73,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Manually Annotated Sub-Corpus:
A Community Resource For and By the People
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Collin Baker
International Computer Science Institute
Berkeley, California USA
collinb@icsi.berkeley.edu
Christiane Fellbaum
Princeton University
Princeton, New Jersey USA
fellbaum@princeton.edu
Rebecca Passonneau
Columbia University
New York, New York USA
becky@cs.columbia.edu
Abstract
The Manually Annotated Sub-Corpus
(MASC) project provides data and annota-
tions to serve as the base for a community-
wide annotation effort of a subset of the
American National Corpus. The MASC
infrastructure enables the incorporation of
contributed annotations into a single, us-
able format that can then be analyzed as
it is or ported to any of a variety of other
formats. MASC includes data from a
much wider variety of genres than exist-
ing multiply-annotated corpora of English,
and the project is committed to a fully
open model of distribution, without re-
striction, for all data and annotations pro-
duced or contributed. As such, MASC
is the first large-scale, open, community-
based effort to create much needed lan-
guage resources for NLP. This paper de-
scribes the MASC project, its corpus and
annotations, and serves as a call for con-
tributions of data and annotations from the
language processing community.
1 Introduction
The need for corpora annotated for multiple phe-
nomena across a variety of linguistic layers is
keenly recognized in the computational linguistics
community. Several multiply-annotated corpora
exist, especially for Western European languages
and for spoken data, but, interestingly, broad-
based English language corpora with robust anno-
tation for diverse linguistic phenomena are rela-
tively rare. The most widely-used corpus of En-
glish, the British National Corpus, contains only
part-of-speech annotation; and although it con-
tains a wider range of annotation types, the fif-
teen million word Open American National Cor-
pus annotations are largely unvalidated. The most
well-known multiply-annotated and validated cor-
pus of English is the one million word Wall Street
Journal corpus known as the Penn Treebank (Mar-
cus et al, 1993), which over the years has been
fully or partially annotated for several phenomena
over and above the original part-of-speech tagging
and phrase structure annotation. The usability of
these annotations is limited, however, by the fact
that many of them were produced by independent
projects using their own tools and formats, mak-
ing it difficult to combine them in order to study
their inter-relations. More recently, the OntoNotes
project (Pradhan et al, 2007) released a one mil-
lion word English corpus of newswire, broadcast
news, and broadcast conversation that is annotated
for Penn Treebank syntax, PropBank predicate ar-
gument structures, coreference, and named enti-
ties. OntoNotes comes closest to providing a cor-
pus with multiple layers of annotation that can be
analyzed as a unit via its representation of the an-
notations in a ?normal form?. However, like the
Wall Street Journal corpus, OntoNotes is limited
in the range of genres it includes. It is also limited
to only those annotations that may be produced by
members of the OntoNotes project. In addition,
use of the data and annotations with software other
than the OntoNotes database API is not necessar-
ily straightforward.
The sparseness of reliable multiply-annotated
corpora can be attributed to several factors. The
greatest obstacle is the high cost of manual pro-
duction and validation of linguistic annotations.
Furthermore, the production and annotation of
corpora, even when they involve significant scien-
tific research, often do not, per se, lead to publish-
able research results. It is therefore understand-
68
able that many research groups are unwilling to
get involved in such a massive undertaking for rel-
atively little reward.
The Manually Annotated Sub-Corpus
(MASC) (Ide et al, 2008) project has been
established to address many of these obstacles
to the creation of large-scale, robust, multiply-
annotated corpora. The project is providing
appropriate data and annotations to serve as the
base for a community-wide annotation effort,
together with an infrastructure that enables the
representation of internally-produced and con-
tributed annotations in a single, usable format
that can then be analyzed as it is or ported to any
of a variety of other formats, thus enabling its
immediate use with many common annotation
platforms as well as off-the-shelf concordance
and analysis software. The MASC project?s aim is
to offset some of the high costs of producing high
quality linguistic annotations via a distribution of
effort, and to solve some of the usability problems
for annotations produced at different sites by
harmonizing their representation formats.
The MASC project provides a resource that is
significantly different from OntoNotes and simi-
lar corpora. It provides data from a much wider
variety of genres than existing multiply-annotated
corpora of English, and all of the data in the cor-
pus are drawn from current American English so
as to be most useful for NLP applications. Per-
haps most importantly, the MASC project is com-
mitted to a fully open model of distribution, with-
out restriction, for all data and annotations. It is
also committed to incorporating diverse annota-
tions contributed by the community, regardless of
format, into the corpus. As such, MASC is the
first large-scale, open, community-based effort to
create a much-needed language resource for NLP.
This paper describes the MASC project, its corpus
and annotations, and serves as a call for contribu-
tions of data and annotations from the language
processing community.
2 MASC: The Corpus
MASC is a balanced subset of 500K words of
written texts and transcribed speech drawn pri-
marily from the Open American National Corpus
(OANC)1. The OANC is a 15 million word (and
growing) corpus of American English produced
since 1990, all of which is in the public domain
1http://www.anc.org
Genre No. texts Total words
Email 2 468
Essay 4 17516
Fiction 4 20413
Gov?t documents 1 6064
Journal 10 25635
Letters 31 10518
Newspaper/newswire 41 17951
Non-fiction 4 17118
Spoken 11 25783
Debate transcript 2 32325
Court transcript 1 20817
Technical 3 15417
Travel guides 4 12463
Total 118 222488
Table 1: MASC Composition (first 220K)
or otherwise free of usage and redistribution re-
strictions.
Where licensing permits, data for inclusion in
MASC is drawn from sources that have already
been heavily annotated by others. So far, the
first 80K increment of MASC data includes a
40K subset consisting of OANC data that has
been previously annotated for PropBank predi-
cate argument structures, Pittsburgh Opinion an-
notation (opinions, evaluations, sentiments, etc.),
TimeML time and events2, and several other lin-
guistic phenomena. It also includes a handful of
small texts from the so-called Language Under-
standing (LU) Corpus3 that has been annotated by
multiple groups for a wide variety of phenomena,
including events and committed belief. All of the
first 80K increment is annotated for Penn Tree-
bank syntax. The second 120K increment includes
5.5K words of Wall Street Journal texts that have
been annotated by several projects, including Penn
Treebank, PropBank, Penn Discourse Treebank,
TimeML, and the Pittsburgh Opinion project. The
composition of the 220K portion of the corpus an-
notated so far is shown in Table 1. The remain-
ing 280K of the corpus fills out the genres that are
under-represented in the first portion and includes
a few additional genres such as blogs and tweets.
3 MASC Annotations
Annotations for a variety of linguistic phenomena,
either manually produced or corrected from output
of automatic annotation systems, are being added
2The TimeML annotations of the data are not yet com-
pleted.
3MASC contains about 2K words of the 10K LU corpus,
eliminating non-English and translated LU texts as well as
texts that are not free of usage and redistribution restrictions.
69
Annotation type Method No. texts No. words
Token Validated 118 222472
Sentence Validated 118 222472
POS/lemma Validated 118 222472
Noun chunks Validated 118 222472
Verb chunks Validated 118 222472
Named entities Validated 118 222472
FrameNet frames Manual 21 17829
HSPG Validated 40* 30106
Discourse Manual 40* 30106
Penn Treebank Validated 97 87383
PropBank Validated 92 50165
Opinion Manual 97 47583
TimeBank Validated 34 5434
Committed belief Manual 13 4614
Event Manual 13 4614
Coreference Manual 2 1877
Table 2: Current MASC Annotations (* projected)
to MASC data in increments of roughly 100K
words. To date, validated or manually produced
annotations for 222K words have been made avail-
able.
The MASC project is itself producing annota-
tions for portions of the corpus forWordNet senses
and FrameNet frames and frame elements. To de-
rive maximal benefit from the semantic informa-
tion provided by these resources, the entire cor-
pus is also annotated and manually validated for
shallow parses (noun and verb chunks) and named
entities (person, location, organization, date and
time). Several additional types of annotation have
either been contracted by the MASC project or
contributed from other sources. The 220K words
ofMASC I and II include seventeen different types
of linguistic annotation4, shown in Table 2.
All MASC annotations, whether contributed or
produced in-house, are transduced to the Graph
Annotation Framework (GrAF) (Ide and Suder-
man, 2007) defined by ISO TC37 SC4?s Linguistic
Annotation Framework (LAF) (Ide and Romary,
2004). GrAF is an XML serialization of the LAF
abstract model of annotations, which consists of
a directed graph decorated with feature structures
providing the annotation content. GrAF?s primary
role is to serve as a ?pivot? format for transducing
among annotations represented in different for-
mats. However, because the underlying data struc-
ture is a graph, the GrAF representation itself can
serve as the basis for analysis via application of
4This includes WordNet sense annotations, which are not
listed in Table 2 because they are not applied to full texts; see
Section 3.1 for a description of the WordNet sense annota-
tions in MASC.
graph-analytic algorithms such as common sub-
tree detection.
The layering of annotations over MASC texts
dictates the use of a stand-off annotation repre-
sentation format, in which each annotation is con-
tained in a separate document linked to the pri-
mary data. Each text in the corpus is provided in
UTF-8 character encoding in a separate file, which
includes no annotation or markup of any kind.
Each file is associated with a set of GrAF standoff
files, one for each annotation type, containing the
annotations for that text. In addition to the anno-
tation types listed in Table 2, a document contain-
ing annotation for logical structure (titles, head-
ings, sections, etc. down to the level of paragraph)
is included. Each text is also associated with
(1) a header document that provides appropriate
metadata together with machine-processable in-
formation about associated annotations and inter-
relations among the annotation layers; and (2) a
segmentation of the primary data into minimal re-
gions, which enables the definition of different to-
kenizations over the text. Contributed annotations
are also included in their original format, where
available.
3.1 WordNet Sense Annotations
A focus of the MASC project is to provide corpus
evidence to support an effort to harmonize sense
distinctions in WordNet and FrameNet (Baker and
Fellbaum, 2009), (Fellbaum and Baker, to appear).
The WordNet and FrameNet teams have selected
for this purpose 100 common polysemous words
whose senses they will study in detail, and the
MASC team is annotating occurrences of these
words in the MASC. As a first step, fifty oc-
currences of each word are annotated using the
WordNet 3.0 inventory and analyzed for prob-
lems in sense assignment, after which the Word-
Net team may make modifications to the inven-
tory if needed. The revised inventory (which will
be released as part of WordNet 3.1) is then used to
annotate 1000 occurrences. Because of its small
size, MASC typically contains less than 1000 oc-
currences of a given word; the remaining occur-
rences are therefore drawn from the 15 million
words of the OANC. Furthermore, the FrameNet
team is also annotating one hundred of the 1000
sentences for each word with FrameNet frames
and frame elements, providing direct comparisons
of WordNet and FrameNet sense assignments in
70
attested sentences.5
For convenience, the annotated sentences are
provided as a stand-alone corpus, with the Word-
Net and FrameNet annotations represented in
standoff files. Each sentence in this corpus is
linked to its occurrence in the original text, so that
the context and other annotations associated with
the sentence may be retrieved.
3.2 Validation
Automatically-produced annotations for sentence,
token, part of speech, shallow parses (noun and
verb chunks), and named entities (person, lo-
cation, organization, date and time) are hand-
validated by a team of students. Each annotation
set is first corrected by one student, after which it
is checked (and corrected where necessary) by a
second student, and finally checked by both auto-
matic extraction of the annotated data and a third
pass over the annotations by a graduate student
or senior researcher. We have performed inter-
annotator agreement studies for shallow parses in
order to establish the number of passes required to
achieve near-100% accuracy.
Annotations produced by other projects and
the FrameNet and Penn Treebank annotations
produced specifically for MASC are semi-
automatically and/or manually produced by those
projects and subjected to their internal quality con-
trols. No additional validation is performed by the
ANC project.
The WordNet sense annotations are being used
as a base for an extensive inter-annotator agree-
ment study, which is described in detail in (Pas-
sonneau et al, 2009), (Passonneau et al, 2010).
All inter-annotator agreement data and statistics
are published along with the sense tags. The re-
lease also includes documentation on the words
annotated in each round, the sense labels for each
word, the sentences for each word, and the anno-
tator or annotators for each sense assignment to
each word in context. For the multiply annotated
data in rounds 2-4, we include raw tables for each
word in the form expected by Ron Artstein?s cal-
culate alpha.pl perl script6, so that the agreement
numbers can be regenerated.
5Note that several MASC texts have been fully annotated
for FrameNet frames and frame elements, in addition to the
WordNet-tagged sentences.
6http://ron.artstein.org/resources/calculate-alpha.perl
4 MASC Availability and Distribution
Like the OANC, MASC is distributed without
license or other restrictions from the American
National Corpus website7. It is also available
from the Linguistic Data Consortium (LDC)8 for
a nominal processing fee.
In addition to enabling download of the entire
MASC, we provide a web application that allows
users to select some or all parts of the corpus and
choose among the available annotations via a web
interface (Ide et al, 2010). Once generated, the
corpus and annotation bundle is made available to
the user for download. Thus, the MASC user need
never deal directly with or see the underlying rep-
resentation of the stand-off annotations, but gains
all the advantages that representation offers. The
following output formats are currently available:
1. in-line XML (XCES9), suitable for use with
the BNCs XAIRA search and access inter-
face and other XML-aware software;
2. token / part of speech, a common input for-
mat for general-purpose concordance soft-
ware such as MonoConc10, as well as the
Natural Language Toolkit (NLTK) (Bird et
al., 2009);
3. CONLL IOB format, used in the Confer-
ence on Natural Language Learning shared
tasks.11
5 Tools
The ANC project provides an API for GrAF an-
notations that can be used to access and manip-
ulate GrAF annotations directly from Java pro-
grams and render GrAF annotations in a format
suitable for input to the open source GraphViz12
graph visualization application.13 Beyond this, the
ANC project does not provide specific tools for
use of the corpus, but rather provides the data in
formats suitable for use with a variety of available
applications, as described in section 4, together
with means to import GrAF annotations into ma-
jor annotation software platforms. In particular,
the ANC project provides plugins for the General
7http://www.anc.org
8http://www.ldc.upenn.edu
9XML Corpus Encoding Standard, http://www.xces.org
10http://www.athel.com/mono.html
11http://ifarm.nl/signll/conll
12http://www.graphviz.org/
13http://www.anc.org/graf-api
71
Architecture for Text Engineering (GATE) (Cun-
ningham et al, 2002) to input and/or output an-
notations in GrAF format; a ?CAS Consumer?
to enable using GrAF annotations in the Un-
structured Information Management Architecture
(UIMA) (Ferrucci and Lally, 2004); and a corpus
reader for importing MASC data and annotations
into NLTK14.
Because the GrAF format is isomorphic to in-
put to many graph-analytic tools, existing graph-
analytic software can also be exploited to search
and manipulate MASC annotations. Trivial merg-
ing of GrAF-based annotations involves simply
combining the graphs for each annotation, after
which graph minimization algorithms15 can be ap-
plied to collapse nodes with edges to common
subgraphs to identify commonly annotated com-
ponents. Graph-traversal and graph-coloring al-
gorithms can also be applied in order to iden-
tify and generate statistics that could reveal in-
teractions among linguistic phenomena that may
have previously been difficult to observe. Other
graph-analytic algorithms ? including common
sub-graph analysis, shortest paths, minimum span-
ning trees, connectedness, identification of artic-
ulation vertices, topological sort, graph partition-
ing, etc. ? may also prove to be useful for mining
information from a graph of annotations at multi-
ple linguistic levels.
6 Community Contributions
The ANC project solicits contributions of anno-
tations of any kind, applied to any part or all of
the MASC data. Annotations may be contributed
in any format, either inline or standoff. All con-
tributed annotations are ported to GrAF standoff
format so that they may be used with other MASC
annotations and rendered in the various formats
the ANC tools generate. To accomplish this, the
ANC project has developed a suite of internal tools
and methods for automatically transducing other
annotation formats to GrAF and for rapid adapta-
tion of previously unseen formats.
Contributions may be emailed to
anc@cs.vassar.edu or uploaded via the
ANC website16. The validity of annotations
and supplemental documentation (if appropriate)
are the responsibility of the contributor. MASC
14Available in September, 2010.
15Efficient algorithms for graph merging exist; see,
e.g., (Habib et al, 2000).
16http://www.anc.org/contributions.html
users may contribute evaluations and error reports
for the various annotations on the ANC/MASC
wiki17.
Contributions of unvalidated annotations for
MASC and OANC data are also welcomed and are
distributed separately. Contributions of unencum-
bered texts in any genre, including stories, papers,
student essays, poetry, blogs, and email, are also
solicited via the ANC web site and the ANC Face-
Book page18, and may be uploaded at the contri-
bution page cited above.
7 Conclusion
MASC is already the most richly annotated corpus
of English available for widespread use. Because
the MASC is an open resource that the commu-
nity can continually enhance with additional an-
notations and modifications, the project serves as a
model for community-wide resource development
in the future. Past experience with corpora such
as the Wall Street Journal shows that the commu-
nity is eager to annotate available language data,
and we anticipate even greater interest in MASC,
which includes language data covering a range of
genres that no existing resource provides. There-
fore, we expect that as MASC evolves, more and
more annotations will be contributed, thus creat-
ing a massive, inter-linked linguistic infrastructure
for the study and processing of current American
English in its many genres and varieties. In addi-
tion, by virtue of its WordNet and FrameNet anno-
tations, MASC will be linked to parallel WordNets
and FrameNets in languages other than English,
thus creating a global resource for multi-lingual
technologies, including machine translation.
Acknowledgments
The MASC project is supported by National
Science Foundation grant CRI-0708952. The
WordNet-FrameNet algnment work is supported
by NSF grant IIS 0705155.
References
Collin F. Baker and Christiane Fellbaum. 2009. Word-
Net and FrameNet as complementary resources for
annotation. In Proceedings of the Third Linguistic
17http://www.anc.org/masc-wiki
18http://www.facebook.com/pages/American-National-
Corpus/42474226671
72
Annotation Workshop, pages 125?129, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, 1st edition.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: A
framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of ACL?02.
Christiane Fellbaum and Collin Baker. to appear.
Aligning verbs in WordNet and FrameNet. Linguis-
tics.
David Ferrucci and Adam Lally. 2004. UIMA: An
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10(3-4):327?348.
Michel Habib, Christophe Paul, and Laurent Viennot.
2000. Partition refinement techniques: an interest-
ing algorithmic tool kit. International Journal of
Foundations of Computer Science, 175.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural Language Engineering, 10(3-4):211?225.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-
based format for linguistic annotations. In Proceed-
ings of the Linguistic Annotation Workshop, pages
1?8, Prague, Czech Republic, June. Association for
Computational Linguistics.
Nancy Ide, Collin Baker, Christiane Fellbaum, Charles
Fillmore, and Rebecca Passonneau. 2008. MASC:
The Manually Annotated Sub-Corpus of American
English. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
Nancy Ide, Keith Suderman, and Brian Simms. 2010.
ANC2Go: A web application for customized cor-
pus creation. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC), Valletta, Malta, May. European Lan-
guage Resources Association.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and
Nancy Ide. 2009. Making sense of word sense
variation. In SEW ?09: Proceedings of the Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 2?9, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Rebecca Passonneau, Ansaf Salleb-Aouissi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense an-
notation of polysemous words by multiple annota-
tors. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC), Valletta, Malta.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A unified relational
semantic representation. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 517?526, Washington, DC, USA.
IEEE Computer Society.
73
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 45?50,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse
Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{josefr,csporled}@coli.uni-sb.de
Roser Morante
CNTS
University of Antwerp
Roser.Morante@ua.ac.be
Collin Baker
ICSI
Berkeley, CA 94704
collin@icsi.berkeley.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
We describe the SemEval-2010 shared
task on ?Linking Events and Their Partic-
ipants in Discourse?. This task is an ex-
tension to the classical semantic role label-
ing task. While semantic role labeling is
traditionally viewed as a sentence-internal
task, local semantic argument structures
clearly interact with each other in a larger
context, e.g., by sharing references to spe-
cific discourse entities or events. In the
shared task we looked at one particular as-
pect of cross-sentence links between ar-
gument structures, namely linking locally
uninstantiated roles to their co-referents
in the wider discourse context (if such
co-referents exist). This task is poten-
tially beneficial for a number of NLP ap-
plications, such as information extraction,
question answering or text summarization.
1 Introduction
Semantic role labeling (SRL) has been defined as
a sentence-level natural-language processing task
in which semantic roles are assigned to the syntac-
tic arguments of a predicate (Gildea and Jurafsky,
2002). Semantic roles describe the function of the
participants in an event. Identifying the seman-
tic roles of the predicates in a text allows knowing
who did what to whom when where how, etc.
However, semantic role labeling as it is cur-
rently defined misses a lot of information due to
the fact that it is viewed as a sentence-internal
task. Hence, relations between different local se-
mantic argument structures are disregarded. This
view of SRL as a sentence-internal task is partly
due to the fact that large-scale manual annotation
projects such as FrameNet
1
and PropBank
2
typ-
ically present their annotations lexicographically
by lemma rather than by source text.
It is clear that there is an interplay between lo-
cal argument structure and the surrounding dis-
course (Fillmore, 1977). In early work, Palmer et
al. (1986) discussed filling null complements from
context by using knowledge about individual pred-
icates and tendencies of referential chaining across
sentences. But so far there have been few attempts
to find links between argument structures across
clause and sentence boundaries explicitly on the
basis of semantic relations between the predicates
involved. Two notable exceptions are Fillmore and
Baker (2001) and Burchardt et al (2005). Fillmore
and Baker (2001) analyse a short newspaper arti-
cle and discuss how frame semantics could benefit
discourse processing but without making concrete
suggestions of how to model this. Burchardt et al
(2005) provide a detailed analysis of the links be-
tween the local semantic argument structures in a
short text; however their system is not fully imple-
mented either.
With the shared task, we aimed to make a first
step towards taking SRL beyond the domain of
individual sentences by linking local semantic ar-
gument structures to the wider discourse context.
The task addresses the problem of finding fillers
for roles which are neither instantiated as direct
dependents of our target predicates nor displaced
through long-distance dependency or coinstantia-
tion constructions. Often a referent for an unin-
stantiated role can be found in the wider context,
i.e. in preceding or following sentences. An ex-
ample is given in (1), where the CHARGES role
1
http://framenet.icsi.berkeley.edu/
2
http://verbs.colorado.edu/
?
mpalmer/
projects/ace.html
45
(ARG2 in PropBank) of cleared is left empty but
can be linked to murder in the previous sentence.
(1) In a lengthy court case the defendant was
tried for murder. In the end, he was
cleared.
Another very rich example is provided by (2),
where, for instance, the experiencer and the ob-
ject of jealousy are not overtly expressed as depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.
This paper is organized as follows. In Section 2
we define how the concept of Null Instantiation
is understood in the task. Section 3 describes the
tasks to be performed, and Section 4, how they
are evaluated. Section 5 presents the participant
systems, and Section 6, their results. Finally, in
Section 7, we put forward some conclusions.
2 Null Instantiations
The theory of null complementation used here is
the one adopted by FrameNet, which derives from
the work of Fillmore (1986).
3
Briefly, omissions
of core arguments of predicates are categorized
along two dimensions, the licensor and the in-
terpretation they receive. The idea of a licensor
refers to the fact that either a particular lexical item
or a particular grammatical construction must be
present for the omission of a frame element (FE)
to occur. For instance, the omission of the agent in
(3) is licensed by the passive construction.
(3) No doubt, mistakes were made
0
Protagonist
.
The omission is a constructional omission be-
cause it can apply to any predicate with an appro-
priate semantics that allows it to combine with the
passive construction. On the other hand, the omis-
sion in (4) is lexically specific: the verb arrive al-
lows the Goal to be unspecified but the verb reach,
also a member of the Arriving frame, does not.
(4) We arrived 0
Goal
at 8pm.
3
Palmer et al?s (1986) treatment of uninstantiated ?essen-
tial roles? is very similar (see also Palmer (1990)).
The above two examples also illustrate the sec-
ond major dimension of variation. Whereas, in (3)
the protagonist making the mistake is only existen-
tially bound within the discourse (instance of in-
definite null instantiation, INI), the Goal location
in (4) is an entity that must be accessible to speaker
and hearer from the discourse or its context (def-
inite null instantiation, DNI). Finally, note that
the licensing construction or lexical item fully and
reliably determines the interpretation. Whereas
missing by-phrases have always an indefinite in-
terpretation, whenever arrive omits the Goal lexi-
cally, the Goal has to be interpreted as definite, as
it is in (4).
The import of this classification to the task here
is that we will concentrate on cases of DNI, be
they licensed lexically or constructionally.
3 Description of the Task
3.1 Tasks
We originally intended to offer the participants a
choice of two different tasks: a full task, in which
the test set was only annotated with gold stan-
dard word senses (i.e., frames) for the target words
and the participants had to perform role recogni-
tion/labeling and null instantiation linking, and a
NI only task, in which the test set was already
annotated with gold standard semantic argument
structures and the participants only had to recog-
nize definite null instantiations and find links to
antecedents in the wider context (NI linking).
However, it turned out that the basic semantic
role labeling task was already quite challenging
for our data set. Previous shared tasks have shown
that frame-semantic SRL of running text is a hard
problem (Baker et al, 2007), partly due to the fact
that running text is bound to contain many frames
for which no or little annotated training data are
available. In our case the difficulty was increased
because our data came from a new genre and do-
main (i.e., crime fiction, see Section 3.2). Hence,
we decided to add standard SRL, i.e., role recogni-
tion and labeling, as a third task (SRL only). This
task did not involve NI linking.
3.2 Data
The participants were allowed to make use of a va-
riety of data sources. We provided a training set
annotated with semantic argument structure and
null instantiation information. The annotations
were originally made using FrameNet-style and
46
later mapped semi-automatically to PropBank an-
notations, so that participants could choose which
framework they wanted to work in. The data for-
mats we used were TIGER/SALSA XML (Erk
and Pad?o, 2004) (FrameNet-style) and a modified
CoNLL-format (PropBank-style). As it turned
out, all participants chose to work on FrameNet-
style annotations, so we will not describe the Prop-
Bank annotation in this paper (see Ruppenhofer et
al. (2009) for more details).
FrameNet-style annotation of full text is ex-
tremely time-consuming. Since we also had to an-
notate null instantiations and co-reference chains
(for evaluation purposes, see Section 4), we could
only make available a limited amount of data.
Hence, we allowed participants to make use of ad-
ditional data, in particular the FrameNet and Prop-
Bank releases.
4
We envisaged that the participants
would want to use these additional data sets to
train SRL systems for the full task and to learn
something about typical fillers for different roles
in order to solve the NI linking task. The anno-
tated data sets we made available were meant to
provide additional information, e.g., about the typ-
ical distance between an NI and its filler and about
how to distinguish DNIs and INIs.
We annotated texts from two of Arthur Conan
Doyle?s fiction works. The text that served as
training data was taken from ?The Adventure of
Wisteria Lodge?. Of this lengthy, two-part story
we annotated the second part, titled ?The Tiger of
San Pedro?. The test set was made up of the last
two chapters of ?The Hound of the Baskervilles?.
We chose fiction rather than news because we be-
lieve that fiction texts with a linear narrative gen-
erally contain more context-resolvable NIs. They
also tend to be longer and have a simpler structure
than news texts, which typically revisit the same
facts repeatedly at different levels of detail (in the
so-called ?inverted pyramid? structure) and which
mix event reports with commentary and evalua-
tion, thus sequencing material that is understood
as running in parallel. Fiction texts should lend
themselves more readily to a first attempt at inte-
grating discourse structure into semantic role la-
beling. We chose Conan Doyle?s work because
most of his books are not subject to copyright any-
more, which allows us to freely release the anno-
tated data. Note, however, that this choice of data
4
For FrameNet we provided an intermediate release,
FrameNet 1.4 alpha, which contained more frames and lexi-
cal units than release 1.3.
means that our texts come from a different domain
and genre than many of the examples in FrameNet
and PropBank as well as making use of a some-
what older variety of English.
5
Table 1 provides basic statistics of the data sets.
The training data had 3.1 frames per sentence and
the test data 3.2, which is lower than the 8.8 frames
per sentence in the test data of the 2007 SemEval
task on Frame Semantic Structure Extraction.
6
We
think this is mainly the result of switching to a do-
main different from the bulk of what FrameNet
has made available in the way of full-text anno-
tation. In doing so, we encountered many new
frames and lexical units for which we could not
ourselves create the necessary frames and pro-
vide lexicographic annotations. The statistics also
show that null-instantiation is relatively common:
in the training data, about 18.7% of all FEs are
omitted, and in the test set, about 18.4%. Of the
DNIs, 80.9% had an antecedent in the training
data, and 74.2% in the test data.
To ensure a high quality of the annotations, both
data sets were annotated by more than one person
and then adjudicated. The training set was an-
notated independently by two experienced anno-
tators and then adjudicated by the same two peo-
ple. The test set was annotated by three annota-
tors and then adjudicated by the two experienced
annotators. Throughout the annotation and adju-
dication process, we discussed difficult cases and
also maintained a wiki. Additionally, we created a
software tool that checked the consistency of our
annotations against the frame, frame element and
FE-relation specifications of FrameNet and alerted
annotators to problems with their annotations. The
average agreement (F-score) for frame assignment
for pairs of annotators on the two chapters in the
test set ranges from 0.7385 to 0.7870. The agree-
ment of individual annotators with the adjudicated
gold standard ranges from 0.666 to 0.798. Given
that the gold standard for the two chapters features
228 and 229 different frame types, respectively,
this level of agreement seems quite good.
5
While PropBank provides annotations for the Penn Tree-
bank and is thus news-based, the lexicographic annotations
in FrameNet are extracted from the BNC, a balanced cor-
pus. The FrameNet full-text annotations, however, only cover
three domains: news, travel guides, and nuclear proliferation
reports.
6
The statistics in Table 1 and all our discussion of the
data includes only instances of semantic frames and ignores
the instances of the Coreference, Support, and Relativization
frames, which we labeled on the data as auxiliary informa-
tion.
47
data set sentences tokens frame inst. frame types overt FEs DNIs (resolved) INIs
train 438 7,941 1,370 317 2,526 303 (245) 277
test 525 9,131 1,703 452 3,141 349 (259) 361
Table 1: Statistics for the provided data sets
For the annotation of NIs and their links to the
surrounding discourse we created new guidelines
as this was a novel annotation task. We adopted
ideas from the annotation of co-reference informa-
tion, linking locally unrealized roles to all men-
tions of the referents in the surrounding discourse,
where available. We marked only identity rela-
tions but not part-whole or bridging relations be-
tween referents. The set of unrealized roles un-
der consideration includes only the core arguments
but not adjuncts (peripheral or extra-thematic roles
in FrameNet?s terminology). Possible antecedents
are not restricted to noun phrases but include all
constituents that can be (local) role fillers for
some predicate plus complete sentences (which
can sometimes fill roles such as MESSAGE).
4 Evaluation
As noted above, we allowed participants to ad-
dress three different tasks: SRL only, NI only,
full task. For role recognition and labeling we
used a standard evaluation set-up, i.e., accuracy for
role labeling and precision, recall, F-Score for role
recognition.
The NI linkings were evaluated slightly differ-
ently. In the gold standard, we identified refer-
ents for null instantiations in the discourse con-
text. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system is given credit
if the NI is linked to any of these expressions. To
achieve this we create equivalence sets for the ref-
erents of NIs (by annotating coreference chains).
If the NI is linked to any item in the equivalence
set, the link is counted as a true positive. We can
then define NI linking precision as the number
of all true positive links divided by the number of
links made by a system, and NI linking recall as
the number of true positive links divided by the
number of links between an NI and its equivalence
set in the gold standard. NI linking F-Score is
then the harmonic mean between NI linking preci-
sion and recall.
Since it may sometimes be difficult to deter-
mine the correct extent of the filler of an NI, we
score an automatic annotation as correct if it in-
cludes the head of the gold standard filler in the
predicted filler. However, in order to not favor sys-
tems which link NIs to very large spans of text to
maximize the likelihood of linking to a correct ref-
erent, we introduce a second evaluation measure,
which computes the overlap (Dice coefficient) be-
tween the words in the predicted filler (P) of an NI
and the words in the gold standard one (G):
NI linking overlap =
2|P ?G|
|P | + |G|
(5)
Example (6) illustrates this point. The verb
won in the second sentence evokes the Fin-
ish competition frame whose COMPETITION role
is omitted. From the context it is clear that the
competition role is semantically filled by their first
TV debate (head: debate) and last night?s debate
(head: debate) in the previous sentences. These
two expressions form the equivalence set for the
COMPETITION role in the last sentence. Any sys-
tem that would predict a linkage to a filler that
covers the head of either of these two expressions
would score a true positive for this NI. However,
a system that linked to last night?s debate would
have an NI linking overlap of 1 (i.e., 2*3/(3+3))
while a system linking the whole second sentence
Last night?s debate was eagerly anticipated to the
NI would have an overlap of 0.67 (i.e., 2*3/(6+3))
(6) US presidential rivals Republican John
McCain and Democrat Barack Obama
have yesterday evening attacked each
other over foreign policy and the econ-
omy, in [their first TV debate]
Competition
.
[Last night?s debate]
Competition
was ea-
gerly anticipated. Two national flash
polls suggest that [Obama]
Competitor
won
Finish competition
0
Competition
.
5 Participating Systems
While a fair number of people expressed an inter-
est in the task and 26 groups or individuals down-
loaded the data sets, only three groups submitted
48
results for evaluation. Feedback from the teams
that downloaded the data suggests that this was
due to coinciding deadlines and to the difficulty
and novelty of the task. Only the SEMAFOR
group addressed the full task, using a pipeline of
argument recognition followed by NI identifica-
tion and resolution. Two groups (GETARUNS++
and SEMAFOR) tackled the NI only task, and
also two groups, the SRL only task (CLR and SE-
MAFOR
7
).
All participating systems were built upon ex-
isting systems for semantic processing which
were modified for the task. Two of the groups,
GETARUNS++ and CLR, employed relatively
deep semantic processing, while the third, SE-
MAFOR, employed a shallower probabilistic sys-
tem. Different approaches were taken for NI link-
ing. The SEMAFOR group modeled NI linking as
a variant of role recognition and labeling by ex-
tending the set of potential arguments beyond the
locally available arguments to also include noun
phrases from the previous sentence. The system
then uses, among other information, distributional
semantic similarity between the heads of potential
arguments and role fillers in the training data. The
GETARUNS++ group applied an existing system
for deep semantic processing, anaphora resolution
and recognition of textual entailment, to the task.
The system analyzes the sentences and assigns its
own set of labels, which are subsequently mapped
to frame semantic categories. For more details of
the participating systems please consult the sepa-
rate system papers.
6 Results and Analysis
6.1 SRL Task
Argument Recognition Label
Prec. Rec. F1 Acc.
SHA 0.6332 0.3884 0.4812 0.3471
SEM 0.6528 0.4674 0.5448 0.4184
CLR 0.6702 0.1121 0.1921 0.1093
Table 2: Shalmaneser (SHA), SEMAFOR (SEM)
and CLR performance on the SRL task (across
both chapters)
The results on the SRL task are shown in Table
2. To get a better sense of how good the perfor-
mance of the submitted systems was on this task,
7
For SEMAFOR, this was the first step of their pipeline.
we applied the Shalmaneser statistical semantic
parser (Erk and Pad?o, 2006) to our test data and
report the results. Note, however, that we used a
Shalmaneser trained only on FrameNet version 1.3
which is different from the version 1.4 alpha that
was used in the task, so its results are lower than
what can be expected with release 1.4 alpha.
We observe that although the SEMAFOR and
the CLR systems score a higher precision than
Shalmaneser for argument recognition, the SE-
MAFOR system scores considerably higher recall
than Shalmaneser, whereas the CLR system scores
a much lower recall.
6.2 NI Task
Tackling the resolution of NIs proved to be a dif-
ficult problem due to a variety of factors. First,
the NI sub-task was completely new and involves
several steps of linguistic processing. It also is
inherently difficult in that a given FE is not al-
ways omitted with the same interpretation. For
instance, the Content FE of the Awareness frame
evoked by know is interpreted as indefinite in
the blog headline More babbling about what it
means to know but as definite in a discourse
like Don?t tell me you didn?t know!. Second,
prior to this SemEval task there was no full-text
training data available that contained annotations
with all the kinds of information that is relevant
to the task, namely overt FEs, null-instantiated
FEs, resolutions of null-instantiations, and coref-
erence. Third, the data we used also represented
a switch to a new domain compared to existing
FrameNet full-text annotation, which comes from
newspapers, travel guides, and the nuclear pro-
liferation domain. Our most frequent frame was
Observable bodyparts, whereas it is Weapons in
FrameNet full-text. Fourth, it was not well un-
derstood at the beginning of the task that, in cer-
tain cases, FrameNet?s null-instantiation annota-
tions for a given FE cannot be treated in isolation
of the annotations of other FEs. Specifically, null-
instantiation annotations interact with the set of re-
lations between core FEs that FrameNet uses in its
analyses. As an example, consider the CoreSet re-
lation, which specifies that from a set of core FEs
at least one must be instantiated overtly, though
more of them can be. As long as one of the FEs
in the set is expressed overtly, null-instantiation is
not annotated for the other FEs in the set. For
instance, in the Statement frame, the two FEs
49
Topic and Message are in one CoreSet and the
two FEs Speaker and Medium are in another. If
a frame instance occurs with an overt Speaker and
an overt Topic, the Medium and Message FEs are
not marked as null-instantiated. Automatic sys-
tems that treat each core FE separately, may pro-
pose DNI annotations for Medium and Message,
resulting in false positives.
Therefore, we think that the evaluation that we
initially defined was too demanding for a novel
task. It would have been better to give sepa-
rate scores for 1) ability to recognize when a core
FE has to be treated as null-instantiated; 2) abil-
ity to distinguish INI and DNI; and 3) ability to
find antecedents. The systems did have to tackle
these steps anyway and an analysis of the sys-
tem output shows that they did so with different
success. The two chapters of our test data con-
tained a total of 710 null instantiations, of which
349 were DNI and 361 INI. The SEMAFOR sys-
tem recognized 63.4% (450/710) of the cases of
NI, while the GETARUNS++ system found only
8.0% (57/710). The distinction between DNI and
INI proved very difficult, too. Of the NIs that
the SEMAFOR system correctly identified, 54.7%
(246/450) received the correct interpretation type
(DNI or INI). For GETARUNS++, the percentage
is higher at 64.2% (35/57), but also based on fewer
proposed classifications. A simple majority-class
baseline gives a 50.8% accuracy. Interestingly, the
SEMAFOR system labeled many more INIs than
DNIs, thus often misclassifying DNIs as INI. The
GETARUNS++ system applied both labels about
equally often.
7 Conclusion
In this paper we described the SemEval-2010
shared task on ?Linking Events and Their Partic-
ipants in Discourse?. The task is novel, in that it
tackles a semantic cross-clausal phenomenon that
has not been treated before in a task, namely, link-
ing locally uninstantiated roles to their coreferents
at the text level. In that sense the task represents
a first step towards taking SRL beyond the sen-
tence level. A new corpus of fiction texts has been
annotated for the task with several types of seman-
tic information: semantic argument structure, co-
reference chains and NIs. The results scored by
the systems in the NI task and the feedback from
participant teams shows that the task was more dif-
ficult than initially estimated and that the evalua-
tion should have focused on more specific aspects
of the NI phenomenon, rather than on the com-
pleteness of the task. Future work will focus on
modeling the task taking this into account.
Acknowledgements
Josef Ruppenhofer and Caroline Sporleder are supported
by the German Research Foundation DFG (under grant PI
154/9-3 and the Cluster of Excellence Multimodal Comput-
ing and Interaction (MMCI), respectively). Roser Morante?s
research is funded by the GOA project BIOGRAPH of the
University of Antwerp. We would like to thank Jinho Choi,
Markus Dr?ager, Lisa Fuchs, Philip John Gorinski, Russell
Lee-Goldman, Ines Rehbein, and Corinna Schorr for their
help with preparing the data and/or implementing software
for the task. Thanks also to the SemEval-2010 Chairs Katrin
Erk and Carlo Strapparava for their support during the task
organization period.
References
C. Baker, M. Ellsworth, K. Erk. 2007. SemEval-2007
Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.
A. Burchardt, A. Frank, M. Pinkal. 2005. Building text
meaning representations from contextually related
frames ? A case study. In Proceedings of IWCS-6.
K. Erk, S. Pad?o. 2004. A powerful and versatile XML
format for representing role-semantic annotation. In
Proceedings of LREC-2004.
K. Erk, S. Pad?o. 2006. Shalmaneser - a flexible tool-
box for semantic role assignment. In Proceedings of
LREC-06.
C. Fillmore, C. Baker. 2001. Frame semantics for text
understanding. In Proc. of the NAACL-01 Workshop
on WordNet and Other Lexical Resources.
C. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In A. Zampolli, ed.,
Fundamental Studies in Computer Science, No. 59,
55?88. North Holland Publishing.
C. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual
Meeting of the Berkeley Liguistics Society.
D. Gildea, D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, J. Dowding. 1986. Recovering im-
plicit information. In Proceedings of ACL-1986.
M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker,
M. Palmer. 2009. Semeval-2010 task 10: Linking
events and their participants in discourse. In The
NAACL-HLT 2009 Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions
(SEW-09).
50
Proceedings of the Fifth Law Workshop (LAW V), pages 30?37,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
How Good is the Crowd at ?real? WSD?
Jisup Hong
International Computer Science Institute
Berkeley, CA
jhong@icsi.berkeley.edu
Collin F. Baker
International Computer Science Institute
Berkeley, CA
collinb@icsi.berkeley.edu
Abstract
There has been a great deal of excitement re-
cently about using the ?wisdom of the crowd?
to collect data of all kinds, quickly and cheaply
(Howe, 2008; von Ahn and Dabbish, 2008).
Snow et al (Snow et al, 2008) were the first
to give a convincing demonstration that at least
some kinds of linguistic data can be gathered
from workers on the web more cheaply than
and as accurately as from local experts, and
there has been a steady stream of papers and
workshops since then with similar results. e.g.
(Callison-Burch and Dredze, 2010).
Many of the tasks which have been success-
fully crowdsourced involve judgments which
are similar to those performed in everyday life,
such as recognizing unclear writing (von Ahn
et al, 2008), or, for those tasks that require con-
siderable judgment, the responses are usually
binary or from a small set of responses, such
as sentiment analysis (Mellebeek et al, 2010)
or ratings (Heilman and Smith, 2010). Since
the FrameNet process is known to be relatively
expensive, we were interested in whether the
FrameNet process of fine word sense discrimi-
nation and marking of dependents with seman-
tic roles could be performed more cheaply and
equally accurately using Amazon?s Mechanical
Turk (AMT) or similar resources. We report on
a partial success in this respect and how it was
achieved.
1 Defining the task
The usual FrameNet process for annotating exam-
ples of a particular lexical unit (LU), is to first ex-
tract examples of this sense from a corpus, based on
collocational and syntactic patterns, storing them in
subcorpora; this process is called subcorporation.
Given an LU, vanguarders begin by composing rules
consisting of syntactic patterns and instructions as
to whether to include or exclude the sentences that
match them. An automated system extracts sentences
containing uses of the LU?s lemma, applies POS tag-
ging and chunk parsing, and then matches the sen-
tences against the rules in their specified order to al-
low for cascading effects. Ultimately, the result is a
set of subcorpora, each corresponding to a pattern,
and containing sentences likely to exhibit a use of
the LU. More recently, a system has been developed
in collaboration with the Sketch Engine ((Kilgarriff
et al, July 2004) http://www.sketchengine.
co.uk) to accelerate this process by giving annota-
tors a graphical interface in which precomputed col-
locational pattern matches can be more directly as-
signed to the various LUs corresponding to a given
lemma. The actual annotation of the frame ele-
ments (FEs) is facilitated by having pre-selected sets
of sentences which are at least likely to contain the
right sense of the word, and which share a syntac-
tic pattern. Therefore, we first focused on the frame
discrimination task (which in other contexts would
be called word sense discrimination), which we as-
sumed to be simpler to collect data for than the FE
annotation task, and which is a prerequisite for it.
We began by evaluating the resources that AMT
provides for designing and implementingHuman In-
telligence Tasks (HITs); we quickly determined that
the UI provided by AMT would not suffice for the
task we planned. Specifically, it lacks the ability to:
? randomize the selection options,
30
? present questions from a set one at a time,
? randomize the order in which a set of questions
are presented, or
? record response times for each question.
We therefore decided to design our HITs using
Amazon?s ?External Question HIT Type?, and to
serve the HITs from our own web server. In this sys-
tem, when workers view or execute a HIT, the con-
tent of the HIT window is supplied from our server,
and responses are stored directly in a database run-
ning our own server, rather than Amazon?s. Workers
log in through AMT and are ultimately paid through
AMT, but the content of the tasks can be completely
controlled though our web server.
The Frame Discrimination Task can be set up in a
number of ways, such as:
1. Present a single sentence with the lemma high-
lighted. Workers must select a frame (or ?none
of the above?) from a multiple-choice list of
frames we provide.
2. Present a list of sentences all containing uses of
the same lemma. Workers must check off all the
sentences that contain uses of a given frame.
3. Present a list of sentences all containing uses of
the same lemma. Provide one example sentence
from each frame and ask users to categorize the
sentences.
In order to get started as quickly as possible and
get a baseline result, we chose the first of the above
methods, which is the most straightforward from a
theoretical point of view. For example, the lemma
might be gain.v, which has two LUs, one in the
Change position on a scale frame, and another in
the Getting frame. The HIT displays one sentence at
a time, with the lemma highlighted; below the sen-
tence, a multiple-choice selection is presented with
the Frame names:
You will have to GAIN their support,
if change is to be brought about.
Change_position_on_a_scale
Getting
None of the above
When users mouse-over the name of a frame,
a pop-up displays an example sentence from that
Frame (from a different LU in the same frame). Users
can also click the name of the frame, which causes
the browser to open another window with the frame
definition. This process repeats for 12 sentences, at
which point the HIT is over, and results are entered
into our database.
Sources of material for testing
We had no shortage of sentences for the frame
discrimination task; we started with some of the
many unannotated sentences already in the FrameNet
database. In the usual process of subcorporation,
each of the subcorpora matches one specific pattern;
the goal is to extract roughly 20 examples of each
collocational/syntactic pattern, and to annotate one or
two of each. The following are examples from among
the patterns used for rip.v in the Removing frame:
NP T NP [PP f="from"]
NP T NP [w "out"]
The first pattern would match sentences like, ?I
ripped the top from my pack of cigarettes,? and the
second, ?She ripped the telephone out of the wall.?
We do not presume, however, that we will al-
ways be able to define patterns for all of the possi-
ble valences of a predicator, so we also include two
?other? subcorpora. The first of these (named ?other-
matched?) contains 50 sentences (provided there are
enough instances in the corpus) which matched any
one of the preceding patterns but were left over af-
ter 20 had been extracted for each pattern. The sec-
ond (?other-unmatched?) contains sentences in which
the lemma occurs (with the right POS) which did
not match any of the earlier patterns. Vanguarders
carefully check these ?other? subcorpora to see if the
lemma is used in a syntactic valence which was not
foreseen; if they find any such new valences, they
are annotated. Typically, this means that there are
roughly 100 extra unannotated sentences for each
LU. For this experiment, we extracted 10 sentences
from the ?other-matched? subcorpus of each of the
LUs for the lemma, meaning that they had already
matched some pattern which was designed for one of
those LUs. In addition to the unannotated sentences,
we randomly selected three annotated sentences from
each LU, two to use as included gold-standard items
31
Frame name Example
Cause to fragment The revolution has RIPPED thousands of Cuban families apart . . .
Damaging . . .Mo?s dress is RIPPED by a drunken admirer.
Removing Sinatra then reportedly RIPPED the phone out of the wall . . .
Self motion A tornado RIPPED through Salt Lake City . . .
Judgment communication (no annotated examples?related to rip into.v)
Position on a scale Eggs, shellfish and cheese are all HIGH in cholesterol . . .
Dimension An adult tiger stands at least 3 ft (90 cm) HIGH at the shoulder.
Intoxication Exhausted but HIGH on adrenalin, he would roam about the house. . .
Measurable attributes Finally we came to a HIGH plastic wall.
Evidence Our results SHOW that unmodified oligonucleotides can provide . . .
Reasoning He uses economics to SHOW how this is so.
Obviousness . . . sighting black mountain tops SHOWING through the ice-cap.
Cotheme When they were SHOWN to their table, . . .
Finish competition (no annotated examples? Fair Lady placed in the second race at Aqueduct.)
Cause to perceive A second inner pylon SHOWS Ptolemy XIII paying homage to Isis . . .
Table 1: LUs (senses) for rip.v, high.a, and show.v
for checking accuracy, and one to use as the exam-
ple in the preview of the HIT. These sentences were
randomized and separated into batches of 12 for each
HIT; all of which were inserted into a database on a
local web server. A local CGI script (reached from
AMT) calls the database for the examples in each
HIT and stores the workers? responses in the same
database.
We ran three trials under this setup, for the lem-
mas rip.v, high.a, and show.v. Based on the success
of earlier studies, our concern initially was to make
our tasks be sufficiently challenging so as to be use-
ful for evaluating AMT. Thus, we chose lemmas with
four to five senses rather than just two or three. In
addition, for these three lemmas, each of the senses
appears with sufficient frequency in the corpus so
that all senses are realistically available for consid-
eration.1 The frames for each of these lemmas are
shown in Table 1; some of these distinctions are fairly
subtle; we will discuss some examples below.
To combine responses, we took the modal response
as the result for each item; in cases of ties, we chose
randomly, and split the response count where neces-
sary. On this basis, for rip.v, the workers had an ac-
curacy of 32.16 correct out of 48 items (67%), for
1An exception is the show.v in the Finish competition
frame, which we excluded for this reason, as in Mucho Macho
Man showed in the 2011 Kentucky Derby.
high.a, they got 22 out of 49 correct (46%), and for
show.v, 37 out of 60 items (62%), as shown in Ta-
ble 2. If we consider that FrameNet has four senses
(LUs) for rip.v and high.a and five for show.v, this
might not sound too awful, but if we think of this as
pre-processing, so that the resulting sentences can be
annotated in the correct frame, it leaves a lot to be
desired. If we raise the agreement criteria, by filter-
ing out items on which the margin between the modal
response and the next highest is 35% or greater (i.e.
those with high agreement among workers), we can
get higher accuracy (shown in the right two columns
of Table 2), at the expense of failing to classify 3/4
of the items, hardly a solution to the problem.
Trials with CrowdFlower
We decided to try our task on CrowdFlower (http:
//crowdflower.com, formerly Dolores Labs), a
company that provides tools and custom solutions to
make crowdsourcing tasks easier to create and man-
age, including techniques to assure a certain level of
quality in the results. While working with Crowd-
Flower, our tasks were running on AMT, although
CrowdFlower also provides other labor pools, such as
Samasource (http://www.samasource.org),
depending on the nature of the task. We tried run-
ning the task for rip.v on Crowdflower?s system, us-
ing the same HIT design as before, (recreated using
32
Lemma No. senses No. Items Accuracy Filtered Items Accuracy.
rip.v 4 48 67% 10 90%
high.a 4 48 46% 12 58%
show.v 5 60 62% 11 64%
Table 2: Results from Trial 1: Rip.v, high.a and show.v
their self-serve UI design tools), but with different
sentences. Once again, we selected 12 sentences for
each of the 4 LUs, for a total of 48 sentences. We
wanted to collect 10 judgments per sentence, for a
total of 480 judgments. Of the 12 sentences in each
HIT, 2 were already annotated and used as a gold
standard.
However, after starting this job, we found that the
CrowdFlower system automatically halted the jobs
after a few hours due to poor average performance on
the gold standard items. After having the job halted
repeatedly, we were finally able to force it to finish
by suspending use of the gold standard to judge ac-
curacy. In other words, the system was telling us that
the task was too hard for the workers.
Revised CrowdFlower Trials
After our difficulties with the first trial on Crowd-
Flower?s system, we visited their offices for an
on-site consultation. We learned more about how
CrowdFlower?s system works, and received sugges-
tions on how to improve performance:
? Run a larger set of data; they recommended at
least 200 sentences for a job.
? Embed 20% gold standard items so that there is
at least one per page of questions, since, without
gold standard items, workers will answer ran-
domly, or always choose the first option.
? Get rid of the frame names and use something
easier to understand.
? Provide more detailed instructions that include
examples.
Based on this consultation, we made the follow-
ing changes in our HITs: (1) Replaced frame names
with hand-crafted synonyms, (2) Renamed the task
and rewrote all instructions to avoid jargon, (3) Re-
moved links and roll-overs giving examples or refer-
ring people to external documentation, and (4) Ex-
tracted 60 sentences per LU, of which 10 are gold
standard.
Although we planned to do this for rip.v, high.a,
and show.v, we found that it was too difficult to come
up with synonyms for high.a, so we ran trials only for
rip.v and show.v. For rip.v, with four senses, we col-
lected 10 judgments each on 240 sentences, for a to-
tal of 2400 judgments. For show.v, with five senses,
we collected 10 judgments each on 300 sentences,
for a total of 3000 judgments. In the final trials,
the weighted majority response provided by Crowd-
Flower was found to be correct 75% for rip.v and
80% for show.v. This was encouraging, but we were
concerned with the limitations of this method: (1)
The calculation used to select the ?weighted major-
ity response? is proprietary to CrowdFlower, so that
we could not know the details or change it, and (2)
the final trials required handcrafted definitions, syn-
onyms, and very clear definitions for each LU, which
is at best time-consuming, and sometimes impossible
(as is likely case for high.a), meaning the method will
not scale well. As researchers, the first limitation is
especially problematic as it is necessary to know ex-
actly what methods we are using in our research and
be able to share them openly. For these reasons, we
decided to go back to building our own interfaces on
AMT, and to look for approaches that would be more
automatic.
Return to AMT
We redesigned the HIT around a pile-sorting model;
instead of seeing one sentence and choosing between
frames (whether by name or by synonym), workers
are shown model sentences for each LU (i.e. in each
frame), and then asked to categorize a list of sen-
tences that are displayed all at once. Consequently,
the worker generates a set of piles each correspond-
ing to a frame/LU. The advantages of this approach
are as follows:
? Workers can more easily exploit paradigmatic
33
contrasts across sentences to decide which cate-
gory to put them in.
? Workers can recategorize sentences after ini-
tially putting them into a pile.
? Workers have example sentences using the LUs
in question, which constitutes more information
than the frame name (assuming that they were
not going to the FrameNet website to peruse an-
notation).
? HITs can be generated automatically, without us
having to manually create synonyms for each
LU, which turned out to be quite difficult.
This approach, however, does have some disadvan-
tages:
? We need to pre-annotate at least 1 sentence per
LU in order to have example sentences.
? Having lots of sentences presented at once clut-
ters up the screen and requires scrolling.
? The HIT interface is much more complex and
potentially more fragile.
Because of the complexity of the new interface and
the increased screen space required for each addi-
tional sense, we decided to begin trials on the lemma
justify.v which (we believe) has just two senses, but
still requires a fairly difficult distinction, between the
Deserving frame, as in The evolutionary analogy is
close enough to JUSTIFY borrowing the term, . . .
and the Justifying frame, as in This final section al-
lows Mr Hicks to JUSTIFY the implementation of abc
as. . . . These two sentences were were annotated in
the FrameNet data, and were randomly selected to
serve as the models for the workers, illustrating the
danger of choosing randomly in such cases!
For all HITs, the sentences were randomized in or-
der, as well as the order of the example sentences.
Example sentences retained the same colors, i.e.
the frame/color correspondence was kept constant,
so as not to confuse workers working on multiple
HITs. Sentences were horizontally aligned so that
the highlighted target word was centered and verti-
cally aligned across the sentences. Each sentence had
a drop-down box to its right where workers could se-
lect a category to place it in. Each sense category was
represented by a model sentence with the frame name
as a label for the category. We collected 10 judgments
each on 132 sentences, with workers being asked to
categorize 18 sentences in each HIT. In the first trial,
accuracy was 55%. In trial 2, the model sentences
were modified to also show frame element annota-
tion, in the hope that the fact that the Justifying uses
have an Agent as the subject, while the Deserving
uses have a State of affairs as the subject would be
clearer. An image of the HIT interface, with FE an-
notation displayed on the model sentences, is shown
in Figure 1. Despite the added information, accuracy
decreased to 45%.
Qualifying the prospects
In trial 3, we kept the HIT interface the same, includ-
ing the model sentences, but added (1) a qualification
test that was designed to evaluate the worker?s ability
in English, (2) required that the workers have regis-
tered a US address with Amazon and (3) required that
workers have an overall HIT acceptance rate greater
than 75%. Although over 100 workers took the qual-
ification test, no workers accepted the HIT. In trial
4 we raised the rate of pay to $.25/HIT, but still got
only 1 worker.
On the suspicion that our problem was partially
caused by not having enough HITs to make it worth
the workers? time to do them, in Trial 5 we posted the
same HITs 3 times, amounting to 24 HITs, worth $6,
from a worker?s point of view; this raised the num-
ber of workers to 5 for all three HITs. Through the
HITs completed by those workers, we collected 1 to
2 judgments on 107 of the 132 sentences posted, with
63% accuracy overall, and 86% accuracy on the gold
sentences. Looking at their answers for each frame,
workers correctly categorized 93% of cases of Justi-
fying but only 52% of cases of Deserving.
In trial 6, we then customized the instructions (this
time automatically, rather than manually) to refer to
the lemma specifically rather than via a generic de-
scription like ?the highlighted word.? In addition, we
removed the qualification test so as to make our HITs
available to a much larger pool of workers, but kept
the other two requirements. We ran HITs again with
18 sentences each, 2 of which were gold. We decided
to try a different lemma with two sense distinctions,
top.a, and to make it more worthwhile for workers
to annotate our data by posting HITs simultaneously
34
5/11/11 1:15 AMPut Sentences into Groups
Page 1 of 2http://framenet.icsi.berkeley.edu:22222/mturkdev/fnsortui_fe_oc.php?assignmentId=ASSIGNMENT_ID_NOT_AVAILABLE&hitId=2OKJENLVWJ5O4YNGNPQ81MWB8BN0S2
This is only a preview. Please accept this HIT before working on it.
Put Sentences into Groups
Instructions: (click to show)
Groups:
The evolutionary analogy is close enough to JUSTIFY borrowing the term , and I make no ...
Deserving
State_of_affairs
Action
3.
... ; certainly their expected sales would not
have
JUSTIFIED their production .
... final section allows Mr Hicks to JUSTIFY the implementation of abc as a better ...
Justifying
Agent
Act
2. uh-huh i could never JUSTIFY owning a personal computer at at home
   None_of_the_above
Sentences to Group: 16 remaining
1. ... US is that there is not enough information yet to JUSTIFY expensive remedial action .
4. ... this extent , the fascination of the experiments is JUSTIFIED .
5.
... were pursued vigorously and with a vengeance
morally
JUSTIFIED
by the offender 's wickedness , then ` our " society
...
6. ... making the point , it does apply but it has to be JUSTIFIED .
7. How does Ormrod J. JUSTIFY his decision ?
8.
that there are some searches the war on drugs can
not
JUSTIFY . ``
9.
... taken care to make just enough extreme
statements to
JUSTIFY
his ' credentials ' with outright racists and neo-
Nazis ...
Change group
Change group
Choose
group
Choose
group
Choose
group
Choose
group
Choose
group
Choose
group
Choose
group
Deserving
Justifying
None_of_the_above
Figure 1: HIT Screen for justify.v (after two sentences have been categorized)
for rip.v and high.a. We posted 8 HITs for top.a, 16
HITs for high.a and 16 for rip.v, for a total of 40 HITs
across all three lemmas, paying $.15/HIT and collect-
ing 10 assignments/HIT.
These results were much more satisfactory, with
accuracy as shown in Table 3. Filtering out items
by raising the agreement criteria (as before) to 35%
or greater between the modal response and the next
highest, yielded even better accuracy, above 90% for
all three lemmas, at the cost of failing to classify ap-
proximately 10% to 30% of the items.
In response to the relative success of this trial, we
posted HITs for three additional lemmas: thirst.n,
range.n, and history.n, with 3, 4, and 5 senses, respec-
tively. We chose these lemmas to ascertain whether
there would be an effect on performance from the
number of senses. Thus all three lemmas were also of
the name POS. For Trial 7, although we kept the same
interface, we experimented with changing the pay,
and offering bonuses in an effort to maintain good
standing among AMT workers concerned with their
HIT acceptance record. For previous HITs, workers
had to correctly categorize both gold sentences in or-
der to receive any payment. We changed this sys-
tem so that the HIT is accepted if the worker catego-
rizes 1 gold sentence correctly, and awards a bonus
if they categorize both correctly. Our hope was that
this change would enable us to experiment with post-
ing difficult HITs without losing our credibility. The
results from this trial, also presented in Table 3, show
accuracy at 92%, 87%, and 73%, respectively for
thirst.n, range.n, and history.n. These results seemed
to suggest that increasing the number of senses to dis-
criminate increases the difficulty of the HIT.
It will be recalled that on every item, the work-
ers have a choice ?none of the above?. One of
the difficulties is that this choice covers a variety of
cases, including those where the word is the wrong
part of speech (a fairly frequent occurrence, despite
the high accuracy cited for POS tagging) and those
where the needed sense has simply not been included
in FrameNet. The latter was the case for the word
range.n, which was run once with three senses and
then again with five senses, after the LUs for (firing,
artillery) range and the ?stove? sense were added.
With the two additional senses, the accuracy actually
went up from 87% to 92%. Although it is possible
that the improvement could be due to a training ef-
fect connected to an increase in the number of items,
it suggests that having more sense distinctions does
not necessarily increase difficulty of discrimination.
35
Lemma No. senses No. Items Accuracy Filtered Items Accuracy
top.a 2 144 92% 134 96%
rip.v 4 288 85% 228 92%
high.a 4 288 80% 198 92%
thirst.n 2 144 92% 128 95%
range.n 3 216 87% 177 93%
history.n 4 288 73% 199 86%
range.n 5 360 92% 335 96%
Table 3: Results from recent trials, including accuracy after filtering on the basis of agreement
Removing Cause to fragment Self motion Damaging None of the above
N= 104 51 33 64 36
Removing 97 93 1 1 2 0
Cause to fragment 45 1 41 0 1 2
Self motion 25 1 0 24 0 0
Damaging 84 8 9 7 58 2
None of the above 37 1 0 1 3 32
Table 4: Confusion matrix for rip.v (rows=gold standard)
2 What we can learn from the Turkers?
difficulties?
Consider the confusion matrix shown in Table 4; here
each row represents the items grouped by the gold
standard sense (?expected?); each column represents
the items grouped by the most frequent worker judg-
ment (?observed?).
The accuracy on this HIT set was 85%, in accord
with the much larger numbers along the diagonal, but
the really interesting cases lie off the diagonal, where
the plurality of the workers disagreed with the ex-
perts. In some cases, the workers are simply right,
and the expert was wrong, as in This new wave of
anonymous buildings . . . has RIPPED the heart out of
Hammersmith., which the gold standard has as Dam-
aging, but where the workers voted 7 to 3 for Re-
moving. In this case, the expert vanguarder appears
to have classified the metaphorical use of rip.v using
the target domain, rather than the source domain, as
is the FrameNet policy on ?productive? (rather than
?lexicalized?) metaphor (Ruppenhofer et al, 2006,
Sec. 6.4)2. In practice, this classification would most
likely have been corrected at the annotation phase, as
the FEs are clearly those of the source domain, in-
2Available from the FrameNet website, http:
//framenet.icsi.berkeley.edu.
volving removing something (a Theme) out of some-
thing else (a Source). In other cases, such as I ripped
open the envelopes., the gold standard correctly has
Damaging, while the workers have 4 Removing, 3
Cause to fragment, and 3 Damaging. There is a
good possibility that the envelopes fragmented (al-
though this is not implied, nor necessary to remove a
letter from an envelope), and the purpose is likely to
remove something from the envelopes, which might
falsely suggest Removing.
In other cases, the senses are so closely enmeshed,
that is seems rather arbitrary to choose one: e.g. I
RIP up an old T-shirt of mine and offer it. The shirt
is certainly damaged and almost certainly fragmented
as a result of the same action. . . . the Oklahoma was
RIPPED apart when seven torpedoes hit her. strictly
speaking, the ship is caused to fragment, but the mil-
itary purpose is to damage her beyond repair, if pos-
sible. And there are fairly often examples where the
sentence in isolation is ambiguous: Rain RIPPED an-
other piece of croissant, The sky RIPPED and hung
in tatters , revealing plasterboard and lath behind.
Such cases are pushing us toward trying to incorpo-
rate blending of senses into our paradigm, along the
lines of (Erk and McCarthy, 2009).
36
3 Conclusion
We have shown that it is possible to set up HITs on
Amazon Mechanical Turk to discriminate the fairly
fine sense distinctions used in FrameNet, if the right
approach is taken, and that the results reach a level
of accuracy that can be useful for further processing,
as well as serving as a cross-check on the expert data
and an invitation to re-think the task itself. Although
the total amount of data collected may not be large
by some standards, it has been sufficient to give a
good sense of which techniques work for the type of
WSD problems we are facing. We intend to continue
investigating the general applicability of this system
for frame disambiguation, including further analysis
of our data to better understand the factors that make
a disambiguation task more or less difficult for crowd
workers. All the data collected in the course of this
study, and the software used to collect and analyze it,
will be made available on the FrameNet website.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
0947841 (CISE EAGER) ?Crowdsourcing for NLP?;
the Sketch Engine GUI was developed under NSF
Grant IIS-00535297 ?Rapid Development of a
Frame-Semantic Lexicon?.
References
Chris Callison-Burch and Mark Dredze, editors. 2010.
Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, Los Angeles, June. Association for
Computational Linguistics.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 440?449, Singapore, August. Associa-
tion for Computational Linguistics.
Michael Heilman and Noah A. Smith. 2010. Rating
computer-generated questions with Mechanical Turk.
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 35?40, Los Angeles, June. As-
sociation for Computational Linguistics.
Jeff Howe. 2008. Crowdsourcing. Crown Business, New
York.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. July 2004. The Sketch Engine. In Proceed-
ings of EURALEX 2004, Lorient, France.
Bart Mellebeek, Francesc Benavent, Jens Grivolla, Joan
Codina, Marta R. Costa-Jussa`, and Rafael Banchs.
2010. Opinion mining of spanish customer comments
with non-expert annotations on mechanical turk. In
Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 114?121, Los Angeles, June.
Association for Computational Linguistics.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Scheffczyk.
2006. FrameNet II: Extended Theory and Practice. In-
ternational Computer Science Institute, Berkeley, Cali-
fornia. Distributed with the FrameNet data.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing, pages
254?263, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Lu??s von Ahn and Laura Dabbish. 2008. Designing games
with a purpose. Communications of the ACM, 51:58?
67., August.
Lu??s von Ahn, Benjamin Maurer, Colin McMillen, David
Abraham, and Manuel Blum. 2008. reCAPTCHA:
Human-based character recognition via web security
measures. Science, 321(5895):1465?1468.
37
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 1?5,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
FrameNet: A Knowledge Base for Natural Language Processing
Collin F. Baker
International Computer Science Institute
1947 Center St., Suite 600
Berkeley, California 94704 U.S.A.
collinb@icsi.berkeley.edu
Abstract
Prof. Charles J. Fillmore had a life-
long interest in lexical semantics, and
this culminated in the latter part of his
life in a major research project, the
FrameNet Project at the International
Computer Science Institute in Berke-
ley, California (http://framenet.
icsi.berkeley.edu). This paper re-
ports on the background of this ongoing
project, its connections to Fillmore?s other
research interests, and briefly outlines ap-
plications and current directions of growth
for FrameNet, including FrameNets in lan-
guages other than English.
1 Introduction
It was my honor to work closely with the late
Charles Fillmore as part of the FrameNet project
at the International Computer Science Institute
in Berkeley, California (http://framenet.
icsi.berkeley.edu) from 1997 until this
year. It was a blessing to be in contact with that
rare combination of a brilliant intellect, a compas-
sionate heart, and genuine humility. This article
will discuss where FrameNet fits in the develop-
ment of Fillmore?s major theoretical contributions
(case grammar, frame semantics and construction
grammar), how FrameNet can be used for NLP,
and where the project is headed.
2 From Case Grammar to Frame
Semantics to FrameNet
The beginnings of case grammar were con-
temporary with the development of what came
to be called the ?Standard Theory? of Gener-
ative Grammar (Chomsky, 1965), and related
?through friendship? to the simultaneous develop-
ment of Generative Semantics. Fillmore (1968)
showed that a limited number of case roles
could provide elegant explanations of quite var-
ied linguistic phenomena, such as the differ-
ences in morphological case marking between
nominative-accusative, nominative-ergative, and
active-inactive languages, and anaphoric pro-
cesses such as subject drop in Japanese. A year
later (Fillmore, 1969), after explaining that verbs
like rob and steal require three arguments, the cul-
prit, the loser, and the loot, he continues in the next
section to say
It seems to me, however, that this sort
of detail is unnecessary, and that what
we need are abstractions from these
specific role descriptions, abstractions
which will allow us to recognize that
certain elementary role notions recur in
many situations,. . . Thus we can iden-
tify the culprit of rob and the critic
of criticize with the more abstract role
of Agent. . . in general. . . the roles that
[predicates?] arguments play are taken
from an inventory of role types fixed by
grammatical theory.
But the search for the ?correct? minimal set of
case roles proved to be difficult and contentious,
and it became apparent that some predicators, such
as replace and resemble, required roles which did
not fit into the usual categories. In fact, the orig-
inal case roles (a.k.a. semantic roles, thematic
roles, theta roles) were increasingly seen as gen-
eralizations over a much larger set of roles which
provide more detailed information about the par-
ticipants in a large variety of situations, described
as semantic frames (Fillmore, 1976; Fillmore,
1977b).
Thus, the formulation of Frame Semantics
should not be seen as a repudiation of the con-
cept of case roles expounded in Fillmore 1968, but
rather a recognition of the inadequacy of case roles
as a characterization of all the different types of
1
interactions of participants that can be linguisti-
cally significant in using language to describe sit-
uations:
. . . [A]s I have conceived them, the
repertory of cases is NOT identical to
the full set of notions that would be
needed to make an analysis of any state
or event. . . . [A] case frame need not
comprise a complete description of all
the relevant aspects of a situation, but
only a particular piece or section of a
situation. (Fillmore (1977a), emphasis
in the original)
The concept of frames became part of the aca-
demic zeitgeist of the 1960s and 70s. Roger Shank
was using the term script to talk about situa-
tions like eating in a restaurant (Schank and Abel-
son, 1977) and the term frame was being used in
a more-or-less similar sense by Marvin Minsky
(1974), and Eugene Charniak (1977).
FrameNet as an Implementation of Frame
Semantics
During the late 1980s and early 1990s, much of
Fillmore?s effort went into joint work with Paul
Kay, Catherine O?Connor, and others on the de-
velopment of Construction Grammar, especially
on linking constructions in which the semantic at-
tributes of various constituents were represented
by thematic roles such as Agent, Patient, Expe-
riencer, Stimulus, etc., (cf. Levin (1993)). But
semantic frames were always presupposed in Fill-
more?s discussion of Construction Grammar (e.g.
Kay and Fillmore (1999)), just as Construction
Grammar was always presupposed in discussions
of Frame Semantics. In fact, some of the inciden-
tal references to semantic frames in the literature
on construction grammar imply the existence of
very sophisticated frame semantics. At the same
time, Fillmore was becoming involved with the
lexicographer Sue Atkins, and increasingly think-
ing about what the dictionary would look like, if
freed from the limitations of publishing on paper
(Fillmore and Atkins, 1994) and based on corpus
data.
The FrameNet Project (Fillmore and Baker,
2010; Ruppenhofer et al., 2010a) at the Interna-
tional Computer Science Institute was launched
in 1997, as an effort to produce a lexicon of En-
glish that is both human- and machine-readable,
based on the theory of Frame Semantics and sup-
ported by annotating corpus examples of the lexi-
cal items. In part, FrameNet (FN) can be thought
of as the implementation of a theory that was al-
ready well-developed, but, like other annotation
projects, we have found that the process of anno-
tating actual text has also pushed the development
of the theory.
So what is a frame? Ruppenhofer et al. (2006)
define a frame as ?a script-like conceptual struc-
ture that describes a particular type of situation,
object, or event along with its participants and
props.? Frames are generalizations over groups of
words which describe similar states of affairs and
which could be expected to share similar sets of
roles, and (to some extent) similar syntactic pat-
terns for them. In the terminology of Frame Se-
mantics, the roles are called frame elements (FEs),
and the words which evoke the frame are referred
to as lexical units (LUs). A lexical unit is thus a
Saussurian ?sign?, an association between a form
and a meaning; the form is a lemma with a given
part of speech, the meaning is represented as a se-
mantic frame plus a short dictionary-style defini-
tion, which is intended to differentiate this lexi-
cal unit from others in the same frame. Each lex-
ical unit is equivalent to a word sense; if a lemma
has more than one sense, it will be linked to more
than one LU in more than one frame; e.g. the
lemma run.v (and all its word forms, run, ran, and
running) is linked to several frames (Self-motion,
Operating a system, etc.).
Some of this literature refers to two types of en-
tities, frames and scenes (Fillmore, 1977c). How-
ever, early in the process of defining the FN data
structure, it was recognized that more than two
levels of generality might be needed, so it was de-
cided to create only one type of data object, called
a frame, and to define relations between frames at
various levels of generality. Therefore, the term
scene is not used in FrameNet today, although
some frames which define complex events have
the term scenario as part of their names, such as
the Employer?s scenario, with subframes Hiring,
Employing and Firing.
In many cases, the framal distinctions proposed
by Fillmore in early work are directly reflected
in current FN frames, as in the pair of frames
Stinginess and Thriftiness, discussed in Fillmore
(1985). In other cases, the frame divisions in
FN differ from those originally proposed, as in
2
the division of the original Commerce frame into
three frames, Commerce, Commerce buy and
Commerce sell, which are connected by frame-
to-frame relations.
Because Frame Semantics began in the study of
verbs and valences, there was emphasis initially on
representing events, but the principle that a con-
ceptual gestalt can be evoked by any member of a
set of words also applies to relations, states, and
entities, and the evoking words can be nouns, ad-
jectives, adverbs, etc., as well as verbs. For ex-
ample, the Leadership frame contains both nouns
(leader, headmaster, maharaja), and verbs (lead,
command); FEs in the Leadership frame include
the LEADER and the GOVERNED, as in [
LEADER
Kurt Helborg] is the CAPTAIN [
GOVERNED
of the
Reiksguard Knights].
3 Applications of FrameNet
Underlying other applications is the need for
middle-ware to carry out automatic semantic role
labeling (ASRL). Beginning with the work of
Gildea and Jurafsky (2000; 2002), many re-
searchers have built ASRL systems trained on the
FrameNet data (Erk and Pad?o, 2006; Johansson
and Nugues, 2007; Das et al., 2013), some of
which are freely available. Other groups have built
software to suggest new LUs for existing frames,
or even new frames (Green, 2004)
Typical end-user applications for FrameNet
include Question answering (Sinha, 2008) and
information extraction (Mohit and Narayanan,
2003), and using FrameNet data has enabled some
improvements on systems attempting the RTE
task (Burchardt, 2008). The FrameNet website
lists the intended uses for hundreds of users of
the FrameNet data, including sentiment analy-
sis, building dialog systems, improving machine
translation, teaching English as a second language,
etc. The FrameNet team have an active partner-
ship with Decisive Analytics Corporation, which
is using FN-based ASRL as for event recognition
and tracking for their govenment and commercial
clients.
4 Some Limitations and Extensions of
the FrameNet Model
FrameNet works almost entirely on edited text, so
directly applying the ASRL systems trained on
current FN data will probably give poor results
on, e.g. Twitter feeds or transcribed conversation.
FrameNet also works strictly within the sentence,
so there is no direct way to deal with text coher-
ence, although FrameNet annotation does indicate
when certain core FEs are missing from a sen-
tence, which typically indicates that that they are
realized elsewhere in the text. This feature can be
used to link arguments across sentences (Ruppen-
hofer et al., 2010b).
Technical terms and Proper Nouns:
FrameNet has taken as its mandate to cover the
?core? lexicon of English, words in common
use, whose definitions are established by their
usage. The number of senses per word is known
to increase with the frequency of occurrence
Zipf (19491965), so the most frequent words are
likely to be the most polysemous and therefore
both the most important and the most challenging
for NLP. In general, the FrameNet team have
assumed that technical vocabulary, whose defi-
nitions are established by domain experts, will
be handled in terminologies for each domain,
such as the Medical Subject Headings of the U.S.
National Library of Medicine (https://www.
nlm.nih.gov/mesh/meshhome.html)
and the Department of Defense Dictionary of
Military Terms (http://www.dtic.mil/
doctrine/dod_dictionary/). For similar
reasons, FrameNet does not annotate proper
nouns, also known in NLP as named entities.
FrameNet cannot and has no reason to compete
with the on-line resources for these domains,
such as Wikipedia, lists of male and female
personal names, and gazetteers. On the other
hand, Frame Semantic resources have been
produced in several specialized domains: Thomas
Schmidt created a Frame-Semantic analysis of
the language associated with soccer (in Ger-
man, English, and French) (Schmidt, 2008),
http://www.kictionary.com; and lexica
in the legal domain have been produced for Italian
(Venturi et al., 2009) and Brazilian Portuguese
(Bertoldi and Oliveira Chishman, 2012).
Negation and Conditionals:
FrameNet does not have representations for nega-
tion and conditional sentences. The words
never.adv and seldom.adv are LUs in the Fre-
quency frame, but there is no recognition of their
status as negatives. The general approach which
the FrameNet team has proposed would be to
treat negative expressions as parts of constructs li-
3
censed by constructions which have a ?negation?
frame as their meaning pole, and license nega-
tive polarity items over some scope in the sen-
tence, but defining that scope is a notoriously dif-
ficult problem. We are just beginning to work a
mental spaces approach to the related problem of
conditional sentences, cf. Dancygier and Sweetser
(2005) and Sweetser (2006). FrameNet does not
include the word if , but does include both LUs and
annotation for a number of modal verbs and other
types of nouns and adjectives which can be used
to express conditionality, incuding the following:
Frame : LUs
Possibility : can, could, might, may
Capability : able.a, ability.n, can.v, poten-
tial.n/a, . . .
Likelihood: likely.a, might.v, may.v, must.v,
possible.a, . . .
5 Future directions: Expert curation vs.
rapid growth
After almost two decades of work at varying lev-
els of intensity, depending on funding, FrameNet
contains almost 1200 Semantic Frames, covering
almost 13,000 word senses (Lexical Units) , docu-
mented with almost 200,000 manual annotations.
This is bigger than a toy lexicon, but far fewer LUs
than WordNet or other lexicons derived automati-
cally from the web. By virtue of expert curation,
the FrameNet lexical database contains a wealth of
semantic knowledge that is unique. The database
is freely available from the FrameNet website.
One challenge we face now is finding a way to
greatly expand FrameNet in a more cost-effective
way while preserving the accuracy and richness
of the annotation. We have recently done some
small-scale experiments on crowd-sourcing vari-
ous parts of the process in partnership with col-
leagues at Google, and the preliminary results are
encouraging.
Another challenge comes as a result of the suc-
cess of Frame Semantics as an interlingua (Boas,
2009). There are now projects building FrameNet-
style lexical databases for many different lan-
guages; funded projects are creating FrameNets
for German, Spanish, Japanese, Swedish, Chinese,
French and Arabic; smaller efforts have created
Frame Semantics-based resources for many other
languages, including Italian, Korean, Polish, Bul-
garian, Russian, Slovenian, Hebrew, and Hindi.
Some are produced almost entirely via manual
annotation, while others are being created semi-
automatically. The good news is that the general
result seems to be that the frames devised for En-
glish can be used for the majority of LUs in each
of these language. The challenge is finding a way
to integrate the frame semantic work being done
around the world, to create a truly multi-lingual
FrameNet.
For more information on all these topics, please
visit
http://framenet.icsi.berkeley.
edu
References
Anderson Bertoldi and Rove Luiza Oliveira Chishman.
2012. Developing a frame-based lexicon for the
Brazilian legal language: The case of the criminal
process frame. In Monica Palmirani, Ugo Pagallo,
Pompeu Casanovas, and Giovanni Sartor, editors, AI
Approaches to the Complexity of Legal Systems, vol-
ume 7639 of Lecture Notes in Computer Science,
pages 256?270. Springer Berlin Heidelberg.
Hans C. Boas, editor. 2009. Multilingual FrameNets
in Computational Lexicography: Methods and Ap-
plications. Mouton de Gruyter.
Aljoscha Burchardt. 2008. Modeling Textual Entail-
ment with Role-Semantic Information. Ph.D. thesis,
Universit?at des Saarlandes.
Eugene Charniak. 1977. Framed PAINTING: The rep-
resentation of a common sense knowledge fragment.
Cognitive Science, 1(4):235?264.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. MIT Press, Cambridge, MA.
Barbara Dancygier and Eve Sweetser. 2005. Men-
tal spaces in grammar: conditional constructions.
Cambridge University Press, Cambridge, UK; New
York.
Dipanjan Das, Desai Chen, Andr?e F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2013.
Frame-Semantic parsing. Computational Linguis-
tics, 40(1).
Katrin Erk and Sebastian Pad?o. 2006. Shalmaneser ?
a flexible toolbox for semantic role assignment. In
Proceedings of the fifth International Conference on
Language Resources and Evaluation (LREC-2006),
Genoa, Italy.
Charles J. Fillmore and B.T.S. Atkins. 1994. Starting
where the dictionaries stop: The challenge for com-
putational lexicography. In Antonio Zampolli and
Sue Atkins, editors, Computational Approaches to
the Lexicon. Oxford University Press.
4
Charles J. Fillmore and Collin F. Baker. 2010. A
frames approach to semantic analysis. In Bernd
Heine and Heiko Narrog, editors, Oxford Handbook
of Linguistic Analysis, pages 313?341. OUP.
Charles J. Fillmore. 1968. The case for case. In
E. Bach and R. Harms, editors, Universals in Lin-
guistic Theory. Holt, Rinehart & Winston, New
York.
Charles J. Fillmore. 1969. Toward a modern the-
ory of case. In David A Reibel and Sanford A.
Shane, editors, Modern Studies in English: Read-
ings in Transformational Grammar, pages 361?375.
Prentice-Hall, Englewood Cliffs, New Jersey.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20?
32.
Charles J. Fillmore. 1977a. Frame semantics. pages
111?137.
Charles J. Fillmore. 1977b. The need for a frame se-
mantics in linguistics. In Hans Karlgren, editor, Sta-
tistical Methods in Linguistics. Scriptor.
Charles J. Fillmore. 1977c. Scenes-and-frames seman-
tics. In Antonio Zampolli, editor, Linguistic Struc-
tures Processing, number 59 in Fundamental Studies
in Computer Science. North Holland Publishing.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 6(2):222?
254.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In ACL 2000: Proceed-
ings of ACL 2000, Hong Kong.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Rebecca Green. 2004. Inducing Semantic Frames
from Lexical Resources. Ph.D. thesis, University of
Maryland, College Park.
Richard Johansson and Pierre Nugues. 2007. LTH:
Semantic structure extraction using nonprojective
dependency trees. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 227?230, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Paul Kay and Charles J. Fillmore. 1999. Grammati-
cal constructions and linguistic generalizations: The
what?s x doing y? construction. Language, 75:1?33.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
Marvin Minsky. 1974. A framework for representing
knowledge. Memo 306, MIT-AI Laboratory, June.
Behrang Mohit and Srini Narayanan. 2003. Seman-
tic extraction with wide-coverage lexical resources.
In Marti Hearst and Mari Ostendorf, editors, HLT-
NAACL 2003: Short Papers, pages 64?66, Edmon-
ton, Alberta, Canada, May 27 - June 1. Association
for Computational Linguistics.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended Theory and
Practice. International Computer Science Institute,
Berkeley, California. Distributed with the FrameNet
data.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010a. FrameNet II: Extended Theory and
Practice. FrameNet Project, September.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010b.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009),
pages 106?111, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: an Inquiry into
Human Knowledge Structures. Lawrence Erlbaum,
Hillsdale, NJ.
Thomas Schmidt. 2008. The Kicktionary: Combining
corpus linguistics and lexical semantics for a multi-
lingual football dictionary. In Eva Lavric, Gerhard
Pisek, Andrew Skinner, and Wolfgang Stadler, edi-
tors, The Linguistics of Football, number 38 in Lan-
guage in Performance, pages 11?23. Gunter Narr,
T?ubingen.
Steve Sinha. 2008. Answering Questions about Com-
plex Events. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, Dec.
Eve Sweetser. 2006. Negative spaces: Levels of nega-
tion and kinds of spaces. In St?ephanie Bonnefille
and S?ebastien Salbayre, editors, Proceedings of the
conference ?Negation: Form, figure of speech, con-
ceptualization?, Tours. Groupe de recherches anglo-
am?ericaines de l?Universit?e de Tours, Publications
universitaires Fran cois Rabelais.
Giulia Venturi, Alessandro Lenci, Simonetta Monte-
magn, Eva Maria Vecchi, Maria Teresa Sagri, and
Daniela Tiscornia. 2009. Towards a FrameNet re-
source for the legal domain. In Proceedings of the
Third Workshop on Legal Ontologies and Artificial
Intelligence Techniques, Barcelona, Spain, June.
George Kingsley Zipf. 1949[1965]. Human behavior
and the principle of least effort: an introduction to
human ecology. Hafner Pub. Co., New York.
5
