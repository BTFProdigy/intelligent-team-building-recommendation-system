Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 65?68,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Assessing the Costs of Sampling Methods in Active Learning for Annotation
Robbie Haertel, Eric Ringger, Kevin Seppi, James Carroll, Peter McClanahan
Department of Computer Science
Brigham Young University
Provo, UT 84602, USA
robbie haertel@byu.edu, ringger@cs.byu.edu, kseppi@cs.byu.edu,
jlcarroll@gmail.com, petermcclanahan@gmail.com
Abstract
Traditional Active Learning (AL) techniques
assume that the annotation of each datum costs
the same. This is not the case when anno-
tating sequences; some sequences will take
longer than others. We show that the AL tech-
nique which performs best depends on how
cost is measured. Applying an hourly cost
model based on the results of an annotation
user study, we approximate the amount of time
necessary to annotate a given sentence. This
model allows us to evaluate the effectiveness
of AL sampling methods in terms of time
spent in annotation. We acheive a 77% re-
duction in hours from a random baseline to
achieve 96.5% tag accuracy on the Penn Tree-
bank. More significantly, we make the case
for measuring cost in assessing AL methods.
1 Introduction
Obtaining human annotations for linguistic data is
labor intensive and typically the costliest part of the
acquisition of an annotated corpus. Hence, there is
strong motivation to reduce annotation costs, but not
at the expense of quality. Active learning (AL) can
be employed to reduce the costs of corpus annotation
(Engelson and Dagan, 1996; Ringger et al, 2007;
Tomanek et al, 2007). With the assistance of AL,
the role of the human oracle is either to label a da-
tum or simply to correct the label from an automatic
labeler. For the present work, we assume that cor-
rection is less costly than annotation from scratch;
testing this assumption is the subject of future work.
In AL, the learner leverages newly provided anno-
tations to select more informative sentences which
in turn can be used by the automatic labeler to pro-
vide more accurate annotations in future iterations.
Ideally, this process yields accurate labels with less
human effort.
Annotation cost is project dependent. For in-
stance, annotators may be paid for the number of an-
notations they produce or by the hour. In the context
of parse tree annotation, Hwa (2004) estimates cost
using the number of constituents needing labeling
and Osborne & Baldridge (2004) use a measure re-
lated to the number of possible parses. With few ex-
ceptions, previous work on AL has largely ignored
the question of actual labeling time. One excep-
tion is (Ngai and Yarowsky, 2000) (discussed later)
which compares the cost of manual rule writing with
AL-based annotation for noun phrase chunking. In
contrast, we focus on the performance of AL algo-
rithms using different estimates of cost (including
time) for part of speech (POS) tagging, although the
results are applicable to AL for sequential labeling
in general. We make the case for measuring cost in
assessing AL methods by showing that the choice of
a cost function significantly affects the choice of AL
algorithm.
2 Benefit and Cost in Active Learning
Every annotation task begins with a set of un-
annotated items U . The ordered set A ? U con-
sists of all annotated data after annotation is com-
plete or after available financial resources (or time)
have been exhausted. We expand the goal of AL
to produce the annotated set A? such that the benefit
gained is maximized and cost is minimized.
In the case of POS tagging, tag accuracy is usu-
65
ally used as the measure of benefit. Several heuristic
AL methods have been investigated for determining
which data will provide the most information and
hopefully the best accuracy. Perhaps the best known
are Query by Committee (QBC) (Seung et al, 1992)
and uncertainty sampling (or Query by Uncertainty,
QBU) (Thrun and Moeller, 1992). Unfortunately,
AL algorithms such as these ignore the cost term of
the maximization problem and thus assume a uni-
form cost of annotating each item. In this case, the
ordering of annotated dataAwill depend entirely on
the algorithm?s estimate of the expected benefit.
However, for AL in POS tagging, the cost term
may not be uniform. If annotators are required to
change only those automatically generated tags that
are incorrect, and depending on how annotators are
paid, the cost of tagging one sentence can depend
greatly on what is known from sentences already an-
notated. Thus, in POS tagging both the benefit (in-
crease in accuracy) and cost of annotating a sentence
depend not only on properties of the sentence but
also on the order in which the items are annotated.
Therefore, when evaluating the performance of an
AL technique, cost should be taken into account. To
illustrate this, consider some basic AL algorithms
evaluated using several simple cost metrics. The re-
sults are presented against a random baseline which
selects sentences at random; the learning curves rep-
resent the average of five runs starting from a ran-
dom initial sentence. If annotators are paid by the
sentence, Figure 1(a) presents a learning curve in-
dicating that the AL policy that selects the longest
sentence (LS) performs rather well. Figure 1(a) also
shows that given this cost model, QBU and QBC are
essentially tied, with QBU enjoying a slight advan-
tage. This indicates that if annotators are paid by
the sentence, QBU is the best solution, and LS is a
reasonable alternative. Next, Figure 1(b) illustrates
that the results differ substantially if annotators are
paid by the word. In this case, using LS as an AL
policy is worse than random selection. Furthermore,
QBC outperforms QBU. Finally, Figure 1(c) shows
what happens if annotators are paid by the number
of word labels corrected. Notice that in this case, the
random selector marginally outperforms the other
techniques. This is because QBU, QBC, and LS tend
to select data that require many corrections. Con-
sidered together, Figures 1(a)-Figure 1(c) show the
significant impact of choosing a cost model on the
relative performance of AL algorithms. This leads
us to conclude that AL techniques should be eval-
uated and compared with respect to a specific cost
function.
While not all of these cost functions are neces-
sarily used in real-life annotation, each can be re-
garded as an important component of a cost model
of payment by the hour. Since each of these func-
tions depends on factors having a significant effect
on the perceived performance of the various AL al-
gorithms, it is important to combine them in a way
that will accurately reflect the true performance of
the selection algorithms.
In prior work, we describe such a cost model for
POS annotation on the basis of the time required for
annotation (Ringger et al, 2008). We refer to this
model as the ?hourly cost model?. This model is
computed from data obtained from a user study in-
volving a POS annotation task. In the study, tim-
ing information was gathered from many subjects
who annotated both sentences and individual words.
This study included tests in which words were pre-
labeled with a candidate labeling obtained from an
automatic tagger (with a known error rate) as would
occur in the context of AL. Linear regression on the
study data yielded a model of POS annotation cost:
h = (3.795 ? l + 5.387 ? c + 12.57)/3600 (1)
where h is the time in hours spent on the sentence, l
is the number of tokens in the sentence, and c is the
number of words in the sentence needing correction.
For this model, the Relative Standard Error (RSE) is
89.5, and the adjusted correlation (R2) is 0.181. This
model reflects the abilities of the annotators in the
study and may not be representative of annotators in
other projects. However, the purpose of this paper is
to create a framework for accounting for cost in AL
algorithms. In contrast to the model presented by
Ngai and Yarowsky (2000), which predicts mone-
tary cost given time spent, this model estimates time
spent from characteristics of a sentence.
3 Evaluation Methodology and Results
Our test data consists of English prose from the
POS-tagged Wall Street Journal text in the Penn
Treebank (PTB) version 3. We use sections 2-21 as
66
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  500  1000  1500  2000
Ta
g 
Ac
cu
ra
cy
Annotated Sentences
Random
LS
QBU
QBC
(a)
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  20000  40000  60000  80000  100000
Ta
g 
Ac
cu
ra
cy
Annotated Words
Random
LS
QBU
QBC
(b)
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0  2000  4000  6000  8000  10000
Ta
g 
Ac
cu
ra
cy
Cumulative Tags Corrected
Random
LS
QBU
QBC
(c)
Figure 1: QBU, LS, QBC, and the random baseline plotted in terms of accuracy versus various cost functions: (a)
number of sentences annotated; (b) number of words annotated; and (c) number of tags corrected.
initially unannotated data. We employ section 24 as
the development test set on which tag accuracy is
computed at the end of every iteration of AL.
For tagging, we employ an order two Maximum
EntropyMarkovModel (MEMM). For decoding, we
found that a beam of size five sped up the decoder
with almost no degradation in accuracy fromViterbi.
The features used in this work are typical for modern
MEMM POS tagging and are mostly based on work
by Toutanova and Manning (2000).
In our implementation, QBU employs a single
MEMM tagger. We approximate the entropy of the
per-sentence tag sequences by summing over per-
word entropy and have found that this approxima-
tion provides equivalent performance to the exact se-
quence entropy. We also consider another selection
algorithm introduced in (Ringger et al, 2007) that
eliminates the overhead of entropy computations al-
together by estimating per-sentence uncertainty with
1 ? P (t?), where t? is the Viterbi (best) tag sequence.
We label this scheme QBUOMM (OMM = ?One
Minus Max?).
Our implementation of QBC employs a commit-
tee of three MEMM taggers to balance computa-
tional cost and diversity, following Tomanek et al
(2007). Each committee member?s training set is a
random bootstrap sample of the available annotated
data, but is otherwise as described above for QBU.
We follow Engelson & Dagan (1996) in the imple-
mentation of vote entropy for sentence selection us-
ing these models.
When comparing the relative performance of AL
algorithms, learning curves can be challenging to in-
terpret. As curves proceed to the right, they can ap-
proach one another so closely that it may be difficult
to see the advantage of one curve over another. For
this reason, we introduce the ?cost reduction curve?.
In such a curve, the accuracy is the independent vari-
able. We then compute the percent reduction in cost
(e.g., number of words or hours) over the cost of the
random baseline for the same accuracy a:
redux(a) = (costrnd(a) ? cost(a))/costrnd(a)
Consequently, the random baseline represents the
trajectory redux(a) = 0.0. Algorithms less costly
than the baseline appear above the baseline. For a
specific accuracy value on a learning curve, the cor-
responding value of the cost on the random baseline
is estimated by interpolation between neighboring
points on the baseline. Using hourly cost, Figure 2
shows the cost reduction curves of several AL al-
gorithms, including those already considered in the
learning curves of Figure 1 (except LS). Restricting
the discussion to the random baseline, QBC, and
QBU: for low accuracies, random selection is the
cheapest according to hourly cost; QBU begins to
be cost-effective at around 91%; and QBC begins to
outperform the baseline and QBU around 80%.
4 Normalized Methods
One approach to convert existing AL algorithms into
cost-conscious algorithms is to normalize the results
of the original algorithm by the estimated cost. It
should be somewhat obvious that many selection al-
gorithms are inherently length-biased for sequence
labeling tasks. For instance, since QBU is the sum
67
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.86  0.88  0.9  0.92  0.94  0.96
R
ed
uc
tio
n 
in
 H
ou
rly
 C
os
t
Tag Accuracy
Random
QBUOMM/N
QBC/N
QBU/N
QBUOMM
QBC
QBU
Figure 2: Cost reduction curves for QBU, QBC,
QBUOMM, their normalized variants, and the random
baseline on the basis of hourly cost
of entropy over all words, longer sentences will tend
to have higher uncertainty. The easiest solution is
to normalize by sentence length, as has been done
previously (Engelson and Dagan, 1996; Tomanek et
al., 2007). This of course assumes that annotators
are paid by the word, which may or may not be true.
Nevertheless, this approach can be justified by the
hourly cost model. Replacing the number of words
needing correction, c, with the product of l (the sen-
tence length) and the accuracy p of the model, equa-
tion 1 can be re-written as the estimate:
h? = ((3.795 + 5.387p) ? l + 12.57)/3600
Within a single iteration of AL, p is constant, so the
cost is approximately proportional to the length of
the sentence. Figure 2 shows that normalized AL al-
gorithms (suffixed with ?/N?) generally outperform
the standard algorithms based on hourly cost (in
contrast to the cost models used in Figures 1(a) -
(c)). All algorithms shown have significant cost
savings over the random baseline for accuracy lev-
els above 92%. Furthermore, all algorithms except
QBU depict trends of further increasing the advan-
tage after 95%. According to the hourly cost model,
QBUOMM/N has an advantage over all other algo-
rithms for accuracies over 91%, achieving a signifi-
cant 77% reduction in cost at 96.5% accuracy.
5 Conclusions
We have shown that annotation cost affects the as-
sessment of AL algorithms used in POS annotation
and advocate the use of a cost estimate that best es-
timates the true cost. For this reason, we employed
an hourly cost model to evaluate AL algorithms for
POS annotation. We have also introduced the cost
reduction plot in order to assess the cost savings pro-
vided by AL. Furthermore, inspired by the notion
of cost, we evaluated normalized variants of well-
known AL algorithms and showed that these vari-
ants out-perform the standard versions with respect
to the proposed hourly cost measure. In future work
we will build better cost-conscious AL algorithms.
References
S. Engelson and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora. In
Proc. of ACL, pages 319?326.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30:253?276.
G. Ngai and D. Yarowsky. 2000. Rule writing or an-
notation: cost-efficient resource usage for base noun
phrase chunking. In Proc. of ACL, pages 117?125.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proc. of HLT-
NAACL, pages 89?96.
E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In Proc. of Linguistic
Annotation Workshop, pages 101?108.
E. Ringger, M. Carmen, R. Haertel, K. Seppi, D. Lond-
sale, P. McClanahan, J. Carroll, and N. Ellison. 2008.
Assessing the costs of machine-assisted corpus anno-
tation through a user study. In Proc. of LREC.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proc. of CoLT, pages 287?
294.
S. Thrun and K. Moeller. 1992. Active exploration in dy-
namic environments. In NIPS, volume 4, pages 531?
538.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts annota-
tion costs and maintains reusability of annotated data.
Proc. of EMNLP-CoNLL, pages 486?495.
K. Toutanova and C. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63?70.
68
Proceedings of the Linguistic Annotation Workshop, pages 101?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
Active Learning for Part-of-Speech Tagging: 
Accelerating Corpus Annotation 
 
Eric Ringger*, Peter McClanahan*, Robbie Haertel*, George Busby*, Marc Carmen**, 
James Carroll*, Kevin Seppi*, Deryle Lonsdale** 
*Computer Science Department; **Linguistics Department 
Brigham Young University 
Provo, Utah, USA 84602 
 
Abstract 
In the construction of a part-of-speech an-
notated corpus, we are constrained by a 
fixed budget. A fully annotated corpus is 
required, but we can afford to label only a 
subset. We train a Maximum Entropy Mar-
kov Model tagger from a labeled subset 
and automatically tag the remainder. This 
paper addresses the question of where to 
focus our manual tagging efforts in order to 
deliver an annotation of highest quality. In 
this context, we find that active learning is 
always helpful. We focus on Query by Un-
certainty (QBU) and Query by Committee 
(QBC) and report on experiments with sev-
eral baselines and new variations of QBC 
and QBU, inspired by weaknesses particu-
lar to their use in this application. Experi-
ments on English prose and poetry test 
these approaches and evaluate their robust-
ness. The results allow us to make recom-
mendations for both types of text and raise 
questions that will lead to further inquiry. 
1 Introduction 
We are operating (as many do) on a fixed budget 
and need annotated text in the context of a larger 
project. We need a fully annotated corpus but can 
afford to annotate only a subset. To address our 
budgetary constraint, we train a model from a ma-
nually annotated subset of the corpus and automat-
ically annotate the remainder. At issue is where to 
focus manual annotation efforts in order to produce 
a complete annotation of highest possible quality. 
A follow-up question is whether these techniques 
work equally well on different types of text. 
In particular, we require part-of-speech (POS) 
annotations. In this paper we employ a state-of-the-
art tagger on both prose and poetry, and we ex-
amine multiple known and novel active learning 
(or sampling) techniques in order to determine 
which work best in this context. We show that the 
results obtained by a state-of-the-art tagger trained 
on a small portion of the data selected through ac-
tive learning can approach the accuracy attained by 
human annotators and are on par with results from 
exhaustively trained automatic taggers. 
In a study based on English language data pre-
sented here, we identify several active learning 
techniques and make several recommendations that 
we hope will be portable for application to other 
text types and to other languages. In section 2 we 
briefly review the state of the art approach to POS 
tagging. In section 3, we survey the approaches to 
active learning employed in this study, including 
variations on commonly known techniques. Sec-
tion 4 introduces the experimental regime and 
presents results and their implications. Section 5 
draws conclusions and identifies opportunities for 
follow-up research. 
2 Part of Speech Tagging 
Labeling natural language data with part-of-speech 
tags can be a complicated task, requiring much 
effort and expense, even for trained annotators. 
Several efforts, notably the Alembic workbench 
(Day et al, 1997) and similar tools, have provided 
interfaces to aid annotators in the process.  
Automatic POS tagging of text using probabilis-
tic models is mostly a solved problem but requires 
supervised learning from substantial amounts of 
training data. Previous work demonstrates the sui-
tability of Hidden Markov Models for POS tagging 
(Kupiec, 1992; Brants, 2000). More recent work 
has achieved state-of-the-art results with Maxi-
101
mum entropy conditional Markov models (MaxEnt 
CMMs, or MEMMs for short) (Ratnaparkhi, 1996; 
Toutanova & Manning, 2000; Toutanova et al, 
2003). Part of the success of MEMMs can be attri-
buted to the absence of independence assumptions 
among predictive features and the resulting ease of 
feature engineering. To the best of our knowledge, 
the present work is the first to present results using 
MEMMs in an active learning framework.  
An MEMM is a probabilistic model for se-
quence labeling. It is a Conditional Markov Model 
(CMM as illustrated in Figure 1) in which a Max-
imum Entropy (MaxEnt) classifier is employed to 
estimate the probability distribution
1.. 1 1 2( | , ) ( | , , , )i i ME i i i i ip t w t p t w f t t? ? ?? over 
possible labels it  for each element in the se-
quence?in our case, for each word iw  in a sen-
tence w . The MaxEnt model is trained from la-
beled data and has access to any predefined 
attributes (represented here by the collection if ) of 
the entire word sequence and to the labels of pre-
vious words ( 1.. 1it ? ). Our implementation employs 
an order-two Markov assumption so the classifier 
has access only to the two previous tags 1 2,i it t? ? . 
We refer to the features 1 2( , , , )i i i iw f t t? ? from 
which the classifier predicts the distribution over 
tags as ?the local trigram context?. 
A Viterbi decoder is a dynamic programming 
algorithm that applies the MaxEnt classifier to 
score multiple competing tag-sequence hypotheses 
efficiently and to produce the best tag sequence, 
according to the model. We approximate Viterbi 
very closely using a fast beam search. Essentially, 
the decoding process involves sequential classifi-
cation, conditioned on the (uncertain) decisions of 
the previous local trigram context classifications. 
The chosen tag sequence t? is the tag sequence 
maximizing the following quantity: 
1 2
1..
? arg max ( | )
arg max ( | , , , )
t
t ME i i i i i
i n
t P t w
p t w f t t? ?
=
=
= ?  
The features used in this work are reasonably 
typical for modern MEMM feature-based POS 
tagging and consist of a combination of lexical, 
orthographic, contextual, and frequency-based in-
formation. In particular, for each word the follow-
ing features are defined: the textual form of the 
word itself, the POS tags of the preceding two 
words, and the textual form of the following word. 
Following Toutanova and Manning (2000) approx-
imately, more information is defined for words that 
are considered rare (which we define here as words 
that occur fewer than fifteen times). We consider 
the tagger to be near-state-of-the-art in terms of 
tagging accuracy. 
 
Figure 1. Simple Markov order 2 CMM, with focus on 
the i-th hidden label (or tag). 
3 Active Learning 
The objective of this research is to produce more 
high quality annotated data with less human anno-
tator time and effort. Active learning is an ap-
proach to machine learning in which a model is 
trained with the selective help of an oracle. The 
oracle provides labels on a sufficient number of 
?tough? cases, as identified by the model. Easy 
cases are assumed to be understood by the model 
and to require no additional annotation by the 
oracle. Many variations have been proposed in the 
broader active learning and decision theory litera-
ture under many names, including ?active sam-
pling? and ?optimal sampling.? 
In active learning for POS tagging, as in other 
applications, the oracle can be a human. For expe-
rimental purposes, a human oracle is simulated 
using pre-labeled data, where the labels are hidden 
until queried. To begin, the active learning process 
requires some small amount of training data to 
seed the model. The process proceeds by identify-
ing the data in the given corpus that should be 
tagged first for maximal impact. 
3.1 Active Learning in the Language Context 
When considering the role of active learning, we 
were initially drawn to the work in active learning 
for classification. In a simple configuration, each 
instance (document, image, etc.) to be labeled can 
be considered to be independent. However, for ac-
tive learning for the POS tagging problem we con-
sidered the nature of human input as an oracle for 
the task. As an approximation, people read sen-
tences as propositional atoms, gathering contextual 
cues from the sentence in order to assemble the 
102
meaning of the whole. Consequently, we thought it 
unreasonable to choose the word as the granularity 
for active learning. Instead, we begin with the as-
sumption that a human will usually require much 
of the sentence or at least local context from the 
sentence in order to label a single word with its 
POS label. While focusing on a single word, the 
human may as well label the entire sentence or at 
least correct the labels assigned by the tagger for 
the sentence. Consequently, the sentence is the 
granularity of annotation for this work. (Future 
work will question this assumption and investigate 
tagging a word or a subsequence of words at a 
time.) This distinguishes our work from active 
learning for classification since labels are not 
drawn from a fixed set of labels. Rather, every sen-
tence of length n can be labeled with a tag se-
quence drawn from a set of size nT , where T  is 
the size of the per-word tag set. Granted, many of 
the options have very low probability. 
To underscore our choice of annotating at the 
granularity of a sentence, we also note that a max-
imum entropy classifier for isolated word tagging 
that leverages attributes of neighboring words?
but is blind to all tags?will underperform an 
MEMM that includes the tags of neighboring 
words (usually on the left) among its features. Pre-
vious experiments demonstrate the usefulness of 
tags in context on the standard Wall Street Journal 
data from the Penn Treebank (Marcus et al, 1999). 
A MaxEnt isolated word tagger achieves 93.7% on 
words observed in the training set and 82.6% on 
words unseen in the training set. Toutanova and 
Manning (2000) achieves 96.9% (on seen) and 
86.9% (on unseen) with an MEMM. They sur-
passed their earlier work in 2003 with a ?cyclic 
dependency network tagger?, achieving 
97.2%/89.05% (seen/unseen) (Toutanova et al, 
2003). The generally agreed upon upper bound is 
around 98%, due to label inconsistencies in the 
Treebank. The main point is that effective use of 
contextual features is necessary to achieve state of 
the art performance in POS tagging. 
In active learning, we employ several sets of 
data that we refer to by the following names: 
? Initial Training: the small set of data used 
to train the original model before active 
learning starts 
? Training: data that has already been la-
beled by the oracle as of step i in the learn-
ing cycle 
? Unannotated: data not yet labeled by the 
oracle as of step i 
? Test (specifically Development Test): la-
beled data used to measure the accuracy of 
the model at each stage of the active learn-
ing process. Labels on this set are held in 
reserve for comparison with the labels 
chosen by the model. It is the accuracy on 
this set that we report in our experimental 
results in Section 4. 
Note that the Training set grows at the expense of 
the Unannotated set as active learning progresses. 
Active Learning for POS Tagging consists of the 
following steps: 
1. Train a model with Initial Training data 
2. Apply model to Unannotated data 
3. Compute potential informativeness of 
each sentence 
4. Remove top n sentences with most po-
tential informativeness from Unanno-
tated data and give to oracle 
5. Add n sentences annotated (or corrected) 
by the oracle to Training data 
6. Retrain model with Training data 
7. Return to step 2 until stopping condition 
is met. 
There are several possible stopping conditions, 
including reaching a quality bar based on accuracy 
on the Test set, the rate of oracle error corrections 
in the given cycle, or even the cumulative number 
of oracle error corrections. In practice, the exhaus-
tion of resources, such as time or money, may 
completely dominate all other desirable stopping 
conditions. 
Several methods are available for determining 
which sentences will provide the most information. 
Expected Value of Sample Information (EVSI) 
(Raiffa & Schlaiffer, 1967) would be the optimal 
approach from a decision theoretic point of view, 
but it is computationally prohibitive and is not con-
sidered here. We also do not consider the related 
notion of query-by-model-improvement or other 
methods (Anderson & Moore, 2005; Roy & 
McCallum, 2001a, 2001b). While worth exploring, 
they do not fit in the context of this current work 
and should be considered in future work. We focus 
here on the more widely used Query by Committee 
(QBC) and Query by Uncertainty (QBU), includ-
ing our new adaptations of these. 
Our implementation of maximum entropy train-
ing employs a convex optimization procedure 
known as LBFGS. Although this procedure is rela-
tively fast, training a model (or models in the case 
103
of QBC) from scratch on the training data during 
every round of the active learning loop would pro-
long our experiments unnecessarily. Instead we 
start each optimization search with a parameter set 
consisting of the model parameters from the pre-
vious iteration of active learning (we call this ?Fast 
MaxEnt?). In practice, this converges quickly and 
produces equivalent results. 
3.2 Query by Committee 
Query by Committee (QBC) was introduced by 
Seung, Opper, and Sompolinsky (1992). Freund, 
Seung, Shamir, and Tishby (1997) provided a care-
ful analysis of the approach. Engelson and Dagan 
(1996) experimented with QBC using HMMs for 
POS tagging and found that selective sampling of 
sentences can significantly reduce the number of 
samples required to achieve desirable tag accura-
cies. Unlike the present work, Engelson & Dagan 
were restricted by computational resources to se-
lection from small windows of the Unannotated set, 
not from the entire Unannotated set. Related work 
includes learning ensembles of POS taggers, as in 
the work of Brill and Wu (1998), where an ensem-
ble consisting of a unigram model, an N-gram 
model, a transformation-based model, and an 
MEMM for POS tagging achieves substantial re-
sults beyond the individual taggers. Their conclu-
sion relevant to this paper is that different taggers 
commit complementary errors, a useful fact to ex-
ploit in active learning. QBC employs a committee 
of N models, in which each model votes on the 
correct tagging of a sentence. The potential infor-
mativeness of a sentence is measured by the total 
number of tag sequence disagreements (compared 
pair-wise) among the committee members. Possi-
ble variants of QBC involve the number of com-
mittee members, how the training data is split 
among the committee members, and whether the 
training data is sampled with or without replace-
ment. 
A potential problem with QBC in this applica-
tion is that words occur with different frequencies 
in the corpus. Because of the potential for greater 
impact across the corpus, querying for the tag of a 
more frequent word may be more desirable than 
querying for the tag of a word that occurs less fre-
quently, even if there is greater disagreement on 
the tags for the less frequent word. We attempted 
to compensate for this by weighting the number of 
disagreements by the corpus frequency of the word 
in the full data set (Training and Unannotated). 
Unfortunately, this resulted in worse performance; 
solving this problem is an interesting avenue for 
future work. 
3.3 Query by Uncertainty 
The idea behind active sampling based on uncer-
tainty appears to originate with Thrun and Moeller 
(1992). QBU has received significant attention in 
general. Early experiments involving QBU were 
conducted by Lewis and Gale (1994) on text classi-
fication, where they demonstrated significant bene-
fits of the approach. Lewis and Catlett (1994) ex-
amined its application for non-probabilistic learn-
ers in conjunction with other probabilistic learners 
under the name ?uncertainty sampling.? Brigham 
Anderson (2005) explored QBU using HMMs and 
concluded that it is sometimes advantageous. We 
are not aware of any published work on the appli-
cation of QBU to POS tagging. In our implementa-
tion, QBU employs a single MEMM tagger. The 
MaxEnt model comprising the tagger can assess 
the probability distribution over tags for any word 
in its local trigram context, as illustrated in the ex-
ample in Figure 2. 
Figure 2. Distribution over tags for the word ?hurdle? in 
italics. The local trigram context is in boldface. 
In Query by Uncertainty (QBU), the informa-
tiveness of a sample is assumed to be the uncer-
tainty in the predicted distribution over tags for 
that sample, that is the entropy of 
1 2( | , , , )ME i i i i ip t w f t t? ? . To determine the poten-
tial informativeness of a word, we can measure the 
entropy in that distribution. Since we are selecting 
sentences, we must extend our measure of uncer-
tainty beyond the word. 
3.4 Adaptations of QBU 
There are several problems with the use of QBU in 
this context: 
? Some words are more important; i.e., they 
contain more information perhaps because 
they occur more frequently. 
   NN 0 .85 
   VB  0.13 
   ... 
RB    DT JJS CD  2.0E-7 
 
Perhaps     the biggest   hurdle ? 
104
? MaxEnt estimates per-word distributions 
over tags, not per-sentence distributions 
over tag sequences. 
? Entropy computations are relatively costly. 
We address the first issue in a new version of QBU 
which we call ?Weighted Query by Uncertainty? 
(WQBU). In WQBU, per-word uncertainty is 
weighted by the word's corpus frequency. 
To address the issue of estimating per-sentence 
uncertainty from distributions over tag sequences, 
we have considered several different approaches. 
The per-word (conditional) entropy is defined as 
follows: 
 
 
 
 
 
 
where iT  is the random variable for the tag it  on 
word iw , and the features of the context in which 
iw  occurs are denoted, as before, by the collection 
if  and the prior tags 1 2,i it t? ? . It is straightforward 
to calculate this entropy for each word in a sen-
tence from the Unannotated set, if we assume that 
previous tags 1 2,i it t? ?  are from the Viterbi (best) 
tag sequence (for the entire sentence) according to 
the model. 
For an entire sentence, we estimate the tag-
sequence entropy by summing over all possible tag 
sequences. However, computing this estimate ex-
actly on a 25-word sentence, where each word can 
be labeled with one of 35 tags, would require 3525 
= 3.99*1038 steps. Instead, we approximate the per-
sentence tag sequence distribution entropy by 
summing per-word entropy: 
 
 
This is the approach we refer to as QBU in the 
experimental results section. We have experi-
mented with a second approach that estimates the 
per-sentence entropy of the tag-sequence distribu-
tion by Monte Carlo decoding. Unfortunately, cur-
rent active learning results involving this MC POS 
tagging decoder are negative on small Training set 
sizes, so we do not present them here. Another al-
ternative approximation worth pursuing is compu-
ting the per-sentence entropy using the n-best POS 
tag sequences. Very recent work by Mann and 
McCallum (2007) proposes an approach in which 
exact sequence entropy can be calculated efficient-
ly. Further experimentation is required to compare 
our approximation to these alternatives. 
An alternative approach that eliminates the 
overhead of entropy computations entirely is to 
estimate per-sentence uncertainty with ?1 ( )P t? , 
where t? is the Viterbi (best) tag sequence. We call 
this scheme QBUV. In essence, it selects a sample 
consisting of the sentences having the highest 
probability that the Viterbi sequence is wrong. To 
our knowledge, this is a novel approach to active 
learning. 
4 Experimental Results 
In this section, we examine the experimental setup, 
the prose and poetry data sets, and the results from 
using the various active learning algorithms on 
these corpora. 
4.1 Setup 
The experiments focus on the annotation scenario 
posed earlier, in which budgetary constraints af-
ford only some number x of sentences to be anno-
tated. The x-axis in each graph captures the num-
ber of sentences. For most of the experiments, the 
graphs present accuracies on the (Development) 
Test set. Later in this section, we present results for 
an alternate metric, namely number of words cor-
rected by the oracle. 
In order to ascertain the usefulness of the active 
learning approaches explored here, the results are 
presented against a baseline in which sentences are 
selected randomly from the Unannotated set. We 
consider this baseline to represent the use of a 
state-of-the-art tagger trained on the same amount 
of data as the active learner. Due to randomization, 
the random baseline is actually distinct from expe-
riment to experiment without any surprising devia-
tions. Also, each result curve in each graph 
represents the average of three distinct runs. 
Worth noting is that most of the graphs include 
active learning curves that are run to completion; 
namely, the rightmost extent of all curves 
represents the exhaustion of the Unannotated data. 
At this extreme point, active learning and random 
sample selection all have the same Training set. In 
the scenarios we are targeting, this far right side is 
not of interest. Points representing smaller amounts 
of annotated data are our primary interest. 
In the experiments that follow, we address sev-
eral natural questions that arise in the course of 
applying active learning. We also compare the va-
1 2
1 2
1 2
( | , , , )
( | , , , )
log ( | , , , )
i
i i i i i
ME i i i i i
t Tagset
ME i i i i i
H T w f t t
p t w f t t
p t w f t t
? ?
? ?
?
? ?
= ?
?
?
1 2
? ( | ) ( | , , , )
i
i i i i i
w w
H T w H T w f t t? ?
?
? ??
105
riants of QBU and QBC. For QBC, committee 
members divide the training set (at each stage of 
the active learning process) evenly. All committee 
members and final models are MEMMs. Likewise, 
all variants of QBU employ MEMMs. 
4.2 Data Sets 
The experiments involve two data sets in search 
of conclusions that generalize over two very dif-
ferent kinds of English text. The first data set con-
sists of English prose from the POS-tagged one-
million-word Wall Street Journal text in the Penn 
Treebank (PTB) version 3. We use a random sam-
ple of the corpus constituting 25% of the tradition-
al training set (sections 2?21). Initial Training data 
consists of 1% of this set. We employ section 24 as 
the Development Test set. Average sentence length 
is approximately 25 words. 
Our second experimental set consists of English 
poetry from the British National Corpus (BNC) 
(Godbert & Ramsay, 1991; Hughes, 1982; Raine, 
1984). The text is also fully tagged with 91 parts of 
speech from a different tag set than the one used 
for the PTB. The BNC XML data was taken from 
the files B1C.xml, CBO.xml, and H8R.xml. This 
results in a set of 60,056 words and 8,917 sen-
tences. 
4.3 General Results 
To begin, each step in the active learning process 
adds a batch of 100 sentences from the Unanno-
tated set at a time. Figure 3 demonstrates (using 
QBU) that the size of a query batch is not signifi-
cant in these experiments.  
The primary question to address is whether ac-
tive learning helps or not. Figure 4 demonstrates 
that QBU, QBUV, and QBC all outperform the 
random baseline in terms of total, per-word accu-
racy on the Test set, given the same amount of 
Training data. Figure 5 is a close-up version of 
Figure 4, placing emphasis on points up to 1000 
annotated sentences. In these figures, QBU and 
QBUV vie for the best performing active learning 
algorithm. These results appear to give some useful 
advice captured in Table 1. The first column in the 
table contains the starting conditions. The remain-
ing columns indicate that for between 800-1600 
sentences of annotation, QBUV takes over from 
QBU as the best selection algorithm. 
The next question to address is how much initial 
training data should be used; i.e., when should we 
start using active learning? The experiment in Fig-
ure 6 demonstrates (using QBU) that one should 
use as little data as possible for Initial Training 
Data. There is always a significant advantage to 
starting early. In the experiment documented in  
 
Figure 3. Varying the size of the query batch in active 
learning yields identical results after the first query batch.  
 
Figure 4. The best representatives of each type of active 
learner beat the baseline. QBU and QBUV trade off the 
top position over QBC and the Baseline. 
Figure 5. Close-up of the low end of the graph from Figure 
4. QBUV and QBU are nearly tied for best performance. 
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
Batch Query Size of 10 Sentences
Batch Query Size of 100 Sentences
Batch Query Size of 500 Sentences
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
ur
ac
y 
(%
)
Number of Sentences in Training Set
QBUV 
QBU
QBC
Baseline
 76
 78
 80
 82
 84
 86
 88
 90
 92
 100  1000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBUV 
QBU
QBC
Baseline
106
this figure, a batch query size of one was employed 
in order to make the point as clearly as possible. 
Larger batch query sizes produce a graph with sim-
ilar trends as do experiments involving larger Un-
annotated sets and other active learners. 
 
 100 200 400 800 1600 3200 6400 
QBU 76.26 86.11 90.63 92.27 93.67 94.65 95.42 
QBUV 76.65 85.09 89.75 92.24 93.72 94.96 95.60 
QBC 76.19 85.77 89.37 91.78 93.49 94.62 95.36 
Base 76.57 82.13 86.68 90.12 92.49 94.02 95.19 
Table 1. The best models (on PTB WSJ data) with various 
amounts of annotation (columns). 
 
Figure 6. Start active learning as early as possible for a 
head start. 
4.4 QBC Results 
An important question to address for QBC is 
what number of committee members produces the 
best results? There was no significant difference in 
results from the QBC experiments when using be-
tween 3 and 7 committee members. For brevity we 
omit the graph. 
4.5 QBU Results 
For Query by Uncertainty, the experiment in Fig-
ure 7 demonstrates that QBU is superior to QBUV 
for low counts, but that QBUV slightly overtakes 
QBU beyond approximately 300 sentences. In fact, 
all QBU variants, including the weighted version, 
surpassed the baseline. WQBU has been omitted 
from the graph, as it was inferior to straight-
forward QBU. 
4.6 Results on the BNC 
Next we introduce results on poetry from the Brit-
ish National Corpus. Recall that the feature set 
employed by the MEMM tagger was optimized for 
performance on the Wall Street Journal. For the 
experiment presented in Figure 8, all data in the 
Training and Unannotated sets is from the BNC, 
but we employ the same feature set from the WSJ 
experiments. This result on the BNC data shows 
first of all that tagging poetry with this tagger 
leaves a final shortfall of approximately 8% from 
the WSJ results. Nonetheless and more importantly, 
the active learning trends observed on the WSJ still 
hold. QBC is better than the baseline, and QBU 
and QBUV trade off for first place. Furthermore, 
for low numbers of sentences, it is overwhelmingly 
to one?s advantage to employ active learning for 
annotation. 
 
 
Figure 7. QBUV is superior to QBU overall, but QBU is 
better for very low counts. Both are superior to the ran-
dom baseline and the Longest Sentence (LS) baseline. 
 
Figure 8. Active learning results on the BNC poetry data. 
Accuracy of QBUV, QBU, and QBC against the random 
baseline. QBU and QBUV are nearly indistinguishable. 
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 10  100
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
1%
5%
10%
25%
 75
 80
 85
 90
 95
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBU
QBUV
LS
Baseline 
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 100  1000  10000
A
cc
u
ra
cy
 (
%
)
Number of Sentences in Training Set
QBU
QBUV
Baseline
QBC
107
4.7 Another Perspective 
Next, briefly consider a different metric on the ver-
tical axis. In Figure 9, the metric is the total num-
ber of words changed (corrected) by the oracle. 
This quantity reflects the cumulative number of 
differences between the tagger?s hypothesis on a 
sentence (at the point in time when the oracle is 
queried) and the oracle?s answer (over the training 
set). It corresponds roughly to the amount of time 
that would be required for a human annotator to 
correct the tags suggested by the model. This fig-
ure reveals that QBUV makes significantly more 
changes than QBU, QBC, or LS (the Longest Sen-
tence baseline). Hence, the superiority of QBU 
over QBUV, as measured by this metric, appears to 
outweigh the small wins provided by QBUV when 
measured by accuracy alone. That said, the random 
baseline makes the fewest changes of all. If this 
metric (and not some combination with accuracy) 
were our only consideration, then active learning 
would appear not to serve our needs. 
This metric is also a measure of how well a par-
ticular query algorithm selects sentences that espe-
cially require assistance from the oracle. In this 
sense, QBUV appears most effective. 
 
Figure 9. Cumulative number of corrections made by the 
oracle for several competitive active learning algorithms. 
QBU requires fewer corrections than QBUV. 
5 Conclusions 
Active learning is a viable way to accelerate the 
efficiency of a human annotator and is most effec-
tive when done as early as possible. We have pre-
sented state-of-the-art tagging results using a frac-
tion of the labeled data. QBUV is a cheap approach 
to performing active learning, only to be surpassed 
by QBU when labeling small numbers of sentences. 
We are in the midst of conducting a user study to 
assess the true costs of annotating a sentence at a 
time or a word at a time. We plan to incorporate 
these specific costs into a model of cost measured 
in time (or money) that will supplant the metrics 
reported here, namely accuracy and number of 
words corrected. As noted earlier, future work will 
also evaluate active learning at the granularity of a 
word or a subsequence of words, to be evaluated 
by the cost metric. 
References 
Anderson, B., and Moore, A. (2005). ?Active Learning for HMM: 
Objective Functions and Algorithms.? ICML, Germany. 
Brants, T., (2000). ?TnT -- a statistical part-of-speech tagger.? ANLP, 
Seattle, WA. 
Brill, E., and Wu, J. (1998). ?Classifier combination for improved 
lexical disambiguation.? Coling/ACL, Montreal, Quebec, Canada. 
Pp. 191-195.  
Day, D., et al (1997). ?Mixed-Initiative Development of Language 
Processing Systems.? ANLP, Washington, D.C. 
Engelson, S. and Dagan, I. (1996). ?Minimizing manual annotation 
cost in supervised training from corpora.? ACL, Santa Cruz, Cali-
fornia. Pp. 319-326. 
Freund, Y., Seung, H., Shamir, E., and Tishby, N. (1997). ?Selective 
sampling using the query by committee algorithm.? Machine 
Learning, 28(2-3):133-168.  
Godbert, G. and Ramsay, J. (1991). ?For now.? In the British National 
Corpus file B1C.xml. London: The Diamond Press (pp. 1-108).  
Hughes, T. (1982). ?Selected Poems.? In the British National Corpus 
file H8R.xml. London: Faber & Faber Ltd. (pp. 35-235).  
Kupiec, J. (1992). ?Robust part-of-speech tagging using a hidden 
Markov model.? Computer Speech and Language 6, pp. 225-242. 
Lewis, D., and Catlett, J. (1994). ?Heterogeneous uncertainty sam-
pling for supervised learning.? ICML. 
Lewis, D., and Gale, W. (1995). ?A sequential algorithm for training 
text classifiers: Corrigendum and additional data.? SIGIR Forum, 
29 (2), 13--19. 
Mann, G., and McCallum, A. (2007). "Efficient Computation of En-
tropy Gradient for Semi-Supervised Conditional Random Fields". 
NAACL-HLT. 
Marcus, M. et al (1999). ?Treebank-3.? Linguistic Data Consortium, 
Philadelphia, PA. 
Raiffa, H. and Schlaiffer, R. (1967). Applied Statistical Decision 
Theory. New York: Wiley Interscience.  
Raine, C. (1984). ?Rich.? In the British National Corpus file CB0.xml. 
London: Faber & Faber Ltd. (pp. 13-101).  
Ratnaparkhi, A. (1996). ?A Maximum Entropy Model for Part-Of-
Speech Tagging.? EMNLP. 
Roy, N., and McCallum, A. (2001a). ?Toward optimal active learning 
through sampling estimation of error reduction.? ICML. 
Roy, N. and McCallum, A. (2001b). ?Toward Optimal Active Learn-
ing through Monte Carlo Estimation of Error Reduction.? ICML, 
Williamstown. 
Seung, H., Opper, M., and Sompolinsky, H. (1992). ?Query by com-
mittee?.  COLT. Pp. 287-294. 
Thrun S., and Moeller, K. (1992). ?Active exploration in dynamic 
environments.? NIPS.  
Toutanova, K., Klein, D., Manning, C., and Singer, Y. (2003). ?Fea-
ture-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-
work.? HLT-NAACL. Pp. 252-259. 
Toutanova, K. and Manning, C. (2000). ?Enriching the Knowledge 
Sources Used in a Maximum Entropy Part-of-Speech Tagger.? 
EMNLP, Hong Kong. Pp. 63-70. 
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 10000
 100  1000  10000
N
u
m
b
er
 o
f 
C
h
an
g
ed
 W
o
rd
s
Number of Sentences in Training Set
QBUV 
QBU 
QBC 
Baseline 
LS 
108
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 810?820,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Probabilistic Morphological Analyzer for Syriac
Peter McClanahan, George Busby, Robbie Haertel, Kristian Heal ?,
Deryle Lonsdale?, Kevin Seppi, Eric Ringger
Department of Computer Science, ?Department of Linguistics,
?Center for the Preservation of Ancient Religious Texts (CPART)
Brigham Young University
Provo, Utah 84604 USA
http://nlp.cs.byu.edu/
Abstract
We define a probabilistic morphological ana-
lyzer using a data-driven approach for Syriac in
order to facilitate the creation of an annotated
corpus. Syriac is an under-resourced Semitic
language for which there are no available lan-
guage tools such as morphological analyzers.
We introduce novel probabilistic models for
segmentation, dictionary linkage, and morpho-
logical tagging and connect them in a pipeline
to create a probabilistic morphological analyzer
requiring only labeled data. We explore the per-
formance of models with varying amounts of
training data and find that with about 34,500
labeled tokens, we can outperform a reason-
able baseline trained on over 99,000 tokens and
achieve an accuracy of just over 80%. When
trained on all available training data, our joint
model achieves 86.47% accuracy, a 29.7% re-
duction in error rate over the baseline.
1 Introduction
Our objective is to facilitate the annotation of a large
corpus of classical Syriac (referred to simply as ?Syr-
iac? throughout the remainder of this work). Syr-
iac is an under-resourced Western Semitic language
of the Christian Near East and a dialect of Aramaic.
It is currently employed almost entirely as a liturgi-
cal language but was a true spoken language up un-
til the eighth century, during which time many pro-
lific authors wrote in Syriac. Even today there are
texts still being composed in or translated into Syr-
iac. By automatically annotating these texts with lin-
guistically useful information, we will facilitate sys-
tematic study by scholars of Syriac, the Near East,
and Eastern Christianity. Furthermore, languages
that are linguistically similar to Syriac (e.g., Arabic
and Hebrew) may benefit from the methodology pre-
sented here.
Our desired annotations include morphological
segmentation, links to dictionary entries, and mor-
phological attributes. Typically, annotations of this
kind are made with the assistance of language tools,
such as morphological analyzers, segmenters, or
part-of-speech (POS) taggers. Such tools do not
exist for Syriac, but some labeled data does exist:
Kiraz (1994) compiled an annotated version of the
Peshitta New Testament (1920) and a concordance
thereof. We aim to replicate this kind of annota-
tion on a much larger scale with more modern tools,
building up from the labeled New Testament data,
our only resource. Motivated by this state of affairs,
our learning and annotation framework requires only
labeled data.
We approach the problem of Syriac morphological
annotation by creating five probabilistic sub-models
that can be trained in a supervised fashion and com-
bined in a joint model of morphological annota-
tion. We introduce novel algorithms for segmenta-
tion, dictionary linkage, and morphological tagging.
We then combine these sub-models into a joint n-
best pipeline. This joint model outperforms a strong,
though na?ve, baseline for all amounts of training
data over about 9,900 word tokens.
1.1 Syriac Background
Since Syriac is an abjad, its writing system does
not require vowels. As a dialect of Aramaic, it
is an inflected language with a templatic (non-
concatenative) morphology, based on a system of
triliteral consonantal roots, with prefixes, suffixes,
infixes, and enclitic particles. Syriac is written from
810
right to left. For the purposes of this work, all Syr-
iac is transliterated according to the Kiraz (1994)
transliteration1 and is written left-to-right whenever
transliterated; the Syriac appearing in the Serto script
in this paper is shown right-to-left.
Since there is no standardized nomenclature for
the parts of a Syriac word, we define the following
terms to facilitate the definitions of segmentation,
dictionary linkage, and morphological tagging:
? word token - contiguous characters delimited by
whitespace and/or punctuation
? stem - an inflected form of the baseform and
the main part of the word to which prefixes and
suffixes can be attached; the affixes do not in-
flect the stem but include prepositions, object
suffixes, and enclitic pronouns
? baseform - the dictionary citation form; also
known as a lexeme or lemma
? root - the form from which the baseform is de-
rived
To clarify, we will use an example word token
???????, LMLCCON, which means ?to your (mas-
culine plural) king?. For this word, the stem is ???,
MLC; the baseform is ????, MLCA ?king?; and the
root is ???,MLC. To clarify, note that the word token
(including the stem) can be spoken and written with
vowels as diacritics; however, since the vowels are
not written in common practice and since most text
does not include them, this work omits any indica-
tion of vowels. Furthermore, the stem is an inflected
baseform and does not necessarily form a word on
its own. Also, the (unvocalized) stem and root are
not necessarily identical. In Syriac, the same root
???, MLC is the foundation for other words such as
promise, counsel, deliberate, reign, queen, kingdom,
and realm.
1.2 Sub-tasks
Segmentation, or tokenization as it is sometimes
called (e.g., Habash and Rambow, 2007), is the pro-
cess of dividing a word token into its prefix(es) (if
any), a stem, and a suffix (if any). For Syriac, each
1According to this transliteration all capital letters including
A (?, olaph) and O (?, waw) are consonants. Additionally, the
semi-colon (;), representing (?, yod), is also a consonant.
word token consists of exactly one stem, from zero
to three prefixes, and zero or one suffix. Each pre-
fix is exactly one character in length. Segmenta-
tion does not include the process of parsing the stem
for its inflectional morphology; that step is handled
separately in subsequent processes described below.
While segmenting a Syriac word, we can handle all
prefixes as a single unit. It is trivial to segment a
prefix cluster into its individual prefixes (one charac-
ter per prefix). Suffixes may be multiple characters
in length and encode the morphological attributes of
the suffix itself (not of the stem); the suffix usually
encodes the object of the stem and has its own gram-
matical attributes, which we list later. As an example
of token segmentation, for the word token ???????,
LMLCCON, the prefix is ?, L ?to?, the stem is ???,
MLC ?king?, and the suffix is ???, CON ?(masculine
plural) your?.
Dictionary linkage is the process of linking a stem
to its associated baseform and root. In most Syriac
dictionaries, all headwords are either baseforms or
roots, and for a given word these are the only rele-
vant entries in the dictionary. Each Syriac stem is
derived from a baseform, and each baseform is de-
rived from a root. There is ambiguity in this cor-
respondence which can be caused by, among other
things, homographic stems generated from different
roots or even from homographic roots. As such, link-
age may be thought of as two separate processes: (1)
baseform linkage, where the stem is mapped to its
most likely baseform; and (2) root linkage, where
the baseform is mapped to its most likely root. For
our example ???????, LMLCCON, baseform linkage
would map stem ???,MLC to baseform ????,MLCA,
and root linkage would map baseform ????,MLCA to
root ???, MLC.
Morphological tagging is the process of labeling
each word token with its morphological attributes.
Morphological tagging may be thought of as two
separate tagging tasks: (1) tagging the stem and (2)
tagging the suffix. For Syriac, scholars have defined
for this task a set of morphological attributes con-
sisting of twelve attributes for the stem and four at-
tributes for the suffix. The attributes for the stem
are as follows: grammatical category, verb conju-
gation, aspect, state, number, person, gender, pro-
noun type, demonstrative category, noun type, nu-
meral type, and participle type. The morphological
811
Attribute Value
Grammatical Category noun
Verb Conjugation N/A
Aspect N/A
State emphatic
Number singular
Person N/A
Gender masculine
Pronoun Type N/A
Demonstrative Category N/A
Noun Type common
Numeral Type N/A
Participle Type N/A
Table 1: The values for the morphological attributes of
the stem ???,MLC, ?king?.
Attribute Value
Gender masculine
Person second
Number plural
Contraction normal suffix
Table 2: The values for the morphological attributes of
the suffix ???, CON, ?(masculine plural) your?.
attributes for the suffix are gender, person, number,
and contraction. The suffix contraction attribute en-
codes whether the suffix is normal or contracted, a
phonological process involving the attachment of an
enclitic pronoun to a participle. These morphologi-
cal attributes were heavily influenced by those used
by Kiraz (1994), but were streamlined in order to fo-
cus directly on grammatical function. During mor-
phological tagging, each stem is labeled for each of
the stem attributes, and each suffix is labeled for each
of the suffix attributes. For a given grammatical cat-
egory (or POS), only a subset of the morphological
attributes is applicable. For those morphological at-
tributes (both of the stem and of the suffix) that do
not apply, the correct label is ?N/A? (not applicable).
Tables 1 and 2 show the correct stem and suffix tags
for the word ???????, LMLCCON.
The remainder of the paper will proceed as fol-
lows: Section 3 outlines our approach. In Section 4,
we describe our experimental setup; we present re-
sults in Section 5. Section 6 contrasts previous work
with our approach. Finally, in Section 7 we briefly
conclude and offer directions for future work.
2 The Syromorph Approach
Since lack language tools, we focus on automatically
annotating Syriac text in a data-driven fashion based
on the labeled data we have available. Since seg-
mentation, linkage, and morphological tagging are
not mutually independent tasks, we desire models
for the sub-tasks to influence each other. To accom-
modate these requirements, we use a joint pipeline
model (Finkel et al, 2006). In this section, we will
first discuss this joint pipeline model, which we call
syromorph. We then examine each of the individual
sub-models.
2.1 Joint Pipeline Model
Our approach is to create a joint pipeline model con-
sisting of a segmenter, a baseform linker, a root
linker, a suffix tagger, and a stem tagger. Figure 1
shows the dependencies among the sub-models in
the pipeline for a single word. Each sub-model
(oval) has access to the data and predictions (rect-
angles) indicated by the arrows. For example, for a
given word, the stem tagger has access to the previ-
ously predicted stem, baseform, root, and suffix tag.
The baseform linker has access to the segmentation,
most importantly the stem.
The training of syromorph is straightforward.
Each of the individual sub-models is trained sepa-
rately on the true labeled data. Features are extracted
from the local context in the sentence. The local con-
text consists first of predictions for the entire sen-
tence from earlier sub-tasks (those sub-tasks upon
which the sub-task in question depends). We cre-
ated the dependencies shown in Figure 1 taking into
account the difficulty of the tasks and natural depen-
dencies in the language. In addition to the predic-
tions for the entire sentence from previous sub-tasks,
the local context also includes the previous o tags of
the current sub-task, as the standard order o Markov
model does. For example, when the stem tagger is
being trained on a particular sentence, the local con-
text consists of the words in the sentence, the pre-
dicted segmentation, baseform, root, and suffix tags
for each word in the sentence, and additionally the
labels for the previous o stems. To further elaborate
812
Figure 1: The syromorph model. Each rectangle is an
input or output and each oval is a process employing a
sub-model.
on the example, since features are extracted from the
local context, for stem tagging we extract features
such as current stem, previous stem, current base-
form, previous baseform, current root, previous root,
current suffix tags, and previous suffix tags. (Here,
?previous? refers to labels on the immediately pre-
ceding word token.)
2.2 Segmentation
The syromorph segmentation model is a hybrid
word- and consonant-level model, based on the
model of Haertel et al (2010) for data-driven dia-
critization. Each of our probabilistic sequence mod-
els is a maximum entropy Markov model (MEMM).
Haertel et al (2010) showed that the distribution
over labels is different for known and words and rare
words. In this work, we only consider words not
seen in training (i.e., ?unknown?) to be rare. Follow-
ing Haertel et al?s (2010) model, a separate model
is trained for each word type seen in training with
the intent of choosing the best segmentation given
that word. This approach is closely related to the
idea of ambiguity classes mentioned in Haji? and
Hladk? (1998).
To handle unknown words, we back off to a
consonant-level model. Our consonant-level seg-
mentation model uses the notion of BI (Beginning
and Inside) tags, which have proven successful in
named-entity recognition. Since there are three
labels in which we are interested (prefix, stem, and
suffix), we apply the beginning and inside notion
to each of them to create six tags: BEGINNING-
PREFIX, INSIDE-PREFIX, BEGINNING-STEM,
INSIDE-STEM, BEGINNING-SUFFIX, and
INSIDE-SUFFIX. We train an MEMM to predict
one of these six tags for each consonant. Further-
more, we constrain the decoder to allow only legal
possible transitions given the current prediction,
so that prefixes must come before stems and stems
before suffixes. In order to capture the unknown
word distributions, we train the consonant-level
model on words occurring only once during training.
We call this word- and consonant-level segmenta-
tion model hybrid. As far as we are aware, this is a
novel approach to segmentation.
2.3 Dictionary Linkage
For dictionary linkage, we divide the problem into
two separate tasks: baseform linkage and root link-
age. For both of these tasks, we use a hybrid model
similar to that used for segmentation, consisting of
a collection of separate MEMMs for each word type
(either a stem or baseform, depending on the linker)
and amodel for unknown (or rare) words. For the un-
known words, we compare two distinct approaches.
The first approach for unknown words is based
on the work of Chrupa?a (2006), including the Mor-
fette system. Instead of predicting a baseform given
a stem, we predict what Chrupa?a calls a lemma-
class. A lemma-class is the transformation specified
by the minimum edit distance between the baseform
(which he calls a lemma) and the stem. The trans-
formation is a series of tuples, where each tuple in-
cludes (1) whether it was an insertion or deletion,
(2) the letter inserted or deleted, and (3) the position
of the insertion or deletion in the string (positions
begin at zero). All operations are assumed to oc-
cur sequentially, as in Morfette. For example, the
transformation of XE;N to XEA would proceed as
follows: delete ; from position 2, insert A into po-
sition 2, delete N from position 3.
813
In hybrid-morfette baseform linkage (respec-
tively, root linkage), we predict a lemma-class (i.e.,
transformation) for each baseform (respectively,
root). The predicted transformation is then applied
to the stem (respectively, baseform) in order to con-
struct the actual target baseform (respectively, root).
The advantage to this method is that common trans-
formations are grouped into a single class, thereby
allowing the model to generalize and adequately
predict baseforms (and roots) that have not been
seen during training, but whose transformations have
been seen. This model is trained on all words in or-
der to capture as many transformations as possible.
The second approach for unknown words, called
hybrid-maxent, uses an MEMM trained on all
words seen in training. Given a stem (respectively,
baseform), this approach predicts only baseforms
(respectively, roots) that were observed in training
data. Thus, this method has a distinct disadvan-
tage when it comes to predicting new forms. This
approach corresponds directly to the approach to
handling unknown -words by Toutanova and Man-
ning (2000) for POS tagging.
With regard to baseform and root linkage, we do
not use the dictionary to constrain possible base-
forms or roots, since we make no initial assumptions
about the completeness of a dictionary.
2.4 Morphological Tagging
For morphological tagging, we break the task into
two separate tasks: tagging the suffix and tagging
the stem. Since there are a number of values that
need to be predicted, we define two ways to ap-
proach the problem. We call the first approach the
monolithic approach, in which the label is the con-
catenation of all the morphological attribute values.
Table 3 illustrates the tagging of an example sen-
tence: the stem tag and suffix tag columns contain
the monolithic tags for stem tagging and suffix tag-
ging. We use an MEMM to predict a monolithic tag
for each stem or suffix and call this model maxent-
mono. No co-occurrence restrictions among related
or complementary morphological tags are directly
enforced. Co-occurrence patterns are observed in
the data, learned, and encoded in the models of the
tagging process. It is worth noting further that con-
straints provided by the baseforms ? predicted by
dictionary linkage ? on the morphological attributes
are likewise not directly enforced. Enforcement of
such constraints would require an infusion of expert
knowledge into the system.
The second approach is to assume that morpho-
logical attributes are independent of each other. We
call this the independent approach. Here, each tag
is predicted by a tagger for a single morphological
attribute. For example, the gender model is ignorant
of the other 11 sub-tags during stem tagging. Using
its local context (which does not include other stem
sub-tags), the model predicts the best gender for a
given word. The top prediction of each of these tag-
gers (12, for stem tagging) is then combined na?vely
with no notion of what combinations may be valid
or invalid. We use MEMMs for each of the single-
attribute taggers. This model is calledmaxent-ind.
2.5 Decoding
Our per-task decoders are beam decoders, with
beam-size b. In particular, we limit the number of
per-stage back-pointers to b due to the large size of
the tagset for some of our sub-models. Although
Viterbi decoding produces the most probable label
sequence given a sequence of unlabeled words, it is
potentially intractible on our hybrid models due to
the unbounded dependence on previous consonant-
level decisions. Our beam decoders produce a good
approximation when tuned properly.
Decoding in syromorph consists of extending the
per-task decoders to allow transitions from each sub-
model to the next sub-model in the pipe. For exam-
ple, in our pipeline, the first sub-model is segmen-
tation. We predict the top n segmentations for the
sentence (i.e., sequences of segmentations), where n
is the number of transitions tomaintain between each
sub-task. Then, we run the remaining sub-tasks with
each of the n sequences as a possible context. After
each sub-task is completed, we narrow the number
of possible contexts back to n.
We swept b and n for various values, and found
b = 5 and n = 5 to be good values that balanced
between accuracy and time; larger values saw only
minute gains in accuracy.
814
Word Transliteration Pre. Stem Suffix Baseform Root Suff. Tags Stem Tags
????? OEBDT O EBDT EBD EBD 0000 011012200000
???? ANON ANON HO HO 0000 300023222000
???? LALHN L ALH N ALHA ALH 1011 200310200200
?????? MLCOTA MLCOTA MLCOTA MLC 0000 200310300200
????? OCHNA O CHNA CHNA CHN 0000 200320200200
?????? OMLCA O MLCA MLCA MLC 0000 200320200200
Table 3: Part of a labeled Syriac sentence ?????? ????? ?????? ???? ???? ?????, ?And you have made them a kingdom and
priests and kings for our God.? (Revelation 5:10)
3 Experimental Setup
We are using the Syriac Peshitta New Testament in
the form compiled by Kiraz (1994).2 This data is
segmented, annotated with baseform and root, and
labeled with morphological attributes. Kiraz and
others in the Syriac community refined and corrected
the original annotation while preparing a digital and
print concordance of the New Testament. We aug-
mented Kiraz?s version of the data by segmenting
suffixes and by streamlining the tagset. The dataset
consists of 109,640 word tokens.
Table 3 shows part of a tagged Syriac sentence us-
ing this tagset. The suffix and stem tags consist of
indices representing morphological attributes. In the
example sentence, the suffix tag 1011 represents the
values ?masculine?, ?N/A?, ?plural?, ?normal suf-
fix? for the suffix attributes of gender, person, num-
ber, and contraction. Each value of 0 for each stem
and suffix attribute represents a value of ?N/A?, ex-
cept for that of grammatical category, which always
must have a value other than ?N/A?. Therefore, the
suffix tag 0000 means there is no suffix.
For the stem tags, the attribute order is the same
as that shown in Table 1 from top to bottom. The
following describes the interpretation of the stem
values represented in Table 3. Grammatical cate-
gory values 0, 2, and 3 represent ?verb?, ?noun?,
and ?pronoun?, respectively. (Grammatical cate-
gory has no ?N/A? value.) The verb conjugation
value 1 represents ?peal conjugation?. Aspect value
1 represents ?perfect?. State value 3 represents ?em-
phatic?. Number values 1 and 2 represent ?singular?
and ?plural?. Person values 2 and 3 represent ?sec-
2The Way International, a Biblical research ministry, anno-
tated this version of the New Testament by hand and required
15 years to do so.
ond? and ?third? person. Gender values 2 and 3 rep-
resent ?masculine? and ?feminine?. Pronoun type
value 2 represents ?demonstrative?. Demonstrative
category value 2 represents ?far?. Finally, noun type
2 represents ?common?. The last two columns of 0
represent ?N/A? for numeral type and particle type.
We implement five sub-tasks: segmentation, base-
form linkage, root linkage, suffix tagging, and stem
tagging. We compare each sub-task to a na?ve ap-
proach as a baseline. In addition to desiring good
sub-models, we also want a joint pipeline model that
significantly outperforms the na?ve joint approach,
which is formed by using each of the following base-
lines in the pipeline framework.
The baseline implementation of segmentation is to
choose the most-frequent label: for a given word,
the baseline predicts the segmentation with which
that word appeared most frequently during training.
For unknown words, it chooses the largest prefix
and largest suffix that is possible for that word from
the list of prefixes and suffixes seen during train-
ing. (This na?ve baseline for unknown words does
not take into account the fact that the stem is often at
least three characters in length.)
For dictionary linkage, the baseline is similar:
both baseform linkage and root linkage use the most-
frequent label approach. Given a stem, the baseline
baseform linker predicts the baseform with which
the stem was seen most frequently during training;
likewise, the baseline root linker predicts the root
from the baseform in a similar manner. For the un-
known stem case, the baseline baseform linker pre-
dicts the baseform to be identical to the stem. For
the unknown baseform case, the baseline root linker
predicts a root identical to the first three consonants
of the baseform, since for Syriac the root is exactly
815
three consonants in a large majority of the cases.
The baselines for stem and suffix tagging are the
most-frequent label approaches. These baselines
are similar to maxent-mono and maxent-ind, us-
ing the monolithic and independent approaches used
by maxent-mono and maxent-ind. The difference
is that instead of using maximum entropy, the na?ve
most-frequent approach is used in its place.
The joint baseline tagger uses each of the compo-
nent baselines in then-best joint pipeline framework.
Because this framework is modular, we can trivially
swap in and out different models for each of the sub-
tasks.
4 Experimental Results
Since we are focusing on under-resourced circum-
stances, we sweep the amount of training data and
produce learning curves to better understand how
our models perform in such circumstances. For each
point in our learning curves and for all other eval-
uations, we employ ten-fold cross-validation. The
learning curves use the chosen percentage of the data
for training and a fixed-size test set from each fold
and report the average accuracy.
The reported task accuracy requires the entire out-
put for that task to be correct in order to be counted as
correct. For example, during stem tagging, if one of
the sub-tags is incorrect, then the entire tag is said to
be incorrect. Furthermore, for syromorph, the out-
puts of every sub-task must be correct in order for
the word token to be counted as correct.
Moving beyond token-level metrics, in order to
understand performance of the system at the level
of individual decisions (including N/A decisions),
we compute decision-level accuracy: we call this
metric total-decisions. For the syromorph method
reported here, there are a total of 20 decisions: 2
for segmentation (prefix and suffix boundaries), 1
for baseform linkage, 1 for root linkage, 4 for suf-
fix tagging, and 12 for stem tagging. This accuracy
helps us to assess the number of decisions a human
annotator would need to correct, if data were pre-
annotated by a given model. Excluding N/A deci-
sions, we compute per-decision coverage and accu-
racy. These metrics are called applicable-coverage
and applicable-accuracy.
We show results on both the individual sub-tasks
and the entire joint task. Since previous sub-
tasks can adversely affect tasks further down in
the pipeline, we evaluate the sub-models by plac-
ing them in the pipeline with other (simulated) sub-
models that correctly predict every instance. For
example, when testing a root linker, we place the
root linker to be evaluated in the pipeline with a
segmenter, baseform linker, and taggers that return
the correct label for every prediction. This gives an
upper-bound for the individual model, removes the
possibility of error propagation, and shows how well
that model performs without the effects of the other
models in the pipeline.
For our results, unknown accuracy is the accuracy
of unknown instances, specific to the task, at training
time. In the case of baseform linkage, for example,
a stem is considered unknown if that stem was not
seen during training. It is therefore possible to have
a known word with an unknown stem and vice versa.
As in other NLP problems, unknown instances are a
manifestation of training data sparsity.
4.1 Baseline Results
Table 4 is grouped by sub-task and reports the results
of each of the baseline sub-tasks in the first row of
each group. Each of the baselines performs surpris-
ingly well. The accuracies of the baselines for most
of the tasks are high because the ambiguity of the
labels given the instance is quite low: the average
ambiguity across word types for segmentation, base-
form linkage, root linkage, suffix tagging, and stem
tagging are 1.01, 1.05, 1.02, 1.35, and 1.47, respec-
tively.
Preliminary experiments indicated that if we had
trained a baseline model using a single prediction (a
monolithic concatenation of the predictions for all
tasks) per token rather than separating the tasks, the
baseline tagging accuracy would have been lower.
Note that the unknown tagging accuracy for the
monolithic suffix tagger is not applicable, because
there were no test suffixes that were not seen during
training.
4.2 Individual Model Results
Table 4 also shows the results for the individual
models. In the table, SEG, BFL, RTL, SUFFIX,
and STEM represent segmentation, baseform link-
age, root linkage, suffix tagging, and stem tagging,
816
Model Total Known Unk
SE
G baseline 96.75 99.64 69.11
hybrid 98.87 99.70 90.83
BF
L baseline 95.64 98.45 22.28
hybrid-morfette 96.19 98.05 78.40
hybrid-maxent 96.19 99.15 67.86
RT
L baseline 98.84 99.56 80.20
hybrid-morfette 99.05 99.44 88.86
hybrid-maxent 98.34 99.45 69.30
SU
FF
IX
mono. baseline 98.75 98.75 N/A
ind. baseline 96.74 98.78 0.01
maxent-mono 98.90 98.90 N/A
maxent-ind 98.90 98.90 N/A
ST
EM
mono. baseline 83.08 86.26 0.01
ind. baseline 53.24 86.90 0.00
maxent-mono 89.48 92.87 57.04
maxent-ind 88.43 90.26 40.59
Table 4: Word-level accuracies for the individual sub-
models used in the syromorph approach.
respectively. Even though the baselines were high,
each individual model outperformed its respective
baseline, with the exception of the root linker. Two
of the most interesting results are the known ac-
curacy of the baseform linkers hybrid-maxent and
hybrid-morfette. As hybrid models, the difference
between them lies only in the treatment of unknown
words; however, the known accuracy of the mor-
fette model drops fairly significantly. This is due
to the unknown words altering the weights for fea-
tures in which those words occur. For instance, if
the previous word is unknown and a baseform that
was never seen was predicted, then the weights on
the next word for all features that contain that un-
known word will be quite different than if that pre-
vious word were a known word.
It is also worth noting that the stem tagger is by
far the worst model in this group of models, but it is
also the most difficult task. The largest gains in im-
proving the entire systemwould come from focusing
attention on that task.
4.3 Joint Model Results
Table 5 shows the accuracies for the joint mod-
els. The joint model incorporating ?maxent? vari-
ants performs best overall and on known cases. The
Model Total Known Unk
Baseline 80.76 85.74 28.07
Morfette Monolithic 85.96 89.85 44.86
Maxent Monolithic 86.47 90.77 40.93
Table 5: Word-level accuracies for various joint syro-
morph models.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  10  20  30  40  50  60  70  80  90  100
To
ta
l A
cc
ur
ac
y
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 2: The total accuracy of the joint model.
joint model incorporating the ?morfette? variants
performs best on unknown cases.
Decision-level metrics for the SEG:hybrid /
BFL and RTL:hybrid-maxent / SUFFIX and
STEM:maxent-mono model are as follows: for
total-decisions, the model achieves an accuracy
of 97.08%, compared to 95.50% accuracy for the
baseline, amounting to a 35.11% reduction in error
rate over the baseline; for applicable-coverage and
applicable-accuracy this model achieved 93.45%
and 93.81%, respectively, compared to the baseline?s
90.03% and 91.44%.
Figures 2, 3, and 4 show learning curves for to-
tal, known, and unknown accuracies for the joint
pipeline model. As can be seen in Figure 2, by the
time we reach 10% of the training data, syromorph
is significantly better than the baseline. In fact, at
35% of the training data, our joint pipeline model
outperforms the baseline trained with all available
training data.
Figure 3 shows the baseline performing quite well
on known words with very low amounts of data.
Since the x-axis varies the amount of training data,
the meaning of ?known? and ?unknown? evolves as
we move to the right of the graph; consequently, the
817
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0  10  20  30  40  50  60  70  80  90  100
Kn
ow
n 
Ac
cu
ra
cy
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 3: The accuracy of the joint model on known
words.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0  10  20  30  40  50  60  70  80  90  100
Un
kn
ow
n 
Ac
cu
ra
cy
Percentage of Training Data
baseline
hybrid / hybrid-maxent / maxent-mono
hybrid / hybrid-morfette / maxent-mono
Figure 4: The accuracy of the joint model on unknown
words.
left and right sides of the graph are incomparable.
When the percentage of training data is very low,
the percentage of unknown words is high, and the
number of known words is relatively low. On this
dataset, the more frequent words tend to be less am-
biguous, giving the most-frequent taggers an advan-
tage in a small random sample. For this reason, the
baseline performs very well on known accuracy with
lower amounts of training data.
Figure 4 clearly shows that hybrid-morfette link-
ers outperform hybrid-maxent linkers on unknown
words. However, Figures 2- 4 show that hybrid-
morfette?s advantage on unknown words is coun-
teracted by its lower performance on known words;
therefore, it has slightly lower overall accuracy than
hybrid-maxent.
5 Related Work
The most closely related work to our approach is
the Morfette tool for labeling inflectional morphol-
ogy (Chrupa?a et al, 2008). Chrupa?a et al cre-
ated a tool that labels Polish, Romanian, and Span-
ish with morphological information as well as base-
forms. It is a supervised learning approach that
requires data labeled with both morphological tags
and baseforms. This approach creates two separate
models (a morphological tagger and a lemmatizer)
and combines the decoding process in order to cre-
ate a joint model that predicts both morphological
tags and the baseform. Morfette uses MEMMs for
both models and has access to predicted labels in
the feature set. Reported accuracy rates are 96.08%,
93.83%, and 81.19% for joint accuracy on datasets
trained with fewer than 100,000 tokens for Roma-
nian, Spanish, and Polish, respectively. The major
difference between this work and ours is the degree
of morphological analysis required by the languages.
Chrupa?a et al neglect segmentation, a task not as
intuitive for their languages as it is for Syriac. These
languages also require only linkage to a baseform, as
no root exists.
Also closely related is the work of Daya, Roth, and
Wintner (2008) on Hebrew. The authors use the no-
tion of patterns into which root consonants are in-
jected to compose Semitic words. They employ lin-
guistic knowledge (specifically, lists of prefixes, suf-
fixes, and ?knowledge of word-formation processes?
combined with SNoW, a multi-class classifier that
has been shown to work well in other NLP tasks.
The major difference between this approach and the
method presented in this paper is that this method
does not require the extra knowledge required to en-
code word-formation processes. A further point of
difference is our use of hybrid word- and consonant-
level models, after Haertel et al (2010). Their work
builds on the work of Shacham and Wintner (2007),
which is also related to that of Habash and Rambow,
described below.
Work by Lee et al (2003) is themost relevant work
for segmentation, since they segment Arabic, closely
related to Syriac, with a data-driven approach. Lee
et al use an unsupervised algorithm bootstrapped
with manually segmented data to learn the segmen-
tation for Arabic without any additional language re-
818
sources. At the heart of the algorithm is a word-level
trigram language model, which captures the correct
weights for prefixes and suffixes. They report an ac-
curacy of 97%. We opted to use our own segmenter
because we felt we could achieve higher accuracy
with the hybrid segmenter.
Mohamed and K?bler (2010a, 2010b) report on
closely related work for morphological tagging.
They use a data-driven approach to find the POS tags
for Arabic, using both word tokens and segmented
words as inputs for their system. Although their seg-
mentation performance is high, they report that ac-
curacy is lower when first segmenting word tokens.
They employ TiMBL, a memory-based learner, as
their model and report an accuracy of 94.74%.
Habash and Rambow (2005) currently have the
most accurate approach for Arabic morphological
analysis using additional language tools. They focus
on morphological disambiguation (tagging), given
morphological segmentation in the output of the
morphological analyzer. For each word, they first
run it through the morphological analyzer to reduce
the number of possible outputs. They then train a
separate Support Vector Machine (SVM) for each
morphological attribute (ten in all). They look at dif-
ferent ways of combining these outputs to match an
output from the morphological analyzer. For their
best model, they report an overall tag accuracy of
97.6%.
Others have used morphological analyzers and
other language tools for morphological disambigua-
tion coupled with segmentation. The following
works exemplify this approach: Diab et al (2004)
use a POS tagger to jointly segment, POS tag, and
chunk base-phrases for Arabic with SVMs. Kudo
et al (2004) use SVMs to morphologically tag
Japanese. Smith et al (2005) use SVMs for seg-
mentation, lemmatization, and POS tagging for Ara-
bic, Korean, and Czech. Petkevi? (2001) use a mor-
phological analyzer and additional simple rules for
morphological disambiguation of Czech. Mansour
et al (2007) and Bar-haim et al (2008) both use hid-
denMarkov models to POS tag Hebrew, with the lat-
ter including segmentation as part of the task.
For Syriac, a morphological analyzer is not avail-
able. Kiraz (2000) created a Syriac morphological
analyzer using finite-state methods; however, it was
developed on outdated and now inaccessible equip-
ment and is no longer working or available to us.
6 Conclusions and Future Work
We have shown that we can effectively model seg-
mentation, linkage to headwords in a dictionary, and
morphological tagging using a joint model called sy-
romorph. We have introduced novel approaches for
segmentation, dictionary linkage, and morphologi-
cal tagging, and each of these approaches has out-
performed its corresponding na?ve baseline. Further-
more, we have shown that for Syriac, a data-driven
approach seems to be an appropriate way to solve
these problems in an under-resourced setting.
We hope to use this combined model for pre-
annotation in an active learning setting to aid anno-
tators in labeling a large Syriac corpus. This corpus
will contain data spanning multiple centuries and a
variety of authors and genres. Future work will re-
quire addressing issues encountered in this corpus.
In addition, there is much to do in getting the over-
all tag accuracy closer to the accuracy of individual
decisions. We leave further feature engineering for
the stem tagger and the exploration of possible new
morphological tagging techniques for future work.
Finally, future work includes the application of the
syromorph methodology to other under-resourced
Semitic languages.
Acknowledgments
We would like to thank David Taylor of the Oriental
Institute at Oxford University for collaboration on
the design of the simplified tagset. We also recog-
nize the assistance of Ben Hansen of BYU on a sub-
set of the experimental results. Finally, we would
like to thank the anonymous reviewers for helpful
guidance.
References
Roy Bar-haim, Khalil Sima?an, and Yoad Winter. 2008.
Part-of-speech tagging ofmodern hebrew text. Natural
Language Engineering, 14(2):223?251.
British and Foreign Bible Society, editors. 1920. The
New Testament in Syriac. Oxford: Frederick Hall.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08).
819
Grzegorz Chrupa?a. 2006. Simple data-driven
context-sensitive lemmatization. In Procesamiento del
Lenguaje Natural, volume 37, pages 121 ? 127.
Ezra Daya, Dan Roth, and Shuly Wintner. 2008.
Identifying Semitic roots: Machine learning with
linguistic constraints. Computational Linguistics,
34(3):429?448.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic tagging of Arabic text: From
raw text to base phrase chunks. In Proceedings of
the 5th Meeting of the North American Chapter of
the Association for Computational Linguistics/Human
Language Technologies Conference (HLT-NAACL04),
pages 149?152.
Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cascading
errors: Approximate Bayesian inference for linguistic
annotation pipelines. In EMNLP ?06: Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, pages 618?626. Association for
Computational Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 573?580. Asso-
ciation for Computational Linguistics.
Nizar Habash and Owen Rambow. 2007. Arabic diacriti-
zation through full morphological tagging. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 53?56. Association for Computational Lin-
guistics.
Robbie Haertel, Peter McClanahan, and Eric K. Ring-
ger. 2010. Automatic diacritization for low-resource
languages using a hybrid word and consonant cmm.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 519?527.
Association for Computational Linguistics.
Jan Haji? and Barbora Hladk?. 1998. Tagging inflective
languages: Prediction of morphological categories for
a rich, structured tagset. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics,
pages 483?490. Association for Computational Lin-
guistics.
George Kiraz. 1994. Automatic concordance generation
of Syriac texts. In R. Lavenant, editor, VI Symposium
Syriacum 1992, pages 461?.
George Anton Kiraz. 2000. Multitiered nonlinear mor-
phology using multitape finite automata: a case study
on Syriac and Arabic. Computational Linguistics,
26:77?105.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to japanese
morphological analysis. In Proceedings of EMNLP,
pages 230?237.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 399?406. Associ-
ation for Computational Linguistics.
Saib Mansour, Khalil Sima?an, and Yoad Winter. 2007.
Smoothing a lexicon-based pos tagger for Arabic and
Hebrew. In Semitic ?07: Proceedings of the 2007
Workshop on Computational Approaches to Semitic
Languages, pages 97?103. Association for Computa-
tional Linguistics.
Emad Mohamed and Sandra K?bler. 2010a. Arabic
part of speech tagging. In Proceedings of the Sev-
enth International Language Resources and Evalua-
tion (LREC?10).
Emad Mohamed and Sandra K?bler. 2010b. Is Arabic
part of speech tagging feasible without word segmen-
tation? In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
705?708. Association for Computational Linguistics.
Vladim?r Petkevi?. 2001. Grammatical agreement
and automatic morphological disambiguation of inflec-
tional languages. In TSD ?01: Proceedings of the
4th International Conference on Text, Speech and Dia-
logue, pages 47?53. Springer-Verlag.
Danny Shacham and Shuly Wintner. 2007. Morpholog-
ical disambiguation of Hebrew: a case study in clas-
sifier combination. In Proceedings of EMNLP-CoNLL
2007, the Conference on Empirical Methods in Natural
Language Processing and the Conference on Compu-
tational Natural Language Learning. Association for
Computational Linguistics.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 475?482. Association for Computational Lin-
guistics.
K. Toutanova and C. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proceedings of EMNLP, pages
63?70.
820
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 519?527,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Diacritization for Low-Resource Languages Using a Hybrid
Word and Consonant CMM
Robbie A. Haertel, Peter McClanahan, and Eric K. Ringger
Department of Computer Science
Brigham Young University
Provo, Utah 84602, USA
rah67@cs.byu.edu, petermcclanahan@gmail.com, ringger@cs.byu.edu
Abstract
We are interested in diacritizing Semitic lan-
guages, especially Syriac, using only dia-
critized texts. Previous methods have required
the use of tools such as part-of-speech taggers,
segmenters, morphological analyzers, and lin-
guistic rules to produce state-of-the-art results.
We present a low-resource, data-driven, and
language-independent approach that uses a
hybrid word- and consonant-level conditional
Markov model. Our approach rivals the best
previously published results in Arabic (15%
WER with case endings), without the use of
a morphological analyzer. In Syriac, we re-
duce the WER over a strong baseline by 30%
to achieve a WER of 10.5%. We also report
results for Hebrew and English.
1 Introduction
Abjad writing systems omit vowels and other di-
acritics. The ability to restore these diacritics is
useful for personal, industrial, and governmental
purposes?especially for Semitic languages. In its
own right, the ability to diacritize can aid language
learning and is necessary for speech-based assis-
tive technologies, including speech recognition and
text-to-speech. Diacritics are also useful for tasks
such as segmentation, morphological disambigua-
tion, and machine translation, making diacritization
important to Natural Language Processing (NLP)
systems and intelligence gathering. In alphabetic
writing systems, similar techniques have been used
to restore accents from plain text (Yarowsky, 1999)
and could be used to recover missing letters in the
compressed writing styles found in email, text, and
instant messages.
We are particularly interested in diacritizing Syr-
iac, a low-resource dialect of Aramaic, which pos-
sesses properties similar to Arabic and Hebrew. This
work employs conditional Markov models (CMMs)
(Klein and Manning, 2002) to diacritize Semitic
(and other) languages and requires only diacritized
texts for training. Such an approach is useful for
languages (like Syriac) in which annotated data and
linguistic tools such as part-of-speech (POS) tag-
gers, segmenters, and morphological analyzers are
not available. Our main contributions are as follows:
(1) we introduce a hybrid word and consonant CMM
that allows access to the diacritized form of the pre-
vious words; (2) we introduce new features avail-
able in the proposed model; and (3) we describe an
efficient, approximate decoder. Our models signifi-
cantly outperform existing low-resource approaches
across multiple related and unrelated languages and
even achieve near state-of-the-art results when com-
pared to resource-rich systems.
In the next section, we review previous work rel-
evant to our approach. Section 3 then motivates and
describes the models and features used in our frame-
work, including a description of the decoder. We
describe our data in Section 4 and detail our exper-
imental setup in Section 5. Section 6 presents our
results. Finally, Section 7 briefly discusses our con-
clusions and offers ideas for future work.
2 Previous Work
Diacritization has been receiving increased attention
due to the rising interest in Semitic languages, cou-
519
pled with the importance of diacritization to other
NLP-related tasks. The existing approaches can be
categorized based on the amount of resources they
require, their basic unit of analysis, and of course
the language they are targeting. Probabilistic sys-
tems can be further divided into generative and con-
ditional approaches.
Existing methodologies can be placed along a
continuum based on the quantity of resources they
require?a reflection of their cost. Examples of
resources used include morphological analyzers
(Habash and Rambow, 2007; Ananthakrishnan et al,
2005; Vergyri and Kirchhoff, 2004; El-Sadany and
Hashish, 1989), rules for grapheme-to-sound con-
version (El-Imam, 2008), transcribed speech (Ver-
gyri and Kirchhoff, 2004), POS tags (Zitouni et al,
2006; Ananthakrishnan et al, 2005), and a list of
prefixes and suffixes (Nelken and Shieber, 2005).
When such resources exist for a particular language,
they typically improve performance. For instance,
Habash and Rambow?s (2007) approach reduces the
error rate of Zitouni et al?s (2006) by as much as
30% through its use of a morphological analyzer. In
fact, such resources are not always available. Sev-
eral data-driven approaches exist that require only
diacritized texts (e.g., Ku?bler and Mohamed, 2008;
Zitouni et al, 2006; Gal, 2002) which are relatively
inexpensive to obtain: most literate speakers of the
target language could readily provide them.
Apart from the quantity of resources required, di-
acritization systems also differ in their basic unit of
analysis. A consonant-based approach treats each
consonant1 in a word as a potential host for one
or more (possibly null) diacritics; the goal is to
predict the correct diacritic(s) for each consonant
(e.g., Ku?bler and Mohamed, 2008). Zitouni et al
(2006) extend the problem to a sequence labeling
task wherein they seek the best sequence of diacrit-
ics for the consonants. Consequently, their approach
has access to previously chosen diacritics.
Alternatively, the basic unit of analysis can be the
full, undiacritized word. Since morphological ana-
lyzers produce analyses of undiacritized words, di-
acritization approaches that employ them typically
fall into this category (e.g., Habash and Rambow,
1We refer to all graphemes present in undiacritized texts as
consonants.
2007; Vergyri and Kirchoff, 2004). Word-based,
low-resource solutions tend to treat the problem as
word-level sequence labeling (e.g., Gal, 2002).
Unfortunately, word-based techniques face prob-
lems due to data sparsity: not all words in the
test set are seen during training. In contrast,
consonant-based approaches rarely face the anal-
ogous problem of previously unseen consonants.
Thus, one low-resource solution to data sparsity is to
use consonant-based techniques for unknown words
(Ananthakrishnan et al, 2005; Nelken and Shieber,
2005).
Many of the existing systems, especially recent
ones, are probabilistic or contain probabilistic com-
ponents. Zitouni et al (2006) show the superior-
ity of their conditional-based approaches over the
best-performing generative approaches. However,
the instance-based learning approach of Ku?bler and
Mohamed (2008) slightly outperforms Zitouni et
al. (2006). In the published literature for Arabic,
the latter two have the best low-resource solutions.
Habash and Rambow (2007) is the state-of-the-art,
high-resource solution for Arabic. To our knowl-
edge, no work has been done in this area for Syriac.
3 Models
In this work, we are concerned with diacritiza-
tion for Syriac for which a POS tagger, segmenter,
and other tools are not readily available, but for
which diacritized text is obtainable.2 Use of a sys-
tem dependent on a morphological analyzer such as
Habash and Rambow?s (2007) is therefore not cost-
effective. Furthermore, we seek a system that is ap-
plicable to a wide variety of languages. Although
Ku?bler and Mohamed?s (2008) approach is compet-
itive to Zitouni et al?s (2006), instance-based ap-
proaches tend to suffer with the addition of new fea-
tures (their own experiments demonstrate this). We
desire to add linguistically relevant features to im-
prove performance and thus choose to use a condi-
tional model. However, unlike Zitouni et al (2006),
we use a hybrid word- and consonant-level approach
based on the following observations (statistics taken
from the Syriac training and development sets ex-
plained in Section 4):
2Kiraz (2000) describes a morphological analyzer for Syriac
that is not publicly available and is costly to reproduce.
520
1. Many undiacritized words are unambiguous:
90.8% of the word types and 63.5% of the to-
kens have a single diacritized form.
2. Most undiacritized word types have only a few
possible diacritizations: the average number of
possible diacritizations is 1.11.
3. Low-frequency words have low ambiguity:
Undiacritized types occurring fewer than 5
times have an average of 1.05 possible diacriti-
zations.
4. Diacritized words not seen in the training data
occur infrequently at test time: 10.5% of the
diacritized test tokens were not seen in training.
5. The diacritics of previous words can provide
useful morphological information such as per-
son, number, and gender.
Contrary to observations 1 and 2, consonant-level
approaches dedicate modeling capacity to an expo-
nential (in the number of consonants) number of
possible diacritizations of a word. In contrast, a
word-level approach directly models the (few) dia-
critized forms seen in training. Furthermore, word-
based approaches naturally have access to the dia-
critics of previous words if used in a sequence la-
beler, as per observation 5. However, without a
?backoff? strategy, word-level models cannot pre-
dict a diacritized form not seen in the training data.
Also, low-frequency words by definition have less
information from which to estimate parameters. In
contrast, abundant information exists for each dia-
critic in a consonant-level system. To the degree
to which they hold, observations 3 and 4 mitigate
these latter two problems. Clearly a hybrid approach
would be advantageous.
To this end we employ a CMM in which we treat
the problem as an instance of sequence labeling at
the word level with less common words being han-
dled by a consonant-level CMM. Let u be the undi-
acriatized words in a sentence. Applying an order o
Markov assumption, the distribution over sequences
of diacritized words d is:
P (d|u) =
?d?
?
i=1
P (di|di?o...i?1,u;?,?, ?) (1)
in which the local conditional distribution of a di-
acritized word is an interpolation of a word-level
model (?ui) and a consonant-level model (?):
P (di|di?o...i?1,u;?,?, ?) =
?P (di|di?o...i?1,u;?ui) +
(1 ? ?)P (di|di?o...i?1,u;?)
We let the consonant-level model be a standard
CMM, similar to Zitouni et al (2006), but with ac-
cess to previously diacritized words. Note that the
order of this ?inner? CMM need not be the same as
that of the outer CMM.
The parameter ? reflects the degree to which we
trust the word-level model. In the most general case,
? can be a function of the undiacritized words and
the previous o diacritized words. Based on our ear-
lier enumerated observations, we use a simple delta
function for ?: we let ? be 0 when ui is rare and 1
otherwise. We leave discussion for what constitutes
a ?rare? undiacritized type for Section 5.2.
Figure 1b presents a graphical model of a sim-
ple example sentence in Syriac. The diacritiza-
tion for non-rare words is predicted for a whole
word, hence the random variable D for each such
word. These diacritized words Di depend on previ-
ous Di?1 as per equation (1) for an order-1 CMM
(note that the capitalized A, I, and O are in fact con-
sonants in this transliteration). Because ?NKTA?
and ?RGT? are rare, their diacritization is repre-
sented by a consonant-level CMM: one variable for
each possible diacritic in the word. Importantly,
these consonant-level models have access to the pre-
viously diacritized word (D4 and D6, respectively).
We use log-linear models for all local distribu-
tions in our CMMs, i.e., we use maximum entropy
(maxent) Markov models (McCallum et al, 2000;
Berger et al, 1996). Due to the phenomenon known
as d-separation (Pearl and Shafer, 1988), it is possi-
ble to independently learn parameters for each word
model ?ui by training only on those instances for
the corresponding word. Similarly, the consonant
model can be learned independent of the word mod-
els. We place a spherical normal prior centered at
zero with a standard deviation of 1 over the weights
of all models and use an L-BFGS minimizer to find
the MAP estimate of the weights for all the models
(words and consonant).
521
?C C C C C C 
CSIA AO
5,1 5,2 5,3 5,4 5,1 5,2
(a)
? DHBA
?CSIA
? AO ?
? LA
D1
CSIA
D2
AO
D3
DHBA
D4
AO NKTA
D6
LA RGT
C5,1 C5,2 C5,3 C5,4 C7,1 C7,2 C7,3
(b)
Figure 1: Graphical models of Acts 20:33 in Syriac, CSIA AO DHBA AO NKTA LA RGT ?silver or gold or
garment I have not coveted,? using Kiraz?s (1994) transliteration for (a) the initial portion of a consonant-
level-only model and (b) a combined word- and consonant-level model. For clarity, both models assume a
consonant-level Markov order of 1; (b) shows a word-level Markov order of 1. For simplicity, the figure
further assumes that additional features come only from the current (undiacritized) word.
Note that Zitouni et al?s (2006) model is a spe-
cial case of equation (1) where all words are rare, the
word-level Markov order (o) is 0, and the consonant-
level Markov order is 2. A simplified version of Zi-
touni?s model is presented in Figure 1a.
3.1 Features
Our features are based on those found in Zitouni et
al. (2006), although we have added a few of our own
which we consider to be one of the contributions of
this paper. Unlike their work, our consonant-level
model has access to previously diacritized words,
allowing us to exploit information noted in obser-
vation 5.
Each of the word-level models shares the same set
of features, defined by the following templates:
? The prefixes and suffixes (up to 4 characters) of
the previously diacritized words.
? The string of the actual diacritics, including the
null diacritic, from each of the previous o dia-
critized words and n-grams of these strings; a
similar set of features is extracted but without
the null diacritics.
? Every possible (overlapping) n-gram of all
sizes from n = 1 to n = 5 of undiacritized
words contained within the window defined by
2 words to the right and 2 to the left. These
templates yield 15 features for each token.
? The count of how far away the current token
is from the beginning/end of the sentence up
to the Markov order; also, their binary equiva-
lents.
The first two templates rely on diacritizations of pre-
vious words, in keeping with observation 5.
The consonant-level model has the following fea-
ture templates:
? The current consonant.
? Previous diacritics (individually, and n-grams
of diacritics ending in the diacritic prior to the
current consonant, where n is the consonant-
level Markov order).
? Conjunctions of the first two templates.
? Indicators as to whether this is the first or last
consonant.
? The first three templates independently con-
joined with the current consonant.
? Every possible (overlapping) n-gram of all
sizes from n = 1 to n = 11 consisting of con-
sonants contained within the window defined
by 5 words to the right and 5 to the left.
? Same as previous, but available diacritics are
included in the window.
? Prefixes and suffixes (of up to length 4) of pre-
viously diacritized words conjoined with previ-
ous diacritics in the current token, both individ-
ually and n-grams of such.
522
This last template is only possible because of our
model?s dependency on previous diacritized words.
3.2 Decoder
Given a sentence consisting of undiacritized words,
we seek the most probable sequence of diacritized
words, i.e., arg maxd P (d|u...). In sentences con-
taining no rare words, the well-known Viterbi algo-
rithm can be used to find the optimum.
However, as can be seen in Figure 1b, predictions
in the consonant-level model (e.g., C5,1...4) depend
on previously diacritized words (D4), and some dia-
critized words (e.g., D6) depend on diacritics in the
previous rare word (C5,1...4). These dependencies
introduce an exponential number of states (in the
length of the word) for rare words, making exact de-
coding intractable. Instead, we apply a non-standard
beam during decoding to limit the number of states
for rare words to the n-best (locally). This is ac-
complished by using an independent ?inner? n-best
decoder for the consonant-level CMM to produce
the n-best diacritizations for the rare word given the
previous diacritized words and other features. These
become the only states to and from which transitions
in the ?outer? word-level decoder can be made. We
note this is the same type of decoding that is done in
pipeline models that use n-best decoders (Finkel et
al., 2006). Additionally, we use a traditional beam-
search of width 5 to further reduce the search space
both in the outer and inner CMMs.
4 Data
Although our primary interest is in the Syriac lan-
guage, we also experimented with the Penn Arabic
Treebank (Maamouri et al, 2004) for the sake of
comparison with other approaches. We include He-
brew to provide results for yet another Semitic lan-
guage. We also apply the models to English to show
that our method and features work well outside of
the Semitic languages. A summary of the datasets,
including the number of diacritics, is found in Fig-
ure 2. The number of diacritics shown in the table
is less than the number of possible predictions since
we treat contiguous diacritics between consonants as
a single prediction.
For our experiments in Syriac, we use the New
Testament portion of the Peshitta (Kiraz, 1994) and
lang diacs train dev test
Syriac 9 87,874 10,747 11,021
Arabic 8 246,512 42,105 51,664
Hebrew 17 239,615 42,133 49,455
English 5 1,004,073 80,156 89,537
Figure 2: Number of diacritics and size (in tokens)
of each dataset
treat each verse as if it were a sentence. The diacrit-
ics we predict are the five short vowels, as well as
Se?ya?me?, Rukka?kha?, Qus?s?a?ya?, and linea ocultans.
For Arabic, we use the training/test split defined
by Zitouni et al (2006). We group all words having
the same P index value into a sentence. We build our
own development set by removing the last 15% of
the sentences of the training set. Like Zitouni, when
no solution exists in the treebank, we take the first
solution as the gold tag. Zitouni et al (2006) report
results on several different conditions, but we focus
on the most challenging of the conditions: we pre-
dict the standard three short vowels, three tanween,
sukuun, shadda, and all case endings. (Preliminary
experiments show that our models perform equally
favorably in the other scenarios as well.)
For Hebrew, we use the Hebrew Bible (Old Tes-
tament) in the Westminster Leningrad Codex (Zefa-
nia XML Project, 2009). As with Syriac, we treat
each verse as a sentence and remove the paragraph
markers (pe and samekh). There is a large number
of diacritics that could be predicted in Hebrew and
no apparent standardization in the literature. For
these reasons, we attempt to predict as many dia-
critics as possible. Specifically, we predict the di-
acritics whose unicode values are 05B0-B9, 05BB-
BD, 05BF, 05C1-C2, and 05C4. We treat the follow-
ing list of punctuation as consonants: maqaf, paseq,
sof pasuq, geresh, and gershayim. The cantillation
marks are removed entirely from the data.
Our English data comes from the Penn Treebank
(Marcus et al, 1994). We used sections 0?20 as
training data, 21?22 as development data, and 23?
24 as our test set. Unlike words in the Semitic lan-
guages, English words can begin with a vowel, re-
quiring us to prepend a prosthetic consonant to every
word; we also convert all English text to lowercase.
523
5 Experiments
For all feature engineering and tuning, we trained
and tested on training and development test sets, re-
spectively (as specified above). Final results are re-
ported by folding the development test set into the
training data and evaluating on the blind test set. We
retain only those features that occur more than once.
For each approach, we report the Word Error Rate
(WER) (i.e., the percentage of words that were in-
correctly diacritized), along with the Diacritic Er-
ror Rate (DER) (i.e., the percentage of diacritics, in-
cluding the null diacritic, that were incorrectly pre-
dicted). We also report both WER and DER for
only those words that were not seen during training
(UWER and UDER, respectively). We found that
precision, recall, and f-score were nearly perfectly
correlated with DER; hence, we omit this informa-
tion for brevity.
5.1 Models for Evaluation
In previous work, Ku?bler et al (2008) report the
lowest error rates of the low-resource models. Al-
though their results are not directly comparable to
Zitouni et al (2006), we have independently con-
firmed that the former slightly outperforms the latter
using the same diacritics and on the same dataset
(see Figure 4), thereby providing the strongest pub-
lished baseline for Arabic on a common dataset. We
denote this model as ku?bler and use it as a strong
baseline for all datasets.
For the Arabic results, we additionally include Zi-
touni et al?s (2006) lexical model (zitouni-lex)
and their model that uses a segmenter and POS
tagger (zitouni-all), which are not immediately
available to us for Syriac. For yet another point of
reference for Arabic, we provide the results from the
state-of-the-art (resource-rich) approach of Habash
and Rambow (2007) (habash). This model is at an
extreme advantage, having access to a full morpho-
logical analyzer. Note that for these three models
we simply report their published results and do not
attempt to reproduce them.
Since ku?bler is of a different model class than
ours, we consider an additional baseline that is a
consonant-level CMM with access to the same in-
formation, namely, only those consonants within a
window of 5 to either side (ccmm). This is equiva-
lent to a special case of our hybrid model wherein
both the word-level and the consonant-level Markov
order are 0. The features that we extract from this
window are the windowed n-gram features.
In order to assess the utility of previous diacritics
and how effectively our features leverage them, we
build a model based on the methodology from Sec-
tion 3 but specify that all words are rare, effectively
creating a consonant-only model that has access to
the diacritics of previous words. We call this model
cons-only. We note that the main difference be-
tween this model and zitouni-lex are features
that depend on previous diacritized words.
Finally, we present results using our full hybrid
model (hybrid). We use a Markov order of 2 at
the word and consonant level for both hybrid and
cons-only.
5.2 Consonant-Level Model and Rare Words
The hybrid nature of hybrid naturally raises the
question of whether or not the inner consonant
model should be trained only on rare words or on
all of the data. In other words, is the distribution
of diacritics different in rare words? If so, the con-
sonant model should be trained only on rare words.
To answer this question, we trained our consonant-
level model (cons-only) on words occurring fewer
than n times. We swept the value of the threshold n
and compared the results to the same model trained
on a random selection of words. As can be seen in
Figure 3, the performance on unknown words (both
UWER and UDER) using a model trained on rare
words can be much lower than using a model trained
on the same amount of randomly selected data. In
fact, training on rare words can lead to a lower error
rate on unknown words than training on all tokens
in the corpus. This suggests that the distribution of
diacritics in rare words is different from the distri-
bution of diacritics in general. This difference may
come from foreign words, especially in the Arabic
news corpus.
While this phenomenon is more pronounced in
some languages and with some models more than
others, it appears to hold in the cases we tried. We
found the WER for unknown words to be lowest for
a threshold of 8, 16, 32, and 32 for Syriac, Arabic,
Hebrew, and English, respectively.
524
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10000  20000  30000  40000  50000  60000  70000  80000  90000
e
rr
o
r 
ra
te
tokens
UWER (random)
UWER (rare)
UDER (random)
UDER (rare)
(a) Syriac
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50000  100000  150000  200000  250000
e
rr
o
r 
ra
te
tokens
UWER (random)
UWER (rare)
UDER (random)
UDER (rare)
(b) Arabic
Figure 3: Learning curves showing impact on consonant-level models when training on rare tokens for
Syriac and Arabic. Series marked ?rare? were trained with the least common tokens in the dataset.
Approach WER DER UWER UDER
Sy
ria
c
ku?bler 15.04 5.23 64.65 18.21
ccmm 13.99 4.82 54.54 15.18
cons-only 12.31 5.03 55.68 19.09
hybrid 10.54 4.29 55.16 18.86
A
ra
bi
c
zitouni-lex 25.1 8.2 NA NA
ku?bler 23.61 7.25 66.69 20.51
ccmm 22.63 6.61 57.71 16.10
cons-only 15.02 5.15 48.10 15.76
hybrid 17.87 5.67 47.85 15.63
zitouni-all 18.0 5.5 NA NA
habash 14.9 4.8 NA NA
H
eb
re
w
ku?bler 30.60 12.96 89.52 36.86
ccmm 29.67 12.05 80.02 29.39
cons-only 23.39 10.92 75.70 33.34
hybrid 22.18 10.71 74.38 32.40
En
gl
ish
ku?bler 10.54 4.38 54.96 16.31
ccmm 11.60 4.71 58.55 16.34
cons-only 8.71 3.87 58.93 17.85
hybrid 5.39 2.38 57.24 16.51
Figure 4: Results for all languages and approaches
6 Discussion of Results
Since Syriac is of primary interest to us, we begin
by examining the results from this dataset. Syriac
appears to be easier to diacritize than Arabic, con-
sidering it has a similar number of diacritics and
only one-third the amount of data. On this dataset,
hybrid has the lowest WER and DER, achieving
nearly 30% and 18% reduction in WER and DER,
respectively, over ku?bler; it reduces both error
rates over cons-only by more than 14%. These
results attest to the effectiveness of our model in ac-
counting for the observations made in Section 3.
A similar pattern holds for the Hebrew and En-
glish datasets, namely that hybrid reduces the
WER over ku?bler by 28% to upwards of 50%;
cons-only also consistently and significantly out-
performs ku?bler and ccmm. However, the reduc-
tion in error rate for our cons-only and hybrid
models tends to be lower for DER than WER in
all languages except for English. In the case of
hybrid, this is probably because it is inherently
word-based. Having access to entire previous dia-
critized words may be a contributing factor as well,
especially in cons-only.
When comparing model classes (ku?bler and
ccmm), it appears that performance is comparable
across all languages, with the maxent approach en-
joying a slight advantage except in English. Interest-
ingly, the maxent solution usually handles unknown
words better, although it does not specifically target
this case. Both models outperform zitouni-lex
in Arabic, despite the fact that they use a much
simpler feature set, most notably, the lack of pre-
vious diacritics. In the case of ccmm this may be at-
tributable in part to our use of an L-BFGS optimizer,
convergence criteria, feature selection, or other po-
tential differences not noted in Zitouni et al (2006).
We note that the maxent-based approaches are much
more time and memory intensive.
Using the Arabic data, we are able to com-
pare our methods to several other published results.
525
The cons-only model significantly outperforms
zitouni-all despite the additional resources to
which the latter has access. This is evidence sup-
porting our hypothesis that the diacritics from pre-
vious words in fact contain useful information for
prediction. This empirically suggests that the inde-
pendence assumptions in consonant-only models are
too strict.
Perhaps even more importantly, our low-resource
method approaches the performance of habash. We
note that the differences may not be statistically sig-
nificant, and also that Habash and Rambow (2007)
omit instances in the data that lack solutions. In fact,
cons-only has a lower WER than all but two of
the seven techniques used by Habash and Rambow
(2007), which use a morphological analyzer.
Interestingly, hybrid does worse than
cons-only on this dataset, although it is still
competitive with zitouni-all. We hypothesize
that the observations from Section 3 do not hold
as strongly for this dataset. For this reason, using
a smooth interpolation function (rather than the
abrupt one we employ) may be advantageous and is
an interesting avenue for future research.
One last observation is that the approaches that
use diacritics from previous words (i.e., cons-only
and hybrid) usually have lower sentence error rates
(not shown in Figure 4). This highlights an advan-
tage of observation 5: that dependencies on previ-
ously diacritized words can help ensure a consistent
tagging within a sentence.
7 Conclusions and Future Work
In this paper, we have presented a low-resource so-
lution for automatic diacritization. Our approach is
motivated by empirical observations of the ambigu-
ity and frequency of undiacritized and diacritized
words as well as by the hypothesis that diacrit-
ics from previous words provide useful informa-
tion. The main contributions of our work, based
on these observations, are (1) a hybrid word-level
CMM combined with a consonant-level model for
rare words, (2) a consonant-level model with depen-
dencies on previous diacritized words, (3) new fea-
tures that leverage these dependencies, and (4) an
efficient, approximate decoder for these models. As
expected, the efficacy of our approach varies across
languages, due to differences in the actual ambigu-
ity and frequency of words in these languages. Nev-
ertheless, our models consistently reduce WER by
15% to nearly 50% over the best performing low-
resource models in the literature. In Arabic, our
models approach state-of-the-art despite not using a
morphological analyzer. Arguably, our results have
brought diacritization very close to being useful for
practical application, especially when considering
that we evaluated our method on the most difficult
task in Arabic, which has been reported to have dou-
ble the WER (Zitouni et al, 2006).
The success of this low-resource solution natu-
rally suggests that where more resources are avail-
able (e.g., in Arabic), they could be used to further
reduce error rates. For instance, it may be fruitful to
incorporate a morphological analyzer or segmenta-
tion and part-of-speech tags.
In future work, we would like to consider using
CRFs in place of MEMMs. Also, other approximate
decoders used in pipeline approaches could be ex-
plored as alternatives to the one we used (e.g., Finkel
et al, 2006). Additionally, we wish to include our
model as a stage in a pipeline that segments, dia-
critizes, and labels morphemes. Since obtaining data
for these tasks is substantially more expensive, we
hope to use active learning to obtain more data.
Our framework is applicable for any sequence la-
beling task that can be done at either a word or a
sub-word (e.g., character) level. Segmentation and
lemmatization are particularly promising tasks to
which our approach could be applied.
Finally, for the sake of completeness, we note that
more recent work has been done based on our base-
line models that has emerged since the preparation
of the current work, particularly Zitouni et al (2009)
and Mohamed et al (2009). We wish to address any
improvements captured by this more recent work
such as the use of different data sets and addressing
problems with the hamza to decrease error rates.
Acknowledgments
We thank Imed Zitouni, Nizar Habash, Sandra
Ku?bler, and Emad Mohamed for their assistance in
reconstructing datasets, models, and features.
526
References
S. Ananthakrishnan, S. Narayanan, and S. Bangalore.
2005. Automatic diacritization of Arabic transcripts
for automatic speech recognition. In Proceedings of
the International Conference on Natural Language
Processing.
A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996.
Amaximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22:39?71.
Y. A. El-Imam. 2008. Synthesis of the intonation of neu-
trally spoken Modern Standard Arabic speech. Signal
Processing, 88(9):2206?2221.
T. A. El-Sadany and M. A. Hashish. 1989. An Ara-
bic morphological system. IBM Systems Journal,
28(4):600?612.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
Bayesian inference for linguistic annotation pipelines.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 618?
626.
Y. Gal. 2002. An HMM approach to vowel restoration
in Arabic and Hebrew. In Proceedings of the ACL-
02 Workshop on Computational Approaches to Semitic
Languages, pages 1?7.
N. Habash and O. Rambow. 2007. Arabic diacritiza-
tion through full morphological tagging. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 53?56.
G. Kiraz. 1994. Automatic concordance generation of
Syriac texts. In R. Lavenant, editor, VI Symposium
Syriacum 1992, pages 461?471, Rome, Italy.
G. A. Kiraz. 2000. Multitiered nonlinear morphology
using multitape finite automata: a case study on Syr-
iac and Arabic. Computational Linguistics, 26(1):77?
105.
D. Klein and C. D. Manning. 2002. Conditional structure
versus conditional estimation in NLP models. In Pro-
ceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing, pages 9?16.
S. Ku?bler and E. Mohamed. 2008. Memory-based vocal-
ization of Arabic. In Proceedings of the LREC Work-
shop on HLT and NLP within the Arabic World.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proceedings of the
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19:313?330.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proceedings of the 17th In-
ternational Conference on Machine Learning, pages
591?598.
E. Mohamed and S. Ku?bler. 2009. Diacritization for
real-world Arabic texts. In Proceedings of Recent Ad-
vances in Natural Language Processing 2009.
R. Nelken and S. M. Shieber. 2005. Arabic diacritiza-
tion using weighted finite-state transducers. In Pro-
ceedings of the ACL Workshop on Computational Ap-
proaches to Semitic Languages, pages 79?86.
J. Pearl and G. Shafer. 1988. Probabilistic reasoning
in intelligent systems: networks of plausible inference.
Morgan Kaufman, San Mateo, CA.
D. Vergyri and K. Kirchhoff. 2004. Automatic diacritiza-
tion of Arabic for acoustic modeling in speech recog-
nition. In Proceedings of the COLING 2004Workshop
on Computational Approaches to Arabic Script-based
Languages, pages 66?73.
D. Yarowsky. 1999. A comparison of corpus-based tech-
niques for restoring accents in Spanish and French
text. Natural language processing using very large
corpora, pages 99?120.
Zefania XML Project. 2009. Zefania XML bible:
Leningrad codex. http://sourceforge.
net/projects/zefania-sharp/files/
Zefania\%20XML\%20Bibles\%204\
%20hebraica/Leningrad\%20Codex/sf_
wcl.zip/download.
I. Zitouni and R. Sarikaya. 2009. Arabic diacritic
restoration approach based on maximum entropymod-
els. Computer Speech & Language, 23(3):257?276.
I. Zitouni, J. S. Sorensen, and R. Sarikaya. 2006. Max-
imum entropy based restoration of Arabic diacritics.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 577?584.
527
