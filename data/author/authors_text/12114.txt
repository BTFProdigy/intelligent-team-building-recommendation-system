Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136?144,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Transliteration Alignment
Vladimir Pervouchine, Haizhou Li
Institute for Infocomm Research
A*STAR, Singapore 138632
{vpervouchine,hli}@i2r.a-star.edu.sg
Bo Lin
School of Computer Engineering
NTU, Singapore 639798
linbo@pmail.ntu.edu.sg
Abstract
This paper studies transliteration align-
ment, its evaluation metrics and applica-
tions. We propose a new evaluation met-
ric, alignment entropy, grounded on the
information theory, to evaluate the align-
ment quality without the need for the gold
standard reference and compare the metric
with F -score. We study the use of phono-
logical features and affinity statistics for
transliteration alignment at phoneme and
grapheme levels. The experiments show
that better alignment consistently leads to
more accurate transliteration. In transliter-
ation modeling application, we achieve a
mean reciprocal rate (MRR) of 0.773 on
Xinhua personal name corpus, a signifi-
cant improvement over other reported re-
sults on the same corpus. In transliteration
validation application, we achieve 4.48%
equal error rate on a large LDC corpus.
1 Introduction
Transliteration is a process of rewriting a word
from a source language to a target language in a
different writing system using the word?s phono-
logical equivalent. The word and its translitera-
tion form a transliteration pair. Many efforts have
been devoted to two areas of studies where there
is a need to establish the correspondence between
graphemes or phonemes between a transliteration
pair, also known as transliteration alignment.
One area is the generative transliteration model-
ing (Knight and Graehl, 1998), which studies how
to convert a word from one language to another us-
ing statistical models. Since the models are trained
on an aligned parallel corpus, the resulting statisti-
cal models can only be as good as the alignment of
the corpus. Another area is the transliteration vali-
dation, which studies the ways to validate translit-
eration pairs. For example Knight and Graehl
(1998) use the lexicon frequency, Qu and Grefen-
stette (2004) use the statistics in a monolingual
corpus and the Web, Kuo et al (2007) use proba-
bilities estimated from the transliteration model to
validate transliteration candidates. In this paper,
we propose using the alignment distance between
the a bilingual pair of words to establish the evi-
dence of transliteration candidacy. An example of
transliteration pair alignment is shown in Figure 1.
e
5
e
1
e
2
e
3
e
4
c
1
c
2
c
3
A   L I  C E
? ? ?
source graphemes
target graphemes
e
1
e
2
e
3
grapheme tokens
Figure 1: An example of grapheme alignment (Al-
ice, ???), where a Chinese grapheme, a char-
acter, is aligned to an English grapheme token.
Like the word alignment in statistical ma-
chine translation (MT), transliteration alignment
becomes one of the important topics in machine
transliteration, which has several unique chal-
lenges. Firstly, the grapheme sequence in a word
is not delimited into grapheme tokens, resulting
in an additional level of complexity. Secondly, to
maintain the phonological equivalence, the align-
ment has to make sense at both grapheme and
phoneme levels of the source and target languages.
This paper reports progress in our ongoing spoken
language translation project, where we are inter-
ested in the alignment problem of personal name
transliteration from English to Chinese.
This paper is organized as follows. In Section 2,
we discuss the prior work. In Section 3, we in-
troduce both statistically and phonologically mo-
tivated alignment techniques and in Section 4 we
advocate an evaluation metric, alignment entropy
that measures the alignment quality. We report the
experiments in Section 5. Finally, we conclude in
Section 6.
136
2 Related Work
A number of transliteration studies have touched
on the alignment issue as a part of the translit-
eration modeling process, where alignment is
needed at levels of graphemes and phonemes. In
their seminal paper Knight and Graehl (1998) de-
scribed a transliteration approach that transfers the
grapheme representation of a word via the pho-
netic representation, which is known as phoneme-
based transliteration technique (Virga and Khu-
danpur, 2003; Meng et al, 2001; Jung et al,
2000; Gao et al, 2004). Another technique is
to directly transfer the grapheme, known as di-
rect orthographic mapping, that was shown to
be simple and effective (Li et al, 2004). Some
other approaches that use both source graphemes
and phonemes were also reported with good per-
formance (Oh and Choi, 2002; Al-Onaizan and
Knight, 2002; Bilac and Tanaka, 2004).
To align a bilingual training corpus, some take a
phonological approach, in which the crafted map-
ping rules encode the prior linguistic knowledge
about the source and target languages directly into
the system (Wan and Verspoor, 1998; Meng et al,
2001; Jiang et al, 2007; Xu et al, 2006). Oth-
ers adopt a statistical approach, in which the affin-
ity between phonemes or graphemes is learned
from the corpus (Gao et al, 2004; AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003).
In the phoneme-based technique where an in-
termediate level of phonetic representation is used
as the pivot, alignment between graphemes and
phonemes of the source and target words is
needed (Oh and Choi, 2005). If source and tar-
get languages have different phoneme sets, align-
ment between the the different phonemes is also
required (Knight and Graehl, 1998). Although
the direct orthographic mapping approach advo-
cates a direct transfer of grapheme at run-time,
we still need to establish the grapheme correspon-
dence at the model training stage, when phoneme
level alignment can help.
It is apparent that the quality of transliteration
alignment of a training corpus has a significant
impact on the resulting transliteration model and
its performance. Although there are many stud-
ies of evaluation metrics of word alignment for
MT (Lambert, 2008), there has been much less re-
ported work on evaluation metrics of translitera-
tion alignment. In MT, the quality of training cor-
pus alignment A is often measured relatively to
the gold standard, or the ground truth alignment
G, which is a manual alignment of the corpus or
a part of it. Three evaluation metrics are used:
precision, recall, and F -score, the latter being a
function of the former two. They indicate how
close the alignment under investigation is to the
gold standard alignment (Mihalcea and Pedersen,
2003). Denoting the number of cross-lingual map-
pings that are common in both A and G as CAG,
the number of cross-lingual mappings in A as CA
and the number of cross-lingual mappings in G as
CG, precision Pr is given as CAG/CA, recall Rc
as CAG/CG and F -score as 2Pr ?Rc/(Pr+Rc).
Note that these metrics hinge on the availability
of the gold standard, which is often not available.
In this paper we propose a novel evaluation metric
for transliteration alignment grounded on the in-
formation theory. One important property of this
metric is that it does not require a gold standard
alignment as a reference. We will also show that
how this metric is used in generative transliteration
modeling and transliteration validation.
3 Transliteration alignment techniques
We assume in this paper that the source language
is English and the target language is Chinese, al-
though the technique is not restricted to English-
Chinese alignment.
Let a word in the source language (English) be
{ei} = {e1 . . . eI} and its transliteration in the
target language (Chinese) be {cj} = {c1 . . . cJ},
ei ? E, cj ? C, and E, C being the English and
Chinese sets of characters, or graphemes, respec-
tively. Aligning {ei} and {cj} means for each tar-
get grapheme token c?j finding a source grapheme
token e?m, which is an English substring in {ei}
that corresponds to cj , as shown in the example in
Figure 1. As Chinese is syllabic, we use a Chinese
character cj as the target grapheme token.
3.1 Grapheme affinity alignment
Given a distance function between graphemes of
the source and target languages d(ei, cj), the prob-
lem of alignment can be formulated as a dynamic
programming problem with the following function
to minimize:
Dij = min(Di?1,j?1 + d(ei, cj),
Di,j?1 + d(?, cj),
Di?1,j + d(ei, ?))
(1)
137
Here the asterisk * denotes a null grapheme that
is introduced to facilitate the alignment between
graphemes of different lengths. The minimum dis-
tance achieved is then given by
D =
I?
i=1
d(ei, c?(i)) (2)
where j = ?(i) is the correspondence between the
source and target graphemes. The alignment can
be performed via the Expectation-Maximization
(EM) by starting with a random initial alignment
and calculating the affinity matrix count(ei, cj)
over the whole parallel corpus, where element
(i, j) is the number of times character ei was
aligned to cj . From the affinity matrix conditional
probabilities P (ei|cj) can be estimated as
P (ei|cj) = count(ei, cj)/
?
j
count(ei, cj) (3)
Alignment j = ?(i) between {ei} and {cj} that
maximizes probability
P =
?
i
P (c?(i)|ei) (4)
is also the same alignment that minimizes align-
ment distance D:
D = ? logP = ?
?
i
logP (c?(i)|ei) (5)
In other words, equations (2) and (5) are the same
when we have the distance function d(ei, cj) =
? logP (cj |ei). Minimizing the overall distance
over a training corpus, we conduct EM iterations
until the convergence is achieved.
This technique solely relies on the affinity
statistics derived from training corpus, thus is
called grapheme affinity alignment. It is also
equally applicable for alignment between a pair of
symbol sequences representing either graphemes
or phonemes. (Gao et al, 2004; AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003).
3.2 Grapheme alignment via phonemes
Transliteration is about finding phonological
equivalent. It is therefore a natural choice to use
the phonetic representation as the pivot. It is
common though that the sound inventory differs
from one language to another, resulting in differ-
ent phonetic representations for source and tar-
get words. Continuing with the earlier example,
?
AE L AH S
A L I C E
AY l i s iz
? ?
graphemes
phonemes
phonemes
graphemes
source
target
Figure 2: An example of English-Chinese translit-
eration alignment via phonetic representations.
Figure 2 shows the correspondence between the
graphemes and phonemes of English word ?Al-
ice? and its Chinese transliteration, with CMU
phoneme set used for English (Chase, 1997) and
IIR phoneme set for Chinese (Li et al, 2007a).
A Chinese character is often mapped to a unique
sequence of Chinese phonemes. Therefore, if
we align English characters {ei} and Chinese
phonemes {cpk} (cpk ? CP set of Chinese
phonemes) well, we almost succeed in aligning
English and Chinese grapheme tokens. Alignment
between {ei} and {cpk} becomes the main task in
this paper.
3.2.1 Phoneme affinity alignment
Let the phonetic transcription of English word
{ei} be {epn}, epn ? EP , where EP is the set of
English phonemes. Alignment between {ei} and
{epn}, as well as between {epn} and {cpk} can
be performed via EM as described above. We esti-
mate conditional probability of Chinese phoneme
cpk after observing English character ei as
P (cpk|ei) =
?
{epn}
P (cpk|epn)P (epn|ei) (6)
We use the distance function between English
graphemes and Chinese phonemes d(ei, cpk) =
? logP (cpk|ei) to perform the initial alignment
between {ei} and {cpk} via dynamic program-
ming, followed by the EM iterations until con-
vergence. The estimates for P (cpk|epn) and
P (epn|ei) are obtained from the affinity matrices:
the former from the alignment of English and Chi-
nese phonetic representations, the latter from the
alignment of English words and their phonetic rep-
resentations.
3.2.2 Phonological alignment
Alignment between the phonetic representations
of source and target words can also be achieved
using the linguistic knowledge of phonetic sim-
ilarity. Oh and Choi (2002) define classes of
138
phonemes and assign various distances between
phonemes of different classes. In contrast, we
make use of phonological descriptors to define the
similarity between phonemes in this paper.
Perhaps the most common way to measure the
phonetic similarity is to compute the distances be-
tween phoneme features (Kessler, 2005). Such
features have been introduced in many ways, such
as perceptual attributes or articulatory attributes.
Recently, Tao et al (2006) and Yoon et al (2007)
have studied the use of phonological features and
manually assigned phonological distance to mea-
sure the similarity of transliterated words for ex-
tracting transliterations from a comparable corpus.
We adopt the binary-valued articulatory at-
tributes as the phonological descriptors, which are
used to describe the CMU and IIR phoneme sets
for English and Chinese Mandarin respectively.
Withgott and Chen (1993) define a feature vec-
tor of phonological descriptors for English sounds.
We extend the idea by defining a 21-element bi-
nary feature vector for each English and Chinese
phoneme. Each element of the feature vector
represents presence or absence of a phonologi-
cal descriptor that differentiates various kinds of
phonemes, e.g. vowels from consonants, front
from back vowels, nasals from fricatives, etc1.
In this way, a phoneme is described by a fea-
ture vector. We express the similarity between
two phonemes by the Hamming distance, also
called the phonological distance, between the two
feature vectors. A difference in one descriptor
between two phonemes increases their distance
by 1. As the descriptors are chosen to differenti-
ate between sounds, the distance between similar
phonemes is low, while that between two very dif-
ferent phonemes, such as a vowel and a consonant,
is high. The null phoneme, added to both English
and Chinese phoneme sets, has a constant distance
to any actual phonemes, which is higher than that
between any two actual phonemes.
We use the phonological distance to perform
the initial alignment between English and Chi-
nese phonetic representations of words. After that
we proceed with recalculation of the distances be-
tween phonemes using the affinity matrix as de-
scribed in Section 3.1 and realign the corpus again.
We continue the iterations until convergence is
1The complete table of English and Chinese phonemes
with their descriptors, as well as the translitera-
tion system demo is available at http://translit.i2r.a-
star.edu.sg/demos/transliteration/
reached. Because of the use of phonological de-
scriptors for the initial alignment, we call this tech-
nique the phonological alignment.
4 Transliteration alignment entropy
Having aligned the graphemes between two lan-
guages, we want to measure how good the align-
ment is. Aligning the graphemes means aligning
the English substrings, called the source grapheme
tokens, to Chinese characters, the target grapheme
tokens. Intuitively, the more consistent the map-
ping is, the better the alignment will be. We can
quantify the consistency of alignment via align-
ment entropy grounded on information theory.
Given a corpus of aligned transliteration pairs,
we calculate count(cj , e?m), the number of times
each Chinese grapheme token (character) cj is
mapped to each English grapheme token e?m. We
use the counts to estimate probabilities
P (e?m, cj) = count(cj , e?m)/
?
m,j
count(cj , e?m)
P (e?m|cj) = count(cj , e?m)/
?
m
count(cj , e?m)
The alignment entropy of the transliteration corpus
is the weighted average of the entropy values for
all Chinese tokens:
H = ?
?
j
P (cj)
?
m
P (e?m|cj) logP (e?m|cj)
= ?
?
m,j
P (e?m, cj) logP (e?m|cj)
(7)
Alignment entropy indicates the uncertainty of
mapping between the English and Chinese tokens
resulting from alignment. We expect and will
show that this estimate is a good indicator of the
alignment quality, and is as effective as the F -
score, but without the need for a gold standard ref-
erence. A lower alignment entropy suggests that
each Chinese token tends to be mapped to fewer
distinct English tokens, reflecting better consis-
tency. We expect a good alignment to have a
sharp cross-lingual mapping with low alignment
entropy.
5 Experiments
We use two transliteration corpora: Xinhua cor-
pus (Xinhua News Agency, 1992) of 37,637
personal name pairs and LDC Chinese-English
139
named entity list LDC2005T34 (Linguistic Data
Consortium, 2005), containing 673,390 personal
name pairs. The LDC corpus is referred to as
LDC05 for short hereafter. For the results to be
comparable with other studies, we follow the same
splitting of Xinhua corpus as that in (Li et al,
2007b) having a training and testing set of 34,777
and 2,896 names respectively. In contrast to the
well edited Xinhua corpus, LDC05 contains erro-
neous entries. We have manually verified and cor-
rected around 240,000 pairs to clean up the corpus.
As a result, we arrive at a set of 560,768 English-
Chinese (EC) pairs that follow the Chinese pho-
netic rules, and a set of 83,403 English-Japanese
Kanji (EJ) pairs, which follow the Japanese pho-
netic rules, and the rest 29,219 pairs (REST) be-
ing labeled as incorrect transliterations. Next we
conduct three experiments to study 1) alignment
entropy vs. F -score, 2) the impact of alignment
quality on transliteration accuracy, and 3) how to
validate transliteration using alignment metrics.
5.1 Alignment entropy vs. F -score
As mentioned earlier, for English-Chinese
grapheme alignment, the main task is to align En-
glish graphemes to Chinese phonemes. Phonetic
transcription for the English names in Xinhua
corpus are obtained by a grapheme-to-phoneme
(G2P) converter (Lenzo, 1997), which generates
phoneme sequence without providing the exact
correspondence between the graphemes and
phonemes. G2P converter is trained on the CMU
dictionary (Lenzo, 2008).
We align English grapheme and phonetic repre-
sentations e? ep with the affinity alignment tech-
nique (Section 3.1) in 3 iterations. We further
align the English and Chinese phonetic represen-
tations ep ? cp via both affinity and phonological
alignment techniques, by carrying out 6 and 7 it-
erations respectively. The alignment methods are
schematically shown in Figure 3.
To study how alignment entropy varies accord-
ing to different quality of alignment, we would
like to have many different alignment results. We
pair the intermediate results from the e ? ep and
ep ? cp alignment iterations (see Figure 3) to
form e ? ep ? cp alignments between English
graphemes and Chinese phonemes and let them
converge through few more iterations, as shown
in Figure 4. In this way, we arrive at a total of 114
phonological and 80 affinity alignments of differ-
ent quality.
{cp
k
}
{e
i
}
English
graphemes
{ep
n
}
English
phonemes
Chinese
phonemes
affinity alignment affinity alignment
e? ep
iteration 1
e? ep
iteration 2
e? ep
iteration 3
ep? cp
iteration 1
ep? cp
iteration 2
...
ep? cp
iteration 6
phonological alignment
ep? cp
iteration 1
ep? cp
iteration 2
...
ep? cp
iteration 7
Figure 3: Aligning English graphemes to
phonemes e?ep and English phonemes to Chinese
phonemes ep?cp. Intermediate e?ep and ep?cp
alignments are used for producing e ? ep ? cp
alignments.
e? ep
alignments
ep? cp
affinity / 
phonological
alignments
iteration 1
iteration 2
iteration 3
iteration 1
iteration 2
iteration n
...
...
calculating
d(e
i
, cp
k
)
affinity
alignment
iteration 1
iteration 2
...
e? ep? cp
etc
Figure 4: Example of aligning English graphemes
to Chinese phonemes. Each combination of e?ep
and ep? cp alignments is used to derive the initial
distance d(ei, cpk), resulting in several e?ep?cp
alignments due to the affinity alignment iterations.
We have manually aligned a random set of
3,000 transliteration pairs from the Xinhua train-
ing set to serve as the gold standard, on which we
calculate the precision, recall and F -score as well
as alignment entropy for each alignment. Each
alignment is reflected as a data point in Figures 5a
and 5b. From the figures, we can observe a clear
correlation between the alignment entropy and F -
score, that validates the effectiveness of alignment
entropy as an evaluation metric. Note that we
don?t need the gold standard reference for report-
ing the alignment entropy.
We also notice that the data points seem to form
clusters inside which the value of F -score changes
insignificantly as the alignment entropy changes.
Further investigation reveals that this could be due
to the limited number of entries in the gold stan-
dard. The 3,000 names in the gold standard are not
enough to effectively reflect the change across dif-
ferent alignments. F -score requires a large gold
standard which is not always available. In con-
trast, because the alignment entropy doesn?t de-
pend on the gold standard, one can easily report
the alignment performance on any unaligned par-
allel corpus.
140
??????
??????
??????
???
??? ??? ??? ???
?? ? ?
???? ?? ? ? ?
?? ??
(a) 80 affinity alignments
??????
??????
??????
???
??? ??? ??? ???
???? ?? ? ? ?
?? ??
?? ? ?
(b) 114 phonological alignments
Figure 5: Correlation between F -score and align-
ment entropy for Xinhua training set algnments.
Results for precision and recall have similar trends
.
5.2 Impact of alignment quality on
transliteration accuracy
We now further study how the alignment affects
the generative transliteration model in the frame-
work of the joint source-channel model (Li et al,
2004). This model performs transliteration by
maximizing the joint probability of the source and
target names P ({ei}, {cj}), where the source and
target names are sequences of English and Chi-
nese grapheme tokens. The joint probability is
expressed as a chain product of a series of condi-
tional probabilities of token pairs P ({ei}, {cj}) =
P ((e?k, ck)|(e?k?1, ck?1)), k = 1 . . . N , where we
limit the history to one preceding pair, resulting in
a bigram model. The conditional probabilities for
token pairs are estimated from the aligned training
corpus. We use this model because it was shown
to be simple yet accurate (Ekbal et al, 2006; Li
et al, 2007b). We train a model for each of the
114 phonological alignments and the 80 affinity
alignments in Section 5.1 and conduct translitera-
tion experiment on the Xinhua test data.
During transliteration, an input English name
is first decoded into a lattice of all possible En-
glish and Chinese grapheme token pairs. Then the
joint source-channel transliteration model is used
to score the lattice to obtain a ranked list ofmmost
likely Chinese transliterations (m-best list).
We measure transliteration accuracy as the
mean reciprocal rank (MRR) (Kantor and
Voorhees, 2000). If there is only one correct
Chinese transliteration of the k-th English word
and it is found at the rk-th position in the m-best
list, its reciprocal rank is 1/rk. If the list contains
no correct transliterations, the reciprocal rank is
0. In case of multiple correct transliterations, we
take the one that gives the highest reciprocal rank.
MRR is the average of the reciprocal ranks across
all words in the test set. It is commonly used as
a measure of transliteration accuracy, and also
allows us to make a direct comparison with other
reported work (Li et al, 2007b).
We take m = 20 and measure MRR on Xinhua
test set for each alignment of Xinhua training set
as described in Section 5.1. We report MRR and
the alignment entropy in Figures 6a and 7a for the
affinity and phonological alignments respectively.
The highest MRR we achieve is 0.771 for affin-
ity alignments and 0.773 for phonological align-
ments. This is a significant improvement over the
MRR of 0.708 reported in (Li et al, 2007b) on the
same data. We also observe that the phonological
alignment technique produces, on average, better
alignments than the affinity alignment technique
in terms of both the alignment entropy and MRR.
We also report the MRR and F -scores for each
alignment in Figures 6b and 7b, from which we
observe that alignment entropy has stronger corre-
lation with MRR than F -score does. The Spear-
man?s rank correlation coefficients are ?0.89 and
?0.88 for data in Figure 6a and 7a respectively.
This once again demonstrates the desired property
of alignment entropy as an evaluation metric of
alignment.
To validate our findings from Xinhua corpus,
we further carry out experiments on the EC set
of LDC05 containing 560,768 entries. We split
the set into 5 almost equal subsets for cross-
validation: in each of 5 experiments one subset is
used for testing and the remaining ones for train-
ing. Since LDC05 contains one-to-many English-
Chinese transliteration pairs, we make sure that an
English name only appears in one subset.
Note that the EC set of LDC05 contains
many names of non-English, and, generally, non-
European origin. This makes the G2P converter
less accurate, as it is trained on an English pho-
netic dictionary. We therefore only apply the affin-
ity alignment technique to align the EC set. We
141
??????
??????
??????
??? ??? ??? ???
MRR
Alignment?entropy
(a) 80 affinity alignments
??????
??????
??????
??? ??? ??? ??? ??? ??? ???
MRR
F?score
(b) 80 affinity alignments
Figure 6: Mean reciprocal ratio on Xinhua test
set vs. alignment entropy and F -score for mod-
els trained with different affinity alignments.
use each iteration of the alignment in the translit-
eration modeling and present the resulting MRR
along with alignment entropy in Figure 8. The
MRR results are the averages of five values pro-
duced in the five-fold cross-validations.
We observe a clear correlation between the
alignment entropy and transliteration accuracy ex-
pressed by MRR on LDC05 corpus, similar to that
on Xinhua corpus, with the Spearman?s rank cor-
relation coefficient of ?0.77. We obtain the high-
est average MRR of 0.720 on the EC set.
5.3 Validating transliteration using
alignment measure
Transliteration validation is a hypothesis test that
decides whether a given transliteration pair is gen-
uine or not. Instead of using the lexicon fre-
quency (Knight and Graehl, 1998) or Web statis-
tics (Qu and Grefenstette, 2004), we propose vali-
dating transliteration pairs according to the align-
ment distance D between the aligned English
graphemes and Chinese phonemes (see equations
(2) and (5)). A distance function d(ei, cpk) is
established from each alignment on the Xinhua
training set as discussed in Section 5.2.
An audit of LDC05 corpus groups the corpus
into three sets: an English-Chinese (EC) set of
560,768 samples, an English-Japanese (EJ) set
of 83,403 samples and the REST set of 29,219
??????
??????
??????
??? ??? ??? ???
MRR
Alignment?entropy
(a) 114 phonological alignments
??????
??????
??????
??? ??? ??? ??? ??? ??? ???
MRR
F?score
(b) 114 phonological alignments
Figure 7: Mean reciprocal ratio on Xinhua test
set vs. alignment entropy and F -score for models
trained with different phonological alignments.
??????
??????
??????
??????
???
??? ??? ??? ???
??
???? ?? ? ? ?
Figure 8: Mean reciprocal ratio vs. alignment en-
tropy for alignments of EC set.
samples that are not transliteration pairs. We
mark the EC name pairs as genuine and the rest
112,622 name pairs that do not follow the Chi-
nese phonetic rules as false transliterations, thus
creating the ground truth labels for an English-
Chinese transliteration validation experiment. In
other words, LDC05 has 560,768 genuine translit-
eration pairs and 112,622 false ones.
We run one iteration of alignment over LDC05
(both genuine and false) with the distance func-
tion d(ei, cpk) derived from the affinity matrix of
one aligned Xinhua training set. In this way, each
transliteration pair in LDC05 provides an align-
ment distance. One can expect that a genuine
transliteration pair typically aligns well, leading
to a low distance, while a false transliteration pair
will do otherwise. To remove the effect of word
length, we normalize the distance by the English
name length, the Chinese phonetic transcription
142
length, and the sum of both, producing score1,
score2 and score3 respectively.
Miss?probability?(%)
F
a
l
s
e
?
a
l
a
r
m
?
p
r
o
b
a
b
i
l
i
t
y
?
(
%
)
2 5
10
1
2
5
1
0
1
20
2
0
score
2
EER:?4.48?%
score
1
EER:?7.13?%
score
3
EER:?4.80?%
(a) DET with score1, score2,
score3.
1 2 5 10
1
2
5
1
0
Miss?probability?(%)
F
a
l
s
e
?
a
l
a
r
m
?
p
r
o
b
a
b
i
l
i
t
y
?
(
%
)
Entropy:?2.396
MRR:?0.773
EER:?4.48?%
Entropy:?2.529
MRR:?0.764
EER:?4.52%
Entropy:?2.625
MRR:?0.754
EER:?4.70%
(b) DET results vs. three different
alignment quality.
Figure 9: Detection error tradeoff (DET) curves
for transliteration validation on LDC05.
We can now classify each LDC05 name pair as
genuine or false by having a hypothesis test. When
the test score is lower than a pre-set threshold, the
name pair is accepted as genuine, otherwise false.
In this way, each pre-set threshold will present two
types of errors, a false alarm and a miss-detect
rate. A common way to present such results is via
the detection error tradeoff (DET) curves, which
show all possible decision points, and the equal er-
ror rate (EER), when false alarm and miss-detect
rates are equal.
Figure 9a shows three DET curves based on
score1, score2 and score3 respectively for one
one alignment solution on the Xinhua training set.
The horizontal axis is the probability of miss-
detecting a genuine transliteration, while the verti-
cal one is the probability of false-alarms. It is clear
that out of the three, score2 gives the best results.
We select the alignments of Xinhua training
set that produce the highest and the lowest MRR.
We also randomly select three other alignments
that produce different MRR values from the pool
of 114 phonological and 80 affinity alignments.
Xinhua train 
set algnment
Alignment entropy 
of Xinhua train set
MRR on Xinhua 
test set
LDC 
classification 
EER, %
1
2
3
4
5
2.396 0.773 4.48
2.529 0.764 4.52
2.586 0.761 4.51
2.621 0.757 4.71
2.625 0.754 4.70
Table 1: Equal error ratio of LDC transliteration
pair validation for different alignments of Xinhua
training set.
We use each alignment to derive distance func-
tion d(ei, cpk). Table 1 shows the EER of LDC05
validation using score2, along with the alignment
entropy of the Xinhua training set that derives
d(ei, cpk), and the MRR on Xinhua test set in the
generative transliteration experiment (see Section
5.2) for all 5 alignments. To avoid cluttering Fig-
ure 9b, we show the DET curves for alignments
1, 2 and 5 only. We observe that distance func-
tion derived from better aligned Xinhua corpus,
as measured by both our alignment entropy met-
ric and MRR, leads to a higher validation accuracy
consistently on LDC05.
6 Conclusions
We conclude that the alignment entropy is a re-
liable indicator of the alignment quality, as con-
firmed by our experiments on both Xinhua and
LDC corpora. Alignment entropy does not re-
quire the gold standard reference, it thus can be
used to evaluate alignments of large transliteration
corpora and is possibly to give more reliable esti-
mate of alignment quality than the F -score metric
as shown in our transliteration experiment.
The alignment quality of training corpus has
a significant impact on the transliteration mod-
els. We achieve the highest MRR of 0.773 on
Xinhua corpus with phonological alignment tech-
nique, which represents a significant performance
gain over other reported results. Phonological
alignment outperforms affinity alignment on clean
database.
We propose using alignment distance to validate
transliterations. A high quality alignment on a
small verified corpus such as Xinhua can be effec-
tively used to validate a large noisy corpus, such
as LDC05. We believe that this property would be
useful in transliteration extraction, cross-lingual
information retrieval applications.
143
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Sta-
tistical transliteration for English-Arabic cross lan-
guage information retrieval. In Proc. ACM CIKM.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc. ACL
Workshop: Computational Apporaches to Semitic
Languages.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In Proc.
COLING, pages 597?603.
Lin L. Chase. 1997. Error-responsive feedback mech-
anisms for speech recognizers. Ph.D. thesis, CMU.
Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration. In Proc. COLING/ACL,
pages 191?198
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with web min-
ing and transliteration. In IJCAI, pages 1629?1634.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An English to Korean transliteration model of
extended Markov window. In Proc. COLING, vol-
ume 1.
Paul. B. Kantor and Ellen. M. Voorhees. 2000. The
TREC-5 confusion track: comparing retrieval meth-
ods for scanned text. Information Retrieval, 2:165?
176.
Brett Kessler. 2005. Phonetic comparison algo-
rithms. Transactions of the Philological Society,
103(2):243?260.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2007.
A phonetic similarity model for automatic extraction
of transliteration pairs. ACM Trans. Asian Language
Information Processing, 6(2).
Patrik Lambert. 2008. Exploiting lexical informa-
tion and discriminative alignment training in statis-
tical machine translation. Ph.D. thesis, Universitat
Polite`cnica de Catalunya, Barcelona, Spain.
Kevin Lenzo. 1997. t2p: text-to-phoneme converter
builder. http://www.cs.cmu.edu/?lenzo/t2p/.
Kevin Lenzo. 2008. The CMU pronounc-
ing dictionary. http://www.speech.cs.cmu.edu/cgi-
bin/cmudict.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. ACL, pages 159?166.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007a. A
vector space modeling approach to spoken language
identification. IEEE Trans. Acoust., Speech, Signal
Process., 15(1):271?284.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007b. Semantic transliteration of personal
names. In Proc. ACL, pages 120?127.
Linguistic Data Consortium. 2005. LDC Chinese-
English name entity lists LDC2005T34.
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Proc. HLT-NAACL,
pages 1?10.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002.
Jong-Hoon Oh and Key-Sun Choi. 2005. Machine
learning based english-to-korean transliteration us-
ing grapheme and phoneme information. IEICE
Trans. Information and Systems, E88-D(7):1737?
1748.
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in
Latin script via language identification and corpus
validation. In Proc. ACL, pages 183?190.
Tao Tao, Su-Youn Yoon, Andrew Fisterd, Richard
Sproat, and ChengXiang Zhai. 2006. Unsupervised
named entity transliteration using temporal and pho-
netic correlation. In Proc. EMNLP, pages 250?257.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
M. M. Withgott and F. R. Chen. 1993. Computational
models of American speech. Centre for the study of
language and information.
Xinhua News Agency. 1992. Chinese transliteration
of foreign personal names. The Commercial Press.
LiLi Xu, Atsushi Fujii, and Tetsuya Ishikawa. 2006.
Modeling impression in probabilistic transliteration
into Chinese. In Proc. EMNLP, pages 242?249.
Su-Youn Yoon, Kyoung-Young Kim, and Richard
Sproat. 2007. Multilingual transliteration using fea-
ture based phonetic method. In Proc. ACL, pages
112?119.
144
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 1?18,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Report of NEWS 2009 Machine Transliteration Shared Task
Haizhou Li?, A Kumaran?, Vladimir Pervouchine? and Min Zhang?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,vpervouchine,mzhang}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the details of the
Machine Transliteration Shared Task con-
ducted as a part of the Named Enti-
ties Workshop (NEWS), an ACL-IJCNLP
2009 workshop. The shared task features
machine transliteration of proper names
from English to a set of languages. This
shared task has witnessed enthusiastic par-
ticipation of 31 teams from all over the
world, with diversity of participation for
a given system and wide coverage for a
given language pair (more than a dozen
participants per language pair). Diverse
transliteration methodologies are repre-
sented adequately in the shared task for a
given language pair, thus underscoring the
fact that the workshop may truly indicate
the state of the art in machine transliter-
ation in these language pairs. We mea-
sure and report 6 performance metrics on
the submitted results. We believe that the
shared task has successfully achieved the
following objectives: (i) bringing together
the community of researchers in the area
of Machine Transliteration to focus on var-
ious research avenues, (ii) Calibrating sys-
tems on common corpora, using common
metrics, thus creating a reasonable base-
line for the state-of-the-art of translitera-
tion systems, and (iii) providing a quan-
titative basis for meaningful comparison
and analysis between various algorithmic
approaches used in machine translitera-
tion. We believe that the results of this
shared task would uncover a host of inter-
esting research problems, giving impetus
to research in this significant research area.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They have a critical role
in Cross Language Information Retrieval (CLIR)
and Machine Translation (MT) systems as the sys-
tems? performances are shown to positively cor-
relate with the correct conversion of names be-
tween the languages in several studies (Demner-
Fushman and Oard, 2002; Mandl and Womser-
Hacker, 2005; Hermjakob et al, 2008; Udupa et
al., 2009). The traditional source for name equiva-
lence, the bilingual dictionaries ? whether hand-
crafted or statistical ? offer only limited support
as they do not have sufficient coverage of names.
New names are introduced to the vocabulary of a
language every day.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. This has attracted attention from the re-
search community. Over the last decade scores of
papers on Machine Transliteration have appeared
in the top Computational Linguistics, Information
Retrieval and Data Management conferences, ex-
ploring diverse algorithmic approaches in a wide
variety of different languages (Knight and Graehl,
1998; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Goldwasser and Roth,
2008; Goldberg and Elhadad, 2008; Klementiev
and Roth, 2006). However, there has not been
any coordinated effort in calibrating the state-of-
the-art technical capabilities of machine translit-
eration: the studies explore different algorithmic
approaches in different language pairs and report
their performance in different metrics and tested
on different corpora.
The overarching objective of this shared task
is to drive the machine transliteration technology
forward, to measure and baseline the state-of-the-
1
art and to provide a meaningful comparison be-
tween the most promising algorithmic approaches
in order to stimulate the discussions among the re-
searchers. The NLP community in Asia is espe-
cially interested in transliteration as several major
Asian languages do not use Latin script in their na-
tive writing systems. The Named Entity Workshop
(NEWS 2009) in ACL-IJCNLP 2009 in Singapore
provides an ideal platform for the shared task to
take off. This is precisely what we address in this
shared task on machine transliteration that is con-
ducted as a part of the Named Entity Workshop
(NEWS-2009), an ACL-IJCNLP 2009 workshop.
The shared task aims at achieving the following
objectives:
? Providing a forum to bring together the com-
munity of researchers in the area of Machine
Transliteration to focus on various research
avenues in this important research area.
? Calibrating systems on common hand-crafted
corpora, using common metrics, in many dif-
ferent languages, thus creating a reasonable
baseline for the state-of-the-art of translitera-
tion systems.
? Analysing the results so that a reason-
able comparison of different algorithmic
approaches and their trade-offs (such as,
transliteration quality vs. generality of ap-
proach across languages vs. training data
size, etc.) may be explored.
We believe that a substantial part of what we have
set out to achieve has been accomplished, and we
present this report as a record of the task pro-
cess, system participation and results and our find-
ings. It is our hope that this reporting will generate
lively discussions during the NEWS workshop and
subsequent research in this important area.
This introduction outlines the purpose of the
transliteration shared task conducted as a part of
the NEWS workshop. Section 2 outlines the ma-
chine transliteration task and the corpora used and
Section 3 discusses the metrics chosen for evalua-
tion, along with the rationale for choosing them.
Section 4 sketches the participation. Section 5
presents the results of the shared task and the anal-
ysis of the results. Section 6, summarises the
queries and feedback we have received from the
participants and Section 7 concludes, presenting
some lessons learnt from the current edition of the
shared task, and some ideas we want to pursue
in the future plan for the Machine Transliteration
tasks.
2 Transliteration Shared Task
In this section, we outline the definition of the task,
the process followed and the rationale for the de-
cisions.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target language,
but also at its ultimate utility for downstream ap-
plications, such as CLIR and MT. We have nar-
rowed down to three specific requirements for the
task, as follows: ?Transliteration is the conver-
sion of a given name in the source language (a
text string in the source writing system or orthog-
raphy) to a name in the target language (another
text string in the target writing system or orthog-
raphy), such that the target language name is:
(i) phonemically equivalent to the source name
(ii) conforms to the phonology of the target lan-
guage and (iii) matches the user intuition of the
equivalent of the source language name in the tar-
get language.?
Given that the phoneme set of languages may
not be exactly the same, the first requirement must
be diluted to ?close to?, instead of ?equivalent?.
The second requirement is needed to ensure that
the target string is a valid string as per the target
language phonology. The third requirement is in-
troduced to produce what a normal user would ex-
pect (at least for the popular names), and in or-
der to make it useful for downstream applications
like MT or CLIR systems. Though the third re-
quirement make systems produce target language
strings that marginally violate the first or second
requirements, it ensures that such transliteration
system is of value to downstream systems. All the
above requirements are implicitly enforced by the
choice of name pairs used to define the training
and test corpora in a given language pair. In cases
where multiple equivalent target language names
are possible for a source language name, we in-
2
clude all of them.
After much debate, we have also retained the
task name as ?transliteration?, though our defi-
nition may be closest to the ?popular transcrip-
tion? (Halpern, 2007), due to the popularity of
term ?Machine Transliteration? among the lan-
guage technology researchers.
2.2 Shared Task Description
The shared task is specified as development of ma-
chine transliteration systems in one or more of the
specified language pairs. Each language pair of
the shared task consists of a source and a target
language, implicitly specifying the transliteration
direction. Training and development data in each
of the language pairs have been made available to
all registered participants for developing a translit-
eration system for that specific language pair using
any approach that they find appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 1,000 and 3,000
source names (approximately 10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair every participant is re-
quired to submit one run (designated as a ?stan-
dard? run) that uses only the data provided by the
NEWS workshop organisers in that language pair,
and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more runs (designated as ?non-standard?) for ev-
ery language pair using either data beyond that
provided by the shared task organisers or linguis-
tic resources in a specific language, or both. This
essentially may enable any participant to demon-
strate the limits of performance of their system in
a given language pair.
The shared task timelines provide adequate time
for development, testing (approximately 2 months
after the release of the training data) and the final
result submission (5 days after the release of the
test data).
2.3 Shared Task Corpora
We have had two specific constraints in selecting
languages for the shared task: language diversity
and data availability. To make the shared task in-
teresting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 7 languages shown in Table 1 for the
task (Li et al, 2004; Kumaran and Kellner, 2007;
MSRI, 2009; CJKI, 2009).
For all of the languages chosen, we have been
able to procure paired names data between En-
glish and the respective languages and were able
to make them available to the participants. In ad-
dition, we have been able to procure a specific
corpus of about 40K Romanised Japanese names
and their Kanji counterparts, and the correspond-
ing language pair (Japanese names from their Ro-
manised form to Kanji) has been included as one
of the task language pair.
It should be noted here that each corpus has a
definite skew in its characteristics: the names in
the Chinese, Japanese and Korean (CJK) language
corpora are Western names; the Indic languages
(Hindi, Kannada and Tamil) corpora consists of a
mix of Indian and Western names. The Roman-
ised Kanji to Kanji corpus consists only of native
Japanese names. While such characteristics may
have provided us an opportunity to specifically
measure the performance for forward translitera-
tions (in CJK) and backward transliterations (in
Romanised Kanji), we do not highlight such fine
distinctions in this edition.
Finally, it should be noted here that the corpora
procured and released for NEWS 2009 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit re-
sults of one standard and up to four non-standard
3
Source language Target language Data Source Data Size (No. source names) Task IDTraining Development Testing
English Hindi Microsoft Research India 9,975 974 1,000 EnHi
English Tamil Microsoft Research India 7,974 987 1,000 EnTa
English Kannada Microsoft Research India 7,990 968 1,000 EnKa
English Russian Microsoft Research India 5,977 943 1,000 EnRu
English Chinese Institute for Infocomm Research 31,961 2,896 2,896 EnCh
English Korean Hangul CJK Institute 4,785 987 989 EnKo
English Japanese Katakana CJK Institute 23,225 1,492 1,489 EnJa
Japanese name (in English) Japanese Kanji CJK Institute 6,785 1,500 1,500 JnJk
Table 1: Source and target languages for the shared task on transliteration.
runs. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 6 evaluation
metrics capturing different aspects of translitera-
tion performance. Since a name may have mul-
tiple correct transliterations, all these alternatives
are treated equally in the evaluation, that is, any
of these alternatives is considered as a correct
transliteration, and all candidates matching any of
the reference transliterations are accepted as cor-
rect ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
4
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
3.5 MAP10
MAP10 measures the precision in the 10-best can-
didates for i-th source name provided by the can-
didate system. In general, the higher MAP10 is,
the better is the quality of the transliteration sys-
tem in capturing the multiple references.
MAP10 =
1
N
N?
i=1
1
10
(
10?
k=1
num(i, k)
)
(10)
3.6 MAPsys
MAPsys measures the precision in the top Ki-best
candidates produced by the system for i-th source
name, for which ni reference transliterations are
available. This measure allows the systems to pro-
duce variable number of transliterations, based on
their confidence in identifying and producing cor-
rect transliterations.
MAPsys =
1
N
N?
i=1
1
Ki
(
Ki?
k=1
num(i, k)
)
(11)
4 Participation in Shared Task
There have been 31 systems from around the
world that participated in the shared task and sub-
mitted the transliteration results for a common test
data, produced by their systems trained on the
common training corpora.
A few teams have participated in all or almost
all tasks (that is, language pairs); most others par-
ticipated in 3 tasks on average. Each language pair
has attracted on average around 13 teams. The par-
ticipation details are shown in Table 3 and the de-
mographics of the participating teams by country
is shown in Figure 1.
? ? ? ? ? ? ? ? ? ? ??
??
???
???
?????????
????
?????
???
????
????
????????
0 1 2 3 4 5 6 7 8 9 10
Figure 1: Participation by country.
Teams are required to submit at least one stan-
dard run for every task they participated in. In total
104 standard and 86 non-standard runs have been
submitted. Table 2 shows the number of standard
and non-standard runs submitted for each task. It
is clear that the most ?popular? tasks are translit-
eration from English to Hindi and from English to
Chinese, attempted by 21 and 18 participants re-
spectively. Overall, as can be noted from the re-
sults, each task has received significant participa-
tion.
5 Task Results and Analysis
5.1 Standard runs
The 8 individual plots in Figure 2 summarise (for
each task) the results of standard runs via 3 mea-
sured metrics concerning output of at least one
correct candidate per source word, namely, ac-
curacy in top-1, F -score and Mean Reciprocal
Rank (MRR). The plots in Figure 3 summarise (for
each task) the results for 3 metrics on ranked or-
dered transliteration output of the systems, namely
MAPref , MAP10 and MAPsys metrics. All the
results are presented numerically in Tables 8?11,
for all evaluation metrics. These are the official
5
English
to Hindi
English
to Tamil
English
to Kan-
nada
English
to Rus-
sian
English
to Chi-
nese
English
to Ko-
rean
English
to
Japanese
Katakana
Japanese
translit-
erated to
Japanese
Kanji
Language pair code EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk
Standard runs 21 13 14 13 18 8 10 7
Non-standard runs 18 5 5 16 20 9 5 8
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
evaluation results published for this edition of the
transliteration shared task. Note that two teams
have updated their results (after fixing bugs in their
systems) after the deadline; their results are iden-
tified specifically.
We find that two approaches to transliteration
are most popular in the shared task submissions.
One of these approaches is Phrase-based statis-
tical machine transliteration (Finch and Sumita,
2008), an approach initially developed for ma-
chine translation (Koehn et al, 2003). Systems
that adopted this approach are (Song, 2009; Haque
et al, 2009; Noeman, 2009; Rama and Gali, 2009;
Chinnakotla and Damani, 2009).1 The other is
Conditional Random Fields(Lafferty et al, 2001)
(CRF), adopted by (Aramaki and Abekawa, 2009;
Shishtla et al, 2009). With only a few exceptions,
most implementations are based on approaches
that are language-independent. Indeed, many of
the participants fielded their systems on multiple
languages, as can be seen from Table 3.
We also note that combination of several differ-
ent models via re-ranking of their outputs (CRF,
Maximum Entropy Model, Margin Infused Re-
laxed Algorithm) proves to be very successful (Oh
et al, 2009); their system (reported as Team ID
6) produced the best or second-best transliteration
performance consistently across all metrics, in all
tasks, except Japanese back-transliteration. Exam-
ples of other model combinations are (Das et al,
2009).
At least two teams (reported as Team IDs 14
and 27) incorporate language origin detection in
their system (Bose and Sarkar, 2009; Khapra and
Bhattacharyya, 2009). The Indian language cor-
pora contains names of both English and Indic ori-
gin. Khapra and Bhattacharyya (2009) demon-
strate how much the transliteration performance
can be improved when language of origin detec-
1To maintain anonymity, papers of the teams that submit-
ted anonymous results are not cited in this report.
tion is employed, followed by a language-specific
transliteration model for decoding.
Some systems merit specific mention as they
adopt are rather unique approaches. Jiampoja-
marn et al (2009) propose DirectTL discrimina-
tive sequence prediction model that is language-
independent (reported as Team ID 7). Their
transliteration accuracy is among the highest in
several tasks (EnCh, EnHi and EnRu). Zelenko
(2009) present an approach to the transliteration
problem based on Minimum Description Length
(MDL) principle. Freitag and Wang (2009) ap-
proach the problem of transliteration with bidirec-
tional perceptron edit models.
Finally, in Figure 4 we present a plot where
each point represents a standard run by a system,
with different tasks marked with specific shape
and colour. This plot gives a bird-eye-view of
the system performances across two most uncorre-
lated evaluation metrics, namely accuracy in top-1
(ACC) and Mean F -score. Not surprisingly, we
notice very high performance in terms of F -score
for English to Russian transliteration task, likely
because Russian orthography follows pronuncia-
tion very closely, except for characters like soft
and hard signs that can hardly be recovered from
English words.
We also observe that Japanese back-
transliteration has proven to be much harder
than other (forward-transliteration) tasks. In
general, we note that a well-performing translit-
eration system performs well across all metrics.
We are curious about the correlation between
different metrics, and the results (specifically,
the Spearman?s rank correlation coefficient) are
presented below:
? Accuracy in top-1 vs. F -score: 0.40
? Accuracy in top-1 vs. MRR: 0.97
? Accuracy in top-1 vs. MAPref : 0.997
6
? Accuracy in top-1 vs. MAP10: 0.89
? Accuracy in top-1 vs. MAPsys: 0.80
We find that F -score is the most uncorrelated met-
ric: the Spearman?s rank correlation coefficient
between F -score and accuracy in top-1 is 0.40 and
between F -score and MRR it is 0.44. This is likely
because all metrics, except for F -score, are based
on word accuracy, while F -score is based on word
similarity allowing non-matching words to have
scores well above 0.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Accuracy in top-10
0.10.2
0.30.4
0.50.6
0.70.8
0.91
F-score English to ChineseEnglish to HindiEnglish to TamilEnglish to KannadaEnglish to RussianEnglish to KoreanEnglish to JapaneseJapanese transliterated to Japanese Kanji
Figure 4: Accuracy in top-1 vs. F -score for dif-
ferent tasks.
5.2 Non-standard runs
For the non-standard runs there exist no restric-
tions for the teams on the use of more data or other
linguistic resources. The purpose of non-standard
runs is to see how accurate personal name translit-
eration can be, for a given language pair. The ap-
proaches used in non-standard runs are typical and
may be summarised as follows:
? Dictionary lookup.
? Pronunciation dictionaries to convert words
to their phonetic transcription.
? Additional corpora for training and dictio-
nary lookup, such as LDC English-Chinese
named entity list LDC2005T34 (Linguistic
Data Consortium, 2005).
? Web search, and in particular, Wikipedia
search. First, transliteration candidates are
generated. Then a Web search is performed
to see if any of the candidates appear in the
search results. Based on the results, the can-
didates are re-ranked.
The results are shown in Tables 16?19. For En-
glish to Chinese and English to Russian transliter-
ation tasks the accuracy in top-1 can go as high as
0.909 and 0.955 respectively when Web search is
used to aid transliteration.
5.3 Post-evaluation
Two participants have found a bug in their system
implementation and re-evaluated the results after
the deadline. Their results are marked specifically
in Tables 4?8 and 16.
6 Process Analysis and Fine-tuning
In this section we highlight some of the sugges-
tions and feedback that we have received from the
participants during the course of this shared task.
While a few of them have been implemented in the
current edition, many of these may be considered
in the future editions of the shared task.
More or different languages There is quite a
bit of interest in enhancing the list of language
pairs short-listed. While we are constrained (in
this edition) due to the availability of manually
verified data, certainly more languages will be in-
cluded in the future editions, as some specific data
have already been promised for future editions.
Bidirectional transliteration Many partic-
ipants express interest in transliterations into
English; and this reflexive task will be added in
the future editions. We believe it will encourage
more participation as it will be easy to read and
verify system output in English for those teams
not familiar with the non-English side of the
language.
Forward vs. backward transliteration There
is quite a bit of interest expressed in specifically
separating forward and backward transliteration
tasks. However, such separation requires specific
corpora with known origin for each name pair, and
clearly we are constrained by the availability of
corpora. When corpora is available, the task may
be designated explicitly in future editions.
Number of standard runs The number of stan-
dard runs that may be submitted may be increased
in the future editions, as many participants would
like to submit many standard runs, trained with
different parameters.
7
Errors in training and development corpora
While we have taken all precautions in acquiring
and creating the corpora, some errors still remain.
We thank those who have sent us the errata. How-
ever, since the affected part is less than 0.5% of
the data, we believe that the effect on final results
is minimal. The errata will be made available to
all participants.
7 Conclusions and Future Plans
We are pleased to report a comprehensive cal-
ibration and baselining of machine translitera-
tion apporaches as most state-of-the-art machine
transliteration techniques are represented in the
shared task. The most popular techniques such as
Phrase-Based Machine Transliteration (Koehn et
al., 2003), and Conditional Random Fields (Laf-
ferty et al, 2001) are inspired by recent progress in
machine translation. As the standard runs are lim-
ited by the use of corpus, most of the systems are
implemented under the direct orthographic map-
ping (DOM) framework (Li et al, 2004). While
the standard runs allow us to conduct meaning-
ful comparison across different algorithms, we
recognise that the non-standard runs open up more
opportunities for exploiting larger linguistic cor-
pora. It is also noted that several systems have re-
ported improved performance over any previously
reported results on similar corpora.
NEWS 2009 Shared Task represents a suc-
cessful debut of a community effort in driving
machine transliteration techniques forward. The
overwhelming responses in the first shared task
also warrant continuation of such an effort in fu-
ture ACL or IJCNLP events.
Acknowledgements
The organisers of the NEWS 2009 Shared Task
would like to thank the Institute for Infocomm Re-
search (Singapore), Microsoft Research India and
CJK Institute (Japan) for providing the corpora
and technical support. Without those, the Shared
Task would not be possible. We thank those par-
ticipants who identified errors in the data and sent
us the errata. We want to thank Monojit Choud-
hury for his contribution to metrics defined for the
shared task. We also want to thank the members
of programme committee for their invaluable com-
ments that improve the quality of the shared task
papers. Finally, we wish to thank all the partici-
pants for their active participation that have made
this first machine transliteration shared task a com-
prehensive one.
8
References
Eiji Aramaki and Takeshi Abekawa. 2009. Fast de-
coding and easy implementation: Transliteration as
a sequential labeling. In Proc. ACL/IJCNLP Named
Entities Workshop Shared Task.
Dipankar Bose and Sudeshna Sarkar. 2009. Learn-
ing multi character alignment rules and classifica-
tion of training data for transliteration. In Proc.
ACL/IJCNLP Named Entities Workshop Shared
Task.
Manoj Kumar Chinnakotla and Om P. Damani. 2009.
Experiences with English-Hindi, English-Tamil and
English-Kannada transliteration tasks at NEWS
2009. In Proc. ACL/IJCNLP Named Entities Work-
shop Shared Task.
CJKI. 2009. CJK Institute. http://www.cjk.org/.
Amitava Das, Asif Ekbal, Tapabrata Mondal, and
Sivaji Bandyopadhyay. 2009. English to Hindi
machine transliteration system at NEWS 2009.
In Proc. ACL/IJCNLP Named Entities Workshop
Shared Task.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Dayne Freitag and Zhiqiang Wang. 2009. Name
transliteration with bidirectional perceptron edit
models. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Yoav Goldberg and Michael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar
Srivastava, Sudip Kumar Naskar, and Andy Way.
2009. English-Hindi transliteration using context-
informed PB-SMT. In Proc. ACL/IJCNLP Named
Entities Workshop Shared Task.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Mitesh Khapra and Pushpak Bhattacharyya. 2009. Im-
proving transliteration accuracy using word-origin
detection and lexicon lookup. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Int?l.
Conf. Machine Learning, pages 282?289.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Linguistic Data Consortium. 2005. LDC Chinese-
English name entity lists LDC2005T34.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Sara Noeman. 2009. Language independent translit-
eration system using phrase based SMT approach
on substring. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Machine transliteration with target-
language grapheme and phoneme: Multi-engine
transliteration approach. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
Taraka Rama and Karthik Gali. 2009. Modeling ma-
chine transliteration as a phrase based statistical ma-
chine translation problem. In Proc. ACL/IJCNLP
Named Entities Workshop Shared Task.
9
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
Praneeth Shishtla, V Surya Ganesh, S Sethurama-
lingam, and Vasudeva Varma. 2009. A language-
independent transliteration schema using character
aligned models. In Proc. ACL/IJCNLP Named Enti-
ties Workshop Shared Task.
Yan Song. 2009. Name entities transliteration via
improved statistical translation on character-level
chunks. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
Dmitry Zelenko. 2009. Combining MDL translitera-
tion training with discriminative modeling. In Proc.
ACL/IJCNLP Named Entities Workshop Shared
Task.
10
Team ID Organisation English to
Hindi
English to
Tamil
English to
Kannada
English to
Russian
English to
Chinese
English
to Ko-
rean
English
to
Japanese
Katakana
Japanese
translit-
erated to
Japanese
Kanji
EnHi EnTa EnKa EnRu EnCh EnKo EnJa JnJk
1 IIT Bombay x x x
2 Institution of Computational
Linguistics Peking Univer-
sity
x
3 University of Tokyo x x x x x x x
4? University of Illinois,
Urbana-Champaign
x x
5 IIT Bombay x x
6 NICT x x x x x x x x
7 University of Alberta x x x x x x
8 x x x x x x x x
9 x x x x x x x x
10 Johns Hopkins University x x x x x
11 x x x
12 x x
13 Jadavpur University x
14 IIIT Hyderabad x
15 x x x
16? ARL-CACI x
17 x x x x x x x x
18 x
19? Chaoyang University of
Technology
x
20 Pondicherry University x x x
21 Microsoft Research x x
22 SRI International x x x x x
23 IBM Cairo TDC x x
24 SRA x x x x x x x x
25 IIT Kharagpur x x x
26 Institute of Software Chinese
Academy of Sciences
x
27 x
28 George Washington Univer-
sity
x
29? x
30 Dublin City University x
31 IIIT x x x x x
Table 3: Participation of teams in different tasks. ?Participants without a system paper.
11
?
??
??
??
??
??
??
??
??
??
?
? ? ?? ?? ? ? ?? ?? ?? ?? ? ?? ?? ?? ?? ?? ? ? ?? ?? ??
???
????
??
Site?ID
(a) English to Hindi
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ? ?? ?? ? ? ?? ? ?? ? ?? ??
???
????
??
Site?ID
(b) English to Kannada
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ?? ? ?? ?? ? ? ?? ? ?? ??
???
????
??
Site?ID
(c) English to Tamil
???
????
?? ??
????
????
?
? ? ?? ?? ? ?? ?? ? ?? ? ? ?? ??
???
????
??
Site?ID
(d) English to Russian
???
????
?? ??
????
????
?
? ? ?? ? ? ?? ? ?? ?? ? ? ?? ?? ?? ?? ?? ?? ??
???
????
??
Site?ID
(e) English to Chinese
?
??
??
??
??
??
??
??
??
?? ? ?? ?? ? ? ? ?
???
????
??
Site?ID
(f) English to Korean
?
??
??
??
??
??
??
??
??
??
?
? ?? ?? ? ?? ? ? ?? ?? ?
???
????
??
Site?ID
(g) English to Japanese Katakana
?
??
??
??
??
??
??
??
??
??
?? ?? ? ? ? ? ??
???
????
??
Site?ID
(h) Japanese transliterated to Japanese Kanji
Figure 2: Accuracy in top-1, F -score and MRR for standard runs.
12
?
??
??
??
??
??
??
? ? ?? ?? ? ? ?? ?? ?? ?? ? ?? ?? ?? ?? ?? ? ? ?? ?? ??
?????
????
?????
Site?ID
(a) English to Hindi
?
??
??
??
??
??
??
? ?? ?? ? ?? ?? ? ? ?? ? ?? ? ?? ??
?????
????
?????
Site?ID
(b) English to Kannada
??
??
??
??
??
??
??
? ?? ?? ?? ? ?? ?? ? ? ?? ? ?? ??
?????
????
?????
Site?ID
(c) English to Tamil
?
??
??
??
??
??
??
? ? ?? ?? ? ?? ?? ? ?? ? ? ?? ??
?????
????
?????
Site?ID
(d) English to Russian
?
??
??
??
??
??
??
??
??
? ? ?? ? ? ?? ? ?? ?? ? ? ?? ?? ?? ?? ?? ?? ??
?????
????
?????
Site?ID
(e) English to Chinese
?
??
??
??
??
??
??
?? ? ?? ?? ? ? ? ?
?????
????
?????
Site?ID
(f) English to Korean
?
??
??
??
??
??
??
? ?? ?? ? ?? ? ? ?? ?? ?
?????
????
?????
Site?ID
(g) English to Japanese Katakana
?
??
??
??
??
??
??
?? ?? ? ? ? ? ??
?????
????
?????
Site?ID
(h) Japanese transliterated to Japanese Kanji
Figure 3: MAPref , MAP10 and MAPsys scores for standard runs.
13
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.498 0.890 0.603 0.488 0.195 0.195 University of Alberta
6 0.483 0.892 0.607 0.477 0.202 0.202 NICT
13 0.471 0.861 0.519 0.463 0.162 0.383 Jadavpur University
14 0.463 0.876 0.573 0.454 0.201 0.201 IIIT Hyderabad
8 0.462 0.876 0.576 0.454 0.189 0.189
1 0.423 0.863 0.544 0.417 0.179 0.202 IIT Bombay
11 0.418 0.879 0.546 0.412 0.183 0.240
21 0.418 0.864 0.522 0.409 0.170 0.170 Microsoft Research
17 0.415 0.858 0.505 0.406 0.164 0.168
24 0.409 0.864 0.527 0.402 0.174 0.176 SRA
5 0.409 0.881 0.546 0.400 0.184 0.184 IIT Bombay
31 0.407 0.877 0.544 0.402 0.195 0.195 IIIT
16 0.406 0.863 0.514 0.397 0.170 0.280 ARL-CACI
30 0.399 0.863 0.488 0.392 0.157 0.157 Dublin City University
10 0.398 0.855 0.515 0.389 0.170 0.170 Johns Hopkins University
25 0.366 0.854 0.493 0.360 0.164 0.164 IIT Kharagpur
3 0.363 0.864 0.503 0.360 0.170 0.170 University of Tokyo
9 0.349 0.829 0.455 0.341 0.151 0.151
22 0.212 0.788 0.317 0.207 0.106 0.106 SRI International
29 0.053 0.664 0.089 0.053 0.037 0.037
20 0.004 0.012 0.004 0.004 0.001 0.004 Pondicherry University
21 0.466 0.881 0.567 0.457 0.183 0.183 Microsoft Research (post-evaluation)
22 0.465 0.886 0.567 0.458 0.185 0.185 SRI International (post-evaluation)
Table 4: Standard runs for English to Hindi task.
TeamID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.474 0.910 0.608 0.465 0.204 0.204 NICT
17 0.436 0.894 0.551 0.427 0.184 0.189
11 0.435 0.902 0.572 0.430 0.195 0.265
31 0.406 0.894 0.542 0.399 0.193 0.193 IIIT
1 0.405 0.892 0.542 0.397 0.181 0.184 IIT Bombay
25 0.404 0.883 0.539 0.398 0.182 0.182 IIT Kharagpur
24 0.374 0.880 0.512 0.369 0.174 0.174 SRA
3 0.365 0.884 0.504 0.360 0.172 0.172 University of Tokyo
8 0.361 0.883 0.510 0.354 0.174 0.174
10 0.327 0.870 0.458 0.317 0.156 0.156 Johns Hopkins University
9 0.316 0.848 0.451 0.307 0.154 0.154
22 0.141 0.760 0.256 0.139 0.090 0.090 SRI International
20 0.061 0.131 0.068 0.059 0.021 0.056 Pondicherry University
22 0.475 0.909 0.581 0.466 0.193 0.193 SRI International (post-evaluation)
Table 5: Standard runs for English to Tamil task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.398 0.880 0.526 0.391 0.178 0.178 NICT
17 0.370 0.867 0.499 0.362 0.170 0.175
11 0.363 0.870 0.482 0.355 0.164 0.218
1 0.360 0.861 0.479 0.351 0.161 0.164 IIT Bombay
31 0.350 0.864 0.482 0.344 0.175 0.175 IIIT
24 0.345 0.854 0.462 0.336 0.157 0.157 SRA
8 0.343 0.855 0.458 0.334 0.155 0.155
5 0.335 0.859 0.453 0.327 0.154 0.154 IIT Bombay
25 0.335 0.856 0.457 0.328 0.154 0.154 IIT Kharagpur
3 0.324 0.856 0.438 0.315 0.148 0.148 University of Tokyo
10 0.235 0.817 0.353 0.229 0.121 0.121 Johns Hopkins University
9 0.177 0.799 0.307 0.178 0.109 0.109
22 0.091 0.735 0.180 0.090 0.064 0.064 SRI International
20 0.004 0.009 0.004 0.004 0.001 0.004 Pondicherry University
22 0.396 0.874 0.494 0.385 0.161 0.161 SRI International (post-evaluation)
Table 6: Standard runs for English to Kannada task.
14
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.613 0.928 0.696 0.613 0.212 0.212 University of Alberta
6 0.605 0.926 0.701 0.605 0.215 0.215 NICT
17 0.597 0.925 0.691 0.597 0.212 0.255
24 0.566 0.919 0.662 0.566 0.203 0.216 SRA
8 0.564 0.917 0.677 0.564 0.210 0.210
31 0.548 0.916 0.640 0.548 0.210 0.210 IIIT
23 0.545 0.917 0.596 0.545 0.286 0.299 IBM Cairo TDC
3 0.531 0.912 0.635 0.531 0.219 0.219 University of Tokyo
10 0.506 0.901 0.609 0.506 0.204 0.204 Johns Hopkins University
4 0.504 0.909 0.618 0.504 0.193 0.193 University of Illinois, Urbana-Champaign
9 0.500 0.906 0.613 0.500 0.192 0.192
22 0.364 0.876 0.440 0.364 0.136 0.136 SRI International
27 0.354 0.869 0.394 0.354 0.134 0.134
22 0.609 0.928 0.686 0.609 0.209 0.209 SRI International (post-evaluation)
Table 7: Standard runs for English to Russian task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.731 0.895 0.812 0.731 0.246 0.246 NICT
7 0.717 0.890 0.785 0.717 0.237 0.237 University of Alberta
15 0.713 0.883 0.794 0.713 0.241 0.241
8 0.666 0.864 0.765 0.666 0.234 0.234
2 0.652 0.858 0.755 0.652 0.232 0.232 Institution of Computational Linguistics Peking
University China
17 0.646 0.867 0.747 0.646 0.229 0.229
9 0.643 0.854 0.745 0.643 0.228 0.229
18 0.621 0.852 0.718 0.621 0.220 0.222
24 0.619 0.847 0.711 0.619 0.217 0.217 SRA
4 0.607 0.840 0.695 0.607 0.213 0.213 University of Illinois, Urbana-Champaign
3 0.580 0.826 0.653 0.580 0.199 0.199 University of Tokyo
26 0.498 0.786 0.603 0.498 0.187 0.189 Institute of Software Chinese Academy of Sci-
ences
31 0.493 0.804 0.600 0.493 0.192 0.192 IIIT
22 0.468 0.768 0.546 0.468 0.168 0.168 SRI International
28 0.456 0.763 0.587 0.456 0.185 0.185 George Washington University
10 0.450 0.755 0.514 0.450 0.157 0.166 Johns Hopkins University
23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC
19 0.199 0.606 0.229 0.199 0.070 0.070 Chaoyang University of Technology
22 0.671 0.872 0.725 0.672 0.218 0.218 SRI International (post-evaluation)
Table 8: Standard runs for English to Chinese task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.476 0.742 0.596 0.477 0.187 0.199
6 0.473 0.740 0.584 0.473 0.182 0.182 NICT
12 0.451 0.720 0.576 0.451 0.181 0.181
24 0.413 0.702 0.524 0.412 0.165 0.165 SRA
7 0.387 0.693 0.469 0.387 0.146 0.146 University of Alberta
8 0.362 0.662 0.460 0.362 0.144 0.144
9 0.332 0.648 0.425 0.331 0.134 0.135
3 0.170 0.512 0.218 0.170 0.069 0.069 University of Tokyo
Table 9: Standard runs for English to Korean task.
15
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.537 0.858 0.657 0.529 0.223 0.223 NICT
15 0.510 0.838 0.624 0.498 0.209 0.209
17 0.503 0.843 0.627 0.491 0.212 0.212
7 0.500 0.847 0.604 0.487 0.199 0.199 University of Alberta
21 0.465 0.827 0.559 0.454 0.183 0.183 Microsoft Research
3 0.457 0.828 0.576 0.445 0.194 0.194 University of Tokyo
8 0.449 0.816 0.571 0.436 0.192 0.192
24 0.420 0.807 0.541 0.410 0.182 0.184 SRA
12 0.408 0.808 0.537 0.398 0.182 0.182
9 0.406 0.800 0.529 0.393 0.180 0.180
21 0.469 0.834 0.567 0.454 0.186 0.186 Microsoft Research (post-evaluation)
Table 10: Standard runs for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
15 0.627 0.763 0.706 0.605 0.292 0.292
17 0.606 0.749 0.695 0.586 0.287 0.288
8 0.596 0.741 0.687 0.575 0.282 0.282
7 0.560 0.730 0.644 0.525 0.244 0.244 University of Alberta
9 0.555 0.708 0.653 0.538 0.261 0.261
6 0.532 0.716 0.583 0.485 0.214 0.218 NICT
24 0.509 0.675 0.600 0.491 0.226 0.226 SRA
Table 11: Standard runs for Japanese Transliterated to Japanese Kanji task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
7 0.509 0.893 0.610 0.498 0.198 0.198 University of Alberta
1 0.487 0.873 0.594 0.481 0.195 0.229 IIT Bombay
6 0.475 0.893 0.601 0.469 0.200 0.200 NICT
6 0.469 0.884 0.581 0.464 0.192 0.193 NICT
6 0.455 0.888 0.575 0.448 0.191 0.191 NICT
5 0.448 0.885 0.570 0.439 0.190 0.190 IIT Bombay
6 0.443 0.879 0.555 0.437 0.184 0.191 NICT
17 0.424 0.862 0.513 0.415 0.166 0.174
30 0.421 0.864 0.519 0.415 0.171 0.171 Dublin City University
30 0.420 0.867 0.519 0.413 0.170 0.170 Dublin City University
30 0.419 0.868 0.464 0.419 0.338 0.338 Dublin City University
16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI
16 0.407 0.862 0.528 0.399 0.175 0.289 ARL-CACI
30 0.407 0.856 0.507 0.399 0.168 0.168 Dublin City University
16 0.400 0.864 0.516 0.391 0.171 0.212 ARL-CACI
13 0.389 0.831 0.487 0.385 0.160 0.328 Jadavpur University
13 0.384 0.828 0.485 0.380 0.160 0.325 Jadavpur University
16 0.273 0.796 0.358 0.266 0.119 0.193 ARL-CACI
Table 12: Non-standard runs for English to Hindi task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.478 0.910 0.606 0.472 0.203 0.203 NICT
6 0.459 0.906 0.583 0.453 0.195 0.196 NICT
6 0.459 0.906 0.583 0.453 0.195 0.196 NICT
6 0.453 0.907 0.584 0.446 0.196 0.196 NICT
17 0.437 0.894 0.555 0.426 0.185 0.193
Table 13: Non-standard runs for English to Tamil task.
16
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.399 0.881 0.522 0.391 0.176 0.176 NICT
6 0.386 0.877 0.503 0.379 0.169 0.169 NICT
6 0.380 0.869 0.488 0.370 0.163 0.163 NICT
17 0.374 0.868 0.502 0.366 0.170 0.176
6 0.373 0.869 0.485 0.362 0.162 0.168 NICT
Table 14: Non-standard runs for English to Kannada task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.955 0.989 0.966 0.955 0.284 0.504
17 0.609 0.928 0.701 0.609 0.214 0.263
7 0.608 0.927 0.694 0.608 0.212 0.212 University of Alberta
7 0.607 0.927 0.690 0.607 0.211 0.211 University of Alberta
6 0.600 0.927 0.634 0.600 0.189 0.189 NICT
6 0.600 0.926 0.699 0.600 0.214 0.214 NICT
7 0.591 0.928 0.679 0.591 0.208 0.208 University of Alberta
6 0.561 0.918 0.595 0.561 0.178 0.182 NICT
6 0.557 0.920 0.596 0.557 0.179 0.233 NICT
23 0.545 0.917 0.618 0.545 0.188 0.206 IBM Cairo TDC
23 0.524 0.913 0.602 0.524 0.184 0.203 IBM Cairo TDC
23 0.524 0.913 0.579 0.524 0.277 0.291 IBM Cairo TDC
4 0.496 0.908 0.613 0.496 0.191 0.191 University of Illinois, Urbana-Champaign
27 0.338 0.872 0.408 0.338 0.128 0.128
27 0.293 0.845 0.325 0.293 0.099 0.099
27 0.162 0.849 0.298 0.162 0.188 0.188
Table 15: Non-standard runs for English to Russian task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.909 0.960 0.933 0.909 0.276 0.276
7 0.746 0.900 0.814 0.746 0.245 0.245 University of Alberta
7 0.734 0.895 0.807 0.734 0.244 0.244 University of Alberta
7 0.732 0.895 0.803 0.732 0.242 0.242 University of Alberta
6 0.731 0.894 0.812 0.731 0.246 0.246 NICT
6 0.715 0.890 0.741 0.715 0.220 0.231 NICT
6 0.699 0.884 0.729 0.699 0.216 0.232 NICT
6 0.684 0.873 0.711 0.684 0.211 0.211 NICT
22 0.663 0.867 0.754 0.663 0.230 0.230 SRI International
17 0.658 0.865 0.752 0.658 0.230 0.230
18 0.587 0.834 0.665 0.587 0.203 0.330
26 0.500 0.786 0.607 0.500 0.189 0.191 Institute of Software Chinese Academy of Sciences
22 0.487 0.787 0.622 0.487 0.196 0.196 SRI International
28 0.462 0.764 0.564 0.462 0.175 0.175 George Washington University
28 0.458 0.763 0.602 0.458 0.191 0.191 George Washington University
23 0.411 0.737 0.464 0.411 0.141 0.173 IBM Cairo TDC
19 0.279 0.668 0.351 0.279 0.110 0.110 Chaoyang University of Technology
28 0.058 0.353 0.269 0.058 0.101 0.101 George Washington University
28 0.050 0.359 0.260 0.050 0.098 0.098 George Washington University
4 0.001 0.249 0.001 0.001 0.000 0.000 University of Illinois, Urbana-Champaign
22 0.674 0.873 0.763 0.674 0.232 0.232 SRI International (post-evaluation)
22 0.500 0.793 0.636 0.500 0.200 0.200 SRI International (post-evaluation)
Table 16: Non-standard runs for English to Chinese task.
17
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.794 0.894 0.836 0.793 0.249 0.323
12 0.785 0.887 0.840 0.785 0.252 0.441
12 0.784 0.889 0.840 0.784 0.252 0.484
12 0.781 0.885 0.839 0.781 0.252 0.460
12 0.740 0.868 0.806 0.740 0.243 0.243
6 0.461 0.737 0.576 0.461 0.180 0.180 NICT
6 0.457 0.734 0.506 0.457 0.153 0.153 NICT
6 0.447 0.718 0.493 0.447 0.149 0.149 NICT
6 0.369 0.679 0.406 0.369 0.123 0.123 NICT
Table 17: Non-standard runs for English to Korean task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
6 0.535 0.858 0.656 0.526 0.222 0.222 NICT
6 0.517 0.850 0.567 0.495 0.177 0.188 NICT
6 0.513 0.854 0.567 0.495 0.178 0.178 NICT
7 0.510 0.848 0.614 0.496 0.202 0.202 University of Alberta
6 0.500 0.842 0.547 0.480 0.170 0.196 NICT
Table 18: Non-standard runs for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref MAP10 MAPsys Organisation
17 0.717 0.818 0.784 0.691 0.319 0.319
17 0.703 0.805 0.768 0.673 0.311 0.311
17 0.698 0.805 0.774 0.676 0.317 0.317
17 0.681 0.790 0.755 0.657 0.308 0.309
6 0.525 0.713 0.607 0.503 0.248 0.249 NICT
6 0.525 0.712 0.606 0.502 0.248 0.248 NICT
6 0.523 0.712 0.572 0.479 0.211 0.213 NICT
6 0.517 0.705 0.603 0.496 0.248 0.249 NICT
Table 19: Non-standard runs for Japanese Transliterated to Japanese Kanji task.
18
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 19?26,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Whitepaper of NEWS 2009 Machine Transliteration Shared Task?
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of the
shared task in the NEWS 2009 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be re-
leased, on which the participants are expected to
produce a ranked list of transliteration candidates
in another language (i.e. n-best transliterations),
and this will be evaluated using common metrics.
For every language pair the participants must sub-
mit one run that uses only the data provided by the
NEWS workshop organisers in a given language
pair (designated as ?standard? runs). Users may
submit more runs (?non-standard?) for each lan-
guage pair that uses other data than those provided
by the NEWS 2009 workshop; such runs would be
evaluated and reported separately.
?http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
2 Important Dates
Research paper submission deadline 1 May 2009
Shared task
Registration opens 16 Feb 2009
Registration closes 9 Apr 2009
Release Training/Development Data 16 Feb 2009
Release Test Data 10 Apr 2009
Results Submission Due 14 Apr 2009
Results Announcement 29 Apr 2009
Task (short) Papers Due 3 May 2009
For all submissions
Acceptance Notification 1 Jun 2009
Camera-Ready Copy Deadline 7 Jun 2009
Workshop Date 7 Aug 2009
3 Participation
1. Registration (16 Feb 2009)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (16 Feb 2009)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
3. Evaluation Script (16 Mar 2009)
(a) A sample test set and expected user out-
put format are to be released.
(b) An evaluation script, which runs on the
above two, is to be released.
(c) The participants must make sure that
their output is produced in a way that
the evaluation script may run and pro-
duce the expected output.
19
(d) The same script (with held out test data
and the user outputs) would be used for
final evaluation.
4. Test data (10 April 2009)
(a) The test data would be released on 10
Apr 2009, and the participants have a
maximum of 4 days to submit their re-
sults in the expected format.
(b) Only 1 ?standard? run must be submit-
ted from every group on a given lan-
guage pair; more ?non-standard? runs (0
to 4) may be submitted. In total, maxi-
mum 5 runs (1 ?standard? run plus up to
4 ?non-standard? runs) can be submit-
ted from each group on a registered lan-
guage pair.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
5. Results (29 April 2009)
(a) On 29 April 2009, the evaluation results
would be announced and will be made
available on the Workshop website.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Further, all participants should agree not
to reveal identities of other participants
in any of their publications unless you
get permission from the other respective
participants. If the participants want
to remain anonymous in published
results, they should inform the or-
ganisers (mzhang@i2r.a-star.edu.sg,
a.kumaran@microsoft.com), at the time
of registration. Note that the results of
their systems would still be published,
but with the participant identities
masked. As a result, in this case, your
organisation name will still appear in
the web site as one of participants, but it
is not linked explicitly with your results.
6. Short Papers on Task (3 May 2009)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) All system short papers will be included
in the proceedings. Selected short pa-
pers will be presented orally in the
NEWS 2009 workshop. Reviewers?
comments for all system short papers
and the acceptance notification for the
system short papers for oral presentation
would be announced on 1 June 2009 to-
gether with that of other papers.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review
will be managed electronically
through https://www.softconf.com/acl-
ijcnlp09/NEWS/.
4 Languages Involved
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1.
20
Source language Target language Data Owner Approx. Data Size Task ID
English Chinese Institute for Infocomm Research 30K EnCh
English Japanese Katakana CJK Institute 25K EnJa
English Korean Hangul CJK Institute 7K EnKo
Japanese name (in English) Japanese Kanji CJK Institute 20K JnJk
English Hindi Microsoft Research India 15K EnHi
English Tamil Microsoft Research India 15K EnTa
English Kannada Microsoft Research India 15K EnKa
English Russian Microsoft Research India 10K EnRu
Table 1: Source and target languages for the shared task on transliteration.
The names given in the training sets for Chi-
nese, Japanese and Korean languages are Western
names and their CJK transliterations; the Japanese
Name (in English)? Japanese Kanji data set con-
sists only of native Japanese names. The Indic data
set (Hindi, Tamil, Kannada) consists of a mix of
Indian and Western names.
English? Chinese
Timothy????
English? Japanese Katakana
Harrington??????
English? Korean Hangul
Bennett ? ??
Japanese name in English? Japanese Kanji
Akihiro???
English? Hindi
San Francisco ? ????????????????
English? Tamil
London ? ??????
English? Kannada
Tokyo ? ??????
English? Russian
Moscow ? ??????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 5K ? 40K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 1K ? 2K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 1K ? 3K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; Kumaran and Kellner, 2007; MSRI,
2009; CJKI, 2009). NEWS 2009 will pro-
vide the contact details of each individual
database. The data would be provided in Uni-
code UTF-8 encoding, in XML format; the
results are expected to be submitted in XML
format. The XML formats will be announced
at the workshop website.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
21
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. We expect that the participants to use only the
data (parallel names) provided by the Shared
Task for transliteration task for a ?standard?
run to ensure a fair evaluation. One such run
(using only the data provided by the shared
task) is mandatory for all participants for a
given language pair that they participate in.
5. If more data (either parallel names data or
monolingual data) were used, then all such
runs using extra data must be marked as
?non-standard?. For such ?non-standard?
runs, it is required to disclose the size and
characteristics of the data used in the system
paper.
6. A participant may submit a maximum of 5
runs for a given language pair (including the
mandatory 1 ?standard? run).
6 Paper Format
Paper submissions to NEWS 2009 should follow
the ACL-IJCNLP-2009 paper submission policy,
including paper format, blind review policy and ti-
tle and author format convention. Full papers (re-
search paper) are in two-column format without
exceeding eight (8) pages of content plus one extra
page for references and short papers (task paper)
are also in two-column format without exceeding
four (4) pages, including references. Submission
must conform to the official ACL-IJCNLP-2009
style guidelines. For details, please refer to the
website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 6 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
2http://www.acl-ijcnlp-2009.org/main/authors/stylefiles/index.html
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
22
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
5. MAP10 measures the precision in the 10-best
candidates for i-th source name provided by the
candidate system. In general, the higher MAP10
is, the better is the quality of the transliteration
system in capturing the multiple references. Note
that the number of reference transliterations may
be more or less than 10. If the number of refer-
ence transliterations is below 10, then MAP10 can
never be equal to 1. Only if the number of ref-
erence transliterations for every source word is at
least 10, then MAP10 could possibly be equal to 1.
MAP10 =
1
N
N?
i=1
1
10
(
10?
k=1
num(i, k)
)
(10)
Note that in general MAPm measures the ?good-
ness in m-best? candidate list. We use m = 10
because we have asked the systems to produce up
to 10 candidates for every source name in the test
set.
6. MAPsys Measures the precision in the top
Ki-best candidates produced by the system for i-
th source name, for which ni reference translit-
erations are available. This measure allows the
systems to produce variable number of translitera-
tions, based on their confidence in identifying and
producing correct transliterations. If all of the ni
references are produced in the top-ni candidates
(that is, Ki = ni, and all of them are correct), then
the MAPsys is 1.
MAPsys =
1
N
N?
i=1
1
Ki
(
Ki?
k=1
num(i, k)
)
(11)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Dr. Haizhou Li
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
hli@i2r.a-star.edu.sg
Dr. A. Kumaran
Microsoft Research India
Scientia, 196/36, Sadashivnagar 2nd Main
Road
Bangalore 560080 INDIA
a.kumaran@microsoft.com
Mr. Kurt Easterwood
The CJK Dictionary Institute (CJK Data)
Komine Building (3rd & 4th floors)
34-14, 2-chome, Tohoku, Niiza-shi
Saitama 352-0001 JAPAN
akurt@cjki.org
23
References
CJKI. 2009. CJK Institute. http://www.cjk.org/.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Appendix A: Training/Development Data
? File Naming Conventions:
NEWS09 train XXYY nnnn.xml
NEWS09 dev XXYY nnnn.xml
NEWS09 test XXYY nnnn.xml
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
Appendix B: Submission of Results
? File Naming Conventions:
NEWS09 result XXYY gggg nn descr.xml
? XX: Source Language
? YY: Target Language
? gggg: Group ID
? nn: run ID. Note that run ID ?1? stands for ?stan-
dard? run where only the provided data are al-
lowed to be used. Run ID ?2?5? means ?non-
standard? run where additional data can be used.
? descr: Description of the run.
? File formats:
All data will be made available in XML formats (Fig-
ure 2).
? Data Encoding Formats:
The results are expected to be submitted in UTF-8 en-
coded files without byte-order mark only, and in the
XML format specified.
24
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2009-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=?1?>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=?2?>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2009 Train EnHi 25K.xml
25
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2009 EnHi TUniv 01 StdRunHMMBased.xml
26
Coling 2010: Poster Volume, pages 972?978,
Beijing, August 2010
Improving Name Origin Recognition with Context Features and
Unlabelled Data
Vladimir Pervouchine, Min Zhang, Ming Liu and Haizhou Li
Institute for Infocomm Research, A-STAR
vpervouchine@gmail.com,{mzhang,mliu,hli}@i2r.a-star.edu.sg
Abstract
We demonstrate the use of context fea-
tures, namely, names of places, and un-
labelled data for the detection of per-
sonal name language of origin.
While some early work used either
rule-based methods or n-gram statisti-
cal models to determine the name lan-
guage of origin, we use the discrimi-
native classification maximum entropy
model and view the task as a classifica-
tion task. We perform bootstrapping of
the learning using list of names out of
context but with known origin and then
using expectation-maximisation algo-
rithm to further train the model on
a large corpus of names of unknown
origin but with context features. Us-
ing a relatively small unlabelled cor-
pus we improve the accuracy of name
origin recognition for names written
in Chinese from 82.7% to 85.8%, a
significant reduction in the error rate.
The improvement in F -score for infre-
quent Japanese names is even greater:
from 77.4% without context features to
82.8% with context features.
1 Introduction
Transliteration is a process of rewriting a
word from a source language to a target lan-
guage in a different writing system using the
word?s phonological equivalent. Many techni-
cal terms and proper nouns, such as personal
names, names of places and organisations are
transliterated during translation of a text from
one language to another. A process reverse
to the transliteration, which is recovering a
word in its native language from its translit-
eration in a foreign language, is called back-
transliteration (Knight and Graehl, 1998). In
many natural language processing (NLP) tasks
such as machine translation and cross-lingual
information retrieval, transliteration is an im-
portant component.
Name origin refers to the language of ori-
gin of a name. For example, the origin of En-
glish name ?Smith? and its Chinese transliter-
ation ???? (Shi-Mi-Si)? is English, while
both ?Tokyo? and ??? (Dong-Jing)? are of
Japanese origin.
For machine transliteration the name origins
dictate the way we re-write a foreign name.
For example, given a name written in Chi-
nese for which we do not have a translation
in an English-Chinese dictionary, we first have
to decide whether the name is of Chinese,
Japanese, Korean, English or another origin.
Then we follow the transliteration rules im-
plied by the origin of the name. Although
all English personal names are rendered in
26 letters, they may come from different ro-
manization systems. Each romanisation sys-
972
tem has its own rewriting rules. English name
?Smith? could be directly transliterated into
Chinese as ???? (Shi-Mi-Si)? since it fol-
lows the English phonetic rules, while the Chi-
nese translation of Japanese name ?Koizumi?
becomes ??? (Xiao-Quan)? following the
Japanese phonetic rules. The name origins
are equally important in back-transliteration.
Li et al (2007b) demonstrated that incorpo-
rating name origin recognition (NOR) into a
transliteration system greatly improves the per-
formance of personal name transliteration. Be-
sides multilingual processing, the name origin
also provides useful semantic information (re-
gional and language information) for common
NLP tasks, such as co-reference resolution and
name entity recognition.
Unfortunately, not much attention has been
given to name origin recognition (NOR) so far
in the literature. In this paper, we are inter-
ested in recognition of the origins of names
written in Chinese, which names can be of
three origins: Chinese, Japanese or English,
where ?English? is a rather broad category that
includes other West European and American
names written natively in Latin script.
Unlike previous work (Qu and Grefenstette,
2004; Li et al, 2007a; Li et al, 2007b),
where NOR was formulated with a genera-
tive model, we follow the approach of Zhang
et al (2008) and regard the NOR task as a
classification problem, using a discriminative
learning algorithm for classification. Further-
more, in the training data with names labelled
with their origin is rather limited, whereas
there is vast data from news articles that con-
tains many personal names without any labels
of their origins. In this research we propose
a method to harness the power of the unla-
belled noisy news data by bootstrapping the
learning process with labelled data and then
using the personal name context in the unla-
belled data to improve the NOR model. We
achieve that by using the maximum entropy
model and the expectation-maximisation train-
ing, and demonstrate that our method can sig-
nificantly improve the accuracy of NOR com-
pared to the baseline model trained only from
the labelled data.
The rest of the paper is organised as follows:
in Section 2 we review the previous research.
In Section 3 we present our approach, and in
Section 4 we describe our experimental setup,
the data used and the evaluation method. We
conclude in Section 5.
2 Related research
Most the research up to date focuses primar-
ily on recognition of origin of names written
in Latin script, called English NOR (ENOR),
although the same methods can be extended to
names in Chinese script (CNOR). We notice
that there are two informative clues that used
in previous work in ENOR. One is the lexi-
cal structure of a romanisation system, for ex-
ample, Hanyu Pinyin, Mandarin Wade-Giles,
Japanese Hepbrun or Korean Yale, each has
a finite set of syllable inventory (Li et al,
2007a). Another is the phonetic and phono-
tactic structure of a language, such as phonetic
composition, syllable structure. For example,
English has unique consonant clusters such
as ?str? and ?ks? which Chinese, Japanese
and Korean (CJK) do not have. Consider-
ing the NOR solutions by the use of these
two clues, we can roughly group them into
two categories: rule-based methods (for solu-
tions based on lexical structures) and statisti-
cal methods (for solutions based on phonotac-
tic structures).
Rule-based method Kuo et al (2007) pro-
posed using a rule-based method to recog-
nise different romanisation system for Chinese
only. The left-to-right longest match-based
lexical segmentation was used to parse a test
word. The romanisation system is confirmed
973
if it gives rise to a successful parse of the test
word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanisation sys-
tems that have a finite set of discriminative
syllable inventory, such as Pinyin for Chinese
Mandarin. For the general tasks of identifying
the language origin and romanisation system,
rule based approach sounds less attractive be-
cause not all languages have a finite set of dis-
criminative syllable inventory.
N-gram statistics methods
N-gram sum method Qu and Grefenstette
(2004) proposed a NOR identifier us-
ing a trigram language model (Cavnar
and Trenkle, 1994) to distinguish per-
sonal names of three language origins,
namely Chinese, Japanese and English.
In their work the training set includes
11,416 Chinese, 83,295 Japanese and
88,000 English name entries. How-
ever, the trigram is defined as the joint
probability p(cici?1ci?2) rather than the
commonly used conditional probability
p(ci|ci?1ci?2). Therefore it is basically
a substring unigram probability. For ori-
gin recognition of Japanese names, this
method works well with an accuracy of
92%. However, for English and Chinese,
the results are far behind with a reported
accuracy of 87% and 70% respectively.
N-gram perplexity method Li et al (2007a)
proposed a method of NOR using n-gram
character perplexity PPc to identify the
origin of names written in Latin script.
Using bigrams, the perplexity is defined
as
PPc = 2
?1
Nc
?Nc
i=1 log p(ci|ci?1)
whereNc is the total number of characters
in a given name, ci is the i-th character
in the name and p(ci|ci?1) is the bigram
probability learned from a list of names
of the same origin. Therefore, PPc can
be used to measure how well a new name
fits the model learned from the training
set of names. The origin is assigned ac-
cording to the model that gives the lowest
perplexity value. Li et al (2007a) demon-
strated that using PPc gives much better
performance than with the substring uni-
gram method.
Classification method Zhang et al (2008)
proposed using a discriminative classification
approach and extract features from the names.
They use Maximum Entropy (MaxEnt) model
and a number of features based on n-grams,
character positions, word length as well as
some rule-based phonetic features. They per-
formed both ENOR and CNOR and demon-
strated that their method indeed leads to better
performance in name origin recognition then
the n-gram statistics method. They attribute
that to the fact their model incorporates more
robust features than the n-gram statistics based
models.
In this paper we too follow the discriminat-
ing classification approach, but we add fea-
tures based on the context of a personal name.
These features require the original text with the
names to be available. Our approach closely
models the real-life situation when large cor-
pora of articles with personal names is read-
ily available in the Web, yet the origins of the
names are unknown.
3 Model and training methods
3.1 Maximum entropy model for NOR
The principle of maximum entropy is that
given a collection of facts we should choose
a model that is consistent with all the facts but
otherwise as uniform as possible (Berger et al,
1996). maximum entropy model (MaxEnt) is
known to easily combine diverse features and
974
has been used widely in natural language pro-
cessing research. Given an observation x the
probability of outcome label ci, i = 1 . . . N
given x is given by
p(ci|x) =
1
Z exp
?
?
n?
j=1
?jfj (x, ci)
?
? (1)
where N is the number of the outcome labels,
which is the number of name origins in our
case, n is the number of features, fj are the
feature functions and ?j are the model param-
eters. Each parameter corresponds to exactly
one feature and can be viewed as a ?weight?
for the corresponding feature. Z is the normal-
isation factor given by
Z =
N?
i=1
p(ci|x) (2)
In the problem at hand x is a personal name
and all the features are binary. The features,
also known as contextual predicates, are in the
form
fi(x, c) =
{
1 if c = ci and cp(x) = true
0 otherwise
(3)
where cp is the contextual predicate that maps
a pair (ci, x) to {true, false}.
In our experiments we use Zhang?s maxi-
mum entropy library1.
3.2 Initial training with labelled data and
n-gram features
For the initial training of MaxEnt model we
use labelled data: personal names of Chinese,
Japanese or English origin written in Chinese.
The origin of each name is known. Following
paper by Zhang et al (2008) and their findings
1http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
regarding the contribution value of each fea-
ture that they studied, we extract unigram, po-
sitional unigram and word length features. For
example, Chinese name ????? has the fol-
lowing features:
??? (?,0) (?,1) (?,2) 3
We restrict the n-gram features to unigram
only to avoid the data sparseness, because our
data contains a number of Chinese surnames
and given names, which have a length of one
or two characters.
3.3 Further training with unlabelled data
and context features
For further training of MaxEnt model we use
unlabelled data collected from news articles.
The name origin is not known but each per-
sonal name is in a context and is often sur-
rounded by names of places that may give a
hint about the personal name origin. For each
personal name we extract all names of places
in the same paragraph and use them as fea-
tures. If a place name is repeated many times
in the same paragraph we only include it once
in the feature list.
For example, paragraph containing passage
?The U.S. President Barack Obama ...? will
result in two personal names ?Barack? and
?Obama? having ?U.S.? as their context fea-
ture. Due to the diversity of place names we
also attempt to map the names of the places
into the country names. In this case, features
like ?U.S.?, ?USA?, ?America? are manually
substituted with ?USA?. In our experiments we
also try to narrow the place name extraction
to windows of different sizes surrounding the
personal name. The rationale here is that the
closer a place name is to the personal name,
the more likely it has a connection to the ori-
gin of the personal name.
In summary, our algorithm includes two
steps.
975
First, we use the boostrap data and n-gram,
positional n-gram and name length features to
do the initial training (the 0-th iteration) of
MaxEnt model with L-BFGS method (Byrd et
al., 1995). After that we use the model to as-
sign origin labels to names of the training set
of the unlabelled data.
Next, we use both the bootstrap data and
the training set of the unlabelled data, labelled
in the previous step, and add the context fea-
tures to the already used n-gram, positional n-
gram and name length features. Since there is
no context available for the bootstrap data, the
context features for it are missing, which can
be handled by the MaxEnt model. We perform
the Expectation-Maximisation (EM) iterations
by using the mixed data to train the i-th itera-
tion of the MaxEnt model, then use the model
to re-label the training set of the unlabelled
data and repeat the training of the model for
the (i + 1)-st iteration. We stop the iterations
when the ratio of patterns that change the ori-
gin labels becomes less than 0.01%.
4 Experiments
4.1 Corpora
The corpora consists of two datasets. One
dataset, called the ?bootstrap data?, is a set of
Chinese, Japanese and English names written
in Chinese following the respective translitera-
tion rules according to the name origins. The
names are a mixture of full names, first (given)
names and surnames. Table 1 shows the num-
ber of names of each origin. This is the la-
belled data; the origin of each name is known.
The data is used to start the MaxEnt model
training.
The second dataset, called the ?unlabelled
data?, is Chinese, Japanese and English per-
sonal names written in Chinese, which have
been extracted from the news articles col-
lected over 6 months from Xinhua news web-
site. The articles have been processed by an
Origin Number of names
Chinese 52,342
Japanese 26,171
English 26,171
Table 1: Number of names of each origin in
the bootstrap dataset.
automatic part-of-speech (POS) tagger, after
which personal names and names of places
have been manually identified (the latter for
extracting the context features). Normally the
first (given) name and surnames are identi-
fied as two separate personal names. The data
is split into a training set of 27,882 names
with unknown origin and a testing set of 1,476
names whose origin was manually assigned.
We split data in such a way that there is no
overlap between patterns in the training and
testing sets, although there may be overlap be-
tween names. For example, if a name may
be present in both training and testing sets but
in a different context, making the two names
two distinct patterns. The number of names
of each origin in the testing set is shown in
Table 2. As seen from the table, the number
Origin Number of names
Chinese 738
Japanese 369
English 422
Table 2: Number of names of each origin in
the testing dataset.
of Chinese names exceeds the number of En-
glish or Japanese names. This is an expected
consequence of using articles from a Chinese
news agency because many of the articles are
reporting on local affairs. We have manually
removed a number of Chinese name patterns
from the testing set, since the original percent-
age of Chinese names in the articles is about
83%.
976
4.2 Evaluation method
Following Zhang et al (2008) to make
our results comparable to theirs, we eval-
uate our system using precision Po, recall
Ro and F -score Fo for each origin o ?
{?Chinese ?? ?Japanese ?? ?English ??}. Let
the number of correctly recognised names of
a given origin o be ko, and the total number of
names recognised as being of origin o be mo,
while the actual number of names of origin o
be no. Then the precision, recall and F -score
are given as:
Po =
ko
mo
Ro =
ko
no
Fo =
2? Po ?Ro
Po +Ro
We also report the overall accuracy of the sys-
tem (or, rather the overall recall), which is the
ratio of the total number of correctly recog-
nised names to the number of all names:
Acc = kChinese + kJapanese + kEnglishnChinese + nJapanese + nEnglish
4.3 Results
After each iteration of our MaxEnt-based EM
algorithm, we record the number of patterns in
the training set that changed their origin labels,
as well as calculate the precision, recall and
F -score for each origin as well as the overall
accuracy. The results are reported in Tables 3
and 4, where for the sake of brevity the origin
subscripts are ?C?, ?J? and ?W? for Chinese,
Japanese and English name origin respectively.
Compared to the 0-th iteration there is an
significant improvement in accuracy, particu-
larly in recognition of Japanese names, which
are relatively infrequent compared to Chinese
and English ones in the unlabelled training
data. This clearly shows the effectiveness of
our proposed method.
Iteration PC PJ PW RC RJ RW0 0.887 0.724 0.857 0.823 0.911 0.761
1 0.914 0.736 0.875 0.823 0.968 0.775
2 0.910 0.736 0.874 0.823 0.968 0.767
3 0.914 0.737 0.874 0.824 0.973 0.767
4 0.913 0.742 0.875 0.825 0.968 0.778
Table 3: Results of running EM iterations,
original names of the places are kept.
Iteration Acc FC FJ FW
0 0.829 0.854 0.807 0.806
1 0.847 0.866 0.836 0.822
2 0.845 0.864 0.836 0.817
3 0.847 0.867 0.839 0.817
4 0.849 0.867 0.840 0.824
Table 4: Results of running EM iterations,
original names of the places are kept.
5 Conclusions
We propose extension of MaxEnt model for
NOR task by using two types of data for train-
ing: origin-labelled names alone and origin-
unlabelled names in their context surrounding.
We show how to apply a simple EM method to
make use of the contextual words as features,
and improve the NOR accuracy from 82.9%
to 84.9% overall, while for rare names such
as Japanese the effect of using unlabelled data
with context features is even greater.
The purpose of this research is to demon-
strate how the unlabelled data can be used. In
the future we hope to investigate the use of
other context features, as well as to study the
effect of data size on the NOR accuracy im-
provement.
The feature of names? places normally ex-
hibit great variation: one country name may be
spelled in many different ways, and often there
are names of cities etc that surround personal
names. We will explore to normalise names
of places by substituting each name with name
of the country where the place is in the future
work.
977
References
[Berger et al1996] Berger, A., Stephen A.
Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural lan-
guage processing. Computational Linguistics,
22(1):39?71.
[Byrd et al1995] Byrd, R. H., P. Lu, and J. Nocedal.
1995. A limited memory algorithm for bound
constrained optimization. SIAM Journal of Sci-
entific and Statistical Computing, 16(5):1190?
1208.
[Cavnar and Trenkle1994] Cavnar, William B. and
John M. Trenkle. 1994. Ngram based text cat-
egorization. In Proc. 3rd Annual Symposium on
Document Analysis and Information Retrieval,
pages 275?282.
[Knight and Graehl1998] Knight, Kevin and
Jonathan Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4).
[Kuo et al2007] Kuo, Jin-Shea, Haizhou Li, and
Ying-Kuei Yang. 2007. A phonetic similarity
model for automatic extraction of transliteration
pairs. ACM Transactions on Asian Language In-
formation Processing, 6(2).
[Li et al2007a] Li, Haizhou, Shuanhu Bai, and Jin-
Shea Kuo. 2007a. Transliteration. In Advances
in Chinese Spoken Language Processing, chap-
ter 15, pages 341?364. World Scientific.
[Li et al2007b] Li, Haizhou, Khe Chai Sim, Jin-
Shea Kuo, and Minghui Dong. 2007b. Semantic
transliteration of personal names. In Proc. 45th
Annual Meeting of the ACL, pages 120?127.
[Qu and Grefenstette2004] Qu, Yan and Gregory
Grefenstette. 2004. Finding ideographic rep-
resentations of Japanese names written in Latin
script via language identification and corpus val-
idation. In Proc. 42nd ACL Annual Meeting,
pages 183?190, Barcelona, Spain.
[Zhang et al2008] Zhang, Min, Chengjie Sun,
Haizhou Li, Aiti Aw, Chew Lim Tan, and Xi-
aolong Wang. 2008. Name origin recognition
using maximum entropy model and diverse fea-
tures. In Proc. 3rd Int?l Conf. NLP, pages 56?63.
978
Coling 2010: Poster Volume, pages 1444?1452,
Beijing, August 2010
Machine Transliteration: Leveraging on Third Languages 
Min Zhang          Xiangyu Duan           Vladimir Pervouchine         Haizhou Li 
Institute for Infocomm Research, A-STAR 
{mzhang, xduan, vpervouchine, hli}@i2r.a-star.edu.sg 
  
Abstract 
This paper presents two pivot strategies 
for statistical machine transliteration, 
namely system-based pivot strategy 
and model-based pivot strategy. Given 
two independent source-pivot and pi-
vot-target name pair corpora, the mod-
el-based strategy learns a direct source-
target transliteration model while the 
system-based strategy learns a source-
pivot model and a pivot-target model, 
respectively. Experimental results on 
benchmark data show that the system-
based pivot strategy is effective in re-
ducing the high resource requirement 
of training corpus for low-density lan-
guage pairs while the model-based pi-
vot strategy performs worse than the 
system-based one. 
1 Introduction 
Many technical terms and proper names, such 
as personal, location and organization names, 
are translated from one language into another 
language with approximate phonetic equiva-
lents. This phonetic translation using computer 
is referred to as machine transliteration. With 
the rapid growth of the Internet data and the 
dramatic changes in the user demographics 
especially among the non-English speaking 
parts of the world, machine transliteration play 
a crucial role in  most multilingual NLP, MT 
and CLIR applications (Hermjakob et al, 
2008; Mandl and Womser-Hacker, 2004). This 
is because proper names account for the major-
ity of OOV issues and translation lexicons 
(even derived from large parallel corpora) 
usually fail to provide good coverage over di-
verse, dynamically increasing names across 
languages.  
Much research effort has been done to ad-
dress the transliteration issue in the research 
community (Knight and Graehl, 1998; Wan 
and Verspoor, 1998; Kang and Choi, 2000; 
Meng et al, 2001; Al-Onaizan and Knight, 
2002; Gao et al, 2004; Klementiev and Roth, 
2006; Sproat, 2006; Zelenko and Aone, 2006; 
Li et al, 2004, 2009a, 2009b; Sherif and Kon-
drak, 2007; Bertoldi et al, 2008; Goldwasser 
and Roth, 2008). These previous work can be 
categorized into three classes, i.e., grapheme-
based, phoneme-based and hybrid methods. 
Grapheme-based method (Li et al, 2004) 
treats transliteration as a direct orthographic 
mapping process and only uses orthography-
related features while phoneme-based method 
(Knight and Graehl, 1998) treats transliteration 
as a phonetic mapping issue, converting source 
grapheme to source phoneme followed by a 
mapping from source phoneme to target pho-
neme/grapheme. Hybrid method in machine 
transliteration refers to the combination of sev-
eral different models or decoders via re-
ranking their outputs. The report of the first 
machine transliteration shared task (Li et al, 
2009a, 2009b) provides benchmarking data in 
diverse language pairs and systemically sum-
marizes and compares different transliteration 
methods and systems using the benchmarking 
data. 
Although promising results have been re-
ported, one of major issues is that the state-of-
the-art machine transliteration approaches rely 
heavily on significant source-target parallel 
name pair corpus to learn transliteration model. 
However, such corpora are not always availa-
1444
ble and the amounts of the current available 
corpora, even for language pairs with English 
involved, are far from enough for training, let-
ting alone many low-density language pairs. 
Indeed, transliteration corpora for most lan-
guage pairs without English involved are un-
available and usually rather expensive to ma-
nually construct. However, to our knowledge, 
almost no previous work touches this issue. 
To address the above issue, this paper 
presents two pivot language-based translitera-
tion strategies for low-density language pairs. 
The first one is system-based strategy (Khapra 
et al, 2010), which learns a source-pivot mod-
el from source-pivot data and a pivot-target 
model from pivot-target data, respectively. In 
decoding, it first transliterates a source name to 
N-best pivot names and then transliterates each 
pivot names to target names which are finally 
re-ranked using the combined two individual 
model scores. The second one is model-based 
strategy. It learns a direct source-target transli-
teration model from two independent1 source-
pivot and pivot-target name pair corpora, and 
then does direct source-target transliteration. 
We verify the proposed methods using the 
benchmarking data released by the 
NEWS20092 (Li et al, 2009a, 2009b). Expe-
riential results show that without relying on 
any source-target parallel data the system-
based pivot strategy performs quite well while 
the model-based strategy is less effective in 
capturing the phonetic equivalent information. 
The remainder of the paper is organized as 
follows. Section 2 introduces the baseline me-
thod. Section 3 discusses the two pivot lan-
guage-based transliteration strategies. Experi-
mental results are reported at section 4. Final-
ly, we conclude the paper in section 5. 
2 The Transliteration Model 
Our study is targeted to be language-
independent so that it can be applied to 
different language pairs without any adaptation 
effort. To achieve this goal, we use joint 
source-channel model (JSCM, also named as 
                                                 
1 Here ?independent? means the source-pivot and 
pivot-target data are not derived from the same 
English name source. 
2  http://www.acl-ijcnlp-2009.org/workshops/NEWS 
2009/pages/sharedtask.html 
n-gram transliteration model) (Li et la., 2004) 
under grapheme-based framework as our 
transliteration model due to its state-of-the-art 
performance by only using orthographical 
information (Li et al, 2009a). In addition, 
unlike other feature-based methods, such as 
CRFs (Lafferty et al, 2001), MaxEnt (Berger 
et al, 1996) or SVM (Vapnik, 1995), the 
JSCM model directly computes model 
probabilities using maximum likelihood 
estimation (Dempster et al, 1977). This 
property facilitates the implementation of the 
model-based strategy.  
JSCM directly models how both source and 
target names can be generated simultaneously.  
Given a source name S and a target name T, it 
estimates the joint probability of S and T as 
follows: 
 
                                 
                              
           
                         
         
                                
    
 
   
 
                                   
    
 
   
 
 
where    and    is an aligned transliteration 
unit3 pair, and n is the n-gram order.  
In implementation, we compare different 
unsupervised transliteration alignment me-
thods, including Giza++ (Och and Ney, 2003), 
the JSCM-based EM algorithm (Li et al, 
2004), the edit distance-based EM algorithm 
(Pervouchine et al, 2009) and Oh et al?s 
alignment tool (Oh et al, 2009). Based on the 
aligned transliteration corpus, we simply learn 
the transliteration model using maximum like-
lihood estimation (Dempster et al, 1977) and 
decode the transliteration result    
              using stack decoder 
(Schwartz and Chow, 1990). 
                                                 
3 Transliteration unit is language dependent. It can 
be a Chinese character, a sub-string of English 
words, a Korean Hangual or a Japanese Kanji or 
several Japanese Katakanas.  
1445
3 Pivot Transliteration Strategies 
3.1 System-based Strategy  
The system-based strategy is first proposed by  
Khapra et al (2010). They worked on system-
based strategy together with CRF and did ex-
tensively empirical studies on In-
dic/Slavic/Semetic languages and English. 
Given a source name S, a target name T and 
let Z(S, ?) be the n-best transliterations of S in 
one or more pivot language ? 4, the system-
based transliteration strategy under JSCM can 
be formalized as follows: 
 
                          
       
 
          
 
 
  
                                
 
 
 
In the above formula, we assume that there is 
only one pivot language used in the derivation 
from the first line to the second line. Under the 
pivot transliteration framework, we can further 
simplify the above formula by assuming that   
is independent of    when given  . The as-
sumption holds because the parallel name cor-
pus between S and T is not available under the 
pivot transliteration framework. The n-best 
transliterations in pivot language are expected 
to be able to carry enough information of the 
source name S for translating S to target name 
T. Then, we have: 
                     
 
 
 
                
             
    
          
 
 
Obviously we can train the two JSCMs of 
       and        using the two parallel cor-
pora of        and      , and train the lan-
guage model      using the monolingual cor-
pus of   . Following the nature of JSCM, Eq. 
                                                 
4 There can be multiple pivot languages used in the 
two strategies. However, without loss of generality, 
we only use one pivot language to facilitate our 
discussion. It is very easy to extend one pivot lan-
guage to multiple ones by considering all the pivot 
transliterations in all pivot languages.  
(1) directly models how the source name S and 
pivot name   and how the pivot name   and 
the target name   are generated simultaneous-
ly. Since   is considered twice in        and 
      , the duplicated impact of   is removed 
by dividing the model by     . 
Given the model as described at Eq. (1), the 
decoder can be formulized as: 
                 
 
       
           
 
  
             
    
 
      
 
If we consider multiple pivot languages, the 
modeling and decoding process are: 
       
    
                       
         
 
       
 
 
       
       
 
   
                       
         
       
  
 
3.2 Model-based Strategy 
Rather than combining the transitive translite-
ration results at system level, the model-based 
strategy aims to learn a direct model       by 
combining the two individual models of 
       and       , which are learned from 
the two parallel corpora of       and      , 
respectively. Now let us use bigram as an ex-
ample to illustrate how to learn the translitera-
tion model                   
 
   
         using the model-based strategy. 
 
                       
 
                  
           
        
where,  
 
                     
                           
                            
       
 
                             
       
                    
1446
The same as the system-based strategy, we 
can further simplify the above formula by as-
suming that   is independent of    when given 
 . Indeed,                            cannot 
be estimated directly from training corpus. 
Then we have:  
                   
                             
       
                    
                    
       
                    
                          
       
                    
                                        
where                   ,                    
and            can be directly learned from 
training corpus.              for Eq (3) can 
also be estimatedas follows.  
 
             
                     
      
 
 
In summary, eq. (1) formulizes the system-
based strategy and eq. (3), (4) and (5) formul-
ize the model-based strategy, where we can 
find that they share the same nature of generat-
ing source, pivot and target names simulta-
neously. The difference is that the model-based 
strategy operates at fine-grained transliteration 
unit level. 
3.3 Comparison with Previous Work  
Almost all previous work on machine translite-
ration focuses on direct transliteration or trans-
literation system combination. There is only 
one recent work (Khapra et al, 2010) touching 
this issue. They work on system-based strategy 
together with CRF. Compared with their work, 
this paper gives more formal definitions and 
derivations of system-based strategy from 
modeling and decoding viewpoints based on 
the JSCM model.  
The pivot-based strategies at both system 
and model levels have been explored in ma-
chine translation. Bertoldi et al (2008) studies 
two pivot approaches for phrase-based statis-
tical machine translation. One is at system lev-
el and one is to re-construct source-target data 
and alignments through pivot data. Cohn and 
Lapata (2007) explores how to utilize multilin-
gual parallel data (rather than pivot data) to 
improve translation performance. Wu and 
Wang (2007, 2009) extensively studies the 
model-level pivot approach and also explores 
how to leverage on rule-based translation re-
sults in pivot language to improve translation 
performance. Utiyama and Isahara (2007) 
compares different pivot approaches for 
phrase-based statistical machine translation. 
All of the previous work on machine transla-
tion works on phrase-based statistical machine 
translation. Therefore, their translation model 
is to calculate phrase-based conditional proba-
bilities at unigram level (        ) while our 
transliteration model is to calculate joint trans-
literation unit-based conditional probabilities 
at bigram level (                   ). 
4 Experimental Results 
4.1 Experimental Settings 
We use the NEWS 2009 benchmark data as 
our experimental data (Li et al, 2009). The 
NEWS 2009 data includes 8 language pairs, 
where we select English to Chinese/Japanese 
/Korean data (E-C/J/K) and based on which we 
further construct Chinese to Japanese/Korean 
and Japanese to Korean for our data.  
 
Language Pair Training Dev Test 
English-Chinese 31,961  2896 2896 
English-Japanese 23,225 1492 1489 
English-Korean 4,785 987 989 
Chinese-Japanese 12,417 75 77 
Chinese-Korean 2,148 32 31 
Japanese-Korean 6,035 65 69 
 
Table 1. Statistics on the data set 
 
Table 1 reports the statistics of all the expe-
rimental data. To have a more accurate evalua-
tion, the test sets have been cleaned up to make 
sure that there is no overlapping between any 
test set with any training set. In addition, the 
three E-C/J/K data are generated independently 
so that there is very small percentage of over-
1447
lapping between them. This can ensures the 
evaluation of the pivot study fair and accurate.  
We compare different alignment algorithms 
on the DEV set. Finally we use Pervouchine et 
al. (2009)?s alignment algorithm for Chinese-
English/Japanese/Korean and Oh et al 
(2009)?s alignment algorithm for English-
Korean and Li et al (2004)?s alignment algo-
rithm for English-Japanese and Japanese-
Korean. Given the aligned corpora, we directly 
learn each individual JSCM model (i.e., n-
gram transliteration model) using SRILM tool-
kits (Stolcke, 2002). We also use SRILM tool-
kits to do decoding. For the system-based 
strategy, we output top-20 pivot transliteration 
results.  
For the evaluation matrix, we mainly use 
top-1 accuracy (ACC) (Li et al, 2009a) to 
measure transliteration performance. For refer-
ence purpose, we also report the performance 
using all the other evaluation matrixes used in 
NEWS 2009 benchmarking (Li et al, 2009a), 
including F-score, MRR, MAP_ref, MAP_10 
and MAP_sys. It is reported that F-score has 
less correlation with other matrixes (Li et al, 
2009a). 
4.2 Experimental Results 
4.2.1 Results of Direct Transliteration 
Table 2 reports the performance of direct trans-
literation. The first three experiments (line 1-3) 
are part of the NEWS 2009 share tasks and the 
others are our additional experiments for our 
pivot studies.  
Comparison of the first three experimental 
results and the results reported at NEWS 2009 
shows that we achieve comparable perfor-
mance with their best-reported systems at the 
same conditions of using single system and 
orthographic features only. This indicates that 
our baseline represents the state-of-the-art per-
formance. In addition, we find that the back-
transliteration (line 4-6) consistently performs 
worse than its corresponding forward-
transliteration (line 1-3). This observation is 
consistent with what reported at previous work 
(Li et al, 2004; Zhang et al, 2004). The main 
reason is because English has much more 
transliteration units than foreign C/J/K lan-
guages. This makes the transliteration from 
English to C/J/K a many-to-few mapping issue 
and back-transliteration a few-to-many map-
ping issue. Therefore back-transliteration has 
more ambiguities and thus is more difficult. 
Overall, the lower six experiments (line 7-
12) shows worse performance than the upper 
six experiments which has English involved. 
This is mainly due to the less available training 
data for the language pairs without English 
involved. This observation motivates our study 
using pivot language for machine translitera-
tion. 
4.2.2 Results of System-based Strategy 
Table 3 reports three empirical studies of sys-
tem-based strategies: Japanese to Chinese 
through English, Chinese to Japanese through 
English and Chinese to Korean through Eng-
lish. Considering the fact that those language 
pairs with English involved have the most 
training data, we select English as pivot lan-
guage in the system-based study. Table 3 
clearly shows that:  
? The system-based pivot strategy is very 
effective, achieving significant perfor-
mance improvement over the direct 
transliteration by 0.09, 0.07 and 0.03 
point of ACC in the three language pairs, 
respectively; 
? Different from other pipeline methodol-
ogies, the system-based pivot strategy 
does not suffer heavily from the error 
propagation issue. Its ACC is significant-
ly better than the product of the ACCs of 
the two individual systems; 
? The combination of pivot system and di-
rect system slightly improves overall 
ACC. 
We then conduct more experiments to figure 
out the reasons. Our further statistics and anal-
ysis show the following reasons for the above 
observations: 
The pivot approach is able to use source-
pivot and pivot-target data whose amount is 
much more than that of the available direct 
source-target data.  
? The nature of transliteration is phonetic 
translation. Therefore a little bit variation 
in orthography may not hurt or even help 
to improve transliteration performance in 
some cases as long as the orthographical 
variations keep the phonetic equivalent 
1448
Language Pairs ACC F-Score MRR MAP_ref MAP_10 MAP_sys 
English  Chinese 0.678867 0.871497 0.771563 0.678867 0.252382 0.252382 
English  Japanese 0.482203 0.831983 0.594235 0.471766 0.201510 0.201510 
English  Korean 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
Chinese  English 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Japanese  English 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Korean  English 0.088505 0.494205 0.109249 0.088759 0.034380 0.034380 
Chinese  Japanese 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Japanese  Chinese 0.402597 0.714193 0.491595 0.402597 0.165581 0.165581 
Chinese  Korean 0.290323 0.571587 0.341129 0.290323 0.178652 0.178652 
Korean  Chinese 0.129032 0.280645 0.156042 0.129032 0.048163 0.048163 
Japanese  Korean 0.313433 0.678240 0.422862 0.313433 0.208310 0.208310 
Korean  Japanese 0.089286 0.321617 0.143948 0.091270 0.049992 0.049992 
 
Table 2. Performance of direct transliterations 
 
 
Language Pairs ACC   F-Score MRR MAP_ref MAP_10 MAP_sys 
Jap Eng Chi (Pivot) 0.493506 0.750711 0.617440 0.493506 0.195151 0.195151 
Jap Eng Chi (Pivot)  
+ Jap  Chi (Direct) 
0.506494 0.753958 0.622851 0.506494 0.196017 0.196017 
Jap  Chi (Direct) 0.402597 0.714193 0.491595 0.402597 0.165581 0.165581 
Jap  Eng (Direct) 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Eng  Chi (Direct) 0.678867 0.871497 0.771563 0.678867 0.252382 0.252382 
Chi Eng Jap (Pivot) 0.456140 0.777494 0.536591 0.414961 0.183222 0.183222 
Chi Eng Jap (Pivot) 
 + Chi  Jap (Direct) 
0.491228 0.801443 0.563297 0.450049 0.191742 0.191742 
Chi  Jap (Direct) 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Chi  Eng (Direct) 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Eng  Jap (Direct) 0.482203 0.831983 0.594235 0.471766 0.201510 0.201510 
Chi Eng Kor (Pivot) 0.322581 0.628146 0.432642 0.322581 0.175822 0.175822 
Chi Eng Kor (Pivot)   
+ Chi  Kor (Direct) 
0.331631 0.632967 0.439143 0.334222 0.176543 0.176543 
Chi  Kor (Direct) 0.290323 0.571587 0.341129 0.290323 0.178652 0.178652 
Chi  Eng (Direct) 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Eng  Kor (Direct) 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
 
Table 3. Performance comparison of system-based strategy on Jap (Japanese) to Chi (Chinese) and 
Chi (Chinese) to Jap (Japanese)/Kor (Korean) through Eng (English) as pivot language, 
where ??(Pivot) + ?(Direct)? means that for the same language pair we merge and re-
rank the pivot transliteration and direct  transliteration results 
 
information. Indeed, given one source 
English names, there are usually more 
than one correct transliteration references 
in Japanese/Korean. This case also hap-
pens to English to Chinese although not 
so heavy as in English to Japa-
nese/Korean. 
 
1449
Language Pairs ACC   F-Score MRR MAP_ref MAP_10 MAP_sys 
Chi Eng Jap  
(Model-based Pivot: O) 
0.087719 0.538454 0.117446 0.085770 0.040645 0.040645 
Chi Eng Jap  
(Model-based Pivot: R) 
0.210526 0.746497 0.381210 0.201267 0.156106 0.156106 
Chi Eng Jap  
(System-based Pivot) 
0.456140 0.777494 0.536591 0.414961 0.183222 0.183222 
Chi  Jap  (Direct) 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Jap Chi Eng  
(Model-based Pivot) 
0.148504 0.724623 0.224253 0.141791 0.088966 0.088966 
Jap Chi Eng 
(System-based Pivot) 
0.201581 0.741627 0.266507 0.191926 0.098024 0.134730 
Jap  Eng (Direct) 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Eng Jap Kor  
(Model-based Pivot) 
0.206269 0.547732 0.300641 0.206269 0.145882 0.145882 
Eng Jap Kor 
(System-based Pivot) 
0.315470 0.629640 0.404769 0.315723 0.167587 0.225892 
Eng  Kor (Direct) 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
 
Table 4. Performance of Model-based Pivot Transliteration Strategy 
 
? The N-best accuracy of machine transli-
teration (of both to and from English) is 
very high5. It means that in most cases 
the correct transliteration in pivot lan-
guage can be found in the top-20 results 
and the other 19 results hold the similar 
pronunciations with the correct one, 
which can serve as alternative ?quasi-
correct? inputs to the second stage trans-
literations and thus largely improve the 
overall accuracy.  
 
The above analysis holds when using Eng-
lish as pivot language. Now let us see the case 
of using non-English as pivot language. Table 
4 reports two system-based strategies using 
Chinese and Japanese as pivot languages, 
                                                 
5  Both our studies and previous work (Li et al, 
2004; Zhang et al, 2004) shows that the top-20 
accuracy from English to J/K is more than 0.85 and 
more than 0.95 in English-Chinese case. The top-20 
accuracy is a little worse from C/J/K to English, but 
still more than 0.7. 
where we can find that the performance of two 
system-based strategies is worse than that of 
the direct transliterations. The main reason is 
because that the direct transliteration utilizes 
much more training data than the pivot ap-
proach. However, the good thing is that the 
system-based pivot strategy using non-English 
as pivot language still does not suffer from 
error propagation issue. Its ACC is significant-
ly better than the product of the ACCs of the 
two individual systems. 
4.2.3 Results of Model-based Strategy 
Table 4 reports the performance of model-
based strategy. It clearly shows that the model-
based strategy is less effective and performs 
much worse than both the system-based strate-
gy and direct transliteration.  
While the model-based strategy works well 
at phrase-based statistical machine translation 
(Wu and Wang, 2007, 2009), it does not work 
at machine transliteration. To investigate the 
reasons, we conduct many additional experi-
ments and do statistics on the model and 
1450
aligned training data6. From this in-depth anal-
ysis, we find that main reason is due to the fact 
that the model-based strategy introduces too 
many entries (ambiguities) to the final transli-
teration model. For example, in the 
Jap Chi Eng experiment, the unigram and 
bigram entries of the transliteration model ob-
tained by the model-based strategy are 45 and 
6.6 times larger than that of the transliteration 
model trained directly from parallel data.  This 
is not surprising. Given a transliteration unit in 
pivot language, it can generate     source-
to-target transliteration unit mappings (unigram 
entry of the model), where  is the number of 
the source units that can be mapped to the pi-
vot unit and   is the number of the target units 
that can be mapped from the pivot unit. 
Besides the ambiguities introduced by the 
large amount of entries in the model, another 
reason that leads to the worse performance of 
model-based strategy is the size inconsistence 
of transliteration unit of pivot language. As 
shown at Table 4, we conduct three experi-
ments. In the first experiment (Chi Eng Jap), 
we use English as pivot language. We find that 
the English transliteration unit size in 
Chi Eng model is much larger than that in 
Eng Jap model. This is because from phonetic 
viewpoint, in Chi Eng model, the English unit 
is at syllable level (corresponding one Chinese 
character) while in Eng Jap model, the English 
unit is at sub-syllable level (consonant or vowel 
or syllable, corresponding one Japanese Kata-
kana). This is the reason why we conduct two 
model-based experiments for Chi Eng Jap. 
One is based on the original alignments (Mod-
el-based Pivot: O) and one is based on the re-
constructed alignments 7  (Model-based Pivot: 
R). Experimental results clearly show that the 
reconstruction improves performance signifi-
cantly. In the second and third experiments 
(Jap Chi Eng, Eng Jap Kor), we use Chi-
nese and Japanese as pivot languages. Therefore 
we do not need to re-construct transliteration 
                                                 
6 However, due to space limitation, we are not al-
lowed to report the details of those experiments.  
7Based on the English transliteration units obtained 
from Chi Eng, we reconstruct the English transli-
teration units and alignments in Eng Jap by merg-
ing the adjacent units of both English and Japanese 
to syllable level. 
units and alignments. However, the perfor-
mance is still very poor. This is due to the first 
reason of the large amount of ambiguities. 
The above two reasons (ambiguities and 
transliteration unit inconsistence) are mixed 
together, leading to the worse performance of 
the model-based strategy. We believe that the 
fundamental reason is because the pivot transli-
teration unit is too small to be able to convey 
enough phonetic information of source lan-
guage to target language and thus generates too 
many alignments and ambiguities. 
5 Conclusions 
A big challenge to statistical-based machine 
transliteration is the lack of the training data, 
esp. to those language pairs without English 
involved. To address this issue, inspired by the 
research in the SMT research community, we 
study two pivot transliteration methods. One is 
at system level while another one is at model 
level. We conduct extensive experiments using 
NEW 2009 benchmarking data. Experimental 
results show that system-based method is very 
effective in capturing the phonetic information 
of source language. It not only avoids success-
fully the error propagation issue, but also fur-
ther boosts the transliteration performance by 
generating more alternative pivot results as the 
inputs of the second stage. In contrast, the 
model-based method in its current form fails to 
convey enough phonetic information from 
source language to target language.  
For the future work, we plan to study how to 
improve the model-based strategy by pruning 
out the so-called ?bad? transliteration unit 
pairs and re-sampling the so-called ?good? unit 
pairs for better model parameters. In addition, 
we also would like to explore other pivot-
based transliteration methods, such as con-
structing source-target training data through 
pivot languages. 
References 
Yaser Al-Onaizan and Kevin Knight. 2002. Trans-
lating named entities using monolingual and bi-
lingual resources. ACL-02 
Adam L. Berger, Stephen A. Della Pietra and 
Vincent J. Della Pietra. 1996. A Maximum En-
tropy Approach to Natural Language Processing. 
Computational Linguistics. 22(1):39?71 
1451
N. Bertoldi, M. Barbaiant, M. Federico and R. Cat-
toni. 2008. Phrase-based Statistical Machine 
Translation with Pivot Languages. IWSLT-08 
Trevor Cohn and Mirella Lapata. 2007. Machine 
Translation by Triangulation: Making Effective 
Use of Multi-Parallel Corpora. ACL-07 
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. IJCNLP-08 
Wei Gao, Kam-Fai Wong and Wai Lam. 2004. 
Phoneme-based Transliteration of Foreign 
Names for OOV Problems. IJCLNP-04  
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. EMNLP-08 
A.P. Dempster, N.M. Laird, D.B.Rubin.1977. Max-
imum likelihood from incomplete data via the 
EM algorithm, J. Roy. Stat. Soc., Ser. B. Vol. 39 
Ulf Hermjakob, K. Knight and Hal Daum e?. 2008. 
Name translation in statistical machine transla-
tion: Learning when to transliterate. ACL-08 
John Lafferty, Fernando Pereira, Andrew McCal-
lum. 2001. Conditional random fields: Probabil-
istic models for segmenting and labeling se-
quence data. ICML-01  
B.J. Kang and Key-Sun Choi. 2000. Automatic 
Transliteration and Back-transliteration by De-
cision Tree Learning. LREC-00 
Mitesh Khapra, Kumaran A and Pushpak Bhatta-
charyya. 2010. Everybody loves a rich cousin: 
An empirical study of transliteration through 
bridge languages. NAACL-HLT-10 
Alexandre Klementiev and Dan Roth. 2006. Weakly 
supervised named entity transliteration and dis-
covery from multilingual comparable corpora. 
COLING-ACL-06 
K. Knight and J. Graehl. 1998. Machine Translite-
ration, Computational Linguistics, Vol 24, No. 4 
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical 
phrase-based translation. HLT-NAACL-03 
J. Lafferty, A. McCallum and F. Pereira. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. 
ICML-01 
Haizhou Li, A Kumaran, Vladimir Pervouchine and 
Min Zhang. 2009a. Report of NEWS 2009 Ma-
chine Transliteration Shared Task. IJCNLP-
ACL-09 Workshop: NEWS-09 
Haizhou Li, A Kumaran, Min Zhang and Vladimir 
Pervouchine. 2009b. Whitepaper of NEWS 2009 
Machine Transliteration Shared Task. IJCNLP-
ACL-09 Workshop: NEWS-09 
Haizhou Li, Ming Zhang and Jian Su. 2004. A Joint 
Source-Channel Model for Machine Translitera-
tion. ACL-04 
Thomas Mandl and Christa Womser-Hacker. 2004. 
How do Named Entities Contribute to Retrieval 
Effectiveness? CLEF-04 
Helen M. Meng, Wai-Kit Lo, Berlin Chen and Ka-
ren Tang. 2001. Generate Phonetic Cognates to 
Handle Name Entities in English-Chinese cross-
language spoken document retrieval. ASRU-01 
Jong-Hoon Oh, Kiyotaka Uchimoto, and k. Torisa-
wa. 2009. Machine Transliteration with Target-
Language Grapheme and Phoneme: Multi-
Engine Transliteration Approach. NEWS 2009 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Align-
ment Models. Computational Linguistics 29(1) 
V. Pervouchine, H. Li and B. Lin. 2009. Translite-
ration Alignment. ACL-IJCNLP-09 
R. Schwartz and Y. L. Chow. 1990. The N-best 
algorithm: An efficient and exact procedure for 
finding the N most likely sentence hypothesis, 
ICASSP-90 
Tarek Sherif and Grzegorz Kondrak. 2007. Sub-
string-based transliteration. ACL-07 
Richard Sproat, Tao Tao and ChengXiang Zhai. 
2006. Named entity transliteration with compa-
rable corpora. COLING-ACL-06 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02 
Masao Utiyama and Hitoshi Isahara. 2007. A Com-
parison of Pivot Methods for Phrase-based Sta-
tistical Machine Translation. NAACL-HLT-07 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer 
Stephen Wan and Cornelia Maria Verspoor. 1998. 
Automatic English-Chinese name transliteration 
for development of multilingual resources. COL-
ING-ACL-98 
Hua Wu and Haifeng Wang. 2007. Pivot Language 
Approach for Phrase-based Statistical Machine 
Translation. ACL-07 
Hua Wu and Haifeng Wang. 2009. Revisiting Pivot 
Language Approach for Machine Translation. 
ACL-09 
Dmitry Zelenko and Chinatsu Aone. 2006. Discri-
minative methods for transliteration. EMNLP-06 
Min Zhang, Haizhou Li and Jian Su. 2004. Direct 
Orthographical Mapping for machine translite-
ration. COLING-04 
1452
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 1?11,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Report of NEWS 2010 Transliteration Generation Shared Task
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the Translitera-
tion Generation Shared Task conducted as
a part of the Named Entities Workshop
(NEWS 2010), an ACL 2010 workshop.
The shared task features machine translit-
eration of proper names from English to
9 languages and from 3 languages to En-
glish. In total, 12 tasks are provided. 7
teams from 5 different countries partici-
pated in the evaluations. Finally, 33 stan-
dard and 8 non-standard runs are submit-
ted, where diverse transliteration method-
ologies are explored and reported on the
evaluation data. We report the results with
4 performance metrics. We believe that the
shared task has successfully achieved its
objective by providing a common bench-
marking platform for the research commu-
nity to evaluate the state-of-the-art tech-
nologies that benefit the future research
and development.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They are important in Cross
Lingual Information Retrieval (CLIR) and Ma-
chine Translation (MT) as the system performance
has been shown to positively correlate with the
correct conversion of names between the lan-
guages in several studies (Demner-Fushman and
Oard, 2002; Mandl and Womser-Hacker, 2005;
Hermjakob et al, 2008; Udupa et al, 2009). The
traditional source for name equivalence, the bilin-
gual dictionaries ? whether handcrafted or sta-
tistical ? offer only limited support because new
names always emerge.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. Much research effort has been made to ad-
dress the transliteration issue in the research com-
munity (Knight and Graehl, 1998; Meng et al,
2001; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Al-Onaizan and Knight,
2002; Goldwasser and Roth, 2008; Goldberg and
Elhadad, 2008; Klementiev and Roth, 2006; Oh
and Choi, 2002; Virga and Khudanpur, 2003; Wan
and Verspoor, 1998; Kang and Choi, 2000; Gao
et al, 2004; Zelenko and Aone, 2006; Li et al,
2009b; Li et al, 2009a). These previous work
fall into three categories, i.e., grapheme-based,
phoneme-based and hybrid methods. Grapheme-
based method (Li et al, 2004) treats translitera-
tion as a direct orthographic mapping and only
uses orthography-related features while phoneme-
based method (Knight and Graehl, 1998) makes
use of phonetic correspondence to generate the
transliteration. Hybrid method refers to the com-
bination of several different models or knowledge
sources to support the transliteration generation.
The first machine transliteration shared task (Li
et al, 2009b; Li et al, 2009a) was held in NEWS
2009 at ACL-IJCNLP 2009. It was the first time
to provide common benchmarking data in diverse
language pairs for evaluation of state-of-the-art
techniques. NEWS 2010 is a continued effort of
NEWS 2009. It builds on the foundations estab-
lished in the first transliteration shared task and
extends the scope to include new language pairs.
The rest of the report is organised as follows.
Section 2 outlines the machine transliteration task
and the corpora used and Section 3 discusses the
metrics chosen for evaluation, along with the ratio-
nale for choosing them. Sections 4 and 5 present
the participation in the shared task and the results
with their analysis, respectively. Section 6 con-
cludes the report.
1
2 Transliteration Shared Task
In this section, we outline the definition and the
description of the shared task.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target lan-
guage, but also at its practical utility for down-
stream applications, such as CLIR and MT. There-
fore, we adopted the same definition of translit-
eration as during the NEWS 2009 workshop (Li
et al, 2009a) to narrow down ?transliteration? to
three specific requirements for the task, as fol-
lows:?Transliteration is the conversion of a given
name in the source language (a text string in the
source writing system or orthography) to a name
in the target language (another text string in the
target writing system or orthography), such that
the target language name is: (i) phonemically
equivalent to the source name (ii) conforms to the
phonology of the target language and (iii) matches
the user intuition of the equivalent of the source
language name in the target language, consider-
ing the culture and orthographic character usage
in the target language.?
In NEWS 2010, we introduce three
back-transliteration tasks. We define back-
transliteration as a process of restoring translit-
erated words to their original languages. For
example, NEWS 2010 offers the tasks to convert
western names written in Chinese and Thai into
their original English spellings, or romanized
Japanese names into their original Kanji writings.
2.2 Shared Task Description
Following the tradition in NEWS 2009, the shared
task at NEWS 2010 is specified as development of
machine transliteration systems in one or more of
the specified language pairs. Each language pair
of the shared task consists of a source and a target
language, implicitly specifying the transliteration
direction. Training and development data in each
of the language pairs have been made available to
all registered participants for developing a translit-
eration system for that specific language pair using
any approach that they find appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 1,000 and 3,000
source names (approximately 10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair every participant is re-
quired to submit at least one run (designated as a
?standard? run) that uses only the data provided by
the NEWS workshop organisers in that language
pair, and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more ?standard? runs, up to 4 in total. If more than
one ?standard? runs is submitted, it is required to
name one of them as a ?primary? run, which is
used to compare results across different systems.
In addition, up to 4 ?non-standard? runs could be
submitted for every language pair using either data
beyond that provided by the shared task organisers
or linguistic resources in a specific language, or
both. This essentially may enable any participant
to demonstrate the limits of performance of their
system in a given language pair.
The shared task timelines provide adequate time
for development, testing (approximately 1 month
after the release of the training data) and the final
result submission (7 days after the release of the
test data).
2.3 Shared Task Corpora
We considered two specific constraints in select-
ing languages for the shared task: language diver-
sity and data availability. To make the shared task
interesting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
2
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 12 tasks shown in Table 1 (Li et al,
2004; Kumaran and Kellner, 2007; MSRI, 2009;
CJKI, 2010).
NEWS 2010 leverages on the success of NEWS
2009 by utilizing the training and dev data of
NEWS 2009 as the training data of NEWS 2010
and the test data of NEWS 2009 as the dev data
of NEWS 2010. NEWS 2010 provides totally new
test data across all 12 tasks for evaluation. In ad-
dition to the 7 tasks inherited from NEWS 2009,
NEWS 2010 is enhanced with 5 new tasks, three
new languages (Arabic, Bangla and Thai) and two
back-transliteration (Chinese to English and Thai
to English).
The names given in the training sets for Chi-
nese, Japanese, Korean and Thai languages are
Western names and their respective translitera-
tions; the Japanese Name (in English)? Japanese
Kanji data set consists only of native Japanese
names; the Arabic data set consists only of native
Arabic names. The Indic data set (Hindi, Tamil,
Kannada, Bangla) consists of a mix of Indian and
Western names.
For all of the tasks chosen, we have been
able to procure paired names data between the
source and the target scripts and were able to
make them available to the participants. For
some language pairs, such as English-Chinese and
English-Thai, there are both transliteration and
back-transliteration tasks. Most of the task are just
one-way transliteration, although Indian data sets
contained mixture of names of both Indian and
Western origins. The language of origin of the
names for each task is indicated in the first column
of Table 1.
Finally, it should be noted here that the corpora
procured and released for NEWS 2010 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit results
of up to four standard and four non-standard runs.
One standard run must be named as the primary
submission and is used for the performance sum-
mary. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 4 evaluation
metrics capturing different aspects of translitera-
tion performance. We have dropped two MAP
metrics used in NEWS 2009 because they don?t
offer additional information to MAPref . Since a
name may have multiple correct transliterations,
all these alternatives are treated equally in the eval-
uation, that is, any of these alternatives is consid-
ered as a correct transliteration, and all candidates
matching any of the reference transliterations are
accepted as correct ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC = 1
N
N
?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) = 1
2
(|c|+ |r| ? ED(c, r)) (2)
3
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 32K 6K 2K EnCh
Western Chinese English Institute for Infocomm Research 25K 5K 2K ChEn
Western English Korean Hangul CJK Institute 5K 2K 2K EnKo
Western English Japanese Katakana CJK Institute 23K 3K 3K EnJa
Japanese English Japanese Kanji CJK Institute 7K 3K 3K JnJk
Arabic Arabic English CJK Institute 25K 2.5K 2.5K ArAe
Mixed English Hindi Microsoft Research India 10K 2K 2K EnHi
Mixed English Tamil Microsoft Research India 8K 2K 2K EnTa
Mixed English Kannada Microsoft Research India 8K 2K 2K EnKa
Mixed English Bangla Microsoft Research India 10K 2K 2K EnBa
Western English Thai NECTEC 26K 2K 2K EnTh
Western Thai English NECTEC 24K 2K 2K ThEn
Table 1: Source and target languages for the shared task on transliteration.
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR = 1
N
N
?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N
?
i
1
ni
( ni
?
k=1
num(i, k)
)
(9)
4 Participation in Shared Task
7 teams from 5 countries and regions (Canada,
Hong Kong, India, Japan, Thailand) submitted
their transliteration results.
Two teams have participated in all or almost all
tasks while others participated in 1 to 4 tasks. Each
language pair has attracted on average around 4
teams. The details are shown in Table 3.
Teams are required to submit at least one stan-
dard run for every task they participated in. In
total, we receive 33 standard and 8. Table 2
shows the number of standard and non-standard
runs submitted for each task. It is clear that the
most ?popular? task is transliteration from English
to Hindi attempted by 5 participants. The next
most popular are other Indic scripts (Tamil, Kan-
nada, Bangla) and Thai, attempted by 3 partici-
pants. This is somewhat different from NEWS
2009, where the two most popular tasks were En-
glish to Hindi and English to Chinese translitera-
tion.
4
English to
Chinese
Chinese to
English
English to
Thai
Thai to En-
glish
English to
Hindi
English to
Tamil
Language pair code EnCh ChEn EnTh ThEn EnHi EnTa
Standard runs 5 2 2 2 7 3
Non-standard runs 0 0 1 1 2 1
English to
Kannada
English to
Japanese
Katakana
English
to Korean
Hangul
English to
Japanese
Kanji
Arabic to
English
English to
Bengali
(Bangla)
Language pair code EnKa EnJa EnKo JnJk ArAe EnBa
Standard runs 3 2 1 1 2 3
Non-standard runs 1 0 0 0 0 2
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
Team
ID
Organisation EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArAe EnBa
1? IIT, Bombay x
2 University of Alberta x x x x x x x x x x x x
3 x
4 City University of
Hong Kong
x x
5 NICT x x x x x x x x
6 x x
7 Jadavpur University x x x x
Table 3: Participation of teams in different tasks. ?Participation without a system paper.
5 Task Results and Analysis
5.1 Standard runs
All the results are presented numerically in Ta-
bles 4?15, for all evaluation metrics. These are the
official evaluation results published for this edition
of the transliteration shared task.
Among the four submitted system papers1,
Song et al (2010) and Finch and Sumita (2010)
adopt the approach of phrase-based statistical ma-
chine transliteration (Finch and Sumita, 2008),
an approach initially developed for machine trans-
lation (Koehn et al, 2003) while Das et al
(2010) adopts the approach of Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001). Jiampo-
jamarn et al (2010) further develop DirectTL ap-
proach presented at the previous NEWS work-
shop (Jiampojamarn et al, 2009), achieving very
good performance in the NEWS 2010.
An example of a completely language-
1To maintain anonymity, papers of the teams that submit-
ted anonymous results are not cited in this report.
independent approach is (Finch and Sumita,
2010). Other participants used language-
independent approach but added language-
specific pre- or post-processing (Jiampojamarn
et al, 2010; Das et al, 2010; Song et al, 2010),
including name origin recognition for English to
Hindi task (Jiampojamarn et al, 2010).
Combination of different models via re-ranking
of their outputs has been used in most of the sys-
tems (Das et al, 2010; Song et al, 2010; Finch and
Sumita, 2010). In fact, one system (Song et al,
2010) is mostly devoted to re-ranking of the sys-
tem output to achieve significant improvement of
the ACC (accuracy in top-1) results compared to
the same system in NEWS 2009 workshop (Song,
2009).
Compared the same seven tasks among the
NEWS 2009 and the NEWS 2010 (almost same
training sets, but different test sets), we can see
that the performance in the NEWS 2010 drops ex-
cept the English to Korean task. This could be due
to the fact that NEWS 2010 introduces a entirely
5
new test set, which come from different sources
than the train and dev sets, while NEWS 2009
have all train, dev and test sets from the same
sources.
As far as back-transliteration is concerned, we
can see that English-to-Thai and Thai-to-English
have the similar performance. However, Chinese-
to-English back transliteration performs much
worse than English-to-Chinese forward transliter-
ation. This could be due to the fact that Thai
and English are alphabet languages in nature while
Chinese is not. As a result, Chinese have much
fewer transliteration units than English and Thai.
In other words, Chinese to English translitera-
tion is a one-to-many mapping while English-to-
Chinese is a many-to-one mapping. The later one
has fewer mapping ambiguities.
5.2 Non-standard runs
For the non-standard runs there exist no restric-
tions on the use of data or other linguistic re-
sources. The purpose of non-standard runs is to
see how best personal name transliteration can be,
for a given language pair. In NEWS 2010, the ap-
proaches used in non-standard runs are typical and
may be summarised as follows:
? Pronunciation dictionaries to convert words
to their phonetic transcription (Jiampojamarn
et al, 2010).
? Web search. First, transliteration candidates
are generated. A Web search is then per-
formed to re-affirm or re-rank the candi-
dacy (Das et al, 2010).
Unfortunately, these additional knowledge used
in the non-standard runs is not helpful since all
non-standard runs perform worse than their cor-
responding standard runs. This would be an inter-
esting issue to look into.
6 Conclusions and Future Plans
The Transliteration Generation Shared Task in
NEWS 2010 shows that the community has a
continued interest in this area. This report sum-
marizes the results of the shared task. Again,
we are pleased to report a comprehensive cal-
ibration and baselining of machine translitera-
tion approaches as most state-of-the-art machine
transliteration techniques are represented in the
shared task. The most popular techniques such
as Phrase-Based Machine Transliteration (Koehn
et al, 2003), system combination and re-ranking,
are inspired by recent progress in statistical ma-
chine translation. As the standard runs are lim-
ited by the use of corpus, most of the systems are
implemented under the direct orthographic map-
ping (DOM) framework (Li et al, 2004). While
the standard runs allow us to conduct meaningful
comparison across different algorithms, we recog-
nise that the non-standard runs open up more op-
portunities for exploiting larger linguistic corpora.
It is also noted that two systems have reported
significant performance improvement over their
NEWS 2009 systems.
NEWS 2010 Shared Task represents a success-
ful debut of a community effort in driving machine
transliteration techniques forward. We would like
to continue this event in the future conference to
promote the machine transliteration research and
development.
Acknowledgements
The organisers of the NEWS 2010 Shared Task
would like to thank the Institute for Infocomm
Research (Singapore), Microsoft Research India,
CJK Institute (Japan) and National Electronics and
Computer Technology Center (Thailand) for pro-
viding the corpora and technical support. Without
those, the Shared Task would not be possible. We
thank those participants who identified errors in
the data and sent us the errata. We also want to
thank the members of programme committee for
their invaluable comments that improve the qual-
ity of the shared task papers. Finally, we wish to
thank all the participants for their active participa-
tion that have made this first machine translitera-
tion shared task a comprehensive one.
6
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc.
ACL-2002Workshop: Computational Apporaches to
Semitic Languages, Philadelphia, PA, USA.
CJKI. 2010. CJK Institute. http://www.cjk.org/.
Amitava Das, Tanik Saikh, Tapabrata Mondal, Asif Ek-
bal, and Sivaji Bandyopadhyay. 2010. English to
Indian languages machine transliteration system at
NEWS 2010. In Proc. ACL Named Entities Work-
shop Shared Task.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Andrew Finch and Eiichiro Sumita. 2010. Transliter-
ation using a phrase-based statistical machine trans-
lation system to re-score the output of a joint multi-
gram model. In Proc. ACL Named Entities Work-
shop Shared Task.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381,
Sanya, Hainan, China.
Yoav Goldberg andMichael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane
Bergsma, Aditya Bhargava, Qing Dou, Mi-Young
Kim, and Grzegorz Kondrak. 2010. Translitera-
tion generation and mining with limited training re-
sources. In Proc. ACL Named Entities Workshop
Shared Task.
Byung-Ju Kang and Key-Sun Choi. 2000.
English-Korean automatic transliteration/back-
transliteration system and character alignment. In
Proc. ACL, pages 17?18, Hong Kong.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Int?l.
Conf. Machine Learning, pages 282?289.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report of NEWS 2009 machine
transliteration shared task. In Proc. Named Entities
Workshop at ACL 2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. ACL-IJCNLP 2009 Named
Entities Workshop ? Shared Task on Translitera-
tion. In Proc. Named Entities Workshop at ACL
2009.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002,
Taipei, Taiwan.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
7
Yan Song, Chunyu Kit, and Hai Zhao. 2010. Rerank-
ing with multiple features for better transliteration.
In Proc. ACL Named Entities Workshop Shared
Task.
Yan Song. 2009. Name entities transliteration via
improved statistical translation on character-level
chunks. In Proc. ACL/IJCNLP Named Entities
Workshop Shared Task.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER, Sapporo, Japan.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
8
Team ID ACC F -score MRR MAPref Organisation
Primary runs
4 0.477333 0.740494 0.506209 0.455491 City University of Hong Kong
2 0.363333 0.707435 0.430168 0.347701 University of Alberta
Non-primary standard runs
2 0.362667 0.704284 0.428854 0.347500 University of Alberta
2 0.360333 0.706765 0.428990 0.345215 University of Alberta
2 0.357000 0.702902 0.419415 0.341567 University of Alberta
Table 4: Runs submitted for English to Chinese task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
4 0.226766 0.749237 0.268557 0.226090 City University of Hong Kong
2 0.137209 0.740364 0.197665 0.136702 University of Alberta
Table 5: Runs submitted for Chinese to English back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.391000 0.872526 0.505264 0.391000 NICT
2 0.377500 0.866254 0.467328 0.377500 University of Alberta
Non-standard runs
6 0.247000 0.842063 0.366959 0.247000
Table 6: Runs submitted for English to Thai task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.396690 0.872642 0.524511 0.396690 NICT
2 0.352056 0.861207 0.450472 0.352056 University of Alberta
Non-standard runs
6 0.092778 0.706995 0.131779 0.092778
Table 7: Runs submitted for Thai to English back-transliteration task.
9
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.456456 0.884199 0.559212 0.456456 University of Alberta
5 0.445445 0.883841 0.574195 0.445445 NICT
3 0.381381 0.860320 0.403172 0.381381
1 0.158158 0.810309 0.231594 0.158158 IIT, Bombay
7 0.150150 0.714490 0.307674 0.150150 Jadavpur University
Non-primary standard runs
2 0.456456 0.885122 0.558203 0.456456 University of Alberta
1 0.142142 0.799092 0.205945 0.142142 IIT, Bombay
Non-standard runs
7 0.254254 0.751766 0.369072 0.254254 Jadavpur University
7 0.170170 0.738777 0.314335 0.170170 Jadavpur University
Table 8: Runs submitted for English to Hindi task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.390000 0.890692 0.515298 0.390000 University of Alberta
5 0.390000 0.886560 0.522088 0.390000 NICT
7 0.013000 0.562917 0.121233 0.013000 Jadavpur University
Non-standard runs
7 0.082000 0.759856 0.142317 0.082000 Jadavpur University
Table 9: Runs submitted for English to Tamil task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.371000 0.871131 0.506010 0.371000 NICT
2 0.341000 0.867133 0.460189 0.341000 University of Alberta
7 0.056000 0.663196 0.111500 0.056000 Jadavpur University
Non-standard runs
7 0.055000 0.662106 0.168750 0.055000 Jadavpur University
Table 10: Runs submitted for English to Kannada task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.397933 0.791233 0.507828 0.398062 University of Alberta
5 0.378295 0.782682 0.510096 0.377778 NICT
Table 11: Runs submitted for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.553604 0.770168 0.672665 0.553835 University of Alberta
Table 12: Runs submitted for English to Korean task.
10
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.125937 0.426349 0.201497 0.127339 University of Alberta
Table 13: Runs submitted for English to Japanese Kanji back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.463679 0.923826 0.535097 0.265379 University of Alberta
5 0.403014 0.891443 0.512337 0.327418 NICT
Table 14: Runs submitted for Arabic to English task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.411705 0.882858 0.549913 0.411705 NICT
2 0.394551 0.876947 0.511876 0.394551 University of Alberta
7 0.232089 0.818470 0.325345 0.232089 Jadavpur University
Non-standard runs
7 0.429869 0.875349 0.526152 0.429869 Jadavpur University
7 0.369324 0.845273 0.450589 0.369324 Jadavpur University
Table 15: Runs submitted for English to Bengali (Bangla) task.
11
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 12?20,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Whitepaper of NEWS 2010 Shared Task on Transliteration Generation?
Haizhou Li?, A Kumaran?, Min Zhang? and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of
shared task in the NEWS 2010 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be
released, on which the participants are expected
to produce a ranked list of transliteration candi-
dates in another language (i.e. n-best translitera-
tions), and this will be evaluated using common
metrics. For every language pair the participants
must submit at least one run that uses only the
data provided by the NEWS workshop organisers
in a given language pair (designated as ?standard?
run, primary submission). Users may submit more
?stanrard? runs. They may also submit several
?non-standard? runs for each language pair that
?http://translit.i2r.a-star.edu.sg/news2010/
use other data than those provided by the NEWS
2010 workshop; such runs would be evaluated and
reported separately.
2 Important Dates
Research paper submission deadline 1 May 2010
Shared task
Registration opens 1 Feb 2010
Registration closes 13 Mar 2010
Training/Development data release 19 Feb 2010
Test data release 13 Mar 2010
Results Submission Due 20 Mar 2010
Results Announcement 27 Mar 2010
Task (short) Papers Due 5 Apr 2010
For all submissions
Acceptance Notification 6 May 2010
Workshop Date 16 Jul 2010
3 Participation
1. Registration (1 Feb 2010)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (19 Feb 2010)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
(b) All registered participants are required
to participate in the evaluation of at least
one language pair, submit the results and
a short paper and attend the workshop at
ACL 2010.
3. Evaluation Script (19 Feb 2010)
12
(a) A sample test set and expected user out-
put format are to be released.
(b) An evaluation script, which runs on the
above two, is to be released.
(c) The participants must make sure that
their output is produced in a way that
the evaluation script may run and pro-
duce the expected output.
(d) The same script (with held out test data
and the user outputs) would be used for
final evaluation.
4. Test data (13 Mar 2010)
(a) The test data would be released on 13
March 2010, and the participants have a
maximum of 7 days to submit their re-
sults in the expected format.
(b) One ?standard? run must be submit-
ted from every group on a given lan-
guage pair. Additional ?standard? runs
may be submitted, up to 4 ?standard?
runs in total. However, the partici-
pants must indicate one of the submit-
ted ?standard? runs as the ?primary sub-
mission?. The primary submission will
be used for the performance summary.
In addition to the ?standard? runs, more
?non-standard? runs may be submitted.
In total, maximum 8 runs (up to 4 ?stan-
dard? runs plus up to 4 ?non-standard?
runs) can be submitted from each group
on a registered language pair. The defi-
nition of ?standard? and ?non-standard?
runs is in Section 5.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
5. Results (27 Mar 2010)
(a) On 27 March 2010, the evaluation re-
sults would be announced and will be
made available on the Workshop web-
site.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Furthermore, all participants should
agree not to reveal identities of other
participants in any of their publications
unless you get permission from the other
respective participants. By default, all
participants remain anonymous in pub-
lished results, unless they indicate oth-
erwise at the time of uploading their re-
sults. Note that the results of all systems
will be published, but the identities of
those participants that choose not to dis-
close their identity to other participants
will be masked. As a result, in this case,
your organisation name will still appear
in the web site as one of participants, but
it will not be linked explicitly to your re-
sults.
6. Short Papers on Task (5 Apr 2010)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) The review of the system papers will be
done to improve paper quality and read-
ability and make sure the authors? ideas
13
and methods can be understood by the
workshop participants. We are aiming
at accepting all system papers, and se-
lected ones will be presented orally in
the NEWS 2010 workshop.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review will be
managed electronically through https://
www.softconf.com/acl2010/NEWS.
4 Language Pairs
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1. NEWS 2010 Shared Task
offers 12 evaluation subtasks, among them ChEn
and ThEn are the back-transliteration of EnCh and
EnTh tasks respectively. NEWS 2010 releases
training, development and testing data for each of
the language pairs. NEWS 2010 continues some
language pairs that were evaluated in NEWS 2009.
In such cases, the training and development data in
the release of NEWS 2010 may overlap with those
in NEWS 2009. However, the test data in NEWS
2010 are entirely new.
The names given in the training sets for Chi-
nese, Japanese, Korean and Thai languages are
Western names and their respective translitera-
tions; the Japanese Name (in English)? Japanese
Kanji data set consists only of native Japanese
names; the Arabic data set consists only of native
Arabic names. The Indic data set (Hindi, Tamil,
Kannada, Bangla) consists of a mix of Indian and
Western names.
Examples of transliteration:
English? Chinese
Timothy????
English? Japanese Katakana
Harrington??????
English? Korean Hangul
Bennett ? ??
Japanese name in English? Japanese Kanji
Akihiro???
English? Hindi
English? Tamil
English? Kannada
Arabic? Arabic name in English
? 
Khalid
????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 5K ? 32K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 2K ? 6K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 2K ? 3K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; MSRI, 2010; CJKI, 2010). NEWS
2010 will provide the contact details of each
individual database. The data would be pro-
vided in Unicode UTF-8 encoding, in XML
format; the results are expected to be sub-
mitted in UTF-8 encoding in XML format.
The XML formats details are available in Ap-
pendix A.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
14
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 32K 6K 2K EnCh
Western Chinese English Institute for Infocomm Research 25K 5K 2K ChEn
Western English Korean Hangul CJK Institute 5K 2K 2K EnKo
Western English Japanese Katakana CJK Institute 23K 3K 3K EnJa
Japanese English Japanese Kanji CJK Institute 7K 3K 3K JnJk
Arabic Arabic English CJK Institute 25K 2.5K 2.5K ArAe
Mixed English Hindi Microsoft Research India 10K 2K 2K EnHi
Mixed English Tamil Microsoft Research India 8K 2K 2K EnTa
Mixed English Kannada Microsoft Research India 8K 2K 2K EnKa
Mixed English Bangla Microsoft Research India 10K 2K 2K EnBa
Western English Thai NECTEC 26K 2K 2K EnTh
Western Thai English NECTEC 24K 2K 2K ThEn
Table 1: Source and target languages for the shared task on transliteration.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. Standard Runs We expect that the partici-
pants to use only the data (parallel names)
provided by the Shared Task for translitera-
tion task for a ?standard? run to ensure a fair
evaluation. One such run (using only the data
provided by the shared task) is mandatory for
all participants for a given language pair that
they participate in.
5. Non-standard Runs If more data (either par-
allel names data or monolingual data) were
used, then all such runs using extra data must
be marked as ?non-standard?. For such ?non-
standard? runs, it is required to disclose the
size and characteristics of the data used in the
system paper.
6. A participant may submit a maximum of 8
runs for a given language pair (including the
mandatory 1 ?standard? run marked as ?pri-
mary submission?).
6 Paper Format
Paper submissions to NEWS 2010 should follow
the ACL 2010 paper submission policy, includ-
ing paper format, blind review policy and title and
author format convention. Full papers (research
paper) are in two-column format without exceed-
ing eight (8) pages of content plus one extra page
for references and short papers (task paper) are
also in two-column format without exceeding four
(4) pages, including references. Submission must
conform to the official ACL 2010 style guidelines.
For details, please refer to the ACL 2010 website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 4 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
The following notation is further assumed:
2http://acl2010.org/authors.html
15
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ? ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses? etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Dr. Haizhou Li
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
hli@i2r.a-star.edu.sg
16
Dr. A. Kumaran
Microsoft Research India
Scientia, 196/36, Sadashivnagar 2nd Main
Road
Bangalore 560080 INDIA
a.kumaran@microsoft.com
Mr. Jack Halpern
CEO, The CJK Dictionary Institute, Inc.
Komine Building (3rd & 4th floors)
34-14, 2-chome, Tohoku, Niiza-shi
Saitama 352-0001 JAPAN
jack@cjki.org
References
[CJKI2010] CJKI. 2010. CJK Institute.
http://www.cjk.org/.
[Li et al2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159?166, Barcelona, Spain.
[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.
17
A Training/Development Data
? File Naming Conventions:
NEWS10 train XXYY nnnn.xml
NEWS10 dev XXYY nnnn.xml
NEWS10 test XXYY nnnn.xml
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
B Submission of Results
? File Naming Conventions:
You can give your files any name you like.
During submission online you will need to
indicate whether this submission belongs to
a ?standard? or ?non-standard? run, and if it
is a ?standard? run, whether it is the primary
submission.
? File formats:
All data will be made available in XML for-
mats (Figure 2).
? Data Encoding Formats:
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.
18
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2010-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=?1?>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=?2?>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2010 Train EnHi 25K.xml
19
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2010 EnHi TUniv 01 StdRunHMMBased.xml
20
