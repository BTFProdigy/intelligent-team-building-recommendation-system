Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 426?435,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Efficient Collective Entity Linking with Stacking
Zhengyan He? Shujie Liu? Yang Song? Mu Li? Ming Zhou? Houfeng Wang??
? Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
? Microsoft Research Asia
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
songyangmagic@gmail.com wanghf@pku.edu.cn
Abstract
Entity disambiguation works by linking am-
biguous mentions in text to their correspond-
ing real-world entities in knowledge base. Re-
cent collective disambiguation methods en-
force coherence among contextual decisions
at the cost of non-trivial inference processes.
We propose a fast collective disambiguation
approach based on stacking. First, we train a
local predictor g0 with learning to rank as base
learner, to generate initial ranking list of can-
didates. Second, top k candidates of related
instances are searched for constructing expres-
sive global coherence features. A global pre-
dictor g1 is trained in the augmented feature
space and stacking is employed to tackle the
train/test mismatch problem. The proposed
method is fast and easy to implement. Exper-
iments show its effectiveness over various al-
gorithms on several public datasets. By learn-
ing a rich semantic relatedness measure be-
tween entity categories and context document,
performance is further improved.
1 Introduction
When extracting knowledge from natural language
text into a machine readable format, ambiguous
names must be resolved in order to tell which real-
world entity the name refers to. The task of linking
names to knowledge base is known as entity linking
or disambiguation (Ji et al, 2011). The resulting text
is populated with semantic rich links to knowledge
base like Wikipedia, and ready for various down-
stream NLP applications.
?Corresponding author
Previous researches have proposed several kinds
of effective approaches for this problem. Learning
to rank (L2R) approaches use hand-crafted features
f(d, e) to describe the similarity or dissimilarity be-
tween contextual document d and entity definition
e. L2R approaches are very flexible and expres-
sive. Features like name matching, context similar-
ity (Li et al, 2009; Zheng et al, 2010; Lehmann et
al., 2010) and category context correlation (Bunescu
and Pasca, 2006) can be incorporated with ease.
Nevertheless, decisions are made independently and
inconsistent results are found from time to time.
Collective approaches utilize dependencies be-
tween different decisions and resolve all ambiguous
mentions within the same context simultaneously
(Han et al, 2011; Hoffart et al, 2011; Kulkarni
et al, 2009; Ratinov et al, 2011). Collective ap-
proaches can improve performance when local ev-
idence is not confident enough. They often utilize
semantic relations across different mentions, and is
why they are called global approaches, while L2R
methods fall into local approaches (Ratinov et al,
2011). However, collective inference processes are
often expensive and involve an exponential search
space.
We propose a collective entity linking method
based on stacking. Stacked generalization (Wolpert,
1992) is a powerful meta learning algorithm that
uses two levels of learners. The predictions of the
first learner are taken as augmented features for the
second learner. The nice property of stacking is that
it does not restrict the form of the base learner. In
this paper, our base learner, an L2R ranker, is first
employed to generate a ranking list of candidates.
426
At the next level, we search for semantic coherent
entities from the top k candidates of neighboring
mentions. The second learner is trained on the aug-
mented feature space to enforce semantic coherence.
Stacking is employed to handle train/test mismatch
problem. Compared with existing collective meth-
ods, the inference process of our method is much
faster because of the simple form of its base learner.
Wikipedians annotate each entity with categories
which provide another source of valuable seman-
tic information. (Bunescu and Pasca, 2006) pro-
pose to generalize beyond context-entity correla-
tion s(d, e) with word-category correlation s(w, c).
However, this method works at word level, and does
not scale well to large number of categories. We
explore a representation learning technique to learn
the category-context association in latent semantic
space, which scales much better to large knowledge
base.
Our contributions are as follows: (1) We pro-
pose a fast and accurate stacking-based collective
entity linking method, which combines the benefits
of both coherence modeling of collective approaches
and expressivity of L2R methods. We show an
effective usage of ranking list as global features,
which is a key improvement for the global predictor.
(2) To overcome problems of scalability and shal-
low word-level comparison, we learn the category-
context correlation with recent advances of repre-
sentation learning, and show that this extra seman-
tic information indeed helps improve entity linking
performance.
2 Related Work
Most popular entity linking systems use the L2R
framework (Bunescu and Pasca, 2006; Li et al,
2009; Zheng et al, 2010; Lehmann et al, 2010).
Its discriminative nature gives the model enough
flexibility and expressivity. It can include any fea-
tures that describe the similarity or dissimilarity of
context d and candidate entity e. They often per-
form well even on small training set, with carefully-
designed features. This category falls into the local
approach as the decision processes for each mention
are made independently (Ratinov et al, 2011).
(Cucerzan, 2007) first suggests to optimize an ob-
jective function that is similar to the collective ap-
proach. However, the author adopts an approxi-
mation method because of the large search space
(which is O(nm) for a document with m mentions,
each with n candidates). Various other methods
like integer linear programming (Kulkarni et al,
2009), personalized PageRank (Han et al, 2011) and
greedy graph cutting (Hoffart et al, 2011) have been
explored in literature. Our method without stacking
resembles the method of (Ratinov et al, 2011) in
that they use the predictions of a local ranker to gen-
erate features for global ranker. The differences are
that we use stacking to train the local ranker to han-
dle the train/test mismatch problem and top k candi-
dates to generate features for the global ranker.
Stacked generalization (Wolpert, 1992) is a meta
learning algorithm that uses multiple learners out-
puts to augment the feature space of subsequent
learners. It utilizes a cross-validation strategy to ad-
dress the train set / testset label mismatch problem.
Various applications of stacking in NLP have been
proposed, such as collective document classification
(Kou and Cohen, 2007), stacked dependency parsing
(Martins et al, 2008) and joint Chinese word seg-
mentation and part-of-speech tagging (Sun, 2011).
(Kou and Cohen, 2007) propose stacked graphical
learning which captures dependencies between data
with relational template. Our method is inspired by
their approach. The difference is our base learner is
an L2R model. We search related entity candidates
in a large semantic relatedness graph, based on the
assumption that true candidates are often semanti-
cally correlated while false ones scattered around.
Wikipedians annotate entries in Wikipedia with
category network. This valuable information gener-
alizes entity-context correlation to category-context
correlation. (Bunescu and Pasca, 2006) utilize
category-word as features in their ranking model.
(Kataria et al, 2011) employ a hierarchical topic
model where each inner node in the hierarchy is a
category. Both approaches must rely on pruned cate-
gories because the large number of noisy categories.
We try to address this problem with recent advances
of representation learning (Bai et al, 2009), which
learns the relatedness of category and context in la-
tent continuous space. This method scales well to
potentially large knowledge base.
427
3 Method
In this section, we first introduce our base learner
and local features used; next, the stacking train-
ing strategy is given, followed by an explana-
tion of our global coherence model with aug-
mented feature space; finally we explain how to
learn category-context correlation with representa-
tion learning technique.
3.1 Base learner and local predictor g0
Entity linking is formalized as follows: given
an ambiguous name mention m with its con-
textual document d, a list of candidate entities
e1, e2, . . . , en(m) ? C(m) is generated for m, our
predictor g will generate a ranking score g(ei) for
each candidate ei. The ranking score will be used
to construct augmented features for the next level
learner, or used by our end system to select the an-
swer:
e? = arg max
e?C(m)
g(e) (1)
In an L2R framework, the model is often defined
as a linear combination of features. Here, our fea-
tures f?(d, e) are derived from document d and can-
didate e. The model is defined as g(e) = w?f?(d, e).
In our problem, we are given a list of training data
D = {(di, ei)}. We want to optimize the parameter
w?, such that the correct entity has a higher score over
negative ones. This is done via a preference learning
technique SVM rank, first introduced by (Joachims,
2002). The following margin based loss is mini-
mized w.r.t w?:
L = 1
2
?w??2 + C
?
?d,e? (2)
s.t. w?(f?(d, e)? f?(d, e?)) ? 1? ?d,e? (3)
?d,e? ? 0 (4)
where C is a trade-off between training error and
margin size; ? is slacking variable and loops over
all query documents d and negative candidates e? ?
C(m)? {e}.
This model is expressive enough to include any
form of features describing the similarity and dis-
similarity of d and e. We only include some typical
features seen in literature. The inclusion of these
features is not meant to be exhaustive. Our purpose
is to build a moderate model in which some of the
Surface matching:
1. mention string m exactly matches candidate
e, i.e. m = e
2. neither m is a substring of e nor e is a sub-
string of m
3. m ?= e and m is a substring of e
4. m ?= e and e is a substring of m
5. m ?= e and m is a redirect pointing to e in
Wikipedia
6. m ?= e and e starts with m
7. m ?= e and e ends with m
Context matching:
1. cosine similarity of TF-IDF score between
context and entire Wikipedia page of candidate
2. cosine similarity of TF-IDF score between
context and introduction of Wikipedia page
3. jaccard distance between context and entire
Wikipedia page of candidate
4. jaccard distance between context and intro-
duction of Wikipedia page
Popularity or prominence feature:
percentage of Wikipedia hyperlinks pointing to
e given mention m, i.e. P(e|m)
Category-context coherence model:
cat0 and cat1 (details in Section 3.4)
Table 1: Features for local predictor g0.
useful features like string matching and entity pop-
ularity cannot be easily expressed by collective ap-
proaches like (Hoffart et al, 2011; Han et al, 2011).
The features for level 0 predictor g0 are described
in Table 1. The reader can consult (Li et al, 2009;
Zheng et al, 2010; Lehmann et al, 2010) for further
reference.
3.2 Stacking training for global predictor g1
Stacked generalization (Wolpert, 1992) is a meta
learning algorithm that stacks two ?levels? of pre-
dictors. Level 0 includes one or more predictors
h(0)1 , h
(0)
2 , . . . , h
(0)
K : Rd ? R, each one is trained on
the original d-dimensional feature space. The level
1 predictor h(1) : Rd+K ? R is trained in the aug-
mented (d+K)-dimensional feature space, in which
predictions at level 0 are taken as extra features in
h(1).
(Kou and Cohen, 2007) proposed stacked graphi-
428
cal learning for learning and inference on relational
data. In stacked graphical learning, dependencies
among data are captured by relational template, with
which one searches for related instances of the cur-
rent instance. The augmented feature space does
not necessarily to be d + K. Instead, one can con-
struct any declarative feature with the original data
and predictions of related instances. For instance,
in collective document classification (Kou and Co-
hen, 2007) employ relational template to extract
documents that link to this document, then apply a
COUNT aggregator over each category on neighbor-
ing documents as level 1 features.
In our entity linking task, we use a single predic-
tor g0 trained with local features at level 0. Com-
pared with (Kou and Cohen, 2007), both g0 and g1
are L2R models rather than classifier. At level 1, for
each document-candidate entity pair, we use the re-
lational templateN (x) to find related entities for en-
tity x, and construct global features with some func-
tion G({g0(n)|n ? N (x)}) (details in Sec. 3.3).
The global predictor g1 receives as input the origi-
nal features plus G.
One problem is that if we use g0 trained on the en-
tire training set to predict related instances in train-
ing set, the accuracy can be somehow different (typ-
ically lower) for future unseen data. g1 with this pre-
diction as input doesn?t generalize well to test data.
This is known as train/test mismatch problem. To
mimic test time behavior, training is performed in a
cross-validation-like way. Let D be the entire train-
ing set:
1. Split D into L partitions {D1, . . . ,DL}
2. For each split Di:
2.1 Train an instance of g0 on D ?Di
2.2 Predict all related instances inDi with this
predictor g0
2.3 Augment feature space for x ? Di, with G
applied on predictions of N (x)
3. Train level 0 predictor g0 on entire D, for ex-
panding feature space for test data
4. Train level 1 predictor g1 on entire D, in the
augmented feature space.
In the next subsection, we will describe how to
construct global features from the predictions of g0
on neighbors N (x) with G.
3.3 Enforcing coherence with global features G
If one wants to identify the correct entity for an am-
biguous name, he would possibly look for related
entities in its surrounding context. However, sur-
rounding entities can also exhibit some degree of
ambiguity. In ideal cases, most true candidates are
inter-connected with semantic links while negative
candidates are scattered around (Fig. 1). Thus, we
ask the following question: Is there any highly rele-
vant entity to this candidate in context? Or, is there
any mention with highly relevant entity to this can-
didate in the top k ranking list of this mention? And
how many those mentions are? The reason to look
up top k candidates is to improve recall. g0 may not
perfectly rank related entity at the first place, e.g.
?Mitt Romney? in Figure 1.
Assume the ambiguous mention set is M . For
each mention mi ? M , we rank each entity ei,j ?
C(mi) by its score g0(ei,j). Denote its rank as
Rank(ei,j). For each entity e in the candidate set
E = {ei,j |?ei,j ? C(mi), ?mi ? M}, we search
related instances for e as follows:
1. search in E for entities with semantic related-
ness above a threshold ({0.1,0.3,0.5,0.7,0.9});
2. select those entities in step (1) with Rank(e)
less than or equal to k (k ? {1, 3, 5});
3. map entities in step (2) to unique set of men-
tions U , excluding current m, i.e. e ? C(m).
This process is relatively fast. It only involves a
sparse matrix slicing operation on the large pre-
computed semantic relatedness matrix in step (1),
and logical operation in step (2,3). The following
features are fired concerning the unique set U :
- if U is empty;
- if U is not empty;
- if the percentage |U |/|M | is above a threshold
(e.g. 0.3).
The above process generates a total of 45 (5?3?3)
global features.
429
Barack Obama Democratic Party (United States)
Mitt Romney
Republican Party (United States)
Obama, Fukui
Obama, Nagasaki
Democratic Party (Italy)
Democratic Party (Serbia)
Republican Party of Minnesota
Republicanism
Romney, West Virginia
HMS Romney (1694)
... ... ... ...
received national attention during his campaign  ...  with his vectory in the March   [[Obama|Barack Obama]]
[[Democratic Party|Democratic Party (United States)]] primary  ...  He was re-elected president in November
2012, defeating [[Republican|Republican Party (United States)]] nominee [[Romney|Mitt Romney]]
Figure 1: Semantic links for collective entity linking. Annotation [[mention|entity]] follows Wikipedia conventions.
Finally, the semantic relatedness measure of two
entities ei,ej is defined as the common in-links of ei
and ej in Wikipedia (Milne and Witten, 2008; Han
et al, 2011):
SR(ei, ej) = 1?
log(max(|A|, |B|))? log(|A ?B|)
log(|W |)? log(min(|A|, |B|))
(5)
where A and B are the set of in-links for entity ei
and ej respectively, andW is the set of all Wikipedia
pages.
Our method is a trade-off between exact collec-
tive inference and approximating related instance
with top ranked entities produced by g0. Most
collective approaches take all ambiguous mentions
into consideration and disambiguate them simulta-
neously, resulting in difficulty when inference in
large search space (Kulkarni et al, 2009; Hoffart
et al, 2011). Others resolve to some kinds of ap-
proximation. (Cucerzan, 2007) construct features as
the average of all candidates for one mention, in-
troducing considerable noise. (Ratinov et al, 2011)
also employ a two level architecture but only take
top 1 prediction for features. This most resembles
our approach, except we use stacking to tackle the
train/test mismatch problem, and construct different
set of features from top k candidates predicted by
g0. We will show in our experiments that this indeed
helps boost performance.
3.4 Learning category-context coherence
model cat
Entities in Wikipedia are annotated with rich se-
mantic structures. Category network provides us
with another valuable information for entity link-
ing. Take the mention ?Romney? as an exam-
ple, one candidate ?Mitt Romney? with category
?Republican party presidential nominee? co-occurs
frequently with context like ?election? and ?cam-
paign?, while another candidate ?Milton Romney?
with category ?Utah Utes football players? is fre-
quently observed with context like ?quarterback?
and ?backfield?. The category network forms a di-
rected acyclic graph (DAG). Some entities can share
category through the network, e.g. ?Barack Obama?
with category ?Democratic Party presidential nom-
inees? shares the category ?United States presiden-
tial candidates by party? with ?Mitt Romney? when
travelling two levels up the network.
(Bunescu and Pasca, 2006) propose to learn the
category-context correlation at word level through
category-word pair features. This method creates
sparsity problem and does not scale well because
the number of features grows linearly with both the
number of categories and the vocabulary size. More-
over, the category network is somewhat noisy, e.g.
travelling up four levels of the hierarchy can result
in over ten thousand categories, with many irrelevant
ones.
Rather than learning the correlation at word level,
we explore a representation learning method that
learns category-context correlation in the latent se-
mantic space. Supervised Semantic Indexing (SSI)
(Bai et al, 2009) is trained on query-document pairs
to predict their degree of matching. The compar-
ison is performed in the latent semantic space, so
that synonymy and polysemy are implicitly handled
by its inner mechanism. The score function between
query q and document d is defined as:
f(q, d) = qTWd (6)
430
where W is learned with supervision like click-
through data.
Given training data {(qi, di)}, training is done by
randomly sampling a negative target d?. The model
optimizes W such that f(q, d+) > f(q, d?). Thus,
the training objective is to minimize the following
margin-based loss function:
?
q,d+,d?
max(0, 1? f(q, d+) + f(q, d?)) (7)
which is also known as contrastive estimation
(Smith and Eisner, 2005).
W can become very large and inefficient when we
have a big vocabulary size. This is addressed by re-
placing W with its low rank approximation:
W = UTV + I (8)
here, the identity term I is a trade-off between the
latent space model and a vector space model. The
gradient step is performed with Stochastic Gradient
Descent (SGD):
U ?U + ?V (d+ ? d?)qT ,
if 1? f(q, d+) + f(q, d?) > 0 (9)
V ?V + ?Uq(d+ ? d?)T ,
if 1? f(q, d+) + f(q, d?) > 0. (10)
where ? is the learning rate.
The query and document are not necessary real
query and document. In our case, we treat our
problem as: given the occurring context of an en-
tity, retrieving categories corresponding to this en-
tity. Thus, we use context as query q and the cat-
egories of this candidate entity as d. We also treat
the definition page of an entity as its context, and
first train the model with definition pages, because
definition pages exhibit more focused topic. This
considerably accelerates the training process. To
reduce noise, We input the categories directly con-
nected with one entity as a word vector. The input
can be a TF-IDF vector or binary vector. We denote
model trained with normalized TF-IDF and with bi-
nary input as cat0 and cat1 respectively.
4 Experiments
4.1 Datasets
Previous researches have used diverse datasets for
evaluation, which makes it hard for comparison
with others? approaches. TAC-KBP has several
years of data for evaluating entity linking system,
but is not well suited for evaluating collective ap-
proaches. Recently, (Hoffart et al, 2011) anno-
tated a clean and much larger dataset AIDA 1 for
collective approaches evaluation based on CoNLL
2003 NER dataset. (Ratinov et al, 2011) also re-
fined previous work and contribute four publicly
available datasets 2. Thanks to their great works,
we have enough data to evaluate against. Accord-
ing to the setting of (Hoffart et al, 2011), we
split the AIDA dataset for train/development/test
with 946/216/231 documents. We train a separate
model on the Wikipedia training set for evaluating
ACE/QUAINT/WIKI dataset (Ratinov et al, 2011).
Table 2 gives a brief overview of the datasets used.
For knowledge base, we use the Wikipedia XML
dump 3 to extract over 3.3 million entities. We use
annotation from Wikipedia to build a name dictio-
nary from mention string m to entity e for can-
didate generation, including redirects, disambigua-
tion pages and hyperlinks, follows the approach of
(Cucerzan, 2007). For candidate generation, we
keep the top 30 candidates by popularity (Tbl. 1).
Note that our name dictionary is different from
(Ratinov et al, 2011) and has a much higher recall.
Since (Ratinov et al, 2011) evaluate on ?solvable?
mentions and we have no way to recover those men-
tions, we re-implement their global features and the
final scores are not directly comparable to theirs.
4.2 Methods under comparison
We compare our algorithm with several state-of-the-
art collective entity disambiguation systems. The
AIDA system proposed by (Hoffart et al, 2011) use
a greedy graph cutting algorithm that iteratively re-
move entities with low confidence scores. (Han et
al., 2011) employ personalized PageRank to prop-
agate evidence between different decisions. Both
algorithms use simple local features without dis-
criminative training. (Kulkarni et al, 2009) pro-
pose to use integer linear programming (ILP) for
inference. Except our re-implementation of Han?s
1available at http://www.mpi-inf.mpg.de/yago-naga/aida/
2http://cogcomp.cs.illinois.edu/Data, we don?t find the
MSNBC dataset in the zip file.
3available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
431
Dataset ndocs non-
NIL
identified solvable
AIDA dev 216 4791 4791 4707
AIDA test 231 4485 4485 4411
ACE 36 257 238 209(185)
AQUAINT 50 727 697 668(588)
Wikipedia 40 928 918 854(843)
Table 2: Number of mentions in each dataset. ?identi-
fied? means the mention exists in our name dictionary
and ?solvable? means the true entity are among the top 30
candidates by popularity. Number in parenthesis shows
the results of (Ratinov et al, 2011).
method, both AIDA and ILP solution are quite slow
at running time. The online demo of AIDA takes
over 10 sec to process one document with mod-
erate size, while the ILP solution takes around 2-
3 sec/doc. In contrast, our method takes only 0.3
sec/doc, and is easy to implement.
(Ratinov et al, 2011) also utilize a two layer
learner architecture. The difference is that their
method use top 1 candidate generated by local
learner for global feature generation , while we
search the top k candidates. Moreover, stacking is
used to tackle the train/test mismatch problem in
our model. We re-implement the global features of
(Ratinov et al, 2011) and use our local predictor
g0 for level 0 predictor. Note that we only imple-
ment their global features concerning common in-
links and inter-connection (totally 9 features) for fair
comparison because all other models don?t use com-
mon outgoing links for global coherence.
4.3 Settings
We implement SVM rank with an adaptation of lin-
ear SVM in scikit-learn (which is a wrapper of Li-
blinear). The category-context coherence model is
implemented with Numpy configured with Open-
Blas library, and we train this model on the entire
Wikipedia hyperlink annotation. It takes about 1.5d
for one pass over the entire dataset. The learning
rate ? is set to 1e-4 and training cost before update
is below 0.02.
Parameter tuning: there aren?t many parameters
to tune for both g0 and g1. The context document
window size is fixed as 100 for compatibility with
(Ratinov et al, 2011; Hoffart et al, 2011). The num-
ber of candidates is fixed to top 30 ranked by entity?s
popularity. Increase this value will generally boost
recall at the cost of lower precision.
We introduce the following default parameter for
global features in g1. The number of fold for stack-
ing is set to {1,5,10} (see Table 4, default is 10; 1
means no stacking, i.e. training g0 with all training
data and generating level 1 features for training data
directly with this g0). The number k for searching
neighboring entities with relational template is set
to {1,3,5,7} (e.g. in step 2 of Section 3.3 k = 5;
default is 5).
For category-context modeling, the vocabulary
sizes of context and category are set to top 10k and
6k unigrams by frequency. The latent dimension of
low rank approximation is set to 200.
Performance measures: For all non-NIL
queries, we evaluate performance with micro pre-
cision averaged over queries and macro precision
averaged over documents. Mean Reciprocal Rank
(MRR) is an information retrieval measure and is
defined as 1|Q|
?|Q|
i
1
ranki , where ranki is the rank
of correct answer in response to query i. For
ACE/AQUAINT/WIKI we also give the accuracy of
?solvable? mentions, but this is not directly compa-
rable to (Ratinov et al, 2011). Our name dictionary
is different from theirs and ours has a higher recall
rate (Tbl. 2). Hence, the ?solvable? set is different.
k recall k recall
1 78.56 6 96.31
2 89.59 7 97.04
3 93.01 8 97.37
4 94.97 9 97.62
5 95.78 10 97.81
Table 3: Top k recall for local predictor g0.
4.4 Discussions
Table 4 shows the evaluation results on AIDA
dataset and Table 5 shows results on datasets
ACE/AQUAINT/WIKI.
Effect of cat:The first group in Table 4 shows
some baseline features for comparison. We can see
even if the categories only carry incomplete and
noisy information about an entity, it performs much
432
Methods Devset Testset
micro
p@1
macro
p@1
MRR micro
p@1
macro
p@1
MRR
cosine 33.25 28.61 46.03 33.33 28.63 46.54
jaccard 44.71 36.56 57.76 45.66 36.89 57.08
cat0 54.75 47.14 67.70 61.52 54.72 72.55
cat1 60.15 54.64 72.98 65.46 61.04 76.84
popularity 69.21 67.59 79.26 69.07 72.63 79.45
g0 76.04 73.63 84.21 76.16 78.17 84.58
g0+global(Ratinov) 81.30 78.03 88.14 81.45 81.89 88.70
g1+1fold 82.01 78.52 88.90 83.59 83.58 90.05
g1+5fold 81.99 78.42 88.87 83.52 83.37 89.99
g1+10fold 82.01 78.53 88.91 83.59 83.55 90.03
g1+top1 81.65 78.76 88.51 81.81 82.55 89.06
g1+top3 82.20 78.64 88.98 83.52 83.34 89.94
g1+top5 82.01 78.57 88.90 83.63 83.76 90.05
g1+top7 82.05 78.40 88.90 83.75 83.58 90.08
g0+cat 79.36 76.14 86.66 79.64 80.47 87.32
g1+cat 82.24 78.49 89.02 84.88 84.49 90.65
g1+cat+all context 82.99 78.56 89.51 86.49 85.11 91.55
(Hoffart et al, 2011) - - - 82.29 82.02 -
(Shirakawa et al, 2011) - - - 81.40 83.57 -
(Kulkarni et al, 2009) - - - 72.87 76.74 -
(Han et al, 2011) - - - 78.97 75.77 -
Table 4: Performance on AIDA dataset. Maximal value in each group are highlighted with bold font. top k means up
to k candidates are used for searching related instances with relational template.
better than word level features. Group 5 in Table
4 shows cat information generally boosts perfor-
mance for both predictor g0 and g1.
Effect of stacking: Group 3 in Table 4 shows the
results with different fold in stacking training. 1 fold
means training g0 with all training data and directly
augment training data with this g0. Surprisingly, we
do not observe any substantial difference with vari-
ous fold size. We deduce it is possible the way we
fire global features with top k candidates that alle-
viates the problem of train/test mismatch when ex-
tending feature space for g1. Despite the ranking of
true entity can be lower in testset than in training
set, the semantic coherence information can still be
captured with searching over top k candidates.
Effect of top k global features: Group 4 in Table
4 shows the effect of k on g1 performance. Clearly,
increasing k generally improves precision and one
possible reason is the improvement in recall when
searching for related instances. Table 3 shows the
top k recall of local predictor g0. Further increasing
k does not show any improvement.
Our method benefits from such a searching strat-
egy, and consistently outperforms the global fea-
tures of (Ratinov et al, 2011). While their method
is a trade-off between expensive exact search over
all mentions and greedy assigning all mentions
with local predictor, we show this idea can be fur-
ther extended, somewhat like increasing the beam
search size without additional computational over-
head. The only exception is the ACE dataset, since
this dataset is so small, the difference translates to
only one mention. One may notice the improvement
on ACE/AQUAINT datasets is a little inconsistent.
These datasets are much smaller and the results only
differ within 4 mentions. Because these models are
433
Method micro
p@1
macro
p@1
MRR correct
/ solv-
able
ACE
g0 77.43 81.30 79.03 95.22
Ratinov 77.43 80.70 78.81 95.22
g1+5fold 77.04 79.85 78.96 94.74
g0+cat 77.82 81.48 79.31 95.69
g1+cat 77.43 80.16 79.25 95.22
AQUAINT
g0 84.46 84.69 87.49 91.92
Ratinov 85.14 85.29 87.90 92.66
g1+5fold 85.83 85.55 88.27 93.41
g0+cat 85.01 85.00 87.89 92.51
g1+cat 85.28 85.14 88.23 92.81
Wikipedia test
g0 83.19 84.30 86.63 90.40
Ratinov 84.48 85.96 87.62 91.80
g1+5fold 84.81 86.29 88.13 92.15
g0+cat 84.38 86.13 87.51 91.69
g1+cat 85.45 87.16 88.31 92.86
Table 5: Evaluation on ACE/AQUAINT/WIKI datasets.
trained on Wikipedia, the annotation style can be
quite different.
Finally, as we analyze the development set of
AIDA, we discover that some location entities rely
on more distant information across the context, as
we increase the context to the entire contextual doc-
ument, we can gain extra performance boost.
4.5 Error analysis
As we analyze the development set of AIDA, we find
some general problems with location names. Loca-
tion name generally is not part of the main topic
of one document. Thus, comparing context with
its definition is not realistic. Most of the time, we
can find some related location names in context; but
other times, it is not easily distinguished. For in-
stance, in ?France beats Turkey in men?s football...?
France refers to ?France national football team? but
our system links it to the country page ?France? be-
cause it is more popular. This can be addressed by
modeling finer context (Sen, 2012) or local syntac-
tic pattern (Hoffart et al, 2011). In other cases,
our system misclassifies ?New York City? for ?New
York? and ?Netherlands? for ?Holland? and ?Peo-
ple?s Republic of China? for ?China?, because in
all these cases, the latter ones are the most popu-
lar in Wikipedia. It is even hard for us humans to
tell the difference based only on context or global
coherence.
5 Conclusions
We propose a stacking based collective entity link-
ing method, which stacks a global predictor on top
of a local predictor to collect coherence information
from neighboring decisions. It is fast and easy to im-
plement. Our method trades off between inefficient
exact search and greedily assigning mention with lo-
cal predictor. It can be seen as searching related
entities with relational template in stacked graphi-
cal learning, with beam size k. Furthermore, we
adopt recent progress in representation learning to
learn category-context coherence model. It scales
better than existing approaches on large knowledge
base and performs comparison in the latent semantic
space. Combining these two techniques, our model
consistently outperforms all existing more sophisti-
cated collective approaches in our experiments.
Acknowledgments
This research was partly supported by Ma-
jor National Social Science Fund of China(No.
12&ZD227),National High Technology Research
and Development Program of China (863 Program)
(No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009).
References
B. Bai, J. Weston, D. Grangier, R. Collobert, O. Chapelle,
and K. Weinberger. 2009. Supervised semantic index-
ing. In The 18th ACM Conference on Information and
Knowledge Management (CIKM).
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP-CoNLL, volume 6, pages 708?716.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: a graph-based method. In Pro-
434
ceedings of the 34th international ACM SIGIR con-
ference on Research and development in Information
Retrieval, pages 765?774. ACM.
J. Hoffart, M.A. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 782?792. Association for Computational Lin-
guistics.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2011. Overview of the tac 2011
knowledge base population track. In Proceedings of
the Fourth Text Analysis Conference.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
ACM.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hierar-
chical topic models. In Proceedings of KDD.
Zhenzhen Kou and William W Cohen. 2007. Stacked
graphical models for efficient inference in markov ran-
dom fields. In SDM.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages
457?466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and Y. Shi.
2010. Lcc approaches to knowledge base population
at tac 2010. In Proc. TAC 2010 Workshop.
F. Li, Z. Zheng, F. Bu, Y. Tang, X. Zhu, and M. Huang.
2009. Thu quanta at tac 2009 kbp and rte track. In
Proceedings of Test Analysis Conference 2009 (TAC
09).
Andre? FT Martins, Dipanjan Das, Noah A Smith, and
Eric P Xing. 2008. Stacking dependency parsers. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 157?166. As-
sociation for Computational Linguistics.
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceedings of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proceedings of the Annual Meeting of
the Association of Computational Linguistics (ACL).
P. Sen. 2012. Collective context-aware topic models
for entity disambiguation. In Proceedings of the 21st
international conference on World Wide Web, pages
729?738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011.
Entity disambiguation based on a probabilistic
taxonomy. Technical report, Technical Report
MSR-TR-2011-125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 354?362. Asso-
ciation for Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In ACL, pages 1385?1394.
David H Wolpert. 1992. Stacked generalization. Neural
networks, 5(2):241?259.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
483?491, Los Angeles, California, June. Association
for Computational Linguistics.
435
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 30?34,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Entity Representation for Entity Disambiguation
Zhengyan He? Shujie Liu? Mu Li? Ming Zhou? Longkai Zhang? Houfeng Wang??
? Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
? Microsoft Research Asia
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
zhlongk@qq.com wanghf@pku.edu.cn
Abstract
We propose a novel entity disambigua-
tion model, based on Deep Neural Net-
work (DNN). Instead of utilizing simple
similarity measures and their disjoint com-
binations, our method directly optimizes
document and entity representations for a
given similarity measure. Stacked Denois-
ing Auto-encoders are first employed to
learn an initial document representation in
an unsupervised pre-training stage. A su-
pervised fine-tuning stage follows to opti-
mize the representation towards the simi-
larity measure. Experiment results show
that our method achieves state-of-the-art
performance on two public datasets with-
out any manually designed features, even
beating complex collective approaches.
1 Introduction
Entity linking or disambiguation has recently re-
ceived much attention in natural language process-
ing community (Bunescu and Pasca, 2006; Han
et al, 2011; Kataria et al, 2011; Sen, 2012). It is
an essential first step for succeeding sub-tasks in
knowledge base construction (Ji and Grishman,
2011) like populating attribute to entities. Given
a sentence with four mentions, ?The [[Python]] of
[[Delphi]] was a creature with the body of a snake.
This creature dwelled on [[Mount Parnassus]], in
central [[Greece]].? How can we determine that
Python is an earth-dragon in Greece mythology
and not the popular programming language, Del-
phi is not the auto parts supplier, and Mount Par-
nassus is in Greece, not in Colorado?
A most straightforward method is to compare
the context of the mention and the definition of
candidate entities. Previous work has explored
many ways of measuring the relatedness of context
?Corresponding author
d and entity e, such as dot product, cosine similar-
ity, Kullback-Leibler divergence, Jaccard distance,
or more complicated ones (Zheng et al, 2010;
Kulkarni et al, 2009; Hoffart et al, 2011; Bunescu
and Pasca, 2006; Cucerzan, 2007; Zhang et al,
2011). However, these measures are often dupli-
cate or over-specified, because they are disjointly
combined and their atomic nature determines that
they have no internal structure.
Another line of work focuses on collective dis-
ambiguation (Kulkarni et al, 2009; Han et al,
2011; Ratinov et al, 2011; Hoffart et al, 2011).
Ambiguous mentions within the same context are
resolved simultaneously based on the coherence
among decisions. Collective approaches often un-
dergo a non-trivial decision process. In fact, (Rati-
nov et al, 2011) show that even though global ap-
proaches can be improved, local methods based on
only similarity sim(d, e) of context d and entity e
are hard to beat. This somehow reveals the impor-
tance of a good modeling of sim(d, e).
Rather than learning context entity associa-
tion at word level, topic model based approaches
(Kataria et al, 2011; Sen, 2012) can learn it in
the semantic space. However, the one-topic-per-
entity assumption makes it impossible to scale to
large knowledge base, as every entity has a sepa-
rate word distribution P (w|e); besides, the train-
ing objective does not directly correspond with
disambiguation performances.
To overcome disadvantages of previous ap-
proaches, we propose a novel method to learn con-
text entity association enriched with deep architec-
ture. Deep neural networks (Hinton et al, 2006;
Bengio et al, 2007) are built in a hierarchical man-
ner, and allow us to compare context and entity
at some higher level abstraction; while at lower
levels, general concepts are shared across entities,
resulting in compact models. Moreover, to make
our model highly correlated with disambiguation
performance, our method directly optimizes doc-
30
ument and entity representations for a fixed simi-
larity measure. In fact, the underlying representa-
tions for computing similarity measure add inter-
nal structure to the given similarity measure. Fea-
tures are learned leveraging large scale annotation
of Wikipedia, without any manual design efforts.
Furthermore, the learned model is compact com-
pared with topic model based approaches, and can
be trained discriminatively without relying on ex-
pensive sampling strategy. Despite its simplicity,
it beats all complex collective approaches in our
experiments. The learned similarity measure can
be readily incorporated into any existing collective
approaches, which further boosts performance.
2 Learning Representation for
Contextual Document
Given a mention string m with its context docu-
ment d, a list of candidate entities C(m) are gen-
erated form, for each candidate entity ei ? C(m),
we compute a ranking score sim(dm, ei) indicat-
ing how likely m refers to ei. The linking result is
e = argmaxei sim(dm, ei).
Our algorithm consists of two stages. In the pre-
training stage, Stacked Denoising Auto-encoders
are built in an unsupervised layer-wise fashion to
discover general concepts encoding d and e. In the
supervised fine-tuning stage, the entire network
weights are fine-tuned to optimize the similarity
score sim(d, e).
2.1 Greedy Layer-wise Pre-training
Stacked Auto-encoders (Bengio et al, 2007) is
one of the building blocks of deep learning. As-
sume the input is a vector x, an auto-encoder con-
sists of an encoding process h(x) and a decod-
ing process g(h(x)). The goal is to minimize the
reconstruction error L(x, g(h(x))), thus retaining
maximum information. By repeatedly stacking
new auto-encoder on top of previously learned
h(x), stacked auto-encoders are obtained. This
way we learn multiple levels of representation of
input x.
One problem of auto-encoder is that it treats all
words equally, no matter it is a function word or
a content word. Denoising Auto-encoder (DA)
(Vincent et al, 2008) seeks to reconstruct x given
a random corruption x? of x. DA can capture global
structure while ignoring noise as the author shows
in image processing. In our case, we input each
document as a binary bag-of-words vector (Fig.
1). DA will capture general concepts and ignore
noise like function words. By applying masking
noise (randomly mask 1 with 0), the model also
exhibits a fill-in-the-blank property (Vincent et
al., 2010): the missing components must be re-
covered from partial input. Take ?greece? for ex-
ample, the model must learn to predict it with
?python? ?mount?, through some hidden unit. The
hidden unit may somehow express the concept of
Greece mythology.
h(x)
g(h(x))
pythondragon delphicoding ... greecemountsnake phd
reconstruct input
reconstruct randomzero nodenot reconstruct
inactive
active, but mask out
active
Figure 1: DA and reconstruction sampling.
In order to distinguish between a large num-
ber of entities, the vocabulary size must be large
enough. This adds considerable computational
overhead because the reconstruction process in-
volves expensive dense matrix multiplication. Re-
construction sampling keeps the sparse property
of matrix multiplication by reconstructing a small
subset of original input, with no loss of quality of
the learned representation (Dauphin et al, 2011).
2.2 Supervised Fine-tuning
This stage we optimize the learned representation
(?hidden layer n? in Fig. 2) towards the ranking
score sim(d, e), with large scale Wikipedia an-
notation as supervision. We collect hyperlinks in
Wikipedia as our training set {(di, ei,mi)}, where
mi is the mention string for candidate generation.
The network weights below ?hidden layer n? are
initialized with the pre-training stage.
Next, we stack another layer on top of the
learned representation. The whole network is
tuned by the final supervised objective. The reason
to stack another layer on top of the learned rep-
resentation, is to capture problem specific struc-
tures. Denote the encoding of d and e as d? and
e? respectively, after stacking the problem-specific
layer, the representation for d is given as f(d) =
sigmoid(W ? d? + b), where W and b are weight
and bias term respectively. f(e) follows the same
31
encoding process.
The similarity score of (d, e) pair is defined as
the dot product of f(d) and f(e) (Fig. 2):
sim(d, e) = Dot(f(d), f(e)) (1)
<.,.>
f(d) f(e)
hidden layer n
stacked auto-encoder
sim(d,e)
Figure 2: Network structure of fine-tuning stage.
Our goal is to rank the correct entity higher
than the rest candidates relative to the context of
the mention. For each training instance (d, e), we
contrast it with one of its negative candidate pair
(d, e?). This gives the pairwise ranking criterion:
L(d, e) = max{0, 1? sim(d, e) + sim(d, e?)}
(2)
Alternatively, we can contrast with all its candi-
date pairs (d, ei). That is, we raise the similarity
score of true pair sim(d, e) and penalize all the
rest sim(d, ei). The loss function is defined as
negative log of softmax function:
L(d, e) = ? log exp sim(d, e)?
ei?C(m) exp sim(d, ei)
(3)
Finally, we seek to minimize the following train-
ing objective across all training instances:
L =
?
d,e
L(d, e) (4)
The loss function is closely related to con-
trastive estimation (Smith and Eisner, 2005),
which defines where the positive example takes
probability mass from. We find that by penaliz-
ing more negative examples, convergence speed
can be greatly accelerated. In our experiments, the
softmax loss function consistently outperforms
pairwise ranking loss function, which is taken as
our default setting.
However, the softmax training criterion adds
additional computational overhead when per-
forming mini-batch Stochastic Gradient Descent
(SGD). Although we can use a plain SGD (i.e.
mini-batch size is 1), mini-batch SGD is faster to
converge and more stable. Assume the mini-batch
size ism and the number of candidates is n, a total
of m ? n forward-backward passes over the net-
work are performed to compute a similarity ma-
trix (Fig. 3), while pairwise ranking criterion only
needs 2?m. We address this problem by grouping
training pairs with same mentionm into one mini-
batch {(d, ei)|ei ? C(m)}. Observe that if candi-
date entities overlap, they share the same forward-
backward path. Only m + n forward-backward
passes are needed for each mini-batch now.
Python (programming language)PythonidaePython (mythology)
... ...
... ...
... ...
d0
d1 ...dm ... =sim(d,e)
e0 e1 e2 en
Figure 3: Sharing path within mini-batch.
The re-organization of mini-batch is similar
in spirit to Backpropagation Through Structure
(BTS) (Goller and Kuchler, 1996). BTS is a vari-
ant of the general backpropagation algorithm for
structured neural network. In BTS, parent node
is computed with its child nodes at the forward
pass stage; child node receives gradient as the sum
of derivatives from all its parents. Here (Fig. 2),
parent node is the score node sim(d, e) and child
nodes are f(d) and f(e). In Figure 3, each row
shares forward path of f(d) while each column
shares forward path of f(e). At backpropagation
stage, gradient is summed over each row of score
nodes for f(d) and over each column for f(e).
Till now, our input simply consists of bag-of-
words binary vector. We can incorporate any
handcrafted feature f(d, e) as:
sim(d, e) = Dot(f(d), f(e)) + ~?~f(d, e) (5)
In fact, we find that with only Dot(f(d), f(e))
as ranking score, the performance is sufficiently
good. So we leave this as our future work.
32
3 Experiments and Analysis
Training settings: In pre-training stage, input
layer has 100,000 units, all hidden layers have
1,000 units with rectifier functionmax(0, x). Fol-
lowing (Glorot et al, 2011), for the first recon-
struction layer, we use sigmoid activation func-
tion and cross-entropy error function. For higher
reconstruction layers, we use softplus (log(1 +
exp(x))) as activation function and squared loss
as error function. For corruption process, we use a
masking noise probability in {0.1,0.4,0.7} for the
first layer, a Gaussian noise with standard devi-
ation of 0.1 for higher layers. For reconstruction
sampling, we set the reconstruction rate to 0.01. In
fine-tuning stage, the final layer has 200 units with
sigmoid activation function. The learning rate is
set to 1e-3. The mini-batch size is set to 20.
We run all our experiments on a Linux ma-
chine with 72GB memory 6 core Xeon CPU. The
model is implemented in Python with C exten-
sions, numpy configured with Openblas library.
Thanks to reconstruction sampling and refined
mini-batch arrangement, it takes about 1 day to
converge for pre-training and 3 days for fine-
tuning, which is fast given our training set size.
Datasets: We use half of Wikipedia 1 plain text
(?1.5M articles split into sections) for pre-training.
We collect a total of 40M hyperlinks grouped by
name string m for fine-tuning stage. We holdout
a subset of hyperlinks for model selection, and we
find that 3 layers network with a higher masking
noise rate (0.7) always gives best performance.
We select TAC-KBP 2010 (Ji and Grishman,
2011) dataset for non-collective approaches, and
AIDA 2 dataset for collective approaches. For both
datasets, we evaluate the non-NIL queries. The
TAC-KBP and AIDA testb dataset contains 1020
and 4485 non-NIL queries respectively.
For candidate generation, mention-to-entity dic-
tionary is built by mining Wikipedia structures,
following (Cucerzan, 2007). We keep top 30 can-
didates by prominence P (e|m) for speed consid-
eration. The candidate generation recall are 94.0%
and 98.5% for TAC and AIDA respectively.
Analysis: Table 1 shows evaluation results
across several best performing systems. (Han et
al., 2011) is a collective approach, using Person-
alized PageRank to propagate evidence between
1available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
2available at http://www.mpi-inf.mpg.de/yago-naga/aida/
different decisions. To our surprise, our method
with only local evidence even beats several com-
plex collective methods with simple word similar-
ity. This reveals the importance of context model-
ing in semantic space. Collective approaches can
improve performance only when local evidence is
not confident enough. When embedding our sim-
ilarity measure sim(d, e) into (Han et al, 2011),
we achieve the best results on AIDA.
A close error analysis shows some typical er-
rors due to the lack of prominence feature and
name matching feature. Some queries acciden-
tally link to rare candidates and some link to en-
tities with completely different names. We will
add these features as mentioned in Eq. 5 in future.
We will also add NIL-detection module, which is
required by more realistic application scenarios.
A first thought is to construct pseudo-NIL with
Wikipedia annotations and automatically learn the
threshold and feature weight as in (Bunescu and
Pasca, 2006; Kulkarni et al, 2009).
Methods micro
P@1
macro
P@1
TAC 2010 eval
Lcc (2010) (top1, noweb) 79.22 -
Siel 2010 (top2, noweb) 71.57 -
our best 80.97 -
AIDA dataset (collective approaches)
AIDA (2011) 82.29 82.02
Shirakawa et al (2011) 81.40 83.57
Kulkarni et al (2009) 72.87 76.74
wordsim (cosine) 48.38 37.30
Han (2011) +wordsim 78.97 75.77
our best (non-collective) 84.82 83.37
Han (2011) + our best 85.62 83.95
Table 1: Evaluation on TAC and AIDA dataset.
4 Conclusion
We propose a deep learning approach that auto-
matically learns context-entity similarity measure
for entity disambiguation. The intermediate rep-
resentations are learned leveraging large scale an-
notations of Wikipedia, without any manual effort
of designing features. The learned representation
of entity is compact and can scale to very large
knowledge base. Furthermore, experiment reveals
the importance of context modeling in this field.
By incorporating our learned measure into collec-
tive approach, performance is further improved.
33
Acknowledgments
We thank Nan Yang, Jie Liu and Fei Wang for helpful discus-
sions. This research was partly supported by National High
Technology Research and Development Program of China
(863 Program) (No. 2012AA011101), National Natural Sci-
ence Foundation of China (No.91024009) and Major Na-
tional Social Science Fund of China(No. 12&ZD227).
References
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle.
2007. Greedy layer-wise training of deep networks.
Advances in neural information processing systems,
19:153.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP-CoNLL, volume 6, pages 708?716.
Y. Dauphin, X. Glorot, and Y. Bengio. 2011.
Large-scale learning of embeddings with recon-
struction sampling. In Proceedings of the Twenty-
eighth International Conference on Machine Learn-
ing (ICML11).
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain
adaptation for large-scale sentiment classification: A
deep learning approach. In Proceedings of the 28th
International Conference on Machine Learning.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347?352. IEEE.
X. Han, L. Sun, and J. Zhao. 2011. Collective en-
tity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765?774. ACM.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527?1554.
J. Hoffart, M.A. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of
named entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 782?792. Association for Com-
putational Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge
base population: Successful approaches and chal-
lenges. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1148?
1158, Portland, Oregon, USA, June. Association for
Computational Linguistics.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hier-
archical topic models. In Proceedings of KDD.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 457?
466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and
Y. Shi. 2010. Lcc approaches to knowledge base
population at tac 2010. In Proc. TAC 2010 Work-
shop.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL).
P. Sen. 2012. Collective context-aware topic mod-
els for entity disambiguation. In Proceedings of the
21st international conference on World Wide Web,
pages 729?738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011. Entity
disambiguation based on a probabilistic taxonomy.
Technical report, Technical Report MSR-TR-2011-
125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 354?
362. Association for Computational Linguistics.
P. Vincent, H. Larochelle, Y. Bengio, and P.A. Man-
zagol. 2008. Extracting and composing robust
features with denoising autoencoders. In Proceed-
ings of the 25th international conference on Ma-
chine learning, pages 1096?1103. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research, 11:3371?3408.
W. Zhang, Y.C. Sim, J. Su, and C.L. Tan. 2011. Entity
linking with effective acronym expansion, instance
selection and topic modeling. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages
1909?1914. AAAI Press.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-
aoyan Zhu. 2010. Learning to link entities with
knowledge base. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 483?491, Los Ange-
les, California, June. Association for Computational
Linguistics.
34
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 177?182,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving Chinese Word Segmentation on Micro-blog Using Rich
Punctuations
Longkai Zhang Li Li Zhengyan He Houfeng Wang? Ni Sun
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, li.l@pku.edu.cn, hezhengyan.hit@gmail.com,
wanghf@pku.edu.cn,sunny.forwork@gmail.com
Abstract
Micro-blog is a new kind of medium
which is short and informal. While no
segmented corpus of micro-blogs is avail-
able to train Chinese word segmentation
model, existing Chinese word segmenta-
tion tools cannot perform equally well
as in ordinary news texts. In this pa-
per we present an effective yet simple ap-
proach to Chinese word segmentation of
micro-blog. In our approach, we incor-
porate punctuation information of unla-
beled micro-blog data by introducing char-
acters behind or ahead of punctuations,
for they indicate the beginning or end of
words. Meanwhile a self-training frame-
work to incorporate confident instances is
also used, which prove to be helpful. Ex-
periments on micro-blog data show that
our approach improves performance, espe-
cially in OOV-recall.
1 INTRODUCTION
Micro-blog (also known as tweets in English) is
a new kind of broadcast medium in the form of
blogging. A micro-blog differs from a traditional
blog in that it is typically smaller in size. Further-
more, texts in micro-blogs tend to be informal and
new words occur more frequently. These new fea-
tures of micro-blogs make the Chinese Word Seg-
mentation (CWS) models trained on the source do-
main, such as news corpus, fail to perform equally
well when transferred to texts from micro-blogs.
For example, the most widely used Chinese seg-
menter ?ICTCLAS? yields 0.95 f-score in news
corpus, only gets 0.82 f-score on micro-blog data.
The poor segmentation results will hurt subse-
quent analysis on micro-blog text.
?Corresponding author
Manually labeling the texts of micro-blog is
time consuming. Luckily, punctuations provide
useful information because they are used as indi-
cators of the end of previous sentence and the be-
ginning of the next one, which also indicate the
start and the end of a word. These ?natural bound-
aries? appear so frequently in micro-blog texts that
we can easily make good use of them. TABLE 1
shows some statistics of the news corpus vs. the
micro-blogs. Besides, English letters and digits
are also more than those in news corpus. They
all are natural delimiters of Chinese characters and
we treat them just the same as punctuations.
We propose a method to enlarge the training
corpus by using punctuation information. We
build a semi-supervised learning (SSL) framework
which can iteratively incorporate newly labeled in-
stances from unlabeled micro-blog data during the
training process. We test our method on micro-
blog texts and experiments show good results.
This paper is organized as follows. In section 1
we introduce the problem. Section 2 gives detailed
description of our approach. We show the experi-
ment and analyze the results in section 3. Section
4 gives the related works and in section 5 we con-
clude the whole work.
2 Our method
2.1 Punctuations
Chinese word segmentation problem might be
treated as a character labeling problem which
gives each character a label indicating its position
in one word. To be simple, one can use label ?B?
to indicate a character is the beginning of a word,
and use ?N? to indicate a character is not the be-
ginning of a word. We also use the 2-tag in our
work. Other tag sets like the ?BIES? tag set are not
suiteable because the puctuation information can-
not decide whether a character after punctuation
should be labeled as ?B? or ?S?(word with Single
177
Chinese English Number Punctuation
News 85.7% 0.6% 0.7% 13.0%
micro-blog 66.3% 11.8% 2.6% 19.3%
Table 1: Percentage of Chinese, English, number, punctuation in the news corpus vs. the micro-blogs.
character).
Punctuations can serve as implicit labels for the
characters before and after them. The character
right after punctuations must be the first character
of a word, meanwhile the character right before
punctuations must be the last character of a word.
An example is given in TABLE 2.
2.2 Algorithm
Our algorithm ?ADD-N? is shown in TABLE 3.
The initially selected character instances are those
right after punctuations. By definition they are all
labeled with ?B?. In this case, the number of train-
ing instances with label ?B? is increased while the
number with label ?N? remains unchanged. Be-
cause of this, the model trained on this unbal-
anced corpus tends to be biased. This problem can
become even worse when there is inexhaustible
supply of texts from the target domain. We as-
sume that labeled corpus of the source domain can
be treated as a balanced reflection of different la-
bels. Therefore we choose to estimate the bal-
anced point by counting characters labeling ?B?
and ?N? and calculate the ratio which we denote
as ?. We assume the enlarged corpus is also bal-
anced if and only if the ratio of ?B? to ?N? is just
the same to? of the source domain.
Our algorithm uses data from source domain to
make the labels balanced. When enlarging corpus
using characters behind punctuations from texts
in target domain, only characters labeling ?B? are
added. We randomly reuse some characters label-
ing ?N? from labeled data until ratio? is reached.
We do not use characters ahead of punctuations,
because the single-character words ahead of punc-
tuations take the label of ?B? instead of ?N?. In
summary our algorithm tackles the problem by du-
plicating labeled data in source domain. We de-
note our algorithm as ?ADD-N?.
We also use baseline feature templates include
the features described in previous works (Sun and
Xu, 2011; Sun et al, 2012). Our algorithm is not
necessarily limited to a specific tagger. For sim-
plicity and reliability, we use a simple Maximum-
Entropy tagger.
3 Experiment
3.1 Data set
We evaluate our method using the data from
weibo.com, which is the biggest micro-blog ser-
vice in China. We use the API provided by
weibo.com1 to crawl 500,000 micro-blog texts of
weibo.com, which contains 24,243,772 charac-
ters. To keep the experiment tractable, we first ran-
domly choose 50,000 of all the texts as unlabeled
data, which contain 2,420,037 characters. We
manually segment 2038 randomly selected micro-
blogs.We follow the segmentation standard as the
PKU corpus.
In micro-blog texts, the user names and URLs
have fixed format. User names start with ?@?, fol-
lowed by Chinese characters, English letters, num-
bers and ? ?, and terminated when meeting punc-
tuations or blanks. URLs also match fixed pat-
terns, which are shortened using ?http://t.
cn/? plus six random English letters or numbers.
Thus user names and URLs can be pre-processed
separately. We follow this principle in following
experiments.
We use the benchmark datasets provided by the
second International Chinese Word Segmentation
Bakeoff2 as the labeled data. We choose the PKU
data in our experiment because our baseline meth-
ods use the same segmentation standard.
We compare our method with three baseline
methods. The first two are both famous Chinese
word segmentation tools: ICTCLAS3 and Stan-
ford Chinese word segmenter4, which are widely
used in NLP related to word segmentation. Stan-
ford Chinese word segmenter is a CRF-based seg-
mentation tool and its segmentation standard is
chosen as the PKU standard, which is the same
to ours. ICTCLAS, on the other hand, is a HMM-
based Chinese word segmenter. Another baseline
is Li and Sun (2009), which also uses punctua-
tion in their semi-supervised framework. F-score
1http://open.weibo.com/wiki
2http://www.sighan.org/bakeoff2005/
3http://ictclas.org/
4http://nlp.stanford.edu/projects/
chinese-nlp.shtml\#cws
178
? ? ? ? ? ? ? ? ? ? ? ?
B - - - - - B - - - - -
B N B B N B B N B B N B
Table 2: The first line represents the original text. The second line indicates whether each character is
the Beginning of sentence. The third line is the tag sequence using ?BN? tag set.
ADD-N algorithm
Input: labeled data {(xi, yi)li?1}, unlabeled data {xj}l+uj=l+1.
1. Initially, let L = {(xi, yi)li?1} and U = {xj}l+uj=l+1.2. Label instances behind punctuations in U as ?B? and add them into
L.
3. Calculate ?B?, ?N? ratio ? in labeled data.
4. Randomly duplicate characters whose labels are ?N? in L to make
?B?/?N?= ?
5. Repeat:
5.1 Train a classifier f from L using supervised learning.
5.2 Apply f to tag the unlabeled instances in U .
5.3 Add confident instances from U to L.
Table 3: ADD-N algorithm.
is used as the accuracy measure. The recall of
out-of-vocabulary is also taken into consideration,
which measures the ability of the model to cor-
rectly segment out of vocabulary words.
3.2 Main results
Method P R F OOV-R
Stanford 0.861 0.853 0.857 0.639
ICTCLAS 0.812 0.861 0.836 0.602
Li-Sun 0.707 0.820 0.760 0.734
Maxent 0.868 0.844 0.856 0.760
No-punc 0.865 0.829 0.846 0.760
No-balance 0.869 0.877 0.873 0.757
Our method 0.875 0.875 0.875 0.773
Table 4: Segmentation performance with different
methods on the development data.
TABLE 4 summarizes the segmentation results.
In TABLE 4, Li-Sun is the method in Li and
Sun (2009). Maxent only uses the PKU data for
training, with neither punctuation information nor
self-training framework incorporated. The next 4
methods all require a 100 iteration of self-training.
No-punc is the method that only uses self-training
while no punctuation information is added. No-
balance is similar to ADD N. The only difference
between No-balance and ADD-N is that the for-
mer does not balance label ?B? and label ?N?.
The comparison of Maxent and No-punctuation
shows that naively adding confident unlabeled in-
stances does not guarantee to improve perfor-
mance. The writing style and word formation of
the source domain is different from target domain.
When segmenting texts of the target domain using
models trained on source domain, the performance
will be hurt with more false segmented instances
added into the training set.
The comparison of Maxent, No-balance and
ADD-N shows that considering punctuation as
well as self-training does improve performance.
Both the f-score and OOV-recall increase. By
comparing No-balance and ADD-N alone we can
find that we achieve relatively high f-score if we
ignore tag balance issue, while slightly hurt the
OOV-Recall. However, considering it will im-
prove OOV-Recall by about +1.6% and the f-
score +0.2%.
We also experimented on different size of un-
labeled data to evaluate the performance when
adding unlabeled target domain data. TABLE 5
shows different f-scores and OOV-Recalls on dif-
ferent unlabeled data set.
We note that when the number of texts changes
from 0 to 50,000, the f-score and OOV both are
improved. However, when unlabeled data changes
to 200,000, the performance is a bit decreased,
while still better than not using unlabeled data.
This result comes from the fact that the method
?ADD-N? only uses characters behind punctua-
179
Size P R F OOV-R
0 0.864 0.846 0.855 0.754
10000 0.872 0.869 0.871 0.765
50000 0.875 0.875 0.875 0.773
100000 0.874 0.879 0.876 0.772
200000 0.865 0.865 0.865 0.759
Table 5: Segmentation performance with different
size of unlabeled data
tions from target domain. Taking more texts into
consideration means selecting more characters la-
beling ?N? from source domain to simulate those
in target domain. If too many ?N?s are introduced,
the training data will be biased against the true dis-
tribution of target domain.
3.3 Characters ahead of punctuations
In the ?BN? tagging method mentioned above,
we incorporate characters after punctuations from
texts in micro-blog to enlarge training set.We also
try an opposite approach, ?EN? tag, which uses
?E? to represent ?End of word?, and ?N? to rep-
resent ?Not the end of word?. In this contrasting
method, we only use characters just ahead of punc-
tuations. We find that the two methods show sim-
ilar results. Experiment results with ADD-N are
shown in TABLE 6 .
Unlabeled ?BN? tag ?EN? tag
Data size F OOV-R F OOV-R
50000 0.875 0.773 0.870 0.763
Table 6: Comparison of BN and EN.
4 Related Work
Recent studies show that character sequence la-
beling is an effective formulation of Chinese
word segmentation (Low et al, 2005; Zhao et al,
2006a,b; Chen et al, 2006; Xue, 2003). These
supervised methods show good results, however,
are unable to incorporate information from new
domain, where OOV problem is a big challenge
for the research community. On the other hand
unsupervised word segmentation Peng and Schu-
urmans (2001); Goldwater et al (2006); Jin and
Tanaka-Ishii (2006); Feng et al (2004); Maosong
et al (1998) takes advantage of the huge amount
of raw text to solve Chinese word segmentation
problems. However, they usually are less accurate
and more complicated than supervised ones.
Meanwhile semi-supervised methods have been
applied into NLP applications. Bickel et al (2007)
learns a scaling factor from data of source domain
and use the distribution to resemble target do-
main distribution. Wu et al (2009) uses a Domain
adaptive bootstrapping (DAB) framework, which
shows good results on Named Entity Recognition.
Similar semi-supervised applications include Shen
et al (2004); Daume? III and Marcu (2006); Jiang
and Zhai (2007); Weinberger et al (2006). Be-
sides, Sun and Xu (2011) uses a sequence labeling
framework, while unsupervised statistics are used
as discrete features in their model, which prove to
be effective in Chinese word segmentation.
There are previous works using punctuations as
implicit annotations. Riley (1989) uses it in sen-
tence boundary detection. Li and Sun (2009) pro-
posed a compromising solution to by using a clas-
sifier to select the most confident characters. We
do not follow this approach because the initial er-
rors will dramatically harm the performance. In-
stead, we only add the characters after punctua-
tions which are sure to be the beginning of words
(which means labeling ?B?) into our training set.
Sun and Xu (2011) uses punctuation information
as discrete feature in a sequence labeling frame-
work, which shows improvement compared to the
pure sequence labeling approach. Our method
is different from theirs. We use characters after
punctuations directly.
5 Conclusion
In this paper we have presented an effective yet
simple approach to Chinese word segmentation on
micro-blog texts. In our approach, punctuation in-
formation of unlabeled micro-blog data is used,
as well as a self-training framework to incorpo-
rate confident instances. Experiments show that
our approach improves performance, especially in
OOV-recall. Both the punctuation information and
the self-training phase contribute to this improve-
ment.
Acknowledgments
This research was partly supported by Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) (No.
2012AA011101), National Natural Science Foun-
dation of China (No.91024009) and Major
National Social Science Fund of China(No.
12&ZD227).
180
References
Bickel, S., Bru?ckner, M., and Scheffer, T. (2007).
Discriminative learning for differing training
and test distributions. In Proceedings of the 24th
international conference on Machine learning,
pages 81?88. ACM.
Chen, W., Zhang, Y., and Isahara, H. (2006). Chi-
nese named entity recognition with conditional
random fields. In 5th SIGHAN Workshop on
Chinese Language Processing, Australia.
Daume? III, H. and Marcu, D. (2006). Domain
adaptation for statistical classifiers. Journal of
Artificial Intelligence Research, 26(1):101?126.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75?93.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
of the Association for Computational Linguis-
tics, pages 673?680. Association for Computa-
tional Linguistics.
Jiang, J. and Zhai, C. (2007). Instance weight-
ing for domain adaptation in nlp. In Annual
Meeting-Association For Computational Lin-
guistics, volume 45, page 264.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsuper-
vised segmentation of chinese text by use of
branching entropy. In Proceedings of the COL-
ING/ACL on Main conference poster sessions,
pages 428?435. Association for Computational
Linguistics.
Li, Z. and Sun, M. (2009). Punctuation as im-
plicit annotations for chinese word segmenta-
tion. Computational Linguistics, 35(4):505?
512.
Low, J., Ng, H., and Guo, W. (2005). A maximum
entropy approach to chinese word segmenta-
tion. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing,
volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265?1271. Association for Computa-
tional Linguistics.
Pan, S. and Yang, Q. (2010). A survey on trans-
fer learning. Knowledge and Data Engineering,
IEEE Transactions on, 22(10):1345?1359.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
vances in Intelligent Data Analysis, pages 238?
247.
Riley, M. (1989). Some applications of tree-based
modelling to speech and language. In Pro-
ceedings of the workshop on Speech and Nat-
ural Language, pages 339?352. Association for
Computational Linguistics.
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan,
C. (2004). Multi-criteria-based active learning
for named entity recognition. In Proceedings
of the 42nd Annual Meeting on Association for
Computational Linguistics, page 589. Associa-
tion for Computational Linguistics.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970?979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 253?262, Jeju Island, Korea. Association
for Computational Linguistics.
Weinberger, K., Blitzer, J., and Saul, L. (2006).
Distance metric learning for large margin near-
est neighbor classification. In In NIPS. Citeseer.
Wu, D., Lee, W., Ye, N., and Chieu, H. (2009).
Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009
Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3-Volume
3, pages 1523?1532. Association for Computa-
tional Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29?48.
Zhao, H., Huang, C., and Li, M. (2006a). An im-
proved chinese word segmentation system with
181
conditional random field. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language
Processing, volume 117. Sydney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field model-
ing. In Proceedings of PACLIC, volume 20,
pages 87?94.
182
A Pipeline Approach to Chinese Personal Name
Disambiguation
Yang Song, Zhengyan He, Chen Chen, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
{ysong, hezhengyan, chenchen, wanghf}@pku.edu.cn
Abstract
In this paper, we describe our sys-
tem for Chinese personal name dis-
ambiguation task in the first CIPS-
SIGHAN joint conference on Chinese
Language Processing(CLP2010). We
use a pipeline approach, in which pre-
processing, unrelated documents dis-
carding, Chinese personal name exten-
sion and document clustering are per-
formed separately. Chinese personal
name extension is the most important
part of the system. It uses two addi-
tional dictionaries to extract full per-
sonal names in Chinese text. And then
document clustering is performed un-
der different personal names. Exper-
imental results show that our system
can achieve good performances.
1 Introduction
Personal name search is one of the most im-
portant tasks for search engines. When a per-
sonal name query is given to a search engine,
a list of related documents will be shown. But
not all of the returned documents refer to the
same person whom users want to find. For ex-
ample, the query name ?jordan? is submitted
to a search engine, we can get a lot of doc-
uments containing ?jordan?. Some of them
may refer to the computer scientist, others
perhaps refer to the basketball player. For
English, there have been three Web People
Search (WePS1) evaluation campaigns on per-
sonal name disambiguation. But for Chinese,
1http://nlp.uned.es/weps/
this is the first time. It encounters more chal-
lenge for Chinese personal name disambigua-
tion. There are no word boundary in Chinese
text, so it becomes difficult to recognize the
full personal names from Chinese text. For ex-
ample, a query name ???? is given, but the
full personal name from some documents may
be an extension of ????, like ????? or
?????, and so on. Meanwhile, ???? can
also be a common Chinese word. So we need
to discard those documents which are not ref-
ered to any person related to the given query
name.
To solve the above-mentioned problem, we
explore a pipeline approach to Chinese per-
sonal name disambiguation. The overview of
our system is illustrated in Figure 1. We split
this task into four parts: preprocessing, unre-
lated documents discarding, Chinese personal
name extension and document clustering. In
preprocessing and unrelated documents dis-
carding, we use word segmentation and part-
of-speech tagging tools to process the given
dataset and documents are discarded when
the given query name is not tagged as a per-
sonal name or part of a personal name. After
that we perform personal name extension in
the documents for a given query name. When
the query name has only two characters. We
extend it to the left or right for one character.
For example, we can extend ???? to ???
?? or ?????. The purpose of extending
the query name is to obtain the full personal
name. In this way, we can get a lot of full per-
sonal names for a given query name from the
documents. And then document clustering
Figure 1: Overview of the System
is performed under different personal names.
HAC (Hierarchical Agglomerative Clustering)
is selected here. We represent documents with
bag of words and solve the problem in vector
space model, nouns, verbs, bigrams of nouns
or verbs and named entities are selected as
features. The feature weight value takes 0 or
1. In HAC, we use group-average link method
as the distance measure and consine similar-
ity as the similarity computing measure. The
stopping criteria is dependent on a threshold
which is obtained from training data. Our sys-
tem produces pretty good results in the final
evaluation.
The remainder of this paper is organized as
follows. Section 2 introduces related work.
Section 3 gives a detailed description about
our pipeline approach. It includes preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering. Section 4 presents the experimental
results. The conclusions are given in Section
5.
2 Related Work
Several important studies have tried to
solve the task introduced in the previous sec-
tion. Most of them treated it as an cluster-
ing problem. Bagga & Baldwin (1998) first
selected tokens from local context as features
to perform intra-document coreference resolu-
tion. Mann & Yarowsky (2003) extracted lo-
cal biographical information as features. Niu
et al (2004) used relation extraction results
in addition to local context features and get a
perfect results. Al-Kamha and Embley (2004)
clustered search results with feature set in-
cluding attributes, links and page similarities.
In recent years, this problem has attracted
a great deal of attention from many research
institutes. Ying Chen et al (2009) used a
Web 1T 5-gram corpus released by Google
to extract additional features for clustering.
Masaki Ikeda et al (2009) proposed a two-
stage clustering algorithm to improve the low
recall values. In the first stage, some reliable
features (like named entities) are used to con-
nect documents about the same person. Af-
ter that, the connected documents (document
cluster) are used as a source from which new
features (compound keyword features) are ex-
tracted. These new features are used in the
second stage to make additional connections
between documents. Their approach is to im-
prove clusters step by step, where each step
refines clusters conservatively. Han & Zhao
(2009) presented a system named CASIANED
to disambiguate personal names based on pro-
fessional categorization. They first catego-
rize different personal name appearances into
a real world professional taxonomy, and then
the personal name appearances are clustered
into a single cluster. Chen Chen et al (2009)
explored a novel feature weight computing
method in clustering. It is based on the point-
wise mutual information between the ambigu-
ous name and features. In their paper, they
also develop a trade-off point based cluster
stopping criteria which find the trade-off point
between intra-cluster compactness and inter-
cluster separation.
Our approach is based on Chinese per-
sonal name extension. We recognize the full
personal names in Chinese text and perform
document clustering under different personal
names.
3 Methodology
In this section, we will explain preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering in order.
3.1 Preprocessing
We use ltp-service2 to process the given Chi-
nese personal name disambiguation dataset
(a detailed introduction to it will be given
in section 4). Training data in the dataset
contains 32 query names. There are 100-300
documents under every query name. All the
documents are collected from Xinhua News
Agency. They contain the exact same string
as query names. Ltp-service is a web ser-
vice interface for LTP3(Language Technology
Platform). LTP has integrated many Chinese
processing modules, including word segmen-
tation, part-of-speech tagging, named entity
recognition, word sense disambiguation, and
so on. Jun Lang et al (2006) give a detailed
introduction to LTP. Here we only use LTP
to generate word segmentation, part-of-speech
tagging and named entity recognition results
for the given dataset.
3.2 Unrelated documents discarding
Under every query name, there are 100-300
documents. But not all of them are really re-
lated. For example, ???? is a query name in
training data. In corresponding documents,
some are refered to real personal names like
???? or ?????. But others may be a sub-
string of an expression such as ??????
??. These documents are needed to be fil-
tered out. We use the preprocessing tool LTP
to slove this problem. LTP can do word seg-
mentation and part-of-speech tagging for us.
For each document under a given query name,
if the query name in the document is tagged as
a personal name or part of some extended per-
sonal name, the document will be marked as
undiscarded, otherwise the document will be
discarded. Generally speaking, for the query
name containing three characters, we don?t
need to discard any of the corresponding doc-
uments. But in practice, we find that for some
query names, LTP always gives the invariable
2http://code.google.com/p/ltp-service/
3http://ir.hit.edu.cn/ltp/
part-of-speech. For example, no matter what
the context of ???? is, it is always tagged
as a geographic name. So we use another pre-
processing tool ICTCLAS4. Only when both
of them mark one document as discarded, we
discard the corresponding document.
3.3 Chinese personal name extension
After discarding unrelated documents, we
need to recognize the full Chinese personal
names. We hypothesize that the full Chinese
personal name has not more than three char-
acters (We don?t consider the compound sur-
names here). So the query names containing
only two Chinese characters are considered to
extend. In our approach, we use two Chinese
personal names dictionaries. One is a sur-
name dictionary containing 423 one-character
entries. We use it to do left extend for the
query name. For example, the query name
is ???? and its left character in a docu-
ment is ???, we will extend it to full per-
sonal name ?????. The other is a non-
ending Chinese character dictionary contain-
ing 64 characters which could not occur at the
end of personal names. It is constructed by a
personal title dictionary. We use every title?s
first character and some other special charac-
ters (such as numbers or punctuations) to con-
stuct the dictionary. Some manual work has
also been done to filter a few incorrect charac-
ters. Several examples of the two dictionaries
are shown in Table 1.
Through the analysis of Xinhua News arti-
cles, we also find that nearly half of the docu-
ments under given query name actually refer
to the reporters. And they often appear in
the first or last brackets in the body of cor-
responding document. For example, ?(??
????????)? is a sentence containing
query name ????. We use some simple but
efficient rules to get full personal names for
this case.
3.4 Document clustering
For every query name, we can get a list of
full peronal names. For example, when the
4http://ictclas.org/
Table 1: Several Examples of the two Dictionaries
Dictionaries Examples
Surnames ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?...
Non-ending Chinese characters ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?...
query name is ????, we can get the per-
sonal names like ?????, ?????, ???
??, ?????. And then document clustering
is performed under different personal names.
3.4.1 Features
We use bag of words to represent docu-
ments. Some representative words need to be
chosen as features. LTP can give us POS tag-
ging and NER results. We select all the nouns,
verbs and named entities which appear in the
same paragraph with given query name as fea-
tures. Meanwhile, the bigrams of nouns or
verbs are also selected. We take 0 or 1 for
feature weight value. 0 represents that the
feature doesn?t appear in corresponding para-
graphs, and 1 represents just the opposite. We
find that this weighting scheme is more effec-
tive than TFIDF.
3.4.2 Clustering
All features are represented in vector space
model. Every document is modeled as a ver-
tex in the vector space. So every document
can be seen as a feature vector. Before cluster-
ing, the similarity between documents is com-
puted by cosine value of the angle between
feature vectors. We use HAC to do document
clustering. It is a bottom-up algorithm which
treats each document as a singleton cluster at
the outset and then successively merges (or
agglomerates) pairs of clusters until all clus-
ters have been merged into a single cluster
that contains all documents. From our ex-
perience, single link and group-average link
method seem to work better than complete
link one. We use group-average link method
in the final submission. The stopping criteria
is a difficult problem for clustering. Here we
use a threshold for terminating condition. So
it is not necessary to determine the number
of clusters beforehand. We select a threshold
which produces the best performance in train-
ing data.
4 Experimental Results
The dataset for Chinese personal name dis-
ambiguation task contains training data and
testing data. The training data contains
32 query names. Every query name folder
contains 100-300 news articles. Given the
query name, all the documents are retrived
by character-based matching from a collection
of Xinhua news documents in a time span of
fourteen years. The testing data contains 25
query names. Two threshold values as termi-
nating conditions are obtained from training
data. They are 0.4 and 0.5. For evaluation,
we use P-IP score and B-cubed score (Bagga
and Baldwin, 1998). Table 2 & Table 3 show
the official evaluation results.
Table 2: Official Results for P-IP score
Threshold P-IP
P IP F score
0.4 88.32 94.9 91.15
0.5 91.3 91.77 91.18
Table 3: Official Results for B-Cubed score
Threshold B-Cubed
Precision Recall F score
0.4 83.68 92.23 86.94
0.5 87.87 87.49 86.84
Besides the formal evaluation, the organizer
also provide a diagnosis test designed to ex-
plore the relationship between Chinese word
segmentation and personal name disambigua-
tion. That means the query names in the
documents are segmented correctly by manual
work. Table 4 & Table 5 show the diagnosis
results.
Table 4: Diagnosis Results for P-IP score
Threshold P-IP
P IP F score
0.4 89.01 95.83 91.96
0.5 91.85 92.68 91.96
Table 5: Diagnosis Results for B-Cubed score
Threshold B-Cubed
Precision Recall F score
0.4 84.53 93.42 87.96
0.5 88.59 88.59 87.8
The official results show that our method
performs pretty good. The diagnosis results
show that correct word segmentation can im-
prove the evaluation results. But the improve-
ment is rather limited. That is mainly because
Chinese personal name extension is done well
in our approach. So the diagnosis results don?t
gain much profit from query names? correct
segmentation.
5 Conclusions
We describe our framework in this paper.
First, we use LTP to do preprocessing for orig-
inal dataset which comes from Xinhua news
articles. LTP can produce good results for
Chinese text processing. And then we use
two additional dictionaries(one is Chinese sur-
name dictionary, the other is Non-ending Chi-
nese character dictionary) to do Chinese per-
sonal name extension. After that we perform
document clustering under different personal
names. Official evaluation results show that
our method can achieve good performances.
In the future, we will attempt to use other
features to represent corresponding persons in
the documents. We will also investigate auto-
matic terminating condition.
6 Acknowledgments
This research is supported by National
Natural Science Foundation of Chinese
(No.60973053) and Research Fund for the
Doctoral Program of Higher Education of
China (No.20090001110047).
References
J. Artiles, J. Gonzalo, and S. Sekine. 2009. WePS
2 evaluation campaign: overview of the web peo-
ple search clustering task. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Bagga and B. Baldwin. 1998. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of 17th Interna-
tional Conference on Computational Linguis-
tics, 79?85.
Mann G. and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In Proceedings
of CoNLL-2003, 33?40, Edmonton, Canada.
C. Niu, W. Li, and R. K. Srihari. 2004. Weakly
Supervised Learning for Cross-document Person
Name Disambiguation Supported by Informa-
tion Extraction. In Proceedings of ACL 2004.
Al-Kamha. R. and D. W. Embley. 2004. Group-
ing search-engine returned citations for person-
name queries. In Proceedings of WIDM 2004,
96-103, Washington, DC, USA.
Ying Chen, Sophia Yat Mei Lee, and Chu-Ren
Huang. 2009. PolyUHK:A Robust Information
Extraction System for Web Personal Names.
In 2nd Web People Search Evaluation Work-
shop(WePS 2009), 18th WWW Conference.
Masaki Ikeda, Shingo Ono, Issei Sato, Minoru
Yoshida, and Hiroshi Nakagawa. 2009. Person
Name Disambiguation on the Web by Two-Stage
Clustering. In 2nd Web People Search Evalua-
tion Workshop(WePS 2009), 18th WWW Con-
ference.
Xianpei Han and Jun Zhao. 2009. CASIANED:
Web Personal Name Disambiguation Based on
Professional Categorization. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Chen Chen, Junfeng Hu, and Houfeng Wang.
2009. Clustering technique in multi-document
personal name disambiguation. In Proceed-
ings of the ACL-IJCNLP 2009 Student Research
Workshop, pages 88?95.
Jun Lang, Ting Liu, Huipeng Zhang and Sheng Li.
2006. LTP: Language Technology Platform. In
Proceedings of SWCL 2006.
Bagga, Amit and B. Baldwin. 1998. Algorithms
for scoring co-reference chains. In Proceedings
of the First International Conference on Lan-
guage Resources and Evaluation Workshop on
Linguistic co-reference.
Applying Spectral Clustering for Chinese Word Sense Induction
Zhengyan He, Yang Song, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
{hezhengyan, ysong, wanghf}@pku.edu.cn
Abstract
Sense Induction is the process of identify-
ing the word sense given its context, often
treated as a clustering task. This paper ex-
plores the use of spectral cluster method
which incorporates word features and n-
gram features to determine which cluster
the word belongs to, each cluster repre-
sents one sense in the given document set.
1 Introduction
Word Sense Induction(WSI) is defined as the
process of identifying different senses of a tar-
get word in a given context in an unsupervised
method. It?s different from word sense disam-
biguation(WSD) in that senses in WSD are as-
sumed to be known. The disadvantage of WSD
is that it derives the senses of word from existing
dictionaries or other corpus and the senses cannot
be extended to other domains. WSI can overcome
this problem as it can automatically derive word
senses from the given document set, or a specific
domain.
Many different approaches based on co-
occurence have been proposed so far. Bordag
(2006) proposes an approach that uses triplets
of co-occurences. The most significant co-
occurences of target word are used to build triplets
that consist of the target word and its two co-
occurences. Then intersection built from the co-
occurence list of each word in the triplet is used
as feature vector. After merging similar triplets
that have more than 80% overlapping words, clus-
tering is performed on the triplets. Triplets with
fewer than 4 intersection words are removed in or-
der to reduce noise.
LDA model has also been applied to WSI
(Brody and Lapata, 2009). Brody proposes a
method that treats document and topics in LDA
as word context and senses respectively. The pro-
cess of generating the context words is as follows:
first generate sense from a multinomial distribu-
tion given context, then generate context words
given sense. They also derive a layered model
to incorporate different kind of features and use
Gibbs sampling method to solve the problem.
Graph-based methods become popular recently.
These methods use the co-occurence graph of
context words to obtain sense clusters based on
sub-graph density. Markov clustering(MCL) has
been used to identify dense regions of graph
(Agirre and Soroa, 2007).
Spectral clustering performs well on problems
in which points cluster based on shape. The
method is that first compute the Laplace matrix
of the affinity matrix, then reform the data points
by stacking the largest eigenvectors of the Laplace
matrix in columns, finally cluster the new data
points using a more simple clustering method like
k-means (Ng et al, 2001).
2 Methodology
Our approach follows a common cluster model
that represents the given context as a word vec-
tor and later uses a spectral clustering method to
group each instance in its own cluster.
Different types of polysemy may arise and the
most significant distinction may be the syntactic
classes of the word and the conceptually differ-
ent senses (Bordag, 2006). Thus we must extract
the features able to distinguish these differences.
They are:
Local tokens: the word occuring in the window
-3 ? +3;
Local bigram feature: bigram within -5 ? +5
Chinese character range;
The above two features model the syntactic us-
age of a specific sense of a Chinese word.
Topical or conceptual feature: the content
words (pos-tagged as noun, verb, adjective) within
the given sentence. As the sentence in the training
set seems generally short, a short window may not
contains enough infomation.
We represent the words in a 0-1 vector accord-
ing to their existence in a given sentence. Then the
similarity measure between two given sentences is
derived from their cosine similarity. We find that it
is difficult to define the relative importance of dif-
ferent types of features in order to combine them
in one vector space, and find that ignoring weight
achieve better result. Brody (2009) achieves this
in LDA model through a layered model with dif-
ferent probability of feature given sense.
Later we use a spectral clustering method from
R kernlab package (Karatzoglou et al, 2004)
which implements the algorithm described in (Ng
et al, 2001). Instead of using the Gaussian kernel
matrix as the similarity matrix we use the cosine
similarity derived above.
One observation is that instances with the same
target word sense often appear in the same con-
text. However, for some verb in Chinese, it is of-
ten the case that one sense relates to a concrete
object while the other relates to a more broad and
abstract concept and the context varies consider-
ably. Simple word co-occurence cannot define a
good similarity measure to group these cases into
one cluster. We must consider semantic related-
ness measures between contexts.
3 Performance
Our system performs well on the training set. Two
methods are used to evaluate the performance un-
der different features.
method precision recall F-score
Purity-based 81.11 83.19 81.99
B-cubed 74.41 76.51 75.33
Table 1: The performance of training set
Our system finally gets a F-score of 0.7598 on
the test set.
4 Conclusion
Our experiment in the Chinese word sense induc-
tion task performs good with respect to the relative
small corpus(only the training set). But only con-
sidering token co-occurence cannot achieve better
result. Moreover, it is difficult to define a simi-
larity measure solely based on lexicon infomation
with no regard to semantic relatedness. Finally,
combining different types of features seems to be
another challenge in our model.
5 Acknowledgments
This research is supported by National Natural
Science Foundation of Chinese (No.9092001).
References
Agirre, Eneko and Aitor Soroa. 2007. Ubc-as: a graph
based unsupervised system for induction and classi-
fication. In SemEval ?07: Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 346?349, Morristown, NJ, USA. Association
for Computational Linguistics.
Bordag, Stefan. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. In
EACL. The Association for Computer Linguistics.
Brody, Samuel and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL, pages 103?111.
The Association for Computer Linguistics.
Karatzoglou, Alexandros, Alex Smola, Kurt Hornik,
and Achim Zeileis. 2004. kernlab ? an S4 pack-
age for kernel methods in R. Journal of Statistical
Software, 11(9):1?20.
Ng, Andrew Y., Michael I. Jordan, and Yair Weiss.
2001. On spectral clustering: Analysis and an al-
gorithm. In Advances in Neural Information Pro-
cessing Systems 14, pages 849?856. MIT Press.
