Improving Word Alignment Quality using Morpho-syntactic
Information
Maja Popovic? and Hermann Ney
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University
Ahornstrasse 55
52056 Aachen
Germany
{popovic,ney}@cs.rwth-aachen.de
Abstract
In this paper, we present an approach to
include morpho-syntactic dependencies into
the training of the statistical alignment
models. Existing statistical translation sys-
tems usually treat different derivations of
the same base form as they were indepen-
dent of each other. We propose a method
which explicitly takes into account such in-
terdependencies during the EM training of
the statistical alignment models. The eval-
uation is done by comparing the obtained
Viterbi alignments with a manually anno-
tated reference alignment. The improve-
ments of the alignment quality compared
to the, to our knowledge, best system are
reported on the German-English Verbmobil
corpus.
1 Introduction
In statistical machine translation, a translation
model Pr(fJ1 |eI1) describes the correspondences
between the words in the source language sen-
tence fJ1 and the words in the target language
sentence eI1. Statistical alignment models are
created by introducing a hidden variable aJ1
representing a mapping from the source word
fj into the target word eaj . So far, most of
the statistical machine translation systems are
based on the single-word alignment models as
described in (Brown et al, 1993) as well as the
Hidden Markov alignment model (Vogel et al,
1996). The lexicon models used in these systems
typically do not include any linguistic or con-
textual information which often results in inad-
equate alignments between the sentence pairs.
In this work, we propose an approach to im-
prove the quality of the statistical alignments
by taking into account the interdependencies of
different derivations of the words. We are get-
ting use of the hierarchical representation of the
statistical lexicon model as proposed in (Nie?en
and Ney, 2001) for the conventional EM training
procedure. Experimental results are reported
for the German-English Verbmobil corpus and
the evaluation is done by comparing the ob-
tained Viterbi alignments after the training of
conventional models and models which are using
morpho-syntactic information with a manually
annotated reference alignment.
2 Related Work
The popular IBM models for statistical ma-
chine translation are described in (Brown et
al., 1993) and the HMM-based alignment model
was introduced in (Vogel et al, 1996). A good
overview of all these models is given in (Och
and Ney, 2003) where the model IBM-6 is also
introduced as the log-linear interpolation of the
other models.
Context dependencies have been introduced
into the training of alignments in (Varea et al,
2002), but they do not take any linguistic infor-
mation into account.
Some recent publications have proposed the
use of morpho-syntactic knowledge for statisti-
cal machine translation, but mostly only for the
preprocessing step whereas training procedure
of the statistical models remains the same (e.g.
(Nie?en and Ney, 2001a)).
Incorporation of the morpho-syntactic knowl-
egde into statistical models has been dealt
in (Nie?en and Ney, 2001): hierarchical lexi-
con models containing base forms and set of
morpho-syntactic tags are proposed for the
translation from German into English. How-
ever, these lexicon models are not used for the
training but have been created from the Viterbi
alignment obtained after the usual training pro-
cedure.
The use of POS information for improving
statistical alignment quality of the HMM-based
model is described in (Toutanova et al, 2002).
They introduce additional lexicon probability
for POS tags in both languages, but actually
are not going beyond full forms.
3 Statistical Alignment Models
The goal of statistical machine translation is to
translate an input word sequence f1, . . . , fJ in
the source language into a target language word
sequence e1, . . . , eI . Given the source language
sequence, we have to choose the target language
sequence that maximises the product of the lan-
guage model probability Pr(eI1) and the trans-
lation model probability Pr(fJ1 |eI1). The trans-
lation model describes the correspondence be-
tween the words in the source and the target
sequence whereas the language model describes
well-formedness of a produced target sequence.
The translation model can be rewritten in the
following way:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1)
where aJ1 are called alignments and represent
a mapping from the source word position j to
the target word position i = aj . Alignments
are introduced into translation model as a hid-
den variable, similar to the concept of Hidden
Markov Models (HMM) in speech recognition.
The translation probability Pr(fJ1 , aJ1 |eI1) can
be further rewritten as follows:
Pr(fJ1 , aJ1 |eI1) =
J?
j=1
Pr(fj , aj |f j?11 , aj?11 , eI1)
=
J?
j=1
Pr(aj |f j?11 , aj?11 , eI1) ?
?Pr(fj |f j?11 , aj1, eI1)
where Pr(aj |f j?11 , aj?11 , eI1) is called alignment
probability and Pr(fj |f j?11 , aj1, eI1) is lexicon
probability.
In all popular translation models IBM-1 to
IBM-5 as well as in HMM translation model,
the lexicon probability Pr(fj |f j?11 , aj1, eI1) is ap-
proximated with the simple single-word lexi-
con probability p(fj |eaj ) which takes into ac-
count only full forms of the words fj and
eaj . The difference between these models is
based on the definition of alignment model
Pr(aj |f j?11 , aj?11 , eI1). Detailed description of
those models can be found in (Brown et al,
1993), (Vogel et al, 1996) and (Och and Ney,
2003).
4 Hierarchical Representation of the
Lexicon Model
Typically, the statistical lexicon model is based
only on the full forms of the words and does not
have any information about the fact that some
different full forms are actually derivations of
the same base form. For highly inflected lan-
guages like German this might cause problems
because the coverage of the lexicon might be
low since the token/type ratio for German is
typically much lower than for English (e.g. for
Verbmobil: English 99.4, German 56.3).
To take these interdependencies into account,
we use the hierarchical representation of the sta-
tistical lexicon model as proposed in (Nie?en
and Ney, 2001). A constraint grammar parser
GERCG for lexical analysis and morphological
and syntactic disambiguation for German lan-
guage is used to obtain morpho-syntactic infor-
mation. For each German word, this tool pro-
vides its base form and the sequence of morpho-
syntactic tags, and this information is then
added into the original corpus. For example,
the German word ?gehe? (go), a verb in the
indicative mood and present tense which is de-
rived from the base form ?gehen? is annotated
as ?gehe#gehen-V-IND-PRES#gehen?.
This new representation of the corpus where
full word forms are enriched with its base forms
and tags enables gradual accessing of informa-
tion with different levels of abstraction. Con-
sider for example the above mentioned German
word ?gehe? which can be translated into the
English word ?go?. Another derivation of the
same base form ?gehen? is ?gehst? which also
can be translated by ?go?. Existing statistical
translation models cannot handle the fact that
?gehe? and ?gehst? are derivatives of the same
base form and both can be translated into the
same English word ?go?, whereas the hierarchi-
cal representation makes it possible to take such
interdependencies into account.
5 EM Training
5.1 Standard EM training (review)
In this section, we will briefly review the stan-
dard EM algorithm for the training of the lexi-
con model.
In the E-step the lexical counts are collected
over all sentences in the corpus:
C(f, e) =
?
s
?
a
p(a|f s, es)
?
i,j
?(f, fjs)?(e, eis)
In the M-step the lexicon probabilities are cal-
culated:
p(f |e) = C(f, e)?
f?
C(f? , e)
The procedure is similar for the other model
parameters, i.e. alignment and fertility proba-
bilities.
For models IBM-1, IBM-2 and HMM, an ef-
ficient computation of the sum over all align-
ments is possible. For the other models, the
sum is approximated using an appropriately de-
fined neighbourhood of the Viterbi alignment
(see (Och and Ney, 2003) for details).
5.2 EM training using hierarchical
counts
In this section we describe the EM training of
the lexicon model using so-called hierarchical
counts which are collected from the hierarchi-
caly annotated corpus.
In the E-step the following types of counts are
collected:
? full form counts:
C(f, e) =
?
s
?
a
p(a|f s, es) ?
?
?
i,j
?(f, fjs)?(e, eis)
where f is the full form of the word, e.g.
?gehe?;
? base form+tag counts:
C(fbt, e) =
?
s
?
a
p(a|f s, es) ?
?
?
i,j
?(fbt, fbtjs)?(e, eis)
where fbt represents the base form of the
word f with sequence of corresponding
tags, e.g. ?gehen-V-IND-PRES?;
? base form counts:
C(fb, e) =
?
s
?
a
p(a|f s, es) ?
?
?
i,j
?(fb, fbjs)?(e, eis)
where fb is the base form of the word f ,
e.g. ?gehen?.
For each full form, refined hierarchical counts
are obtained in the following way:
Chier(f, e) = C(f, e) + C(fbt, e) + C(fb, e)
and the M-step is then performed using hier-
archical counts:
p(f |e) = Chier(f, e)?
f?
Chier(f? , e)
The training procedure for the other model
parameters remains unchanged.
6 Experimental Results
We performed our experiments on the Verbmo-
bil corpus. The Verbmobil task (W. Wahlster,
editor, 2000) is a speech translation task in the
domain of appointment scheduling, travel plan-
ning and hotel reservation. The corpus statis-
tics is shown in Table 1. The number of sure and
possible alignments in the manual reference is
given as well. We also used a small training cor-
pus consisting of only 500 sentences randomly
chosen from the main corpus.
We carried out the training scheme
14H5334365 using the toolkit GIZA++.
The scheme is defined according to the number
of iterations for each model. For example, 43
means three iterations of the model IBM-4. We
trained the IBM-1 and HMM model using hier-
archical lexicon counts, and the parameters of
the other models were also indirectly improved
thanks to the refined parameters of the initial
models.
German English
Train Sentences 34446
Words 329625 343076
Vocabulary 5936 3505
Singletons 2600 1305
Test Sentences 354
Words 3233 3109
S relations 2559
P relations 4596
Table 1: Corpus statistics for Verbmobil task
6.1 Evaluation Method
We use the evaluation criterion described in
(Och and Ney, 2000). The obtained word
alignment is compared to a reference alignment
produced by human experts. The annotation
scheme explicitly takes into account the ambi-
guity of the word alignment. The unambiguous
alignments are annotated as sure alignments (S)
and the ambiguous ones as possible alignments
(P ). The set of possible alignments P is used
especially for idiomatic expressions, free trans-
lations and missing function words. The set S
is subset of the set P (S ? P ).
The quality of an alignment A is computed
as appropriately redefined precision and recall
measures. Additionally, we use the alignment
error rate (AER) which is derived from the well-
known F-measure.
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER = 1? |A ? S|+ |A ? P ||A|+ |S|
Thus, a recall error can only occur if a S(ure)
alignment is not found and a precision error
can only occur if a found alignment is not even
P (ossible).
6.2 Alignment Quality Results
Table 2 shows the alignment quality for the two
corpus sizes of the Verbmobil task. Results
are presented for the Viterbi alignments from
both translation directions (German?English
and English?German) as well as for combina-
tion of those two alignments.
The table shows the baseline AER for dif-
ferent training schemes and the corresponding
AER when the hierarchical counts are used. We
see that there is a consistent decrease in AER
for all training schemes, especially for the small
training corpus. It can be also seen that greater
improvements are yielded for the simpler mod-
els.
7 Conclusions
In this work we have presented an approach
for including morpho-syntactic knowledge into
a maximum likelihood training of statistical
translation models. As can be seen in Section
5, going beyond full forms during the training
by taking into account the interdependencies of
the different derivations of the same base form
results in the improvements of the alignment
corpus size = 0.5k
Training Model D ? E E ? D combined
14 ibm1 27.5 33.4 22.7
+hier 24.8 30.3 20.5
14H5 hmm 18.8 24.0 16.9
+hier 16.9 21.5 14.8
14H533 ibm3 18.4 22.8 17.0
+hier 16.7 22.1 15.5
14H53343 ibm4 16.9 21.5 16.2
+hier 15.8 20.7 14.9
14H5334365 ibm6 16.7 21.1 15.9
+hier 15.6 20.9 14.8
corpus size = 34k
Training Model D ? E E ? D combined
14 ibm1 17.6 24.1 14.1
+hier 16.8 21.8 13.7
14H5 hmm 8.9 14.9 7.9
+hier 8.4 13.7 7.3
14H533 ibm3 8.4 12.8 7.7
+hier 8.2 12.7 7.4
14H53343 ibm4 6.3 10.9 6.0
+hier 6.1 10.8 5.7
14H5334365 ibm6 5.7 10.0 5.5
+hier 5.5 9.7 5.0
Table 2: AER [%] for Verbmobil corpus for the
baseline system (name of the model) and the
system using hierarchical method (+hier)
quality, especially for the small training corpus.
We assume that the method can be very effec-
tive for cases where only small amount of data is
available. We also expect further improvements
by performing a special modelling for the rare
words.
We are planning to investigate possibilities
of improving the alignment quality for different
language pairs using different types of morpho-
syntactic information, like for example to use
word stems and suffixes for morphologicaly rich
languages where some parts of the words have
to be aligned to the whole English words (e.g.
Spanish verbs, Finnish in general, etc.) We are
also planning to use the refined alignments for
the translation process.
References
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J. Della Pietra and Robert L. Mercer.
1993. The mathematics of statistical machine
translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311
Ismael Garc??a Varea, Franz Josef Och, Her-
mann Ney and Francisco Casacuberta. 2002.
Improving alignment quality in statistical
machine translation using context-dependent
maximum entropy models. In Proc. of the
19th International Conference on Computa-
tional Linguistics (COLING), pages 1051?
1057, Taipei, Taiwan, August.
Sonja Nie?en and Hermann Ney. 2001a.
Morpho-syntactic analysis for reordering in
statistical machine translation. In Proc. MT
Summit VIII, pages 247?252, Santiago de
Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001. Toward
hierarchical models for statistical machine
translation of inflected languages. In 39th
Annual Meeting of the Assoc. for Computa-
tional Linguistics - joint with EACL 2001:
Proc. Workshop on Data-Driven Machine
Translation, pages 47?54, Toulouse, France,
July.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proc.
of the 38th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
440?447, Hong Kong, October.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguis-
tics, 29(1):19?51
Kristina Toutanova, H. Tolga Ilhan and
Christopher D. Manning. 2002. Extensions
to HMM-based statistical word alignment
models. In Proc. Conf. on Empirical Methods
for Natural Language Processing (EMNLP),
pages 87?94, Philadelphia, PA, July.
Stephan Vogel, Hermann Ney and Cristoph Till-
mann. 1996. HMM-based word alignment
in statistical translation. In Proc. of the
16th International Conference on Computa-
tional Linguistics (COLING) , pages 836?
841, Copenhagen, Denmark, August.
W. Wahlster, editor 2000. Verbmobil: Foun-
dations of speech-to-speech translations.
Springer Verlag, Berlin, Germany, July.
Error Measures and Bayes Decision Rules Revisited
with Applications to POS Tagging
Hermann Ney, Maja Popovic?, David Su?ndermann
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University
Ahornstrasse 55
52056 Aachen, Germany
{popovic,ney}@informatik.rwth-aachen.de
Abstract
Starting from first principles, we re-visit the
statistical approach and study two forms of
the Bayes decision rule: the common rule for
minimizing the number of string errors and a
novel rule for minimizing the number of symbols
errors. The Bayes decision rule for minimizing
the number of string errors is widely used, e.g.
in speech recognition, POS tagging and machine
translation, but its justification is rarely questioned.
To minimize the number of symbol errors as is
more suitable for a task like POS tagging, we show
that another form of the Bayes decision rule can
be derived. The major purpose of this paper is to
show that the form of the Bayes decision rule should
not be taken for granted (as it is done in virtually
all statistical NLP work), but should be adapted
to the error measure being used. We present first
experimental results for POS tagging tasks.
1 Introduction
Meanwhile, the statistical approach to natural
language processing (NLP) tasks like speech
recognition, POS tagging and machine translation
has found widespread use. There are three
ingredients to any statistical approach to NLP,
namely the Bayes decision rule, the probability
models (like trigram model, HMM, ...) and the
training criterion (like maximum likelihood, mutual
information, ...).
The topic of this paper is to re-consider the form
of the Bayes decision rule. In virtually all NLP
tasks, the specific form of the Bayes decision rule
is never questioned, and the decision rule is adapted
from speech recognition. In speech recognition, the
typical decision rule is to maximize the sentence
probability over all possible sentences. However,
this decision rule is optimal for the sentence error
rate and not for the word error rate. This difference
is rarely studied in the literature.
As a specific NLP task, we will consider part-
of-speech (POS) tagging. However, the problem
addressed comes up in any NLP task which is
tackled by the statistical approach and which makes
use of a Bayes decision rule. Other prominent
examples are speech recognition and machine
translation. The advantage of the POS tagging
task is that it will be easier to handle from the
mathematical point of view and will result in closed-
form solutions for the decision rules. From this
point-of-view, the POS tagging task serves as a
good opportunity to illustrate the key concepts of
the statistical approach to NLP.
Related Work: For the task of POS tagging,
statistical approaches were proposed already in the
60?s and 70?s (Stolz et al, 1965; Bahl and Mercer,
1976), before they started to find widespread use
in the 80?s (Beale, 1985; DeRose, 1989; Church,
1989).
To the best of our knowledge, the ?standard?
version of the Bayes decision rule, which minimizes
the number of string errors, is used in virtually all
approaches to POS tagging and other NLP tasks.
There are only two research groups that do not take
this type of decision rule for granted:
(Merialdo, 1994): In the context of POS tagging,
the author introduces a method that he calls
maximum likelihood tagging. The spirit of this
method is similar to that of this work. However, this
method is mentioned as an aside and its implications
for the Bayes decision rule and the statistical
approach are not addressed. Part of this work
goes back to (Bahl et al, 1974) who considered
a problem in coding theory.
(Goel and Byrne, 2003): The error measure
considered by the authors is the word error rate in
speech recognition, i.e. the edit distance. Due to
the mathematical complexity of this error measure,
the authors resort to numeric approximations
to compute the Bayes risk (see next section).
Since this approach does not results in explicit
closed-form equations and involves many numeric
approximations, it is not easy to draw conclusions
from this work.
2 Bayes Decision Rule for Minimum Error
Rate
2.1 The Bayes Posterior Risk
Knowing that any task in NLP tasks is a difficult
one, we want to keep the number of wrong
decisions as small as possible. This point-of-view
has been used already for more than 40 years in
pattern classification as the starting point for many
techniques in pattern classification. To classify an
observation vector y into one out of several classes
c, we resort to the so-called statistical decision
theory and try to minimize the average risk or loss
in taking a decision. The result is known as Bayes
decision rule (Chapter 2 in (Duda and Hart, 1973)):
y ? c? = argmin
c
{
?
c?
Pr(c|y) ? L[c, c?]
}
where L[c, c?] is the so-called loss function or error
measure, i.e. the loss we incur in making decision c
when the true class is c?.
In the following, we will consider two specific
forms of the loss function or error measure L[c, c?].
The first will be the measure for string errors,
which is the typical loss function used in virtually
all statistical approaches. The second is the
measure for symbol errors, which is the more
appropriate measure for POS tagging and also
speech recognition with no insertion and deletion
errors (such as isolated word recognition).
2.2 String Error
For POS tagging, the starting point is the observed
sequence of words y = wN1 = w1...wN , i.e. the
sequence of words for which the POS tag sequence
has c = gN1 = g1...gN has to be determined.
The first error measure we consider is the string
error: the error is equal to zero only if the POS
symbols of the two strings are identical at each
position. In this case, the loss function is:
L[gN1 , g?N1 ] = 1 ?
N
?
n=1
?(gn, g?n)
with the Kronecker delta ?(c, c?). In other words,
the errors are counted at the string level and not
at the level of single symbols. Inserting this cost
function into the Bayes risk (see Section 2.1), we
immediately obtain the following form of Bayes
decision rule for minimum string error:
wN1 ? g?N1 = argmax
gN1
{
Pr(gN1 |wN1 )
}
= argmax
gN1
{
Pr(gN1 , wN1 )
}
This is the starting point for virtually all statistical
approaches in NLP like speech recognition and
machine translation. However, this decision rule is
only optimal when we consider string errors, e.g.
sentence error rate in POS tagging and in speech
recognition. In practice, however, the empirical
errors are counted at the symbol level. Apart
from (Goel and Byrne, 2003), this inconsistency of
decision rule and error measure is never addressed
in the literature.
2.3 Symbol Error
Instead of the string error rate, we can also consider
the error rate of single POS tag symbols (Bahl et
al., 1974; Merialdo, 1994).
This error measure is defined by the loss function:
L[gN1 , g?N1 ] =
N
?
n=1
[1 ? ?(gn, g?n)]
This loss function has to be inserted into the Bayes
decision rule in Section 2.1. The computation of the
expected loss, i.e. the averaging over all classes c? =
g?N1 , can be performed in a closed form. We omit
the details of the straightforward calculations and
state only the result. It turns out that we will need
the marginal (and posterior) probability distribution
Prm(g|wN1 ) at positions m = 1, ..., N :
Prm(g|wN1 ) :=
?
gN1 : gm=g
Pr(gN1 |wN1 )
where the sum is carried out over all POS tag strings
gN1 with gm = g, i.e. the tag gm at position m is
fixed at gm = g. The question of how to perform
this summation efficiently will be considered later
after we have introduced the model distributions.
Thus we have obtained the Bayes decision rule
for minimum symbol error at position m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
Prm(g|wN1 )
}
= argmax
g
{
Prm(g,wN1 )
}
By construction this decision rule has the special
property that it does not put direct emphasis on
local coherency of the POS tags produced. In other
words, this decision rule may produce a POS tag
string which is linguistically less likely.
3 The Modelling Approaches to POS
Tagging
The derivation of the Bayes decision rule assumes
that the probability distribution Pr(gN1 , wN1 ) (or
Pr(gN1 |wN1 )) is known. Unfortunately, this is not
the case in practice. Therefore, the usual approach
is to approximate the true but unknown distribution
by a model distribution p(gN1 , wN1 ) (or p(gN1 |wN1 )).
We will review two popular modelling approaches,
namely the generative model and the direct model,
and consider the associated Bayes decision rules for
both minimum string error and minimum symbol
error.
3.1 Generative Model: Trigram Model
We replace the true but unknown joint distribution
Pr(gN1 , wN1 ) by a model-based probability distribu-
tion p(gN1 , wN1 ):
Pr(gN1 , wN1 ) ? p(gN1 , wN1 ) = p(gN1 ) ? p(wN1 |gN1 )
We apply the so-called chain rule to factorize each
of the distributions p(gN1 ) and p(wN1 |gN1 ) into a
product of conditional probabilities using specific
dependence assumptions:
p(gN1 , wN1 ) =
N
?
n=1
[
p(gn|gn?1n?2) ? p(wn|gn)
]
with suitable definitions for the case n = 1.
Here, the specific dependence assumptions are that
the conditional probabilities can be represented
by a POS trigram model p(gn|gn?1n?2) and a word
membership model p(wn|gn). Thus we obtain
a probability model whose structure fits into
the mathematical framework of so-called Hidden
Markov Model (HMM). Therefore, this approach is
often also referred to as HMM-based POS tagging.
However, this terminology is misleading: The POS
tag sequence is observable whereas in the Hidden
Markov Model the state sequence is always hidden
and cannot be observed. In the experiments, we will
use a 7-gram POS model. It is clear how to extend
the equations from the trigram case to the 7-gram
case.
3.1.1 String Error
Using the above model distribution, we directly
obtain the decision rule for minimum string error:
wN1 ? g?N1 = argmax
gN1
{
p(gN1 , wN1 )
}
Since the model distribution is a basically a second-
order model (or trigram model), there is an efficient
algorithm for finding the most probable POS tag
string. This is achieved by a suitable dynamic
programming algorithm, which is often referred to
as Viterbi algorithm in the literature.
3.1.2 Symbol Error
To apply the Bayes decision rule for minimum
symbol error rate, we first compute the marginal
probability pm(g,wN1 ):
pm(g,wN1 ) =
?
gN1 : gm=g
p(gN1 , wN1 )
=
?
gN1 : gm=g
?
n
[
p(gn|gn?1n?2) ? p(wn|gn)
]
Again, since the model is a second-order model,
the sum over all possible POS tag strings gN1(with gm = g) can be computed efficiently
using a suitable extension of the forward-backward
algorithm (Bahl et al, 1974).
Thus we obtain the decision rule for minimum
symbol error at positions m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
pm(g,wN1 )
}
Here, after the the marginal probability pm(g,wN1 )
has been computed, the task of finding the most
probable POS tag at position m is computationally
easy. Instead, the lion?s share for the computational
effort is required to compute the marginal probabil-
ity pm(g,wN1 ).
3.2 Direct Model: Maximum Entropy
We replace the true but unknown posterior distri-
bution Pr(gN1 |wN1 ) by a model-based probability
distribution p(gN1 |wN1 ):
Pr(gN1 |wN1 ) ? p(gN1 |wN1 )
and apply the chain rule:
p(gN1 |wN1 ) =
N
?
n=1
p(gn|gn?11 , wN1 )
=
N
?
n=1
p(gn|gn?1n?2 , wn+2n?2)
As for the generative model, we have made specific
assumptions: There is a second-order dependence
for the tags gn1 , and the dependence on the words
wN1 is limited to a window wn+2n?2 around position
n. The resulting model is still rather complex
and requires further specifications. The typical
procedure is to resort to log-linear modelling, which
is also referred to as maximum entropy modelling
(Ratnaparkhi, 1996; Berger et al, 1996).
3.2.1 String Error
For the minimum string error, we obtain the
decision rule:
wN1 ? g?N1 = argmax
gN1
{
p(gN1 |wN1 )
}
Since this is still a second-order model, we can use
dynamic programming to compute the most likely
POS string.
3.2.2 Symbol Error
For the minimum symbol error, the marginal
(and posterior) probability pm(g|wN1 ) has to be
computed:
pm(g|wN1 ) =
?
gN1 : gm=g
Pr(gN1 |wN1 )
=
?
gN1 : gm=g
?
n
p(gn|gn?1n?2 , wn+2n?2)
which, due to the specific structure of the model
p(gn|gn?1n?2 , wn+2n?2), can be calculated efficiently
using only a forward algorithm (without a
?backward? part).
Thus we obtain the decision rule for minimum
symbol error at positions m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
pm(g|wN1 )
}
As in the case of the generative model, the
computational effort is to compute the posterior
probability pm(g|wN1 ) rather than to find the most
probable tag at position m.
4 The Training Procedure
So far, we have said nothing about how we train
the free parameters of the model distributions. We
use fairly conventional training procedures that we
mention only for the sake of completeness.
4.1 Generative Model
We consider the trigram-based model. The free
parameters here are the entries of the POS trigram
distribution p(g|g??, g?) and of the word membership
distribution p(w|g). These unknown parameters are
computed from a labelled training corpus, i.e. a
collection of sentences where for each word the
associated POS tag is given.
In principle, the free parameters of the models
are estimated as relative frequencies. For the test
data, we have to allow for both POS trigrams (or n-
grams) and (single) words that were not seen in the
training data. This problem is tackled by applying
smoothing methods that were originally designed
for language modelling in speech recognition (Ney
et al, 1997).
4.2 Direct Model
For the maximum entropy model, the free param-
eters are the so-called ?i or feature parameters
(Berger et al, 1996; Ratnaparkhi, 1996). The
training criterion is to optimize the logarithm
of the model probabilities p(gn|gn?2n?1 , wn+2n?2) over
all positions n in the training corpus. The
corresponding algorithm is referred to as GIS
algorithm (Berger et al, 1996). As usual
with maximum entropy models, the problem of
smoothing does not seem to be critical and is not
addressed explicitly.
5 Experimental Results
Of course, there have already been many papers
about POS tagging using statistical methods. The
goal of the experiments is to compare the two
decision rules and to analyze the differences in
performance. As the results for the WSJ corpus will
show, both the trigram method and the maximum
entropy method have an tagging error rate of 3.0%
to 3.5% and are thus comparable to the best results
reported in the literature, e.g. (Ratnaparkhi, 1996).
5.1 Task and Corpus
The experiments are performed on the Wall Street
Journal (WSJ) English corpus and on the Mu?nster
Tagging Project (MTP) German corpus.
The POS tagging part of The WSJ corpus
(Table 1) was compiled by the University of
Pennsylvania and consists of about one million
English words with manually annotated POS tags.
Text POS
Train Sentences 43508
Words+PMs 1061772
Singletons 21522 0
Word Vocabulary 46806 45
PM Vocabulary 25 9
Test Sentences 4478
Words+PMs 111220
OOVs 2879 0
Table 1: WSJ corpus statistics.
The MTP corpus (Table 2) was compiled at the
University of Mu?nster and contains tagged German
words from articles of the newspapers Die Zeit
and Frankfurter Allgemeine Zeitung (Kinscher and
Steiner, 1995).
For the corpus statistics, it is helpful to
distinguish between the true words and the
punctuation marks (see Table 1 and Table 2). This
distinction is made for both the text and the POS
corpus. In addition, the tables show the vocabulary
size (number of different tokens) for the words and
for the punctuation marks.
Punctuation marks (PMs) are all tokens which
do not contain letters or digits. The total number
of running tokens is indicated as Words+PMs.
Singletons are the tokens which occur only once in
Text POS
Train Sentences 19845
Words+PMs 349699
Singletons 32678 11
Word Vocabulary 51491 68
PM Vocabulary 27 5
Test Sentences 2206
Words+PMs 39052
OOVs 3584 2
Table 2: MTP corpus statistics.
the training data. Out-of-Vocabulary words (OOVs)
are the words in the test data that did not not occur
in the training corpus.
5.2 POS Tagging Results
The tagging experiments were performed for both
types of models, each of them with both types of
the decision rules. The generative model is based on
the approach described in (Su?ndermann and Ney,
2003). Here the optimal value of the n-gram order
is determined from the corpus statistics and has a
maximum of n = 7. The experiments for the direct
model were performed using the maximum entropy
tagger described in (Ratnaparkhi, 1996).
The tagging error rates are showed in Table 3 and
Table 4. In addition to the overall tagging error rate
(Overall), the tables show the tagging error rates for
the Out-of-Vocabulary words (OOVs) and for the
punctuation marks (PMs).
For the generative model, both decision rules
yield similar results. For the direct model, the
overall tagging error rate increases on each of the
two tasks (from 3.0 % to 3.3 % on WSJ and from
5.4 % to 5.6 % on MTP) when we use the symbol
decision rule instead of the string decision rule. In
particular, for OOVs, the error rate goes up clearly.
Right now, we do not have a clear explanation
for this difference between the generative model
and the direct model. It might be related to the
?forward? structure of the direct model as opposed to
the ?forward-backward? structure of the generative
model. Anyway, the refined bootstrap method
(Bisani and Ney, 2004) has shown that differences
in the overall tagging error rate are statistically not
significant.
5.3 Examples
A detailed analysis of the tagging results showed
that for both models there are sentences where the
one decision rule is more efficient and sentences
where the other decision rule is better.
For the generative model, these differences seem
to occur at random, but for the direct model, some
distinct tendencies can be observed. For example,
WSJ Task Decision Overall OOVs PMs
Rule
Generative string 3.5 16.9 0
Model symbol 3.5 16.7 0
Direct string 3.0 15.4 0.08
Model symbol 3.3 16.6 0.1
Table 3: POS tagging error rates [%] for WSJ task.
MTP Task Decision Overall OOVs PMs
Rule
Generative string 5.4 13.4 3.6
Model symbol 5.4 13.4 3.6
Direct string 5.4 12.7 3.8
Model symbol 5.6 13.4 3.7
Table 4: POS tagging error rates [%] for MTP task.
for the WSJ corpus, the string decision rule is
significantly better for the present and past tense of
verbs (VBP, VBN), and the symbol decision rule
is better for adverb (RB) and verb past participle
(VBN). Typical errors generated by the symbol
decision rule are tagging present tense as infinitive
(VB) and past tense as past participle (VBN), and
for string decision rule, adverbs are often tagged as
preposition (IN) or adjective (JJ) and past participle
as past tense (VBD).
For the German corpus, the string decision
rule better handles demonstrative determiners
(Rr) and subordinate conjunctions (Cs) whereas
symbol decision rule is better for definite articles
(Db). The symbol decision rule typically tags
the demonstrative determiner as definite article
(Db) and subordinate conjunctions as interrogative
adverbs (Bi), and the string decision rule tends to
assign the demonstrative determiner tag to definite
articles.
These typical errors for the symbol decision rule
are shown in Table 5, and for the string decision rule
in Table 6.
6 Conclusion
So far, the experimental tests have shown no
improvement when we use the Bayes decision rule
for minimizing the number of symbol errors rather
than the number of string errors. However, the
important result is that the new approach results in
comparable performance. More work is needed to
contrast the two approaches.
The main purpose of this paper has been to show
that, in addition to the widely used decision rule for
minimizing the string errors, it is possible to derive a
decision rule for minimizing the number of symbol
errors and to build up the associated mathematical
framework.
There are a number of open questions for future
work:
1) The error rates for the two decision rules are
comparable. Is that an experimental coincidence?
Are there situations for which we must expect a
significance difference between the two decision
rules? We speculate that the two decision rules
could always have similar performance if the error
rates are small.
2) Ideally, the training criterion should be closely
related to the error measure used in the decision
rule. Right now, we have used the training criteria
that had been developed in the past and that had
been (more or less) designed for the string error rate
as error measure. Can we come up with a training
criterion tailored to the symbol error rate?
3) In speech recognition and machine translation,
more complicated error measures such as the edit
distance and the BLEU measure are used. Is it
possible to derive closed-form Bayes decision rules
(or suitable analytic approximations) for these error
measures? What are the implications?
References
L. Bahl, J. Cocke, F. Jelinek and J. Raviv.
1974. Optimal Decoding of Linear Codes for
Minimizing Symbol Error Rate. IEEE Trans. on
Information Theory, No. 20, pages 284?287
L. Bahl and L. R. Mercer. 1976. Part of Speech
Assignment by a Statistical Decision Algorithm.
In IEEE Symposium on Information Theory,
abstract, pages 88?89, Ronneby, Sweden.
A. D. Beale. 1985. A Probabilistic Approach
to Grammatical Analysis of Written English by
Computer. In 2nd Conf. of the European Chapter
of the ACL, pages 159?169, Geneva, Switzerland.
A. L. Berger, S. Della Pietra and V. Della Pietra.
1996. A Maximum Entropy Approach to
Natural Language Processing. Computational
Linguistics, No. 22, Vol. 1, pages 39?71.
M. Bisani and H. Ney. 2004. Bootstrap Estimates
for Confidence Intervals in ASR Performance
Evaluation. In IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, pages 409?412,
Montreal, Canada.
K. W. Church. 1989. A Stochastic Parts Program
Noun Phrase Parser for Unrestricted Text. In
IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, pages 695?698, Glasgow, Scotland.
S. DeRose. 1989. Grammatical Category Disam-
biguation by Statistical Optimization. Computa-
tional Linguistics, No. 14, Vol. 1, pages 31?39
R. O. Duda and P. E. Hart. 1973. Pattern
Classification and Scene Analysis. John Wiley &
Sons, New York.
V. Goel and W. Byrne. 2003. Minimum Bayes-
risk Automatic Speech Recognition. In W. Chou
and B. H. Juang (editors): Pattern Recognition
in Speech and Language Processing. CRC Press,
Boca Rota, Florida.
J. Kinscher and P. Steiner. 1995. Mu?nster Tagging
Project (MTP). Handout for the 4th Northern
German Linguistic Colloquium, University of
Mu?nster, Internal report.
B. Merialdo. 1994. Tagging English Text with a
Probabilistic Model. Computational Linguistics,
No. 20, Vol. 2, pages 155?168.
H. Ney, S. Martin and F. Wessel. 1997.
Statistical Language Modelling by Leaving-One-
Out. In G. Bloothooft and S. Young (editors):
Corpus-Based Methods in Speech and Language,
pages 174?207. Kluwer Academic Publishers,
Dordrecht.
A. Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Conf.
on Empirical Methods in Natural Language
Processing and Very Large Corpora , pages 133?
142, Sommerset, NJ.
W. S. Stolz, P. H. Tannenbaum and F. V. Carstensen.
1965. Stochastic Approach to the Grammatical
Coding of English. Communications of the ACM,
No. 8, pages 399?405.
D. Su?ndermann and H. Ney. 2003. SYNTHER
- a New m-gram POS Tagger. In Proc. of
the Int. Conf. on Natural Language Processing
and Knowledge Engineering, pages 628?633,
Beijing, China.
VBP ? VB
reference ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...
string ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...
symbol ... investors/NNS already/RB have/VB sharply/RB scaled/VBN ...
reference We/PRP basically/RB think/VBP that/IN ...
string We/PRP basically/RB think/VBP that/IN ...
symbol We/PRP basically/RB think/VB that/IN ...
VBD ? VBN
reference ... plant-expansion/JJ program/NN started/VBD this/DT year/NN ...
string ... plant-expansion/NN program/NN started/VBD this/DT year/NN ...
symbol ... plant-expansion/NN program/NN started/VBN this/DT year/NN ...
reference ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...
string ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...
symbol ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBN agreements/NNS ...
Rr ? Db
reference Das/Db Sandma?nnchen/Ne ,/Fi das/Rr uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
string Das/Db Sandma?nnchen/Ng ,/Fi das/Rr uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
symbol Das/Db Sandma?nnchen/Ng ,/Fi das/Db uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
reference ... fu?r/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...
string ... fu?r/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...
symbol ... fu?r/Po Leute/Ng ,/Fi die/Db glauben/Vf ...
Cs ? Bi
reference Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
string Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
symbol Denke/Vf ich/Rp nach/Qv ,/Fi warum/Bi mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
Table 5: Examples of tagging errors for the symbol decision rule (direct model)
RB ? IN, JJ
reference The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...
string The/DT negotiations/NNS allocate/VBP about/IN 15/CD %/NN ...
symbol The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...
reference ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...
string ... will/MD lead/VB to/TO a/DT much/JJ stronger/JJR performance/NN ...
symbol ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...
VBN ? VBD
reference ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...
string ... by/IN a/DT police/NN officer/NN named/VBD John/NNP Klute/NNP ...
symbol ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...
Db ? Rr
reference er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...
string er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Rr Emotionen/Ng zu/Qi kanalisieren/Vi ...
symbol er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...
Table 6: Examples of tagging errors for the string decision rule (direct model)
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 41?48,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Augmenting a Small Parallel Text with Morpho-syntactic Language
Resources for Serbian-English Statistical Machine Translation
Maja Popovic?, David Vilar, Hermann Ney
Lehrstuhl fu?r Informatik VI
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{popovic,vilar,ney}@informatik.rwth-aachen.de
Slobodan Jovic?ic?, Zoran ?Saric?
Faculty of Electrical Engineering
University of Belgrade
Serbia and Montenegro
jovicic@etf.bg.ac.yu
Abstract
In this work, we examine the quality of
several statistical machine translation sys-
tems constructed on a small amount of
parallel Serbian-English text. The main
bilingual parallel corpus consists of about
3k sentences and 20k running words from
an unrestricted domain. The translation
systems are built on the full corpus as well
as on a reduced corpus containing only
200 parallel sentences. A small set of
about 350 short phrases from the web is
used as additional bilingual knowledge. In
addition, we investigate the use of mono-
lingual morpho-syntactic knowledge i.e.
base forms and POS tags.
1 Introduction and Related Work
The goal of statistical machine translation (SMT) is
to translate a source language sequence f1, . . . , fJ
into a target language sequence e1, . . . , eI by max-
imising the conditional probability Pr(eI1|fJ1 ). This
probability can be factorised into the translation
model probability P (fJ1 |eI1) which describes the
correspondence between the words in the source and
the target sequence, and the language model proba-
bility P (eJ1 ) which describes well-formedness of the
produced target sequence. These two probabilities
can be modelled independently of each other. For
detailed descriptions of SMT models see for exam-
ple (Brown et al, 1993; Och and Ney, 2003).
Translation probabilities are learnt from a bilin-
gual parallel text corpus and language model proba-
bilities are learnt from a monolingual text in the tar-
get language. Usually, the performance of a trans-
lation system strongly depends on the size of the
available training corpus. However, acquisition of
a large high-quality bilingual parallel text for the de-
sired domain and language pair requires lot of time
and effort, and, for many language pairs, is even not
possible. Besides, small corpora have certain advan-
tages - the acquisition does not require too much
effort and also manual creation and correction are
possible. Therefore there is an increasing number of
publications dealing with limited amounts of bilin-
gual data (Al-Onaizan et al, 2000; Nie?en and Ney,
2004).
For the Serbian language, as a rather minor and
not widely studied language, there are not many
language resources available, especially not parallel
texts. On the other side, investigations on this lan-
guage may be quite useful since the majority of prin-
ciples can be extended to the wider group of Slavic
languages (e.g. Czech, Polish, Russian, etc.).
In this work, we exploit small Serbian-English
parallel texts as a bilingual knowledge source for
statistical machine translation. In addition, we in-
vestigate the possibilities for improving the trans-
lation quality using morpho-syntactic information
in the source language. Some preliminary transla-
tion results on this language pair have been reported
in (Popovic? et al, 2004; Popovic? and Ney, 2004),
but no systematic investigation has been done so far.
This work presents several translation systems cre-
ated with different amounts and types of training
data and gives a detailed description of the language
resources used.
41
2 Language Resources
2.1 Language Characteristics
Serbian, as a Slavic language, has a very rich inflec-
tional morphology for all open word classes. There
are six distinct cases affecting not only common
nouns but also proper nouns as well as pronouns,
adjectives and some numbers. Some nouns and ad-
jectives have two distinct plural forms depending on
the number (if it is larger than four or not). There
are also three genders for the nouns, pronouns, ad-
jectives and some numbers leading to differences be-
tween the cases and also between the verb participles
for past tense and passive voice.
As for verbs, person and many tenses are ex-
pressed by the suffix, and the subject pronoun (e.g.
I, we, it) is often omitted (similarly as in Spanish and
Italian). In addition, negation of three quite impor-
tant verbs, ?biti? (to be, auxiliary verb for past tense,
conditional and passive voice), ?imati? (to have) and
?hteti? (to want, auxiliary verb for the future tense),
is done by adding the negative particle to the verb as
a prefix.
As for syntax, Serbian has a quite free word or-
der, and there are no articles, neither indefinite nor
definite.
All these characteristics indicate that morpho-
syntactic knowledge might be very useful for sta-
tistical machine translation involving Serbian lan-
guage, especially when only scarce amounts of par-
allel text are available.
2.2 Parallel Corpora
Finding high-quality bilingual or multilingual paral-
lel corpora involving Serbian language is a difficult
task. For example, there are several web-sites with
the news in both Serbian and English (some of them
in other languages as well), but these texts are only
comparable and not parallel at all. To our knowl-
edge, the only currently available Serbian-English
parallel text suitable for statistical machine trans-
lation is a manually created electronic version of
the Assimil language course which has been used
for some preliminary experiments in (Popovic? et al,
2004; Popovic? and Ney, 2004). We have used this
corpus for systematical investigations described in
this work.
2.2.1 Assimil Language Course
The electronic form of Assimil language course
contains about 3k sentences and 25k running words
of various types of conversations and descriptions as
well as a few short newspaper articles. Detailed cor-
pus statistics can be seen in Table 1. Since the do-
main of the corpus is basically not restricted, the vo-
cabulary size is relatively large. Due to the rich mor-
phology, the vocabulary for Serbian is almost two
times larger than for English. The average sentence
length for Serbian is about 8.5 words per sentence,
and for English about 9.5. This difference is mainly
caused by the lack of articles and omission of some
subject pronouns in Serbian .
The development and test set (500 sentences) are
randomly extracted from the original corpus and the
rest is used for training (referred to as 2.6k).
In order to investigate the scenario with extremely
scarce training material, a reduced training corpus
(referred to as 200) has been created by random ex-
traction of 200 sentences from the original training
corpus.
The morpho-syntactic annotation of the En-
glish part of the corpus has been done by the con-
straint grammar parser ENGCG for morphological
and syntactic analysis of English language. For each
word, this tool provides its base form and sequence
of morpho-syntactic tags.
For the Serbian corpus, to our knowlegde there
is no available tool for automatic annotation of this
language. Therefore, the base forms have been in-
troduced manually and the POS tags have been pro-
vided partly manually and partly automatically us-
ing a statistical maximum-entropy based POS tagger
similar to the one described in (Ratnaparkhi, 1996).
First, the 200 sentences of the reduced training cor-
pus have been annotated completely manually. Then
the first 500 sentences of the rest of the training cor-
pus have been tagged automatically and the errors
have been manually corrected. Afterwards, the POS
tagger has been trained on the extended corpus (700
sentences), the next 500 sentences of the rest are an-
notated, and the procedure has been repeated until
the annotation has been finished for the complete
corpus.
42
Table 1: Statistics of the Serbian-English Assimil corpus
Serbian English
Training: original base forms original no article
full corpus Sentences 2632 2632
(2.6k) Running Words + Punct. 22227 24808 23308
Average Sentence Length 8.4 9.5 8.8
Vocabulary Size 4546 2605 2645 2642
Singletons 2728 1253 1211
reduced corpus Sentences 200 200
(200) Running Words + Punct. 1666 1878 1761
Average Sentence Length 8.3 10.4 8.8
Vocabulary Size 778 596 603 600
Singletons 618 417 395
Dev+Test Sentences 500 500
Running Words + Punct. 4161 4657 4362
Average Sentence Length 8.3 9.3 8.7
Vocabulary Size 1457 1030 1055 1052
Running OOVs - 2.6k 12.1% 5.2% 4.8%
Running OOVs - 200 34.5% 27.6% 21.4%
OOVs - 2.6k 32.7% 19.5% 19.7%
OOVs - 200 76.2% 66.0% 66.8%
External Test Sentences 22 22
Running Words + Punct. 395 446 412
Average Sentence Length 18.0 20.3 18.7
Vocabulary Size 213 176 202 199
Running OOVs - 2.6k 44.3% 35.4% 32.1% 34.7%
Running OOVs - 200 53.7% 44.6% 43.7% 47.3 %
OOVs - 2.6k 61.5% 45.4% 44.0% 44.7%
OOVs - 200 74.6% 63.1% 63.9% 64.8%
Table 2: Statistics of the Serbian-English short phrases
Serbian English
Phrases original base forms original no article
Entries 351 351 351 351
Running Words + Punct. 617 617 730 700
Average Entry Length 1.8 1.8 2.1 2.0
Vocabulary Size 335 303 315 312
Singletons 239 209 209 208
New Running 2.6k 20.6% 14.4% 11.8% 11.8%
Words 200 50.6% 41.3% 36.7% 37.8%
New Vocabulary 2.6k 30.1% 22.1% 21.6% 21.2%
Words 200 70.7% 63.0% 63.2% 63.1%
43
2.2.2 Short Phrases
The short phrases used as an additional bilingual
knowledge source in our experiments have been col-
lected from the web and contain about 350 standard
words and short expressions with an average entry
length of 1.8 words for Serbian and 2 words for En-
glish. Table 2 shows that about 30% of words from
the phrase vocabulary are not present in the origi-
nal Serbian corpus and about 70% of those words
are not contained in the reduced corpus. For the
English language those numbers are smaller, about
20% for the original corpus and 60% for the reduced
one. These percentages are indicating that this par-
allel text, although very scarce, might be an useful
additional training material.
The phrases have also been morpho-syntactically
annotated in the same way as the main corpus.
2.2.3 External Test
In addition to the standard development and test
set described in Section 2.2.1, we also tested our
translation systems on a short external parallel text
collected from the BBC News web-site contain-
ing 22 sentences about relations between USA and
Ukraine after the revolution. As can be seen in Ta-
ble 1, this text contains very large portion of out-
of-vocabulary words (almost two thirds of Serbian
words and almost half of English words are not seen
in the training corpus), and has an average sentence
length about two times larger than the training cor-
pus.
3 Transformations in the Source Language
Standard SMT systems usually regard only full
forms of the words, so that translation of full forms
which have not been seen in the training corpus is
not possible even if the base form has been seen.
Since the inflectional morphology of the Serbian
language is very rich, as described in Section 2.1, we
investigate the use of the base forms instead of the
full forms to overcome this problem for the transla-
tion into English. We propose two types of trans-
formations of the Serbian corpus: conversion of the
full forms into the base forms and additional treat-
ment of the verbs.
For the other translation direction, we propose re-
moving the articles in the English part of the corpus
as the Serbian language does not have any.
3.1 Transformations of the Serbian Text
3.1.1 Base Forms
Serbian full forms of the words usually contain
information which is not relevant for translation into
English. Therefore, we propose conversion of all
Serbian words in their base forms. Although for
some other inflected languages like German and
Spanish this method did not yield any translation
improvement, we still considered it as promising be-
cause the number of Serbian inflections is consider-
ably higher than in the other two languages. Table 1
shows that this transformation significantly reduces
the Serbian vocabulary size so that it becomes com-
parable to the English one.
3.1.2 Treatment of Verbs
Inflections of Serbian verbs might contain rel-
evant information about the person, which is es-
pecially important when the pronoun is omitted.
Therefore, we apply an additional treatment of the
verbs. Whereas all other word classes are still re-
placed only by their base forms, for each verb a part
of the POS tag referring to the person is taken and
the verb is converted into a sequence of this tag and
its base form. For the three verbs described in Sec-
tion 2.1, the separation of the negative particle is also
applied: each negative full form is transformed into
the sequence of the POS tag, negative particle and
base form. The detailed statistics of this corpus is
not reported since there are no significant changes,
only the number of running words and average sen-
tence length increase thus becoming closer to the
values of the English corpus.
3.2 Transformations of the English Text
3.2.1 Removing Articles
Since the articles are one of the most frequent
word classes in English, but on the other side there
are no arcticles at all in Serbian, we propose remov-
ing the articles from the English corpus for trans-
lation into Serbian. Each English word which has
been detected as an article by means of its POS tag
has been removed from the corpus. In Table 1, it
can be seen that this method significantly reduces
the number of running words and the average sen-
tence length of the English corpus thus becoming
comparable to the values of the Serbian corpus.
44
4 Translation Experiments and Results
4.1 Experimental Settings
In order to systematically investigate the impact of
the bilingual training corpus size and the effects
of the morpho-syntactic information on the trans-
lation quality, the translation systems were trained
on the full training corpus (2.6k) and on the re-
duced training corpus (200), both with and with-
out short phrases. The translation is performed in
both directions, i.e. from Serbian to English and
other way round. For the Serbian to English trans-
lation systems, three versions of the Serbian corpus
have been used: original (baseline), base forms only
(sr base) and base forms with additional treatment
of the verbs (sr base+v-pos). For the translation into
Serbian, the systems were trained on two versions of
the English corpus: original (baseline) and without
articles (en no-article).
The baseline translation system is the Alignment
Templates system with scaling factors (Och and
Ney, 2002). Word alignments are produced using
GIZA++ toolkit without symmetrisation (Och and
Ney, 2003). Preprocessing of the source data has
been done before the training of the system, there-
fore modifications of the training and search pro-
cedure were not necessary for the translation of the
transformed source language corpora.
Although the development set has been used to
optimise the scaling factors, results obtained for this
set do not differ from those for the test set. There-
fore only the joint error rates (Development+Test)
are reported.
As for the external test set, results for this text are
reported only for the full corpus systems, since for
the reduced corpus the error rates are higher but the
effects of using phrases and morpho-syntactic infor-
mation are basically the same.
4.2 Translation Results
The evaluation metrics used in our experiments
are WER (Word Error Rate), PER (Position-
independent word Error Rate) and BLEU (BiLin-
gual Evaluation Understudy) (Papineni et al, 2002).
Since BLEU is an accuracy measure, we use 1-
BLEU as an error measure.
4.2.1 Translation from Serbian into English
Error rates for the translation from Serbian into
English are shown in Table 3 and some examples
are shown in Table 6. It can be seen that there is a
significant decrease in all error rates when the full
forms are replaced with their base forms. Since the
redundant information contained in the inflection is
removed, the system can better capture the relevant
information and is capable of producing correct or
approximatively correct translations even for unseen
full forms of the words (marked by ?UNKNOWN ?
in the baseline result example). The treatment of the
verbs yields some additional improvements.
From the first translation example in Table 6 it can
be seen how the problem of some out-of-vocabulary
words can be overcomed with the use of the base
forms. The second and third example are showing
the advantages of the verb treatment, the third one
illustrates the effect of separating the negative parti-
cle.
Reduction of the training corpus to only 200 sen-
tences (about 8% of the original corpus) leads to a
loss of error rates of about 45% relative. However,
the degradation is not higher than 35% if phrases and
morpho-syntactic information are available in addi-
tion to the reduced corpus.
The use of the phrases can improve the transla-
tion quality to some extent, especially for the sys-
tems with the reduced training corpus, but these im-
provements are less remarkable than those obtained
by replacing words with the base forms.
The best system with the complete corpus as well
as the best one with the reduced corpus use the
phrases and the transformed Serbian corpus where
the verb treatment has been applied.
4.2.2 Translation from English into Serbian
Table 4 shows results for the translation from En-
glish into Serbian. As expected, all error rates are
higher than for the other translation direction. Trans-
lation into the morphologically richer language al-
ways has poorer quality because it is difficult to find
the correct inflection.
The performance of the reduced corpus is de-
graded for about 40% relative for the baseline sys-
tem and for about 30% when the phrases are used
and the transformation of the English corpus has
been applied.
45
Table 3: Translation error rates [%] for Serbian?English
Serbian ? English Development+Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 45.6 39.6 70.0
2.6k sr base 43.5 38.2 68.9
2.6k sr base+v-pos 42.5 35.3 66.2
2.6k+phrases baseline 46.0 39.6 69.5
2.6k+phrases sr base 44.6 39.1 70.2
2.6k+phrases sr base+v-pos 42.1 35.3 66.0
200 baseline 66.5 61.1 91.6
200 sr base 63.2 58.2 90.3
200 sr base+v-pos 63.3 56.2 88.5
200+phrases baseline 65.2 59.5 90.2
200+phrases sr base 62.3 56.9 87.7
200+phrases sr base+v-pos 61.3 53.2 86.2
Table 4: Translation error rates [%] for English?Serbian
English ? Serbian Development+Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 53.1 46.9 78.6
2.6k en no-article 52.6 47.2 79.4
2.6k+phrases baseline 52.5 46.5 76.6
2.6k+phrases en no-article 52.3 47.0 79.6
200 baseline 73.6 68.0 93.0
200 en no-article 71.5 66.5 93.4
200+phrases baseline 71.7 66.7 92.3
200+phrases en no-article 67.9 62.9 92.1
Table 5: Translation error rates [%] for the external test
Serbian ? English External Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 72.2 64.8 92.2
2.6k sr base 66.8 61.4 86.9
2.6k sr base+v-pos 67.5 61.4 88.3
2.6k+phrases baseline 71.3 63.9 91.9
2.6k+phrases sr base 67.0 61.2 88.4
2.6k+phrases sr base+v-pos 69.7 61.2 89.8
English ? Serbian
2.6k baseline 85.3 77.0 96.4
2.6k en no-article 77.5 69.9 95.8
2.6k+phrases baseline 84.1 74.9 95.2
2.6k+phrases en no-article 77.7 70.1 94.8
46
The importance of the phrases seems to be larger
for this translation direction. Removing the English
articles does not have the significant role for the
translation systems with full corpus, but for the re-
duced corpus it has basically the same effect as the
use of phrases. The best system with the reduced
corpus has been built with the use of phrases and
removal of the articles.
Table 7 shows some examples of the translation
into Serbian with and without English articles. Al-
though these effects are not directly obvious, it can
be seen that removing of the redundant information
enables better learning of the relevant information
so that system is better capable of producing seman-
tically correct output. The first example illustrates
an syntactically incorrect output with the wrong in-
flection of the verb (?c?itam? means ?I read?). The
output of the system without articles is still not com-
pletely correct, but the semantic is completely pre-
served. The second example illustrates an output
produced by the baseline system which is neither
syntactically nor semantically correct (?you have I
drink?). The output of the new system still has an
error in the verb, informal form of ?you? instead of
the formal one, but nevertheless both the syntax and
semantics are correct.
4.2.3 Translation of the External Text
Translation results for the external test can be
seen in Table 5. As expected, the high number of
out-of-vocabulary words results in very high error
rates. Certain improvement is achieved with the
phrases, but the most significant improvements are
yielded by the use of Serbian base forms and re-
moval of English articles. Verb treatment in this case
does not outperform the base forms system, prob-
ably because there are not so many different verb
forms as in the other corpus, and only a small num-
ber of pronouns is missing.
5 Conclusions
In this work, we have examined the possibilities
for building a statistical machine translation system
with a small bilingual Serbian-English parallel text.
Our experiments showed that the translation results
for this language pair are comparable with results for
other language pairs, especially if the small size of
the corpus, unrestricted domain and rich inflectional
morphology of Serbian language are taken into ac-
count. With the baseline system, we obtained about
45% WER for translation into English and about
53% for translation into Serbian.
We have systematically investigated the impact of
the corpus size on translation quality, as well as the
importance of additional bilingual knowledge in the
form of short phrases. In addition, we have shown
that morpho-syntactic information is a valuable lan-
guage resource for translation of this language pair.
Depending on the availability of resources and
tools, we plan to examine parallel texts with other
languages, and also to do further investigations on
this language pair. We believe that more refined use
of the morpho-syntactic information can yield better
results (for example the hierarchical lexicon model
proposed in (Nie?en and Ney, 2001)). We also be-
lieve that the use of the conventional dictionaries
could improve the Serbian-English translation.
Acknowledgement
This work was partly funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistical Methods for Written Language Transla-
tion? (Ne572/5).
References
Y. Al-Onaizan, U. Germann, U. Hermjakob, K. Knight,
P. Koehn, D. Marcu, and K. Yamada. 2000. Translat-
ing with scarce resources. In National Conference on
Artificial Intelligence (AAAI).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
Sonja Nie?en and Hermann Ney. 2001. Toward hi-
erarchical models for statistical machine translation
of inflected languages. In 39th Annual Meeting of
the Assoc. for Computational Linguistics - joint with
EACL 2001: Proc. Workshop on Data-Driven Ma-
chine Translation, pages 47?54, Toulouse, France,
July.
Sonja Nie?en and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204, June.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
47
Table 6: Examples of Serbian?English translations with and without transformations
to je suvishe skupo . ? to biti suvishe skup . ? to SG3 biti suvishe skup .
base forms verb treatment
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
it is it is it is
too UNKNOWN skupo . too expensive . too expensive .
on ne igra . ? on ne igrati . ? on ne SG3 igrati .
base forms verb treatment
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
he he does not . he do not play . he does not play .
da , ali nemam ? da , ali nemati ? da , ali SG1 ne imati
mnogo vremena . base forms mnogo vreme . verb treatment mnogo vreme .
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
yes , but I have yes , but not yes , but I have not got
much time . much time . much time .
Table 7: Examples of English?Serbian translations with and without transformations
you should not ? you should not
read in bed . remove articles read in bed .
? En ? Sr (baseline) ? En? ? Sr reference translation:
treba ne ne bi trebalo ne bi trebalo
c?itam u krevet . c?itate u krevet . da c?itate u krevetu .
have a drink . ? have drink .
remove articles
? En ? Sr (baseline) ? En? ? Sr reference translation:
imate pijem . uzmi nes?to za pic?e . uzmite nes?to za pic?e .
machine translation. In Proc. 40th Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. 40th Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
M. Popovic? and H. Ney. 2004. Towards the use of word
stems and suffixes for statistical machine translation.
In Proc. 4th Int. Conf. on Language Resources and
Evaluation (LREC), pages 1585?1588, Lisbon, Portu-
gal, May.
M. Popovic?, S. Jovic?ic?, and Z. ?Saric?. 2004. Statistical
machine translation of Serbian-English. In Proc. of
Int. Workshop on Speech and Computer (SPECOM),
pages 410?414, St. Petersburg, Russia, September.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
pages 133?142, Sommerset, NJ.
48
Proceedings of the Workshop on Statistical Machine Translation, pages 1?6,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Information for Automatic Error Analysis of Statistical
Machine Translation Output
Maja Popovic??
Hermann Ney?
Adria` de Gispert?
Jose? B. Marin?o?
Deepa Gupta?
Marcello Federico?
Patrik Lambert?
Rafael Banchs?
? Lehrstuhl fu?r Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany
? TALP Research Center, Universitat Polite`cnica de Catalunya (UPC), Barcelona, Spain
? ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy
{popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es
{gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es
Abstract
Evaluation of machine translation output
is an important but difficult task. Over the
last years, a variety of automatic evalua-
tion measures have been studied, some of
them like Word Error Rate (WER), Posi-
tion Independent Word Error Rate (PER)
and BLEU and NIST scores have become
widely used tools for comparing different
systems as well as for evaluating improve-
ments within one system. However, these
measures do not give any details about
the nature of translation errors. Therefore
some analysis of the generated output is
needed in order to identify the main prob-
lems and to focus the research efforts. On
the other hand, human evaluation is a time
consuming and expensive task. In this
paper, we investigate methods for using
of morpho-syntactic information for auto-
matic evaluation: standard error measures
WER and PER are calculated on distinct
word classes and forms in order to get a
better idea about the nature of translation
errors and possibilities for improvements.
1 Introduction
The evaluation of the generated output is an impor-
tant issue for all natural language processing (NLP)
tasks, especially for machine translation (MT). Au-
tomatic evaluation is preferred because human eval-
uation is a time consuming and expensive task.
A variety of automatic evaluation measures have
been proposed and studied over the last years, some
of them are shown to be a very useful tool for com-
paring different systems as well as for evaluating
improvements within one system. The most widely
used are Word Error Rate (WER), Position Indepen-
dent Word Error Rate (PER), the BLEU score (Pap-
ineni et al, 2002) and the NIST score (Doddington,
2002). However, none of these measures give any
details about the nature of translation errors. A rela-
tionship between these error measures and the actual
errors in the translation outputs is not easy to find.
Therefore some analysis of the translation errors is
necessary in order to define the main problems and
to focus the research efforts. A framework for hu-
man error analysis and error classification has been
proposed in (Vilar et al, 2006), but like human eval-
uation, this is also a time consuming task.
The goal of this work is to present a framework
for automatic error analysis of machine translation
output based on morpho-syntactic information.
2 Related Work
There is a number of publications dealing with
various automatic evaluation measures for machine
translation output, some of them proposing new
measures, some proposing improvements and exten-
sions of the existing ones (Doddington, 2002; Pap-
ineni et al, 2002; Babych and Hartley, 2004; Ma-
tusov et al, 2005). Semi-automatic evaluation mea-
sures have been also investigated, for example in
(Nie?en et al, 2000). An automatic metric which
uses base forms and synonyms of the words in or-
der to correlate better to human judgements has been
1
proposed in (Banerjee and Lavie, 2005). However,
error analysis is still a rather unexplored area. A
framework for human error analysis and error clas-
sification has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. Automatic methods for error anal-
ysis to our knowledge have not been studied yet.
Many publications propose the use of morpho-
syntactic information for improving the perfor-
mance of a statistical machine translation system.
Various methods for treating morphological and
syntactical differences between German and English
are investigated in (Nie?en and Ney, 2000; Nie?en
and Ney, 2001a; Nie?en and Ney, 2001b). Mor-
phological analysis has been used for improving
Arabic-English translation (Lee, 2004), for Serbian-
English translation (Popovic? et al, 2005) as well as
for Czech-English translation (Goldwater and Mc-
Closky, 2005). Inflectional morphology of Spanish
verbs is dealt with in (Popovic? and Ney, 2004; de
Gispert et al, 2005). To the best of our knowledge,
the use of morpho-syntactic information for error
analysis of translation output has not been investi-
gated so far.
3 Morpho-syntactic Information and
Automatic Evaluation
We propose the use of morpho-syntactic informa-
tion in combination with the automatic evaluation
measures WER and PER in order to get more details
about the translation errors.
We investigate two types of potential problems for
the translation with the Spanish-English language
pair:
? syntactic differences between the two lan-
guages considering nouns and adjectives
? inflections in the Spanish language considering
mainly verbs, adjectives and nouns
As any other automatic evaluation measures,
these novel measures will be far from perfect. Pos-
sible POS-tagging errors may introduce additional
noise. However, we expect this noise to be suffi-
ciently small and the new measures to be able to give
sufficiently clear ideas about particular errors.
3.1 Syntactic differences
Adjectives in the Spanish language are usually
placed after the corresponding noun, whereas in En-
glish is the other way round. Although in most cases
the phrase based translation system is able to han-
dle these local permutations correctly, some errors
are still present, especially for unseen or rarely seen
noun-adjective groups. In order to investigate this
type of errors, we extract the nouns and adjectives
from both the reference translations and the sys-
tem output and then calculate WER and PER. If the
difference between the obtained WER and PER is
large, this indicates reordering errors: a number of
nouns and adjectives is translated correctly but in the
wrong order.
3.2 Spanish inflections
Spanish has a rich inflectional morphology, espe-
cially for verbs. Person and tense are expressed
by the suffix so that many different full forms of
one verb exist. Spanish adjectives, in contrast to
English, have four possible inflectional forms de-
pending on gender and number. Therefore the er-
ror rates for those word classes are expected to be
higher for Spanish than for English. Also, the er-
ror rates for the Spanish base forms are expected to
be lower than for the full forms. In order to investi-
gate potential inflection errors, we compare the PER
for verbs, adjectives and nouns for both languages.
For the Spanish language, we also investigate differ-
ences between full form PER and base form PER:
the larger these differences, more inflection errors
are present.
4 Experimental Settings
4.1 Task and Corpus
The corpus analysed in this work is built in the
framework of the TC-Star project. It contains more
than one million sentences and about 35 million run-
ning words of the Spanish and English European
Parliament Plenary Sessions (EPPS). A description
of the EPPS data can be found in (Vilar et al, 2005).
In order to analyse effects of data sparseness, we
have randomly extracted a small subset referred to
as 13k containing about thirteen thousand sentences
and 370k running words (about 1% of the original
2
Training corpus: Spanish English
full Sentences 1281427
Running Words 36578514 34918192
Vocabulary 153124 106496
Singletons [%] 35.2 36.2
13k Sentences 13360
Running Words 385198 366055
Vocabulary 22425 16326
Singletons [%] 47.6 43.7
Dev: Sentences 1008
Running Words 25778 26070
Distinct Words 3895 3173
OOVs (full) [%] 0.15 0.09
OOVs (13k) [%] 2.7 1.7
Test: Sentences 840 1094
Running Words 22774 26917
Distinct Words 4081 3958
OOVs (full) [%] 0.14 0.25
OOVs (13k) [%] 2.8 2.6
Table 1: Corpus statistics for the Spanish-English
EPPS task (running words include punctuation
marks)
corpus). The statistics of the corpora can be seen in
Table 1.
4.2 Translation System
The statistical machine translation system used in
this work is based on a log-linear combination of
seven different models. The most important ones are
phrase based models in both directions, additionally
IBM1 models at the phrase level in both directions
as well as phrase and length penalty are used. A
more detailed description of the system can be found
in (Vilar et al, 2005; Zens et al, 2005).
4.3 Experiments
The translation experiments have been done in both
translation directions on both sizes of the corpus. In
order to examine improvements of the baseline sys-
tem, a new system with POS-based word reorderings
of nouns and adjectives as proposed in (Popovic? and
Ney, 2006) is also analysed. Adjectives in the Span-
ish language are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, local reorderings of nouns and ad-
Spanish?English WER PER BLEU
full baseline 34.5 25.5 54.7
reorder 33.5 25.2 56.4
13k baseline 41.8 30.7 43.2
reorder 38.9 29.5 48.5
English?Spanish WER PER BLEU
full baseline 39.7 30.6 47.8
reorder 39.6 30.5 48.3
13k baseline 49.6 37.4 36.2
reorder 48.1 36.5 37.7
Table 2: Translation Results [%]
jective groups in the source language have been ap-
plied. If the source language is Spanish, each noun is
moved behind the corresponding adjective group. If
the source language is English, each adjective group
is moved behind the corresponding noun. An adverb
followed by an adjective (e.g. ?more important?) or
two adjectives with a coordinate conjunction in be-
tween (e.g. ?economic and political?) are treated as
an adjective group. Standard translation results are
presented in Table 2.
5 Error Analysis
5.1 Syntactic errors
As explained in Section 3.1, reordering errors due
to syntactic differences between two languages have
been measured by the relative difference between
WER and PER calculated on nouns and adjectives.
Corresponding relative differences are calculated
also for verbs as well as adjectives and nouns sep-
arately.
Table 3 presents the relative differences for the
English and Spanish output. It can be seen that
the PER/WER difference for nouns and adjectives
is relatively high for both language pairs (more than
20%), and for the English output is higher than for
the Spanish one. This corresponds to the fact that
the Spanish language has a rather free word order:
although the adjective usually is placed behind the
noun, this is not always the case. On the other hand,
adjectives in English are always placed before the
corresponding noun. It can also be seen that the
difference is higher for the reduced corpus for both
outputs indicating that the local reordering problem
3
English output 1? PERWER
full nouns+adjectives 24.7
+reordering 20.8
verbs 4.1
adjectives 10.2
nouns 20.1
13k nouns+adjectives 25.7
+reordering 20.1
verbs 4.6
adjectives 8.4
nouns 19.1
Spanish output 1? PERWER
full nouns+adjectives 21.5
+reordering 20.3
verbs 3.3
adjectives 5.6
nouns 16.9
13k nouns+adjectives 22.9
+reordering 19.8
verbs 3.9
adjectives 5.4
nouns 19.3
Table 3: Relative difference between PER and
WER [%] for different word classes
is more important when only small amount of train-
ing data is available. As mentioned in Section 3.1,
the phrase based translation system is able to gen-
erate frequent noun-adjective groups in the correct
word order, but unseen or rarely seen groups intro-
duce difficulties.
Furthermore, the results show that the POS-based
reordering of adjectives and nouns leads to a de-
crease of the PER/WER difference for both out-
puts and for both corpora. Relative decrease of the
PER/WER difference is larger for the small corpus
than for the full corpus. It can also be noted that the
relative decrease for both corpora is larger for the
English output than for the Spanish one due to free
word order - since the Spanish adjective group is not
always placed behind the noun, some reorderings in
English are not really needed.
For the verbs, PER/WER difference is less than
5% for both outputs and both training corpora, in-
dicating that the word order of verbs is not an im-
English output PER
full verbs 44.8
adjectives 27.3
nouns 23.0
13k verbs 56.1
adjectives 38.1
nouns 31.7
Spanish output PER
full verbs 61.4
adjectives 41.8
nouns 28.5
13k verbs 73.0
adjectives 50.9
nouns 37.0
Table 4: PER [%] for different word classes
portant issue for the Spanish-English language pair.
PER/WER difference for adjectives and nouns is
higher than for verbs, for the nouns being signifi-
cantly higher than for adjectives. The reason for this
is probably the fact that word order differences in-
volving only the nouns are also present, for example
?export control = control de exportacio?n?.
5.2 Inflectional errors
Table 4 presents the PER for different word classes
for the English and Spanish output respectively. It
can be seen that all PERs are higher for the Spanish
output than for the English one due to the rich in-
flectional morphology of the Spanish language. It
can be also seen that the Spanish verbs are espe-
cially problematic (as stated in (Vilar et al, 2006))
reaching 60% of PER for the full corpus and more
than 70% for the reduced corpus. Spanish adjectives
also have a significantly higher PER than the English
ones, whereas for the nouns this difference is not so
high.
Results of the further analysis of inflectional er-
rors are presented in Table 5. Relative difference
between full form PER and base form PER is sig-
nificantly lower for adjectives and nouns than for
verbs, thus showing that the verb inflections are the
main source of translation errors into the Spanish
language.
Furthermore, it can be seen that for the small cor-
4
Spanish output 1? PERbPERf
full verbs 26.9
adjectives 9.3
nouns 8.4
13k verbs 23.7
adjectives 15.1
nouns 6.5
Table 5: Relative difference between PER of base
forms and PER of full forms [%] for the Spanish
output
pus base/full PER difference for verbs and nouns is
basically the same as for the full corpus. Since nouns
in Spanish only have singular and plural form as in
English, the number of unseen forms is not partic-
ularly enlarged by the reduction of the training cor-
pus. On the other hand, base/full PER difference of
adjectives is significantly higher for the small corpus
due to an increased number of unseen adjective full
forms.
As for verbs, intuitively it might be expected that
the number of inflectional errors for this word class
also increases by reducing the training corpus, even
more than for adjectives. However, the base/full
PER difference is not larger for the small corpus,
but even smaller. This is indicating that the problem
of choosing the right inflection of a Spanish verb ap-
parently is not related to the number of unseen full
forms since the number of inflectional errors is very
high even when the translation system is trained on
a very large corpus.
6 Conclusion
In this work, we presented a framework for auto-
matic analysis of translation errors based on the use
of morpho-syntactic information. We carried out a
detailed analysis which has shown that the results
obtained by our method correspond to those ob-
tained by human error analysis in (Vilar et al, 2006).
Additionally, it has been shown that the improve-
ments of the baseline system can be adequately mea-
sured as well.
This work is just a first step towards the devel-
opment of linguistically-informed evaluation mea-
sures which provide partial and more specific infor-
mation of certain translation problems. Such mea-
sures are very important to understand what are the
weaknesses of a statistical machine translation sys-
tem, and what are the best ways and methods for
improvements.
For our future work, we plan to extend the pro-
posed measures in order to carry out a more de-
tailed error analysis, for example examinating dif-
ferent types of inflection errors for Spanish verbs.
We also plan to investigate other types of translation
errors and other language pairs.
Acknowledgements
This work was partly supported by the TC-STAR
project by the European Community (FP6-506738)
and partly by the Generalitat de Catalunya and the
European Social Fund.
References
Bogdan Babych and Anthony Hartley. 2004. Extending
bleu mt evaluation method with frequency weighting.
In Proc. of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for mt evaluation with improved
correlation with human judgements. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, pages 65?72,
Ann Arbor, MI, June.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Proc. of the 9th European Conf. on Speech Commu-
nication and Technology (Interspeech), pages 3185?
3188, Lisbon, Portugal, September.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In Proc. 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), Boston, MA, May.
5
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating machine transla-
tion output with automatic sentence segmentation. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 148?154, Pitts-
burgh, PA, October.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-
syntactic analysis for reordering in statistical machine
translation. In Proc. MT Summit VIII, pages 247?252,
Santiago de Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hier-
archical models for statistical machine translation of
inflected languages. In Data-Driven Machine Trans-
lation Workshop, pages 47?54, Toulouse, France, July.
Sonja Nie?en, Franz J. Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research. In
Proc. Second Int. Conf. on Language Resources and
Evaluation (LREC), pages 39?45, Athens, Greece,
May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2004. Towards the use
of word stems & suffixes for statistical machine trans-
lation. In Proc. 4th Int. Conf. on Language Resources
and Evaluation (LREC), pages 1585?1588, Lissabon,
Portugal, May.
Maja Popovic? and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation. In
Proc. of the Fifth Int. Conf. on Language Resources
and Evaluation (LREC), Genova, Italy, May.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran ?Saric?. 2005. Augmenting a small
parallel text with morpho-syntactic language resources
for Serbian?English statistical machine translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 41?48, Ann Arbor, MI, June.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical machine transla-
tion of european parliamentary speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proc. of the Fifth Int. Conf. on
Language Resources and Evaluation (LREC), page to
appear, Genova, Italy, May.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based statis-
tical machine translation system. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 155?162, Pittsburgh, PA, October.
6
Proceedings of the Second Workshop on Statistical Machine Translation, pages 48?55,
Prague, June 2007. c?2007 Association for Computational Linguistics
Word Error Rates: Decomposition over POS Classes and Applications for
Error Analysis
Maja Popovic?
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
Aachen, Germany
popovic@cs.rwth-aachen.de
Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
Aachen, Germany
ney@cs.rwth-aachen.de
Abstract
Evaluation and error analysis of machine
translation output are important but difficult
tasks. In this work, we propose a novel
method for obtaining more details about ac-
tual translation errors in the generated output
by introducing the decomposition of Word
Error Rate (WER) and Position independent
word Error Rate (PER) over different Part-
of-Speech (POS) classes. Furthermore, we
investigate two possible aspects of the use
of these decompositions for automatic er-
ror analysis: estimation of inflectional errors
and distribution of missing words over POS
classes. The obtained results are shown to
correspond to the results of a human error
analysis. The results obtained on the Euro-
pean Parliament Plenary Session corpus in
Spanish and English give a better overview
of the nature of translation errors as well as
ideas of where to put efforts for possible im-
provements of the translation system.
1 Introduction
Evaluation of machine translation output is a very
important but difficult task. Human evaluation is
expensive and time consuming. Therefore a variety
of automatic evaluation measures have been studied
over the last years. The most widely used are Word
Error Rate (WER), Position independent word Error
Rate (PER), the BLEU score (Papineni et al, 2002)
and the NIST score (Doddington, 2002). These mea-
sures have shown to be valuable tools for comparing
different systems as well as for evaluating improve-
ments within one system. However, these measures
do not give any details about the nature of translation
errors. Therefore some more detailed analysis of the
generated output is needed in order to identify the
main problems and to focus the research efforts. A
framework for human error analysis has been pro-
posed in (Vilar et al, 2006), but as every human
evaluation, this is also a time consuming task.
This article presents a framework for calculating
the decomposition of WER and PER over different
POS classes, i.e. for estimating the contribution of
each POS class to the overall word error rate. Al-
though this work focuses on POS classes, the method
can be easily extended to other types of linguis-
tic information. In addition, two methods for error
analysis using the WER and PER decompositons to-
gether with base forms are proposed: estimation of
inflectional errors and distribution of missing words
over POS classes. The translation corpus used for
our error analysis is built in the framework of the
TC-STAR project (tcs, 2005) and contains the tran-
scriptions of the European Parliament Plenary Ses-
sions (EPPS) in Spanish and English. The translation
system used is the phrase-based statistical machine
translation system described in (Vilar et al, 2005;
Matusov et al, 2006).
2 Related Work
Automatic evaluation measures for machine trans-
lation output are receiving more and more atten-
tion in the last years. The BLEU metric (Pap-
ineni et al, 2002) and the closely related NIST met-
ric (Doddington, 2002) along with WER and PER
48
have been widely used by many machine translation
researchers. An extended version of BLEU which
uses n-grams weighted according to their frequency
estimated from a monolingual corpus is proposed
in (Babych and Hartley, 2004). (Leusch et al, 2005)
investigate preprocessing and normalisation meth-
ods for improving the evaluation using the standard
measures WER, PER, BLEU and NIST. The same set
of measures is examined in (Matusov et al, 2005)
in combination with automatic sentence segmenta-
tion in order to enable evaluation of translation out-
put without sentence boundaries (e.g. translation of
speech recognition output). A new automatic met-
ric METEOR (Banerjee and Lavie, 2005) uses stems
and synonyms of the words. This measure counts
the number of exact word matches between the out-
put and the reference. In a second step, unmatched
words are converted into stems or synonyms and
then matched. The TER metric (Snover et al, 2006)
measures the amount of editing that a human would
have to perform to change the system output so that
it exactly matches the reference. The CDER mea-
sure (Leusch et al, 2006) is based on edit distance,
such as the well-known WER, but allows reordering
of blocks. Nevertheless, none of these measures or
extensions takes into account linguistic knowledge
about actual translation errors, for example what is
the contribution of verbs in the overall error rate,
how many full forms are wrong whereas their base
forms are correct, etc. A framework for human error
analysis has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. However, human error analysis,
like any human evaluation, is a time consuming task.
Whereas the use of linguistic knowledge for im-
proving the performance of a statistical machine
translation system is investigated in many publi-
cations for various language pairs (like for exam-
ple (Nie?en and Ney, 2000), (Goldwater and Mc-
Closky, 2005)), its use for the analysis of translation
errors is still a rather unexplored area. Some auto-
matic methods for error analysis using base forms
and POS tags are proposed in (Popovic? et al, 2006;
Popovic? and Ney, 2006). These measures are based
on differences between WER and PER which are cal-
culated separately for each POS class using subsets
extracted from the original texts. Standard overall
WER and PER of the original texts are not at all
taken into account. In this work, the standard WER
and PER are decomposed and analysed.
3 Decomposition of WER and PER over
POS classes
The standard procedure for evaluating machine
translation output is done by comparing the hypoth-
esis document hyp with given reference translations
ref , each one consisting of K sentences (or seg-
ments). The reference document ref consists of
R reference translations for each sentence. Let the
length of the hypothesis sentence hypk be denoted
as Nhypk , and the reference lengths of each sentence
Nref k,r . Then, the total hypothesis length of the doc-
ument is Nhyp =
?
k Nhypk , and the total reference
length is Nref =
?
k N?ref k where N
?
ref k is defined
as the length of the reference sentence with the low-
est sentence-level error rate as shown to be optimal
in (Leusch et al, 2005).
3.1 Standard word error rates (overview)
The word error rate (WER) is based on the Lev-
enshtein distance (Levenshtein, 1966) - the mini-
mum number of substitutions, deletions and inser-
tions that have to be performed to convert the gen-
erated text hyp into the reference text ref . A short-
coming of the WER is the fact that it does not allow
reorderings of words, whereas the word order of the
hypothesis can be different from word order of the
reference even though it is correct translation. In
order to overcome this problem, the position inde-
pendent word error rate (PER) compares the words
in the two sentences without taking the word order
into account. The PER is always lower than or equal
to the WER. On the other hand, shortcoming of the
PER is the fact that the word order can be impor-
tant in some cases. Therefore the best solution is to
calculate both word error rates.
Calculation of WER: The WER of the hypothe-
sis hyp with respect to the reference ref is calculated
as:
WER = 1N?ref
K
?
k=1
min
r
dL(ref k,r, hypk)
where dL(ref k,r, hypk) is the Levenshtein dis-
tance between the reference sentence ref k,r and the
hypothesis sentence hypk. The calculation of WER
49
is performed using a dynamic programming algo-
rithm.
Calculation of PER: The PER can be calcu-
lated using the counts n(e, hypk) and n(e, ref k,r)
of a word e in the hypothesis sentence hypk and the
reference sentence ref k,r respectively:
PER = 1N?ref
K
?
k=1
min
r
dPER(ref k,r, hypk)
where
dPER(ref k,r, hypk) =
1
2
(
|Nref k,r ? Nhypk |+
?
e
|n(e, ref k,r) ? n(e, hypk)|
)
3.2 WER decomposition over POS classes
The dynamic programming algorithm for WER en-
ables a simple and straightforward identification of
each erroneous word which actually contributes to
WER. Let errk denote the set of erroneous words
in sentence k with respect to the best reference and
p be a POS class. Then n(p, errk) is the number of
errors in errk produced by words with POS class p.
It should be noted that for the substitution errors, the
POS class of the involved reference word is taken
into account. POS tags of the reference words are
also used for the deletion errors, and for the inser-
tion errors the POS class of the hypothesis word is
taken. The WER for the word class p can be calcu-
lated as:
WER(p) = 1N?ref
K
?
k=1
n(p, errk)
The sum over all classes is equal to the standard
overall WER.
An example of a reference sentence and hypothe-
sis sentence along with the corresponding POS tags
is shown in Table 1. The WER errors, i.e. actual
words participating in WER together with their POS
classes can be seen in Table 2. The reference words
involved in WER are denoted as reference errors,
and hypothesis errors refer to the hypothesis words
participating in WER.
Standard WER of the whole sentence is equal
to 4/12 = 33.3%. The contribution of nouns is
reference:
Mister#N Commissioner#N ,#PUN
twenty-four#NUM hours#N
sometimes#ADV can#V be#V too#ADV
much#PRON time#N .#PUN
hypothesis:
Mrs#N Commissioner#N ,#PUN
twenty-four#NUM hours#N is#V
sometimes#ADV too#ADV
much#PRON time#N .#PUN
Table 1: Example for illustration of actual errors: a
POS tagged reference sentence and a corresponding
hypothesis sentence
reference errors hypothesis errors error type
Mister#N Mrs#N substitution
sometimes#ADV is#V substitution
can#V deletion
be#V sometimes#ADV substitution
Table 2: WER errors: actual words which are partici-
pating in the word error rate and their corresponding
POS classes
WER(N) = 1/12 = 8.3%, of verbs WER(V) =
2/12 = 16.7% and of adverbs WER(ADV) =
1/12 = 8.3%
3.3 PER decomposition over POS classes
In contrast to WER, standard efficient algorithms for
the calculation of PER do not give precise informa-
tion about contributing words. However, it is pos-
sible to identify all words in the hypothesis which
do not have a counterpart in the reference, and vice
versa. These words will be referred to as PER errors.
reference errors hypothesis errors
Mister#N Mrs#N
be#V is#V
can#V
Table 3: PER errors: actual words which are partic-
ipating in the position independent word error rate
and their corresponding POS classes
An illustration of PER errors is given in Table 3.
50
The number of errors contributing to the standard
PER according to the algorithm described in 3.1 is 3
- there are two substitutions and one deletion. The
problem with standard PER is that it is not possible
to detect which words are the deletion errors, which
are the insertion errors, and which words are the sub-
stitution errors. Therefore we introduce an alterna-
tive PER based measure which corresponds to the
F-measure. Let herrk refer to the set of words in the
hypothesis sentence k which do not appear in the
reference sentence k (referred to as hypothesis er-
rors). Analogously, let rerrk denote the set of words
in the reference sentence k which do not appear in
the hypothesis sentence k (referred to as reference
errors). Then the following measures can be calcu-
lated:
? reference PER (RPER) (similar to recall):
RPER(p) = 1N?ref
K
?
k=1
n(p, rerrk)
? hypothesis PER (HPER) (similar to precision):
HPER(p) = 1Nhyp
K
?
k=1
n(p, herrk)
? F-based PER (FPER):
FPER(p) = 1N?ref + Nhyp
?
?
K
?
k=1
(n(p, rerrk) + n(p, herrk))
Since we are basically interested in all words with-
out a counterpart, both in the reference and in the
hypothesis, this work will be focused on FPER. The
sum of FPER over all POS classes is equal to the
overall FPER, and the latter is always less or equal
to the standard PER.
For the example sentence presented in Table 1, the
number of hypothesis errors n(e, herrk) is 2 and the
number of reference errors n(e, rerrk) is 3 where e
denotes the word. The number of errors contributing
to the standard PER is 3, since |Nref ? Nhyp | = 1
and
?
e |n(e, ref k) ? n(e, hypk)| = 5. The stan-
dard PER is normalised over the reference length
Nref = 12 thus being equal to 25%. The FPER is the
sum of hypothesis and reference errors divided by
the sum of hypothesis and reference length: FPER =
(2 + 3)/(11 + 12) = 5/23 = 21.7%. The contribu-
tion of nouns is FPER(N) = 2/23 = 8.7% and the
contribution of verbs is FPER(V) = 3/23 = 13%.
4 Applications for error analysis
The decomposed error rates described in Section 3.2
and Section 3.3 contain more details than the stan-
dard error rates. However, for more precise informa-
tion about certain phenomena some kind of further
analysis is required. In this work, we investigate two
possible aspects for error analysis:
? estimation of inflectional errors by the use of
FPER errors and base forms
? extracting the distribution of missing words
over POS classes using WER errors, FPER er-
rors and base forms.
4.1 Inflectional errors
Inflectional errors can be estimated using FPER
errors and base forms. From each reference-
hypothesis sentence pair, only erroneous words
which have the common base forms are taken
into account. The inflectional error rate of each POS
class is then calculated in the same way as FPER.
For example, from the PER errors presented in Ta-
ble 3, the words ?is? and ?be? are candidates for an
inflectional error because they are sharing the same
base form ?be?. Inflectional error rate in this exam-
ple is present only for the verbs, and is calculated in
the same way as FPER, i.e. IFPER(V) = 2/23 =
8.7%.
4.2 Missing words
Distribution of missing words over POS classes can
be extracted from the WER and FPER errors in the
following way: the words considered as missing are
those which occur as deletions in WER errors and
at the same time occur only as reference PER errors
without sharing the base form with any hypothesis
error. The use of both WER and PER errors is much
more reliable than using only the WER deletion er-
ros because not all deletion errors are produced by
missing words: a number of WER deletions appears
51
due to reordering errors. The information about the
base form is used in order to eliminate inflectional
errors. The number of missing words is extracted for
each word class and then normalised over the sum of
all classes. For the example sentence pair presented
in Table 1, from the WER errors in Table 2 and the
PER errors in Table 3 the word ?can? will be identi-
fied as missing.
5 Experimental settings
5.1 Translation System
The machine translation system used in this work
is based on the statistical aproach. It is built as
a log-linear combination of seven different statisti-
cal models: phrase based models in both directions,
IBM1 models at the phrase level in both directions,
as well as target language model, phrase penalty and
length penalty are used. A detailed description of the
system can be found in (Vilar et al, 2005; Matusov
et al, 2006).
5.2 Task and corpus
The corpus analysed in this work is built in the
framework of the TC-STAR project. The training
corpus contains more than one million sentences and
about 35 million running words of the European Par-
liament Plenary Sessions (EPPS) in Spanish and En-
glish. The test corpus contains about 1 000 sentences
and 28 000 running words. The OOV rates are low,
about 0.5% of the running words for Spanish and
0.2% for English. The corpus statistics can be seen
in Table 4. More details about the EPPS data can be
found in (Vilar et al, 2005).
TRAIN Spanish English
Sentences 1 167 627
Running words 35 320 646 33 945 468
Vocabulary 159 080 110 636
TEST
Sentences 894 1 117
Running words 28 591 28 492
OOVs 0.52% 0.25%
Table 4: Statistics of the training and test corpora
of the TC-STAR EPPS Spanish-English task. Test
corpus is provided with two references.
6 Error analysis
The translation is performed in both directions
(Spanish to English and English to Spanish) and the
error analysis is done on both the English and the
Spanish output. Morpho-syntactic annotation of the
English references and hypotheses is performed us-
ing the constraint grammar parser ENGCG (Vouti-
lainen, 1995), and the Spanish texts are annotated
using the FreeLing analyser (Carreras et al, 2004).
In this way, all references and hypotheses are pro-
vided with POS tags and base forms. The decom-
position of WER and FPER is done over the ten
main POS classes: nouns (N), verbs (V), adjectives
(A), adverbs (ADV), pronouns (PRON), determiners
(DET), prepositions (PREP), conjunctions (CON),
numerals (NUM) and punctuation marks (PUN). In-
flectional error rates are also estimated for each POS
class using FPER counts and base forms. Addition-
ally, details about the verb tense and person inflec-
tions for both languages as well as about the adjec-
tive gender and person inflections for the Spanish
output are extracted. Apart from that, the distribu-
tion of missing words over the ten POS classes is
estimated using the WER and FPER errors.
6.1 WER and PER (FPER) decompositions
Figure 1 presents the decompositions of WER and
FPER over the ten basic POS classes for both lan-
guages. The largest part of both word error rates
comes from the two most important word classes,
namely nouns and verbs, and that the least critical
classes are punctuations, conjunctions and numbers.
Adjectives, determiners and prepositions are sig-
nificantly worse in the Spanish output. This is partly
due to the richer morphology of the Spanish lan-
guage. Furthermore, the histograms indicate that the
number of erroneus nouns and pronouns is higher
in the English output. As for verbs, WER is higher
for English and FPER for Spanish. This indicates
that there are more problems with word order in the
English output, and more problems with the correct
verb or verb form in the Spanish output.
In addition, the decomposed error rates give an
idea of where to put efforts for possible improve-
ments of the system. For example, working on im-
provements of verb translations could reduce up to
about 10% WER and 7% FPER, working on nouns
52
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
PUNNUMPREP CONDETPRONADVAVN
WER over POS classes [%]
English
Spanish
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
PUNNUMPREP CONDETPRONADVAVN
FPER over POS classes [%]
English
Spanish
Figure 1: Decomposition of WER and FPER [%]
over the ten basic POS classes for English and Span-
ish output
up to 8% WER and 5% FPER, whereas there is no
reason to put too much efforts on e.g. adverbs since
this could lead only to about 2% of WER and FPER
reduction. 1
6.2 Inflectional errors
Inflectional error rates for the ten POS classes are
presented in Figure 2. For the English language,
these errors are significant only for two POS classes:
nouns and verbs. The verbs are the most problem-
atic category in both languages, for Spanish having
almost two times higher error rate than for English.
This is due to the very rich morphology of Spanish
verbs - one base form might have up to about fourty
different inflections.
1Reduction of FPER leads to a similar reduction of PER.
 0
 0.5
 1
 1.5
 2
 2.5
PUNNUMPREP CONDETPRONADVAVN
inflectional errors [%]
English
Spanish
Figure 2: Inflectional error rates [%] for English and
Spanish output
Nouns have a higher error rate for English than
for Spanish. The reason for this difference is not
clear, since the noun morphology of neither of the
languages is particularly rich - there is only distinc-
tion between singular and plural. One possible ex-
planation might be the numerous occurences of dif-
ferent variants of the same word, like for example
?Mr? and ?Mister?.
In the Spanish output, two additional POS classes
are showing significant error rate: determiners and
adjectives. This is due to the gender and number in-
flections of those classes which do not exist in the
English language - for each determiner or adjective,
there are four variants in Spanish and only one in En-
glish. Working on inflections of Spanish verbs might
reduce approximately 2% of FPER, on English verbs
about 1%. Improvements of Spanish determiners
could lead up to about 2% of improvements.
6.2.1 Comparison with human error analysis
The results obtained for inflectional errors are
comparable with the results of a human error anal-
ysis carried out in (Vilar et al, 2006). Although it
is difficult to compare all the numbers directly, the
overall tendencies are the same: the largest num-
ber of translation errors are caused by Spanish verbs,
and much less but still a large number of errors by
English verbs. A much smaller but still significant
number of errors is due to Spanish adjectives, and
only a few errors of English adjectives are present.
Human analysis was done also for the tense and
53
person of verbs, as well as for the number and gen-
der of adjectives. We use more detailed POS tags in
order to extract this additional information and cal-
culate inflectional error rates for such tags. It should
be noted that in contrast to all previous error rates,
these error rates are not disjunct but overlapping:
many words are contributing to both.
The results are shown in Figure 3, and the tenden-
cies are again the same as those reported in (Vilar
et al, 2006). As for verbs, tense errors are much
more frequent than person errors for both languages.
Adjective inflections cause certain amount of errors
only in the Spanish output. Contributions of gender
and of number are aproximately equal.
 0
 0.5
 1
 1.5
 2
A numberA genderV personV tense
inflectional errors of verbs and adjectives [%]
English
Spanish
Figure 3: More details about inflections: verb tense
and person error rates and adjective gender and num-
ber error rates [%]
6.3 Missing words
Figure 4 presents the distribution of missing words
over POS classes. This distribution has a same be-
haviour as the one obtained by human error analysis.
Most missing words for both languages are verbs.
For English, the percentage of missing verbs is sig-
nificantly higher than for Spanish. The same thing
happens for pronouns. The probable reason for this
is the nature of Spanish verbs. Since person and
tense are contained in the suffix, Spanish pronouns
are often omitted, and auxiliary verbs do not exist
for all tenses. This could be problematic for a trans-
lation system, because it processes only one Spanish
word which actually contains two (or more) English
words.
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
PUNNUMPREP CONDETPRONADVAVN
missing words [%]
eng
esp
Figure 4: Distribution of missing words over POS
classes [%] for English and Spanish output
Prepositions are more often missing in Spanish
than in English, as well as determiners. A probable
reason is the disproportion of the number of occur-
rences for those classes between two languages.
7 Conclusions
This work presents a framework for extraction of lin-
guistic details from standard word error rates WER
and PER and their use for an automatic error analy-
sis. We presented a method for the decomposition of
standard word error rates WER and PER over ten ba-
sic POS classes. We also carried out a detailed anal-
ysis of inflectional errors which has shown that the
results obtained by our method correspond to those
obtained by a human error analysis. In addition, we
proposed a method for analysing missing word er-
rors.
We plan to extend the proposed methods in order
to carry out a more detailed error analysis, for ex-
ample examining different types of verb inflections.
We also plan to examine other types of translation
errors like for example errors caused by word order.
Acknowledgements
This work was partly funded by the European Union
under the integrated project TC-STAR? Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738).
54
References
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing BLEU MT Evaluation Method with Frequency
Weighting. In Proc. of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL),
Barcelona, Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In 43rd
Annual Meeting of the Assoc. for Computational Lin-
guistics: Proc. Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 65?72, Ann Arbor, MI, June.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proc. 4th Int. Conf. on Lan-
guage Resources and Evaluation (LREC), pages 239?
242, Lisbon, Portugal, May.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Gregor Leusch, Nicola Ueffing, David Vilar, and Her-
mann Ney. 2005. Preprocessing and Normalization
for Automatic Evaluation of Machine Translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 17?24, Ann Arbor, MI, June. Association
for Computational Linguistics.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In EACL06, pages 241?248, Trento, Italy,
April.
Vladimir Iosifovich Levenshtein. 1966. Binary Codes
Capable of Correcting Deletions, Insertions and Re-
versals. Soviet Physics Doklady, 10(8):707?710,
February.
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating Machine Transla-
tion Output with Automatic Sentence Segmentation.
In Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 148?154,
Pittsburgh, PA, October.
Evgeny Matusov, Richard Zens, David Vilar, Arne
Mauser, Maja Popovic?, and Hermann Ney. 2006.
The RWTH Machine Translation System. In TC-Star
Workshop on Speech-to-Speech Translation, pages 31?
36, Barcelona, Spain, June.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2006. Error Analysis
of Verb Inflections in Spanish Translation Output. In
TC-Star Workshop on Speech-to-Speech Translation,
pages 99?103, Barcelona, Spain, June.
Maja Popovic?, Adria` de Gispert, Deepa Gupta, Patrik
Lambert, Hermann Ney, Jose? B. Marin?o, Marcello
Federico, and Rafael Banchs. 2006. Morpho-syntactic
Information for Automatic Error Analysis of Statisti-
cal Machine Translation Output. In Proc. of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 1?6, New York, NY, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Error Rate with Targeted Human An-
notation. In Proc. of the 7th Conf. of the Association
for Machine Translation in the Americas (AMTA 06),
pages 223?231, Boston, MA.
2005. TC-STAR - technology and corpora for speech to
speech translation. Integrated project TCSTAR (IST-
2002-FP6-506738) funded by the European Commis-
sion. http://www.tc-star.org/.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical Machine Transla-
tion of European Parliamentary Speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In Proc. of the Fifth Int.
Conf. on Language Resources and Evaluation (LREC),
pages 697?702, Genoa, Italy, May.
Atro Voutilainen. 1995. ENGCG -
Constraint Grammar Parser of English.
http://www2.lingsoft.fi/doc/engcg/intro/.
55
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 29?32,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Syntax-oriented evaluation measures for machine translation output
Maja Popovic? and Hermann Ney
RWTH Aachen University
Aachen, Germany
popovic,ney@informatik.rwth-aachen.de
Abstract
We explored novel automatic evaluation
measures for machine translation output
oriented to the syntactic structure of the
sentence: the BLEU score on the detailed
Part-of-Speech (POS) tags as well as the
precision, recall and F-measure obtained
on POS n-grams. We also introduced F-
measure based on both word and POS n-
grams. Correlations between the new met-
rics and human judgments were calcu-
lated on the data of the first, second and
third shared task of the Statistical Machine
Translation Workshop. Machine transla-
tion outputs in four different European
languages were taken into account: En-
glish, Spanish, French and German. The
results show that the new measures cor-
relate very well with the human judge-
ments and that they are competitive with
the widely used BLEU, METEOR and TER
metrics.
1 Introduction
We proposed several syntax-oriented automatic
evaluation measures based on sequences of POS
tags and investigated how they correlate with hu-
man judgments. The new measures are the POS-
BLEU score, i.e. the BLEU score calculated on
POS tags instead of words, as well as the POSP, the
POSR and the POSF score: precision, recall and F-
measure calculated on POS n-grams. In addition
to the metrics based only on POS tags, we investi-
gated a WPF score, i.e. an F-measure which takes
into account both word and POS n-grams.
The correlations on the document level were
computed on the English, French, Spanish and
German texts generated by various translation sys-
tems in the framework of the first (Koehn and
Monz, 2006), second (Callison-Burch et al, 2007)
and third shared translation task (Callison-Burch
et al, 2008). Preliminary experiments were car-
ried out on the data from the first (2006) and
the second task (2007) ? Spearman?s rank corre-
lation coefficients between the adequacy and flu-
ency scores and the POSBLEU, POSP, POSR and
POSF scores were calculated. The POSBLEU and
the POSF score were shown to be the most promis-
ing, so that these metrics were submitted to the
official shared evaluation task 2008. The results
of this evaluation showed that these metrics also
correlate well on the document level with another
human score, i.e. the sentence ranking. However,
on the sentence level the results were less promis-
ing. The possible reason for this is the main draw-
back of the metrics based on pure POS tags, i.e.
neglecting the lexical aspect. Therefore we also
introduced a WPF score which takes into account
both word n-grams and POS n-grams.
2 Syntactic-oriented evaluation metrics
We investigated the following metrics oriented on
the syntactic structure of a translation output:
? POSBLEU
The standard BLEU score (Papineni et al,
2002) calculated on POS tags instead of
words;
? POSP
POS n-gram precision: percentage of POS n-
grams in the hypothesis which have a coun-
terpart in the reference;
? POSR
Recall measure based on POS n-grams: per-
centage of POS n-grams in the reference
which are also present in the hypothesis;
? POSF
POS n-gram based F-measure: takes into ac-
count all POS n-grams which have a counter-
29
part, both in the reference and in the hypoth-
esis.
? WPF
F-measure based both on word and POS n-
grams: takes into account all word n-grams
and all POS n-grams which have a counter-
part both in the corresponding reference and
hypothesis.
The prerequisite for all metrics is availability of
an appropriate POS tagger for the target language.
It should be noted that the POS tags cannot be only
basic but must have all details (e.g. verb tenses,
cases, number, gender, etc.).
The n-gram scores as well as the POSBLEU
score are based on fourgrams (i.e. the value of
maximal n is 4). For the n-gram-based measures,
two types of n-gram averaging were investigated:
geometric mean and aritmetic mean. Geometric
mean is already widely used in the BLEU score, but
is also argued not to be optimal because the score
becomes equal to zero even if only one of the n-
gram counts is equal to zero. However, this prob-
lem is probably less critical for POS-based metrics
because the tag set sizes are much smaller than vo-
cabulary sizes.
3 Correlations between the new metrics
and human judgments
The syntax-oriented evaluation metrics were com-
pared with human judgments by means of Spear-
man correlation coefficients ?. Spearman?s rank
correlation coefficient is equivalent to Pearson cor-
relation on ranks, and its advantage is that it makes
fewer assumptions about the data. The possible
values of ? range between 1 (if all systems are
ranked in the same order) and -1 (if all systems are
ranked in the reverse order). Thus the higher value
of ? for an automatic metric, the more similar it
is to the human metric. Correlation coefficients
between human scores and three well-known au-
tomatic measures BLEU, METEOR and TER were
calculated as well, in order to see how the new
metrics perform in comparison with widely used
metrics. The scores were calculated for outputs
of translation from Spanish, French and German
into English and vice versa. English and Ger-
man POS tags were produced using the TnT tag-
ger (Brants, 2000), Spanish texts were annotated
using the FreeLing analyser (Carreras et al, 2004),
and French texts using the TreeTagger1. In this
way, all references and hypotheses were provided
with detailed POS tags.
Experiments on 2006 and 2007 test data
The preliminary experiments with the new eval-
uation metrics were performed on the data from
the first two shared tasks in order to investigate
Spearman correlation coefficients ? between POS-
based evaluation measures and the human scores
adequacy and fluency. The metrics described in
Section 2 (except the WPF score) were calculated
for all translation outputs. For each new metric,
the ? coefficient with the adequacy and with the
fluency score on the document level were calcu-
lated. Then the results were summarised by aver-
aging obtained coefficients over all translation out-
puts, and the average correlations are presented in
Table 1.
2006+2007 adequacy fluency
BLEU 0.590 0.544
METEOR 0.598 0.538
TER 0.496 0.479
POSBLEU 0.642 0.626
POSF gm 0.586 0.551
am 0.584 0.570
POSR gm 0.572 0.576
am 0.542 0.544
POSP gm 0.551 0.481
am 0.531 0.461
Table 1: Average system-level correlations be-
tween automatic evaluation measures and ade-
quacy/fluency scores for 2006 and 2007 test data
(gm = geometric mean for n-gram averaging, am
= arithmetic mean).
Table 1 shows that the new measures have
high ? coefficients both with respect to the ade-
quacy and to the fluency score. The POSBLEU
score has the highest correlations, followed by the
POSF score. Furthermore, the POSBLEU score has
higher correlations than each of the three widely
used metrics, and all the new metrics except the
POSP have higher correlations than the TER. The
POSF correlations with the fluency are higher than
those for the standard metrics, and with the ad-
equacy are comparable to those for the METEOR
and the BLEU score.
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
30
Table 2 presents the percentage of the docu-
ments for which the particular new metric has
higher correlation than BLEU, METEOR or TER. It
can be seen that on the majority of the documents
the POSBLEU metric outperforms all three stan-
dard measures, especially the correlation with the
fluency score. The geometric mean POSF shows
similar behaviour, having higher correlation than
the standard measures in majority of the cases but
slightly less often than the POSBLEU. The POSR
has higher correlation than the standard measures
in 50-70% of cases, and the POSP score has the
lowest percentage, 30-60%. It can be also seen
that the geometric mean averaging of the n-grams
correlates better with the human judgments more
often than the artimetic mean.
Experiments on 2008 test data
For the official shared evaluation task in 2008, the
human evaluation scores were different ? the ad-
equacy and fluency scores were abandoned being
rather time consuming and often inconsistent, and
the sentence ranking was proposed as one of the
human evaluation scores: the manual evaluators
were asked to rank translated sentences relative
to each other. RWTH participated in this shared
task with the two most promising metrics accord-
ing to the previous experiments, i.e. POSBLEU
and POSF, and the detailed results can be found
in (Callison-Burch et al, 2008). It was shown that
these metrics also correlate very well with the sen-
tence ranking on the document level. However,
on the sentence level the performance was much
weaker: a percentage of sentence pairs for which
the human comparison yields the same result as
the comparison using particular automatic metric
was not very high. We believe that the main rea-
son for this is the fact that the metrics based only
on the POS tags can assign high scores to transla-
tions without correct semantic meaning, because
they are taking into account only syntactic struc-
ture without taking into account the actual words.
For example, if the reference translation is ?This
sentence is correct?, a translation output ?This tree
is high? would have a POS-based matching score
of 100%. Therefore we introduced the WPF score
? an F-measure metrics which counts both match-
ing POS n-grams and matching word n-grams.
The ? coefficients for the POSBLEU, POSF and
WPF with the sentence ranking averaged over all
translation outputs are shown in Table 3. The cor-
relations for several known metrics are shown as
well, i.e. for the BLEU, METEOR and TER along
with their variants: METEOR-r denotes the vari-
ant optimised for ranking, whereas MBLEU and
MTER are BLEU and TER computed using the
flexible matching as used in METEOR. It can be
seen that the correlation coefficients for all three
syntactic metrics are high. The POSBLEU score
has the highest correlation with the sentence rank-
ing, followed by POSF and WPF. All three mea-
sures have higher average correlation than MTER,
MBLEU and BLEU. The purely syntactic metrics
outperform also the METEOR scores, whereas the
WPF correlations are comparable with those of the
METEOR scores.
2008 sentence ranking
BLEU 0.526
MBLEU 0.504
METEOR 0.638
METEOR-r 0.603
MTER 0.318
POSBLEU 0.712
POSF gm 0.663
am 0.661
WPF gm 0.600
am 0.628
Table 3: Average system-level correlations be-
tween automatic evaluation measures and human
ranking for 2008 test data.
Table 4 presents the percentage of the docu-
ments where the particular syntactic metric has
higher correlation with the sentence ranking than
the particular standard metric. All syntactic met-
rics have higher correlation than the MTER on al-
most all documents, and on a large number of doc-
uments than the MBLEU score. The correlations
for syntactic measures are better than those for the
BLEU score for more than 60% of documents. As
for the METEOR scores, the syntactic metrics are
comparable (about 50%).
4 Conclusions
The results presented in this article suggest that
the syntactic information has the potential to
strenghten automatic evaluation metrics, and there
are many possible directions for future work. We
proposed several syntax-oriented evaluation met-
rics based on the detailed POS tags: the POS-
BLEU score and POS-n-gram precision, recall and
31
adequacy fluency
2006+2007 BLEU METEOR TER BLEU METEOR TER
POSBLEU 77.3 58.3 75.0 81.8 83.3 83.3
POSF gm 72.7 58.3 75.0 63.6 75.0 83.3
am 68.2 58.3 75.0 63.6 66.7 68.1
POSR gm 63.6 75.0 58.3 68.1 66.7 58.3
am 54.5 75.0 58.3 63.6 58.3 50.0
POSP gm 63.6 50.0 75.0 45.4 50.0 58.3
am 54.5 41.7 66.7 36.4 50.0 58.3
Table 2: Percentage of documents from the 2006 and 2007 shared tasks where the particular new metric
has better correlation with adequacy/fluency than the particular standard metric.
2008 BLEU MBLEU MTER METEOR METEOR-r
POSBLEU 71.4 85.7 92.8 57.1 64.3
POSF am 64.3 78.6 92.8 50.0 50.0
gm 64.3 78.6 92.8 57.1 50.0
WPF am 57.1 64.3 100 42.8 50.0
gm 57.1 64.3 92.8 42.8 50.0
Table 4: Percentage of documents from the 2008 shared task where the new metric has better correlation
with the human sentence ranking than the standard metric.
F-measure, i.e. the POSP, POSR, and POSF score.
In addition, we introduced a measure which takes
into account both POS tags and words: the WPF
score. We carried out an extensive analysis of
the Spearman?s rank correlation coefficients be-
tween the syntactic evaluation metrics and the hu-
man judgments. The obtained results showed that
the new metrics correlate well with human judg-
ments, namely the adequacy and fluency scores,
as well as the sentence ranking. The results also
showed that the syntax-oriented metrics are com-
petitive with the widely used evaluation measures
BLEU, METEOR and TER. Especially promising
are the POSBLEU and the POSF score. The cor-
relations of the WPF score are slightly lower than
those of the purely POS based metrics ? however,
this metric has advantage of taking both syntactic
and lexical aspect into account.
Acknowledgments
This work was realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovati on.
References
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
pages 224?231, Seattle, WA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-)Evaluation of Machine Translation. In Pro-
ceedings of the ACL Workshop on Statistical Ma-
chine Translation, pages 136?158, Prague, Czech
Republic, June.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, Columbus, Ohio, June.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 239?242, Lisbon, Portugal,
May.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proceedings on the Work-
shop on Statistical Machine Translation, New York
City, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
32
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 66?69,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH Machine Translation System for WMT 2009
Maja Popovic?, David Vilar, Daniel Stein, Evgeny Matusov and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the shared transla-
tion task of the Fourth Workshop of Sta-
tistical Machine Translation (WMT 2009)
with the German-English, French-English
and Spanish-English pair in each transla-
tion direction. The submissions were gen-
erated using a phrase-based and a hierar-
chical statistical machine translation sys-
tems with appropriate morpho-syntactic
enhancements. POS-based reorderings of
the source language for the phrase-based
systems and splitting of German com-
pounds for both systems were applied. For
some tasks, a system combination was
used to generate a final hypothesis. An ad-
ditional English hypothesis was produced
by combining all three final systems for
translation into English.
1 Introduction
For the WMT 2009 shared task, RWTH submit-
ted translations for the German-English, French-
English and Spanish-English language pair in both
directions. A phrase-based translation system en-
hanced with appropriate morpho-syntactic trans-
formations was used for all translation direc-
tions. Local POS-based word reorderings were ap-
plied for the Spanish-English and French-English
pair, and long range reorderings for the German-
English pair. For this language pair splitting
of German compounds was also applied. Spe-
cial efforts were made for the French-English and
German-English translation, where a hierarchi-
cal system was also used and the final submis-
sions are the result of a system combination. For
translation into English, an additional hypothesis
was produced as a result of combination of the
final German-to-English, French-to-English and
Spanish-to-English systems.
2 Translation models
2.1 Phrase-based model
We used a standard phrase-based system similar to
the one described in (Zens et al, 2002). The pairs
of source and corresponding target phrases are ex-
tracted from the word-aligned bilingual training
corpus. Phrases are defined as non-empty contigu-
ous sequences of words. The phrase translation
probabilities are estimated using relative frequen-
cies. In order to obtain a more symmetric model,
the phrase-based model is used in both directions.
2.2 Hierarchical model
The hierarchical phrase-based approach can be
considered as an extension of the standard phrase-
based model. In this model we allow the phrases
to have ?gaps?, i.e. we allow non-contiguous parts
of the source sentence to be translated into pos-
sibly non-contiguous parts of the target sentence.
The model can be formalized as a synchronous
context-free grammar (Chiang, 2007). The model
also included some additional heuristics which
have shown to be helpful for improving translation
quality, as proposed in (Vilar et al, 2008).
The first step in the hierarchical phrase extrac-
tion is the same as for the phrased-based model.
Having a set of initial phrases, we search for
phrases which contain other smaller sub-phrases
and produce a new phrase with gaps. In our sys-
tem, we restricted the number of non-terminals for
each hierarchical phrase to a maximum of two,
which were also not allowed to be adjacent. The
scores of the phrases are again computed as rela-
tive frequencies.
2.3 Common models
For both translation models, phrase-based and hi-
erarchical, additional common models were used:
word-based lexicon model, phrase penalty, word
penalty and target language model.
66
The target language model was a standard n-
gram language model trained by the SRI language
modeling toolkit (Stolcke, 2002). The smooth-
ing technique we apply was the modified Kneser-
Ney discounting with interpolation. In our case we
used a 4-gram language model.
3 Morpho-syntactic transformations
3.1 POS-based word reorderings
For the phrase-based systems, the local and
long range POS-based reordering rules described
in (Popovic? and Ney, 2006) were applied on the
training and test corpora as a preprocessing step.
Local reorderings were used for the Spanish-
English and French-English language pairs in or-
der to handle differences between the positions of
nouns and adjectives in the two languages. Adjec-
tives in Spanish and French, as in most Romanic
languages, are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, for these language pairs local
reorderings of nouns and adjective groups in the
source language were applied. The following se-
quences of words are considered to be an adjective
group: a single adjective, two or more consecutive
adjectives, a sequence of adjectives and coordinate
conjunctions, as well as an adjective along with its
corresponding adverb. If the source language is
Spanish or French, each noun is moved behind the
corresponding adjective group. If the source lan-
guage is English, each adjective group is moved
behind the corresponding noun.
Long range reorderings were applied on the
verb groups for the German-English language pair.
Verbs in the German language can often be placed
at the end of a clause. This is mostly the case
with infinitives and past participles, but there are
many cases when other verb forms also occur at
the clause end. For the translation from German
into English, following verb types were moved to-
wards the beginning of a clause: infinitives, infini-
tives+zu, finite verbs, past participles and negative
particles. For the translation from English to Ger-
man, infinitives and past participles were moved
to the end of a clause, where punctuation marks,
subordinate conjunctions and finite verbs are con-
sidered as the beginning of the next clause.
3.2 German compound words
For the translation from German into English, Ger-
man compounds were split using the frequency-
based method described in (Koehn and Knight,
2003). For the other translation direction, the En-
glish text was first translated into the modified
German language with split compounds. The gen-
erated output was then postprocessed, i.e. the
components were merged using the method de-
scribed in (Popovic? et al, 2006): a list of com-
pounds and a list of components are extracted from
the original German training corpus. If the word
in the generated output is in the component list,
check if this word merged with the next word is in
the compound list. If it is, merge the two words.
4 System combination
For system combination we used the approach de-
scribed in (Matusov et al, 2006). The method is
based on the generation of a consensus transla-
tion out of the output of different translation sys-
tems. The core of the method consists in building
a confusion network for each sentence by align-
ing and combining the (single-best) translation hy-
pothesis from one MT system with the translations
produced by the other MT systems (and the other
translations from the same system, if n-best lists
are used in combination). For each sentence, each
MT system is selected once as ?primary? system,
and the other hypotheses are aligned to this hy-
pothesis. The resulting confusion networks are
combined into a signle word graph, which is then
weighted with system-specific factors, similar to
the approach of (Rosti et al, 2007), and a trigram
LM trained on the MT hypotheses. The translation
with the best total score within this word graph is
selected as consensus translation. The scaling fac-
tors of these models are optimized using the Con-
dor toolkit (Berghen and Bersini, 2005) to achieve
optimal BLEU score on the dev set.
5 Experimental results
5.1 Experimental settings
For all translation directions, we used the provided
EuroParl and News parallel corpora to train the
translation models and the News monolingual cor-
pora to train the language models. All systems
were optimised for the BLEU score on the develop-
ment data (the ?dev-a? part of the 2008 evaluation
data). The other part of the 2008 evaluation set
(?dev-b?) is used as a blind test set. The results re-
ported in the next section will be referring to this
test set. For the tasks including a system combi-
nation, the parameters for the system combination
67
were also trained on the ?dev-b? set. The reported
evaluation metrics are the BLEU score and two
syntax-oriented metrics which have shown a high
correlation with human evaluations: the PBLEU
score (BLEU calculated on POS sequences) and
the POS-F-score PF (similar to the BLEU score but
based on the F-measure instead of precision and
on arithmetic mean instead of geometric mean).
The POS tags used for reorderings and for syn-
tactic evaluation metrics for the English and the
German corpora were generated using the statisti-
cal n-gram-based TnT-tagger (Brants, 2000). The
Spanish corpora are annotated using the FreeLing
analyser (Carreras et al, 2004), and the French
texts using the TreeTagger1.
5.2 Translation results
Table 1 presents the results for the German-
English language pair. For translation from Ger-
man into English, results for the phrase-based sys-
tem with and without verb reordering and com-
pound splitting are shown. The hierarchical sys-
tem was trained with split German compounds.
The final submission was produced by combining
those five systems. The improvement obtained by
system combination on the unseen test data 2009
is similar, i.e. from the systems with BLEU scores
of 17.0%, 17.2%, 17.5%, 17.6% and 17.7% to the
final system with 18.5%.
German?English BLEU PBLEU PF
phrase-based 17.8 31.6 39.7
+reorder verbs 18.2 32.6 40.3
+split compounds 18.0 31.9 40.0
+reord+split 18.4 33.1 40.7
hierarchical+split 18.5 33.5 40.1
system combination 19.2 33.8 40.9
English?German BLEU PBLEU PF
phrase-based 13.6 31.6 39.7
+reorder verbs 13.7 32.4 40.2
+split compounds 13.7 32.3 40.1
+reord+split 13.7 32.3 40.1
system combination 14.0 32.7 40.3
Table 1: Translation results [%] for the German-
English language pair, News2008 dev-b.
The other translation direction is more difficult
and improvements from morpho-syntactic trans-
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
formations are smaller. No hierarchical system
was trained for this translation direction. The com-
bination of the four phrase-based systems leads
to further improvements (on the unseen test set
as well: contrastive hypotheses have the BLEU
scores in the range from 12.7% to 13.0%, and the
final BLEU score is 13.2%).
The results for the French-English language
pair are shown in Table 2. For the French-to-
English system, we submitted the result of the
combination of three systems: a phrase-based with
and without local reorderings and a hierarchical
system. For the unseen test set, the BLEU score of
the system combination output is 24.4%, whereas
the contrastive hypotheses have 23.2%, 23.4% and
24.1%. For the other translation direction we did
not use the system combination, the submission is
produced by the phrase-based system with local
adjective reorderings.
French?English BLEU PBLEU PF
phrase-based 20.9 37.1 43.2
+reorder adjectives 21.3 38.2 43.6
hierarchical 20.3 36.7 42.6
system combination 21.7 38.5 43.8
English?French BLEU PBLEU PF
phrase-based 20.2 39.5 45.9
+reorder adjectives 20.7 40.6 46.4
Table 2: Translation results [%] for the French-
English language pair, News2008 dev-b.
Table 3 presents the results for the Spanish-
English language pair. As in the English-to-
French translation, the phrase-based system with
adjective reorderings is used to produce the sub-
mitted hypothesis for both translation directions.
Spanish?English BLEU PBLEU PF
phrase-based 22.1 38.5 44.1
+reorder adjectives 22.5 39.2 44.6
English?Spanish BLEU PBLEU PF
phrase-based 20.6 29.3 35.7
+reorder adjectives 21.1 29.7 35.9
Table 3: Translation results [%] for the Spanish-
English language pair, News2008 dev-b.
68
The result of the additional experiment, i.e. for
the multisource translation int English is presented
in Table 4. The English hypothesis is produced by
the combination of the three best systems for each
language pair, and it can be seen that the transla-
tion performance increases in all measures. This
suggests that each language pair poses different
difficulties for the translation task, and the com-
bination of all three can improve performance.
F+S+G?English BLEU PBLEU PF
system combination 25.1 41.0 46.4
Table 4: Multisource translation results [%]:
the English hypothesis is obtained as result of
a system combination of all language pairs,
News2008 dev-b.
6 Conclusions
The RWTH system submitted to the WMT 2009
shared translation task used a phrase-based sys-
tem and a hierarchical system with appropriate
morpho-syntactic extensions, i.e. POS based word
reorderings and splitting of German compounds
were used. System combination produced gains
in BLEU score over phrasal-system baselines in
the German-to-English, English-to-German and
French-to-English tasks.
Acknowledgments
This work was realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
Powell?s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
pages 224?231, Seattle, WA.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 239?242, Lisbon, Portugal,
May.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, (33):201?228.
Philipp Koehn and Kevin Knight. 2003. Empiri-
cal methods for compound splitting. In Proceed-
ings 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 347?354, Budapest, Hungary, April.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems Using Enhanced
Hypotheses Alignment. In Proceedings of EACL
2006 (11th Conference of the European Chapter
of the Association for Computational Linguistics),
pages 33?40, Trento, Italy, April.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Trans-
lation. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC), pages 1278?1283, Genoa, Italy, May.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of german compound
words. In Proceedings of the 5th International Con-
ference on Natural Language Processing (FinTAL),
pages 616?624, Turku, Finland, August. Lecture
Notes in Computer Science, Springer Verlag.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining Outputs from Multiple Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
25th German Conference on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artifi-
cial Intelligence (LNAI), pages 18?32, Aachen, Ger-
many, September. Springer Verlag.
69
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65?70,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Evaluate with Confidence Estimation: Machine ranking of translation
outputs using grammatical features
Eleftherios Avramidis, Maja Popovic, David Vilar, Aljoscha Burchardt
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
name.surname@dfki.de
Abstract
We present a pilot study on an evaluation
method which is able to rank translation out-
puts with no reference translation, given only
their source sentence. The system employs a
statistical classifier trained upon existing hu-
man rankings, using several features derived
from analysis of both the source and the tar-
get sentences. Development experiments on
one language pair showed that the method
has considerably good correlation with human
ranking when using features obtained from a
PCFG parser.
1 Introduction
Automatic evaluation metrics for Machine Transla-
tion (MT) have mainly relied on analyzing both the
MT output against (one or more) reference transla-
tions. Though, several paradigms in Machine Trans-
lation Research pose the need to estimate the quality
through many translation outputs, when no reference
translation is given (n-best rescoring of SMT sys-
tems, system combination etc.). Such metrics have
been known as Confidence Estimation metrics and
quite a few projects have suggested solutions on this
direction. With our submission to the Shared Task,
we allow such a metric to be systematically com-
pared with the state-of-the-art reference-aware MT
metrics.
Our approach suggests building a Confidence Es-
timation metric using already existing human judg-
ments. This has been motivated by the existence
of human-annotated data containing comparisons of
the outputs of several systems, as a result of the
evaluation tasks run by the Workshops on Statistical
Machine Translation (WMT) (Callison-Burch et al,
2008; Callison-Burch et al, 2009; Callison-Burch
et al, 2010). This amount of data, which has been
freely available for further research, gives an op-
portunity for applying machine learning techniques
to model the human annotators? choices. Machine
Learning methods over previously released evalua-
tion data have been already used for tuning com-
plex statistical evaluation metrics (e.g. SVM-Rank
in Callison-Burch et al (2010)). Our proposition
is similar, but works without reference translations.
We develop a solution of applying machine learning
in order to build a statistical classifier that performs
similar to the human ranking: it is trained to rank
several MT outputs, given analysis of possible qual-
itative criteria on both the source and the target side
of every given sentence. As qualitative criteria, we
use statistical features indicating the quality and the
grammaticality of the output.
2 Automatic ranking method
2.1 From Confidence Estimation to ranking
Confidence estimation has been seen from the Nat-
ural Language Processing (NLP) perspective as a
problem of binary classification in order to assess
the correctness of a NLP system output. Previ-
ous work focusing on Machine Translation includes
statistical methods for estimating correctness scores
or correctness probabilities, following a rich search
over the spectrum of possible features (Blatz et al,
2004a; Ueffing and Ney, 2005; Specia et al, 2009;
Raybaud and Caroline Lavecchia, 2009; Rosti et al,
65
2007).
In this work we slightly transform the binary clas-
sification practice to fit the standard WMT human
evaluation process. As human annotators have pro-
vided their evaluation in the form of ranking of five
system outputs at a sentence level, we build our eval-
uation mechanism with similar functionality, aim-
ing to training from and evaluating against this data.
Evaluation scores and results can be then calculated
based on comparative analysis of the performance of
each system.
Whereas latest work, such as Specia et al (2010),
has focused on learning to assess segment perfor-
mance independently for each system output, our
contribution measures the performance by compar-
ing the system outputs with each other and con-
sequently ranking them. The exact method is de-
scribed below.
2.2 Internal pairwise decomposition
We build one classifier over all input sentences.
While the evaluation mechanism is trained and eval-
uated on a multi-class (ranking) basis as explained
above, the classifier is expected to work on a binary
level: we provide the features from the analysis of
the two system outputs and the source, and the clas-
sifier should decide if the first system output is better
than the second one or not.
In order to accomplish such training, the n sys-
tems? outputs for each sentence are broken down to
n ? (n ? 1) pairs, of all possible comparisons be-
tween two system outputs, in both directions (sim-
ilar to the calculation of the Spearman correlation).
For each pair, the classifier is trained with a class
value c, for the pairwise comparison of system out-
puts ti and tj with respective ranks ri and rj , deter-
mined as:
c(ri, rj) =
{
1 ri < rj
?1 ri > rj
At testing time, after the classifier has made all
the pairwise decisions, those need to be converted
back to ranks. System entries are ordered, according
to how many times each of them won in the pair-
wise comparison, leading to rank lists similar to the
ones provided by human annotators. Note that this
kind of decomposition allows for ties when there are
equal times of winnings.
2.3 Acquiring features
In order to obtain features indicating the quality of
the MT output, automatic NLP analysis tools are ap-
plied on both the source and the two target (MT-
generated) sentences of every pairwise comparison.
Features considered can be seen in the following cat-
egories, according to their origin:
? Sentence length: Number of words of source
and target sentences, source-length to target-
length ratio.
? Target language model: Language models
provide statistics concerning the correctness of
the words? sequence on the target language.
Such language model features include:
? the smoothed n-gram probability of the
entire target sentence for a language
model of order 5, along with
? uni-gram, bi-gram, tri-gram probabilities
and a
? count of unknown words
? Parsing: Processing features acquired from
PCFG parsing (Petrov et al, 2006) for both
source and target side include:
? parse log likelihood,
? number of n-best trees,
? confidence for the best parse,
? average confidence of all trees.
Ratios of the above target features to their re-
spective source features were included.
? Shallow grammatical match: The number of
occurences of particular node tags on both the
source and the target was counted on the PCFG
parses. In particular, NPs, VPs, PPs, NNs and
punctuation occurences were counted. Then
the ratio of the occurences of each tag in the
target sentence by its occurences on the source
sentence was also calculated.
2.4 Classifiers
The machine learning core of the system was built
supporting two classification approaches.
66
? Na?ve Bayes allows prediction of a binary
class, given the assumption that the features are
statistically independent.
p(C,F1, . . . , Fn) = p(C)
i=1?
n
p(Fi|C)
p(C) is estimated by relative frequencies of
the training pairwise examples, while p(Fi|C)
for our continuous features are estimated with
LOESS (locally weighted linear regression
similar to Cleveland (1979))
? k-nearest neighbour (knn) algorithm allows
classifying based on the closest training exam-
ples in the feature space.
3 Experiment
3.1 Experiment setup
A basic experiment was designed in order to deter-
mine the exact setup and the feature set of the metric
prior to the shared task submission. The classifiers
for the task were learnt using the German-English
testset of the WMT 2008 and 2010 (about 700 sen-
tences)1. For testing, the classifiers were used to per-
form ranking on a test set of 184 sentences which
had been kept apart from the 2010 data, with the cri-
terion that they do not contain contradictions among
human judgments.
In order to allow further comparison with other
evaluation metrics, we performed an extended ex-
periment: we trained the classifiers over the WMT
2008 and 2009 data and let them perform automatic
ranking on the full WMT 2010 test set, this time
without any restriction on human evaluation agree-
ment.
In both experiments, tokenization was performed
with the PUNKT tokenizer (Kiss et al, 2006; Gar-
rette and Klein, 2009), while n-gram features were
generated with the SRILM toolkit (Stolcke, 2002).
The language model was relatively big and had been
built upon all lowercased monolingual training sets
for the WMT 2011 Shared Task, interpolated on
the 2007 test set. As a PCFG parser, the Berkeley
Parser (Petrov and Klein, 2007) was preferred, due
1data acquired from http://www.statmt.org/wmt11
to the possibility of easily obtaining complex inter-
nal statistics, including n-best trees. Unfortunately,
the time required for parsing leads to significant de-
lays at the overall processing. The machine learn-
ing algorithms were implemented with the Orange
toolkit (Dem?ar et al, 2004).
3.2 Feature selection
Although the automatic NLP tools provided a lot of
features (section 2.3), the classification methods we
used (and particularly na?ve Bayes were the develop-
ment was focused on) would be expected to perform
better given a smaller group of statistically inde-
pendent features. Since exhaustive training/testing
of all possible feature subsets was not possible,
we performed feature selection based on the Reli-
eff method (Kononenko, 1994; Kira and Rendell,
1992). Automatic ranking was performed based on
the most promising feature subsets. The results are
examined below.
3.3 Results
The performance of the classifier is measured after
the classifier output has been converted back to rank
lists, similar to the WMT 2010 evaluation. We there-
fore calculated two types of rank coefficients: aver-
aged Kendall?s tau on a segment level, and Spear-
man?s rho on a system level, based on the percentage
that the each system?s translations performed better
than or equal to the translations of any other system.
The results for the various combinations of fea-
tures and classifiers are depicted on Table 1. Na?ve
Bayes provides the best score on the test set, with
? = 0.81 on a system level and ? = 0.26 on a
segment level, trained with features including the
number of the unknown words, the source-length
by target-length ratio, the VP count ratio and the
source-target ratio of the parsing log-likelihood. The
number of unknown words particularly appears to be
a strong indicator for the quality of the sentence. On
the first part of the table we can also observe that
language model features do not perform as well as
the features deriving from the processing informa-
tion delivered by the parser. On the second part of
the table we compare the use of various grammatical
combinations. The third part contains the correlation
obtained by various similar internal parsing-related
features.
67
features na?ve Bayes knn
rho tau rho tau
basic experiment
ngram 0.19 0.05 0.13 0.01
unk, len 0.67 0.20 0.73 0.24
unk, len, bigram 0.61 0.21 0.74 0.21
unk, len, ngram 0.63 0.19 0.59 0.21
unk, len, trigram 0.67 0.20 0.76 0.21
unk, len, logparse 0.75 0.21 0.74 0.25
unk, len, nparse, VP 0.67 0.24 0.61 0.20
unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24
unk, len, nparse, NP, confbestparse 0.78 0.23 0.74 0.23
unk, len, nparse, VP, confavg 0.75 0.21 0.78 0.23
unk, len, nparse, VP, confbestparse 0.78 0.25 0.75 0.24
unk, len, nparse, VP, logparse 0.81 0.26 0.75 0.23
extended experiment
unk, len, nparse, VP, logparse 0.60 0.23 0.28 0.02
Table 1: System-level Spearman?s rho and segment-level Kendall?s tau correlation coefficients achieved on automatic
ranking (average absolute value)
The correlation coefficients of the extended exper-
iment, allowing comparison with last year?s shared
task, are shown on the last line of the table. With
coefficients ? = 0.60 and ? = 0.23, our metric
performs relatively low compared to the other met-
rics of WMT10 (indicatively iBLEU: ? = 0.95,
? = 0.39 according to Callison-Burch et al (2010).
Though, it still has a position in the list, scoring bet-
ter than several other reference-aware metrics (e.g.
of ? = 0.47 and ? = 0.12 respectively) for the par-
ticular language pair.
4 Discussion
A concern on the use of Confidence Estimation for
MT evaluation has to do with the possibility of a
system ?tricking? such metrics. This would for ex-
ample be the case when a system offers a well-
formed candidate translation and gets a good score,
despite having no relation to the source sentence
in terms of meaning. We should note that we are
not capable of fully investigating this case based
on the current set of experiments, because all of
the systems in our data sets have shown acceptable
scores (11-25 BLEU and 0.58-0.78 TERp accord-
ing to Callison-Burch et al (2010)), when evaluated
against reference translations. Though, we would
assume that we partially address this problem by us-
ing ratios of source to target features (length, syn-
tactic constituents), which means that in order for a
sentence to trick the metric, it would need a com-
parable sentence length and a grammatical structure
that would allow it to achieve feature ratios similar
to the other systems? outputs. Previous work (Blatz
et al, 2004b; Ueffing and Ney, 2005) has used fea-
tures based on word alignment, such as IBM Mod-
els, which would be a meaningful addition from this
aspect.
Although k-nearest-neighbour is considered to be
a superior classifier, best results are obtained by
na?ve Bayes. This may have been due of the fact
that feature selection has led to small sets of uncor-
related features, where na?ve Bayes is known to per-
form well. K-nearest-neighbour and other complex
classification methods are expected to prove useful
when more complex feature sets are employed.
5 Conclusion and Further work
The experiments presented in this article indicate
that confidence metrics trained over human rankings
can be possibly used for several tasks of evaluation,
given particular conditions, where e.g. there is no
reference translation given. Features obtained from
68
a PCFG parser seem to be leading to better correla-
tions, given our basic test set. Although correlation
is not particularly high, compared to other reference-
aware metrics in WMT 10, there is clearly a poten-
tial for further improvement.
Nevertheless this is still a small-scale experiment,
given the restricted data size and the single transla-
tion direction. The performance of the system on
broader training and test sets will be evaluated in the
future. Feature selection is also subject to change
if other language pairs are introduced, while more
sophisticated machine learning algorithms, allowing
richer feature sets, may also lead to better results.
Acknowledgments
This work was done with the support of the
TaraXU? Project2, financed by TSB Technologie-
stiftung Berlin?Zukunftsfonds Berlin, co-financed
by the European Union?European fund for regional
development.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004a. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004b. Confidence estimation for
machine translation. In M. Rollins (Ed.), Mental Im-
agery. Yale University Press.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2http://taraxu.dfki.de
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American statistical association, 74(368):829?836.
Janez Dem?ar, Blaz Zupan, Gregor Leban, and Tomaz
Curk. 2004. Orange: From experimental machine
learning to interactive data mining. In Principles of
Data Mining and Knowledge Discovery, pages 537?
539.
Dan Garrette and Ewan Klein. 2009. An extensi-
ble toolkit for computational semantics. In Proceed-
ings of the Eighth International Conference on Com-
putational Semantics, IWCS-8 ?09, pages 116?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kenji Kira and Larry A. Rendell. 1992. The feature se-
lection problem: traditional methods and a new algo-
rithm. In Proceedings of the tenth national conference
on Artificial intelligence, AAAI?92, pages 129?134.
AAAI Press.
Tibor Kiss, Jan Strunk, Ruhr universit?t Bochum, and
Ruhr universit?t Bochum. 2006. Unsupervised mul-
tilingual sentence boundary detection. In Proceedings
of IICS-04, Guadalajara, Mexico and Springer LNCS
3473.
Igor Kononenko. 1994. Estimating attributes: analy-
sis and extensions of relief. In Proceedings of the
European conference on machine learning on Ma-
chine Learning, pages 171?182, Secaucus, NJ, USA.
Springer-Verlag New York, Inc.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In In HLT-NAACL ?07.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In In ACL ?06, pages 433?
440.
Sylvain Raybaud and Kamel Smaili Caroline Lavecchia,
David Langlois. 2009. Word-and sentence-level con-
fidence measures for machine translation. In Euro-
pean Association of Machine Translation 2009.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of the North
American Chapter of the Association for Compu-
tational Linguistics Human Language Technologies,
pages 228?235.
69
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009. Improv-
ing the confidence of machine translation quality es-
timates. In Machine Translation Summit XII, Ottawa,
Canada.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24:39?50, March.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP 2002, pages 901?904.
Nicola Ueffing and Hermann Ney. 2005. Word-level
confidence estimation for machine translation using
phrase-based translation models. Computational Lin-
guistics, pages 763?770.
70
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99?103,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Evaluation without references:
IBM1 scores as evaluation metrics
Maja Popovic?, David Vilar, Eleftherios Avramidis, Aljoscha Burchardt
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
name.surname@dfki.de
Abstract
Current metrics for evaluating machine trans-
lation quality have the huge drawback that
they require human-quality reference transla-
tions. We propose a truly automatic evalua-
tion metric based on IBM1 lexicon probabili-
ties which does not need any reference transla-
tions. Several variants of IBM1 scores are sys-
tematically explored in order to find the most
promising directions. Correlations between
the new metrics and human judgments are cal-
culated on the data of the third, fourth and fifth
shared tasks of the Statistical Machine Trans-
lation Workshop. Five different European lan-
guages are taken into account: English, Span-
ish, French, German and Czech. The results
show that the IBM1 scores are competitive
with the classic evaluation metrics, the most
promising being IBM1 scores calculated on
morphemes and POS-4grams.
1 Introduction
Currently used evaluation metrics such as BLEU (Pa-
pineni et al, 2002), METEOR (Banerjee and Lavie,
2005), etc. are based on the comparison between
human reference translations and the automatically
generated hypotheses in the target language to be
evaluated. While this scenario helps in the design
of machine translation systems, it has two major
drawbacks. The first one is the practical criticism
that using reference translations is inefficient and ex-
pensive: in real-life situations, the quality of ma-
chine translation must be evaluated without having
to pay humans for producing reference translations
first. The second criticism is methodological: in
using reference translation, the problem of evalu-
ating translation quality (e.g., completeness, order-
ing, domain fit, etc.) is transformed into a kind of
paraphrase evaluation in the target language, which
is a very difficult problem itself. In addition, the
set of selected references always represents only a
small subset of all good translations. To remedy
these drawbacks, we propose a truly automatic eval-
uation metric which is based on the IBM1 lexicon
scores (Brown et al, 1993).
The inclusion of IBM1 scores in translation sys-
tems has shown experimentally to improve transla-
tion quality (Och et al, 2003). They also have been
used for confidence estimation for machine transla-
tion (Blatz et al, 2003). To the best of our knowl-
edge, these scores have not yet been used as an eval-
uation metric.
We carry out a systematic comparison between
several variants of IBM1 scores. The Spearman?s
rank correlation coefficients on the document (sys-
tem) level between the IBM1 metrics and the hu-
man ranking are computed on the English, French,
Spanish, German and Czech texts generated by var-
ious translation systems in the framework of the
third (Callison-Burch et al, 2008), fourth (Callison-
Burch et al, 2009) and fifth (Callison-Burch et al,
2010) shared translation tasks.
2 IBM1 scores
The IBM1 model is a bag-of-word translation model
which gives the sum of all possible alignment proba-
bilities between the words in the source sentence and
the words in the target sentence. Brown et al (1993)
defined the IBM1 probability score for a translation
99
pair fJ1 and eI1 in the following way:
P (fJ1 |eI1) =
1
(I + 1)J
J
?
j=1
I
?
i=0
p(fj |ei) (1)
where fJ1 is the source language sentence of length
J and eI1 is the target language sentence of length I .
As it is a conditional probability distribution, we
investigated both directions as evaluation metrics. In
order to avoid frequent confusions about what is the
source and what the target language, we defined our
scores in the following way:
? source-to-hypothesis (sh) IBM1 score:
IBM1sh =
1
(H + 1)S
S
?
j=1
H
?
i=0
p(sj|hi) (2)
? hypothesis-to-source (hs) IBM1 score:
IBM1hs =
1
(S + 1)H
H
?
i=1
S
?
j=0
p(hi|sj) (3)
where sj are the words of the original source lan-
guage sentence, S is the length of this sentence, hi
are the words of the target language hypothesis, and
H is the length of this hypothesis.
In addition to the standard IBM1 scores calculated
on words, we also investigated:
? MIBM1 scores ? IBM1 scores of word mor-
phemes in each direction;
? PnIBM1 scores ? IBM1 scores of POS n-grams
in each direction.
A parallel bilingual corpus for the desired lan-
guage pair and a tool for training the IBM1 model
are required in order to obtain IBM1 probabilities
p(fj|ei). For the POS n-gram scores, appropriate
POS taggers for each of the languages are necessary.
The POS tags cannot be only basic but must have
all details (e.g. verb tenses, cases, number, gender,
etc.). For the morpheme scores, a tool for splitting
words into morphemes is necessary.
3 Experiments on WMT 2008, WMT 2009
and WMT 2010 test data
3.1 Experimental set-up
The IBM1 probabilities necessary for the IBM1
scores are learnt using the WMT 2010 News
Commentary bilingual corpora consisting of the
Spanish-English, French-English, German-English
and Czech-English parallel texts. Spanish, French,
German and English POS tags were produced using
the TreeTagger1, and the Czech texts are tagged us-
ing the COMPOST tagger (Spoustova? et al, 2009).
The morphemes for all languages are obtained us-
ing the Morfessor tool (Creutz and Lagus, 2005).
The tool is corpus-based and language-independent:
it takes a text as input and produces a segmenta-
tion of the word forms observed in the text. The
obtained results are not strictly linguistic, however
they often resemble a linguistic morpheme segmen-
tation. Once a morpheme segmentation has been
learnt from some text, it can be used for segment-
ing new texts. In our experiments, the splitting are
learnt from the training corpus used for the IBM1
lexicon probabilities. The obtained segmentation is
then used for splitting the corresponding source texts
and hypotheses. Detailed corpus statistics are shown
in Table 1.
Using the obtained IBM1 probabilities of words,
morphemes and POS n-grams, the scores de-
scribed in Section 2 are calculated for the
Spanish-English, French-English, German-English
and Czech-English translation outputs from each
translation direction. For each of the IBM1 scores,
the system level Spearman correlation coefficients ?
with the human ranking are calculated for each doc-
ument. In total, 32 correlation coefficients are ob-
tained for each score ? four English outputs from
the WMT 2010 task, four from the WMT 2009 and
eight from the WMT 2008 task, together with six-
teen outputs in other four target languages. The ob-
tained correlation results were then summarised into
the following three values:
? mean
a correlation coefficient averaged over all trans-
lation outputs;
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
100
Spanish English French English German English Czech English
sentences 97122 83967 100222 94693
running words 2661344 2338495 2395141 2042085 2475359 2398780 2061422 2249365
vocabulary:
words 69620 53527 56295 50082 107278 54270 125614 52081
morphemes 14178 13449 12004 12485 22211 13499 18789 12961
POS tags 69 44 33 44 54 44 611 44
POS-2grams 2459 1443 826 1443 1611 1454 27835 1457
POS-3grams 27350 20474 10409 19838 19928 20769 209481 20522
POS-4grams 135166 121182 62177 114555 114314 123550 637337 120646
Table 1: Statistics of the corpora for training IBM1 lexicon models.
? rank>
percentage of documents where the particular
score has better correlation than the other IBM1
scores;
? rank?
percentage of documents where the particular
score has better or equal correlation than the
other IBM1 scores.
3.2 Comparison of IBM1 scores
The first step towards deciding which IBM1 score
to submit to the WMT 2011 evaluation task was a
comparison of the average correlations i.e. mean
values. These values for each of the IBM1 scores
are presented in Table 2. The left column shows
average correlations of the source-hypothesis (sh)
scores, and the right one of the hypothesis-source
(hs) scores.
mean IBM1sh IBM1hs
words 0.066 0.308
morphemes 0.227 0.445
POS tags 0.006 0.337
POS-2grams 0.058 0.337
POS-3grams 0.172 0.376
POS-4grams 0.196 0.442
Table 2: Average correlations of source-hypothesis (left
column) and hypothesis-source (right column) IBM1
scores.
It can be seen that the morpheme, POS-3gram and
POS-4gram scores have the best correlations in both
directions. Apart from that, it can be observed that
all the hs scores have better correlations than sh
scores. Therefore, all the further experiments will
deal only with the hs scores, and the subscript hs is
omitted.
In the next step, all the hs scores are sorted ac-
cording to each of the three values described in
Section 3.1, i.e. average correlation mean, rank>
and rank?, and the results are shown in Table 3.
The most promising scores according to each of
the three values are morpheme score MIBM1, POS-
3gram score P3IBM1 and POS-4gram score P4IBM1.
3.2.1 Combined IBM1 scores
The last experiment was to combine the most
promising IBM1 scores in order to see if the correla-
tion with human rankings can be further improved.
In general, a combined IBM1 score is defined as
arithmetic mean of various individual IBM1hs scores
described in Section 2:
COMBIBM1 =
K
?
k=1
wk ? IBM1k (4)
The following combinations were investigated:
? P1234IBM1
combination of all POS n-gram scores;
? MP1234IBM1
combination of all POS n-gram scores and the
morpheme score;
? MP34IBM1
combination of the most promising individual
scores, i.e. POS-3gram, POS-4gram and mor-
pheme scores;
101
mean rank> rank?
0.445 morphemes 60.6 POS-4grams 71.3 POS-4grams
0.442 POS-4grams 54.4 morphemes 61.3 POS-3grams
0.376 POS-3grams 50.6 POS-3grams 56.3 morphemes
0.337 POS-2grams 39.4 POS tags 48.1 POS tags
0.337 POS tags 36.3 words 43.7 POS-2grams
0.308 words 35.6 POS-2grams 42.5 words
Table 3: IBM1hs scores sorted by average correlation (column 1), rank> value (column 2) and rank? value (column
3). The most promising scores are those calculated on morphemes (MIBM1), POS-3grams (P3IBM1) and POS-4grams
(P4IBM1).
? MP4IBM1
combination of the two most promising indi-
vidual scores, i.e. POS-4gram score and mor-
pheme score.
For each of the scores, two variants were investi-
gated, with and without (i.e. with uniform) weights
wk. The weigths were choosen proportionally to
the average correlation of each individual score. Ta-
ble 4 contains average correlations for all combined
scores, together with the weight values.
combined score mean
P1234IBM1 0.403
+weights (0.15, 0.15, 0.3, 0.4) 0.414
MP1234IBM1 0.466
+weights (0.2, 0.05, 0.05, 0.2, 0.5) 0.486
MP34IBM1 0.480
+weights (0.25, 0.25, 0.5) 0.498
MP4IBM1 0.494
+weights (0.4, 0.6) 0.496
Table 4: Average correlations of the investigated IBM1hs
combinations. The weight values are choosen accord-
ing to the average correlation of the particular individual
IBM1 score.
The POS n-gram combination alone does not yield
any improvement over the best individual scores.
Introduction of the morpheme score increases the
average correlation, especially when only the best
n-gram scores are chosen. Apart from that, intro-
ducing weights improves the average correlation for
each of the combined scores.
The final step in our experiments consists of rank-
ing the weighted combined scores. The rank> and
rank? values for these scores are presented in Ta-
ble 5. According to the rank> values, the MP4IBM1
score clearly outperforms all other scores. This
score also has the highest mean value together with
the MP34IBM1 score. As for rank? values, all
morpheme-POS scores have similar values signifi-
cantly outperforming the P1234IBM1 score.
combined score rank> rank?
P1234IBM1 25.0 36.4
MP1234IBM1 44.8 68.7
MP34IBM1 39.6 64.6
MP4IBM1 55.2 65.7
Table 5: rank> (column 1) and rank? (column 2) values
of the weighted IBM1hs combinations.
Following all these observations, we decided to
submit the MP4IBM1 score to the WMT 2011 evalu-
ation task.
4 Conclusions and outlook
The results presented in this article show that the
IBM1 scores have the potential to be used as replace-
ment of current evaluation metrics based on refer-
ence translations. Especially the scores abstracting
away from word surface particularities (i.e. vocabu-
lary, domain) based on morphemes, POS-3grams and
4grams show a high average correlation of about 0.5
(the average correlation of the BLEU score on the
same data is 0.566).
An important point for future optimisation is to
investigate effects of the selection of training data
for the IBM1 models (and its similarity to the train-
ing data of the involved statistical translation sys-
tems). Furthermore, investigation of how to assign
the weights for combining the corresponding indi-
102
vidual scores, as well as of the possible impact of
different morpheme splittings should be carried out.
Other direction for future work is combination with
other features (i.e. POS language models).
This method is currently being tested and fur-
ther developed in the framework of the TARAX ?U
project2. In this project, three industry and one re-
search partners develop a hybrid machine transla-
tion architecture that satisfies current industry needs,
which includes a number of large-scale evalua-
tion rounds involving various languages: English,
French, German, Czech, Spanish, Russian, Chinese
and Japanese. By the time of writing this article, the
first human evaluation round in TARAX ?U on a pilot
set of about 7000 sentences is running. The metrics
proposed in this paper will be tested on the TARAX ?U
data as soon as they are available. First results will
be reported in the presentation of this paper.
Acknowledgments
This work has been partly developed within the
TARAX ?U project financed by TSB Technologies-
tiftung Berlin ? Zukunftsfonds Berlin, co-financed
by the European Union ? European fund for regional
development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In Pro-
ceedings of the ACL 05 Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for MT and/or Summa-
rization, pages 65?72, Ann Arbor, MI, June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Final report, JHU/CLSP Summer
Workshop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
Meta-Evaluation of Machine Translation. In Proceed-
ings of the 3rd ACL 08 Workshop on Statistical Ma-
2http://taraxu.dfki.de/
chine Translation (WMT 08), pages 70?106, Colum-
bus, Ohio, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR
(WMT 10), pages 17?53, Uppsala, Sweden, July.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical Re-
port Report A81, Computer and Information Science,
Helsinki University of Technology, Helsinki, Finland,
March.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for statistical machine translation. Technical re-
port, Johns Hopkins University 2003 Summer Work-
shop on Language Engineering, Center for Language
and Speech Processing, Baltimore, MD, USA, August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 02), pages 311?318, Philadel-
phia, PA, July.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 763?771,
Athens, Greece, March.
103
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 104?107,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Morphemes and POS tags for n-gram based evaluation metrics
Maja Popovic?
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
maja.popovic@dfki.de
Abstract
We propose the use of morphemes for auto-
matic evaluation of machine translation out-
put, and systematically investigate a set of F
score and BLEU score based metrics calculated
on words, morphemes and POS tags along with
all corresponding combinations. Correlations
between the new metrics and human judg-
ments are calculated on the data of the third,
fourth and fifth shared tasks of the Statisti-
cal Machine Translation Workshop. Machine
translation outputs in five different European
languages are used: English, Spanish, French,
German and Czech. The results show that the
F scores which take into account morphemes
and POS tags are the most promising metrics.
1 Introduction
Recent investigations have shown that the n-gram
based evaluation metrics calculated on Part-of-
Speech (POS) sequences correlate very well with
human judgments (Callison-Burch et al, 2008;
Callison-Burch et al, 2009; Popovic? and Ney, 2009)
clearly outperforming the widely used metrics BLEU
and TER. The BLEU score measured on morphemes
is shown to be useful for evaluation of morpholog-
ically rich languages (Luong et al, 2010). We pro-
pose the use of morphemes for a set of n-gram based
automatic evaluation metrics and investigate the cor-
relation of the novel metrics with human judgments.
We carry out a systematic comparison between the
F and BLEU based metrics calculated on various
combinations of words, morphemes and POS tags.
The focus of this work is not a comparison of the
morpheme and POS based metrics with the standard
evaluation metrics1 as in (Popovic? and Ney, 2009),
but rather a comparison within the proposed set of
metrics in order to decide which score(s) should be
submitted to the WMT 2011 evaluation task. There
are fifteen evaluation metrics in total, which can be
divided in three groups: the metrics calculated on
single units, i.e. words, morphemes or POS tags
alone, the metrics calculated on pairs, i.e. words
and POS tags, words and morphemes as well as mor-
phemes and POS tags, and the metrics which take ev-
erything into account ? lexical, morphological and
syntactic information, i.e. words, morphemes and
POS tags.
Spearman?s rank correlation coefficients on the
document (system) level between all the metrics
and the human ranking are computed on the En-
glish, French, Spanish, German and Czech texts
generated by various translation systems in the
framework of the third (Callison-Burch et al,
2008), fourth (Callison-Burch et al, 2009) and
fifth (Callison-Burch et al, 2010) shared translation
tasks.
2 Evaluation metrics
We carried out a systematic comparison between the
following metrics:
? single unit (word/morpheme/POS) metrics:
? WORDF
Standard F score: takes into account all
word n-grams which have a counterpart
1Apart from the standard BLEU score which is tightly re-
lated.
104
both in the corresponding reference and in
the hypothesis.
? MORPHF
Morpheme F score: takes into account all
morpheme n-grams which have a counter-
part both in the corresponding reference
and in the hypothesis.
? POSF
POS F score: takes into account all POS
n-grams which have a counterpart both in
the corresponding reference and in the hy-
pothesis.
? BLEU
The standard BLEU score (Papineni et al,
2002).
? POSBLEU
The standard BLEU score calculated on
POS tags.
? MORPHBLEU
The standard BLEU score calculated on
morphemes.
? pairwise metrics:
? WPF
F score of word and POS n-grams.
? WMF
F score of word and morpheme n-grams.
? MPF
F score of morpheme and POS n-grams.
? WPBLEU
Arithmetic mean of BLEU and POSBLEU
scores.
? WMBLEU
Arithmetic mean of BLEU and MOR-
PHBLEU scores.
? MPBLEU
Arithmetic mean of MORPHBLEU and
POSBLEU scores.
? metrics taking everything into account:
? WMPF
F score on word, morpheme and POS n-
grams.
? WMPBLEU
Arithmetic mean of BLEU, MORPHBLEU
and POSBLEU scores.
? WMPFBLEU
Arithmetic mean of all F and BLEU scores.
The prerequisite for POS based metrics is avail-
ability of an appropriate POS tagger for the target
language. It should be noted that the POS tags can-
not be only basic but must have all details (e.g. verb
tenses, cases, number, gender, etc.). For the mor-
pheme based metrics, a tool for splitting words into
morphemes is necessary.
All the F scores and the BLEU scores are based on
four-grams (i.e. the value of maximal n is 4). Pre-
liminary experiments on the morpheme based mea-
sures showed that there is no improvement by us-
ing six-grams, seven-grams or eight-grams. As for
the n-gram averaging, BLEU scores use geometric
mean. However, it is also argued not to be optimal
because the score becomes equal to zero even if only
one of the n-gram counts is equal to zero. In ad-
dition, previous experiments on the syntax-oriented
n-gram metrics (Popovic? and Ney, 2009) showed
that there is no significant difference between arith-
metic and geometric mean in the terms of correlation
coefficients. Therefore, arithmetic averaging with-
out weights is used for all F-scores. For the WMPF
score, an additional experiment with weights is car-
ried out as well.
3 Experiments on WMT 2008, WMT 2009
and WMT 2010 test data
Experimental set-up
The evaluation metrics were compared with human
rankings by means of Spearman correlation coeffi-
cients ?. Spearman?s rank correlation coefficient is
equivalent to Pearson correlation on ranks, and its
advantage is that it makes fewer assumptions about
the data. The possible values of ? range between 1
(if all systems are ranked in the same order) and -1
(if all systems are ranked in the reverse order). Thus
the higher the value of ? for an automatic metric, the
more similar is to the human metric.
The scores were calculated for outputs of transla-
tions from Spanish, French, German and Czech into
English and vice versa. Spanish, French, German
and English POS tags were produced using the Tree-
Tagger2, and the Czech texts are tagged using the
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
105
COMPOST tagger (Spoustova? et al, 2009). In this
way, all references and hypotheses were provided
with detailed POS tags.
The words of all outputs were split into mor-
phemes using the Morfessor tool (Creutz and La-
gus, 2005). The tool is corpus-based and language-
independent: it takes a text as input and produces
a segmentation of the word forms observed in the
text. The obtained results are not strictly linguistic,
however they often resemble a linguistic morpheme
segmentation. Once a morpheme segmentation has
been learnt from some text, it can be used for seg-
menting new texts. In our experiments, for each doc-
ument, first a corresponding reference translation
has been split, and then this segmentation is used for
splitting all translation hypotheses. In this way, pos-
sible discrepancies between reference and hypothe-
sis segmentation of the same word are avoided. Ef-
fects of the training on the large(r) monolingual cor-
pora have not been investigated yet.
In Table 1, an English reference sentence can be
seen along with its morpheme and POS equivalents.
words Another leading role in the film
is played by Matt Damon .
morphemes An other lead ing role in the film
is play ed by Ma tt Da mon .
POS tags DT VBG NN IN DT NN
VBZ VBN IN NP NP SENT
Table 1: Example of an English sentence with its corre-
sponding morpheme and POS sequences.
Comparison of metrics
For each evaluation metric described in Section 2,
the system level Spearman correlation coefficients ?
were calculated for each document. In total, 33 cor-
relation coefficients were obtained for each metric ?
four English outputs from the WMT 2010 task, five
from the WMT 2009 and eight from the WMT 2008
task, together with sixteen outputs in other four tar-
get languages. The obtained correlation results were
then summarised into the following three values:
? mean
a correlation coefficient averaged over all trans-
lation outputs;
? rank>
percentage of documents where the particular
metric has better correlation than the other met-
rics investigated in this work;
? rank?
percentage of documents where the particular
metric has better or equal correlation than the
other metrics investigated in this work.
These values for each metric are presented in Ta-
ble 2.
metric mean rank> rank?
WORDF 0.550 24.2 42.6
MORPHF 0.608 40.0 58.0
POSF 0.673 63.4 78.0
BLEU 0.566 20.6 38.6
MORPHBLEU 0.567 29.9 44.6
POSBLEU 0.674 54.7 66.9
WPF 0.627 44.0 66.9
WMF 0.587 37.0 53.9
MPF 0.669 51.9 77.4
WPBLEU 0.629 41.0 57.4
WMBLEU 0.557 23.6 41.0
MPBLEU 0.634 44.6 66.6
WMPF 0.645 46.3 71.1
WMPBLEU 0.610 32.7 54.7
WMPFBLEU 0.628 35.8 61.6
WMPF? 0.668 51.9 78.8
Table 2: Average correlation mean (column 1), rank>
(column 2) and rank? (column 3) for each evaluation
metric. Bold represents the best value in the particu-
lar metric group. The most promising metrics are the
F scores containing POS and morpheme information,
namely WMPF?, MPF and POSF, as well as the POSBLEU
score. The standard BLEU score has very low values.
It can be observed that the morpheme based met-
rics outperform the word based metrics, however not
the POS based metrics. As for pairwise metrics, the
MPF score seems to be very promising. Adding the
actual original words unfortunately deteriorates the
system level correlations, nevertheless omitting the
words can possibly lead to the poor sentence level
correlations. Therefore an additional experiment is
carried out with the most promising metric contain-
ing words, namely the WMPF score: a weighted
106
WMPF? score is introduced, with word weight of
0.2, morpheme weight of 0.3 and POS weight of
0.5. WMPF? clearly outperforms the simple WMPF
score without weights, and it is comparable to the
morpheme-POS F score MPF as well as POS-based
metrics POSF and POSBLEU. Apart from that, it can
be observed that, in general, the F scores are bet-
ter than the BLEU scores. The combination of all F
and all BLEU scores (WMPFBLEU) is better than the
WMPBLEU score, but does not yield any improve-
ments over the WMPF score.
The most promising metrics are the F scores con-
taining POS and morpheme information, namely
POSF, MPF and WMPF? together with the WMPF,
as well as the POSBLEU score. The standard BLEU
score has the third lowest average correlation and the
lowest rank values.
4 Conclusions
The results presented in this article show that the use
of morphemes improves n-gram based automatic
evaluation metrics, particularly in combination with
syntactic information in the form of detailed POS
tags. Especially promising are the weighted WMPF
and the MPF scores, which have been submitted to
the WMT 2011 evaluation task. Weights for these
two metrics should be further investigated in fu-
ture work, as well as the possible impact of differ-
ent morpheme splittings (such as training on larger
texts).
Acknowledgments
This work has partly been developed within the
TARAX ?U project3 financed by TSB Technologies-
tiftung Berlin ? Zukunftsfonds Berlin, co-financed
by the European Union ? European fund for regional
development.
References
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
Meta-Evaluation of Machine Translation. In Proceed-
ings of the 3rd ACL 08 Workshop on Statistical Ma-
chine Translation (WMT 08), pages 70?106, Colum-
bus, Ohio, June.
3http://taraxu.dfki.de/
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR
(WMT 10), pages 17?53, Uppsala, Sweden, July.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical Re-
port Report A81, Computer and Information Science,
Helsinki University of Technology, Helsinki, Finland,
March.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A Hybrid Morpheme-Word Representation
for Machine Translation of Morphologically Rich
Languages. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 10), pages 148?157, Cambridge, MA, Octo-
ber.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 02), pages 311?318, Philadel-
phia, PA, July.
Maja Popovic? and Hermann Ney. 2009. Syntax-oriented
evaluation measures for machine translation output. In
Proceedings of the 4th EACL 09 Workshop on Sta-
tistical Machine Translation (WMT 09), pages 29?32,
Athens, Greece, March.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 763?771,
Athens, Greece, March.
107
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 64?70,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
TerrorCat: a Translation Error Categorization-based MT Quality Metric
Mark Fishel,? Rico Sennrich,? Maja Popovic?,? Ondr?ej Bojar?
? Institute of Computational Linguistics, University of Zurich
{fishel,sennrich}@cl.uzh.ch
? German Research Center for Artificial Intelligence (DFKI), Berlin
maja.popovic@dfki.de
? Charles University in Prague, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
bojar@ufal.mff.cuni.cz
Abstract
We present TerrorCat, a submission to the
WMT?12 metrics shared task. TerrorCat uses
frequencies of automatically obtained transla-
tion error categories as base for pairwise com-
parison of translation hypotheses, which is in
turn used to generate a score for every trans-
lation. The metric shows high overall corre-
lation with human judgements on the system
level and more modest results on the level of
individual sentences.
1 The Idea
Recently a couple of methods of automatic trans-
lation error analysis have emerged (Zeman et al,
2011; Popovic? and Ney, 2011). Initial experiments
have shown that while agreement with human error
analysis is low, these methods show better perfor-
mance on tasks with a lower granularity, e.g. ranking
error categories by frequency (Fishel et al, 2012).
In this work we apply translation error analysis to a
task with an even lower granularity: ranking transla-
tions, one of the shared tasks of WMT?12.
The aim of translation error analysis is to identify
the errors that translation systems make and catego-
rize them into different types: e.g. lexical, reorder-
ing, punctuation errors, etc. The two tools that we
will use ? Hjerson and Addicter ? both rely on a ref-
erence translation. The hypothesis translation that is
being analyzed is first aligned to the reference on the
word level, and then mistranslated, misplaced, mis-
inflected, missing or superfluous words and other er-
rors are identified.
The main idea of our work is to quantify trans-
lation quality based on the frequencies of different
error categories. The basic assumption is that differ-
ent error categories have different importance from
the point of view of overall translation quality: for
instance, it would be natural to assume that punc-
tuation errors influence translation quality less than
missing words or lexical choice errors. Furthermore,
an error category can be more important for one out-
put language than the other: for example, word or-
der can influence the meaning in an English sentence
more than in a Czech or German one, whereas in-
flection errors are probably more frequent in the lat-
ter two and can thus cause more damage.
In the context of the ranking task, the absolute
value of a numeric score has no importance, apart
from being greater than, smaller than or equal to the
other systems? scores. We therefore start by per-
forming pairwise comparison of the translations ?
the basic task is to compare two translations and re-
port which one is better. To conform with the WMT
submission format we need to generate a numeric
score as the output ? which is obtained by compar-
ing every possible pair of translations and then using
the (normalized) total number of wins per translation
as its final score.
The general architecture of the metric is thus this:
? automatic error analysis is applied to the sys-
tem outputs, yielding the frequencies of every
error category for each sentence
? every possible pair of all system outputs is rep-
resented as a vector of features, based on the
error category frequencies
64
? a binary classifier takes these feature vectors as
input and assigns a win to one of the sentences
in every pair (apart from ties)
? the final score of a system equals to the normal-
ized total number of wins per sentence
? the system-level score is averaged out over the
individual sentence scores
An illustrative example is given in Figure 1.
We call the result TerrorCat, the translation error
categorization-based metric.
2 The Details
In this section we will describe the specifics of
the current implementation of the TerrorCat met-
ric: translation error analysis, lemmatization, binary
classifier and training data for the binary classifier.
2.1 Translation Error Analysis
Addicter (Zeman et al, 2011) and Hjerson (Popovic?
and Ney, 2011) use different methods for automatic
error analysis. Addicter explicitly aligns the hy-
pothesis and reference translations and induces error
categories based on the alignment coverage while
Hjerson compares words encompassed in the WER
(word error rate) and PER (position-independent
word error rate) scores to the same end.
Previous evaluation of Addicter shows that
hypothesis-reference alignment coverage (in terms
of discovered word pairs) directly influences er-
ror analysis quality; to increase alignment cover-
age we used Berkeley aligner (Liang et al, 2006)
and trained it on and applied it to the whole set of
reference-hypothesis pairs for every language pair.
Both tools use word lemmas for their analysis;
we used TreeTagger (Schmid, 1995) for analyzing
English, Spanish, German and French and Morc?e
(Spoustova? et al, 2007) to analyze Czech. The same
tools are used for PoS-tagging in some experiments.
2.2 Binary Classification
Pairwise comparison of sentence pairs is achieved
with a binary SVM classifier, trained via sequential
minimal optimization (Platt, 1998), implemented in
Weka (Hall et al, 2009).
The input feature vectors are composed of fre-
quency differences of every error category; since the
Source: Wir sind Meister!
Translations:
Reference: We are the champions!
HYP-1: Us champions!
HYP-2: The champions we are .
HYP-3: We are the champignons!
Error Frequencies:
HYP-1: 1?inflection, 2?missing
HYP-2: 2?order, 1?punctuation
HYP-3: 1?lex.choice
Classifier Output: (or manually created
input in the training phase)
HYP-1 < HYP-2
HYP-1 < HYP-3
HYP-2 > HYP-3
Scores:
HYP-1: 0
HYP-2: 1
HYP-3: 0.5
Figure 1: Illustration of TerrorCat?s process for a single
sentence: translation errors in the hypothesis translations
are discovered by comparing them to the reference, error
frequencies are extracted, pairwise comparisons are done
by the classifier and then converted to scores. The shown
translation errors correspond to Hjerson?s output.
maximum (normalized) frequency of any error rate
is 1, the feature value range is [?1, 1]. To include
error analysis from both Addicter and Hjerson their
respective features are used side-by-side.
2.3 Data Extraction
Training data for the SVM classifier is taken from
the WMT shared task manual ranking evaluations
of previous years (2007?2011), which consist of tu-
ples of 2 to 5 ranked sentences for every language
pair. Equal ranks are allowed, and translations of
the same sentence by the same pair of systems can
be present in several tuples, possibly having conflict-
ing comparison results.
To convert the WMT manual ranking data into
the training data for the SVM classifier, we collect
all rankings for each pair of translation hypothe-
65
2007-2010 2007-2011
fr-en 34 152 46 070
de-en 36 792 53 790
es-en 30 374 41 966
cs-en 19 268 26 418
en-fr 22 734 35 854
en-de 36 076 56 054
en-es 19 352 35 700
en-cs 31 728 52 954
Table 1: Dataset sizes for every language pair, based
on manual rankings from WMT shared tasks of previ-
ous years: the number of pairs with non-conflicting, non-
equivalent ranks.
ses. Pairs with equal ranks are discarded, conflicting
ranks for the same pairs are resolved with voting. If
the voting is tied, the pair is also discarded.
The kept translation pairs are mirrored (i.e. both
directions of every pair are added to the training set
as independent entries) to ensure no bias towards the
first or second translation in a pair. We will later
present analysis of how well that works.
2.4 TerrorCat+You
TerrorCat is distributed via GitHub; information on
downloading and using it can be found online.1 Ad-
ditionally we are planning to provide more recent
evaluations with new datasets, as well as pre-trained
models for various languages and language pairs.
3 The Experiments
In the experimental part of our work, we search for
the best performing model variant, the aim of which
is to evaluate different input features, score calcula-
tion strategies and other alternations. The search is
done empirically: we evaluate one alternation at a
time, and if it successful, it is added to the system
before proceeding to test further alternations.
Performance of the models is estimated on a held-
out development set, taken from the WMT?11 data;
the training data during the optimization phase is
composed of ranking data from WMT 2007?2010.
In the end we re-trained our system on the whole
data set (WMT 2007?2011) and applied it to the un-
1http://terra.cl.uzh.ch/terrorcat.html
labeled data from this year?s shared task. The result-
ing dataset sizes are given in Table 1.
All of the resulting scores obtained by different
variants of our metric are presented in Tables 2 (for
system-level correlations) and 3 (for sentence-level
correlations), compared to BLEU and other selected
entries in the WMT?11 evaluation shared task. Cor-
relations are computed in the same way as in the
WMT evaluations.
3.1 Model Optimization
The following is a brief description of successful
modifications to the baseline system.
Weighted Wins
In the baseline model, the score of the winning
system in each pairwise comparison is increased by
1. To reduce the impact of low-confidence decisions
of the classifier on the final score we tested replac-
ing the constant rewards to the winning system with
variable ones, proportional to the classifier?s confi-
dence ? a measure of which was obtained by fitting
a logistic regression model to the SVM output.
As the results show, this leads to minor improve-
ments in sentence-level correlation and more notice-
able improvements in system-level correlation (es-
pecially English-French and Czech-English). A pos-
sible explanation for this difference in performance
on different levels is that low classification confi-
dence on the sentence-level does not necessarily af-
fect our ranking for that sentence, but reduces the
impact of that sentence on the system-level ranking.
PoS-Split Features
The original model only makes a difference be-
tween individual error categories as produced by
Hjerson and Addicter. It seems reasonable to assume
that errors may be more or less important, depending
on the part-of-speech of the words they occur in. We
therefore tested using the number of errors per er-
ror category per PoS-tag as input features. In other
words, unlike the baseline, which relied on counts
of missing, misplaced and other erroneous words,
this alternation makes a difference between miss-
ing nouns/verbs/etc., misplaced nouns, misinflected
nouns/adjectives, and so on.
The downside of this approach is that the number
of features is multiplied by the size of the PoS tag
66
Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*
TerrorCat:
Baseline 0.73 0.74 0.82 0.76 0.76 0.70 0.81 0.69 0.84 0.76
Weighted wins 0.73 0.74 0.82 0.79 0.77 0.75 0.81 0.69 0.84 0.77
PoS-features 0.87 0.76 0.80 0.86 0.82 0.76 0.86 0.74 0.87 0.81
GenPoS-features 0.86 0.77 0.84 0.88 0.84 0.80 0.85 0.75 0.90 0.83
No 2007 data (GenPoS) 0.89 0.80 0.80 0.95 0.86 0.85 0.84 0.81 0.90 0.85
Other:
BLEU 0.85 0.48 0.90 0.88 0.78 0.86 0.44 0.87 0.65 0.70
mp4ibm1 0.08 0.56 0.12 0.91 0.42 0.61 0.91 0.71 0.76 0.75
MTeRater-Plus 0.93 0.90 0.91 0.95 0.92 ? ? ? ? ?
AMBER ti 0.94 0.63 0.85 0.88 0.83 0.84 0.54 0.88 0.56 0.70
meteor-1.3-rank 0.93 0.71 0.88 0.91 0.86 0.85 0.30 0.74 0.65 0.63
Table 2: System-level Spearman?s rank correlation coefficients (?) between different variants of TerrorCat and hu-
man judgements, based on WMT?11 data. Other metric submissions are shown for comparison. Highest scores per
language pair are highlighted in bold separately for TerrorCat variants and for other metrics.
set. Additionally, too specific distinctions can cause
data sparsity, especially on the sentence level.
As shown by the results, PoS-tag splitting of the
features is successful on the system level, but quite
hurtful to the sentence-level correlations. The poor
performance on the sentence level can be attributed
to the aforementioned data sparsity: the number of
different features is higher than the number of words
(and hence, the biggest possible number of errors)
in the sentences. However, we cannot quite ex-
plain, how a sum of these less reliable sentence-level
scores leads to more reliable system-level scores.
To somewhat relieve data sparsity we defined sub-
sets of the original PoS tag sets, mostly leaving out
morphological information and keeping just the gen-
eral word types (nouns, verbs, adjectives, etc.). This
reduced the number of PoS-tags (and thus, the num-
ber of input features) from 2 to 4 times and produced
further increase in system-level and a smaller de-
crease in sentence-level scores, see GenPoS results.
To avoid splitting the metric into different ver-
sions for system-level and sentence-level, we gave
priority to system-level correlations and adopted the
generalized PoS-splitting of the features.
Out-of-Domain Data
The human ranking data from WMT of previ-
ous years do not constitute a completely homo-
geneous dataset. For starters, the test sets are
taken from different domains (News/News Com-
mentary/Europarl), whereas the 2012 test set is from
the News domain only. Added to this, there might be
a difference in the manual data, coming from differ-
ent organization of the competition ? e.g. WMT?07
was the only year when manual scoring of the trans-
lations with adequacy/fluency was performed, and
ranking had just been introduced into the competi-
tion. Therefore we tested whether some subsets of
the training data can result in better overall scores.
Interestingly enough, leaving out News Commen-
tary and Europarl test sets caused decreased correla-
tions, although these account for just around 10%
of the training data. On the other hand, leaving out
the data from WMT?07 led to a significant gain in
overall performance.
3.2 Error Meta-Analysis
To better understand why sentence-level correlations
are low, we analyzed the core of TerrorCat ? its pair-
wise classifier. Here, we focus on the most success-
ful variant of the metric, which uses general PoS-
tags and was trained on the WMT manual rankings
from 2008 to 2010. Table 4 presents the confusion
matrices of the classifier (one for precision and one
for recall), taking into consideration the confidence
estimate.
Evaluation is based on the data from 2011; the
prediction data was mirrored in the same way as for
67
Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*
TerrorCat:
Baseline 0.20 0.22 0.33 0.25 0.25 0.30 0.19 0.24 0.20 0.23
Weighted wins 0.20 0.23 0.33 0.25 0.25 0.31 0.20 0.24 0.20 0.24
PoS-features 0.13 0.18 0.24 0.15 0.18 0.27 0.15 0.15 0.17 0.19
GenPoS-features 0.16 0.24 0.31 0.22 0.23 0.27 0.18 0.22 0.19 0.22
No 2007 data (GenPoS) 0.21 0.30 0.33 0.23 0.27 0.29 0.20 0.23 0.20 0.23
Other:
mp4ibm1 0.15 0.16 0.18 0.12 0.15 0.21 0.13 0.13 0.06 0.13
MTeRater-Plus 0.30 0.36 0.45 0.36 0.37 ? ? ? ? ?
AMBER ti 0.24 0.26 0.33 0.27 0.28 0.32 0.22 0.31 0.21 0.27
meteor-1.3-rank 0.23 0.25 0.38 0.28 0.29 0.31 0.14 0.26 0.19 0.23
Table 3: Sentence-level Kendall?s rank correlation coefficients (? ) between different variants of TerrorCat and hu-
man judgements, based on WMT?11 data. Other metric submissions are shown for comparison. Highest scores per
language pair are highlighted in bold separately for TerrorCat variants and for other metrics.
the training set. Our aim was to measure the bias
of the classifier towards first or second translations
in a pair (which is obviously an undesired effect).
It can be seen that the confusion matrices are com-
pletely symmetrical, indicating no position bias of
the classifier ? even lower-confidence decisions are
absolutely consistent.
To make sure that this can be attributed to the mir-
roring of the training set, we re-trained the classifier
on non-mirrored training sets. As a result, 9% of the
instances were labelled inconsistently, with the av-
erage confidence of such inconsistent decisions be-
ing extremely low (2.1%, compared to the overall
average of 28.4%). The resulting correlations have
slightly dropped as well ? all indicating that mirror-
ing the training sets does indeed remove the posi-
tional bias and leads to slightly better performance.
Looking at the confusion matrices overall, most
decisions fall within the main diagonals (i.e. the
cells indicating correct decisions of the classifier).
Looking strictly at the classifier?s decisions, the re-
calls and precisions of the non-tied comparison out-
puts (?<? and ?>?) are 57% precision, 69% recall.
However, such strict estimates are too pessimistic in
our case, since the effect of the classifier?s decisions
is proportional to the confidence estimate. On the
sentence level it means that low-confidence decision
errors have less effect on the total score of a system.
A definite source of error is the instability of the in-
dividual translation errors on the sentence level, an
effect both Addicter and Hjerson are known to suffer
from (Fishel et al, 2012).
The precision of the classifier predictably drops
together with the confidence, and almost half of the
misclassifications come from unrecognized equiva-
lent translations ? as a result the recall of such pairs
of equivalent translations is only 20%. This can be
explained by the fact that the binary classifier was
trained on instances with just these two labels and
with no ties allowed.
On the other hand the classifier?s 0-confidence de-
cisions have a high precision (84%) on detecting the
equivalent translations; after re-examining the data
it turned out that 96% of the 0-confidence decisions
were made on input feature vectors containing only
zero frequency differences. Such vectors represent
pairs of sentences with identical translation error
analyses, which are very often simply identical sen-
tences ? in which case the classifier cannot (and in
fact, should not) make an informed decision of one
being better than the other.
4 Related Work
Traditional MT metrics such as BLEU (Papineni et
al., 2002) are based on a comparison of the trans-
lation hypothesis to one or more human references.
TerrorCat still uses a human reference to extract fea-
tures from the error analysis with Addicter and Hjer-
son, but at the core, TerrorCat compares hypotheses
not to a reference, but to each other.
68
Manual Classifier Output and Confidence: Precision
label < < or > >
0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0
< 81% 60% 45% 8% 32% 23% 10%
= 9% 17% 23% 84% 23% 17% 9%
> 10% 23% 32% 8% 45% 60% 81%
Manual Classifier Output and Confidence: Recall
label < < or > >
0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0
< 23% 18% 28% 1% 20% 7% 3%
= 5% 9% 26% 20% 26% 9% 5%
> 3% 7% 20% 1% 28% 18% 23%
Table 4: The precision and recall confusion matrices of the classifier ? judgements on whether one hypothesis is worse
than, equivalent to or better than another hypothesis are compared to the classifier?s output and confidence.
It is thus most similar to SVM-RANK and Tesla
metrics, submissions to the WMT?10 shared met-
rics task (Callison-Burch et al, 2010) which also
used SVMs for ranking translations. However, both
metrics used SVMrank (Joachims, 2006) directly for
ranking (unlike TerrorCat, which uses a binary clas-
sifier for pairwise comparisons). Their features in-
cluded some of the metric outputs (BLEU, ROUGE,
etc.) for SVM-RANK and similarity scores between
bags of n-grams for Tesla (Dahlmeier et al, 2011).
5 Conclusions
We introduced the TerrorCat metric, which performs
pairwise comparison of translation hypotheses based
on frequencies of automatically obtained error cate-
gories using a binary classifier, trained on manually
ranked data. The comparison outcome is then con-
verted to a numeric score for every sentence or doc-
ument translation by averaging out the number of
wins per translation system.
Our submitted system achieved an average
system-level correlation with human judgements in
the WMT?11 development set of 0.86 for transla-
tion into English and 0.85 for translations from En-
glish into other languages. Particularly good per-
formance was achieved on translations from English
into Czech (0.90) and back (0.95). Sentence-level
scores are more modest: average 0.27 for transla-
tion into English and 0.23 for those out of English.
The scores remain to be checked against the human
judgments from WMT?12.
The introduced TerrorCat metric has certain de-
pendencies. For one thing, in order to apply it to
new languages, a training set of manual rankings is
required ? although this can be viewed as an advan-
tage, since it enables the user to tune the metric to
his/her own preference. Additionally, the metric de-
pends on lemmatization and PoS-tagging.
There is a number of directions to explore in the
future. For one, both Addicter and Hjerson report
MT errors related more to adequacy than fluency, al-
though it was shown last year (Parton et al, 2011)
that fluency is an important component in rating
translation quality. It is also important to test how
well the metric performs if lemmatization and PoS-
tagging are not available.
For this year?s competition, training data was
taken separately for every language pair; it remains
to be tested whether combining human judgements
with the same target language and different source
languages leads to better or worse performance.
To conclude, we have described TerrorCat, one
of the submissions to the metrics shared task of
WMT?12. TerrorCat is rather demanding to apply on
one hand, having more requirements than the com-
mon reference-hypothesis translation pair, but at the
same time correlates rather well with human judge-
ments on the system level.
69
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
Tesla at wmt 2011: Translation evaluation and tunable
metric. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 78?84, Edinburgh,
Scotland.
Mark Fishel, Ondr?ej Bojar, and Maja Popovic?. 2012.
Terra: a collection of translation error-annotated cor-
pora. In Proceedings of the 8th LREC, page in print,
Istanbul, Turkey.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
Philadelphia, USA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the HLT-
NAACL Conference, pages 104?111, New York, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 108?115, Edinburgh, Scot-
land.
John C. Platt. 1998. Using analytic qp and sparseness
to speed training of support vector machines. In Pro-
ceedings of Neural Information Processing Systems
11, pages 557?564, Denver, CO.
Maja Popovic? and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Computational Linguistics, 37(4):657?688.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceedings
of the ACL SIGDAT-Workshop, Dublin, Ireland.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
Czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Daniel Zeman, Mark Fishel, Jan Berka, and Ondr?ej Bo-
jar. 2011. Addicter: What is wrong with my transla-
tions? The Prague Bulletin of Mathematical Linguis-
tics, 96:79?88.
70
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 329?336,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Machine learning methods for comparative and time-oriented
Quality Estimation of Machine Translation output
Eleftherios Avramidis and Maja Popovic?
German Research Center for Artificial Intelligence (DFKI GmbH)
Language Technology Lab
Alt Moabit 91c, 10559 Berlin
eleftherios.avramidis@dfki.de and maja.popovic@dfki.de
Abstract
This paper describes a set of experi-
ments on two sub-tasks of Quality Esti-
mation of Machine Translation (MT) out-
put. Sentence-level ranking of alternative
MT outputs is done with pairwise classi-
fiers using Logistic Regression with black-
box features originating from PCFG Pars-
ing, language models and various counts.
Post-editing time prediction uses regres-
sion models, additionally fed with new
elaborate features from the Statistical MT
decoding process. These seem to be better
indicators of post-editing time than black-
box features. Prior to training the models,
feature scoring with ReliefF and Informa-
tion Gain is used to choose feature sets of
decent size and avoid computational com-
plexity.
1 Introduction
During the recent years, Machine Translation
(MT) has reached levels of performance which al-
low for its integration into real-world translation
workflows. Despite the high speed and various ad-
vantages of this technology, the fact that the MT
results are rarely perfect and often require man-
ual corrections has raised a need to assess their
quality, predict the required post-editing effort and
compare outputs from various systems on applica-
tion time. This has been the aim of current re-
search on Quality Estimation, which investigates
solutions for several variations of such problems.
We describe possible solutions for two prob-
lems of MT Quality Estimation, as part of
the 8th Shared Task on Machine Translation:
(a) sentence-level quality ranking (1.2) of multi-
ple translations of the same source sentence and
(b) prediction of post-editing time (1.3). We
present our approach on acquiring (section 2.1)
and selecting features (section 2.2), we explain
the generation of the statistical estimation systems
(section 2.3) and we evaluate the developed solu-
tions with some of the standard metrics (section 3).
2 Methods: Quality Estimation as
machine learning
These two Quality Estimation solutions have been
seen as typical supervised machine learning prob-
lems. MT output has been given to humans, so that
they perform either (a) ranking of the multiple MT
system outputs in terms of meaning or (b) post-
editing of single MT system output, where time
needed per sentence is measured. The output of
these tasks has been provided by the shared task
organizers as a training material, whereas a small
keep-out set has been reserved for testing pur-
poses.
Our task is therefore to perform automatic qual-
ity analysis of the translation output and the trans-
lation process in order to provide features for the
supervised machine learning mechanism, which is
then trained over the corresponding to the respec-
tive human behaviour. The task is first optimized
in a development phase in order to produce the two
best shared task submissions for each task. These
are finally tested on the keep-out set so that their
performance is compared with the ones submitted
by all other shared-task participants.
2.1 Feature acquisition
We acquire two types of sentence-level features,
that are expected to provide hints about the quality
of the generated translation, depending on whether
they have access to internal details of the MT de-
coding process (glass-box) or they are only de-
rived from characteristics of the processed and
generated sentence text (black-box).
329
2.1.1 Black-box features
Features of this type are generated as a result of
automatic analysis of both the source sentence and
the MT output (when applicable), whereas many
of them are already part of the baseline infrastruc-
ture. For all features we also calculate the ratios
of the source to the target sentence. These features
include:
PCFG Features: We parse the text with a PCFG
grammar (Petrov et al, 2006) and we derive the
counts of all node labels (e.g. count of VPs, NPs
etc.), the parse log-likelihood and the number of
the n-best parse trees generated (Avramidis et al,
2011).
Rule-based language correction is a result of
hand-written controlled language rules, that indi-
cate mistakes on several pre-defined error cate-
gories (Naber, 2003). We include the number of
errors of each category as a feature.
Language model scores include the smoothed
n-gram probability and the n-gram perplexity of
the sentence.
Count-based features include count and per-
centage of tokens, unknown words, punctuation
marks, numbers, tokens which do or do not con-
tain characters ?a-z?; the absolute difference be-
tween number of tokens in source and target nor-
malized by source length, number of occurrences
of the target word within the target hypothesis av-
eraged for all words in the hypothesis (type/token
ratio).
Source frequency: A set of eight features in-
cludes the percentage of uni-grams, bi-grams and
tri-grams of the processed sentence in frequency
quartiles 1 (lower frequency words) and 4 (higher
frequency words) in the source side of a parallel
corpus (Callison-Burch et al, 2012).
Contrastive evaluation scores: For the ranking
task, each translation is scored with an automatic
metric (Papineni et al, 2002; Lavie and Agarwal,
2007), using the other translations as references
(Soricut et al, 2012).
2.1.2 Glass-box features
Glass-box features are available only for the time-
prediction task, as a result of analyzing the verbose
output of the Minimum Bayes Risk decoding pro-
cess.
Counts from the best hypothesis: Count
of phrases, tokens, average/minimum/maximum
phrase length, position of longest and shortest
phrase in the source sentence; count of words
unknown to the phrase table, average number of
unknown words first/last position of an unknown
word in the sentence normalized to the number of
tokens, variance and deviation of the position of
the unknown words.
Log probability (pC) and future cost esti-
mate (c) of the phrases chosen as part of the best
translation: minimum and maximum values and
their position in the sentence averaged to the num-
ber of sentences, and also their average, variance,
standard deviation; count of the phrases whose
probability or future cost estimate is lower and
higher than their standard deviation; the ratio of
these phrases to the total number of phrases.
Alternative translations from the search path
of the decoder: average phrase length, average of
the average/variance/standard deviation of phrase
log probability and future cost estimate, count of
alternative phrases whose log probability or future
cost estimate is lower and higher than their stan-
dard deviation.
2.2 Feature selection
Feature acquisition results in a huge number of
features. Although the machine learning mech-
anisms already include feature selection or regu-
larization, huge feature sets may be unusable for
training, due to the high processing needs and the
sparsity or noise they may infer. For this purpose
we first reduce the number of features by scoring
them with two popular correlation measurement
methods.
2.2.1 Information gain
Information gain (Hunt et al, 1966) estimates the
difference between the prior entropy of the classes
and the posterior entropy given the attribute val-
ues. It is useful for estimating the quality of each
attribute but it works under the assumption that
features are independent, so it is not suitable when
strong feature inter-correlation exists. Information
gain is only used for the sentence ranking task af-
ter discretization of the feature values.
2.2.2 ReliefF
ReliefF assesses the ability of each feature to dis-
tinguish between very similar instances from dif-
330
ferent classes (Kononenko, 1994). It picks up a
number of instances in random and calculates a
feature contribution based on the nearest hits and
misses. It is a robust method which can deal with
incomplete and noisy data (Robnik-S?ikonja and
Kononenko, 2003).
2.3 Machine learning algorithms
Machine learning is performed for the two sub-
tasks using common pairwise classification and
regression methods, respectively.
2.3.1 Ranking with pairwise binary
classifiers
For the sub-task on sentence-ranking we used pair-
wise classification, so that we can take advantage
of several powerful binary classification methods
(Avramidis, 2012). We used logistic regression,
which optimizes a logistic function to predict val-
ues in the range between zero and one (Cameron,
1998), given a feature set X:
P (X) = 11 + e?1(a+bX) (1)
The logistic function is fitted using the Newton-
Raphson algorithm to iteratively minimize the
least squares error computed from training data
(Miller, 2002). Experiments are repeated with two
variations of Logistic Regression concerning inter-
nal features treatment: Stepwise Feature Set Selec-
tion (Hosmer, 1989) and L2-Regularization (Lin
et al, 2007).
2.3.2 Regression
For the sub-task on post-editing time prediction,
we experimented with several regression meth-
ods, such as Linear Regression, Partial Least
Squares (Stone and Brooks, 1990), Multivariate
Adaptive Regression Splines (Friedman, 1991),
LASSO (Tibshirani, 1996), Support Vector Regres-
sion (Basak et al, 2007) and Tree-based regres-
sors. Indicatively, Linear regression optimizes co-
efficient ? for predicting a value y, given a feature
vector X:
y = X? + ? (2)
2.4 Evaluation
The ranking task is evaluated by measuring cor-
relation between the predicted and the human
ranking, with the use of Kendall tau (Kendall,
1938) including penalization of ties. We addi-
tionally consider two more metrics specialized in
ranking tasks: Mean Reciprocal Rank - MRR
(Voorhees, 1999) and Normalized Discounted Cu-
mulative Gain - NDGC (Ja?rvelin and Keka?la?inen,
2002), which give better scores to models when
higher ranks (i.e. better translations) are ordered
correctly, as these are more important than lower
ranks.
The regression task is evaluated in terms of Root
Mean Square Error (RMSE) and Mean Average
Error (MAE).
3 Experiment and Results
3.1 Implementation
Relieff is implemented for k=5 nearest neighbours
sampling m=100 reference instances. Information
gain is calculated after discretizing features into
n=100 values
N-gram features are computed with the SRILM
toolkit (Stolcke, 2002) with an order of 5, based
on monolingual training material from Europarl
(Koehn, 2005) and News Commentary (Callison-
Burch et al, 2011). PCFG parsing features are
generated on the output of the Berkeley Parser
(Petrov and Klein, 2007) trained over an English,
a German and a Spanish treebank (Taule? et al,
2008). The open source language tool1 is used
to annotate source and target sentences with lan-
guage suggestions. The annotation process is or-
ganised with the Ruffus library (Goodstadt, 2010)
and the learning algorithms are executed using the
Orange toolkit (Dems?ar et al, 2004).
3.2 Sentence-ranking
The sentence-ranking sub-task has provided train-
ing data for two language pairs, German-English
and English-Spanish. For both sentence pairs,
we train the systems using the provided an-
notated data sets WMT2010, WMT2011 and
WMT2012, while the data set WMT2009 is used
for the evaluation during the development phase.
Data sets are analyzed with black-box feature gen-
eration. For each language pair, the two systems
with the highest correlation are submitted.
We start the development with two feature sets
that have shown to perform well in previous ex-
periments: #24 (Avramidis, 2012) including fea-
tures from PCFG parsing, and #31 which is the
baseline feature set of the previous year?s shared
task (Callison-Burch et al, 2012) and we combine
them (#33). Additionally, we create feature sets by
1Open source at http://languagetool.org
331
de-en en-es
id feature-set tau MRR NDGC tau MRR NDGC
#24 previous (Avramidis, 2012) 0.28 0.57 0.78 0.09 0.52 0.75
#31 baseline WMT2012 0.04 0.51 0.74 -0.16 0.43 0.69
#32 vanilla WMT2013 0.04 0.51 0.74 -0.13 0.45 0.70
#33 combine #24 and #31 0.29 0.57 0.78 0.10 0.53 0.75
#41 ReliefF 15 best 0.20 0.56 0.77 0.02 0.48 0.72
#411 ReliefF 5 best 0.22 0.53 0.76 0.19 0.49 0.73
#42 InfGain 15 best 0.15 0.53 0.75 -0.14 0.43 0.69
#43 combine #41 and #42 0.22 0.56 0.77 -0.12 0.44 0.70
#431 combine #41, #42 and #24 0.27 0.60 0.80 0.11 0.54 0.75
Table 1: Development experiments for task 1.2, reporting correlation and ranking scores, tested on the
development set WMT2009.
target feature ?
avg target word occurrence 2.18
pseudoMETEOR 0.71
count of unknown words 0.55
count of dots -0.25
count of commas 0.15
count of tokens -0.13
count of VPs -0.06
PCFGlog -0.02
lmprob 0.01
Table 3: Beta coefficients of the best fitted logistic
regression on the German-English data set (set #33
with Stepwise Feature Set Selection)
scoring features with ReliefF (features #41x) and
Information Gain (#42). Many combinations of all
the above feature-sets are tested and the most im-
portant of them are shown in Table 1. Feature sets
are described briefly in Table 2.
For German-English, we experiment with 14
feature sets, using both variations of Logistic Re-
gression. The two highest tau scores are given by
Stepwise Feature Set Selection using feature sets
#33 and #24. We see that although baseline fea-
tures #31 alone have very low correlation, when
combined with previously successful #24, provide
the best system in terms of tau. Feature set #431
(which combines the 15 features scored higher
with ReliefF, the 15 features scored higher with In-
formation Gain and the feature set #24) succeeds
pretty well on the additional metrics MRR and
NDGC, but it provides slightly lower tau correla-
tion.
For English-Spanish, the correlation of the pro-
duced systems is significantly lower and it ap-
pears that the L2-regularized logistic regression
performs better as classification method. We ex-
periment with 24 feature sets, after more scor-
ing with ReliefF and Inf. Gain. Surprisingly
enough, Kendall tau correlation indicates that the
best model is trained only with features based
target feature ?
count of unknown words -0.55
count of VPs 0.19
count of of PCFG parse trees -0.16
count of tokens 0.15
% of tokens with only letters -0.07
lmprob -0.06
pseudoMETEOR precision -0.05
source/target ratio of parse trees -0.03
Table 4: Most indicative beta coefficients of
the best fitted logistic regression on the English-
Spanish data set (set #431 with L2-regularization)
on counts of numbers and punctuation, combined
with contrastive BLEU score. This seems to rather
overfit a peculiarity of the particular development
set and indeed performs much lower on the final
test set of the shared task (tau=0.04). The second
best feature set (#431) has been described above
and luckily generalizes better on an unknown set.
It is interesting to see that this issue would have
been avoided, if the decision was taken based on
the ranking metrics MRR and NDGC, which pri-
oritize other feature sets. We assume that further
work is needed to see whether these measures are
more expressive and reliable than Kendall tau for
similar tasks.
The fitted ? coefficients (in tables 3 and 4) give
an indication of the importance of each feature
(see equation 1), for each language pair. In both
language pairs, target-side features prevail upon
other features. On the comparison of the models
for the two language pairs (and the ? coefficients
as well) we can see that the model settings and
performance may vary from one language pair to
another. This also requires further investigation,
given that Kendall tau and the other two metrics
indicate different models as the best ones.
The fact that the German-English set is bet-
ter fitted with Stepwise Feature Set Selec-
332
set features
#24 From previous work (Avramidis, 2012):
[s+t]: PCFGlog , count of: unknown words, tokens, PCFG trees, VPs
[t]: pseudoMETEOR
#31 Baseline from WMT12 (Callison-Burch et al, 2012)
[s+t]: tokensavg , lmprob, count of: commas, dots, tokens, avg translations per source word
[s]: avg freq. of low and high freq. bi-grams/tri-grams, % of distinct uni-grams in the corpus
[t]: type/token radio
#32 All 50 ?vanilla? features provided by shared-task baseline software ?Quest?
#411 ReliefF best 5 features
[s+t]: % of numbers, difference between periods of source and target (plain and averaged)
[t]: pseudoBLEU
Table 2: Description of most important feature sets for task 1.2, before internal feature selection of
Logistic Regression is applied. [s] indicates source, [t] indicates target
de-en en-es
set StepFSS L2reg StepFSS L2reg
#24 0.28 0.25 0.09 0.09
#33 0.29 0.26 0.08 0.10
#411 0.22 0.17 -0.25 0.19
#431 0.27 0.25 0.09 0.11
Table 5: Higher Kendall tau correlation (on the
dev. set) is achieved on German-English by us-
ing Stepwise Feature Set Selection, whereas on
English-Spanish by using L2-regularization
tion, whereas the English-Spanish one with L2-
Regularization (table 5) may be explained by
the statistical theory about these two methods:
The Stepwise method has has been proven to be
too bound to particular characteristics of the de-
velopment set (Flom and Cassell, 2007). L2-
Regularization has been suggested as an alterna-
tive, since it generalizes better on broader data
sets, which is probably the case for English-
Spanish.
Our method also seems to perform well when
compared to evaluation metrics which have access
to reference translations, as shown in this year?s
Metrics Shared Task (Macha?c?ek and Ondr?ej,
2013).
3.3 Post-editing time prediction
The training for the model predicting post-editing
time is performed over the entire given data set
and the evaluation is done with 10-fold cross-
validation. We evaluated 8 feature sets with 6 re-
gression methods each, ending up with 48 experi-
ments.
The evaluation of the most indicative regression
models (two best performing ones per feature set)
can be seen in Table 6. We start with a glass-
1 7310 19 28 37 46 55 64 82 91 1001091181271361451541631721811901992082172262350
50
100
150
200
250
300
350
400
450
500
REFHYP
Figure 1: Graphical representation of the values
predicted by the linear regression model with fea-
ture set #6 (blue) against the actual values of the
development set (red)
box feature set, scored with ReliefF and conse-
quently add black-box features. We note the mod-
els that have the lowest Root Mean Square Error
and Mean Average Error.
Our best model seems to be the one built linear
regression using feature set #6. This feature set is
chosen by collecting the 17 best features as scored
by ReliefF and includes both black-box and glass-
box features. How well this model fits the devel-
opment test is represented in Figure 1.
The second best feature set (#8) includes 29
glass-box features with the highest absolute Reli-
efF, joined with the black-box features of the suc-
cessful feature set #6.
More details about the contribution of the most
important features in the linear regression (equa-
tion 2) can be seen in table 7, where the fitted ?
coefficients of each feature are given. The vast
majority of the best contributing features are glass-
box features. Some draft conclusions out of the
coefficients may be that post-editing time is lower
when:
333
id feature set method RMSE MAE
#1 20 glass-box features with highest absolute ReliefF MARS 91.54 59.07SVR 93.57 55.87
#2 9 glass-box features with highest positive ReliefF Lasso 83.20 51.57Linear 83.32 51.72
#3 16 glass-box features with highest positive ReliefF Lasso 77.54 47.16Linear 77.60 47.27
#4 22 glass-box features with highest positive ReliefF Lasso 76.05 46.37Linear 76.17 46.48
#5 Combination of feature sets #1 and #2 MARS 91.54 59.07SVR 93.57 55.87
#6 17 features of any type with highest positive ReliefF Linear 74.70 45.20Lasso 74.75 44.99
#8 Combination of #5 and #6 + counts of tokens Lasso 75.14 44.99PLS 77.63 47.48
#6 First submission Linear 84.27 52.41
#8 Second submission PLS 88.34 53.49
Best models 82.60 47.52
Table 6: Development and submitted experiments for task 1.3
? the longest of the source phrases used for pro-
ducing the best hypothesis appears closer to
the end of the sentence
? the phrases with the highest and the lowest
probability appear closer to the end of the
translated sentence
? there are more determiners in the source
and/or less determiners in the translation
? there are more verbs in the translation and/or
less verbs in the source
? there are fewer alternative phrases with very
high probability
Further conclusions can be drawn after examining
these observations along with the exact operation
of the statistical MT system, which is subject to
further work.
4 Conclusion
We describe two approaches for two respective
problems of quality estimation, namely sentence-
level ranking of alternative translations and pre-
diction of time for post-editing MT output. We
present efforts on compiling several feature sets
and we examine the final contribution of the fea-
tures after training Machine Learning models.
Elaborate decoding features seem to be quite help-
ful for predicting post-editing time.
feature ?
best hyp: position of the longest aligned
phrase in the source sentence averaged to
the number of phrases
-16.652
best hyp: position of phrase with highest
prob. averaged to the num. of phrases -14.824
source: number of determiners -9.312
best hyp: number of determiners 6.189
best hyp: position of phrase with lowest
prob. averaged to the num. of phrases -5.261
best hyp: position of phrase with lowest
future cost estimate averaged to the
number of phrases
-4.282
best hyp: number of verbs -2.818
best hyp: position of phrase with highest
future cost estimate averaged to the
number of phrases
1.002
search: number of alternative phrases
with very low future cost est. -0.528
source: number of verbs 0.467
search: number of alternative phrases
with very high probability 0.355
search: total num. of translation options -0.153
search: number of alternative phrases
with very high future cost estimate -0.142
best hyp: number of parse trees 0.007
source: number of parse trees 0.002
search: total number of hypotheses 0.001
Table 7: Linear regression coefficients for feature
set #6 indicate the contribution of each feature in
the fitted model
334
Acknowledgments
This work has been developed within the TaraXU?
project, financed by TSB Technologiestiftung
Berlin ? Zukunftsfonds Berlin, co-financed by the
European Union ? European fund for regional de-
velopment. Many thanks to Prof. Hans Uszko-
reit for the supervision, Dr. Aljoscha Burchardt,
and Dr. David Vilar for their useful feedback and
to Lukas Poustka for his technical help on feature
acquisition.
References
Avramidis, E. (2012). Comparative Quality Estima-
tion: Automatic Sentence-Level Ranking of Multi-
ple Machine Translation Outputs. In Proceedings
of 24th International Conference on Computational
Linguistics, pages 115?132, Mumbai, India. The
COLING 2012 Organizing Committee.
Avramidis, E., Popovic, M., Vilar, D., and Burchardt,
A. (2011). Evaluate with Confidence Estimation :
Machine ranking of translation outputs using gram-
matical features. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 65?
70, Edinburgh, Scotland. Association for Computa-
tional Linguistics.
Basak, D., Pal, S., and Patranabis, D. C. (2007).
Support vector regression. Neural Information
Processing-Letters and Reviews, 11(10):203?224.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of the
2012 Workshop on Statistical Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montre?al,
Canada. Association for Computational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and Zaidan,
O. (2011). Findings of the 2011 Workshop on Sta-
tistical Machine Translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 22?64, Edinburgh, Scotland. Association for
Computational Linguistics.
Cameron, A. (1998). Regression analysis of count
data. Cambridge University Press, Cambridge UK;
New York NY USA.
Dems?ar, J., Zupan, B., Leban, G., and Curk, T. (2004).
Orange: From Experimental Machine Learning to
Interactive Data Mining. In Principles of Data Min-
ing and Knowledge Discovery, pages 537?539.
Flom, P. L. and Cassell, D. L. (2007). Stopping step-
wise: Why stepwise and similar selection methods
are bad, and what you should use. In NorthEast
SAS Users Group Inc 20th Annual Conference, Bal-
timore, Maryland. 2007.
Friedman, J. H. (1991). Multivariate Adaptive Regres-
sion Splines. The Annals of Statistics, 19(1):1?67.
Goodstadt, L. (2010). Ruffus: a lightweight Python
library for computational pipelines. Bioinformatics,
26(21):2778?2779.
Hosmer, D. (1989). Applied logistic regression. Wiley,
New York [u.a.], 8th edition.
Hunt, E., Martin, J., and Stone, P. (1966). Experiments
in Induction. Academic Press, New York.
Ja?rvelin, K. and Keka?la?inen, J. (2002). Cumulated
gain-based evaluation of IR techniques. ACM Trans.
Inf. Syst., 20(4):422?446.
Kendall, M. G. (1938). A New Measure of Rank Cor-
relation. Biometrika, 30(1-2):81?93.
Koehn, P. (2005). Europarl: A parallel corpus for sta-
tistical machine translation. Proceedings of the tenth
Machine Translation Summit, 5:79?86.
Kononenko, I. (1994). Estimating attributes: analy-
sis and extensions of RELIEF. In Proceedings of
the European conference on machine learning on
Machine Learning, pages 171?182, Secaucus, NJ,
USA. Springer-Verlag New York, Inc.
Lavie, A. and Agarwal, A. (2007). METEOR: An Au-
tomatic Metric for MT Evaluation with High Levels
of Correlation with Human Judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007). Trust
region Newton methods for large-scale logistic re-
gression. In Proceedings of the 24th international
conference on Machine learning - ICML ?07, pages
561?568, New York, New York, USA. ACM Press.
Macha?c?ek, M. . and Ondr?ej, B. (2013). Results of the
WMT13 Metrics Shared Task. In Proceedings of the
8th Workshop on Machine Translation, Sofia, Bul-
garia. Association for Computational Linguistics.
Miller, A. (2002). Subset Selection in Regression.
Chapman & Hall, London, 2nd edition.
Naber, D. (2003). A rule-based style and grammar
checker. Technical report, Bielefeld University,
Bielefeld, Germany.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Petrov, S., Barrett, L., Thibaux, R., and Klein, D.
(2006). Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
335
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 433?440, Syd-
ney, Australia. Association for Computational Lin-
guistics.
Petrov, S. and Klein, D. (2007). Improved inference for
unlexicalized parsing. In Proceedings of the 2007
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Rochester, New York. Association for Compu-
tational Linguistics.
Robnik-S?ikonja, M. and Kononenko, I. (2003). Theo-
retical and Empirical Analysis of ReliefF and RRe-
liefF. Machine Learning, 53(1-2):23?69.
Soricut, R., Wang, Z., and Bach, N. (2012). The SDL
Language Weaver Systems in the WMT12 Quality
Estimation Shared Task. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
pages 145?151, Montre?al, Canada. Association for
Computational Linguistics.
Stolcke, A. (2002). SRILM ? An Extensible Language
Modeling Toolkit. In Proceedings of the Seventh
International Conference on Spoken Language Pro-
cessing, pages 901?904. ISCA.
Stone, M. and Brooks, R. J. (1990). Continuum re-
gression: cross-validated sequentially constructed
prediction embracing ordinary least squares, par-
tial least squares and principal components regres-
sion. Journal of the Royal Statistical Society Series
B Methodological, 52(2):237?269.
Taule?, M., Mart??, A., and Recasens, M. (2008). An-
Cora: Multilevel Annotated Corpora for Catalan and
Spanish. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. European Lan-
guage Resources Association (ELRA).
Tibshirani, R. (1996). Regression shrinkage and selec-
tion via the lasso. Series B:267?288.
Voorhees, E. (1999). TREC-8 Question Answering
Track Report. In 8th Text Retrieval Conference,
pages 77?82, Gaithersburg, Maryland, USA.
336
