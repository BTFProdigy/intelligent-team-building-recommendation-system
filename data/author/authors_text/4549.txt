Unit Completion for a Computer-aided Translation 
System 
Ph i l ippe  Lang la i s ,  George  Foster  and  Guy  Lapa lme 
RAL I  / D IRO 
Universit6 de Montrea l  
C.P. 6128, succursale Centre-vi l le 
Montra l  (Qubec) ,  Canada,  H3C 3J7 
{ f elipe,f oster, lapalme }@iro. umontreal, ca 
Typing 
Abst ract  
This work is in the context of TRANSTYPE, a sys- 
tem that observes its user as he or she types a trans- 
lation and repeatedly suggests completions for the 
text already entered. The user may either accept, 
modify, or ignore these suggestions. We describe the 
design, implementation, and performance of a pro- 
totype which suggests completions of units of texts 
that are longer than one word. 
1 I n t roduct ion  
TRANSTYPE is part of a project set up to explore 
an appealing solution to Interactive Machine Trans- 
lation (IMT). In constrast to classical IMT systems, 
where the user's role consists mainly of assisting the 
computer to analyse the source text (by answering 
questions about word sense, ellipses, phrasal attach- 
ments, etc), in TRANSTYPE the interaction is direct- 
ly concerned with establishing the target ext. 
Our interactive translation system works as fol- 
lows: a translator selects a sentence and begins typ- 
ing its translation. After each character typed by 
the translator, the system displays a proposed com- 
pletion, which may either be accepted using a spe- 
cial key or rejected by continuing to type. Thus 
the translator remains in control of the translation 
process and the machine must continually adapt it- 
s suggestions in response to his or her input. We 
are currently undertaking a study to measure the 
extent o which our word-completion prototype can 
improve translator productivity. The conclusions of 
this study will be presented elsewhere. 
The first version of TrtANSTYPE (Foster et al, 
1997) only proposed completions for the current 
word. This paper deals with predictions which ex- 
tend to the next several words in the text. The po- 
tential gain from multiple-word predictions can be 
appreciated in the one-sentence translation task re- 
ported in table 1, where a hypothetical user saves 
over 60% of the keystrokes needed to produce a 
translation i a word completion scenario, and about 
85% in a "unit" completion scenario. 
In all the figures that follow, we use different fonts 
to differentiate he various input and output: italics 
are used for the source text, sans-serif for characters 
typed by the user and typewr i te r - l i ke  for charac- 
ters completed by the system. 
The first few lines of the table 1 give an idea of 
how TransType functions. Let us assume the unit s- 
cenario (see column 2 of the table) and suppose that 
the user wants to produce the sentence "Ce projet 
de loi est examin~ ~ la chambre des communes" as a 
translation for the source sentence "This bill is ex- 
amined in the house of commons". The first hypoth- 
esis that the system produces before the user enters 
a character is lo i  (law). As this is not a good guess 
from TRANSTYPE the user types the first character 
(c) of the words he or she wants as a translation. 
Taking this new input into account, TRANSTYPE 
then modifies its proposal so that it is compatible 
whith what the translator has typed. It suggests 
the desired sequence ce projet de Ioi, which the user 
can simply validate by typing a dedicated key. Con- 
tinuing in this way, the user and TRANSTYPE alter- 
nately contribute to the final translation. A screen 
copy of this prototype is provided in figure 1. 
2 The  Core  Eng ine  
The core of TRANSTYPE is a completion engine 
which comprises two main parts: an evaluator which 
assigns probabilistic scores to completion hypotheses 
and a generator which uses the evaluation function 
to select he best candidate for completion. 
2.1 The  Eva luator  
The evaluator is a function p(t\[t', s) which assigns to 
each target-text unit t an estimate of its probability 
given a source text s and the tokens t' which precede 
t in the current ranslation of s. 1 Our approach to 
modeling this distribution is based to a large extent 
on that of the IBM group (Brown et al, 1993), but 
it differs in one significant aspect: whereas the IB- 
M model involves a "noisy channel" decomposition, 
we use a linear combination of separate prediction- 
s from a language model p(tlt ~) and a translation 
model p(tls ). Although the noisy channel technique 
1We assume the existence of a determinist ic  procedure for 
tokenizing the target text. 
135 
This bill is examined in the house of commons 
word-completion task unit-completion task 
ce 
projet 
de 
Ioi 
est 
examin~ 
chambre 
des 
communes 
preL completions 
ce+ / lo i  ? C/' 
p+ /es t ?  p / ro je t  
d+ / t rbs  ? d/e 
I+ / t=~s ? I /o i  
e+ /de ? e / s t  
e+ /en ? e/xamin6 
~+ /par ? ~/ 1~ 
+ /chambre 
de+ /co,~unes ? d/e 
+ /communes 
? de/s 
pref. completions 
c-l- /loJ. ? c/e pro je t  de 1oi 
e+ /de ? e / s t  
ex+ /~ la  chambre des communes. 
+ /b l a  chambre des con~unes 
e/n ? ex /min~ 
Table 1: A one-sentence s ssion illustrating the word- and unit-completion tasks. The first column indicates 
the target words the user is expected to produce. The next two columns indicate respectively the prefixes 
typed by the user and the completions proposed by the system in a word-completion task. The last two 
columns provide the same information for the unit-completion task. The total number of keystrokes for 
both tasks is reported in the last line. + indicates the acceptance key typed by the user. A completion is
denoted by a/13 where a is the typed prefix and 13 the completed part. Completions for different prefixes 
are separated by ?. 
is powerful, it has the disadvantage that p(slt' , t) is 
more expensive to compute than p(tls ) when using 
IBM-style translation models. Since speed is cru- 
cial for our application, we chose to forego the noisy 
channel approach in the work described here. Our 
linear combination model is described as follows: 
pCtlt',s) = pCtlt') a(t ' ,s)  + pCtls) \[1 - exit',s)\] (1) 
? ~ ? ? v J 
language translation 
where a(t', s) E \[0, 1\] are context-dependent inter- 
polation coefficients. For example, the translation 
model could have a higher weight at the start of a 
sentence but the contribution of the language mod- 
el might become more important in the middle or 
the end of the sentence? A study of the weightings 
for these two models is described elsewhere? In the 
work described here we did not use the contribution 
of the language model (that is, a(t' ,  s) = O, V t', s). 
Techniques for weakening the independence as- 
sumptions made by the IBM models 1 and 2 have 
been proposed in recent work (Brown et al, 1993; 
Berger et al, 1996; Och and Weber, 98; Wang and 
Waibel, 98; Wu and Wong, 98). These studies report 
improvements on some specific tasks (task-oriented 
limited vocabulary) which by nature are very differ- 
ent from the task TRANSTYPE is devoted to. Fur- 
thermore, the underlying decoding strategies are too 
time consuming for our application? We therefore 
use a translation model based on the simple linear in- 
terpolation given in equation 2 which combines pre- 
dictions of two translation models - -  Ms and M~ - -  
both based on IBM-like model 2(Brown et al, 1993). 
Ms was trained on single words and Mu, described 
in section 3, was trained on both words and units. 
- -  _ (2 )  
word unit 
where Ps and Pu stand for the probabilities given re- 
spectively by Ms and M~. G(s) represents he new 
sequence of tokens obtained after grouping the to- 
kens of s into units. The grouping operator G is 
illustrated in table 2 and is described in section 3. 
2.2  The  Generator  
The task of the generator is to identify units that 
match the current prefix typed by the user, and pick 
the best candidate according to the evaluator. Due 
to time considerations, the generator introduces a
division of the target vocabulary into two parts: a 
small active component whose contents are always 
searched for a match to the current prefix, and a 
much larger passive part over (380,000 word form- 
s) which comes into play only when no candidates 
are found in the active vocabulary. The active part 
is computed ynamically when a new sentence is s- 
elected by the translator. It is composed of a few 
entities (tokens and units) that are likely to appear 
in the translation. It is a union of the best can- 
didates provided by each model Ms and M~ over 
the set of all possible target tokens (resp. units) 
that have a non-null translation probability of being 
translated by any of the current source tokens (resp. 
units). Table 2 shows the 10 most likely tokens and 
units in the active vocabulary for an example source 
sentence. 
136 
that.  is ? what .  the . p r ime,  minister . said 
? and .  i ? have.  outlined? what .  has .  
happened . since? then . .  
c' - est. ce -que ,  le- premier - ministre, a- 
d i t . , .e t . j ' ,  ai. r4sum4- ce. qui .s ' -  est- 
produit - depuis ? . 
g(s) that is what ? the prime minister said ? , and i 
? have . outlined ? what has happened ? since 
then ? . 
As 
A~ 
? ? ? es t  ? ce  ? m in i s t re  ? que .  e t  ? a ? p remier  
l i e  
ce  qu i  s' es t  p rodu i t  ? e t  je  - c '  es t  ce  que .  vo i l~  
ce  que  ? qu '  es t  - c '  es t  ? ,  e t  ? le p remier  min is t re  
d i sa i t  
Table 2: Role of the generator for a sample pair of 
sentences (t is the translation of s in our corpus). 
G(s) is the sequence of source tokens recasted by 
the grouping operator G. A8 indicates the 10 best 
tokens according to the word model, Au the 10 best 
units according to the unit model. 
3 Mode l ing  Un i t  Assoc ia t ions  
Automatically identifying which source words or 
groups of words will give rise to which target words 
or groups of words is a fundamental problem which 
remains open. In this work, we decided to proceed 
in two steps: a) monolingually identifying roups of 
words that would be better handled as units in a giv- 
en context, and b) mapping the resulting source and 
target units. To train our unit models, we used a 
segment of the Hansard corpus consisting of 15,377 
pairs of sentences, totaling 278,127 english token- 
s (13,543 forms) and 292,865 french tokens (16,399 
forms). 
3.1 F inding Monol ingual  Uni ts  
Finding relevant units in a text has been explored in 
many areas of natural anguage processing. Our ap- 
proach relies on distributional and frequency statis- 
tics computed on each sequence of words found in a 
training corpus. For sake of efficiency, we used the 
suffix array technique to get a compact representa- 
tion of our training corpus. This method allows the 
efficient retrieval of arbitrary length n-grams (Nagao 
and Mori, 94; Haruno et al, 96; Ikehara et al, 96; 
Shimohata et al, 1997; Russell, 1998). 
The literature abounds in measures that can help 
to decide whether words that co-occur are linguisti- 
cally significant or not. In this work, the strength of 
association of a sequence of words w\[ = w l , . . . ,  wn 
is computed by two measures: a likelihood-based one 
p(w'~) (where g is the likelihood ratio given in (Dun- 
ning, 93)) and an entropy-based one e(w'~) (Shimo- 
hata et al, 1997). Letting T stand for the training 
text and m a token: 
p(w~) = argming(w~, uS1  ) (3) 
ie\]l,n\[ 
e(w'~) = 0.5x  +k 
~rnlw,~meT h ( Ireq(w'~ m) k Ir~q(wT) \] 
Intuitively, the first measurement accounts for the 
fact that parts of a sequence of words that should 
be considered as a whole should not appear often by 
themselves. The second one reflects the fact that a 
salient unit should appear in various contexts (i.e. 
should have a high entropy score). 
We implemented a cascade filtering strategy based 
on the likelihood score p, the frequency f ,  the length 
l and the entropy value e of the sequences. A 
first filter (.~"1 (lmin, fmin, Pmin, emin)) removes any 
sequence s for which l (s) < lmin or p(s) < Pmin 
or e(s) < e,nin or f ( s )  < fmin.  A second filter 
(~'2) removes sequences that are included in pre- 
ferred ones. In terms of sequence reduction, apply- 
ing ~1 (2, 2, 5.0, 0.2) on the 81,974 English sequences 
of at least two tokens een at least twice in our train- 
ing corpus, less than 50% of them (39,093) were fil- 
tered: 17,063 (21%) were removed because of their 
low entropy value, 25,818 (31%) because of their low 
likelihood value. 
3.2 Mapping 
Mapping the identified units (tokens or sequences) to 
their equivalents in the other language was achieved 
by training a new translation model (IBM 2) us- 
ing the EM algorithm as described in (Brown et al, 
1993). This required grouping the tokens in our 
training corpus into sequences, on the basis of the 
unit lexicons identified in the previous tep (we will 
refer to the results of this grouping as the sequence- 
based corpus). To deal with overlapping possibilities, 
we used a dynamic programming scheme which opti- 
mized a criterion C given by equation 4 over a set S 
of all units collected for a given language plus all sin- 
gle words. G(w~) is obtained by returning the path 
that maximized B(n) .  We investigated several C- 
criteria and we found C~--a length-based measurc 
to be the most satisfactory. Table 2 shows an output 
of the grouping function. 
Oi l  i=o  
B( i )  = argmax 
/~\[1,i\[ ,w~_les ) + B( i  - I - 1) (4) 
0 i f j<=i  
with: Cl (w~)= j - - i  + l e lse 
137 
source unit (s) 
we have 1748 
we must 720 
this bill 640 
people of canada 282 
mr. speaker : 269 
what is happening 190 
of course , 178 
is it the pleasure of the house to 14 
adopt the 
the world 
child care 
the free trade agreement 
post-secondary education 
the first time 
the canadian aviation safety board 
the next five years 
the people of china 
f(8) target units (\[a,p\]) 
\[nous,0.49\] \[avons,0.41\] \[, nous avons,0.07\] 
\[nous devons,0.61\] \[ilrant,0.19\] [nous,0.14\] 
\[ce projet de 1oi,0.35\] \[projet de loi .,0.21\] [projet de loi,0.18\] 
\[les canadiens,0.26\] \[des canadiens,0.21\] \[la population,0.07\] 
\[m. le prdsident :,0.80\] [a,0.07\] \[h la,0.06\] 
Ice qui se passe,0.21\] Ice qui se,0.16\] [et,0.15\] 
\[dvidemment ,0.26\] \[naturellement,0.08\] \[bien stir,0.08\] 
\[plait-il h la chambre d' adopter,0.49\] \[la motion ?,0.42\] [motion 
?,0.04\] 
201 \[le monde,O.46\] [du monde,O.33\] lie monde entier,O.19\] 
86 lies garderies,O.59\] \[la garde d' enfants,O.23\] \[des services de 
garde d' enfants,O.13\] 
75 \[1' accord de libre-dchange,O.96\] \[la ddcision du gatt,O.04\] 
66 \[1' euseignement postsecondaize,O.75\] \[1' dducation postsec- 
ondaire,O.15\] \[des fonds,O.06\] 
62 \[la premiere fois,l.00\] 
36 lie bureau canadien de la s~urit~ adrienne,O.55\] \[du bureau cana- 
dien de la sdcurit~ adrienne,O.31\] \[1'un,O.14\] 
26 \[au cours des cinq prochaines ann~es,O.53\] \[cinq prochaines an- 
ndes,O.27\] \[25 milliards de d ollars,O.lO\] 
17 \[le peuple chinois,0.38\] \[la population chinoise,0.25\] \[les chi- 
nois,O.13\] 
Table 3: Bilingual associations. The first column indicates a source unit, the second one its frequency in the 
training corpus. The third column reports its 3-best ranked target associations (a being a token or a unit, 
p being the translation probability). The second half of the table reports NP-associations obtained after the 
filter described in the text. 
We investigated three ways of estimating the pa- 
rameters of the unit model. In the first one, El, 
the translation parameters are estimated by apply- 
ing the EM algorithm in a straightforward fashion 
over all entities (tokens and units) present at least 
twice in the sequence-based corpus 2. The two next 
methods filter the probabilities obtained with the Ez 
method. In E2, all probabilities p(tls ) are set to 0 
whenever s is a token (not a unit), thus forcing the 
model to contain only associations between source 
units and target entities (tokens or units). In E3 
any parameter of the model that involves a token 
is removed (that is, p(tls ) = 0 if t or s is a token). 
The resulting model will thus contain only unit as- 
sociations. In both cases, the final probabilities are 
renormalized. Table 3 shows a few entries from a 
unit model (Mu) obtained after 15 iterations of the 
EM-algorithm on a sequence corpus resulting from 
the application of the length-grouping criterion (dr) 
over a lexicon of units whose likelihood score is above 
5.0. The probabilities have been obtained by appli- 
cation of the method E2. 
We found many partially correct associations 
Cover the years/au fils des, we have/nous, etc) that 
illustrate the weakness of decoupling the unit iden- 
tification from the mapping problem. In most cas- 
2The entities een only once are mapped to a special "un- 
known" word 
es however, these associations have a lower proba- 
bility than the good ones. We also found few er- 
ratic associations (the first time/e'dtait, some hon. 
members/t, etc) due to distributional rtifacts. It is 
also interesting to note that the good associations 
we found are not necessary compositional in nature 
(we must/il Iaut, people of canada/les canadiens, of 
eourse/6videmment, etc). 
3.3 F i l ter ing  
One way to increase the precision of the mapping 
process is to impose some linguistic constraints on 
the sequences such as simple noun-phrase contraints 
(Ganssier, 1995; Kupiec, 1993; hua Chen and Chen, 
94; Fung, 1995; Evans and Zhai, 1996). It is also 
possible to focus on non-compositional compounds, 
a key point in bilingual applications (Su et al, 1994; 
Melamed, 1997; Lin, 99). Another interesting ap- 
proach is to restrict sequences to those that do not 
cross constituent boundary patterns (Wu, 1995; Fu- 
ruse and Iida, 96). In this study, we filtered for po- 
tential sequences that are likely to be noun phrases, 
using simple regular expressions over the associated 
part-of-speech tags. An excerpt of the association 
probabilities of a unit model trained considering on- 
ly the NP-sequences i given in table 3. Applying 
this filter (referred to as JrNp in the following) to the 
39,093 english sequences still surviving after previ- 
ous filters ~'1 and ~'2 removes 35,939 of them (92%). 
138 
model spared ok good nu u 
1 baseline - model 1 48.98 0 0 747 0 
2 basel ine - model 2 51.83 0 0 747 0 
3 E1 + ~'1(2, 2, 0, 0.2) 50.98 527 1702 5 626 
4 E1+~'1(2,2,5,0.2)  51.61 596 2149 5 658 
5 E1 + ~-~ (2, 2, 5, 0.2) + 9r2 51.72 633 2265 5 657 
6 E2 + ~'~(2,2,0,0.2) 51.39 514 1551 43 578 
7 ?2 + ~-~ (2, 2, 5, 0.2) 51.99 470 1889 46 614 
8 E2 + ~'~(2,2,5,0.2) + ~'2 52.12 493 1951 46 606 
9 E3 + ~-1(2, 2, 0, 0.2) 51.07 577 1699 43 588 
10 E2 + ~-1(2, 2, 5, 0.2) 51.47 629 2124 46 618 
11 E2+~'~(2 ,2 ,5 ,0 .2 )+~'2  51.68 665 2209 46 615 
12 ~1 -}- .~1 (2, 2, 5, 0.2) -}- .~2 -}- ~:NP 52.83 416 1302 4 564 
13 E2 + ~'1(2, 2, 5, 0.2) + ~NP 53.12 439 1031 228 425 
14 ?2 + ~'~ (2, 2, 5, 0.2) + 5r2 + ~'NP 53.16 458 1052 199 439 
15 ~3 -{- ~ : 0.4 -}- ~-1(2, 2, 5, 0.2) 4- .~NP 53.22 495 1031 228 425 
Table 4: Completion results of several translation models, spared: theoretical proportion of characters 
saved; ok: number of target units accepted by the user; good: number of target units that matched the 
expected whether they were proposed or not; nu: number of sentences for which no target unit was found 
by the translation model; u: number of sentences for which at least one helpful unit has been found by the 
model, but not necessarily proposed. 
More than half of the 3,154 remaining NP-sequences 
contain only two words. 
4 Resu l t s  
We collected completion results on a test corpus 
of 747 sentences (13,386 english tokens and 14,506 
french ones) taken from the Hansard corpus. These 
sentences have been selected randomly among sen- 
tences that have not been used for the training. 
Around 18% of the source and target words are not 
known by the translation model. 
The baseline models (line 1 and 2) are obtained 
without any unit model (i.e. /~ = 1 in equation 2). 
The first one is obtained with an IBM-like model 1 
while the second is an IBM-like model 2. We observe 
that for the pair of languages we considered, model 
2 improves the amount of saved keystrokes of almost 
3% compared to model 1. Therefore we made use of 
alignment probabilities for the other models. 
The three next blocks in table 4 show how the 
parameter estimation method affects performance. 
Training models under the C1 method gives the worst 
results. This results from the fact that the word- 
to-word probabilities trained on the sequence based 
corpus (predicted by Mu in equation 2) are less ac- 
curate than the ones learned from the token based 
corpus. The reason is simply that there are less oc- 
currences of each token, especially if many units are 
identified by the grouping operator. 
In methods C2 and C3, the unit model of equation 
2 only makes predictions pu(tls ) when s is a source u- 
nit, thus lowering the noise compared to method ?1. 
We also observe in these three blocks the influence 
of sequence filtering: the more we filter, the better 
the results. This holds true for all estimation meth- 
ods tried. In the fifth block of table 4 we observe 
the positive influence of the NP-filtering, especially 
when using the third estimation method. 
The best combination we found is reported in line 
15. It outperforms the baseline by around 1.5%. 
This model has been obtained by retaining all se- 
quences een at least two times in the training cor- 
pus for which the likelihood test value was above 5 
and the entropy score above 0.2 (5rl (2, 2, 5, 0.2)). In 
terms of the coverage of this unit model, it is in- 
teresting to note that among the 747 sentences of 
the test session, there were 228 for which the model 
did not propose any units at all. For 425 of the re- 
maining sentences, the model proposed at least one 
helpful (good or partially good) unit. The active vo- 
cabulary for these sentences contained an average of 
around 2.5 good units per sentence, of which only 
half (495) were proposed during the session. The 
fact that this model outperforms others despite it- 
s relatively poor coverage (compared to the others) 
may be explained by the fact that it also removes 
part of the noise introduced by decoupling the i- 
dentification of the salient units from the training 
procedure. Furthermore, as we mentionned earlier, 
the more we filter, the less the grouping scheeme 
presented in equation 4 remains necessary, thus re- 
ducing a possible source of noise. 
The fact that this model outperforms others, de- 
spite its relatively poor coverage, is due to the fact 
139 
E ich le r  C )pt lons  
l am p leased  to  t~ lce  ]par t  in  th i s  debate  tod  W . 
Us ing  rod  W "s techno log ies ,  i t  i s  poss ib le  fo r  a l l  C~m~dia~s  to  
reg is ter  the i r  votes  on  i s s t les  of  pub l i c  spend ing  and  pub l i c  
I )o r ro~v ing .  
II me fa l t  p la le l r  de  prendre  la paro le  au Jourd 'hu i  dana  le cadre  de  ?e  
d~bat .  
Gr~ice  & la  techno log le  moderne ,  toue  lea  Canad len= peuvent  6e  
prononcer  sur  le=;  quest ion= de  d6pen=e== et  d" e rnprunta  de  I" I~tat  . 
Not re  p 
Figure 1: Example of an i teraction i  TRANSTYPE with the source text in the top half of the screen. The 
target text is typed in the bottom half with suggestions given by the menu at the insertion point. 
that it also removes part of the noise that is intro- 
duced by dissociating the identification ofthe salient 
units from the training procedure. ~rthermore, as 
we mentioned earlier, the more we filter, the less the 
grouping scheme presented in equation 4 remains 
necessary, thus further reducing an other possible 
source of noise. 
5 Conclusion 
We have described a prototype system called 
TRANSTYPE which embodies an innovative ap- 
proach to interactive machine translation in which 
the interaction is directly concerned with establish- 
ing the target ext. We proposed and tested a mech- 
anism to enhance TRANSTYPE by having it predic- 
t sequences of words rather than just completions 
for the current word. The results show a modest 
improvement in prediction performance which will 
serve as a baseline for our future investigations. One 
obvious direction for future research is to revise our 
current strategy of decoupling the selection of units 
from their bilingual context. 
Acknowlegments 
TRANSTYPE is a project funded by the Natural Sci- 
ences and Engineering Research Council of Canada. 
We are undebted to Elliott Macklovitch and Pierre 
Isabelle for the fruitful orientations they gave to this 
work. 
References 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent J. Della Pietra. 1996. A maximum entropy 
approach to natural language processing. Compu- 
tational Linguistics, 22(1):39-71. 
Peter F. Brown, Stephen A. Della Pietra, Vincen- 
t Della J. Pietra, and Robert L. Mercer. 1993. 
The mathematics of machine trmaslation: Pa- 
rameter estimation. Computational Linguistics, 
19(2):263-312, June. 
Ted Dunning. 93. Accurate methods for the statis- 
tics of surprise and coincidence. Computational 
Linguistics, 19(1):61-74. 
David A. Evans and Chengxiang Zhai. 1996. Noun- 
phrase analysis in unrestricted text for informa- 
tion retrieval. In Proceedings of the 34th Annu- 
al Meeting of the Association for Computational 
Linguistics, pages 17-24, Santa Cruz, California. 
George Foster, Pierre Isabelle, and Pierre Plamon- 
don. 1997. Target-text Mediated Interactive Ma- 
chine Translation. Machine Translation, 12:175- 
194. 
Pascale Fung. 1995. A pattern matching method for 
finding noun and proper noun translations from 
noisy parallel corpora. In Proceedings ofthe 33rd 
Annual Meeting of the Association for Compu- 
tational Linguistics, pages 236-243, Cambridge, 
Massachusetts. 
Osamu Furuse and Hitoshi Iida. 96. Incremen- 
140 
tal translation utilizing constituent boundray pat- 
terns. In Proceedings of the 16th International 
Conference On Computational Linguistics, pages 
412-417, Copenhagen, Denmark. 
Eric Gaussier. 1995. Modles statistiques et patron- 
s morphosyntaxiques pour l'extraction de lcxiques 
bilingues. Ph.D. thesis, Universit de Paris 7, jan- 
vier. 
Masahiko Haruno, Satoru Ikehara, and Takefumi 
Yamazaki. 96. Learning bilingual collocations by 
word-level sorting. In Proceedings of the 16th In- 
ternational Conference On Computational Lin- 
guistics, pages 525-530, Copenhagen, Denmark. 
Kuang hua Chen and Hsin-Hsi Chen. 94. Extract- 
ing noun phrases from large-scale texts: A hybrid 
approach and its automatic evaluation. In Pro- 
ceedings of the 32nd Annual Meeting of the Asso- 
ciation for Computational Linguistics, pages 234- 
241, Las Cruces, New Mexico. 
Satoru Ikehara, Satoshi Shirai, and Hajine Uchino. 
96. A statistical method for extracting uinterupt- 
ed and interrupted collocations from very large 
corpora. In Proceedings of the 16th International 
Conference On Computational Linguistics, pages 
574-579, Copenhagen, Denmark. 
Julian Kupiec. 1993. An algorithm for finding noun 
phrase correspondences in bilingual corpora. In 
Proceedings of the 31st Annual Meeting of the 
Association for Computational Linguistics, pages 
17-22, Colombus, Ohio. 
Dekang Lin. 99. Automatic identification of non- 
compositional phrases. In Proceedings of the 37th 
Annual Meeting of the Association for Computa- 
tional Linguistics, pages 317-324, College Park, 
Maryland. 
I. Dan Melamed. 1997. Automatic discovery of non- 
compositional coumpounds in parallel data. In 
Proceedings of the 2nd Conference on Empirical 
Methods in Natural Language Processing, pages 
97-108, Providence, RI, August, lst-2nd. 
Makoto Nagao and Shinsuke Mori. 94. A new 
method of n-gram statistics for large number of 
n and automatic extraction of words and phrases 
from large text data of japanese. In Proceedings 
of the 16th International Conference On Com- 
putational Linguistics, volume 1, pages 611-615, 
Copenhagen, Denmark. 
Franz Josef Och and Hans Weber. 98. Improving 
statistical natural anguage translation with cate- 
gories and rules. In Proceedings of the 36th Annu- 
al Meeting of the Association for Computational 
Linguistics, pages 985-989, Montreal, Canada. 
Graham Russell. 1998. Identification of salient to- 
ken sequences. Internal report, RALI, University 
of Montreal, Canada. 
Sayori Shimohata, Toshiyuki Sugio, and Junji 
Nagata. 1997. Retrieving collocations by co- 
occurrences and word order constraints. In Pro- 
ceedings of the 35th Annual Meeting of the Asso- 
ciation for Computational Linguistics, pages 476- 
481, Madrid Spain. 
Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 
1994. A corpus-based approach to automatic om- 
pound extraction. In Proceedings of the 32nd An- 
nual Meeting of the Association for Computation- 
al Linguistics, pages 242-247, Las Cruces, New 
Mexico. 
Ye-Yi Wang and Alex Waibel. 98. Modeling with 
structures in statistical machine translation. In 
Proceedings of the 36th Annual Meeting of the 
Association for Computational Linguistics, vol- 
ume 2, pages 1357-1363, Montreal, Canada. 
Dekai Wu and Hongsing Wong. 98. Machine trans- 
lation with a stochastic grammatical channel. In 
Proceedings of the 36th Annual Meeting of the 
Association for Computational Linguistics, pages 
1408-1414, Montreal, Canada. 
Dekai Wu. 1995. Stochastic inversion transduc- 
tion grammars, with application to segmentation, 
bracketing, and alignment of parallel corpora. In 
Proceedings of the International Joint Conference 
on Artificial Intelligence, volume 2, pages 1328- 
1335, Montreal, Canada. 
141 
c? 2002 Association for Computational Linguistics
Generating Indicative-Informative
Summaries with SumUM
Horacio Saggion? Guy Lapalme?
University of Sheffield Universite? de Montre?al
We present and evaluate SumUM, a text summarization system that takes a raw technical text
as input and produces an indicative informative summary. The indicative part of the summary
identifies the topics of the document, and the informative part elaborates on some of these topics
according to the reader?s interest. SumUM motivates the topics, describes entities, and defines
concepts. It is a first step for exploring the issue of dynamic summarization. This is accomplished
through a process of shallow syntactic and semantic analysis, concept identification, and text
regeneration. Our method was developed through the study of a corpus of abstracts written
by professional abstractors. Relying on human judgment, we have evaluated indicativeness,
informativeness, and text acceptability of the automatic summaries. The results thus far indicate
good performance when compared with other summarization technologies.
1. Introduction
A summary is a condensed version of a source document having a recognizable genre
and a very specific purpose: to give the reader an exact and concise idea of the contents
of the source. In most cases, summaries are written by humans, but nowadays, the
overwhelming quantity of information,1 and the need to access the essential content
of documents accurately in order to satisfy users? demands calls for the development
of computer programs able to produce text summaries. The process of automatically
producing a summary from a source text consists of the following steps:
1. interpreting the text
2. extracting the relevant information, which ideally includes the ?topics?
of the source
3. condensing the extracted information and constructing a summary
representation
4. presenting the summary representation to the reader in natural language.
Even though some approaches to text summarization produce acceptable summaries
for specific tasks, it is generally agreed that the problem of coherent selection and
expression of information in text summarization is far from being resolved. Sparck
Jones and Endres-Niggemeyer (1995) stated the need for a research program in text
? Department of Computer Science, University of Sheffield, Sheffield, England, United Kingdom, S1 4DP.
E-mail: saggion@dcs.shef.ac.uk
? De?partement d?Informatique et Recherche Ope?rationnelle, Universite? de Montre?al, CP 6128, Succ
Centre-Ville, Montre?al, Que?bec, Canada, H3C 3J7. E-mail: lapalme@iro.umontreal.ca
1 In 1998, the volume of this information was calculated at somewhere between 400 and 500 million
documents (Filman and Pant 1998).
498
Computational Linguistics Volume 28, Number 4
summarization that would study the relation between source document and summary,
the different types of summaries and their functions, the development of new methods
and/or combination of already existing techniques for text summarization, and the
development of evaluation procedures for summaries and systems. Rowley (1982)
proposes the following typology of different types of document condensations:
? the extract, which is a set of passages selected from a source document
to represent the whole document
? the summary, which occurs at the end of the document and is a
restatement of the salient findings of a work
? the abridgment, which is a reduction of the original document that
necessarily omits secondary points
? the precis, which stands for the main points of an argument
? the digest, which is a condensation of a book or news article
? the highlight, which is a comment included in specific parts of a
document to alert a reader
? the synopsis, which in cinematography represents a script of a film.
In our research, we are concerned only with summaries of technical articles, which
are called abstracts. In this context, two main types of abstracts are considered (ANSI
1979; ERIC 1980; Maizell, Smith, and Singer 1971): indicative abstracts, which point to
information alerting the reader about the content of an article in a given domain (these
abstracts will contain sentences like ?The work of Consumer Advice Centres is exam-
ined.?), and informative abstracts, which provide as much quantitative or qualitative
information contained in the source document as possible (these abstracts will contain
sentences like ?Consumer Advice Centres have dealt with preshopping advice, edu-
cation on consumers? rights and complaints about goods and services, advising the
client and often obtaining expert assessments.?). In the course of our research, we have
studied the relation between abstracts and source documents, and as a result, we have
developed SumUM (Summarization at Universite? de Montre?al), a text summarization
system that produces an indicative-informative abstract for technical documents. The
abstracts are produced in two steps: First, the reader is presented with an indicative
abstract that identifies the topics of the document (what the authors present, discuss,
etc.). Then, if the reader is interested in some of the topics, specific information about
them from the source document is presented in an informative abstract.
Figure 1 shows an automatic abstract produced by our system. The abstract was
produced by a process of conceptual identification and text re-generation we call se-
lective analysis. The indicative abstract contains information about the topic of the
document. It describes the topics of sections and introduces relevant entities. The iden-
tified topics are terms either appearing in the indicative abstract or obtained from the
terms and words of the indicative abstract through a process of term expansion. The
one particular feature of these terms is that they can be used to obtain more conceptual
information from the source document, such as definitions or statements of relevance,
usefulness, and development, as can be seen in Figure 2.
This article is organized as follows. In the next section, we describe the analysis
of a corpus of professional abstracts used to specify selective analysis; conceptual and
linguistic information for the task of summarization of technical texts deduced from
this corpus is also presented. An overview of selective analysis and the implementation
499
Saggion and Lapalme Generating Summaries with SumUM
Designing for human-robot symbiosis
Presents the views on the development of intelligent interactive service robots.
The authors have observed that a key research issue in service robotics is the inte-
gration of humans into the system. Discusses some of the technologies with par-
ticular emphasis on human-robot interaction, and system integration; describes
human direct local autonomy (HuDL) in greater detail; and also discusses system
integration and intelligent machine architecture (IMA). Gives an example imple-
mentation; discusses some issues in software development; and also presents the
solution for integration, the IMA. Shows the mobile robot.
Identified Topics: HuDL - IMA - aid systems - architecture - holonic manu-
facturing system - human - human-robot interaction - intelligent interactive
service robots - intelligent machine architecture - intelligent machine software
- interaction - key issue - widely used interaction - novel software architecture
- overall interaction - robot - second issue - service - service robots - software
- system - Technologies
Figure 1
Indicative abstract and identified topics for the text ?Designing for Human-Robot Symbiosis,?
D. M. Wilkes et al, Industrial Robot, 26(1), 1999, 49?58.
Development of a service robot is an extremely challenging task.
In the IRL, we are using HuDL to guide the development of a cooperative service
robot team.
IMA is a two-level software architecture for rapidly integrating these elements,
for an intelligent machine such as a service robot.
A holonic manufacturing system is a manufacturing system having autonomous
but cooperative elements called holons (Koestler, 1971).
Communication between the robot and the human is a key concern for intelligent
service robotics.
Figure 2
Informative abstract elaborating some topics.
of our experimental prototype, SumUM, is then presented in section 3. In section 4, we
discuss the limitations of our approach; then, in section 5, we present an evaluation and
comparison of our method with state-of-the art summarization systems and human
abstracts. Related work on text summarization is discussed in section 6. Finally, in
section 7, we draw our conclusions and discuss prospects for future research.
2. Observations from a Corpus
We have developed our method of text summarization by studying a corpus of profes-
sional abstracts and source documents. Our corpus contains 100 items, each composed
of a professional abstract and its source document. As sources for the abstracts we used
the journals Library & Information Science Abstracts (LISA), Information Science Abstracts
500
Computational Linguistics Volume 28, Number 4
(ISA), and Computer & Control Abstracts. The source documents were found in journals
of computer science (CS) and information science (IS), such as AI Communications, AI
Magazine, American Libraries, Annals of Library Science & Documentation, Artificial Intelli-
gence, Computers in Libraries, and IEEE Expert, among others (a total of 44 publications
were examined). The professional abstracts contained three sentences on the average,
with a maximum of seven and a minimum of one. The source documents covered a va-
riety of subjects from IS and CS. We examined 62 documents in CS and 38 in IS, some
of them containing author-provided abstracts. Most of the documents are structured in
sections; but apart from conceptual sections such as ?Introduction? and ?Conclusion,?
they do not follow any particular style (articles from medicine, for example, usually
have a fixed structure like ?Introduction,? ?Method,? ?Statistical Analysis,? ?Result,?
?Discussion,? ?Previous Work,? ?Limitations,? ?Conclusion,? but this was not the case
in our corpus). The documents were 7 pages on average, with a minimum of 2 and
a maximum of 45. Neither the abstracts nor the source documents were electronically
available, so the information was collected through photocopies. Thus we do not have
information regarding number of sentences and words in the source document.
Our methodological approach consisted of the manual alignment of sentences
from the professional abstract with elements of the source document. This was accom-
plished by looking for a match between the information in the professional abstract
and the information in the source document. The structural parts of the source doc-
ument we examined were the title of the source document, the author abstract, the
first section, the last section, the section headings, and the captions of tables and fig-
ures. When the information was not found, we looked in other parts of the source
document. The information is not always found anywhere in the source document, in
which case we acknowledge that fact. This methodological process was established af-
ter studying procedures for abstract writing (Cremmins 1982; Rowley 1982) and some
initial observations from our corpus. One alignment is shown in Table 1. All align-
ments are available for research purposes at the SumUM Web page ?http://www-
rali.iro.umontreal.ca/sumum.html?.
In this example, the three sentences of the professional abstract were aligned with
four elements of the source document, two in the introduction and two in the author-
provided abstract. The information of the abstract was found ?literally? in the source
document. The differences between the sentences of the professional abstract and those
of the source document are the persons of the verbs (?Presents? vs. ?We present? in
alignment (1)), the verbs (?were discovered? vs. ?We found? in alignment (3)), the
impersonal versus personal styles (?Uses? vs. ?Our experiment used? in alignment
(2)), and the use of markers in the source document (?In this paper? in alignment (1)).
This example shows that the organization of the abstract does not always mirror the
organization of the source document.
2.1 Distributional Results
The 309 sentences of the professional abstracts in our corpus were manually aligned
with 568 elements in the source documents. (We were not able to align six sentences
of the professional abstracts.) Other studies have already investigated the alignment
between sentences in the abstract and sentences in the source document. Kupiec, Ped-
ersen, and Chen (1995) report on the semiautomatic alignment of 79% of sentences of
professional abstracts in a corpus of 188 documents with professional abstracts. Us-
ing automatic means, it is difficult to deal with conceptual alignments that appeared
in our corpus. Teufel and Moens (1998) report on a similar work, but this time on
the alignment of sentences from author-provided abstracts. They use a corpus of 201
articles, obtaining only 31% of alignable sentences by automatic means. No informa-
501
Saggion and Lapalme Generating Summaries with SumUM
Table 1
Item of corpus. Professional abstract: Library & Information Science abstract 3024 and source
document: ?Movement Characteristics Using a Mouse with Tactile and Force Feedback,?
International Journal of Human-Computer Studies, 45(5), October 1996, pages 483?493.
Ex. Professional Abstract Source Document Position/Type
(1) Presents the results of an
empirical study that investigates
the movement characteristics of
a multi-modal mouse?a mouse
that includes tactile and relevance
feedback.
In this paper, we present the
results of an empirical study
that investigates the movement
characteristics of a multi-modal
mouse?a mouse that includes
tactile and force feedback.
1st/Intr.
(2) Uses a simple target selection
task while varying the target
distance, target size, and the
sensory modality.
Our experiment used a simple
target selection task while varying
the target distance, target size, and
the sensory modality.
1st/Intr.
(3) Significant reduction in the overall
movement times and in the time
taken to stop the cursor after en-
tering the target were discovered,
indicating that modifying a mouse
to include tactile feedback, and
to a lesser extent, force feedback,
offers performance advantages in
target selecting tasks.
We found significant reductions in
the overall movement time and in
the time to stop the cursor after
entering the target.
?/Abs.
The results indicate that modi-
fying a mouse to include tac-
tile feedback, and to a lesser ex-
tent, force feedback, offers perfor-
mance advantages in target selec-
tion tasks.
?/Abs.
Table 2
Distribution of information.
Documents With Author Abstract Without Author Abstract Average
# % # % # % %
Title 10 2 6 2 4 1 2
Author abstract 83 15 83 34 20
First section 195 34 61 26 134 42 40
Last section 18 3 6 2 12 4 4
Headlines
and captions 191 33 76 31 115 36 23
Other sections 71 13 13 5 58 17 11
Total 568 100 245 100 323 100 100
tion is given about the distribution of the sentences in structural parts in the source
document.
In Table 2, we present the distribution of the sentences in the source documents
that were aligned with the professional abstracts in our corpus. We consider all the
structured documents of our corpus (97 documents). The first three columns contain
the information for all documents, for documents with author abstracts, and for docu-
ments without author abstracts (the information is given in terms of total elements and
502
Computational Linguistics Volume 28, Number 4
percentage of elements). We also recorded how the types of information are distributed
in the professional abstract. For each abstract, we computed the ratio of the number
of elements of each type contributing to the abstract to the total number of elements
in the abstract (for example, the abstract in Table 1 contains 50% of first section and
50% of author abstract). The last column gives the average of the information over all
abstracts. In this corpus, we found that 72% of the information for the abstracts comes
from the following structural parts of the source documents: the title of the document,
the first section, the last section, and the section headers and captions of tables and
figures (sum of these entries on the first column of Table 2). Sharp (1989) reports on
experiments carried out with abstractors in which it is shown that introductions and
conclusions provide a basis for producing a coherent and informative abstract. In fact
abstractors use a short cut strategy (looking at the introduction and conclusion) prior
to looking at the whole paper. But our results indicate that using just those parts is not
enough to produce a good informative abstract. Important information is also found
in sections other than the introduction and conclusion. Abstractors not only select the
information for the abstract because of its particular position in the source document,
but they also look for specific types of information that happen to be lexically marked.
In Table 1 the information reported is the topic of the document, the method, and the
author?s discovery. This information is lexically marked in the source document by
expressions such as we, paper, present, study, experiment, use, find, and indicate. Based
on these observations we have defined a conceptual and linguistic model for the task
of text summarization of technical articles.
2.2 Conceptual Information for Text Summarization
A scientific and technical article is the result of the complex process of scientific in-
quiry, which starts with the identification of a problem and ends with its solution. It is
a complex linguistic record of knowledge referring to a variety of real and hypothetical
concepts and relations. Some of them are domain dependent (like diseases and treat-
ments in medical science; atoms and fusion in physics; and algorithms and proofs in
computer science), whereas others are generic to the technical literature (authors, the
research article, the problem, the solution, etc.). We have identified 55 concepts and 39
relations that are typical of a technical article and relevant for identifying types of in-
formation for text summarization by collecting domain-independent lexical items and
linguistic constructions from the corpus and classifying them using thesauri (Vianna
1980; Fellbaum 1998). We expanded the initial set with other linguistic constructions
not observed in the corpus.
Concepts. Concepts can be classified in categories referring to the authors (the
authors of the article, their affiliation, researchers, etc.), the work of the authors (work,
study, etc.), the research activity (current situation, need for research, problem, solu-
tion, method, etc.), the research article (the paper, the paper components, etc.), the
objectives (objective, focus, etc.), and the cognitive activities (presentation, introduc-
tion, argument, etc.).
Relations. Relations refer to general activities of the author during the research
and writing of the work: studying (investigate, study, etc.), reporting the work (present,
report, etc.), motivating (objective, focus, etc.), thinking (interest, opinion, etc.), and
identifying (define, describe, etc.).
Types of Information. We have identified 52 types of information for the process
of automatic text summarization referring to the following aspects of the technical
503
Saggion and Lapalme Generating Summaries with SumUM
Table 3
Conceptual information for text summarization.
Domain concepts author, institutions, affiliation, author related, research group, project, research
paper, others? paper, study, research, problem, solution, method, result, experiment,
need, goal, focus, conclusion, recommendation, summary, researcher, work,
hypothesis, research question, future plan, reference, acronym, expansion,
structural, title, caption, quantity, mathematical, paper component, date,
conceptual goal, conceptual focus, topic, introduction, overview, survey,
development, analysis, comparison, discussion, presentation, definition,
explanation, suggestion, discovery, situation, advantage, example
Domain relations make known, show graphical material, study, investigate, summarize, situation,
need, experiment, discover, infer, problem, solution, objective, focus, conclude,
recommend, create, open, close, interest, explain, opinion, argue, comment, suggest,
evidence, relevance, define, describe, elaborate, essential, advantage, use, identify
entity, exemplify, effective, positive, novel, practical
Indicative types topic of document, possible topic, topic of section, conceptual goal, conceptual
focus, author development, development, inference, author interest, interest,
author study, study, opening, closing, problem, solution, topic, entity introduction,
acronym identification, signaling structure, signaling concept, experiments,
methodology, explaining, commenting, giving evidence, need for research,
situation, opinion, discovery, demonstration, investigation, suggestion, conclusion,
summarization
Informative types relevance, goal, focus, essential, positiveness, usefulness, effectiveness, description,
definition, advantage, practicality, novelty, elaboration, exemplification,
introduction, identification, development
article: background information (situation, need, problem, etc.), reporting of informa-
tion (presenting entities, topic, subtopics, objectives, etc.), referring to the work of the
author (study, investigate, method, hypothesis, etc.), cognitive activities (argue, infer,
conclude, etc.), and elaboration of the contents (definitions, advantages, etc.).
The complete list of concepts, relations, and types of information is provided in
Table 3. Concepts and relations are the basis for the classification of types of infor-
mation referring to the essential contents of a technical abstract. Nevertheless, the
presence of a single concept or relation in a sentence is not enough to understand
the type of information it conveys. The co-occurrence of concepts and relations in
appropriate linguistic-conceptual patterns is used in our case as the basis for the clas-
sification of the sentences. The types of information are classified as Indicative or
Informative depending on the type of abstract to which they will contribute. For ex-
ample, Topic of Document and Topic of Section are indicative, whereas Goal of
Entity and Description of Entity are informative. Note that we have identified only a
few linguistic expressions used to express particular elements of the conceptual model,
because we were mainly concerned with the development of a general method of text
summarization and because the task of constructing such linguistic resources is time
consuming.
2.3 From Source to Abstract
According to Cremmins (1982), the last step in the human production of the sum-
mary text is the ?extracting? into ?abstracting? step in which the extracted informa-
tion will be mentally sorted into a preestablished format and will be ?edited? using
cognitive techniques. The editing of the raw material ranges from minor to major
operations. Cremmins gives little indication, however, about the process of editing.
504
Computational Linguistics Volume 28, Number 4
Table 4
Text editing in human abstracting.
Professional Abstract Source Document
Mortality in rats and mice of both sexes was
dose related.
There were significant positive associations
between the concentrations of the substance
administered and mortality in rats and mice
of both sexes.
No treatment related tumors were found in
any of the animals.
There was no convincing evidence to indicate
that endrin ingestion induced any of the
different types of tumors which were found
in the treated animals.
Major transformations are those of the complex process of language understanding
and production, such as deduction, generalization, and paraphrase. Some examples
of editing given by Cremmins are shown in Table 4. In the first example, the con-
cept mortality in rats and mice of both sexes is stated with the wording of the source
document; however, the concept expressed by the concentrations of the substance admin-
istered is stated with the expression dose. In the second example, the relation between
the tumors and endrin ingestion is expressed through the complex nominal treatment
related tumors.
In his rules for abstracting, Bernier (1985) states that redundancy, repetition, and
circumlocutions are to be avoided. He gives a list of linguistic expressions that can be
safely removed from extracted sentences or reexpressed in order to gain conciseness.
These include expressions such as It was concluded that X, to be replaced by X, and It
appears that, to be replaced by Apparently. Also, Mathis and Rush (1985) indicate that
some transformations in the source material are allowed, such as concatenation, trun-
cation, phrase deletion, voice transformation, paraphrase, division, and word deletion.
Rowley (1982) mentions the inclusion of the lead or topical sentence and the use of
active voice and advocates conciseness. But in fact, the issue of editing in text summa-
rization has usually been neglected, notable exceptions being the works by Jing and
McKeown (2000) and Mani, Gates, and Bloedorn (1999). In our work, we partially ad-
dress this issue by enumerating some transformations frequently found in our corpus
that are computationally implementable. The transformations are always conceptual
in nature and not textual (they do not operate on the string level), even if some of
them seem to take the form of simple string deletion or substitution. The rephras-
ing transformations we have identified are outlined below. We also include for each
transformation the number and percentage of times the transformation was used to
produce a sentence of the professional abstract. (Note that the percentages do not add
up to 100, as sentences can be involved in more than one operation.)
Syntactic verb transformation: Some verbs from the source document are
reexpressed in the abstract, usually in order to make the style impersonal.
The person, tense, and voice of the original verb are changed. Also, verbs
that are used to state the topic of the document are generally expressed in
the present tense (in active or passive voice). The same applies to verbs
introducing the objective of the research paper or investigation (according to
convention, objectives are reported in the present tense and results in the
past tense). This transformation was observed 48 times (15%).
505
Saggion and Lapalme Generating Summaries with SumUM
Lexical verb transformation: A verb used to introduce a topic is changed and
restated in the impersonal form. This transformation was observed 13 times
(4%).
Verb selection: The topic or subtopic of the document is introduced by a
domain verb, usually when information from titles is used to create a
sentence. This transformation was observed 70 times (21%).
Conceptual deletion: Domain concepts such as research paper and author are
avoided in the abstract. This transformation was observed 43 times (13%).
Concept reexpression: Domain concepts such as author, research paper, and
author-related entity are stated in the impersonal form. This transformation
was observed 4 times (1%).
Structural deletion: Discourse markers (contrast, structuring, logical
consequence, adding, etc.) such as first, next, finally, however, and although are
deleted. This transformation was observed 7 times (2%).
Clause deletion: One or more clauses (principal or complement) of the
sentence are deleted. This transformation was observed 47 times (14%).
Parenthetical deletion: Some parenthetical expressions are eliminated. This
transformation was observed 10 times (3%).
Acronym expansion: Acronyms introduced for the first time are presented
along with their expansions, or only the expansion is presented. This
transformation was observed 7 times (2%).
Abbreviation: A shorter expression (e.g., acronym or anaphoric expression) is
used to refer to an entity. This transformation was observed 3 times (1%).
Merge: Information from several parts of the source document are merged
into a single sentence. This is the usual case when reporting entities stated in
titles and captions. This transformation was observed 124 times (38%).
Split: Information from one sentence of the source document is presented in
separate sentences in the abstract. This transformation was observed 3 times
(1%).
Complex reformulation: A complex reformulation takes place. This could
involve several cognitive processes, such as generalization and paraphrase.
This transformation was observed 75 times (23%).
Noun transformations: Other transformations take place, such as
nominalization, generalization, restatement of complex nominals, deletion of
complex nominals, expansion of complex nominals (different classes of
aggregation), and change of initial uppercase to lowercase (e.g., when words
from titles or headlines, usually in upper initial, are used for the summary).
This transformation was observed 70 times (21%).
No transformation: The information is reported as in the source. This
transformation was observed 35 times (11%).
We found that transformations involving domain verbs appeared in 40% of the
sentences, noun editing occurred in 38% of the sentences, discourse level editing oc-
curred in 19% of the sentences, merging and splitting of information occurred in 38%
of the sentences, complex reformulation accounts for 23% of the sentences, and finally,
506
Computational Linguistics Volume 28, Number 4
only 11% of the information from the source document is stated without transfor-
mation. Although most approaches to automatic text summarization present the ex-
tracted information in both the order and the form of the original, this is not the case
in human-produced abstracts. Nevertheless, some transformations in the source doc-
ument could be implemented by computers with state-of-the-art techniques in natural
language processing in order to improve the quality of the automatic summaries.
In this section, we have studied relations between abstracts and their source doc-
uments. This study was motivated by the need to answer to the question of content
selection in text summarization (Sparck Jones 1993). We have also addressed here an-
other important research question: how the information is expressed in the summary.
Our study was based on the manual construction of alignments between sentences
of professional abstracts and elements of source documents. In order to obtain an ap-
propriate coverage, abstracts from different secondary sources and source documents
from different journals were used. We have shown that more than 70% of the informa-
tion for abstracts comes from the introduction, conclusion, titles, and captioning of the
source document. This is an empirical verification of what is generally acknowledged
in practical abstract writing in professional settings. We have also identified 15 types of
transformation usually applied to the source document in order to produce a coherent
piece of text. Of the sentences of our corpus, 89% have been edited. In section 3.1, we
detail the specification of patterns of sentence and text production inspired from our
corpus study that were implemented in our automatic system.
Although the linguistic information for our model has been manually collected,
Teufel (1998) has shown how this labor-intensive task can be accomplished in a semi-
automatic fashion. The analysis presented here and the idea of the alignments have
been greatly influenced by the exploration of abstracting manuals (Cremmins 1982).
Our conceptual model comes mainly from the empirical analysis of the corpus but
has also been influenced by work on discourse modeling (Liddy 1991) and in the phi-
losophy of science (Bunge 1967). It is interesting to note that our concerns regarding
the presentation and editing of the information for text summarization are now be-
ing addressed by other researchers as well. Jing and McKeown (2000) and Jing (2000)
propose a cut-and-paste strategy as a computational process of automatic abstracting
and a sentence reduction strategy to produce concise sentences. They have identified
six ?editing? operations in human abstracting that are a subset of the transformation
found in our study. Jing and McKeown?s work on sentence reduction will be dis-
cussed in section 6. Knight and Marcu (2000) propose a noisy-channel model and a
decision-based model for sentence reduction also aiming at conciseness.
3. Selective Analysis and Its Implementation
Selective analysis is a method for text summarization of technical articles whose design
is based on the study of the corpus described in section 2. The method emphasizes
the selection of particular types of information and its elaboration, exploring the is-
sue of dynamic summarization. It is independent of any particular implementation.
Nevertheless, its design was motivated by actual needs for accessing the content of
long documents and the current limitations of natural language processing of domain-
independent texts. Selective analysis is composed of four main steps, which are briefly
motivated here and fully explained in the rest of the section.
? Indicative selection: The function of indicative selection is to identify
potential topics of the document and to instantiate a set of indicative
templates. These templates are instantiated with sentences matching
507
Saggion and Lapalme Generating Summaries with SumUM
specific patterns. A subset of templates is retained based on a matching
process between terms from titles and terms from the indicative
templates. From the selected templates, terms are extracted for further
analysis (i.e., potential topics).
? Informative selection: The information selection process determines the
subset of topics computed by the indicative selection that can be
informatively expanded according to the interest of the reader. This
process considers sentences in which informative markers and
interesting topics co-occur and instantiates a set of informative templates
that elaborate the topics.
? Indicative generation: In indicative generation, the set of templates
detected by the indicative selection are first sorted using a preestablished
conceptual order. Then, the templates are used to generate sentences
according to the style observed in the corpus of professional abstracts
(i.e., verbs in the impersonal and reformulation of some domain
concepts). When possible, information from different templates is
integrated in order to produce a single sentence. A list of topics is also
presented to the reader.
? Informative generation: In informative generation, the reader selects some
of the topics presented as a result of indicative generation, thereby
asking for more information about those topics. Templates instantiated
by the informative selection associated with the selected topics are used
to present additional information to the reader.
Whereas the indicative abstract depends on the structure, content, and to some ex-
tent, on specific types of information generally reported in this kind of summary, the
informative abstract relies on the interests of the reader to determine the topics to
expand.
3.1 Implementing SumUM
The architecture of SumUM is depicted in Figure 3. Our approach to text summariza-
tion is based on a superficial analysis of the source document to extract appropriate
types of information and on the implementation of some text regeneration techniques.
SumUM has been implemented in SICStus Prolog (release 3.7.1) (SICStus 1998) and
Perl (Wall, Christiansen, and Schwartz 1996) running on Sun workstations (5.6) and
Linux machines (RH 6.0). For a complete description of the system and its implemen-
tation, the reader is referred to Saggion (2000).
The sources of information we use for implementing our system are a POS tag-
ger (Foster 1991); linguistic and conceptual patterns specified by regular expressions
combining POS tags, our syntactic categories, domain concepts, and words; and a
conceptual dictionary that implements our conceptual model (241 domain verbs, 163
domain nouns, and 129 adjectives); see Table 5.
3.1.1 Preprocessing and Interpretation. The input article (plain ASCII text in English
without markup) is segmented in main units (title, author information, main sections
and references) using typographic information (i.e., nonblank lines ending with a char-
acter different from punctuation surrounded by blank lines) and some keywords like
?Introduction? and ?References.? Each unit is passed through the statistical tagger
(based on bigrams). A scanning process reads each element of the tagged files and
508
Computational Linguistics Volume 28, Number 4
DATABASE
INDICATIVE
INFORMATIVE DATABASE
INFORMATIVE ABSTRACT
TOPICS
INDICATIVE
INDICATIVE ABSTRACT
SELECTED TOPICS
GENERATION 
GENERATION
USER
INDICATIVE
SELECTION 
INFORMATIVE
INFORMATIVE
SELECTION
RAW TEXT
PREPROCESSING
INTERPRETATION
POTENTIAL TOPICS INDICATIVE CONTENT
TEXT REPRESENTATIONCONCEPTUAL INDEX TOPICAL STRUCTURETERM TREE
ACRONYM
INFORMATION
CONCEPTUAL DICTIONARY
Figure 3
SumUM architecture.
transforms sequences of tagged words into lists of elements, each element being a
list of attribute-value pairs. For instance, the word systems, which is a common noun
is represented with the following attributes (cat,?NomC?), (Nbr,plur), (canon,system)
in addition to the original word. The frequency of each noun (proper or common) is
also computed. SumUM gradually determines the paragraph structure of the docu-
ment, relying on end of paragraph markers. Sentences are interpreted using finite-state
transducers we developed (implementing 334 linguistic and domain-specific patterns)
and the conceptual dictionary. The interpretation process produces a partial represen-
tation that consists of the sentence position (section and sentence numbers) and a list
of syntactic constituents annotated with conceptual information. As title and section
headers are recognized by position (i.e., sentence number 0 of the section), only noun
group identification is carried out in those components. Each sentence constituent is
represented by a list of attribute-value pairs. The parse of each element is as follows:
? Noun group parsing. We identify only nonrecursive, base noun groups.
The parse of a noun group contains information about the original
string, the canonical or citation form, syntactic features, the semantics
509
Saggion and Lapalme Generating Summaries with SumUM
Table 5
Overview of the conceptual dictionary.
Concept/Relation Lexical Item
make known cover, describe, examine, explore, present, report, overview, outline, . . .
create create, construct, ideate, develop, design, implement, produce, project,
. . .
study investigate, compare, analyze, measure, study, estimate, contrast, . . .
interest address, interest, concern, matter, worry, . . .
infer demonstrate, infer, deduce, show, conclude, draw, indicate, . . .
identify entity include, classify, call, contain, categorize, divide, . . .
paper paper, article, report, . . .
paper component section, subsection, appendix, . . .
structural figure, table, picture, graphic, . . .
problem complexity, intricacy, problem, difficulty, lack, . . .
goal goal, objective, . . .
result finding, result, . . .
important important, relevant, outstanding, . . .
necessary needed, necessary, indispensable, mandatory, vital, . . .
novelty innovative, new, novel, original, . . .
(i.e., the head of the group in citation form), adjectives, and information
referring to the conceptual model that is optional.
? Verb group parsing. The parse of a verb group contains information about
the original string, the semantics (i.e., the head of the group in citation
form), the syntactic features, information about adverbs, and the
conceptual information that is optional.
? Adjectives and adverbials. The parse of adjectival and adverbial groups
contains the original string, the citation form, and the optional
information from the conceptual model.
? Other. The rest of the elements (i.e., conjunctions, prepositions, etc.) are
left unanalyzed.
In order to assess the accuracy of the parsing process, we manually extracted base
noun groups and base verb groups from a set of 42 abstracts found on the INSPEC
(2000) service (about 5,000 words). Then, we parsed the abstracts and automatically
extracted noun groups and verb groups with our finite-state machinery and computed
recall and precision measures. Recall measures the ratio of the number of correct
syntactic constructions identified by the algorithm to the number of correct syntactic
constructions. Precision is the ratio of the number of correct syntactic constructions
identified by the algorithm to the total number of constructions identified by the
algorithm. We found the parser to perform at 86% recall and 86% precision for noun
groups and 85% recall and 76% precision for verb groups.
Term extraction. Terms are constructed from the citation form of noun groups.
They are extracted from sentences and stored along with their semantics and position
in the term tree, an AVL tree structure for efficient access from the SICStus Prolog
association lists package. As each term is extracted from a sentence, its frequency is
updated. We also build a conceptual index that specifies the types of information of
each sentence using the concepts and relations identified before. Finally, terms and
510
Computational Linguistics Volume 28, Number 4
Table 6
Specification of indicative templates for the topic of the document and the topic of a section.
Type: topic
Id: integer identifier
Predicate: instance of make known
Where: instance of {research paper, study, work, research}
Who: instance of {research paper, author, study, work, research}
What: parsed sentence fragment
Position: section and sentence id
Topic candidates: list of terms from the What filler
Weight: number
Type: sec desc
Id: integer identifier
Predicate: instance of make known
Section: instance of section(Id)
Argument: parsed sentence fragment
Position: section and sentence id
Topic candidates: list of terms from the Argument filler
Weight: number
words are extracted from titles (identified as those sentences with numeral 0 in the
representation) and stored in a list, the topical structure, and acronyms and their
expansions are identified and recorded.
3.1.2 Indicative Selection. Simple templates are used to represent the types of informa-
tion. We have implemented 21 indicative templates in this version of SumUM. Table 6
presents two of these indicative templates and their slots. The slot Topic candidates is
filled with terms and acronym expansions. Term relevance is the total frequency of all
nominal components of the term divided by the total number of nominal components.
It is computed using the following formula:
relevance(Term) =
?
{N?Term? noun(N)} noun frequency(N)
|N : N ? Term ? noun(N)|
where noun(N) is true if N is a noun, noun frequency(N) is a function computed during
preprocessing and interpretation that gives the word count for noun N, and the nota-
tion |S| stands for the number of elements in the set S. As complex terms have lower
distribution than single terms, this formula gives us an estimate of the distribution of
the term and its components in the document. In doing so, a low-frequency term like
robot architecture is assigned a high degree of relevance because chances are that robot
and architecture occur frequently on their own. Other techniques exist for boosting the
score of longer phrases, such as adjusting the score of the phrase by a fixed factor that
depends on the length of the phrase (Turney 1999). The Weight slot is filled in with
the sum of the relevance of the terms on the Topic candidates slot.
For determining the content of the indicative abstract, SumUM considers only
sentences that have been identified as carrying indicative information; excludes sen-
tences containing problematic anaphoric references (?the first. . . ,? ?the previous. . . ,?
?that. . . ,? quantifiers in sentence initial, etc.), those that are not domain concepts (e.g.,
?These results,? ?The first section,? etc.), and some connectives (?although,? ?how-
ever,? etc.); and checks whether the sentence matches an indicative pattern. Indicative
511
Saggion and Lapalme Generating Summaries with SumUM
Table 7
Indicative pattern specification and sentence fragments matching the patterns (in parentheses).
Signaling structural SKIP1 + GN + Prep + GN + show graphical material + Prep + structural (In our
case, the architecture of the self-tuner is shown in Figure 3 Auto-tuning. . . )
Topic SKIP1 + research paper + SKIP2 + author + make known + ARGUMENT (In this
article, we overview the main techniques used in order. . . )
Author?s Goal SKIP + conceptual goal + SKIP + define + GOAL (Our goals within the HMS project
are to develop a holonic architecture for. . . )
Signaling concept SKIP + development + Prep + GN (Implementation of industrial robots)
Section Topic paper component + make known + ARGUMENT + ConC + paper component
(Section 2 describes HuDL in greater detail and Section 3. . . )
Problem/Solution SKIP + solution (dr) + problem (The proposed methodology overcomes the problems
caused by. . . )
Introduce Entity GN + define + SKIP (Rapid Prototyping (RP) is a technique. . . )
patterns contain variables, syntactic constructions, domain concepts, and relations.
One hundred seventy-four indicative patterns have been implemented; some of them
are shown in Table 7.
For each matched pattern, SumUM verifies some restrictions, such as verb tenses
and voice, extracts information from pattern variables, and instantiates a template of
the appropriate type. All the instantiated templates constitute the indicative database
(IDB). SumUM matches each element of the topical structure with the terms of the
Topic candidate slots of templates in the IDB. Two terms Term1 and Term2 match if
Term1 is a substring of Term2 or if Term2 is a substring of Term1 (e.g., robotic fruit harvester
matches harvester).
Then, SumUM selects the template with the greatest Weight. In case of conflict,
types are selected following the precedence given in Table 8. This order gives prefer-
ence to explicit topical information more usually found in indicative abstracts. Where
there is conflict, the Position and the Id slots are used to decide: If two topic tem-
plates have the same Weight, the template with position closer to the beginning of the
document is selected, and if they are still equal, the template with lower Id is used.
SumUM prioritizes topical information by selecting the topical template with greatest
weight. The selected templates constitute the indicative content (IC), and the terms
and words appearing in the Topic candidate slots and their expansions constitute the
potential topics (PTs) of the document. Expansions are obtained by looking for terms
in the term tree sharing the semantics of any term in the IC.
3.1.3 Informative Selection. For each potential topic PT and sentence in which it
appears, SumUM checks whether the sentence contains an informative marker and
matches a dynamic informative pattern. Dynamic patterns include a TOPIC slot in-
stantiated with the PT before trying a match. They also include concepts, relations,
and linguistic information. Eighty-seven informative patterns have been implemented,
Table 8
Precedence for content selection.
Topic of Document > Topic of Section > Topic Description > Possible Topic > Author
Study > Author Development > Author Interest > Conceptual Goal, Research Goal
> Conceptual Focus, Focus > Entity Introduction > Entity Identification > Signaling
Structural, Signaling Concepts > Other Indicative Types
512
Computational Linguistics Volume 28, Number 4
Table 9
Informative pattern specification and sentence fragments matching the patterns (in
parentheses).
Definition SKIP + TOPIC + define + GN (The RIMHO walking robot is a prototype developed
with the aim of. . . )
Description SKIP + TOPIC + describe (The hardware of the MMI consists of a main pendant (MP),
an operator pendant. . . )
Use SKIP + use + TOPIC (To realize the control using an industrial robot, such as. . . )
Advantage SKIP + advantage + Prep + TOPIC (The biggest advantage of SWERS is the easier and
faster. . . )
Effectiveness SKIP + TOPIC + define + effective (The system is effective in the task of. . . )
some of which are presented in Table 9. If a sentence satisfies an informative pattern,
the PT is considered a topic of the document, and an informative template is instan-
tiated with the sentence. The informative templates contain a Content slot to record
the information from the sentence, a Topic slot to record the topic, and a Position
slot to record positional information. Examples are presented in Tables 10 and 11. The
templates obtained by this process constitute the Informative Data Base (InfoDB), and
the topics are the terms appearing in the slot Topic of the templates in the InfoDB.2
3.1.4 Generation. The process of generation consists of the arrangement of the infor-
mation in a preestablished conceptual order, the merging of some types of information,
and the reformulation of the information in one text paragraph. The IC is sorted us-
ing positional information and the order presented in Table 12, which is typical of
technical articles.
SumUM merges groups of up to three templates of type Topic of Document to
produce more complex sentences (Merge transformation). The same is done for tem-
plates of type Topic of Section, Signaling Concept, and Signaling Structural. The
template Signaling Concept contains information about concepts found on section
headings; SumUM selects an appropriate verb to introduce that information in the
abstract (Verb Selection). In this way, for example, given the section heading ?Ex-
perimental Results,? SumUM is able to produce the sentence ?Presents experimental
results.?
The sorted templates constitute the text plan. Each element in the text plan is used
to produce a sentence the structure of which depends on the template. The schema of
presentation of a text plan composed of n(? 1) templates Tmpli is as follows:
Text =
n
?
i=1
[Tmpli ? ?.?].
The notation A? means the string produced by the generation of A, ? denotes con-
catenation, and
?n
i=1 Ai stands for the concatenation of all Ai. We assume that all
the parameters necessary for the generation are available (i.e., voice, tense, number,
position, etc.).
The schema of presentation of a template Tmpl of type Topic of the Document is:3
Tmpl = Tmpl.Predicate ? Tmpl.What
2 TOPIC = {Term : ?Template ? InfoDB ? Template.Topic = Term}.
3 The notation Tmpl.Slot denotes the content of slot Slot of template Tmpl.
513
Saggion and Lapalme Generating Summaries with SumUM
Table 10
Specification of the templates for the description and definition of a topic.
Type: description
Id: integer identifier
Topic: term
Predicate: instance of describe (i.e., X is composed of Y)
Content: parsed sentence fragment
Position: section and sentence id
Type: definition
Id: integer identifier
Topic: term
Predicate: instance of define (i.e., X is a Y)
Content: parsed sentence fragment
Position: section and sentence id
Table 11
Definition template instantiated with sentence ?REVERSA is a dual viewpoint noncontact
laser scanner which comes complete with scanning software and data manipulation tools.?
Type: definition
Id: 41
Topic: REVERSA
Predicate: be, . . .
Content: REVERSA is a dual viewpoint noncontact laser scanner which. . .
Position: Sentence 1 from Section 2
The predicate is generated in the present tense of the third-person singular (Syntactic
Verb Transformation). So sentences like ?X will be presented? or ?X have been pre-
sented? or ?We have presented here X,? which are usually found in source documents,
will be avoided because they are awkward in an abstract. Arguments are generated by
a procedure that expands/abbreviates acronyms (Acronym Expansion and Abbrevi-
ation), presents author-related entities in the impersonal form (concept reexpression),
uses fixed expressions in order to refer to the authors and the research paper, and
produces correct case and punctuation. Examples of sentences generated by the sys-
tem have been presented in Saggion and Lapalme (2000a). In this way we implement
some of the transformations studied in section 2.3. The schema of presentation of the
Table 12
Conceptual order for content expression.
Problem Solution, Problem Identification, Need and Situation in positional order
Topic of Document sorted in descending order of Weight
Possible Topic sorted in descending order of Weight
Topic Description, Study, Interest, Development, Entity Introduction, Research Goal,
Conceptual Goal, Conceptual Focus and Focus in positional order
Method and Experiment in positional order
Results, Inference, Knowledge and Summarization in positional order
Entity Identification in positional order
Topic of Section in section order
Signaling Structural and Signaling Concepts in positional order
514
Computational Linguistics Volume 28, Number 4
Topic of Section is
Tmpl = Tmpl.Predicate ? Tmpl.Argument.
The schema of generation of a merged template Tmpl is
Tmpl =
(
n?1
?
i=1
[Tmpl.Templatesi ? ?;?]
)
? ?and also? ? Tmpl.Templatesn,
where Tmpl.Templatesi is the ith template in the merge. Note that if n adjacent templates
in the merge share the same predicate, then only one verb is generated, and the
arguments are presented as a conjunction (i.e., ?Presents X and Y.? instead of ?Presents
X and presents Y.?). This is specified with the following schema:
Tmpl = Predicate ? Tmpl1.Arg ?
(
n?1
?
i=2
[Tmpli.Arg ? ?;?]
)
? ?and? ? Tmpln.Arg,
where Predicate is the predicate common to the merged templates.
The indicative abstract is presented along with the list of topics that are obtained
from the list Topics. SumUM presents in alphabetical order the first superficial occur-
rence of the term in the source document (this information is found in the term tree).
For the informative abstract, the system retrieves from the InfoDB those templates
matching the topics selected by the user (using the slot Topic for that purpose) and
presents the information on the Content slots in the order of the original text (using
the Position for that purpose).
4. Limitations of the Approach
Our approach is based on the empirical examination of abstracts published by sec-
ond services and on assumptions about technical text organization (Paice 1991; Bhatia
1993; Jordan 1993, 1996). In our first study, we examined 100 abstracts and source
documents in order to deduce a conceptual and linguistic model for the task of sum-
marization of technical articles. Then we expanded the corpus with 100 more items
in order to validate the model. We believe that the concepts, relations, and types of
information identified account for interesting phenomena appearing in the corpus and
constitute a sound basis for text summarization. The conceptual information has not
been formalized in ontological form, opening an avenue for future developments. All
the knowledge of the system (syntactic and conceptual) was manually acquired dur-
ing specification, implementation, and testing. The coverage and completeness of the
model have not been assessed in this work and will be the subject of future studies.
Nevertheless SumUM has been tested in different technical domains.
The implementation of our method relies on noun and verb group identification,
conceptual tagging, pattern matching, and template instantiation we have developed
for the purpose of this research. The interpreter relies on the output produced by a shal-
low text segmenter and on a statistical part-of-speech tagger. Our prototype analyzes
sentences for the specific purpose of text summarization and implements some pat-
terns of generation observed in the corpus, including the reformulation of verb groups
and noun groups, sentence combination or fusion, and conceptual deletion, among
others. We have not addressed here the question of text understanding: SumUM is
able to produce text summaries, but it is not able to demonstrate intelligent behavior
515
Saggion and Lapalme Generating Summaries with SumUM
(answering questions, paraphrasing, anaphora resolution, etc.). Concerning the prob-
lem of text coherence, we have not properly addressed the problem of identification of
anaphoric expressions in technical documents: SumUM excludes from the content of
the indicative abstract sentences containing expressions considered problematic. The
problem of anaphoric expressions in technical articles has been extensively addressed
in research work carried out under the British Library Automatic Abstracting Project
(BLAB) (Johnson et al 1993; Paice et al 1994). Although some of the exclusion rules
implemented in the BLAB project are considered in SumUM (exclusion of sentences
with quantifier subject, sentences with demonstratives, some initial connectives, and
pronouns), our approach lacks coverage of some important cases dealt with in the
BLAB rules, such as the inclusion of sentences because of dangling anaphora.
This implementation of SumUM ignores some aspects of the structure of textlike
lists and enumerations, and most of the process overlooks the information about para-
graph structure. Nevertheless, in future improvements of SumUM, these will be taken
into consideration to produce better results.
5. Evaluating the Summaries
Abstracts are texts used in tasks such as assessing the content of a source document
and deciding if it is worth reading. If text summarization systems are designed to
fulfill the requirements of those tasks, the quality of the generated texts has to be eval-
uated according to their intended function. The quality of human-produced abstracts
has been examined in the literature (Grant 1992; Kaplan et al 1994; Gibson 1993),
using linguistic criteria such as cohesion and coherence, thematic structure, sentence
structure, and lexical density; in automatic text summarization, however, such detailed
analysis is only just emerging. Content evaluation assesses whether an automatic sys-
tem is able to identify the intended ?topics? of the source document. Text quality
evaluation assesses the readability, grammar, and coherence of a summary. The eval-
uations can be made in intrinsic or extrinsic fashions as defined by Sparck Jones and
Galliers (1995).
An intrinsic evaluation measures the quality of the summary itself by compar-
ing the summary with the source document, by measuring how many ?main? ideas
of the source document are covered by the abstract, or by comparing the content of
the automatic summary with an ideal abstract (gold standard) produced by a human
(Mariani 1995). An extrinsic evaluation measures how helpful a summary is in the
completion of a given task. For example, given a document that contains the answers
to some predefined questions, readers are asked to answer those questions using the
document?s abstract. If the reader correctly answers the questions, the abstract is con-
sidered of good quality for the given question-answering task. Variables measured can
be the number of correct answers and the time to complete the task. Recent experi-
ments (Jing et al 1998) have shown how different parameters such as the length of
the abstract can affect the outcome of the evaluation.
5.1 Evaluation of Indicative Content and Text Quality
Our objective in the evaluation of indicative content is to see whether the abstracts
produced by our method convey the essential content of the source documents in
order to help readers complete a categorization task. In the evaluation of text quality,
we want to determine whether the abstracts produced by our method are acceptable
according to a number of acceptability criteria.
516
Computational Linguistics Volume 28, Number 4
5.1.1 Design. In both evaluations we are interested in comparing our summaries with
summaries produced using other methodologies, including human-written ones. In or-
der to evaluate the content, we presented evaluators with abstracts and five descriptors
(lists of keywords) for each abstract. The evaluators had to find the correct descrip-
tor for the abstract. One of the descriptors was the correct descriptor of the abstract
and the others were descriptors from the same domain, obtained from the journals
in which the source documents were published. In order to evaluate text quality, we
asked the evaluators to provide an acceptability score between 0?5 for the abstract
(0 for unacceptable and 5 for acceptable) based on the following criteria taken from
Rowley (1982): good spelling and grammar, clear indication of the topic of the source
document, conciseness, readability and understandability, and whether acronyms are
presented along with their expansions. We told the evaluators that we would consider
the abstracts with scores above 2.5 acceptable; with this information, they could use
scores below or above that borderline to enforce acceptability. The design of this ex-
periment was validated by three IS specialists. The experiment was run three times
with different data each time and with a different set of summarizers (human or auto-
matic). When we first designed this experiment, only one text summarization system
was available to us, so we performed the experiment comparing automatic abstracts
produced by two summarizers and abstracts published with the source documents.
Later on, we found two other summarizers, and we decided to repeat the experiment
only considering three automatic systems.
Our evaluation mirrors the TIPSTER SUMMAC categorization task (Firmin and
Chrzanowski 1999; Mani et al 1998) in which given a generic summary (or a full
document), the human participant chooses a single category (out of five categories)
to which the document is relevant. The evaluation seeks to determine whether the
summary is effective in capturing whatever information in the document is needed
to correctly categorize the document. In the TIPSTER SUMMAC evaluation, 10 Text
Retrieval Conference (TREC) topics and 100 documents per topic were used, and
16 systems participated. The results for TREC indicate that there are no significant
differences among the systems for the categorization task and that the performance
using the full document is not much better.
5.1.2 Subjects and Materials. All our evaluators were IS students/staff from Uni-
versite? de Montre?al, McGill University, and John Abbott College. They were chosen
because they have knowledge about what constitutes a good indicative abstract. We
used the Latin square experimental design, whereby forms included n abstracts from
n different documents, where n depends on the number of subjects (thus an evaluator
never compared different summaries of the same document). Each abstract was printed
on a different page including the five descriptors, a field to be completed with the qual-
ity score associated with the abstract, and a field to be filled with comments about the
abstract. In order to produce the evaluation forms, we used source documents (all tech-
nical articles) from the journal Industrial Robots, found in the Emerald Electronic Library
?http://www.emerald-library.com?. In addition to the abstracts published with source
documents, we produced automatic abstracts using the following systems: SumUM,
Microsoft?97 Autosummarize, Extractor, and n-STEIN. Microsoft?97 Autosummarize is
distributed with Word?97. Extractor (Turney 1999) is a system that takes a text file
as input (plain ASCII text, HTML, or e-mail) and generates a list of keywords and
keyphrases as output. On average, it generates the number of phrases requested by
the user, but the actual number for any given document may be slightly below or above
the requested number, depending mainly on the length of the input document. Ex-
tractor has 12 parameters relevant for keyphrase extraction that are tuned by a genetic
517
Saggion and Lapalme Generating Summaries with SumUM
algorithm to maximize performance on training data. We used Extractor 5.1, which
is distributed for demonstration (downloaded from ?http://extractor.iit.nrc.ca/?). n-
STEIN is a commercial system that was available for demonstration purposes at the
time we were conducting our research (n-STEIN 2000) (January 2000). The system
is based on a combination of statistical and linguistic processing. Unfortunately no
technical details of the system are given.
5.1.3 Procedure. Each abstract was evaluated by three different evaluators, who were
not aware of the method used to produce the abstracts. In order to measure the out-
come of the categorization task, we considered the abstract to have helped in catego-
rizing the source document if two or more evaluators were able to chose the correct
descriptor for the abstract. In order to measure the quality of the abstract, we computed
the average quality using the scores given by the evaluators.
5.1.4 Results and Discussion. In Table 13 we present the average information for
three runs of this experiment. ?Success? refers to the percentage of cases in which
subjects identified the correct descriptor. ?Quality? refers to subjects? summary quality
score. Note that because of this particular design, we cannot compare numbers across
experiments, we can only discuss results for each experiment.
Overall, for each experiment no significant differences were observed between the
different automatic systems in the categorization task. All automatic methods per-
formed similarly, though we believe that documents and descriptors of narrower do-
mains are needed in order to correctly assess the effectiveness of each summarization
method. Unfortunately, the construction of such resources goes beyond our present
research and will be addressed in future work.
The figures for text acceptability indicate that abstracts produced by Autosum-
marize are below the acceptability level of 2.5. The abstracts produced by SumUM,
Extractor, and n-STEIN are above the acceptability level of 2.5, and the human abstracts
are highly acceptable. In the first experiment, an analysis of the variance (ANOVA) for
text quality (Oakes 1998) showed differences among the three methods at p ? 0.005
(observed F(2, 27) = 9.66). Tukey?s multiple-comparison test (Byrkit 1987) shows statis-
Table 13
Results of human judgment in a categorization task and assessment about text quality.
Experiment Summarization Methods
First Autosummarize SumUM Human
15 evaluators Success Quality Success Quality Success Quality
10 documents 80% 1.46 80% 3.23 100% 4.25
Second Autosummarize SumUM Human
18 evaluators Success Quality Success Quality Success Quality
12 documents 70% 1.98 70% 3.15 80% 4.04
Third n-STEIN SumUM Extractor
20 evaluators Success Quality Success Quality Success Quality
15 documents 67% 2.76 80% 3.13 73% 3.47
518
Computational Linguistics Volume 28, Number 4
tical differences in text quality at p ? 0.01 for the two automatic systems (SumUM and
Autosummarize), but no conclusion can be drawn about differences between the ab-
stracts produced by those systems and the author abstract at levels 0.01 or 0.05. In
the second experiment, the ANOVA showed differences at p ? 0.01 between the
three methods (observed F(2, 33) = 10.35). Tukey?s test shows statistical differences
at p ? 0.01 between the two automatic systems (SumUM and Autosummarize) and
differences with the author abstract at 0.05. In the third experiment, the ANOVA for
text quality did not allow us to draw any conclusions about differences in text quality
(F(2, 42) = 0.83).
5.2 Evaluation of Content in a Coselection Experiment
Our objective in the evaluation of content in a coselection experiment is to measure
coselection between sentences selected by our system and a set of ?correct? extracted
sentences. This method of evaluation has already been used in other summarization
evaluations such as Edmundson (1969) and Marcu (1997). The idea is that if we find
a high degree of overlap between the sentences selected by an automatic method
and the sentences selected by a human, the method can be regarded as effective.
Nevertheless, this method of evaluation has been criticized not only because of the
low rate of agreement between human subjects in this task (Jing et al 1998), but also
because there is no unique ideal or target abstract for a given document. Instead,
there is a set of main ideas that a good abstract should contain (Johnson 1995). In our
coselection experiment, we were also interested in comparing our system with other
summarization technologies.
5.2.1 Materials.
Data used. We used 10 technical articles from two different sources: 5 from the
journal Rapid Prototyping and 5 from the journal Internet Research. The documents were
downloaded from the Emerald Electronic Library. The abstracts and lists of keywords
provided with the documents were deleted before the documents were used in the
evaluation.
Reference extracts. We used 30 automatic abstracts (three for each article) and nine
assessors with a background in dealing with technical articles, on whom we relied to
obtain an assessment of important sentences in the source documents. Eight assessors
read two articles each, and one read four articles, because no other participants were
available when the experiment was conducted. The assessor of each article chose a
number of important sentences from that article (up to a maximum of Ni, the number
of sentences chosen by the summarization methods). Each article was read by two
different assessors; we thus had two sets of sentences for each article. We call these
sets Si,j (i ? [1 . . . 10]? j ? [1 . . . 2]). Most of the assessors found the task quite complex.
Agreement between human assessors was only 37%.
Automatic extracts. We considered three automatic systems in this evaluation:
SumUM, Autosummarize, and Extractor. We produced three abstracts for each doc-
ument. First we produced an abstract using SumUM. We counted the number of
sentences selected by SumUM in order to produce the indicative-informative abstract
(we verified that the number of sentences selected by the system represented between
10% and 25% of source documents). Then, we produced two other automatic abstracts,
one using Autosummarize and another using Extractor. We specified to each system
that it should select the same number of sentences as SumUM selected.
519
Saggion and Lapalme Generating Summaries with SumUM
5.2.2 Procedure. We measure coselection between sentences produced by each method
and the sentences selected by the assessors, computing recall, precision, and F-score
as in Firmin and Chrzanowski (1999). In order to obtain a clear picture, we borrowed
the scoring methodology proposed by Salton et al (1997), additionally considering the
following situations:
? Union scenario: For each document we considered the union of the
sentences selected by the two assessors (Si,1 ? Si,2) and computed recall,
precision, and F-score for each method.
? Intersection scenario: For each document we considered the intersection of
the sentences selected by the two assessors (Si,1 ? Si,2) and computed
recall, precision, and F-score for each method.
? Optimistic scenario: For each document and method we considered the
case in which the method performed the best (highest F-score) and
computed recall, precision, and F-score.
? Pessimistic scenario: For each document and method we considered the
case in which the method performed the worst (lowest F-score) and
computed recall, precision, and F-score.
5.2.3 Results and Discussion. For each scenario we present the average information in
Table 14 (Saggion and Lapalme [2000c] presented detailed results of this experiment).
For the scenario in which we consider the 20 human abstracts, SumUM obtained the
best F-score in 60% of the cases, Extractor in 25% of the cases, and Autosummarize in
15% of the cases. If we assume that the sentences selected by the human assessors rep-
resent the most important or interesting information in the documents, then we can
conclude that on average, SumUM performed better than the other two summarization
technologies, even if these results are not exceptional in individual cases. An ANOVA
showed statistical differences in the F-score measure at p ? 0.01 between the differ-
ent automatic abstracts (observed F(2, 57) = 5.28). Tukey?s tests showed differences
between SumUM and the two other automatic methods at p ? 0.01.
Here, we have compared three different methods of producing abstracts that are
domain independent. Nevertheless, whereas Autosummarize and Extractor are truly
text independent, SumUM is genre dependent: It was designed for the technical arti-
cle and takes advantage of this fact in order to produce abstracts. We think that this
Table 14
Coselection between sentences selected by human assessors and sentences selected by three
automatic summarization methods in recall (R), precision (P) and F-score (F).
SumUM Autosummarize Extractor
R P F R P F R P F
Average .23 .20 .21 .14 .11 .12 .12 .18 .14
Union .21 .31 .25 .16 .19 .17 .11 .26 .15
Intersection .28 .09 .14 .13 .04 .06 .08 .04 .06
Optimistic .26 .23 .25 .16 .14 .15 .14 .25 .18
Pessimistic .19 .17 .18 .11 .08 .09 .08 .11 .09
520
Computational Linguistics Volume 28, Number 4
is the reason for the better performance of SumUM in this evaluation. The results
of this experiment are encouraging considering the limited capacities of the actual
implementation. We expect to improve the results in future versions of SumUM. Ad-
ditional evaluations of SumUM using sentence acceptability criteria and content-based
measures of indicativeness have been presented in Saggion and Lapalme (2000b) and
Saggion (2000).
6. Related Work on Summarization
As a human activity, the production of summaries is directly associated with the
processes of language understanding and production: A source text is read and un-
derstood to recognize its content, which is then compiled in a concise text. In order to
explain this process, several theories have been proposed and tested in text linguistics,
cognitive science, and artificial intelligence, including macro structures (Kintsch and
van Dijk 1975; van Dijk 1977), history grammars (Rumelhart 1975), plot units (Lehn-
ert 1981), and concept/coherence relations (Alterman and Bookman 1990). Computers
have been producing summaries since the original work of Luhn (1958). Since then
several methods and theories have been applied, including the use of term frequency
? inverse document frequency (TF ? IDF) measures, sentence position, and cue and ti-
tle words (Luhn 1958; Edmundson 1969; Kupiec, Pedersen, and Chen 1995; Brandow,
Mitze, and Rau 1995); partial understanding using conceptual structures (DeJong 1982;
Tait 1982); bottom-up understanding, top-down parsing, and automatic linguistic ac-
quisition (Rau, Jacobs, and Zernik 1989); recognition of thematic text structures (Hahn
1990); cohesive properties of texts (Benbrahim and Ahmad 1995; Barzilay and Elhadad
1997); and rhetorical structure theory (Ono, Sumita, and Miike 1994; Marcu 1997).
In the context of the scientific article, Rino and Scott (1996) have addressed the
problem of coherent selection for text summarization, but they depend on the avail-
ability of a complex meaning representation, which in practice is difficult to obtain
from the raw text. Instead, superficial analysis in scientific text summarization using
lexical information was applied by Lehmam (1997) for the French language. Liddy
(1991) produced one of the most complete descriptions of conceptual information for
abstracts of empirical research. In our work, we concentrated instead on conceptual in-
formation that is common across domains. Liddy?s model includes three typical levels
of information. The most representative level, called the prototypical structure, in-
cludes the information categories subjects, purpose, conclusions, methods, references,
and hypotheses. The other two levels are the typical structure and the elaborated
structure, which include information less frequently found in abstracts of empirical
research. To our knowledge Liddy?s model has never been implemented; nevertheless,
it could be used as a starting point for improving our flat-domain model. Relevant
work in rhetorical classification for scientific articles, which is the first step toward
the production of scientific abstracts, is due to Teufel and Moens (1998), who used
statistical approaches borrowed from Kupiec, Pedersen, and Chen (1995).
Our method is close to concept-based abstracting (CBA) (Jones and Paice 1992;
Paice and Jones 1993) but differs from this approach in several aspects. CBA is used to
produce abstracts of technical articles in specific domains, for example, in the domain
of agriculture. Semantic roles such as species, cultivar, high-level property, and low-
level property are first identified by the manual analysis of a corpus, and then patterns
are specified that account for stylistic regularities of expression of the semantic roles
in texts. These patterns are used in an information extraction process that instantiates
the semantic roles. Selective analysis, although genre dependent, was developed as
domain independent and tested in different technical domains without the need to
521
Saggion and Lapalme Generating Summaries with SumUM
adapt the conceptual model, the patterns, or the conceptual dictionary. In order to
adapt CBA to new domains, the semantic roles representing the ?key? information in
the new domain need to be identified, and new templates and patterns need to be con-
structed (Oakes and Paice 2001). Although such adaptation is generally done manually,
recent work has shown how to export CBA to new domains automatically (Oakes and
Paice 1999). CBA uses a fixed canned template for summary generation, whereas our
method allows greater stylistic variability because the main ?content? of the summary
generated is expressed in the words of the authors of the paper. Selective analysis
is used to produce indicative-informative abstracts, whereas CBA is mainly used to
produce indicative abstracts, though some informative content is included in the form
of extracted sentences containing results and conclusions (Paice and Oakes 1999). Our
method can be seen as an extension of CBA that allows for domain independence and
informativeness. We believe that the indicative patterns we have designed are genre
dependent, whereas the informative patterns are general and can be used in any do-
main. Our implementation of patterns for information extraction is similar to Black?s
(1990) implementation of Paice?s (1981) indicative phrases method, but whereas Black
scores sentences based on indicative phrases contained in the sentences, our method
scores the information from the sentences based on term distribution.
Our work in sentence reformulation is different from cut-and-paste summariza-
tion (Jing and McKeown 2000) in many ways. Jing (2000) proposes a novel algorithm
for sentence reduction that takes into account different sources of information to de-
cide whether or not to remove a particular component from a sentence to be included
in a summary. The decision is made based on (1) the relation of the component to
its context, (2) the probability of deleting such a component (estimated from a cor-
pus of reduced sentences), and (3) linguistic knowledge about the essentiality of the
component in the syntactic structure. Sentence reduction is concerned only with the
removal of sentence components, so it cannot explain transformations observed in our
corpus and in summarization in general, such as the reexpression of domain concepts
and verbs. We achieve sentence reduction through a process of information extraction
that extracts verbs and arguments, sometimes considering only sentence fragments
(for example, initial prepositional phrases, parenthetical expressions, and some adver-
bials are ignored for some templates). The process removes domain concepts, avoids
unnecessary grammatical subjects, and generates coordinate structures, avoiding verb
repetition. Whereas our algorithm is genre dependent, requiring only shallow parsing,
Jing?s algorithm is genre and domain independent and requires full syntactic parsing
and disambiguation and extensive linguistic resources.
Regarding the fusion of information, we have concentrated only on the fusion
of explicit topical information (document topic, section topic, and signaling struc-
tural and conceptual elements). Jing and McKeown (2000) have proposed a rule-based
algorithm for sentence combination, but no results have been reported. Radev and
McKeown (1998) have already addressed the issue of information fusion in the con-
text of multidocument summarization in one specific domain (i.e., terrorism): The
fusion of information is achieved through the implementation of summary operators
that integrate the information of different templates from different documents refer-
ring to the same event. Although those operators are dependent on the specific task
of multidocument summarization, and to some extent on the particular domain they
deal with, it is interesting to observe that some of Radev and McKeown?s ideas could
be applied in order to improve our texts. For example, their ?refinement? operator
could be used to improve the descriptions of the entities of the indicative abstract.
The entities from the indicative abstract could be refined with definitions or descrip-
tions from the InfoDB in order to obtain a better and more compact text. The idea of
522
Computational Linguistics Volume 28, Number 4
elaborating topics has also been addressed by Mani, Gates, and Bloedorn (1999). They
have proposed a number of rules for summary revision aiming at conciseness; their
elimination rule discards parenthetical and initial prepositional phrases, as does our
approach. Their aggregation operation combines two constituents on the basis of ref-
erential identity and so is more general than our combination of topical information.
Although their approach is domain independent, it requires full syntactic analysis and
coreference resolution.
7. Conclusions
SumUM has been fully implemented to take a raw text as input and produce a sum-
mary. This involves the following successive steps: text segmentation, part-of-speech
tagging, partial syntactic and semantic analysis, sentence classification, template in-
stantiation, content selection, text regeneration, and topic elaboration. Our research
was based on the intensive study of manual alignments between sentences of pro-
fessional abstracts and elements of source documents and on the exploration of the
essential differences between indicative and informative abstracts.
Although our method was deeply influenced by the results of our corpus study,
it nevertheless has many points in common with recent theoretical and program-
matic directions in automatic text summarization. For example, Sparck Jones (1997)
argues in favor of a kind of ?indicative, skeletal summary? and the need to explore dy-
namic, context-sensitive summarization in interactive situations in which the summary
changes according to the user needs. Hutchins (1995) advocates indicative summaries,
produced from parts of a document in which the topics are likely to be stated. These
abstracts are well suited for situations in which the actual user is unknown (i.e., a
general reader), since the abstract will provide the reader with good entry points
for retrieving more detailed information. If the users are known, the abstract can be
tailored to their specific profiles; such profiles might specify the reader?s interest in
various types of information, such as conclusions, definitions, methods, or user needs
expressed in a ?query? to an information retrieval system (Tombros, Sanderson, and
Gray 1998). Our method, however, was designed without any particular reader in
mind and with the assumption that a text does have a ?main? topic.
In this article, we have presented an evaluation of automatically generated
indicative-informative abstracts in terms of content and text quality. In the evalua-
tion of the indicative content in a categorization task, no differences were observed
among the different automatic systems. The automatic abstracts generated by SumUM
were considered more acceptable than other systems? abstracts. In the evaluation of
the informative content, SumUM selected sentences that were evaluated as more rele-
vant by human assessors than sentences selected by other summarization technologies;
statistical tests showed significant differences between the automatic methods in that
evaluation. In the future, we plan to address several issues, including the study of
robust automatic text classification techniques, anaphora resolution, and lexical cohe-
sion for improving elaboration of topics as well as the incorporation of local discourse
analysis to improve the coherence of abstracts.
Acknowledgments
We would like specially to thank Eduard
Hovy for his valuable comments and
suggestions, which helped improve and
clarify the present work. We are indebted to
the three anonymous reviewers for their
extensive suggestions, which also helped
improve this work. We are grateful to
Professor Miche`le Hudon from Universite?
de Montre?al for fruitful discussion and to
Professor John E. Leide from McGill
University, to Mme. Gracia Pagola from
523
Saggion and Lapalme Generating Summaries with SumUM
Universite? de Montre?al, and to Christine
Jacobs from John Abbott College for their
help in recruiting assessors for the
experiments. We thank also Elliott
Macklovitch and Diana Maynard, who
helped us improve the quality of our article,
and the members of the Laboratoire de
Recherche Applique?e en Linguistique
Informatique (RALI) for their participation
in our experiments. The first author was
supported by Agence Canadienne de
De?veloppement International (ACDI)
during his Ph.D. research. He also received
support from Fundacio?n Antorchas
(A-13671/1-47), Ministerio de Educacio?n de
la Nacio?n de la Repu?blica Argentina
(Resolucio?n 1041/96) and Departamento de
Computacio?n, Facultad de Ciencias Exactas
y Naturales, Universidad de Buenos Aires,
Argentina.
References
Alterman, Richard and
Lawrence A. Bookman. 1990. Some
computational experiments in
summarization. Discourse Processes,
13:143?174.
American National Standards Institute.
(ANSI). 1979. Writing Abstracts.
Barzilay, Regina and Michael Elhadad. 1997.
Using lexical chains for text
summarization. In Proceedings of the
ACL/EACL?97 Workshop on Intelligent
Scalable Text Summarization, pages 10?17,
Madrid, July.
Benbrahim, Mohamed and Kurshid Ahmad.
1995. Text summarisation: The role of
lexical cohesion analysis. New Review of
Document & Text Management, 1:321?335.
Bernier, Charles L. 1985. Abstracts and
abstracting. In E. D. Dym, editor, Subject
and Information Analysis, volume 47 of
Books in Library and Information Science.
Marcel Dekker, Inc., pages 423?444.
Bhatia, Vijay K. 1993. Analysing Genre:
Language Use in Professional Settings.
Longman.
Black, William J. 1990. Knowledge based
abstracting. Online Review, 14(5):327?340.
Brandow, Ronald, K. Mitze, and Lisa F. Rau.
1995. Automatic condensation of
electronic publications by sentence
selection. Information Processing &
Management, 31(5):675?685.
Bunge, Mario. 1967. Scientific Research I. The
Search for System. Springer-Verlag, New
York.
Byrkit, Donald R. 1987. Statistics Today: A
Comprehensive Introduction.
Benjamin/Cummings.
Cremmins, Eduard T. 1982. The Art of
Abstracting. ISI Press.
DeJong, Gerald. 1982. An overview of the
FRUMP system. In W. G. Lehnert and
M. H. Ringle, editors, Strategies for Natural
Language Processing. Lawrence Erlbaum,
pages 149?176.
Edmundson, H. P. 1969. New methods in
automatic extracting. Journal of the
Association for Computing Machinery,
16(2):264?285.
Educational Resources Information Center
(ERIC). 1980. Processing Manual: Rules and
Guidelines for the Acquisition, Selection, and
Technical Processing of Documents and
Journal Articles by the Various Components of
the ERIC Network. ERIC.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press.
Filman, Roger E. and Sangan Pant. 1998.
Searching the Internet. IEEE Internet
Computing, 2(4):21?23.
Firmin, The?re`se and
Michael J. Chrzanowski. 1999. An
evaluation of automatic text
summarization systems. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
pages 325?336.
Foster, George. 1991. Statistical lexical
disambiguation. Master?s thesis, School of
Computer Science, McGill University,
Montre?al, Que?bec, Canada.
Gibson, Timothy R. 1993. Towards a Discourse
Theory of Abstracts and Abstracting.
Department of English Studies,
University of Nottingham.
Grant, Pamela. 1992. The Integration of Theory
and Practice in the Development of
Summary-Writing Strategies. Ph.D. thesis,
Faculte? des E?tudes Supe?rieures,
Universite? de Montre?al.
Hahn, Udo. 1990. Topic parsing: Accounting
for text macro structures in full-text
analysis. Information Processing &
Management, 26(1):135?170.
Hutchins, John. 1995. Introduction to text
summarization workshop. In
B. Engres-Niggemeyer, J. Hobbs, and
K. Sparck Jones, editors, Summarising Text
for Intelligent Communication, Dagstuhl
Seminar Report 79. IBFI, Schloss
Dagstuhl, Wadern, Germany,
INSPEC. 2000. INSPEC database for physics,
electronics and computing.
http://www.iee.org.uk/publish/inspec/
Jing, Hongyan. 2000. Sentence reduction for
automatic text summarization. In
Proceedings of the Sixth Applied Natural
Language Processing Conference, pages
310?315, Seattle, April 29?May 4.
524
Computational Linguistics Volume 28, Number 4
Jing, Hongyan and Kathleen McKeown.
2000. Cut and paste based text
summarization. In Proceedings of the First
Meeting of the North American Chapter of the
Association for Computational Linguistics,
pages 178?185, Seattle, April 29?May 4.
Jing, Hongyan, Kathleen McKeown, Regina
Barzilay, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In Intelligent
Text Summarization: Papers from the 1998
AAAI Spring Symposium. Stanford, March
23?25. Technical Report SS-98-06, AAAI
Press, pages 60?68.
Johnson, Frances. 1995. Automatic
abstracting research. Library Review,
44(8):28?36.
Johnson, Frances C., Chris D. Paice,
William J. Black, and A. P. Neal. 1993. The
application of linguistic processing to
automatic abstract generation. Journal of
Document & Text Management, 1(3):215?241.
Jones, Paul A. and Chris D. Paice. 1992. A
?select and generate? approach to
automatic abstracting. In A. M. McEnry
and C. D. Paice, editors, Proceedings of the
14th British Computer Society Information
Retrieval Colloquium. Springer Verlag,
pages 151?154.
Jordan, Michael P. 1993. Openings in very
formal technical texts. Technostyle,
11(1):1?26.
Jordan, Michael P. 1996. The Language of
Technical Communication: A Practical Guide
for Engineers, Technologists and Technicians.
Quarry.
Kaplan, Robert B., Selena Cantor, Cynthia
Hagstrom, Lia D. Kamhi-Stein, Yumiko
Shiotani, and Cheryl B. Zimmerman. 1994.
On abstract writing. Text, 14(3):401?426.
Kintsch, Walter and Teun A. van Dijk. 1975.
Comment on se rappelle et on re?sume des
histoires. Langages, 40:98?116.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?Step one:
Sentence compression. In Proceedings of the
17th National Conference of the American
Association for Artificial Intelligence (AAAI),
July 30?August 3.
Kupiec, Julian, Jan Pedersen, and Francine
Chen. 1995. A trainable document
summarizer. In Proceedings of the 18th
ACM-SIGIR Conference, pages 68?73.
Lehmam, Abderrafih. 1997. Une
structuration de texte conduisant a` la
construction d?un syste`me de re?sume?
automatique. In Actes des journe?es
scientifiques et techniques du re?seau
francophone de l?inge?nierie de la langue de
l?AUPELF-UREF, pages 175?182, 15?16
April.
Lehnert, Wendy. 1981. Plot units and
narrative summarization. Cognitive
Science, 5:293?331.
Liddy, Elizabeth D. 1991. The
discourse-level structure of empirical
abstracts: An exploratory study.
Information Processing & Management,
27(1):55?81.
Luhn, Hans P. 1958. The automatic creation
of literature abstracts. IBM Journal of
Research Development, 2(2):159?165.
Maizell, Robert E., Julian F. Smith, and
Tibor E. R. Singer. 1971. Abstracting
Scientific and Technical Literature.
Wiley-Interscience.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 558?565,
College Park, MD, 20?26 June.
Mani, Inderjeet, David House, Gary Klein,
Lynette Hirshman, Leo Obrst, The?re`se
Firmin, Michael Chrzanowski, and Beth
Sundheim. 1998. The TIPSTER SUMMAC
text summarization evaluation. Technical
Report, Mitre Corporation.
Marcu, Daniel. 1997. From discourse
structures to text summaries. In
Proceedings of the ACL?97/EACL?97 Workshop
on Intelligent Scalable Text Summarization,
pages 82?88, Madrid, July 11.
Mariani, Joseph. 1995. Evaluation. In
Ronald E. Cole, editor, Survey of the State of
the Art in Human Language Technology.
Cambridge University Press, chapter 13,
pages 475?518.
Mathis, Betty A. and James E. Rush. 1985.
Abstracting. In E. D. Dym, editor, Subject
and Information Analysis, volume 47 of
Books in Library and Information Science.
Marcel Dekker, pages 445?484.
n-STEIN. 2000. n-STEIN Web page.
http://www.gespro.com.
Oakes, Michael P. 1998. Statistics for Corpus
Linguistics. Edinburgh University Press.
Oakes, Michael P. and Chris D. Paice. 1999.
The automatic generation of templates for
automatic abstracting. In 21st BCS IRSG
Colloquium on IR, Glasgow.
Oakes, Michael P. and Chris D. Paice. 2001.
Term extraction for automatic abstracting.
In D. Bourigault, C. Jacquemin, and
M.-C. L?Homme, editors, Recent Advances
in Computational Terminology, volume 2 of
Natural Language Processing. John
Benjamins, chapter 17, pages 353?370.
Ono, Kenji, Kazuo Sumita, and Seiji Miike.
1994. Abstract generation based on
rhetorical structure extraction. In
Proceedings of the International Conference on
525
Saggion and Lapalme Generating Summaries with SumUM
Computational Linguistics, pages 344?348.
Paice, Chris D. 1981. The automatic
generation of literary abtracts: An
approach based on identification of
self-indicating phrases. In O. R. Norman,
S. E. Robertson, C. J. van Rijsbergen, and
P. W. Williams, editors, Information
Retrieval Research. London: Butterworth,
pages 172?191.
Paice, Chris D. 1991. The rhetorical
structure of expository text. In K. P. Jones,
editor, Informatics 11: The Structuring of
Information, University of York, 20?22
March. Aslib.
Paice, Chris D., William J. Black, Frances C.
Johnson, and A. P. Neal. 1994. Automatic
abstracting. R&D Report 6166, British
Library.
Paice, Chris D. and Paul A. Jones. 1993. The
identification of important concepts in
highly structured technical papers. In
R. Korfhage, E. Rasmussen, and P. Willett,
editors, Proceedings of the 16th ACM-SIGIR
Conference, pages 69?78.
Paice, Chris D. and Michael P. Oakes. 1999.
A concept-based method for automatic
abstracting. Research Report 27, Library
and Information Commission.
Radev, Dragomir R. and Kathleen R.
McKeown. 1998. Generating natural
language summaries from multiple
on-line sources. Computational Linguistics,
24(3):469?500.
Rau, Lisa F., Paul S. Jacobs, and Uri Zernik.
1989. Information extraction and text
summarization using linguistic
knowledge acquisition. Information
Processing & Management, 25(4):419?428.
Rino, Lucia H. M. and Donia Scott. 1996. A
discourse model for gist preservation. In
D. L. Borges and C. A. A. Kaestner,
editors, Proceedings of the 13th Brazilian
Symposium on Artificial Intelligence
(SBIA?96): Advances in Artificial Intelligence,
October 23?25, Curitiba, Brazil. Springer,
pages 131?140.
Rowley, Jennifer. 1982. Abstracting and
Indexing. Clive Bingley, London.
Rumelhart, David E. 1975. Notes on a
schema for stories. In Language, Thought,
and Culture: Advances in the Study of
Cognition. Academic Press.
Saggion, Horacio. 2000. Ge?ne?ration
automatique de re?sume?s par analyse se?lective.
Ph.D. thesis, De?partement d?informatique
et de recherche ope?rationnelle, Faculte?
des arts et des sciences, Universite? de
Montre?al, Montre?al.
Saggion, Horacio and Guy Lapalme. 2000a.
Concept identification and presentation in
the context of technical text
summarization. In Proceedings of the
Workshop on Automatic Summarization
(ANLP-NAACL2000), Seattle, 30 April.
Association for Computational
Linguistics.
Saggion, Horacio and Guy Lapalme. 2000b.
Selective analysis for automatic
abstracting: Evaluating indicativeness and
acceptability. In Proceedings of the
Computer-Assisted Information Searching on
Internet Conference (RIAO?2000), Paris,
12?14 April.
Saggion, Horacio and Guy Lapalme. 2000c.
Summary generation and evaluation in
SumUM. In Advances in Artificial
Intelligence. International Joint Conference:
Seventh Ibero-American Conference on
Artificial Intelligence and 15th Brazilian
Symposium on Artificial Intelligence
(IBERAMIA-SBIA 2000), volume 1952 of
Lecture Notes in Artificial Intelligence.
Springer-Verlag, pages 329?338.
Salton, Gerald, Amit Singhal, Mandar Mitra,
and Chris Buckley. 1997. Automatic text
structuring and summarization.
Information Processing & Management,
33(2):193?207.
Sharp, Bernadette. 1998. Elaboration and
Testing of New Methodologies for Automatic
Abstracting. Ph.D. thesis, University of
Aston in Birmingham.
SICStus. 1998. SICStus Prolog User?s Manual.
SICStus.
Sparck Jones, Karen. 1993. What might be in
a summary? In K. Knorz and
C. Womser-Hacker, editors, Information
Retrieval 93: Von der Modellierung zur
Anwendung.
Sparck Jones, Karen. 1997. Document
processing: Summarization. In R. Cole,
editor, Survey of the State of the Art in
Human Language Technology. Cambridge
University Press, chapter 7, pages
266?269.
Sparck Jones, Karen and Brigitte
Endres-Niggemeyer. 1995. Automatic
summarizing. Information Processing &
Management, 31(5):625?630.
Sparck Jones, Karen and Julia R. Galliers.
1995. Evaluating Natural Language
Processing Systems: An Analysis and Review,
number 1083 in Lecture Notes in Artificial
Intelligence. Springer.
Tait, John I. 1982. Automatic Summarising of
English Texts. Ph.D. thesis, Computer
Laboratory, Cambridge University,
Cambridge.
Teufel, S. 1998. Meta-discourse markers and
problem-structuring in scientific texts. In
M. Stede, L. Wanner, and E. Hovy,
editors, Proceedings of the Workshop on
526
Computational Linguistics Volume 28, Number 4
Discourse Relations and Discourse Markers
(COLING-ACL?98), pages 43?49, 15
August.
Teufel, S. and M. Moens. 1998. Sentence
extraction and rhetorical classification for
flexible abstracts. In Intelligent Text
Summarization: Papers from the 1998 AAAI
Spring Symposium, Stanford, March 23?25.
Technical Report SS-98-06, AAAI Press,
pages 16?25.
Tombros, Anastasios, Mark Sanderson, and
Phil Gray. 1998. Advantages of query
biased summaries in information
retrieval. In Intelligent Text Summarization:
Papers from the 1998 AAAI Spring
Symposium, Stanford, March 23?25.
Technical Report SS-98-06, AAAI Press,
pages 34?43.
Turney, Peter D. 1999. Learning to extract
keyphrases from text. Technical Report
ERB-1051, National Research Council of
Canada.
van Dijk, Teun A. 1977. Recalling and
summarizing complex discourse. In
W. Burghardt and K. Holzer, editors, Text
Processing. De Gruyter, New York and
Berlin, pages 49?118.
Vianna, Fernando de Melo, editor. 1980.
Roget?s II: The New Thesaurus. Houghton
Mifflin, Boston.
Wall, Larry, Tom Christiansen, and
Randal L. Schwartz. 1996. Programming
Perl. O?Reilly & Associates, second
edition.
TransType2 ? An Innovative Computer-Assisted Translation System 
Jos? Esteban and Jos? Lorenzo 
Atos Origin 
Albarrac?n 25 
28037 Madrid, Spain 
jfernando.esteban@atosorigin.com 
jose.lorenzo@atosorigin.com 
Antonio S. 
Valderr?banos 
Bitext.com 
General Or?a 3 
28006 Madrid, Spain 
asv@bitext.com 
Guy Lapalme 
RALI Laboratory 
Universit? de Montr?al 
C.P. 6128, Succ Centreville 
Montr?al, Qu?bec 
Canada H3C 3J7 
lapalme@iro.umontreal.ca 
 
Abstract 
TT2 is an innovative tool for speeding up and 
facilitating the work of translators by 
automatically suggesting translation 
completions. Different versions of the system 
are being developed for English, French, 
Spanish and German by an international team 
of researchers from Europe and Canada. Two 
professional translation agencies are currently 
evaluating successive prototypes. 
1 Introduction 
TransType2 (TT2)1 is an innovative tool for 
speeding up and facilitating the work of translators 
by automatically suggesting translation 
completions. The system uses probabilistic 
translation and language models to calculate 
completions that are compatible with translator's 
input and, furthermore, revises its suggestions in 
real time with each new character the translator 
enters. If the system provides a correct suggestion, 
the translator has only to accept it, thereby saving 
time in producing the target text. Otherwise, the 
translator ignores the system's suggestions and 
continues to type his or her intended translation. 
TT2 is based on a new Machine Assisted 
Translation paradigm that sits between fully 
automatic MT and translation memory in order to 
significantly increase translator productivity on 
non-repetitive texts. TT2 is unique in the way in 
which it combines the strengths of MT technology 
with the competence of the human translator.  
The project is an extension of the TransType 
project that was developed from 1997 to 2000 by 
the RALI at Universit? de Montr?al (Foster 1997, 
Langlais 2002), which demonstrated the interest of 
target text mediated computer aided translation. 
 
                                                     
1 For further details, see http://tt2.sema.es 
Different versions of the system are being 
developed for English, French, Spanish and 
German (with English as the pivot). To ensure that 
TT2 corresponds to translators? needs, two 
professional translation agencies are currently 
evaluating successive prototypes. To date, 
translation technology has not been able to keep 
pace with the demand for high-quality translation. 
TT2 has the ability to significantly increase 
translator productivity and thus has enormous 
commercial potential. 
TT2 is a RTD project funded by the European 
Commission under the Information Society 
Technologies Programme and includes five 
European partners: 
Atos Origin (Spain): administrative and 
technical coordinator, system design and 
integration.  
Lehrstuhl f?r Informatik VI, Computer 
Science Department, RWTH Aachen - University 
of Technology (Germany): statistical translation, 
speech recognition. 
Instituto Tecnol?gico de Inform?tica, 
Universidad Polit?cnica de Valencia, (Spain): 
finite-state techniques for translation and speech 
recognition. 
Xerox Research Centre Europe, Grenoble 
(France): corpus provider and statistical translation 
modeling. 
Celer Soluciones, Madrid (Spain): evaluation in 
the operational context of a translation bureau. 
And two Canadian partners: 
RALI Laboratory, University of Montreal 
(Canada): user-interface, statistical modeling, 
evaluation coordination. 
Soci?t? Gamma, Ottawa (Canada): evaluation in 
the operational context of a translation bureau. 
 
Figure 1. User-view of TT2 with the source text on the left highlighting the sentence under translation. The 
translator types in the right pane in which TT2 suggests completions that appear in the menu in real-time. 
Completions can be accepted either by clicking an item from the menu or by the keyboard. This picture 
displays in red (appearing in gray in black and white) characters that have been suggested and accepted by 
the translator. 
 
2 TT2 as seen by a translator 
TransType is a tool that observes a translator as 
he or she is typing, tries to predict what will be 
typed next and displays its predictions to the user. 
The translator can incorporate these suggestions 
into the current target text if they are useful, or 
simply ignore them by continuing typing.  The 
system will then adapt itself to the new text typed 
by the translator. The suggestions can potentially 
improve a translator's productivity both by 
speeding up the keying in of the target text and by 
contributing to the translation process itself. If the 
underlying machine translation technology is good 
enough, TransType2's contributions may reduce 
the need to consult conventional tools such as a 
bilingual dictionary, term bank, or translation 
memory. 
The user interface (Figure 1) allows a real-time 
interaction with the output of the 
translation/language model to help a translator 
produce a translation. TransType2's main window 
is divided into two panes, one containing the 
source text and another containing the target text. 
The panes are displayed side by side, with their 
contents divided into aligned segments. They are 
also synchronized, so that scrolling one moves the 
other in parallel. Many aspects of the main 
window's behavior and appearance, such as the 
orientation of the source and target panes, can be 
changed using the commands accessible from the 
menu or keyboard shortcuts. 
The source pane is read-only in which the only 
operation allowed is the selection of a new 
sentence that triggers a new translation in the target 
window. The target window is a normal text 
editing window, except that after each character 
typed by the user, the system displays a pop-up 
menu of suggestions for completing the current 
input. If the user types a return or a tab, this 
suggestion is inserted in the text. Suggestions can 
be scrolled up or down with arrow keys or selected 
with the mouse. At initialization time, the user 
selects the prediction engine to be used according 
to one of six source-to-target translation pairs and 
one of the following domains: technical manuals, 
European Community official documents and 
official reports of the debates of the House of 
Commons of Canada (Hansards). 
3 System Architecture 
The TT2 system consists of two major 
subsystems that interact closely: 
user interface (UI), written in Java, provides the 
typing and pointing modalities; a second UI 
supplements those with speech for operating the 
prototype via short commands uttered by the user . 
The user interface also produces a trace of all user-
actions that can later be replayed by a special 
program or analyzed in order to evaluate the 
effectiveness of TransType2 both in terms of 
number of keystrokes needed for typing a 
translation and the various patterns of use. 
prediction engine (PE), written in C/C++, of 
which there are multiple realizations available, 
several per language pair and specific domain 
(either technical documentation, EC official 
documents or Hansards). The translation engines 
developed by research partners are: 
RALI (French?English) is a maximum-entropy 
minimum-divergence translation model (Foster 
2000) that proposes multiple completions for the 
next few words. 
ITI (French?English, Spanish?English) are 
based on finite-state techniques (Cubel et al 2003) 
and suggest a single completion of a whole 
sentence. 
RWTH (French?English, Spanish?English, 
German?English) are statistical based (Och et al 
2003) and suggest a single completion of a whole 
sentence. 
The main communications between the UI and 
the PE are the following: 
1. To initialize the PE, the UI calls a generic create 
method API function with the appropriate 
parameters required by each PE and checks its 
successful completion. 
2. Once the user has selected the file he/she wants 
to work with, the UI produces a list of text 
segments (sentences) and displays them in the 
source text pane of the interface. 
3. The selection of a source sentence is 
communicated to the PE by the UI. The sentence 
becomes the source text context prediction for 
the PE until the user selects another sentence. 
4. The UI communicates to the PE every single 
modification of the target text: insertion/removal 
of a new character (letter, digit, punctuation sign 
or white space) and cursor movements within the 
target text. The UI communicates left-right one-
character-at-a-time movements in the target text 
area. However, the PE does not take into account 
the text to the right of the cursor for making its 
predictions. 
5. In response to the request, the PE initiates the 
search for completions that are eventually re-
turned to the UI for their display. 
6. As part of the general exit procedure, the UI calls 
a generic destroy method API function with the 
appropriate parameters required by each PE and 
checks its successful completion. 
All communication exchanges between the UI 
and the PE are initiated by the UI, while the PE is 
in charge of responding by doing some actual 
work. This is particularly the case in 5 (producing 
a list of completions), while the others are more of 
an informative nature (cases 3 and 4) or can hardly 
considered communication exchanges at all: cases 
1, 2 (loading a text file and producing a list of 
sentences) and 6 (termination). 
Prediction engines and the speech recognizers 
are developed and tested under an operating 
platform (Linux) different than the one chosen for 
user testing (MS Windows). This duality implies 
that prediction engines and speech recognizers, 
while developed under Linux, should be able to 
run under Windows. The users (i.e. the two 
translation bureaus) voiced early in the project that 
TT2 system should run at least under Windows, 
although preferably it should also run under Linux. 
TT2 runs currently on both platforms, the 
dissemination and awareness of the TT2 prototype 
are broader, and go further than the initial 
objectives proposed inside the IST project. 
Given that developers of the prediction engines 
and speech recognizers were in favor of using 
C/C++ as their principal programming language, 
two practical alternatives were discussed: 
? Write code without operating platform 
dependencies and according to standards, that 
would allow compilers for both platforms to 
build functionally equivalent binary versions. 
? Employ tools that lessen to a certain extent the 
requirement of written C/C++ platform 
independent code, while allowing the porting of 
code from the Linux to the Windows platform. 
This was the preferred option and the three PE?s 
actually make use of one of such tool: Cygwin2. 
Cygwin provides a C/C++ compiler for the 
Windows platform and a library (cygwin1.dll) 
that gives support to Linux/Unix operating 
system services under the Windows 
environment. 
The partners responsible for developing the user 
interface have opted for JAVA as the programming 
language because of its graphical user capabilities, 
in particular its text components, which are fully 
configurable and compatible with external C/C++ 
programs. This option solves the portability 
problem, since the resulting code will run under 
any JAVA-enabled operating system.  
                                                     
2 http://www.cygwin.com/ 
4 System requirements 
Generally speaking, running the TT2 system 
demands a high-end personal computer or work-
station in order to be able to provide translation 
completions in real-time and also to be able to 
incorporate multi-modal user input.  
The minimum user equipment is a high-end 
personal computer running under Windows with a 
minimum of 1GB of RAM; however, 2 GB of 
RAM and Windows XP Professional operating 
system is preferable. If a Linux operating system is 
used, the kernel version must be 2.4.20 or higher. 
It is also required to have installed the Java 2 
Runtime Environment, preferably version 
1.3.1_09. To produce the PE, cygwin1.dll version 
1.5.5-1 is required. 
The interface requirements of both scenarios 
include standard keyboard and mouse equipment; 
video display capable of resolutions of 1024x768 
pixels or higher and voice input hardware 
(microphone, a headset preferably, and sound card) 
if the optional speech recognition module is used.  
5 Evaluation 
TT2 is based on the premise that we can improve 
the productivity of translators by reducing the 
number of keystrokes needed for entering a 
translation. Professionals at two translation bureaus 
are currently testing the prototypes. Even though 
translators are not used to working with this kind 
of environment, some of them need about 50% less 
keystrokes to enter a translation and can thus 
produce a translation faster.  Many user interface 
improvements suggested by the translators will be 
included in the next prototypes. 
6 Conclusion 
TT2 is the outcome of a successful cooperation 
between European countries and Canada to 
develop an innovative approach to machine aided 
translation. It is based on advances in statistical 
machine translation research and on a seamless 
integration in a word processing environment of 
the same type as the one currently used by 
translators. 
7 Acknowledgements 
TT2 is a RTD project funded by the European 
Commission under the Information Society 
Technologies Programme (IST-2001-32091). In 
Canada it is funded by the National Science and 
Engineering Research Council and the Minist?re 
du D?veloppement ?conomique et R?gional du 
Qu?bec (Mission Recherche). 
References  
E. Cubel, J. Gonz?lez, A. Lagarda, F. Casacuberta, 
A. Juan and E.Vidal. Adapting finite-state 
translation to the TransType2 project. 
Proceedings of the 8th International Workshop 
of the European Association for Machine 
Translation and the 4th Controlled Language 
Applications Workshop Dublin City University 
Joint Conference, Ireland, 2003. 
Foster G., Isabelle P., Plamondon P. Target-Text 
Mediated Interactive Machine Translation, 
Machine Translation, 12:1-2, 175-194, 1997. 
Foster G., A Maximum Entropy / Minimum 
Divergence Translation Model, Proceedings of 
the 38th Annual Meeting of the Association for 
Computational Linguistics, pp. 37-42, Hong-
Kong, October 2000. 
Philippe Langlais, Guy Lapalme and Marie 
Loranger. TransType: Development-Evaluation 
Cycles to Boost Translator's Productivity. 
Machine Translation (Special Issue on 
Embedded MT Systems), vol. 17, num. 2, pp. 
77-98, Feb 2002. 
F.J. Och, R. Zens, H. Ney. Efficient Search for 
Interactive Statistical Machine Translation. 
Proceedings of the 10th Conference of the 
European Chapter of the Association for 
Computational Linguistics (EACL). Budapest, 
Hungary, pp. 387-393, April 2003. 
Antonio S. Valderr?banos, Jos? Esteban and Luis 
Iraola. TransType2 - A New Paradigm for 
Translation Automation. MT Summit 2003, New 
Orleans, USA. 
Concept Identif ication and Presentat ion in the Context  of  
Technical Text Summarizat ion 
Horac io  Sagg ion*and Guy  Lapa lme 
Ddpartement d' Informatique et Recherche Op~rationnelle 
Universit~ de Montreal 
CP 6128, Succ Centre-Ville 
Montreal, Quebec, Canada, H3C 3J7 
Fax: +1-514-343-5834 
{sagg ion ,  lapalme}@iro,  umontrea l ,  ca 
Abst rac t  
We describe a method of text summarization 
that produces indicative-informative abstracts 
/ 
for technical papers. The abstracts are gener- 
ated by a process of conceptual identification, 
topic extraction and re-generation. We have 
carried out an evaluation to assess indicative- 
ness and text acceptability relying on human 
judgment. The results o far indicate good per- 
formance in both tasks when compared with 
other summarization technologies. 
1 In t roduct ion  
We have specified a method of text summa- 
rization which produces indicative-informative 
abstracts for technical documents. The method 
was designed to identify the "topics" of a 
document and present them in an indicative 
abstract. Eventually, they can be elaborated in  
.specific ways. 
In Figure 1, we present an indicative abstract 
for the document "Facilitating designer-. 
? customer communication i the World Wide 
Web" (Internet Research: Electronic Net- 
working Applications and Policy, Vol 8, Issue 
5,1998) produced with our implementation 
of this method. The abstract includes a list 
of topics which are terms appearing in the 
automatic abstract (e.g. WebShaman)  or 
obtained from the source document by the 
process of term expansion (e.g. WWW 
technique obtained from technique).  It also 
* The first author is supported by Agence Canadienne 
de D~veloppement International (ACDI) and FundaciSn 
Antorchas (A-13671/1-47), Argentin a. He was previ- 
ously supported by Ministerio de EducaciSn de la Naci6n 
de la Repfiblica Argentina (ResoluciSn 1041/96) and De- 
partamento de ComputaciSn, Facultad e Ciencias Exac- 
tas y Naturales, UBA, Argentina. 
includes term elaborations which can be used 
to answer specific questions about the topics 
such as what  is topic? how top ic  is used? 
who developed topic? and what  are the  
advantages of topic? 
. 
In this paper, we will describe how we dealt 
with the problem of content selection and 
presentation and how we have evaluated our 
method of text summarization. 
2 Text  Summar izat ion  
The process of producing a summary from 
a source text consists of the following steps: 
(i) the interpretation of the text; (ii) the 
extraction of the relevant information which 
ideally includes the "topics" of the source; (iii) 
the condensation of the extracted information 
and construction of a summary representation; 
and (iv) the presentation of the summary rep- 
resentation to the reader in natural anguage. 
While some techniques exist for producing 
summaries for domain independent texts 
(Luhn, 1958; Marcu, 1997) it seems that 
domain specific texts require domain specific 
techniques (DeJong, 1982; Paice and Jones, 
1993). In our case, we are dealing with techni- 
cal articles which are the result of the complex 
process of scientific inquiry that starts with 
the. identification of a knowledge problem and 
eventually culminates with the discovery of 
an answer to it. Even if authors of technical 
articles write about several concepts in their 
articles, not all of them are topics. In order to 
address the issue of topic identification, content 
selection and presentation, we have studied 
alignments (manually produced) of sentences 
from professional bstracts with sentences from 
Abst ract  In t roduc ing  the Topics 
! 
! 
Virtual prototyping is a technique which has been suggested for use in, for example, telecommuni- 
cation product development asa high-end technology to achieve a quick digital model that could 
be used in the same way as a real prototype. Presents the design rationale of WebShaman, starting 
from the concept design perspective by introducing a set of requirements to support communication 
via a concept model between i dustrial designer and a customer. In the article, the authors uggest 
that virtual prototyping in collaborative use between designers i a potential technique to facilitate 
design and alleviate the problems created by geographical distance and complexities in the work 
between different parties. The technique, was implemented in the VRP project, allows compo- 
nent level manipulation ofa virtual prototype in a WWW (World Wide Web) browser. The user 
services, the software architecture, and the techniques ofWebShaman were developed iteratively 
during the fieldwork in order to illustrate the ideas and the feasibility of the system. The server is 
not much different from the other servers constructed tosupport synchronous collaboration. 
Identif ied Topics: 3D mode l  - V IRP I  p ro jec t  - WWW - WW-vV techn ique  - WebShaman - 
CAD sys tem - conceptua l .mode l -  cus tomer  - ob jec t -o r iented  mode l -  p roduct  - p roduct  
concept  - p roduct  des ign  - requ i rement  - s imu la t ion  mode l  - smar t  v i r tua l  p ro to type  
- so f tware  component  - sys tem-  techn ique  - techno logy  - use  - v i r tua l  component -  
v i r tua l  p ro to type  - v i r tua l  p ro to type  sys tem-  v i r tua l  p ro to typ ing  
I n fo rmat ion  about  the  Top ics  
An example of a conceptual model, a pen-shaped 'w i re less  user  in ter face  for a mobile 
telephone. 
A v i r tua l  p ro to type  is a computer -based  s imulat ion  of a prototype or a subsystem with a 
degree of functional realism, comparable to that of a physical prototype. 
A computer system implementing the high-end aspects  o f  v i r tua l  prototyping has been 
deve loped in the VRP project (VRP, 1998) at VTT Electronics, in Oulu, Finland. 
The two-and-a-half-year VIRPI pro jec t  cons is ts  o f  th ree  parts. 
Nowadays, CAD (computer -a ided  des ign)  sys tems are  used  as an aid in industrial, mechan- 
ical and electronics design for the specification and deve lopment  o f  a product .  
A v i r tua l  p ro to type  sys tem can  be  used  for concept testing in the early phase of product 
development. 
Figure h Indicative Abstract, Topics and Topic Elaboration 
I 
I 
i 
l 
i 
H 
I 
i 
I 
I 
I 
source documents. One of the alignments is 
presented in Table 1. The first column contains 
the information of the professional abstract. 
The second and third columns contain the infor- 
mation from the source document that matches 
the sentences of the professional bstract, and 
its location in the source document. We have 
produced 100 of these tables containing a 
total of 309 sentences of professional bstracts 
aligned with 568 sentences ofsource documents. 
These alignments allowed us to identify on 
one hand, concepts, relations and types of 
information usually conveyed in abstracts; and 
on the other hand, valid transformations in 
the source in order to produce a compact and 
coherent ext. The transformations include 
verb transformation, concept deletion, concept 
reformulation, structural deletion, parenthetical 
deletion, clause deletion, acronym expansion, 
2 
Professional  Abst ract  
Presents a more efficient 
Distributed Breadth-First 
Search algorithm for an 
asynchronous communication 
network. 
Source Document  
Efficient distributed breadth-first earch algo- 
rithm. 
In this paper we have presented a more effi- 
cient distributed algorithm which construct a
breadth-first search tree in an asynchronous 
communication network 
P /T  
-/Title 
Lst/- 
Presents a model and gives an First we present a model and give overview of lst/- 
overview of related research, related research. 
Analyzes the complexity of the algo- We analyse the complexity of our algorithm, lst/- 
rithm, and gives some examples of per- and give some examples of performance on 
formance on typical networks, typical networks. 
Table 1: LISA Abstract 1955 - Source Document: "Efficient distributed breadth-first search algo- 
rithm." S.A.M. Makki. Computer Communications, 19(8) Jul 96, p628-36. 
abbreviation, merge and split. In our corpus, 
89% of the sentences from the professional 
abstracts included at least one transformation. 
Results of the corpus study are detailed in 
(Saggion and Lapalme, 1998) and (Saggion and 
Lapalme, 2000). 
We have identified a total of 52 different 
types of information (coming from the corpus 
and from technical articles) for technical text 
summarization that we use to identify some of 
the main themes. These types include: the 
explicit topic of the document, the  situa- 
tion, the  ident i f icat ion of the  problem, the  
' identif ication of the  solution, the  research 
goal ,  the  explicit topic of a section, the  
? authors '  deve lopment ,  he  inferences, the  
descr ipt ion of a topical entity, the  def init ion 
? of a topical entity, the  re levance of a topical 
enthy, the  advantages,  etc. Information 
types are classified as indicative or informative 
depending on the type of abstract they con- 
tribute to (i.e. the  topic of a document is 
indicative while the  descr ipt ion of a topical 
entity is informative). Types of information are 
identified in sentences of the source document 
using co-occurrence of concepts and relations 
and specific linguistic patterns. Technical 
articles from different domains refer to specific 
concepts and relations (diseases and treatments 
in Medicine, atoms and chemical reactions 
in Chemistry, and theorems and proofs in 
Mathematics). We have focused on concepts 
and relations that are common across domains 
such as problem, solution, research need, 
experiment, relevance, researchers~ etc. 
3 Text  In terpretat ion  
Our approach to text summarization is based 
on a superficial analysis of the source docu- 
ment and on the implementation f some text 
re-generation techniques such as merging of top- 
ical information, re-expression of concepts and 
acronym expansion. The article (plain text 
in English without mark-up) is segmented in 
main units (title, author information, author 
abstract, keywords, main sections and refer- 
ences) using typographic information and some 
keywords. Each unit is passed through a bi- 
pos statistical tagger. In each unit, the sys- 
tem identifies titles, sentences and paragraphs, 
and then, sentences are interpreted using finite 
state transducers identifying and packing lin- 
guistic constructions and domain specific con- 
structions. Following that, a conceptual dictio- 
nary that relates lexical items to domain con- 
cepts and relations is used to associate seman- 
tic tags to the different structural elements in 
the sentence. Subsequently, terms (canonical 
form of noun groups), their associated semantic 
(head of the noun group) and theirs positions 
are extracted from each sentence and stored in 
an AVL tree (te~ t ree)  along with their fre- 
quency. A conceptual  index is created which 
specifies to which particular type of informa- 
tion each sentence could contribute. Finally, 
terms and words are extracted from titles and 
3 
stored in a list (the top ica l  s t ructure)  and 
acronyms and their expansions are recorded. 
3.1 Content  Select ion 
In order to represent types of information we use 
templates. In Table 2, we present he Topic 
of the  Document ,  Topic of the  Sect ion 
and Signal ing In fo rmat ion  templates. Also 
presented are some indicative and informative 
patterns. Indicative patterns contain variables, 
syntactic constructions, domain concepts and 
relations. Informative patterns also include one 
specific position for the topic under considera- 
tion. Each element of the pattern matches one 
or more elements of the sentence (conceptual, 
syntactic and lexical elements match one ele- 
ment while variables match zero or more). 
3.1.1 Ind icat iveness  
The system considers entences ~hat were iden- 
tified as carrying indicative information (their 
position is found in the conceptual  index). 
Given a sentence? S and a type of information 
T the system verifies if the sentence matches 
some of the patterns associated with type T. 
For each matched pattern, the system extracts 
information from the sentence and instantiates 
a template of type T. For example, the 
Content slot of the prob lem ident i f icat ion 
template is instantiated with all the sentence 
? :(avoiding references, structural elements and 
parenthetical expressions) while the What slot 
'of the topic of the  document  template is 
instantiated with a parsed sentence fragment 
? to the left or to the right of the make known 
relation depending on the attribute voice of the 
verb (active vs. passive). All the instantiated 
templates constitute the Indicative Data Base 
(IDB). 
The system matches the topical structure 
with the topic candidate slots from the IDB. 
The system selects one template for each term 
in that structure: the one with the greatest 
weight (heuristics are applied if there are more 
than one). The selected templates constitute 
the indicative content and the terms ap- 
pearing in the topic candidate slots and their 
expansions constitute the potential topics 
of the document. Expansions are obtained 
looking for terms in the term tree sharing 
the semantic of some terms in the indicative 
4 
content.  
The ind icat ive  content  is sorted using 
positional information and the following con- 
ceptual order: s i tuat ion ,  need for research,  
problem,  solut ion, ent i ty  in t roduct ion ,  
topical  in format ion,  goal of concep- 
tua l  ent i ty,  focus of conceptua l  ent i ty,  
methodo log ica l  aspects,  inferences and 
s t ruc tura l  in format ion.  Templates of the 
same type are grouped together if they ap- 
peared in sequence in the list. The types 
considered in this process are: the  topic, 
sect ion topic and s t ruc tura l  in format ion.  
The sorted templates constitute the text  plan. 
3.1.2 Informativeness 
For each potential: topic and sentence where 
it appears (that information is found on the 
term tree) the system verifies if the sentence 
contains an informative marker (conceptual 
index) and satisfies an informative pattern. If 
so, the potential topic is considered a topic 
of the document and a link will be created be- 
tween the topic and the sentence which will be 
part of the informative abstract. 
4 Content  P resentat ion  
Our approach to text generation is based 
on the regularities observed in the corpus 
of professional abstracts and so, it does not 
implement a general theory of text generation 
by computers. Each element in the text  p lan 
is used to produce a sentence. The structure of 
the sentence depends on the type of template. 
The information about the s i tuat ion,  the  
problem, the need for research,  etc. is 
reported as in the original document with few 
modifications (concept re-expression). Instead 
other types require additional re-generation: 
for the topic of the  document  template the 
generation procedure is as follows: (i) the verb 
form for the predicate in the Pred icate  slot 
is generated in the present  tense (topical 
information is always reported in present 
tense), 3rd person of s ingu lar  in ac t ive  
voice at the beginning of the sentence; 
(ii) the parsed sentence fragment from the N'hat 
slot is generated in the middle of the sentence 
(so the appropriate case for the first element 
I 
I 
I 
i 
I 
Topic of the Document  
topic Type: 
Id: 
Predicate: 
Where: 
Who: 
What: 
Position: 
Topic candidates: 
Weight: 
integer identifier 
instance of make known 
instance of {research paper, study, work, research} 
instance of{research paper, author,  study, work, research,  none} 
parsed sentence fragment 
section and sentence id
list of terms from the What filler 
number 
Topic o f  Sect ion  
Type: 
Id: 
Predicate:  
Section: 
Argument: 
Position: 
Topic candidates: 
Weight: 
sec_desc 
integer identifier 
instance of make known 
instance of paper component 
parsed sentence fragment 
section and sentence id
list of terms from the Argument filler 
number 
Type: 
Id: 
Predicate : 
Structural : 
Argument : 
Po s it ion: 
Topic candidates : 
Weight : 
structure-2 
integer identifier 
instance of show graphical material 
instance of structural element 
parsed sentence fragment 
section and sentence id 
list of terms from the Argument filler 
number 
Signaling 
(indicative) 
Topic  (indica- 
tive) 
SKIP1 + s t ructura l  + SKIP2 + show graphica l ly  + ARGUMENT + eos 
noun group + author + make known + prepos i t ion + research paper + 
DESCRIPT ION + eos 
Author 's  Goal SKIP1 + goal of author + def ine + GOAL + eos 
(indicative) 
Goal of SKIP  + goal + preposi t ion + TOPIC + def ine + GOAL + eos 
TOPIC (in- 
formative) 
Definition SKIP  + TOPIC + def ine + noun group 
of TOPIC 
(informative) 
Table 2: Templates and Patterns. 
has, to be generated); and (iii) a full stop is 
generated. This schema of generation avoids 
the formulation of expressions like "X will be 
presented", "X have been presented" or "We 
have presented here X" which are usually found 
on source documents but which are awkward 
in the context of the abstract ext-type. Note 
that each type of information prescribes its 
own schema of generation. 
Some elements in the parsed sentence frag- 
ment require re-expression while others are 
presented in "the words of the author." If the 
system detects an acronym without expansion 
in the string it would expand it and record that 
situation in order to avoid repetitions. Note 
that as the templates contain parsed sentence 
fragments, the correct punctuation has to 
be re-generated. For merged templates the 
generator implements the following patterns 
of production: if n adjacent emplates are to 
be presented using the same predicate, only 
one verb will be generated whose argument is 
the conjunction of the arguments from the n 
templates. If the sequence of templates have 
no common predicate, the information will 
be presented as a conjunction of propositions. 
These patterns of sentence production are 
exemplified in Table 3. 
The elaboration of the topics is presented 
upon reader's demand. The information is pre- 
sented in the order of the original text. The in- 
formative abstract is the information obtained 
by this process as it is shown in Figure 1. 
5 L imi ta t ions  o f  the  Approach  
Our approach is based on the empirical ex- 
amination of abstracts published by second 
services. In our first study, we examined 100 
abstracts and source documents in order to 
deduce a conceptual and linguistic model for 
the task of summarization of technical articles. 
Then, we expanded the corpus with 100 more 
items in order to validate the model. We 
believe that the concepts, relations and types 
.of information identified account for interesting 
,phenomena appearing in the corpus and con- 
stitute a sound basis for text summarization. 
'Nevertheless, we have identified only a few 
? linguistic expressions used in order to express 
-particular elements of the conceptual model 
(241 domain verbs, 163 domain nouns, 129 
adj.ectives , 174 indicative patterns, 87 informa- 
tive patterns). This is because we are mainly 
concerned with the development of a general 
method of automatic abstracting and the task 
of constructing such linguistic resources i time 
consuming as recent work have shown (Minel 
et al, 2000). 
The implementation f our method relies? on 
State-of-the-art techniques in natural anguage 
processing including noun and verb group iden- 
tification and conceptual tagging. The inter- 
preter relies on the output produced by a shal- 
low text segmenter and on a statistical POS- 
tagger. Our prototype only analyses entences 
for the specific purpose of text summarization 
and implements some patterns of generation ob- 
served in the corpus. Additional analysis could 
be done on the obtained representation to pro- 
duce better esults. 
6 Re la ted  Work  
(Paice and Jones, 1993) have already addressed 
the issue of content identification and expression 
in technical summarization using templates, but 
while they produced indicative abstracts for a 
specific domain, we are producing domain inde- 
pendent indicative-informative abstracts. Being 
designed for one specific domain, their abstracts 
are fixed in structure while our abstracts are 
dynamically constructed. Radev and McKeown 
(1998) also used instantiated templates, but in 
order to produce summaries of multiple docu- 
ments in one specific domain. They focus on 
the generation of the text while we are address- 
ing the overall process of automatic abstracting. 
Our concern regarding the presentation of the 
information is now being addressed by other re- 
searchers as well (Jing and McKeown, 1999). 
7 Eva luat ing  Content  and  Qua l i ty  in  
Text  Summar izat ion  
Abstracts are texts used in tasks such as assess- 
ing the content of the document and deciding 
if the source is worth reading. If text summa- 
rization systems are designed to fulfill those re- 
quirements, the generated texts have to be eval- 
uated according to their intended function and 
its quality. The quality and success of human 
produced abstracts have already been addressed 
in the literature (Grant, 1992; Gibson, 1993) us- 
ing linguistic criteria such as cohesion and co- 
herence, thematic structure, sentence structure 
and lexical density. But in automatic text sum- 
marization, this is an emergent research topic. 
(Minel et al, 1997) have proposed two meth- 
ods of evaluation addressing the content of the 
abstract and its quality. For content evalua- 
tion, they asked human judges to classify sum- 
maries in broad categories and also verify if 
the key ideas of source documents are appropri- 
ately expressed in the Summaries. For text qual- 
ity, they asked human judges to identify prob- 
lems such as dangling anaphora nd broken tex- 
tual segments and also to make subjective judg- 
ments about readability. In the context of the 
TIPSTER program, (Firmin and Chrzanowski, 
6 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
Re-Generated Sentences Sentences from Source Documents 
Illustrates the principle of virtual prototyping and 
the different echniques and models required. 
Presents the mechanical and electronic design o\] 
the robot harvester including all subsystems, 
namely, fruit localisation module, harvesting arm 
and gripper-cutter as well as the integration of 
subsystems and the specific mechanical design of 
the picking arm addressing the reduction of 
undesirable dynamic effects during high velocity 
operation. 
Shows configuration of the robotic fruit harvester 
Agribot and schematic view of the detaching tool. 
PAWS (the programmable automated welding sys- 
tem) was designed to provide an automated means 
of planning, controlling, and performing critical 
welding operations for improving productivity and 
quality. 
Describes HuDL (local autonomy) in greater 
detail; discusses ystem integration and the 1MA 
(the intelligent machine architecture); and also 
gives an example implementation. 
Figure 1 Virtual prototyping models and techniques 
illustrates the principle of virtual prototyping and the 
different techniques and models required. 
After a brief introduction, we present the mechanical 
and electronic design of the robot harvester includ- 
ing all subsystems, namely, fruit localisation module, 
harvesting arm and gripper-cutter aswell as the inte- 
gration of subsystems. 
Throughout this work, we present the specific mechan- 
ical design of the picking arm addressing the reduction 
of undesirable dynamic effects during high velocity op- 
eration. 
The final prototype consists of two jointed harvesting 
arms mounted on a human guided vehicle as shown 
schematically in Figure 1 Configuration ofthe robotic 
. fruit' harvester Agribot. 
Schematic representation f the operations involved in 
the detaching step can be seen in Figure 5 Schematic 
view of the detaching tool and operation. 
PAWS was designed to provide an automated means of 
planning, controlling, and performing critical welding 
operations for improving productivity and quality. 
Section 2 describes HuDL in greater detail and section 
3 discusses system integration and the IMA. 
An example implementation is given in section 4 and 
section 5 contains the conclusions. 
Table 3: Re-Generated Sentences 
1999) and (Mani et al, 1998) also used a cat -  
.egorization task using TREC topics. For text 
quality, they addressed subjective aspects uch 
? as the length of the summary, its intelligibility 
and its usefulness. We have carried out an eval- 
? uation of our summarization method in order to 
assess the function of the abstract and its text 
quality. 
7.1 Exper iment  
We compared abstrac?s produced by our 
method with abstracts produced by Mi- 
crosoft'97 Summarizer and with others 
published with source documents (usually au- 
thor abstracts). We have chosen Microsoft'97 
Summarizer because, even if it only produces 
extracts, it was the only summarizer available 
in order to carry out this evaluation and 
because it has already been used in other eval- 
uations (Marcu, 1997; Barzilay and Elhadad, 
1997). 
In order to evaluate content, we presented 
judges with randomly selected abstracts and 
five lists of keywords (content indicators). The 
judges had to decide to which list of keywords 
the abstract belongs given that different lists 
share some keywords and that they belong 
to the same technical domain. Those. lists 
were obtained from the journals where the 
source documents were published. The idea 
behind this evaluation is to see if the abstract 
convey the very essential content of the source 
document. 
In Order to evaluate the quality of the text, we 
asked the judges to provide an acceptability 
score between 0-5 for the abstract (0 for un- 
acceptable and 5 for acceptable) based on the 
following criteria taken from (Rowley, 1982) 
(they were only suggestions to the evaluators 
and were not enforced): good spelling and 
grammar; clear indication of the topic of 
7 
the  source document; impersonal style; one 
paragraph; conciseness; readable and under- 
standable; acronyms are presented along with 
their expansions; and other criteria that the 
judge considered important as an experienced 
reader of abstracts of technical documents. 
We told the judges that we would consider 
the abstracts with scores above 2.5 as accept- 
able. Some criteria are more important han 
other, for example judges do not care about 
impersonal style but care about readability. 
7.1.1 Mater ia ls  
Source  Documents :  we used twelve source 
documents from the journal Industrial Robots 
found on the Emerald Electronic Library (all 
technical articles). The articles were down- 
loaded in plain text format. These documents 
are quite long texts with an average of 23K 
characters (minimum of l lK  characters and a 
maximum of 41K characters). They contain 
an average of 3472 words (minimum of 1756 
words and a maximum of 6196 words excluding 
punctuation), and an average of 154 sentences 
(with a minimum of 85 and a maximum of 288). 
Abstracts :  we produced twelve abstracts us- 
:ing our method and computed the compression 
,ratio in number of words, then we produced 
twelve abstracts by Microsoft'97 Summarizer 1 
us ing  a compression rate at least as high as 
our (i.e. if our method produced an abstract 
with a compression rate of 3.3% of the source, 
we produced the Microsoft abstract with a 
compression rate of 4% of the source). We 
extracted the twelve abstracts and the twelve 
lists of keywords publ ished with the source 
documents. We thus obtained 36 different 
abstracts and twelve lists of keywords. 
Forms:  we produced 6 different forms each con- 
taining six different abstracts randomly 2 chosen 
out of twelve different documents (for a total of 
36 abstracts). Each abstract was printed in a 
1We had to format the source document in order for 
the Microsoft Summarizer to be able to recognize the 
structure of the document (titles, sections, paragraphs 
and sentences). 
2Random numbers for this evaluation were produced 
using software provided by SICSTus Prolog. 
different page. It included 5 lists of keywords, a
field to be completed with the quality score as- 
sociated to the abstract and a field to be f i l led 
with comments about the abstract. One of the 
lists of keywords was the one published with the 
source document, he other four were randomly 
selected from the set of 11 remaining keyword 
lists, they were printed in the form in random 
order. One page was also available to be com- 
pleted with comments about the task, in partic- 
ular with the time it took to the judges to com- 
plete the evaluation. We produced three copies 
of each form for a total of 18 forms. 
7.1.2 Sub jec ts  
We had a total of 18 human judges or eval- 
uators. Our evaluators were 18 students of 
the M.Sc. program in Information Science at 
McGill Graduate School of Library & Informa- 
tion Studies. All of the subjects had good read- 
ing and comprehension skills in English. This 
group was chosen because they have knowledge 
about what constitutes a good abstract and 
they are educated to become professionals in In- 
formation Science. 
7.1.3 Eva luat ion  Procedure  
The evaluation was performed in one hour ses- 
sion at McGill University. Each human judge 
received a form (so he/she evaluated six dif- 
ferent abstracts) and an instruction booklet. 
No other material was required for the evalu- 
ation (i.e. dictionary). We asked the judges to 
read carefully the abstract. They had to decide 
which was the list of keywords that matched 
the abstract (they could chose more than one 
or none at all) and then, they had to associate 
a numeric score to the abstract representing its 
quality based on the given criteria. This pro- 
cedure produced three different evaluations of 
content and text quality for each of the 36 ab- 
stracts. The overall evaluation was completed 
in a maximum of 40 minutes. 
7.2 Resu l ts  
For each abstract, we computed the average 
quality using the scores given by the judges. 
We considered that the abstract indicated the 
essential content of the source document if two 
or more judges were able to chose the correct 
list of keywords for the abstract. The results for 
individual articles and the average information 
8 
I 
I 
i 
i 
g 
I 
I 
i 
I 
I 
I 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
I 
I 
I 
i 
Microsoft Abstract 
# Art. Indic? Quality 
1 yes 2.66 
2 no 1.36 
3 no 1.16 
4 yes 3.00 
5 no 2.16 
6 yes 2.16 
7 no 0.83 
8 yes 2.33 
9 yes 2.16 
10 yes 2.16 
11 yes 2.40 
12 no 1.16 
Average 70% 1.98 
\] Average l 
Our Method 
Indic? Quality 
yes 2.93 
yes 3.66 
no 3.00 
yes 4.00 
no 1.76 
yes 4.00 
yes 2.50 
yes 3.00 
no 2.66 
yes 4.00 
no 2.70 
no 3.33 
70% 3.15 
S.D. Abstract 
Indic? Quality 
yes 4.16 
yes 4.00 
no 4.06 
yes 4.33 
yes 4.00 
no 4.53 
yes 4.40 
yes 4.00 
yes 3.66 
yes 3.31 
no 4.26 
no 4.00 
80% 4.04 
Results with Different Documents and Subjects 
80% I 1.46\[ 80%\] 3.23\[ 100% \[ 4.25 I 
Table 4: Results of Human Judgment about Indicativeness and Text Quality 
are shown in Table 4. For a given source docu- 
ment and type of abstract, the value in column 
'Indic?' contains the value 'yes' if the majority 
of the evaluator have chosen the source docu- 
ment list of keywords for the abstract and 'no' 
on the contrary. The value in column 'Qual- 
ity' is the average acceptability for the abstract. 
Content:  In 80% of the cases, the abstracts 
published with the source documents were 
correctly classified by the evaluators. Instead, 
the automatic abstracts were correctly classi- 
fied in 70% of the cases. It is worth noting 
"that the automatic systems did not use the 
? journal abstracts nor the lists of keywords or  
the.information about the journal. 
Quality: The figures about text acceptabil- 
ity indicate that the abstracts produced by 
Microsoft'97 Summarizer are below the accept- 
abil!ty level of 2.5, the abstracts produced by 
our method are above the acceptability level of 
2.5 and that the human abstracts are highly 
acceptable. 
In a run of this experiment using 30 ab- 
stracts from a different set of 10 articles and 15 
judges from \]~cole de Biblioth6conomie et des 
Sciences de l'Information (EBSI) at Universit6 
de Montr@al we have obtained similar results 
(last row in Table 4). 
8 Conc lus ions  
In this paper, we have presented a method of 
text summarization which produces indicative- 
informative abstracts. We have described the 
techniques we are using to implement our 
method and some experiments howing the 
viability of the approach. 
Our method was specified for summarization 
of one specific type of text: the scientific and 
technical document. Nevertheless, it is domain 
independent because the concepts, relations 
and types of information we use are common 
across different domains. The question of the 
coverage of the model will be addressed in 
our future work. Our method was designed 
without any particular reader in mind and 
with the assumption that a text does have 
a "main" topic. If readers were known, the 
abstract could be tailored towards their specific 
profiles. User profiles could be used in order to 
produce the informative abstracts elaborating 
those specific aspects the reader is "usually" 
interested in. This aspect will be elaborated in
future work. 
The experiments reported here addressed 
9 
the evaluation of the indicative abstracts using 
a categorization task. Using the automatic 
abstracts reader have chosen the correct 
category for the articles in 70% of the cases 
compared with 80% of the cases when using the 
author abstracts. Readers found the abstracts 
produced by our method of better quality than 
a sentence-extraction based system. 
Acknowledgments  
We would like to thank three anonymous re- 
viewers for their comments which helped us im- 
prove the final version of this paper. We are 
grateful to Professor Mich~le Hudon from Uni- 
versit~ de Montreal for fruitful discussion and to 
Professor John E. Leide from McGill University 
and to Mme Gracia Pagola from Universit~ de 
Montreal for their help in recruiting informants 
for the experiments. 
References 
R. Barzilay and M. Elhadad. 1997. Using 
Lexical Chains for Text Summarization. In
Proceedings of the A CL/EA CL '97 Workshop 
on Intelligent Scalable Text Summarization, 
pages 10-17, Madrid, Spain, July. 
G. DeJong. 1982. An Overview of the FRUMP 
System. In W.G. Lehnert and M.H. Ringle, 
editors, Strategies for Natural Language Pro- 
cessing, pages 149-176. Lawrence Erlbaum 
. Associates, Publishers. 
T. Firmin and M.J. Chrzanowski. 1999. An 
Evaluation of Automatic Text Summariza- 
tion Systems. In I. Mani and M.T. Maybury,. 
editors, Advances in Automatic Text Summa- 
~ization, pages 325-336. 
T.R. Gibson. 1993. Towards a Discourse The- 
ory of Abstracts and Abstracting. Depart- 
ment of English Studies. University of Not- 
tingham. 
P. Grant. 1992. The Integration of Theory and 
Practice in the Development of Summary- 
Writting Strategies. Ph.D. thesis, Universit~ 
de Montreal. Facult~ des ~tudes up~rieures. 
H. Jing and K.R. McKeown. 1999. The Decom- 
position of Human-Written Summary Sen- 
tences. In M. Hearst, Gey. F., and R. Tong, 
editors, Proceedings of SIGIR '99. 22nd Inter- 
national Conference on Research and Devel- 
opment in Information Retrieval, pages 129- 
136, University of California, Beekely, Au- 
gust. 
H.P. Luhn. 1958. The Automatic Creation of " 
Literature Abstracts. IBM Journal of Re- 
search Development, 2(2):159-165. 
I. Mani, D. House, G. Klein, L. Hirshman, 
L. Obrst, T. Firmin, M. Chrzanowski, and 
B. Sundheim. 1998. The TIPSTER SUM- 
MAC Text Summarization Evaluation. Tech- 
nical report, The Mitre Corporation. 
D. Marcu. 1997. From Discourse Structures 
to Text Summaries. In The Proceedings of 
the A CL '97lEA CL '97 Workshop on Intelli- 
gent Scalable Text Summarization, pages 82- 
88, Madrid, Spain, July 11. 
J-L. Minel, S. Nugier, and G. Piat. 1997. Com- 
ment Appr~cier la Qualit~ des R~sum~s Au- 
tomatiques de Textes? Les Exemples des Pro- 
tocoles FAN et MLUCE et leurs R~sultats 
sur SERAPHIN. In ldres Journdes Scientific- 
ques et Techniques du Rdseau Francophone 
de l'Ingdnierie de la Langue de I'AUPELF- 
UREF., pages 227-232, 15-16 avril. 
J-L. Minel, J-P. Descl~s, E. Cartier, 
G. Crispino, S.B. Hazez, and A. Jack- 
iewicz. 2000. R~sum~ automatique par 
filtrage s~mantique d'informations dans des 
textes. TSI, X(X/2000):l-23. 
C.D. Paice and P.A. Jones. 1993. The Iden- 
tification of Important Concepts in Highly 
Structured Technical Papers. In R. Korfhage, 
E. Rasmussen, and P. Willett, editors, Proc. 
of the 16th ACM-SIGIR Conference, pages 
69-78. 
D.R. Radev and K.R. McKeown. 1998. Gen- 
erating Natural Language Summaries from 
Multiple On-Line Sources. Computational 
Linguistics, 24(3):469-500. 
J. Rowley. 1982. Abstracting and Indexing. 
Clive Bingley, London. 
H. Saggion and G. Lapalme. 1998. Where does 
Information come from? Corpus Analysis for 
Automatic Abstracting. In RIFRA'98. Ren- 
contre Internationale sur l'extraction le Fil- 
trage et le Rdsumd Automatique, pages 72-83. 
H. Saggion and G. Lapalme. 2000. Evaluation 
of Content and Text Quality in the Context 
of Technical Text Summarization. In Pro- 
ceedings of RIAO'2000, Paris, France, 12-14 
April, 2000. 
10 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
I 
I 
I! 
I, 
I 
I 
I 
I 
i, 
TransType: a Computer--Aided Translation Typing System 
Ph i l ippe  Lang la i s  and George  Foster  and Guy  Lapa lme 
RAL I /D IRO - -  Universit@ de Montr@al 
C.P. 6128, succursale Centre-ville 
H3C 3J7 Montr4al, Canada 
Phone:: +1 (514) 343-2145 
Fax: +1 (514) 343-5834 
email: {felipe, foster, lapalme}?iro, umontreal, ca 
Abst ract  
This paper describes the embedding of a sta- 
tistical translation system within a text editor 
to produce TRANSTYPE, a system that watches 
over the user as he or she types a translation and 
repeatedly suggests completions for the text al- 
ready entered. This innovative Embedded Ma- 
chine Translation system is thus a specialized 
means of helping produce high quality transla- 
tions. 
1 In t roduct ion  
TRANSTYPE is a project set up to explore an 
appealing solution to the problem of using In- 
teractive Machine Translation (IMT) as a tool 
for professional or other highly-skilled transla- 
tors. IMT first appeared as part of Kay's MIND 
system (Kay, 1973), where the user's role was 
to help the computer analyze the source text 
by answering questions about word sense, el- 
lipsis, phrasal attachments, etc. Most later 
work on IMT, eg (Blanchon, 1991; Brown and 
Nirenburg, 1990; Maruyama and Watanabe, 
1990; Whitelock et al, 1986), has followed in 
this vein, concentrating on improving the ques- 
tion/answer process by having less questions, 
more friendly ones, etc. Despite progress in 
these endeavors, systems of this sort are gen- 
erally unsuitable as tools for skilled trans\]\[ators 
because the user serves only as an advisor, with 
the MT components keeping overall control over 
the translation process. 
TRANSTYPE originated from the conviction 
that a better approach to IMT for competent 
translators would be to shift the focus of in- 
teraction from the meaning of the source text 
to the form of the target text. This would re- 
lieve the translator of the burden of having to 
provide explicit analyses of the source text and 
allow him to translate naturally, assisted by the 
machine whenever possible. 
In this approach, a translation emerges from 
a series of alternating contributions by human 
and machine. The machine's contributions are 
basically proposals for parts of the target text, 
while the translator's can take many forms, in- 
cluding pieces of target text, corrections to a 
previous machine contribution, hints about the 
nature of the desired translation, etc. In all 
cases, the translator remains directly in control 
of the process: the machine must respect he 
constraints implicit in his contributions, and he 
or she is free to accept, modify, or completely 
ignore its proposals. 
So TRANSTYPE is a specialized text editor 
with an embedded Machine translation engine 
as one of its components. In this project we 
had to address the following problems: how to 
interact with the user and how to find appro- 
priate multi-word units for suggestions that can 
be computed in real time. 
2 The  TransType  mode l  
2.1 User  V iewpo int  
Our interactive translation system is illustrated 
in figure 1 for an English to French translation. 
It works as follows: a translator selects a sen- 
tence and beg!ns typing its translation. After 
each character typed by the translator, the sys- 
tem displays a proposed completion, which may 
either be accepted using a special key or rejected 
by continuing to type. This interface is simple 
and its performance may be measured by the 
proportion of characters or keystrokes aved in 
typing a translation. Note that, throughout this 
process, the translator emains in control, and 
the machine must continually adapt its sugges- 
tions to the translator's input. This differs from 
the usual machine translation set-ups where it is 
the machine that produces the first draft which 
46 
? . ? . : ,  ? , ,  - - - - - - ~  
..... Fich:ier :::= ptions 
? : ' .  " - : . ' .  . . .  " . . :  . . . .  " .11 . . .  I t "  I: am:pleased:to: akepart:m this debate today. 
Usingitoday'S technologies,it:is possiblefOrall ~ad iaqs  to 
. . . .  . a . . . . .  borrowing:. 
. . . . .  ? . . . . . . . . .  :::i~ : 
d~batl.. 
GraCel ~i~la t~chnOIogiemoderne, tousles Can adiehs peuVent se 
prononcer:sur:lesquestions de:depenses: et~:d em runts:de/EZra 
Figure 1: Example of an interaction in TRANSTYPE with the source text in the top half of the 
screen. The target text is typed in the bottom half with suggestions given by the menu at the 
insertion point. 
then has to be corrected by the translator. 
The first version of TRANSTYPE (Foster et 
al., 1997) only proposed completions for the cur- 
rent word. We are now working on predictions 
which extend to the next several words in the 
text. The potential gain from multiple-word 
predictions (Langlais et al, 2000) can be ap- 
preciated in the one-sentence translation task 
reported in table 1, where a hypothetical user 
saves over 60% of the keystrokes needed to pro- 
duce a translation in a word completion sce- 
nario, and about 75% in a "unit" completion 
scenario 
2.2 System V iewpo int  
The core of TRANSTYPE is a completion engine 
which comprises two main parts: an evaluator 
which assigns probabilistic scores to completion 
47 
This bill is very similar to its companion bill which we dealt with yesterday 
in the house of commons 
word-completion task. unit-completion task 
pref. completions pref. completions 
C+ /loi ? c/e pro jet de loi ce 
pro jet 
de 
Ioi 
est 
tr~s 
semblable 
au 
pro jet 
de 
Ioi 
que 
nous 
avons 
examin4 
hier 
la 
chambre 
des 
communes 
ce+ / lOi" C/' 
p+ /est p/rojet 
d+ /tr~s d/e 
I+ /tr~s I/oi 
e+ /de e/st 
t+ /de ? t / r~s  
se+ /de ? s/es 
au+ /loi ? a/vec 
p+ /loi p/rojet 
d+ /loi ? d/e 
I+ /nous ? I/oi 
qu+ /nous ? q /u i  ? 
+ /nous 
av+ /nous 
ex+ /hier 
+ /hier 
se/mblable 
qu/e 
?  v/ons 
? e /n .  ex/amin~ 
~+ /b ie r  ? ~/ la  
+ /chambre 
de+ / communes  ? dle ? 
+ /communes 
de/s 
e+ 
t+ 
se+ 
a+ 
/de e/st 
/de .  t/r~s 
/de ? s/es se/mblable 
/loi ? a/u projet de loi sur 
qu+ /nous ? q /u i  ? qu/e 
+ /nous 
av+ /nous. a/vec, av/ons 
exa+ /& la chambre des communes 
e/n. ex/istence, exa/min~ 
h-F /& la chambre des communes 
h/let 
+ /& la chambre des communes 
106 char. 23 20 accept. 14 11 accept. -t- 1 correc. 
43 keyst rokes  26 keyst rokes  
Table h A one-sentence s ssion illustrating the word- and unit- completion tasks. The first col- 
umn indicates the target words the user is expected to produce. The next two columns indicate 
respectively the prefixes typed by the user and the completions made by the system under a word- 
completion task. The last two columns provide the same information for the unit-completion task. 
The total number of keystrokes for both tasks is reported in the last line. + indicates the accep- 
tance key typed by the user. A Completion is denoted by a/ f l  where a is the typed prefix and fl 
the completed part. Completions for different prefixes are separated by ? . 
hypotheses and a generator which uses the eval- 
uation function to select the best candidate for 
completion. 
2.2.1 The  eva luator  
The evaluator is a function p(t\[t', s) which as- 
signs to each target-text unit t an estimate of 
its probability given a source text s and the to- 
kens t' which precede t in the current ranslation 
of s. Our approach to modeling this distribu- 
tion is based to a large extent on that of the 
IBM group (Brown et al, 1993), but it diflhrs in 
one significant aspect: whereas the IBM model 
involves a "noisy channel" decomposition, we 
use a linear combination of separate predictions 
from a language model p(t\[t') and a transla- 
tion model p(t\[s). Although the noisy channel 
technique is powerful, it has the disadvantage 
that p(s\[t', t) is more expensive to compute than 
p(t\[s) when using IBM-style translation models. 
Since speed is crucial for our application, we 
chose to forego it in the work described here. 
Our linear combination model is fully described 
in (Langlais and Foster, 2000) but can be seen 
as follows: 
48 
p(tlt ' ,s ) = p(tlt' ) A(O(t',s)), (1) 
language 
+ p(tls)\[1-~(O(t',s))! 
translation 
where .~(O(t',s)) e \[0,1\] are context- 
dependent interpolation coefficients. O(t~,s) 
stands for any function which maps t~,s into a 
set of equivalence classes. Intuitively, ),(O(t r, s)) 
should be high when s is more informative than 
t r and low otherwise. For example, the trans- 
lation model could have a higher weight at the 
start of sentence but the contribution of the lan- 
guage model can become more important in the 
middle or the end of the sentence. 
2.2.2 The  language mode l  
We experimented with various simple linear 
combinations of four different French language 
models: a cache model, similar to the cache 
component in Kuhn's model (Kuhn and Mori, 
1990); a unigram model; a trielass model (Der- 
ouault and Merialdo, 1986); and an interpolated 
trigram (Jelinek, 1990). 
We opted for the trigram, which gave signifi- 
cantly better results than the other three mod- 
els. The trigram was trained on the Hansard 
corpus (about 50 million words), with 75% of 
the corpus used for relative-frequency parame- 
ter estimates, and 25% used to reestimate inter- 
polation coefficients. 
2.2.3 The  t rans la t ion  mode l  
Our translation model is based on the linear in- 
terpolation given in equation 2 which combines 
predictions of two translation models - -  Ms and 
Mu - -  both based on an IBM-like model 2 (see 
equation 3). Ms was trained on single words 
and Mu was trained on both words and units. 
p( tls) = Z pt( tls) ,+ (1 - Z).p2 ( (s ) ) 
word unit 
(2) 
where Ps and Pu stand for the probabilities 
given respectively by Ms and M~. ~(s) repre- 
sents the new sequence of tokens obtained after 
grouping the tokens of s into units. 
Both models are based on IBM translation 
model 2 (Brown et al, 1993) which has the 
49 
property that it generates tokens independently. 
The total probability of the ith target-text to- 
ken ti is just the average of the probabilities 
with which it is generated by each source text 
token sj; this is a weighted average that takes 
the distance from the generating token into ac- 
count: 
is1 
p(tils) = ~p( t i l s j )  a(jli, Is\[) 
j=O 
(3) 
where p(ti Is j) is a word-for-word translation 
probability, Isl is the length (counted in tokens) 
o f the  source segment s under translation, and 
a(jli , Is\]) is the a priori alignment probability 
that the target-text token at position i will be 
generated by the source text token at position 
j; this is equal to a constant value of 1~(Is I + 1) 
for model 1. This formula follows the conven- 
tion of (Brown et al, 1993) in letting so des- 
ignate the null state. We modified IBM model 
2 to account for invariant entities such as En- 
glish forms that almost invariably translate into 
French either verbatim or after having under- 
gone a predictable transformation e.g. numbers 
or dates. These forms are very frequent in the 
Hansard corpus. 
2.3 The Generator  
The task of the generator is to identify units 
matching the current prefix typed by the user, 
and pick the best candidate using the evalua- 
tion function. Given the real time constraints 
of an IMT system, we divided the French vocab- 
ulary into two parts: a small active component 
whose contents are always searched for a match 
to the current prefix, and a much larger passive 
part which comes into play only when no candi- 
dates are found in the active vocabulary. Both 
vocabularies are coded as tries. 
The passive vocabulary is a large dictionary 
containing over 380,000 word forms. The ac- 
tive part is computed ynamically when a new 
sentence is selected by the translator. It relies 
on the fact that a small number of words ac- 
count for most of the tokens in a text. It is 
composed of a few entities (tokens and units) 
that are likely to appear in the translation. In 
practice, we found that keeping 500 words and 
50 units yields good performance. 
3 Implementat ion  
From an implementation point of view, the core 
of TransType relies on a flexible object ori- 
ented architecture, which facilitates the integra- 
tion of any model that can predict units (words 
or sequence of words) from what has been al- 
ready typed and the source text being trans- 
lated. This part is written in C?+.  Statisti- 
cal translation and language models have been 
integrated among others into this architecture 
(Lapalme et al, 2000). 
The graphical user interface is implemented 
in Tcl/Tk, a multi-platform script language well 
suited to interfacing problems. It offers all the 
classical functions for text edition plus a pop-up 
menu which contains the more probable words 
or sequences of words that may complete the 
ongoing translation. The proposed completions 
are updated after each keystroke the translator 
enters. 
4 Evaluation 
We have conducted a theoretical evaluation of 
TransType on a word completion task, which 
assumes that a translator carefully observes 
each completion proposed by the system, and 
accepts it as soon as it is correct. Under 
these optimistic onditions, we have shown that 
TransType allows for the production of a trans- 
lation typing less than a third of its characters. 
In order to better grasp the usefulness of 
TRANSTYPE, we also performed a more prac- 
tical evaluation by asking ten translators to 
use the prototype for about one hour to trans- 
late isolated sentences. We first asked them to 
translate without any help from TRANSTYPE 
and then we compared their typing speed with 
TRANSTYPE suggestions turned on. Overall, 
translators liked the concept and found it very 
useful; they all liked the suggestions although 
it seemed to induce a literal style of transla- 
tion. We also asked them if they thought hat 
TRANSTYPE improved their typing speed and 
the majority of them said so; unfortunately the 
figures showed that none of them did so ... The 
typing rates are nevertheless quite good, given 
that the users were new to this environment and 
this style of looking at suggestions while trans- 
lating. But interestingly this practical ew~lua- 
tion confirmed our theoretical evaluation that a- 
translation can be produced with TRANSTYPE 
by typing less than 40% of the characters of a 
translation. Results of this evaluation and com- 
parisons with our theoretical figures are further 
described in (Foster et al, 2000). 
This experiment made us realize that this 
concept of real-time suggestions depends very 
much on the usability of the prototype; we had 
first developed a much simpler editor but its 
limitations were such that the translators found 
it unusable. So we are convinced that the user- 
interface aspects of this prototype should be 
thoroughly studied. But the TRANSTYPE ap- 
proach would be much more useful if it was 
combined with other text editing tasks related 
to translation: for example TRANSTYPE could 
format the translation in the same way as the 
source text, this would be especially useful for 
titles and tables; it would also be possible to 
localize automatically specific entities such as 
dates, numbers and amounts of money. It would 
also be possible to check that some translations 
given by the user are correct with respect with 
some normative usage of words or terminologi- 
cal coherence; these facilities are already part of 
TRANSCHECK, another computer aided transla- 
tion tool prototype developed in our laboratory 
(Jutras, 2000). 
5 Conc lus ion  
We have presented an innovative way of em- 
bedding machine translation by means of a pro- 
totype which implements an appealing interac- 
tive machine translation scenario where the in- 
teraction is mediated via the target text under 
production. Among other advantages, this ap- 
proach relieves the translator of the burden of 
source analyses, and gives him or her direct con- 
trol over the final translation without having to 
resort to post-edition. 
Acknowledgements  
TRANSTYPE is a project funded by the Natu- 
ral Sciences and Engineering Research Council 
of Canada. We are greatly indebted to Elliott 
Macklovitch and Pierre Isabelle for the fruitful 
orientations they gave to this work. 
References 
Herv6 Blanchon. 1991. Probl~mes de 
d@sambigffisation i teractive et TAO per- 
sonnelle. In L 'environnement Traductionnel, 
50 
Journ@es cientifiques du R@seau th@matique 
de recherche "Lexicologie, terminologie, 
traduction", pages 31-48, Mons, April. 
Ralf D. Brown and Sergei Nirenburg. 1990. 
Human-computer interaction for semantic 
disambiguation. In Proceedings off the Inter- 
national Conference on Computational Lin- 
guistics (COLING), pages 42-47, Helsinki, 
Finland, August. 
Peter F. Brown, Stephen A. Della Pietra, Vin- 
cent Della J. Pietra, and Robert L. Mercer. 
1993. The mathematics of machine transla- 
tion: Parameter estimation. Computational 
Linguistics, 19(2):263-312, June. 
A.-M. Derouault and B. Merialdo. 1986. Nat- 
ural language modeling for phoneme-to-text 
transcription. IEEE Transactions on Pattern 
Analysis and Machine Intelligence (PAMI), 
8 (6): 742-749, November. 
George Foster, Pierre Isabelle, and Pierre Pla- 
mondon. 1997. Target-text Mediated Inter- 
active Machine Translation. Machine Trans- 
lation, 12:175-194. 
George Foster, Philippe Langlais, Guy 
Lapalme, Dominique Letarte, Elliott 
Macklovitch, and S@bastien Sauv@. 2000. 
Evaluation of transtype, a computer-aided 
translation typing system: A comparison of 
a theoretical- and a user- oriented evaluation 
procedures. In Conference on Language 
Resources and Evaluation (LREC), page 8 
pages, Athens, Greece, June. 
Frederick Jelinek. 1990. Self-organized lan- 
guage modeling for speech recognition. In 
A. Waibel and K. Lee, editors, Readings in 
Speech Recognition, pages 450-506. Morgan 
Kaufmann, San Mateo, California. 
Jean-Marc Jutras. 2000. An automatic reviser: 
The TransCheck system. In Applied Natu- 
ral Language Processing 2000, page 10 pages, 
Seattle, Washington, May. 
Martin Kay. 1973. The MIND system. In 
R. Rustin, editor, Natural Language Process- 
ing, pages 155-188. Algorithmics Press, New 
York. 
Roland Kuhn and Renato De Mori. 1990. 
A cache-based natural language model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence 
(PAMI), 12(6):570-583, June. 
Philippe Langlais and George Foster. 2000. Us- 
ing context-dependent i terpolation to com- 
bine statistical language and translation 
models for interactive machine translation. 
In Computer-Assisted Information Retrieval, 
Paris, April. 
Philippe Langlais, George Foster, and Guy 
Lapalme. 2000. Unit completion for a 
computer-aided translation typing system. In 
Applied Natural Language Processing 2000, 
page 10 pages, Seattle, Washington, May. 
Guy Lapalme, George Foster, and Philippe 
Langlais. 2000. La programmation rient@e- 
objet pour le d~veloppement de modules de 
langages. In Christophe Dony and Houari A. 
Sahraoui, editors, LMO'O0 - Langages et 
modules ~ objets, pages 139-147, Mont St- 
Hilaire, Qu@bec, 27 Janvier. Hermes Science. 
Conference invit@e. 
Hiroshi Maruyama nd Hideo Watanabe. 1990. 
An interactive Japanese parser for machine 
translation. In Proceedings of the Interna- 
tional Conference on Computational Linguis- 
tics (COLING), pages 257-262, Helsinki, Fin- 
land, August. 
P. J. Whitelock, M. McGee Wood, B. J. Chan- 
dler, N. Holden, and H. J. Horsfall. 1986. 
Strategies for interactive machine transla- 
tion: the experience and implications of the 
UMIST Japanese project. In Proceedings of 
the International Conference on Computa- 
tional Linguistics (COLING), pages 329-334, 
Bonn, West Germany. 
51 
Generat ing a Controlled Language 
Laurence Danlos Guy Lapalme 
Universitd Paris 7 Ddpartement  d' informatique t RO 
TALANA UFR Linguist ique Universitd de Montrdal 
Case 7003-2, Place Jussieu C.P. 6128, Succ Centre-Vil le 
75251 Paris;' France- " Montreal ;  QuEbec, CCana~dai::~3C 337 
danlos?linguist, jussieu, fr lapalme@iro, umontreal, ca 
Veron ika  Lux  * 
Xerox Research Centre Europe 
6, chemin de Maupertu is  
38240 Meylan, France 
Veron ika .  Lux@xrce. xerox ,  com 
Abst rac t  
This paper argues for looking at Controlled Lan- 
guages (CL) from a Natural Language Genera- 
tion (NLG) perspective. We show that CLs are 
used in a normative nvironment in which dif- 
ferent textual modules can be identified, each 
having its own set of rules constraining the text. 
These rules can be used as a basis for natural 
language generation. These ideas were tested in 
a proof of concept generator for the domain of 
aircraft maintenance manuals. 
1 What  is a Cont ro l led  Language? 
Controlled Languages (CLs) result from a grow- 
ing concern about technical documentation 
quality and translation, be it human or auto- 
matic. A CL consists of a glossary and of writ- 
ing rules for the linguistic aspect of the doc- 
umentation. These rules are given as recom- 
mendations or prohibitions for both the lexicon 
and the grammar. Currently, most CLs are 
varieties of "controlled English" which derive 
froth the Caterpillar Tractor Company Funda- 
veloped for CL users, the best known being con- 
formity checkers/controllers such as AlethCL or 
SECC (CLA, 1996). 
A writer expects that the checking tool should 
not only detect errors but also propose a CL 
conformable xpression. A. Nasr (Nasr, 1996), 
who worked on the problem of CL refornmla- 
tion, underlines the difficulties of this task. Re- 
formulation cannot make any hypotheses about 
the conformity of the input sentences, and 
therefore must deal with a wider variety of 
lexico-syntactical constructions than those al- 
lowed in a CL. Some instances of noncompliance 
are relatively easy to detect but much more dif- 
ficult to correct: for example, sentences that are 
longer than the prescribed number of words. 
So there is little hope that human writers will 
ever produce documentation complying strictly 
with a CL even with the help of a conformity 
checker. We argue that it may be more promis- 
ing to use NLG technology for generating doc- 
.umentation in. CL instead of analyzing it af- 
terwards, as it is the case with a conformity 
checker. Few researchers have looked at CLs 
mental English tha t vi~..elab0rated.in the S~: ...... from-:a~-~generation p int  of view.. (Nasr, 1996; 
ties (Scheursand Adriaens, 1992). However CLs Hartley and Paris, 1996); but we think that 
are presently being defined for German, Swedish 
and French. 
Technical writers find it difficult to comply 
with the writing rules of a CL which are often 
hard to justify (CLA. 1996). For them, a CL is 
seen as an additional constraint on an already 
complex task. This is why tools have been de- 
" Work done while at the Adrospatiale Research Center 
there are very compelling reasons for taking a 
generation perspective, in addition to the ad- 
vantages of NLG for CLs that will be presented 
in section 3: 
* As CLs can be viewed as linguistic specifi- 
cations for human beings, it seems natural 
"to, .consider them 'a:s specifica'tkms for the 
linguistic component of an NLG system. 
141 
e CL writing specifications come on top of 
other writing norms which deal with docu- 
ment structuring. For example, in the aero- 
nautical industry, CLs such Simplified En- 
glish (SE) (AEC, 1995) and Fran~ais Ra- 
tionalisd (FR) (GIFAS, 1996) extend the 
ATA 100 norms (Bur, 1995) which describe 
the divisionof the document into chapl:ers, 
sections, subsections, etc. reflecting a tree- 
structured functional organization of the 
airplane: a chapter corresponds to a sys- 
tem (e.g. main rotor), a section to a sub- 
system (e.g. gear box), a subsection to a 
sub-sub-system (e.g. set of gears), and so 
on. Over this thematic structure is added a 
communicative structure to fulfill two main 
goals: describe all systems of the airplane 
and prescribe all maintenance instructions 
for the airplane. The norms of the ATA 
can be viewed as specifications for the text 
structuring component of an NLG system. 
? The thematic and communicative structur- 
ing of the document must also conform 
to a systematic non-linear page number- 
ing system and strict formatting rules us- 
ing SGML tags. These constraints can be 
viewed as specifications for the layout com- 
ponent of an NLG system. 
So we claim that CLs should not be con- 
sidered outside the context of the production 
of complex structured ocuments, which natu- 
rally raises the question of the automatic gen- 
eration of this documentation given some for- 
real representation. This claim led V. Lux (Lux, 
1998) to redefine the notion of a CL. Her study 
has shown that only a few syntactic constraints 
(e.g. coordination constraints) are applicable to 
the whole document. Most constraints are only 
valid for sub-parts of the document, identified 
as "textual modules". Each textual module has 
a particular communicative goal  and a precise 
theme according to the ATA 100 norms. It can 
be divided into smaller modules: for example, 
the Task module is divided into simpler Sub- 
Task modules which are themselves composed 
of simpler Instructions modules. From a lin- 
guistic point of view, a textual module uses only 
a controlled sublanguage. V. Lux thus extended 
FR to a new CL .called.:RREM (.Fr.aa~gais Ra- 
tionalise'. Etendu Modulaire) comprising many 
CLs, each having its own syntactic rules for 
a specific textual module. She also performed 
a corpus study showing that the same textual 
modules could be identified for both French and 
English. It should thus be possible to remodu- 
larize SE similarly to what has been done to 
FR with FREM. In this paper, we therefore 
in t roduce  the: not ion of aii Extei ided Modular 
Controlled Language (EMCL) which first de- 
fines some general rules and then some more 
specific ones for each textual module. We now 
look at the problem of automatical ly generat- 
ing technical documentation complying both to 
structuration orms such as ATA 100 and to the 
rules of an EMCL. 
2 How to  generate  techn ica l  
documentat ion?  
We assume that a generation system can be di- 
vided into a What to say and How to say it 
components, even though this may be consid- 
ered as a gross simplification. 
2.1 What  to say component  
The main difficulty for NLG in a real environ- 
ment lies in knowledge modeling. For aircraft 
maintenance manuals, existing ontologies could 
probably be reused, but even then the model- 
ing efforts required are huge. Nevertheless, we 
assume that it is possible to design forms which 
are sequentially presented to the user to be 
filled, as in Drafter (Paris et al, 1995), through 
which the technical writer provides the infor- 
mation to convey in an appropriate fornlalism. 
These forms can be derived directly fi'om the 
tree-like structure of the document given in the 
ATA norms. The goal is that, once the writer 
has finished filling in these forms, the technical 
docunmntation is already properly structured in 
an abstract language instead of a natural one. 
In a general text generation setting, using forms 
to describe What is to be said might seem like 
a difficult task; but in the context of techni- 
cal writing, the informational content is almost 
already prescribed and forms are thus a sin> 
ple way of complying with the rules of a CL. 
Indeed in the now comlnon web enviromnents, 
forms are frequently used for eliciting informa- 
tion from users. This input can then be pro- 
cessed by the "tIow to say it and layout compo- 
nents. 
142 
The writers who find it very difficult to com- 
ply with the rules of a CL have no problem 
complying with the ATA 100 norms, thereby 
producing documents with the right thematic 
and communicative structuration. This can be 
seen as an illustration of observations made in - 
However, many writing rules in a CL place 
particular syntactic constraints on the use of, 
a given lexical item, e.g. in FR a rule forbids 
the use of emp~cher (prevent) when followed by 
an infinitive clause. To handle such numerous 
lexically dependent syntactic rules, a formal- 
psycholinguistics. 
describes a model of the speaker's activity in 
which choices in the What to say component 
are conscious, while choices in the How to say it 
component are automatic. This model helps un- 
derstand some of the difficulties that CL users 
face. A CL forces the writer to become con- 
scious of behavioral mechanisms that are usu- 
ally automatic; The writer is thus distracted 
from choices made earlier in her/his writing 
task. So s/he often ends up writing it in the  
way  it has  to be written but does not write ex- 
actly what  had to be written, thus defeating 
the whole purpose of a CL which was meant to 
produce a better expression of the information. 
This model also explains why a human writer 
has less difficulties following the ATA norms: 
this part of the job corresponds to conscious 
choices. In the NLG scenario, this is replaced 
by filling in some information in the forms that 
are presented. 
To sum up, the What to say component re- 
quires a modelization of the domain model and 
the design of a series of forms to be filled. A 
human writer using the NLG system has to fill 
forms but on the other hand, s/he does not have 
to learn a CL, since compliance with the CL 
norms is taken care by the How to say it com- 
ponent which we now describe. 
2.2 How to say it component  
In this section, it is assumed that if a CL is 
in fact an EMCL such as FREM, a specific How 
to say it component is designed for each textual 
module, but always retaining the same formal- 
ism. 
The lexicon used in the How to s~zyit corn- . . . .  
ponent should be exactly the one enforced by 
the CL. Similarly, the syntactic constructions 
and the discourse structures of this component 
should correspond to the set of allowed con- 
structions / structures in the CL. This can sim- 
plify some lexical, syntactic and even discourse 
choices to be made within the generation sys- 
tern and thus ensure that .the gener~ed text 
complies with the rules of the CL. 
Levelt (Levelt , 1989, p. 9): ism based on a lexicalized grammax:is needed. 
We chose Lexicalized Tree Adjoining Grammar 
(LTAG) for the following reasons: 
* A text generation formalism inspired from 
LTAG, called G-TAG, has been designed, 
implemented and used in several applica- 
tions (Danlos and Meunier, 1996; Meunier, 
1997; Danlos, 1998; Meunier and Danlos, 
1998; Danlos, 2000). G-TAG takes as in- 
put an event graph which can be provided 
by the user  by filling in some forms which 
ensure that all the necessary information 
for generation is provided. 
o G-TAG deals with textual phenomena such 
as sentence connectors by extending LTAG 
to handle discourse comprised of more than 
one sentence. One of the major innovations 
of FREM compared to FR (and of EMCL 
compared to CL) is to implement rules for 
connecting sentences (clauses). The way to 
connect sentences has largely been ignored 
in CLs, although this linguistic issue raises 
ambiguities which can lead to maintenance 
errors. For example, simple juxtaposition 
of sentences i allowed in FR but disallowed 
in FREM because it is highly dangerous. A 
technician reading Nettoyer X.  Verser Y 
sur X.  (Clean X. Pour Y on X.) could in- 
terpret this to mean either "Clean X with 
Y" or "Clean X with Z, and next pour 
Y on X". Only one of these operations is 
right, the other one may lead to a mainte- 
nance error. On the other hand, traditional 
syntactical ambiguities uch as a preposi- 
.... tional attaehment...will-.not, usually lead to 
maintenance rrors because the technician 
can usually solve them on the basis of some 
domain knowledge. 
o The lexicalized grammar in G-TAG is com- 
piled from the recta-grammar designed and 
implemented by M.H. Candito (Candito. 
1996). This makes it easy to follow the 
evolution ofru les  of an (EM)CL. For ex- 
ample, if the rule to write an Instruction 
143 
changes from "Put a verb in the infini- 
tive" to "Insert an imperative", then this 
must be changed everywhere in the lexi- 
calized grammar. Using the metagrammar 
we can achieve this quite easily because of 
the hierarchical organization of a LTAG: 
with only one rule, an imperative can be 
allowed and an-infinitive ~disallowed ( in a 
main clause) for every verb, whatever its 
argument structure and syntactic onstruc- 
tion. 
G-TAG thus seems a good candidate for pro- 
ducing technical documentation complying with 
the constraints of an (EM)CL. A technical doc- 
umentation generator prototype in the aeronau- 
tical domain is described in Section 4. It is writ- 
ten in Flaubert, an implementation f G-TAG 
(Danlos and Meunier, 1996). The How to say 
it component would have to be completed by 
adding a layout component complying with the 
norms of ATA 100. We should also provide re- 
vision tools to allow the writer to fine tune the 
final text. 
So, automatically generating technical docu- 
mentation seems technically possible provided 
the technical writer is willing to fill forms which 
in principle should be less demanding than 
learning the rules of an (EM)CL. This approach 
also has other advantages, described in the next 
section. 
3 Advantages  o f  automat ic  
generat ion  o f  techn ica l  
documentat ion  
3.1 Mult i l ingual i ty  
One of tile major assets of NLG is its capacity 
to simultaneously generate texts in several lan- 
guages, and to regenerate updates as often as 
necessary, using a single input representation, 
thus ensuring coherence among the generated 
texts. 
Until now, CLs-have .dealt-withr muttitingual- 
ity by means of the translation hypothesis. It 
is for this reason that FR was developed by 
adapting SE, in order to ease the translation 
from French to English. FR authors try to en- 
sure that everything that can also be written 
in FR can be translated into SE. From this 
point of view, the definition of a source CLt, 
depends on the. defini.tion:.of, a tin:get CL2. De- 
velopers of CL1 are more likely to select struc- 
tures which can be easily or even literally trans- 
lated into CL2. What then happens if CLt and 
CL2 are structurally different? This can lead 
to a situation where CL1 imposes a cumber- 
some writing style that contravene conventions 
shared by native speakers of Li, thereby con- 
tradicting CLs' aim of enhancing understand- 
ability. Rules 0f-aii (EM)CL should be elabo- 
rated without such multilingual considerations. 
Their definition should principally pay atten- 
tion to the characteristics of one language, try- 
ing to avoid typical ambiguities. Such criteria 
are difficult enough to deal within a single lan- 
guage without taking translation problems into 
account. 
Now if we consider multilingual generation i
(EM)CLs, we find that there are major benefits 
from the multilingualism odeling proposed by 
NLG. In particular, defining a common repre- 
sentation is possible since the structure of the 
documentation is language independent. Recall 
from section 1 that the thematic structure of the 
documentation in the aeronautical domain must 
reflect the functional decomposition of the air- 
plane and that the same textual modules can be 
identified in many languages. Thus nothing has 
to be changed in the What to say component 
(Section 2.1) going from one language to the 
other. Only the How to say it component (Sec- 
tion 2.2) need be adapted to the target (EM)CL 
which should be monolingually defined. 
3.2 NLCI as an aid for test ing and 
developing a CL 
An NLG system can provide concrete assistance 
for the testing and for tile development of a CL. 
An NLG system that integrates the CL con- 
straints can help discover contradictions in the 
CL definition. As an illustration, a major dif- 
ficulty in CL definition concerns the coherence 
between the lexicon and the writing rules, as il- 
lustrated by (Emorine, 1994) with the following 
example: 
o Emp~cher l'oxyg~ne de s'accumuler (Pre- 
vent the oxygen from accumulating) does 
not conform to a FR lexically depen- 
dent syntactic rule, according to which 
empdcher (prevent) should not be followed 
by an infinitive clause. 
....... ~ .~ Emp~cher I ~uccumulation ' d.'~ozyg~ne ? (Pre- 
vent oxygen accumulat ion) does not con- 
144 
agent object 
U0 O5 
Titled sub-task 
title :Sub-Task  
DISPOSER 
lst-inst 
I 
Precond-Inst 
lst-order pre-cond 
I I 
DISPOSER ENLEVER 
agent object agent object 
I I I I 
Uo 04 Uo Ingo 
2nd-inst 
I 
Simul-Inst 
lst-order 2nd-order 
I I 
EXTRAIRE DEPOSER 
agent object agent object 
I I I I 
U0 O2 u0 Oa 
3rd-inst 
I 
Inst 
I 
lst-order 
I 
DEPOSER 
agent object tool 
I I I 
u0 o5 To 
Figure 1: Event graph given as input to Flaubert. In the prototype, this information is entered in 
textual form. 
form to FR lexicon, according to which the 
verb s'accumuler (accumulate) should be 
used instead of the noun accumulation (ac- 
cumulation) 
Emp~cher que l'oxyg~ne ne s'accumule 
(Prevent that the oxygen accumulates) 
does not conform to the writing rule that 
forbids the use of the subjunctive mode. 
So we come to a dead end if we want to use the " 
verb empdcher (prevent). This problem can be 
detected automatically by the NLG system.and 
an appropriate fix be made in the grammar. 
NLG can be used for checking a CL, which 
is helpful even if the CL is intended for a hu- 
man writer because it may avoid the discovery 
of various cases of incoherence by the writer. If 
tile writers can justify their writing difficulties 
by pointing out inconsistencies in the CL defini- 
tion, they won't be motivated to use what they 
will tend to consider'as an~-abmird invention, by .... 
people who understand nothing about the .job. 
NLG can also help strengthen CLs' claim to 
lead to more homogeneous texts, which is equiv- 
alent to forbidding certain paraphrases. NLG 
precisely deals with paraphrase as, for some in- 
puts, a NLG system will produce several texts. 
In this way, NLG helps identify which para- 
phrases till remain possible in the CL. In prac- 
tice, when an NLG system proposes several 
texts for one input, it raises the question for 
the CL developer: Should a constraint be added 
to the CL definition in order to forbid some of 
these texts ? 
4 P roo f  o f  concept  generator  
The previous ections have argued for the inter- 
est of dealing with CL from all NLG perspec- 
tive which to our knowledge had never been ex- 
amined ill such details. To further pursue, V. 
Lux (Lux, 1998) has developed a proof of con- 
cept generator using Flaubert (Meunier, 1997; 
Meunier-and "Danlos., :1998)" ~o"gee ? ?howthese "
theoretical concerns could be applied in prac- 
145 
Sous-t~che 60-007 
3.1 DEpose du segment d'arr~t (5) 
- Apr~s avoir enlev~ le mastic PR, d~poser le segment d'arr~t (4). 
- Extraire le porte joint (2) et d~poser le joint (3). 
- D@oser le segment d'arr~t (5) g l'aide de l'outillage (Z). 
: ..-~ I~igure~2:~::-Text~gffaera~ed~:by.~-Elu~aber~t.,-from.the~:input of~Figure-1 . . . . . . . . . .  
tice. The generator can produce text for about 
ten subtasks in FREM. These tasks comprise 
from two to eleven instructions, illustrating ten 
different instruction types such as: simple in- 
struction with a goal, simple instruction with a 
condition, complex instruction with simultane- 
ous actions, etc. They involve the use of various 
syntactical constructions uch as infinitive or 
sentential subordinates, nominalisation, nega- 
tion, etc. 
Input to the prototype are event graphs such 
as the one given in Figure 1. The output is a 
well formed French text such as the one in Fig- 
ure 2 which was generated from Figure 1. In 
Lux's prototype, the event graphs were hand 
coded, but now Flaubert has been rewritten 
in CLEF (Meunier, 1999; Meunier and Reyes, 
1999), which has a better graphical input mech- 
anism that would have eased the input process. 
The output text is a sub-task including a ti- 
tle and instructions of different ypes (only the 
first three instructions are given in the Figures) 
to be performed by the same person (e.g. U0). 
FREM defines which connector to use for each 
instruction type (e.g. conjunction et for an in- 
struction with simultaneous actions). 
The generation of noun groups for the ob- 
jects (Oi), ingredients ( Ingi)  and tools (Ti) re- 
lies on a mapping table between these labels 
and their denominations; this was a temporary 
solution for problems outside the scope of the 
prototype. We should have relied on existing 
nomenclatures for tools'andi'ngredients; and on  
the fact that objects are systematically repre- 
sented in drawings associated with various sub- 
tasks e.g. O5, called segment d'arr~t, is labeled 
(5) on the drawing associated with the exam- 
ple above. In a graphical interface nvironment, 
authors would select these objects linked to a 
controlled terminology data base. 
This proof of,concept ,ge~erator .ser-~ed :, well ..... 
our purpose of testing our theoretical ideas but 
unfortunately it could not be evaluated in a re- 
alistic CL text production environment. Our 
sponsors were very interested in the results we 
have produced but changes in their organisation 
made it impossible to carry further investiga- 
tions. We intend to further pursue our research 
and use the new implementation of Flaubert to 
generate controlled language in an other area 
of application while keeping the concept of an 
extended modular CL. 
5 Conc lus ion  
This paper has argued that linguistic norms im- 
posed by CLs should not be considered in iso- 
lation. They are only a part of a set of more 
comprehensive norms on the document struc- 
ture and layout. This insight led us to define a 
notion of textual modules, each with its own 
linguistic norms, and to envisage the genera- 
tion of technical documentation using an ex- 
tended modular controlled language (EMCL). 
Norms for document structure such as ATA100, 
its linguistic characteristics and its layout re- 
quirements may be seen to respectively define 
the text structuring, the linguistic and the lay- 
out components of an NLG system. 
We have also shown that a generation point of 
view can help refine tile definition of an EMCL. 
The EMCL can be defined monolingually, mul- 
tilinguality being obtained through NLG. These 
ideas were tested within a proof of concept ext 
generator, in thedomain  of' aircraftmaintenance 
manuals. 
Acknowledgment  
We thank our former colleagues at Aerospa- 
tiale Research Center and Frdd~ric Meunier 
who implemented Flaubert. We also thank EI- 
-liottqkffacMowitch,-who suggestedmany improve- " " 
ments to the paper. 
146 
References 
AECMA Document PSC-85-16598, 1995. Sim- 
plified English Standard, a guide for the 
preparation of Aircraft Maintenance Doc- 
umentation in the International Aerospace 
Maintenance Language. 
? Bureau de Normatisation :de l~,A~ronaufique 
et de l'Espace (BNAE), Issy-les-Moulineaux, 
1995. Spdcification ATA no 100, traduc- 
tion frangaise. Specification for Manufactur- 
er's Technical Data - ATA Specification 10, 
October. 
M.-H. Candito. 1996. A principle-based hierar- 
chical representation f LTAGs. In Proceed- 
ings of the 16th International Conference on 
Computational Linguistics, pages 194-199, 
Copenhagen. 
CLAW. 1996. Proceedings of the First Interna- 
tional Workshop on Controlled Language Ap- 
plications (CLAW), Leuven. 
L. Danlos and F. Meunier. 1996. G-TAG, 
un formalisme pour la gEnEration de texte : 
presentation et applications industrielles. In 
Actes du colloque Informatique t Langue Na- 
turelle, Nantes. 
L. Danlos. 1998. G-TAG: un formalisme lex- 
icalis~ de gdn~ration de textes inspire de 
TAG. Traitement Automatique des Langues 
- T.A.L., 39(2):4-32. 
L. Danlos, 2000. Tag Grammars, chapter G- 
TAG: A Lexicalized Formalism for Text Gen- 
eration inspired by Tree Adjoining Grammar. 
CSLI. 
M. Emorine. 1994. Projet de recherche sur la 
mod61isation des entr6es verbales du fran~ais 
rationalis6. Technical report, Universit6 de 
Clermont II. 
GIFAS. 1996. Guide du r6dacteur - partie 2: 
Fran~ais rationalis6. Technical report, GI- 
FAS, Paris. 
A. Hartley and C. Paris, 1996. Le tezte 
procddural : langage, action et cognition, 
chapter Une analyse fonctionnelle de textes 
proc6duraux : apport de la g6n6ration au- 
tomatique ~ la d6finition des langues ratio- 
nalis6es, pages 211-222. Toulouse. 
W. Levelt. 1989. Speaking -.h'om intention 
to articulation. MIT Press. Cambridge Mas- 
sachuset ts. 
V. Lux. 1998. Elaboration d'unffangais ratio- 
nalisd dtendu pour un manuel de maintenance 
adronautique, test en gdndration automatique. 
Th~se de doctorat en linguistique, Universitd 
Paris 7. 
F. Meunier and L. Danlos. 1998. FLAUBERT: 
an user-friendly system for multilingual text 
generation. In Proceedings of the 9th Interna- 
tianal. Workshop. on Natural Language Gener- 
ation (INLG'98), pages 284-287, Niagara-on- 
the-Lake. 
F. Meunier and R. Reyes. 1999. Plate-forme de 
ddveloppement de gdn~rateurs multilingues. 
In Actes de la confdrence de Gdndration Au- 
tomatique de Texte CAT'99, pages 145-155, 
Grenoble, France. 
F. Meunier. 1997. Impldmentation de G-TAG, 
formalisme pour la gdndration inspirde des 
grammaires d'arbres adjoints. Th~se de doc- 
torat en informatique, Universitd Paris 7. 
F. Meunier. 1999. Mod~lisation des ressources 
linguistiques d'une application industrielle. 
In TALN'99, pages 243-252, Carg~se, Corse, 
12-17 juillet. 
A. Nasr. 1996. Un module de reformulation au- 
tomatique fondd sur la thdorie Sens-Texte - 
application aux Langues Controldes. Ph.D. 
thesis, Universit~ Paris 7. 
C. Paris, K. Vander Linden, M. Fischer, 
A. Hartley, L. Pemberton, R. Power, and 
D. Scott. 1995. A support tool for writ- 
ing multilingaul instructions. In Proceedings 
of the 14th International Joint Conference 
on Artificial Intelligence (IJCAI'95), pages 
1398-1404, MontrEal. 
J. Scheurs and G. Adriaens, 1992. Comput- 
ers and writing - state of the art, chapter 
From cogram to alcogram : toward a con- 
trolled english grammar checker, pages 206- 
221. Kluwer Academic Publishers, London. 
147 
User-Friendly Text Prediction for Translators
George Foster and Philippe Langlais and Guy Lapalme
RALI, Universite? de Montre?al
{foster,felipe,lapalme}@iro.umontreal.ca
Abstract
Text prediction is a form of interactive
machine translation that is well suited to
skilled translators. In principle it can as-
sist in the production of a target text with
minimal disruption to a translator?s nor-
mal routine. However, recent evaluations
of a prototype prediction system showed
that it significantly decreased the produc-
tivity of most translators who used it. In
this paper, we analyze the reasons for this
and propose a solution which consists in
seeking predictions that maximize the ex-
pected benefit to the translator, rather than
just trying to anticipate some amount of
upcoming text. Using a model of a ?typ-
ical translator? constructed from data col-
lected in the evaluations of the prediction
prototype, we show that this approach has
the potential to turn text prediction into a
help rather than a hindrance to a translator.
1 Introduction
The idea of using text prediction as a tool for trans-
lators was first introduced by Church and Hovy as
one of many possible applications for ?crummy?
machine translation technology (Church and Hovy,
1993). Text prediction can be seen as a form of in-
teractive MT that is well suited to skilled transla-
tors. Compared to the traditional form of IMT based
on Kay?s original work (Kay, 1973)?in which the
user?s role is to help disambiguate the source text?
prediction is less obtrusive and more natural, allow-
ing the translator to focus on and directly control the
contents of the target text. Predictions can benefit
a translator in several ways: by accelerating typing,
by suggesting translations, and by serving as an im-
plicit check against errors.
The first implementation of a predictive tool for
translators was described in (Foster et al, 1997), in
the form of a simple word-completion system based
on statistical models. Various enhancements to this
were carried out as part of the TransType project
(Langlais et al, 2000), including the addition of a re-
alistic user interface, better models, and the capabil-
ity of predicting multi-word lexical units. In the fi-
nal TransType prototype for English to French trans-
lation, the translator is presented with a short pop-
up menu of predictions after each character typed.
These may be incorporated into the text with a spe-
cial command or rejected by continuing to type nor-
mally.
Although TransType is capable of correctly antic-
ipating over 70% of the characters in a freely-typed
translation (within the domain of its training cor-
pus), this does not mean that users can translate in
70% less time when using the tool. In fact, in a trial
with skilled translators, the users? rate of text pro-
duction declined by an average of 17% as a result
of using TransType (Langlais et al, 2002). There
are two main reasons for this. First, it takes time to
read the system?s proposals, so that in cases where
they are wrong or too short, the net effect will be to
slow the translator down. Second, translators do not
always act ?rationally? when confronted with a pro-
posal; that is, they do not always accept correct pro-
posals and they occasionally accept incorrect ones.
Many of the former cases correspond to translators
simply ignoring proposals altogether, which is un-
derstandable behaviour given the first point.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 148-155.
                         Proceedings of the Conference on Empirical Methods in Natural
This paper describes a new approach to text pre-
diction intended to address these problems. The
main idea is to make predictions that maximize the
expected benefit to the user in each context, rather
than systematically proposing a fixed amount of text
after each character typed. The expected benefit is
estimated from two components: a statistical trans-
lation model that gives the probability that a can-
didate prediction will be correct or incorrect, and a
user model that determines the benefit to the trans-
lator in either case. The user model takes into ac-
count the cost of reading a proposal, as well as the
random nature of the decision to accept it or not.
This approach can be characterized as making fewer
but better predictions: in general, predictions will
be longer in contexts where the translation model is
confident, shorter where it is less so, and absent in
contexts where it is very uncertain.
Other novel aspects of the work we describe here
are the use of a more accurate statistical translation
model than has previously been employed for text
prediction, and the use of a decoder to generate pre-
dictions of arbitrary length, rather than just single
words or lexicalized units as in the TransType pro-
totype. The translation model is based on the max-
imum entropy principle and is designed specifically
for this application.
To evaluate our approach to prediction, we simu-
lated the actions of a translator over a large corpus of
previously-translated text. The result is an increase
of over 10% in translator productivity when using
the predictive tool. This is a considerable improve-
ment over the -17% observed in the TransType trials.
2 The Text Prediction Task
In the basic prediction task, the input to the predictor
is a source sentence s and a prefix h of its translation
(ie, the target text before the current cursor position);
the output is a proposed extension x to h. Figure 1
gives an example. Unlike the TransType prototype,
which proposes a set of single-word (or single-unit)
suggestions, we assume that each prediction consists
of only a single proposal, but one that may span an
arbitrary number of words.
As described above, the goal of the predictor is
to find the prediction x? that maximizes the expected
s: Let us return to serious matters.
t:
h
? ?? ?
On va r
x?
? ?? ?
evenir aux choses se?rieuses.
x: evenir a`
Figure 1: Example of a prediction for English to
French translation. s is the source sentence, h is the
part of its translation that has already been typed,
x? is what the translator wants to type, and x is the
prediction.
benefit to the user:
x? = argmax
x
B(x,h, s), (1)
where B(x,h, s) measures typing time saved. This
obviously depends on how much of x is correct, and
how long it would take to edit it into the desired text.
A major simplifying assumption we make is that the
user edits only by erasing wrong characters from the
end of a proposal. Given a TransType-style interface
where acceptance places the cursor at the end of a
proposal, this is the most common editing method,
and it gives a conservative estimate of the cost at-
tainable by other methods. With this assumption,
the key determinant of edit cost is the length of the
correct prefix of x, so the expected benefit can be
written as:
B(x,h, s) =
l?
k=0
p(k|x,h, s)B(x,h, s, k), (2)
where p(k|x,h, s) is the probability that exactly k
characters from the beginning of x will be correct,
l is the length of x, and B(x,h, s, k) is the benefit
to the user given that the first k characters of x are
correct.
Equations (1) and (2) define three main problems:
estimating the prefix probabilities p(k|x,h, s), esti-
mating the user benefit function B(x,h, s, k), and
searching for x?. The following three sections de-
scribe our solutions to these.
3 Translation Model
The correct-prefix probabilities p(k|x,h, s) are
derived from a word-based statistical translation
model. The first step in the derivation is to con-
vert these into a form that deals explicitly with char-
acter strings. This is accomplished by noting that
p(k|x,h, s) is the probability that the first k charac-
ters of x are correct and that the k + 1th character
(if there is one) is incorrect. For k < l:
p(k|x,h, s) = p(xk1|h, s)? p(x
k+1
1 |h, s)
where xk1 = x1 . . . xk. If k = l, p(k|x,h, s) =
p(x|h, s). Also, p(x01) ? 1.
The next step is to convert string probabilities
into word probabilities. To do this, we assume
that strings map one-to-one into token sequences, so
that:
p(xk1|h, s) ? p(v1, w2, . . . , wm?1, um|h, s),
where v1 is a possibly-empty word suffix, each wi is
a complete word, and um is a possibly empty word
prefix. For example, if x in figure 1 were evenir aux
choses, then x141 would map to v1 = evenir, w2 =
aux, and u3 = cho. The one-to-one assumption is
reasonable given that entries in our lexicon contain
neither whitespace nor internal punctuation.
To model word-sequence probabilities, we apply
the chain rule:
p(v1, w2, . . . , wm?1, um|h, s) =
p(v1|h, s)
m?1?
i=2
p(wi|h, v1, w
i?1
2 , s)?
p(um|h, v1, w
m?1
2 , s). (3)
The probabilities of v1 and um can be expressed in
terms of word probabilities as follows. Letting u1
be the prefix of the word that ends in v1 (eg, r in
figure 1), w1 = u1v1, and h = h?u1:
p(v1|h, s) = p(w1|h?, s)/
?
w:w=u1v
p(w|h?, s),
where the sum is over all words that start with u1.
Similarly:
p(um|h?, w
m?1
1 , s) =
?
w:w=umv
p(w|h?, wm?11 , s). (4)
Thus all factors in (3) can be calculated from
probabilities of the form p(w|h, s) which give the
likelihood that a word w will follow a previous se-
quence of words h in the translation of s.1 This is
the family of distributions we have concentrated on
modeling.
Our model for p(w|h, s) is a log-linear combina-
tion of a trigram language model for p(w|h) and a
maximum-entropy translation model for p(w|s), de-
scribed in (Foster, 2000a; Foster, 2000b). The trans-
lation component is an analog of the IBM model 2
(Brown et al, 1993), with parameters that are op-
timized for use with the trigram. The combined
model is shown in (Foster, 2000a) to have signif-
icantly lower test corpus perplexity than the linear
combination of a trigram and IBM 2 used in the
TransType experiments (Langlais et al, 2002). Both
models supportO(mJV 3) Viterbi-style searches for
the most likely sequence of m words that follows h,
where J is the number of tokens in s and V is the
size of the target-language vocabulary.
Compared to an equivalent noisy-channel combi-
nation of the form p(t)p(s|t), where t is the tar-
get sentence, our model is faster but less accurate.
It is faster because the search problem for noisy-
channel models is NP-complete (Knight, 1999), and
even the fastest dynamic-programming heuristics
used in statistical MT (Niessen et al, 1998; Till-
mann and Ney, 2000), are polynomial in J?for in-
stance O(mJ4V 3) in (Tillmann and Ney, 2000). It
is less accurate because it ignores the alignment rela-
tion between s and h, which is captured by even the
simplest noisy-channel models. Our model is there-
fore suitable for making predictions in real time, but
not for establishing complete translations unassisted
by a human.
3.1 Implementation
The most expensive part of the calculation in equa-
tion (3) is the sum in (4) over all words in the vo-
cabulary, which according to (2) must be carried out
for every character position k in a given prediction
x. We reduce the cost of this by performing sums
only at the end of each sequence of complete tokens
in x (eg, after revenir and revenir aux in the above
example). At these points, probabilities for all pos-
sible prefixes of the next word are calculated in a
1Here we ignore the distinction between previous words that
have been sanctioned by the translator and those that are hy-
pothesized as part of the current prediction.
single recursive pass over the vocabulary and stored
in a trie for later access.
In addition to the exact calculation, we also ex-
perimented with establishing exact probabilities via
p(w|h, s) only at the end of each token in x, and as-
suming that the probabilities of the intervening char-
acters vary linearly between these points. As a re-
sult of this assumption, p(k|x,h, s) = p(xk1|h, s)?
p(xk+11 |h, s) is constant for all k between the end of
one word and the next, and therefore can be factored
out of the sum in equation (2) between these points.
4 User Model
The purpose of the user model is to determine the
expected benefit B(x,h, s, k) to the translator of a
prediction x whose first k characters match the text
that the translator wishes to type. This will depend
on whether the translator decides to accept or reject
the prediction, so the first step in our model is the
following expansion:
B(x,h, s, k) =
?
a?{0,1}
p(a|x,h, s, k)B(x,h, s, k, a),
where p(a|x,h, s, k) is the probability that the trans-
lator accepts or rejects x, B(x,h, s, k, a) is the ben-
efit they derive from doing so, and a is a random
variable that takes on the values 1 for acceptance and
0 for rejection. The first two quantities are the main
elements in the user model, and are described in fol-
lowing sections. The parameters of both were esti-
mated from data collected during the TransType trial
described in (Langlais et al, 2002), which involved
nine accomplished translators using a prototype pre-
diction tool for approximately half an hour each. In
all cases, estimates were made by pooling the data
for all nine translators.
4.1 Acceptance Probability
Ideally, a model for p(a|x,h, s, k) would take into
account whether the user actually reads the proposal
before accepting or rejecting it, eg:
p(a|x,h, s, k) =
?
r?{0,1}
p(a|r,x,h, s, k)p(r|x,h, s, k)
where r is a boolean ?read? variable. However, this
information is hard to extract reliably from the avail-
able data; and even if were obtainable, many of the
?60 ?50 ?40 ?30 ?20 ?10 0 10 20 30 40 50 600
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
pro
ba
bili
ty 
of 
ac
ce
pti
ng
gain (length of correct prefix ? length of incorrect suffix)
raw
smoothed
model
Figure 2: Probability that a prediction will be ac-
cepted versus its gain.
factors which influence whether a user is likely to
read a proposal?such as a record of how many pre-
vious predictions have been accepted?are not avail-
able to the predictor in our formulation. We thus
model p(a|x,h, s, k) directly.
Our model is based on the assumption that the
probability of accepting x depends only on what the
user stands to gain from it, defined according to the
editing scenario given in section 2 as the amount by
which the length of the correct prefix of x exceeds
the length of the incorrect suffix:
p(a|x,h, s, k) ? p(a|2k ? l),
where k?(l?k) = 2k? l is called the gain. For in-
stance, the gain for the prediction in figure 1 would
be 2? 7? 8 = 6. The strongest part of this assump-
tion is dropping the dependence on h, because there
is some evidence from the data that users are more
likely to accept at the beginnings of words. How-
ever, this does not appear to have a severe effect on
the quality of the model.
Figure 2 shows empirical estimates of p(a =
1|2k? l) from the TransType data. There is a certain
amount of noise intrinsic to the estimation proce-
dure, since it is difficult to determine x?, and there-
fore k, reliably from the data in some cases (when
the user is editing the text heavily). Nonetheless, it
is apparent from the plot that gain is a useful abstrac-
0 10 20 30 40 50 600
500
1000
1500
2000
2500
3000
3500
4000
av
er
ag
e t
ime
 to
 ac
cep
t (m
sec
s)
length of proposal (chars)
raw
least?squares fit
0 10 20 30 40 50 600
500
1000
1500
2000
2500
3000
3500
4000
av
er
ag
e t
ime
 to
 re
ject
 (m
sec
s)
length of proposal (chars)
raw
least?squares fit
Figure 3: Time to read and accept or reject proposals versus their length
tion, because the empirical probability of acceptance
is very low when it is less than zero and rises rapidly
as it increases. This relatively clean separation sup-
ports the basic assumption in section 2 that benefit
depends on k.
The points labelled smoothed in figure 2 were
obtained using a sliding-average smoother, and the
model curve was obtained using two-component
Gaussian mixtures to fit the smoothed empirical
likelihoods p(gain|a = 0) and p(gain|a = 1). The
model probabilities are taken from the curve at in-
tegral values. As an example, the probability of ac-
cepting the prediction in figure 1 is about .25.
4.2 Benefit
The benefit B(x,h, s, k, a) is defined as the typing
time the translator saves by accepting or rejecting
a prediction x whose first k characters are correct.
To determine this, we assume that the translator first
reads x, then, if he or she decides to accept, uses a
special command to place the cursor at the end of x
and erases its last l ? k characters. Assuming inde-
pendence from h, s as before, our model is:
B(x, k, a) =
{
?R1(x) + T (x, k)? E(x, k), a = 1
?R0(x), a = 0
where Ra(x) is the cost of reading x when it ul-
timately gets accepted (a= 1) or rejected (a= 0),
T (x, k) is the cost of manually typing xk1 , and
E(x, k) is the edit cost of accepting x and erasing
to the end of its first k characters.
A natural unit for B(x, k, a) is the number of
keystrokes saved, so all elements of the above equa-
tion are converted to this measure. This is straight-
forward in the case of T (x, k) and E(x, k), which
are estimated as k and l ? k + 1 respectively?for
E(x, k), this corresponds to one keystroke for the
command to accept a prediction, and one to erase
each wrong character. This is likely to slightly un-
derestimate the true benefit, because it is usually
harder to type n characters than to erase them.
As in the previous section, read costs are inter-
preted as expected values with respect to the proba-
bility that the user actually does read x, eg, assuming
0 cost for not reading, R0(x) = p(r=1|x)R?0(x),
where R?0(x) is the unknown true cost of reading
and rejecting x. To determine Ra(x), we measured
the average elapsed time in the TransType data from
the point at which a proposal was displayed to the
point at which the next user action occurred?either
an acceptance or some other command signalling a
rejection. Times greater than 5 seconds were treated
as indicating that the translator was distracted and
were filtered out. As shown in figure 3, read times
are much higher for predictions that get accepted, re-
flecting both a more careful perusal by the translator
and the fact the rejected predictions are often simply
ignored.2 In both cases there is a weak linear rela-
2Here the number of characters read was assumed to include
the whole contents of the TransType menu in the case of rejec-
tions, and only the proposal that was ultimately accepted in the
case of acceptances.
tionship between the number of characters read and
the time taken to read them, so we used the least-
squares lines shown as our models. Both plots are
noisy and would benefit from a more sophisticated
psycholinguistic analysis, but they are plausible and
empirically-grounded first approximations.
To convert reading times to keystrokes for the
benefit function we calculated an average time per
keystroke (304 milliseconds) based on sections of
the trial where translators were rapidly typing and
when predictions were not displayed. This gives an
upper bound for the per-keystroke cost of reading?
compare to, for instance, simply dividing the total
time required to produce a text by the number of
characters in it?and therefore results in a conser-
vative estimate of benefit.
To illustrate the complete user model, in the fig-
ure 1 example the benefit of accepting would be
7?2?4.2 = .8 keystrokes and the benefit of reject-
ing would be?.2 keystrokes. Combining these with
the acceptance probability of .25 gives an overall ex-
pected benefit B(x,h, s, k = 7) for this proposal of
0.05 keystrokes.
5 Search
Searching directly through all character strings x
in order to find x? according to equation (1) would
be very expensive. The fact that B(x,h, s) is non-
monotonic in the length of x makes it difficult to or-
ganize efficient dynamic-programming search tech-
niques or use heuristics to prune partial hypotheses.
Because of this, we adopted a fairly radical search
strategy that involves first finding the most likely se-
quence of words of each length, then calculating the
benefit of each of these sequences to determine the
best proposal. The algorithm is:
1. For each length m = 1 . . .M , find the best
word sequence:
w?m = argmax
w1:(w1=u1v), wm2
p(wm1 |h
?, s),
where u1 and h? are as defined in section 3.
2. Convert each w?m to a corresponding character
string x?m.
3. Output x? = argmaxm B(x?m,h, s), or the
empty string if all B(x?m,h, s) are non-
positive.
M average time maximum time
1 0.0012 0.01
2 0.0038 0.23
3 0.0097 0.51
4 0.0184 0.55
5 0.0285 0.57
Table 1: Approximate times in seconds to generate
predictions of maximum word sequence length M ,
on a 1.2GHz processor, for the MEMD model.
In all experiments reported below, M was set to a
maximum of 5 to allow for convenient testing. Step
1 is carried out using a Viterbi beam search. To
speed this up, the search is limited to an active vo-
cabulary of target words likely to appear in transla-
tions of s, defined as the set of all words connected
by some word-pair feature in our translation model
to some word in s. Step 2 is a trivial deterministic
procedure that mainly involves deciding whether or
not to introduce blanks between adjacent words (eg
yes in the case of la + vie, no in the case of l? +
an). This also removes the prefix u1 from the pro-
posal. Step 3 involves a straightforward evaluation
of m strings according to equation (2).
Table 1 shows empirical search timings for vari-
ous values of M , for the MEMD model described
in the next section. Times for the linear model are
similar. Although the maximum times shown would
cause perceptible delays for M > 1, these occur
very rarely, and in practice typing is usually not no-
ticeably impeded when using the TransType inter-
face, even at M = 5.
6 Evaluation
We evaluated the predictor for English to French
translation on a section of the Canadian Hansard
corpus, after training the model on a chronologi-
cally earlier section. The test corpus consisted of
5,020 sentence pairs and approximately 100k words
in each language; details of the training corpus are
given in (Foster, 2000b).
To simulate a translator?s responses to predic-
tions, we relied on the user model, accepting prob-
abilistically according to p(a|x,h, s, k), determin-
ing the associated benefit using B(x,h, s, k, a), and
advancing the cursor k characters in the case of an
config M
1 2 3 4 5
fixed -8.5 -0.4 -3.60 -11.6 -20.8
linear 6.1 9.40 8.8 8.1 7.8
exact 5.3 10.10 10.7 10.0 9.7
corr 5.8 10.7 12.0 12.5 12.6
best 7.9 17.90 24.5 27.7 29.2
fixed -11.5 -9.3 -15.1 -22.0 -28.2
exact 3.0 4.3 5.0 5.2 5.2
best 6.2 12.1 15.4 16.7 17.3
Table 2: Results for different predictor configura-
tions. Numbers give % reductions in keystrokes.
user M
1 2 3 4 5
superman 48.6 53.5 51.8 51.1 50.9
rational 11.7 17.8 17.2 16.4 16.1
real 5.3 10.10 10.7 10.0 9.7
Table 3: Results for different user simulations.
Numbers give % reductions in keystrokes.
acceptance, 1 otherwise. Here k was obtained by
comparing x to the known x? from the test corpus.
It may seem artificial to measure performance ac-
cording to the objective function for the predictor,
but this is biased only to the extent that it misrepre-
sents an actual user?s characteristics. There are two
cases: either the user is a better candidate?types
more slowly, reacts more quickly and rationally?
than assumed by the model, or a worse one. The
predictor will not be optimized in either case, but
the simulation will only overestimate the benefit in
the second case. By being conservative in estimating
the parameters of the user model, we feel we have
minimized the number of translators who would fall
into this category, and thus can hope to obtain real-
istic lower bounds for the average benefit across all
translators.
Table 2 contains results for two different trans-
lation models. The top portion corresponds to the
MEMD2B maximum entropy model described in
(Foster, 2000a); the bottom portion corresponds to
the linear combination of a trigram and IBM 2 used
in the TransType experiments (Langlais et al, 2002).
Columns give the maximum permitted number of
words in predictions. Rows show different predic-
tor configurations: fixed ignores the user model and
makes fixedM -word predictions; linear uses the lin-
ear character-probability estimates described in sec-
tion 3.1; exact uses the exact character-probability
calculation; corr is described below; and best gives
an upper bound on performance by choosing m in
step 3 of the search algorithm so as to maximize
B(x,h, s, k) using the true value of k.
Table 3 illustrates the effects of different compo-
nents of the user model by showing results for sim-
ulated users who read infinitely fast and accept only
predictions having positive benefit (superman); who
read normally but accept like superman (rational);
and who match the standard user model (real). For
each simulation, the predictor optimized benefits for
the corresponding user model.
Several conclusions can be drawn from these re-
sults. First, it is clear that estimating expected bene-
fit is a much better strategy than making fixed-word-
length proposals, since the latter causes an increase
in time for all values of M . In general, making ?ex-
act? estimates of string prefix probabilities works
better than a linear approximation, but the difference
is fairly small.
Second, the MEMD2B model significantly out-
performs the trigram+IBM2 combination, produc-
ing better results for every predictor configuration
tested. The figure of -11.5% in bold corresponds
to the TransType configuration, and corroborates the
validity of the simulation.3
Third, there are large drops in benefit due to read-
ing times and probabilistic acceptance. The biggest
cost is due to reading, which lowers the best possi-
ble keystroke reduction by almost 50% for M = 5.
Probabilistic acceptance causes a further drop of
about 15% for M = 5.
The main disappointment in these results is that
performance peaks at M = 3 rather than continu-
ing to improve as the predictor is allowed to con-
sider longer word sequences. Since the predictor
knows B(x,h, s, k), the most likely cause for this
is that the estimates for p(w?m|h, s) become worse
with increasing m. Significantly, performance lev-
3Although the drop observed with real users was greater at
about 20% (= 17% reduction in speed), there are many dif-
ferences between experimental setups that could account for
the discrepancy. For instance, part of the corpus used for the
TransType trials was drawn from a different domain, which
would adversely affect predictor performance.
els off at three words, just as the search loses di-
rect contact with h through the trigram. To correct
for this, we used modified probabilities of the form
?m p(w?m|h, s), where ?m is a length-specific cor-
rection factor, tuned so as to optimize benefit on a
cross-validation corpus. The results are shown in the
corr row of table 2, for exact character-probability
estimates. In this case, performance improves with
M , reaching a maximum keystroke reduction of
12.6% at M = 5.
7 Conclusion and Future Work
We have described an approach to text prediction for
translators that is based on maximizing the benefit
to the translator according to an explicit user model
whose parameters were set from data collected in
user evaluations of an existing text prediction proto-
type. Using this approach, we demonstrate in sim-
ulated results that our current predictor can reduce
the time required for an average user to type a text
in the domain of our training corpus by over 10%.
We look forward to corroborating this result in tests
with real translators.
There are many ways to build on the work de-
scribed here. The statistical models which are
the backbone of the predictor could be improved
by making them adaptive?taking advantage of the
user?s input?and by adding features to capture the
alignment relation between h and s in such a way as
to preserve the efficient search properties. The user
model could also be made adaptive, and it could be
enriched in many other ways, for instance so as to
capture the propensity of translators to accept at the
beginnings of words.
We feel that the idea of creating explicit user mod-
els to guide the behaviour of interactive systems is
likely to have applications in areas of NLP apart
from translators? tools. For one thing, most of the
approach described here carries over more or less
directly to monolingual text prediction, which is an
important tool for the handicapped (Carlberger et al,
1997). Other possibilities include virtually any ap-
plication where a human and a machine communi-
cate through a language-rich interface.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathematics
of Machine Translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?312, June.
Alice Carlberger, Johan Carlberger, Tina Magnuson,
Sira E. Palazuelos-Cagigas, M. Sharon Hunnicutt, and
Santiago Aguilera Navarro. 1997. Profet, a new gen-
eration of word prediction: an evaluation study. In
Proceedings of the 2nd Workshop on NLP for Commu-
nication Aids, Madrid, Spain, July.
Kenneth W. Church and Eduard H. Hovy. 1993. Good
applications for crummy machine translation. Ma-
chine Translation, 8:239?258.
George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text Mediated Interactive Machine
Translation. Machine Translation, 12:175?194.
George Foster. 2000a. Incorporating position infor-
mation into a Maximum Entropy / Minimum Di-
vergence translation model. In Proceedings of the
4th Computational Natural Language Learning Work-
shop (CoNLL), Lisbon, Portugal, September. ACL
SigNLL.
George Foster. 2000b. A Maximum Entropy / Minimum
Divergence translation model. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Hong Kong, October.
Martin Kay. 1973. The MIND system. In R. Rustin,
editor, Natural Language Processing, pages 155?188.
Algorithmics Press, New York.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs and Discussion, 25(4).
Philippe Langlais, George Foster, and Guy Lapalme.
2000. Unit completion for a computer-aided transla-
tion typing system. Machine Translation, 15(4):267?
294, December.
Philippe Langlais, Guy Lapalme, and Marie Loranger.
2002. TransType: From an idea to a system. Machine
Translation. To Appear.
S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 1998.
A DP based search algorithm for statistical machine
translation. In Proceedings of the 36th Annual Meet-
ing of the ACL and 17th COLING 1998, pages 960?
967, Montre?al, Canada, August.
C. Tillmann and H. Ney. 2000. Word re-ordering and
DP-based search in statistical machine translation. In
Proceedings of the International Conference on Com-
putational Linguistics (COLING) 2000, Saarbrucken,
Luxembourg, Nancy, August.
Legal Texts Summarization by Exploration of the
Thematic Structures and Argumentative Roles
Atefeh Farzindar and Guy Lapalme
RALI, De?partement d?Informatique et Recherche Ope?rationnelle
Universite? de Montre?al, Que?bec, Canada, H3C 3J7
{farzinda,lapalme}@iro.umontreal.ca
Abstract
In this paper we describe our method for the sum-
marization of legal documents helping a legal ex-
pert determine the key ideas of a judgment. Our
approach is based on the exploration of the docu-
ment?s architecture and its thematic structures in or-
der to build a table style summary for improving co-
herency and readability of the text. We present the
components of a system, called LetSum, built with
this approach, its implementation and some prelim-
inary evaluation results.
1 Introduction
The goal of a summary is to give the reader an accu-
rate and complete idea of the contents of the source
(Mani, 2001). In this research, we focused on a
problem referred to as legal text summarization.
As ever larger amounts of legal documents become
available electronically, interest in automatic sum-
marization has continued to grow in recent years. In
this paper, we present our method for producing a
very short text from a long legal document (a record
of the proceedings of federal courts in Canada) and
present it as a table style summary. The goal of
this project is to develop a system to create a sum-
mary for the needs of lawyers, judges and experts
in the legal domain. Our approach investigates the
extraction of the most important units based on the
identification of thematic structures of the document
and the determination of semantic roles of the tex-
tual units in the judgment (Farzindar, 2004). The
remainder of the paper is organized as follows. Sec-
tion 2 introduces the motivation of the research and
the context of the work. Section 3 reports on the
results of our analysis of a corpus of legal abstracts
written by professional abstractors. Section 4 de-
scribes our method for the exploration of document
architecture and the components of the system that
we have developed to produce a summary. Section 5
presents some related work in this domain. Section
6 concludes the paper and presents some prelimi-
nary evaluation results for the components of our
system.
2 Context of the Work
In Canada, the Canadian Legal Information Insti-
tute project (CANLII) aims at gathering legisla-
tive and judicial texts, as well as legal commen-
taries, from federal, provincial and territorial ju-
risdictions in order to make primary sources of
Canadian law accessible for free on the Internet
(http://www.canlii.org). The large vol-
ume of legal information in electronic form creates
a need for the creation and production of powerful
computational tools in order to extract relevant in-
formation in a condensed form.
But why are we interested in the processing of
previous legal decisions and in their summaries?
First, because a court order generally gives a so-
lution to a legal problem between two or several
parties. The decision also contains the reasons
which justify the solution and constitute a law ju-
risprudence precedent from which it is possible to
extract a legal rule that can be applied to simi-
lar cases. To find a solution to a legal problem
not directly indicated in the law, lawyers look for
precedents of similar cases. For a single query in
a data base of law reports, we often receive hun-
dreds of documents that are very long to study for
which legal experts and law students request sum-
maries. In Quebec REJB (R e?pertoire e?lectronique
de jurisprudence du Barreau) and SOQUIJ (Soci e?t e?
qu e?b e?coise d?information juridique) are two orga-
nizations which provide manual summaries for le-
gal resources, but the human time and expertise re-
quired makes their services very expensive. For ex-
ample the price of only one summary with its full
text, provided by SOQUIJ is 7.50 $ can. Some legal
information systems have been developed by private
companies like QuickLaw in Canada and WEST-
LAW and LEXIS in the United States, however no
existing system completely satisfies the specific re-
quirements of this field.
One reason for the difficulty of this work is the
complexity of the domain: specific vocabularies of
Between:
JASPER NATIONAL PARK Applicants and THE ATTORNEY GENERAL OF CANADA Respondent,
Docket: T-1557-98
Judgment Professional abstract Role
[1] This application for judicial review arises
out of a decision (the Decision) announced on
or about the 30th of June 1998 by the Minister
of Canadian Heritage (the Minister) to close
the Maligne River (the River) in Jasper Na-
tional Park to all boating activity, beginning
in 1999.
Judicial review of Minister of Canadian
Heritage?s decision to close Maligne River
in Jasper National Park to all boating activ-
ity beginning in 1999 to protect habitat of
harlequin ducks.
INTRO-
DUCTION
[7] The applicants offer commercial rafting
trips to Park visitors in this area each year
from mid-June to sometime in September.
Applicants offer commercial rafting trips on
River.
CONTEXT
[10] Consequently, a further environmental
assessment regarding commercial rafting on
the Maligne River was prepared in 1991. The
assessment indicated that rafting activity had
expanded since 1986, with an adverse impact
on Harlequin ducks along the Maligne River.
1991 environmental assessment indicating
rafting having adverse impact on harlequin
ducks along river.
CONTEXT
Table 1: Alignment of the units of the original judgment with the professional abstract
the legal domain and legal interpretations of expres-
sions produce many ambiguities. For example, the
word sentence can have two very different mean-
ings: one is a sequence of words and the other is a
more particular meaning in law, the decision as to
what punishment is to be imposed. Similarly dis-
position which means nature, effort, mental attitude
or property but in legal terms it means the final part
of a judgement indicating the nature of a decision:
acceptation of a inquiry or dismission.
Most previous systems of automatic summariza-
tion are limited to newspaper articles and scien-
tific articles (Saggion and Lapalme, 2002). There
are important differences between news style and
the legal language: statistics of words, probability
of selection of textual units, position of paragraphs
and sentences, words of title and lexical chains rela-
tions between words of the title and the key ideas of
the text, relations between sentences and paragraphs
and structures of the text.
For judgments, we show that we can identify dis-
cursive structures for the different parts of the deci-
sion and assign some argumentative roles to them.
Newspapers articles often repeat the most important
message but, in law, important information may ap-
pear only once. The processing of a legal document
requires detailed attention and it is not straight for-
ward to adapt the techniques developed for other
types of document to the legal domain.
3 Observations from a Corpus
3.1 Composition
Our corpus contains 3500 judgments of the Federal
Court of Canada, which are available in HTML on
http://www.canlii.org/ca/cas/fct/.
We analyzed manually 50 judgments in English and
15 judgments in French as well as their summaries
written by professional legal abstractors. The
average size of the documents that are input to
our system are judgments between 500 and 4000
words long (2 to 8 pages), which form 80% of all
3500 judgments; 10% of the documents having
less than 500 words (about one page) and so
they do not need a summary. Only 10% of the
decisions have more than 4000 words. Contrary
to some existing systems (Moens et al, 1999) that
focus only on limited types of judgments, such
as criminal cases, our research deals with many
categories of texts such as: Access to information,
Administrative law, Air law, Broadcasting, Com-
petition, Constitutional law, Copyright, Customs
and Excise - Customs Act, Environment, Evidence,
Human rights, Maritime law, Official languages,
Penitentiaries, Unemployment insurance and etc.
3.2 Structure of Legal Judgments
During our corpus analysis, we compared model
summaries written by humans with the texts of
the original judgments. We have identified the
organisational architecture of a typical judgment.
Thematic structures Content Judgment Summary
DECISION DATA Name of the jurisdiction,
place of the hearing,
date of the decision,
identity of the author,
names of parties,
title of proceeding and
Authority and doctrine
INTRODUCTION Who? did what? to whom? 5 % 12 %
CONTEXT Facts in chronological order or by description 24 % 20 %
JURIDICAL ANALYSIS Comments by the judge, finding of facts and
application of the law
67 % 60 %
CONCLUSION Final decision of the court 4 % 8 %
Table 2: Table of summary shows the thematic structures in a jugement and percentage of the contribution
of each thematic structure in source judgment and its human made summary
!"#$%&?(
)#*$#+&%&?,+
)#-#(&?,+
./,01(&?,+
20#+&?3(%&?,+4,5
610*$#+&
,/*%+?7%&?,+4
8?+04&"#
/#-#9%+&41+?&744
:;&/%(&4&"#
"?*"#7&47(,/#0
1+?&744
817?,+4&"#
#;&/%(&#04<%/&7
:-?$?+%&?,+4,5
1+?$<,/&%+&
#-#$#+&7
!%=-#47&>-#
71$$%/>
8?-&#/?+*
?,?7#
@#01(&?,+4
A#*%-
B,(1$#+&
:-?$?+%&?,+4,5
C1,&%&?,+74
B#&#/$?+%&?,+4,5
)#$%+&?(4@,-#7
Figure 1: The procedural steps for generating of table style summary
The paragraphs that address the same subject are
grouped as members of a block. We annotated the
blocks with a label describing their semantic roles.
We also manually annotated citations which are tex-
tual units (sentence or paragraph) quoted by the
judge as reference, for example an article of law
or other jurisprudence. The citations account for a
large part of the text of the judgment, but they are
not considered relevant for the summary, therefore
these segments will be eliminated during the infor-
mation filtering stage.
The textual units considered as important by the
professional abstractors were aligned manually with
one or more elements of the source text. Table 1
shows an example of an alignment between a human
summary and the original judgment. We look for a
match between the information considered impor-
tant in the professional abstract and the information
in the source documents. Our observation shows
that, for producing a summary, a professional ab-
stractor mainly relies on the manual extraction of
important units while conforming to general guide-
lines. The collection of these selected units forms a
summary.
During this analysis, we observed that texts
of jurisprudence are organized according to a
macrostructure and contain various levels of infor-
mation, independently of the category of judgment.
Proposed guidelines by Judge Mailhot of the Court
of Appeal of Quebec (Mailhot, 1998) and (Branting
et al, 1997) on legal judgments support this idea
that it is possible to define organisational structures
for decisions. Jurisprudence is organized by the dis-
course itself, which makes it possible to segment the
texts thematically.
Textual units dealing with the same subject form
a thematic segment set. In this context, we distin-
guish the layered thematic segments, which divide
the legal decisions into different discursive struc-
tures. The identification of these structures sepa-
rates the key ideas from the details of a judgment
and improves readability and coherency in the sum-
mary. We will present the argumentative roles of
each level of discourse, and their importance in
the judgment from the point of view of the key
and principal ideas. Table 2 shows the structure
of a jurisprudence and its different discourse lev-
els. Therefore, in the presentation of a final sum-
mary, we propose to preserve this organization of
the structures of the text in order to build a table
style summary with five themes:
DECISION DATA contains the name of the jurisdic-
tion, the place of the hearing, the date of the de-
cision, the identity of the author, names of par-
ties, title of proceeding, authority and doctrine.
It groups all the basic preliminary information
which is needed for planning the decision.
INTRODUCTION describes the situation before the
court and answers these questions: who are the
parties? what did they do to whom?
CONTEXT explains the facts in chronological or-
der, or by description. It recomposes the story
from the facts and events between the par-
ties and findings of credibility on the disputed
facts.
JURIDICAL ANALYSIS describes the comments
of the judge and finding of facts, and the ap-
plication of the law to the facts as found. For
the legal expert this section of judgment is the
most important part because it gives a solution
to the problem of the parties and leads the judg-
ment to a conclusion.
CONCLUSION expresses the disposition which is
the final part of a decision containing the infor-
mation about what is decided by the court. For
example, it specifies if the person is discharged
or not or the cost for a party.
During our corpus analysis, we computed the
distribution of the information (number of words
shown in Table 2) in each level of thematic structure
of the judgment. The average length of a judgment
is 3500 words and 350 words for its summary i.e. a
compression rate of about 10%.
4 Method for Producing Table Style
Summary
Our approach for producing the summary first iden-
tifies thematic structures and argumentative roles in
the document. We extract the relevant sentences and
present them as a table style summary. Showing the
information considered important which could help
the user read and navigate easily between the sum-
mary and the source judgment. For each sentence
of the summary, the user can determine the theme
by looking at its rhetorical role. If a sentence seems
more important for a user and more information is
needed about this topic, the complete thematic seg-
ment containing the selected sentence could be pre-
sented. The summary is built in four phases (Fig-
ure 1): thematic segmentation, filtering of less im-
portant units such as citations of law articles, selec-
tion of relevant textual units and production of the
summary within the size limit of the abstract.
The implementation of our approach is a system
called LetSum (Legal text Summarizer), which has
been developed in Java and Perl. Input to the sys-
tem is a legal judgment in English. To determine the
Part-of-Speech tags, the tagger described by (Hep-
ple, 2000) is used. The semantic grammars and
rules are developed in JAPE language (Java Anno-
tations Pattern Engine) and executed by a GATE
transducer (Cunningham et al, 2002).
4.1 Components of LetSum
Thematic segmentation for which we performed
some experiments with two statistic segmenters:
one described by Hearst for the TexTiling system
(Hearst, 1994) and the C99 segmenter described by
Choi (Choi, 2000), both of which apply a clustering
function on a document to find classes divided by
theme. But because the results of these numerical
segmenters were not satisfactory enough to find the
thematic structures of the legal judgments, we de-
cided to develop a segmentation process based on
the specific knowledge of the legal field.
Category of section title Linguistic markers Examples of section title
Begin of the judgment decision, judgment, reason,
order
Reasons for order, Reasons for
judgment and order
INTRODUCTION introduction, summary Introduction, Summary
CONTEXT facts, background The factual background, Agreed
statement of facts
JURIDICAL ANALYSIS analysis, decision, discussion Analysis and Decision of the court
CONCLUSION conclusion, disposiotion, cost Conclusion and Costs
Table 3: The linguistic markers in section titles
Each thematical segment can be associated with
an argumentative role in the judgment based on the
following information: the presence of significant
section titles (Table 3 shows categories and features
of the section titles), the absolute and relative posi-
tions of a segment, the identification of direct or nar-
rative style (as the border of CONTEXT and JURIDI-
CAL ANALYSIS segments), certain linguistic mark-
ers.
The linguistic markers used for each thematic
segment are organized as follows:
CONTEXT introduces the parties with the verb to
be (eg. the application is company X), describes the
application request like: advise, indicate, request
and explains the situation in the past tense and nar-
ration form.
In JURIDICAL ANALYSIS, the judge gives his ex-
planation on the subject thus the style of expression
is direct such as: I, we, this court, the cue phrases
(Paice, 1981) like: In reviewing the sections No. of
the Act, Pursuant to section No., As I have stated, In
the present case, The case at bar is.
In CONCLUSION the classes of verbs are: note,
accept, summarise, scrutinize, think, say, satisfy,
discuss, conclude, find, believe, reach, persuade,
agree, indicate, review, the concepts such as: opin-
ion, conclusion, summary, because, cost, action, the
cue phrases: in the case at bar, for all the above
reasons, in my view, my review of, in view of the evi-
dence, finally, thus, consequently, in the result. This
segment contains the final result of court decision
using phrases such as: The motion is dismissed, the
application must be granted. The important verbs
are: allow, deny, dismiss, grant, refuse.
Filtering identifies parts of the text which can be
eliminated, without losing relevant information for
the summary. In a judgment, the citation units (sen-
tence or paragraph) occupy a large volume in the
text, up to 30%, of the judgment, whereas their con-
tents are less important for the summary. This is
why we remove citations inside blocks of thematic
segments. We thus filter two categories of segments:
submissions and arguments that report the points of
view of the parties in the litigation and citations re-
lated for previous issues or references to applicable
legislation. In the case of eliminating a citation of
a legislation (eg. law?s article), we save the refer-
ence of the citation in DECISION DATA in the field
of authority and doctrine.
The identification of citations is based on two
types of markers: direct and indirect. A direct
marker is one of the linguistic indicators that we
classified into three classes: verbs, concepts (noun,
adverb, adjective) and complementary indications.
Examples of verbs of citation are: conclude, define,
indicate, provide, read, reference, refer, say, state,
summarize. Examples of the concepts are: follow-
ing, section, subsection, page, paragraph, pursuant.
Complementary indications include numbers, cer-
tain preposition, relative clauses and typographic
marks (colon, quotation marks).
The indirect citations are the neighboring units of
a quoted phrase. For example, in Table 4 a citation
is shown. For detecting CITATION segment units
such as paragraph 78(1), which reads as follows:
are identified using direct markers (shown here in
bold) but surrounding textual units with numbers
are also quotations. We thus developed a linear inte-
gration identification mechanism for sentences fol-
lowing a quoted sentence for determining a group
of citations.
Selection builds a list of the best candidate units
for each structural level of the summary. LetSum
computes a score for each sentence in the judgment
based on heuristic functions related to the following
information: position of the paragraphs in the doc-
ument, position of the paragraphs in the thematic
segment, position of the sentences in the paragraph,
distribution of the words in document and corpus
(tf ? idf ). Depending on the given information in
each layered segment, we have identified some cue
words and linguistic markers. The thematic segment
can change the value of linguistic indicators. For ex-
ample, the phrase application is dismissed that can
be considered as a important feature in the CON-
CLUSION might not have the same value in CON-
TEXT segment. At the end of this stage, the pas-
sages with the highest resulting scores are sorted to
determine the most relevant ones.
Production of the final summary in which the
selected sentences are normalized and displayed in
tabular format. The final summary is about 10%
of source document. The elimination of the unim-
portant sentences takes into account length statistics
presented in Table 2. In the INTRODUCTION seg-
ment, units with the highest score are kept within
10% of the size of summary. In the CONTEXT seg-
ment, the selected units occupy 24% of the sum-
mary length. The contribution of the JURIDICAL
ANALYSIS segment is 60% and the units with the
role CONCLUSION occupy 6% of the summary.
4.2 Current State of LetSum
Table 4 shows an example of the output after the ex-
ecution of the Selection module of LetSum (mod-
ules of Figure 1 up to the horizontal line) applied
on a judgment of Federal Court of Canada (2468
words). Thematic segmentation module has di-
vided the text into structural blocks according to
the rhetorical roles (given to the left of braces in
Table 4). The Filtering module removes citation
blocks and its enumerated quoted paragraphs (e.g.
paragraph (15) in tablet). Selection module chooses
total relevant textual units (shown in bold in Table 4)
in each thematic segment. The units are selected
according to their argumentative role in the judge-
ment. Here the length of all extracted units is 313
words.
Preliminary evaluations of components of Let-
Sum are very promising; we obtained 0.90 F-
measure for thematic segmentation and 0.97 F-
measure for filtering stage (detection of 57 quoted
segment correctly on 60).
From this information, the Production module
(currently being implemented) could concatenate
textual units with some grammatical modification to
produce a short summary.
5 Related Research
LetSum is the one of the few systems developed
specifically for the summarization of legal docu-
ments. All of these approaches attest the impor-
tance of the exploration of thematic structures in le-
gal documents.
The FLEXICON project (Smith and Deedman,
1987) generates a summary of legal cases by us-
ing information retrieval based on location heuris-
tics, occurrence frequency of index terms and the
use of indicator phrases. A term extraction module
that recognizes concepts, case citations, statute ci-
tations and fact phrases leads to a document profile.
This project was developed for the decision reports
of Canadian courts, which are similar to our corpus.
SALOMON (Moens et al, 1999) automatically
extracts informative paragraphs of text from Belgian
legal cases. In this project a double methodology
was used. First, the case category, the case struc-
ture and irrelevant text units are identified based
on a knowledge base represented as a text gram-
mar. Consequently, general data and legal foun-
dations concerning the essence of the case are ex-
tracted. Secondly, the system extracts informative
text units of the alleged offences and of the opinion
of the court based on the selection of representative
objects.
More recently, SUM (Grover et al, 2003) exam-
ined the use of rhetorical and discourse structure in
level of the sentence of legal cases for finding the
main verbes. The methodology is based on (Teufel
and Moens, 2002) where sentences are classified ac-
cording to their argumentative role.
These studies have shown the interest of summa-
rization in a specialized domain such as legal texts
but none of these systems was implemented in an
environment such as CANLII which has to deal with
thousands of texts and produce summaries for each.
6 Conclusion
In this paper, we have presented our approach for
dealing with automatic summarization techniques.
This work refers to the problem of processing of a
huge volume of electronic documents in the legal
field which becomes more and more difficult to ac-
cess. Our method is based on the extraction of rele-
vant units in the source judgment by identifying the
discourse structures and determining the semantic
roles of thematic segments in the document. The
presentation of the summary is in a tabular form di-
vided by the following thematic structures: DECI-
SION DATA, INTRODUCTION, CONTEXT, JURIDI-
CAL ANALYSIS and CONCLUSION. The generation
of summary is done in four steps: thematic segmen-
tation to detect the document structures, filtering to
eliminate unimportant quotations and noises, selec-
tion of the candidate units and production of table
style summary. The system is currently being fi-
nalized and preliminary evaluation results are very
promising.
7 Acknowledgements
We would like to thanks LexUM group of le-
gal information-processing laboratory of the Public
DECISION DATA
?
?
?
?
?
?
?
?
?
?
?
Name of the jurisdiction: Federal Court of Canada, Place of the hearing: Ottawa
Date of the decision: 31/12/97, Identity of the author: J.E. Dub e?
Names of parties: Commissioner of official languages of canad, Applicant
- and - Air Canada, Respondent
Title of proceeding: Official languages, Docket number: T-1989-96
Authority and doctrine : Official Languages Act, R.S.C., 1985 (4th Supp.), c. 31
INTRODUCTION
?
?
?
?
?
?
?
(1) An order was made by this Court on February 4, 1997 authorizing the respondent (Air Canada) to
raise preliminary objections to the notice of an originating motion filed by the applicant (the Commis-
sioner). As a result, this motion filed by Air Canada on March 18, 1997 raises six alternative prelim-
inary objections asking the Court to strike out in part the motion made by the Commissioner on
September 6, 1996 under section 78 of the Official Languages Act.
CONTEXT
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1. Facts
(2) The Commissioner?s originating motion, which was filed with the consent of the com-
plainant Paul Comeau, concerns Air Canada?s failure to provide ground services in the
French language at the Halifax airport. The Commissioner asks this Court to declare that
there is a significant demand for services in French in Air Canada?s office at the Halifax airport
and that Air Canada is failing to discharge its duties under Part IV of the Act. Part IV estab-
lishes language-related duties for communications with and services to the public, including the
travelling public, where there is significant demand.
(3) The Commissioner?s motion is filed by the complainant Paul Comeau.
...
CITATION
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(15) The point of departure is paragraph 78(1), which reads as follows:
78. (1) The Commissioner may
(a) within the time limits prescribed by paragraph 77(2)( a) or ( b), apply to the Court for
a remedy under this Part in relation to a complaint investigated by the Commissioner if the
Commissioner has the consent of the complainant.
(b) appear before the Court on behalf of any person who has applied under section 77 for a
remedy under this Part; or
(c) with leave of the Court, appear as a party to any proceedings under this Part.
ANALYSIS
?
?
?
?
?
?
?
?
?
(16) Air Canada?s position is therefore that the Commissioner may only apply for a rem-
edy limited to facts relating to a specific complaint, the investigation of that complaint
and the resulting reports and recommendations. In my view, this interpretation is too
narrow and is inconsistent with the general objectives of the Act and its remedial and
quasi-constitutional nature.
...
CONCLUSION
?
?
?
?
?
?
?
?
?
?
?
7. Conclusion
(29) Thus, to ensure that the judge presiding at the hearing on the merits can correctly assess the
situation in light of all the material evidence, no reference or evidence filed by the Commissioner
in the three affidavits mentioned above should be struck out.
(30) This motion to strike by Air Canada with respect to the preliminary objections must accord-
ingly be dismissed.
Table 4: Output produced by the LetSum?s modules: Thematic segmentation, Filtering and Selection.
Source judgment is divided into thematic blocks associated with rhetorical roles, citation block will be
removed in the filtering phase and textual units (shown in bold) have been selected as relevant.
Law Research Center at the University of Montreal
for their valuable suggestions. This project sup-
ported by Public Law Research Center and Natu-
ral Sciences and Engineering Research Council of
Canada.
References
L. Karl Branting, Charles B. Callaway, Bradford W.
Mott, and James C. Lester. 1997. A frame-
work for self-explaining legal documents. In
Proceedings of the Sixth International Confer-
ence on Artificial Intelligence and Law (ICAIL-
97), pages 72?81, University of Melbourne, Mel-
bourne, Australia, June 30-July 3.
Freddy Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proceding of
the 1 st North American Chapter of the Associa-
tion for Computational Linguistics, pages 26?33,
Seattle, Washington.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: A framework and graphi-
cal development environment for robust nlp tools
and applications. In Proceedings of the 40th An-
niversary Meeting of the Association for Compu-
tational Linguistics (ACL?02), Philadelphia, July.
Atefeh Farzindar. 2004. D e?veloppement d?un
syste`me de r e?sum e? automatique de textes ju-
ridiques. In TALN-RECITAL?2004, pages 39?44,
Fe`s, Maroc, 19-22 April.
Claire Grover, Ben Hachey, and Chris Korycinski.
2003. Summarising legal texts: Sentential tense
and argumentative roles. In Dragomir Radev and
Simone Teufel, editors, HLT-NAACL 2003 Work-
shop: Text Summarization (DUC03), pages 33?
40, Edmonton, Alberta, Canada, May 31 - June
1.
Marti A. Hearst. 1994. Multi-paragraph segmenta-
tion of expository text. In the 32nd Meeting of the
Association for Computational Linguistics, Los
Cruces, NM, June.
Mark Hepple. 2000. Independence and commit-
ment: Assumptions for rapid training and execu-
tion of rule-based part-of-speech taggers. In the
38th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2000), pages 278?
285, October.
Louise Mailhot. 1998. Decisions, Decisions: a
handbook for judicial writing. Editions Yvon
Blais, Qu e?bec, Canada.
Inderjeet Mani. 2001. Automatic Text Summariza-
tion. John Benjamins Publishing Company.
Marie-Francine Moens, C. Uyttendaele, and J. Du-
mortier. 1999. Abstracting of legal cases: the
potential of clustering based on the selection of
representative objects. Journal of the American
Society for Information Science, 50(2):151?161.
Chris D. Paice. 1981. The automatic generation of
literary abstracts: An approach based on identi-
fication of self-indicating phrases. In O. R. Nor-
man, S. E. Robertson, C. J. van Rijsbergen, and
P. W. Williams, editors, Information Retrieval
Research, London: Butterworth.
Horacio Saggion and Guy Lapalme. 2002. Gener-
ating indicative-informative summaries with su-
mum. Computational Linguistics, 28(4).
J. C. Smith and Cal Deedman. 1987. The applica-
tion of expert systems technology to case-based
law. ICAIL, pages 84?93.
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles - experiments with rele-
vance and rhetorical status. Computational Lin-
guistics, 28(4):409?445.
Adaptive Language and Translation Models
for Interactive Machine Translation
Laurent Nepveu, Guy Lapalme
Philippe Langlais
RALI/DIRO - Universite? de Montre?al,
C.P. 6128, succursale Centre-ville
Montre?al, Que?bec, Canada H3C 3J7
{nepveul,lapalme,felipe}
@iro.umontreal.ca
George Foster
Language Technologies Research Centre
National Research Council Canada
A-1330, 101 rue Saint-Jean Bosco,
Gatineau, Que?bec, Canada K1A 0R6
George.Foster@nrc-cnrc.gc.ca
Abstract
We describe experiments carried out with adaptive
language and translation models in the context of an
interactive computer-assisted translation program.
We developed cache-based language models which
were then extended to the bilingual case for a cache-
based translation model. We present the improve-
ments we obtained in two contexts: in a theoretical
setting, we achieved a drop in perplexity for the new
models and, in a more practical situation simulat-
ing a user working with the system, we showed that
fewer keystrokes would be needed to enter a trans-
lation.
1 Introduction
Cache-based language models were introduced by
Kuhn and de Mori (1990) for the dynamic adap-
tation of speech language models. These models,
inspired by the memory caches on modern com-
puter architectures, are motivated by the principle
of locality which states that a program tends to re-
peatedly use memory cells that are physically close.
Similarly, when speaking or writing, humans tend
to use the same words and phrase constructs from
paragraph to paragraph and from sentence to sen-
tence. This leads us to believe that, when processing
a document, the part of a document that is already
processed (e.g. for speech recognition, translation
or text prediction) gives us very useful information
for future processing in the same document or in
other related documents.
A cache-based language model is a language
model to which is added a smaller model trained
only on the history of the document being pro-
cessed. The history is usually the last N words or
sentences seen in the document.
Kuhn and de Mori (1990) obtained a drop in per-
plexity of nearly 68% when adding an unigram POS
(part-of-speech) cache on a 3g-gram model. Martin
and al. (1997) obtained a drop of nearly 21% when
adding a bigram cache to a trigram model. Clarkson
and Robertson (1997) also obtained similar results
with an exponentially decaying unigram cache.
The major problem with these theoretical results
is that they assume the correctness of the material
entering the cache. In practice, this assumption does
not always hold, and so a cache can sometimes do
more harm than good.
1.1 Interactive translation context
Over the last few years, an interactive machine
translation (IMT) system (Foster et al, 2002) has
been developed which, as the translator is typing,
suggests word and phrase completions that the user
can accept or ignore. The system uses a transla-
tion engine to propose the words or phrases which
it judges the most probable to be immediately typed.
This engine includes a translation model (TM) and
a language model (LM) used jointly to produce pro-
posals that are appropriate translations of source
words and plausible completions of the current text
in the target language. The translator remains in
control of the translation because what is typed by
the user is taken as a constraint to which the model
must continually adapt its completions. Experi-
ments have shown that the use of this system can
save about 50% of the keystrokes needed for enter-
ing a translation. As the translation and language
models are built only once, before the user starts to
work with the system, the translator is often forced
to repeatedly correct similar suggestions from the
system.
The interactive nature of this setup made us be-
lieve that it is a good prospect for dynamic adaptive
modeling. If the dynamic nature of the system can
be disadvantageous for static language and transla-
tion models, it is an incomparable advantage for a
cache based approach because human correction in-
tervenes before words go in the cache. As the trans-
lator is using the system to correctly enter his trans-
lation progressively, we can expect the theoretical
results presented in the literature to be obtainable in
practice in the IMT context.
The first advantage of dynamic adaptation would
be to help the translation engine make better predic-
tions, but it has a further psychological advantage:
as the translator works and potentially corrects the
proposals of the engine, the user would feel that the
software is learning from its errors.
The next section describes the models currently
embedded within our IMT prototype. Section 3 de-
scribes the cache-based adaptation we performed on
the target language model. In section 4, we present
the different types of adaptations we performed on
the translation model. Section 5 then puts the results
in the context of our IMT application. Section 6 dis-
cusses the implications of our experiments and sug-
gests some improvements that could be made to the
system.
2 Current IMT models
The word-based translation model embedded within
the IMT system has been designed by Foster (2000).
It is a Maximum Entropy/Minimum Divergence
(MEMD) translation model (Berger et al, 1996),
which mimics the parameters of the IBM model 2
(Brown et al, 1993) within a log-linear setting.
The resulting model (named MDI2B) is of the
following form, where h is the current target text,
s the source sentence being translated, s a particular
word in s and w the next word to be predicted:
p(w|h, s) =
q(w|h) exp(
?
s?s ?sw + ?AB)
Z(h, s)
(1)
The q distribution represents the prior knowledge
that we have about the true distribution and is mod-
eled by an interpolated trigram in this study. The
? coefficients are the familiar transfer or lexical pa-
rameters, and the ? ones can be understood as their
position dependent correction. Z is a normalizing
factor, the sum of the numerator for every w in the
target vocabulary.
Our baseline model used an interpolated trigram
of the following form as the q distribution:
p(w|h) = ?1(wi?2wi?1) ? ptri(wi|wi?2wi?1)
+ ?2(wi?2wi?1) ? pbi(wi|wi?1)
+ ?3(wi?2wi?1) ? puni(wi)
+ ?4(wi?2wi?1) ? 1|V |+1
where ?1(wi?2wi?1) + ?2(wi?2wi?1) +
?3(wi?2wi?1) + ?4(wi?2wi?1) = 1 and |V | + 1
is the size of the event space (including a special
unknown word).
As mentioned above, the MDI2B model is closely
related to the IBM2 model (Brown et al, 1988). It
contains two classes of features: word pair features
and positional features. The word pair feature func-
tions are defined as follows:
fst(w,h, s) =
{
1 if s ? s and t = w
0 otherwise
This function is on if the predicted word is t and s
is in the current source sentence. Each feature fst
has a corresponding weight ?st (for brevity, this is
defined to be 0 in equation 1 if the pair s, t is not
included in the model).
The positional feature functions are defined as
follows:
fA,B(w, i, s) =
J?
j=1
?[(i, j, J) ? A ? (sj , w) ? B ? j = ??sj ]
where ?[X] is 1 if X is true, otherwise 0; and ??sj
is the position of the occurrence of sj that is clos-
est to i according to an IBM2 model. A is a class
that groups positional (i, j, J) configurations having
similar IBM2 alignment probabilities, in order to re-
duce data sparseness. B is a class of word pairs
having similar weights ?st. Its purpose is to simu-
late the way IBM2 alignment probabilities modulate
IBM1 word-pair probabilities, by allowing the value
of the positional feature weight to depend on the
magnitude of the corresponding word-pair weight.
As with the word pair features, each fA,B has a cor-
responding weight ?AB .
Since feature selection is applied at training time
in order to improve speed, avoid overfitting, and
keep the model compact, the summation in the ex-
ponential term in (1) is only carried out over the set
of active pairs maintained by the model and not over
all pairs as might be inferred from the formulation.
To give an example of how the model works, if
the source sentence is the fruit I am eating is a ba-
nana and we are predicting the word banane follow-
ing the target words: Le fruit que je mange est une,
the active pairs involving banana would be (fruit,
banana) and (banane, banana) since, of all the pairs
(s, t) they would be the only ones kept by the fea-
ture selection algorithm1. The probability of banane
would therefore depend on the weights of those two
pairs, along with position weights which capture the
relative proximity of the words involved.
3 Language model adaptation
We implemented a first monolingual dynamic adap-
tation of this model by inserting a cache compo-
nent in its reference distribution, thus only affect-
ing the q distribution. We obtained similar results
1See (Foster, 2000) for the description of this algorithm.
as for classical ngram models: the unigram cache
model proved to be less efficient than the bigram
one, and the trigram cache suffered from sparsity.
We also tested a model where we interpolated the
three cache models to gain information from each
of the unigram, bigram, and trigram cache mod-
els. For completeness, this generalized model is de-
scribed in equation 2 under the usual constraints that?
i ?i(h) = 1 for all h.
p(w|h) = ?1(h) ? ptri(wi|wi?2wi?1)
+ ?2(h) ? pbi(wi|wi?1)
+ ?3(h) ? puni(wi)
+ ?4(h) ? 1|V |+1
+ ?5(h) ? ptric(wi|wi?2wi?1)
+ ?6(h) ? pbic(wi|wi?1)
+ ?7(h) ? punic(wi)
(2)
Those models were trained from splits of the
Canadian Hansard corpus. The base ngram model
was estimated with a 30M word split of the corpus.
The weighting coefficients of both the base trigram
and the cache models were estimated with an EM
algorithm trained with 1M words.
We tested our models, translating from English
to French, on two corpora of different types: the
first one hansard is a document taken from the
same large corpus that was used for training (the
testing and training corpora were exclusive splits).
The second one sniper, which describes the job
of a sniper, is from another domain characterized
by lexical and phrasal constructions very different
from those used to estimate the probabilities of our
models.
Table 1 shows the perplexity on the hansard
and the sniper corpora. Preliminary experiments
led us to two sizes of cache which seemed promis-
ing: 2000 and 5000 corresponding to the last 2000
and 5000 words seen during the processing of a doc-
ument. The BI column gives the results of the bi-
gram cache model and the 1+2+3 gives the results
of the interpolated cache model which included the
unigram, bigram and trigram cache.
The results show that our models improve the
base static model by 5% on documents supposedly
well known by the models and by more that 52%
on documents that are unknown to the model. Sec-
tion 5 puts these results in the perspective of our
actual IMT system. Note that he addition of a cache
component to a language model involves negligible
extra training time.
Taille BI ? 1+2+3 ?
base hansard=17.6584
2000 16.937 -4.1% 16.840 -4.6%
5000 16.903 -4.3% 16.777 -5.0%
base sniper=135.808
2000 73.936 -45.6% 67.780 -50.1%
5000 70.514 -48.1% 64.204 -52.7%
Table 1: Perplexities of the MDI2B model with a
cache component included in the reference distribu-
tion on the hansard and sniper corpora.
4 Translation model adaptation
With those excellent results in mind, we extended
the idea of dynamic adaptation to the bilingual case
which, to our knowledge, has never been tried be-
fore.
We developed a model called MDI2BCache
which is a MDI2B model to which we added a cache
component based on word pairs. Recall that, when
predicting a word w at a certain point in a document,
the probability depends on the weights of the pairs
(s, w) for each active word s in the current source
sentence. As the prediction of the words of the doc-
ument goes on, our model keeps in a cache each
active pair used for the prediction of each word. In
the example above, if the translator accepts the word
banane, then the two pairs (fruit, banana) and (ba-
nane, banana) will be added to the cache.
We added a new feature to the MEMD model to
take into account the presence of a certain pair in
the recent history of the processed document:
fcache st(w,h, s) =
?
????
????
1 if
?
??
??
s ? s,
t = w,
(s, t) ? cache
?st > p
0 otherwise
We added a threshold value p to the feature func-
tion because while analyzing the pair weights, we
discovered that low weight pairs are usually pairs of
utility words such as conjunctions and punctuation.
We also came to the conclusion that they are not the
kind of words we want to have in the cache, since
their presence in a sentence implies little about their
presence in the next.
The resulting model is of the form:
p(w|h, s) =
q(w|h)exp(
?
s?s ?sw + ?AB + ?sw)
Z(h, s)
Thus, every fcache sw has a corresponding weight
?sw for the calculation of the probability of w.
Size 0.3 ? 0.5 ? 0.7 ?
base One feature weight, no Viterbi orig perp=17.6584
1000 17.5676 -0.51% 17.5756 -0.47% 17.5983 -0.34%
2000 17.5698 -0.50% 17.5766 -0.46% 17.5976 -0.34%
5000 17.5743 -0.48% 17.5776 -0.46% 17.5965 -0.35%
10000 17.5777 -0.46% 17.5791 -0.45% 17.5962 -0.35%
base One feature weight per pair, no Viterbi orig perp=17.6584
1000 17.5817 -0.43% 17.5858 -0.41% 17.6065 -0.29%
2000 17.5933 -0.37% 17.5918 -0.38% 17.6061 -0.30%
5000 17.5849 -0.42% 17.5874 -0.40% 17.6076 -0.29%
10000 17.5890 -0.39% 17.5891 -0.39% 17.6069 -0.29%
base One feature weight, Viterbi orig perp=17.6584
1000 17.5602 -0.56% 17.5697 -0.50% 17.5940 -0.36%
2000 17.5676 -0.51% 17.5695 -0.50% 17.5896 -0.39%
5000 17.5614 -0.55% 17.5687 -0.51% 17.5925 -0.37%
10000 17.5650 -0.53% 17.5687 -0.51% 17.5906 -0.38%
Table 2: MDI2BCache test perplexities. One feature weight, Viterbi alignment version.
4.1 Number of cache features
We implemented two versions of the model, one in
which we estimated only one cache feature weight
for the whole model and another in which we esti-
mated one cache feature weight for every word pair
in the model.
The first model is simpler and is easier to esti-
mate. The assumption is made that every pair in the
model has the same tendency to repeat itself.
The second model doubles the number of word-
pair parameters compared to MDI2B, and thus leads
to a linear increase in training time. Extra training
time is negligible in the first model.
4.2 Word alignment
One of the main difficulties of automatic MT is de-
termining which source word(s) translate to which
target word(s). It is very difficult to do this task
automatically, in part because it is also very diffi-
cult manually. If a pair of sentences are given to
10 translators for alignment, the results would likely
not be identical in all cases. As it is nearly impossi-
ble to determine such an alignment, most translation
models consider every source word to have an effect
on the translation of every target word.
This difficulty shows up in our cache-based
model. When adding word pairs to the cache, we
ideally would like to add only word pairs that were
really in a translation relation in the given sentence.
This is why we also implemented a version of our
model in which a word alignment is first carried out
in order to select good pairs to be added to the cache.
For this purpose, we computed a Viterbi alignment
based on an IBM model 2. This results in a subset of
the good active pairs to be added to the cache. The
Viterbi algorithm gives us a higher confidence level
that the pair of words added to the cache were really
in a translation relation. But it can also lead to word
pairs not added to the cache that should have been
added.
4.3 Results
Table 2 shows the results of the different configura-
tions of the MDI2BCache model. For every config-
uration we trained and tested on splits of the Cana-
dian Hansard with threshold values of 0.3, 0.5, and
0.7 and cache sizes of 1000, 2000, 5000, and 10000.
The top of the table is the version of the model with
only one feature weight without Viterbi alignment.
The middle of the table is the version with one fea-
ture weight per word pair without Viterbi alignment.
Finally, the bottom is for the version with only one
feature weight and a Viterbi alignment made prior
to adding pairs to the cache.
Threshold values of 0.3, 0.5, and 0.7 led to 75%,
50%, and 25% of the pairs considered for addition
to the cache respectively. The results show that the
threshold values of 0.5 and 0.7 are removing too
many pairs. The best results are obtained with a
threshold of 0.3 in all tests. Since the number of
pairs kept in the model appears to vary in proportion
to the threshold value, we did not consider it neces-
sary to use an automatic search algorithm to find an
optimal threshold value. The gain in performance
would have been negligible.
The results also show that having one feature
weight per word pair leads to lower results. This
can be explained by the fact that it is much more
Size 0.3 ? 0.5 ?
base MDI2B=135.808
1000 132.865 -2.17% 132.751 -2.25%
2000 132.771 -2.23% 132.752 -2.25%
5000 132.733 -2.26% 132.628 -2.34%
10000 132.997 -2.07% 132.674 -2.31%
Table 3: MDI2BCache test perplexities. One fea-
ture weight, Viterbi alignment version. Sniper test
difficult to estimate a weight for every pair that one
weight for all pairs. Since we use only thousands of
words in the cache, the training process suffers from
a poor data representation.
The Viterbi alignment seems to be helping the
models. The best results are obtained with the ver-
sion of our model with Viterbi alignment. However,
this gives only a 0.56% percent drop in perplexity.
We then tested our best configuration on the
sniper corpus. Table 3 shows the results. We
dropped threshold value 0.7 and tested only the
model with only one feature weight and a Viterbi
alignment.
Results show that our bilingual cache model
shows improvement (four times higher) in drop of
perplexity when used on documents very different
from the training corpus. In general, results give
lower perplexity than our base model showing that
the bilingual cache is helpful to the model, but the
results are not as good as that the ones obtained in
the unilingual case. Section 6 discusses these results
further.
5 Evaluation of IMT
As stated earlier, drops in perplexity are theoreti-
cal results that have been obtained previously in the
case of unilingual dynamic adaptation but for which
a corresponding level of practical success was rarely
attained because of the cache correctness problem.
To show that the interactive nature of our assisted-
translation application can really benefit from dy-
namic adaptation, we tested our models in a more
realistic translation context. This test consists of
simulating a translator using the IMT system as it
proposes words and phrases and accepting, correct-
ing or rejecting the proposals by trying to reproduce
a given target translation (Foster et al, 2002). The
metric used is the percentage of keystrokes saved
by the use of the system instead of having to type
directly all the target text.
For these simulations, we used only a 10K word
split of the hansard and of the sniper cor-
pus. The reason is that the IMT application poten-
Taille BI ? 1+2+3 ?
base hansard=27.435
2000 27.784 +1.3% 27.719 +1.0%
5000 27.837 +1.5% 27.821 +1.4%
base sniper=9.686
2000 11.404 +15.1% 11.294 +14.2%
5000 11.498 +15.8% 11.623 +16.7%
Table 4: Saved keystrokes raises for the MDI2B
model with cache component in the reference dis-
tribution on the hansard and sniper corpora.
0.3 ?
base hansard=27.4358
1000 27.557 +0.44%
2000 27.531 +0.35%
5000 27.488 +0.18%
10000 27.468 +0.12%
base sniper=9.686
1000 9.896 +2.17%
2000 10.023 +3.48%
5000 9.983 +3.07%
10000 9.957 +2.80%
Table 5: Saved keystrokes raises for the
MDI2BCache model with only one feature
weight and Viterbi alignment on the hansard and
sniper corpora.
tially proposes new completions after every charac-
ter typed by the user. For a 10K word document, it
needs to search about 1 million times for high prob-
ability words and phrases. This leads to relatively
long simulation times, even though predictions are
made at real time speeds.
Table 4 shows the results obtained with the
MDI2B model to which we added a cache compo-
nent for the reference interpolated trigram distribu-
tion.
We can see that the saved keystroke percentages
are proportional to the perplexity drops reported in
section 3. The use of our models raises the saved
keystrokes by nearly 1.5% in the case of well known
documents and by nearly 17% in the case of very
different documents. These are very interesting re-
sults for a potential professional use of TransType.
Table 5 shows an increase in the number of saved
keystrokes: 0.44% on the hansard and 3.5% on
the sniper corpora. Once again, the results are
not as impressive as the ones obtained for the mono-
lingual dynamic adaptation case.
6 Discussion
The results presented in section 3 on language
model adaptation confirmed what had been reported
in the literature: adding a cache component to a lan-
guage model leads to a drop in perplexity. More-
over, we were able to demonstrate that using a
cache-based language model inside a translation
model leads to better performance for the whole
translation model. We obtained drops in perplexity
of 5% on a corpus of the same type as the training
corpus and of 50% on a different one. These theo-
retical results lead to very good practical results. We
were able to increase the saved keystroke percent-
age by 1.5% on the similar corpus as the training
and by nearly 17% on the different corpus. These
results confirm our hypothesis that dynamic adapta-
tion with cache-based language model can be useful
in the context of IMT, particularly for new types of
texts.
Results presented in section 4 on translation
model adaptation show that our approach has led
to drops in perplexity although not as high as we
would have hoped. To understand these disappoint-
ing results, we analyzed the content of the cache for
different configurations of our MDI2BCache model.
base 0.3 viterbi + 0.3
(is,qu?) (to,afin) (offence,crime)
(.,sa) (was,a) (was,e?te?)
(this,,) (UNK,UNK) (very,tre`s)
(all,toutes) (piece,le?gislative) (today,aujourd?hui)
(have,du) (this,ce) (jobs,emploi)
(the,pour) (per,100) (concern,inquie?tude)
(on,du) (that,soient) (skin,peau)
(of,un) (,,,) (there,y)
(we,nous) (?,il) (government,le)
(the,du) (any,tout) (an,un)
18 68 86
Table 6: Cache sampling of different configurations
of MDI2BCache model.
Table 6 shows the results of our sampling. We
tested three model configurations. The first one, in
the first column, was the base MDI2BCache model
which adds all active pairs to the cache. The second
configuration, in the second column, was a thresh-
old value of 0.3 that brings about 75% of the pairs
being added to the cache. The last configuration was
a model with threshold value of 0.3 and a Viterbi
alignment made prior to the addition of pairs in the
cache. The three model configuration were with
only one feature weight. For all three configura-
tions, we took a sample of 10 pairs (shown in table
6) and a sample of 100 pairs. With the second sam-
ple, we manually analyzed each pair and counted
the number of pairs (shown in the last row of the ta-
ble) we believed were useful for the model (words
that are occasionally translations of one another).
The results obtained in section 4 seem to agree
with the current analysis. From left to right in the ta-
ble, the pairs seem to contain more information and
to be more appropriate additions to the cache. The
configuration with Viterbi alignment which contains
86 good pairs clearly seems to be the configuration
with the most interesting pairs.
The problem with such a cache-based translation
model seem to be similar to the balance between
precision and recall in information retrieval. On one
hand, we want to add in the cache every word pair
in which the two words are in translation relation in
the text. We further want to add only the pairs in
which the two words are really in translation rela-
tion in the text. It seems that with our base model,
we add most of the good pairs, but also a lot of bad
ones. With the Viterbi alignment and a threshold
value of 0.3, most of the pairs added are good ones,
but we are probably missing a number of other ap-
propriate ones. This comes back to the task of word
alignment, which is a very difficult task for comput-
ers (Mihalcea and Pedersen, 2003).
Moreover, we would want to add in the cache
only those words for which more than one transla-
tion is possible. For example, the pair (today, au-
jourd?hui), though it is a very useful pair for the
base model, is unlikely to help when added to the
cache. The reason is simple: they are two words
that are always translations of one another, so the
model will have no problem predicting them. This
ideal of precision and recall and of useful pairs in
the cache is obtained by our model with threshold
of 0.3, a Viterbi alignment and a cache size of 1000.
One disadvantage of our bilingual adaptive model
is the way it handles unknown words. In the cache-
based language model, the unknown words were
dealt with normally, i.e. they were added to the
cache and given a certain probability afterwards.
So, if an unknown word was seen in a certain sen-
tence and then later on, it would receive a proba-
bility mass of its own but not the one given to any
unknown word. By having its own probability mass
due to its presence in the cache, such previously un-
known word can be predicted by the model. In the
case of our MDI2BCache model, because we have
not yet implemented an algorithm for guessing the
translations of unknown words, they are simply rep-
resented within the model as UNK words, which
means that the model never learns them.
The results obtained with the sniper corpus
shows us that dynamic adaptation is also more help-
ful for documents that are little known to the model
in the bilingual context. The results are four times
better on the sniper corpus than on the Hansard
testing corpus.
Once again for the bilingual case, the practical
test results in the number of saved keystrokes agree
with the theoretical results of drops in perplexity.
This result shows that bilingual dynamic adaptation
also can be implemented in a practical context and
obtain results similar to the theoretical results.
All things considered, we believe that a cache-
based translation model shows a great potential
for bilingual adaptation and that greater perplexity
drops and keystroke savings could be obtained by
either reengineering the model or by improving the
MDI2BCache model.
6.1 Key improvements to the model
Following the analysis of the results obtained by our
model, we have pointed out some key improvements
that the model would need in order to get better re-
sults. In this list we focus on ways of improving
adaptation strategies for the current model, omitting
other obvious enhancements such as adding phrase
translations.
Unknown word processing Learning new words
would be a very important feature to add to
the model and would lead to better results. We
did not incorporate the processing of unknown
words in the MDI2BCache because the struc-
ture of model did not lend itself to this addi-
tion. Especially with documents such as the
sniper corpus, we believe that this could
be a key improvement for a dynamic adaptive
model.
Better alignment As mentioned before, the ulti-
mate goal for our cache is that it contains only
the pairs present in the perfect alignment. Bet-
ter performance from the alignment would lead
to pairs in the cache closer to this ideal. In this
study we computed Viterbi alignments from an
IBM model 2, because it is very efficient to
compute and also because for training MDI2B,
we do use the IBM model 2. We could consider
also more advanced word alignment models
(Och and Ney, 2000; Lin and Cherry, 2003;
Moore, 2001). To keep the alignment model
simple, we could still use an IBM model 2, but
with the compositionality constraint that has
been shown to give better word alignment than
the Viterbi one (Simard and Langlais, 2003).
Feature weights We implemented two versions of
our model: one with only one feature weight
and another with one feature weight for each
word pair. The second model suffered from
poor data representation and our training algo-
rithm wasn?t able to estimate good cache fea-
ture weights. We think that creating classes
of word pairs, such as it was done for posi-
tional alignment features, would lead to better
results. It would enable the model to take into
account the tendency that a pair has to repeat
itself in a document.
Relative weighting Another key improvement is
that changes to word-pair weights should be
relative to each source word. For example,
if (house, maison) is a pair in the cache, we
would like to favour maison over possible al-
ternatives such as chambre as a translation of
house. In the existing model this is done by
boosting the weight on (house,maison), which
has the undesirable side-effect of making mai-
son more important in the model than transla-
tions of other source words in the current sen-
tence which have not appeared in the cache.
One way of eliminating this behaviour would
be to learn negative weights on alternatives like
(house,chambre) which do not appear in the
cache.
We believe these improvements would better show
the potential of bilingual dynamic adaptation.
7 Conclusion
We have presented dynamic adaptive translation
models using cache-based implementations. We
have shown that monolingual dynamic adaptive
models exhibit good theoretical performance in a
bilingual translation context. We observed that
these theoretical results carry over to practical gains
in the context of an IMT application.
We have developed bilingual dynamic adaptation
through a cache-based translation model. Our re-
sults show the potential of bilingual dynamic adap-
tation. We have given explanations about why the
results obtained are not as high as hoped and pre-
sented some key improvements that should be made
to our model or should be taken into account in the
development of a new model.
We believe that this study reveals the potential for
adaptive interactive machine translation system and
we hope to read similar reports for other implemen-
tations of the same interactive scenario e.g. (Och et
al., 2003).
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy
approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39?71.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Jelinek,
Robert L. Mercer, and Paul Roossin. 1988. A
statistical approach to language translation. In
Proceedings of the International Conference on
Computational Linguistics (COLING), pages 71?
76, Budapest, Hungary, August.
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993.
The mathematics of Machine Translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?312, June.
P.R. Clarkson and P.R. Robertson. 1997. Language
model adaptation using mixtures and an expo-
nentially decaying cache. In IEEE Int. Confer-
ence on Acoustics, Speech, and Signal Process-
ing, Munich.
George Foster, Philippe Langlais, and Guy La-
palme. 2002. User-friendly text prediction
for translators. In 2002 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP 2002), Philadelphia, July. TT2
TransType2.
George Foster. 2000. A Maximum Entropy / Mini-
mum Divergence translation model. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages
37?42, Hong Kong, October.
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recog-
nition. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 12(6):570?
583, June.
Dekang Lin and Colin Cherry. 2003. Proalign:
Shared task system description. In NAACL 2003
Workshop on Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,
pages 11?14, Edmonton Canada, May 31. TT2.
S.C. Martin, J. Liermann, and H. Ney. 1997. Adap-
tative topic-dependent language modelling using
word-based varigrams. In Eurospeech.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In Rada Mihal-
cea and Ted Pedersen, editors, HLT-NAACL 2003
Workshop: Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,
pages 1?10, Edmonton, Alberta, Canada, May
31. Association for Computational Linguistics.
Robert C. Moore. 2001. Towards a simple and
accurate statistical approach to learning transla-
tion relationships among words. In Workshop on
Data-driven Machine Translation, 39th Annual
Meeting and 10th Conference of the European
Chapter, pages 79?86, Toulouse, France. Asso-
ciation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages
440?447, Hong Kong, October.
F.J. Och, R. Zens, and H. Ney. 2003. Efficient
search for interactive statistical machine trans-
lation. In Proceedings of the 10th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 387?
393, Budapest, Hungary, April. TT2.
Michel Simard and Philippe Langlais. 2003. Statis-
tical translation alignment with compositionality
constraints. In NAACL 2003 Workshop on Build-
ing and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 19?22, Ed-
monton Canada, May 31. TT2.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 354?358,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fully Abstractive Approach to Guided Summarization
Pierre-Etienne Genest, Guy Lapalme
RALI-DIRO
Universite? de Montre?al
P.O. Box 6128, Succ. Centre-Ville
Montre?al, Que?bec
Canada, H3C 3J7
{genestpe,lapalme}@iro.umontreal.ca
Abstract
This paper shows that full abstraction can be
accomplished in the context of guided sum-
marization. We describe a work in progress
that relies on Information Extraction, statis-
tical content selection and Natural Language
Generation. Early results already demonstrate
the effectiveness of the approach.
1 Introduction
In the last decade, automatic text summarization has
been dominated by extractive approaches that rely
purely on shallow statistics. In the latest evalu-
ation campaign of the Text Analysis Conference1
(TAC), the top systems were considered only ?barely
acceptable? by human assessment (Owczarzak and
Dang, 2011). The field is also getting saturated near
what appears to be a ceiling in performance. Sys-
tems that claim to be very different from one an-
other have all become statistically indistinguishable
in evaluation results. An experiment (Genest et al,
2009) found a performance ceiling to pure sentence
extraction that is very low compared to regular (ab-
stractive) human summaries, but not that much bet-
ter than the current best automatic systems.
Abstractive summarization has been explored to
some extent in recent years: sentence compression
(Knight and Marcu, 2000) (Cohn and Lapata, 2009),
sentence fusion (Barzilay and McKeown, 2005) or
revision (Tanaka et al, 2009), and a generation-
based approach that could be called sentence split-
ting (Genest and Lapalme, 2011). They are all
1www.nist.gov/tac
rewriting techniques based on syntactical analysis,
offering little improvement over extractive methods
in the content selection process.
We believe that a fully abstractive approach with a
separate process for the analysis of the text, the con-
tent selection, and the generation of the summary
has the most potential for generating summaries at a
level comparable to human. For the foreseeable fu-
ture, we think that such a process for full abstraction
is impossible in the general case, since it is almost
equivalent to perfect text understanding. In specific
domains, however, an approximation of full abstrac-
tion is possible.
This paper shows that full abstraction can be ac-
complished in the context of guided summarization.
We propose a methodology that relies on Informa-
tion Extraction and Natural Language Generation,
and discuss our early results.
2 Guided Summarization
The stated goal of the guided summarization task
at TAC is to motivate a move towards abstractive
approaches. It is an oriented multidocument sum-
marization task in which a category is attributed
to a cluster of 10 source documents to be summa-
rized in 100 words or less. There are five cate-
gories: Accidents and Natural Disasters, Attacks,
Health and Safety, Endangered Resources, and In-
vestigations/Trials. Each category is associated with
a list of aspects to address in the summary. Figure 1
shows the aspects for the Attacks category. We use
this specification of categories and aspects to accom-
plish domain-specific summarization.
354
2.1 WHAT: what happened
2.2 WHEN: date, time, other temporal placement markers
2.3 WHERE: physical location
2.4 PERPETRATORS: individuals or groups responsible for the attack
2.5 WHY: reasons for the attack
2.6 WHO AFFECTED: casualties (death, injury), or individuals otherwise negatively affected
2.7 DAMAGES: damages caused by the attack
2.8 COUNTERMEASURES: countermeasures, rescue efforts, prevention efforts, other reactions
Figure 1: Aspects for TAC?s guided summarization task, category 2: Attacks
3 Fully Abstractive Approach
Guided summarization categories and aspects define
an information need, and using Information Extrac-
tion (IE) seems appropriate to address it. The idea
to use an IE system for summarization can be traced
back to the FRUMP system (DeJong, 1982), which
generates brief summaries about various kinds of
stories; (White et al, 2001) also wrote abstractive
summaries using the output of an IE system applied
to events such as natural disasters. In both cases, the
end result is a generated summary from the informa-
tion available. A lot of other work has instead used
IE to improve the performance of extraction-based
systems, like (Barzilay and Lee, 2004) and (Ji et al,
2010).
What is common to all these approaches is that
the IE system is designed for a specific purpose, sep-
arate from summarization. However, to properly ad-
dress each aspect requires a system designed specifi-
cally for that task. To our knowledge, tailoring IE to
the needs of abstractive summarization has not been
done before. Our methodology uses a rule-based,
custom-designed IE module, integrated with Con-
tent Selection and Generation in order to write short,
well-written abstractive summaries.
Before tackling these, we perform some prepro-
cessing on the cluster of documents. It includes:
cleaning up and normalization of the input using reg-
ular expressions, sentence segmentation, tokeniza-
tion and lemmatization using GATE (Cunningham
et al, 2002), syntactical parsing and dependency
parsing (collapsed) using the Stanford Parser (de
Marneffe et al, 2006), and Named Entity Recogni-
tion using Stanford NER (Finkel et al, 2005). We
have also developed a date resolution engine that fo-
cuses on days of the week and relative terms.
3.1 Information Extraction
Our architecture is based on Abstraction Schemes.
An abstraction scheme consists of IE rules, con-
tent selection heuristics and one or more genera-
tion patterns, all created by hand. Each abstrac-
tion scheme is designed to address a theme or sub-
category. Thus, rules that extract information for
the same aspect within the same scheme will share a
similar meaning. An abstraction scheme aims to an-
swer one or more aspects of its category, and more
than one scheme can be linked to the same aspect.
Figure 2 shows two of the schemes that we have
created. For the scheme killing, the IE rules would
match X as the perpetrator and Y as a victim for
all of the following phrases: X killed Y, Y was
assassinated by X, and the murder of X
by Y. Other schemes have similar structure and pur-
pose, such as wounding, abducting, damaging
and destroying. To create extraction rules for a
scheme, we must find several verbs and nouns shar-
ing a similar meaning and identify the syntactical
position of the roles we are interested in. Three re-
sources have helped us in designing extraction rules:
a thesaurus to find semantically related nouns and
verbs; VerbNet (Kipper et al, 2006), which provides
amongst other things the semantic roles of the syn-
tactical dependents of verbs; and a hand-crafted list
of aspect-relevant word stems provided by the team
that made CLASSY (Conroy et al, 2010).
Schemes and their extraction rules can also be
quite different from this first example, as shown with
the scheme event. This scheme gathers the basic in-
formation about the attack event: WHAT category of
attack, WHEN and WHERE it occurred. A list of key
words is used to identify words that imply an attack
event, while a list of EVENT NOUNs is used to iden-
tify specifically words that refer to a type of attack.
355
Scheme: killing
Information Extraction
SUBJ(kill, X) ? WHO(X)
OBJ(kill, Y) ? WHO AFFECTED(Y)
SUBJ(assassinate, X) ? WHO(X)
OBJ(assassinate, Y) ? WHO AFFECTED(Y)
...
PREP OF(murder, Y) ? WHO AFFECTED(Y)
PREP BY(murder, X) ? WHO(X)
...
Content Selection Select best candidates for kill verb, WHO(X) and WHO AFFECTED(Y)
Generation X kill verb Y
Scheme: event
Information Extraction
PREP IN(key word, X), LOCATION(X) ? WHERE(X)
PREP IN(key word, X), ORGANIZATION(X) ? WHERE(X)
PREP AT(key word, X), LOCATION(X) ? WHERE(X)
PREP AT(key word, X), ORGANIZATION(X) ? WHERE(X)
DEP(key word, Y), DATE(Y) ? WHEN(Y)
EVENT NOUN(Z) ? WHAT(Z)
Content Selection Select best candidates for at or in, WHERE(X), WHEN(Y) and WHAT(Z)
Generation On Y, Z occurred at/in X
Figure 2: Abstraction schemes killing and event. The information extraction rules translate preprocessing annota-
tions into candidate answers for a specific aspect. Content selection determines which candidate will be included in the
generated sentence for each aspect. Finally, a pattern is used to determine the structure of the generated sentence. No-
tation: word or lemma, variable, group of words, PREDICATE OR ASPECT. Note that the predicate DEP matches
any syntactical dependency and that key words refer to a premade list of category-relevant verbs and nouns.
3.2 Content Selection
A large number of candidates are found by the IE
rules for each aspect. The content selection module
selects the best ones and sends them to the genera-
tion module. The basic heuristic is to select the can-
didate most often mentioned for an aspect, and simi-
larly for the choice of a preposition or a verb for gen-
eration. More than one candidate may be selected
for the aspect WHO AFFECTED, the victims of
the attack. Several heuristics are used to avoid re-
dundancies and uninformative answers.
News articles may contain references to more
than one event of a given category, but our sum-
maries describe only one. To avoid mixing candi-
dates from two different event instances that might
appear in the same cluster of documents, we rely on
dates. The ancestors of a date in the dependency
tree are associated with that date, and excluded from
the summary if the main event occurs on a different
date.
3.3 Generation
The text of a summary must be fluid and feel natu-
ral, while being straightforward and concise. From
our observation of human-written summaries, it also
does not require a great deal of originality to be
considered excellent by human standards. Thus,
we have designed straightforward generation pat-
terns for each scheme. They are implemented us-
ing the SimpleNLG realizer (Gatt and Reiter, 2009),
which takes a sentence structure and words in their
root form as input and gives a sentence with re-
solved agreements and sentence markers as output.
The greatest difficulty in the structure is in realizing
noun phrases. The content selection module selects
a lemma that should serve as noun phrase head, and
its number, modifiers and specifier must be deter-
mined during generation. Frequencies and heuristics
are again used to identify appropriate modifiers, this
time from all those used with that head within the
source documents. We apply the constraint that the
356
On April 20, 1999, a massacre occurred at Columbine High School.
Two student gunmen killed 12 students, a teacher and themselves.
On November 2, 2004, a brutal murder occurred in Amsterdam.
A gunman stabbed and shot Dutch filmmaker Theo van Gogh.
A policeman and the suspect were wounded.
On February 14, 2005, a suicide car bombing occurred in Beirut.
Former Lebanese Prime Minister Rafik Hariri and 14 others were killed.
Figure 3: Brief fully abstractive summaries on clusters D1001A-A, D1039G-A and D1043H-A, respectively on the
Columbine massacre, the murder of Theo van Gogh and the assassination of Rafik Hariri.
combination of number and modifiers chosen must
appear at least once as an IE rule match.
As for any generated text, a good summary also
requires a text plan (Hovy, 1988) (McKeown, 1985).
Ours consists of an ordering of the schemes. For ex-
ample, an Attack summary begins with the scheme
event. This ordering also determines which scheme
to favor in the case of redundancy, e.g. given that a
building was both damaged and destroyed, only the
fact that is was destroyed will be mentioned.
4 Results and Discussion
We have implemented this fully abstractive summa-
rization methodology. The abstraction schemes and
text plan for the Attack category are written in an
XML document, designed to easily allow the addi-
tion of more schemes and the design of new cate-
gories. The language processing of the source docu-
ments and the domain-specific knowledge are com-
pletely separate in the program.
Our system, which is meant as a proof of concept,
can generate useful summaries for the Attack cate-
gory, as can be seen in Figure 3. The key elements
of information are present in each case, stated in a
way that is easy to understand.
These short summaries have a high density of in-
formation, in terms of how much content from the
source documents they cover for a given number of
words. For example, using the most widely used
content metric, Pyramid (Nenkova et al, 2007), the
two sentences generated for the cluster D1001A-
A contain 8 Semantic Content Units (SCU) for a
weighted total of 30 out of a maximum of 56, for
a raw Pyramid score of 0.54. Only 3 of the 43 auto-
matic summaries beat this score on this cluster that
year (the average was 0.31). Note that the sum-
maries that we compare against contain up to 100
words, whereas ours is only 21 words long. We con-
clude that our method has the potential for creating
summaries with much greater information density
than the current state of the art.
In fact, our approach does not only have the po-
tential to increase a summary?s coverage, but also its
linguistic quality and the reader satisfaction as well,
since the most relevant information now appears at
the beginning of the summary.
5 Conclusion and Future Work
We have developed and implemented a fully abstrac-
tive summarization methodology in the context of
guided summarization. The higher density of infor-
mation in our short summaries is one key to address
the performance ceiling of extractive summarization
methods. Although fully abstractive summarization
is a daunting challenge, our work shows the feasibil-
ity and usefulness of this new direction for summa-
rization research.
We are now expanding the variety and complexity
of the abstraction schemes and generation patterns
to deal with more aspects and other categories. We
should then be able to compare on a greater scale
the output of our system with the ones produced by
other automatic systems and by humans on all the
clusters used at TAC 2010 and 2011.
6 Acknowledgements
The authors want to thank Dr. Eduard Hovy, of ISI,
and Prof. Kathy McKeown, of Columbia Univer-
sity, for fruitful discussions on abstractive summa-
rization, and Dr. Judith Schlesinger and Dr. John
Conroy, both of the IDA / Center for Computing Sci-
ences, for providing us with their hand-crafted list of
category- and aspect-relevant keywords.
357
References
R. Barzilay and L. Lee. 2004. Catching the Drift: Prob-
abilistic Content Models, with Applications to Gen-
eration and Summarization. eprint arXiv:cs/0405039,
May.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Trevor Cohn and Mirella Lapata. 2009. Sentence
compression as tree transduction. J. Artif. Int. Res.,
34(1):637?674.
John M. Conroy, Judith D. Schlesinger, Peter A. Rankel,
and Dianne P. O?Leary. 2010. CLASSY 2010: Sum-
marization and metrics. In Proceedings of the Third
Text Analysis Conference, Gaithersburg, Maryland,
USA. National Institute of Standards and Technology.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, Philadelphia, PA, USA.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology. The Stanford Natural
Language Processing Group.
Gerald DeJong, 1982. An Overview of the FRUMP Sys-
tem, pages 149?176. Lawrence Erlbaum.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a Re-
alisation Engine for Practical Applications. In ENLG
?09: Proceedings of the 12th European Workshop on
Natural Language Generation, pages 90?93, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Pierre-Etienne Genest and Guy Lapalme. 2011. Frame-
work for Abstractive Summarization using Text-to-
Text Generation. In Proceedings of the Workshop on
Monolingual Text-To-Text Generation, pages 64?73,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-
Monod. 2009. HexTac: the Creation of a Manual Ex-
tractive Run. In Proceedings of the Second Text Anal-
ysis Conference, Gaithersburg, Maryland, USA. Na-
tional Institute of Standards and Technology.
Eduard H. Hovy. 1988. Planning coherent multisenten-
tial text. In Proceedings of the 26th annual meeting
on Association for Computational Linguistics, pages
163?169, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Heng Ji, Juan Liu, Benoit Favre, Dan Gillick, and Dilek
Hakkani-Tur. 2010. Re-ranking summaries based
on cross-document information extraction. In Pu-Jen
Cheng, Min-Yen Kan, Wai Lam, and Preslav Nakov,
editors, Information Retrieval Technology, volume
6458 of Lecture Notes in Computer Science, pages
432?442. Springer Berlin / Heidelberg. 10.1007/978-
3-642-17187-1 42.
Karen Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending VerbNet with Novel
Verb Classes. In LREC 2006.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National Con-
ference on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intelli-
gence, pages 703?710. AAAI Press.
Kathleen R. McKeown. 1985. Discourse strategies for
generating natural-language text. Artif. Intell., 27:1?
41, September.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Trans. Speech Lang. Process., 4, May.
Karolina Owczarzak and Hoa Trang Dang. 2011.
Overview of the TAC 2011 summarization track:
Guided task and aesop task. In Proceedings of the
Fourth Text Analysis Conference, Gaithersburg, Mary-
land, USA. National Institute of Standards and Tech-
nology. http://www.nist.gov/tac/publications/.
Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa,
Tadashi Kumano, and Naoto Kato. 2009. Syntax-
driven sentence revision for broadcast news summa-
rization. In Proceedings of the 2009 Workshop on Lan-
guage Generation and Summarisation, UCNLG+Sum
?09, pages 39?47, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Multi-
document summarization via information extraction.
In Proceedings of the first international conference on
Human language technology research, HLT ?01, pages
1?7, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
358
Workshop on Monolingual Text-To-Text Generation, pages 64?73,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 64?73,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Framework for Abstractive Summarization using Text-to-Text Generation
Pierre-Etienne Genest, Guy Lapalme
RALI-DIRO
Universite? de Montre?al
P.O. Box 6128, Succ. Centre-Ville
Montre?al, Que?bec
Canada, H3C 3J7
{genestpe,lapalme}@iro.umontreal.ca
Abstract
We propose a new, ambitious framework for
abstractive summarization, which aims at se-
lecting the content of a summary not from sen-
tences, but from an abstract representation of
the source documents. This abstract repre-
sentation relies on the concept of Information
Items (INIT), which we define as the smallest
element of coherent information in a text or a
sentence. Our framework differs from previ-
ous abstractive summarization models in re-
quiring a semantic analysis of the text. We
present a first attempt made at developing a
system from this framework, along with eval-
uation results for it from TAC 2010. We also
present related work, both from within and
outside of the automatic summarization do-
main.
1 Introduction
Summarization approaches can generally be cate-
gorized as extractive or abstractive (Mani, 2001).
Most systems developped for the main international
conference on text summarization, the Text Analy-
sis Conference (TAC) (Owczarzak and Dang, 2010),
predominantly use sentence extraction, including all
the top-ranked systems, which make only minor
post-editing of extracted sentences (Conroy et al,
2010) (Gillick et al, 2009) (Genest et al, 2008)
(Chen et al, 2008).
Abstractive methods require a deeper analysis of
the text and the ability to generate new sentences,
which provide an obvious advantage in improving
the focus of a summary, reducing its redundancy
and keeping a good compression rate. According
to a recent study (Genest et al, 2009b), there is an
empirical limit intrinsic to pure extraction, as com-
pared to abstraction. For these reasons, as well as for
the technical and theoretical challenges involved, we
were motivated to come up with an abstractive sum-
marization model.
Recent abstractive approaches, such as sentence
compression (Knight and Marcu, 2000) (Cohn and
Lapata, 2009) and sentence fusion (Barzilay and
McKeown, 2005) or revision (Tanaka et al, 2009)
have focused on rewriting techniques, without con-
sideration for a complete model which would in-
clude a transition to an abstract representation for
content selection. We believe that a ?fully abstrac-
tive? approach requires a separate process for the
analysis of the text that serves as an intermediate
step before the generation of sentences. This way,
content selection can be applied to an abstract repre-
sentation rather than to original sentences or gener-
ated sentences.
We propose the concept of Information Items
(INIT) to help define the abstract representation. An
INIT is the smallest element of coherent informa-
tion in a text or a sentence. It can be something as
simple as some entity?s property or as complex as a
whole description of an event or action. We believe
that such a representation could eventually allow for
directly answering queries or guided topic aspects,
by generating sentences targeted to address specific
information needs.
Figure 1 compares the workflow of our approach
with other possibilities. Extractive summarization
consists of selecting sentences directly from the
64
Source
Documents
Summary
Information Items
(       )
Short
Sentences
Summary
 Items
Compressed
Sentences
Themes
InIt Selection
Sentence
Selection
Compression
InIt
Retrieval
Generation
Sentence Compression Sentence Fusion
Abstractive
Summarization
Extractive
Summarization
Fusion
N T
N T
InIt
N T
Figure 1: Workflow diagram of our suggested approach for abstractive summarization, compared to pure extractive
summarization, sentence compression, and sentence fusion for summarization. The dashed line represents the simpli-
fied framework used in our first attempt at abstractive summarization (see section 2.4).
source documents and generating a summary from
them. Sentence compression first compresses the
sentences and chooses from those and the source
documents? sentences to form a summary; it may
also be completed in the reverse order, which is
to select sentences from the source documents and
then compress them for the summary. Sentence
fusion first identifies themes (clusters of similar
sentences) from the source documents and selects
which themes are important for the summary (a pro-
cess similar to the sentence selection of centroid-
based extractive summarization methods (Radev et
al., 2004)) and then generates a representative sen-
tence for each theme by sentence fusion.
Our proposed abstractive summarization ap-
proach is fundamentally different because the selec-
tion of content is on Information Items rather than on
sentences. The text-to-text generation aspect is also
changed. Instead of purely going from whole sen-
tences to generated sentences directly, there is now
a text planning phase that occurs at the conceptual
level, like in Natural Language Generation (NLG).
This approach has the advantage of generating
typically short, information-focused sentences to
produce a coherent, information rich, and less re-
dundant summary. However, the difficulties are
great: it is difficult for a machine to properly extract
information from sentences at an abstract level, and
text generated from noisy data will often be flawed.
Generating sentences that do not all sound similar
and generic is an additional challenge that we have
for now circumvented by re-using the original sen-
65
tence structure to a large extent, which is a type of
text-to-text generation. Even considering those diffi-
culties, we believe that efforts in abstractive summa-
rization constitute the future of summarization re-
search, and thus that it is worthwhile to work to-
wards that end.
In this paper, we present our new abstractive sum-
marization framework in section 2. Section 3 de-
scribes and analyses our first attempt at using this
framework, for the TAC 2010 multi-document news
summarization task, followed by the competition?s
results in section 4. In this first attempt, we simpli-
fied the framework of section 2 to obtain early re-
sults which can help us as we move forward in this
project. Related work is discussed in section 5, and
we conclude in section 6.
2 Abstractive Summarization Framework
Our proposed framework for fully abstractive sum-
marization is illustrated in figure 1. This section dis-
cusses how each step could be accomplished.
2.1 INIT Retrieval
An Information Item is the smallest element of co-
herent information in a text or a sentence. This in-
tentionally vague definition leaves the implementa-
tion details to be decided based on resources avail-
able. The goal is to identify all entities in the text,
their properties, predicates between them, and char-
acteristics of the predicates. This seemingly un-
reachable goal, equivalent to machine reading, can
be limited to the extent that we only need INITs to
be precise and accurate enough to generate a sum-
mary from them.
The implementation of INITs is critical, as every-
thing will depend on the abstract information avail-
able. Semantic Role Labeling (SRL) and predicate-
logic analysis of text are two potential candidates for
developing INIT Retrieval. Word-sense disambigua-
tion, co-reference resolution and an analysis of word
similarity seem important as well to complement the
semantic analysis of the text.
2.2 INIT Selection
Given an analysis of the source documents that leads
to a list of INITs, we may now proceed to select
content for the summary. Frequency-based mod-
els, such as those used for extractive summarization,
could be applied to INIT selection instead of sen-
tence selection. This would result in favoring the
most frequently occurring entities, predicates, and
properties.
INIT selection could also easily be applied to
tasks such as query-driven or guided summariza-
tion, in which the user information need is known
and the summarization system attempts to address it.
With smaller building blocks (INITs rather than sen-
tences), it would be much easier to tailor summaries
so that they include only relevant information.
2.3 Generation
Planning, summary planning in our case, provides
the structure of the generated text. Most INITs do not
lead to full sentences, and need to be combined into
a sentence structure before being realized as text.
Global decisions of the INIT selection step now lead
to local decisions as to how to present the informa-
tion to the reader, and in what order.
Text generation patterns can be used, based on
some knowledge about the topic or the information
needs of the user. One could use heuristic rules with
different priority levels or pre-generated summary
scenarios, to help decide how to structure sentences
and order the summary. We believe that machine
learning could be used to learn good summary struc-
tures as well.
Once the detailed planning is completed, the sum-
mary is realized with coherent syntax and punctu-
ation. This phase may involve text-to-text genera-
tion, since the source documents? sentences provide
a good starting point to generate sentences with var-
ied and complex structures. The work of (Barzilay
and McKeown, 2005) on sentence fusion shows an
example of re-using the same syntactical structure of
a source sentence to create a new one with a slightly
different meaning.
2.4 First Attempt at Abstractive
Summarization
The three-step plan that we laid down is very hard,
and instead of tackling it head on, we decided to fo-
cus on certain aspects of it for now. We followed
a simplified version of our framework, illustrated
by the dashed line in Figure 1. It defers the con-
tent selection step to the selection of generated short
sentences, rather than actually doing it abstractly as
66
Original Sentence The Cypriot airliner that crashed in Greece may have suffered a sudden loss of cabin
pressure at high altitude, causing temperatures and oxygen levels to plummet and leaving everyone
aboard suffocating and freezing to death, experts said Monday.
Information Items
1. airliner ? crash ? null (Greece, August 15, 2005)
2. airliner ? suffer ? loss (Greece, August 15, 2005)
3. loss ? cause ? null (Greece, August 15, 2005)
4. loss ? leave ? null (Greece, August 15, 2005)
Generated Sentences
1. A Cypriot airliner crashed.
2. A Cypriot airliner may have suffered a sudden loss of cabin pressure at high altitude.
3. A sudden loss of cabin pressure at high altitude caused temperatures and oxygen levels to plum-
met.
4. A sudden loss of cabin pressure at high altitude left everyone aboard suffocating and freezing to
death.
Selected Generated Sentence as it appears in the summary
1. On August 15, 2005, a Cypriot airliner crashed in Greece.
Original Sentence At least 25 bears died in the greater Yellowstone area last year, including eight breeding-
age females killed by people.
Information Items
1. bear ? die ? null (greater Yellowstone area, last year)
2. person ? kill ? female (greater Yellowstone area, last year)
Generated Sentences
1. 25 bears died.
2. Some people killed eight breeding-age females.
Selected Generated Sentence as it appears in the summary
1. Last year, 25 bears died in greater Yellowstone area.
Figure 2: Two example sentences and their processing by our 2010 system. In the summary, the date and location
associated with an INIT are added to its generated sentence.
planned. The summary planning has to occur after
generation and selection, in a Summary Generation
step not shown explicitly on the workflow.
We have restricted our implementation of INITs to
dated and located subject?verb?object(SVO) triples,
thus relying purely on syntactical knowledge, rather
than including the semantics required for our frame-
work. Dates and locations receive a special treat-
ment because we were interested in news summa-
rization for this first attempt, and news articles are
factual and give a lot of importance to date and lo-
cation.
We did not try to combine more than one INIT in
the same sentence, relying instead on short, to-the-
67
point sentences, with one INIT each. Figure 2 shows
two examples of sentences that were generated from
a source document sentence using the simplified ab-
stractive summarization framework.
At first glance, the simplified version of our ap-
proach for generating sentences may seem similar
to sentence compression. However, it differs in
three important ways from the definition of the task
of compression usually cited (Knight and Marcu,
2000):
? Our generated sentences intend to cover only
one item of information and not all the impor-
tant information of the original sentence.
? An input sentence may have several generated
sentences associated to it, one for each of its
INITs, where it normally has only one com-
pressed sentence.
? Generated sentences sometimes include words
that do not appear in the original sentence (like
?some? in the second example), whereas sen-
tence compression is usually limited to word
deletion.
3 Abstractive Summarization at TAC 2010
Our first attempt at full abstractive summarization
took place in the context of the TAC 2010 multi-
document news summarization task. This section
describes briefly each module of our system, while
(Genest and Lapalme, 2010) provides the implemen-
tation details.
3.1 INIT Retrieval
An INIT is defined as a dated and located subject?
verb?object triple, relying mostly on syntactical
analyses from the MINIPAR parser (Lin, 1998) and
linguistic annotations from the GATE information
extraction engine (Cunningham et al, 2002).
Every verb encountered forms the basis of a can-
didate INIT. The verb?s subject and object are ex-
tracted, if they exist, from the parse tree. Each INIT
is also tagged with a date and a location, if appropri-
ate.
Many candidate INITs are rejected, for various
reasons: the difficulty of generating a grammatical
and meaningful sentence from them, the observed
unreliability of parses that include them, or because
it would lead to incorrect INITs most of the time.
The rejection rules were created manually and cover
a number of syntactical situations. Cases in which
bad sentences can be generated remain, of course,
even though about half the candidates are rejected.
Examples of rejected Inits include those with verbs
in infinitive form and those that are part of a con-
ditional clause. Discarding a lot of available infor-
mation is a significant limitation of this first attempt,
which we will address as the first priority in the fu-
ture.
3.2 Generation
From each INIT retrieved, we directly generate a
new sentence, instead of first selecting INITs and
planning the summary. This is accomplished using
the original parse tree of the sentence from which
the INIT is taken, and the NLG realizer SimpleNLG
(Gatt and Reiter, 2009) to generate an actual sen-
tence. Sample generated sentences are illustrated in
Figure 2.
This process ? a type of text-to-text generation ?
can be described as translating the parts that we want
to keep from the dependency tree provided by the
parser, into a format that the realizer understands.
This way we keep track of what words play what
role in the generated sentence and we select directly
which parts of a sentence appear in a generated sen-
tence for the summary. All of this is driven by the
previous identification of INITs. We do not include
any words identified as a date or a location in the
sentence generation process, they will be generated
if needed at the summary generation step, section
3.4.
Sentence generation follows the following steps:
? Generate a Noun Phrase (NP) to represent the
subject if present
? Generate a NP to represent the object if present
? Generate a NP to represent the indirect object
if present
? Generate a complement for the verb if one is
present and only if there was no object
? Generate the Verb Phrase (VP) and link all the
components together, ignoring anything else
present in the original sentence
NP Generation
Noun phrase generation is based on the subtree of
its head word in the dependency parse tree. The head
68
in the subtree becomes the head of the NP and chil-
dren in its parse subtree are added based on manual
rules that determine which children are realized and
how.
Verb Complement Generation
When an INIT has no object, then we attempt to
find another complement instead, in case the verb
would have no interesting meaning without a com-
plement. The first verb modifier that follows it in the
sentence order is used, including for example prepo-
sitional phrases and infinitive clauses.
VP Generation
Finally, the verb phrases are generated from each
verb and some of its children. The NPs generated for
the subject, object and indirect object are added, as
well as the verb complement if it was generated. If
there is an object but no subject, the VP is set to pas-
sive, otherwise the active form is always used. The
tense (past or present) of the VP is set to the tense of
the verb in the original sentence, and most modifiers
like auxiliaries and negation are conserved.
3.3 Sentence Selection
To determine which of the generated sentences
should be used in the summary, we would have liked
to choose from among the INITs directly. For exam-
ple, selecting the most frequent INIT, or INITs con-
taining the most frequent subject-verb pair seem rea-
sonable at first. However, during development, no
such naive implementation of selecting INITs pro-
vided satisfactory results, because of the low fre-
quency of those constructs, and the difficulty to
compare them semantically in our current level of
abstraction. Thus this critical content selection step
occurs after the sentence generation process. Only
the generated sentences are considered for the sen-
tence selection process; original sentences from the
source documents are ignored.
We compute a score based on the frequencies of
the terms in the sentences generated from the INITs
and select sentences that way. Document frequency
(DF) ? the number of documents that include an en-
tity in its original text ? of the lemmas included in
the generated sentence is the main scoring criterion.
This criterion is commonly used for summaries of
groups of similar documents. The generated sen-
tences are ranked based on their average DF (the
sum of the DF of all the unique lemmas in the sen-
tence, divided by the total number of words in the
sentence). Lemmas in a stop list and lemmas that are
included in a sentence already selected in the sum-
mary have their DF reduced to 0, to avoid favoring
frequent empty words, and to diminish redundancy
in the summary.
3.4 Summary Generation
A final summary generation step is required in this
first attempt, to account for the planning stage and
to incorporate dates and locations for the generated
sentences.
Sentence selection provides a ranking of the gen-
erated sentences and a number of sentences inten-
tionally in excess of the size limit of the summary
is first selected. Those sentences are ordered by the
date of their INIT when it can be determined. Oth-
erwise, the day before the date of publication of the
article that included the INIT is used instead. All
generated sentences with the same known date are
grouped in a single coordinated sentence. The date
is included directly as a pre-modifier ?On date,? at
the beginning of the coordination.
Each INIT with a known location has its generated
sentence appended with a post-modifier ?in loca-
tion?, except if that location has already been men-
tioned in a previous INIT of the summary.
At the end of this process, the size of the summary
is always above the size limit. We remove the least
relevant generated sentence and restart the summary
generation process. We keep taking away the least
relevant generated sentence in a greedy way, until
the length of the summary is under the size limit.
This naive solution to never exceed the limit was
chosen because we originally believed that our INITs
always lead to short generated sentences. However,
it turns out that some of the generated summaries
are a bit too short because some sentences that were
removed last were quite long.
4 Results and Discussion
Here, we present and discuss the results obtained by
our system in the TAC 2010 summarization system
evaluation. We only show results for the evaluation
of standard multi-document summaries; there was
69
also an update task, but we did not develop a spe-
cific module for it. After ranking at or near the top
with extractive approaches in past years (Genest et
al., 2008) (Genest et al, 2009a), we expected a large
drop in our evaluation results with our first attempt
at abstractive summarization. In general, they are
indeed on the low side, but mostly with regards to
linguistic quality.
As shown in Table 1, the linguistic quality of our
summaries was very low, in the bottom 5 of 43 par-
ticipating automatic systems. This low linguistic
score is understandable, because this was our first
try at text generation and abstractive summarization,
whereas the other systems that year used sentence
extraction, with at most minor modifications made
to the extracted sentences.
The cause of this low score is mostly our method
for text generation, which still needs to be refined
in several ways. The way we identify INITs, as we
have already discussed, is not yet developped fully.
Even in the context of the methodology outlined in
section 3, and specifically 3.2, many improvements
can still be made. Errors specific to the current state
of our approach came from two major sources: in-
correct parses, and insufficiently detailed and some-
times inappropriate rules for ?translating? a part of
a parse into generated text. A better parser would
be helpful here and we will try other alternatives for
dependency parsing in future work.
Pyr. Ling. Q. Overall R.
AS 0.315 2.174 2.304
Avg 0.309 2.820 2.576
Best 0.425 3.457 3.174
Models 0.785 4.910 4.760
AS Rank 29 39 29
Table 1: Scores of pyramid, linguistic quality and overall
responsiveness for our Abstractive Summarization (AS)
system, the average of automatic systems (Avg), the best
score of any automatic system (Best), and the average
of the human-written models (Models). The rank is com-
puted from amongst the 43 automatic summarization sys-
tems that participated in TAC 2010.
Although the linguistic quality was very low,
our approach was given relatively good Pyramid
(Nenkova et al, 2007) (a content metric) and overall
responsiveness scores, near the average of automatic
!"#"$%&'&()"* $%+,-'./"#"$%0'&0
$%$$$
$%'$$
+%$$$
+%'$$
,%$$$
,%'$$
(%$$$
(%'$$
$%$$$ $%'$$ +%$$$ +%'$$ ,%$$$ ,%'$$ (%$$$ (%'$$ 1%$$$
!"#$
%&&'(#
)*+,
)-"#,
#))
.-,/0-)1-2'30%&-14
AS' AS
!"#"$%&''()"*"+%,'-./"#"0%-+-1
0%000
0%,00
+%000
+%,00
1%000
1%,00
$%000
$%,00
(%000
0%000 0%0,0 0%+00 0%+,0 0%100 0%1,0 0%$00 0%$,0 0%(00 0%(,0
!"#$%
"&'"()
*%+,
"'-
.-/+0"1
AS AS'
Figure 3: Scatter plots of overall responsiveness with re-
spect to linguistic quality (top) and pyramid score with
respect to linguistic quality (bottom), for all the systems
competing in TAC 2010. The two runs identified with
an arrow, AS and AS?, were two similar versions of our
abstractive summarization approach.
systems. This indicates that, even in a rough first try
where content selection was not the main focus, our
method is capable of producing summaries with rea-
sonably good content and of reasonably good over-
all quality. There is a correlation between linguis-
tic quality and the other two manual scores for most
runs, but, as we can see in Figure 3, the two runs
that we submitted stand out, even though linguistic
quality plays a large role in establishing the overall
responsiveness scores. We believe this to be rep-
resentative of the great difference of our approach
compared to extraction. By extension, following the
trend, we hope that increasing the linguistic quality
of our approach to the level of the top systems would
yield content and overall scores above their current
ones.
The type of summaries that our approach pro-
duces might also explain why it receives good con-
tent and overall scores, even with poor linguistic
70
quality. The generated sentences tend to be short,
and although some few may have bad grammar or
even little meaning, the fact that we can pack a lot of
them shows that INITs give a lot more flexibility to
the content selection module than whole sentences,
that only few can fit in a small size limit such as
100 words. Large improvements are to be expected,
since this system was developped over only a few
months, and we haven?t implemented the full scale
of our framework described in section 2.
5 Related Work
We have already discussed alternative approaches to
abstractive summarization in the introduction. This
section focuses on other work dealing with the tech-
niques we used.
Subject?Verb?Object (SVO) extraction is not
new. Previous work by (Rusu et al, 2007) deals
specifically with what the authors call triplet extrac-
tion, which is the same as SVO extraction. They
have tried a variety of parsers, including MINIPAR,
and they build parse trees to extract SVOs simi-
larly to us. They applied this technique to extrac-
tive summarization in (Rusu et al, 2009) by building
what the authors call semantic graphs, derived from
triplets, and then using said graphs to identify the
most interesting sentences for the summary. This
purpose is not the same as ours, and triplet extrac-
tion was conducted quite superficially (and thus in-
cluded a lot of noise), whereas we used several rules
to clean up the SVOs that would serve as INITs.
Rewriting sentences one idea at a time, as we
have done in this work, is also related to the field
of text simplification. Text simplification has been
associated with techniques that deal not only with
helping readers with reading disabilities, but also to
help NLP systems (Chandrasekar et al, 1996). The
work of (Beigman Klebanov et al, 2004) simplifies
sentences by using MINIPAR parses as a starting
point, in a process similar to ours, for the purpose
of helping information-seeking applications in their
own task. (Vickrey and Koller, 2008) applies similar
techniques, using a sequence of rule-based simpli-
fications of sentences, to preprocess documents for
Semantic Role Labeling. (Siddharthan et al, 2004)
uses shallow techniques for syntactical simplifica-
tion of text by removing relative clauses and apposi-
tives, before running a sentence clustering algorithm
for multi-document summarization.
The kind of text-to-text generation involved in our
work is related to approaches in paraphrasing (An-
droutsopoulos and Malakasiotis, 2010). Paraphrase
generation produces sentences with similar mean-
ings, but paraphrase extraction from texts requires
a certain level of analysis. In our case, we are in-
terested both in reformulating specific aspects of a
sentence, but also in identifying parts of sentences
(INITs) with similar meanings, for content selection.
We believe that there will be more and more similar-
ities between our work and the field of paraphrasing
as we improve on our model and techniques.
6 Conclusion
We have proposed an ambitious new way of look-
ing at abstractive summarization, with our proposed
framework. We believe that this framework aims at
the real goals of automatic summarization ? control-
ling the content and structure of the summary. This
requires both an ability to correctly analyze text, and
an ability to generate text. We have described a first
attempt at fully abstractive summarization that relies
on text-to-text generation.
We find the early results of TAC 2010 quite sat-
isfactory. Receiving a low linguistic quality score
was expected, and we are satisfied with average per-
formance in content and in overall responsiveness.
It means that our text-to-text generation was good
enough to produce understandable summaries.
Our next step will be to go deeper into the analysis
of sentences. Generating sentences should rely less
on the original sentence structure and more on the
information meant to be transmitted. Thus, we want
to move away from the current way we generate sen-
tences, which is too similar to rule-based sentence
compression. At the core of moving toward full ab-
straction, we need to redefine INITs so that they can
be manipulated (compared, grouped, realized as sen-
tences, etc.) more effectively. We intend to use tools
and techniques that will enable us to find words and
phrases of similar meanings, and to allow the gener-
ation of a sentence that is an aggregate of informa-
tion found in several source sentences. In this way,
we would be moving away from purely syntactical
analysis and toward the use of semantics.
71
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. J. Artif. Int. Res., 38:135?187, May.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Beata Beigman Klebanov, Kevin Knight, and Daniel
Marcu. 2004. Text simplification for information-
seeking applications. In Robert Meersman and Zahir
Tari, editors, Proceedings of Ontologies, Dabases, and
Applications of Semantics (ODBASE) International
Conference, volume 3290 of Lecture Notes in Com-
puter Science, pages 735?747, Agia Napa, Cyprus,
October. Springer.
R. Chandrasekar, Christine Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of the 16th conference on Computational
linguistics - Volume 2, COLING ?96, pages 1041?
1044, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Shouyuan Chen, Yuanming Yu, Chong Long, Feng Jin,
Lijing Qin, Minlie Huang, and Xiaoyan Zhu. 2008.
Tsinghua University at the Summarization Track of
TAC 2008. In Proceedings of the First Text Analysis
Conference, Gaithersburg, Maryland, USA. National
Institute of Standards and Technology.
Trevor Cohn and Mirella Lapata. 2009. Sentence
compression as tree transduction. J. Artif. Int. Res.,
34(1):637?674.
John M. Conroy, Judith D. Schlesinger, Peter A. Rankel,
and Dianne P. O?Leary. 2010. CLASSY 2010: Sum-
marization and metrics. In Proceedings of the Third
Text Analysis Conference, Gaithersburg, Maryland,
USA. National Institute of Standards and Technology.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, Philadelphia, PA, USA.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a Re-
alisation Engine for Practical Applications. In ENLG
?09: Proceedings of the 12th European Workshop on
Natural Language Generation, pages 90?93, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Pierre-Etienne Genest and Guy Lapalme. 2010. Text
generation for abstractive summarization. In Proceed-
ings of the Third Text Analysis Conference, Gaithers-
burg, Maryland, USA. National Institute of Standards
and Technology.
Pierre-Etienne Genest, Guy Lapalme, Luka Nerima, and
Eric Wehrli. 2008. A Symbolic Summarizer for the
Update Task of TAC 2008. In Proceedings of the
First Text Analysis Conference, Gaithersburg, Mary-
land, USA. National Institute of Standards and Tech-
nology.
Pierre-Etienne Genest, Guy Lapalme, Luka Nerima, and
Eric Wehrli. 2009a. A symbolic summarizer with 2
steps of sentence selection for TAC 2009. In Proceed-
ings of the Second Text Analysis Conference, Gaithers-
burg, Maryland, USA. National Institute of Standards
and Technology.
Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-
Monod. 2009b. HexTac: the Creation of a Manual
Extractive Run. In Proceedings of the Second Text
Analysis Conference, Gaithersburg, Maryland, USA.
National Institute of Standards and Technology.
David Gillick, Benoit Favre, Dilek-Hakkani Tu?r, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
ICSI/UTD Summarization System at TAC 2009. In
Proceedings of the Second Text Analysis Conference,
Gaithersburg, Maryland, USA. National Institute of
Standards and Technology.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National Con-
ference on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intelli-
gence, pages 703?710. AAAI Press.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proc. Workshop on the Evaluation of Pars-
ing Systems, Granada.
Inderjeet Mani. 2001. Automatic Summarization, vol-
ume 3 of Natural Language Processing. John Ben-
jamins Publishing Company.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Trans. Speech Lang. Process., 4, May.
Karolina Owczarzak and Hoa Trang Dang. 2010.
Overview of the TAC 2009 summarization
track. In Proceedings of the Third Text Analy-
sis Conference, Gaithersburg, Maryland, USA.
National Institute of Standards and Technology.
http://www.nist.gov/tac/publications/.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40(6):919?938.
Delia Rusu, Lorand Dali, Blaz Fortuna, Marko Gro-
belnik, and Dunja Mladenic. 2007. Triplet extrac-
tion from sentences. Proceedings of the 10th Inter-
national Multiconference ?Information Society ? IS
2007?, A:218?222, October.
72
Delia Rusu, Blaz Fortuna, Marko Grobelnik, and Dunja
Mladenic. 2009. Semantic graphs derived from
triplets with application in document summarization.
Informatica, 33, October.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2004. Syntactic simplification for improving
content selection in multi-document summarization.
In Proceedings of the 20th international conference
on Computational Linguistics, COLING ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa,
Tadashi Kumano, and Naoto Kato. 2009. Syntax-
driven sentence revision for broadcast news summa-
rization. In Proceedings of the 2009 Workshop on Lan-
guage Generation and Summarisation, UCNLG+Sum
?09, pages 39?47, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Vickrey and Daphne Koller. 2008. Sentence
Simplification for Semantic Role Labeling. In Pro-
ceedings of ACL-08: HLT, pages 344?352, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
73
Proceedings of the 14th European Workshop on Natural Language Generation, pages 92?93,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Natural Language Generation and Summarization at RALI
Guy Lapalme
RALI - DIRO
Universite? de Montre?al
C.P. 6128, Succ. Centre-Ville
Montre?al, Que?bec, Canada, H3C 3J7
lapalme@iro.umontreal.ca
Processing language in written or spoken form,
in a mother tongue or in another language is a very
complex and important problem. Hence the idea
of building automatic or semi-automatic tools to
support people during their attempt to understand
what they read or to translate a given message into
an adequate linguistic form. Since the eighties,
I have worked with my students on many NLP
projects, this talk focusses on some of them, past
and present, dealing with generation and summa-
rization.
We have always thrived to produce working sys-
tems that deal with real texts or use data to pro-
duce texts that can be easily understood by hu-
mans. This fundamental motivation imposes some
challenging constraints but also produces interest-
ing payoffs. Given the fact that our lab is in French
speaking university in a mostly English speaking
country, we have often worked in either of these
languages and often in both.
1 Generation
PRE?TEXTE (Gagnon and Lapalme, 1996) was a
system for generating French texts conveying tem-
poral information. Temporal information and lo-
calization expressed by temporal adverbial and
verbal phrases was represented with DRT. Sys-
temic Grammar Theory was used to translate the
DRT representation into a syntactic form to pro-
duce the final text.
SPIN (Kosseim and Lapalme, 2000) deals with
a fundamental problem in natural language gener-
ation: how to organize the content of a text in a
coherent and natural way. From a corpus analy-
sis of French instructional texts, we determined 9
senses typically communicated in these texts and
7 rhetorical relations used to present them. We
then developed presentation heuristics to deter-
mine how the senses should be organized rhetor-
ically to create a coherent and natural text.
POSTGRAPHE (Fasciano and Lapalme, 2000)
generated a report integrating graphics and text
from a set of writer?s intentions. The system was
given data in tabular form and a declaration of the
types of values in the columns of the table. Also
indicated were intentions to be conveyed in the
graphics (e.g., compare two variables or show the
evolution of a set of variables) and the system gen-
erated a report in LATEX. PostGraphe also gener-
ated the accompanying text to help the reader fo-
cus on the important points of the graphics.
SIMPLENLG-EN-FR (Vaudry and Lapalme,
2013) is a bilingual adaptation of the English real-
izer SimpleNLG. Its French grammatical coverage
is equivalent to the English one and covers the es-
sential notions that are taught to learners of French
as a second language as defined by Le franc?ais
fondamental (1er Degre?). The French lexicon con-
tains a commonly used French vocabulary, includ-
ing function words. JSREAL is a work in progress
describing a French text realizer in Javascript that
can be easily embedded in a web browser. Its main
originality is the fact that it produces DOM ele-
ments and not text strings so that they can easily
produce parts of web pages from JSON inputs sent
by the server for example.
In a project of interactive generation, we de-
velop a cognitively inspired methodology to as-
sist people during the production process, as
the route between input and output can be full
of hurdles and quite long. For each step, we
want to develop web based applications that ad-
dress a specific problem and help induce some
pattern reaction in the production of language.
For the moment we have produced two proto-
types: DRILLTUTOR (Zock and Lapalme, 2010)
which is goal-oriented multilingual phrasebook
and WEBREG (Zock et al, 2012) to practice the
generation of appropriate referring expressions.
92
2 Summarization
Summarization is in principle strongly related to
NLG because it implies reading and understand-
ing one or many documents in order to produce a
short text describing the main ideas of the original.
Summarization approaches are often classified as
either abstractive or extractive, the former being
the selection of the most important sentences from
the original documents.
In much the same way as NLG has suffered
from the fact that it is often possible to trick the
readers with canned text or formatted templates,
abstractive summarization had to compete with ac-
ceptable results produced by scorers of sentences,
the ones with the best scores being then concate-
nated to produce a summary. In our group, we
tried to stay away from such approaches that in our
view did not give any new insights even though it
did not always allow us to win the summarization
competitions at DUC or TAC.
SUMUM (Saggion and Lapalme, 2002) ex-
plored the idea of dynamic summarization by tak-
ing a raw technical text as input and produced an
indicative-informative summary. The indicative
part of the summary identifies the topics of the
document, and the informative part elaborates on
some of these topics according to the reader?s in-
terest. SumUM motivates the topics, describes en-
tities, and defines concepts. This is accomplished
through a process of shallow syntactic and seman-
tic analysis, concept identification, and text regen-
eration.
LETSUM (Farzindar and Lapalme, 2004) de-
veloped an approach for the summarization of le-
gal documents by helping a legal expert determine
the key ideas of a judgment. It is based on the
exploration of the document?s architecture and its
thematic structures in order to build a table style
summary for improving coherency and readability
of the text. Although LetSUM extracted full sen-
tences from the original document, it reorganized,
merged and displayed different parts in order to
better give an idea of the document and focus the
reader, a legal expert, to the important parts.
ABSUM (Genest and Lapalme, 2013) intro-
duces a flexible and scalable methodology for ab-
stractive summarization that analyzes the source
documents using a knowledge base to identify pat-
terns in the the source documents and generate
summary text from them. This knowledge-based
approach allows for implicit understanding and
transformation of the source documents? content
because it is carefully crafted for the summariza-
tion task and domain of interest.
3 Conclusion
These examples illustrate some links that we have
established between generation and summariza-
tion over the last few years and that are promising
for the future of these two research areas.
References
Atefeh Farzindar and Guy Lapalme. 2004. Legal texts
summarization by exploration of the thematic struc-
tures and argumentative roles. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 27?34, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
M. Fasciano and G. Lapalme. 2000. Intentions in
the coordinated generation of graphics and text from
tabular data. Knowledge and Information Systems,
2(3):310?339, Aug.
M. Gagnon and G. Lapalme. 1996. From conceptual
time to linguistic time. Computational Linguistics,
22(1):91?127, March.
Pierre-Etienne Genest and Guy Lapalme. 2013. Ab-
sum: a knowledge-based abstractive summarizer.
Computational Linguistics, page 30 pages, July. In
preparation.
L. Kosseim and G. Lapalme. 2000. Choosing rhetor-
ical structures to plan instructional texts. Computa-
tional Intelligence, 16(3):408?445, Aug.
Horacio Saggion and Guy Lapalme. 2002. Generating
informative and indicative summaries with SumUM.
Computational Linguistics, 28(4):497?526, Dec.
Pierre-Luc Vaudry and Guy Lapalme. 2013. Adapt-
ing SimpleNLG for bilingual English-French real-
isation. In 14th European Conference on Natural
Language Generation, Sofia, Bulgaria, Aug. This
volume.
Michael Zock and Guy Lapalme. 2010. A generic tool
for creating and using multilingual phrasebooks. In
Bernadette Sharp and Michael Zock eds., editors,
Proceedings of NLPCS 2010 (Natural Language
Processing and Cognitive Science), pages 79?89,
Funchal, Madeira - Portugal, Jun.
Michael Zock, Guy Lapalme, and Mehdi Yousfi-
Monod. 2012. Learn to speak like normal people
do: the case of object descriptions. In 9th Interna-
tional Workshop on Natural Language Processing
and Cognitive Science (NLPCS 2012), pages 120?
129, Wraclow, jun.
93
Proceedings of the 14th European Workshop on Natural Language Generation, pages 183?187,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Adapting SimpleNLG for bilingual English-French realisation 
  Pierre-Luc Vaudry and Guy Lapalme RALI-DIRO ? Universit? de Montr?al C.P. 6128, Succ. Centre-Ville Montr?al, Qu?bec, Canada, H3C 3J8 {vaudrypl,lapalme}@iro.umontreal.ca     Abstract 
This paper describes SimpleNLG-EnFr, an adaption of the English realisation engine SimpleNLG (Gatt and Reiter, 2009) for bilin-gual English-French realisation. Grammatical similarities between English and French that could be exploited and specifics of French that needed adaptation are discussed. 1 Introduction Surface realisation is the last step in natural language generation. It takes as input an abstract representation where lexical units and syntactic structures have been determined. Its output is formatted natural language text. SimpleNLG, as described in Gatt and Reiter (2009), is a realisa-tion engine for English in the form of a Java li-brary. It handles inflection, derivation, word or-der, auxiliaries, agreement, pronominalisation, punctuation, spacing, etc. This paper describes SimpleNLG-EnFr 1.1 1 , a bilingual realisation engine for English and French derived from SimpleNLG 4.2, and explains the design choices and the challenges encountered. Grammatical similarities and differences between English and French that influenced the design are dis-cussed. The current version of SimpleNLG is 4.4, but all mentions of SimpleNLG in this paper refer to version 4.2. 2 Subset of French covered The English grammatical coverage of Sim-pleNLG-EnFr is the same as that of SimpleNLG 4.2. Its French grammatical coverage is equiva-lent to its English one.                                                 1 Available online, along with the source code, at http://www-etud.iro.umontreal.ca/~vaudrypl/snlgbil/snlgEnFr_english.html 
Le fran?ais fondamental (1er Degr?) (Minis-t?re de l'?ducation nationale, 1959) was used as a reference for French grammatical coverage. That document results from empirical studies and aims at describing the essential notions for teach-ing French as a foreign language. Almost all of the grammar points enumerated in this document are covered by SimpleNLG-EnFr. The detailed French grammar rules used in the implementa-tion come mainly from Grevisse (1993) and Mansouri (1996). SimpleNLG-EnFr has a 3871 entry default French lexicon covering L'?chelle Dubois-Buyse d'orthographe usuelle fran?aise (Ters et al, 1964). It contains the most important and com-monly used French vocabulary (including func-tion words), so as not to interfere with a particu-lar application domain vocabulary. A domain specific lexicon can easily be added as Sim-pleNLG supports using multiple lexicons. Most of the inflected forms in the default French lexi-con were taken from Morphalou 2.0 (CNRTL). 3 SimpleNLG parts pooled for English and French Most of the basic framework, which defined the class hierarchy covering lexical units, phrases and document elements such as paragraphs, could be kept in common for both English and French. Some shared grammar rules and princi-ples were put in abstract classes from which lan-guage-specific modules could be derived. The other grammar rules were rewritten for French, with the corresponding English ones serving as references. Many static methods in the English modules in SimpleNLG were changed to regular instance methods in order to be able to override them in the new subclasses. 
183
3.1 General characteristics Features: SimpleNLG uses a system of features for various functions: encoding morphological and syntactic properties of lexical units; letting the user set the parameters of a particular phrase (plural, verb tenses, etc.); and internally keeping track of the content of a phrase and various in-formation needed during realisation. This system is generic enough to be used for other languages. Most features are reusable and others can be added as needed. In SimpleNLG-EnFr, most of the already present features were reused for French. Lexicon: In SimpleNLG, the lexicon is al-ready relatively well separated from the gram-mar. The basic lexicon class provides an inter-face to a simple XML file containing the neces-sary information about the lexical units. The list of available fields in this file can easily be ex-tended by adding lexical features to the ones used for English. In SimpleNLG-EnFr, many lexical features were added mainly to account for the higher complexity of French morphology. 3.2 Syntax Verb phrase and clause: First, English and French have the same basic clause constituent order: Subject-Verb-Object (SVO). Even more importantly for SimpleNLG-EnFr, this constitu-ent order is relatively stable (compared with oth-er languages like German or Russian), at least for the purpose of practical NLG applications. This frees us in most cases from having to choose be-tween different syntactically correct word orders. We thus did not have to make such big changes to the syntactic representation as were needed in adapting SimpleNLG to German (Bollmann, 2011). Indeed, in German the subject has the same syntactic status in the clause than the ob-ject(s) and they can all occupy the same varying positions relative to the verb. However, Boll-mann (2011) had more leeway because he had decided not to keep the English grammar along-side the German one in his implementation. In contrast, in SimpleNLG-EnFr we wanted to be able to change freely between English and French grammars during the generation of a sin-gle text. English and French also have a very similar passive construction. In French, it is used less frequently because other options exist to avoid mentioning the subject of a sentence (for exam-ple, using the indefinite personal pronoun on), 
but choosing between those constructions is not the role of the realisation engine. Noun phrase: English and French can both have a determiner at the beginning of a noun phrase. Prepositional phrase: Both languages use prepositions (not postpositions) for introducing various complements. Coordinated phrase: Both have a coordina-tion conjunction in penultimate position and both use commas as separators between coordinates. 3.3 Morphology In both languages, nouns and verbs are marked morphologically for singular/plural. In addition, personal pronoun forms differ based not only on number and person, but also on grammatical function and gender. This last similarity facilitat-ed adapting pronominalisation. 4 Adaptations for French The rules for each processing level are encoded in separate modules for each language. The fol-lowing adaptations were made for French by adding syntactic and lexical features and encod-ing the corresponding rules in the French ver-sions of the grammar rules modules. 4.1 Syntax Verb phrase and clause: French negation has some similarities but also big differences with its English counterpart. It is usually expressed with not one but two adverbs (ne and pas), which come respectively before and after the first word of the verb group, as in example (1). Moreover, pas can be replaced by other negation auxiliaries to specify a different kind of negation, as in (2). Finally, no negation auxiliary is used (only ne) when the sentence already carries another nega-tive element, for example a negative indefinite pronoun as in (3).  (1) il ne parle pas ?he does not speak? (2) il ne parle plus he not speaks more ?he does not speak anymore? (3) personne ne parle nobody not speaks ?nobody speaks? In French, some complement pronouns, in-stead of being placed after the verb as in the reg-ular SVO word order, are placed just before it. Furthermore, some of them sometimes take in that case a different form. The rules governing 
184
the acceptable combinations and sequencings of those complements that can be cliticised in this way are very precise. Examples (4) and (5) illus-trate this phenomenon. (4) il la leur r?f?re he her them refers ?he refers her to them? (5) il nous r?f?re ? eux  he us refers to them ?he refers us to them? The complexity of French past participle agreement is well known, particularly because it manifests itself mostly in written French. French verbs can have ?tre (to be) or avoir (to have) as auxiliaries in compound tenses. This influences whether the past participle agrees with the sub-ject (?tre) or the direct object if it is placed be-fore the past participle (avoir). Combined with clitic complement pronouns and relative clauses, among others, it can get very complex. In addi-tion, French past participles are inflected in gen-der and number, like adjectives. Noun phrase: In SimpleNLG, a noun phrase can have pre-modifiers and post-modifiers. Ad-jectives are by default considered pre-modifiers and everything else post-modifiers. In contrast, in French, most adjectives are placed after the noun, but some (the most common) are most fre-quently placed before the noun. In SimpleNLG-EnFr this is achieved by referring to an extra lex-ical feature. In addition, in French the determiner and ad-jectives agree with the noun in number and gen-der. Instead of adding a new mechanism to prop-agate relevant features of the noun phrase to where they are needed, as with subject-verb agreement in SimpleNLG, the solution imple-mented was to let the determiner and adjectives get themselves the information they needed from their parent constituent. This more flexible way of managing agreement is more amenable to multilingual realisation. Interrogative clause: A simple way of build-ing an interrogative sentence in French is to pre-pend the expression est-ce que (is it that), like in (6). This is what we chose. (6) est-ce que tu as mang??  is it that you have eaten? ?did you eat?? This kind of interrogative clause can be built in part by using the relative clause rules (see be-low). Relative clause: A mechanism for building relative clauses has been added to the French part of SimpleNLG-EnFr that has no direct equivalent 
in the English implementation. The phrase that must be replaced by a relative pronoun is speci-fied by setting a feature on the clause. This phrase will not appear in the realised clause. Even if this phrase was not present in the clause, it will still be used to choose a relative pronoun, which can be useful. The grammatical function of that phrase can in that case be set manually.  The resulting relative pronoun takes the place that is normally reserved for the complementiser. Its form is chosen according to two sources: the grammatical function and preposition, if any, of the phrase it replaces; and the person and gender of its antecedent (the noun or pronoun that the relative clause modifies). Examples (7), (8) and (9) illustrate this. (7) la tarte que tu as mang?e  the pie that.obj you have eaten.fem ?the pie that you ate? (8) la tarte qui a ?t? mang?e  the pie that.subj has been eaten.fem ?the pie that was eaten? (9) l?homme dont j?ai mang? la tarte  the man whose I have eaten the pie ?the man whose pie I ate? 4.2 Morphology Number and gender: French determiners and adjectives must be inflected in number and gen-der. Additionally, number and gender interact with each other in the inflection process. Verb tenses: Verb inflected forms are more varied in French than in English. In addition, French verbs are classified in three conjugation groups. The first group is comprised of the regu-lar verbs. The third group is a catchall category for miscellaneous irregular verbs. Several mor-phological rules govern the combination of the verb inflection morphemes. Detached form of personal pronouns: In French, personal pronouns are often cliticised (see subsection 5.1), but where they are not, they take a different form, which is called forme dis-jointe (detached form). See leur versus eux in examples (4) and (5). 4.3 Morphophonology The morphophonological level is a new pro-cessing level introduced in SimpleNLG-EnFr to account for a range of phenomena very common in French and other languages. They are best de-scribed using rules that use both morphological and phonological conditions. The only obvious example of this kind of rule in written English, which was included in the morphology module 
185
in SimpleNLG, is illustrated by examples (10) and (11). (10) a + book ? a book (11) a + apple ? an apple Here the morphological condition is the pres-ence of the indefinite singular determiner a and the phonological one is the presence of a vowel at the beginning of the next word. The morphology rules operate on one word at a time. The morphophonology rules may need to have access to adjacent words and to be applied after all inflection and derivation rules have been applied. This justifies a separate processing level. In SimpleNLG-EnFr, the morphophonological level is used mainly for external sandhi, i.e. phe-nomena occurring at word boundaries. Elision: In French some words have their last vowel elided when in front of a word beginning by a vowel or a so-called h aspir? (aspired h). Indeed, an extra lexical feature is needed for French words beginning with the letter h to know if that kind of rule applies. Note that the letter h itself is never pronounced in French. Examples (12) and (13) illustrate elision, while it does not occur in (14). (12) la + amiti? ? l?amiti? the friendship (13) le + homme ? l?homme the man (14) la + honte ? la honte the shame Liaison: Liaison is a phenomenon akin to eli-sion, except that it involves adding and/or replac-ing phonemes. Its ?goal? is to avoid contact be-tween the vowel at the end of some words and the beginning vowel of the next word. It is most-ly apparent in speech, although it sometimes has an effect in written French, as in (15). (15) le + beau + homme ? le bel homme the handsome man Prepositions: Some prepositions interact with definite determiners in French, as in (16). (16) ? + le ? au at the 5 Bilingual generation Building a bilingual realisation engine rather than just adapting SimpleNLG for unilingual French realisation was a design choice dictated mainly by practical considerations. Being able to use the same realisation engine (and thus the same API) for several or all target languages when developing a multilingual NLG application is convenient. In the case of English and French, 
this could be most useful when targeting Canadi-an or European populations, for example. In SimpleNLG-EnFr, bilingual generation is implemented by being able to determine dynami-cally the language of each processing unit: phrases for the syntax module, lexical units for the morphology module, etc. The factories used by the library?s user to create syntactic structure specifications and access or create lexical units each use a language-specific lexicon. Each pro-cessing module then chooses at realisation time which set of rules to apply to a given processing unit based on the language of its lexicon. Thus, sentences, phrases and words of different lan-guages can be mixed freely. 6 Conclusion A bilingual realisation engine for English and French was built. It took five months to com-plete, including the writing of a detailed French manual. Despite many internal changes, it retains almost the same API as the original. Future improvements could include enlarging the default lexicon and adding specialised lexi-cons for French, implementing a complete textu-al representation for numbers, and adapting the changes in SimpleNLG since version 4.2, like the XML realiser. More languages could be added to Sim-pleNLG-EnFr. However, it would perhaps be easier to include many languages if the grammar of each language could be specified in a common grammar formalism, instead of programmatically in the processing modules themselves. This would necessitate changing the architecture. In the process of developing SimpleNLG-EnFr, a great deal was learned about what kind of challenges multilingual realisation poses. A common grammatical ground must be found and exploited for the group of languages considered, which should not be too far apart in that respect. For the rest, care must be taken not to make too many assumptions about the inner workings of the grammar of each language. Indeed, every language has its own grammatical peculiarities.  Acknowledgments  We would like to thank the SimpleNLG team for having made available its source code. This work was supported by two Undergraduate Student Research Awards (USRA) from the Natural Sci-ences and Engineering Research Council of Can-ada (NSERC). 
186
References Marcel Bollmann. 2011. Adapting SimpleNLG to German. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG 2011), pages 133-138. CNRTL. CNRTL : Centre National de Ressources Textuelles et Lexicales ? Morphalou, button T?l?-charger Morphalou 2.0, [http://www.cnrtl.fr/lexiques/morphalou/] (consult-ed on 14 July 2011). Albert Gatt and Ehud Reiter. 2009. SimpleNLG: A realisation engine for practical applications. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009), pages 90-93. Maurice Grevisse, (1993). Le bon usage, grammaire fran?aise, 12e ?dition refondue par Andr? Goosse, 8e tirage, ?ditions Duculot, Louvain-la-Neuve, Belgique. Mohammed Issaoui Mansouri, (1996). Le Mansouris, tous les verbes usuels enti?rement conjugu?s et or-thographi?s. CAPT, ?diteurs, Montr?al, Canada. Minist?re de l'?ducation nationale, Direction de la Coop?ration avec la Communaut? et l'?tranger (France) (1959). Le fran?ais fondamental (1er Degr?), Publication de l'Institut P?dagogique Na-tional, Paris, France. Fran?ois Ters, Daniel Reichenbach and Georges Mayer. (1964). L'?chelle Dubois-Buyse d'ortho-graphe usuelle fran?aise. Messeiller. 
187
