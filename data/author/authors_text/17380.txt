Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 530?534, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Kea: Expression-level Sentiment Analysis from Twitter Data
Ameeta Agrawal
Computer Science and Engineering
York University
Toronto, Canada
ameeta@cse.yorku.ca
Aijun An
Computer Science and Engineering
York University
Toronto, Canada
aan@cse.yorku.ca
Abstract
This paper describes an expression-level senti-
ment detection system that participated in the
subtask A of SemEval-2013 Task 2: Senti-
ment Analysis in Twitter. Our system uses a
supervised approach to learn the features from
the training data to classify expressions in new
tweets as positive, negative or neutral. The
proposed approach helps to understand the rel-
evant features that contribute most in this clas-
sification task.
1 Introduction
In recent years, Twitter has emerged as an ubiquitous
and an opportune platform for social activity. Ana-
lyzing the sentiments of the tweets expressed by an
international user-base can provide an approximate
view of how people feel. One of the biggest chal-
lenges of working with tweets is their short length.
Additionally, the language used in tweets is very
informal, with creative spellings and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, such as, RT
for ?re-tweet? and #hashtags, which are a type of
tagging for tweets. Although several systems tackle
the task of analyzing sentiments from tweets, the
task of analyzing sentiments at term or phrase-level
within a tweet has remained largely unexplored.
This paper describes the details of our expression-
level sentiment detection system that participated in
the subtask A of SemEval-2013 Task 2: Sentiment
Analysis in Twitter (Wilson et al, 2013). The goal
is to mark expressions (a term or short phrases) in
a tweet with their contextual polarity. This is chal-
lenging given the fact that the entire length of a tweet
is restricted to just 140 characters. We describe the
creation of an SVM classifier that is used to classify
the contextual polarity of expressions within tweets.
A feature set derived from various linguistic fea-
tures, parts-of-speech tagging and prior sentiment
lexicons was used to train the classifier.
2 Related Work
Sentiment detection from Twitter data has attracted
much attention from the research community in re-
cent times (Go et al, 2009; Pang et al, 2002; Pang
and Lee, 2004; Wilson et al, 2005; T. et al, 2012).
However, most of these approaches classify entire
tweets by their overall sentiment (positive, negative
or neutral).
The task at hand is to classify expressions with
their contextual sentiment. Most of these expres-
sions can be found in sentiment lexicons already an-
notated with their general polarity, but the focus of
this task is to detect the polarity of that expression
within the context of the tweet it appears in, and
therefore, given the context, the polarity of the ex-
pression might differ from that found in any lexicon.
One of the primary goals of this task is to facilitate
the creation of a corpus of tweets with sentiment ex-
pressions marked with their contextual sentiments.
Wilson, Wiebe and Hoffman (Wilson et al, 2005)
explored the challenges of contextual polarity of
sentiment expressions by first determining whether
an expression is neutral or polar and then disam-
biguating the polarity of the polar expressions. Na-
sukawa and Yi (Nasukawa and Yi, 2003) classified
530
the polarity of target expressions using manually de-
veloped patterns. Both these approaches, however,
experimented with general webpages and online re-
views but not Twitter data.
3 Task Setup
This paper describes the task of recognizing con-
textual sentiments of expressions within a tweet.
Formally, given a message containing a marked in-
stance of a word or a phrase, the task is to determine
whether that instance is positive, negative or neutral
in that context.
A corpus of roughly 8343 twitter messages was
made available by the task organizers, where each
tweet included an expression marked as positive,
negative or neutral. Also available was a develop-
ment data set containing 1011 tweets with similarly
marked expressions. The data sets included mes-
sages on a broad range of topics such as a mix-
ture of entities (e.g., Gadafi, Steve Jobs), products
(e.g., kindle, android phone), and events (e.g., Japan
earthquake, NHL playoffs). Keywords and hashtags
were used to identify and collect messages relevant
to the selected topic, which were then annotated us-
ing Mechanical Turk. Further details regarding the
task setup may be found in the task description paper
(Wilson et al, 2013).
The evaluation consisted of classifying 4435 ex-
pressions in a Twitter data set. Furthermore, to test
the generalizability of the systems, the task organiz-
ers provided a test data set consisting of 2334 SMS
messages, each containing a marked expression, for
which no prior training data set was made available.
4 System Description
Our aim by participating in the SemEval-2013 Sen-
timent Analysis in Twitter task was to investigate
what features are most useful in distinguishing the
different polarities. The various steps of building our
system are described in detail as follows.
4.1 Tokenization
Tweets are known for being notoriously noisy due
to their length restricted to just 140 characters which
forces users to be creative in order to get their mes-
sages across. This poses an inherent challenge when
analyzing tweets which need to undergo some sig-
nificant preprocessing. The first step includes tok-
enizing the words in the tweet. Punctuation is identi-
fied during the tokenization process and marked for
inclusion as one of the features in the feature set.
This includes Twitter-specific punctuation such as
?#? hashtags, specific emoticons such as ?:)? and
any URL links are replaced by a ?URL? placeholder.
4.2 n-gram features
Each expression consists of one or more words, with
the average number of words in an expression in the
training data set found to be 2. We derive lower-case
unigram and bigram as well as the full string features
from the expressions which are represented by their
frequency counts in the feature set. The n-grams
were cleaned (stripped of any punctuation) before
being included in the feature set as they were ob-
served to provide better results than noisy n-grams.
Note that the presence of punctuation did become a
part of the feature set as described in 4.3. We also
experimented with word-splitting, especially found
in hashtags (e.g., #iamsohappy); however, contrary
to our initial supposition, this step resulted in poorer
results overall due to word-splitting error propaga-
tion and was therefore avoided.
4.3 POS tagging
For tagging the various parts-of-speech of a tweet,
we use the POS tagger (Gimpel et al, 2011) that is
especially designed to work with English data from
Twitter. The tagging scheme encompasses 25 tags
(please see (Gimpel et al, 2011) for the full listing),
including some Twitter-specific tags (which could
make up as much as 13% of all tags as shown in
their annotated data set) such as ?#? hashtag (indi-
cates topic/category for tweet), ?@? at-mention (in-
dicates another user as a recipient of a tweet), ?RT?
re-tweets and URL or email addresses. The punctu-
ation (such as ?:-)?, ?:b?, ?(:?, amongst others) from
the n-grams is captured using the ?emoticon? and
?punctuation? tags that are explicitly identified by
this POS tagger trained especially for tweets.
Table 1 shows an example using a subset of two
POS tags for an expression (# Adj. and # Emoti-
con denotes the number of adjectives and emoticons
respectively). Other POS tags include nouns (NN),
verbs (VB) and so on. Features incorporating the
information about the parts-of-speech of the expres-
531
Esperance will be without star player Youssef Msakni for the first leg of the
Champions League final against Al Ahly on Saturday. #AFRICA
Prior Polarity Length POS in Expression POS in Tweet n-grams
Pos. Neg. Exp. Tweet #Adj. #Emoticon #Adj. #NN ?without? ?star? ?without star? ...
0 0 3 23 0 0 1 13 1 1 1 ...
Table 1: Sample feature set for an expression (denoted in bold)
sion as well as the tweet denoted by their frequencies
produced better results than using a binary notation.
Hence frequency counts were used in the feature set.
4.4 Prior sentiment lexicon
A prior sentiment lexicon was generated by combin-
ing four already existing polarity lexicons including
the Opinion Lexicon (Hu and Liu, 2004), the Sen-
tiWordNet (Esuli and Sebastiani, 2006), the Subjec-
tivity Clues database (Wilson et al, 2005) and the
General Inquirer (Stone and Hunt, 1963). If any
of the words in the expression are also found in the
prior sentiment lexicon, then the frequencies of such
prior positive and negative words are included as
features in the feature set.
4.5 Other features
Other features found to be useful in the classification
process include the length of the expression as well
as the length of the tweet. A sample of the feature
set is shown in Table 1.
4.6 Classifier
During development time, we experimented with
different classifiers but in the end, the Support Vec-
tor Machines (SVM), using the polynomial kernel,
trained over tweets from the provided train and de-
velopment data outperformed all the other classi-
fiers. The final feature set included four main fea-
tures plus the n-grams as well as the features depict-
ing the presence or absence of a POS in the expres-
sion and the tweet.
5 Experiments and Discussion
The task organizers made available a test data set
composed of 4435 tweets where each tweet con-
tained an instance of an expression whose sentiment
was to be detected. Another test corpus of 2334
SMS messages was also used in the evaluation to
test how well a system trained on tweets generalizes
on other data types.
The metric for evaluating the systems is F-
measure. We participated in the ?constrained? ver-
sion of the task which meant working with only the
provided training data and no additional tweets/SMS
messages or sentences with sentiment annotations
were used. However, other resources such as sen-
timent lexicons can be incorporated into the system.
Table 2, which presents the results of our submis-
sion in this task, lists the F-score of the positive,
negative and neutral classes on the Twitter test data,
whereas Table 3 lists the results of the SMS mes-
sage data. As it can be observed from the results,
the negative sentiments are classified better than the
positive ones. We reckon this may be due to the
comparatively fewer ways of expressing a positive
emotion, while the negative sentiment seems to have
a much wider vocabulary (our sentiment lexicon has
25% less positive words than negative). Whereas
the positive class has a higher precision, the nega-
tive class seems to have a more notable recall. The
most striking observation, however, is the extremely
low F-score for the neutral class. This may be due to
the highly skewed proportion (less than 5%) of neu-
tral instances in the training data. In future work, it
will be interesting to see how balancing out the pro-
portions of the three classes affects the classification
accuracy.
We also ran some ablation experiments on the
provided Twitter and SMS test data sets after the
submission. Table 4 reports the findings of exper-
iments where, for example, ?- prior polarities? in-
dicates a feature set excluding the prior polarities.
The metric used here is the macro-averaged F-score
of the positive and the negative class. The baseline
measure implements a simple SVM classifier using
only the words as unigram features in the expres-
sion. Interestingly, contrary to our hypothesis dur-
532
ing development time, using the POS of the entire
tweet was the least helpful feature. Since this was
an expression level classification task, it seems that
using the POS features of the entire tweet may mis-
guide the classifier. Unsurprisingly, the prior po-
larities turned out to be the most important part of
the feature set for this classification task as it seems
that many of the expressions? contextual polarities
remained same as their prior polarities.
Class Precision Recall F-score
Positive 0.93 0.47 0.62
Negative 0.50 0.95 0.65
Neutral 0.15 0.12 0.13
Macro-average 0.6394
Table 2: Submitted results: Twitter test data
Class Precision Recall F-score
Positive 0.85 0.39 0.53
Negative 0.59 0.96 0.73
Neutral 0.18 0.06 0.09
Macro-average 0.6327
Table 3: Submitted results: SMS test data
Twitter SMS
Baseline 0.821 0.824
Full feature set (submitted) 0.639 0.632
- Prior polarities 0.487 0.494
- Lengths 0.612 0.576
- POS expressions 0.646 0.615
- POS tweets 0.855 0.856
Table 4: Macro-averaged F-score results using different
feature sets
6 Conclusion
This paper presented the details of our system which
participated in the subtask A of SemEval-2013 Task
2: Sentiment Analysis in Twitter. An SVM classifier
was trained on a feature set consiting of prior po-
larities, various POS and other Twitter-specific fea-
tures. Our experiments indicate that prior polari-
ties from sentiment lexicons are significant features
in this expression level classification task. Further-
more, a classifier trained on just tweets can general-
ize considerably well on SMS message data as well.
As part of our future work, we would like to explore
what features are more helpful in not only classify-
ing the positive class better, but also distinguishing
neutrality from polarity.
Acknowledgments
We would like to thank the organizers of this task
and the reviewers for their useful feedback. This
research is funded in part by the Centre for In-
formation Visualization and Data Driven Design
(CIV/DDD) established by the Ontario Research
Fund.
References
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opin-
ion mining. In In Proceedings of the 5th Conference
on Language Resources and Evaluation (LRECa?06,
pages 417?422.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 42?47, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In Proceedings of the 2nd international
conference on Knowledge capture, K-CAP ?03, pages
70?77, New York, NY, USA. ACM.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
533
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing - Volume 10, EMNLP ?02, pages 79?
86, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philip J. Stone and Earl B. Hunt. 1963. A computer ap-
proach to content analysis: studies using the general
inquirer system. In Proceedings of the May 21-23,
1963, spring joint computer conference, AFIPS ?63
(Spring), pages 241?256, New York, NY, USA. ACM.
Amir Asiaee T., Mariano Tepper, Arindam Banerjee, and
Guillermo Sapiro. 2012. If you are happy and you
know it... tweet. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, CIKM ?12, pages 1602?1606, New
York, NY, USA. ACM.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter. In
Proceedings of the International Workshop on Seman-
tic Evaluation, SemEval ?13, June.
534
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380?384,
Dublin, Ireland, August 23-24, 2014.
Kea: Sentiment Analysis of Phrases Within Short Texts
Ameeta Agrawal, Aijun An
Department of Computer Science and Engineering
York University, Toronto, Canada M3J 1P3
{ameeta, aan}@cse.yorku.ca
Abstract
Sentiment Analysis has become an in-
creasingly important research topic. This
paper describes our approach to building a
system for the Sentiment Analysis in Twit-
ter task of the SemEval-2014 evaluation.
The goal is to classify a phrase within a
short piece of text as positive, negative
or neutral. In the evaluation, classifiers
trained on Twitter data are tested on data
from other domains such as SMS, blogs as
well as sarcasm. The results indicate that
apart from sarcasm, classifiers built for
sentiment analysis of phrases from tweets
can be generalized to other short text do-
mains quite effectively. However, in cross-
domain experiments, SMS data is found to
generalize even better than Twitter data.
1 Introduction
In recent years, new forms of communication such
as microblogging and text messaging have become
quite popular. While there is no limit to the range
of information conveyed by tweets and short texts,
people often use these messages to share their sen-
timents. Working with these informal text gen-
res presents challenges for natural language pro-
cessing beyond those typically encountered when
working with more traditional text genres. Tweets
and short texts are shorter, the language is very
informal, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology such as, RT for ?re-tweet?
and #hashtags for tagging (Rosenthal et al., 2014).
Although several systems have tackled the task
of analyzing sentiment from entire tweets, the
task of analyzing sentiments of phrases (a word
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
or more) within a tweet has remained largely un-
explored. This paper describes the details of
our system that participated in the subtask A
of Semeval-2014 Task 9: Sentiment Analysis in
Twitter (Rosenthal et al., 2014). The goal of this
task is to determine whether a phrase within a mes-
sage is positive, negative or neutral in that context.
Here, a message indicates any short informal piece
of text such as a tweet, SMS data, or a sentence
from Live Journal blog, which is a social network-
ing service where Internet users keep an online di-
ary. A phrase could be a word or a few consecutive
words within a message.
The novelty of this task lies in the fact that a
model built using only Twitter data is used to clas-
sify instances from other short text domains such
as SMS and Live Journal. Moreover, a short test
corpus of sarcastic tweets is also used to test the
performance of the sentiment classifier.
The main contributions of this paper include
a) developing a sentiment analysis classifer for
phrases; b) training on Twitter data and testing on
other domains such as SMS and Live Journal data
to see how well the classifier generalizes to differ-
ent types of text, and c) testing on sarcastic tweets.
2 Related Work
Sentiment analysis from Twitter data has attracted
much attention from the research community in
the past few years (Asiaee T. et al., 2012; Go et
al., 2009; Pang et al., 2002; Pang and Lee, 2004;
Wilson et al., 2005). However, most of these
approaches classify entire tweets by their overall
sentiment (positive, negative, or neutral).
The task at hand is to classify the sentiment of a
phrase within a short message. The challenges of
classifying contextual polarity of phrases has been
previously explored by first determining whether
the phrase is neutral or polar, and then disam-
biguating the polarity of the polar phrases (Wil-
son et al., 2005). Another approach entails using
380
manually developed patterns (Nasukawa and Yi,
2003). Both these techniques, however, experi-
mented with general web pages and online reviews
but not Twitter data.
Previously, a few systems that participated in
Semeval-2013: Sentiment Analysis in Twitter task
(Wilson et al., 2013; Mohammad et al., 2013;
Gunther and Furrer, 2013) tackled the problem of
sentiment analysis of phrases by training on data
that exclusively came from tweets and tested on
a corpus made up of tweets and SMS data. This
time though, the task is to see how well a system
trained on tweets will perform on not only SMS
data, but also blog sentences from Live Journal, as
well as sarcastic tweets.
3 Task Setup
Formally, given a message containing a phrase
(one or more words), the task is to determine
whether that phrase is positive, negative or neutral
in that context. We were able to download 8880
tweets (7910 for training, and 970 for develop-
ment) from the corpus made available by the task
organizers, where each tweet includes a phrase
marked as positive, negative or neutral. Keywords
and hashtags were used to identify and collect
messages, which were then annotated using Ama-
zon Mechanical Turk. This task setup is further
described in the task description paper (Rosenthal
et al., 2014).
The evaluation consists of Twitter data as well
as surprise genres such as SMS, Live Journal and
Twitter Sarcasm. The purpose of hidden test gen-
res was to see how well a system trained on tweets
will perform on previously unseen domains.
4 System Description
This section describes the system components.
4.1 Supervised Machine Learning
During development time, we experimented with
various supervised machine learning classifiers,
but the final model was trained using Support Vec-
tor Machines (SVM) with a linear kernel as it out-
performed all other classifiers. The c value was
empirically selected and set to 1.
4.2 Features
For all tweets, the URL links and @username
mentions are replaced by ?URL? and ?username?
placeholders, respectively. The following features
were included in the final model:
? Prior polarities: Previous research (Agrawal
and An, 2013; Mohammad et al., 2013) has
shown prior polarities of words to be one
of the most important features in contex-
tual sentiment analysis of phrases. So, for
one of the features, the sum of the sentis-
cores of all the terms in the phrase was com-
puted from SentiWordNet (Esuli and Sebas-
tiani, 2006). For another feature, the prior
polarity of the phrase was estimated by aver-
aging the positive/negative strength of all its
terms by looking them up in the Subjectivity
Clues database (Wilson et al., 2005).
? Emoticons: An emoticon lexicon containing
frequent positive and negative emoticons, as
well as some of their misspellings that are
generally found in tweets, was created manu-
ally
1
. The prior positive and negative emoti-
con features contain the counts of all positive
and negative emoticons in the phrase.
? Lengths: Counts of the total number of words
in the phrase, the average number of char-
acters in the phrase, and the total number of
words in the message were included.
? Punctuation: Whether the phrase contains
punctuation such as ???, ?!?, ?...?, etc.
? Clusters: Word cluster IDs were obtained for
each term via unsupervised Brown clustering
of tweets (Owoputi et al., 2013). For exam-
ple, words such as anyone, anybody, any1,
ne1 and anyonee are all represented by clus-
ter path 0111011110. This allows grouping
multiple (mis)spellings of a word together,
which would otherwise be unique unigrams.
? Unigrams: Each phrase consists of one or
more words, with the average number of
words in a phrase being 2. We used only un-
igrams as bigrams were found to reduce the
accuracy on the development set.
5 Experiments and Discussion
The task organizers made available a test data set
composed of 10681 instances. Table 1 describes
1
http://goo.gl/fh6Pjr
381
Test sets (# instances) Sentiment Example Phrase to be classified (in bold)
Twitter (6908) positive No school at the Cuse till Wednesday #hyped
negative i know it?s in january, but i can?t wait for Winter Jam !
neutral Bye bye Kyiv! See you in December :-*
SMS (2334) positive later on wanna catch a movie?
negative U had ur dinner already? She just wont believe wat i said, haiz..
neutral Im free on sat ... Ok we watch together lor
LiveJournal (1315) positive And Tess you are going to prom too on the same day as us as well
negative Does not seem likely that there would be any confusion .
neutral if i am ever king i will make it up to you .
TwitterSarcasm (124) positive @ImagineMore CHEER up. It?s Monday after all. #mondayblues
negative I may or may not be getting sick...perfect. #idontwantit
neutral @Ken Rosenthal mistakes? C?mon Kenny!! ;)
Table 1: Test corpus details.
the breakdown of the various types of text, with
example phrases that are to be classified.
As expected, Live Journal has a slightly more
formal sentence structure with properly spelt
words, whereas Twitter and SMS data include
more creative spellings. Clearly, the sarcasm cat-
egory includes messages with two contradictory
sentiments in close proximity. The challenge of
this task lies precisely in the fact that one classifier
trained on Twitter data should be able to general-
ize reasonably well on different types of text.
5.1 Task Results
We participated in the constrained version of the
task which meant working with only the provided
Twitter training data without any additional an-
notated messages. The macro-average F1-scores
of the positive and negative classes, which were
the evaluation criteria for the task, of our sys-
tem (trained on Twitter training data and tested on
Twitter test, SMS and Live Journal blog data) are
presented in Table 2.
There are two interesting observations here:
firstly, even though the classifier was trained solely
on tweets, it performs equally well on SMS and
Live Journal data; and secondly, the sarcasm cate-
gory has the poorest overall performance, unsur-
prisingly. This suggests that cross-domain sen-
timent classification of phrases in short texts is
a feasible option. However, sarcasm seems to
be a subtle sentiment and calls for exploring fea-
tures that capture not only semantic but also syn-
tactic nuances. The low recall of the negative
sarcastic instances could be due to the fact that
30% of the negative phrases are hashtags (e.g.,
#don?tjudge, #smartmove, #killmenow, #sadlife,
#runninglate, #asthmaticproblems, #idontwantit),
that require term-splitting.
Further analysis reveals that generally the pos-
itive class has better F1-scores than the negative
class across all domains, except for the SMS data.
One possible reason for this could be the fact that,
while in all data sets (Twitter train, Twitter test,
Sarcasm test) the ratio of positive to negative in-
stances is nearly 2:1, the SMS test set is the only
one with class distribution different from the train-
ing set (with less positive instances than negative).
The extremely low F1-score for the neutral class is
perhaps also due to the skewed class distribution,
where in all data sets, the neutral instances only
make up about 4 to 9% of the data.
The positive class also has a better recall than
the negative class across all domains, which sug-
gests that the system is able to identify most of the
positive test instances, perhaps due to the bigger
proportion of positive training instances as well as
positive words in the polarity lexicons. One simple
way of improving the recall of the negative class
could be by increasing the number of negative in-
stances in the training set. In fact, in a prelimi-
nary experiment with an increased number of neg-
ative instances (resampled using SMOTE (Chawla
et al., 2002)), the macro-average F1-score of the
SMS data set improved by 0.5 points and that of
the Sarcasm set by almost 2 points. However,
there was no notable improvement in the Twitter
and Live Journal test sets.
We also ran some ablation experiments on the
test corpus after the submission to observe the in-
fluence of individual features on the classification
382
POS. NEG. NEU. AVG.
P R F P R F P R F
Twitter 87.6 89.7 88.6 82.4 76.2 79.2 23.3 28.2 25.5 83.90
SMS 75.9 89.9 82.3 89.8 82.4 86.0 32.7 10.7 16.1 84.14
LiveJournal 76.1 87.3 81.3 81.8 80.2 81.0 42.1 16.7 23.9 81.16
Sarcasm 77.0 93.9 84.6 72.2 35.1 47.3 16.7 20.0 18.2 65.94
Table 2: Macro-average F1-scores. P, R and F represent precision, recall and F1-score, respectively.
process. Table 3 reports the macro-average F1-
scores of the experiments. The ?all features*?
scores here are different from those submitted as
the four test corpora were tested individually here
as opposed to all instances mixed into one data set.
The row ?- prior polarities? indicates a feature set
that excludes the prior polarities feature, and its ef-
fect on the F1-score. MCB is the Majority Class
Baseline, whereas unigrams uses only the phrase
unigrams, with no additional features.
Twitter SMS Jour. Sarc.
MCB 39.65 31.45 33.40 39.80
unigrams 81.85 82.15 79.95 74.85
all features* 86.20 87.80 81.90 78.05
- prior polarity -1.8 -0.1 -0.05 -1.95
- lengths -0.3 0 -0.20 -1.3
- punctuation -0.45 -0.45 +0.10 -2.95
- emoticon lex -0.15 0 +0.05 0
- word clusters -0.15 -1.25 +0.05 -0.25
Table 3: Ablation tests: Trained on Twitter only.
A few observations from the feature ablation
study include:
? The prior polarities and lengths seem to be
two of the most distinguishing features for
Twitter and Twitter Sarcasm, whereas for
SMS data, the word clusters are quite useful.
? While for Twitter Sarcasm, punctuation
seems to be the most important feature, it
has the opposite effect on the Live Journal
blog data. This may be because the punctua-
tion features learned from Twitter data do not
translate that well to blog data due to their
dissimilar writing styles.
? Even though the classifier was trained on
Twitter data, it has quite a strong performance
on the SMS data, which is rather unsurprising
in retrospect as both genres have similar char-
acter limits, which leads to creative spellings
and slang.
? While using all the features leads to almost 5
F1-score points improvement over unigrams
baseline in Twitter, SMS and Sarcasm data
sets, they increase only 2 F1-score points in
Live Journal blog data set, suggesting that
this feature set is only marginally suited for
blog instances. This prompted us to explore
the hypothesis: how well do SMS and Live
Journal data generalize to other domains, dis-
cussed in the following section.
5.2 Cross-domain Experiments
In this section, we test how well the classifiers
trained on one type of text classify other types of
text. In table 4, for example, the last row shows the
results of a model trained on Journal data (1000 in-
stances) and tested on Twitter, SMS and Sarcasm
test sets, and 10-fold cross-validated on Journal
data. Since this experiment measures the gener-
alizability of different data sets, we randomly se-
lected 500 positive and 500 negative instances for
each data set, in order to minimize the influence
of the size of the training data set on the classifi-
cation process. Note that this experiment does not
include the neutral class. As expected, the best
results on the test sets are obtained when using
cross-validation (except on Twitter set). However,
the model built using SMS data has the best or the
second-best result overall, which suggests that out
of the three types of text, it is the SMS data that
generalize the best.
Test
Twitter SMS Journal
Twitter (1000) 76.4 (cv) 80.2 78.1
SMS (1000) 76.8 87.1 (cv) 79.4
Journal (1000) 73.8 82.8 85.3 (cv)
Table 4: Cross-domain training and tests.
383
6 Conclusion
This paper presents the details of our system that
participated in the subtask A of SemEval:2014:
Sentiment Analysis in Twitter. An SVM classifier
was trained on a feature set consisting of prior po-
larities, word clusters and various Twitter-specific
features. Our experiments indicate that prior po-
larities are one of the most important features in
the sentiment analysis of phrases from short texts.
Furthermore, a classifier trained on just tweets can
generalize considerably well to other texts such
as SMS and blog sentences, but not to sarcasm,
which calls for more research. Lastly, SMS data
generalizes to other texts better than Twitter data.
Acknowledgements
We would like to thank the organizers of this task
for their effort and the reviewers for their use-
ful feedback. This research is funded in part by
the Centre for Information Visualization and Data
Driven Design (CIV/DDD) established by the On-
tario Research Fund.
References
Ameeta Agrawal and Aijun An. 2013. Kea:
Expression-level sentiment analysis from Twitter
data. In Proceedings of the Seventh International
Workshop on Semantic Evaluation, SemEval ?13,
June.
Amir Asiaee T., Mariano Tepper, Arindam Banerjee,
and Guillermo Sapiro. 2012. If you are happy and
you know it... tweet. In Proceedings of the 21st
ACM International Conference on Information and
Knowledge Management, CIKM ?12, pages 1602?
1606, New York, NY, USA. ACM.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. SMOTE:
Synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 16(1):321?
357, June.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation, pages
417?422.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?6.
Tobias Gunther and Lenz Furrer. 2013. Gu-mlt-lt:
Sentiment analysis of short messages using linguis-
tic features and stochastic gradient descent. In Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation, SemEval ?13, June.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the seventh international workshop on
Semantic Evaluation Exercises (SemEval-2013), At-
lanta, Georgia, USA, June.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural lan-
guage processing. In Proceedings of the 2nd inter-
national conference on Knowledge capture, K-CAP
?03, pages 70?77, New York, NY, USA. ACM.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using summarization based
on minimum cuts. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?04, Stroudsburg, PA, USA.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. Semeval-2013 task 9:
Sentiment analysis in twitter. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), August.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation (SemEval-2013), June.
384
