Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076?1085,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Re-Ranking Models Based-on Small Training Data for Spoken Language
Understanding
Marco Dinarelli
University of Trento
Italy
dinarelli@disi.unitn.it
Alessandro Moschitti
University of Trento
Italy
moschitti@disi.unitn.it
Giuseppe Riccardi
University of Trento
Italy
riccardi@disi.unitn.it
Abstract
The design of practical language applica-
tions by means of statistical approaches
requires annotated data, which is one of
the most critical constraint. This is par-
ticularly true for Spoken Dialog Systems
since considerably domain-specific con-
ceptual annotation is needed to obtain ac-
curate Language Understanding models.
Since data annotation is usually costly,
methods to reduce the amount of data are
needed. In this paper, we show that bet-
ter feature representations serve the above
purpose and that structure kernels pro-
vide the needed improved representation.
Given the relatively high computational
cost of kernel methods, we apply them to
just re-rank the list of hypotheses provided
by a fast generative model. Experiments
with Support Vector Machines and differ-
ent kernels on two different dialog cor-
pora show that our re-ranking models can
achieve better results than state-of-the-art
approaches when small data is available.
1 Introduction
Spoken Dialog Systems carry out automatic
speech recognition and shallow natural language
understanding by heavily relying on statistical
models. These in turn need annotated data de-
scribing the application domain. Such annotation
is far the most expensive part of the system de-
sign. Therefore, methods reducing the amount of
labeled data can speed up and lower the overall
amount of work.
Among others, Spoken Language Understand-
ing (SLU) is an important component of the sys-
tems above, which requires training data to trans-
late a spoken sentence into its meaning repre-
sentation based on semantic constituents. These
are conceptual units instantiated by sequences of
words.
In the last decade two major approaches have
been proposed to automatically map words in con-
cepts: (i) generative models, whose parameters re-
fer to the joint probability of concepts and con-
stituents; and (ii) discriminative models, which
learn a classification function based on conditional
probabilities of concepts given words.
A simple but effective generative model is the
one based on Finite State Transducers. It performs
SLU as a translation process from words to con-
cepts using Finite State Transducers (FST). An ex-
ample of discriminative model used for SLU is the
one based on Support Vector Machines (SVMs)
(Vapnik, 1995), as shown in (Raymond and Ric-
cardi, 2007). In this approach, data is mapped into
a vector space and SLU is performed as a clas-
sification problem using Maximal Margin Clas-
sifiers (Vapnik, 1995). A relatively more recent
approach for SLU is based on Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001). CRFs
are undirected graphical and discriminative mod-
els. They use conditional probabilities to account
for many feature dependencies without the need of
explicitly representing such dependencies.
Generative models have the advantage to be
more robust to overfitting on training data, while
discriminative models are more robust to irrele-
vant features. Both approaches, used separately,
have shown good accuracy (Raymond and Ric-
cardi, 2007), but they have very different charac-
teristics and the way they encode prior knowledge
is very different, thus designing models that take
into account characteristics of both approaches are
particularly promising.
In this paper, we propose a method for SLU
based on generative and discriminative models:
the former uses FSTs to generate a list of SLU
hypotheses, which are re-ranked by SVMs. To
effectively design our re-ranker, we use all pos-
1076
sible word/concept subsequences with gaps of the
spoken sentence as features (i.e. all possible n-
grams). Gaps allow for encoding long distance de-
pendencies between words in relatively small se-
quences. Since the space of such features is huge,
we adopted kernel methods, i.e. sequence kernels
(Shawe-Taylor and Cristianini, 2004) and tree ker-
nels (Collins and Duffy, 2002; Moschitti, 2006a)
to implicitly encode them along with other struc-
tural information in SVMs.
We experimented with different approaches for
training the discriminative models and two differ-
ent corpora: the french MEDIA corpus (Bonneau-
Maynard et al, 2005) and a corpus made available
by the European project LUNA
1
(Dinarelli et al,
2009b). In particular, the new contents with re-
spect to our previous work (Dinarelli et al, 2009a)
are:
? We designed a new sequential structure
(SK2) and two new hierarchical tree struc-
tures (MULTILEVEL and FEATURES) for
re-ranking models (see Section 4.2). The lat-
ter combined with two different tree kernels
originate four new different models.
? We experimented with automatic speech
transcriptions thus assessing the robustness to
noise of our models.
? We compare our models against Conditional
Random Field (CRF) approaches described
in (Hahn et al, 2008), which are the cur-
rent state-of-the-art in SLU. Learning curves
clearly show that our models improve CRF,
especially when small data sets are used.
The remainder of the paper is organized as fol-
lows: Section 2 introduces kernel methods for
structured data, Section 3 describes the generative
model producing the initial hypotheses whereas
Section 4 presents the discriminative models for
re-ranking them. The experiments and results
are reported in Section 5 and the conclusions are
drawn in Section 6.
2 Feature Engineering via Structure
Kernels
Kernel methods are viable approaches to engi-
neer features for text processing, e.g. (Collins and
Duffy, 2002; Kudo and Matsumoto, 2003; Cumby
1
Contract n. 33549
and Roth, 2003; Cancedda et al, 2003; Culotta
and Sorensen, 2004; Toutanova et al, 2004; Kudo
et al, 2005; Moschitti, 2006a; Moschitti et al,
2007; Moschitti, 2008; Moschitti et al, 2008;
Moschitti and Quarteroni, 2008). In the follow-
ing, we describe structure kernels, which will be
used to engineer features for our discriminative re-
ranker.
2.1 String Kernels
The String Kernels that we consider count the
number of substrings containing gaps shared by
two sequences, i.e. some of the symbols of the
original string are skipped. We adopted the ef-
ficient algorithm described in (Shawe-Taylor and
Cristianini, 2004; Lodhi et al, 2000). More
specifically, we used words and markers as sym-
bols in a style similar to (Cancedda et al, 2003;
Moschitti, 2008). For example, given the sen-
tence: How may I help you ? sample substrings,
extracted by the Sequence Kernel (SK), are: How
help you ?, How help ?, help you, may help you,
etc.
2.2 Tree kernels
Tree kernels represent trees in terms of their sub-
structures (fragments). The kernel function detects
if a tree subpart (common to both trees) belongs to
the feature space that we intend to generate. For
such purpose, the desired fragments need to be de-
scribed. We consider two important characteriza-
tions: the syntactic tree (STF) and the partial tree
(PTF) fragments.
2.2.1 Tree Fragment Types
An STF is a general subtree whose leaves can
be non-terminal symbols (also called SubSet Tree
(SST) in (Moschitti, 2006a)). For example, Fig-
ure 1(a) shows 10 STFs (out of 17) of the sub-
tree rooted in VP (of the left tree). The STFs sat-
isfy the constraint that grammatical rules cannot
be broken. For example, [VP [V NP]] is an
STF, which has two non-terminal symbols, V and
NP, as leaves whereas [VP [V]] is not an STF.
If we relax the constraint over the STFs, we ob-
tain more general substructures called partial trees
fragments (PTFs). These can be generated by the
application of partial production rules of the gram-
mar, consequently [VP [V]] and [VP [NP]]
are valid PTFs. Figure 1(b) shows that the num-
ber of PTFs derived from the same tree as before
is still higher (i.e. 30 PTs).
1077
(a) Syntactic Tree fragments (STF) (b) Partial Tree fragments (PTF)
Figure 1: Examples of different classes of tree fragments.
2.3 Counting Shared Subtrees
The main idea of tree kernels is to compute the
number of common substructures between two
trees T
1
and T
2
without explicitly considering the
whole fragment space. To evaluate the above ker-
nels between two T
1
and T
2
, we need to define a
set F = {f
1
, f
2
, . . . , f
|F|
}, i.e. a tree fragment
space and an indicator function I
i
(n), equal to 1
if the target f
i
is rooted at node n and equal to 0
otherwise. A tree-kernel function over T
1
and T
2
is TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
),
where N
T
1
and N
T
2
are the sets of the T
1
?s
and T
2
?s nodes, respectively and ?(n
1
, n
2
) =
?
|F|
i=1
I
i
(n
1
)I
i
(n
2
). The latter is equal to the num-
ber of common fragments rooted in the n
1
and n
2
nodes.
The algorithm for the efficient evaluation of ?
for the syntactic tree kernel (STK) has been widely
discussed in (Collins and Duffy, 2002) whereas its
fast evaluation is proposed in (Moschitti, 2006b),
so we only describe the equations of the partial
tree kernel (PTK).
2.4 The Partial Tree Kernel (PTK)
PTFs have been defined in (Moschitti, 2006a).
Their computation is carried out by the following
? function:
1. if the node labels of n
1
and n
2
are different
then ?(n
1
, n
2
) = 0;
2. else ?(n
1
, n
2
) =
1 +
?
~
I
1
,
~
I
2
,l(
~
I
1
)=l(
~
I
2
)
?
l(
~
I
1
)
j=1
?(c
n
1
(
~
I
1j
), c
n
2
(
~
I
2j
))
where
~
I
1
= ?h
1
, h
2
, h
3
, ..? and
~
I
2
=
?k
1
, k
2
, k
3
, ..? are index sequences associated with
the ordered child sequences c
n
1
of n
1
and c
n
2
of
n
2
, respectively,
~
I
1j
and
~
I
2j
point to the j-th child
in the corresponding sequence, and, again, l(?) re-
turns the sequence length, i.e. the number of chil-
dren.
Furthermore, we add two decay factors: ? for
the depth of the tree and ? for the length of the
child subsequences with respect to the original se-
quence, i.e. we account for gaps. It follows that
?(n
1
, n
2
) =
?
(
?
2
+
?
~
I
1
,
~
I
2
,l(
~
I
1
)=l(
~
I
2
)
?
d(
~
I
1
)+d(
~
I
2
)
l(
~
I
1
)
?
j=1
?(c
n
1
(
~
I
1j
), c
n
2
(
~
I
2j
))
)
,
(1)
where d(
~
I
1
) =
~
I
1l(
~
I
1
)
?
~
I
11
and d(
~
I
2
) =
~
I
2l(
~
I
2
)
?
~
I
21
. This way, we penalize both larger trees and
child subsequences with gaps. Eq. 1 is more gen-
eral than the ? equation for STK. Indeed, if we
only consider the contribution of the longest child
sequence from node pairs that have the same chil-
dren, we implement STK.
3 Generative Model: Stochastic
Conceptual Language Model (SCLM)
The first step of our approach is to produce a list
of SLU hypotheses using a Stochastic Conceptual
Language Model. This is the same described in
(Raymond and Riccardi, 2007) with the only dif-
ference that we train the language model using the
SRILM toolkit (Stolcke, 2002) and we then con-
vert it into a Stochastic Finite State Transducer
(SFST). Such method allows us to use a wide
group of language models, backed-off or inter-
polated with many kind of smoothing techniques
(Chen and Goodman, 1998).
To exemplify our SCLM let us consider the
following input italian sentence taken from the
LUNA corpus along with its English translation:
Ho un problema col monitor.
(I have a problem with my screen).
A possible semantic annotation is:
null{ho} PROBLEM{un problema} HARD-
WARE{col monitor},
where PROBLEM and HARDWARE are two
domain concepts and null is the label used for
words not meaningful for the task. To associate
word sequences with concepts, we use begin
1078
(B) and inside (I) markers after each word of a
sequence, e.g.:
null{ho} PROBLEM-B{un} PROBLEM-
I{problema} HARDWARE-B{col} HARD-
WARE-I{monitor}
This annotation is automatically performed
by a model based on a combination of three
transducers:
?
SLU
= ?
W
? ?
W2C
? ?
SLM
,
where ?
W
is the transducer representation of the
input sentence, ?
W2C
is the transducer mapping
words to concepts and ?
SLM
is the Stochastic
Conceptual Language Model trained with SRILM
toolkit and converted in FST. The SCLM repre-
sents joint probability of word and concept se-
quences by using the joint probability:
P (W,C) =
k
?
i=1
P (w
i
, c
i
|h
i
),
where W = w
1
..w
k
, C = c
1
..c
k
and h
i
=
w
i?1
c
i?1
..w
1
c
1
.
4 Discriminative re-ranking
Our discriminative re-ranking is based on SVMs
trained with pairs of conceptually annotated sen-
tences produced by the FST-based generative
model described in the previous section. An SVM
learn to classify which annotation has an error rate
lower than the others so that it can be used to sort
the m-best annotations based on their correctness.
While for SVMs details we remaind to the wide
literature available, for example (Vapnik, 1995) or
(Shawe-Taylor and Cristianini, 2004), in this sec-
tion we focus on hypotheses generation and on the
kernels used to implement our re-ranking model.
4.1 Generation of m-best concept labeling
Using the FST-based model described in Section
3, we can generate the list of m best hypotheses
ranked by the joint probability of the Stochastic
Conceptual Language Model (SCLM). The Re-
ranking model proposed in this paper re-ranks
such list.
After an analysis of the m-best hypothesis list,
we noticed that many times the first hypothesis
ranked by SCLM is not the most accurate, i.e.
the error rate evaluated with its Levenshtein dis-
tance from the manual annotation is not the low-
est among the m hypotheses. This means that re-
ranking hypotheses could improve the SLU ac-
curacy. Intuitively, to achieve satisfactory re-
sults, different features from those used by SCLM
should be considered to exploit in a different way
the information encoded in the training data.
4.2 Structural features for re-ranking
The kernels described in previous sections pro-
vide a powerful technology for exploiting features
of structured data. These kernels were originally
designed for data annotated with syntactic parse
trees. In Spoken Language Understanding the data
available are text sentences with their semantic
annotation based on basic semantic constituents.
This kind of data has a rather flat structure with
respect to syntactic parse trees. Thus, to exploit
the power of kernels, a careful design of the struc-
tures used to represent data must be carried out,
where the goal is to build tree-like annotation from
the semantic annotation. For this purpose, we
note that the latter is made upon sentence chunks,
which implicitly define syntactic structures as long
as the annotation is consistent in the corpus.
We took into account the characteristics of the
presented kernels and the structure of semantic an-
notated data. As a result we designed the tree
structures shown in figures 2(a), 2(b) and 3 for
STK and PTK and sequential structures for SK
defined in the following (where all the structures
refer to the same example presented in Section 3,
i.e. Ho un problema col monitor). The structures
used with SK are:
(SK1) NULL ho PROBLEM-B un
PROBLEM-I problema HARDWARE-B col
HARDWARE-I monitor
(SK2) NULL ho PROBLEM B un PROB-
LEM I problema HARDWARE B col HARD-
WARE I monitor,
For simplicity, from now on, the two structures
will be referred as SK1 and SK2 (String Kernel 1
and 2). They differer in the use of chunk mark-
ers B and I. In SK1, markers are part of the con-
cept, thus they increase the number of semantic
tags in the data whereas in SK2 markers are put
apart as separated words so that they can mark ef-
fectively the beginning and the end of a concept,
but for the same reason they can add noise in the
sentence. Notice that the order of words and con-
cepts is meaningful since each word is preceded
by its corresponding concepts.
The structures shown in Figure 2(a), 2(b) and 3
1079
have been designed for STK and PTK. They pro-
vide trees with increasing structure complexity as
described in the following.
The first structure (FLAT) is a simple tree
providing direct dependency between words and
chunked concepts. From it, STK and PTK can ex-
tract relevant features (tree fragments).
The second structure (MULTILEVEL) has one
more level of nodes and yields the same separation
of concepts and markers shown in SK1. Notice
that the same separation can be carried out putting
the markers B and I as features at the same level of
the words. This would increase exponentially (in
the number of leaves) the number of subtrees taken
into account by the STK computation. Since STK
doesn?t separate children, as described in Section
2.3, the structure we chose is lighter but also more
rigid.
The third structure (FEATURES) is a more
complex structure. It allows to use a wide num-
ber of features (like Word categories, POS tags,
morpho-syntactic features), which are commonly
used in this kind of task. As described above, the
use of features exponentially increases the num-
ber of subtrees taken into account by kernel com-
putations but they also increase the robustness of
the model. In this work we only used Word Cate-
gories as features. They are domain independent,
e.g. ?Months?, ?Dates?, ?Number? etc. or POS
tags, which are useful to generalize target words.
Note also that the features in common between
two trees must appear in the same child-position,
hence we sort them based on their indices, e.g.?F0?
for words and ?F1? for word categories.
4.3 Re-ranking models using sequences
The FST generates the m most likely concept an-
notations. These are used to build annotation
pairs,
?
s
i
, s
j
?
, which are positive instances if s
i
has a lower concept annotation error than s
j
, with
respect to the manual annotation. Thus, a trained
binary classifier can decide if s
i
is more accurate
than s
j
. Each candidate annotation s
i
is described
by a word sequence with its concept annotation.
Considering the example in the previous section, a
pair of annotations
?
s
i
, s
j
?
could be
s
i
: NULL ho PROBLEM-B un PROBLEM-
I problema HARDWARE-B col HARDWARE-I
monitor
s
j
: NULL ho ACTION-B un ACTION-I prob-
lema HARDWARE-B col HARDWARE-B moni-
tor
where NULL, ACTION and HARDWARE are
the assigned concepts. The second annotation is
less accurate than the first since problema is erro-
neously annotated as ACTION and ?col monitor?
is split in two different concepts.
Given the above data, the sequence kernel
is used to evaluate the number of common n-
grams between s
i
and s
j
. Since the string ker-
nel skips some elements of the target sequences,
the counted n-grams include: concept sequences,
word sequences and any subsequence of words
and concepts at any distance in the sentence.
Such counts are used in our re-ranking function
as follows: let e
k
be the pair
?
s
1
k
, s
2
k
?
we evaluate
the kernel:
K
R
(e
1
, e
2
) = SK(s
1
1
, s
1
2
) + SK(s
2
1
, s
2
2
) (2)
? SK(s
1
1
, s
2
2
)? SK(s
2
1
, s
1
2
)
This schema, consisting in summing four different
kernels, has been already applied in (Collins and
Duffy, 2002; Shen et al, 2003) for syntactic pars-
ing re-ranking, where the basic kernel was a tree
kernel instead of SK. It was also used also in (Shen
et al, 2004) to re-rank different candidates of the
same hypothesis for machine translation. Notice
that our goal is different from the one tackled in
such paper and, in general, it is more difficult: we
try to learn which is the best annotation of a given
input sentence, while in (Shen et al, 2004), they
learn to distinguish between ?good? and ?bad?
translations of a sentence. Even if our goal is more
difficult, our approach is very effective, as shown
in (Dinarelli et al, 2009a). It is more appropriate
since in parse re-ranking there is only one best hy-
pothesis, while in machine translation a sentence
can have more than one correct translations.
Additionally, in (Moschitti et al, 2006; Mos-
chitti et al, 2008) a tree kernel was applied to se-
mantic trees similar to the one introduced in the
next section to re-rank Semantic Role Labeling an-
notations.
4.4 Re-ranking models using trees
Since the aim of concept annotation re-ranking is
to exploit innovative and effective source of infor-
mation, we can use, in addition to sequence ker-
nels, the power of tree kernels to generate correla-
tion between concepts and word structures.
Figures 2(a), 2(b) and 3 describe the struc-
tural association between the concept and the word
1080
(a) FLAT Tree (b) MULTILEVEL Tree
Figure 2: Examples of structures used for STK and PTK
Figure 3: The FEATURES semantic tree used for STK or PTK
Corpus Train set Test set
LUNA words concepts words concepts
Dialogs 183 67
Turns 1.019 373
Tokens 8.512 2.887 2.888 984
Vocab. 1.172 34 - -
OOV rate - - 3.2% 0.1%
Table 1: Statistics on the LUNA corpus
Corpus Train set Test set
Media words concepts words concepts
Turns 12,922 3,518
# of tokens 94,912 43,078 26,676 12,022
Vocabulary 5,307 80 - -
OOV rate - - 0.01% 0.0%
Table 2: Statistics on the MEDIA corpus
level. This kind of trees allows us to engineer new
kernels and consequently new features (Moschitti
et al, 2008), e.g. their subparts extracted by STK
or PTK, like the tree fragments in figures 1(a) and
1(b). These can be used in SVMs to learn the clas-
sification of words in concepts.
More specifically, in our approach, we use tree
fragments to establish the order of correctness
between two alternative annotations. Therefore,
given two trees associated with two annotations, a
re-ranker based on tree kernel can be built in the
same way of the sequence-based kernel by substi-
tuting SK in Eq. 2 with STK or PTK. The major
advantage of using trees is the hierarchical depen-
dencies between its nodes, allowing for the use of
richer n-grams with back-off models.
5 Experiments
In this section, we describe the corpora, parame-
ters, models and results of our experiments on re-
ranking for SLU. Our baseline is constituted by the
error rate of systems solely based on either FST
or SVMs. The re-ranking models are built on the
FST output, which in turn is applied to both man-
ual or automatic transcriptions.
5.1 Corpora
We used two different speech corpora:
The LUNA corpus, produced in the homony-
mous European project, is the first Italian dataset
of spontaneous speech on spoken dialogs. It is
based on help-desk conversations in a domain
of software/hardware repairing (Dinarelli et al,
2009b). The data is organized in transcriptions
and annotations of speech based on a new multi-
level protocol. Although data acquisition is still in
progress, 250 dialogs have been already acquired
with a WOZ approach and other 180 Human-
Human (HH) dialogs have been annotated. In this
work, we only use WOZ dialogs, whose statistics
are reported in Table 1.
The corpus MEDIA was collected within
the French project MEDIA-EVALDA (Bonneau-
Maynard et al, 2005) for development and evalu-
ation of spoken understanding models and linguis-
tic studies. The corpus is composed of 1257 di-
alogs (from 250 different speakers) acquired with
a Wizard of Oz (WOZ) approach in the context
of hotel room reservations and tourist information.
1081
Statistics on transcribed and conceptually anno-
tated data are reported in Table 2.
5.2 Experimental setup
Given the small size of LUNA corpus, we did not
carried out any parameterization thus we used de-
fault or a priori parameters. We experimented with
LUNA and three different re-rankers obtained with
the combination of SVMs with STK, PTK and SK,
described in Section 4. The initial annotation to be
re-ranked is the list of the ten best hypotheses out-
put by an FST model.
We point out that, on the large Media dataset the
processing time is considerably high
2
so we could
not run all the models.
We trained all the SCLMs used in our experi-
ments with the SRILM toolkit (Stolcke, 2002) and
we used an interpolated model for probability es-
timation with the Kneser-Ney discount (Chen and
Goodman, 1998). We then converted the model in
an FST again with SRILM toolkit.
The model used to obtain the SVM baseline for
concept classification was trained using YamCHA
(Kudo and Matsumoto, 2001). As re-ranking
models based on structure kernels and SVMs,
we used the SVM-Light-TK toolkit (available at
disi.unitn.it/moschitti). For ? (see Section 3), cost-
factor and trade-off parameters, we used, 0.4, 1
and 1, respectively (i.e. the default parameters).
The number m of hypotheses was always set to 10.
The CRF model we compare with was
trained with the CRF++ tool, available at
http://crfpp.sourceforge.net/. The model is equiva-
lent to the one described in (Hahn et al, 2008). As
features, we used word and morpho-syntactic cat-
egories in a window of [-2, +2] with respect to the
current token, plus bigrams of concept tags (see
(Hahn et al, 2008) and the CRF++ web site for
more details).
Such model is very effective for SLU. In (Hahn
et al, 2008), it is compared with other four models
(Stochastic Finite State Transducers, Support Vec-
tor Machines, Machine Translation, Positional-
Based Log-linear model) and it is by far the best
on MEDIA. Additionally, in (Raymond and Ric-
cardi, 2007), a similar CRF model was compared
with FST and SVMs on ATIS and on a different
2
The number of parameters of the models and the number
of training approaches make the exhaustive experimentation
very expensive in terms of processing time, which would be
roughly between 2 and 3 months of a typical workstation.
Structure STK PTK SK
FLAT 18.5 19.3 -
MULTILEVEL 20.6 19.1 -
FEATURES 19.9 18.4 -
SK1 - - 16.2
SK2 - - 18.5
Table 3: CER of SVMs using STK, PTK and SK
on LUNA (manual transcriptions). The Baselines,
FST and SVMs alone, show a CER of 23.2% and
26.3%, respectively.
Model MEDIA (CER) LUNA (CER)
FST 13.7% 23.2%
CRF 11.5% 20.4%
SVM-RR (PTK) 12.1% 18.4%
Table 4: Results of SLU experiments on MEDIA
and LUNA test set (manual transcriptions).
version of MEDIA, showing again to be very ef-
fective.
We ran SLU experiments on manual and auto-
matic transcriptions. The latter are produced by
a speech recognizer with a WER of 41.0% and
31.4% on the LUNA and the MEDIA test sets, re-
spectively.
5.3 Training approaches
The FST model generates the 10-best annotations,
i.e. the data used to train the re-ranker based on
SVMs. Different training approaches can be car-
ried out based on the use of the data. We divided
the training set in two parts. We train FSTs on
part 1 and generate the 10-best hypotheses using
part 2, thus providing the first chunk of re-ranking
data. Then, we re-apply these steps inverting part
1 with part 2 to provide the second data chunk.
Finally, we train the re-ranker on the merged data.
For classification, we generate the 10-best hy-
potheses of the whole test set using the FST
trained on all training data.
5.4 Re-ranking results
In Tables 3, 4 and 5 and Figures 4(a) and 4(b) we
report the results of our experiments, expressed in
terms of concept error rate (CER). CER is a stan-
dard measure based on the Levensthein alignment
of sentences and it is computed as the ratio be-
tween inserted, deleted and confused concepts and
the number of concepts in the reference sentence.
Table 3 shows the results on the LUNA cor-
pus using the different training approaches, ker-
nels and structures described in this paper. The
1082
15 
20 
25 
30 
35 
40 
45 
50 
55 
60 
100 500 1000 2000 3000 4000 5000 6000 
CER
 
Training Sentences 
FST CRF RR 
(a) Learning Curve on MEDIA corpus using the RR model
based on SVMs and STK
15 
20 
25 
30 
35 
40 
45 
50 
55 
100 300 500 700 1000 
CER
 
Training Sentences 
FST CRF RR 
(b) Learning Curve on LUNA corpus using the RR model
based on SVMs and SK
Figure 4: Learning curves on MEDIA and LUNA corpora using FST, CRF and RR on the FST hypotheses
Model MEDIA (CER) LUNA (CER)
FST 28.6% 42.7%
CRF 24.0% 41.8%
SVM-RR (PTK) 25.0% 38.9%
Table 5: Results of SLU experiments on MEDIA
and LUNA test set (automatic transcriptions with
a WER 31.4% on MEDIA and 41% on LUNA)
dash symbol means that the structure cannot be
applied to the corresponding kernel. We note that
our re-rankers significantly improve our baselines,
i.e. 23.2% CER for FST and 26.3% CER for SVM
concept classifiers. For example, SVM re-ranker
using SK, in the best case, improves FST concept
classifier of 23.2-16.2 = 7 points.
Note also that the structures designed for trees
yield quite different results depending on which
kernel is used. We can see in Table 3 that the
best result using STK is obtained with the simplest
structure (FLAT), while with PTK the best result
is achieved with the most complex structure (FEA-
TURES). This is due to the fact that STK does
not split the children of each node, as explained in
Section 2.2, and so structures like MULTILEVEL
and FEATURES are too rigid and prevent the STK
to be effective.
For lack of space we do not report all the results
using different kernels and structures on MEDIA,
but we underline that as MEDIA is a more com-
plex task (34 concepts in LUNA, 80 in MEDIA),
the more complex structures are more effective to
capture word-concept dependencies and the best
results were obtained using the FEATURES tree.
Table 4 shows the results of the SLU exper-
iments on the MEDIA and LUNA test sets us-
ing the manual transcriptions of spoken sentences
and a re-ranker based on PTK and the FEATURES
structure (already reported in the previous table).
We used PTK since it is enough efficient to carry
out the computation on the much larger Media cor-
pus although as previously shown it is less accu-
rate than SK.
We note that on a big corpus like MEDIA, the
baseline models (FST and CRF) can be accurately
learned thus less errors can be ?corrected?. As
a consequence, our re-ranking approach does not
improve CRF but it still improves the FSTs base-
line of 1.6% points (11.7% of relative improve-
ment).
The same behavior is reproduced for the SLU
experiments on automatic transcriptions, shown in
Table 5. We note that, on the LUNA corpus, CRFs
are more accurate than FSTs (0.9% points), but
they are significantly improved by the re-ranking
model (2.9% points), which also improves the
FSTs baseline by 3.8% points. On the MEDIA
corpus, the re-ranking model is again very accu-
rate improving the FSTs baseline of 3.6% points
(12.6% relative improvement) on attribute anno-
tation, but the most accurate model is again CRF
(1% points better than the re-ranking model).
5.5 Discussion
The different behavior of the re-ranking model in
the LUNA and MEDIA corpora is due partially to
the task complexity, but it is mainly due to the fact
that CRFs have been deeply studied and experi-
mented (see (Hahn et al, 2008)) on MEDIA. Thus
CRF parameters and features have been largely
optimized. We believe that the re-ranking model
can be relevantly improved by carrying out param-
eter optimization and new structural feature de-
1083
sign.
Moreover, our re-ranking models achieve the
highest accuracy for automatic concept annota-
tion when small data sets are available. To show
this, we report in Figure 4(a) and 4(b) the learning
curves according to an increasing number of train-
ing sentences on the MEDIA and LUNA corpora,
respectively. To draw the first plot, we used a re-
ranker based on STK (and the FLAT tree), which
is less accurate than the other kernels but also the
most efficient in terms of training time. In the sec-
ond plot, we report the re-ranker accuracy using
SK applied to SK1 structure.
In these figures, the FST baseline performance
is compared with our re-ranking (RR) and a Con-
ditional Random Field (CRF) model. The above
curves clearly shows that for small datasets our
RR model is better than CRF whereas when the
data increases, CRF accuracy approaches the one
of the RR.
Regarding the use of kernels two main findings
can be derived:
? Kernels producing a high number of features,
e.g. SK, produce accuracy higher than ker-
nels less rich in terms of features, i.e. STK. In
particular STK is improved by 18.5-16.2=2.3
points (Table 3). This is an interesting re-
sult since it shows that (a) a kernel producing
more features also produces better re-ranking
models and (b) kernel methods give a remark-
able help in feature design.
? Although the training data is small, the re-
rankers based on kernels appear to be very
effective. This may also alleviate the burden
of annotating large amount of data.
6 Conclusions
In this paper, we propose discriminative re-
ranking of concept annotation to jointly exploit
generative and discriminative models. We im-
prove the FST-based generative approach, which
is a state-of-the-art model in LUNA, by 7 points,
where the more limited availability of annotated
data leaves a larger room for improvement. Our
re-ranking model also improves FST and CRF on
MEDIA when small data sets are used.
Kernel methods show that combinations of fea-
ture vectors, sequence kernels and other structural
kernels, e.g. on shallow or deep syntactic parse
trees, appear to be a promising future research
line
3
. Finally, the experimentation with automatic
speech transcriptions revealed that to test the ro-
bustness of our models to transcription errors.
In the future we would like to extend this re-
search by focusing on advanced shallow semantic
approaches such as predicate argument structures,
e.g. (Giuglea and Moschitti, 2004; Moschitti and
Cosmin, 2004; Moschitti et al, 2008). Addition-
ally, term similarity kernels, e.g. (Basili et al,
2005; Bloehdorn et al, 2006), will be likely im-
prove our models, especially when combined syn-
tactic and semantic kernels are used, i.e. (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b).
References
Roberto Basili, Alessandro Moschitti, and
Maria Teresa Pazienza. 1999. A text classifier
based on linguistic processing. In Proceedings
of IJCAI 99, Machine Learning for Information
Filtering.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet seman-
tics via kernel-based learning. In Proceedings of
CoNLL-2005, Ann Arbor, Michigan.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Proceedings of ECIR 2007, Rome,
Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In In proceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of ICDM 06,
Hong Kong, 2006.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of In-
terspeech2005, Lisbon, Portugal.
N. Cancedda, E. Gaussier, C. Goutte, and J. M. Ren-
ders. 2003. Word sequence kernels. J. Mach.
Learn. Res., 3.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techniques for language modeling. In
Technical Report of Computer Science Group, Har-
vard, USA.
3
A basic approach is the use of part-of-speech tags like for
example in text categorization (Basili et al, 1999) but given
the high efficiency of modern syntactic parsers we can use the
complete parse tree.
1084
M. Collins and N. Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over
Discrete structures, and the voted perceptron. In
ACL02, pages 263?270.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceed-
ings of ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2009a. Re-ranking models for spoken lan-
guage understanding. In Proceedings of EACL2009,
Athens, Greece.
Marco Dinarelli, Silvia Quarteroni, Sara Tonelli,
Alessandro Moschitti, and Giuseppe Riccardi.
2009b. Annotating spoken dialogs: from speech
segments to dialog acts and frame semantics. In
Proceedings of SRSL 2009 Workshop of EACL,
Athens, Greece.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovery using Framenet, Verbnet and
Propbank. In A. Meyers, editor, Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
Stefan Hahn, Patrick Lehnen, Christian Raymond, and
Hermann Ney. 2008. A comparison of various
methods for concept tagging for spoken language
understanding. In Proceedings of LREC, Mar-
rakech, Morocco.
T. Kudo and Y. Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
NAACL2001, Pittsburg, USA.
Taku Kudo and Yuji Matsumoto. 2003. Fast meth-
ods for kernel-based text analysis. In Proceedings
of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In Proceedings of ACL?05.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML2001, US.
Huma Lodhi, John S. Taylor, Nello Cristianini, and
Christopher J. C. H. Watkins. 2000. Text classifi-
cation using string kernels. In NIPS.
Alessandro Moschitti and Adrian Bejan Cosmin. 2004.
A semantic kernel for predicate argument classifica-
tion. In CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In Proceedings of ACL-08: HLT, Short Papers,
Columbus, Ohio.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question/answer classification. In Proceedings of
ACL?07, Prague, Czech Republic.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006a. Efficient Convolution
Kernels for Dependency and Constituent Syntactic
Trees. In Proceedings of ECML 2006, pages 318?
329, Berlin, Germany.
Alessandro Moschitti. 2006b. Making Tree Kernels
Practical for Natural Language Learning. In Pro-
ceedings of EACL2006.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
Proceeding of CIKM ?08, NY, USA.
C. Raymond and G. Riccardi. 2007. Generative and
discriminative algorithms for spoken language un-
derstanding. In Proceedings of Interspeech2007,
Antwerp,Belgium.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Proceedings of EMNLP?06.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
HLT-NAACL, pages 177?184.
A. Stolcke. 2002. Srilm: an extensible language mod-
eling toolkit. In Proceedings of SLP2002, Denver,
USA.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View
of Parse Trees: Exploring String Kernels for HPSG
Parse Selection. In Proceedings of EMNLP 2004.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
1085
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 202?210,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Re-Ranking Models For Spoken Language Understanding
Marco Dinarelli
University of Trento
Italy
dinarelli@disi.unitn.it
Alessandro Moschitti
University of Trento
Italy
moschitti@disi.unitn.it
Giuseppe Riccardi
University of Trento
Italy
riccardi@disi.unitn.it
Abstract
Spoken Language Understanding aims at
mapping a natural language spoken sen-
tence into a semantic representation. In
the last decade two main approaches have
been pursued: generative and discrimi-
native models. The former is more ro-
bust to overfitting whereas the latter is
more robust to many irrelevant features.
Additionally, the way in which these ap-
proaches encode prior knowledge is very
different and their relative performance
changes based on the task. In this pa-
per we describe a machine learning frame-
work where both models are used: a gen-
erative model produces a list of ranked hy-
potheses whereas a discriminative model
based on structure kernels and Support
Vector Machines, re-ranks such list. We
tested our approach on the MEDIA cor-
pus (human-machine dialogs) and on a
new corpus (human-machine and human-
human dialogs) produced in the Euro-
pean LUNA project. The results show a
large improvement on the state-of-the-art
in concept segmentation and labeling.
1 Introduction
In Spoken Dialog Systems, the Language Under-
standing module performs the task of translating
a spoken sentence into its meaning representation
based on semantic constituents. These are the
units for meaning representation and are often re-
ferred to as concepts. Concepts are instantiated by
sequences of words, therefore a Spoken Language
Understanding (SLU) module finds the association
between words and concepts.
In the last decade two major approaches have
been proposed to find this correlation: (i) gener-
ative models, whose parameters refer to the joint
probability of concepts and constituents; and (ii)
discriminative models, which learn a classifica-
tion function to map words into concepts based
on geometric and statistical properties. An ex-
ample of generative model is the Hidden Vector
State model (HVS) (He and Young, 2005). This
approach extends the discrete Markov model en-
coding the context of each state as a vector. State
transitions are performed as stack shift operations
followed by a push of a preterminal semantic cat-
egory label. In this way the model can capture se-
mantic hierarchical structures without the use of
tree-structured data. Another simpler but effec-
tive generative model is the one based on Finite
State Transducers. It performs SLU as a transla-
tion process from words to concepts using Finite
State Transducers (FST). An example of discrim-
inative model used for SLU is the one based on
Support Vector Machines (SVMs) (Vapnik, 1995),
as shown in (Raymond and Riccardi, 2007). In
this approach, data are mapped into a vector space
and SLU is performed as a classification problem
using Maximal Margin Classifiers (Shawe-Taylor
and Cristianini, 2004).
Generative models have the advantage to be
more robust to overfitting on training data, while
discriminative models are more robust to irrele-
vant features. Both approaches, used separately,
have shown a good performance (Raymond and
Riccardi, 2007), but they have very different char-
acteristics and the way they encode prior knowl-
edge is very different, thus designing models able
to take into account characteristics of both ap-
proaches are particularly promising.
In this paper we propose a method for SLU
based on generative and discriminative models:
the former uses FSTs to generate a list of SLU hy-
potheses, which are re-ranked by SVMs. These
exploit all possible word/concept subsequences
(with gaps) of the spoken sentence as features (i.e.
all possible n-grams). Gaps allow for the encod-
202
ing of long distance dependencies between words
in relatively small n-grams. Given the huge size
of this feature space, we adopted kernel methods
and in particular sequence kernels (Shawe-Taylor
and Cristianini, 2004) and tree kernels (Raymond
and Riccardi, 2007; Moschitti and Bejan, 2004;
Moschitti, 2006) to implicitly encode n-grams and
other structural information in SVMs.
We experimented with different approaches for
training the discriminative models and two dif-
ferent corpora: the well-known MEDIA corpus
(Bonneau-Maynard et al, 2005) and a new corpus
acquired in the European project LUNA1 (Ray-
mond et al, 2007). The results show a great
improvement with respect to both the FST-based
model and the SVM model alone, which are the
current state-of-the-art for concept classification
on such corpora. The rest of the paper is orga-
nized as follows: Sections 2 and 3 show the gener-
ative and discriminative models, respectively. The
experiments and results are reported in Section 4
whereas the conclusions are drawn in Section 5.
2 Generative approach for concept
classification
In the context of Spoken Language Understanding
(SLU), concept classification is the task of asso-
ciating the best sequence of concepts to a given
sentence, i.e. word sequence. A concept is a class
containing all the words carrying out the same se-
mantic meaning with respect to the application do-
main. In SLU, concepts are used as semantic units
and are represented with concept tags. The associ-
ation between words and concepts is learned from
an annotated corpus.
The Generative model used in our work for con-
cept classification is the same used in (Raymond
and Riccardi, 2007). Given a sequence of words
as input, a translation process based on FST is
performed to output a sequence of concept tags.
The translation process involves three steps: (1)
the mapping of words into classes (2) the mapping
of classes into concepts and (3) the selection of the
best concept sequence.
The first step is used to improve the generaliza-
tion power of the model. The word classes at this
level can be both domain-dependent, e.g. ?Hotel?
in MEDIA or ?Software? in the LUNA corpus, or
domain-independent, e.g. numbers, dates, months
1Contract n. 33549
etc. The class of a word not belonging to any class
is the word itself.
In the second step, classes are mapped into con-
cepts. The mapping is not one-to-one: a class
may be associated with more than one concept, i.e.
more than one SLU hypothesis can be generated.
In the third step, the best or the m-best hy-
potheses are selected among those produced in the
previous step. They are chosen according to the
maximum probability evaluated by the Conceptual
Language Model, described in the next section.
2.1 Stochastic Conceptual Language Model
(SCLM)
An SCLM is an n-gram language model built on
semantic tags. Using the same notation proposed
in (Moschitti et al, 2007) and (Raymond and Ric-
cardi, 2007), our SCLM trains joint probability
P (W,C) of word and concept sequences from an
annotated corpus:
P (W,C) =
k
?
i=1
P (wi, ci|hi),
where W = w1..wk, C = c1..ck and
hi = wi?1ci?1..w1c1. Since we use a 3-gram
conceptual language model, the history hi is
{wi?1ci?1, wi?2ci?2}.
All the steps of the translation process described
here and above are implemented as Finite State
Transducers (FST) using the AT&T FSM/GRM
tools and the SRILM (Stolcke, 2002) tools. In
particular the SCLM is trained using SRILM tools
and then converted to an FST. This allows the use
of a wide set of stochastic language models (both
back-off and interpolated models with several dis-
counting techniques like Good-Turing, Witten-
Bell, Natural, Kneser-Ney, Unchanged Kneser-
Ney etc). We represent the combination of all the
translation steps as a transducer ?SLU (Raymond
and Riccardi, 2007) in terms of FST operations:
?SLU = ?W ? ?W2C ? ?SLM ,
where ?W is the transducer representation of the
input sentence, ?W2C is the transducer mapping
words to classes and ?SLM is the Semantic Lan-
guage Model (SLM) described above. The best
SLU hypothesis is given by
C = projectC(bestpath1(?SLU )),
where bestpathn (in this case n is 1 for the 1-best
hypothesis) performs a Viterbi search on the FST
203
and outputs the n-best hypotheses and projectC
performs a projection of the FST on the output la-
bels, in this case the concepts.
2.2 Generation of m-best concept labeling
Using the FSTs described above, we can generate
m best hypotheses ranked by the joint probability
of the SCLM.
After an analysis of the m-best hypotheses of
our SLU model, we noticed that many times the
hypothesis ranked first by the SCLM is not the
closest to the correct concept sequence, i.e. its er-
ror rate using the Levenshtein alignment with the
manual annotation of the corpus is not the low-
est among the m hypotheses. This means that
re-ranking the m-best hypotheses in a convenient
way could improve the SLU performance. The
best choice in this case is a discriminative model,
since it allows for the use of informative features,
which, in turn, can model easily feature dependen-
cies (also if they are infrequent in the training set).
3 Discriminative re-ranking
Our discriminative re-ranking is based on SVMs
or a perceptron trained with pairs of conceptually
annotated sentences. The classifiers learn to select
which annotation has an error rate lower than the
others so that the m-best annotations can be sorted
based on their correctness.
3.1 SVMs and Kernel Methods
Kernel Methods refer to a large class of learning
algorithms based on inner product vector spaces,
among which Support Vector Machines (SVMs)
are one of the most well known algorithms. SVMs
and perceptron learn a hyperplane H(~x) = ~w~x +
b = 0, where ~x is the feature vector represen-
tation of a classifying object o, ~w ? Rn (a
vector space) and b ? R are parameters (Vap-
nik, 1995). The classifying object o is mapped
into ~x by a feature function ?. The kernel trick
allows us to rewrite the decision hyperplane as
?
i=1..l yi?i?(oi)?(o) + b = 0, where yi is equal
to 1 for positive and -1 for negative examples,
?i ? R+, oi?i ? {1..l} are the training instances
and the product K(oi, o) = ??(oi)?(o)? is the ker-
nel function associated with the mapping ?. Note
that we do not need to apply the mapping ?, we
can use K(oi, o) directly (Shawe-Taylor and Cris-
tianini, 2004). For example, next section shows a
kernel function that counts the number of word se-
quences in common between two sentences, in the
space of n-grams (for any n).
3.2 String Kernels
The String Kernels that we consider count the
number of substrings containing gaps shared by
two sequences, i.e. some of the symbols of the
original string are skipped. Gaps modify the
weight associated with the target substrings as
shown in the following.
Let ? be a finite alphabet, ?? = ??n=0 ?n is the
set of all strings. Given a string s ? ??, |s| denotes
the length of the strings and si its compounding
symbols, i.e s = s1..s|s|, whereas s[i : j] selects
the substring sisi+1..sj?1sj from the i-th to the
j-th character. u is a subsequence of s if there
is a sequence of indexes ~I = (i1, ..., i|u|), with
1 ? i1 < ... < i|u| ? |s|, such that u = si1 ..si|u|
or u = s[~I] for short. d(~I) is the distance between
the first and last character of the subsequence u in
s, i.e. d(~I) = i|u| ? i1 + 1. Finally, given s1, s2
? ??, s1s2 indicates their concatenation.
The set of all substrings of a text corpus forms a
feature space denoted by F = {u1, u2, ..} ? ??.
To map a string s in R? space, we can use the
following functions: ?u(s) =
P
~I:u=s[~I] ?
d(~I) for
some ? ? 1. These functions count the num-
ber of occurrences of u in the string s and assign
them a weight ?d(~I) proportional to their lengths.
Hence, the inner product of the feature vectors for
two strings s1 and s2 returns the sum of all com-
mon subsequences weighted according to their
frequency of occurrences and lengths, i.e.
SK(s1, s2) =
X
u???
?u(s1) ??u(s2) =
X
u???
X
~I1:u=s1[~I1]
?d( ~I1)
X
~I2:u=s2[~I2]
?d( ~I2) =
X
u???
X
~I1:u=s1[~I1]
X
~I2:u=s2[~I2]
?d( ~I1)+d( ~I2),
where d(.) counts the number of characters in the
substrings as well as the gaps that were skipped in
the original string. It is worth noting that:
(a) longer subsequences receive lower weights;
(b) some characters can be omitted, i.e. gaps;
and
(c) gaps determine a weight since the exponent
of ? is the number of characters and gaps be-
tween the first and last character.
204
Characters in the sequences can be substituted
with any set of symbols. In our study we pre-
ferred to use words so that we can obtain word
sequences. For example, given the sentence: How
may I help you ? sample substrings, extracted by
the Sequence Kernel (SK), are: How help you ?,
How help ?, help you, may help you, etc.
3.3 Tree kernels
Tree kernels represent trees in terms of their sub-
structures (fragments). The kernel function de-
tects if a tree subpart (common to both trees) be-
longs to the feature space that we intend to gen-
erate. For such purpose, the desired fragments
need to be described. We consider two important
characterizations: the syntactic tree (STF) and the
partial tree (PTF) fragments.
3.3.1 Tree Fragment Types
An STF is a general subtree whose leaves can be
non-terminal symbols. For example, Figure 1(a)
shows 10 STFs (out of 17) of the subtree rooted in
VP (of the left tree). The STFs satisfy the con-
straint that grammatical rules cannot be broken.
For example, [VP [V NP]] is an STF, which
has two non-terminal symbols, V and NP, as leaves
whereas [VP [V]] is not an STF. If we relax
the constraint over the STFs, we obtain more gen-
eral substructures called partial trees fragments
(PTFs). These can be generated by the application
of partial production rules of the grammar, con-
sequently [VP [V]] and [VP [NP]] are valid
PTFs. Figure 1(b) shows that the number of PTFs
derived from the same tree as before is still higher
(i.e. 30 PTs).
3.4 Counting Shared SubTrees
The main idea of tree kernels is to compute the
number of common substructures between two
trees T1 and T2 without explicitly considering the
whole fragment space. To evaluate the above ker-
nels between two T1 and T2, we need to define a
set F = {f1, f2, . . . , f|F|}, i.e. a tree fragment
space and an indicator function Ii(n), equal to 1
if the target fi is rooted at node n and equal to 0
otherwise. A tree-kernel function over T1 and T2
is TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2),
where NT1 and NT2 are the sets of the T1?s
and T2?s nodes, respectively and ?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2). The latter is equal to the num-
ber of common fragments rooted in the n1 and
n2 nodes. In the following sections we report the
equation for the efficient evaluation of ? for ST
and PT kernels.
3.5 Syntactic Tree Kernels (STK)
The ? function depends on the type of fragments
that we consider as basic features. For example,
to evaluate the fragments of type STF, it can be
defined as:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the
same, and n1 and n2 have only leaf children
(i.e. they are pre-terminals symbols) then
?(n1, n2) = 1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) =
nc(n1)
?
j=1
(? + ?(cjn1 , c
j
n2)) (1)
where ? ? {0, 1}, nc(n1) is the number of chil-
dren of n1 and cjn is the j-th child of the node
n. Note that, since the productions are the same,
nc(n1) = nc(n2). ?(n1, n2) evaluates the num-
ber of STFs common to n1 and n2 as proved in
(Collins and Duffy, 2002).
Moreover, a decay factor ? can be added by
modifying steps (2) and (3) as follows2:
2. ?(n1, n2) = ?,
3. ?(n1, n2) = ?
?nc(n1)
j=1 (? + ?(c
j
n1 , c
j
n2)).
The computational complexity of Eq. 1 is
O(|NT1 | ? |NT2 |) but as shown in (Moschitti,
2006), the average running time tends to be lin-
ear, i.e. O(|NT1 | + |NT2 |), for natural language
syntactic trees.
3.6 The Partial Tree Kernel (PTK)
PTFs have been defined in (Moschitti, 2006).
Their computation is carried out by the following
? function:
1. if the node labels of n1 and n2 are different
then ?(n1, n2) = 0;
2. else ?(n1, n2) =
1+?~I1,~I2,l(~I1)=l(~I2)
?l(~I1)
j=1 ?(cn1(~I1j), cn2(~I2j))
2To have a similarity score between 0 and 1, we also apply
the normalization in the kernel space, i.e.:
K?(T1, T2) = TK(T1 ,T2)?TK(T1 ,T1)?TK(T2 ,T2) .
205
NP 
D N 
a 
  cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a 
   cat 
  cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
(a) Syntactic Tree fragments (STF)
NP 
D N 
VP 
V 
brought 
a 
   cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
a 
   cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
(b) Partial Tree fragments (PTF)
Figure 1: Examples of different classes of tree fragments.
where ~I1 = ?h1, h2, h3, ..? and ~I2 =
?k1, k2, k3, ..? are index sequences associated with
the ordered child sequences cn1 of n1 and cn2 of
n2, respectively, ~I1j and ~I2j point to the j-th child
in the corresponding sequence, and, again, l(?) re-
turns the sequence length, i.e. the number of chil-
dren.
Furthermore, we add two decay factors: ? for
the depth of the tree and ? for the length of the
child subsequences with respect to the original se-
quence, i.e. we account for gaps. It follows that
?(n1, n2) =
?
(
?2+
?
~I1,~I2,l(~I1)=l(~I2)
?d(~I1)+d(~I2)
l(~I1)
?
j=1
?(cn1(~I1j), cn2(~I2j))
)
,
(2)
where d(~I1) = ~I1l(~I1) ? ~I11 and d(~I2) = ~I2l(~I2) ?
~I21. This way, we penalize both larger trees and
child subsequences with gaps. Eq. 2 is more gen-
eral than Eq. 1. Indeed, if we only consider the
contribution of the longest child sequence from
node pairs that have the same children, we imple-
ment the STK kernel.
3.7 Re-ranking models using sequences
The FST generates the m most likely concept an-
notations. These are used to build annotation
pairs,
?
si, sj
?
, which are positive instances if si
has a lower concept annotation error than sj , with
respect to the manual annotation in the corpus.
Thus, a trained binary classifier can decide if si
is more accurate than sj . Each candidate anno-
tation si is described by a word sequence where
each word is followed by its concept annotation.
For example, given the sentence:
ho (I have) un (a) problema (problem) con
(with) la (the) scheda di rete (network card) ora
(now)
a pair of annotations
?
si, sj
?
could be
si: ho NULL un NULL problema PROBLEM-B con
NULL la NULL scheda HW-B di HW-I rete HW-I ora
RELATIVETIME-B
sj: ho NULL un NULL problema ACTION-B con
NULL la NULL scheda HW-B di HW-B rete HW-B ora
RELATIVETIME-B
where NULL, ACTION, RELATIVETIME,
and HW are the assigned concepts whereas B and
I are the usual begin and internal tags for concept
subparts. The second annotation is less accurate
than the first since problema is annotated as an ac-
tion and ?scheda di rete? is split in three different
concepts.
Given the above data, the sequence kernel
is used to evaluate the number of common n-
grams between si and sj . Since the string ker-
nel skips some elements of the target sequences,
the counted n-grams include: concept sequences,
word sequences and any subsequence of words
and concepts at any distance in the sentence.
Such counts are used in our re-ranking function
as follows: let ei be the pair
?
s1i , s2i
?
we evaluate
the kernel:
KR(e1, e2) = SK(s11, s12) + SK(s21, s22) (3)
? SK(s11, s22)? SK(s21, s12)
This schema, consisting in summing four differ-
ent kernels, has been already applied in (Collins
and Duffy, 2002) for syntactic parsing re-ranking,
where the basic kernel was a tree kernel instead of
SK and in (Moschitti et al, 2006), where, to re-
rank Semantic Role Labeling annotations, a tree
kernel was used on a semantic tree similar to the
one introduced in the next section.
3.8 Re-ranking models using trees
Since the aim in concept annotation re-ranking is
to exploit innovative and effective source of infor-
mation, we can use the power of tree kernels to
generate correlation between concepts and word
structures.
Fig. 2 describes the structural association be-
tween the concept and the word level. This kind of
trees allows us to engineer new kernels and con-
sequently new features (Moschitti et al, 2008),
206
Figure 2: An example of the semantic tree used for STK or PTK
Corpus Train set Test set
LUNA words concepts words concepts
Dialogs WOZ 183 67
Dialogs HH 180 -
Turns WOZ 1.019 373
Turns HH 6.999 -
Tokens WOZ 8.512 2.887 2.888 984
Tokens WOZ 62.639 17.423 - -
Vocab. WOZ 1.172 34 - -
Vocab. HH 4.692 49 - -
OOV rate - - 3.2% 0.1%
Table 1: Statistics on the LUNA corpus
Corpus Train set Test set
Media words concepts words concepts
Turns 12,922 3,518
# of tokens 94,912 43,078 26,676 12,022
Vocabulary 5,307 80 - -
OOV rate - - 0.01% 0.0%
Table 2: Statistics on the MEDIA corpus
e.g. their subparts extracted by STK or PTK, like
the tree fragments in figures 1(a) and 1(b). These
can be used in SVMs to learn the classification of
words in concepts.
More specifically, in our approach, we use tree
fragments to establish the order of correctness
between two alternative annotations. Therefore,
given two trees associated with two annotations, a
re-ranker based on tree kernel, KR, can be built
in the same way of the sequence-based kernel by
substituting SK in Eq. 3 with STK or PTK.
4 Experiments
In this section, we describe the corpora, param-
eters, models and results of our experiments of
word chunking and concept classification. Our
baseline relates to the error rate of systems based
on only FST and SVMs. The re-ranking models
are built on the FST output. Different ways of
producing training data for the re-ranking models
determine different results.
4.1 Corpora
We used two different speech corpora:
The corpus LUNA, produced in the homony-
mous European project is the first Italian corpus
of spontaneous speech on spoken dialog: it is
based on the help-desk conversation in the domain
of software/hardware repairing (Raymond et al,
2007). The data are organized in transcriptions
and annotations of speech based on a new multi-
level protocol. Data acquisition is still in progress.
Currently, 250 dialogs acquired with a WOZ ap-
proach and 180 Human-Human (HH) dialogs are
available. Statistics on LUNA corpus are reported
in Table 1.
The corpus MEDIA was collected within
the French project MEDIA-EVALDA (Bonneau-
Maynard et al, 2005) for development and evalu-
ation of spoken understanding models and linguis-
tic studies. The corpus is composed of 1257 di-
alogs, from 250 different speakers, acquired with
a Wizard of Oz (WOZ) approach in the context
of hotel room reservations and tourist information.
Statistics on transcribed and conceptually anno-
tated data are reported in Table 2.
4.2 Experimental setup
We defined two different training sets in the
LUNA corpus: one using only the WOZ train-
ing dialogs and one merging them with the HH
dialogs. Given the small size of LUNA corpus, we
did not carried out parameterization on a develop-
ment set but we used default or a priori parameters.
We experimented with LUNA WOZ and six re-
rankers obtained with the combination of SVMs
and perceptron (PCT) with three different types
of kernels: Syntactic Tree Kernel (STK), Partial
Tree kernels (PTK) and the String Kernel (SK) de-
scribed in Section 3.3.
Given the high number and the cost of these ex-
periments, we ran only one model, i.e. the one
207
Corpus LUNA WOZ+HH MEDIA
Approach (STK) MT ST MT
FST 18.2 18.2 12.6
SVM 23.4 23.4 13.7
RR-A 15.6 17.0 11.6
RR-B 16.2 16.5 11.8
RR-C 16.1 16.4 11.7
Table 3: Results of experiments (CER) using FST
and SVMs with the Sytntactic Tree Kernel (STK)
on two different corpora: LUNA WOZ + HH, and
MEDIA.
based on SVMs and STK3 , on the largest datasets,
i.e. WOZ merged with HH dialogs and Media.
We trained all the SCLMs used in our experiments
with the SRILM toolkit (Stolcke, 2002) and we
used an interpolated model for probability esti-
mation with the Kneser-Ney discount (Chen and
Goodman, 1998). We then converted the model in
an FST as described in Section 2.1.
The model used to obtain the SVM baseline
for concept classification was trained using Yam-
CHA (Kudo and Matsumoto, 2001). For the re-
ranking models based on structure kernels, SVMs
or perceptron, we used the SVM-Light-TK toolkit
(available at dit.unitn.it/moschitti). For ? (see Sec-
tion 3.2), cost-factor and trade-off parameters, we
used, 0.4, 1 and 1, respectively.
4.3 Training approaches
The FST model generates the m-best annotations,
i.e. the data used to train the re-ranker based
on SVMs and perceptron. Different training ap-
proaches can be carried out based on the use of the
corpus and the method to generate the m-best. We
apply two different methods for training: Mono-
lithic Training and Split Training.
In the former, FSTs are learned with the whole
training set. The m-best hypotheses generated by
such models are then used to train the re-ranker
classifier. In Split Training, the training data are
divided in two parts to avoid bias in the FST gen-
eration step. More in detail, we train FSTs on part
1 and generate the m-best hypotheses using part 2.
Then, we re-apply these procedures inverting part
1 with part 2. Finally, we train the re-ranker on the
merged m-best data. At the classification time, we
generate the m-best of the test set using the FST
trained on all training data.
3The number of parameters, models and training ap-
proaches make the exhaustive experimentation expensive in
terms of processing time, which approximately requires 2 or
3 months.
Monolithic Training
WOZ SVM PCT
STK PTK SK STK PTK SK
RR-A 18.5 19.3 19.1 24.2 28.3 23.3
RR-B 18.5 19.3 19.0 29.4 23.7 20.3
RR-C 18.5 19.3 19.1 31.5 30.0 20.2
Table 4: Results of experiments, in terms of Con-
cept Error Rate (CER), on the LUNA WOZ corpus
using Monolithic Training approach. The baseline
with FST and SVMs used separately are 23.2%
and 26.7% respectively.
Split Training
WOZ SVM PCT
STK PTK SK STK PTK SK
RR-A 20.0 18.0 16.1 28.4 29.8 27.8
RR-B 19.0 19.0 19.0 26.3 30.0 25.6
RR-C 19.0 18.4 16.6 27.1 26.2 30.3
Table 5: Results of experiments, in terms of Con-
cept Error Rate (CER), on the LUNA WOZ cor-
pus using Split Training approach. The baseline
with FST and SVMs used separately are 23.2%
and 26.7% respectively.
Regarding the generation of the training in-
stances
?
si, sj
?
, we set m to 10 and we choose one
of the 10-best hypotheses as the second element of
the pair, sj , thus generating 10 different pairs.
The first element instead can be selected accord-
ing to three different approaches:
(A): si is the manual annotation taken from the
corpus;
(B) si is the most accurate annotation, in terms
of the edit distance from the manual annotation,
among the 10-best hypotheses of the FST model;
(C) as above but si is selected among the 100-
best hypotheses. The pairs are also inverted to
generate negative examples.
4.4 Re-ranking results
All the results of our experiments, expressed in
terms of concept error rate (CER), are reported in
Table 3, 4 and 5.
In Table 3, the corpora, i.e. LUNA (WOZ+HH)
and Media, and the training approaches, i.e.
Monolithic Training (MT) and Split Training (ST),
are reported in the first and second row. Column
1 shows the concept classification model used, i.e.
the baselines FST and SVMs, and the re-ranking
models (RR) applied to FST. A, B and C refer
to the three approaches for generating training in-
stances described above. As already mentioned
for these large datasets, SVMs only use STK.
208
We note that our re-rankers relevantly improve
our baselines, i.e. the FST and SVM concept clas-
sifiers on both corpora. For example, SVM re-
ranker using STK, MT and RR-A improves FST
concept classifier of 23.2-15.6 = 7.6 points.
Moreover, the monolithic training seems the
most appropriate to train the re-rankers whereas
approach A is the best in producing training in-
stances for the re-rankers. This is not surprising
since method A considers the manual annotation
as a referent gold standard and it always allows
comparing candidate annotations with the perfect
one.
Tables 4 and 5 have a similar structure of Ta-
ble 3 but they only show experiments on LUNA
WOZ corpus with respect to the monolithic and
split training approach, respectively. In these ta-
bles, we also report the result for SVMs and per-
ceptron (PCT) using STK, PTK and SK. We note
that:
First, the small size of WOZ training set (only
1,019 turns) impacts on the accuracy of the sys-
tems, e.g. FST and SVMs, which achieved a
CER of 18.2% and 23.4%, respectively, using also
HH dialogs, with only the WOZ data, they obtain
23.2% and 26.7%, respectively.
Second, the perceptron algorithm appears to be
ineffective for re-ranking. This is mainly due to
the reduced size of the WOZ data, which clearly
prevents an on line algorithm like PCT to ade-
quately refine its model by observing many exam-
ples4.
Third, the kernels which produce higher number
of substructures, i.e. PTK and SK, improves the
kernel less rich in terms of features, i.e. STK. For
example, using split training and approach A, STK
is improved by 20.0-16.1=3.9. This is an interest-
ing result since it shows that (a) richer structures
do produce better ranking models and (b) kernel
methods give a remarkable help in feature design.
Next, although the training data is small, the re-
rankers based on kernels appear to be very effec-
tive. This may also alleviate the burden of anno-
tating a lot of data.
Finally, the experiments of MEDIA show a not
so high improvement using re-rankers. This is due
to: (a) the baseline, i.e. the FST model is very
accurate since MEDIA is a large corpus thus the
re-ranker can only ?correct? small number of er-
rors; and (b) we could only experiment with the
4We use only one iteration of the algorithm.
less expensive but also less accurate models, i.e.
monolithic training and STK.
Media also offers the possibility to compare
with the state-of-the-art, which our re-rankers
seem to improve. However, we need to consider
that many Media corpus versions exist and this
makes such comparisons not completely reliable.
Future work on the paper research line appears
to be very interesting: the assessment of our best
models on Media and WOZ+HH as well as other
corpora is required. More importantly, the struc-
tures that we have proposed for re-ranking are
just two of the many possibilities to encode both
word/concept statistical distributions and linguis-
tic knowledge encoded in syntactic/semantic parse
trees.
5 Conclusions
In this paper, we propose discriminative re-
ranking of concept annotation to capitalize from
the benefits of generative and discriminative ap-
proaches. Our generative approach is the state-
of-the-art in concept classification since we used
the same FST model used in (Raymond and Ric-
cardi, 2007). We could improve it by 1% point
in MEDIA and 7.6 points (until 30% of relative
improvement) on LUNA, where the more limited
availability of annotated data leaves a larger room
for improvement.
It should be noted that to design the re-ranking
model, we only used two different structures,
i.e. one sequence and one tree. Kernel meth-
ods show that combinations of feature vectors, se-
quence kernels and other structural kernels, e.g.
on shallow or deep syntactic parse trees, appear
to be a promising research line (Moschitti, 2008).
Also, the approach used in (Zanzotto and Mos-
chitti, 2006) to define cross pair relations may be
exploited to carry out a more effective pair re-
ranking. Finally, the experimentation with auto-
matic speech transcriptions is interesting to test the
robustness of our models to transcription errors.
Acknowledgments
This work has been partially supported by the Eu-
ropean Commission - LUNA project, contract n.
33549.
209
References
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of In-
terspeech2005, Lisbon, Portugal.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techniques for language modeling. In
Technical Report of Computer Science Group, Har-
vard, USA.
M. Collins and N. Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over
Discrete structures, and the voted perceptron. In
ACL02, pages 263?270.
Y. He and S. Young. 2005. Semantic processing us-
ing the hidden vector state model. Computer Speech
and Language, 19:85?106.
T. Kudo and Y. Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
NAACL2001, Pittsburg, USA.
A. Moschitti and C. Bejan. 2004. A semantic ker-
nel for predicate argument classification. In CoNLL-
2004, Boston, MA, USA.
A. Moschitti, D. Pighin, and R. Basili. 2006. Seman-
tic role labeling via tree kernel joint inference. In
Proceedings of CoNLL-X, New York City.
A. Moschitti, G. Riccardi, and C. Raymond. 2007.
Spoken language understanding with kernels for
syntactic/semantic structures. In Proceedings of
ASRU2007, Kyoto, Japan.
A. Moschitti, D. Pighin, and R. Basili. 2008. Tree
kernels for semantic role labeling. Computational
Linguistics, 34(2):193?224.
A. Moschitti. 2006. Efficient Convolution Kernels
for Dependency and Constituent Syntactic Trees. In
Proceedings of ECML 2006, pages 318?329, Berlin,
Germany.
A. Moschitti. 2008. Kernel methods, syntax and se-
mantics for relational text categorization. In CIKM
?08: Proceeding of the 17th ACM conference on In-
formation and knowledge management, pages 253?
262, New York, NY, USA. ACM.
C. Raymond and G. Riccardi. 2007. Generative and
discriminative algorithms for spoken language un-
derstanding. In Proceedings of Interspeech2007,
Antwerp,Belgium.
C. Raymond, G. Riccardi, K. J. Rodrigez, and J. Wis-
niewska. 2007. The luna corpus: an annotation
scheme for a multi-domain multi-lingual dialogue
corpus. In Proceedings of Decalog2007, Trento,
Italy.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
A. Stolcke. 2002. Srilm: an extensible language mod-
eling toolkit. In Proceedings of SLP2002, Denver,
USA.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
F. M. Zanzotto and A. Moschitti. 2006. Automatic
learning of textual entailments with cross-pair simi-
larities. In Proceedings of the 21st Coling and 44th
ACL, pages 401?408, Sydney, Australia, July.
210
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 34?41,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Annotating Spoken Dialogs: from Speech Segments to Dialog Acts and
Frame Semantics
Marco Dinarelli, Silvia Quarteroni, Sara Tonelli, Alessandro Moschitti, Giuseppe Riccardi?
University of Trento
38050 Povo - Trento, Italy
{dinarelli,silviaq,moschitti,riccardi}@disi.unitn.it, satonelli@fbk.eu
Abstract
We are interested in extracting semantic
structures from spoken utterances gener-
ated within conversational systems. Cur-
rent Spoken Language Understanding sys-
tems rely either on hand-written seman-
tic grammars or on flat attribute-value se-
quence labeling. While the former ap-
proach is known to be limited in coverage
and robustness, the latter lacks detailed re-
lations amongst attribute-value pairs. In
this paper, we describe and analyze the hu-
man annotation process of rich semantic
structures in order to train semantic statis-
tical parsers. We have annotated spoken
conversations from both a human-machine
and a human-human spoken dialog cor-
pus. Given a sentence of the transcribed
corpora, domain concepts and other lin-
guistic features are annotated, ranging
from e.g. part-of-speech tagging and con-
stituent chunking, to more advanced anno-
tations, such as syntactic, dialog act and
predicate argument structure. In particu-
lar, the two latter annotation layers appear
to be promising for the design of complex
dialog systems. Statistics and mutual in-
formation estimates amongst such features
are reported and compared across corpora.
1 Introduction
Spoken language understanding (SLU) addresses
the problem of extracting and annotating the
meaning structure from spoken utterances in the
context of human dialogs (De Mori et al, 2008).
In spoken dialog systems (SDS) most used models
of SLU are based on the identification of slots (en-
?This work was partially funded by the European Com-
mission projects LUNA (contract 33549) and ADAMACH
(contract 022593).
tities) within one or more frames (frame-slot se-
mantics) that is defined by the application. While
this model is simple and clearly insufficient to
cope with interpretation and reasoning, it has sup-
ported the first generation of spoken dialog sys-
tems. Such dialog systems are thus limited by the
ability to parse semantic features such as predi-
cates and to perform logical computation in the
context of a specific dialog act (Bechet et al,
2004). This limitation is reflected in the type of
human-machine interactions which are mostly di-
rected at querying the user for specific slots (e.g.
?What is the departure city??) or implementing
simple dialog acts (e.g. confirmation). We believe
that an important step in overcoming such limita-
tion relies on the study of models of human-human
dialogs at different levels of representation: lexi-
cal, syntactic, semantic and discourse.
In this paper, we present our results in address-
ing the above issues in the context of the LUNA
research project for next-generation spoken dialog
interfaces (De Mori et al, 2008). We propose
models for different levels of annotation of the
LUNA spoken dialog corpus, including attribute-
value, predicate argument structures and dialog
acts. We describe the tools and the adaptation of
off-the-shelf resources to carry out annotation of
the predicate argument structures (PAS) of spoken
utterances. We present a quantitative analysis of
such semantic structures for both human-machine
and human-human conversations.
To the best of our knowledge this is the first
(human-machine and human-human) SDS corpus
denoting a multilayer approach to the annotation
of lexical, semantic and dialog features, which al-
lows us to investigate statistical relations between
the layers such as shallow semantic and discourse
features used by humans or machines. In the fol-
lowing sections we describe the corpus, as well as
a quantitative analysis and statistical correlations
between annotation layers.
34
2 Annotation model
Our corpus is planned to contain 1000 equally
partitioned Human-Human (HH) and Human-
Machine (HM) dialogs. These are recorded by
the customer care and technical support center of
an Italian company. While HH dialogs refer to
real conversations of users engaged in a problem
solving task in the domain of software/hardware
troubleshooting, HM dialogs are acquired with a
Wizard of Oz approach (WOZ). The human agent
(wizard) reacts to user?s spontaneous spoken re-
quests following one of ten possible dialog scenar-
ios inspired by the services provided by the com-
pany.
The above data is organized in transcrip-
tions and annotations of speech based on a new
multi-level protocol studied specifically within the
project, i.e. the annotation levels of words, turns1,
attribute-value pairs, dialog acts, predicate argu-
ment structures. The annotation at word level
is made with part-of-speech and morphosyntac-
tic information following the recommendations of
EAGLES corpora annotation (Leech and Wilson,
2006). The attribute-value annotation uses a pre-
defined domain ontology to specify concepts and
their relations. Dialog acts are used to annotate in-
tention in an utterance and can be useful to find
relations between different utterances as the next
section will show. For predicate structure annota-
tion, we followed the FrameNet model (Baker et
al., 1998) (see Section 2.2).
2.1 Dialog Act annotation
Dialog act annotation is the task of identifying
the function or goal of a given utterance (Sinclair
and Coulthard, 1975): thus, it provides a comple-
mentary information to the identification of do-
main concepts in the utterance, and a domain-
independent dialog act scheme can be applied.
For our corpus, we used a dialog act taxonomy
which follows initiatives such as DAMSL (Core
and Allen, 1997), TRAINS (Traum, 1996) and
DIT++ (Bunt, 2005). Although the level of granu-
larity and coverage varies across such taxonomies,
a careful analysis leads to identifying three main
groups of dialog acts:
1. Core acts, which represent the fundamen-
tal actions performed in the dialog, e.g. re-
1A turn is defined as the interval when a speaker is active,
between two pauses in his/her speech flow.
questing and providing information, or exe-
cuting a task. These include initiatives (often
called forward-looking acts) and responses
(backward-looking acts);
2. Conventional/Discourse management acts,
which maintain dialog cohesion and delimit
specific phases, such as opening, continua-
tion, closing, and apologizing;
3. Feedback/Grounding acts,used to elicit and
provide feedback in order to establish or re-
store a common ground in the conversation.
Our taxonomy, following the same three-fold
partition, is summarized in Table 1.
Table 1: Dialog act taxonomy
Core dialog acts
Info-request Speaker wants information from ad-
dressee
Action-request Speaker wants addressee to perform
an action
Yes-answer Affirmative answer
No-answer Negative answer
Answer Other kinds of answer
Offer Speaker offers or commits to perform
an action
ReportOnAction Speaker notifies an action is being/has
been performed
Inform Speaker provides addressee with in-
formation not explicitly required (via
an Info-request)
Conventional dialog acts
Greet Conversation opening
Quit Conversation closing
Apology Apology
Thank Thanking (and down-playing)
Feedback/turn management dialog acts
Clarif-request Speaker asks addressee for confirma-
tion/repetition of previous utterance
for clarification.
Ack Speaker expresses agreement with
previous utterance, or provides feed-
back to signal understanding of what
the addressee said
Filler Utterance whose main goal is to man-
age conversational time (i.e. dpeaker
taking time while keeping the turn)
Non-interpretable/non-classifiable dialog acts
Other Default tag for non-interpretable and
non-classifiable utterances
It can be noted that we have decided to retain
only the most frequent dialog act types from the
schemes that inspired our work. Rather than as-
piring to the full discriminative power of possible
conversational situations, we have opted for a sim-
ple taxonomy that would cover the vast majority
35
of utterances and at the same time would be able
to generalize them. Its small number of classes is
meant to allow a supervised classification method
to achieve reasonable performance with limited
data. The taxonomy is currently used by the sta-
tistical Dialogue Manager in the ADAMACH EU
project (Varges et al, 2008); the limited number
of classes allows to reduce the number of hypoth-
esized current dialogue acts, thus reducing the di-
alogue state space.
Dialog act annotation was performed manually
by a linguist on speech transcriptions previously
segmented into turns as mentioned above. The an-
notation unit for dialog acts, is the utterance; how-
ever, utterances are complex semantic entities that
do not necessarily correspond to turns. Hence, a
segmentation of the dialog transcription into ut-
terances was performed by the annotator before
dialog act labeling. Both utterance segmentation
and dialog act labeling were performed through
the MMAX tool (Mu?ller and Strube, 2003).
The annotator proceeded according to the fol-
lowing guidelines:
1. by default, a turn is also an utterance;
2. if more than one tag is applicable to an ut-
terance, choose the tag corresponding to its
main function;
3. in case of doubt among several tags, give pri-
ority to tags in core dialog acts group;
4. when needed, split the turn into several utter-
ances or merge several turns into one utter-
ance.
Utterance segmentation provides the basis not
only for dialog act labeling but also for the other
semantic annotations. See Fig. 1 for a dialog sam-
ple where each line represents an utterance anno-
tated according to the three levels.
2.2 Predicate Argument annotation
We carried out predicate argument structure an-
notation applying the FrameNet paradigm as de-
scribed in (Baker et al, 1998). This model
comprises a set of prototypical situations called
frames, the frame-evoking words or expressions
called lexical units and the roles or participants in-
volved in these situations, called frame elements.
The latter are typically the syntactic dependents of
the lexical units. All lexical units belonging to
the same frame have similar semantics and show
                                              PERSON-NAME 
Info: Buongiorno, sono   Paola.  
  
          GREETING    B._NAMED Name 
Good morning, this is Paola. 
 
Info-req: Come la posso aiutare? 
                      
                    Benefitted_party   ASSISTANCE 
How may I help you? 
 
                                                       CONCEPT         HARDWARE-COMPONENT 
Info: Buongiorno. Ho un problema con la stampante.  
 
          GREETING            PR._DESCRIPTION     Affected_device 
Good morning. I have a problem with the printer. 
 
           PART-OF-DAY   NEGAT. ACTION                ACTION 
Info: Da stamattina non   riesco pi? a  stampare 
                                       
                                    Problem 
Since this morning I can?t print. 
 
Info-req:   Mi  pu?  dire   nome e cognome per favore? 
 
              Addressee      TELLING               Message 
Can you tell me your name and surname, please? 
 
                                       PERSON-NAME  PERSON-SURNAME 
Answer: Mi chiamo  Alessandro  Manzoni. 
 
               Entity B._NAMED                   Name 
My name is Alessandro Manzoni. 
Figure 1: Annotated dialog extract. Each utterance
is preceded by dialog act annotation. Attribute-
value annotation appears above the text, PAS an-
notation below the text.
the same valence. A particular feature of the
FrameNet project both for English and for other
languages is its corpus-based nature, i.e. every el-
ement described in the resource has to be instanti-
ated in a corpus. To annotate our SDS corpus, we
adopted where possible the already existing frame
and frame element descriptions defined for the En-
glish FrameNet project, and introduced new def-
initions only in case of missing elements in the
original model.
Figure 1 shows a dialog sample with PAS an-
notation reported below the utterance. All lexi-
cal units are underlined and the frame is written in
capitals, while the other labels refer to frame el-
ements. In particular, ASSISTANCE is evoked by
the lexical unit aiutare and has one attested frame
element (Benefitted party), GREETING has no
frame element, and PROBLEM DESCRIPTION
and TELLING have two frame elements each.
Figure 2 gives a comprehensive view of the an-
notation process, from audio file transcription to
the annotation of three semantic layers. Whereas
36
Figure 2: The annotation process
Audio file 
Turn segmentation & 
Transcription 
Utterance segmentation 
POS tagging Domain attribute 
annotation 
PAS annotation 
Dialog Act 
annotation 
Syntactic parsing 
attribute-value and DA annotation are carried
out on the segmented dialogs at utterance level,
PAS annotation requires POS-tagging and syntac-
tic parsing (via Bikel?s parser trained for Italian
(Corazza et al, 2007)). Finally, a shallow manual
correction is carried out to make sure that the tree
nodes that may carry semantic information have
correct constituent boundaries. For the annotation
of frame information, we used the Salto tool (Bur-
chardt et al, 2006), that stores the dialog file in
TIGER-XML format and allows to easily intro-
duce word tags and frame flags. Frame informa-
tion is recorded on top of parse trees, with target
information pointing to terminal words and frame
elements pointing to tree nodes.
3 Quantitative comparison of the
Annotation
We evaluated the outcome of dialog act and
PAS annotation levels on both the human-human
(henceforth HH) and human-machine (HM) cor-
pora by not only analyzing frequencies and occur-
rences in the separate levels, but also their interac-
tion, as discussed in the following sections.
3.1 Dialog Act annotation
Analyzing the annotation of 50 HM and 50 HH
dialogs at the dialog act level, we note that an
HH dialog is composed in average by 48.9?17.4
(standard deviation) dialog acts, whereas a HM
dialog is composed of 18.9?4.4. The difference
between average lengths shows how HH sponta-
neous speech can be redundant, while HM dialogs
are more limited to an exchange of essential infor-
mation. The standard deviation of a conversation
in terms of dialog acts is considerably higher in
the HH corpus than in the HM one. This can be ex-
plained by the fact that the WOZ follows a unique,
previously defined task-solving strategy that does
not allow for digressions. Utterance segmentation
was also performed differently on the two corpora.
In HH we performed 167 turn mergings and 225
turn splittings; in HM dialogs, only turn splittings
(158) but no turn mergings were performed.
Tables 2 and 3 report the dialog acts occurring
in the HM and HH corpora, respectively, ranked
by their frequencies.
Table 2: Dialog acts ranked by frequency in the
human-machine (HM) corpus
human-machine (HM)
DA count rel. freq.
Info-request 249 26.3%
Answer 171 18.1%
Inform 163 17.2%
Yes-answer 70 7.4%
Quit 60 6.3%
Thank 56 5.9%
Greet 50 5.3%
Offer 49 5.2%
Clarification-request 26 2.7%
Action-request 25 2.6%
Ack 12 1.3%
Filler 6 0.6%
No-answer 5 0.5%
Other, ReportOnAction 2 0.2%
Apology 1 0.1%
TOTAL 947
From a comparative analysis, we note that:
1. info-request is by far the most common dia-
log act in HM, whereas in HH ack and info
share the top ranking position;
2. the most frequently occurring dialog act in
HH, i.e. ack, is only ranked 11th in HM;
3. the relative frequency of clarification-request
(4,7%) is considerably higher in HH than in
HM.
We also analyzed the ranking of the most fre-
quent dialog act bigrams in the two corpora. We
can summarize our comparative analysis, reported
in Table 4, to the following: in both corpora,
most bigram types contain info and info-request,
37
Table 3: Dialog acts ranked by frequency in the
human-human (HH) corpus
human-human (HH)
DA count rel. freq.
Ack 582 23.8%
Inform 562 23.0%
Info-request 303 12.4%
Answer 192 7.8%
Clarification-request 116 4.7%
Offer 114 4.7%
Yes-answer 112 4.6%
Quit 101 4.1%
ReportOnAction 91 3.7%
Other 70 2.9%
Action-request 69 2.8%
Filler 61 2.5%
Thank 33 1.3%
No-answer 26 1.1%
Greet, Apology 7 0.3%
TOTAL 2446
as expected in a troubleshooting system. How-
ever, the bigram info-request answer, which we
expected to form the core of a task-solving dia-
log, is only ranked 5th in the HH corpus, while 5
out of the top 10 bigram types contain ack. We
believe that this is because HH dialogs primarily
contain spontaneous information-providing turns
(e.g. several info info by the same speaker) and
acknowledgements for the purpose of backchan-
nel. Instead, HM dialogs, structured as sequences
of info-request answers pairs, are more minimal
and brittle, showing how users tend to avoid re-
dundancy when addressing a machine.
Table 4: The 10 most frequent dialog act bigrams
human-machine (HM) human-human (HH)
info-req answer ack info
answer info-req info ack
info info-req info info
info-req y-answer ack ack
sentence beginning greet info-req answer
greet info info info-req
info quit info-req y-answer
offer info ack info-req
thank info answer ack
y-answer thank quit sentence end
3.2 Predicate Argument annotation
We annotated 50 HM and 50 HH dialogs with
frame information. Differently from the English
FrameNet database, we didn?t annotate one frame
per sentence. On the contrary, we identified all
lexical units corresponding to ?semantically rele-
vant? verbs, nouns and adjectives with a syntac-
tic subcategorization pattern, eventually skipping
the utterances with empty semantics (e.g. dis-
fluencies). In particular, we annotated all lexical
units that imply an action, introduce the speaker?s
opinion or describe the office environment. We
introduced 20 new frames out of the 174 iden-
tified in the corpus because the original defini-
tion of frames related to hardware/software, data-
handling and customer assistance was sometimes
too coarse-grained. Few new frame elements were
introduced as well, mostly expressing syntactic re-
alizations that are typical of spoken Italian.
Table 5 shows some statistics about the cor-
pus dimension and the results of our annotation.
The human-human dialogs contain less frame in-
stances in average than the human-machine group,
meaning that speech disfluencies, not present in
turns uttered by the WOZ, negatively affect the se-
mantic density of a turn. For the same reason, the
percentage of turns in HH dialogs that were manu-
ally corrected in the pre-processing step (see Sec-
tion 2.2) is lower than for HM turns, since HH di-
alogs have more turns that are semantically empty
and that were skipped in the correction phase. Be-
sides, HH dialogs show a higher frame variabil-
ity than HM, which can be explained by the fact
that spontaneous conversation may concern mi-
nor topics, whereas HM dialogs follow a previ-
ously defined structure, designed to solve soft-
ware/hardware problems.
Tables 6 and 7 report the 10 most frequent
frames occurring in the human-machine resp.
human-human dialogs. The relative frame fre-
quency in HH dialogs is more sparse than in HM
dialogs, meaning that the task-solving strategy fol-
lowed by the WOZ limits the number of digres-
sions, whereas the semantics of HH dialogs is
richer and more variable.
As mentioned above, we had to introduce and
define new frames which were not present in the
original FrameNet database for English in order to
capture all relevant situations described in the di-
alogs. A number of these frames appear in both
tables, suggesting that the latter are indeed rel-
38
Table 5: Dialog turn and frame statistics for the
human-machine (HM) resp. human-human (HH)
corpus
HM HH
Total number of turns 662 1,997
Mean dialog length (turns) 13.2 39.9
Mean turn length (tokens) 11.4 10.8
Corrected turns (%) 50 39
Total number of annotations 923 1951
Mean number of frame annota-
tions per dialog
18.5 39.0
Mean number of frame elements
per frame annotation
1.6 1.7
evant to model the general semantics of the di-
alogs we are approaching. The most frequent
frame group comprises frames relating to infor-
mation exchange that is typical of the help-desk
activity, including Telling, Greeting, Contacting,
Statement, Recording, Communication. Another
relevant group encompasses frames related to the
operational state of a device, for example Be-
ing operational, Change operational state, Oper-
ational testing, Being in operation.
The two groups also show high variability of
lexical units. Telling, Change operational state
and Greeting have the richest lexical unit set,
with 11 verbs/nouns/adjectives each. Arriving
and Awareness are expressed by 10 different lexi-
cal units, while Statement, Being operational, Re-
moving and Undergo change of operational state
have 9 different lexical units each. The informal
nature of the spoken dialogs influences the com-
position of the lexical unit sets. In fact, they are
rich in verbs and multiwords used only in collo-
quial contexts, for which there are generally few
attestations in the English FrameNet database.
Similarly to the dialog act statistics, we also
analyzed the most frequent frame bigrams and
trigrams in HM and HH dialogs. Results are
reported in Tables 8 and 9. Both HH bigrams
and trigrams show a more sparse distribution and
lower relative frequency than HM ones, implying
that HH dialogs follow a more flexible structure
with a richer set of topics, thus the sequence of
themes is less predictable. In particular, 79%
of HH bigrams and 97% of HH trigrams occur
only once (vs. 68% HM bigrams and 82% HM
trigrams). On the contrary, HM dialogs deal with
Table 6: The 10 most frequent frames in the HM
corpus (* =newly introduced)
HM corpus
Frame count freq-%
Greeting* 146 15.8
Telling 134 14.5
Recording 83 8.9
Being named 74 8.0
Contacting 52 5.6
Usefulness 50 5.4
Being operational 28 3.0
Problem description* 24 2.6
Inspecting 24 2.6
Perception experience 21 2.3
Table 7: The 10 most frequent frames in the HH
corpus (* =newly introduced)
HH corpus
Frame count freq-%
Telling 143 7.3
Greeting* 124 6.3
Awareness 74 3.8
Contacting 63 3.2
Giving 62 3.2
Navigation* 61 3.1
Change operational state 51 2.6
Perception experience 46 2.3
Insert data* 46 2.3
Come to sight* 38 1.9
a fix sequence of topics driven by the turns uttered
by the WOZ. For instance, the most frequent
HM bigram and trigram both correspond to the
opening utterance of the WOZ:
Help desk buongiornoGREETING, sonoBEING NAMED
Paola, in cosa posso esserti utileUSEFULNESS?
(Good morning, help-desk service, Paola speaking, how can
I help you?)
3.3 Mutual information between PAS and
dialog acts
A unique feature of our corpus is the availabil-
ity of both a semantic and a dialog act annota-
tion level: it is intuitive to seek relationships in
the purpose of improving the recognition and un-
derstanding of each level by using features from
the other. We considered a subset of 20 HH and
50 HM dialogs and computed an initial analysis
39
Table 8: The 5 most frequent frame bigrams
human-machine (HM) freq-%
Greeting Being named 17.1
Being named Usefulness 15.3
Telling Recording 12.9
Recording Contacting 10.9
Contacting Greeting 10.6
human-human (HH) freq-%
Greeting Greeting 4.7
Navigation Navigation 1.2
Telling Telling 1.0
Change op. state Change op. state 0.9
Telling Problem description 0.8
Table 9: The 5 most frequent frame trigrams
human-machine (HM) freq-%
Greeting Being named Usefulness 9.5
Recording Contacting Greeting 5.7
Being named Usefulness Greeting 3.7
Telling Recording Contacting 3.5
Telling Recording Recording 2.2
human-human (HH) freq-%
Greeting Greeting Greeting 1.6
Greeting Being named Greeting 0.5
Contacting Greeting Greeting 0.3
Navigation Navigation Navigation 0.2
Working on Greeting Greeting 0.2
of the co-occurrences of dialog acts and PAS. We
noted that each PAS tended to co-occur only with a
limited subset of the available dialog act tags, and
moreover in most cases the co-occurrence hap-
pened with only one dialog act. For a more thor-
ough analysis, we computed the weighted condi-
tional entropy between PAS and dialog acts, which
yields a direct estimate of the mutual information
between the two levels of annotation2.
2Let H(yj |xi) be the weighted conditional entropy of ob-
servation yj of variable Y given observation xi of variable
X:
H(yj |xi) = ?p(xi; yj)log
p(xi; yj)
p(xi)
,
where p(xi; yj) is the probability of co-occurrence of xi and
yj , and p(xi) and p(yj) are the marginal probabilities of oc-
currence of xi resp. yj in the corpus. There is an obvious re-
lation with the weighted mutual information between xi and
yj , defined following e.g. (Bechet et al, 2004) as:
wMI(xi; yj) = p(xi; yj)log
p(xi; yj)
p(xi)p(yj)
.
(a) human-machine dialogs (filtering co-occurrences below 3)
(b) human-human dialogs (filtering co-occurrences below 5)
Figure 3: Weighted conditional entropy between
PAS and dialog acts in the HM (a) and HH corpus
(b). To lower entropies correspond higher values
of mutual information (darker color in the scale)
Our results are illustrated in Figure 3. In the
HM corpus (Fig. 3(a)), we noted some interesting
associations between dialog acts and PAS. First,
info-req has the maximal MI with PAS like Be-
ing in operation and Being attached, as requests
are typically used by the operator to get informa-
tion about the status of device. Several PAS de-
note a high MI with the info dialog act, includ-
ing Activity resume, Information, Being named,
Contacting, and Resolve problem. Contacting
refers to the description of the situation and of the
speaker?s point of view (usually the caller). Be-
ing named is primarily employed when the caller
introduces himself, while Activity resume usually
refers to the operator?s description of the sched-
Indeed, the higher is H(yj |xi), the lower is wMI(xi; yj).
We approximate all probabilities using frequency of occur-
rence.
40
uled interventions.
As for the remaining acts, clarif has the high-
est MI with Perception experience and Statement,
used to warn the addressee about understanding
problems and asking him to repeat/rephrase an ut-
terance, respectively. The two strategies can be
combined in the same utterance, as in the utter-
ance: Non ho sentito bene: per favore ripeti cer-
cando di parlare piu` forte. (I haven?t quite heard
that, please repeat trying to speak up.).
The answer tag is highly informative with Suc-
cessful action, Change operational state, Becom-
ing nonfunctional, Being detached, Read data.
These PAS refer to the exchange of infor-
mation (Read data) or to actions performed
by the user after a suggestion of the system
(Change operational state). Action requests (act-
req) seem to be correlated to Replacing as it usu-
ally occurs when the operator requests the caller
to carry out an action to solve a problem, typically
to replace a component with another. Another fre-
quent request may refer to some device that the
operator has to test.
In the HH corpus (Fig. 3(b)), most of the PAS
are highly mutually informative with info: in-
deed, as shown in Table 3, this is the most fre-
quently occurring act in HH except for ack, which
rarely contain verbs that can be annotated by a
frame. As for the remaining acts, there is an easily
explainable high MI between quit and Greeting;
moreover, info-req denote its highest MI with
Giving, as in requests to give information, while
rep-action denotes a strong co-occurrence with
Inchoative attaching: indeed, interlocutors often
report on the action of connecting a device.
These results corroborate our initial observation
that for most PAS, the mutual information tends
to be very high in correspondence of one dialog
act type: this suggests the beneficial effect of in-
cluding shallow semantic information as features
for dialog act classification. The converse is less
clear as the same dialog act can relate to a span
of words covered by multiple PAS and generally,
several PAS co-occur with the same dialog act.
4 Conclusions
In this paper we have proposed an approach to
the annotation of spoken dialogs using seman-
tic and discourse features. Such effort is crucial
to investigate the complex dependencies between
the layers of semantic processing. We have de-
signed the annotation model to incorporate fea-
tures and models developed both in the speech
and language research community and bridging
the gap between the two communities. Our multi-
layer annotation corpus allows the investigation
of cross-layer dependencies and across human-
machine and human-human dialogs as well as
training of semantic models which accounts for
predicate interpretation.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
ACL/Coling?98, pages 86?90.
F. Bechet, G. Riccardi, and D. Hakkani-Tur. 2004.
Mining spoken dialogue corpora for system evalu-
ation and modeling. In Proceedings of EMNLP?04,
pages 134?141.
H. Bunt. 2005. A framework for dialogue act specica-
tion. In Proceedings of SIGSEM WG on Represen-
tation of Multimodal Semantic Information.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. Salto - a versatile multi-
level annotation tool. In Proceedings of LREC 2006,
pages 517?520, Genoa, Italy.
A. Corazza, A. Lavelli, and G. Satta. 2007. Anal-
isi sintattica-statistica basata su costituenti. Intelli-
genza Artificiale, 4(2):38?39.
M. G. Core and J. F. Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communicative
Actions in Humans and Machines.
R. De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,
G. Riccardi, and G. Tur. 2008. Spoken language
understanding: A survey. IEEE Signal Processing
magazine, 25(3):50?58.
G. Leech and A. Wilson. 2006. EAGLES recommen-
dations for the morphosyntactic annotation of cor-
pora. Technical report, ILC-CNR.
C. Mu?ller and M. Strube. 2003. Multi-level annotation
in MMAX. In Proceedings of SIGDIAL?03.
J. M. Sinclair and R. M. Coulthard. 1975. Towards an
Analysis of Discourse: The English Used by Teach-
ers and Pupils. Oxford University Press, Oxford.
D. Traum. 1996. Conversational agency: The
TRAINS-93 dialogue manager. In Proceedings of
TWLT 11: Dialogue Management in Natural Lan-
guage Systems, pages 1?11, June.
S. Varges, G. Riccardi, and S. Quarteroni. 2008. Per-
sistent information state in a data-centric architec-
ture. In Proceedings of SIGDIAL?08.
41
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1104?1115,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Hypotheses Selection Criteria in a Reranking Framework for Spoken
Language Understanding
Marco Dinarelli
LIMSI-CNRS
B.P. 133, 91403 Orsay Cedex
France
marcod@limsi.fr
Sophie Rosset
LIMSI-CNRS
B.P. 133, 91403 Orsay Cedex
France
rosset@limsi.fr
Abstract
Reranking models have been successfully ap-
plied to many tasks of Natural Language Pro-
cessing. However, there are two aspects of
this approach that need a deeper investiga-
tion: (i) Assessment of hypotheses generated
for reranking at classification phase: baseline
models generate a list of hypotheses and these
are used for reranking without any assess-
ment; (ii) Detection of cases where rerank-
ing models provide a worst result: the best
hypothesis provided by the reranking model
is assumed to be always the best result. In
some cases the reranking model provides an
incorrect hypothesis while the baseline best
hypothesis is correct, especially when base-
line models are accurate. In this paper we
propose solutions for these two aspects: (i)
a semantic inconsistency metric to select pos-
sibly more correct n-best hypotheses, from a
large set generated by an SLU basiline model.
The selected hypotheses are reranked apply-
ing a state-of-the-art model based on Partial
Tree Kernels, which encode SLU hypothe-
ses in Support Vector Machines with com-
plex structured features; (ii) finally, we apply
a decision strategy, based on confidence val-
ues, to select the final hypothesis between the
first ranked hypothesis provided by the base-
line SLU model and the first ranked hypothe-
sis provided by the re-ranker. We show the ef-
fectiveness of these solutions presenting com-
parative results obtained reranking hypothe-
ses generated by a very accurate Conditional
Random Field model. We evaluate our ap-
proach on the French MEDIA corpus. The re-
sults show significant improvements with re-
spect to current state-of-the-art and previous
re-ranking models.
1 Introduction
Discriminative reranking is a widely used approach
for several Natural Language Processing (NLP)
tasks: Syntactic Parsing (Collins, 2000), Named En-
tity Recognition (Collins, 2000; Collins and Duffy,
2001), Semantic Role Labelling (Moschitti et al,
2008), Machine Translation (Shen et al, 2004),
Question Answering (Moschitti et al, 2007). Re-
cently reranking approaches have been successfully
applied also to Spoken Language Understanding
(SLU) (Dinarelli et al, 2009b).
Discriminative Reranking combines two models:
a first SLU model is used to generate a ranked list
of n-best hypotheses; a reranking model sorts the
list based on a different score and the final result
is the new top ranked hypothesis. The advantage of
reranking approaches is in the possibility to learn di-
rectly complex dependencies in the output domain,
as this is provided in the hypotheses generated by
the baseline model.
In previous approaches complex features are ex-
tracted from the hypotheses for both training and
classification phase, but there are very few stud-
ies on approaches that can be applied to search in
the hypotheses space generated by the baseline SLU
model. Moreover, to keep overall computational
cost reasonable, the size of the n-best list is typically
small (few tens). This is a limitation since the larger
is the hypotheses space generated, the more likely is
to find a better hypothesis. On the other hand, re-
ranking a large set of hypotheses is computationally
1104
expensive, thus a strategy to select the best hypothe-
ses to be re-ranked would overcome this problem.
Another aspect of reranking that deserves to be
deeper studied is its applicability. Although a
reranking model improves the baseline model in the
overall performance, in some cases the reranked best
hypotheses can contain more mistakes than the base-
line best hypothesis. A strategy to decide when the
reranking model should be applied and when the
first hypothesis of the baseline model is more accu-
rate would improve reranking performances.
In this paper, we propose two new models for
improving discriminative reranking: (a) a seman-
tic inconsistency metric that can be applied to SLU
hypotheses to select those that are more likely to
be correct; (b) a model selection strategy based on
the confidence scores provided by the baseline SLU
model and the reranker. This provides a decision
function that detects if the original top ranked hy-
pothesis is more accurate than the reranked best hy-
pothesis.
Our re-ranking strategies turn out to be effective
on very accurate baseline models based on state-of-
the-art Conditinal Random Fields (CRF) implemen-
tation (Lavergne et al, 2010). We evaluate our ap-
proach on the well-known French MEDIA corpus
for SLU (Bonneau-Maynard et al, 2006). The re-
sults show that our approach significantly improves
both ?traditional? reranking approaches and state-
of-the-art SLU models.
The remainder of the paper is organized as fol-
lows: in Section 2 we introduce the SLU task. Sec-
tion 3 describes our discriminative reranking frame-
work for SLU, in particular the baseline model
adopted, in sub-section 3.1, and the reranking
model, in sub-section 3.2. Section 4 describes
the two strategies proposed in this paper for SLU
reranking, whereas the experiments to evaluate our
approaches are described in Section 5. Finally, after
a discussion in Section 6, in Section 7 we draw some
conclusions.
2 Spoken Language Understanding
Spoken Language Understanding is the task of rep-
resenting and extracting the meaning of natural lan-
guage sentences. Designing a general meaning rep-
resentation which can capture the semantics of a
spoken language is very complex. Therefore, in
practice, the meaning representations depend on the
specific application domain being modeled.
For the corpus used in this work, the semantic rep-
resentation is defined in an ontology described in
(Bonneau-Maynard et al, 2006). As an example,
given the following natural language sentence trans-
lated from the MEDIA corpus:
?Good morning I would like to book an hotel room in Lon-
don?
The semantic representation extraction for the
SLU task is performed in two steps:
1. Automatic Concept Labeling
Null{Good morning} command-task{I would like to book}
object-bd{an hotel room} localization-city{in London}
2. Attribute-Value Extraction
command-task[reservation] object-bd[hotel] localization-
city[London]
command-task, object-bd and localization-city
are three domain concepts, called also ?attributes?,
defined in the ontology and Null is the concept for
words not associated to any concept. As shown in
the example, Null concepts are removed from the fi-
nal output since they don?t bring any semantic con-
tent with respect to the application domain. reserva-
tion, hotel and London are the normalized attribute
values, defined also in the application ontology. This
representation is usually called attribute-value repre-
sentation.
In the last decade several probabilistic models
have been proposed for the Automatic Concept La-
beling step: in (Raymond et al, 2006) a conceptual
language model encoded in Stochastic Finite State
Transducers (SFST) is proposed. In (Raymond and
Riccardi, 2007), the SFST-based model is compared
with Support Vector Machines (SVM) (Vapnik,
1998) and Conditional Random Fields (CRF) (Laf-
ferty et al, 2001). Moreover, in (Hahn et al, 2008a)
two more models are applied to SLU: a Maximum
Entropy (EM) model and a model coming from
the Statistical Machine Translation (SMT) commu-
nity (it is actually a log-linear combination of SMT
models). Among these models, CRF has shown in
general superior performances on sequence labeling
tasks like Named Entity Recognition (NER) (Tjong
Kim Sang and De Meulder, 2003), Grapheme-to-
Phoneme transcription (Sejnowski and Rosenberg,
1105
1987) and also Spoken Language Understanding
(Hahn et al, 2008a).
In addition to individual systems, more recently
also some system combination approaches have
been tried on SLU. In (Hahn et al, 2010), two such
approaches are compared, one based on weighted
ROVER (Fiscus, 1997) while the other is the rerank-
ing approach proposed in (Dinarelli et al, 2009b).
Both system combination approaches are applied on
the MEDIA corpus, thus we will refer to (Hahn et
al., 2010) for a comparison with our approach.
Like the other tasks mentioned above, SLU is usu-
ally a supervised learning task, this means that mod-
els are learned from annotated data. This is an im-
portant aspect to take into account when designing
SLU systems. In this respect accurate SLU models
can in part alleviate the problem of manually anno-
tating data.
The second step of SLU, that is Attribute Value
Extraction (from now on AVE) is performed with
two approaches: a) Rule-based approaches apply
Regular Expressions (RE) to map the words realiz-
ing a concept into a normalized value. Regular ex-
pressions are defined for each attribute-value pair.
Given a concept and its realizing surface form, if a
RE for that concept matches the surface, the corre-
sponding value is returned.
An example of surfaces that can be mapped into
the value hotel given the concept object-bd is:
1. an hotel room
2. a hotel room
3. the hotel
...
Note that these surfaces share the same keyword
for the concept object-bd, which is ?hotel?. Thus,
a possible rule extracted from data, for the concept
object-bd can be:
Robject?bd(S) =
if S = ?an hotel room? or
S = ?a hotel room? or
S = ?the hotel? then
return ?hotel?
end
This kind of rules can be easily refined using reg-
ular expressions, so that they can capture all possible
linguistic patterns containing the triggering keyword
(?hotel? in the example).
b) The other approach used for attribute value ex-
traction is based on probabilistic models. In this case
the model learns from data the conditional probabil-
ity of values V , given the concept C and the cor-
responding sequence of words W realizing the con-
cept: P (V |W,C).
The most meaningful work about AVE ap-
proaches in SLU tasks is (Hahn et al, 2010).
The model used in this work for Automatic Con-
cept Labeling is based on CRF. For the Attribute-
Value Extraction phase we use a combination of
rule based and probabilistic approaches. The first
is made of regular expressions, as explained above.
The probabilistic approach is based again on CRF.
3 Reranking Framework
This section describes the different models involved
in the pipeline realising our reranking framework:
? Conditional Random Fields
? Semantic Inconsistency Metric for hypotheses
selection, which is optional and is applied only
at the classification phase
? Support Vector Machines with Partial Tree Ker-
nel
? Decision Strategy to detect when the top ranked
hypothesis of the baseline model is more accu-
rate than the reranked best hypothesis
It is important to underline that the phases in-
volved in the reranking framewrok are distinguished
for a matter of clarity. In principle, the phases
from the hypotheses selection to the last, the deci-
sion strategy, can be thought of as a whole reranking
model.
In the next two subsection we describe the two
models used for hypotheses generation and for
reranking: CRF and SVM with kernel methods. The
two improvements proposed in this paper and listed
above are presented in a dedicated section (4).
3.1 Conditional Random Fields
CRFs have been proposed for the first time for se-
quence segmentation and labeling tasks in (Lafferty
et al, 2001). This model belongs to the family of
exponential or log-linear models. Its main charac-
teristics are the possibility to include a huge number
1106
of features, like the Maximum Entropy (ME) model,
but computing global conditional probabilities nor-
malized at sentence level, instead of position level
like in ME. In particular this last point results very
effective since it solves the label bias problem, as
pointed out in (Lafferty et al, 2001).
Given a sequence of N words WN1 = w1, ..., wN
and its corresponding sequence of concepts CN1 =
c1, ..., cN , CRF trains the conditional probabilities
P (CN1 |WN1 ) =
1
Z
N?
n=1
exp
( M?
m=1
?m ? hm(cn?1, cn, wn+2n?2)
)
(1)
where ?m are the training parameters.
hm(cn?1, cn, wn+2n?2) are the feature functions
capturing conditional dependencies of concepts and
words. Z is a probability normalization factor in
order to model well defined probability distribution:
Z =
?
c?N1
N?
n=1
H(c?n?1, c?n, wn+2n?2) (2)
here c?n?1 and c?n are the concepts hypoth-
esized for the previous and current words,
H(c?n?1, c?n, wn+2n?2) is an abbreviation for?M
m=1 ?m ? hm(cn?1, cn, wn+2n?2).
The CRF model used for the Attribute-Value Ex-
traction phase learns in the same way the conditional
probability P (V N1 |CN1 ,WN1 ). In particular we use
attributes-words concatenations on the source side
and attribute values on the target side.
Two particular effective implementations of CRFs
have been recently proposed. One is described in
(Hahn et al, 2009) and uses a margin based criterion
for probabilities estimation. The other is described
in (Lavergne et al, 2010) and has been implemented
in the software wapiti1. The latter solution in partic-
ular trains the model using two different regulariza-
tion factors at the same time:
Gaussian prior, used as l2 regularization and used
in many softwares to avoid overfitting;
Laplacian prior, used as l1 regularization (Riezler
and Vasserman, 2010), which has the effect to filter
out features with very low scores.
1available at http://wapiti.limsi.fr
The two regularization parameters are used to-
gether in the model implementing the so-called elas-
tic net regularization (Zou and Hastie, 2005):
l(?) + ?1???1 +
?2
2 ???
2
2 (3)
? is the set of parameters of the model introduced
in equation 1, l(?) is the minus-logarithm of equa-
tion 1, used as loss function for training CRF. ???1
and ???2 are the l1 and l2 regularization, respec-
tively, while ?1 and ?2 are two parameters that can
be optimized as usual on development data or with
cross validation.
As explained in (Lavergne et al, 2010), using l1
regularization is an effective way for feature selec-
tion in CRF at training time. Note that other ap-
proaches have been proposed for feature selection,
e.g. in (McCallum, 2003). This type of features se-
lection, performed directly at training time, yields
very accurate models, since only the most meaning-
ful features are kept in the final model, which guar-
antee a strong robustness on unseen data.
In this work we refer in particular to the CRF im-
plementation described in (Lavergne et al, 2010).
3.2 SVM and Kernel Methods
Our reranking model is based on SVM (Vapnik,
1998) with the use of the Partial Tree Kernel defined
in (Moschitti, 2006).
SVMs are well-known machine learning algo-
rithms belonging to the class of maximal-margin lin-
ear classifiers (Vapnik, 1998). The model represents
a hyperplane which separates the training examples
with a maximum margin. The hyperplane is learned
using optimization theory and is represented in the
dual form as a linear combination of training exam-
ples:?
i=1..l yi?i ~xi~x + b = 0,
where ~xi, i ? [1, .., l] are training examples rep-
resenting objects oi and o in any feature space, yi is
the label associated with ~xi and ?i are the lagrange
multipliers. The dual form of the hyperplane shows
that SVM training depends on the inner product be-
tween instances. Kernel methods theory (Shawe-
Taylor and Cristianini, 2004), allows us to substitute
the inner product with a so-called kernel function,
computing the same result: K(oi, o) = ~xi ? ~x.
1107
The interesting aspect of using such formulation
is the possibility to compare objects in arbitrar-
ily complex feature spaces implicitly, i.e. without
knowing the features to be used. Since in real world
scenarios data cannot be classified using a simple
linear classifier, kernel methods can be used to carry
out learning in complex feature spaces. In this work
we use the Partial Tree Kernel (PTK) (Moschitti,
2006).
3.3 Reranking Model
In order to give an effective representation to SLU
hypotheses in SVM, since we are using PTK, we
need to represent as trees SLU hypotheses like the
one described in section 2.
This problem is easily solved by transforming the
hypotheses into trees like the one depicted in fig-
ure 1. Although there may be more formal solutions
to represent semantic information of SLU hypothe-
ses as trees, we would like to remark that the tree
structure shown in figure 1 contains all the key in-
formation needed for our purposes: the first level of
the tree represents the concept sequence annotated
on surface form. The second level of the tree al-
low to implicitly represent the segmentation of each
concept, while the third level, i.e. the leaves, are the
input words. Moreover, from figure 1 we removed
word categories associated to words in order to keep
the figure readable. Word categories are provided
together with the corpus as an application knowl-
edge base. They comprise domain categories like
city names, hotel names, street names etc., and some
domain independent categories like numbers, dates,
months etc. The categories are used at the same level
of words, they provide a generalization over words
and alleviate the effect of Out-of-Vocabulary (OOV)
words.
The CRF model used as baseline generates the
n most likely conceptual annotations for each input
sentence. These are ranked by the global conditional
probability of the concept sequence, given the input
word sequence of CRF. The n-best list produced by
the baseline model is the list of candidate hypotheses
H1, H2, .., Hn used in the reranking step.
The candidate hypotheses are organized into
pairs, e.g. (H1, H2) or (H1, H3). We build train-
ing pairs such that a reranker can learn to select the
best one between the two hypotheses in a pair, i.e.
the more correct hypothesis with respect to a refer-
ence annotation and a given metric. In particular,
we compute the edit distance of each hypothesis in
the list, with respect to the manual annotation taken
from the corpus. The best hypothesis Hb is used
to build positive instances for the reranker as pairs
(Hb, Hi) for i ? [1..n] and i 6= b, negative instances
are built as (Hi, Hb), with same constraints on index
i. This means that, if n hypotheses are generated for
a sentence, 2 ? n instances are generated from them.
Note that by construction of pairs the model is sym-
metric, this provides a property that will be exploited
at classification phase, as described in (Shen et al,
2003b).
Hypotheses are then converted into trees like the
one shown in figure 1. Pairs of trees ek = (ti,k, tj,k),
for k varying along all the training or classification
instances, are given as input to the SVM model to
train the reranker using the following reranking ker-
nel:
KR(e1, e2) = PTK(t1,1, t1,2) + PTK(t2,1, t2,2) (4)
? PTK(t1,1, t2,2)? PTK(t2,1, t1,2),
where e1 and e2 are two pairs of trees to be com-
pared.
The reranking kernel in equation 4, consisting in
summing four different kernels, has been proposed
in (Shen et al, 2003b) for syntactic parsing rerank-
ing, where the basic kernel was a Tree Kernel, and
the idea was taken in turn from (Heibrich et al,
2000), where pairs where used to learn preference
ranking. The same idea appears also, in a slightly
different form, in early work about reranking, e.g.
(Collins and Duffy, 2002). The same reranking
schema has been used also in (Shen et al, 2004)
for reranking different candidate hypotheses for ma-
chine translation.
For classification, observing that the model is
symmetric and exploiting kernel properties, we can
use, as classification instances, simple hypotheses
instead of pairs. More precisely we use pairs where
the second hypothesis is empty, i.e. (Hi, 0), for
i ? [1..n]. This simplification allow a relatively fast
classification phase, since only n instances are gen-
erated for each sentence, instead of n2. This simpli-
fication has been proposed in (Shen et al, 2003b).
1108
Figure 1: An example of semantic tree constructed from an SLU hypothesis from the MEDIA corpus and used in PTK
4 Hypotheses Selection Criteria
This section describes the main contribution of our
work: first, a semantic inconsistency metric based
on the AVE phase of SLU and allowing to select hy-
potheses generated by the baseline model; second, a
strategy to decide, after the reranking phase, if it is
more likely that the baseline best hypothesis is more
accurate than the best reranked hypothesis and al-
lowing to recover the mistake. Similar ideas have
been proposed in (Dinarelli et al, 2010), here we
propose a significant evolution and we give a much
wider description and evaluation.
4.1 Hypotheses Selection via Attribute Value
Extraction (AVE)
In previous reranking approaches (Collins, 2000;
Collins and Duffy, 2002; Shen et al, 2003a; Shen
et al, 2003b; Shen et al, 2004; Collins and Koo,
2005; Kudo et al, 2005; Dinarelli et al, 2009b), few
hypotheses are generated with the baseline model,
ranked by the model probability. These are then
used for the reranking model. An interesting strat-
egy to improve reranking performance is the selec-
tion of the best set of hypotheses to be reranked.
In this work we propose a semantic inconsistency
metric (SIM) based on the attribute-value extraction
phase that allows to select better n-best hypotheses.
We combine the scores provided by the rule based
approach and the CRF approach for AVE, comput-
ing a confidence measure.
The rule-based approach for AVE is defined by
a set of rules that map concepts and their realiz-
ing words into the corresponding value. The rules
are extracted from the training data, thus they are
defined to extract correct values from well formed
phrases annotated with correct concepts. This means
that when the corresponding words are annotated
with a wrong concept, the extracted value will prob-
ably be wrong. We use this property to compute a
semantic inconsistency value for hypotheses, which
in turn allows to select hypotheses with higher prob-
abilities to be correct.
We show the application of SIM using the same
example of Section 2. For space issues we ab-
breviate command-task with com-task, object-bd
with obj-bd and localization-city with loc-city. We
also suppose to have already removedNull concepts.
From the same sentence, the three first hypotheses
that may be generated by the baseline model are:
1. obj-bd{I would like to book} obj-bd{an hotel room} loc-
city{in London}
2. com-task{I would like to book} obj-bd{an hotel room} loc-
city{in London}
3. com-task{I would like to book} obj-bd{an hotel} obj-
bd{room} loc-city{in London}
Two of these annotations show typical errors of an
SLU model:
(i) wrong concepts annotation: in the first hypothe-
sis the phrase ?I would like to book? is erroneously
annotated as obj-bd;
(ii) wrong concept segmentation: in the third hy-
pothesis the phrase ?an hotel room? is split in two
concepts.
If we apply the AVE module to these hypotheses
the result is:
1. obj-bd[] obj-bd[hotel] loc-city[london]
2. cmd-task[reservation] obj-bd[hotel] loc-city[london]
3. cmd-task[reservation] obj-bd[hotel] obj-bd[] loc-city[london]
As we can see the first concept obj-bd in the first
hypothesis has an empty value since it was incor-
rectly annotated and, therefore, it is not supported
1109
MEDIA training dev test
# sentences 12,908 1,259 3,005
words concepts words concepts words concepts
# tokens 94,466 43,078 10,849 4,705 25,606 11,383
# vocabulary 2,210 99 838 66 1,276 78
# singletons 798 16 338 4 494 10
# OOV rate [%] ? ? 1.33 0.02 1.39 0.04
Table 1: Statistics of the MEDIA training and evaluation sets used for all experiments.
by words from which the AVE module can extract
a correct value. In this case, the output of AVE is
empty. In the same way, in the third hypothesis, the
AVE module cannot extract a correct value from the
phrase ?room? since it doesn?t contain any keyword
for a obj-bd concept.
For each hypothesis, our SIM simply counts the
number of wrong (or empty) values. In the example
above, we have 1, 0 and 1 for the three hypothe-
sis, respectively. Accordingly, the most accurate hy-
pothesis under SIM is the second, which is also the
correct one.
In order to combine the SIM score computed by
the rule-based AVE module with the score provided
by the CRF AVE model, we consider per-concept
scores from both approaches. In particular, we for-
malize the definition of the SIM metric above on a
concept ci as SIM(ci, w1,...,mi ). The value of SIM
is simply 0 if the rule-based AVE module can extract
a value from the surface form w1,...,mi realizing the
concept ci. 1 otherwise. For each concept in a hy-
pothesis, we compute its semantic consistency s(ci)
as
s(ci) =
P (vi|ci, w1,...,mi )
SIM(ci, w1,...,mi ) + 1
(5)
where P (vi|ci, w1,...,mi ) is the conditional prob-
ability output by the CRF model for the value vi,
given the concept ci and its realizing surfacew1,...,mi .
Equation 5 means that the CRF score provided for a
given value is halved if SIM returns 1, i.e. if the
AVE module cannot extract any value. Otherwise
the score output by the CRF AVE model is kept
unchanged. The semantic inconsistency metric of
an hypothesis Hk containing the concept sequence
CN1 = c1, ..., cN is then defined as
S(Hk) =
N?
i=1
s(ci) (6)
Using S(Hk) as semantic inconsistency metric,
we generate a huge number of hypotheses with the
baseline model and we select only the top n-best. We
use these hypotheses in the discriminative reranking
model, instead of the original n-best generated by
the CRF model. For simplicity, in general context
we denote S(Hk) as SIM.
4.2 Wrong Rerank Rejection
After the reranking model is applied, the first hy-
pothesis is selected as final result. This choice as-
sumes that the new hypothesis is more accurate than
the one provided by the baseline model. In gen-
eral this assumption is not true. Indeed, a reranking
model must be carefully tuned in order to correctly
rerank wrong first best hypotheses but keeping the
original baseline best for correct hypotheses. When
the baseline model is relatively accurate, the latter
case occurs in most of the cases. In this situation it
becomes hard to train an accurate reranking model.
Our idea to overcome this problem is to apply the
reranking model and then post-process results to de-
tect when the original best hypothesis is actually bet-
ter than the reranked best.
For this purpose we propose a simple strategy
based on the scores computed by the two models in-
volved in reranking: CRF for the baseline and SVM
with PTK for reranking.
Let Hcrf and HRR be the best hypothesis of the
CRF and reranking (RR) models, respectively. Let
Scrf (Hcrf ) and Scrf (HRR) be the scores of the
CRF model for Hcrf and HRR. In the same way,
let SRR(Hcrf ) and SRR(HRR) be the scores of the
reranking model on the same hypotheses. We define
the confidence margin of the CRF model the quan-
tity: Mcrf = Scrf (Hcrf )? Scrf (HRR).
In the same way we define the confidence mar-
gin of the RR model: MRR = SRR(Hcrf ) ?
SRR(HRR).
We compute two thresholds Tcrf and TRR for the
1110
Average score Feature type
0.0528186 Pref2
0.044189 CATEGORY-2
0.0355579 CATEGORY
0.0354006 Pref3-2
0.0338949 Pref4-2
0.0332647 Suff3-2
0.0314831 Suff2
0.030613 Suff4-2
... ...
0.0165602 Suff1
0.000579602 Pref1
Table 2: Ranks of average score given by the CRF model to feature
types
two margins with respect to error rate minimization
(with a ?line search? algorithm).
We select the final best interpretation hypothesis
for a given sentence with the decision function:
BestHypothesis =
{ HRR if Mcrf ? Tcrf and MRR ? TRR
Hcrf otherwise.
Since this strategy allows to recover from rerank-
ing mistakes, we call it Wrong Rerank Rejection
(WRR).
5 Experiments
The data used in our experiments are taken from
the French MEDIA corpus (Bonneau-Maynard et
al., 2006). The corpus is made of 1.250 Human-
Machine dialogs acquired with a Wizard-of-Oz ap-
proach in the domain of informtation and reservation
of French hotels. The data are split into training, de-
velopment and test set. Statistics of the corpus are
presented in table 1.
For our CRFmodels, both Automatic Concept An-
notation and Attribute Value Extraction SLU phases,
we used wapiti2 (Lavergne et al, 2010). The CRF
model for the first SLU phase integrates a tradi-
tional set of features like word prefixes and suffixes
(of length up to 5), plus some Yes/No features like
?Does the word start with capital letter ??, ?Does
the word contain non alphanumeric characters ??,
?Is the word preceded by non alphanumeric char-
acteris ?? etc. The CRF model for AVE integrates
only words, prefixes and suffixes (length 3 and 4)
concatenated with concepts. Since in this case la-
bels are attribute values, which are a huge set with
2available at http://wapiti.limsi.fr
MEDIA Text Input DEV TEST
Model Attr Attr+Val Attr Attr+Val
CRF 12.1% 14.8% 11.5% 13.8%
CRF+RR 12.0% 14.6% 11.5% 13.7%
CRF+RRSIM 11.7% 13.9% 11.3% 13.4%
CRF+RRWRR 11.2% 13.4% 11.3% 13.0%
Table 3: Results of baseline CRF model and reranking models on
MEDIA text input
respect to concepts (7?00 VS 99), using a lot of fea-
tures would make model training problematic. De-
spite the reduced set of features, training error rate
at both token and sentence level is under 1%. We
didn?t carry out optimization for parameters ?1 and
?2 of the elastic net (see section 3.1), default values
lead in most cases to very accurate models.
Reranking models based on SVM and PTK have
been trained with ?SVM-Light-TK?3. Kernel param-
eters M and SVM parameter C have been optimized
on the development set, as well as thresholds for the
WRR (see section 4.2).
Concerning hypotheses generation, for training
we generate 100 hypotheses, we select the best with
respect to the edit distance and the reference anno-
tation and we keep a total of 10 hypotheses to build
pairs. For classification, with the ?standard? rerank-
ing approach we generate and we keep the 10 best
hypotheses. While using SIM for hypotheses selec-
tion, we generate 1.000 hypotheses and we keep the
10 best with respect to SIM. 1.000 is the best thresh-
old between oracle accuracy and computational cost
for evaluating the hypotheses.
Experiments have been performed on both man-
ual and automatic transcriptions of dialog turns. For
automatic transcriptions the WER of the ASR is
30.3% on development set and 31.4% on test set.
All results are reported in terms of Concept Er-
ror Rate (CER), which is the same as WER, but it is
computed on concept sequences. In all cases we give
results for both attributes only and attributes and val-
ues extraction
5.1 Results
In order to understand feature relevance, in table 2
we report feature types ranked by the average score
given by the CRF model. Each type correspond to
features at any position with respect to the target
3available at http://disi.unitn.it/moschitti/Tree-Kernel.htm
1111
(a) M kernel parameter VS CER (on
attribute-value extraction)
(b) C SVM parameter VS training time (c) C SVM parameter VS CER (on
attribute-value extraction)
Figure 2: Optimization of the PTK M parameter and C parameter of SVM
MEDIA Speech Input DEV TEST
Model Attr Attr+Val Attr Attr+Val
CRF 24.1% 29.1% 23.7% 27.6%
CRF+RR 23.9% 29.1% 23.5% 27.6%
CRF+RRSIM 23.9% 28.3% 23.2% 26.8%
CRF+RRWRR 23.3% 27.5% 22.7% 26.1%
Table 4: Results of baseline CRF model and reranking models on
MEDIA speech input
word, with label unigrams. In contrast observation
unigrams are distinguished from bigrams using suf-
fixes -1 and -2 respectively. Feature types wrd are
words converted to lower case, Wrd are words kept
with original capitalization. Feature types Pren are
word prefixes of length n, Sufn are word suffixes of
length n. CATEGORY features are word categories
(see section 3.3). As we can see from the table,
although feature relevance depends of course from
the task, surprisingly word prefixes of length 2 are
the most meaningful features. As expected, CATE-
GORY features are also very relevant features, since
they provide a strong generalization over words. An-
other expected outcome is the fact that prefixes and
suffixes of length 1 are the least relevant features.
In figure 2(a), 2(b) and 2(c) we show the curves
resulting from optimization of parameters of rerank-
ing models. In particular we optimized the M kernel
parameter (? decay factor, see (Moschitti, 2006) for
details), and the C SVM parameter, i.e. the scale
factor for the soft margin (please refer to (Vapnik,
1998) for SVM details). Figure 2(b) shows the learn-
ing time as a function of the C SVM parameter. This
gives an idea of how long takes training our rerank-
ing models.
In table 3 and 4 we report comparative results
over the baseline CRF model, the baseline rerank-
ing model (CRF+RR) and the reranking models ob-
tained applying the two improvements proposed in
this work (CRF+RRSIM and CRF+RRWRR). As
we can see, the baseline reranking model does not
improve significantly the baseline CRF model. This
outcome is expected since we don?t use any other in-
formation in the reranking model than the semantic
tree shown in figure 1. Previous approaches like for
example (Collins and Duffy, 2002), use the baseline
model score as feature, as that the reranking model
cannot do worst than the baseline model. As we
pointed out in section 4.2, this solution require a fine
tuning of the reranking model, especially when the
baseline model is relatively accurate. In our case,
the CRF model has a Sentence Error Rate of 25.0%
on the MEDIA test set. This means that 75% of
the times the best hypothesis of CRF is correct. In
turn this implies that the reranking model must not
rerank 75% of times and rerank the other 25% of
times, someway contrasting the evidence provided
by the baseline model score. In contrast, using our
WRR strategy, we can tune the reranking model to
maximize reranking effect and recover from rerank-
ing errors applying WRR. As shown in tables 3 and
4, we consistently improve CRF baseline as well
as reranking baseline CRF+RR, especially applying
both SIM and WRR (CRF+RRWRR). Comparing
our results with those reported in (Hahn et al, 2010),
we can see that our model reaches, and even im-
1112
MEDIA Test set OER[%] correct found/present
Model
CRF 9.5 2359/2657
CRF+RR 9.5 2375/2657
CRF+RRSIM 7.5 2381/2758
CRF+RRWRR 7.5 2444/2758
Table 5: Analysis over 10-best hypotheses for CRF baseline and the
reranking models showing the effect of hypotheses selection
MEDIA Text Input DEV TEST
Model Pair Attr+Val Attr+Val
CRF vs. CRF+RR 0.2235 0.4075
CRF vs. CRF+RRSIM 0.0299 0.065
CRF vs. CRF+RRWRR 0.0044 1.9998E-4
CRF+RR vs. CRF+RRSIM 0.002 5.9994E-4
CRF+RR vs. CRF+RRWRR 4.9995E-4 9.999E-5
CRF+RRSIM vs. CRF+RRWRR 0.1355 0.0031
Table 6: Significance tests on results of models described in this
work. The significance test is based on computationally-intensive ran-
domizations as described in (Yeh and Church, 2000).
proves in some cases, state-of-the-art performance.
This is particularly meaningful since best results re-
ported in (Hahn et al, 2010) are obtained combining
6 different SLU models.
In table 5 we report some statistics to show the
effect of SIM on the 10-best hypotheses list. It is
particularly interesting to see that when hypothe-
ses selection is applied, oracle error rate (OER)
drops of 2% points from an already accurate OER
of 9.5%. This is reflected also by the number of ora-
cles present in the 10-best list without applying and
applying SIM. We pass from 2657 without SIM to
2758 applying our hypotheses selection metric.
Finally, in table 6 we report statistical signifi-
cance tests over the models described in this work.
We used the significance test described in (Yeh
and Church, 2000), it is based on computationally-
intensive randomizations of data and tests the null
hypothesis, i.e. the lower the score, the higher the
statistical significance of results difference. Scores
in table 5 reflect results given in terms of CER. We
can see that when the difference between results is
small, this is not statistically significant, when the
score is above 0.05, the difference between the two
corresponding models is not significant. We can thus
conclude that the reranking model we propose, using
hypotheses selection and reranking errors recover,
significantly improves baseline CRFmodel and ?tra-
ditional? reranking models.
6 Discussion
Although the new ideas proposed in this paper are
effective and interesting, an important issue is their
applicability to other tasks and domains. In this re-
spect, it is sufficient to note that our ideas comes
from the multi-stage nature of the task and of the
proposed reranking framework. SLU is performed
in two intertwined steps, since attribute values are
extracted from syntactic chunks annotated with con-
cept in the first step. This allows to use the model for
the second step to validate the output of the first step,
and vice versa, which is the principle of our hypothe-
ses selection metric. There are many other tasks,
in NLP and in other domains, that can be modeled
with multiple steps and thus the same idea of ?val-
idation? of the output of one step with the other?s
model output can be applied. An example is syntac-
tic parsing, where in most cases parsing is performed
upon POS tagging output.
7 Conclusions
In this paper we propose two improvements for
reranking models to be integrated in a reranking
framework for Spoken Language Understanding.
The reranking model is based on a CRF baseline
model and Support Vector Machines with the Par-
tial Tree Kernel for the reraning model. The two
improvements we propose are: i) hypotheses selec-
tion criteria, used before applying reranking to select
better hypotheses amongst those generated by CRF.
ii) a strategy to recover from reranking errors called
Wrong Rerank Rejection.
We presented a full set of comparative results
showing the viability of our approach. We can reach
performances of state-of-the-art models, improving
them in some cases, especially on automatic tran-
scriptions coming from ASR (speech input).
In particular, the effectiveness of hypotheses se-
lection is shown reporting the improvement of the
Oracle Error Rate on the 10-best hypotheses list.
Acknowledgments
This work has been funded by OSEO under the
Quaero program.
1113
References
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003 - Volume 4, pages 142?147,
Morristown, NJ, USA. Association for Computational
Linguistics.
Ellen M. Voorhees. 2001. The trec question answering
track. Nat. Lang. Eng., 7:361?378, December.
X. Carreras and Lluis Marquez. 2005. Introduction to
the conll-2005 shared task: Semantic role labeling.
R. De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,
G. Riccardi, and G. Tur. 2008. Spoken language un-
derstanding: A survey. IEEE Signal Processing Mag-
azine, 25:50?58.
Sylvain Galliano, Guillaume Gravier, and Maura
Chaubard. 2009. The ester 2 evaluation campaign
for the rich transcription of french radio broadcasts.
In Proceedings of the International Conference of
the Speech Communication Assosiation (Interspeech),
Brighton, U.K.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL), page 363370, Ann
Arbor, MI.
Michael Collins and Terry Koo. 2005. Discriminative re-
ranking for natural language parsing. Computational
Linguistic (CL), 31(1):25?70.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML), pages 282?289, Williamstown,
MA, USA, June.
Brigitte Krenn and Christer Samuelsson. 1997. The lin-
guist?s guide to statistics - don?t panic.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics, July.
Stefan Hahn, Patrick Lehnen, Georg Heigold, and Her-
mann Ney. 2009. Optimizing crfs for slu tasks in vari-
ous languages using modified training criteria. In Pro-
ceedings of the International Conference of the Speech
Communication Assosiation (Interspeech), Brighton,
U.K.
Stefan Riezler and Alexander Vasserman. 2010. Incre-
mental feature selection and l1 regularization for re-
laxed maximum-entropy modeling.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the Elastic Net. Journal of the
Royal Statistical Society B, 67:301?320.
Andrew McCallum. 2003. Efficiently inducing features
of conditional random fields. In 19th Conference on
Uncertainty in Artificial Intelligence.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Mark Johnson. 1998. Pcfg models of linguistic tree rep-
resentations. Computational Linguistics, 24:613?632.
Olivier Galibert, Ludovic Quintard, Sophie Rosset, Pierre
Zweigenbaum, Claire Ndellec, Sophie Aubin, Lau-
rent Gillard, Jean-Pierre Raysz, Delphine Pois, Xavier
Tannier, Louise Delger, and Dominique Laurent.
2010. Named and specific entity detection in var-
ied data: The quro named entity baseline evalu-
ation. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Olivier Galibert. 2009. Approches et me?thodologies pour
la re?ponse automatique a` des questions adapte?es un
cadre interactif en domaine ouvert. Ph.D. thesis, Uni-
versit Paris Sud, Orsay.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic
Content Extraction (ACE) Program?Tasks, Data, and
Evaluation. Proceedings of LREC 2004, pages 837?
840.
He?le`ne Bonneau-Maynard, Christelle Ayache, F. Bechet,
A Denis, A Kuhn, Fabrice Lefe`vre, D. Mostefa,
M. Qugnard, S. Rosset, and J. Servan, S. Vilaneau.
2006. Results of the french evalda-media evaluation
campaign for literal understanding. In LREC, pages
2054?2059, Genoa, Italy, May.
Christian Raymond, Frdric Bchet, Renato De Mori, and
Graldine Damnati. 2006. On the use of finite state
transducers for semantic interpretation. Speech Com-
munication, 48(3-4):288?304, March-April.
Christian Raymond and Giuseppe Riccardi. 2007. Gen-
erative and discriminative algorithms for spoken lan-
guage understanding. In Proceedings of the Interna-
tional Conference of the Speech Communication As-
sosiation (Interspeech), pages 1605?1608, Antwerp,
Belgium, August.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons.
T. J. Sejnowski and C. S. Rosenberg. 1987. Parallel net-
works that learn to pronounce English text. Complex
Systems, 1:145?168.
1114
Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-
rice Lefe`vre, Patrick Lehen, Renato De Mori, Alessan-
dro Moschitti, Hermann Ney, and Giuseppe Riccardi.
2010. Comparing stochastic approaches to spoken
language understanding in multiple languages. IEEE
Transactions on Audio, Speech and Language Pro-
cessing (TASLP), 99.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2009b. Re-ranking models based on small
training data for spoken language understanding. In
Conference of Empirical Methods for Natural Lan-
guage Processing, pages 11?18, Singapore, August.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2010. Hypotheses Selection for Reranking
Semantic Annotation. In IEEE Workshop of Spoken
Language Technology (SLT), Berkeley, USA.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of ECML 2006, pages 318?329, Berlin,
Germany.
M. Collins and N. Duffy. 2002. New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Discrete
structures, and the voted perceptron. In Proceedings of
the Association for Computational Linguistics, pages
263?270.
Libin Shen, Anoop Sarkar, and Aravind K. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Proceedings of EMNLP?06.
Herbrich, Ralf and Graepel, Thore and Obermayer,
Klaus. 2000. Large Margin Rank Boundaries for Or-
dinal Regression. In Advances in Large Margin Clas-
sifiers.
Libin Shen, and Aravind K. Joshi. 2003. An SVM Based
Voting Algorithm with Application to Parse Rerank-
ing. In Proceedings of CoNLL 2003.
Libin Shen, Anoop Sarkar, and Franz J. Och. 2004. Dis-
criminative reranking for machine translation. In HLT-
NAACL, pages 177?184.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008a.
System combination for spoken language understand-
ing. In Proceedings of the International Conference of
the Speech Communication Assosiation (Interspeech),
pages 236?239, Brisbane, Australia.
J. G. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recogniser output voting er-
ror reduction (ROVER). In Proceedings 1997 IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding (ASRU), pages 347?352, Santa Barbara,
CA, December.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In ICML, pages 175?182.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, pages 625?632.
MIT Press.
Rush, Alexander M. and Sontag, David and Collins,
Michael and Jaakkola, Tommi. 2010. On dual decom-
position and linear programming relaxations for nat-
ural language processing. In Empirical Methods for
Natural Language Processing (EMNLP). Cambridge,
Massachusetts, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL?07, Prague, Czech
Republic.
Alexander Yeh and Kelmeth Church. 2000. More accu-
rate tests for the statistical significance of result differ-
ences.
1115
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 174?184,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Tree Representations in Probabilistic Models for Extended Named
Entities Detection
Marco Dinarelli
LIMSI-CNRS
Orsay, France
marcod@limsi.fr
Sophie Rosset
LIMSI-CNRS
Orsay, France
rosset@limsi.fr
Abstract
In this paper we deal with Named En-
tity Recognition (NER) on transcriptions of
French broadcast data. Two aspects make
the task more difficult with respect to previ-
ous NER tasks: i) named entities annotated
used in this work have a tree structure, thus
the task cannot be tackled as a sequence la-
belling task; ii) the data used are more noisy
than data used for previous NER tasks. We
approach the task in two steps, involving
Conditional Random Fields and Probabilis-
tic Context-Free Grammars, integrated in a
single parsing algorithm. We analyse the
effect of using several tree representations.
Our system outperforms the best system of
the evaluation campaign by a significant
margin.
1 Introduction
Named Entity Recognition is a traditinal task of
the Natural Language Processing domain. The
task aims at mapping words in a text into seman-
tic classes, such like persons, organizations or lo-
calizations. While at first the NER task was quite
simple, involving a limited number of classes (Gr-
ishman and Sundheim, 1996), along the years
the task complexity increased as more complex
class taxonomies were defined (Sekine and No-
bata, 2004). The interest in the task is related to
its use in complex frameworks for (semantic) con-
tent extraction, such like Relation Extraction ap-
plications (Doddington et al 2004).
This work presents research on a Named Entity
Recognition task defined with a new set of named
entities. The characteristic of such set is in that
named entities have a tree structure. As conce-
quence the task cannot be tackled as a sequence
labelling approach. Additionally, the use of noisy
data like transcriptions of French broadcast data,
makes the task very challenging for traditional
NLP solutions. To deal with such problems, we
adopt a two-steps approach, the first being real-
ized with Conditional Random Fields (CRF) (Laf-
ferty et al 2001), the second with a Probabilistic
Context-Free Grammar (PCFG) (Johnson, 1998).
The motivations behind that are:
? Since the named entities have a tree struc-
ture, it is reasonable to use a solution com-
ing from syntactic parsing. However pre-
liminary experiments using such approaches
gave poor results.
? Despite the tree-structure of the entities,
trees are not as complex as syntactic trees,
thus, before designing an ad-hoc solution for
the task, which require a remarkable effort
and yet it doesn?t guarantee better perfor-
mances, we designed a solution providing
good results and which required a limited de-
velopment effort.
? Conditional Random Fields are models ro-
bust to noisy data, like automatic transcrip-
tions of ASR systems (Hahn et al 2010),
thus it is the best choice to deal with tran-
scriptions of broadcast data. Once words
have been annotated with basic entity con-
stituents, the tree structure of named entities
is simple enough to be reconstructed with
relatively simple model like PCFG (Johnson,
1998).
The two models are integrated in a single pars-
ing algorithm. We analyze the effect of the use of
174
Zahra
name.first
Abouch
name.last
pers.ind
Zahrnme .n faisnthn Anhb
omh.
mtuomnh
.nAahcA
atlpu.A
Figure 1: Examples of structured named entities annotated on the
data used in this work
several tree representations, which result in differ-
ent parsing models with different performances.
We provide a detailed evaluation of our mod-
els. Results can be compared with those obtained
in the evaluation campaign where the same data
were used. Our system outperforms the best sys-
tem of the evaluation campaign by a significant
margin.
The rest of the paper is structured as follows: in
the next section we introduce the extended named
entities used in this work, in section 3 we describe
our two-steps algorithm for parsing entity trees,
in section 4 we detail the second step of our ap-
proach based on syntactic parsing approaches, in
particular we describe the different tree represen-
tations used in this work to encode entity trees
in parsing models. In section 6 we describe and
comment experiments, and finally, in section 7,
we draw some conclusions.
2 Extended Named Entities
The most important aspect of the NER task we
investigated is provided by the tree structure of
named entities. Examples of such entities are
given in figure 1 and 2, where words have been re-
move for readability issues and are: (?90 persons
are still present at Atambua. It?s there that 3 employ-
ees of the High Conseil of United Nations for refugees
have been killed yesterday morning?):
90 personnes toujours pre?sentes a`
Atambua c? est la` qu? hier matin ont
e?te? tue?s 3 employe?s du haut commis-
sariat des Nations unies aux re?fugie?s ,
le HCR
Words realizing entities in figure 2 are in bold,
and they correspond to the tree leaves in the
picture. As we see in the figures, entities
can have complex structures. Beyond the use
of subtypes, like individual in person (to give
pers.ind), or administrative in organization
(to give org.adm), entities with more specific con-
tent can be constituents of more general enti-
ties to form tree structures, like name.first and
Zah rnme.f
airstf
hr.AabiAfrot taie f uiecirbuluep
fuieAbafeApeh
Zah dutb taie
rpAabi
lst.A.rhh
rnme.f
airstf

Figure 2: An example of named entity tree corresponding to en-
tities of a whole sentence. Tree leaves, corresponding to sentence
words have been removed to keep readability
Quaero training dev
# sentences 43,251 112
words entities words entities
# tokens 1,251,432 245,880 2,659 570
# vocabulary 39,631 134 891 30
# components ? 133662 ? 971
# components dict. ? 28 ? 18
# OOV rate [%] ? ? 17.15 0
Table 1: Statistics on the training and development sets of the
Quaero corpus
name.last for pers.ind or val (for value) and ob-
ject for amount.
These named entities have been annotated on
transcriptions of French broadcast news coming
from several radio channels. The transcriptions
constitute a corpus that has been split into train-
ing, development and evaluation sets.The evalu-
ation set, in particular, is composed of two set
of data, Broadcast News (BN in the table) and
Broadcast Conversations (BC in the table). The
evaluation of the models presented in this work
is performed on the merge of the two data types.
Some statistics of the corpus are reported in ta-
ble 1 and 2. This set of named entities has been
defined in order to provide more fine semantic in-
formation for entities found in the data, e.g. a
person is better specified by first and last name,
and is fully described in (Grouin, 2011) . In or-
der to avoid confusion, entities that can be associ-
ated directly to words, like name.first, name.last,
val and object, are called entity constituents, com-
ponents or entity pre-terminals (as they are pre-
terminals nodes in the trees). The other entities,
like pers.ind or amount, are called entities or non-
terminal entities, depending on the context.
3 Models Cascade for Extended Named
Entities
Since the task of Named Entity Recognition pre-
sented here cannot be modeled as sequence la-
belling and, as mentioned previously, an approach
175
Quaero test BN test BC
# sentences 1704 3933
words entities words entities
# tokens 32945 2762 69414 2769
# vocabulary 28 28
# components ? 4128 ? 4017
# components dict. ? 21 ? 20
# OOV rate [%] 3.63 0 3.84 0
Table 2: Statistics on the test set of the Quaero corpus, divided in
Broadcast News (BN) and Broadcast Conversations (BC)
Figure 3: Processing schema of the two-steps approach proposed
in this work: CRF plus PCFG
coming from syntactic parsing to perform named
entity annotation in ?one-shot? is not robust on
the data used in this work, we adopt a two-steps.
The first is designed to be robust to noisy data and
is used to annotate entity components, while the
second is used to parse complete entity trees and
is based on a relatively simple model. Since we
are dealing with noisy data, the hardest part of the
task is indeed to annotate components on words.
On the other hand, since entity trees are relatively
simple, at least much simpler than syntactic trees,
once entity components have been annotated in a
first step, for the second step, a complex model is
not required, which would also make the process-
ing slower. Taking all these issues into account,
the two steps of our system for tree-structured
named entity recognition are performed as fol-
lows:
1. A CRF model (Lafferty et al 2001) is used
to annotate components on words.
2. A PCFG model (Johnson, 1998) is used
to parse complete entity trees upon compo-
nents, i.e. using components annotated by
CRF as starting point.
This processing schema is depicted in figure 3.
Conditional Random Fields are described shortly
in the next subsection. PCFG models, constituting
the main part of this work together with the analy-
sis over tree representations, is described more in
details in the next sections.
3.1 Conditional Random Fields
CRFs are particularly suitable for sequence la-
belling tasks (Lafferty et al 2001). Beyond the
possibility to include a huge number of features
using the same framework as Maximum Entropy
models (Berger et al 1996), CRF models en-
code global conditional probabilities normalized
at sentence level.
Given a sequence of N words WN1 =
w1, ..., wN and its corresponding components se-
quence EN1 = e1, ..., eN , CRF trains the condi-
tional probabilities
P (EN1 |W
N
1 ) =
1
Z
NY
n=1
exp
 
MX
m=1
?m ? hm(en?1, en, w
n+2
n?2)
!
(1)
where ?m are the training parameters.
hm(en?1, en, w
n+2
n?2) are the feature functions
capturing dependencies of entities and words. Z
is the partition function:
Z =
X
e?N1
NY
n=1
H(e?n?1, e?n, w
n+2
n?2) (2)
which ensures that probabilities sum up to one.
e?n?1 and e?n are components for previous and cur-
rent words, H(e?n?1, e?n, w
n+2
n?2) is an abbreviation
for
?M
m=1 ?m ? hm(en?1, en, w
n+2
n?2), i.e. the set
of active feature functions at current position in
the sequence.
In the last few years different CRF implemen-
tations have been realized. The implementation
we refer in this work is the one described in
(Lavergne et al 2010), which optimize the fol-
lowing objective function:
?log(P (EN1 |W
N
1 )) + ?1???1 +
?2
2
???22 (3)
???1 and ???22 are the l1 and l2 regulariz-
ers (Riezler and Vasserman, 2004), and together
in a linear combination implement the elastic net
regularizer (Zou and Hastie, 2005). As mentioned
in (Lavergne et al 2010), this kind of regulariz-
ers are very effective for feature selection at train-
ing time, which is a very good point when dealing
with noisy data and big set of features.
176
4 Models for Parsing Trees
The models used in this work for parsing en-
tity trees refer to the models described in (John-
son, 1998), in (Charniak, 1997; Caraballo and
Charniak, 1997) and (Charniak et al 1998), and
which constitutes the basis of the maximum en-
tropy model for parsing described in (Charniak,
2000). A similar lexicalized model has been pro-
posed also by Collins (Collins, 1997). All these
models are based on a PCFG trained from data
and used in a chart parsing algorithm to find the
best parse for the given input. The PCFG model
of (Johnson, 1998) is made of rules of the form:
? Xi ? XjXk
? Xi ? w
where X are non-terminal entities and w are
terminal symbols (words in our case).1 The prob-
ability associated to these rules are:
pi?j,k =
P (Xi ? Xj , Xk)
P (Xi)
(4)
pi?w =
P (Xi ? w)
P (Xi)
(5)
The models described in (Charniak, 1997;
Caraballo and Charniak, 1997) encode probabil-
ities involving more information, such as head
words. In order to have a PCFG model made of
rules with their associated probabilities, we ex-
tract rules from the entity trees of our corpus. This
processing is straightforward, for example from
the tree depicted in figure 2, the following rules
are extracted:
S? amount loc.adm.town time.dat.rel amount
amount? val object
time.date.rel? name time-modifier
object? func.coll
func.coll? kind org.adm
org.adm? name
Using counts of these rules we then compute
maximum likelihood probabilities of the Right
Hand Side (RHS) of the rule given its Left Hand
Side (LHS). Also binarization of rules, applied to
1These rules are actually in Chomsky Normal Form, i.e.
unary or binary rules only. A PCFG, in general, can have any
rule, however, the algorithm we are discussing convert the
PCFG rules into Chomsky Normal Form, thus for simplicity
we provide directly such formulation.
Figure 4: Baseline tree representations used in the PCFG parsing
model
Figure 5: Filler-parent tree representations used in the PCFG pars-
ing model
have all rules in the form of 4 and 5, is straight-
forward and can be done with simple algorithms
not discussed here.
4.1 Tree Representations for Extended
Named Entities
As discussed in (Johnson, 1998), an important
point for a parsing algorithm is the representation
of trees being parsed. Changing the tree represen-
tation can change significantly the performances
of the parser. Since there is a large difference be-
tween entity trees used in this work and syntac-
tic trees, from both meaning and structure point
of view, it is worth performing an analysis with
the aim of finding the most suitable representa-
tion for our task. In order to perform this analy-
sis, we start from a named entity annotated on the
words de notre president , M. Nicolas Sarkozy(of
our president, Mr. Nicolas Sarkozy). The corre-
sponding named entity is shown in figure 4. As
decided in the annotation guidelines, fillers can be
part of a named entity. This can happen for com-
plex named entities involving several words. The
representation shown in figure 4 is the default rep-
resentation and will be referred to as baseline. A
problem created by this representation is the fact
that fillers are present also outside entities. Fillers
of named entities should be, in principle, distin-
guished from any other filler, since they may be
informative to discriminate entities.
Following this intuition, we designed two dif-
ferent representations where entity fillers are con-
177
Figure 6: Parent-context tree representations used in the PCFG
parsing model
Figure 7: Parent-node tree representations used in the PCFG pars-
ing model
textualized so that to be distinguished from the
other fillers. In the first representation we give to
the filler the same label of the parent node, while
in the second representation we use a concatena-
tion of the filler and the label of the parent node.
These two representations are shown in figure 5
and 6, respectively. The first one will be referred
to as filler-parent, while the second will be re-
ferred as parent-context. A problem that may be
introduced by the first representation is that some
entities that originally were used only for non-
terminal entities will appear also as components,
i.e. entities annotated on words. This may intro-
duce some ambiguity.
Another possible contextualization can be to
annotate each node with the label of the parent
node. This representation is shown in figure 7
and will be referred to as parent-node. Intuitively,
this representation is effective since entities an-
notated directly on words provide also the en-
tity of the parent node. However this representa-
tion increases drastically the number of entities,
in particular the number of components, which
in our case are the set of labels to be learned by
the CRF model. For the same reason this repre-
sentation produces more rigid models, since label
sequences vary widely and thus is not likely to
match sequences not seen in the training data.
Finally, another interesting tree representation
is a variation of the parent-node tree, where en-
tity fillers are only distinguished from fillers not
in an entity, using the label ne-filler, but they are
not contextualized with entity information. This
representation is shown in figure 8 and it will be
Figure 8: Parent-node-filler tree representations used in the PCFG
parsing model
referred to as parent-node-filler. This representa-
tion is a good trade-off between contextual infor-
mation and rigidity, by still representing entities
as concatenation of labels, while using a common
special label for entity fillers. This allows to keep
lower the number of entities annotated on words,
i.e. components.
Using different tree representations affects both
the structure and the performance of the parsing
model. The structure is described in the next sec-
tion, the performance in the evaluation section.
4.2 Structure of the Model
Lexicalized models for syntactic parsing de-
scribed in (Charniak, 2000; Charniak et al 1998)
and (Collins, 1997), integrate more information
than what is used in equations 4 and 5. Consider-
ing a particular node in the entity tree, not includ-
ing terminals, the information used is:
? s: the head word of the node, i.e. the most
important word of the chunk covered by the
current node
? h: the head word of the parent node
? t: the entity tag of the current node
? l: the entity tag of the parent node
The head word of the parent node is defined
percolating head words from children nodes to
parent nodes, giving the priority to verbs. They
can be found using automatic approaches based
on words and entity tag co-occurrence or mutual
information. Using this information, the model
described in (Charniak et al 1998) is P (s|h, t, l).
This model being conditioned on several pieces
of information, it can be affected by data sparsity
problems. Thus, the model is actually approxi-
mated as an interpolation of probabilities:
P (s|h, t, l) =
?1P (s|h, t, l) + ?2P (s|ch, t, l)+
?3P (s|t, l) + ?4P (s|t) (6)
178
where ?i, i = 1, ..., 4, are parameters of the
model to be tuned, and ch is the cluster of head
words for a given entity tag t. With such model,
when not all pieces of information are available to
estimate reliably the probability with more con-
ditioning, the model can still provide a proba-
bility with terms conditioned with less informa-
tion. The use of head words and their percola-
tion over the tree is called lexicalization. The
goal of tree lexicalization is to add lexical infor-
mation all over the tree. This way the probabil-
ity of all rules can be conditioned also on lexi-
cal information, allowing to define the probabili-
ties P (s|h, t, l) and P (s|ch, t, l). Tree lexicaliza-
tion reflects the characteristics of syntactic pars-
ing, for which the models described in (Charniak,
2000; Charniak et al 1998) and (Collins, 1997)
were defined. Head words are very informative
since they constitute keywords instantiating la-
bels, regardless if they are syntactic constituents
or named entities. However, for named entity
recognition it doesn?t make sense to give prior-
ity to verbs when percolating head words over the
tree, even more because head words of named en-
tities are most of the time nouns. Moreover, it
doesn?t make sense to give priority to the head
word of a particular entity with respect to the oth-
ers, all entities in a sentence have the same im-
portance. Intuitively, lexicalization of entity trees
is not straightforward as lexicalization of syntac-
tic trees. At the same time, using not lexicalized
trees doesn?t make sense with models like 6, since
all the terms involve lexical information. Instead,
we can use the model of (Johnson, 1998), which
define the probability of a tree ? as:
P (?) =
Y
X??
P (X ? ?)C? (X??) (7)
here the RHS of rules has been generalized with
?, representing RHS of both unary and binary
rules 4 and 5. C? (X ? ?) is the number of times
the rule X ? ? appears in the tree ? . The model
7 is instantiated when using tree representations
shown in Fig. 4, 5 and 6. When using representa-
tions given in Fig. 7 and 8, the model is:
P (? |l) (8)
where l is the entity label of the parent node.
Although non-lexicalized models like 7 and 8
have shown less effective for syntactic parsing
than their lexicalized couter-parts, there are evi-
dences showing that they can be effective in our
task. With reference to figure 4, considering the
entity pers.ind instantiated by Nicolas Sarkozy,
our algorithm detects first name.first for Nicolas
and name.last for Sarkozy using the CRF model.
As mentioned earlier, once the CRF model has de-
tected components, since entity trees have not a
complex structure with respect to syntactic trees,
even a simple model like the one in equation 7
or 8 is effective for entity tree parsing. For ex-
ample, once name.first and name.last have been
detected by CRF, pers.ind is the only entity hav-
ing name.first and name.last as children. Am-
biguities, like for example for kind or qualifier,
which can appear in many entities, can affect the
model 7, but they are overcome by the model 8,
taking the entity tag of the parent node into ac-
count. Moreover, the use of CRF allows to in-
clude in the model much more features than the
lexicalized model in equation 6. Using features
like word prefixes (P), suffixes (S), capitalization
(C), morpho-syntactic features (MS) and other
features indicated as F2, the CRF model encodes
the conditional probability:
P (t|w,P, S, C,MS, F ) (9)
where w is an input word and t is the corre-
sponding component.
The probability of the CRF model, used in the
first step to tag input words with components,
is combined with the probability of the PCFG
model, used to parse entity trees starting from
components. Thus the structure of our model is:
P (t|w,P, S, C,MS, F ) ? P (?) (10)
or
P (t|w,P, S, C,MS, F ) ? P (? |l) (11)
depending if we are using the tree representa-
tion given in figure 4, 5 and 6 or in figure 7 and 8,
respectively. A scale factor could be used to com-
bine the two scores, but this is optional as CRFs
can provide normalized posterior probabilities.
2The set of features used in the CRF model will be de-
scribed in more details in the evaluation section.
179
5 Related Work
While the models used for named entity detection
and the set of named entities defined along the
years have been discussed in the introduction and
in section 2, since CRFs and models for parsing
constitute the main issue in our work, we discuss
some important models here.
Beyond the models for parsing discussed in
section 4, together with motivations for using or
not in our work, another important model for syn-
tactic parsing has been proposed in (Ratnaparkhi,
1999). Such model is made of four Maximum
Entropy models used in cascade for parsing at
different stages. Also this model makes use of
head words, like those described in section 4, thus
the same considerations hold, moreover it seems
quite complex for real applications, as it involves
the use of four different models together. The
models described in (Johnson, 1998), (Charniak,
1997; Caraballo and Charniak, 1997), (Charniak
et al 1998), (Charniak, 2000), (Collins, 1997)
and (Ratnaparkhi, 1999), constitute the main in-
dividual models proposed for constituent-based
syntactic parsing. Later other approaches based
on models combination have been proposed, like
e.g. the reranking approach described in (Collins
and Koo, 2005), among many, and also evolutions
or improvements of these models.
More recently, approaches based on log-linear
models have been proposed (Clark and Curran,
2007; Finkel et al 2008) for parsing, called also
?Tree CRF?, using also different training criteria
(Auli and Lopez, 2011). Using such models in our
work has basically two problems: one related to
scaling issues, since our data present a large num-
ber of labels, which makes CRF training problem-
atic, even more when using ?Tree CRF?; another
problem is related to the difference between syn-
tactic parsing and named entity detection tasks,
as mentioned in sub-section 4.2. Adapting ?Tree
CRF? to our task is thus a quite complex work, it
constitutes an entire work by itself, we leave it as
feature work.
Concerning linear-chain CRF models, the
one we use is a state-of-the-art implementation
(Lavergne et al 2010), as it implements the
most effective optimization algorithms as well as
state-of-the-art regularizers (see sub-section 3.1).
Some improvement of linear-chain CRF have
been proposed, trying to integrate higher order
target-side features (Tang et al 2006). An inte-
gration of the same kind of features has been tried
also in the model used in this work, without giv-
ing significant improvements, but making model
training much harder. Thus, this direction has not
been further investigated.
6 Evaluation
In this section we describe experiments performed
to evaluate our models. We first describe the set-
tings used for the two models involved in the en-
tity tree parsing, and then describe and comment
the results obtained on the test corpus.
6.1 Settings
The CRF implementation used in this work is de-
scribed in (Lavergne et al 2010), named wapiti.3
We didn?t optimize parameters ?1 and ?2 of the
elastic net (see section 3.1), although this im-
proves significantly the performances and leads
to more compact models, default values lead in
most cases to very accurate models. We used a
wide set of features in CRF models, in a window
of [?2,+2] around the target word:
? A set of standard features like word prefixes
and suffixes of length from 1 to 6, plus some
Yes/No features like Does the word start with
capital letter?, etc.
? Morpho-syntactic features extracted from
the output of the tool tagger (Allauzen and
Bonneau-Maynard, 2008)
? Features extracted from the output of the se-
mantic analyzer (Rosset et al (2009)) pro-
vided by the tool WMatch (Galibert, 2009).
This analysis morpho-syntactic information as
well as semantic information at the same level
of named entities. Using two different sets of
morpho-syntactic features results in more effec-
tive models, as they create a kind of agreement
for a given word in case of match. Concerning
the PCFG model, grammars, tree binarization and
the different tree representations are created with
our own scripts, while entity tree parsing is per-
formed with the chart parsing algorithm described
in (Johnson, 1998).4
3available at http://wapiti.limsi.fr
4available at http://web.science.mq.edu.au/
?mjohnson/Software.htm
180
CRF PCFG
Model # features # labels # rules
baseline 3,041,797 55 29,611
filler-parent 3,637,990 112 29,611
parent-context 3,605,019 120 29,611
parent-node 3,718,089 441 31,110
parent-node-filler 3,723,964 378 31,110
Table 3: Statistics showing the characteristics of the different
models used in this work
6.2 Evaluation Metrics
All results are expressed in terms of Slot Error
Rate (SER) (Makhoul et al 1999) which has a
similar definition of word error rate for ASR sys-
tems, with the difference that substitution errors
are split in three types: i) correct entity type with
wrong segmentation; ii) wrong entity type with
correct segmentation; iii) wrong entity type with
wrong segmentation; here, i) and ii) are given half
points, while iii), as well as insertion and deletion
errors, are given full points. Moreover, results are
given using the well known F1 measure, defined
as a function of precision and recall.
6.3 Results
In this section we provide evaluations of the mod-
els described in this work, based on combination
of CRF and PCFG and using different tree repre-
sentations of named entity trees.
6.3.1 Model Statistics
As a first evaluation, we describe some statis-
tics computed from the CRF and PCFG models
using the tree representations. Such statistics pro-
vide interesting clues of how difficult is learning
the task and which performance we can expect
from the model. Statistics for this evaluation are
presented in table 3. Rows corresponds to the dif-
ferent tree representations described in this work,
while in the columns we show the number of fea-
tures and labels for the CRF models (# features
and # labels), and the number of rules for PCFG
models (# rules).
As we can see from the table, the number
of rules is the same for the tree representations
baseline, filler-parent and parent-context, and
for the representations parent-node and parent-
node-filler. This is the consequence of the con-
textualization applied by the latter representa-
tions, i.e. parent-node and parent-node-filler
create several different labels depending from
the context, thus the corresponding grammar
DEV TEST
Model SER F1 SER F1
baseline 20.0% 73.4% 14.2% 79.4%
filler-parent 16.2% 77.8% 12.5% 81.2%
parent-context 15.2% 78.6% 11.9% 81.4%
parent-node 6.6% 96.7% 5.9% 96.7%
parent-node-filler 6.8% 95.9% 5.7% 96.8%
Table 4: Results computed from oracle predictions obtained with
the different models presented in this work
DEV TEST
Model SER F1 SER F1
baseline 33.5% 72.5% 33.4% 72.8%
filler-parent 31.3% 74.4% 33.4% 72.7%
parent-context 30.9% 74.6% 33.3% 72.8%
parent-node 31.2% 77.8% 31.4% 79.5%
parent-node-filler 28.7% 78.9% 30.2% 80.3%
Table 5: Results obtained with our combined algorithm based on
CRF and PCFG
will have more rules. For example, the rule
pers.ind? name.first name.last can
appear as it is or contextualized with func.ind,
like in figure 8. In contrast the other tree repre-
sentations modify only fillers, thus the number of
rules is not affected.
Concerning CRF models, as shown in table 3,
the use of the different tree representations results
in an increasing number of labels to be learned by
CRF. This aspect is quite critical in CRF learn-
ing, as training time is exponential in the number
of labels. Indeed, the most complex models, ob-
tained with parent-node and parent-node-filler
tree representations, took roughly 8 days for train-
ing. Additionally, increasing the number of labels
can create data sparseness problems, however this
problem doesn?t seem to arise in our case since,
apart the baseline model which has quite less fea-
tures, all the others have approximately the same
number of features, meaning that there are actu-
ally enough data to learn the models, regardless
the number of labels.
6.3.2 Evaluations of Tree Representations
In this section we evaluate the models in terms
of the evaluation metrics described in previous
section, Slot Error Rate (SER) and F1 measure.
In order to evaluate PCFG models alone, we
performed entity tree parsing using as input ref-
erence transcriptions, i.e. manual transcriptions
and reference component annotations taken from
development and test sets. This can be consid-
ered a kind of oracle evaluations and provides us
an upper bound of the performance of the PCFG
models. Results for this evaluation are reported in
181
Participant SER
P1 48.9
P2 41.0
parent-context 33.3
parent-node 31.4
parent-node-filler 30.2
Table 6: Results obtained with our combined algorithm based on
CRF and PCFG
table 4. As it can be intuitively expected, adding
more contextualization in the trees results in more
accurate models, the simplest model, baseline,
has the worst oracle performance, filler-parent
and parent-context models, adding similar con-
textualization information, have very similar ora-
cle performances. Same line of reasoning applies
to models parent-node and parent-node-filler,
which also add similar contextualization and have
very similar oracle predictions. These last two
models have also the best absolute oracle perfor-
mances. However, adding more contextualization
in the trees results also in more rigid models, the
fact that models are robust on reference transcrip-
tions and based on reference component annota-
tions, doesn?t imply a proportional robustness on
component sequences generated by CRF models.
This intuition is confirmed from results re-
ported in table 5, where a real evaluation of our
models is reported, using this time CRF out-
put components as input to PCFG models, to
parse entity trees. The results reported in ta-
ble 5 show in particular that models using base-
line, filler-parent and parent-context tree repre-
sentations have similar performances, especially
on test set. Models characterized by parent-node
and parent-node-filler tree representations have
indeed the best performances, although the gain
with respect to the other models is not as much
as it could be expected given the difference in
the oracle performances discussed above. In par-
ticular the best absolute performance is obtained
with the model parent-node-filler. As we men-
tioned in subsection 4.1, this model represents the
best trade-off between rigidity and accuracy using
the same label for all entity fillers, but still distin-
guishing between fillers found in entity structures
and other fillers found in words not instantiating
any entity.
6.3.3 Comparison with Official Results
As a final evaluation of our models, we pro-
vide a comparison of official results obtained at
the 2011 evaluation campaign of extended named
entity recognition (Galibert et al 2011; 2) Re-
sults are reported in table 6, where the other two
participants to the campaign are indicated as P1
and P2. These two participants P1 and P2, used
a system based on CRF, and rules for deep syn-
tactic analysis, respectively. In particular, P2 ob-
tained superior performances in previous evalua-
tion campaign on named entity recognition. The
system we proposed at the evaluation campaign
used a parent-context tree representation. The
results obtained at the evaluation campaign are
in the first three lines of Table 6. We compare
such results with those obtained with the parent-
node and parent-node-filler tree representations,
reported in the last two rows of the same table. As
we can see, the new tree representations described
in this work allow to achieve the best absolute per-
formances.
7 Conclusions
In this paper we have presented a Named Entity
Recognition system dealing with extended named
entities with a tree structure. Given such represen-
tation of named entities, the task cannot be mod-
eled as a sequence labelling approach. We thus
proposed a two-steps system based on CRF and
PCFG. CRF annotate entity components directly
on words, while PCFG apply parsing techniques
to predict the whole entity tree. We motivated
our choice by showing that it is not effective to
apply techniques used widely for syntactic pars-
ing, like for example tree lexicalization. We pre-
sented an analysis of different tree representations
for PCFG, which affect significantly parsing per-
formances.
We provided and discussed a detailed evalua-
tion of all the models obtained by combining CRF
and PCFG with the different tree representation
proposed. Our combined models result in better
performances with respect to other models pro-
posed at the official evaluation campaign, as well
as our previous model used also at the evaluation
campaign.
Acknowledgments
This work has been funded by the project Quaero,
under the program Oseo, French State agency for
innovation.
182
References
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference-6: a brief history.
In Proceedings of the 16th conference on Com-
putational linguistics - Volume 1, pages 466?471,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Satoshi Sekine and Chikashi Nobata. 2004. Defini-
tion, Dictionaries and Tagger for Extended Named
Entity Hierarchy. In Proceedings of LREC.
G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The Automatic Content Extraction (ACE)
Program?Tasks, Data, and Evaluation. Proceedings
of LREC 2004, pages 837?840.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Karn Fort, Olivier Galibert, Ludovic Quintard.
2011. Proposal for an extension or traditional
named entities: From guidelines to evaluation, an
overview. In Proceedings of the Linguistic Annota-
tion Workshop (LAW).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Pro-
ceedings of the Eighteenth International Confer-
ence on Machine Learning (ICML), pages 282?289,
Williamstown, MA, USA, June.
Mark Johnson. 1998. Pcfg models of linguistic
tree representations. Computational Linguistics,
24:613?632.
Stefan Hahn, Marco Dinarelli, Christian Raymond,
Fabrice Lefe`vre, Patrick Lehen, Renato De Mori,
Alessandro Moschitti, Hermann Ney, and Giuseppe
Riccardi. 2010. Comparing stochastic approaches
to spoken language understanding in multiple lan-
guages. IEEE Transactions on Audio, Speech and
Language Processing (TASLP), 99.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. COMPU-
TATIONAL LINGUISTICS, 22:39?71.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Stefan Riezler and Alexander Vasserman. 2004. In-
cremental feature selection and l1 regularization
for relaxed maximum-entropy modeling. In Pro-
ceedings of the International Conference on Em-
pirical Methods for Natural Language Processing
(EMNLP).
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the Elastic Net. Journal of the
Royal Statistical Society B, 67:301?320.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the fourteenth national conference
on artificial intelligence and ninth conference on
Innovative applications of artificial intelligence,
AAAI?97/IAAI?97, pages 598?603. AAAI Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139, San
Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Sharon A. Caraballo and Eugene Charniak. 1997.
New figures of merit for best-first probabilistic chart
parsing. Computational Linguistics, 24:275?298.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, ACL ?98, pages 16?23, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
In Proceedings of the Sixth Workshop on Very Large
Corpora, pages 127?133. Morgan Kaufmann.
Alexandre Allauzen and He?le?ne Bonneau-Maynard.
2008. Training and evaluation of pos taggers on the
french multitag corpus. In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may.
Olivier Galibert. 2009. Approches et me?thodologies
pour la re?ponse automatique a` des questions
adapte?es a` un cadre interactif en domaine ouvert.
Ph.D. thesis, Universite? Paris Sud, Orsay.
Rosset Sophie, Galibert Olivier, Bernard Guillaume,
Bilinski Eric, and Adda Gilles. The LIMSI mul-
tilingual, multitask QAst system. In Proceed-
ings of the 9th Cross-language evaluation forum
conference on Evaluating systems for multilin-
gual and multimodal information access, CLEF?08,
pages 480?487, Berlin, Heidelberg, 2009. Springer-
Verlag.
Azeddine Zidouni, Sophie Rosset, and Herve? Glotin.
2010. Efficient combined approach for named en-
tity recognition in spoken language. In Proceedings
of the International Conference of the Speech Com-
munication Assosiation (Interspeech), Makuhari,
Japan
John Makhoul, Francis Kubala, Richard Schwartz,
and Ralph Weischedel. 1999. Performance mea-
sures for information extraction. In Proceedings of
DARPA Broadcast News Workshop, pages 249?252.
Adwait Ratnaparkhi. 1999. Learning to Parse Natural
Language with Maximum Entropy Models. Journal
of Machine Learning, vol. 34, issue 1-3, pages 151?
175.
183
Michael Collins and Terry Koo. 2005. Discriminative
Re-ranking for Natural Language Parsing. Journal
of Machine Learning, vol. 31, issue 1, pages 25?70.
Clark, Stephen and Curran, James R. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Journal of Computational Lin-
guistics, vol. 33, issue 4, pages 493?552.
Finkel, Jenny R. and Kleeman, Alex and Manning,
Christopher D. 2008. Efficient, Feature-based,
Conditional Random Field Parsing. Proceedings
of the Association for Computational Linguistics,
pages 959?967, Columbus, Ohio.
Michael Auli and Adam Lopez 2011. Training a Log-
Linear Parser with Loss Functions via Softmax-
Margin. Proceedings of Empirical Methods for
Natural Language Processing, pages 333?343, Ed-
inburgh, U.K.
Tang, Jie and Hong, MingCai and Li, Juan-Zi and
Liang, Bangyong. 2006. Tree-Structured Con-
ditional Random Fields for Semantic Annotation.
Proceedgins of the International Semantic Web
Conference, pages 640?653, Edited by Springer.
Olivier Galibert; Sophie Rosset; Cyril Grouin; Pierre
Zweigenbaum; Ludovic Quintard. 2011. Struc-
tured and Extended Named Entity Evaluation in Au-
tomatic Speech Transcriptions. IJCNLP 2011.
Marco Dinarelli, Sophie Rosset. Models Cascade for
Tree-Structured Named Entity Detection IJCNLP
2011.
184
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62?69,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI @ WMT?13
Alexandre Allauzen1,2, Nicolas Pe?cheux1,2, Quoc Khanh Do1,2, Marco Dinarelli2,
Thomas Lavergne1,2, Aure?lien Max1,2, Hai-Son Le3, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Vietnamese Academy of Science and Technology3, Hanoi, Vietnam
lehaison@ioit.ac.vn
Abstract
This paper describes LIMSI?s submis-
sions to the shared WMT?13 translation
task. We report results for French-English,
German-English and Spanish-English in
both directions. Our submissions use
n-code, an open source system based on
bilingual n-grams, and continuous space
models in a post-processing step. The
main novelties of this year?s participation
are the following: our first participation
to the Spanish-English task; experiments
with source pre-ordering; a tighter integra-
tion of continuous space language mod-
els using artificial text generation (for Ger-
man); and the use of different tuning sets
according to the original language of the
text to be translated.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Eighth Workshop on
Statistical Machine Translation. LIMSI partici-
pated in the French-English, German-English and
Spanish-English tasks in both directions. For this
evaluation, we used n-code, an open source in-
house Statistical Machine Translation (SMT) sys-
tem based on bilingual n-grams1, and continuous
space models in a post-processing step, both for
translation and target language modeling.
This paper is organized as follows. Section 2
contains an overview of the baseline systems built
with n-code, including the continuous space mod-
els. As in our previous participations, several
steps of data pre-processing, cleaning and filter-
ing are applied, and their improvement took a non-
negligible part of our work. These steps are sum-
marized in Section 3. The rest of the paper is de-
voted to the novelties of the systems submitted this
1http://ncode.limsi.fr/
year. Section 4 describes the system developed for
our first participation to the Spanish-English trans-
lation task in both directions. To translate from
German into English, the impact of source pre-
ordering is investigated, and experimental results
are reported in Section 5, while for the reverse di-
rection, we explored a text sampling strategy us-
ing a 10-gram SOUL model to allow a tighter in-
tegration of continuous space models during the
translation process (see Section 6). A final section
discusses the main lessons of this study.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o
et al, 2006; Crego and Marin?o, 2006). In this
framework, translation is divided in two steps: a
source reordering step and a (monotonic) transla-
tion step. Source reordering is based on a set of
learned rewrite rules that non-deterministically re-
order the input words. Applying these rules result
in a finite-state graph of possible source reorder-
ings, which is then searched for the best possible
candidate translation.
2.1 Features
Given a source sentence s of I words, the best
translation hypothesis t? is defined as the sequence
of J words that maximizes a linear combination of
feature functions:
t? = argmax
t,a
{ M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature
function hm and a denotes an alignment between
source and target phrases. Among the feature
functions, the peculiar form of the translation
model constitutes one of the main difference be-
tween the n-gram approach and standard phrase-
based systems.
62
In addition to the translation model (TM), four-
teen feature functions are combined: a target-
language model; four lexicon models; six lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two
lexical weights are estimated from the automatic
word alignments. The weight vector ? is learned
using the Minimum Error Rate Training frame-
work (MERT) (Och, 2003) and BLEU (Papineni
et al, 2002) measured on nt09 (newstest2009) as
the optimization criteria.
2.2 Translation Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to repro-
duce the word order modifications introduced dur-
ing the tuple extraction process. Hence, only those
reordering hypotheses are translated and are intro-
duced using a set of reordering rules automatically
learned from the word alignments. Part-of-speech
(POS) information is used to increase the gen-
eralization power of these rules. Hence, rewrite
rules are built using POS, rather than surface word
forms (Crego and Marin?o, 2006).
2.3 SOUL rescoring
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al, 2003; Schwenk
et al, 2006) as a potential means to improve dis-
crete language models (LMs). As for our last year
participation (Le et al, 2012c), we take advantage
of the recent proposal of Le et al (2011). Using
a specific neural network architecture (the Struc-
tured OUtput Layer or SOUL model), it becomes
possible to estimate n-gram models that use large
vocabulary, thereby making the training of large
neural network LMs (NNLMs) feasible both for
target language models and translation models (Le
et al, 2012a). We use the same models as last year,
meaning that the SOUL rescoring was used for all
systems, except for translating into Spanish. See
section 6 and (Le et al, 2012c) for more details.
3 Corpora and data pre-processing
Concerning data pre-processing, we started from
our submissions from last year (Le et al, 2012c)
and mainly upgraded the corpora and the associ-
ated language-dependent pre-processing routines.
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte
et al, 2008). Previous experiments have demon-
strated that better normalization tools provide bet-
ter BLEU scores: all systems are thus built using
the ?true-case? scheme.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to un-
known forms). When translating from German
into English, the German side is thus normalized
using a specific pre-processing scheme (Allauzen
et al, 2010; Durgar El-Kahlout and Yvon, 2010)
which aims at reducing the lexical redundancy by
(i) normalizing the orthography, (ii) neutralizing
most inflections and (iii) splitting complex com-
pounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition,
for German, fine-grained POS labels were also
needed for pre-processing and were obtained us-
ing the RFTagger (Schmid and Laws, 2008).
For Spanish, all the availaible data are tokenized
using FreeLing2 toolkit (Padro? and Stanilovsky,
2012), with default settings and some added rules.
Sentence splitting and morphological analysis are
disabled except for del ? de el and al ? a el.
Moreover, a simple ?true-caser? based on upper-
case word frequency is used, and the specific
Spanish punctuation signs ??? and ??? are removed
and heuristically reintroduced in a post-processing
step. All Spanish texts are POS-tagged also using
Freeling. The EAGLES tag set is however sim-
plified by truncating the category label to the first
two symbols, in order to reduce the sparsity of the
reordering rules estimated by n-code.
For the CommonCrawl corpus, we found that
many sentences are not in the expected language.
For example, in the French side of the French-
English version, most of the first sentences are
in English. Therefore, foreign sentence pairs are
filtered out with a MaxEnt classifier that uses n-
grams of characters as features (n is between 1
and 4). This filter discards approximatively 10%
2http://nlp.lsi.upc.edu/freeling/
63
of the sentence pairs. Moreover, we also observe
that a lot of sentence pairs are not translation of
each other. Therefore, an extra sentence alignment
step is carried out using an in-house implementa-
tion of the tool described in (Moore, 2002). This
last step discards approximately 20% of the cor-
pus. For the Spanish-English task, the same filter-
ing is applied to all the available corpora.
4 System development for the
Spanish-English task
This is our first participation to the Spanish-
English translation task in both directions. This
section provides details about the development of
n-code systems for this language pair.
4.1 Data selection and filtering
The CommonCrawl and UN corpora can be con-
sidered as very noisy and out-of-domain. As de-
scribed in (Allauzen et al, 2011), to select a subset
of parallel sentences, trigram LMs were trained for
both Spanish and English languages on a subset of
the available News data: the Spanish (resp. En-
glish) LM was used to rank the Spanish (resp. En-
glish) side of the corpus, and only those sentences
with perplexity above a given threshold were se-
lected. Finally, the two selected sets were in-
tersected. In the following experiments, the fil-
tered versions of these corpora are used to train
the translation systems unless explicitly stated.
4.2 Spanish language model
To train the language models, we assumed that the
test set would consist in a selection of recent news
texts and all the available monolingual data for
Spanish were used, including the Spanish Giga-
word, Third Edition. A vocabulary is first defined
by including all tokens observed in the News-
Commentary and Europarl corpora. This vocab-
ulary is then expanded with all words that occur
more than 10 times in the recent news texts (LDC-
2007-2011 and news-crawl-2011-2012). This pro-
cedure results in a vocabulary containing 372k
words. Then, the training data are divided into
7 sets based on dates or genres. On each set, a
standard 4-gram LM is estimated from the vocab-
ulary using absolute discounting interpolated with
lower order models (Kneser and Ney, 1995; Chen
and Goodman, 1998). The resulting LMs are then
linearly interpolated using coefficients chosen so
Corpora BLEU
dev nt11 test nt12
es2en N,E 30.2 33.2
N,E,C 30.6 33.7
N,E,U 30.3 33.6
N,E,C,U 30.6 33.7
N,E,C,U (nf) 30.7 33.6
en2es N,E 32.2 33.3
N,E,C,U 32.3 33.6
N,E,C,U (nf) 32.5 33.9
Table 1: BLEU scores achieved with different
sets of parallel corpora. All systems are base-
line n-code with POS factor models. The follow-
ing shorthands are used to denote corpora, : ?N?
stands for News-Commentary, ?E? for Europarl,
?C? for CommonCrawl, ?U? for UN and (nf) for
non filtered corpora.
as to minimise the perplexity evaluated on the de-
velopment set (nt08).
4.3 Experiments
All reported results are averaged on 3 MERT runs.
Table 1 shows the BLEU scores obtained with dif-
ferent corpora setups. We can observe that us-
ing the CommonCrawl corpus improves the per-
formances in both directions, while the impact of
the UN data is less important, especially when
combined with CommonCrawl. The filtering strat-
egy described in Section 4.2 has a slightly posi-
tive impact of +0.1 BLEU point for the Spanish-
to-English direction but yields a 0.2 BLEU point
decrease in the opposite direction.
For the following experiments, all the available
corpora are therefore used: News-Commentary,
Europarl, filtered CommonCrawl and UN. For
each of these corpora, a bilingual n-gram model
is estimated and used by n-code as one individual
model score. An additionnal TM is trained on the
concatenation all these corpora, resulting in a to-
tal of 5 TMs. Moreover, n-code is able to handle
additional ?factored? bilingual models where the
source side words are replaced by the correspond-
ing lemma or even POS tag (Koehn and Hoang,
2007). Table 2 reports the scores obtained with
different settings.
In Table 2, big denotes the use of a wider
context for n-gram TMs (n = 4, 5, 4 instead
of 3, 4, 3 respectively for word-based, POS-based
and lemma-based TMs). Using POS factored
64
Condition BLEU
dev nt11 test nt12
es2en base 30.3 33.5
pos 30.6 33.7
big-pos 30.7 33.7
big-pos-lem 30.7 33.8
en2es base 32.0 33.4
pos 32.3 33.6
big-pos 32.3 33.8
big-pos-pos+ 32.2 33.4
Table 2: BLEU scores for different configuration
of factored translation models. The big prefix de-
notes experiments with the larger context for n-
gram translation models.
models yields a significant BLEU improvement,
as well as using a wider context for n-gram TMs.
Since Spanish is morphologically richer than En-
glish, lemmas are introduced only on the Span-
ish side. An additionnal BLEU improvement is
achieved by adding factored models based on lem-
mas when translating from Spanish to English,
while in the opposite direction it does not seem
to have any clear impact.
For English to Spanish, we also experimented
with a 5-gram target factored model, using the
whole morphosyntactic EAGLES tagset, (pos+ in
Table 2), to add some syntactic information, but
this, in fact, proved harmful.
As several tuning sets were available, experi-
ments were carried out with the concatenation of
nt09 to nt11 as a tuning data set. This yields an im-
provement between 0.1 and 0.3 BLEU point when
testing on nt12 when translating from Spanish to
English.
4.4 Submitted systems
For both directions, the submitted systems are
trained on all the available training data, the cor-
pora CommonCrawl and UN being filtered as de-
scribed previously. A word-based TM and a POS
factored TM are estimated for each training set.
To translate from Spanish to English, the system
is tuned on the concatenation of the nt09 to nt11
datasets with an additionnal 4-gram lemma-based
factored model, while in the opposite direction, we
only use nt11.
dev nt09 test nt11
en2de 15.43 15.35
en-mod2de 15.06 15.00
Table 3: BLEU scores for pre-ordering experi-
ments with a n-code system and the approach pro-
posed by (Neubig et al, 2012)
5 Source pre-ordering for English to
German translation
While distorsion models can efficiently handle
short range reorderings, they are inadequate to
capture long-range reorderings, especially for lan-
guage pairs that differ significantly in their syn-
tax. A promising workaround is the source pre-
ordering method that can be considered similar,
to some extent, to the reordering strategy imple-
mented in n-code; the main difference is that the
latter uses one deterministic (long-range) reorder-
ing on top of conventional distortion-based mod-
els, while the former only considers one single
model delivering permutation lattices. The pre-
ordering approach is illustrated by the recent work
of Neubig et al (2012), where the authors use a
discriminatively trained ITG parser to infer a sin-
gle permutation of the source sentence.
In this section, we investigate the use of this
pre-ordering model in conjunction with the bilin-
gual n-gram approach for translating English into
German (see (Collins et al, 2005) for similar ex-
periments with the reverse translation direction).
Experiments are carried out with the same settings
as described in (Neubig et al, 2012): given the
source side of the parallel data (en), the parser is
estimated to modify the original word order and to
generate a new source side (en-mod); then a SMT
system is built for the new language pair (en-mod
? de). The same reordering model is used to re-
order the test set, which is then translated with the
en-mod? de system.
Results for these experiments are reported in Ta-
ble 3, where nt09 and nt11 are respectively used
as development and test sets. We can observe that
applying pre-ordering on source sentences leads to
small drops in performance for this language pair.
To explain this degradation, the histogram of to-
ken movements performed by the model on the
pre-ordered training data is represented in Fig-
ure 1. We can observe that most of the movements
are in the range [?4,+6] (92% of the total occur-
65
Figure 1: Histogram of token movement size ver-
sus its occurrences performed by the model Neu-
big on the source english data.
rences), which can be already taken into account
by the standard reordering model of the baseline
system. This is reflected also by the following
statistics: surprisingly, only 16% of the total num-
ber of sentences are changed by the pre-ordering
model, and the average sentence-wise Kendall?s ?
and the average displacement of these small parts
of modified sentences are, respectively, 0.027 and
3.5. These numbers are striking for two reasons:
first, English and German have in general quite
different word order, thus our experimental con-
dition should be somehow similar to the English-
Japanese scenario studied in (Neubig et al, 2012);
second, since the model is able to perform pre-
ordering basically at any distance, it is surprising
that a large part of the data remains unmodified.
6 Artificial Text generation with SOUL
While the context size for BOLMs is limited (usu-
ally up to 4-grams) because of sparsity issues,
NNLMs can efficiently handle larger contexts up
to 10-grams without a prohibitive increase of the
overall number of parameters (see for instance the
study in (Le et al, 2012b)). However the major
bottleneck of NNLMs is the computation cost dur-
ing both training and inference. In fact, the pro-
hibitive inference time usually implies to resort to
a two-pass approach: the first pass uses a conven-
tional BOLM to produce a k-best list (the k most
likely translations); in the second pass, the prob-
ability of a NNLM is computed for each hypoth-
esis, which is then added as a new feature before
the k-best list is reranked. Note that to produce the
k-best list, the decoder uses a beam search strategy
to prune the search space. Crucially, this pruning
does not use the NNLMs scores and results in po-
tentially sub-optimal k-best-lists.
6.1 Sampling texts with SOUL
In language modeling, a language is represented
by a corpus that is approximated by a n-gram
model. Following (Sutskever et al, 2011; Deoras
et al, 2013), we propose an additionnal approxi-
mation to allow a tighter integration of the NNLM:
a 10-gram NNLM is first estimated on the training
corpus; texts then are sampled from this model to
create an artificial training corpus; finally, this arti-
ficial corpus is approximated by a 4-gram BOLM.
The training procedure for the SOUL NNLM is
the same as the one described in (Le et al, 2012c).
To sample a sentence from the SOUL model, first
the sentence length is randomly drawn from the
empirical distribution, then each word of the sen-
tence is sampled from the 10-gram distribution es-
timated with the SOUL model.
The convergence of this sampling strategy can
be evaluated by monitoring the perplexity evolu-
tion vs. the number of sentences that are gener-
ated. Figure 2 depicts this evolution by measuring
perplexity on the nt08 set with a step size of 400M
sampled sentences. The baseline BOLM (std) is
estimated on all the available training data that
consist of approximately 300M of running words.
We can observe that the perplexity of the BOLM
estimated on sampled texts (generated texts) de-
creases when the number of sample sentences in-
creases, and tends to reach slowly the perplex-
ity of the baseline BOLM. Moreover, when both
BOLMs are interpolated, an even lower perplex-
ity is obtained, which further decreases with the
amount of sampled training texts.
6.2 Translation results
Experiments are run for translation into German,
which lacks a GigaWord corpus. An artificial cor-
pus containing 3 billions of running words is first
generated as described in Section 6.1. This corpus
is used to estimate a BOLM with standard settings,
that is then used for decoding, thereby approxi-
mating the use of a NNLM during the first pass.
Results reported in Table 4 show that adding gen-
erated texts improves the BLEU scores even when
the SOUL model is added in a rescoring step. Also
note that using the LM trained on the sampled cor-
pus yields the same BLEU score that using the
standard LM.
66
 190 200 210 220 230 240 250 260
 270 280
 2  4  6  8  10  12ppx times 400M sampled sentences
artificial textsartificial texts+stdstd
Figure 2: Perplexity measured on nt08 with the
baseline LM (std), with the LM estimated on the
sampled texts (generated texts), and with the inter-
polation of both.
Therefore, to translate from English to German,
the submitted system includes three BOLMs: one
trained on all the monolingual data, one on artifi-
cial texts and a third one that uses the freely avail-
able deWack corpus3 (1.7 billion words).
target LM BLEU
dev nt09 test nt10
base 15.3 16.5
+genText 15.5 16.8
+SOUL 16.4 17.6
+genText+SOUL 16.5 17.8
Table 4: Impact of the use of sampled texts.
7 Different tunings for different original
languages
As shown by Lembersky et al (2012), the original
language of a text can have a significant impact on
translation performance. In this section, this effect
is assessed on the French to English translation
task. Training one SMT system per original lan-
guage is impractical, since the required informa-
tion is not available for most of parallel corpora.
However, metadata provided by the WMT evalua-
tion allows us to split the development and test sets
according to the original language of the text. To
ensure a sufficient amount of texts for each con-
dition, we used the concatenation of newstest cor-
pora for the years 2008, 2009, 2011, and 2012,
leaving nt10 for testing purposes.
Five different development sets have been cre-
ated to tune five different systems. Experimental
results are reported in Table 7 and show a drastic
3http://wacky.sslmit.unibo.it/doku.php
baseline adapted
original language tuning
cz 22.31 23.83
en 36.41 39.21
fr 31.61 32.41
de 18.46 18.49
es 30.17 29.34
all 29.43 30.12
Table 5: BLEU scores for the French-to-English
translation task measured on nt10 with systems
tuned on development sets selected according to
their original language (adapted tuning).
improvement in terms of BLEU score when trans-
lating back to the original English and a significant
increase for original text in Czech and French. In
this year?s evaluation, Russian was introduced as
a new language, so for sentences originally in this
language, the baseline system was used. This sys-
tem is used as our primary submission to the eval-
uation, with additional SOUL rescoring step.
8 Conclusion
In this paper, we have described our submis-
sions to the translation task of WMT?13 for
the French-English, German-English and Spanish-
English language pairs. Similarly to last year?s
systems, our main submissions use n-code, and
continuous space models are introduced in a post-
processing step, both for translation and target lan-
guage modeling. To translate from English to
German, we showed a slight improvement with
a tighter integration of the continuous space lan-
guage model using a text sampling strategy. Ex-
periments with pre-ordering were disappointing,
and the reasons for this failure need to be better
understood. We also explored the impact of using
different tuning sets according to the original lan-
guage of the text to be translated. Even though the
gain vanishes when adding the SOUL model in a
post-processing step, it should be noted that due to
time limitation this second step was not tuned ac-
cordingly to the original language. We therefore
plan to assess the impact of using different tuning
sets on the post-processing step.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
67
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard Un iversity.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 531?540, Ann Arbor,
Michigan.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Marin?o.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink, and
Kenneth Church. 2013. Approximate inference: A
sampling based modeling technique to capture com-
plex dependencies in a language model. Speech
Communication, 55(1):162 ? 177.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012b. Measuring the influence of long range de-
pendencies with neural network language models.
In Proceedings of the NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model? On
the Future of Language Modeling for HLT, pages 1?
10, Montre?al, Canada.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012c. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada.
Gennadi Lembersky, Noam Ordan, and Shuly Wint-
ner. 2012. Language models for machine trans-
lation: Original vs. translated texts. Comput. Lin-
guist., 38(4):799?825, December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Proceed-
ings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, AMTA
?02, pages 135?144, Tiburon, CA, USA. Springer-
Verlag.
68
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02:
Proc. of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318. Associ-
ation for Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. of Interna-
tional Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-
works. In Lise Getoor and Tobias Scheffer, editors,
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ?11, pages
1017?1024, New York, NY, USA, June. ACM.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
69
