Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3?14,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Your click decides your fate: Inferring Information Processing and
Attrition Behavior from MOOC Video Clickstream Interactions
Tanmay Sinha
1
, Patrick Jermann
2
, Nan Li
3
, Pierre Dillenbourg
3
1
Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA 15213, USA
2
Center for Digital Education, EPFL, CH 1015, Switzerland
3
Computer-Human Interaction in Learning and Instruction, EPFL, CH 1015, Switzerland
1
tanmays@andrew.cmu.edu,
2,3
<firstname.lastname>@epfl.ch
Abstract
In this work, we explore video lec-
ture interaction in Massive Open Online
Courses (MOOCs), which is central to stu-
dent learning experience on these educa-
tional platforms. As a research contribu-
tion, we operationalize video lecture click-
streams of students into cognitively plau-
sible higher level behaviors, and construct
a quantitative information processing in-
dex, which can aid instructors to better un-
derstand MOOC hurdles and reason about
unsatisfactory learning outcomes. Our re-
sults illustrate how such a metric inspired
by cognitive psychology can help answer
critical questions regarding students? en-
gagement, their future click interactions
and participation trajectories that lead to
in-video & course dropouts. Implications
for research and practice are discussed.
1 Introduction
Mushrooming as a scalable lifelong learn-
ing paradigm, Massive Open Online Courses
(MOOCs) have enjoyed significant limelight in re-
cent years, both in industry and academia (Hag-
gard et al., 2013). The euphoria is about the
transformative potential of MOOCs to revolution-
ize online education (North et al., 2014), by con-
necting and fostering interaction among millions
of learners who otherwise would never have met
and providing autonomy to these learners to grap-
ple with the course instruction at their own pace of
understanding. However, despite this expediency,
there is also considerable skepticism in the learn-
ing analytics research community about MOOC
productiveness (Nawrot and Antoine, 2014), pri-
marily because of unsatisfactory learning out-
comes that plague these educational platforms and
induce a funnel of participation (Clow, 2013).
With a ?one size fits all? approach that MOOCs
follow, scaled up class sizes and lack of face to
face interaction coupled with such high student
teacher ratios (Guo and Katharina, 2014), stu-
dents? motivation to follow the course oscillates
(Davis et al., 2014). This is comprehensibly re-
flected in escalating attrition rates in MOOCs, ever
since they have started maturing (Belanger and
Thornton, 2013; Schmidt and Zach, 2013; Yang et
al., 2013). Because it is not feasible for MOOC in-
structors to manually provide individualized atten-
tion that caters to different backgrounds, diverse
skill levels, learning goals and preferences of stu-
dents, there is an increasing need to make directed
efforts towards automatically providing better per-
sonalized content in e-learning (Sinha et al., 2013;
Lie et al., 2014; Sinha, 2014a). The provision
of guidance with regard to the organization of the
study and regulation of learning is a domain that
also needs to be addressed.
A prerequisite for such an undertaking is that
we, as MOOC researchers, understand how di-
verse ecologies of participation develop as stu-
dents interact with the course material (Fischer,
2011), and how learners distribute their attention
with multiple forms of computer mediated inputs
in MOOCs. Learning in a MOOC requires that
students apply self regulation. While substantial
research has been done on studying MOOC dis-
cussion forums (Ramesh et al., 2013; Brinton et
al., 2013; Anderson et al., 2014; Sinha, 2014b),
grading strategies for assignments (Tillmann et al.,
2013; Kulkarni et al., 2014) and deployment of
reputation systems (Coetzee et al., 2014), inner
workings of students? interaction while watching
MOOC video lectures have been much less fo-
cused upon. Given that roughly 5% (Huang et al.,
2014) of students actually participate in MOOC
discussion forums, it would be legitimate to ask
whether choosing video lectures as units of analy-
sis would be more insightful. After 330,000 reg-
3
istrations in MOOC courses at EPFL in 2013, our
experience reflects that out of the 100% students
who register, 75% show up: 50% of them primar-
ily watch video lectures and the rest 25% addition-
ally work out homeworks and assignments. Thus,
majority of students have video lecture viewing as
their primary MOOC activity.
Video lectures form a primary and an extremely
crucial part of MOOC instruction design. They
serve as gateways to draw students into the course.
Concept discussions, demos and tutorials that are
held within these short video lectures, not only
guide learners to complete course assignments,
but also encourage them to discuss the taught
syllabus on MOOC discussion forums. Specific
to the context of video lectures, prior work has
cut teeth on a)how video production style (slides,
code, classroom, khan academy style etc) relates
to students? engagement (Guo et al., 2014), b)what
features of the video lecture and instruction de-
livery, such as slide transitions (change in visual
content), instructor changing topic (topic model-
ing and ngram analysis) or variations in instruc-
tor?s acoustic stream (volume, pitch, speaking
rate), lead to peaks in viewership activity (Kim
et al., 2014b). There has been increasing focus
on unveiling numerous facets of complexity of
raw click-level interactions resulting from student
activities within individual MOOC videos (Kim
et al., 2014a; Sinha et al., 2014). However, to
the best of our knowledge, we present the first
study that describes usage of such detailed click-
stream information to form cognitive video watch-
ing states that summarize student clickstream. In-
stead of using summative features that express stu-
dent engagement, we leverage recurring click be-
haviors of students interacting with MOOC video
lectures, to construct their video watching profile.
Based on these richly logged interactions of stu-
dents, we develop computational methods that an-
swer critical questions such as a)how long will stu-
dents grapple with the course material and what
will their engagement trajectory look like, b)what
future click interactions will characterize their be-
havior, c)whether students are ultimately going to
survive through the end of the video and course.
As an effort to improve the second generation of
MOOC offerings, we perform a hierarchical three
level clickstream analysis, rooted in foundations
of cognitive psychology. Incidentally, we explore
at a micro level whether, and how, cognitive mind
states govern the formation and occurrence of mi-
cro level click patterns. Towards this end, we also
develop a quantitative information processing in-
dex and monitor its variations among different stu-
dent partitions that we define for the MOOC. Such
an operationalization can help course instructors
to reason how students? navigational style reflects
cognitive resource allocation for meaning process-
ing and retention of concepts taught in the MOOC.
Our metric aids MOOC designers in identifying
which part of the videos might require editing.
The goal is to develop an explanatory techno-
cognitive model which shows that a set of metrics
derived from low-level behaviors are meaningful,
and can in turn be used to make effective predic-
tions on high-level behaviors intuitively.
In the remainder of this paper, we describe our
study context in the next section. In section 3,
we motivate our three level hierarchical MOOC
video clickstream analysis (operations, actions, in-
formation processing activities), describing rele-
vant related work along the way, along with the
technical approach followed. In section 4, we val-
idate our developed methodology by setting up
certain machine learning experiments, specifically
engagement prediction, next click state prediction,
in-video and complete course dropout prediction.
Implications for future work and conclusion is pre-
sented in section 5.
2 Study Context
The data for our current study comes from an in-
troductory programming MOOC ?Functional Pro-
gramming in Scala? that was offered on the Cours-
era MOOC platform in 2012. This MOOC com-
prises 48 video lectures (10 Gb of JSON data),
which has been parsed and preprocessed into a
convenient format for experimentation. In these
interaction logs, every click of students on the
MOOC video player is registered (play, pause,
seek forward, seek backward, scroll forward,
scroll backward, ratechange). We have informa-
tion about the rate at which the video lecture is
played, total time spent on playing the video and
time spent on/in-between various click events such
as play, pause, seek etc. In total, 65969 stu-
dents registered for the course, and 36536 of them
had 762137 logged video interaction sessions con-
taining the aforementioned types of click events.
If a video is played till the end, then an auto-
matic video-end pause is generated. Otherwise,
4
the Coursera platform unfortunately does not log
whether or not a student has left the video in the
middle, leaving the true video engagement time
unknown. To avoid biased data, we only include
video sessions containing video-end pauses. This
has yielded a dataset of 222021 video sessions
from 21952 students for our analysis in this paper.
3 Operationalizing the Clickstream
3.1 Level 1 (Operations)
From our raw clickstream data, we construct a de-
tailed encoding of students? clicks in the follow-
ing 8 categories: Play (Pl), Pause (Pa), SeekFw
(Sf), SeekBw (Sb), ScrollFw (SSf), ScrollBw
(SSb), RatechangeFast (Rf), RatechangeSlow
(Rs). When two seeks happen within a small time
range (< 1 sec), we group these seek events into a
scroll. Additionally, to encode ?Rf? and ?Rs?, we
look for the playrate of the click event that occurs
just before the ?Ratechange? click and compare it
with students? currently changed playrate, to de-
termine whether he has sped up/slowed down his
playing speed. The reason behind encoding click-
streams to such specific categories, accommodat-
ing scrolling behavior and clicks representative of
increase and decrease in video playing speed, is to
experimentally analyze and understand the impact
of such a granularity on our experiments, which
are designed with an objective to capture the mot-
ley of differently motivated behavioral watching
style in students.
As a next step, we concatenate these click
events for every student, for every video lec-
ture watched. Thus, the output from level 1 is
this string of symbols that characterizes the se-
quence of clickstream events (video watching state
sequence). For e.g: PlPaSfSfPaSbPa.., PlSSb-
PaRsRsPl..
3.2 Level 2 (Behavioral Actions)
Existing literature on web usage mining says that
representing clicks using higher level categories,
instead of raw clicks, better exposes the brows-
ing pattern of users. This might be because high
level categories have better noise tolerance than
naive clickstream logs. The results obtained from
grouping clickstream sequences at per click res-
olution are often difficult to interpret, as such
a fine resolution leads to a wide variety of se-
quences, many of which are semantically equiv-
alent. Therefore, to get more insights into stu-
dent behavior in MOOCs, we group clicks en-
coded at very fine granularity into meaningful be-
havioral categories. Doing this also reduces se-
quence length which is easily interpretable. There
is some existing literature (Banerjee and Ghosh,
2000; Wang et al., 2013), that just considers click
as a binary event (yes/no) and discusses formation
of concept based categories based on the area/sub
area of the stimulus where the click was made.
To summarize a students? clickstream, we ob-
tain n-grams with maximum frequency from the
clickstream sequence (a contiguous sequence of
?n? click actions). Such a simple n-gram represen-
tation convincingly captures the most frequently
occurring click actions that students make in con-
junction with each other (n=4 was empirically de-
termined as a good limit on clickstream subse-
quence overspecificity). Then, we construct seven
semantically meaningful behavioral categories us-
ing these discovered n-grams, selecting represen-
tative click groups that occur within top ?k? most
frequent n-grams (k=100). Each behavioral cate-
gory acts like a latent variable, which is difficult to
measure from data directly.
? Rewatch: PlPaSbPl, PlSbPaPl, PaSbPlSb,
SbSbPaPl, SbPaPlPa, PaPlSbPa
? Skipping:SfSfSfSf, PaPlSfSf, PlSfSfSf, SfS-
fSfPa, SfSfPaPl, SfSfSfSSf, SfSfSSfSf, Sf-
PaPlPa, PlPaPlSf
? Fast Watching: PaPlRfRf, RfPaPlPa, RfRf-
PaPl, RsPaPlRf, PlPaPlRf (click group of
Ratechange fast clicks while playing or paus-
ing video lecture content, indicating speeding
up)
? Slow Watching: RsRsPaPl, RsPaPlPa,
PaPlRsRs, PlPaPlRs, PaPlRsPa, PlRsPaPl
(click group of Ratechange slow clicks while
playing or pausing video lecture content, in-
dicating slowing down)
? Clear Concept: PaSbPlSSb, SSbSbPaPl,
PaPlSSbSb, PlSSbSbPa (a combination of
SeekBw and ScrollBw clicks, indicating high
tussle with the video lecture content)
? Checkback Reference: SbSbSbSb, PlSbS-
bSb, SbSbSbPa, SbSbSbSf, SfSbSbSb, Sb-
PlSbSb, SSbSbSbSb (a wave of SeekBw
clicks)
? Playrate Transition: RfRfRsRs, RfRfRfRs,
RfRsRsRs, RsRsRsRf, RsRsRfRf, RfRfRfRf
(a wave of ratechange clicks)
5
Case (Full, No, Partial
match)
Clickstream A Clickstream B Fuzzy string matching
verdict
1:Varying clickstream
length
PlPaPlSfPaSfSbSbPl PlPaPlSfPaSfSbSbPlPaSbSbSbRfRsRf
(learner has performed lot more clicks)
Weight(P,A)>Weight(P,B)
2:Behavioral pattern ap-
pears more than once
PlPaPlSfPaSfSbSbPl PlPaPlSfPaSfSbSbPlPlSfPaSf
(pattern is more characteristic as it ap-
pears 2 times)
Weight(P,A)<Weight(P,B)
3:No appearance of be-
havioral pattern
RfSbSbRs SSfSSfRsRsRsSfSfSfSfRfRfRfRfRf
(string length doesn?t matter)
Weight(P,A)6=(P,B)
(very low weight)
4:Variation in number of
individual clicks
RfSbSbRsPlSbPaSb RfSbSbRsPlSbSfPaSfSb
(more clicks from pattern appear)
Weight(P,A)<Weight(P,B)
5:Variation in scattering
of individual clicks
RfSbRsPlSbSfPaSfSb
(less scattering)
RfSbRsPlSbSSbSfPlSbRsPaSbRfSf
(more scattering)
Weight(P,A)>Weight(P,B)
6:Reverse order of indi-
vidual click appearance
RfRsSbSfPaSfSbPl
(order reversed)
RfRsPlSbSfPaSfSb
(order maintained)
Weight(P,A)<Weight(P,B)
Table 1: Fuzzy string similarity weights for the sample behavioral action P(?PlSfPaSf?). Weight(P, A/B)
represents the similarity of the pattern P w.r.t clickstream sequence A or B.
In an attempt to quantify the importance of each
of the above behavioral actions in characterizing
the clickstream, we adopt a fuzzy string match-
ing approach. Using this approach, we assign
a weight to each of the grouped behavioral pat-
terns for a given students? video watching state se-
quence (based on similarity of click groups present
in each behavioral category, with the full click-
stream sequence). The fuzzy string method (Van,
2014) is justified because it caters to the noise that
might be present in raw clickstream logs of stu-
dents, in six different ways, as mentioned in Ta-
ble 1. After identifying these cases and meticu-
lous experimental evaluation, we apply the follow-
ing distance metrics and tuning parameters: Co-
sine similarity metric between the vector of counts
of n-gram (n=4) occurrences for Cases 1 and 2,
Levenshtein similarity metric for Cases 3 (weight
for deletion=0, weight for insertion and substitu-
tion=1), 4, 5, 6 (weight for deletion=0.1, weight
for insertion, substitution=1).
As a next step, all subcategories of click groups
that lie within each behavioral category are aggre-
gated by summing up the individual fuzzy string
similarity weights that are obtained with respect
to every students? clickstream sequence. Then,
we perform a discretization of these summed up
weights, for each behavioral category, by equal
frequency (High/Low). The concern of adding up
two distance metrics that do not lie in the same
range, is thus alleviated, because the dichotomiza-
tion automatically places highly negative values in
the ?Low? category and positive values closer to
0 in the ?High? category. The result is a click-
stream vector for each video viewing session of
the student, where every element of the vector
tells us about the weight (importance) of a behav-
ioral category for characterizing the clickstream.
Thus, the output from level 2 is such a summarized
clickstream vector. For e.g: (Skipping=High, Fast
Watching=High, Checkback Reference=Low, Re-
watch=Low, ....).
3.3 Level 3 (Information Processing)
Watching MOOC videos is an interaction between
the student and the medium, and therefore the con-
ceptualization of higher-order thinking eventually
leading to knowledge acquisition (Chi, 2000), is
under control of both the a)student (who decides
what video segment to watch, when and in what
order to watch, how hard an effort be made to
try and understand a specific video segment) and,
b)medium/video lecture (the content or features
of which decides what capacity allocation is re-
quired by the student to fully process the informa-
tion contained).
Research has consistently found that the level
of cognitive engagement is an important aspect of
student participation (Carini et al., 2006). This
cognitive processing is influenced by the appeti-
tive (approach) and aversive (avoidance) motiva-
tional systems of a student, which activate in re-
sponse to motivationally relevant stimuli in the en-
vironment (Cacioppo and Wendi, 1999). For ex-
ample, in the context of MOOCs, the appetitive
system?s goal is in-depth exploration and infor-
mation intake, while the aversive system primar-
ily serves as a motivator for not attending to cer-
tain MOOC video segments. Thus, click behaviors
representative of appetitive motivational system
are rewatch/clear concept/slow watching, while
click behaviors representative of aversive motiva-
6
Figure 1: Relating students? information processing to click behaviors exhibited in the MOOC, based on
video lecture perception
tional system are skipping/fast watching. In this
work, we try to construct students? information
processing index, based on the ?Limited Capacity
Information Processing Approach? (Basil, 1994;
Lang et al., 1996; Lang, 2000), which asserts that
people independently allocate limited amount of
cognitive resources to tasks from a shared pool.
Figure 1 depicts this idea.
We must acknowledge the fact that video watch-
ing in MOOCs requires students to recall facts that
they already know (specific chunks of declarative
knowledge (Anderson, 2014). This helps them to
build a mental representation of the information
presented in a MOOC video lecture segment, fol-
low and understand the concept being currently
taught. However, it must be noted that depending
on the a)expertise level, which decides how avail-
able the past knowledge is and how hard is it to
retrieve the previously known facts, b)perception
of video lecture as difficult or simple to under-
stand, c)motivation to learn or just have a look at
the video lecture to seek specific outcomes, cog-
nitive resource allocation would vary among these
time sensitive subprocesses in stage 1 and 2 of the
pipeline (depicted in Figure 1). This in turn, would
be reflected by the underlying non linear navi-
gational patterns that students have, specifically
the nature of clicks which they make to adjust
the speed of information processing (by pausing,
seeking forward/backward, ratechange clicks), as
responses to the stimuli.
Consider an example of students who watch the
MOOC lecture, primarily because of reasons such
as gaining familiarity with the topic. Such stu-
dents would purposely not allocate their process-
ing resources to ?memory? part of the information
processing pipeline (encode, store, retrieve). Ad-
ditionally, they will decode and process minimal
information that is required to follow the story.
On the contrary, students who watch the MOOC
lecture, with the aim of scoring well in post-tests
(MOOC quizzes and assignments), would allocate
high cognitive processing to understand, learn and
retain information from the lecture. Thus, such
students would process information more fully
and thoroughly, despite a possibility of cognitive
overload.
In order to relate our behavioral actions con-
structed from the raw clickstream with this rich
and informative stream of literature, we create a
taxonomy of behavioral actions exhibited in the
clickstream to construct a quantitative ?Informa-
tion Processing Index (IPI)?. Figure 2 reflects
the proposed hierarchy of information processing
from high to low using linear weight assignments.
We omit the line of reasoning that goes behind
defining the precise position of each behavioral ac-
tion in this hierarchy due to lack of space. How-
ever, the details can be found in (Sinha, 2014c).
Negative weights are necessary to distinguish be-
tween ?high? and ?low? weights for each behav-
ioral action. For example, if skipping=high is
weighted -3, skipping=low will be weighted +3 on
the information processing index. Students? infor-
7
mation processing index is defined as follows:
Information Processing Index (IPI) =
(?1)
j
7
?
i=1
WeightAssign(Behavioral Action i),
j=1,2 depending on whether the behavioral action
is weighted low or high.
Figure 2: Linear weight assignments for behav-
ioral clickstream actions, according to the infor-
mation processing hierarchy developed
One of the focal utilities of developing such
a quantitative index is that meaningful interven-
tion could be provided in real time to students, as
they steadily build up their video watching pro-
file while interacting with MOOC video lectures.
Viewing throught the lens of the Goldilocks prin-
ciple (Kidd et al., 2012), our metric can poten-
tially help instructors in understanding and differ-
entiating between students looking away from the
MOOC visual sequence, because of too simple or
too complex representation. Adaptive presenta-
tion of instructional materials is another learning
science application where leveraging our metric
would be beneficial.
Specifically, when IPI > 0, it can be inferred
that high information processing is being done
by students. Therefore MOOC instructors need
to check for coherency in pace of instruction de-
livery and students? understanding. This might
also hint towards redesigning specific video lec-
ture segments and simplifying them so that they
become easier to follow. On the contrary, when
IPI < 0, low information processing is being done
by students. Therefore MOOC instructors need
to help students better engage with the course,
by providing them additional interesting read-
ing/assignment material, or fixing video lecture
content such that it captures students? attention.
The neutral case of IPI = 0 occurs when students?
locally exhibited high and low information pro-
cessing needs in their evolving clickstream se-
quence counterbalance each other. So, interven-
tions need to made depending on the video lecture
segment, where IPI was >0 or <0.
4 Validation Experiments
We use machine learning to validate the method-
ology developed in section 3.1 and 3.2 for sum-
marizing students? clickstream, ensuring that the
same student does not appear in the train and test
folds. The motivation behind setting up these ex-
periments is to automatically measure students?
length of interaction with MOOC video lectures,
understand how they develop their video watch-
ing profile and discern what viewing profile of stu-
dents leads to in-video and course dropouts. Fur-
thermore, we validate the methodology developed
in section 3.3 by statistically analyzing variations
of IPI and testing its sensitivity to student attrition
using survival models.
4.1 Machine Learning Experiment Design
4.1.1 How much do you engage?
Students, while watching MOOC video lectures
can pause, seek, scroll and change rate of the
video. Thus, it is meaningful to quantify students?
engagement as the summation of video playing
time, seeks & pauses, multiplied by the playback
rate. For example, if a student plays 700 secs out
of a 1000 sec video, pauses 2 times for 100 secs
each, at an average play rate of 1.5, he effectively
engages with the video for (700+200)?1.5=1350
secs. Such an interaction measure multiplied by
playback rate, is representative of effective video
lecture content covered.
Research Question 1: Can students? click-
stream sequence predict length of students? inter-
action with the video lecture?
Settings: The data for this experiment comes
from a randomly chosen video lecture 4-6 (6th lec-
ture in the 4th week of the course, with not too
many initial lurkers and not too many dropouts).
For experimental purposes, engagement times for
students are discretized by equal frequency into 2
categories (High/Low). The dependent variable is
student engagement (High: 1742 examples, Low:
1741 examples). L2 regularized Logistic Regres-
sion is used as the training algorithm (with 10
fold cross validation annotated by student-id and
8
rare feature extraction threshold being 2). As fea-
tures, we extract N-grams of length 4 and 5, se-
quence length and regular expressions from stu-
dents? clickstream sequences. In the changed
setup, we consider summarized behavioral action
vectors (output from level 2) as column features.
4.1.2 Are you bored or challenged?
Next, we focus our attention on how clickstream
sequences evolve. If we know that students? in-
teraction with the video lecture is going to be
for a long time (reflected by high engagement),
it could have been the case that they were strug-
gling at the current level of instruction (for exam-
ple, a high combination of pause/seek backward
events). Therefore, if such a phenomenon can
be detected in real time video lecture interaction,
such learners can be presented with reinforcement
course material before moving forward. Alterna-
tively, if we know that students? interaction with
the video lecture is going to be for a short time (re-
flected by low engagement), they could be bored
or are quite likely to skip course content forward
often. Such students can be presented with ad-
vanced study material. However, in order to de-
velop such a real time knowledge model and tailor
targeted interventions at students, we need to study
the trajectory of click sequence formation.
Research Question 2: Can we precisely predict
what will be the next sequence of clicks that leads
students to different engagement states?
Settings: The data for this experiment comes
from the same video lecture 4-6 (6th lecture in
the 4th week of the course). The dependent
variable is next click state of students (Pa, Pl,
Sf, SSf, Sb, SSb, Rf, Rs). L2 regularized Lo-
gistic Regression is used as the training algo-
rithm (with 5 fold cross validation annotated by
student-id and rare feature extraction threshold
being 5). If we want to predict the click at
the i
th
instant, we extract the following features
from 0 till (i-1)
th
instant: a)Engagement with the
video lecture as defined for Research Question
1(High/Low); b)Proportion of click events belong-
ing to Pl/Pa/Sf/SSf/Sb/SSb/Rf/Rs (representative
of kind of interaction with the stimulus); c)N-
grams of length 4,5 and sequence length from
students? clickstream sequences. In the changed
setup, we consider summarized behavioral action
vectors (output from level 2) as column features.
4.1.3 Will you drop out of the video?
As students progress through the video, they
slowly build up their video watching profile by
interacting with the stimulus in different propor-
tions, which in turn depend on their click action
sequences. This motivates our next machine learn-
ing experiment, which seeks to derive utility from
the first two experiments. Navigating away from
the video without completing it fully is an out-
come of low student engagement. A student is
more likely to watch till the end of a video, if the
lecture activates his thinking. Thus, it would be in-
teresting to investigate, whether the nature of stu-
dents? interaction provides us a hint about in-video
dropout behavior. Prior work has made a prelim-
inary study on how in-video dropout is correlated
with length of the video, and how in-video dropout
varies among first time watchers and rewatchers
(Kim et al., 2014a). However, we consider video
interaction features at a much finer granularity,
representative of how students progress through
the video. In doing so, we use detailed clickstream
information, including seek, scroll and ratechange
behavior, in addition to merely play and pause in-
formation.
Research Question 3: What video watching
profile of students leads to in-video dropouts?
Settings: The data for this experiment comes
from the same video lecture 4-6 (6th lecture in
the 4th week of the course). The dependent
variable is the binary variable, in-video dropout
(0/1). To address the skewed class distribution,
cost sensitive L2 regularized Logistic Regression
is used as the training algorithm (with 10 fold
cross validation annotated by student-id and rare
feature extraction threshold being 2). To ex-
tract the interaction footprint of students before
they drop out of the video, we extract the fol-
lowing features: a)N-grams of length 4,5 and
sequence length from students? clickstream se-
quences; b)Proportion of click events belonging to
Pl/Pa/Sf/SSf/Sb/SSb/Rf/Rs (representative of kind
of interaction with the stimulus); c)Engagement
with the video lecture as defined for Research
Question 1(High/Low); e)Last click action before
dropout happened; f)Time spent after the last click
action was made (discretized by equal frequency
to High/Low). In the changed setup, we con-
sider summarized behavioral action vectors (out-
put from level 2) as column features.
9
4.1.4 Will you watch videos and stay till the
course end?
We may expect that when students find the course
too tough to follow, uninteresting or boring, they
will not engage with future videos. On the con-
trary, when students seem very interested in un-
derstanding the video and exhibit lots of rewatch-
ing behavior, we might expect them to stay on till
the course end video lectures. Students who do
not stay till the last week of the course (exhibit any
video lecture viewing), are considered as complete
course dropouts. One principal application of de-
tecting these dropouts early could be recommen-
dation of selected future video lectures to watch
(for example, where an interesting concept, case
study or application is going to be discussed), to
positively motivate and pull these students back
into the MOOC.
Research Question 4: Can we discover pat-
terns in the video watching trajectory of students
that can predict when are students most likely not
to view future video lectures?
Settings: The data for this experiment comes
from all 48 videos of ?Functional Program-
ming in Scala? MOOC (4710 non-dropouts, 9596
dropouts). To address the skewed class distribu-
tion, cost sensitive L2 regularized Logistic Re-
gression is used as the training algorithm (with 5
fold cross validation annotated by student-id and
rare feature extraction threshold being 5). The
dependent variable is the binary variable, com-
plete course dropout (0/1), indicating whether the
student ultimately stayed on (watched any video
lecture) till the last course week. Engagement
(time in seconds) of a student is discretized by
equal frequency into High and Low categories,
considering all interactions in each video lecture
separately (because length of each video differs,
so the discretization criteria would also differ for
each video). Video play proportion((video played
length/video length)*100*average play rate) for a
student is discretized by equal width (Very Low:
<50%, Low: 50-100%, High: 100-150%, Very
High: >150%). IPI for a student is discretized by
equal frequency (Very Low: <-1.00, Low: [-1.00,
1.00], High: [1.00, 3.00], Very High: >3.00).
The discretization criteria (equal width, frequency
and number of bins) was experimentally deter-
mined. Development of trajectories for each of
these factors is indicated in Figure 3. To extract
the interaction footprint of students before they
drop out of the course, we extract the following
features: a)N-grams of length 4,5 and sequence
length from ?Engagement trajectory?, ?Video Play
Proportion trajectory? and ?IPI trajectory? of stu-
dents for the videos watched from 0 to (n-1)th in-
stant, b)Engagement, Video Play Proportion and
IPI trajectories for the nth instance (attribute for
the last video lecture watched before dropping
out), c)Proportion of different symbol representa-
tions in the trajectories (for example, in a trajec-
tory such as HLLHH, proportion(H)=60%, pro-
portion(L)=40%.
Figure 3: Example depicting how different opera-
tionalized trajectories of students are formed
4.2 Results
Results of the four machine learning experiments,
along with the most representative (weighted) fea-
tures that characterize classes, are reported in table
2. There are two important positives here: a)The
summarized behavioral action vectors from level
2 are able to achieve nearly similar values of ac-
curacy and kappa when compared to the raw level
clicks. This means that we can reason different
meaningful video viewing behaviors of students
without getting our hands dirty in examining noisy
and continually occurring raw clicks, b)Our met-
ric of interest, i.e the false negative rate
1
is lower
for Case 1.B and Case 3.B, as compared to Case
1.A and Case 3.A, which shows the effectiveness
of the clickstream summarization approach (level
2) in pre-deciphering the fate of students to some
extent.
Additionally, we leverage a statistical analy-
sis technique referred as survival analysis (Miller,
2011), to quantify the extent to which our summa-
rized behavioral clickstream action vectors and IPI
are sensitive to students? dropout. In this model-
ing scheme, dropout variable is 1 on the students?
last week of active participation (in terms of video
lecture viewing), and is 0 for all other weeks.
Our investigation results indicate that a)Students?
1
False negative rate of 0.x means that we correctly iden-
tify (100-(100*0.x))% of dropouts
10
Research
Question
Condition Accuracy
Kappa
False Negative
Rate
Most representative (weighted) features that char-
acterize classes
1. Engagement
Prediction
A)Raw Clicks
0.81
0.63
0.24 High (skipping=low, playrate transition=low, re-
watch=high, slow watching=low, checkback refer-
ence=low, clear concept=high)
B)Summarized
Behavioral
Action
Vectors
0.75
0.49
0.15 Low (skipping=high, playrate transition=high, re-
watch=low, slow watching=high, checkback refer-
ence=high, clear concept=low)
2. Next Click
Prediction
A)Raw Clicks
0.68
0.57
- SeekFw (playratetransition=low, skipping=low, fast
watching=high, clearconcept=low)
SeekBw (checkbackreference=high, rewatch=low,
playratetransition=low, propSeekBw, clearcon-
cept=high)
B)Summarized
Behavioral
Action
Vectors
0.66
0.54
- Ratechangefast (playratetransition=high, re-
watch=low, checkbackreference=low)
Ratechangeslow (playratetransition=high, clearcon-
cept=high)
3. In-video
dropout
Prediction
A)Raw Clicks
0.90
0.69
0.19 Non dropouts (skipping=low, clearconcept=high,
slow watching=high, Checkbackreference=low,
rewatch=high, engagementfromStart=low, engage-
mentlastClick=high)
B)Summarized
Behavioral
Action
Vectors
0.90
0.70
0.15 Dropouts (skipping=high, clearconcept=low,
slow watching=low, engagementfromStart=high,
rewatch=low, engagementlastClick=low, checkback-
reference=high)
4. Complete
Course dropout
Prediction
Operationalized
trajectories
0.80
0.57
0.143 Non dropouts (trajectory IPI=H H H H, trajec-
tory eng=H H H VL H, trajectory vpp=H H H L H)
Dropouts (trajectory IPI=H H VL VL VL, trajec-
tory eng=H L H L L, trajectory vpp=H H H H VL)
Table 2: Performance metrics for machine learning experiments. Random baseline performance is 0.5
dropout in the MOOC is 37% less likely, if they
have one standard deviation greater IPI than aver-
age (Hazard ratio: 0.6367, p<0.001). Such stu-
dents grapple more with the course material to
achieve their desired learning outcomes (as re-
flected by their video lecture participation), b)If
students? rewatching behavior changes from low
to high, they are 33% less likely to dropout (Haz-
ard ratio: 0.6734, p<0.001), c)As students start
watching more proportion of the video lecture,
they are 37% less likely to dropout of the MOOC
(Hazard ratio: 0.6334, p<0.001). This is indica-
tive of their continued interest in the video lecture.
Next, to discern how IPI fluctuates among dif-
ferent student partitions and validate whether our
operationalization produces meaningful results,
we plot figures 4, 5 and perform statistical tests,
specifically z test (testing significance of differ-
ence between means for large sample sizes, when
population standard deviation is known). Pop-
ulation refers to all students in the MOOC be-
ing currently studied. The right half of figure 4
depicts the variation of average IPI, among high
versus low engagers and in-video dropouts ver-
sus non dropouts, in the same video lecture 4-
6 from the course, that we have been perform-
ing our experiments on. Similar findings were
also confirmed with other randomly chosen course
videos. The left half of figure 4 shows the fre-
quency distribution of average IPI. This figure
concurs with our intuitions. The average IPI is
significantly higher for students with ?High? en-
gagement (|z|=8.296, p<0.01) and ?Non In-video
dropouts? (|z|=22.54, p<0.01). This is also re-
flected in the histogram, which clearly shows
that many non in-video dropouts have positive
IPI that pushes up the average. Because the ef-
fect is smaller in low engagers versus high en-
gagers, we see a more similar frequency distri-
bution of average information processing indices
in these 2 bins, as compared to contrasting differ-
ences in the histogram for in-video dropouts and
non dropouts. In order to generalize these find-
ings, we also look at the variations of average
IPI among some other student partitions that we
made for the whole course. ?Viewers? are stu-
dents who have watched or interacted with some
video lecture but have not done the exercises; the
?Active? students additionally turn in homework
also. MOOC dropouts are those students who
cease to actively participate in the MOOC (we are
concerned with video lecture viewing only) before
11
Figure 4: Variation of Average Information Processing Indices(IPI) for Video 4-6
Figure 5: Variation of Average Information Processing Indices(IPI) for the full course
the last week, i.e, students who do not finish the
course. An important observation in figure 5 is
that IPI is clearly able to distinguish between Non-
dropouts and Dropouts (|z|=9.06, p<0.01). This
is also reflected in the histogram in the left half of
figure 5, which verifies that more ?Non dropouts?
have positive IPI. More is the information pro-
cessing done by students, greater is the video lec-
ture involvement, higher are the chances to derive
true utility from video lecture and remain excited
and motivated to stay in the course. We also ob-
tain striking differences between ?Active? versus
?Viewers? (|z|=10.45, p<0.01). Intuitively too,
we expect ?Viewers? to have higher IPI than ?Ac-
tive? class, because as their primary MOOC activ-
ity, ?Viewers? grapple more with the video lecture.
5 Conclusion
In this work, we have begun to lay a foundation for
research investigating students? information pro-
cessing behavior while interacting with MOOC
video lectures. Focusing the center of gravity on
the human mind, we applied a cognitive video
watching model to explain the dynamic process
of cognition involved in MOOC video clickstream
interaction. This paved way for the development
of a simple, yet potent IPI using linear weight as-
signments, which can be effectively used as an
operationalization for making predictions regard-
ing critical learner behavior. We could contem-
plate that IPI significantly varies among different
student partitions. This actually happens because
of presence of smaller substructures inside these
larger groupings, that are similar in their click be-
haviors. Deciphering unique ways of video lec-
ture interaction in such smaller clusters using ap-
proaches such as Markov based clustering, would
be very meaningful for course instructors, to de-
sign customized learning solutions for students
within them (Sinha, 2014c). It would make sense
to incorporate student demographics to better un-
derstand some latent factors, such as playback
speed choices due to native language differences
versus engagement etc. In our recent work (Sinha
et al., 2014), we have been seeking to gain bet-
ter visibility into how combined representations of
video clickstream behavior and discussion forum
footprint can provide insights on interaction path-
ways that lead students to central activities.
12
References
Anderson, A., Huttenlocher, D., Kleinberg, J., &
Leskovec, J. (2014, April). ?Engaging with massive
online courses?. In Proceedings of the 23rd interna-
tional conference on World wide web (pp. 687-698).
International World Wide Web Conferences Steering
Committee.
Anderson, J. R. (2014). ?Rules of the mind?. Psychol-
ogy Press.
Banerjee, A., & Ghosh, J. (2000). ?Concept-based
clustering of clickstream data?.
Basil, M. D. (1994). ?Multiple resource theory I ap-
plication to television viewing?. Communication Re-
search, 21(2), 177-207
Belanger, Y., & Thornton, J. (2013). ?Bioelectricity:
A Quantitative Approach Duke Universitys First
MOOC?.
Brinton, C. G., Chiang, M., Jain, S., Lam, H., Liu,
Z., & Wong, F. M. F. (2013). ?Learning about so-
cial learning in moocs: From statistical analysis to
generative model?. arXiv preprint arXiv:1312.2159.
Cacioppo, J. T., and Wendi L. G. (1999). ?Emotion?.
Annual Reviews: Psychology, 50, 191-214.
Carini, R. M., Kuh, G. D., & Klein, S. P. (2006). ?Stu-
dent engagement and student learning: Testing the
linkages?. Research in Higher Education, 47(1), 1-
32.
Chi, M. T. (2000). ?Self-explaining expository texts:
The dual processes of generating inferences and re-
pairing mental models?. Advances in instructional
psychology, 5, 161-238.
Clow, D. (2013). ?MOOCs and the funnel of participa-
tion? In Proceedings of the Third International Con-
ference on Learning Analytics and Knowledge, pp.
185-189. ACM
Coetzee, D., Fox, A., Hearst, M. A., & Hartmann,
B. (2014, February). ?Should your MOOC forum
use a reputation system??. In Proceedings of the
17th ACM conference on Computer supported coop-
erative work & social computing (pp. 1176-1187).
ACM.
Davis, H. C., Dickens, K., Leon Urrutia, M., Vera, S.,
del Mar, M., & White, S. (2014). ?MOOCs for Uni-
versities and Learners An analysis of motivating fac-
tors?.
Fischer, G. (2011). ?Understanding, fostering, and sup-
porting cultures of participation?. Interactions 18,
no. 3: 42-53.
Guo, P. J., & Reinecke, K. (2014, March). ?Demo-
graphic differences in how students navigate through
MOOCs?. In Proceedings of the first ACM confer-
ence on Learning@ scale conference (pp. 21-30).
ACM.
Guo, Philip J., Juho Kim, and Rob Rubin. (2014).
?How video production affects student engagement:
An empirical study of mooc videos? ACM Learing
at Scale(L@S), pp. 41-50.
Haggard, S., S. Brown, R. Mills, A. Tait, S. Warburton,
W. Lawton, and T. Angulo. (2013). ?The maturing
of the MOOC: Literature review of Massive Open
Online Courses and other forms of online distance
learning? BIS Research Paper 130
Huang, J., Dasgupta, A., Ghosh, A., Manning, J., &
Sanders, M. (2014, March). ?Superposter behav-
ior in MOOC forums?. In Proceedings of the first
ACM conference on Learning@ scale conference
(pp. 117-126). ACM.
Kidd, C., Piantadosi, S. T., & Aslin, R. N. (2012). ?The
Goldilocks effect: Human infants allocate attention
to visual sequences that are neither too simple nor
too complex?. PLoS One, 7(5), e36399.
Kim, J., Guo, P. J., Seaton, D. T., Mitros, P., Gajos,
K. Z., & Miller, R. C. (2014a, March). ?Understand-
ing in-video dropouts and interaction peaks inonline
lecture videos?. In Proceedings of the first ACM con-
ference on Learning@ scale conference (pp. 31-40).
ACM.
Kim, J., Shang-Wen L., Carrie J. C., Krzysztof Z. G.,
Robert C. M. (2014b). ?Leveraging Video Interac-
tion Data and Content Analysis to Improve Video
Learning? CHI 2014 Workshop on Learning Inno-
vation at Scale
Kulkarni, C. E., Socher, R., Bernstein, M. S., & Klem-
mer, S. R. (2014, March). ?Scaling short-answer
grading by combining peer assessment with algo-
rithmic scoring?. In Proceedings of the first ACM
conference on Learning@ scale conference (pp. 99-
108). ACM.
Lang, A., John N., and Byron R. (1996). ?Negative
video as structure: Emotion, attention, capacity,
and memory?. Journal of Broadcasting & Electronic
Media, 40(4), 460-477
Lang, A. (2000). ?The limited capacity model of me-
diated message processing?. Journal of communica-
tion, 50(1), 46-70.
Lie M. T., Debjanee B., Judy K. (2014). ?Online learn-
ing at scale: user modeling requirements towards
motivation and personalisation?. In Learning Inno-
vations at Scale CHI? 14 Workshop
Miller Jr, Rupert G. (2011). ?Survival analysis?. Vol.
66. John Wiley & Sons
Nawrot, I., and Antoine D. (2014). ?Building engage-
ment for MOOC students: introducing support for
time management on online learning platforms.? In
Proceedings of the companion publication of the
23rd international conference on World wide web
companion, pp. 1077-1082.
13
North, S. M., Ronny R., and Max M. N. (2014). ?To
Adapt MOOCS, or Not? That is No Longer the
Question.? Universal Journal of Educational Re-
search 2(1): 69-72
Ramesh, A., Goldwasser, D., Huang, B., Daum H. III,
and Getoor, L. (2013). ?Modeling Learner Engage-
ment in MOOCs using Probabilistic Soft Logic?. In
NIPS Workshop on Data Driven Education
Schmidt, D. C., and Zach M. (2013). ?Producing and
Delivering a Coursera MOOC on Pattern-Oriented
Software Architecture for Concurrent and Net-
worked Software?
Sinha, T., Banka, A., Kang, D. K., (2013). ?Leveraging
user profile attributes for improving pedagogical ac-
curacy of learning pathways?. In Proceedings of 3rd
Annual International Conference on Education and
E-Learning(EeL 2013), Singapore
Sinha, T. (2014a). ?Together we stand, Together we
fall, Together we win: Dynamic team formation in
massive open online courses? In Fifth International
Conference on the Applications of Digital Informa-
tion and Web Technologies (ICADIWT) pp. 107-112.
IEEE
Sinha, T. (2014b). ?Supporting MOOC Instruction
with Social Network Analysis?. arXiv preprint
arXiv:1401.5175
Sinha, T. (2014c). ?Your click decides your fate?:
Leveraging clickstream patterns from MOOC videos
to infer students? information processing & attrition
behavior. arXiv preprint arXiv:1407.7143.
Sinha, T., Li, N., Jermann, P., Dillenbourg, P. (2014).
Capturing attrition intensifying structural traits from
didactic interaction sequences of MOOC learners.
Proceedings of the 2014 Empirical Methods in Nat-
ural Language Processing Workshop on Modeling
Large Scale Social Interaction in Massively Open
Online Courses, Qatar, October 2014
Tillmann, N., De Halleux, J., Xie, T., Gulwani, S., and
Bishop, J. (2013). ?Teaching and learning program-
ming and software engineering via interactive gam-
ing?. In ICSE, 11171126
Van der L., Mark PJ. (2014). ?The stringdist Package
for Approximate String Matching? The R Journal
Wang, G., Tristan K., Christo W., Xiao W., Haitao
Z., and Ben Y. Z. (2013). ?You are how you click:
Clickstream analysis for sybil detection? In USENIX
Security Symposium (Washington, DC)
Yang, D., Sinha T., Adamson D., and Rose C. P. (2013).
?Turn on, Tune in, Drop out: Anticipating student
dropouts in Massive Open Online Courses? In NIPS
Workshop on Data Driven Education
14
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 42?49,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Capturing ?attrition intensifying? structural traits from didactic
interaction sequences of MOOC learners
Tanmay Sinha
1
, Nan Li
2
, Patrick Jermann
3
, Pierre Dillenbourg
2
1
Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA 15213, USA
2
Computer-Human Interaction in Learning and Instruction, EPFL, CH 1015, Switzerland
3
Center for Digital Education, EPFL, CH 1015, Switzerland
1
tanmays@andrew.cmu.edu,
2,3
<firstname.lastname>@epfl.ch
Abstract
This work is an attempt to discover hidden
structural configurations in learning activ-
ity sequences of students in Massive Open
Online Courses (MOOCs). Leveraging
combined representations of video click-
stream interactions and forum activities,
we seek to fundamentally understand traits
that are predictive of decreasing engage-
ment over time. Grounded in the inter-
disciplinary field of network science, we
follow a graph based approach to success-
fully extract indicators of active and pas-
sive MOOC participation that reflect per-
sistence and regularity in the overall in-
teraction footprint. Using these rich edu-
cational semantics, we focus on the prob-
lem of predicting student attrition, one of
the major highlights of MOOC literature
in the recent years. Our results indicate an
improvement over a baseline ngram based
approach in capturing ?attrition intensify-
ing? features from the learning activities
that MOOC learners engage in. Implica-
tions for some compelling future research
are discussed.
1 Introduction
Massive Open Online Courses (MOOCs) have at-
tracted millions of students, and yet, their peda-
gogy is often less elaborated than the state of the
art in learning sciences. Scaling up learning activi-
ties in MOOCs can be viewed as a sacrifice of ped-
agogical support, made acceptable by the benefits
of giving broad access to education for a marginal
increase of costs. Even with students volunteering
as teaching assistants in MOOCs, it is not possible
to provide at a distance the same support quality in
a class of ten thousand as in a class of a hundred,
because of the difficulty to collect and analyse data
from such a high number of learners. This means
that MOOC instructors need to rely on rich com-
putational methods that capture the formalism of
how learners progress through the course and what
traits of decreasing engagement with the course
are predictive of attrition over time. The interpre-
tation of the state of the students can then either be
performed by the students themselves, by a human
coach or by an automated agent that can deliver
recommendations to the students.
In this work, we model the sequence of learning
activities in the MOOC as a graph with specific
properties. Describing the participants actions se-
quence as a graph may initially sound as a futile
complexity since most MOOCs are built as a sim-
ple linear sequence of activities (watch video, do
assignments, read forums). However, when look-
ing at the activity in more detail, some sequences
are richer and justify a more powerful descrip-
tive modeling. The descriptive power of the graph
model is to capture the underlying structure of the
learning activity. The hypothesis is that formaliz-
ing the workflow of such heterogeneous behavior
in MOOCs, is one solution to be able to a) scale up
learning activities that may initially appear as non
scalable, b) help instructors reason out how educa-
tional scenarios concretely unfold with time, such
as what happened during the course (at what times
were learners active and performing well, lost, dis-
oriented or trapped) and what needs to be repaired.
2 Related Work
In this section we outline perspectives on stu-
dent attrition that have been explored so far in the
literature on MOOCs. Much of this work suc-
cessfully leverages effective feature engineering
and advanced statistical methods. However, the
biggest limitation of most of these emerging works
is that they focus solely on discussion forum be-
havior or video lecture activity, but do not fuse
and take them into account. Some of these works
42
have grown out of research on predicting academic
progress of students and identifying students those
who are at dropout risk (Kotsiantis et al., 2003;
Dekker et al., 2009; Pal, 2012; M?arquez-Vera et
al., 2013; Manhaes et al., 2014).
Some prior research has focused on deriving so-
cial positioning metrics within discussion forums
to understand influencing factors that lead to dif-
ferently motivated behaviors of students. For ex-
ample, (Yang et al., 2013; Ros?e et al., 2014) used
aggregate post-reply discussion forum graph per
week, with an aim to investigate posting behavior
and collaborative aspects of participation through
operationalizations of social positioning. How-
ever, we work at a much finer granularity in the
current study and our focus is on individual stu-
dent modeling instead. We capture not only fo-
rum participation trajectory, but also video lecture
viewing activity of every student in their partici-
pation week. Modeling the combined interaction
footprint as an activity network, allows us to de-
cipher the type of engagement and organization of
behavior for each student, which are reflective of
attrition.
Similarly (Ramesh et al., 2014; Wen et al.,
2014a; Wen et al., 2014b) published results that
describe longitudinal discussion forum behavior
affecting student dropout, in terms of posting,
viewing, voting activity, level of subjectivity (cog-
nitive engagement) and positivity (sentiment) in
students? posts. Related to this, one recent work of
(Rossi and Gnawali, 2014) have made an attempt
to overcome the language dependency drawback
of these works and capture language indepen-
dent discussion forum features related to structure,
popularity, temporal dynamics of threads and di-
versity of students.
It is important to note, however, that all this sub-
stantial research caters to only about 5% of stu-
dents who participate in MOOC discussion forums
(Huang et al., 2014). Our recent work has laid a
preliminary foundation for research investigating
students? information processing behavior while
interacting with MOOC video lectures (Sinha et
al., 2014). We apply a cognitive video watching
model to explain the dynamic process of cogni-
tion involved in MOOC video clickstream interac-
tion and develop a simple, yet potent information
processing index that can be effectively used as an
operationalization for making predictions regard-
ing critical learner behavior, specifically in-video
and course dropouts. In an attempt to better under-
stand what features are predictive of students ceas-
ing to actively participate in the MOOC, (Veera-
machaneni et al., 2014) have integrated a crowd
sourcing approach for effective feature engineer-
ing at scale. Among posting, assignment and grad-
ing metrics, students? cohort membership depend-
ing on their MOOC engagement was identified as
an influential feature for dropout prediction.
3 Study Context
The current study is a part of the shared task
for EMNLP 2014 Workshop on Modeling Large
Scale Social Interaction in Massively Open On-
line Courses (Ros?e and Siemens, 2014). We have
both video clickstream data (JSON) and discus-
sion forum activity data (SQL) from one Cours-
era MOOC as training data, that we use in this
work. Our predictive models will also be tested
on 5 other Coursera MOOCs.
In general, Coursera forums, divided into var-
ious subforums, have a thread starter post that
serves as a prompt for discussion. The thread
builds up as people start following up discus-
sions by their posts and comments. As far as our
forum dataset is concerned, we have 31532 in-
stances of forum viewing and 35306 instances of
thread viewing. In addition to this view data, we
have 4840 posts and 2652 comments among 1393
threads initiated in the discussion forums during
the span of the course, which received 5060 up-
votes and 1763 downvotes in total.
To supplement the forum data, we additionally
leverage rich video interaction data from the click-
stream data. The clickstream data contains many
errors. We obtained 82 unique video ids from the
clickstream data, but only 45 of them are valid
(watched by large number of unique students).
The 37 invalid video ids may be simply due to log-
ging errors. They are also likely to be videos that
were uploaded by the course staff for testing pur-
poses. There are in total 27739 students registered
the course, however, only 14312 students had on-
line video interactions. The rest of the students
may have never logged in, or only have viewed the
course pages, or have downloaded the videos with-
out further online engagement. Among the 14312
students who have video interactions, 14264 of
them have valid video events logged, which lead
to 181100 valid video sessions for our analy-
sis. These valid video sessions further contain
43
462341 play events, 295103 pause events, 87585
forward jumps, 98169 backward jumps, 6707 for-
ward scrolls, 5311 backward scrolls, 18051 video-
play rate increase and 16163 decrease events, re-
spectively.
Our dropout prediction approach that will be de-
scribed in the next section is applied to student
interactions comprising of only online forum and
video viewing activities. Currently, we do not
make use of the pageview click data.
4 Technical Approach
1. To capture the behaviors exhibited in two pri-
mary MOOC activities, namely video lec-
ture viewing and forum interaction, we op-
erationalize the following metrics:
? Video lecture clickstream activi-
ties: Play (PL), Pause (PA), SeekFw
(FW), SeekBw (BW), ScrollFw (FS),
ScrollBw (BS), Ratechange Increase
(RCI), Ratechange Decrease (RCD).
When two seek events happen in < 1
second, we group them into a scroll.
We encode ratechange event based on
whether students sped up or slowed
down with respect to playrate of the last
click event.
? Discussion forum activities: Post (Po),
Comment (Co), Thread (Th), Upvote
(Uv), Downvote (Dv), Viewforum (Vf),
Viewthread (Vt)
2. Because timing of all such MOOC events are
logged in our data, we sort all these activities
by timestamp to obtain the sequence of activ-
ities done by students. This gives us a sim-
ple sequentially ordered time series that can
be used to reason about behavioral pattern of
students.
3. We form the interaction footprint sequence
for students by concatenating all their differ-
ent timestamped MOOC activities for every
week of MOOC activity. For example, if a
student watched a video (PL, PA, FW, RCI,
PA) at [time ?i?, week ?j?], viewed a forum at
time [?i+1?, week ?j?] and consequently made
a post at [time ?i+2?, week ?j?], his interaction
footprint sequence for week ?j? would be: PL
PA FW RCI PA Vf Po. Forming such a se-
quence captures in some essence, the cogni-
tive mind state that govern students? interac-
tion, as they progress through the MOOC by
engaging with these multiple forms of com-
puter mediated inputs. Most MOOCs are
based on a weekly rhythm with a new set of
videos and new assignments released every
week.
4. To find subsequences that might help us to
predict student dropout before it occurs, we
extract the following set of features for each
student in each of his participation weeks:
? N-grams from the interaction footprint
sequence (n = 2 to 5). Such ?n? consec-
utively occurring MOOC activities not
only characterize suspicious behaviors
that might lead to student attrition but
also help us to automatically determine
the elements of what might be consid-
ered ?best MOOC interaction practices?
that keep students engaged.
? Proportion of video viewing activities
among all video interactions, that are
active or passive. We define passive
video viewing as mere play and pause
(PL, PA), while rest of the video lecture
clickstream activities (FW, BW, FS, BS,
RCD, RCI) are considered elements of
active video viewing.
? Proportion of discussion forum activi-
ties among all forum interactions, that
are active or passive. We define passive
forum activities as viewing a forum or
thread (Vf, Vt), upvoting and downvot-
ing (Uv, Dv). The forum activities of
starting a thread (Th), posting (Po) and
commenting (Co) are indicative of ac-
tive forum interaction.
In general, because passive video lecture
viewing is high (for example, 48% of all
video clickstream activities in our dataset
comprise of activity sequences having only
PL event), discussion forum conversation
networks in MOOCs are sparse (only 10%
of forum activities relate to explicitly post-
ing, commenting or starting a thread) and
passive forum activities are very predominant
(90% of forum interactions in our dataset are
just passively viewing a thread/forum, upvot-
ing or downvoting), differentiating between
such active and passive forms of involvement
might clarify participation profiles that are
most likely to lead to disengagement of stu-
44
dents from the MOOC.
5. In an attempt to enrich the basic ngram rep-
resentation and better infer traits of active
and passive participation, we extract the fol-
lowing set of graph metrics from the over-
all interaction footprint sequence. Specifi-
cally, in this modeling scheme, we extract
consecutive windows of length two and cre-
ate a directed edge of weight one between
the activities appearing in sequential order.
This results in a directed graph (having self
loops and parallel edges), with nodes repre-
senting activities done by a student in particu-
lar week, while the weighted edges represent-
ing the frequencies of activities appearing af-
ter one another. For example, in a sequence,
(Vt Po Vt Po Po), corresponding nodes in the
graph are Vt and Po, while edges are (Vt,
Po), (Po, Vt), (Vt, Po) and (Po, Po). The
activity graph thus describes the visible part
of the educational activities (who does what
and when) and models the structure of activ-
ity sequences, rather than the details of each
activity. Features from the syntactic structure
of the graph along with their educational se-
mantics are described below.
? Number of nodes and edges: Indica-
tive of whether overall participation of
students in different MOOC activities is
high or low.
? Density: Graph density is a tight-
knittedness indicator of how involved
students are in different MOOC activ-
ities, how clustered their activities are
or how frequently they switch back and
forth between different activities. Tech-
nically, for a directed network, density =
m/n(n?1), where m=number of edges,
n=number of nodes. For our multidi-
graph representation, density can be >1,
because self loops are counted in the
total number of edges. This also im-
plies that values of density >1 denote
high persistence in doing particular set
of MOOC activities, because of greater
number of self loops.
? Number of self loops: Though graph
density provides meaningful interpreta-
tions when > 1, we can?t conclusively
infer activity persistence in an activity
graph with low density. So, we addition-
ally extract number of self loops to refer
to the regularity in interaction behavior.
? Number of Strongly Connected Com-
ponents (SCC): SCC define a special
relationship among a set of graph ver-
tices that can be exploited (each vertex
can be reached from every other vertex
in the component via a directed path). If
the number of SCC in an activity graph
are high, there is a high probability that
students performs certain set of activ-
ities frequently to successfully achieve
their desired learning outcomes in the
course. This might be an influential in-
dicator for behavioral organization and
continuity reflected in overall interac-
tion footprint of students. Dense net-
works are more likely to have greater
number of SCC.
? Central activity: We extract top three
activities of students with maximum in-
degree centrality, for each of their par-
ticipation weeks. Technically, indegree
centrality for a node ?v? is the fraction of
nodes its incoming edges are connected
to. Depending on which are the central
activities of students, we can character-
ize how active or passive is the partic-
ipation. For example, Viewthread and
Viewforum (Vt, Vf) are more passive
forms of participation than Upvote and
Downvote (Uv, Dv), which are in turn
more passive than Posting, Comment-
ing, Thread starting (Po, Co, Th) and
other intense forms of video lecture par-
ticipation that represent high grappling
with the course material.
? Central transition: We extract the edge
(activity transition) with maximum be-
tweenness centrality, which acts like a
facilitator in sustaining or decreasing
participation. Technically, betweenness
centrality of an edge ?e? is the sum of
the fraction of all-pairs shortest paths
that pass through ?e?. We normalize by
1/n(n ? 1) for our directed graphical
representation, where ?n? is the number
of nodes. For example, Vt-Po (view
thread-post) could be one of the central
edges for Th (thread starting activ-
ity), which in turn is a strong student
45
(a) Active video viewing (b) Passive video viewing (c) Active forum activity (d) Passive forum activity
Figure 1: Interaction graphs representing 4 contrasting MOOC scenarios in our dataset
participation indicator. Alternately,
Po/Co/Th-Dv (post/comment/thread
initiate-downvote) could serve as
decision conduits that increase dis-
satisfaction of students because of
others? off content/off-conduct post-
ing. Such lack of exposure to useful
and informative posts on forums can
potentially aggravate feelings of ?lack
of peer support? and ?healthy commu-
nity involvement?, inturn leading to
decreasing engagement.
6. We add certain control variables in our fea-
ture set to account for inherently present
student characteristics, namely courseweek
(number of weeks since the course has been
running), userweek (number of weeks since
the student joined the course) and a nominal
variable indicating whether student activity
in a week comprised of only video lecture
viewing, only forum activity, both or none.
Because we are interested in investigating a)how
behavior within a week affects students? dropout
in the next course week, b)how cumulative be-
havior exhibited up till a week affects students?
dropout in the next course week, we create two
experimental setups: one using data from the cur-
rent participation week (Curr) and the second us-
ing data from the beginning participation week till
the current week (TCurr). For the second setup,
all feature engineering is done from the cumula-
tive interaction footprint sequence.
Some of the interaction graphs culled out from
the footprint sequence, which are representative
of active and passive MOOC participation are de-
picted in figure 1. Each graph has a begin (Be)
and end (En) node, with nodes sized by indegree
centrality and directed edges sized by tie strength.
5 Results
5.1 Evaluating Our Features
As we would intuitively expect, mean and stan-
dard deviations for all our extracted graph metrics
are higher in the TCurr setup. Another evident
pattern is that all these graph metrics follow long
tailed distributions for both Curr and TCurr se-
tups, with very few students exhibiting high val-
ues. These distributions concur with the 90-9-1
rule in online communities which says that 90%
of the participants only view content (for example,
watch video, Vf, Vt), 9% of the participants edit
content (for example, Uv, Dv), and 1% of the par-
ticipants actively create new content (for example,
Po, Co, Th). Moreover, we notice that the top three
central activities with maximum frequency and
central edges that describe interactions between
them, are passive interaction events. Among the
top 20, we can observe central edges such as RCI-
RCI or PL-FW that hint towards skipping video
and hence decreasing participation, while Th-PL,
Po-PL, Th-Po that point towards facilitating par-
ticipation. Thus, in order to graphically visualize
interactions among features and their relationship
to the class distribution (dropout and non dropout),
we utilize mosaic plot representation. The mo-
tivating question being two-fold: a)How do the
extracted features vary among dropouts and non
dropouts? b)When viewing more than one features
together, what can we say about association of dif-
ferent feature combinations to survival of students
in the MOOC? After ranking feature projections
on basis of interaction gain (in % of class entropy
removed), we discern the following:
? For both Curr and TCurr setups, the mosaic
plots reveal that dropout is higher for students
having low number of nodes, edges, SCC and
self loops, low activity graph density, low
46
Model Performance Metric Setup Curr Setup TCurr
1. Baseline Accuracy/Kappa 0.623/0.297 0.647/0.173
False Negative Rate 0.095 0.485
2. Graph Accuracy/Kappa 0.692/0.365 # 0.693/0.277 #
False Negative Rate 0.157 0.397
3. Baseline + Graph Accuracy/Kappa 0.624/0.298 0.646/0.173
False Negative Rate 0.095 0.482
Table 1: Performance metrics for machine learning experiments. Random classifier performance is 0.5.
Values marked # are significantly better (p<0.01, pairwise t-test) than other results in same column
proportion of active forum and video viewing
activity. This reflects that our operationaliza-
tions drawn from overall interaction footprint
are successfully able to capture features ex-
pressing student behavior that might escalate
attrition.
? Student dropout is higher if they join in
later course weeks and have a sparse activ-
ity graph. There could be 2 possible expla-
nations: a)Students join later and do min-
imal activity because they only have spe-
cific information needs. So, they do not
stay after interacting with the course mate-
rial in a short non linear fashion and satisfy-
ing their needs, b)Students who join later are
overwhelmed with lots of introductory and
prerequisite MOOC video lectures to watch,
pending assignments to be completed to suc-
cessfully pass the course and discussion fo-
rum content already posted. Finding diffi-
culty in coping up with the ongoing pace of
the MOOC, they do not stay for prolonged
periods in the course.
5.2 Dropout Prediction and Analysis
We leverage machine learning techniques to pre-
dict student attrition along the way based on our
extracted feature set. The dependent class variable
is dropout, which is 0 for all active student partic-
ipation weeks and 1 only for the last participation
week (student ceased to participate in the MOOC
after that week), leading to an extremely skewed
class distribution. Note that by active student par-
ticipation, we refer to only forum and video view-
ing interactions. We construct the following two
models for validation. For each model, there is a
Curr and a TCurr setup:
? Baseline Ngram Model: Features used are
Coursweek, Userweek, Ngrams from full in-
teraction footprint sequence (2 to 5), Ngram
length, proportion of active/passive video
viewing and forum activity (dichotomized by
equal width), nominal variable.
? Graph Model: Features used are Cour-
sweek, Userweek, Ngram length, Graph met-
rics (top 3 central activities, density (di-
chotomized by equal frequency), central tran-
sition, no. of nodes (dichotomized by equal
frequency), no. of edges (dichotomized by
equal frequency), no. of self loops (di-
chotomized by equal frequency), no. of
SCC), nominal variable.
For both these models, we use cost sensitive Lib-
SVM with radial basis kernel function (RBF) as
the learning algorithm (Hsu et al., 2003). The ad-
vantage of RBF is that it nonlinearly maps sam-
ples into a higher dimensional space so it, unlike
the linear kernel, can handle the case when the re-
lation between class labels and attributes is non-
linear. Rare threshold for feature extraction is set
to 4, while cross validation is done using a sup-
plied test set with held out students having sql id
798619 through 1882807.
The important take away messages from these
results are:
? Graph model performs significantly better
than Baseline ngram model for both Curr
(t=-17.903, p<0.01) and TCurr (t=-11.834,
p<0.01) setups, in terms of higher accu-
racy/kappa and comparable false negative
rates
1
. This is because the graph models
the integration of heterogeneous MOOC ac-
tivities into a structured activity. The edges
of the graph, which connect consecutive ac-
tivities represent a two-fold relationship be-
tween these activities: how they relate to each
1
False negative rate of 0.x means that we correctly iden-
tify (100-(100*0.x))% of dropouts
47
other from a pedagogical and from an oper-
ational viewpoint. In addition to capturing
just the order and mere presence of active and
passive MOOC events scatterred throughout
the activity sequence, the activity network
representation additionally captures differ-
ent properties of MOOC interaction such as
a)how recurring behaviors develop in the par-
ticipation trajectory of students, and how the
most central ones thrust towards increasing
or decreasing engagement, b)how the num-
ber and distribution of such activities are in-
dicative of persistence in interaction behav-
ior. The baseline+graph approach does not
lead to improvement in results over the base-
line approach.
? TCurr setup does not necessarily lead to bet-
ter results than Curr setup. This indicates
that students? attrition is more strongly in-
fluenced by the most recent week?s exhib-
ited behavioral patterns, rather than aggre-
gated MOOC interactions from the begin-
ning of participation. The extremely small
false negative rates in Curr setup indicate the
effectiveness of our feature engineering ap-
proach in predicting attririon behavior, even
with an extremely skewed class distribution.
However, more studies would be required to
corroborate the relation between change in
interaction sequences from one week to an-
other and factors such as students? confusion
(?I am unable to follow the course video lec-
tures?) or negative exposure (?I am not moti-
vated enough to engage because of less pro-
ductive discussion forums?), which gradually
build up like negative waves before dropout
happens (Sinha, 2014).
6 Conclusion and Future Work
In this work, we formed operationalizations that
quantify active and passive participation exhibited
by students in video lecture viewing and discus-
sion forum behavior. We were successful in de-
veloping meaningful indicators of overall inter-
action footprint that suggest systematization and
continuity in behavior, which are in turn predictive
of student attrition. In our work going forward,
we seek to differentiate the interaction footprint
sequences further using potent markov clustering
based approaches. The underlying motivation is to
decipher sequences having lot of activity overlap
as well as similar transition probabilities. These
cluster assignments can then serve as features that
help segregating interaction sequences predictive
of dropout versus non-dropouts.
Another interesting enhancement to our work
would include grouping commonly occurring ac-
tivities that learners perform in conjunction with
each other and form higher level latent cate-
gories indicative of different participation traits.
In our computational work, we have recently been
developing techniques for operationalizing video
lecture clickstreams of students into cognitively
plausible higher level behaviors to aid instructors
to better understand MOOC hurdles and reason
about unsatisfactory learning outcomes (Sinha et
al., 2014).
One limitation of the above work is that we
are concerned merely with the timestamped order
of activities done by a student and not the time
gap between activities appearing in the interac-
tion footprint sequence. The effect of an activ-
ity on a subsequent activity often fades out with
time, i.e. as the lag between two activities in-
creases: learners forget what they learned in a pre-
vious activity. For example, the motivation cre-
ated at the beginning of a lesson by presenting an
interesting application example does not last for-
ever, so as to initiate productive forum discussions.
Similarly, the situation of a thread being started
(Th) and a post being made (Po) within 60 secs
of completing video lecture viewing, might im-
ply a different behavior, than if these forum activ-
ities occur five days after video lecture viewing.
Therefore, we seek to better understand context
of the most and least central activities of students
in MOOCs, differentiating between subsequences
lying within and outside user specified temporal
windows. Our goal is to view the interaction foot-
print sequence formation in a sequential data min-
ing perspective (Mooney and Roddick, 2013) and
discover a)most frequently occurring interaction
pathways that lead students to such central activ-
ities, b)association rules with high statistical con-
fidences that help MOOC instructors to trace why
students engage in certain MOOC activities. For
example, a rule of the form AB? C, such as ?Vf?,
?Uv? [15s] ? ?Po? [30s] (confidence = 0.7), is
read as if a student navigated and viewed a forum
page followed by doing an upvote within 15 sec-
onds, then within the next 30 seconds he would
make a post 70% of the time.
48
References
Dekker, G. W., Pechenizkiy, M., & Vleeshouwers,
J. M. (2009). ?Predicting Students Drop Out: A
Case Study?. International Working Group on Ed-
ucational Data Mining.
Huang, J., Dasgupta, A., Ghosh, A., Manning, J., and
Sanders, M. 2014. ?Superposter behavior in MOOC
forums?. ACM Learing at Scale(L@S)
Hsu, C. W., Chang, C. C., & Lin, C. J. (2003). ?A prac-
tical guide to support vector classification?
Kotsiantis, S. B., Pierrakeas, C. J., & Pintelas, P. E.
(2003, January). ?Preventing student dropout in dis-
tance learning using machine learning techniques?.
In Knowledge-Based Intelligent Information and
Engineering Systems (pp. 267-274). Springer Berlin
Heidelberg.
Manhaes, L. M. B., da Cruz, S. M. S., & Zimbrao, G.
(2014, March). ?WAVE: an architecture for predict-
ing dropout in undergraduate courses using EDM?.
In Proceedings of the 29th Annual ACM Symposium
on Applied Computing (pp. 243-247). ACM.
M?arquez-Vera, C., Cano, A., Romero, C., & Ventura,
S. (2013). ?Predicting student failure at school us-
ing genetic programming and different data mining
approaches with high dimensional and imbalanced
data?. Applied intelligence, 38(3), 315-330.
Mooney, C. H., & Roddick, J. F. (2013). ?Sequential
pattern mining?approaches and algorithms?. ACM
Computing Surveys (CSUR), 45(2), 19.
Pal, S. (2012). ?Mining educational data to reduce
dropout rates of engineering students?. International
Journal of Information Engineering and Electronic
Business (IJIEEB), 4(2), 1.
Ramesh, A., Goldwasser, D., Huang, B., Daume III,
H., & Getoor, L. (2014, June). ?Learning latent en-
gagement patterns of students in online courses?. In
Twenty-Eighth AAAI Conference on Artificial Intel-
ligence.
Ros?e, C. P., Carlson, R., Yang, D., Wen, M., Resnick,
L., Goldman, P., & Sherer, J. (2014, March). ?Social
factors that contribute to attrition in moocs. In Pro-
ceedings of the first ACM conference on Learning@
scale conference (pp. 197-198). ACM.
Ros?e, C. P., Siemens, G. (2014). ?Shared Task on Pre-
diction of Dropout Over Time in Massively Open
Online Courses?, Proceedings of the 2014 Empirical
Methods in Natural Language Processing Workshop
on Modeling Large Scale Social Interaction in Mas-
sively Open Online Courses, Qatar, October, 2014.
Rossi, L. A., & Gnawali, O. ?Language Independent
Analysis and Classification of Discussion Threads
in Coursera MOOC Forums?.
Sinha, T., Jermann, P., Li, N., Dillenbourg, P. (2014).
?Your click decides your fate: Inferring Informa-
tion Processing and Attrition Behavior from MOOC
Video Clickstream Interactions?. Proceedings of the
2014 Empirical Methods in Natural Language Pro-
cessing Workshop on Modeling Large Scale So-
cial Interaction in Massively Open Online Courses,
Qatar, October, 2014.
Sinha, T. (2014). ?Who negatively influences me? For-
malizing diffusion dynamics of negative exposure
leading to student attrition in MOOCs?. LTI Student
Research Symposium, Carnegie Mellon University
Veeramachaneni, K., O?Reilly, U. M., & Taylor, C.
(2014). ?Towards Feature Engineering at Scale for
Data from Massive Open Online Courses?. arXiv
preprint arXiv:1407.5238.
Wen, M., Yang, D., & Ros?e, C. P. (2014a). ?Linguis-
tic Reflections of Student Engagement in Massive
Open Online Courses?. In Proceedings of the Inter-
national Conference on Weblogs and Social Media
Wen, M., Yang, D., & Ros?e, C. P. (2014b). ?Sentiment
Analysis in MOOC Discussion Forums: What does
it tell us??. In Proceedings of Educational Data Min-
ing
Yang, D., Sinha T., Adamson D., and Rose, C. P. 2013.
?Turn on, Tune in, Drop out: Anticipating student
dropouts in Massive Open Online Courses? In NIPS
Workshop on Data Driven Education
49
