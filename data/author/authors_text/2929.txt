Multi-Human Dialogue Understanding for Assisting
Artifact-Producing Meetings
John Niekrasz and Alexander Gruenstein and Lawrence Cavedon
Center for the Study of Language and Information (CSLI)
Stanford University
Cordura Hall, Stanford, CA, 94305-4115, USA
http://www-csli.stanford.edu/semlab/
{niekrasz, alexgru, lcavedon}@csli.stanford.edu
Abstract
In this paper we present the dialogue-
understanding components of an architec-
ture for assisting multi-human conversa-
tions in artifact-producing meetings: meet-
ings in which tangible products such as
project planning charts are created. Novel
aspects of our system include multimodal
ambiguity resolution, modular ontology-
driven artifact manipulation, and a meeting
browser for use during and after meetings.
We describe the software architecture and
demonstrate the system using an example
multimodal dialogue.
1 Introduction
Recently, much attention has been focused on
the domain of multi-person meeting under-
standing. Meeting dialogue presents a wide
range of challenges including continuous multi-
speaker automatic speech recognition (ASR),
2D whiteboard gesture and handwriting recog-
nition, 3D body and eye tracking, and multi-
modal multi-human dialogue management and
understanding. A significant amount of re-
search has gone toward understanding the prob-
lems facing the collection, organization, and
visualization of meeting data (Moore, 2002;
Waibel et al, 2001), and meeting corpora like
the ICSI Meeting Corpus (Janin et al, 2003) are
being made available. Continuing research in
the multimodal meeting domain has since blos-
somed, including ongoing work from projects
such as AMI1 and M42, and efforts from sev-
eral institutions.
Previous work on automatic meeting un-
derstanding has mostly focused on surface-
level recognition, such as speech segmentation,
for obvious reasons: understanding free multi-
human speech at any level is an extremely diffi-
cult problem for which best performance is cur-
rently poor. In addition, the primary focus for
1http://www.amiproject.org/
2http://www.m4project.org/
applications has been on off-line tools such as
post-meeting multimodal information browsing.
In parallel to such efforts we are applying
dialogue-management techniques to attempt to
understand and monitor meeting dialogues as
they occur, and to supplement multimodal
meeting records with information relating to the
structure and purpose of the meeting.
Our efforts are focused on assisting artifact-
producing meetings, i.e. meetings for which the
intended outcome is a tangible product such as
a project management plan or a budget. The
dialogue-understanding system helps to create
and manipulate the artifact, delivering a final
product at the end of the meeting, while the
state of the artifact is used as part of the dia-
logue context under which interpretation of fu-
ture utterances is performed, serving a num-
ber of useful roles in the dialogue-understanding
process:
? The dialogue manager employs generic di-
alogue moves with plugin points to be de-
fined by specific artifact types, e.g. project
plan, budget;
? The artifact state helps resolve ambiguity
by providing evidence for multimodal fu-
sion and constraining topic-recognition;
? The artifact type can be used to bias ASR
language-models;
? The constructed artifact provides a inter-
face for a meeting browser that supports
directed queries about discussion that took
place in the meeting, e.g. ?Why did we
decide on that date??
In addition, we focus our attention on the
handling of ambiguities produced on many
levels, including those produced during au-
tomatic speech recognition, multimodal com-
munication, and artifact manipulation. The
present dialogue manager uses several tech-
niques to do this, including the maintenance of
Multimodal
Integrator
3-D
Gesture
Recognizer
2-D
Drawing
Recognizer
ASR
CIA
NL
Parser
Information State
Ontology
KB DMT,
Active Node,
Salience List,
etc.
Dialogue
Manager
Meeting Browser
Hypothesis
Repository (CMU)
(OGI)
(OGI)
(MIT)
Figure 1: The meeting assistant architecture,
highlighting the dialogue-management compo-
nents.
multiple dialogue-move hypotheses, fusion with
multimodal gestures, and the incorporation of
artifact-specific plug-ins.
The software architecture we use for manag-
ing multi-human dialogue is an enhancement of
a dialogue-management toolkit previously used
at CSLI in a range of applications, including
command-and-control of autonomous systems
(Lemon et al, 2002) and intelligent tutoring
(Clark et al, 2002). In this paper, we detail the
dialogue-management components (Section 3),
which support a larger project involving mul-
tiple collaborating institutions (Section 2) to
build a multimodal meeting-understanding sys-
tem capable of integrating speech, drawing and
writing on a whiteboard, and physical gesture
recognition.
We also describe our toolkit for on-line and
off-line meeting browsing (Section 4), which al-
lows a meeting participant, observer, or devel-
oper to visually and interactively answer ques-
tions about the history of a meeting, the pro-
cesses performed to understand it, and the
causal relationships between dialogue and ar-
tifact manipulation.
2 Meeting Assistant Architecture
The complete meeting assistant architecture is
a highly collaborative effort from several insti-
tutions. Its overall architecture, focusing on our
contributions to the system is illustrated in Fig-
ure 1.
The components for drawing and writing
recognition and multimodal integration (Kaiser
et al, 2003) were developed at The Oregon
Graduate Institute (OGI) Center for Human-
Computer Communication3; the component for
physical gesture recognition (Ko et al, 2003)
was developed at The Massachusetts Institute
of Technology (MIT) AI Lab4. Integration be-
tween all components was performed by project
members at those sites and at SRI Interna-
tional5, and integration between our CSLI Con-
versational Intelligence Architecture and OGI?s
Multimodal Integrator (MI) was performed by
members of both teams. ASR is done using
CMU Sphinx6, from which the n-best list of re-
sults are passed to SRI?s Gemini parser (Dowd-
ing et al, 1993). Gemini incorporates a suite
of techniques for handling noisy input, includ-
ing fragment detection, and its dynamic gram-
mar capabilities are used to register new lexical
items, such as names of tasks that may be out-
of-grammar.
An example of a multimodal meeting conver-
sation that the meeting assistant currently sup-
ports can be found in Figure 2.7 There are two
meeting participants in a conference room with
an electronic whiteboard which can record their
pen strokes and a video camera that tracks their
body movements; A is standing at the white-
board and drawing while B is sitting at the
table. A gloss of how the system behaves in
response to each utterance and gesture follows
each utterance; these glosses will be explained
in greater detail throughout the rest of the pa-
per. The drawing made on the whiteboard is
in Figure 3(a), and the chart artifact as it was
constructed by the system is displayed in Figure
3(b).
3 Conversational Intelligence
Architecture
To meet the challenges presented by multi-
person meeting dialogue, we have extended
and enhanced our previously used Conversa-
tional Intelligence Architecture (CIA). The CIA
is a modular and highly configurable multi-
application system: a separation is made be-
tween generic dialogue processes and those spe-
cific to a particular domain. Creating a new
application may involve writing new dialogue
moves and configuring the CIA to use these. We
3http://www.cse.ogi.edu/CHCC/
4http://www.ai.mit.edu/
5http://www.sri.com/
6http://www.speech.cs.cmu.edu/sphinx/
7A video demonstration will be available soon at
http://www-csli.stanford.edu/semlab/calo/
A: So, lets uh figure out what uh needs uh needs to be done. Let?s
look at the schedule. [draws a chart axes] utterance and gesture
information fused, a new milestone chart artifact is created
B: So, if all goes well, we?ve got funding for five years. system sets
unit on axis to ?years?
A: Yeah. Let?s see one, two ... [draws five tick marks on the x-axis]
system assumes tick marks are years
B: Well, the way I see it, uh we?ve got three tasks. dialogue man-
ager hypothesizes three tasks should be added, waits for multimodal
confirmation
A: Yeah right [draws three task lines horizontally on the axis] multi-
modal confirmation is given, information about task start and end
dates is fused from the drawing
A: Let?s call this task line demo [touches the top line with the
pen], call this task line signoff [touches the middle line with the
pen], and call this task line system [touches the bottom line with
the pen]. each utterance causes the dialogue manager to hypoth-
esize three distinct hypotheses, in each task a different hypothesis
is named, the gestures disambiguate these in the multimodal inte-
grator
B: So we have two demos to get done.
A: uh huh
B: Darpatech is at the end of month fifteen [A draws a diamond
at month fifteen on the demo task line] dialogue manager hy-
pothesizes a milestone called ?darpatech? at month fifteen; gesture
confirms this and pinpoints appropriate task line
B: And the final demonstrations are at the end of year five [A draws
a diamond at year five on the demo task line] same processing as
previous
A: Hmm, so when do the signoffs need to happen do you think?
dialogue manager expects next utterance to be an answer
B: Six months before the demos [A draws two diamonds on the sig-
noff task line, each one about 6 months before the demo mile-
stones drawn above] answer arrives; dialogue manager hypothe-
sizes two new milestones which are confirmed by gesture
A: And we?ll need the systems by then too [A draws two diamonds
on the system task line] dialogue manager hypothesizes two more
milestones, confirmed by gesture
B: That?s a bit aggressive I think. Let?s move the system milestone
back six months. [B points finger at rightmost system milestone.
A crosses it out and draws another one six months earlier] di-
alogue manager hypothesizes a move of the milestone, 3D gesture
and drawing confirm this
Figure 2: Example conversation understood by
the system.
(a) The whiteboard in-
put captured by OGI?s
Charter gesture recog-
nizer
(b) The artifact as
maintained in the
dialogue system
Figure 3: Ink-captured vs ?idealized? artifact
output.
have successfully used this ?toolkit? approach
in our previous applications at CSLI to inter-
face novel devices without modifying the core
dialogue manager.
The present application is however very dif-
ferent to our previous applications, and those
commonly encountered in the literature, which
typically involve a single human user interact-
ing with a dialogue-enabled artificial agent. In
the meeting environment, the dialogue manager
should at most very rarely interpose itself into
the discussion?to do so would be disruptive
to the interaction between the human partic-
ipants. This requirement prohibits ambiguity
and uncertainty from being resolved with, say,
a clarification question, which is the usual strat-
egy in conversational interfaces. Instead, uncer-
tainty must be maintained in the system until
it can be resolved by leveraging context, using
evidence from another modality, or by a future
utterance.
The meeting-understanding domain has thus
prompted several extensions to our existing
CIA, many of which we expect will be applied
in other conversational domains. These include:
? Support for handling multiple competing
speech parses; (Section 3.2)
? A generic artifact ontology which enables
designing generically useful artifact-savvy
dialogue applications; (Section 3.3)
? Support for the generation and subsequent
confirmation of dialogue-move hypotheses
in a multimodal integration framework;
(Section 3.4)
? The acceptance of non-verbal unimodal
gestures into the dialogue-move repertoire.
(Section 3.5)
? A preliminary mechanism for supporting
uncertainty across multiple conversational
moves; (Section 3.6)
Before discussing these new features in detail,
the following section introduces the CIA and
its persisting core dialogue-management com-
ponents.
3.1 Core Components: Information
State and Context
The core dialogue management components of
the CIA maintain dialogue context using the
information-state and dialogue-move approach
(Larsson and Traum, 2000) where each con-
tributed utterance modifies the current context,
or information state, of the dialogue. Each new
utterance is then interpreted within the current
context (see (Lemon et al, 2002) for a detailed
description).
A number of data structures are employed
in this process. The central dialogue state-
maintaining structure is the Dialogue Move Tree
(DMT). The DMT represents the historical con-
text of a dialogue. An incoming utterance, clas-
sified by dialogue move, is interpreted in con-
text by attaching itself to an appropriate active
node on the DMT; e.g., an answer attaches to
an active corresponding question node. Cur-
rently, active nodes are kept on an Active Node
List , which is ordered so that those most likely
to be relevant to the current conversation are
at the front of the list. Incoming utterances
are displayed to each node in turn, and at-
tach to the first appropriate node (determined
by information-state-update functions). Other
structures include the context-specific Salience
List , which maintains recently used terms for
performing anaphora resolution, and a Knowl-
edge Base containing application specific infor-
mation, which may be leveraged to interpret in-
coming utterances.8
We now present the various enhancements
made to the CIA for use in the meeting domain.
3.2 ASR and Robust Parsing
The first step in understanding any dialogue is
recognizing and interpreting spoken utterances.
In the meeting domain, we are presented with
the particularly difficult task of doing this for
spontaneous human-human speech. We there-
fore chose to perform ASR using a statisti-
cal language model (LM) and employ CMU?s
Sphinx to generate an n-best list of recogni-
tion results. The recognition engine uses a tri-
gram LM trained on the complete set of pos-
sible utterances expected given a small hand-
crafted scenario like that in the example dia-
logue. Despite the task?s limited domain, the re-
alized speech is very disfluent, generating an ex-
tremely broad range of possible utterances that
the system must handle. The resulting n-best
list is therefore often extremely varied.
To handle the ASR results of disfluent utter-
ances, we employ SRI?s Gemini robust language
parser (Dowding et al, 1993). In particular,
we use Gemini to retrieve the longest strings
of valid S and NP fragments in each ASR re-
sult. Currently, we reject all but the parsed S
fragments?and NP fragments when expected
8Command-and-control applications have also made
use of an Activity Tree, which represents activities being
carried out by the dialogue-enabled device (Gruenstein,
2002); however, this application currently makes no use
of this.
by the system (e.g. an answer to a question
containing an NP gap). The parser uses generic
syntactic rules, but is constrained semantically
by sorts specific to the domain. In Section 3.4,
we describe how the dialogue manager handles
the multiple parses for a single utterance and
how it uses the uncertainty they represent.
3.3 Artifact Knowledge Base and
Ontology
In the present version of the CIA, all static do-
main knowledge about meeting artifacts is de-
fined in a modularized class-based ontology. In
conjunction with the ontology, we also maintain
a dynamic knowledge base (KB) which holds
the current state of any artifacts. This is stored
as a collection of instances of the ontological
classes, and both components are maintained
together using the Prote?ge?-20009 ontology and
knowledge-base toolkit (Grosso et al, 1999).
The principal base classes in the artifact on-
tology are designed to be both architecturally
elegant and intuitive. To this end, we charac-
terize the world of artifacts as being made up
of three essential classes: entities which repre-
sent the tangible objects themselves, relations
which represent how the entities relate to one
another, and events which change the state of
entities or relations. Events are the most im-
portant tool aiding the dialogue management
algorithm. They comprehensively characterize
the set of actions which can change the current
state of an artifact. They may be classified into
three categories: insert changes which insert a
new entity or relation instance into the KB, re-
move changes which remove an instance, and
value changes which modify the value of a slot
in an instance. All changes to the KB can be
characterized as one of these three atomic events
or a combination of them.
3.4 Hypothesizers: A plugin
architecture for artifact-driven
multimodal integration
Abmiguities and uncertainties are both ram-
pant in multimodal meeting dialogues, and in
artifact-producing meetings, the majority per-
tain to artifacts and the utterances performed
to change them. In this section we explain how
the CIA?s dialogue manager uses the artifact on-
tology, and the repertoire of event classes in it,
to formulate sets of artifact-changing dialogue-
move hypotheses from single utterances. We
9http://protege.stanford.edu/
also demonstrate how it uses the current state
of the artifact in the KB to constrain the in-
terpretation of utterances in context, and how
multimodal gestures help to resolve ambiguous
interpretations.
To begin, each dialogue-move hypothesis con-
sists of the following elements: (1) the DMT
node associated with this hypothesis, (2) the
parse that gave rise to the hypothesis, (3) the
probability of the hypothesis, (4) an isUnimodal
flag indicating whether or not the dialogue move
requires confirmation from other modalities, (5)
a list of artifact-change events to be made to
the KB, and (6) the information state update
function to be invoked if this hypothesis is con-
firmed by the multimodal integrator. Each of
these elements participate in the generation and
confirmation process as detailed below.
First, consider the utterance Darpatech is at
the end of month fifteen. from the example dia-
logue. This utterance is much more likely to
indicate the creation of a new milestone if a
task line is pertinent to the current dialogue
context, e.g. the user has just created a new
task line. In our system, the ambiguous or un-
certain utterance, the current dialogue context,
and the current state of the chart is delegated to
artifact-type specific components called hypoth-
esizers. Hypothesizers take the above as input,
and using the set of events available to its cor-
responding artifact in the ontology, they pro-
grammatically generate a list of dialogue-move
hypotheses appropriate in the given context?
or they can return the empty list to indicate
that there is no reasonable interpretation of the
utterance given the current context.
Hypothesizers work directly with the DMT
architecture: as an incoming utterance is se-
quentially presented to each active node in the
DMT, the dialogue context and the proposed
active node are passed into a hypothesizer cor-
responding to the particular artifact associated
with that node. If the hypothesizer can create
one or more valid hypotheses, then the utter-
ance is attached to the DMT as a child of that
active node.10
In a multimodal domain, some hypotheses re-
quire confirmation in other modalities before
the dialogue manager can confidently update
10There are, in fact, other rules as well which allow for
attachment. For example, questions?which don?t im-
mediately generate hypotheses?can also be attached to
various nodes depending on the dialogue context. While
the emphasis here is on hypothesizers, these are just one
part of the dialogue processing toolkit
the information state. In this particular system,
in fact, the dialogue manager does not directly
update the KB?s current artifact state; rather, it
hypothesizes a set of dialogue-move hypotheses
and assigns each a confidence derived from ASR
confidence, the fragmentedness of the parse, and
confidence in the proposed attachment to a con-
versational thread. Each conversational move is
then provided a Hypothesis Repository for stor-
ing the hypotheses associated with it. When
dialogue processing is completed for a partic-
ular conversational move, i.e. when all pos-
sible attachments of all possible parses on the
n-best list have been made, the set of hypothe-
ses is sent to the Multimodal Integrator (MI)
for potential fusion with gesture. Depending on
the information from other modalities, the MI
confirms or rejects the hypotheses?moreover, a
confirmed hypothesis might be augmented with
information provided by other modalities. Such
an augmentation occurs for the utterance We
have three tasks from the example dialogue. In
this situation, the dialogue manager hypothe-
sizes that the user may be creating three new
task lines on the chart. When the user actually
draws the three task lines, the MI infers the
start and stop date based on where the lines
start and stop on the axis. In this case, it not
only confirms the dialogue manager?s hypoth-
esis, but augments it to reflect the additional
date information yielded from the whiteboard
input.
3.5 Unimodal Gestures
In addition to the Information State updates
based on both speech and gesture, multimodal
meeting dialogue can often include gestures in
which a participant makes a change to an ar-
tifact using a unimodal gesture not associated
with an utterance. For example, a user may
draw a diamond on a task line but say nothing.
Even in the absence of speech, this can be unam-
biguously understood as the creation of a mile-
stone at a particular point on the line. These
unimodally produced changes to the chart must
be noted by the dialogue manager, as they are
potential targets for later conversation. To ac-
commodate this, we introduce a new DMT node
of type Unimodal Gesture, thus implicitly in-
cluding gesture as a communicative act that can
stand on its own in a conversation
3.6 Uncertain DMT Node Attachment
Since hypotheses are not always immediately
confirmed, uncertainty must be maintained
Figure 4: A snapshot from the meeting browser.
across multiple dialogue moves. The system ac-
complishes this by extending the CIA to main-
tain multiple competing Information States. In
particular, the DMT has been extended to al-
low for the same parse to attach in multiple
locations?these multiple attachments are even-
tually pruned as more evidence is accumulated
in the form of further speech or gestures?that
is, as hypotheses are confirmed or rejected over
time.
4 Meeting Viewer Toolkit
Throughout an artifact-producing meeting, the
dialogue system processes a complex chronolog-
ical sequence of events and information states
that form structures rich in information useful
to dialogue researchers and the dialogue partic-
ipants themselves. To harness the power of this
information, we have constructed a toolkit for
visualizing and investigating the meeting infor-
mation state and its history.
Central to the toolkit is our meeting history
browser, which can be seen in Figure 4, dis-
playing a portion of the example dialogue, with
the results of a search for ?demo? highlighted.
This record of the meeting is available both dur-
ing the meeting and afterwards to assist users
in answering questions they might have about
the meeting. Many kinds of questions can be
answered in the browser, like those a manager
might ask the day after a meeting: ?Why did we
move the deadline on that task 6 months later??,
?Did I approve setting that deadline so early??,
and ?What were we thinking when we put that
milestone at month fifteen??. A meeting partic-
ipant might have questions as the meeting oc-
curs, like ?What did the chart look like 5 min-
utes ago??, ?What did we say to make the sys-
tem move that milestone??, and ?What did Mr.
Smith say at the beginning of the meeting??.
To help answer these questions, the browser
performs many of the functions found in current
multimodal meeting browsers. For example,
it provides concise display of a meeting tran-
scription, advanced searching capabilities, sav-
ing and loading of meeting sessions, and person-
alization of its own display characteristics. As
a novel addition to these basic behaviors, the
browser is also designed to display artifacts and
the causal relationships between artifacts and
the utterances that cause them to change.
To effectively convey this information, the
record of components monitored by the history
toolkit is presented to the user through a win-
dow which chronologically displays the visual
embodiment of those components. Recognized
utterances are shown as text, parses are shown
as grouped string fragments, and artifacts and
their sub-components are shown in their pro-
totypical graphical form. The window orga-
nizes these visual representations of the meet-
ing?s events and states into chronological tracks,
each of which monitors a unified conceptual part
of the meeting. The user is then able to link the
elements causally.
Beyond the history browser, the toolkit also
displays the current state of all artifacts in an
artifact-state window (e.g. Figure 3(b)). In the
window, the user not only confirms the state of
the artifact but can also gain insight into the
currently interpreted dialogue context by mon-
itoring how the artifact is highlighted. In the
figure, the third task is highlighted because it is
the most recently talked-about task. A meeting
participant can therefore see that subsequent
anaphoric references to an unknown task will
be resolved to the third one.
Another GUI component of the toolkit is
a small hypothesis window which shows the
current set of unresolved artifact-changing hy-
potheses. It does this by displaying an artifact
for each hypothesis, reflecting the artifact?s fu-
ture state given confirmation of the hypothe-
sis. The hypothesis? probability and associated
parse is displayed under the artifact. The user
may even directly click a hypothesis to confirm
it. The hypothesized future states are however
not displayed in the artifact-state window or
artifact-history browser, which show only the
results of confirmed actions.
In addition to being a GUI front-end, the
toolkit maintains a fully generic architecture for
recording the history of any object in the sys-
tem software. These objects can be anything
from the utterances of a participant, to the state
history of an artifact component, or the record
of hypotheses formulated by the dialogue man-
ager. This generic functionality provides the
toolkit the ability to answer a wide variety of
questions for the user about absolutely any as-
pect of the dialogue context history.
5 Future Work
Work is currently proceeding in a number of
directions. Firstly, we plan to incorporate fur-
ther techniques for robust language understand-
ing, including word-spotting and other topic-
recognition techniques, within the context of
the constructed artifact. We also plan to in-
vestigate using the current state of the artifact
to further bias the ASR language model. We
also plan on generalizing the uncertainty man-
agement within the dialogue manager, allowing
multiple competing hypotheses to be supported
over multiple dialogue moves. Topic and other
ambiguity management techniques will be used
to statistically filter and bias hypotheses, based
on artifact state.
We are currently expanding the meeting
browser to categorize utterances by dialogue
act, and to recognize and categorize aggrega-
tions as multi-move strategies, such as negoti-
ations. This will allow at-a-glance detection of
where disagreements took place, and where is-
sues may have been left unresolved. A longer-
term aim of the project is to provide further
support to the participants in the meeting, e.g.
by detecting opportunities to provide useful in-
formation (e.g. schedules, when discussing who
to allocate to a task; documents pertinent to a
topic under discussion) to meeting participants
automatically. Evaluation criteria are currently
being designed that include both standard mea-
sures, such as word error rate, and measures in-
volving recognition of meeting-level phenomena,
such as detecting agreement on action-items.
Evaluation will be performed using both corpus-
based approaches (e.g. for evaluating recog-
nition of meeting phenomena) and real (con-
trolled) meetings with human subjects.
6 Acknowledgements
We would like to gratefully acknowledge Phil
Cohen?s group at OGI, especially Ed Kaiser,
Xiaoguang Li, and Matt Wesson, and David
Demirdjian at MIT. This work was funded by
DARPA grant NBCH-D-03-0010(1).
References
B. Clark, E. Owen Bratt, O. Lemon, S. Pe-
ters, H. Pon-Barry, Z. Thomsen-Gray, and
P. Treeratpituk. 2002. A general purpose ar-
chitecture for intelligent tutoring systems. In
International CLASS Workshop on Natural,
Intelligent and Effective Interaction in Multi-
modal Dialogue Systems.
J. Dowding, J.M. Gawron, D. Appelt, J. Bear,
L. Cherny, R. Moore, and D. Moran. 1993.
Gemini: a natural language system for
spoken-language understanding. In Proc.
ACL 93.
W. E. Grosso, H. Eriksson, R. W. Fergerson,
J. H. Gennari, S. W. Tu, and M. A. Musen.
1999. Knowledge modeling at the millen-
nium: (the design and evolution of Prote?ge?-
2000). In Proc. KAW 99.
A. Gruenstein. 2002. Conversational interfaces:
A domain-independent architecture for task-
oriented dialogues. Master?s thesis, Stanford
University.
A. Janin, D. Baron, J. Edwards, D. Ellis,
D. Gelbart, N. Morgan, B. Peskin, T. Pfau,
E. Shriberg, A. Stolcke, and C. Wooters.
2003. The ICSI meeting corpus. In Proc.
ICASSP 2003.
E. Kaiser, A. Olwal, D. McGee, H. Benko,
A. Corradini, X. Li, P. Cohen, and S. Feiner.
2003. Mutual disambiguation of 3D multi-
modal interaction in augmented and virtual
reality. In Proc. ICMI 2003.
T. Ko, D. Demirdjian, and T. Darrell. 2003.
Untethered gesture acquisition and recogni-
tion for a multimodal conversational system.
In Proc. ICMI 2003.
S. Larsson and D. Traum. 2000. Informa-
tion state and dialogue management in the
TRINDI dialogue move engine toolkit. Natu-
ral Language Engineering, 6.
O. Lemon, A. Gruenstein, and S. Peters. 2002.
Collaborative activities and multi-tasking in
dialogue systems. Traitment Automatique
des Langues, 43(2).
D. Moore. 2002. The IDIAP smart meeting
room. Technical Report IDIAP Communica-
tion 02-07.
A. Waibel, M. Bett, F. Metze, K. Ries,
T. Schaaf, T. Schultz, H. Soltau, H. Yu, and
K. Zechner. 2001. Advances in automatic
meeting record creation and access. In Proc.
ICASSP 2001.
Multi-tasking and Collaborative Activities in Dialogue Systems
Oliver Lemon, Alexander Gruenstein, Alexis Battle, and Stanley Peters
Center for the Study of Language and Information
Stanford University, CA 94305
lemon,alexgru,ajbattle,peters@csli.stanford.edu
Abstract
We explain dialogue management tech-
niques for collaborative activities with hu-
mans, involving multiple concurrent tasks.
Conversational context for multiple con-
current activities is represented using a
?Dialogue Move Tree? and an ?Activity
Tree? which support multiple interleaved
threads of dialogue about different activi-
ties and their execution status. We also de-
scribe the incremental message selection,
aggregation, and generation method em-
ployed in the system.
1 Introduction
This paper describes implemented multi-modal dia-
logue systems1 which support collaboration with au-
tonomous devices in their execution of multiple con-
current tasks. We will focus on the particular mod-
elling and processing aspects which allow the sys-
tems to handle dialogues about multiple concurrent
tasks in a coherent and natural manner. Many con-
versations between humans have this property, and
dialogues between humans and semi-autonomous
devices will have this feature in as much as devices
are able to carry out activities concurrently. This
ability to easily interleave communication streams is
a very useful property of conversational interactions.
Humans are adept at carrying out conversations with
1This research was (partially) funded under the Wallenberg
laboratory for research on Information Technology and Au-
tonomous Systems (WITAS) Project, Linko?ping University, by
the Wallenberg Foundation, Sweden.
multiple threads, or topics, and this capability en-
ables fluid and efficient communication, and thus ef-
fective co-ordination of actions (see (Lemon et al,
2002) for a more extensive discussion). We will
show how to endow a dialogue system with some
of these capabilities.
The main issues which we address in this paper
are:
  Representation of dialogue context such that
collaborative activities and multi-tasking are
supported.
  Dialogue management methods such that free
and natural communication over several con-
versational topics is supported.
  Natural generation of messages in multi-
tasking collaborative dialogues.
In Section 2 we discuss the demands of multi-
tasking and collaboration with autonomous devices.
Section 3 covers the robot with which our current
dialogue system interacts, and the architecture of
the dialogue system. In Section 4 we introduce the
?joint activities? and Activity Models which repre-
sent collaborative tasks and handle multi-tasking in
an interface layer between the dialogue system and
autonomous devices. Section 5 presents the dia-
logue modelling and management techniques used
to handle multiple topics and collaborative activi-
ties. Section 6 surveys the message selection, ag-
gregation, and generation component of the system,
in the context of multi-tasking.
     Philadelphia, July 2002, pp. 113-124.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
2 Multi-tasking and Collaboration
A useful dialogue system for interaction with au-
tonomous devices will enable collaboration with hu-
mans in the planning and execution of tasks. Dia-
logue will be used to specify and clarify instructions
and goals for the device, to monitor its progress,
and also to jointly solve problems. Before we deal
with such issues in detail, we note that such devices
also have the following properties which are relevant
from the point of view of dialogue management:
  Devices exist within dynamic environments,
where new objects appear and are available for
discussion. Device sensors may give rise to
new information at any time, and this may need
to be communicated urgently.
  Devices may perform multiple concurrent ac-
tivities which may succeed, fail, become can-
celled, or be revised. These activities can be
topics of conversation.
(Allen et al, 2001) present a taxonomy of dia-
logue systems ranging from ?finite-state script? di-
alogues for simple tasks (such as making a long-
distance call) to the most complex ?agent-based
models? which cover dialogues where different pos-
sibilities, such as future plans, are discussed. Within
this taxonomy, a useful dialogue system for interac-
tion with autonomous devices must be located at or
near the ?agent-based? point since we wish to com-
municate with devices about their possible actions,
their plans, and the tasks they are currently attempt-
ing. For these reasons we built a dialogue manager
that represents (possibly collaborative) activities and
their execution status, and tracks multiple threads of
dialogue about concurrent and planned activities.
For these sorts of reasons it is clear that form-
filling or data-base query style dialogues (e.g. the
CSLU Toolkit, (McTear, 1998)) will not suffice here
(see (Elio and Haddadi, 1999; Allen et al, 2001) for
similar arguments).
3 The WITAS Dialogue System
In our current application, the autonomous system
is the WITAS2 UAV (?unmanned aerial vehicle?) ?
a small robotic helicopter with on-board planning
2See http://www.ida.liu.se/ext/witas
and deliberative systems, and vision capabilities (for
details see e.g. (Doherty et al, 2000)). This robot
helicopter will ultimately be controlled by the dia-
logue system developed at CSLI, though at the mo-
ment we interact with a simulated3 UAV. Mission
goals are provided by a human operator, and an on-
board planning system then responds. While the he-
licopter is airborne, an on-board active vision sys-
tem interprets the scene or focus below to interpret
ongoing events, which may be reported (via NL gen-
eration) to the operator (see Section 6). The robot
can carry out various ?activities? such as flying to a
location, or following a vehicle, or landing. These
activities are specified by the user during dialogue,
or can be initiated by the UAV?s on-board AI. In any
case, a major component of the dialogue, and a way
of maintaining its coherence, is tracking the state of
current or planned activities of the device.
A more interesting and problematic notion is that
of ?joint-activities? between an autonomous system
and a human operator. These are activities which
the autonomous system cannot complete alone, but
which require some human intervention. In our cur-
rent scenarios, the UAV?s vision system is not good
enough to determine whether a particular vehicle is
the one sought-after, and only the human operator
has the authority to determine this, so that human
and robot must collaborate in order to find and track
a vehicle. The dialogue in Figure 2 shows how a
typical interaction works4 (other capabilities, such
as clarification subdialogues, are covered in (Lemon
et al, 2001)). Note here that the user is able to make
explicit queries about the robot?s activities (both cur-
rent and future), that there are concurrent activi-
ties, and that conversational initiative centers around
the joint activities currently being specified and ex-
ecuted.
4 Activity Models
The idea of Activity Modelling in our system is
the vision that dialogue systems can, in generality,
be built for ?devices? which carry out certain well-
3Our UAV simulator uses KIF statements under JTP (the
Java Theorem Prover) to represent and non-montonically up-
date UAV state information.
4The system runs on a laptop computer under Windows
2000. Video footage of the system can be found at http:
//www-csli.stanford.edu/semlab/witas/
Figure 2: A demonstration of the WITAS dialogue system (November 2001)
Multi-modal Utterances Dialogue Move
Operator (O): Our job is to look for a red car Command (Joint Activity)
UAV (U): Ok. I am looking for one. Report (Confirm Activity)
O: Fly here please [+click on map] Command (Deictic)
U: Okay. I will fly to waypoint one Report (Confirm Activity)
U: Now taking off and flying there. Report (Current Activity)
O: Stop that. Go to the tower instead. Command, Revision
U: I have cancelled flying to waypoint one. I will fly to the
tower.
Report (Activity status)
O: What are you doing? Wh-question (Current Activity)
U: I am searching for a red car and flying to the tower Answer (Current Activity)
O: What will you do next? Wh-question (Planned Activity)
U: I have nothing planned. Answer(Planned Activity)
U: I see a red car on main street [display on map, show video
images], Is this the right car?
Report, Yn-question (Activity)
O: Yes, that?s the right car Yn-answer (Positive)
U: Okay. I am following it . Report (Current activity)
facilitator
OAA2
Synthesizer
Generator
Gemini
Parser and 
Recognizer
Speech
Festival
Display
Interactive Map
NL
SR
TTS
DM
GUI
Activities
Model
Interface
       
Dialogue Move Tree (DMT)
Activity Tree (AT)
System Agenda (SA)
Pending List (PL)
Modality Buffer (MB)
ROBOT
Salience List (SL)
Speech
Nuance
DIALOGUE MANAGER
Figure 1: The WITAS dialogue system architecture
defined activities (e.g. switch lights on, record on
channel   , send email  to  , search for vehicle  ),
and that an important part of the dialogue context to
be modelled in such a system is the device?s planned
activities, current activities, and their execution sta-
tus5. We choose to focus on building this class of
dialogue systems because we share with (Allen et
al., 2001), a version of the the Practical Dialogue
5Compare this with the motivation behind the ?Pragmatic
Adapter? idea of (LuperFoy et al, 1998).
Hypothesis:
?The conversational competence required
for practical dialogues, although still com-
plex, is significantly simpler to achieve
than general human conversational com-
petence.?
We also share with (Rich et al, 2001) the idea that
declarative descriptions of the goal decomposition
of activities (COLLAGEN?s ?recipes?, our ?Activ-
ity Models?) are a vital layer of representation, be-
tween a dialogue system and the device with which
it interacts.
In general we assume that a device is capable of
performing some ?atomic? activities or actions (pos-
sibly simultaneously), which are the lowest-level ac-
tions that it can perform. Some devices will only
know how to carry out sequences of atomic activ-
ities, in which case it is the dialogue system?s job
to decompose linguistically specified high-level ac-
tivities (e.g. ?record the film on channel 4 tonight?)
into a sequence of appropriate atomic actions for the
device. In this case the dialogue system is provided
with a declarative ?Activities Model? (see e.g. Fig-
ure 3) for the device which states how high-level
linguistically-specified activities can be decomposed
into sequences of atomic actions. This model con-
tains traditional planning constraints such as precon-
ditions and postconditions of actions. In this way, a
relatively ?stupid? device (i.e. with little or no plan-
ning capabilities) can be made into a more intelli-
gent device when it is dialogue-enabled.
At the other end of the spectrum, more intelli-
gent devices are able to plan their own sequences of
atomic actions, based on some higher level input. In
this case, it is the dialogue system?s role to translate
natural language into constraints (including tempo-
ral constraints) that the device?s planner recognizes.
The device itself then carries out planning, and in-
forms the dialogue manager of the sequence of ac-
tivities that it proposes. Dialogue can then be used
to re-specify constraints, revise activities, and mon-
itor the progress of tasks. We propose that the pro-
cess of decomposing a linguistically specified com-
mand (e.g. ?vacuum in the main bedroom and the
lounge, and before that, the hall?) into an appropri-
ate sequence of constraints for the device?s on-board
planner, is an aspect of ?conversational intelligence?
that can be added to devices by dialogue-enabling
them.
We are developing one representation and reason-
ing scheme to cover this spectrum of cases from de-
vices with no planning capabilities to some more
impressive on-board AI. Both dialogue manager
and robot/device have access to a single ?Activity
Tree? which is a shared representation of current
and planned activities and their execution status, in-
volving temporal and hierarchical ordering (in fact,
one can think of the Activity Tree as a Hierarchical
Task Network for the device). This tree is built top-
down by processing verbal input from the user, and
its nodes are then expanded by the device?s planner
(if it has one). In cases where no planner exists, the
dialogue manager itself expands the whole tree (via
the Activity Model for the device) until only leaves
with atomic actions are left for the device to execute
in sequence. The device reports completion of activ-
ities that it is performing and any errors that occur
for an activity.
Note that because the device and dialogue system
share the same representation of the device?s activ-
ities, they are always properly coordinated. They
also share responsibility for different aspects of con-
structing and managing the whole Activity Tree.
Note also that some activities can themselves be
speech acts, and that this allows us to build collabo-
rative dialogue into the system. For example, in Fig-
ure 3 the ASK-COMPLETE activity is a speech act,
generating a yes-no question to be answered by the
user.
4.1 An example Activity Model
An example LOCATE activity model for the UAV
is shown in Figure 3. It is used when constructing
parts of the activity tree involving commands such
as ?search for?, ?look for? and so on. For instance,
if the user says ?We?re looking for a truck?, that ut-
terance is parsed into a logical form involving the
structure (locate, np[det(a),truck]).
The dialogue manager then accesses the Activity
Model for LOCATE and adds a node to the Activ-
ity Tree describing it. The Activity Model speci-
fies what sub-activities should be invoked, and un-
der what conditions they should be invoked, what
the postconditions of the activity are. Activity Mod-
els are similar to the ?recipes? of (Rich et al, 2001).
For example, in Figure 3 the Activity Model for LO-
CATE states that,
  it uses the camera resource (so that any other
activity using the camera must be suspended,
or a dialogue about resource conflict must be
initiated),
  that the preconditions of the activity are that the
UAV must be airborne, with fuel and engine in-
dicators satisfactory,
  that the whole activity can be skipped if the
UAV is already ?locked-on? to the sought ob-
ject,
  that the postcondition of the activity is that the
UAV is ?locked-on? to the sought object,
  that the activity breaks into three sequen-
tial sub-activities: WATCH-FOR, FOLLOW-OBJ,
and ASK-COMPLETE.
Nodes on the Activity Tree can be either: ac-
tive, complete, failed, suspended, or canceled. Any
change in the state of a node (typically because of
a report from the robot) is placed onto the System
Agenda (see Section 5) for possible verbal report to
the user, via the message selection and generation
module (see Section 6).
Figure 3: A ?Locate? Activity Model for a UAV, exhibiting collaborative dialogue
Locate// locate is "find-by-type", collaborative activity.
// Breaks into subactivities: watch_for, follow, ask_complete.
{ResourcesUsed {camera;} // will be checked for conflicts.
PreConditions //check truth of KIF statements.
{(Status flight inair) (Status engine ok) (Status fuel ok);}
SkipConditions // skip this Activity if KIF condition true.
{(Status locked-on THIS.np);}
PostConditions// assert these KIF statements when completed.
{(Status locked-on THIS.np) ;}
Children SEQ //sequential sub-activities.
{TaskProperties
{command = "watch_for"; // basic robot action ---
np = THIS.np;} // set sensors to search.
TaskProperties
{command = "follow_obj"; //triggers complex activity --
np = THIS.np;} //following a candidate object.
TaskProperties //collaborative speech action:
{command = "ask_complete";//asks user whether this is
np = THIS.np; }}} //object we are looking for.
5 The Dialogue Context Model
Dialogue management falls into two parts ? dialogue
modelling (representation), and dialogue control (al-
gorithm). In this section we focus on the representa-
tional aspects, and section 5.2 surveys the main al-
gorithms. As a representation of conversational con-
text, the dialogue manager uses the following data
structures which make up the dialogue Information
State (IS);
  Dialogue Move Tree (DMT)
  Activity Tree (AT)
  System Agenda (SA)
  Pending List (PL)
  Salience List (SL)
  Modality Buffer (MB)
Figure 4 shows how the Dialogue Move Tree re-
lates to other parts of the dialogue manager as a
whole. The solid arrows represent possible update
functions, and the dashed arrows represent query
functions. For example, the Dialogue Move Tree
can update Salience List, System Agenda, Pend-
ing List, and Activity Tree, while the Activity Tree
can update only the System Agenda and send ex-
ecution requests to the robot, and it can query the
Activity Model (when adding nodes). Likewise, the
Message Generation component queries the System
Agenda and the Pending List, and updates the Dia-
logue Move Tree whenever a synthesized utterance
is produced.
Figure 5 shows an example Information State
logged by the system, displaying the interpretation
of the system?s utterance ?now taking off? as a re-
port about an ongoing ?go to the tower? activity (the
Pending List and System Agenda are empty, and
thus are not shown).
5.1 The Dialogue Move Tree
Dialogue management uses a set of abstract dia-
logue move classes which are domain independent
(e.g. command, activity-query, wh-question, revi-
sion,     ). Any ongoing dialogue constructs a par-
ticular Dialogue Move Tree (DMT) representing the
current state of the conversation, whose nodes are
DIALOGUE
ACTIVITY
MOVE
TREE
AGENDA
SYSTEM TREE
Activities)
(NPs,
(Selection and Aggregation)
SALIENCE
ACTIVITY
LAYER
speech
synthesis
INFORMATION
INDEXICAL
(Active Node List)
MESSAGE 
GENERATION
ACTIVITY
MODEL
DEVICE
LIST
PENDING
LIST
MODALITY
BUFFER
Map Display Inputs
(parsed human speech)
(mouse clicks)
Conversational Move Inputs
Figure 4: Dialogue Manager Architecture (solid arrows denote possible updates, dashed arrows represent
possible queries)
instances of the dialogue move classes, and which
are linked to nodes on the Activity Tree where ap-
propriate, via an activity tag (see below).
Incoming logical forms (LFs) from the pars-
ing process are always tagged with a dialogue
move (see e.g. (Ginzburg et al, 2001)), which pre-
cedes more detailed information about an utter-
ance. For instance the logical form: command([go],
[param-list ([pp-loc(to, arg([np(det([def],the),
[n(tower,sg)])]))])])
corresponds to the utterance ?go to the tower?,
which is flagged as a command.
A slightly more complex example is; re-
port(inform, agent([np([n(uav,sg)])]), compl-
activity([command([take-off])]))
which corresponds to ?I have taken off? ? a re-
port from the UAV about a completed ?taking-off?
activity.
The first problem in dialogue management is
to figure out how these incoming ?Conversational
Moves? relate to the current dialogue context. In
other words, what dialogue moves do they consti-
tute, and how do they relate to previous moves in
the conversation? In particular, given multi-tasking,
to which thread of the conversation does an incom-
ing utterance belong? We use the Dialogue Move
Tree to answer these questions:
1. A DMT is a history or ?message board? of
dialogue contributions, organized by ?thread?,
based on activities.
2. A DMT classifies which incoming utterances
can be interpreted in the current dialogue con-
text, and which cannot be. It thus delimits
a space of possible Information State update
functions.
3. A DMT has an Active Node List which con-
trols the order in which this function space is
searched 6.
4. A DMT classifies how incoming utterances are
to be interpreted in the current dialogue con-
text.
In general, then, we can think of the DMT as
representing a function space of dialogue Informa-
6It also defines an ordering on language models for speech
recognition.
tion State update functions. The details of any par-
ticular update function are determined by the node
type (e.g. command, question) and incoming dia-
logue move type and their contents, as well as the
values of Activity Tag and Agent.
Note that this notion of ?Dialogue Move Tree? is
quite different from previous work on dialogue trees,
in that the DMT does not represent a ?parse? of the
dialogue using a dialogue grammar (e.g. (Ahrenberg
et al, 1990)), but instead represents all the threads
in the dialogue, where a thread is the set of utter-
ances which serve a particular dialogue goal. In the
dialogue grammar approach, new dialogue moves
are attached to a node on the right frontier of the
tree, but in our approach, a new move can attach
to any thread, no matter where it appears in the
tree. This means that the system can flexibly in-
terpret user moves which are not directly related to
the current thread (e.g. a user can ignore a system
question, and give a new command, or ask their
own question). Finite-state representations of dia-
logue games have the restriction that the user is con-
strained by the dialogue state to follow a particular
dialogue path (e.g. state the destination, clarify, state
preferred time,     ). No such restriction exists with
DMTs, where dialogue participants can begin and
discontinue threads at any time.
We discuss this further below.
5.2 Interpretation and State Update
The central algorithm controlling dialogue manage-
ment has two main steps, Attachment, and Process
Node;
1. Attachment: Process incoming input conversa-
tional move   with respect to the current DMT
and Active Node List, and ?attach? a new node

interpreting   to the tree if possible.
2. Process Node: process the new node  , if it
exists, with respect to the current information
state. Perform an Information State update us-
ing the dialogue move type and content of  .
When an update function  exists, its effects de-
pend on the details of the incoming input   (in par-
ticular, to the dialogue move type and the contents
of the logical form) and the DMT node to which it
attaches. The possible attachments can be thought
of as adjacency pairs, and each dialogue move class
contains information about which node types it can
attach. For instance the command node type can at-
tach confirmation, yn-question, wh-question, and re-
port nodes.
Examples of different attachments available in our
current system can be seen in Figure 67. For exam-
ple, the first entry in the table states that a command
node, generated by the user, with activity tag  , is
able to attach any system confirmation move with
the same activity tag, any system yes-no question
with that tag, any system wh- question with that tag,
or any system report with that activity tag. Similarly,
the rows for wh-question nodes state that:
  a wh-question by the system with activity tag 
can attach a user?s wh-answer (if it is a possible
answer for that activity)
  a user?s wh-question can attach a system wh-
answer, and no particular activity need be spec-
ified.
These possible attachments delimit the ways in
which dialogue move trees can grow, and thus clas-
sify the dialogue structures which can be captured in
the current system. As new dialogue move types are
added to the system, this table is being extended to
cover other conversation types (e.g. tutoring (Clark
et al, 2001)).
It is worth noting that the node type created af-
ter attachment may not be the same as the dialogue
move type of the incoming conversational move   .
Depending on the particular node which attaches the
new input, and the move type of that input, the cre-
ated node may be of a different type. For exam-
ple, if a wh-question node attaches an input which is
simply a command, the wh-question node may inter-
pret the input as an answer, and attach a wh-answer.
These interpretation rules are local to the node to
which the input is attached. In this way, the DMT
interprets new input in context, and the pragmatics
of each new input is contextually determined, rather
than completely specified via parsing using conver-
sational move types. Note that Figure 6 does not
state what move type new input is attached as, when
it is attached.
7Where Activity Tags are not specified, attachment does not
depend on sharing of Activity Tags.
In the current system, if the user produces an ut-
terance which can attach to several nodes on the
DMT, only the ?most active? node (as defined by the
Active Node List) will attach the incoming move. It
would be interesting to explore such events as trig-
gers for clarification questions, in future work.
6 Message generation
Since the robot is potentially carrying out multiple
activities at once, a particular problem is how to de-
termine appropriate generation of utterances about
those activities, in a way which does not overload
the user with information, yet which establishes and
maintains appropriate context in a natural way.
Generation for dialogue systems in general is
problematic in that dialogue contributions arise in-
crementally, often in response to another partici-
pant?s utterances. For this reason, generation of
large pieces of text is not appropriate, especially
since the user is able to interrupt the system. Other
differences abound, for example that aggregation
rules must be sensitive to incremental aspects of
message generation.
As well as the general problems of message selec-
tion and aggregation in dialogue systems, this par-
ticular type of application domain presents specific
problems in comparison with, say, travel-planning
dialogue systems ? e.g. (Seneff et al, 1991). An au-
tonomous device will, in general, need to communi-
cate about,
  its perceptions of a changing environment,
  progress towards user-specified goals,
  execution status of activities or tasks,
  its own internal state changes,
  the progress of the dialogue itself.
For these reasons, the message selection and gen-
eration component of such a system needs to be
of wider coverage and more flexible than template-
based approaches, while remaining in real, or near-
real, time (Stent, 1999). As well as this, the system
must potentially be able to deal with a large band-
width stream of communications from the robot,
and so must be able to intelligently filter them for
?relevance? so that the user is not overloaded with
unimportant information, or repetitious utterances.
In general, the system should appear as ?natural? as
possible from the user?s point of view ? using the
same language as the user if possible (?echoing?),
using anaphoric referring expressions where possi-
ble, and aggregating utterances where appropriate.
A ?natural? system should also exhibit ?variability?
in that it can convey the same content in a variety
of ways. A further desirable feature is that the sys-
tem?s generated utterances should be in the cover-
age of the dialogue system?s speech recognizer, so
that system-generated utterances effectively prime
the user to speak in-grammar.
Consequently we attempted to implement the fol-
lowing features in message selection and generation:
relevance filtering; recency filtering; echoing; vari-
ability; aggregation; symmetry; real-time genera-
tion.
Our general method is to take as inputs to the pro-
cess various communicative goals of the system, ex-
pressed as logical forms, and use them to construct a
single new logical form to be input to Gemini?s Se-
mantic Head-Driven Generation algorithm (Shieber
et al, 1990), which produces strings for Festival
speech synthesis. We now describe how to use com-
plex dialogue context to produce natural generation
in multitasking contexts.
6.1 Message selection - filtering
Inputs to the selection and generation module are
?concept? logical forms (LFs) describing the com-
municative goals of the system. These are struc-
tures consisting of context tags (e.g. activity identi-
fier, dialogue move tree node, turn tag) and a con-
tent logical form consisting of a Dialogue Move
(e.g. report, wh-question), a priority tag (e.g. warn
or inform), and some additional content tags (e.g.
for objects referred to). An example input logical
form is, ?report(inform, agent(AgentID), cancel-
activity(ActivityID))?, which corresponds to the re-
port ?I have cancelled flying to the tower? when
AgentID refers to the robot and ActivityID refers to
a ?fly to the tower? task.
Items which the system will consider for genera-
tion are placed (either directly by the robot, or indi-
rectly by the Activity Tree) on the ?System Agenda?
(SA), which is the part of the dialogue Information
State which stores communicative goals of the sys-
tem. Communicative goals may also exist on the
?Pending List? (PL) which is the part of the infor-
mation state which stores questions that the system
has asked, but which the user has not answered, so
that they may be re-raised by the system. Only ques-
tions previously asked by the system can exist on the
Pending List.
Due to multi-tasking, at any time there is a num-
ber of ?Current Activities? which the user and sys-
tem are performing (e.g. fly to the tower, search for
a red car). These activities are topics of conversa-
tion (defining threads of the DMT) represented in
the dialogue information state, and the system?s re-
ports can be generated by them (in which case the
are tagged with that activity label) or can be rele-
vant to an activity in virtue of being about an object
which is in focus because it is involved in that activ-
ity.
Some system reports are more urgent that others
(e.g. ?I am running out of fuel?) and these carry the
label warning. Warnings are always relevant, no
matter what activities are current ? they always pass
the recency and relevance filters.
Echoing (for noun-phrases) is achieved by access-
ing the Salience List whenever generating referential
terms, and using whatever noun-phrase (if any) the
user has previously employed to refer to the object
in question. If the object is top of the salience list,
the generator will select an anaphoric expression.
The end result of our selection and aggregation
module (see section 6.2) is a fully specified logi-
cal form which is to be sent to the Semantic-Head-
Driven Generation component of Gemini (Shieber
et al, 1990). The bi-directionality of Gemini (i.e.
that we use the same grammar for both parsing and
generation) automatically confers a useful ?symme-
try? property on the system ? that it only utters sen-
tences which it can also understand. This means that
the user will not be misled by the system into em-
ploying out-of-vocabulary items, or out-of-grammar
constructions. Another side effect of this is that
the system utterances prime the user to make in-
grammar utterances, thus enhancing co-ordination
between user and system in the dialogues.
6.2 Incremental aggregation
Aggregation combines and compresses utterances to
make them more concise, avoid repetitious language
structure, and make the system?s speech more nat-
ural and understandable. In a dialogue system ag-
gregation should function incrementally because ut-
terances are generated on the fly. In dialogue sys-
tems, when constructing an utterance we often have
no information about the utterances that will follow
it, and thus the best we can do is to compress it
or ?retro-aggregate? it with utterances that preceded
it. Only occasionally does the System Agenda con-
tain enough unsaid utterances to perform reasonable
?pre-aggregation?.
Each dialogue move type (e.g. report, wh-
question) has its own aggregation rules, stored in
the class for that LF type. In each type, rules spec-
ify which other dialogue move types can aggregate
with it, and exactly how aggregation works. The
rules note identical portions of LFs and unify them,
and then combine the non-identical portions appro-
priately.
For example, the LF that represents the phrase ?I
will fly to the tower and I will land at the parking
lot?, will be converted to one representing ?I will fly
to the tower and land at the parking lot? according
to the compression rules. Similarly, ?I will fly to the
tower and fly to the hospital? gets converted to ?I
will fly to the tower and the hospital?.
The ?retro-aggregation? rules result in sequences
of system utterances such as, ?I have cancelled fly-
ing to the school. And the tower. And landing at the
base.?
7 Summary
We explained the dialogue modelling techniques
which we implemented in order to build a real-
time multi-modal conversational interface to an au-
tonomous device. The novel issues tackled by the
system and its dialogue model are that it is able to
manage conversations about multiple tasks and col-
laborative activities in a robust and natural way.
We argued that in the case of dialogues with
devices, a dialogue management mechanism has
to be particularly robust and flexible, especially
in comparison with finite-state or frame-based di-
alogue managers which have been developed for
information-seeking dialogues, such as travel plan-
ning, where topics of conversation are predeter-
mined. Another challenge was that conversations
may have multiple open topics at any one time, and
this complicates utterance interpretation and gener-
ation.
We discussed the dialogue context model and al-
gorithms used to produce a system with the follow-
ing features:
  supports multi-tasking, multiple topics, and
collaboration,
  support of commands, questions, revisions, and
reports, over a dynamic environment,
  multi-modal, mixed-initiative, open-ended dia-
logues,
  echoic and variable message generation, fil-
tered for relevance and recency
  asynchronous, real-time operation.
An video demonstration of the system is avail-
able at www-csli.stanford.edu/semlab/
witas/.
References
Lars Ahrenberg, Arne Jonsson, and Nils Dalhbeck. 1990.
Discourse representation and discourse management
for natural language interfaces. In In Proceedings of
the Second Nordic Conference on Text Comprehension
in Man and machine.
James Allen, Donna Byron, Myroslva Dzikovska, George
Ferguson, Lucian Galescu, and Amanda Stent. 2001.
Toward conversational human-computer interaction.
AI Magazine, 22(4):27?37.
Brady Clark, John Fry, Matt Ginzton, Stanley Pe-
ters, Heather Pon-Barry, and Zachary Thomsen-Gray.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proceedings of SIGdial
2001.
Patrick Doherty, Go?sta Granlund, Krzystof Kuchcinski,
Erik Sandewall, Klas Nordberg, Erik Skarman, and Jo-
han Wiklund. 2000. The WITAS unmanned aerial
vehicle project. In European Conference on Artificial
Intelligence (ECAI 2000).
Renee Elio and Afsaneh Haddadi. 1999. On abstract
task models and conversation policies. In Workshop
on Specifying and Implementing Conversation Poli-
cies, Autonomous Agents?99, Seattle.
Jonathan Ginzburg, Ivan A. Sag, and Matthew Purver.
2001. Integrating Conversational Move Types in
the Grammar of Conversation. In Bi-Dialog 2001?
Proceedings of the 5th Workshop on Formal Semantics
and Pragmatics of Dialogue, pages 45?56.
Beth-Ann Hockey, Gregory Aist, Jim Hieronymous,
Oliver Lemon, and John Dowding. 2002. Targeted
help: Embedded training and methods for evaluation.
In Proceedings of Intelligent Tutoring Systems (ITS).
(to appear).
Oliver Lemon, Anne Bracy, Alexander Gruenstein, and
Stanley Peters. 2001. Information states in a multi-
modal dialogue system for human-robot conversation.
In Peter Ku?hnlein, Hans Reiser, and Henk Zeevat, edi-
tors, 5th Workshop on Formal Semantics and Pragmat-
ics of Dialogue (Bi-Dialog 2001), pages 57 ? 67.
Oliver Lemon, Alexander Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in di-
alogue systems. Traitement Automatique des Langues
(TAL). Special Issue on Dialogue (to appear).
Susann LuperFoy, Dan Loehr, David Duff, Keith Miller,
Florence Reeder, and Lisa Harper. 1998. An architec-
ture for dialogue management, context tracking, and
pragmatic adaptation in spoken dialogue systems. In
COLING-ACL, pages 794 ? 801.
Micheal McTear. 1998. Modelling spoken dialogues
with state transition diagrams: Experiences with the
CSLU toolkit. In Proc 5th International Conference
on Spoken Language Processing.
Charles Rich, Candace Sidner, and Neal Lesh. 2001.
Collagen: applying collaborative discourse theory to
human-computer interaction. AI Magazine, 22(4):15?
25.
S. Seneff, L. Hirschman, and V. W. Zue. 1991. Interac-
tive problem solving and dialogue in the ATIS domain.
In Proceedings of the Fourth DARPA Speech and Nat-
ural Language Workshop. Morgan Kaufmann.
Stuart M. Shieber, Gertjan van Noord, Fernando C. N.
Pereira, and Robert C. Moore. 1990. Semantic-
head-driven generation. Computational Linguistics,
16(1):30?42.
Amanda Stent. 1999. Content planning and generation
in continuous-speech spoken dialog systems. In Pro-
ceedings of KI?99 workshop ?May I Speak Freely??.
Figure 5: A snapshot of an Information State (from the HTML system logs)
Utterance: ??now taking off?? (by System 11/7/01 4:50 PM)
Conversational Move:
report(inform,agent([np([n(uav,sg)])]),curr_activity([command([take_off])]))
Dialogue Move Tree (position on active node list in parens [0 = most active])
* Root (1)
Root
o Command (0)
command([go],[param_list([pp_loc(to,arg([np(det([def],the),[n(tower,
sg)])]))])]) [[dmtask0] current]
+ Report
report(inform,agent([np([n(uav,sg)])]),curr_activity([command
([take_off])]))[]
o Report
report(inform,agent([np([n(uav,sg)])]),confirm_activity([command([go],
[param_list([pp_loc(to,arg([np(det([def],the),[n(tower,sg)],
)]))])])])) [[dmtask0] current]
Activity Tree
* root
o [dmtask0] current
relation = SEQuential
command = go
pp = pp_loc(to,Args)
np = np(det([def],the),[n(tower,sg)])
+ [sim3] current
relation = none
command = take_off
pp = null, np = null
Salience List (least salient -- most salient)
* [np(det([def],the),[n(tower,sg)])] (speech)
* [np(det([def],the),[n(tower,sg)])] (speech)
Figure 6: Attachment in the Dialogue Move Classes
DMT Node Attaches
Node Type Activity
Tag
Speaker Node Type Activity
Tag
Speaker
command t user confirmation, t system
y-n question, t system
wh-question, t system
report t system
confirmation t system
report t system command t user
wh-question t system wh-answer t user
wh-question user wh-answer system
yn-question t system yn-answer t user
revision t user wh-question t system
yn-answer t user confirmation t system
wh-answer user confirmation system
wh-answer system confirmation user
root n/a n/a command, user
question, user
revision user
root n/a n/a report system
Figure 7: Part of the Graphical User Interface, showing a flight plan
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 11?20,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Response-Based Confidence Annotation for Spoken Dialogue Systems
Alexander Gruenstein
Spoken Language Systems Group
M.I.T. Computer Science and Artificial Intelligence Laboratory
32 Vassar St, Cambridge, MA 02139 USA
alexgru@csail.mit.edu
Abstract
Spoken and multimodal dialogue systems typ-
ically make use of confidence scores to choose
among (or reject) a speech recognizer?s N-
best hypotheses for a particular utterance. We
argue that it is beneficial to instead choose
among a list of candidate system responses.
We propose a novel method in which a con-
fidence score for each response is derived
from a classifier trained on acoustic and lex-
ical features emitted by the recognizer, as
well as features culled from the generation of
the candidate response itself. Our response-
based method yields statistically significant
improvements in F-measure over a baseline in
which hypotheses are chosen based on recog-
nition confidence scores only.
1 Introduction
The fundamental task for any spoken dialogue sys-
tem is to determine how to respond at any given time
to a user?s utterance. The challenge of understand-
ing and correctly responding to a user?s natural lan-
guage utterance is formidable even when the words
have been perfectly transcribed. However, dialogue
system designers face a greater challenge because
the speech recognition hypotheses which serve as
input to the natural language understanding compo-
nents of a system are often quite errorful; indeed, it
is not uncommon to find word error rates of 20-30%
for many dialogue systems under development in re-
search labs. Such high error rates often arise due to
the use of out-of-vocabulary words, noise, and the
increasingly large vocabularies of more capable sys-
tems which try to allow for greater naturalness and
variation in user input.
Traditionally, dialogue systems have relied on
confidence scores assigned by the speech recognizer
to detect speech recognition errors. In a typical
setup, the dialogue system will choose to either ac-
cept (that is, attempt to understand and respond to)
or reject (that is, respond to the user with an indica-
tion of non-understanding) an utterance by thresh-
olding this confidence score.
Stating the problem in terms of choosing whether
or not to accept a particular utterance for process-
ing, however, misses the larger picture. From the
user?s perspective, what is truly important is whether
or not the system?s response to the utterance is cor-
rect. Sometimes, an errorful recognition hypothe-
sis may result in a correct response if, for example,
proper names are correctly recognized; conversely,
a near-perfect hypothesis may evoke an incorrect re-
sponse. In light of this, the problem at hand is better
formulated as one of assigning a confidence score
to a system?s candidate response which reflects the
probability that the response is an acceptable one.
If the system can?t formulate a response in which it
has high confidence, then it should clarify, indicate
non-understanding, and/or provide appropriate help.
In this paper, we present a method for assign-
ing confidence scores to candidate system responses
by making use not only of features obtained from
the speech recognizer, but also of features culled
from the process of generating a candidate system
response, and derived from the distribution of can-
didate responses themselves. We first compile a list
of unique candidate system responses by processing
11
each hypothesis on the recognizer?s N-best list. We
then train a Support Vector Machine (SVM) to iden-
tify acceptable responses. When given a novel ut-
terance, candidate responses are ranked with scores
output from the SVM. Based on the scores, the sys-
tem can then either respond with the highest-scoring
candidate, or reject all of the candidate responses
and respond by indicating non-understanding.
Part of the motivation for focusing our efforts on
selecting a system response, rather than a recogni-
tion hypothesis, can be demonstrated by counting
the number of unique responses which can be de-
rived from an N-best list. Figure 1 plots the mean
number of unique system responses, parses, and
recognition hypotheses given a particular maximum
N-best list length; it was generated using the data
described in section 3. Generally, we observe that
about half as many unique parses are generated as
recognition hypotheses, and then half again as many
unique responses. Since many hypotheses evoke the
same response, there is no value in discriminating
among these hypotheses. Instead, we should aim
to gain information about the quality of a response
by pooling knowledge gleaned from each hypothesis
evoking that response.
We expect a similar trend of multiple hypothe-
ses mapping to a single parse in any dialogue sys-
tem where parses contain a mixture of key syntac-
tic and semantic structure?as is the case here?or
where they contain only semantic information (e.g.,
slot/value pairs). Parsers which retain more syn-
tactic structure would likely generate more unique
parses, however many of these parses would prob-
ably map to the same system response since a re-
sponse doesn?t typically hinge on every syntactic de-
tail of an input utterance.
The remainder of our discussion proceeds as fol-
lows. In section 2 we place the method presented
here in context in relation to other research. In sec-
tion 3, we describe the City Browser multimodal di-
alogue system, and the process used to collect data
from users? interactions with the system. We then
turn to our techniques for annotating the data in
section 4 and describe the features which are ex-
tracted from the labeled data in section 5. Finally,
we demonstrate how to build a classifier to rank can-
didate system responses in section 6, which we eval-
uate in section 7.
0 10 20 30 40 500
10
20
30
40
50
Maximum N?best length
 
 
Mean N?best Length
Mean Unique Parses
Mean Unique Responses
Figure 1: The mean N-best recognition hypothesis list
length, mean number of unique parses derived from the
N-best list of recognition hypotheses, and mean number
of unique system responses derived from those parses,
given a maximum recognition N-best list length.
2 Related Work
There has been much research into deriving
utterance-level confidence scores based on features
derived from the process of speech recognition. The
baseline utterance-level confidence module we make
use of in this paper was introduced in (Hazen et al,
2002); we use a subset of the recognizer-derived fea-
tures used by this module. In it, confidence scores
are derived by training a linear projection model to
differentiate utterances with high word error rates.
The utterance-level confidence scores are used to de-
cide whether or not the entire utterance should be
accepted or rejected, while the decision as to how
to respond is left out of the classification process.
Of course, most other recognizers make use of utter-
ance or hypothesis level confidence scores as well;
see, for example (San-Segundo et al, 2000; Chase,
1997).
(Litman et al, 2000) demonstrate the additional
use of prosodic features in deriving confidence
scores, and transition the problem from one of word
error rate to one involving concept error rate, which
is more appropriate in the context of spoken dia-
logue systems. However, they consider only the top
recognition hypothesis.
Our work has been heavily influenced by (Gabs-
dil and Lemon, 2004), (Bohus and Rudnicky, 2002),
(Walker et al, 2000), and (Chotimongkol and Rud-
12
nicky, 2001) all of which demonstrate the utility of
training a classifier with features derived from the
natural language and dialogue management compo-
nents of a spoken dialogue system to better predict
the quality of speech recognition results. The work
described in (Gabsdil and Lemon, 2004) is espe-
cially relevant, because, as in our experiments, the
dialogue system of interest provides for map-based
multimodal dialogue. Indeed, we view the exper-
iments presented here as extending and validating
the techniques developed by Gabsdil and Lemon.
Our work is novel, however, in that we reframe
the problem as choosing among system responses,
rather than among recognizer hypotheses. By re-
casting the problem in these terms, we are able to
integrate information from all recognition hypothe-
ses which contribute to a single response, and to ex-
tract distributional features from the set of candi-
date responses. Another key difference is that our
method produces confidence scores for the candi-
date responses themselves, while the cited methods
produce a decision as to whether an utterance, or
a particular recognition hypothesis, should be ac-
cepted, rejected, or (in some cases), ignored by the
dialogue system.
In addition, because of the small size of the
dataset used in (Gabsdil and Lemon, 2004), the au-
thors were limited to testing their approach with
leave-one-out cross validation, which means that,
when testing a particular user?s utterance, other ut-
terances from the same user also contributed to
the training set. Their method also does not pro-
vide for optimizing a particular metric?such as F-
measure?although, it does solve a more difficult
3-class decision problem. Finally, another key dif-
ference is that we make use of an n-gram language
model with a large vocabulary of proper names,
whereas theirs is a context-free grammar with a
smaller vocabulary.
(Niemann et al, 2005) create a dialogue sys-
tem architecture in which uncertainty is propagated
across each layer of processing through the use of
probabilities, eventually leading to posterior proba-
bilities being assigned to candidate utterance inter-
pretations. Unlike our system, in which we train a
single classifier using arbitrary features derived from
each stage of processing, each component (recog-
nizer, parser, etc) is trained separately and must be
capable of assigning conditional probabilities to its
output given its input. The method hinges on proba-
bilistic inference, yet it is often problematic to map
a speech recognizer?s score to a probability as their
approach requires. In addition, the method is evalu-
ated only in a toy domain, using a few sample utter-
ances.
3 Experimental Data
The data used for the experiments which follow
were collected from user interactions with City
Browser, a web-based, multimodal dialogue system.
A thorough description of the architecture and ca-
pabilities can be found in (Gruenstein et al, 2006;
Gruenstein and Seneff, 2007). Briefly, the version
of City Browser used for the experiments in this pa-
per allows users to access information about restau-
rants, museums, and subway stations by navigating
to a web page on their own computers. They can
also locate addresses on the map, and obtain driving
directions. Users can interact with City Browser?s
map-based graphical user interface by clicking and
drawing; and they can speak with it by talking into
their computer microphone and listening to a re-
sponse from their speakers. Speech recognition is
performed via the SUMMIT recognizer, using a tri-
gram language model with dynamically updatable
classes for proper nouns such as city, street, and
restaurant names?see (Chung et al, 2004) for a de-
scription of this capability. Speech recognition re-
sults were parsed by the TINA parser (Seneff, 1992)
using a hand-crafted grammar. A discourse mod-
ule (Filisko and Seneff, 2003) then integrates con-
textual knowledge. The fully formed request is sent
to the dialogue manager, which attempts to craft
an appropriate system response?both in terms of
a verbal and graphical response. The GENESIS
system (Seneff, 2002) uses hand-crafted generation
rules to produce a natural language string, which is
sent to an off-the-shelf text-to-speech synthesizer.
Finally, the user hears the response, and the graphi-
cal user interface is updated to show, for example, a
set of search results on the map.
3.1 Data Collection
The set of data used in this paper was collected
as part of a controlled experiment in which users
13
worked through a set of scenarios by accessing the
City Browser web page from their own computers,
whenever and from wherever they liked. Interested
readers may refer to (Gruenstein and Seneff, 2007)
for more information on the experimental setup, as
well as for an initial analysis of a subset of the data
used here. Users completed a warmup scenario in
which they were simply told to utter ?Hello City
Browser? to ensure that their audio setup and web
browser were working properly. They then worked
through ten scenarios presented sequentially, fol-
lowed by time for ?free play? in which they could
use the system however they pleased.
As users interact with City Browser, logs are
made recording their interactions. In addition to
recording each utterance, every time a user clicks
or draws with the mouse, these actions are recorded
and time-stamped. The outputs of the various stages
of natural language processing are also logged, so
that the ?dialogue state? of the system is tracked.
This means that, associated with each utterance in
the dataset is, among other things, the following in-
formation:
? a recording of the utterance;
? the current dialogue state, which includes in-
formation such as recently referred to entities
for anaphora resolution;
? the state of the GUI, including: the current po-
sition and bounds of the map, any points of in-
terest (POIs) displayed on the map, etc.;
? the contents of any dynamically updatable lan-
guage model classes; and
? time-stamped clicks, gestures, and other user
interface interaction performed by the user be-
fore and during speech.
The utterances of 38 users who attempted most
or all of the scenarios were transcribed, providing
1,912 utterances used in this study. The utterances
were drawn only from the 10 ?real? scenarios; ut-
terances from the initial warmup and final free play
tasks were discarded. In addition, a small number of
utterances were eliminated because logging glitches
made it impossible to accurately recover the dia-
logue system?s state at the time of the utterance.
The class n-gram language model used for data
collection has a vocabulary of approximately 1,200
words, plus about 25,000 proper nouns.
4 Data Annotation
Given the information associated with each utter-
ance in the dataset, it is possible to ?replay? an ut-
terance to the dialogue system and obtain the same
response?both the spoken response and any up-
dates made to the GUI?which was originally pro-
vided to the user in response to the utterance. In
particular, we can replicate the reply frame which
is passed to GENESIS in order to produce a nat-
ural language response; and we can replicate the
gui reply frame which is sent to the GUI so that it
can be properly updated (e.g., to show the results of
a search on the map).
The ability to replicate the system?s response to
each utterance also gives us the flexibility to try out
alternative inputs to the dialogue system, given the
dialogue state at the time of the utterance. So, in ad-
dition to transcribing each utterance, we also passed
each transcript through the dialogue system, yield-
ing a system response. In the experiments that fol-
low, we considered the system?s response to the tran-
scribed utterance to be the correct response for that
utterance. It should be noted that in some cases,
even given the transcript, the dialogue system may
reject and respond by signally non-understanding?
if, for example, the utterance can?t be parsed. In
these cases, we take the response reject to be the
correct response.
We note that labeling the data in this fashion
has limitations. Most importantly, the system may
respond inappropriately even to a perfectly tran-
scribed utterance. Such responses, given our label-
ing methodology, would incorrectly be labeled as
correct. In addition, sometimes it may be the case
that there are actually several acceptable responses
to a particular utterances.
5 Feature Extraction
For each utterance, our goal is to produce a set of
candidate system responses, where each response is
also associated with a vector of feature values to be
used to classify it as acceptable or unacceptable.
Responses are labeled as acceptable if they match
the system response produced from the transcrip-
tion, and as unacceptable otherwise.
We start with the N-best list output by the speech
recognizer. For each hypothesis, we extract a set
14
Recognition Distributional Response
(a) Best across hyps: (b) Drop: (c) Other: percent top 3 response type
total score per word total drop mean words percent top 5 num found
acoustic score per bound acoustic drop top rank percent top 10 POI type
lexical score per word lexical drop n-best length percent nbest is subset
top response type parse status
response rank geographical filter
num distinct
Table 1: Features used to train the acceptability classifier. Nine features are derived from the recognizer; seven have
to do with the distribution of responses; and six come from the process of generating the candidate response.
of acoustic, lexical, and total scores from the recog-
nizer. These scores are easily obtained, as they com-
prise a subset of the features used to train the rec-
ognizer?s existing confidence module; see (Hazen et
al., 2002). The features used are shown in Table 1a.
We then map each hypothesis to a candidate sys-
tem response, by running it through the dialogue
system given the original dialogue state. From these
outputs, we collect a list of unique responses, which
is typically shorter than the recognizer?s N-best list,
as multiple hypotheses typically map to the same re-
sponse.
We now derive a set of features for each unique
response. First, each response inherits the best value
for each recognizer score associated with a hypoth-
esis which evoked that response (see Table 1a). In
addition, the drop in score between the response?s
score for each recognition feature and the top value
occurring in the N-best list is used as a feature (see
Table 1b). Finally, the rank of the highest hypothe-
sis on the N-best list which evoked the response, the
mean number of words per hypothesis evoking the
responses, and the length of the recognizer?s N-best
list are used as features (see Table 1c).
Distributional features are also generated based
on the distribution of hypotheses on the N-best list
which evoked the same response. The percent of
times a particular response is evoked by the top 3,
top 5, top 10, and by all hypotheses on the N-best
list are used as features. Features are generated, as
well, based on the distribution of responses on the
list of unique responses. These features are: the ini-
tial ranking of this response on the list, the number
of distinct responses on the list, and the type of re-
sponse that was evoked by the top hypothesis on the
recognizer N-best list.
Finally, features derived from the response itself,
and natural language processing performed to de-
rive that response, are also calculated. The high-
level type of the response, as well as the type and
number of any POIs returned by a database query
are used as features if they exist, as is a boolean
indicator as to whether or not these results are a
subset of the results currently shown on the dis-
play. If any sort of ?geographical filter?, such as
an address or circled region, is used to constrain the
search, then the type of this filter is also used as a
feature. Finally, the ?best? parse status of any hy-
potheses leading to this response is also used, where
full parse  robust parse  no parse.
Table 1 lists all of the features used to train the
classifier, while Table 3 (in the appendix) lists the
possible values for the non-numerical features. Fig-
ure 3 (in the appendix) gives an overview of the fea-
ture extraction process, as well as the classification
method described in the next section.
6 Classifier Training and Scoring
For a given utterance, we now have a candidate list
of responses derived from the speech recognizer?s
N-best list, a feature vector associated with each re-
sponse, and a label telling us the ?correct? response,
as derived from the transcript. In order to build a
classifier, we first label each response as either ac-
ceptable or unacceptable by comparing it to the sys-
tem?s response to the transcribed utterance. If the
two responses are identical, then the response is la-
beled as acceptable; otherwise, it is labeled as un-
acceptable. This yields a binary decision problem
for each response, given a set of features. We train
a Support Vector Machine (SVM) to make this deci-
15
sion, using the Weka toolkit, version 3.4.12 (Witten
and Frank, 2005).
Given a trained SVM model, the procedure for
processing a novel utterance is as follows. First,
classify each response (and its associated feature
vector) on the response list for that utterance using
the SVM. By using a logistic regression model fit on
the training data, an SVM score between ?1 and 1
for each response is yielded, where responses with
positive scores are more likely to be acceptable, and
those with negative scores are more likely to be un-
acceptable.
Next, the SVM scores are used to rank the list of
responses. Given a ranked list of such responses, the
dialogue system has two options: it can choose the
top scoring response, or it can abstain from choos-
ing any response. The most straightforward method
for making such a decision is via a threshold: if the
score of the top response is above a certain thresh-
old, this response is accepted; otherwise, the system
abstains from choosing a response, and instead re-
sponds by indicating non-understanding. Figure 3
(in the appendix) provides a graphical overview of
the response confidence scoring process.
At first blush, a natural threshold to choose is 0,
as this marks the boundary between acceptable and
unacceptable. However, it may be desirable to opti-
mize this threshold based on the desired characteris-
tics of the dialogue system?in a mission-critical ap-
plication, for example, it may be preferable to accept
only high-confidence responses, and to clarify other-
wise. We can optimize the threshold as we like using
either the same training data, or a held-out develop-
ment set, so long as we have an objective function
with which to optimize. In the evaluation that fol-
lows, we optimize the threshold using the F-measure
on the training data as the objective function. It
would also be interesting to optimize the threshold
in a more sophisticated manner, such as that devel-
oped in (Bohus and Rudnicky, 2005) where task suc-
cess is used to derive the cost of misunderstandings
and false rejections, which in turn are used to set a
rejection threshold.
While a thresholding approach makes sense, other
approaches are feasible as well. For instance, a sec-
ond classifier could be used to decide whether or not
to accept the top ranking response. The classifier
could take into account such features as the spread
in scores among the responses, the number classi-
fied as acceptable, the drop between the top score
and the second-ranked score, etc.
7 Evaluation
We evaluated the response-based method using the
data described in section 3, N-best lists with a maxi-
mum length of 10, and an SVM with a linear kernel.
We note that, in the live system, two-pass recogni-
tion is performed for some utterances, in which a
key concept recognized in the first pass (e.g., a city
name) causes a dynamic update to the contents of
a class in the n-gram language model (e.g., a set
of street names) for the second pass?as in the ut-
terance Show me thirty two Vassar Street in Cam-
bridge where the city name (Cambridge) triggers
a second pass in which the streets in that city are
given a higher weight. This two-pass approach has
been shown previously to decrease word and con-
cept error rates (Gruenstein and Seneff, 2006), even
though it can be susceptible to errors in understand-
ing. However, since all street names, for example,
are active in the vocabulary at all times, the two-
pass approach is not strictly necessary to arrive at
the correct hypotheses. Hence, for simplicity, in the
experiments reported here, we do not integrate the
two-pass approach?as this would require us to po-
tentially do a second recognition pass for every can-
didate response. In a live system, a good strategy
might be to consider a second recognition pass based
on the top few candidate responses alone, which
would produce a new set of candidates to be scored.
We performed 38-fold cross validation, where in
each case the held-out test set was comprised of all
the utterances of a single user. This ensured that we
obtained an accurate prediction of a novel user?s ex-
perience, although it meant that the test sets were not
of equal size. We calculated F-measure for each test
set, using the methodology described in figure 4 (in
the appendix).
7.1 Baseline
As a baseline, we made use of the existing confi-
dence module in the SUMMIT recognizer (Hazen
et al, 2002). The module uses a linear projection
model to produce an utterance level confidence score
based on 15 features derived from recognizer scores,
16
Method F
Recognition Confidence (Baseline) .62
Recog Features Only .62
Recog + Distributional .67
Recog + Response .71*
Recog + Response + Distributional .72**
Table 2: Average F-measures obtained via per-user
cross-validation of the response-based confidence scor-
ing method using the feature sets described in Section 5,
as compared to a baseline system which chooses the top
hypothesis if the recognizer confidence score exceeds an
optimized rejection threshold. The starred scores are a
statistically significant (* indicates p < .05, ** indicates
p < .01) improvement over the baseline, as determined
by a paired t-test.
and from comparing hypotheses on the N-best list.
In our evaluation, the module was trained and tested
on the same data as the SVM model using cross-
validation.
An optimal rejection threshold was determined,
as for the SVM method, using the training data with
F-measure as the objective function. For each utter-
ance, if the confidence score exceeded the threshold,
then the response evoked from the top hypothesis on
the N-best list was chosen.
7.2 Results
Table 2 compares the baseline recognizer confidence
module to our response-based confidence annotator.
The method was evaluated using several subsets of
the features listed in Table 1. Using features derived
from the recognizer only, we obtain results compa-
rable to the baseline. Adding the response and dis-
tributional features yields a 16% improvement over
the baseline system, which is statistically significant
with p < .01 according to a paired t-test. While the
distributional features appear to be helpful, the fea-
ture values derived from the response itself are the
most beneficial, as they allow for a statistically sig-
nificant improvement over the baseline when paired
on their own with the recognizer-derived features.
Figure 2 plots ROC curves comparing the perfor-
mance of the baseline model to the best response-
based model. The curves were obtained by varying
the value of the rejection threshold. We observe that
the response-based model outperforms the baseline
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True
 Pos
itive 
Rate
 
 
Recognition + Response + DistributionalRecognition Confidence (Baseline)
Figure 2: Receiver Operator Characteristic (ROC) curves
(averaged across each cross-validation fold) comparing
the baseline to the best response-based model.
no matter what we set our tolerance for false posi-
tives to be.
The above results were obtained by using an SVM
with a linear kernel, where feature values were nor-
malized to be on the unit interval. We also tried
using a quadratic kernel, retaining the raw feature
values, and reducing the number of binary features
by manually binning the non-numeric feature val-
ues. Each change resulted in a slight decrease in
F-measure.
8 Conclusion and Future Work
We recast the problem of choosing among an N-best
list of recognition hypotheses as one of choosing the
best candidate system response which can be gen-
erated from the recognition hypotheses on that list.
We then demonstrated a framework for assigning
confidence scores to those responses, by using the
scores output by an SVM trained to discriminate be-
tween acceptable and unacceptable responses. The
classifier was trained using a set of features derived
from the speech recognizer, culled from the genera-
tion of each response, and calculated based on each
response?s distribution. We tested our methods us-
ing data collected by users interacting with the City
Browser multimodal dialogue system, and showed
that they lead to a significant improvement over a
baseline which makes an acceptance decision based
on an utterance-level recognizer confidence score.
The technique developed herein could be refined
in several ways. First and foremost, it may well be
17
possible to find additional features with discrimina-
tory power. Also, the decision as to whether or not
to choose the top-scoring response could potentially
be improved by choosing a more appropriate metric
than F-measure as the objective function, or perhaps
by using a second classifier at this stage.
Finally, our experiments were performed off-line.
In order to better test the approach, we plan to de-
ploy the classifier as a component in the running di-
alogue system. This presents some processing time
constraints (as multiple candidate responses must be
generated); and it introduces the confounding factor
of working with a recognizer that can make multi-
ple recognition passes after language model recon-
figuration. These challenges should be tractable for
N-best lists of modest length.
Acknowledgments
Thank you to Stephanie Seneff for her guidance
and advice. Thanks to Timothy J. Hazen for his
assistance with the confidence module. Thanks to
Ali Mohammad for discussions about the machine
learning aspects of this paper and his comments on
drafts. And thanks to four anonymous reviewers for
constructive criticism. This research is sponsored
by the T-Party Project, a joint research program be-
tween MIT and Quanta Computer Inc., Taiwan.
References
Dan Bohus and Alex Rudnicky. 2002. Integrating mul-
tiple knowledge sources for utterance-level confidence
annotation in the CMU Communicator spoken dialog
system. Technical Report CS-190, Carnegie Mellon
University.
Dan Bohus and Alexander I. Rudnicky. 2005. A princi-
pled approach for rejection threshold optimization in
spoken dialog systems. In Proc. of INTERSPEECH.
Lin Chase. 1997. Word and acoustic confidence annota-
tion for large vocabulary speech recognition. In Proc.
of 5th European Conference on Speech Communica-
tion and Technology, pages 815?818.
Ananlada Chotimongkol and Alexander I. Rudnicky.
2001. N-best speech hypotheses reordering using lin-
ear regression. In Proc. of 7th European Conference
on Speech Communication and Technology.
Grace Chung, Stephanie Seneff, Chao Wang, and Lee
Hetherington. 2004. A dynamic vocabulary spoken
dialogue interface. In Proc. of INTERSPEECH, pages
327?330.
Ed Filisko and Stephanie Seneff. 2003. A context res-
olution server for the Galaxy conversational systems.
In Proc. of EUROSPEECH.
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recognition
performance in spoken dialogue systems. In Proc. of
Association for Computational Linguistics.
Alexander Gruenstein and Stephanie Seneff. 2006.
Context-sensitive language modeling for large sets of
proper nouns in multimodal dialogue systems. In
Proc. of IEEE/ACL 2006 Workshop on Spoken Lan-
guage Technology.
Alexander Gruenstein and Stephanie Seneff. 2007. Re-
leasing a multimodal dialogue system into the wild:
User support mechanisms. In Proc. of the 8th SIGdial
Workshop on Discourse and Dialogue, pages 111?119.
Alexander Gruenstein, Stephanie Seneff, and Chao
Wang. 2006. Scalable and portable web-based
multimodal dialogue interaction with geographical
databases. In Proc. of INTERSPEECH.
Timothy J. Hazen, Stephanie Seneff, and Joseph Po-
lifroni. 2002. Recognition confidence scoring and
its use in speech understanding systems. Computer
Speech and Language, 16:49?67.
Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2000. Predicting automatic speech recognition perfor-
mance using prosodic cues. In Proc. of NAACL, pages
218 ? 225.
Michael Niemann, Sarah George, and Ingrid Zukerman.
2005. Towards a probabilistic, multi-layered spoken
language interpretation system. In Proc. of 4th IJCAI
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, pages 8?15.
Rube?n San-Segundo, Bryan Pellom, Wayne Ward, and
Jose? M. Pardo. 2000. Confidence measures for dia-
logue management in the CU Communicator System.
In Proc. of ICASSP.
Stephanie Seneff. 1992. TINA: A natural language sys-
tem for spoken language applications. Computational
Linguistics, 18(1):61?86.
Stephanie Seneff. 2002. Response planning and gen-
eration in the MERCURY flight reservation system.
Computer Speech and Language, 16:283?312.
Marilyn Walker, Jerry Wright, and Irene Langkilde.
2000. Using natural language processing and dis-
course features to identify understanding errors in a
spoken dialogue system. In Proc. 17th International
Conf. on Machine Learning, pages 1111?1118.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
18
R
ec
og
n
it
io
n
N
-b
es
t
H
yp
ot
h
es
is
R
an
k
S
t
S
a
S
l
?
D
S
R
es
p
on
se
P
ar
se
?
th
ir
ty
 tw
o 
va
ss
al
st
re
et
 in
 c
am
br
id
ge
0
45
.3
28
.5
26
.5
?
R
0
F
U
L
L
th
ir
ty
 tw
o 
va
ss
ar
st
re
et
 in
 c
am
br
id
ge
1
45
.0
27
.1
30
.5
?
R
1
F
U
L
L
th
ir
ty
 tw
o 
va
ss
ar
st
re
et
 in
 in
 c
am
br
id
ge
2
44
.2
26
.0
30
.4
?
R
1
R
O
B
U
ST
at
 th
ir
ty
 tw
o 
va
ss
ar
st
re
et
 <
no
is
e>
3
40
.1
26
.5
29
.4
?
R
1
F
U
L
L
at
th
ir
ty
 tw
o 
va
ss
al
 s
tr
ee
t i
n 
ca
m
br
id
ge
4
39
.5
26
.3
29
.0
?
R
1
F
U
L
L
th
ir
ty
 tw
o 
va
ss
ar
st
re
et
 c
am
br
id
ge
<
no
is
e>
5
38
.4
25
.8
28
.4
?
R
1
F
U
L
L
th
ir
ty
 tw
o 
va
ss
ar
st
re
et
 in
 c
an
to
n
6
38
.0
25
.8
28
.3
?
R
2
F
U
L
L
th
ir
ty
 tw
o 
va
ss
al
 s
tr
ee
t i
n 
in
 c
an
to
n
7
33
.5
22
.5
27
.5
?
R
3
R
O
B
U
ST
tw
en
ty
 v
as
sa
r
in
 s
tr
ee
t i
n 
zo
om
8
32
.4
22
.3
26
.3
?
R
4
N
O
N
E
th
ir
ty
 tw
o 
va
ss
ar
st
re
et
 in
 c
am
br
id
ge
<
no
is
e>
9
32
.0
19
.5
26
.7
?
R
1
F
U
L
L
R
es
p
on
se
 L
is
t
R
es
p
on
se
R
an
k
S
t
S
a
S
l
%
T
op
3
%
T
op
5
D
is
t.
P
ar
se
?
SV
M
S
co
re
R
0
0
45
.3
28
.5
26
.5
.3
3
.8
5
F
U
L
L
?
.4
2
R
1
1
45
.0
27
.1
30
.5
.6
6
.2
5
F
U
L
L
?
.7
3
?
R
1
R
2
6
38
.0
25
.8
28
.3
0.
0
0.
0
5
F
U
L
L
?
-.
32
R
3
7
33
.5
22
.5
27
.5
0.
0
0.
0
5
R
O
B
U
ST
?
-.
55
R
4
8
32
.4
22
.3
36
.3
0.
0
0.
0
5
N
O
N
E
?
-.
92
Figure 3: The feature extraction and classification process. The top half of the digram shows how an N-best list
of recognizer hypotheses, with associated scores from the recognizer, are processed by the dialogue system (DS) to
produce a list of responses. Associated with each response is a set of feature values derived from the response itself,
as well as the process of evoking the response (e.g. the parse status). The bottom half of the figure shows how the
unique responses are collapsed into a list. Each response in the list inherits the best recognition scores available from
hypotheses evoking that response; each also has feature values associated with it derived from the distribution of that
response on the recognizer N-best list. Each set of feature values is classified by a Support Vector Machine, and the
resulting score is used to rank the responses. If the highest scoring response exceeds the rejection threshold, then it is
chosen as the system?s response.
19
Feature Possible Values
response type
top response type
geography, give directions, goodbye, greetings, help directions did not understand from place,
help directions did not understand to place, help directions no to or from place,
help directions subway, hide subway map, history cleared, list cuisine, list name, list street,
no circled data, no data, no match near, non unique near, ok, panning down, panning east,
panning south, panning up, panning west, presupp failure, provide city for address, refined result,
reject or give help, show address, show subway map, speak properties, speak property,
speak verify false, speak verify true, welcome gui, zooming, zooming in, zooming out
POI type none, city, museum, neighborhood, restaurant, subway station
parse status no parse, robust parse, full parse
geographical filter none, address, circle, line, list item, map bounds, museum, neighborhood, point, polygon, restaurant,
subway station, city
Table 3: The set of possible values for non-numerical features, which are converted to sets of binary features.
R
es
p
on
se
S
co
re
T
yp
e
L
ab
el
R
0
S
0
sp
ea
k_
pr
op
er
ty
ac
ce
pt
ab
le
R
1
S
1
li
st
_c
ui
si
ne
un
ac
ce
pt
ab
le
R
2
S
2
sp
ea
k_
pr
op
er
ty
un
ac
ce
pt
ab
le
C
as
e 
I:
 E
xa
m
p
le
 R
an
ke
d
R
es
p
on
se
 L
is
t
C
as
e 
I
R
0
is
 a
cc
ep
ta
bl
e
an
d 
is
 n
ot
 r
ej
ec
t
S 0
? 
T
?
T
.P
.
S 0
<
 T
?
F
.N
.
R
es
p
on
se
S
co
re
T
yp
e
L
ab
el
R
0
S
0
sp
ea
k_
pr
op
er
ty
un
ac
ce
pt
ab
le
R
1
S
1
li
st
_c
ui
si
ne
un
ac
ce
pt
ab
le
R
2
S
2
sp
ea
k_
pr
op
er
ty
un
ac
ce
pt
ab
le
R
3
S
3
re
je
ct
un
ac
ce
pt
ab
le
R
4
S
4
zo
om
in
g_
ou
t
un
ac
ce
pt
ab
le
C
as
e 
II
:
E
xa
m
p
le
R
an
ke
d
R
es
p
on
se
 L
is
t
C
as
e 
II
N
o 
ca
nd
id
at
e 
re
sp
on
se
s 
ac
ce
pt
ab
le
, 
or
 a
cc
ep
ta
bl
e
re
sp
on
se
 is
 r
ej
ec
t
(a
) 
R
0
is
 n
ot
 r
ej
ec
t
S 0
? 
T
 ?
F
.P
.
S 0
<
 T
 ?
T
.N
.
(b
) 
R
0
is
 r
ej
ec
t
S 0
? 
T
 ?
T
.N
.
S 0
<
 T
 ?
T
.N
.
R
es
p
on
se
S
co
re
T
yp
e
L
ab
el
R
0
S
0
sp
ea
k_
pr
op
er
ty
un
ac
ce
pt
ab
le
R
1
S
1
li
st
_c
ui
si
ne
ac
ce
pt
ab
le
R
2
S
2
sp
ea
k_
pr
op
er
ty
un
ac
ce
pt
ab
le
R
3
S
3
re
je
ct
un
ac
ce
pt
ab
le
R
4
S
4
zo
om
in
g_
ou
t
un
ac
ce
pt
ab
le
C
as
e 
II
I:
E
xa
m
p
le
R
an
ke
d
R
es
p
on
se
 L
is
t
C
as
e 
II
I
R
n
(w
it
h 
n 
>
 0
) 
 is
 a
cc
ep
ta
bl
e
an
d 
is
 n
ot
 r
ej
ec
t
(a
) 
R
0
is
 n
ot
 r
ej
ec
t
S 0
? 
T
 ?
F
.P
.
S 0
<
 T
 ?
F
.N
.
(b
) 
R
0
is
 r
ej
ec
t
S 0
? 
T
 ?
F
.N
.
S 0
<
 T
 ?
F
.N
.
Figure 4: Algorithm for calculating the F-measure confusion matrix of True Positives (T.P.), False Positives (F.P.),
True Negatives (T.N.), and False Negatives (F.N.). The ranking technique described in this paper creates a list of
candidate system responses ranked by their scores. The top scoring response is then accepted if its score exceeds a
threshold T, otherwise all candidate responses are rejected. As such, the problem is not a standard binary decision.
We show all possible outcomes from the ranking process, and note whether each case is counted as a T.P., F.P., T.N.,
or F.N. We note that given this algorithm for calculating the confusion matrix, no matter how we set the threshold T,
F-measure will always be penalized if Case III occurs.
20
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Multimodal Home Entertainment Interface via a Mobile Device
Alexander Gruenstein Bo-June (Paul) Hsu James Glass Stephanie Seneff
Lee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean Liu
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar St, Cambridge, MA 02139 USA
http://www.sls.csail.mit.edu/
Abstract
We describe a multimodal dialogue system for
interacting with a home entertainment center
via a mobile device. In our working proto-
type, users may utilize both a graphical and
speech user interface to search TV listings,
record and play television programs, and listen
to music. The developed framework is quite
generic, potentially supporting a wide variety
of applications, as we demonstrate by integrat-
ing a weather forecast application. In the pro-
totype, the mobile device serves as the locus
of interaction, providing both a small touch-
screen display, and speech input and output;
while the TV screen features a larger, richer
GUI. The system architecture is agnostic to
the location of the natural language process-
ing components: a consistent user experience
is maintained regardless of whether they run
on a remote server or on the device itself.
1 Introduction
People have access to large libraries of digital con-
tent both in their living rooms and on their mobile
devices. Digital video recorders (DVRs) allow peo-
ple to record TV programs from hundreds of chan-
nels for subsequent viewing at home?or, increas-
ingly, on their mobile devices. Similarly, having
accumulated vast libraries of digital music, people
yearn for an easy way to sift through them from the
comfort of their couches, in their cars, and on the go.
Mobile devices are already central to accessing
digital media libraries while users are away from
home: people listen to music or watch video record-
ings. Mobile devices also play an increasingly im-
portant role in managing digital media libraries. For
instance, a web-enabled mobile phone can be used to
remotely schedule TV recordings through a web site
or via a custom application. Such management tasks
often prove cumbersome, however, as it is challeng-
ing to browse through listings for hundreds of TV
channels on a small display. Indeed, even on a large
screen in the living room, browsing alphabetically,
or by time and channel, for a particular show using
the remote control quickly becomes unwieldy.
Speech and multimodal interfaces provide a nat-
ural means of addressing many of these challenges.
It is effortless for people to say the name of a pro-
gram, for instance, in order to search for existing
recordings. Moreover, such a speech browsing ca-
pability is useful both in the living room and away
from home. Thus, a natural way to provide speech-
based control of a media library is through the user?s
mobile device itself.
In this paper we describe just such a prototype
system. A mobile phone plays a central role in pro-
viding a multimodal, natural language interface to
both a digital video recorder and a music library.
Users can interact with the system?presented as a
dynamic web page on the mobile browser?using
the navigation keys, the stylus, or spoken natural
language. In front of the TV, a much richer GUI is
also available, along with support for playing video
recordings and music.
In the prototype described herein, the mobile de-
vice serves as the locus of natural language in-
teraction, whether a user is in the living room or
walking down the street. Since these environments
may be very different in terms of computational re-
1
sources and network bandwidth, it is important that
the architecture allows for multiple configurations in
terms of the location of the natural language pro-
cessing components. For instance, when a device
is connected to a Wi-Fi network at home, recogni-
tion latency may be reduced by performing speech
and natural language processing on the home me-
dia server. Moreover, a powerful server may enable
more sophisticated processing techniques, such as
multipass speech recognition (Hetherington, 2005;
Chung et al, 2004), for improved accuracy. In sit-
uations with reduced network connectivity, latency
may be improved by performing speech recognition
and natural language processing tasks on the mobile
device itself. Given resource constraints, however,
less detailed acoustic and language models may be
required. We have developed just such a flexible ar-
chitecture, with many of the natural language pro-
cessing components able to run on either a server or
the mobile device itself. Regardless of the configu-
ration, a consistent user experience is maintained.
2 Related Work
Various academic researchers and commercial busi-
nesses have demonstrated speech-enabled interfaces
to entertainment centers. A good deal of the work
focuses on adding a microphone to a remote con-
trol, so that speech input may be used in addition
to a traditional remote control. Much commercial
work, for example (Fujita et al, 2003), tends to fo-
cus on constrained grammar systems, where speech
input is limited to a small set of templates corre-
sponding to menu choices. (Berglund and Johans-
son, 2004) present a remote-control based speech
interface for navigating an existing interactive tele-
vision on-screen menu, though experimenters man-
ually transcribed user utterances as they spoke in-
stead of using a speech recognizer. (Oh et al, 2007)
present a dialogue system for TV control that makes
use of concept spotting and statistical dialogue man-
agement to understand queries. A version of their
system can run independently on low-resource de-
vices such as PDAs; however, it has a smaller vo-
cabulary and supports a limited set of user utterance
templates. Finally, (Wittenburg et al, 2006) look
mainly at the problem of searching for television
programs using speech, an on-screen display, and a
remote control. They explore a Speech-In List-Out
interface to searching for episodes of television pro-
grams.
(Portele et al, 2003) depart from the model of
adding a speech interface component to an exist-
ing on-screen menu. Instead, they a create a tablet
PC interface to an electronic program guide, though
they do not use the television display as well. Users
may search an electronic program guide using con-
straints such as date, time, and genre; however, they
can?t search by title. Users can also perform typi-
cal remote-control tasks like turning the television
on and off, and changing the channel. (Johnston et
al., 2007) also use a tablet PC to provide an inter-
face to television content?in this case a database of
movies. The search can be constrained by attributes
such as title, director, or starring actors. The tablet
PC pen can be used to handwrite queries and to point
at items (such as actor names) while the user speaks.
We were also inspired by previous prototypes in
which mobile devices have been used in conjunc-
tion with larger, shared displays. For instance, (Paek
et al, 2004) demonstrate a framework for building
such applications. The prototype we demonstrate
here fits into their ?Jukebox? model of interaction.
Interactive workspaces, such as the one described in
(Johanson et al, 2002), also demonstrate the utility
of integrating mobile and large screen displays. Our
prototype is a departure from these systems, how-
ever, in that it provides for spoken interactions.
Finally, there is related work in the use of mobile
devices for various kinds of search. For instance, of-
ferings from Microsoft (Acero et al, 2008), Vlingo,1
and Promptu2 allow users to search for items like
businesses and songs using their mobile phones.
These applications differ from ours in that speech
is used only for search, without any accompanying
command and control capabilities. Also, these ser-
vices do not allow interaction with your own de-
vices at home. Efforts have been made to use mo-
bile devices for control of devices in the home, such
as in (Nichols and Myers, 2006), however these ef-
forts have not involved the use of speech as an input
modality.
1http://www.vlingo.com
2http://www.promptu.com
2
Na
vig
ati
on
 pa
d o
r s
tyl
us 
can
 
be
 us
ed
 to
 na
vig
ate
 G
UI
Sta
tus
 ic
on
 in
dic
ate
s c
on
ne
cti
on
 to
 se
rve
r, 
mi
cro
ph
on
e i
co
n w
he
n l
ist
en
ing
Ho
ld-
to-
tal
k 
bu
tto
n
Op
era we
b 
bro
ws
er
Sy
nc
hro
niz
ed
 se
lec
tio
n o
n m
ob
ile
 an
d 
tel
ev
isi
on
 di
spl
ay
s
Fir
efo
x
we
b b
row
ser
Ta
bs 
all
ow
 ea
sy 
sw
itc
hin
g 
am
on
g d
om
ain
s
Me
dia
 pl
ay
ba
ck
 
co
ntr
ols
Figure 1: User interface overview.
(a) Program Guide
(b) Scheduled Recordings
(c) Music Library
U: What?s on tonight at seven o?clock?
S: I found seventy entries. [Displays program guide.]
U: Are there any action movies on HBO tomorrow?
S: I found two entries. [Displays results.]
U: When is American Idol on?
S: I found three entries. [Displays results.]
U: Record the first one.
S: I have added American Idol to the list of programs to be
recorded. [Displays recording settings.]
U: Please record Nova on Wednesday night.
S: I have added Nova to the list of programs to be recorded.
[Displays recording settings.]
U: Show all my scheduled recordings.
S: There are seven scheduled recordings. [Shows list.]
U: Delete BBC World News.
S: I have deleted the scheduled recording: BBC World
News. [Updates List.]
U: Show my videos.
S: [Displays list of videos.]
U: Delete Frasier.
S: I have deleted the recording Frasier. [Updates list.]
U: Play Grey?s Anatomy.
S: Playing Grey?s Anatomy. [Plays recording.]
U: Stop Playing.
S: Stopping. [Stops playback.]
U: [Clicks on Music tab.]
S: [Displays artist list.]
U: Show albums by Billie Holiday.
S: I found three albums by Billie Holiday. [Shows albums.]
U: Please play A Hard Rain?s A-Gonna Fall by Bob Dylan.
S: Playing A Hard Rain?s A-Gonna Fall. [Plays song.]
Figure 2: Screenshots and an example interaction. Utterances are labeled with U for user and S for system.
3
3 User Experience
Our current prototype system implements the basic
functionalities that one expects from a home enter-
tainment center. Users can navigate through and
record programs from the television?s electronic pro-
gram guide, manage recording settings, and play
recorded videos. They can also browse and listen to
selections from their music libraries. However, un-
like existing prototypes, ours employs a smartphone
with a navigation pad, touch-sensitive screen, and
built-in microphone as the remote control. Figure 1
provides an overview of the graphical user interface
on both the TV and mobile device.
Mirroring the TV?s on-screen display, the proto-
type system presents a reduced view on the mobile
device with synchronized cursors. Users can navi-
gate the hierarchical menu structure using the arrow
keys or directly click on the target item with the sty-
lus. While away from the living room, or when a
recording is playing full screen, users can browse
and manage their media libraries using only the mo-
bile device.
While the navigation pad and stylus are great for
basic navigation and control, searching for media
with specific attributes, such as title, remains cum-
bersome. To facilitate such interactions, the cur-
rent system supports spoken natural language inter-
actions. For example, the user can press the hold-to-
talk button located on the side of the mobile device
and ask ?What?s on the National Geographic Chan-
nel this afternoon?? to retrieve a list of shows with
the specified channel and time. The system responds
with a short verbal summary ?I found six entries on
January seventh? and presents the resulting list on
both the TV and mobile displays. The user can then
browse the list using the navigation pad or press the
hold-to-talk button to barge in with another com-
mand, e.g. ?Please record the second one.? Depress-
ing the hold-to-talk button not only terminates any
current spoken response, but also mutes the TV to
minimize interference with speech recognition. As
the previous example demonstrates, contextual in-
formation is used to resolve list position references
and disambiguate commands.
The speech interface to the user?s music library
works in a similar fashion. Users can search by
artist, album, and song name, and then play the
songs found. To demonstrate the extensibility of
the architecture, we have also integrated an exist-
ing weather information system (Zue et al, 2000),
which has been previously deployed as a telephony
application. Users simply click on the Weather tab
to switch to this domain, allowing them to ask a wide
range of weather queries. The system responds ver-
bally and with a simple graphical forecast.
To create a natural user experience, we designed
the multimodal interface to allow users to seam-
lessly switch among the different input modalities
available on the mobile device. Figure 2 demon-
strates an example interaction with the prototype, as
well as several screenshots of the user interface.
4 System Architecture
The system architecture is quite flexible with re-
gards to the placement of the natural language pro-
cessing components. Figure 3 presents two possible
configurations of the system components distributed
across the mobile device, home media server, and
TV display. In 3(a), all speech recognition and nat-
ural language processing components reside on the
server, with the mobile device acting as the micro-
phone, speaker, display, and remote control. In 3(b),
the speech recognizer, language understanding com-
ponent, language generation component, and text-
to-speech (TTS) synthesizer run on the mobile de-
vice. Depending on the capabilities of the mobile
device and network connection, different configu-
rations may be optimal. For instance, on a power-
ful device with slow network connection, recogni-
tion latency may be reduced by performing speech
recognition and natural language processing on the
device. On the other hand, streaming audio via a fast
wireless network to the server for processing may
result in improved accuracy.
In the prototype system, flexible and reusable
speech recognition and natural language processing
capabilities are provided via generic components de-
veloped and deployed in numerous spoken dialogue
systems by our group, with the exception of an off-
the-shelf speech synthesizer. Speech input from the
mobile device is recognized using the landmark-
based SUMMIT system (Glass, 2003). The result-
ing N-best hypotheses are processed by the TINA
language understanding component (Seneff, 1992).
4
Gala
xy
Spee
ch?R
ecog
nize
r
Lang
uage
?Und
ersta
ndin
g
Dialo
gue?
Man
ager
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Web
?
Serv
er
Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Gala
xy
Dialo
gue?
Man
ager
Web
?
Serv
er Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Spee
ch?R
ecog
nize
r
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Lang
uage
?Und
ersta
ndin
g
(a) (b)
Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server,
while in (b) processing is primarily performed on the device.
Based on the resulting meaning representation, the
dialogue manager (Polifroni et al, 2003) incorpo-
rates contextual information (Filisko and Seneff,
2003), and then determines an appropriate response.
The response consists of an update to the graph-
ical display, and a spoken system response which
is realized via the GENESIS (Baptist and Seneff,
2000) language generation module. To support on-
device processing, all the components are linked via
the GALAXY framework (Seneff et al, 1998) with
an additional Mobile Manager component responsi-
ble for coordinating the communication between the
mobile device and the home media server.
In the currently deployed system, we use a mo-
bile phone with a 624 MHz ARM processor run-
ning the Windows Mobile operating system and
Opera Mobile web browser. The TV program and
music databases reside on the home media server
running GNU/Linux. The TV program guide data
and recording capabilities are provided via MythTV,
a full-featured, open-source digital video recorder
software package.3 Daily updates to the program
guide information typically contain hundreds of
unique channel names and thousands of unique pro-
gram names. The music library is comprised of
5,000 songs from over 80 artists and 13 major gen-
res, indexed using the open-source text search en-
gine Lucene.4 Lastly, the TV display can be driven
by a web browser on either the home media server or
a separate computer connected to the server via a fast
Ethernet connection, for high quality video stream-
ing.
3http://www.mythtv.org/
4http://lucene.apache.org/
While the focus of this paper is on the natural lan-
guage processing and user interface aspects of the
system, our work is actually situated within a larger
collaborative project at MIT that also includes sim-
plified device configuration (Mazzola Paluska et al,
2008; Mazzola Paluska et al, 2006), transparent ac-
cess to remote servers (Ford et al, 2006), and im-
proved security.
5 Mobile Natural Language Components
Porting the implementation of the various speech
recognizer and natural language processing com-
ponents to mobile devices with limited computa-
tion and memory presents both a research and en-
gineering challenge. Instead of creating a small vo-
cabulary, fixed phrase dialogue system, we aim to
support?on the mobile device?the same flexible
and natural language interactions currently available
on our desktop, tablet, and telephony systems; see
e.g., (Gruenstein et al, 2006; Seneff, 2002; Zue et
al., 2000). In this section, we summarize our ef-
forts thus far in implementing the SUMMIT speech
recognizer and TINA natural language parser. Ports
of the GENESIS language generation system and of
our dialogue manager are well underway, and we ex-
pect to have these components working on the mo-
bile device in the near future.
5.1 PocketSUMMIT
To significantly reduce the memory footprint and
overall computation, we chose to reimplement our
segment-based speech recognizer from scratch, uti-
lizing fixed-point arithmetic, parameter quantiza-
tion, and bit-packing in the binary model files.
The resulting PocketSUMMIT recognizer (Hether-
5
ington, 2007) utilizes only the landmark features,
initially forgoing segment features such as phonetic
duration, as they introduce algorithmic complexities
for relatively small word error rate (WER) improve-
ments.
In the current system, we quantize the mean and
variance of each Gaussian mixture model dimension
to 5 and 3 bits, respectively. Such quantization not
only results in an 8-fold reduction in model size, but
also yields about a 50% speedup by enabling table
lookups for Gaussian evaluations. Likewise, in the
finite-state transducers (FSTs) used to represent the
language model, lexical, phonological, and class di-
phone constraints, quantizing the FST weights and
bit-packing not only compress the resulting binary
model files, but also reduce the processing time with
improved processor cache locality.
In the aforementioned TV, music, and weather do-
mains with a moderate vocabulary of a few thou-
sand words, the resulting PocketSUMMIT recog-
nizer performs in approximately real-time on 400-
600 MHz ARM processors, using a total of 2-4
MB of memory, including 1-2 MB for memory-
mapped model files. Compared with equivalent non-
quantized models, PocketSUMMIT achieves dra-
matic improvements in speed and memory while
maintaining comparable WER performance.
5.2 PocketTINA
Porting the TINA natural language parser to mobile
devices involved significant software engineering to
reduce the memory and computational requirements
of the core data structures and algorithms. TINA
utilizes a best-first search that explores thousands of
partial parses when processing an input utterance.
To efficiently manage memory allocation given the
unpredictability of pruning invalid parses (e.g. due
to subject-verb agreement), we implemented a mark
and sweep garbage collection mechanism. Com-
bined with a more efficient implementation of the
priority queue and the use of aggressive ?beam?
pruning, the resulting PocketTINA system provides
identical output as server-side TINA, but can parse
a 10-best recognition hypothesis list into the corre-
sponding meaning representation in under 0.1 sec-
onds, using about 2 MB of memory.
6 Rapid Dialogue System Development
Over the course of developing dialogue systems for
many domains, we have built generic natural lan-
guage understanding components that enable the
rapid development of flexible and natural spoken di-
alogue systems for novel domains. Creating such
prototype systems typically involves customizing
the following to the target domain: recognizer lan-
guage model, language understanding parser gram-
mar, context resolution rules, dialogue management
control script, and language generation rules.
Recognizer Language Model Given a new do-
main, we first identify a set of semantic classes
which correspond to the back-end application?s
database, such as artist, album, and genre. Ideally,
we would have a corpus of tagged utterances col-
lected from real users. However, when building pro-
totypes such as the one described here, little or no
training data is usually available. Thus, we create
a domain-specific context-free grammar to generate
a supplemental corpus of synthetic utterances. The
corpus is used to train probabilities for the natural
language parsing grammar (described immediately
below), which in turn is used to derive a class n-
gram language model (Seneff et al, 2003).
Classes in the language model which corre-
spond to contents of the database are marked as
dynamic, and are populated at runtime from the
database (Chung et al, 2004; Hetherington, 2005).
Database entries are heuristically normalized into
spoken forms. Pronunciations not in our 150,000
word lexicon are automatically generated (Seneff,
2007).
Parser Grammar The TINA parser uses a prob-
abilistic context-free grammar enhanced with sup-
port for wh-movement and grammatical agreement
constraints. We have developed a generic syntac-
tic grammar by examining hundreds of thousands
of utterances collected from real user interactions
with various existing dialogue systems. In addition,
we have developed libraries which parse and inter-
pret common semantic classes like dates, times, and
numbers. The grammar and semantic libraries pro-
vide good coverage for spoken dialogue systems in
database-query domains.
6
To build a grammar for a new domain, a devel-
oper extends the generic syntactic grammar by aug-
menting it with domain-specific semantic categories
and their lexical entries. A probability model which
conditions each node category on its left sibling and
parent is then estimated from a training corpus of
utterances (Seneff et al, 2003).
At runtime, the recognizer tags the hypothesized
dynamic class expansions with their class names,
allowing the parser grammar to be independent of
the database contents. Furthermore, each semantic
class is designated either as a semantic entity, or as
an attribute associated with a particular entity. This
enables the generation of a semantic representation
from the parse tree.
Dialogue Management & Language Generation
Once an utterance is recognized and parsed, the
meaning representation is passed to the context res-
olution and dialogue manager component. The con-
text resolution module (Filisko and Seneff, 2003)
applies generic and domain-specific rules to re-
solve anaphora and deixis, and to interpret frag-
ments and ellipsis in context. The dialogue man-
ager then interacts with the application back-end
and database, controlled by a script customized for
the domain (Polifroni et al, 2003). Finally, the
GENESIS module (Baptist and Seneff, 2000) ap-
plies domain-specific rules to generate a natural lan-
guage representation of the dialogue manager?s re-
sponse, which is sent to a speech synthesizer. The
dialogue manager also sends an update to the GUI,
so that, for example, the appropriate database search
results are displayed.
7 Mobile Design Challenges
Dialogue systems for mobile devices present a
unique set of design challenges not found in tele-
phony and desktop applications. Here we describe
some of the design choices made while developing
this prototype, and discuss their tradeoffs.
7.1 Client/Server Tradeoffs
Towards supporting network-less scenarios, we have
begun porting various natural language processing
components to mobile platforms, as discussed in
Section 5. Having efficient mobile implementations
further allows the natural language processing tasks
to be performed on either the mobile device or the
server. While building the prototype, we observed
that the Wi-Fi network performance can often be un-
predictable, resulting in erratic recognition latency
that occasionally exceeds on-device recognition la-
tency. However, utilizing the mobile processor for
computationally intensive tasks rapidly drains the
battery. Currently, the component architecture in the
prototype system is pre-configured. A more robust
implementation would dynamically adjust the con-
figuration to optimize the tradeoffs among network
use, CPU utilization, power consumption, and user-
perceived latency/accuracy.
7.2 Speech User Interface
As neither open-mic nor push-to-talk with automatic
endpoint detection is practical on mobile devices
with limited battery life, our prototype system em-
ploys a hold-to-talk hardware button for microphone
control. To guide users to speak commands only
while the button is depressed, a short beep is played
as an earcon both when the button is pushed and
released. Since users are less likely to talk over
short audio clips, the use of earcons mitigates the
tendency for users to start speaking before pushing
down the microphone button.
In the current system, media audio is played over
the TV speakers, whereas TTS output is sent to
the mobile device speakers. To reduce background
noise captured from the mobile device?s far-field mi-
crophone, the TV is muted while the microphone
button is depressed. Unlike telephony spoken di-
alogue systems where the recognizer has to con-
stantly monitor for barge-in, the use of a hold-to-
talk button significantly simplifies barge-in support,
while reducing power consumption.
7.3 Graphical User Interface
In addition to supporting interactive natural lan-
guage dialogues via the spoken user interface, the
prototype system implements a graphical user in-
terface (GUI) on the mobile device to supplement
the TV?s on-screen interface. To faciliate rapid pro-
totyping, we chose to implement both the mobile
and TV GUI using web pages with AJAX (Asyn-
chronous Javascript and XML) techniques, an ap-
proach we have leveraged in several existing mul-
timodal dialogue systems, e.g. (Gruenstein et al,
7
2006; McGraw and Seneff, 2007). The resulting in-
terface is largely platform-independent and allows
display updates to be ?pushed? to the client browser.
As many users are already familiar with the TV?s
on-screen interface, we chose to mirror the same in-
terface on the mobile device and synchronize the
selection cursor. However, unlike desktop GUIs,
mobile devices are constrained by a small display,
limited computational power, and reduced network
bandwidth. Thus, both the page layout and infor-
mation detail were adjusted for the mobile browser.
Although AJAX is more responsive than traditional
web technology, rendering large formatted pages?
such as the program guide grid?is often still un-
acceptably slow. In the current implementation, we
addressed this problem by displaying only the first
section of the content and providing a ?Show More?
button that downloads and renders the full content.
While browser-based GUIs expedite rapid prototyp-
ing, deployed systems may want to take advantage
of native interfaces specific to the device for more
responsive user interactions. Instead of limiting the
mobile interface to reflect the TV GUI, improved us-
ability may be obtained by designing the interface
for the mobile device first and then expanding the
visual content to the TV display.
7.4 Client/Server Communication
In the current prototype, communication between
the mobile device and the media server consists of
AJAX HTTP and XML-RPC requests. To enable
server-side ?push? updates, the client periodically
pings the server for messages. While such an im-
plementation provides a responsive user interface, it
quickly drains the battery and is not robust to net-
work outages resulting from the device being moved
or switching to power-saving mode. Reestablish-
ing connection with the server further introduces la-
tency. In future implementations, we would like to
examine the use of Bluetooth for lower power con-
sumption, and infrared for immediate response to
common controls and basic navigation.
8 Conclusions & Future Work
We have presented a prototype system that demon-
strates the feasibility of deploying a multimodal,
natural language interface on a mobile device for
browsing and managing one?s home media library.
In developing the prototype, we have experimented
with a novel role for a mobile device?that of a
speech-enabled remote control. We have demon-
strated a flexible natural language understanding ar-
chitecture, in which various processing stages may
be performed on either the server or mobile device,
as networking and processing power considerations
require.
While the mobile platform presents many chal-
lenges, it also provides unique opportunities.
Whereas desktop computers and TV remote controls
tend to be shared by multiple users, a mobile device
is typically used by a single individual. By collect-
ing and adapting to the usage data, the system can
personalize the recognition and understanding mod-
els to improve the system accuracy. In future sys-
tems, we hope to not only explore such adaptation
possibilities, but also study how real users interact
with the system to further improve the user interface.
Acknowledgments
This research is sponsored by the TParty Project,
a joint research program between MIT and Quanta
Computer, Inc.; and by Nokia, as part of a joint MIT-
Nokia collaboration. We are also thankful to three
anonymous reviewers for their constructive feed-
back.
References
A. Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li,
J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proc. of ICASSP.
L. Baptist and S. Seneff. 2000. Genesis-II: A versatile
system for language generation in conversational sys-
tem applications. In Proc. of ICSLP.
A. Berglund and P. Johansson. 2004. Using speech and
dialogue for interactive TV navigation. Universal Ac-
cess in the Information Society, 3(3-4):224?238.
G. Chung, S. Seneff, C. Wang, and L. Hetherington.
2004. A dynamic vocabulary spoken dialogue inter-
face. In Proc. of INTERSPEECH, pages 327?330.
E. Filisko and S. Seneff. 2003. A context resolution
server for the GALAXY conversational systems. In
Proc. of EUROSPEECH.
B. Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea,
F. Kaashoek, and R. Morris. 2006. Persistent personal
names for globally connected mobile devices. In Pro-
ceedings of the 7th USENIX Symposium on Operating
Systems Design and Implementation (OSDI ?06).
8
K. Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono. 2003.
A new digital TV interface employing speech recog-
nition. IEEE Transactions on Consumer Electronics,
49(3):765?769.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17:137?152.
A. Gruenstein, S. Seneff, and C. Wang. 2006. Scalable
and portable web-based multimodal dialogue interac-
tion with geographical databases. In Proc. of INTER-
SPEECH.
I. L. Hetherington. 2005. A multi-pass, dynamic-
vocabulary approach to real-time, large-vocabulary
speech recognition. In Proc. of INTERSPEECH.
I. L. Hetherington. 2007. PocketSUMMIT: Small-
footprint continuous speech recognition. In Proc. of
INTERSPEECH, pages 1465?1468.
B. Johanson, A. Fox, and T. Winograd. 2002. The in-
teractive workspaces project: Experiences with ubiq-
uitous computing rooms. IEEE Pervasive Computing,
1(2):67?74.
M. Johnston, L. F. D?Haro, M. Levine, and B. Renger.
2007. A multimodal interface for access to content in
the home. In Proc. of ACL, pages 376?383.
J. Mazzola Paluska, H. Pham, U. Saif, C. Terman, and
S. Ward. 2006. Reducing configuration overhead with
goal-oriented programming. In PerCom Workshops,
pages 596?599. IEEE Computer Society.
J. Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Ter-
man, and S. Ward. 2008. Structured decomposition of
adapative applications. In Proc. of 6th IEEE Confer-
ence on Pervasive Computing and Communications.
I. McGraw and S. Seneff. 2007. Immersive second lan-
guage acquisition in narrow domains: A prototype IS-
LAND dialogue system. In Proc. of the Speech and
Language Technology in Education Workshop.
J. Nichols and B. A. Myers. 2006. Controlling home and
office appliances with smartphones. IEEE Pervasive
Computing, special issue on SmartPhones, 5(3):60?
67, July-Sept.
H.-J. Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee. 2007.
An intelligent TV interface based on statistical dia-
logue management. IEEE Transactions on Consumer
Electronics, 53(4).
T. Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjans-
son, R. Logan, K. Toyama, and A. Wilson. 2004. To-
ward universal mobile interaction for shared displays.
In Proc. of Computer Supported Cooperative Work.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the automatic generation of mixed-initiative dialogue
systems from web content. In Proc. EUROSPEECH,
pages 193?196.
T. Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge,
and J. te Vrugt. 2003. SmartKom-Home - an ad-
vanced multi-modal interface to home entertainment.
In Proc. of INTERSPEECH.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. IC-
SLP.
S. Seneff, C. Wang, and T. J. Hazen. 2003. Automatic in-
duction of n-gram language models from a natural lan-
guage grammar. In Proceedings of EUROSPEECH.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. Computational Lin-
guistics, 18(1):61?86.
S. Seneff. 2002. Response planning and generation in
the MERCURY flight reservation system. Computer
Speech and Language, 16:283?312.
S. Seneff. 2007. Reversible sound-to-letter/letter-to-
sound modeling based on syllable structure. In Proc.
of HLT-NAACL.
K. Wittenburg, T. Lanning, D. Schwenke, H. Shubin, and
A. Vetro. 2006. The prospects for unrestricted speech
input for TV content search. In Proc. of AVI?06.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J.
Hazen, and L. Hetherington. 2000. JUPITER: A
telephone-based conversational interface for weather
information. IEEE Transactions on Speech and Audio
Processing, 8(1), January.
9
147
148
149
150
151
152
153
154
