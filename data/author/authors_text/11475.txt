Learning Decision Lists with Known Rules for Text Mining
Venkatesan Chakravarthy
IBM India Research Lab
vechakra@in.ibm.com
Sachindra Joshi
IBM India Research Lab
jsachind@in.ibm.com
Ganesh Ramakrishnan
IBM India Research Lab
ganramkr@in.ibm.com
Shantanu Godbole
IBM India Research Lab
shgodbol@in.ibm.com
Sreeram Balakrishnan
IBM Silicon Valley Lab
sreevb@us.ibm.com
Abstract
Many real-world systems for handling unstructured
text data are rule-based. Examples of such systems
are named entity annotators, information extraction
systems, and text classifiers. In each of these appli-
cations, ordering rules into a decision list is an im-
portant issue. In this paper, we assume that a set of
rules is given and study the problem (MaxDL) of or-
dering them into an optimal decision list with respect
to a given training set. We formalize this problem
and show that it is NP-Hard and cannot be approxi-
mated within any reasonable factors. We then propose
some heuristic algorithms and conduct exhaustive ex-
periments to evaluate their performance. In our ex-
periments we also observe performance improvement
over an existing decision list learning algorithm, by
merely re-ordering the rules output by it.
1 Introduction
Rule-based systems have been extensively used for
several problems in text mining. Some problems
in text mining where rule-based systems have been
successfully used are part of speech tagging (Brill,
1992), named entity annotation (Grishman, 1997;
Appelt et al, 1995), information extraction (May-
nard et al, 2001), question answering (Riloff and
Thelen, 2000) and classification (Han et al, 2003; Li
and Yamanishi, 1999; Sasaki and Kita, 1998). Sev-
eral studies have been conducted that compare the
performance of rule-based systems and other ma-
chine learning techniques with mixed results. While
there is no clear winner between the two approaches
in terms of performance, the rule-based approach
is clearly preferred in operational settings (Borth-
wick, 1999; Varadarajan et al, 2002). Rule-based
systems are human comprehensible and can be im-
proved over time. Therefore, it is imperative to de-
velop methods that assist in building rule-based sys-
tems.
A rule-based system consists of a set of rules.
These rules can either be manually designed or
could be learnt from a training set using rule-
induction techniques (J. and G, 1994; Cohen, 1995).
Each rule consists of an antecedent or pattern and
a consequent or predicted annotation. In this paper,
we will restrict our attention to a broad class of rules
in which the antecedent describes a series of condi-
tions on the input item and the consequent specifies
the label that applies to instances covered by the an-
tecedent. The conditions could also be expressed as
patterns in regular or more powerful grammars.
In general, rules could be ambiguous, i.e., multi-
ple rules could cover an instance. A common ap-
proach for resolving this ambiguity is to define an
ordering on the rules (Maynard et al, 2001; Borth-
wick, 1999). A decision list is one such mecha-
nism (Rivest, 1987). A set of rules that are intended
to be interpreted in a sequence is called a decision
list. In other words, a decision list is an ordering of
the given set of rules. Given an instance t, the rules
are applied in the specified order until a pattern of a
rule R covers t. The instance t is assigned the pre-
dicted annotation associated with R.
In this paper, we study the problem of arranging a
given set of rules into the ?best? decision list. Learn-
ing decision lists using training data has been stud-
ied in the past (Rivest, 1987; J. and G, 1994; Cohen,
1995; Li and Yamanishi, 1999). These methods at-
tempt to simultaneously learn rules and their order-
ing. Typically they use separate and conquer (Wit-
ten and Frank, 2005) strategy and order generated
rules as they are discovered. The generation and or-
dering of rules are not considered as two separate
835
tasks. In contrast, we assume that the rules are given
to us and study the problem of arranging them into
an optimal decision list, where optimality is deter-
mined over a training data set. Our approach is mo-
tivated by the observation that in many operational
settings, it is easier and preferred to get a set of rules
designed by domain experts (Lewis et al, 2003). Al-
ternatively, the set of rules can be determined using
existing techniques for rule learning (J. and G, 1994;
Cohen, 1995; Califf and Mooney, 1998). The sepa-
ration of rule ordering from rule generation allows
us to analyze the problem of ordering in detail and
to develop effective methods for rule ordering. We
demonstrate the usefulness of the proposed methods
for ordering manually designed rules in the task of
named entity annotation and machine learnt rules in
the task of classification.
We determine the ordering of the given set of rules
based on a training set. A training set consists of a
set of pairs (ti, ai) where ti is an instance and ai
is its actual annotation. Given a set of rules and a
training data set, we define the problem as follows:
Arrange the rules into a decision list such that max-
imum number of instances are assigned the correct
annotation. We refer to this problem as the MAXDL
problem. We show that this problem is NP hard
and cannot approximated within a factor of n1?,
for any  > 0. We then propose some heuristics
and present an experimental study of these heuris-
tics. Our experimental results show performance im-
provement over an existing decision list learning al-
gorithm, by merely reordering the rules output by
that algorithm. We also illustrate the performance
improvements obtained by applying our algorithms
for ordering named entity annotation rules and clas-
sification rules.
In the rest of the paper we formalize the MAXDL
problem (?2), show it is NP-hard and can?t be
approximated within reasonable factors (?3), and
propose heuristics in a greedy framework (?4).
We present experiments (?5) and conclude with
Section?6.
2 MAXDL Problem Definition and
Notations
The input consists of a set of instances T =
{t1, t2, . . . , tm}, a set of annotations A and a set of
rulesR = {R1, R2, . . . , Rn}. Each ruleRi = (p, a)
is a pair, where p is called the pattern and a ? A is
called the predicted annotation. The patten p will be
given as a set p ? I; we say that the instances in
p are covered by R. The input also includes a map-
ping A : T 7? A, that provides for each instance t
an annotation A(t), called the actual annotation of
t. The pair (T , A) is the training data.
Given the above input, a decision list L is an or-
dering (i.e. permutation) of the input rules. The list
L assigns an annotation to each instance t as defined
below. We consider each rule according to the order-
ing given by L until we find a rule Ri = (p, a) that
covers t and assign the annotation a to t. We denote
by L(t) the annotation assigned by L to t. Thus, L
defines a function L : I 7? A. We say that the list
L correctly annotates an instance t, if the annota-
tion assigned by L matches the actual annotation of
t, i.e., L(t) = A(t).
Given the above input, the MAXDL problem is to
to construct a decision list L such that the number
of instances correctly annotated by L, is maximized
i.e., we want to maximize |{t|A(t) = L(t)}| .
Notations:
LetR = (p, a) be a rule and t be an instance covered
by R. We say that a rule R correctly covers t, if
a = A(t). Similarly, R said to incorrectly cover t, if
a 6= A(t).
Let L be a decision list. We say that an instance
t is happy under L, if L correctly annotates t, i.e.,
L(t) = A(t). Let Happy(L) denote the set of in-
stances that are happy under L. Notice that the
MAXDL problem asks for a decision list L such that
|Happy(L)| is maximized.
3 NP-Hardness and Inapproximability
In this section, we prove that the MAXDL problem
is NP-Hard and also show that the problem cannot
even be approximated with any constant factor.
Theorem 1 The MAXDL problem is NP-Hard.
Proof: We give a reduction from the maximum inde-
pendent set problem (MAXIS ), a well-known NP-
Hard problem (Garey and Johnson, 1979). Recall
that an independent set in a graph refers to any sub-
set of vertices such that no two vertices from the set
share an edge. The MAXIS problem is to find the
largest independent set in a given undirected graph.
836
Let G = (V,E) be the input graph having vertex
set V = {v1, v2, . . . , vn}. We create an instance of
the MAXDL problem as follows. For each vertex
vi, we add an annotation ai toA, an instance ti to T
and a rule Ri to R. We declare ai to be the actual
annotation of ti. The predicted annotation of Ri is
set to ai. We define Ri to cover only the instance
ti and the instances corresponding to the neighbors
of vi. Meaning, Ri covers the instances in the set
{ti} ? {tj |(vi, vj) ? E}. This completes the reduc-
tion. We claim that given a decision list L having
k happy instances, we can construct an independent
set of size k and vice versa. The NP-Hardness of
MAXDL follows from the claim. We now proceed
to prove the claim.
Consider a decision list L. Notice that for any
instance ti, Ri is the only rule that correctly covers
ti. Take any two different instances ti and tj that are
happy under L. Without loss of generality, assume
that Ri appears before Rj in L. Now, if Ri covers
tj , tj would be unhappy under L. So, Ri does not
cover tj , which implies that vj is not a neighbor of
vi (i.e., (vi, vj) 6? E). Hence, the set I = {vi|ti ?
Happy(L)} is an independent set ofG. We note that
|I| = |Happy(L)|.
Conversely, consider an independent set I of G.
Let R(I) = {Ri|vi ? I}. Form a decision list L by
first arranging the rules from R(I) in any arbitrary
order followed by arranging the rest of rules in any
arbitray order. Notice that for any vertex vi ? I ,
Ri correctly covers ti and no other rule appearing
before Ri covers ti. Thus, ti is happy under L. It
follows that |Happy(L)| ? |I|. We have proved
that the MAXDL problem is NP-Hard. 2
In our NP-Hardness reduction, we had shown that
given a decision list L, we can construct an inde-
pendent set I such that |Happy(L)| = |I|, and
vice versa. This means that any approximation algo-
rithm for the MAXDL problem can be translated (by
combining it with our NP-Hardness reduction) into
an equally good approximation algorithm for the
MAXIS problem. Corollary 1 follows from (Zuck-
erman, 2006).
Corollary 1 If NP 6= P then for any  > 0,
the MAXDL problem cannot approximated within
a factor of n1?. In particular, the problem is not
approximable within any constant factor.
4 Heuristic Algorithms for the MAXDL
Problem
As the MAXDL problem is hard to approximate, we
turn to heuristic approaches. All our heuristics fall
into a natural greedy framework, described below.
4.1 A Greedy Framework
Our greedy framework for finding a decision list is
as follows. In each iteration we greedily choose a
rule and output it. For this purpose, we use some
scoring function for assigning scores to the rules and
choose the rule having the maximum score. Then
the chosen rule is deleted. The process is contin-
ued until all the rules are output. The above proce-
dure gives us a decision list. We present this general
framework in the Figure 1. The only unspecified part
in the above framework is the scoring function. In-
tuitively, the scoring function tries to measure the
goodness of a rule.
Given rule set R = {R1, R2, . . . , Rn}, instance set T and the actual annotations
A(?)
while R 6= null do
(re)compute scores for each rule in R, based on the scoring function
select the rule R that has the maximum score
remove R from the set R
remove from T all the instances covered by R
end while
Figure 1: A Greedy Framework for MAXDL prob-
lem
For a rule R and an instance t, we define follow-
ing notations for further use:
InstR = {t|R covers t}
Inst+R = {t|R correctly covers t}
Inst?R = {t|R incorrectly covers t}
Rulest = {R|t is covered by R}
Rules+t = {R|t is correctly covered by R}
Rules?t = {R|t is incorrectly covered by R}
4.2 Simple Precision Scoring
We now present our first candidate scoring function,
which we call simple precision scoring. A natural
score for a rule R is its precision: the fraction of in-
stances covered correctly by R among the instances
covered by it.
ScoreSP(R) =
|Inst+R|
|InstR|
=
|Inst+R|
|Inst+R| + |Inst
?
R |
4.3 Weighted Precision Scoring
Under ScoreSP, the score of a rule R is determined
only by the number of instances covered correctly
(|Inst+R|) and incorrectly (|Inst
?
R|). The nature of
instances are not taken into account. The variants of
ScoreSP proposed here assigns weights to instances,
based on which the scores are computed. We assign
weights to the instances based on how easy it is to
837
make them happy. For an instance t, define the hap-
piness quotient h(t) to be the fraction of rules that
correctly cover t among all the rules that cover t:
h(t) =
|Rules+t |
|Rulest|
.
The value h(t) is a measure of how easy it is to
make t happy; the larger the value of h(t), it is
easier to make t happy. For instance, if h(t) ? 1,
then |Rules+t | ? |Rulest|, meaning that almost any
rule that covers t will annotate it correctly. Thus,
it is easy to make t happy. On the other extreme,
if h(t) ? 0, then only a small fraction of the rules
that cover t annotate it correctly. Thus it is harder to
make t happy.
When we schedule a rule R, the instances in
Inst+R become happy and those in Inst
?
R become
unhappy. Our new scoring functions give credit to
R for each instance in Inst+R and award a penalty
R for each instance in Inst?R. The credit and the
penalty depend on the happiness quotient of the in-
stance. Informally, we want to give more credit R
for making hard instances happy; similarly, we want
to penalize R for making easy instances unhappy. A
natural way of accomplishing the above is to award
a credit of (1 ? h(t)) for each instance t ? Inst+R
and a penalty of h(t) for each instance t ? Inst?R.
Below, we formally define the above quantities as
gain and loss associated with R. For each rule R,
define
Gain(R) =
X
t?Inst+R
(1 ? h(t))
Loss(R) =
X
t?Inst?R
h(t)
Based on the above quantities, we define a natural
scoring function, called Weighted Precision:
ScoreWP(R) =
Gain(R)
Gain(R) + Loss(R)
4.4 Refined Weighted Precision Scoring
Our third scoring function is a refinement of the
weighted precision scoring. In ScoreWP, we com-
pute the happiness quotient of a token by taking in
account the number of rules that cover the token and
among those the ones that cover it correctly. The re-
finement is obtained by also considering the nature
of these rules. We define
hRP(t) =
P
R?Rules+t
precision(R)
P
R?Rulest
precision(R)
.
Gain, loss and the scoring function are defined sim-
ilar to that of ScoreWP:
GainRP(R) =
X
t?Inst+R
(1 ? hRP(t))
LossRP(R) =
X
t?Inst?R
hRP(t)
ScoreRP(R) =
GainRP(R)
GainRP(R) + LossRP(R)
5 Experiments
In this section, we describe rule-ordering experi-
ments on two real-world tasks. 1) named-entity
(NE) annotation that relied on hand-crafted rules for
MUC-7 dataset. 2) The second application we con-
sider is rule-based multi-class text classification. We
order rules learnt on benchmark text classification
datasets and observe consistent improvements by
merely re-ordering rules learnt by other rule learn-
ers.
5.1 Named Entity Annotation
Rule-based named entity annotation is a natural in-
stance of a decision list problem. Typically, rule-
based NE annotation systems (Cunningham et al,
2002) require rules to be manually written as well
as ordered manually. In this section, we show that
our proposed rule-ordering algorithms perform bet-
ter than the natural heuristic. Note that we do not
intend to build a rule-based decision list which per-
forms better than existing methods.
Setup: In our problem formulation of MAXDL ,
the set of instances T and mapping A from in-
stances to actual annotations, together form a train-
ing set. We have access to a set of documents
D = {d1, d2, . . . , dm}, that have all its named en-
tities annotated. To generate pairs (T , A) using the
set of documentsD, let Tdi represent the set of token
sequences that are annotated in a document di ? D.
Let A(t) be the actual annotation for an instance
t ? Tdi . Given a set of rules R and a document
collection D, each rule R ? R is applied to each
document di ? D. The set of token sequences (in-
stances here) which R covers (InstR), is included
in the set of instances T . For all instances t ? Tdi ,
we add a mapping t ? A(t) in A. For all other
instances t ? {InstR ? Tdi}, we have a mapping
t ? null included in A. We perform these addi-
tions for each document and rule pair. Finally, we
add a rule R? = (?, null) to the rule setR. The pat-
tern ? matches every instance t ?
?
R?R,R 6=R?
InstR
838
and associates a null annotation with the instance.
We only consider ?person name?, ?organization?
and ?place name? annotations. We use two different
rule sets containing about 30 rules each.
Table 1 presents accuracy achieved by our pro-
posed algorithms for the two chosen rule sets. In all
the cases our proposed methods perform better than
ScoreSP. The result also shows that our proposed
methods generalize better than simple ScoreSP.
Rule-sets Accuracy ScoreSP ScoreWP ScorePRWP
Rule-set 1
Trng 76.4 76.7 78.9
Test 50.0 52.7 54.5
Rule-set 2
Training 70.1 71.6 73.3
Test 49.1 51.4 52.0
Table 1: Accuracies (in %) for different algorithms
Dataset Acc-
(avg. # rules) -uracy JRip ScoreSP ScoreWP ScorePRWP
la2s (37)
Trng 86.16?0.39 86.02?0.16 86.68?0.16 87.04?0.17
Test 76.93?0.43 77.88?0.16 78.05?0.17 78.1?0.15
oh5 (28)
Trng 86.95?0.41 88.26?0.21 88.8?0.16 89.06?0.17
Test 76.43?0.58 79.08?0.37 79.37?0.38 79.24?0.35
tr45 (17)
Trng 91.88?0.38 92.61?0.18 92.84?0.23 93.3?0.21
Test 78.9?0.47 80.99?0.29 81.19?0.28 81.3?0.3
Table 2: Accuracies (in %) for RipRules
Data set Accu- Multi-class
-racy J48 NaiveBayes ScoreSP ScoreWP ScorePRWP
la2s (18)
Trng 94.75?0.39 85.78?0.29 94.64?0.14 95.9?0.03 95.99?0.01
Test 73.43?0.64 73.68?0.37 78.0?0.21 78.46?0.23 78.64?0.29
oh5 (30)
Trng 95.08?0.21 99.56?0.09 96.27?0.14 98.43?0.09 98.45?0.09
Test 78.08?0.76 74.16?0.77 82.72?0.25 83.16?0.24 83.98?0.26
tr45 (30)
Trng 97.91?0.11 87.16?1.18 97.71?0.14 98.93?0.06 98.98?0.05
Test 85.25?1.02 69.91?1.33 84.06?0.44 86.1?0.39 86.42?0.41
Table 3: Accuracies (in %) for BinRules
5.2 Ordering classification rules
In this section, we show another application of our
algorithms in ordering classification rules. The
antecedent of a classification rule is a series of tests
on the input and the consequent gives the class label.
Since different rules can assign conflicting classes,
rule-ordering becomes important in choosing a
correct class. These rules come from a variety of
sources and could be hand-crafted or machine-
learnt. Machine learnt rules could be generated
using association mining (Agrawal and Srikant,
1994), inductive logic programming (Lavrac and
Dzeroski, 1994), or Ripper (Cohen, 1995). Even
classifiers can be seen as rules, e.g., linear discrim-
inants are rules that assign one of two classes to
exclusive partitions of input space. Due to domain
specificity and unavailability of hand-tuned rules
we illustrate rule-ordering on: (1) rules induced
by Ripper (Cohen, 1995) (RipRules), and (2) a
heterogeneous set of rules obtained from naive
Bayes and decision trees (BinRules).
Setup: We used benchmark text classification
datasets (Forman, 2003) available from the Weka
site1. These multi-class datasets represent 229
binary text classification problems, with positive
class size avg. 149, and class skews avg. 1 : 31.
These are subsets of various benchmark tasks like
Reuters, TREC, and Ohsumed (oh). We present
only a subset of the results (with only ScoreWP
and ScorePRWP) here for lack of space. We report
experiments over 10 random 50 : 50 train-test splits.
The training split is used to learn rules and their
ordering. The orderings are evaluated on the test
split and average train and test accuracies reported.
Results:
The RipRules setting: We induce rules (from
the train split) using the JRip implementation in
Weka2 (Witten and Frank, 2005). We apply our vari-
ous algorithms to merely re-order the rules output by
JRip. In Table 2 we present results comparing JRip
output with their re-ordered versions obtained from
ScoreSP, ScoreWP and ScorePRWP. Along with the
name of each data set, the average number of rules
induced from the training splits are also mentioned
in parentheses. The best accuracies are marked in
bold. We observe that the re-ordered rule-sets us-
ing ScoreWP and ScorePRWP perform better than both
baselines ScoreSP and JRip with lower deviations.
The BinRules setting: For an n-class problem we
obtain classification rules by training a heteroge-
neous collection of one-vs-rest binary classifiers.
Each classifier is either a naive Bayes or a decision
tree classifier trained to discriminate one class from
the rest (2n classifiers). We treat each binary clas-
sifier as a classification rule that covers an instance
if the binary classifier assigns its associated class to
that instance. In addition, corresponding to every
class, we introduce a default classification rule that
assigns the associated class to any instance it en-
1http://www.cs.waikato.ac.nz/ml/weka/
index_datasets.html
2http://www.cs.waikato.ac.nz/ml/weka/
839
counters. This gives us 3n rules. We used the naive
Bayes and J48 implementations in Weka to obtain
binary rules, ordered using ScoreWP and ScorePRWP,
and compared with ScoreSP baseline in Table 3.
We also show individual classifier accuracy, and the
best are marked bold. It is encouraging to note that
all our rule-ordering techniques always outperform
their multi-class counterparts on the test data set. We
outperform the baseline ScoreSP method on all data
sets with lower deviations.
6 Conclusions
In this paper, we formulated and studied the
MAXDL problem. We proved the hardness of the
problem. We then proposed some heuristic ap-
proaches and established the usefulness of our meth-
ods experimentally. We observed improved perfor-
mance in classification task by merely reordering the
rules obtained by an existing decision list learning
algorithm. In future work, we would like to ex-
plore how rule-ordering formulation can be applied
to ordering heterogeneous classifiers in the ensem-
ble learning setting.
References
Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast
algorithms for mining association rules. In VLDB,
pages 487?499.
D. Appelt, J. Hobbs, J. Bear, D. Israel, M. Kameyama,
D. Martin, K. Myers, and M. Tyson. 1995. Sri inter-
national fastus system: Muc-6 test results and analysis.
In MUC6 ?95: Proc. of the 6th conf. on Message un-
derstanding.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York
University.
Eric Brill. 1992. A simple rule-based part-of-speech tag-
ger. In Proceedings of ANLP.
M. E. Califf and R. J. Mooney. 1998. Relational learning
of pattern-match rules for information extraction. In
Working Notes of AAAI Spring Symposium on Apply-
ing Machine Learning to Discourse Processing.
William W. Cohen. 1995. Fast effective rule induction.
In ICML, pages 115?123.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of ACL.
George Forman. 2003. An extensive empirical study
of feature selection metrics for text classification.
JMLR Special Issue on Variable and Feature Selection,
3:1289?1305.
M. R. Garey and D. S. Johnson. 1979. Computers and
Intractability. Freeman.
R. Grishman. 1997. Information extraction: Techniques
and challenges. In SCIE ?97: Intnl. summer School on
Information Extraction.
Hui Han, Eren Manavoglu, C. Lee Giles, and Hongyuan
Zha. 2003. Rule-based word clustering for text classi-
fication. In SIGIR, pages 445?446. ACM Press.
Furnkranz J. and Widmer G. 1994. Incremental re-
duced error pruning. In Machine Learning: Proc. of
the Eleventh International Conference.
Nada Lavrac and Saso Dzeroski. 1994. Inductive
Logic Programming:Techniques and Applications. El-
lis Horwood, New York.
David D. Lewis, Rayid Ghani, Dunja Mladenic, Isabelle
Moulinier, and Mark Wasson. 2003. Workshop on
operational text classification. In conjunction with
SIGKDD.
Hang Li and Kenji Yamanishi. 1999. Text classification
using ESC-based stochastic decision lists. In CIKM.
D. Maynard, V. Tablan, C. Ursu, H. Cunningham, and
Y. Wilks. 2001. Named entity recognition from di-
verse text types. In RANLP.
Ellen Riloff and Michael Thelen. 2000. A rule-based
question answering system for reading comprehension
tests. In ANLP/NAACL 2000 Workshop on Reading
comprehension tests as evaluation for computer-based
language understanding sytems.
Ronald L. Rivest. 1987. Learning decision lists. Ma-
chine Learning, 2(3):229?246.
Minoru Sasaki and Kenji Kita. 1998. Rule-based text
categorization using hierarchical categories. In Pro-
ceedings of SMC-98, IEEE International Conference
on Systems, Man, and Cybernetics, pages 2827?2830.
Sundar Varadarajan, Kas Kasravi, and Ronen Feldman.
2002. Text-mining: Application development chal-
lenges. In Proceedings of the Twenty-second SGAI In-
ternational Conference on Knowledge Based Systems
and Applied Artificial Intelligence.
Ian H.Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
D. Zuckerman. 2006. Linear degree extractors and the
inapproximability of max-clique and chromatic num-
ber. In STOC.
840
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 852?860,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
SMS based Interface for FAQ Retrieval
Govind Kothari
IBM India Research Lab
gokothar@in.ibm.com
Sumit Negi
IBM India Research Lab
sumitneg@in.ibm.com
Tanveer A. Faruquie
IBM India Research Lab
ftanveer@in.ibm.com
Venkatesan T. Chakaravarthy
IBM India Research Lab
vechakra@in.ibm.com
L. Venkata Subramaniam
IBM India Research Lab
lvsubram@in.ibm.com
Abstract
Short Messaging Service (SMS) is popu-
larly used to provide information access to
people on the move. This has resulted in
the growth of SMS based Question An-
swering (QA) services. However auto-
matically handling SMS questions poses
significant challenges due to the inherent
noise in SMS questions. In this work we
present an automatic FAQ-based question
answering system for SMS users. We han-
dle the noise in a SMS query by formu-
lating the query similarity over FAQ ques-
tions as a combinatorial search problem.
The search space consists of combinations
of all possible dictionary variations of to-
kens in the noisy query. We present an ef-
ficient search algorithm that does not re-
quire any training data or SMS normaliza-
tion and can handle semantic variations in
question formulation. We demonstrate the
effectiveness of our approach on two real-
life datasets.
1 Introduction
The number of mobile users is growing at an
amazing rate. In India alone a few million sub-
scribers are added each month with the total sub-
scriber base now crossing 370 million. The any-
time anywhere access provided by mobile net-
works and portability of handsets coupled with the
strong human urge to quickly find answers has fu-
eled the growth of information based services on
mobile devices. These services can be simple ad-
vertisements, polls, alerts or complex applications
such as browsing, search and e-commerce. The
latest mobile devices come equipped with high
resolution screen space, inbuilt web browsers and
full message keypads, however a majority of the
users still use cheaper models that have limited
screen space and basic keypad. On such devices,
SMS is the only mode of text communication.
This has encouraged service providers to build in-
formation based services around SMS technology.
Today, a majority of SMS based information ser-
vices require users to type specific codes to re-
trieve information. For example to get a duplicate
bill for a specific month, say June, the user has
to type DUPBILLJUN. This unnecessarily con-
straints users who generally find it easy and intu-
itive to type in a ?texting? language.
Some businesses have recently allowed users to
formulate queries in natural language using SMS.
For example, many contact centers now allow cus-
tomers to ?text? their complaints and requests for
information over SMS. This mode of communica-
tion not only makes economic sense but also saves
the customer from the hassle of waiting in a call
queue. Most of these contact center based services
and other regular services like ?AQA 63336?1 by
Issuebits Ltd, GTIP2 by AlienPant Ltd., ?Tex-
perts?3 by Number UK Ltd. and ?ChaCha?4 use
human agents to understand the SMS text and re-
spond to these SMS queries. The nature of tex-
ting language, which often as a rule rather than ex-
ception, has misspellings, non-standard abbrevia-
tions, transliterations, phonetic substitutions and
omissions, makes it difficult to build automated
question answering systems around SMS technol-
ogy. This is true even for questions whose answers
are well documented like a FAQ database. Un-
like other automatic question answering systems
that focus on generating or searching answers, in
a FAQ database the question and answers are al-
ready provided by an expert. The task is then
to identify the best matching question-answer pair
for a given query.
In this paper we present a FAQ-based ques-
tion answering system over a SMS interface. Our
1http://www.aqa.63336.com/
2http://www.gtip.co.uk/
3http://www.texperts.com/
4http://www.chacha.com/
852
system allows the user to enter a question in
the SMS texting language. Such questions are
noisy and contain spelling mistakes, abbrevia-
tions, deletions, phonetic spellings, translitera-
tions etc. Since mobile handsets have limited
screen space, it necessitates that the system have
high accuracy. We handle the noise in a SMS
query by formulating the query similarity over
FAQ questions as a combinatorial search prob-
lem. The search space consists of combinations
of all possible dictionary variations of tokens in
the noisy query. The quality of the solution, i.e.
the retrieved questions is formalized using a scor-
ing function. Unlike other SMS processing sys-
tems our model does not require training data or
human intervention. Our system handles not only
the noisy variations of SMS query tokens but also
semantic variations. We demonstrate the effective-
ness of our system on real-world data sets.
The rest of the paper is organized as follows.
Section 2 describes the relevant prior work in this
area and talks about our specific contributions.
In Section 3 we give the problem formulation.
Section 4 describes the Pruning Algorithm which
finds the best matching question for a given SMS
query. Section 5 provides system implementation
details. Section 6 provides details about our exper-
iments. Finally we conclude in Section 7.
2 Prior Work
There has been growing interest in providing ac-
cess to applications, traditionally available on In-
ternet, on mobile devices using SMS. Examples
include Search (Schusteritsch et al, 2005), access
to Yellow Page services (Kopparapu et al, 2007),
Email 5, Blog 6 , FAQ retrieval 7 etc. As high-
lighted earlier, these SMS-based FAQ retrieval ser-
vices use human experts to answer questions.
There are other research and commercial sys-
tems which have been developed for general ques-
tion and answering. These systems generally
adopt one of the following three approaches:
Human intervention based, Information Retrieval
based, or Natural language processing based. Hu-
man intervention based systems exploit human
communities to answer questions. These sys-
tems 8 are interesting because they suggest simi-
lar questions resolved in the past. Other systems
5http://www.sms2email.com/
6http://www.letmeparty.com/
7http://www.chacha.com/
8http://www.answers.yahoo.com/
like Chacha and Askme9 use qualified human ex-
perts to answer questions in a timely manner. The
information retrieval based system treat question
answering as an information retrieval problem.
They search large corpus of text for specific text,
phrases or paragraphs relevant to a given question
(Voorhees, 1999). In FAQ based question answer-
ing, where FAQ provide a ready made database of
question-answer, the main task is to find the clos-
est matching question to retrieve the relevant an-
swer (Sneiders, 1999) (Song et al, 2007). The
natural language processing based system tries to
fully parse a question to discover semantic struc-
ture and then apply logic to formulate the answer
(Molla et al, 2003). In another approach the ques-
tions are converted into a template representation
which is then used to extract answers from some
structured representation (Sneiders, 2002) (Katz et
al., 2002). Except for human intervention based
QA systems most of the other QA systems work
in restricted domains and employ techniques such
as named entity recognition, co-reference resolu-
tion, logic form transformation etc which require
the question to be represented in linguistically cor-
rect format. These methods do not work for SMS
based FAQ answering because of the high level of
noise present in SMS text.
There exists some work to remove noise from
SMS (Choudhury et al, 2007) (Byun et al, 2007)
(Aw et al, 2006) (Kobus et al, 2008). How-
ever, all of these techniques require aligned cor-
pus of SMS and conventional language for train-
ing. Building this aligned corpus is a difficult task
and requires considerable human effort. (Acharya
et al, 2008) propose an unsupervised technique
that maps non-standard words to their correspond-
ing conventional frequent form. Their method can
identify non-standard transliteration of a given to-
ken only if the context surrounding that token is
frequent in the corpus. This might not be true in
all domains.
2.1 Our Contribution
To the best of our knowledge we are the first to
handle issues relating to SMS based automatic
question-answering. We address the challenges
in building a FAQ-based question answering sys-
tem over a SMS interface. Our method is unsu-
pervised and does not require aligned corpus or
explicit SMS normalization to handle noise. We
propose an efficient algorithm that handles noisy
9http://www.askmehelpdesk.com/
853
lexical and semantic variations.
3 Problem Formulation
We view the input SMS S as a sequence of tokens
S = s1, s2, . . . , sn. Let Q denote the set of ques-
tions in the FAQ corpus. Each question Q ? Q
is also viewed as a sequence of terms. Our goal
is to find the question Q? from the corpus Q that
best matches the SMS S. As mentioned in the in-
troduction, the SMS string is bound to have mis-
spellings and other distortions, which needs to be
taken care of while performing the match.
In the preprocessing stage, we develop a Do-
main dictionary D consisting of all the terms that
appear in the corpusQ. For each term t in the dic-
tionary and each SMS token si, we define a simi-
larity measure ?(t, si) that measures how closely
the term t matches the SMS token si. We say that
the term t is a variant of si, if ?(t, si) > 0; this is
denoted as t ? si. Combining the similarity mea-
sure and the inverse document frequency (idf) of t
in the corpus, we define a weight function ?(t, si).
The similarity measure and the weight function are
discussed in detail in Section 5.1.
Based on the weight function, we define a scor-
ing function for assigning a score to each question
in the corpus Q. The score measures how closely
the question matches the SMS string S. Consider
a question Q ? Q. For each token si, the scor-
ing function chooses the term from Q having the
maximum weight; then the weight of the n chosen
terms are summed up to get the score.
Score(Q) =
n?
i=1
[
max
t:t?Q and t?si
?(t, si)
]
(1)
Our goal is to efficiently find the question Q? hav-
ing the maximum score.
4 Pruning Algorithm
We now describe algorithms for computing the
maximum scoring question Q?. For each token
si, we create a list Li consisting of all terms from
the dictionary that are variants of si. Consider a
token si. We collect all the variants of si from the
dictionary and compute their weights. The vari-
ants are then sorted in the descending order of
their weights. At the end of the process we have n
ranked lists. As an illustration, consider an SMS
query ?gud plc buy 10s strng on9?. Here, n = 6
and six lists of variants will be created as shown
Figure 1: Ranked List of Variations
in Figure 1. The process of creating the lists is
speeded up using suitable indices, as explained in
detail in Section 5.
Now, we assume that the lists L1, L2, . . . , Ln
are created and explain the algorithms for com-
puting the maximum scoring question Q?. We de-
scribe two algorithms for accomplishing the above
task. The two algorithms have the same function-
ality i.e. they compute Q?, but the second algo-
rithm called the Pruning algorithm has a better
run time efficiency compared to the first algorithm
called the naive algorithm. Both the algorithms re-
quire an index which takes as input a term t from
the dictionary and returns Qt, the set of all ques-
tions in the corpus that contain the term t. We
call the above process as querying the index on
the term t. The details of the index creation is dis-
cussed in Section 5.2.
Naive Algorithm: In this algorithm, we scan
each list Li and query the index on each term ap-
pearing in Li. The returned questions are added to
a collection C. That is,
C =
n?
i=1
?
?
?
t?Li
Qt
?
?
The collection C is called the candidate set. No-
tice that any question not appearing in the candi-
date set has a score 0 and thus can be ignored. It
follows that the candidate set contains the maxi-
mum scoring question Q?. So, we focus on the
questions in the collection C, compute their scores
and find the maximum scoring question Q?. The
scores of the question appearing in C can be com-
puted using Equation 1.
The main disadvantage with the naive algorithm
is that it queries each term appearing in each list
and hence, suffers from high run time cost. We
next explain the Pruning algorithm that avoids this
pitfall and queries only a substantially small subset
of terms appearing in the lists.
Pruning Algorithm: The pruning algorithm
854
is inspired by the Threshold Algorithm (Fagin et
al., 2001). The Pruning algorithm has the prop-
erty that it queries fewer terms and ends up with
a smaller candidate set as compared to the naive
algorithm. The algorithm maintains a candidate
set C of questions that can potentially be the max-
imum scoring question. The algorithm works in
an iterative manner. In each iteration, it picks
the term that has maximum weight among all the
terms appearing in the lists L1, L2, . . . , Ln. As
the lists are sorted in the descending order of the
weights, this amounts to picking the maximum
weight term amongst the first terms of the n lists.
The chosen term t is queried to find the setQt. The
set Qt is added to the candidate set C. For each
question Q ? Qt, we compute its score Score(Q)
and keep it along with Q. The score can be com-
puted by Equation 1 (For each SMS token si, we
choose the term from Q which is a variant of si
and has the maximum weight. The sum of the
weights of chosen terms yields Score(Q)). Next,
the chosen term t is removed from the list. Each
iteration proceeds as above. We shall now develop
a thresholding condition such that when it is sat-
isfied, the candidate set C is guaranteed to contain
the maximum scoring questionQ?. Thus, once the
condition is met, we stop the above iterative pro-
cess and focus only on the questions in C to find
the maximum scoring question.
Consider end of some iteration in the above pro-
cess. Suppose Q is a question not included in C.
We can upperbound the score achievable by Q, as
follows. At best, Q may include the top-most to-
ken from every list L1, L2, . . . , Ln. Thus, score of
Q is bounded by
Score(Q) ?
n?
i=0
?(Li[1]).
(Since the lists are sorted Li[1] is the term having
the maximum weight in Li). We refer to the RHS
of the above inequality as UB.
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set
C cannot be the maximum scoring question. Thus,
the condition ?Q? ? UB? serves as the termination
condition. At the end of each iteration, we check
if the termination condition is satisfied and if so,
we can stop the iterative process. Then, we simply
pick the question in C having the maximum score
and return it. The algorithm is shown in Figure 2.
In this section, we presented the Pruning algo-
Procedure Pruning Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?.
Begin
Construct lists L1, L2, . . . , Ln //(see Section 5.3).
// Li lists variants of si in descending
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(Li[1])
t? = Lj? [1]
// t? is the term having maximum weight among
// all terms appearing in the n lists.
Delete t? from the list Lj? .
Query the index and fetch Qt?
// Qt? : the set of all questions inQ
//having the term t?
For each Q ? Qt?
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(Li[1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 2: Pruning Algorithm
rithm that efficiently finds the best matching ques-
tion for the given SMS query without the need to
go through all the questions in the FAQ corpus.
The next section describes the system implemen-
tation details of the Pruning Algorithm.
5 System Implementation
In this section we describe the weight function,
the preprocessing step and the creation of lists
L1, L2, . . . , Ln.
5.1 Weight Function
We calculate the weight for a term t in the dic-
tionary w.r.t. a given SMS token si. The weight
function is a combination of similarity measure
between t and si and Inverse Document Frequency
(idf) of t. The next two subsections explain the
calculation of the similarity measure and the idf in
detail.
5.1.1 Similarity Measure
Let D be the dictionary of all the terms in the cor-
pus Q. For term t ? D and token si of the SMS,
the similarity measure ?(t, si) between them is
855
?(t, si) =
?
????
????
LCSRatio(t,si)
EditDistanceSMS(t,si)
if t and si share same
starting character *
0 otherwise
(2)
where LCSRatio(t, si) =
length(LCS(t,si))
length(t) and LCS(t, si) is
the Longest common subsequence between t and si.
* The rationale behind this heuristic is that while typing a SMS, people
typically type the first few characters correctly. Also, this heuristic helps limit
the variants possible for a given token.
The Longest Common Subsequence Ratio
(LCSR) (Melamed, 1999) of two strings is the ra-
tio of the length of their LCS and the length of the
longer string. Since in SMS text, the dictionary
term will always be longer than the SMS token,
the denominator of LCSR is taken as the length of
the dictionary term. We call this modified LCSR
as the LCSRatio.
Procedure EditDistanceSMS
Input: term t, token si
Output: Consonant Skeleton Edit distance
Begin
return LevenshteinDistance(CS(si), CS(t)) + 1
// 1 is added to handle the case where
// Levenshtein Distance is 0
End
Consonant Skeleton Generation (CS)
1. remove consecutive repeated characters
// (call? cal)
2. remove all vowels
//(waiting ? wtng, great? grt)
Figure 3: EditDistanceSMS
The EditDistanceSMS shown in Figure 3
compares the Consonant Skeletons (Prochasson et
al., 2007) of the dictionary term and the SMS to-
ken. If the consonant keys are similar, i.e. the Lev-
enshtein distance between them is less, the simi-
larity measure defined in Equation 2 will be high.
We explain the rationale behind using the
EditDistanceSMS in the similarity measure
?(t, si) through an example. For the SMS
token ?gud? the most likely correct form is
?good?. The two dictionary terms ?good? and
?guided? have the same LCSRatio of 0.5 w.r.t
?gud?, but the EditDistanceSMS of ?good? is
1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result
the similarity measure between ?gud? and ?good?
will be higher than that of ?gud? and ?guided?.
5.1.2 Inverse Document Frequency
If f number of documents in corpus Q contain a
term t and the total number of documents in Q is
N, the Inverse Document Frequency (idf) of t is
idf(t) = log
N
f
(3)
Combining the similarity measure and the idf
of t in the corpus, we define the weight function
?(t, si) as
?(t, si) = ?(t, si) ? idf(t) (4)
The objective behind the weight function is
1. We prefer terms that have high similarity
measure i.e. terms that are similar to the
SMS token. Higher the LCSRatio and lower
the EditDistanceSMS , higher will be the
similarity measure. Thus for example, for a
given SMS token ?byk?, similarity measure
of word ?bike? is higher than that of ?break?.
2. We prefer words that are highly discrimi-
native i.e. words with a high idf score.
The rationale for this stems from the fact
that queries, in general, are composed of in-
formative words. Thus for example, for a
given SMS token ?byk?, idf of ?bike? will
be more than that of commonly occurring
word ?back?. Thus, even though the similar-
ity measure of ?bike? and ?back? are same
w.r.t. ?byk?, ?bike? will get a higher weight
than ?back? due to its idf.
We combine these two objectives into a single
weight function multiplicatively.
5.2 Preprocessing
Preprocessing involves indexing of the FAQ cor-
pus, formation of Domain and Synonym dictionar-
ies and calculation of the Inverse Document Fre-
quency for each term in the Domain dictionary.
As explained earlier the Pruning algorithm re-
quires retrieval of all questions Qt that contains a
given term t. To do this efficiently we index the
FAQ corpus using Lucene10. Each question in the
FAQ corpus is treated as a Document; it is tok-
enized using whitespace as delimiter and indexed.
10http://lucene.apache.org/java/docs/
856
The Domain dictionaryD is built from all terms
that appear in the corpus Q.
The weight calculation for Pruning algorithm
requires the idf for a given term t. For each term t
in the Domain dictionary, we query the Lucene in-
dexer to get the number of Documents containing
t. Using Equation 3, the idf(t) is calculated. The
idf for each term t is stored in a Hashtable, with t
as the key and idf as its value.
Another key step in the preprocessing stage is
the creation of the Synonym dictionary. The Prun-
ing algorithm uses this dictionary to retrieve se-
mantically similar questions. Details of this step is
further elaborated in the List Creation sub-section.
The Synonym Dictionary creation involves map-
ping each word in the Domain dictionary to it?s
corresponding Synset obtained from WordNet11.
5.3 List Creation
Given a SMS S, it is tokenized using white-spaces
to get a sequence of tokens s1, s2, . . . , sn. Digits
occurring in SMS token (e.g ?10s? , ?4get?) are re-
placed by string based on a manually crafted digit-
to-string mapping (?10? ? ?ten?). A list Li is
setup for each token si using terms in the domain
dictionary. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop
word . A term t from domain dictionary is in-
cluded in Li if its first character is same as that of
the token si and it satisfies the threshold condition
length(LCS(t, si)) > 1.
Each term t that is added to the list is assigned a
weight given by Equation 4.
Terms in the list are ranked in descending or-
der of their weights. Henceforth, the term ?list?
implies a ranked list.
For example the SMS query ?gud plc 2 buy 10s
strng on9? (corresponding question ?Where is a
good place to buy tennis strings online??), is to-
kenized to get a set of tokens {?gud?, ?plc?, ?2?,
?buy?, ?10s?, ?strng?, ?on9?}. Single character to-
kens such as ?2? are neglected as they are most
likely to be stop words. From these tokens cor-
responding lists are setup as shown in Figure 1.
5.3.1 Synonym Dictionary Lookup
To retrieve answers for SMS queries that are
semantically similar but lexically different from
questions in the FAQ corpus we use the Synonym
dictionary described in Section 5.2. Figure 4 illus-
trates some examples of such SMS queries.
11http://wordnet.princeton.edu/
Figure 4: Semantically similar SMS and questions
Figure 5: Synonym Dictionary LookUp
For a given SMS token si, the list of variations
Li is further augmented using this Synonym dic-
tionary. For each token si a fuzzy match is per-
formed between si and the terms in the Synonym
dictionary and the best matching term from the
Synonym dictionary, ? is identified. As the map-
pings between the Synonym and the Domain dic-
tionary terms are maintained, we obtain the corre-
sponding Domain dictionary term ? for the Syn-
onym term ? and add that term to the list Li. ? is
assigned a weight given by
?(?, si) = ?(?, si) ? idf(?) (5)
It should be noted that weight for ? is based on
the similarity measure between Synonym dictio-
nary term ? and SMS token si.
For example, the SMS query ?hw2 countr quik
srv?( corresponding question ?How to return a
very fast serve??) has two terms ?countr? ?
?counter? and ?quik? ? ?quick? belonging to
the Synonym dictionary. Their associated map-
pings in the Domain dictionary are ?return? and
?fast? respectively as shown in Figure 5. During
the list setup process the token ?countr? is looked
857
up in the Domain dictionary. Terms from the Do-
main dictionary that begin with the same character
as that of the token ?countr? and have a LCS > 1
such as ?country?,?count?, etc. are added to the
list and assigned a weight given by Equation 4.
After that, the token ?countr? is looked up in the
Synonym dictionary using Fuzzy match. In this
example the term ?counter? from the Synonym
dictionary fuzzy matches the SMS token. The Do-
main dictionary term corresponding to the Syn-
onym dictionary term ?counter? is looked up and
added to the list. In the current example the cor-
responding Domain dictionary term is ?return?.
This term is assigned a weight given by Equation
5 and is added to the list as shown in Figure 5.
5.4 FAQ retrieval
Once the lists are created, the Pruning Algorithm
as shown in Figure 2 is used to find the FAQ ques-
tionQ? that best matches the SMS query. The cor-
responding answer to Q? from the FAQ corpus is
returned to the user.
The next section describes the experimental
setup and results.
6 Experiments
We validated the effectiveness and usability of
our system by carrying out experiments on two
FAQ data sets. The first FAQ data set, referred
to as the Telecom Data-Set, consists of 1500 fre-
quently asked questions, collected from a Telecom
service provider?s website. The questions in this
data set are related to the Telecom providers prod-
ucts or services. For example queries about call
rates/charges, bill drop locations, how to install
caller tunes, how to activate GPRS etc. The sec-
ond FAQ corpus, referred to as the Yahoo DataSet,
consists of 7500 questions from three Yahoo!
Answers12 categories namely Sports.Swimming,
Sports.Tennis, Sports.Running.
To measure the effectiveness of our system, a
user evaluation study was performed. Ten human
evaluators were asked to choose 10 questions ran-
domly from the FAQ data set. None of the eval-
uators were authors of the paper. They were pro-
vided with a mobile keypad interface and asked to
?text? the selected 10 questions as SMS queries.
Through that exercise 100 relevant SMS queries
per FAQ data set were collected. Figure 6 shows
sample SMS queries. In order to validate that the
system was able to handle queries that were out of
12http://answers.yahoo.com/
Figure 6: Sample SMS queries
Data Set Relevant Queries Irrelevant Queries
Telecom 100 50
Yahoo 100 50
Table 1: SMS Data Set.
the FAQ domain, we collected 5 irrelevant SMS
queries from each of the 10 human-evaluators for
both the data sets. Irrelevant queries were (a)
Queries out of the FAQ domain e.g. queries re-
lated to Cricket, Billiards, activating GPS etc (b)
Absurd queries e.g. ?ama ameyu tuem? (sequence
of meaningless words) and (c) General Queries
e.g. ?what is sports?. Table 1 gives the number
of relevant and irrelevant queries used in our ex-
periments.
The average word length of the collected SMS
messages for Telecom and Yahoo datasets was 4
and 7 respectively. We manually cleaned the SMS
query data word by word to create a clean SMS
test-set. For example, the SMS query ?h2 mke a
pdl bke fstr? was manually cleaned to get ?how
to make pedal bike faster?. In order to quantify
the level of noise in the collected SMS data, we
built a character-level language model(LM)13 us-
ing the questions in the FAQ data-set (vocabulary
size is 44 characters) and computed the perplex-
ity14 of the language model on the noisy and the
cleaned SMS test-set. The perplexity of the LM on
a corpus gives an indication of the average num-
ber of bits needed per n-gram to encode the cor-
pus. Noise will result in the introduction of many
previously unseen n-grams in the corpus. Higher
number of bits are needed to encode these improb-
able n-grams which results in increased perplexity.
From Table 2 we can see the difference in perplex-
ity for noisy and clean SMS data for the Yahoo
and Telecom data-set. The high level of perplexity
in the SMS data set indicates the extent of noise
present in the SMS corpus.
To handle irrelevant queries the algorithm de-
scribed in Section 4 is modified. Only if the
Score(Q?) is above a certain threshold, it?s answer
is returned, else we return ?null?. The threshold
13http://en.wikipedia.org/wiki/Language model
14bits = log2(perplexity)
858
Cleaned SMS Noisy SMS
Yahoo bigram 14.92 74.58trigram 8.11 93.13
Telecom bigram 17.62 59.26trigram 10.27 63.21
Table 2: Perplexity for Cleaned and Noisy SMS
Figure 7: Accuracy on Telecom FAQ Dataset
was determined experimentally.
To retrieve the correct answer for the posed
SMS query, the SMS query is matched against
questions in the FAQ data set and the best match-
ing question(Q?) is identified using the Pruning al-
gorithm. The system then returns the answer to
this best matching question to the human evalua-
tor. The evaluator then scores the response on a bi-
nary scale. A score of 1 is given if the returned an-
swer is the correct response to the SMS query, else
it is assigned 0. The scoring procedure is reversed
for irrelevant queries i.e. a score of 0 is assigned
if the system returns an answer and 1 is assigned
if it returns ?null? for an ?irrelevant? query. The
result of this evaluation on both data-sets is shown
in Figure 7 and 8.
Figure 8: Accuracy on Yahoo FAQ Dataset
In order to compare the performance of our sys-
tem, we benchmark our results against Lucene?s
15 Fuzzy match feature. Lucene supports fuzzy
searches based on the Levenshtein Distance, or
Edit Distance algorithm. To do a fuzzy search
15http://lucene.apache.org
we specify the ? symbol at the end of each to-
ken of the SMS query. For example, the SMS
query ?romg actvt? on the FAQ corpus is refor-
mulated as ?romg? 0.3 actvt? 0.3?. The param-
eter after the ? specifies the required similarity.
The parameter value is between 0 and 1, with a
value closer to 1 only terms with higher similar-
ity will be matched. These queries are run on the
indexed FAQs. The results of this evaluation on
both data-sets is shown in Figure 7 and 8. The
results clearly demonstrate that our method per-
forms 2 to 2.5 times better than Lucene?s Fuzzy
match. It was observed that with higher values
of similarity parameter (? 0.6, ? 0.8), the num-
ber of correctly answered queries was even lower.
In Figure 9 we show the runtime performance of
the Naive vs Pruning algorithm on the Yahoo FAQ
Dataset for 150 SMS queries. It is evident from
Figure 9 that not only does the Pruning Algorithm
outperform the Naive one but also gives a near-
constant runtime performance over all the queries.
The substantially better performance of the Prun-
ing algorithm is due to the fact that it queries much
less number of terms and ends up with a smaller
candidate set compared to the Naive algorithm.
Figure 9: Runtime of Pruning vs Naive Algorithm
for Yahoo FAQ Dataset
7 Conclusion
In recent times there has been a rise in SMS based
QA services. However, automating such services
has been a challenge due to the inherent noise in
SMS language. In this paper we gave an efficient
algorithm for answering FAQ questions over an
SMS interface. Results of applying this on two
different FAQ datasets shows that such a system
can be very effective in automating SMS based
FAQ retrieval.
859
References
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Design-
ing the User Experience for Google SMS. CHI,
Portland, Oregon.
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Inter-
face to Yellow Pages Directory, In Proceedings of
the 4th International conference on mobile technol-
ogy, applications, and systems and the 1st Interna-
tional symposium on Computer human interaction
in mobile technology, Singapore.
Monojit Choudhury, Rahul Saraf, Sudeshna Sarkar, Vi-
jit Jain, and Anupam Basu. 2007. Investigation and
Modeling of the Structure of Texting Language, In
Proceedings of IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data, Hyderabad.
E. Voorhees. 1999. The TREC-8 question answering
track report.
D. Molla. 2003. NLP for Answer Extraction in Tech-
nical Domains, In Proceedings of EACL, USA.
E. Sneiders. 2002. Automated question answering
using question templates that cover the conceptual
model of the database, In Proceedings of NLDB,
pages 235?239.
B. Katz, S. Felshin, D. Yuret, A. Ibrahim, J. Lin, G.
Marton, and B. Temelkuran. 2002. Omnibase: Uni-
form access to heterogeneous data for question an-
swering, Natural Language Processing and Infor-
mation Systems, pages 230?234.
E. Sneiders. 1999. Automated FAQ Answering: Con-
tinued Experience with Shallow Language Under-
standing, Question Answering Systems. Papers from
the 1999 AAAI Fall Symposium. Technical Report
FS-99?02, November 5?7, North Falmouth, Mas-
sachusetts, USA, AAAI Press, pp.97?107
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007.
Question similarity calculation for FAQ answering,
In Proceeding of SKG 07, pages 298?301.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization, In Proceedings of COLING/ACL, pages
33?40.
Catherine Kobus, Franois Yvon and Graldine Damnati.
2008. Normalizing SMS: are two metaphors bet-
ter than one?, In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 441?448 Manchester.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement, Association for the Ad-
vancement of Artificial Intelligence. AAAI Workshop
on Enhanced Messaging
Ronald Fagin , Amnon Lotem , Moni Naor. 2001.
Optimal aggregation algorithms for middleware, In
Proceedings of the 20th ACM SIGMOD-SIGACT-
SIGART symposium on Principles of database sys-
tems.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition, Computational Linguistics.
E. Prochasson, Christian Viard-Gaudin, Emmanuel
Morin. 2007. Language Models for Handwritten
Short Message Services, In Proceedings of the 9th
International Conference on Document Analysis and
Recognition.
Sreangsu Acharya, Sumit Negi, L. V. Subramaniam,
Shourya Roy. 2008. Unsupervised learning of mul-
tilingual short message service (SMS) dialect from
noisy examples, In Proceedings of the second work-
shop on Analytics for noisy unstructured text data.
860
