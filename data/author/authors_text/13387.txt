Converting Mikrokosmos frames into Description Logics
P.J. Beltra?n-Ferruz and P.A. Gonza?lez-Calero and P.Gerva?s
GAIA - Applied Articial Intelligence Group
Dep. Sistemas Inform?aticos y Programaci?on
Universidad Complutense de Madrid
C/ Juan del Rosal, 8. 28040 Madrid (Spain)
pablo@fdi.ucm.es, {pedro,pgervas}@sip.ucm.es
Abstract
Mikrokosmos contains an ontology plus a number
of lexicons in different languages that were origi-
nally developed for machine translation. The under-
lying representation formalism for these resources
is an ad-hoc frame-based language which makes it
difficult to inter-operate Mikrokosmos with state-of-
the-art knowledge-based systems.
In this paper we propose a translation from the
frame-based representation of Mikrokosmos into
Description logics. This translation allows us to
automatically transform Mikrokosmos sources into
OWL and thus provide a powerful ontology in the
formalism of the semantic web. Furthermore, the
reasoning mechanisms of Description Logics may
also support knowledge acquisition and mainte-
nance as well as its application in natural language
processing systems.
1 Introduction
The Mikrokosmos project was originally an interlin-
gual system for Knowledge-Based Machine Trans-
lation (KBMT) (Nirenburg, 1987) developed in the
Computing Research Laboratory from New Mexico
State University. Although KBMT was conceived
for translation of domain specific texts, no further
restrictions are imposed in the contents of the text.
Therefore the creators of Mikrokosmos built a rich
ontology that contains a lot of general concepts,
more than 4.700 concepts that are connected with
an average of other 14 concepts using attributes and
relations (de Quesada, 2001).
KBMT is an expensive approach that requires
a big effort on knowledge acquisition, and it has
been considered impractical by some authors. For
that reason, the creators of Mikrokosmos were es-
pecially concerned about developing real-size sys-
tems that would demonstrate the feasibility of their
approach. Generating contents for the ontology was
their first concern, while the use of a rigorous for-
malism for knowledge representation was not con-
sidered a priority (Moreno-Ortiz et al, 2002).
The work presented here is an effort to port
Mikrokosmos into Description Logics (DL) in or-
der to incorporate this resource into the systems we
are developing. Our work on natural language gen-
eration integrates ontologies and case-based reason-
ing (CBR) (Diaz-Agudo et al, 2002), an approach
that heavily relies on classification-based reasoning
from DL.
Representing Mikrokosmos in DL should bring
several benefits. Since DL is the underlying knowl-
edge representation approach in the Semantic Web,
a big number of supporting tools are being de-
veloped for acquiring and maintaining ontologies
represented in some version of DL, such as OWL
(Bechhofer et al, 2004). Giving a well-founded
formal representation to Mikrokosmos should im-
prove its quality by uncovering inconsistencies. Fi-
nally, porting Mikrokosmos to a formalism as pop-
ular as OWL will definitively increase its potential
user community.
There are other efforts that convert ontologies
from different representations to Description Log-
ics languages. OntoMap is a web-site that pro-
vides access to upper-level ontologies and hand-
crafted mappings between them (Kiryakov et al,
2001). The initial set of ontologies contains Cyc,
EuroWordnet and Mikrokosmos ontology, but it
only deals with the top-level hierarchy. In case of
Mikrokosmos it only contains 13 concepts. Their
main effort involves the mapping between differ-
ent ontologies. They also provide the resources in
DAML+OIL. The project?s goal is to facilitate easy
access, understanding, and reuse of general-purpose
ontologies and upper-level models.
The rest of this paper runs as follows. Next
section describes the frame-based language used in
Mikrokosmos ontology, and Section 3 describes the
DL which is the target of the translation. Section 4
is dedicated to the mapping process and Section 5
evaluates this process. Section 6 points out future
work, and finally Section 7 concludes the paper.
Concept Slot Facet Filler(s)
REPLACEMENT-FOR DEFINITION VALUE ?when x is a replacement for y?
IS-A VALUE PHYSICAL-OBJECT-RELATION, EVENT-RELATION
INVERSE VALUE REPLACED-BY
DOMAIN SEM EVENT, OBJECT
RANGE SEM EVENT, OBJECT
Table 1: Example frame: REPLACEMENT-FOR
Figure 1: Mikrokosmos top hierarchy
2 Mikrokosmos ontology
In Mikrokosmos, ontology lists the definitions of
concepts that are understood as corresponding to
classes of thing and events in the world. Concepts
are primitive symbols of a world model which in-
cludes objects, events and properties organized in
a complex hierarchy of language-independent con-
cepts (See top hierarchy of Mikrokosmos in Figure
1). The concepts are constructed following super or-
dinates, or hyponymy relations (IS-A links). In ad-
dition to its organization into a taxonomy via IS-A
links, the ontology contain numerous other links be-
tween concepts, such as links using properties (Lon-
ergan, 2001). For example DECEMBER has a rela-
tion with WINTER using the property PART-OF-
OBJECT.
Each concept that makes up the ontology is lan-
guage independent and is represented using frames.
For example we can see the frame for concept
REPLACEMENT-FOR in Table 1.
The format of Mikrokosmos Ontology is de-
scribed in detail in (Nirenburg and Raskin, 2004).
It formally introduces the syntax and the semantics
of the ontology using a BNF grammar. We are most
interested in how we can access to this information.
Ontology is saved in a text file using Spencer no-
tation that is based on XML. There is another nota-
tion called Beale notation that is based on Lisp, but
we will focus on Spencer notation.
In the XML based format we have the whole on-
tology represented in a list of RECORD entries.
Definition of one CONCEPT requires one or more
of these RECORD entries. Each entry contains four
fields, that are: CONCEPT, SLOT, FACET, and
FILLER.
The CONCEPT field can be filled by any Name
of a concept of the ontology.
The second field in each entry is SLOT. This
field can be filled with PROPERTY or any of its
subclasses using IS-A links. There are two kind
of slot llers. One type are descendants of AT-
TRIBUTE or RELATION, that represent links be-
tween concepts in the hierarchy. The other type are
descendants of ONTOLOGY-SLOT. We will call
them special slots, and all of them have the sense of
determining the structure of the ontology. Possible
descendants of ONTOLOGY-SLOT are: DEFINI-
TION, DOMAIN, INSTANCES, INVERSE, IS-A,
RANGE, SUBCLASSES and some others that are
less important; later in this section we will explain
them in detail.
The third field is FACET, and it describes some
finer distinctions between the possible fillers of the
slot. Possibles FACET fillers are: VALUE, SEM,
DEFAULT, INV, NOT, DEFAULT, DEFAULT-
MEASURE and RELAXABLE-TO.
The last field is FILLER, and its value depends
on the other fields, but generally it contains either a
Name of a concept of the ontology or an instance.
Initially we can think that there are no restrictions
in these representations, but there are some spe-
cial slots that limit expressiveness. All CONCEPT
frames have non-special and special slots. Special
slots for all kinds of concepts are:
? DEFINITION: Definition in English of the
concept.
? IS-A: It is used for asserting parents in the hi-
erarchy.
? SUBCLASSES: It is used for listing concept
children.
? INSTANCES, SPANISH1, ENGLISH1: They
are only used in the leaves of OBJECT and
EVENT, and contains words of the dictionary.
Special slots which can only be present in
all PROPERTY and only in PROPERTY concept
frames are:
? DOMAIN: It has fillers usually filled with
EVENTs1 and/or OBJECTs and it determines
whether a CONCEPT can have it as a SLOT.
? RANGE: It is used in RELATIONs and AT-
TRIBUTEs. In RELATIONs the RANGE slot
has only the SEM facet. The fillers of the SEM
facet are the names of concepts that are in the
range of this RELATION. In ATTRIBUTEs
the RANGE slot has only a VALUE facet. The
VALUE facet is filled by all the possible literal
or numerical values permissible for that AT-
TRIBUTE. The filler can also be a numerical
range specified using appropriate mathemati-
cal comparison operators (such as >, <, ...).
? INVERSE: It is defined only for RELATIONs.
It is mandatory for all RELATION frames. The
INVERSE slot has only the Value facet which
is filled by the name of the RELATION which
is the Inverse of the given RELATION.
? MEASURED-IN: It is defined only for the de-
scendants of the SCALAR-ATTRIBUTE con-
cept frame. The MEASURED-IN slot is used
to add a measuring unit for the number or
scalar range that fills facets of the RANGE slot
in SCALAR-ATTRIBUTE concept frames.
The facet fillers of the MEASURED-IN
slot are the daughters of the MEASURING-
UNIT concept. The MEASURED-IN slot
is used only in those SCALAR-ATTRIBUTE
frames where MEASURING-UNIT has physi-
cal sense (e.g. for SIZE, AGE, etc.).
3 Description logics language: SHIQ
DL are a family of logical formalisms that origi-
nated in the field of artificial intelligence as a tool
for representation of conceptual knowledge. Since
then, DLs have been successfully used in a wide
range of application areas such as knowledge repre-
sentation, reasoning about class-based formalisms
(e.g. conceptual database models and UML dia-
grams), and ontology engineering in the context of
the semantic web. The basic syntactic entities of DL
are concepts, which are constructed from concept
1In this paper when we say a concept name in plural we
are refering to this concept and his children, using links IS-A
defined in the ontology.
names (unary predicates) and role names (binary
predicates) using the set of concept and role con-
structors provided by a particular DL (Lutz, 2003).
Our interest in Mikrokosmos ontology is to map
its contents to a DL language. We have chosen
ALCQHIR+ also known as SHIQ (Horrocks et
al., 2000).
SHIQ is the basic logic ALC augmented with
qualifying number restrictions, role restrictions,
role hierarchies, inverse roles, and transitive roles.
ALC comprises concepts ?denoting sets? as
well as roles ?denoting binary relations. Unlike
roles, concepts can be compound. Compound con-
cepts are constructed by the following operators: in-
tersection u, union t, complementation ??taking
concepts as arguments?, and the value restrictions
?, and ? ?taking a role and a concept as their ar-
guments. Formally, ALC is given by the following
formation rules, where c denotes a concept symbol
and r a role symbol (Schild, 1991):
C,D ?? c | > | C uD | ?C | ?R.C
R ?? r
DL SHIQ is implemented in the RACER sys-
tem (Haarslev and Moller, 2003). This makes it a
desirable target representation for our ontology. For
describing our ontology in SHIQ we will use the
notation explained in Table 2, that contains denota-
tional semantics for our language translation.
4 Mikrokosmos mapping to SHIQ
Once we have identified DL language we want
to use ?SHIQ? and we have described the
Mikrokosmos ontology, we can proceed to map it.
The first step is to determine whether a concept
is a class or a slot. Although in the Mikrokosmos
ontology everything is a concept we need to dis-
tinguish between Mikrokosmos concepts that cor-
respond to unary predicates ?which map to DL
classes? and Mikrokosmos concepts that corre-
spond to binary predicates ?which map to DL rela-
tions. EVENT, OBJECT and all of their subclasses
will be unary predicates so they will be classes.
Meanwhile PROPERTY and all its hierarchy ex-
cept ONTOLOGY-SLOTs (see Figure 1) will be bi-
nary predicates so they will be slots. There are
a few exceptions: concept ALL is top in DL and
ONTOLOGY-SLOT and all of their subclasses are
not mapped to DL language because they have the
2?(C) is the interpretation of a concept. Interpretation of a
concept is the set of all individuals in the domain that satisfies
description of the concept.
class-def (primitive | defined) CN CN(v| .=)>
subclass-of C1 . . . Cn u?2(C1) u . . . u ?(Cn)
slot-constraint1 u?(slot-constraint1)... ...
slot-constraintm u?(slot-constraintm)
top | thing | bottom C t ?C | C t ?C | C u ?C
(C1 and . . . and Cn) (?(C1) u . . . u ?(Cn))
(C1 or . . . or Cn) (?(C1) t . . . t ?(Cn))
(not C) (??(C))
(one-of i1 . . . in) (Pi1 t . . . t Pin)slot-constraint SN >
has-value C1 . . . Cn u ? SN.?(C1) u . . . u ?SN.?(Cn)
value-type C1 . . . Cn u ? SN.?(C1) u . . . u ?SN.?(Cn)
max-cardinality n C u ? n SN.?(C)
min-cardinality n C u ? n SN.?(C)
cardinality n C u ? n SN.?(C) u ? nSN.?(C)
has-filler d u ? SN.?(d)
slot-def SN
subslot-of SN1 . . . SNn (SN v SN1) . . . (SN v SNn)
domain C1 . . . Cn ? SN.> v ?(C1) u . . . u ?(Cn)
range C1 . . . Cn > v ? SN.?(C1) u . . . u ?(Cn)
inverse RN (SN? v RN)(RN? v SN)
properties transitive SN ? S+
properties symmetric (SN v SN?)(SN? v SN)
properties functional > v ? 1SN
disjoint C1 C2 . . . Cn (?(C1) v ??(C2))
covered C by C1 . . . Cn ?(C) v ?(C1) t . . . t ?(Cn)
disjoint-covered C by C1 . . . Cn (?(C1) v ??(C2))
(?(C) v ?(C1) t . . . t ?(Cn))
equivalent C C1 . . . Cn (?(C) = ?(C1)) . . . (?(Cn?1) = ?(Cn))
instance-of i C1 . . . Cn Pi v ?(C1) u . . . u ?(Cn)
related SN i j Pi v ? SN.Pj
Table 2: Denotational semantics for language definition
sense of structuring the ontology. ONTOLOGY-
SLOT and all of their subclasses encode the struc-
ture of the Mikrokosmos ontology. They are not
mapped as DL classes or slots. Instead they are in-
corporated into the DL definition of the Mikrokos-
mos concepts that they refer to.
Mikrokosmos has some information that can not
be mapped to a DL language. We will address this
problem in two ways. First we will make some an-
notations to class and slots that are not supported
by DL language, but which could be provided by
RDFS based languages. Second, extra information
about slots that is not supported by DL language
will be stored in special concepts created from the
corresponding slots.
4.1 Building DL classes
Now we will discuss how we extract information
stored in the XML based file to build classes in DL
language.
The information that has to be extracted is:
class-def (primitive | defined) CN
subclass-of C1 . . . Cn
slot-constraint1...
slot-constraintm
Having identified the set of DL classes we need
to identify their superclasses and slot-constraints.
Information about superclasses is encoded in XML
records of the form shown in Figure 2. Additional
sources of information about superclasses ?such as
RECORDs where CN appears as FILLER and SUB-
CLASSES appears as SLOT? actually encode re-
dundant information and are therefore discarded.
<RECORD>
<CONCEPT> CN </CONCEPT>
<SLOT>IS-A</SLOT>
<FACET>VALUE</FACET>
<FILLER> Ci </FILLER>
</RECORD>
Figure 2: XML encoding of superclass information
Information about slot-constraints is encoded in
records having PROPERTYs as a slot. But there are
also some ONTOLOGY-SLOT used in class defini-
tion and we will assign them a representation.
We collect information about slot-constraints
from XML records of the form shown in Figure 3:
<RECORD>
<CONCEPT> CN </CONCEPT>
<SLOT> SN </SLOT>
<FACET> FACET </FACET>
<FILLER> C </FILLER>
</RECORD>
Figure 3: XML encoding for slot-constraints
We obtain different information depending on the
value of FACET
? If FACET = DEFAULT-MEASURE
CN slot-constraint SN value-type C is added to
the corresponding class definition.
? If FACET = DEFAULT. This information is
stored as an annotation
? If FACET = INV. This information comes from
another slot, that it is inverse to SN. There is no
need to handle this information here because
DL has automatic handling for such type of in-
formation.
? If FACET = NOT. This entry appears when we
restrict inheritance of one SLOT in the hier-
archy. Information contained in Mikrokosmos
about these is affirmative information and neg-
ative information, DL only uses affirmative in-
formation to handle it, so we do nothing with
this information.
? If FACET = RELAXABLE-TO. This informa-
tion is stored as an annotation
? If FACET = SEM
CN slot-constraint SN value-type C is added.
? If FACET = VALUE
CN slot-constraint SN has-value C is added.
Additional information encoded in terms of
records with ONTOLOGY-SLOTS ?as slots?,
must be handled and incorporated into the corre-
sponding class definitions.
The ONTOLOGY-SLOTs to be identified are
DEFINITION, SPANISH1 and ENGLISH1.
? If SLOT = DEFINITION. We will make an an-
notation in class definition.
? If SLOT = SPANISH1 or ENGLISH1. We
create two SLOTs called SPANISH1 and EN-
GLISH1, so we can assert:
slot-constraint ENGLISH1 has-filler d. 3
4.2 Building DL relations
Information required to build DL relations is en-
coded in XML records with ONTOLOGY-SLOTS
in their SLOT field of the form shown in Figure 4
<RECORD>
<CONCEPT> SN </CONCEPT>
<SLOT>SLOT</SLOT>
<FACET>FACET</FACET>
<FILLER> X </FILLER>
</RECORD>
Figure 4: XML encoding of slot information
Possible relevant fillers of the ONTOLOGY-
SLOTS are:
? DEFINITION, IS-A and SUBCLASSES: This
information is handled for DL relations in the
same way as for DL classes.
? INVERSE: It can be used with SEM and
VALUE FACET and represents inverse slots.
slot-def SN inverses X is added.
? DOMAIN: As before when there is a restric-
tion in inheritance Mikrokosmos asserts affir-
mative and negative information so there is
a FACET NOT that is rejected, and has no
translation to DL language. There are more
possibilities for filling the FACET: VALUE,
DEFAULT, RELAXABLE-TO and SEM, we
make no distinction among them:
slot-def SN domain disjoint X1 . . . Xn is
added.
? RANGE: FACET NOT is treated as above.
When we have other FACETs there are two
possible kinds of FILLERs: CONCEPTS or
numeric ranges. For CONCEPTS
slot-def SN range disjoint X1 . . . Xn is added.
For numeric range we create a subclass of
Numeric-Range (See Figure 5 and example in
Figure 6).
? MEASURED-IN: This information is consid-
ered the same as RANGE. It can only have
SEM or DEFAULT FACETs.
slot-def SN range X is added.
3These slots encode cross indexing with lexical informa-
tion. Another possible mapping would have been to add them
as instances, but this would result in loss of this cross indexing
information.
class-def primitive Numeric-Range
slot-constraint Left-Range-Margin
max-cardinality 1 int
slot-constraint Right-Range-Margin
max-cardinality 1 int
slot-def Numeric-Left-Margin
range int
slot-def Numeric-Right-Margin
range int
class-def defined Numeric-Right-Range
subclass-of Numeric-Range
slot-constraint Right-Range-Margin
min-cardinality 1 int
class-def defined Numeric-Left-Range
subclass-of Numeric-Range
slot-constraint Left-Range-Margin
min-cardinality 1 int
class-def defined Numeric-Closed-Range
subclass-of Numeric-Right-Range
subclass-of Numeric-Left-Range
Figure 5: Range definitions
<RECORD>
<concept>VISCOSITY</concept>
<slot>RANGE</slot>
<facet>SEM</facet>
<filler>(<;>; 0 1)</filler>
<uid>256</uid>
</RECORD>
class-def VISCOSITY
subclass-of Numeric-Range
slot-constraint Left-Range-Margin
has-filler 0
slot-constraint Right-Range-Margin
has-filler 1
Figure 6: Example of range restriction
4.3 Building Mikrokosmos PROPERTYs as
DL classes
As we have seen in previous subsection, not all in-
formation about PROPERTYs can be mapped easily
to slots. Because of that we have decided to include
an extra hierarchy of concepts created from PROP-
ERTYs.
For each slot we create a class that inherits
from CLASS-SLOT called CLASS-<PROPERTY-
NAME>. These classes contain all information
about the PROPERTYs that we could not represent
in a DL relation.
For each SLOT applied to a CONCEPT we will
create a class that inherits from CLASS-SLOT-
CONCEPT called CLASS-<PROPERTY-NAME>-
<CONCEPT-NAME>. These classes have slot-
constraints in order to define information not cap-
tured in the respective concept.
With this structure of classes we do not lose any
information about slots and slot-constraints but al-
most all information stored in that way is not useful
for reasoning in current tools like RACER (Haarslev
and Moller, 2001).
5 Evaluation of the translation process
DL provide the way to carry out complex inference
and reasoning tasks. In order to achieve this goal
our DL language is less expressive than Mikrokos-
mos. Among all restrictions in the expressiveness of
DL languages we will mention two. DL languages
are not able to reason with default values for the re-
strictions. And they do not manage finite domains
such as enumerates or sets.
These differences in expressivity between
Mikrokosmos and our DL language has as a result
some interesting points in the translation process.
There were two possible solutions to this problem.
First one was to discard all information that has not
a direct mapping to our DL language. And second
one ?which we have chosen? is to make some
artifices in order to preserve all information, but
obviously we cannot reason with this information.
There are two places where we have made this
kind of artifices:
? Default values: Mikrokosmos is able of man-
aging default values for restrictions while DL
is not. So we have decided to store it as an
annotation.
? Numeric restrictions: For example in
Mikrokosmos we can restrict the age of a
person to be plus than 0 and minus that 120,
but our DL language is not capable. Because
of that we have created the complex structure
of Numeric-Range concepts.
So we can say that we have no loss of information
in the translation process. But we were incapable to
use all information contained in Mikrokosmos for
reasoning and inference tasks.
6 Applications of Mikrokosmos and future
work
One of the distinguishing features of the origi-
nal Mikrokosmos resources for machine transla-
tion was the explicit isolation between the pseudo-
taxonomical structure used to represent the concepts
on one hand, and the particular lexical information
ALL
EVENT
MENTAL-EVENT
PASIVE-COGNITIVE-EVENT
REMEMBERKNOW
EMOTIONAL-EVENT
ACTIVE-COGNITIVE-EVENT
CONSIDER
SOCIAL-EVENT
CHANGE-EVENT
DIVIDE
ABSTRACT-SOCIAL-ABILITY
ELIMINATE
PHYSICAL-EVENT
APPLY-FORCE
CHANGE-LOCATION
CUTJUMP
CHANGE-POSITION
CLIMB
MOUNT
SALTAR
CORTAR PENSARSABER
WORRY
Figure 7: Mikrokosmos ontology with some instances
that was associated with those concepts for realiza-
tion in a particular language. This peculiarity al-
lowed relatively easy bidirectional translation be-
tween different languages via an intermediate con-
ceptual representation.
Subsequent uses and/or transformations of these
resources must take into account this peculiarity. In
our case, the work carried out so far in transport-
ing the Mikrokosmos ontology to OWL has been
restricted to the part of the ontology concerned
with the conceptual representation. Although this
transformation already opens up avenues of re-
search for knowledge representation for problem
solving (D??az-Agudo and Gonza?lez-Calero, 2002),
the number of useful applications of the results of
this process in the field of natural language process-
ing will increase greatly once the corresponding lex-
icons ?there are currently versions in Spanish and
English? are also transformed into OWL.
For instance, use of this resource provides the
means for intelligently substituting a given word for
a different one - as required for example in our po-
etry generation system (Diaz-Agudo et al, 2002)
during the adaptation of the structure of poem from
the case base to obtain a verse approximation of a
user query. Assuming that a structure such as:
Sabed que en mi perfecta edad y armado
con mis ojos abiertos me he rendido
al nin?o que sabe?is ciego y desnudo.
needs to be adapted, and the adaptation requires the
substitution of the verb ?sabed? for a related one out
of a list of candidates - possibly obtained from the
given user query - such as ?pensad?, ?cortad? and
?saltad?. By conuslting the structure of the ontology
?see extract in Figure 7 for illustration? the sys-
tem may correctly select ?pensad? as a preferable
candidate in view of its proximity in the ontology to
the original word.
Our future lines of research in this field will focus
in a deeper study of which concepts are primitive
and which ones are defined. Now we have decided
that all concepts having any restriction are defined
concepts. This decission was taken in orden to rea-
son with the ontology but it is necessary to examine
it in detail.
7 Conclusions
Mikrokosmos ontology is a rich and extensive
knowledge resource that was developed with a pro-
prietary formalism, with a weak theoretical founda-
tion. We have analysed the contents of the ontology
which have lead us to propose a possible translation
into description logics.
All this effort of understanding Mikrokosmos
ontology and mapping it to a description logics
language has resulted in a concrete implementa-
tion. We have chosen OWL ?an RDFS based
language?, in its version idOWL DL. This version
implements reasoning using JENA (McBride, 2001)
and the DIG interface (Bechhofer et al, 2003).
There are two inference engines that implement the
DIG interface: RACER and FaCT4. As part of this
implementation we have developed an import plu-
gin for Prote?ge? 2.0 (See Figure 7).
With this work we can profit from all the knowl-
edge stored in the Mikrokosmos ontology for other
tasks related to Artificial Intelligence. These tasks
are Natural Language Processing and Knowledge-
Intensive Case-Based Reasoning. We still have to
translate the Mikrokosmos lexicon in order to fully
exploit the original resource.
Acknowledgements
The first author is supported by a FPI Predoctoral
Grant form Universidad Complutense, Madrid. The
4http://dl-web.man.ac.uk/dig/
Figure 8: Screen capture of Prote?ge? 2.0 with
Mikrokosmos ontology.
work was partially funded by the Spanish Commit-
tee of Science & Technology (TIC2002-01961).
References
Sean Bechhofer, Ralf Moller, and Peter Crowther.
2003. The DIG description logic interface. In
Description Logics 2003, CEUR Workshop Pro-
ceedings.
Sean Bechhofer, Frank van Harmelen, Jim Hendler,
Ian Horrocks, Deborah L. McGuinness, Pe-
ter F. Patel-Schneider, and Andrea Stein. 2004.
Owl web ontology language reference, February.
W3C http://www.w3.org/TR/2004/REC-owl-ref-
20040210/.
Mercedes Garc??a de Quesada. 2001. Estructura
definicional terminogrfica en el subdominio de
la oncologa cl??nica. In Estudios de Linguistica
Espa
?
nola, volume 14.
Bele?n D??az-Agudo and Pedro Antonio Gonza?lez-
Calero. 2002. CBROnto: a task/method ontol-
ogy for CBR. In S. Haller and G. Simmons, edi-
tors, Procs. of the 15th International FLAIRS?02
Conference (Special Track on CBR), pages 101?
106. AAAI Press.
B. Diaz-Agudo, P. Gerva?s, and P A. Gonzalez-
Calero. 2002. Poetry generation in COLIBRI. In
S. Craw and A. Preece, editors, ECCBR 2002,
Advances in Case Based Reasoning, pages 73?
102. Springer. Lecture Notes in Artificial Intelli-
gence.
Volker Haarslev and Ralf Moller. 2001. Descrip-
tion of the RACER system and its applica-
tions. In Proceedubgs International Workshop on
Description Logics (DL-2001), pages 132?142,
Stanford, USA.
Volker Haarslev and Ralf Moller, 2003. RACER
User s Guide and Reference Manual Version
1.7.7. Concordia University and Univ. of Appl.
Sciences in Wedel, November. http://www.sts.tu-
harburg.de/?r.f.moeller/racer/racer-manual-1-7-
7.pdf.
I. Horrocks, U. Sattler, and S. Tobies. 2000. Rea-
soning with individuals for the description logic
SHIQ. In David MacAllester, editor, Proceed-
ings of the 17th International Conference on Au-
tomated Deduction (CADE-17), number 1831,
pages 482?496, Germany. Springer Verlag.
Atanas K. Kiryakov, Marin Dimitrov, and
Kiril Iv. Simov. 2001. Ontomap - the
guide to the upper-level. In Proceedings of
SWWS?01, The rst Semantic Web Work-
ing Symposium, Stanford University, Cal-
ifornia, USA, July 30 - August 1, 2001.
http://www.ontotext.com/publications/swws01.pdf.
E. Lonergan. 2001. Lexical knowledge engineer-
ing: Mikrokosmos revisited. In PACLING2001
- Pacic Association for Computational Linguis-
tics 2001, Kitakyushu, Japan.
C. Lutz. 2003. Description logics with concrete
domains?a survey. In Advances in Modal Log-
ics Volume 4. King?s College Publications.
Brian McBride. 2001. Jena: Implementing the rdf
model and syntax specification. In Proceedings
of the Second International Workshop on the Se-
mantic Web - SemWeb?2001. Hongkong, China,
May 1, 2001. http://SunSITE.Informatik.RWTH-
Aachen.de/Publications/CEUR-WS/Vol-
40/mcbride.pdf.
Antonio Moreno-Ortiz, Victor Raskin, and Sergei
Nirenburg. 2002. New developments in onto-
logical semantics. In Proceedings of LREC-02,
Spain, June.
Sergei Nirenburg and Victor Raskin, 2004. On-
tological Semantics, chapter 7, pages 159?207.
The MIT Press, September.
S. Nirenburg. 1987. Knowledge-based machine
translation, the cmu approach. In Machine Trans-
lation: theoretical and methodological issues,
Studies in Natural Language Processing, pages
68?69, Cambridge. Cambridge University Press.
Klaus Schild. 1991. A correspondence theory
for terminological logics: preliminary report.
In Proceedings of IJCAI-91, 12th International
Joint Conference on Articial Intelligence, pages
466?471, Sidney, AU.
An Evolutionary Approach to Referring Expression Generation and Aggregation
Raquel Herva?s and Pablo Gerva?s
Departamento de Sistemas Informa?ticos y Programacio?n
Universidad Complutense de Madrid, Spain
raquelhb@fdi.ucm.es,pgervas@sip.ucm.es
Abstract
The work presented here is intended as an evolu-
tionary task-specific module for referring expres-
sion generation and aggregation to be enclosed in a
generic flexible architecture. Appearances of con-
cepts are considered as genes, each one encoding
the type of reference used. Three genetic opera-
tors are used: classic crossover and mutation, plus
a specific operator dealing with aggregation. Fit-
ness functions are defined to achieve elementary
coherence and stylistic validity. Experiments are
described and discussed.
1 Introduction
In this paper we present a first approach to the idea of using
Natural Language Generation (NLG) and Evolutionary Algo-
rithms (EAs) together.
To test the feasibility of our idea, we decided to select only
some particular features of the text on which to put it to the
test. Given the complexity of all the changes that are pos-
sible to a text, at the levels of syntax, semantics, discourse
structure and pragmatics, it seemed impractical to tackle them
all at once. For the purpose of illustration, we decided that
the problems of the referring expressions and the aggregation
were the most suitable to be solved using EAs. Referring
Expression Generation involves deciding how each element
ocurring in the input is described in the output text. Aggre-
gation involves deciding how compact the presentation of in-
formation should be in a given text. It operates at several lin-
guistic levels, but we only consider it here with respect to con-
cepts and their attributes. For instance, the system must de-
cide between generating ?The princess is blonde. She sleeps.?
and generating ?The blonde princess sleeps.?. Aggregation is
generally desirable, but may result in adjective-heavy texts
when the information to impart becomes dense in terms of
attributes, as in ?The pretty blonde princess lived in a strong
fancy castle with her stern rich parents.?. It is necessary to
find the balance between the use of compound or single sen-
tences, or in the case of the modifiers of a concept between
the description of the attributes of the concept using only a
phrase or various.
We analysed the features of a human generated text from
the point of view of the referring expressions, and we found
five different features of simple texts that might be susceptible
of easy treatment by means of evolutionary techniques. They
are described below.
Correct Referent.
When writing a text, we cannot use a pronoun for something
that we have not mentioned before, or readers would get con-
fused. An example could be:
She lived in a castle. A princess was the daugh-
ter of parents.
In addition, if the full noun reference and the pronoun are
far, the reader can also get confused and be unable to link the
two occurrences of the same concept, as we can see in the
following text:
A princess lived in a castle. She was the daugh-
ter of parents. She loved a knight. She was pretty.
She was blonde. It had towers. It was strong. They
lived in it.
Redundant Attributes.
When describing a concept in an ?X is Y? sentence, people do
not use the attribute they are going to describe in the reference
to the concept. Sentences such as the one below are incorrect:
The blonde princess was blonde.
Reference Repetition.
Using always the same reference together with the same set of
attributes results in repetitive text. For example, it is accept-
able to use ?the princess? every time we refer to the princess
character, but it would be striking to use always ?the pretty
princess?, as in this example:
A pretty princess lived in a castle.
The pretty princess was the daughter of par-
ents. The pretty princess loved a knight.
The pretty princess was blonde.
To avoid that, repetitive use of references is penalized.
Coherence.
If we use different subsets of attributes in different references
to the same concept, the reader may mistakenly assume that
we are referring to different concepts. For example, if we use
?the pretty princess? and ?the blonde princess? in different
places, and we have not specified that the princess is both
pretty and blonde, it could seem that there are two princess, a
pretty one and a blonde one:
A princess lived in a castle. The pretty princess
was the daughter of parents. The blonde princess
loved a knight.
Overlooked Information.
When processing the conceptual representation of a given
input, some information about a concept may disappear from
the final text. This should be avoided.
This paper describe an evolutionary solution that guaran-
tees the satisfaction of these restrictions in the conceptual
rendition of a given input by means of shallow techniques
that rely on very little knowledge about the domain and no
reasoning or common sense capabilities.
2 Natural Language Generation Tasks and
Evolutionary Algorithms
This section outlines the elementary requirements of the two
generation tasks addressed in this paper, and sketches the ba-
sic principles of the evolutionary techniques that are used.
2.1 Referring Expression Generation and
Aggregation
The correct use of referring expressions to compete with hu-
man generated texts involves a certain difficulty. Possible
simple algorithms for deciding when to use a pronoun and
when to use the full noun produce poor results. Two occur-
rences of the same concept in a paragraph can be far apart,
and this may confuse the reader. Knowledge intensive ap-
proaches modelled on the way humans do it require a certain
measure of content understanding that is resource hungry.
As shown in [Reiter and Dale, 1992], a referring expression
must communicate enough information to be able to uniquely
identify the intended referent in the current discourse context,
but avoiding the presence of redundant or otherwise unneces-
sary modifiers. Therefore, it is essential to choose a reference
which matches these constraints. Taking into account these
features, Reiter and Dale proposed an algorithm to generate
definite noun phrases to identify objects in the current focus
of attention of the reader or the hearer. However, Krahmer
and Theune [Krahmer and Theune, 2000] argue that due to
the original motivation of the work of Reiter and Dale of mak-
ing distinguishing descriptions, various other aspects of the
generation of definites remained somewhat underdeveloped.
In particular they focus on the role of context-sensitivity for
referring expression generation.
Kibble and Power [Kibble and Power, 2000] propose a sys-
tem which uses Centering Theory [Walker et al, 1998] for
planning of coherent texts and choice of referring expres-
sions. They argue that text and sentence planning need to
be driven in part by the goal of maintaining referential con-
tinuity: obtaining a favourable ordering of clauses, and of
arguments within clauses, is likely to increase opportunities
for non-ambiguous pronoun use.
Aggregation can be seen as the NLG task that involves de-
ciding how compact the presentation of information should be
in a given text, although there is no exact definition in the lit-
erature about what aggregation is [Reape and Mellish, 1999].
It operates at several linguistic levels, and due to that Reape
and Mellish make a classification of the different types of ag-
gregation: conceptual, discourse, semantic, syntactic, lexical
and referential. However, the line between them is very nar-
row, and in some cases a specific example could be classified
as different types of aggregation.
2.2 Evolutionary Algorithms
We propose the use of evolutionary algorithms (EAs) [Hol-
land, 1992] to deal with the referring expression generation
and aggregation tasks. Evolutionary algorithms are an ex-
tended set of problem resolution techniques inspired by evo-
lutionary phenomena and natural evolution. They work on
a population of individuals (representations of possible solu-
tions for the problem we are solving) that evolve according to
selection rules and genetic operators like crossover and mu-
tation. The fitness function is a metric which allows the eval-
uation of each of the possible solutions, in such way that the
average adaptation of the population would increase in each
generation. Repeating this process hundreds or thousands of
times it is possible to find very good solutions for the prob-
lem.
Evolutionary algorithms combine random search, because
the genetic operators are applied randomly, with oriented
search, given by the fitness values. These algorithms find
generally good solutions, but not always the best ones. How-
ever, this is enough for simple applications. In the case under
consideration, the main advantage we can find in evolution-
ary algorithms is that they do not need specific rules to build
a solution, only measurements of its goodness.
Evolutionary techniques have been shown in the past to be
particularly well suited for the generation of verse. The work
of Manurung [Manurung, 2003] and Levy [Levy, 2001] pro-
posed different computational models of the composition of
verse based on evolutionary approaches. In both cases, the
main difficulty lay in the choice of a fitness function to guide
the process. Although Levy only addressed a simple model
concerned with syllabic information, his overall description
of the architecture in terms of a population of poem drafts that
evolve, with priority given to those drafts that are evaluated
more highly, is an important insight. Levy uses a neural net-
work, trained with examples of valid verse, to evaluate these
drafts. The work of Manurung addresses the complete task,
and it presents a set of evaluators that grade the candidates
solutions according to particular heuristics.
Evolutionary algorithms have been also used in text plan-
ning. In [Duboue and McKeown, 2002] the authors present
a technique to learn a tree-like structure for a content plan-
ner from an aligned corpus of semantic inputs and corre-
sponding, human produced, outputs. They apply a stochastic
search mechanism with a two-level fitness function to create
the structure of the planner. Genetic algorithms are also used
in [Mellish et al, 1998] where the authors state the problem
of given a set of facts to convey and a set of rhetorical re-
lations that can be used to link them together, how one can
arrange this material so as to yield the best possible text.
An important conclusion to draw from these efforts is the
suitability of evolutionary techniques for natural language
generation tasks in which the form plays a significant role,
to the extent of sometimes interfering with the intended con-
tent, such as is the case for lyrics generation.
3 An Evolutionary Submodule for a Simple
Generator
The work presented here is intended to be a module for the
tasks of referring expressions generation and aggregation en-
closed in the architecture of cFROGS [Garc??a et al, 2004].
cFROGS is a framework-like library of architectural classes
intended to facilitate the development of NLG applications.
cFROGS identifies three basic design decisions: what set of
modules to use, how control should flow between them, and
what data structures are used to communicate between the
modules.
We have tested the implementation of the module in an ex-
isting application: ProtoPropp [Gerva?s et al, 2004]. This
is a system for automatic story generation. The natural lan-
guage generator module of ProtoPropp ? implemented as a
pipeline architecture of cFROGS modules ? perform tasks
such as content determination - selecting the particular con-
cepts that are relevant - and discourse planning - organising
them in an orderly fashion. These tasks are currently carried
out in a traditional manner and simply provide the data for the
evolutionary stages. In the previous prototype of ProtoPropp
the referring expression to use for a concrete concept was de-
termined using a very simple heuristic: the first time that the
concept appears in the paragraph, the generator uses its full
noun, in all other cases it uses a pronoun. When using a full
noun reference, it is indefinite for the first appearance of the
concept in the text and definite for the rest.
The input of the evolutionary algorithm is a basic discourse
structure where each phrase is a message about a relation be-
tween two concepts or a description of some attribute of an
element. Additionally, this submodule has access to a knowl-
edge base of conceptual information about the discourse el-
ements that appear in the input (characters, locations, at-
tributes, relations).
In this simple evolutionary algorithm, the appearances of
the concepts are considered as the genes. The initial popu-
lation is generated randomly, using for each concept its full
noun or its pronoun. When using the full noun, a selection of
the attributes the concept has in the knowledge base is cho-
sen. These attributes will appear just before the noun of the
concept, as it is usual in English. The system works over
this population for a number of generations determined by
the user. In each generation three genetic operators are used:
crossover, mutation and aggregation. Finally, at the end of
each generation each tale is evaluated and a selection of the
population is passed to the next one, in such way that the tales
with a higher fitness have more possibilities of being chosen.
3.1 Data Representation and Genes
Within the context of the larger cFROGS architecture, data
are represented as complex data structures with generic inter-
faces to ensure easy connectivity between different modules
[Garc??a et al, 2004]. These data follow ideas from the RAGS
[Cahill et al, 2001] generic architecture. However, the no-
tation described here corresponds to a representation internal
to the module intended to facilitate the operation of the evo-
lutionary techniques.
Characters, locations and attributes are represented as
simple facts containing an unique identifier (to distinguish
each specific character and location from the others) and their
names. The identifier in attributes corresponds to the con-
cept that holds the attribute, and the name corresponds to the
attribute itself. The current prototype operates over simple
linguistic constructs: the description of a concept using an
attribute, or a relation between two concepts. Pronominal
reference is indicated by changing the name of the concept
for ?pron?, and definite and indefinite reference is indicated
by adding a fact ?ref? indicating if the reference is definite or
indefinite. Finally, the concepts may go along with some at-
tributes preceding the name of the concept, as in ?the pretty
blonde princess?. This list of attributes is represented be-
tween -> and <-.
A sample part of a draft for the evolutionary algorithm
would be the following:
[character(ch26,princess),
ref(ind),
->attribute(ch26,pretty)<-,
relation(ch26,l14,live),
location(l14,castle),
ref(ind)]
[character(ch26,pron),
relation(ch26,ch25,love),
character(ch25,knight),
ref(ind)]
[character(ch26,princess),
ref(def),
isa(),
attribute(ch26,blonde)]
In this example, the set of genes would be this:
Genes:
0: character(ch26,princess),
ref(ind),
->attribute(ch26,pretty)<-
1: location(l14,castle),
ref(ind)
2: character(ch26,pron)
3: character(ch25,knight),
ref(ind)
4: character(ch26,princess),
ref(def)
3.2 The Genetic Operators
Three genetic operators are used: crossover, mutation and ag-
gregation.
For the crossover operator, two drafts are selected ran-
domly and crossed by a random point of their structure. So,
each of the sons will have part of each of the parents.
In the case of the mutation operator, some of the genes are
chosen randomly to be mutated. If the gene is a pronoun -
as in ?she lived in a castle? -, it will change into the cor-
responding full noun, always associated with a subset of its
possible attributes - for example ?the princess lived in a cas-
tle? or ?the pretty princess lived in a castle? -. In case the
Correct Referent error1 =
?
pronominal references to a concept not referred in full in the two previous genes
Redundant Attributes error2 =
?
?<adj> X is <adj>? sentences
Reference Repetition error3 =
?
repeated use of same set of attributes ? att(geni) ? to refer to the concept in geni
Coherence error4 =
?N
i=1(att(geni) ? I) with I the set of attributes used before for the concept in geni
Overlooked Information error5 =
?
subset of attributes of concept i in the ontology not mentioned in the text
Table 1: Definition of fitness functions
gene was a full noun - as in ?the pretty princess? -, there are
two options: to change it into a pronoun - in this case ?she?
-, or to change the subset of attributes that appear with it -
for example ?the princess? or ?the pretty blonde princess? -.
One of these two options is chosen randomly.
The aggregation operator addresses the task of deciding on
the aggregation between concepts and their attributes. This
involves a certain modification of the structure of the text,
because sentences in the text may be deleted if the informa-
tion they impart becomes part of a previous sentence. The
aggregation operator acts only on genes corresponding to ex-
plicitly mentioned concepts: concepts referred by pronouns
are excluded. It can act in two directions:
? If the reference to the concept appears with one or more
attributes - as in ?A blonde princess lived in a castle.?
-, the operator disaggregates the attributes by eliminat-
ing their mention and adding a corresponding ?X is Y?
sentence - resulting in ?A princess lived in a castle. She
was blonde.?
? If the reference to X has no attributes - as in ?A princess
lived in a castle.? -, the algorithm looks for an ?X is Y?
sentence - such as ?The princess was blonde.? -, adds
the corresponding attributes to the reference, and deletes
the ?X is Y? sentence - resulting in ?A blonde princess
lived in a castle.?
The goal of this definition of the aggregation is to ensure
that the attributes of a concept are mentioned in the appear-
ance of a concept or in the correspondent ?X is Y? sentences,
but not in both. As the aggregation operator is used randomly,
the desired result is obtained only in some cases.
3.3 The Fitness Function
The key to the evolutionary algorithm lies in the choice of
fitness function. A simple approach would be to require that
in each generation the user reads all the texts and gives them
a fitness value. The number of generations and individuals in
the population for a simple experiment makes this approach
impractical.
We have defined five different fitness functions as shown in
Table 1. This definitions are the results of the analysis of the
features of human-generated text.
For the evaluation of each of the drafts that form the popu-
lation, we use the following formula:
fitness = 1/(
?
i
errori + k)
In this way, the fitness would be greater when the error is
smaller. The constant k is used to avoid divisions by zero. In
our experiments it was set with the value 1, so the maximum
possible fitness was 1.
4 Experiments and Results
To test the feasibility of the idea of using together NLG and
EAs, we have formalized five different fairy tales, mainly dif-
ferentiated by their lengths in number of genes, that is, in
appearances of concepts. We must take into account that the
number of genes shown below are not completely exact, be-
cause the aggregation operator can erase or add new sentences
to the tale. These are the tales formalized and used to do the
experiments:
? Cinderella: 102 genes
? Hansel and Gretel: 90 genes
? The Lioness: 50 genes
? The Dragon: 32 genes
? The Merchant: 31 genes
For each of these tales we have made several experiments
using different population sizes (10, 25, 50, 100, 200, 300,
500) and number of generations (10, 25, 50). The three ge-
netic operators mentioned before (crossover, mutation and
aggregation) are applied, and the five fitness functions used
for the evaluation of the tales.
Table 2: Table of numerical results
In Table 2 we can see the numerical results of the experi-
ments. For each combination of population size and number
Figure 1: Legend for the tales
of generations results shown have been averaged over a num-
ber of runs.
We can analyse these results taking into account the three
different number of generations used. The legend for the fol-
lowing graphics is shown in Figure 1.
4.1 10 Generations
As we can see in Figure 2, only 10 generations are not enough
for the bigger tales. However, in the case of the smaller ones,
the fitness values increase with the size of the population, and
at certain point they achieve the maximum value of 1.
Figure 2: Fitness values of the tales with 10 generations
4.2 25 Generations
In Figure 3 the fitness values for the bigger tales are higher
than in the case of 10 generations, but still not good enough.
For the smaller tales we achieve the maximum fitness value
of 1 quicker than with only 10 generations.
4.3 50 Generations
We can see in Figure 4 the best values achieved in the ex-
periments. For the smaller tales, we get the maximum fitness
value of 1 very quickly. In the case of the bigger ones, the
fitness values are higher than in the previous experiments, but
not very good yet, except in the case of ?The Lioness?, where
the maximum value of 1 is achieved with 50 generations and
500 individuals in the population.
5 Discussion
To start with, EAs seem to be a good approach to solve the
tasks addressed, and in all the experiments the results ob-
tained are better than the ones achieved using previous heuris-
tics. An example of generated text with the initial simple
heuristic is:
Figure 3: Fitness values of the tales with 25 generations
Figure 4: Fitness values of the tales with 50 generations
A princess lived in a castle. She loved a knight.
She was pretty. She was blonde. It had towers. It
was strong.
Using the evolutionary module the same piece of tale is
generated as follows:
A pretty princess lived in a strong castle. She
was blonde. The princess loved a brave knight. The
castle had towers.
The second example shows that the texts generated by the
evolutionary module are richer from the point of view of ad-
jectives and structure.
Note that depending on the number of genes you need a
certain number of individuals and generations to achieve a
good fitness value. For example, ?The Lioness?, with 50
genes, gets the maximum fitness with 50 generations and 500
individuals, as long as ?Hansel and Gretel? and ?Cinderella?
would need more generations and individuals to get the max-
imum fitness.
Another important point is that in a specific tale, with a
specific number of genes, you can achieve the same results
increasing the number of generations or the size of the popu-
lation. For instance, ?The Merchant?, with 31 genes, gets the
maximum fitness with both 25 or 50 generations with small
populations or 10 generations with populations of more than
100 individuals.
Finally, it is important to note that our approach presents
some differences respect to the one of Reiter and Dale [Re-
iter and Dale, 1992]. As we have already mentioned, we are
working in the field of the fairy tales, with the specific re-
quirements of story generation. An important point is that
these are not informative texts, and therefore we can relax
some constraints taken into account in other works in the area
of referring expressions.
6 Conclusions and future work
With respect to both of the tasks addressed, the output texts
respect the specific constraints required for the text to be ac-
ceptable, while at the same time showing reasonable variation
between the different options much as a human-generated text
would. We are working on extending the system to allow the
use of proper nouns to describe some concepts, as an addi-
tional option to pronouns and descriptive references, includ-
ing the revision of the genetic operators and the introduction
of new evaluation functions to estimate the correct applica-
tion of proper nouns.
In view of these results, in future work we want to apply
EA techniques to other tasks of NLG, such as content de-
termination and discourse planning. The particular advan-
tages of evolutionary techniques, combined stage by stage in
this manner, may be an extremely powerful method for solv-
ing natural language generation problems while also profiting
from classic NLG techniques.
It would be also interesting to compare our solution with
different approaches found in the literature, as for example
[Reiter and Dale, 1992] or [Krahmer and Theune, 2000] for
the referring expression generation, and the one of Dalianis
and Hovy [Dalianis and Hovy, 1996] for the aggregation.
Finally, an evaluation as the one proposed in [Callaway and
Lester, 2001] would be useful to estimate the goodness of the
generated texts. The authors describe the evaluation of STO-
RYBOOK, a narrative prose generation system that produces
original fairy tales in the Little Red Riding Hood domain.
They pretend to evaluate multiple versions of a single story
assuring that the content is identical across them. Five ver-
sions of two separate stories are produced, a pool of twenty
students in English compare them, and at last they are ana-
lyzed with an ANOVA test.
References
[Cahill et al, 2001] L. Cahill, R. Evans, C. Mellish,
D. Paiva, M. Reape, and D. Scott. The RAGS reference
manual. Technical Report ITRI-01-07, Information Tech-
nology Research Institute, University of Brighton, 2001.
[Callaway and Lester, 2001] C. Callaway and J. Lester.
Evaluating the effects of natural language generation tech-
niques on reader satisfaction. In Proceedings of the
Twenty-Third Annual Conference of the Cognitive Science
Society, Edinburgh, UK, 2001.
[Dalianis and Hovy, 1996] H. Dalianis and E. Hovy. Ag-
gregation in natural language generation. In G. Ardoni
and M. Zock, editors, Trends in Natural Language Gen-
eration: an Artificial Intelligence Perspective,EWNLG?93,
pages 88?105. Springer Verlag, 1996.
[Duboue and McKeown, 2002] P.A. Duboue and K.R. McK-
eown. Content planner construction via evolutionary algo-
rithms and a corpus-based fitness function. In Proceedings
of the Second International Natural Language Generation
Conference (INLG 2002), Ramapo Mountains, NY, 2002.
[Garc??a et al, 2004] C. Garc??a, R. Herva?s, and P. Gerva?s.
Una arquitectura software para el desarrollo de aplica-
ciones de generacio?n de lenguaje natural. Sociedad
Espan?ola para el Procesamiento del Lenguaje Natural,
Procesamiento de Lenguaje Natural, 33:111?118, 2004.
[Gerva?s et al, 2004] P. Gerva?s, B. D??az-Agudo, F. Peinado,
and R. Herva?s. Story plot generation based on CBR. In
Anne Macintosh, Richard Ellis, and Tony Allen, editors,
12th Conference on Applications and Innovations in In-
telligent Systems, Cambridge, UK, 2004. Springer, WICS
series.
[Holland, 1992] J.H. Holland. Adaptation in Natural and
Artificial Systems. An Introductory Analysis with Applica-
tions to Biology, Control and Artificial Intelligence. MIT
Press, Cambridge, Massachusetts, Second Edition, 1992.
[Kibble and Power, 2000] R. Kibble and R. Power. An inte-
grated framework for text planning and pronominalization.
In Proc. of the International Conference on Natural Lan-
guage Generation (INLG), Israel, 2000.
[Krahmer and Theune, 2000] E. Krahmer and M. Theune.
Efficient context-sensitive generation of referring expres-
sions, 2000.
[Levy, 2001] R. P. Levy. A computational model of poetic
creativity with neural network as measure of adaptive fit-
ness. In Proccedings of the ICCBR-01 Workshop on Cre-
ative Systems, 2001.
[Manurung, 2003] H.M. Manurung. An evolutionary algo-
rithm approach to poetry generation. PhD thesis, School
of Informatics, University of Edinburgh, 2003.
[Mellish et al, 1998] C. Mellish, A. Knott, J. Oberlander,
and M. O?Donnell. Experiments using stochastic search
for text planning. In Eduard Hovy, editor, Proceedings
of the Ninth International Workshop on Natural Language
Generation, pages 98?107. Association for Computational
Linguistics, New Brunswick, New Jersey, 1998.
[Reape and Mellish, 1999] M. Reape and C. Mellish. Just
what is aggregation anyway? In Proceedings of the
7th European Workshop on Natural Language Generation,
Toulouse, France, 1999.
[Reiter and Dale, 1992] E. Reiter and R. Dale. A fast algo-
rithm for the generation of referring expressions. In Pro-
ceedings of the 14th conference on Computational linguis-
tics, Nantes, France, 1992.
[Walker et al, 1998] M.A. Walker, A.K. Joshi, and E.F.
Prince. Centering Theory in Discourse. Clarendon Press,
Oxford, 1998.
Proceedings of the 12th European Workshop on Natural Language Generation, pages 66?73,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Model for Human Readable Instruction Generation Using Level-Based
Discourse Planning and Dynamic Inference of Attributes Disambiguation
Daniel Dionne, Salvador de la Puente, Carlos Leo?n, Raquel Herva?s, Pablo Gerva?s
Universidad Complutense de Madrid
Madrid, Spain
{dionnegonzalez,neo.salvador}@gmail.com,
{cleon,raquelhb}@fdi.ucm.es,pgervas@sip.ucm.es
Abstract
This paper shows a model of automatic in-
struction giving for guiding human users
in virtual 3D environments. A multilevel
model for choosing what instruction to
give in every state is presented, and so
are the different modules that compose the
whole generation system. How 3D in-
formation in the virtual world is used is
explained, and the final order generation
is detailed. This model has been imple-
mented as a solution for the GIVE Chal-
lenge, an instruction generation challenge.
1 Introduction
Recent technology advances have made it possi-
ble to use handheld devices, like mobile phones
or PDAs, to guide the user by issuing commands
or descriptions about the world the user is per-
ceiving in some sense (Muller, 2002). This pos-
sibility opens interesting avenues of research in
the shape of Natural Language Generation (NLG)
Systems that adapt to the user in order to provide
him with the most accurate expression. However,
fully operational systems applicable in real life sit-
uations are difficult and expensive to implement.
Under these circumstances, virtual environments
may be seen as an intermediate solution, suitable
for fast prototyping of experimental solutions. Vir-
tual environments permit experimenting in a re-
duced, closed world, where everything that is rel-
evant for the purpose at hand is explicitly repre-
sented in a graphical model and under the direct
control of the researcher. This allows fast set up of
experimental situations where the topography, the
position of landscape features, colour, light con-
ditions and visibility factors can be modified and
adapted to suit the best conditions for testing par-
ticular approaches (Blue et al, 2002) or challenges
(such as guidance for disabled users with different
disabilities, for instance). In view of these obser-
vations, our research is focused on developing an
interactive virtual guide (VG), based on NLG, to
give to a human user the required set of instruc-
tions to complete a specific task.
Such a set of instructions is called a plan. For-
mally, a plan is a sorted-in-time list of instructions
that the user must fulfill in order to reach some
goal. There are many planning algorithms that,
with the proper world representation and a list of
goals, can return a list like this (LaValle, 2006).
The VG can take this basic plan as the actual set
of instructions to convert into natural language to
explain what the user must do to complete the task.
However, these instructions are usually exhaustive
(step by step) and very simple because they are
based on basic world representations (and inter-
pretations) and are simple enough to perform com-
putational operations on them. A VG that gener-
ates this kind of simple instructions, from the point
of view of a human user, can be tedious, boring
and a time wasting. Consider the discourse ?Turn
right. Turn right. Go ahead. Turn left. Press
button-1. Turn around. Go ahead. Go ahead. Take
item-1. . . ? as an example. Instead, the VG should
take advantage of the environmental knowledge of
the user inferring higher level instructions (less de-
tailed and more human-like) from the basic plan
(something more along the lines of ?Go press the
buton in the far wall, come back and take item-1?).
The difference is shown graphically for a simple
example in Figure 1.
There are several aspects to be considered in
achieving this goal. First, a human guide would
phrase his or her instructions at different levels of
abstraction, to optimise the communicative effect
of his/her utterances in terms of striking balance
between sufficient informative content and econ-
omy of expression. Second, a human guide may
operate in a reactive manner, providing additional
feedback whenever the user requests help. But
66
7 instructions 1 instruction
Figure 1: A comparison of a step by step plan
versus a human readable plan like ?Walk out the
door?. Note the difference in the number of in-
structions given.
human guides are also likely to observe the per-
son that is being guided, and be ready to intervene
proactively if they notice the user seems lost or at
risk. These two points are elaborated below.
In order to build more human levels, a VG must
consider the virtual environment in a manner as
close as possible to the way a human being senses
the real world. To model the different levels of ab-
straction employed by human guides, a good solu-
tion may be to model the world as a hierarchy of
spatial levels. People tend to limit the area where
they do certain activities by some kind of logical
borders. Sometimes, these borders match physi-
cal borders such as the walls that define a room
or a corridor, the outside perimeter of a building,
the limits of a park, or a city boundary. In other
cases, such as outdoor settings, borders can be
more abstract, such as the line of horizon in all di-
rections from the observer?s current position. The
areas defined by these borders may be contained
inside one other, resulting in a tree-like structure
from the smallest spaces to greater areas, i.e. from
the room where the user is standing to the city he
lives in. Of course, the areas are connected in a
multigraph way where each edge is a connection
like a door or a natural transition. To build a us-
able model of this type of cognitive representation
of the world is far from trivial. We will describe
how we faced this point in Section 3.1 (Construct-
ing the World). Considering such a hierarchical
view of the environment when generating instruc-
tions, results in more natural and human-friendly
results. Instructing someone to ?exit the room?
works better than asking them to ?advance until
passing through the door?; ?leave the building
using the main entrance? is better than a set of
instructions refering to more specific spaces like
?exit this room, now go down the stairs, now go to
the elevator? and so on. We return to this matter
in Section 3.2 (Planning the Discourse).
The issue of abstractions in world modelling
also affects a different language generation task:
referring expression generation. In providing in-
structions, human guides often refer to abstract
concepts such as corners or ?the middle of the
room?. These are not usually represented explic-
itly in your run of the mill world representation,
which usually prevents NLG systems from em-
ploying them as means of optimising references.
In Section 3.4 (Hidden Reference Discovery), we
will see how, besides visible information, a natural
approach based on the inference of other ?hidden?
elements or references that can be extracted from
the environment helps to reduce the length of the
explanation needed, and to build better references.
These elements are hidden because they are not
visible or trivial, and they require a specific study
and calculation.
The second point to consider is reactive versus
proactive guidance. A reactive guidance system
may rely on feedback from the user to decide when
to intervene. Consider the following two represen-
tative examples: the user can say ?I did not un-
dertand last instruction? and the VG system can
answer by repeating the instruction or building a
new one phrased in a different way but with the
same meaning; or the user can say ?I am lost?
and the VG will ask the planning software to re-
calculate the plan considering the new user?s sit-
uation. However, there are situations where the
user may not realize that he is lost or that he is
about to perform a dangerous action (like walking
on a slippery surface, pressing an incorrect button,
going in the wrong direction or crossing a street
when the traffic light is red). A good guide will
warn the user before he does something wrong but
it should not oppress the user each time he decides
to explore another route to reach the goal. In other
words, the VG must watch the user actions and
take part when he is on the verge of commiting
a serious mistake. We will discuss about how to
warn the user in Section 3.3 (Warning the User).
2 Previous Work
Many NLG systems have considered generation
of instructions in the past. A good review is pro-
vided in (Bourne, 1999). However, most existing
instruction generating system focused on perform-
67
ing different types of static actions (actions that do
not involve changes of location of the user). The
present work is focused on the task of guiding the
user through virtual environments.
The GIVE (Generating Instructions in Virtual
Environments) Challenge (Byron et al, 2007) op-
erates on a scenario where a user has to solve a
particular task in a simulated 3D space. A gen-
eration module has to guide the human user using
natural language instructions. A software architec-
ture is provided that allows the generation module
to abstract away from the rest of the system, while
having access to world information from the 3D
environment, user feedback from the client mod-
ule, and plans generated by an off-the-shelf plan-
ner. The work presented in this paper arose from
the author?s participation in the GIVE Challenge,
and relies on the software architecture provided
for the challenge to implement all details of the
system other than the NLG module.
A fundamental task to be solved for correct in-
struction generation is the construction of appro-
priate referring expressions. This task has been
the object of many research efforts in the recent
past. To construct a reference to a particular en-
tity, the algorithm takes as input a symbol corre-
sponding to the intended referent and a list of sym-
bols corresponding to other entities in focus based
the intended referent, known as the contrast set.
The algorithm returns a list of attribute-value pairs
that correspond to the semantic content of the re-
ferring expression to be realized. The algorithm
operates by iterating over the list of available at-
tributes, looking for one that is known to the user
and rules out the largest number of elements of the
contrast set that have not already been ruled out.
Referring Expression Generation in physically
situated environments has been studied in (Kelle-
her and Kruijff, 2005). The goal of this work is to
develop embodied conversational robots that are
capable of natural, fluent visually situated dialog
with one or more interlocutors. In this kind of
situation a very important aspect to take into ac-
count is how to refer to objects located in the phys-
ical environment. The authors present in the paper
a computational framework for the generation of
spatial locative expressions in such contexts, rely-
ing on the Reiter and Dale (Reiter and Dale, 1992)
algorithm.
Another interesting work related to referring ex-
pression generation in spatial environments can be
found in (Varges, 2005). The author uses the maps
of the Map Task dialogue corpus as domain mod-
els, and treats spatial descriptions as referring ex-
pressions that distinguish particular points on the
map from all other points (considered as distrac-
tors).
Related research can be found in (Stoia et al,
2006), where a study of how humans give orders in
navigation environmnets and an algorithm imple-
menting the observed behaviour is shown. There
are many other approaches to instruction giving.
Directly related with this work, it is worth men-
tioning CORAL (Dale and Geldof, 2003), which
shows a full architecture for instruction giving,
and REAL (Muller, 2002), which shows a multi-
modal system (graphics and text) for communicat-
ing with the user, adapting them to user behaviour.
3 A Functional Model of a Virtual Guide
The model of a virtual guide presented here ad-
dresses four specific issues: how to construct a
representation of the world with higher levels of
representation, how to generate higher instructions
referring to the more abstract levels of represen-
tation, how the construction of references is im-
plemented in terms of reference agents. A brief
overview of the complete architecture of the mod-
ule is also included.
3.1 Constructing the World
In GIVE, the world is discretized as a set of tiles.
These tiles are the minimum portions of space and
the user can move around from tile to tile. Orienta-
tions are discretized: the user can only face North,
East, South or West. By default, the world consists
of an infinite area of adjacent and accesible tiles.
World representation assertations may state there
is a wall between two adjacent tiles, blocking ac-
cess from one to other. A 3D representation of this
basic world gives the user an illusion of rooms but,
from the point of view of the VG there is no data
structure that reflects a hierarchy of rooms. This
representation does not fit very well with the hu-
man sense of space, so a more abstract one had to
be built to provide the abstract referents (rooms,
corners, intersections, doors...) which we wanted
our guide to use.
The first problem we had was defining a room.
In architecture, a definition of room is ?any dis-
tinguishable space within a structure?, but distin-
guishable is too vague to be of use. Figure 2 illus-
68
A) One big room
B) Three smaller rooms
Figure 2: Defining a distinguishable space.
trates the problem of defining when two spaces are
distiguishable. Notice the only difference betwen
A and B is the width of the gaps in relation to the
size of the rooms. This problem has been exten-
sively studied in robotics. An interesting exam-
ple (Galindo et al, 2005) consists on identifying
interconected ?open spaces? in order to obtain an
adjacency graph. From that graph, another graph
can be calculated, grouping spaces to form rooms,
corridors, etc.
For practical purposes, we have decided to con-
sider that two spaces are distinguishable when the
user has to go through a door to get from one to the
other, with a door being a one-tile gap in a wall.
Based on this definition, we have developed an
algorithm to group adjacent tiles into rooms. The
idea is to follow a wall around the room until the
starting point is reached, thereby establishing the
perimeter of the room, then establish the set of
tiles corresponding to the room using a floodfill
algorithm. Breaks in walls are handled by check-
ing whether they are small enough to be consid-
ered doors into other rooms or not. If they are
doors, they are noted as entrances to other rooms
(which are stored in a room list for subsequent pro-
cessing). If they are not, the wall beyond the gap
is followed as part of the boundary of the current
room. A small practical example of the algorithm
in operation is shown in Figure 3.
Adjoining rooms stored in the room list are
recursively processed. Each new room discovered
is connected to its adjacent rooms to obtain a high
level map of the available space. An analyzer is
applied to each room to establish its type (room,
hall, corridor, etc) and additional properties such
as size or shape. This new world representation
A) First, nd anywall B) Not a door,the gap is too big
C) Was not a door,so go back. D) Small gap (door),so add it to DC.
Figure 3: Looking for rooms.
GOAL
A5
A3
H1 H2 H3 H5H4
A5
Time ow 
H7H6
Figure 4: Tree representation of the plan at several
levels.
allows the VG to refer to doors and rooms.
3.2 Planning the Discourse
Discourse planning must take place at two differ-
ent levels of detail. The VG must plan the dis-
course corresponding to the whole set of instruc-
tions to be imparted until the final goal is reached.
But it also needs to plan how much of that is to
be communicated to the user in the next turn of
the dialogue. We solve the first issue by build-
ing a multi-level representation of the expected
discourse for the whole of the plan to be carried
out by the user. This representation is structured
like a tree, with the set of low-level instructions as
leafs, and subsequent nodes of the tree represent-
ing higher level instructions that group together
the lower level instructions represented by their
subtrees. The solution to the second issue is de-
scribed below.
We define action as anything the user can do
69
Line of sightCheckpoint User?sroute
Figure 5: An n-shaped room does not let the user
see the exit of the room so VG can guide the user
from checkpoint to checkpoint.
that modifies the state of the world and instruc-
tion as an action that the user should perform in
order to advance in the plan. Instructions are de-
fined in terms of preconditions and postconditions.
Preconditions are conditions that must be satis-
fied for the instruction to be performed, and post-
conditions are the conditions that must be satisfied
to consider the instruction done. The instruction
tree representation of the plan is built by group-
ing together sets of low-level instructions into a
single high-level instruction. For instance, we
group all tile-by-tile steps inside the same room
to build a new instruction such as ?go from room1
to room2?. We do not discard any low-level in-
struction, we just group them under the new high-
level instruction, building a tree that represents the
plan at different levels of abstraction (see Figure
4). This allows the user to fall back on low-level
instructions at need (if, for instance, the light goes
out and the VG has to guide him step by step).
An additional abstraction has been introduced
to account for the tendency of humans to break
the description of a complex path (where not all
of the path is visible at the start) into segments
made of the portions of the path that are visible at
each particular point (see Figure 5). The concept
of checkpoint is introduced for the end of each of
these segments.
We have defined five types of high-level in-
structions: MovementInstruction (guides the
user from tile to tile), CheckPointInstruction
(guides the user from a his current position to
a checkpoint), Room2RoomInstruction (guides
the user from room to room), ActionInstruc-
tion (tells the user to interact with some ele-
ment) and GoalInstruction (subtype of ActionIn-
struction concerned with achieving the final goal).
Each of these high-level instructions has its own
preconditions and postconditions.
The issue of how much of the instruction tree
representation of the plan is addressed in terms of
two conditions: how far in the original plan the
user has advanced, and what level of abstraction is
required for the next instruction. The first condi-
tion is easily checked over the state of the world, to
establish what the current situation is. The second
condition is determined by checking for satisfac-
tion of preconditions and postconditions of the in-
structions at all possible levels that start from the
current situation. The check starts at the highest
possible level.
Instructions whose postconditions are already
satisfied are pruned from the tree, as there is no
longer any need to provide that instruction to the
user. If preconditions are met but postconditions
are not, the VG uses this instruction in the next
turn, and then waits for a user action. If neither
postconditions nor preconditions are satisfied for
this instruction, the next (lower) level of instruc-
tions in the instruction tree is considered instead
of this one. These decisions are handled by mod-
ules known as Guide Agents.
3.3 Warning the User
If the user is going to cross a street when the traffic
light is red, the VG will have to warn him about it.
If the warning information is more important than
the guiding, the VG will have to delay instruction
giving, and warn the user first. To decide about the
importance of the warning part of the discourse,
we defined agents as entities in charge of watch-
ing for special situations. Each agent takes care of
a specific kind of situation that may imply some
sort of hazardous or bad result. They are all inde-
pendent, and may differ depending on the kind of
environment, goals or even the kind of user.
Each agent has a weight that reflects its priority
when being considered. An agent always evalu-
ates its situation and returns a value in the [0, 1]
interval. A near zero value means there are low
probabilities for the situation to happen and a near
to one value means the situation is on the verge
to happening. All agents that exceed a threshold
value will be considered as contributors to the dis-
course. We sort them in descending order based
on the result of multiplying each return value by
the weight of the agent. If an agent is considered
70
as a contributor, its warning is introduced in the
discourse.
We defined three types of agents: information
agents watch for interesting hotspots in an area,
status agents watch over the user?s status, and
area agents watch over special areas, including
dangerous areas.
In our entry for the GIVE challenge there was
a status agent that checked how much time had
passed since the last user action to identify when
the user might be lost. There was one agent that
checked for booby traps the user might step on
(some of them resulted in loosing the game in-
mediately). Another one ensured the user re-
mained within a security area that abstracted all
possible common routes to reach the intended des-
tination. If a user leaves the security area, he is
going in the wrong direction.This security area is
dynamicaly updated attending to the current user?s
position. Finally, alarm agents watch for wrong
actions, controlling if user is on the verge of press-
ing the wrong button or leaving the room using
a wrong exit. We implemented no information
agents, but they would be interesting in real sit-
uations.
3.4 Hidden Reference Discovery
The center spot in a room is not a visible or tan-
gible object, and finding it requires a non-trivial
calculation of the room?s shape. Adding it to
the references container can help creating simpler
and richer sentences. A reference like ?the table
across the room? can be generated when the lis-
tener and the target are in line with the center spot
of the room, on opposite sides, independently of
where the user is facing. In an indoor environ-
ment, architectural elements usually make many
inferences possible. Two hallways that intersect
make an intersection, two walls make a corner, etc.
and though these elements might not be referenced
as they are in the given environment, they should
be taken into account. In a similar way, hidden
relations discovery can be accomplished. Object
alignments or arrangements can be revealed and
used for the same purpose. Sentences like ?the car
in line with these pillars? can be generated. All of
these additional high-level concepts and relations
between them and low-level world entities are ob-
tained by abstraction over the available represen-
tation. We create a family of reference agents,
each one specialized in identifying candidate dis-
Oppositethe green door
Room Center
Corner
Betweenthe bluedoors
Figure 6: Hidden references in a room.
ambiguating properties of a different kind. Some
of these properties are already explicit in the world
representation (colour) and some require a pro-
cess of abstraction (relations to corners, for in-
stance). Once obtained, they become available as
additional properties that may be used to disam-
biguate references.
The goal of our design is to leverage the sys-
tem?s ability to express itself using different com-
binations of the complete set of disambiguating
properties made available in this manner. This
gives system designers a choice between having
many simple agents or fewer more expressive,
complex agents. This choice should be considered
in terms of particular implementation details.
Reference agents rely on the Reiter and Dale al-
gorithm (Reiter and Dale, 1992). Considering a
list of distractors and the reference object, the goal
is to filter the distractors list, building a reference
that takes out all the distractors, so that the refer-
ence is good, not ambiguous. Each reference agent
has the ability of taking out a different set of dis-
tractors, using different properties that are trivial
or hidden, as explained above. Combining these
agents in different ways generates different refer-
ence sentences, some of them longer but more spe-
cific, others shorter but ambiguous. What we tried
to achieve is to find the right combination of refer-
ence agents that create the shortest non-ambiguous
sentence. This is not a natural approach, as some-
one could prefer to have an ambiguous (but more
human) spatial relation (Viethen and Dale, 2008)
in a reference sentence. Or for example, someone
could prefer having a longer reference like ?the big
red box that?s on the third shelf from the bottom?
than a perfectly specific (but not natural) reference
like ?the 3 kg box?.
71
REALWORLD
WORLDANALYSIS EXPANDEDWORLD
GOALS
DISAMBIGUATION
APPROXIMATION STAGE
ALERTS
INSTRUCTION TREE1
2
3
LEVELS
GUIDE MANAG
ERGUIDE AGENT 1GUIDE AGENT 2
G. 3.1G. 3.n ...
REFER
RER M
ANAGE
R ReferrerReferrerReferrer
Referrer
ReferrerReferrerALARM
 MANA
GERAlarm AgentAlarm AgentAlarm AgentAlarm Agent
Alarm Agent
Alarm Agent
GOAL SUBSETCURRENTINSTRUCTION
GOAL
PLANNER
generatedoutput
GENERATIONMANAGER
Figure 7: General design.
3.5 Guide architecture
The architecture design can be divided into two
main parts. The instruction tree, shown as three
interconnected lists in Figure 7, that contains all
the generated levels of instructions as explained
in section 3.2, and a set of components that per-
form the different guiding tasks. One input for the
system is the ?Real World?, as opposed to the Ex-
panded World that is generated after the analysis,
as explained in sections 3.1 and 3.4. The second
input is the set of goals to be achieved. After the
basic instruction set is generated by the planner
from the given set of goals, the instruction tree is
generated, level by level.
Figure 7 represents a state of the guiding pro-
cess where the user is trying to achieve some in-
termediate GOAL. The current instruction marker
represents the location of the instruction that is to
be given to the user to achieve the current GOAL
(the one on the upper level). Since at this point
the system has determined that level 2 instructions
should be used, the level 2 subset of instructions
are represented here as part of the current instruc-
tion. As explained in section 3.2, the algorithm
chooses what level should be used at each mo-
ment.
The Guide Manager makes use of the Alarm
Manager and Referrer Manager to create the
proper output. As explained in 3.3, the Alarm
Agents examine the environment, and tell the
Guide Manager if the user should be warned about
any hazardous situation. The Referrers help build-
ing the proper reference sentences, as explained
in sections 3.2 and 3.4, finally the different Guide
help building the proper guiding sentences. The
Guide Manager sends the output to the Genera-
tion Manager, which is in charge of generating the
final output.
4 Discussion
The layered, multilevel hierarchy tries to imitate
the way humans think about local plans, and the
agent based view attemps to make instruction giv-
ing proactive rather than reactive. The algorithm
first gives generalistic, global orders to get the
user near the particular objective. Then, once
the irrelevant information has been removed from
the user point of view and it can not confuse the
user, more specific orders are given. In this way,
the algorithm decides what to say the ?human
way?. Although the ?human? generation of in-
structions could have been obtained with different
algorithms, doing it the same way creates a more
maintainable, natural form of expressing the oper-
ation. It would be interesting to input real human
data, as done in (Stoia et al, 2006), in order to
guarantee this objective.
Traditionally, planning systems have certain
world representation based on discrete states
which are more or less useful for finding a good
solution (Chih-Wei Hsu and Chen, 2006). How-
ever, this representation is not necessarily useful
for creating a natural language representation of
each planning operator. For a good instruction
to be generated, plain operators like ?turn right?
usually do not contain much information. Instruc-
tion generation systems have to find a compromise
between planning efficiency and natural language
content. Creating the instruction tree depends di-
rectly on figuring out what elements to include in
the discourse.
The architecture shown in Section 3 has been
designed with adaptability in mind, following the
architecture presented in (Dale and Geldof, 2003).
This shows a module layout where the text plan-
72
ner and the surface realizer are independently con-
nected in the generation pipeline.
5 Conclusions and Future Work
The decisions to consider higher level of abstrac-
tion for both the representation of the world and
the granularity of instructions, and the introduc-
tion of alarms have shown very satisfactory results
over informal tests with users. Further evaluation
is in process as part of the GIVE Challenge (Koller
et al, 2007)1. The decisions presented in this pa-
per should be revised in view of these results. The
definition of a security area enables the system to
provide suitable warning when the user really goes
out of the way, but makes the system robust with
respect to minor variations with respect to the lit-
eral plan provided by the planner.
The GIVE challenge set up was a good starting
point to begin our experiments, but we are con-
sidering more complex environments to test ad-
vanced features. Extensions that promise interest-
ing challenges are: the consideration of a contin-
uous world representation (rather than discretised
in terms of tiles and four cardinal points), more re-
alistic test maps to extend the level of hierarchy to
buildings and urban areas, and new environments
designed to experiment with distorted representa-
tions of the scenary in order to simulate physical
impediments like blindness.
Acknowledgments
This research is funded by the Ministerio de In-
vestigacio?n, Ciencia e Innovacio?n (GALANTE:
TIN2006-14433-C02-01), and Universidad Com-
plutense de Madrid and Comunidad de Madrid
(MILU: CCG07-UCM/TIC 2803).
References
Russell S. Blue, Jeff Wampler, G. Bowden Wise,
Louis J. Hoebel, Boris Yamrom, Christopher R.
Volpe, Bruce Wilde, Pascale Rondot, Ann E. Kelly,
Anne Gilman, Wesley Turner, Steve Linthicum, and
George Ryon. 2002. An automated approach and
virtual environment for generating maintenance in-
structions. In CHI ?02: CHI ?02 extended abstracts
on Human factors in computing systems, pages 494?
495, New York, NY, USA. ACM.
Juliet C. Bourne. 1999. Generating Effective Natu-
ral Language Instructions based on Agent Expertise.
Ph.D. thesis, University of Pennsylvania.
1The results of this challenge will be made available as
part of the ENLG 2009 Workshop.
Donna Byron, Alexander Koller, Jon Oberlander, Laura
Stoia, and Kristina Striegnitz. 2007. Generating in-
structions in virtual environments (GIVE): A chal-
lenge and evaluation testbed for NLG. In Proceed-
ings of the Workshop on Shared Tasks and Compar-
ative Evaluation in Natural Language Generation,
Arlington.
Ruoyun Huang Chih-Wei Hsu, Benjamin W. Wah and
Yixin Chen. 2006. Handling soft constraints and
goals preferences in SGPlan. In ICAPS Workshop
on Preferences and Soft Constraints in Planning.
Robert Dale and Sabine Geldof. 2003. Coral: Using
natural language generation for navigational assis-
tance. In Proceedings of the 26th Australasian Com-
puter Science Conference.
C. Galindo, A. Saffiotti, S. Coradeschi, P. Buschka,
J.A. Fernandez-Madrigal, and J. Gonzalez. 2005.
Multi-hierarchical semantic maps for mobile
robotics. Intelligent Robots and Systems, 2005.
(IROS 2005). 2005 IEEE/RSJ International Confer-
ence on, pages 2278?2283, Aug.
John D. Kelleher and Geert-Jan M. Kruijff. 2005. A
context-dependent algorithm for generating locative
expressions in physically situated environments. In
Proceedings of ENLG-05, Aberdeen, Scotland.
Alexander Koller, Johanna Moore, Barbara di Eugenio,
James Lester, Laura Stoia, Donna Byron, Jon Ober-
lander, and Kristina Striegnitz. 2007. Shared task
proposal: Instruction giving in virtual worlds. In
Michael White and Robert Dale, editors, Working
group reports of the Workshop on Shared Tasks and
Comparative Evaluation in Natural Language Gen-
eration.
S. M. LaValle. 2006. Planning Algorithms. Cam-
bridge University Press, Cambridge, U.K. Available
at http://planning.cs.uiuc.edu/.
Christian Muller. 2002. Multimodal dialog in a mobile
pedestrian navigation system. IDS-2002.
E. Reiter and R. Dale. 1992. A fast algorithm for the
generation of referring expressions. In Proceedings
of the 14th conference on Computational linguistics,
Nantes, France.
Laura Stoia, Donna Byron, Darla Shockley, and Eric
Fosler-Lussier. 2006. Sentence planning for real-
time navigational instructions. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL.
Sebastian Varges. 2005. Spatial descriptions as refer-
ring expressions in the maptask domain. In Proc. of
the 10th European Workshop on Natural Language
Generation.
Jett Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Fifth
International Natural Language Generation Confer-
ence.
73
Proceedings of the 12th European Workshop on Natural Language Generation, pages 187?188,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Evolutionary and Case-Based Approaches to REG: NIL-UCM-EvoTAP,
NIL-UCM-ValuesCBR and NIL-UCM-EvoCBR
Raquel Herva?s and Pablo Gerva?s
Natural Interaction based on Language (NIL)
Universidad Complutense de Madrid
raquelhb@fdi.ucm.es, pgervas@sip.ucm.es
1 Evolutionary Approach to Attribute
Selection
We propose the use of evolutionary algorithms
(EAs) (Holland, 1992) to deal with the attribute
selection task of referring expression generation.
Evolutionary algorithms operate over a population
of individuals (possible solutions for a problem)
that evolve according to selection rules and ge-
netic operators. The fitness function is a metric
that evaluates each of the possible solutions, en-
suring that the average adaptation of the popula-
tion increases each generation. Repeating this pro-
cess hundreds or thousands of times leads to very
good solutions for the problem.
We encode as a fitness function the specific con-
straints required for the reference to be acceptable.
The crossover and mutation genetic operators en-
sure a reasonable variation between the different
options much as a human-generated text would.
Each individual is represented by a set of genes
that are the list of possible attributes in the refer-
ence. Each gene has an associated value of 0 (if the
attribute is not included in the reference), or 1 (if
the attribute is included in the reference). The ini-
tial population should have a low number of genes
set to 1, because references tend to be short and the
use of all the possible attributes should be avoided.
For the crossover operator, two individuals are
selected randomly and crossed by a random point
of their structure. For the mutation operator, some
of the genes are chosen randomly to be mutated
from 1 to 0, or vice versa.
The fitness function must find a balance be-
tween the univocal identification of a referent, and
a natural use of attributes. The formula used as
fitness function is defined in Equation 1:
fitindi = fatti?weightatt+ident?weightid (1)
where ident represents whether the reference is
univocally identifying the target among the dis-
tractors, and fatti computes the role of attributes
as the normalised sum of the weight (depending
on its absolute frecuency in ATTRIBUTE-SET
elements in the corpus) of all attributes present
(gene=1), as defined by Equation 2:
fatti =
?
geneatti ? weightatti
#attsRef
(2)
2 Case-Based Reasoning for Realization
Template-based solutions for natural language
generation rely on reusing fragments of text ex-
tracted from typical texts in a given domain, apply-
ing a process of abstraction that identifies which
part of them is common to all uses, and leaving
certain gaps to be filled with details correspond-
ing to a new use. A case-based solution (Aamodt
and Plaza, 1994) to reference realization can ob-
tain the information needed to realize a reference
from the original examples of appropriate use that
originated the templates.
In our approach, a case consists of a de-
scription of the problem (ATTRIBUTE-SET) and
a solution (ANNOTATED-WORD-STRING inter-
preted as a template). Cases are stored in a
Case Retrieval Net (CRN) (Lenz and Burkhard,
1996), a memory model developed to improve
the efficiency of the retrieval tasks of the
CBR cycle. Each attribute-value pair from the
ATTRIBUTE-SET is a node in the net. Templates
in ANNOTATED-WORD-STRING are considered
as solutions to the cases. Similarities between the
nodes are established for the retrieval stage of the
CBR process. For example, we have considered
that ?back? and ?right? orientation values have a
higher similarity than ?back? and ?front? that are
exactly the opposite.
The attribute-value pairs of ATTRIBUTE-SET
that must be realized in a final string are used
to query the net, which returns the more similar
cases. Only one of them must be chosen to be
adapted for the solution. We consider four differ-
ent types of retrieved cases: preferred (cases with
exactly the same attributes than the query), more
(cases with the same attributes as the query and
187
String Edit Norm. Edit BLEU 1 BLEU 2 BLEU 3 BLEU 4
Acc. Dist. Distance Score Score Score Score
Furniture 0,08 4,87 0,51 0,44 0,33 0,24 0,18
EvoTAP People 0,03 6,04 0,59 0,39 0,25 0,15 0,00
Both 0,06 5,41 0,55 0,41 0,29 0,20 0,13
Furniture 0,01 5,91 0,55 0,44 0,31 0,20 0,13
ValuesCBR People 0,01 5,80 0,56 0,43 0,28 0,17 0,08
Both 0,01 5,86 0,55 0,44 0,30 0,19 0,11
Furniture 0,04 5,77 0,58 0,39 0,26 0,18 0,13
EvoCBR People 0,01 6,94 0,61 0,41 0,25 0,16 0,08
Both 0,03 6,31 0,59 0,41 0,26 0,17 0,11
Table 1: Results over development data for the three systems
some more), lessExtra (cases that lack some at-
tribute from the query but have some extra ones),
and lessNoExtra (cases that lack some attribute
from the query and have no extra ones). The or-
der given is the preferred order to chose the most
suitable case for the query.
Adaptation of the chosen case depends on its
type. The idea is to keep all the parts of the tem-
plate that correspond to attributes common to the
query and the case. Extra attributes in the case
that do not appear in the query are discarded. At-
tributes in the query not appearing in the case are
lost.
3 Results and Discussion
We have tested both solutions (evolutionary and
case-based) separately and together in three differ-
ent systems, relying on solutions presented in last
year?s challenge.
? NIL-UCM-EvoTAP. Selects attributes using
the evolutionary solution and realises using
the NIL-UCM-BSC solution (Gerva?s et al,
2008).
? NIL-UCM-ValuesCBR. Selects attributes
using the NIL-UCM-MFVF solution (Gerva?s
et al, 2008) and realizes using the case-based
approach.
? NIL-UCM-EvoCBR. Selects attributes us-
ing the evolutionary solution and realizes us-
ing the case-based approach.
The results obtained by the three systems over
development data are shown in Table 1.
The evolutionary approach performs poorly but
might be improved by using a more refined al-
gorithm for calculating attribute weights, such as
done in the last year NIL-UCM-MFVF solution.
The reported CBR results were obtained over
a case base built from a selection of the avail-
able training data (samples that relied on data
not available in the input were omitted). This
approach could be further refined by generating
style-specific subsets of the case base.
Acknowledgments
This research is funded by the Spanish Ministry of
Education and Science (TIN2006-14433-C02-01).
References
Aamodt, A. and Plaza, E.. 1994. Case-based reason-
ing: Foundational issues, methodological variations,
and system approaches AI Communications, 7(1).
Gerva?s, P. and Herva?s, R. and Leo?n, C. 2008. NIL-
UCM: Most-Frequent-Value-First Attribute Selec-
tion and Best-Scoring-Choice Realization. Refer-
ring Expression Generation Challenge 2008, INGL-
08, USA.
Holland, J.H. 1992. Adaptation in Natural and Arti-
ficial Systems. An Introductory Analysis with Ap-
plications to Biology, Control and Artificial Intelli-
gence. MIT Press, Cambridge, Massachusetts, Sec-
ond Edition.
M. Lenz and H. Burkhard 1996. Case Retrieval Nets:
Basic Ideas and Extensions. Kunstliche Intelligenz.
188
Degree of Abstraction in Referring Expression Generation and its Relation
with the Construction of the Contrast Set
Raquel Herva?s
Facultad de Informa?tica
Universidad Complutense de Madrid
Madrid, Spain
raquelhb@fdi.ucm.es
Pablo Gerva?s
Facultad de Informa?tica
Universidad Complutense de Madrid
Madrid, Spain
pgervas@sip.ucm.es
Abstract
Referring Expression Generation (REG) is the
task that deals with references to entities ap-
pearing in a spoken or written discourse. If
these referents are organized in terms of a tax-
onomy, there are two problems when estab-
lishing a reference that would distinguish an
intended referent from its possible distractors.
The first one is the choice of the set of possible
distractrors or contrast set in the given situa-
tion. The second is to identify at what level of
the taxonomy to phrase the reference so that
it unambiguously picks out only the intended
referent, leaving all possible distractors in dif-
ferent branches of the taxonomy. We discuss
the use of ontologies to deal with the REG
task, paying special attention to the choice of
the the contrast set and to the use of the in-
formation of the ontology to select the most
appropriate type to be used for the referent.
1 Introduction
Referring Expression Generation (REG) is the task
that deals with references of entities appearing in a
discourse. In a context where possible referents are
organized in terms of a taxonomy (or subsumption
hierarchy) and may additionally be differentiated by
their attributes, there are two possible ways of estab-
lishing a reference that will distinguish an intended
referent from its possible distractors.
One is to identify at what level of the taxonomy to
phrase the reference so that it unambiguously picks
out only the intended referent, leaving all possible
distractors in different branches of the taxonomy.
Another, applied once a particular level of reference
has been chosen, is to resort to mentioning addi-
tional attributes of the intended referents that distin-
guish it from any remaining distractors that share the
same branch of the taxonomy.
While the second task has been addressed often
in existing literature, the first one is often glossed
over by requiring that the levels to be used for each
element come specified in the input. However, if
this task is to be considered as a specific problem
to be solved computationally, it opens up an addi-
tional problem. If the elements in the universe are
classified in a taxonomy with a single root and the
reference was established at a high enough level in
the taxonomy, potentially everything in the universe
could be a distractor for any other element.
In this paper we will discuss the use of ontolo-
gies to deal with the referring expression generation
task. We will pay special attention to the choice of
the contrast set and the use of ontology information
to select the most appropriate type to be used for the
referent. This work has been centered in the gener-
ation of definite noun phrases where the type of an
element and a set of its properties are given to dis-
tinguish it from the other elements in focus. We are
also supposing that the situations in which the ref-
erence is produced are static, that is, the addressee?s
perception of the world does not change during the
process of reference generation.
2 Related Work
The appropriate use of referring expressions to com-
pete with human-generated texts involves a certain
difficulty. According to Reiter and Dale (2000), a re-
ferring expression must communicate enough infor-
161
mation to identify univocally the intended referent
within the context of the current discourse, but al-
ways avoiding unnecessary or redundant modifiers.
Reiter and Dale (1992) describe a fast algorithm
for generating referring expressions in the context
of a natural language generation system. Their al-
gorithm relies on the following set of assumptions
about the underlying knowledge base that must be
used: (1) every entity is characterized in terms of
a collection of attributes and their values, (2) every
entity has as one of its attributes a type, and (3) the
knowledge base may organize some attribute values
as a subsumption hierarchy. Additionally, each ob-
ject represented in the system should have an associ-
ated basic level value, which corresponds to the con-
cept which is preferred when referring to that object.
These assumptions are satisfied if a description
logic ontology is used for this purpose. Entities
would correspond to instances of concepts from the
ontology, the attribute corresponding to the type
would be the concept of which they are immediate
instances, and the taxonomical structure of the on-
tology of concepts would provide the subsumption
hierarchy. To construct a reference to a particular
entity, the algorithm takes as input a symbol corre-
sponding to the intended referent and a list of sym-
bols corresponding to other entities in focus, known
as the contrast set. The algorithm returns a list of
attribute-value pairs that correspond to the semantic
content of the referring expression.
3 Generating References Using Ontologies
A previously developed ontology about wines has
been used to test the ideas presented in this work.
This is a sample ontology implemented following
a version published by Brachman and colleagues
(Brachman et al, 1991) and distributed along with
the CLASSIC knowledge representation system.
We have focused on the taxonomy of wines pro-
vided by the ontology. Wines are divided in three
main categories: Red Wine, White Wine and
Rose Wine. Inside these main categories there is
a complex taxonomy of different kinds of wines.
In addition, the ontology also contains several in-
stances of the different concepts. Each of these
instances is described using features such as body,
color, flavor, producer, and so on.
The aim is to generate references for different in-
stances of wines which are together in a discourse.
The first step is to select the set of distractors or con-
trast set for the specific referent. Then, an algorithm
for deciding which is the best reference to use is ap-
plied. We have considered as the best reference pos-
sible the use of the type that distinguishes the refer-
ent from the distractors and at the same time is as
general as possible. For example, if we are referring
to an instance of Chardonnaywine (that is a white
one) in a situation where the rest of wines are all
red wines, the most suitable reference is ?the white
wine? and not ?the chardonnay?. On the contrary,
the more specific (but unnecessary) reference might
lead the addressee to infer that this information is
somehow relevant. If only white wines (as direct
type of the referent) are considered for the contrast
set, only the more specific (and inappropriate) refer-
ence may be generated. Therefore, a wide enough
contrast set must be considered in each case.
Finally, if the type chosen is not enough to dis-
tinguish the referent from the contrast set, attribute
selection is applied to select a subset of the element
properties that distinguish it.
3.1 Composing the Contrast Set
Information about type is generally used to deter-
mine which elements of the world must be consid-
ered in the contrast set. In this work, all the informa-
tion about the world is located in an ontology. Each
instance of the world contained in it has a direct type
(the most specific concept it belongs to) and a set of
undirect types that are all the types between the di-
rect type and the root of the ontology.
In the work developed we have used the whole
ontology as contrast set. We have considered it as
the most suitable option for most situations where
the elements involved can belong to quite different
types. As we will see later, this choice avoids the
use of references more specific than desired while at
the same time it allows the algorithm to choose the
type that is more suitable in a given situation.
3.2 An Appropriate Type for the Referent
Our approach takes as initial distinguishing attribute
the type of the elements appearing in the world. This
kind of solution is enough when the types defined for
each of the entities of the world are fixed and there is
162
not a close relation for different types. For example,
a solution that takes as type the strict one defined
in an element would not consider a doberman and a
chihuahua as being both of them dogs.
The algorithm we have implemented can be seen
in Figure 1. Here, r is the intended referent, C is
the contrast set, A is the list of attributes that the in-
stances of the ontology hold, typeValue is the type
that would be assigned to the referent by the algo-
rithm, and L is the list of attribute-value pairs re-
turned if the type is not enough to rule out all the
distractors. The rules-out function works as the
one used in the Incremental algorithm, and the func-
tion incremental-algorithm calls directly to
the original algorithm by Reiter and Dale.
The function find-best-value-type is the
one that delivers the most appropriate type for the
intended referent r taking into account the informa-
tion in the ontology. We have considered as basic
level value for the type the most specific of the com-
mon types of the instances of the ontology. From
this basic level type, the branch of concepts between
it and the direct type of the intended referent r is vis-
ited. The type that will be used in the reference is the
most general concept from this branch that discards
a bigger number of distractors.
3.3 Attribute Selection for Reference
Completion
In some cases the type would not be enough to dis-
tinguish a referent from the other elements of the
world. This situation is produced when they belong
to the same type. In this situation it will be necessary
to use their properties to distinguish between them.
The attribute selection carried out in the Incremental
algorithm from Reiter and Dale has been applied to
these situations.
4 Some Examples
We have tested the implemented algorithm over dif-
ferent situations in which a set of wines is presented.
For each of them, a distinguishing description is pro-
vided using the appropriate type found using the on-
tology and a set of attributes when they were re-
quired. The instances of the world we have con-
sidered are shown in Table 1 (the properties of the
wines that have not been used by the algorithm are
Figure 1: The Algorithm
not shown). The references generated for each of
the referents are (numbers correspond to examples
in the table):
1. ?The Riesling?. There is another white wine
but not a Dry Riesling one, so the most
general type discarding all the distractors is
Riesling.
2. ?The moderate Cabernet Sauvignon?. Here
the type is not enough to distinguish this
referent, so its attributes are used. The
property that distinguish it from the other
Cabernet Sauvignon is the flavor.
3. ?The strong Cabernet Sauvignon?. As in
the previous case the strong flavor is used
to distinguish the wine from the other
Cabernet Sauvignon.
4. ?The Rose Wine?. In this case there are no
more rose wines, so this generic type is enough
to distinguish the referent.
163
Table 1: Examples
5 Conclusions and Future Work
The main advantage of this approach is that the al-
gorithm always finds the most suitable value for the
type, taking into account the other entities of the
world. Since this solution is completely generic and
domain-independent, the algorithm would work in
the same way with more general ontologies. For
example, if the considered ontology contains infor-
mation not only about wines, but also about other
kinds of drinks, the values to be used as types of
the referents would also be chosen in the same way.
In this situation the referent could be the only wine
among other drinks, and the reference generated for
it would be the most appropriate one: ?the wine?.
In the Incremental algorithm, Reiter and Dale do
not address the question of how the contrast set is
constructed, stating that the contrast set is one of the
inputs of their algorithm. In our work, we have cho-
sen as contrast set al the instances that can be found
in the ontology. This solution allows the algorithm
to work with enough information to choose exactly
at which level of the ontology the discourse is being
displayed (more general or more specific). With this
information the generated references are adapted to
the level of specificity required in each case.
The Incremental algorithm also states that the ba-
sic level value is obtained from the knowledge base
or the user model. In this paper we have imple-
mented a dynamic way to obtain this value that only
depends on the knowledge available about the world.
However, the use of some kind of user model repre-
senting the expertise level of the addressee in a spe-
cific domain could be explored in the future.
Acknowledgments
This research is funded by the Spanish Ministry
of Education and Science (TIN2006-14433-C02-01
project) and the UCM and the Direccio?n General de
Universidades e Investigacio?n of the CAM (CCG07-
UCM/TIC-2803).
References
Brachman. Ronald J. and McGuiness, Deborah L. and
Patel-Schneider, Peter F. and Resnick, Lori A. 1991.
Living with CLASSIC: when and how to use a KL-
ONE-like language. Principles in Semantic Net-
works: Explorations in the Representation of Knowl-
edge, pages 401?456. Morgan Kaufmann, California.
Reiter, Ehud and Dale, Robert. 1992. A fast algorithm
for the generation of referring expressions. Proc. of
the 14th conference on Computational Linguistics, pp.
232-238. Association for Computational Linguistics.
Reiter, Ehud and Dale, Robert. 2000. Building Natural
Language Generation Systems. Cambridge University
Press.
164
NIL-UCM: Most-Frequent-Value-First Attribute Selection and
Best-Scoring-Choice Realization
Pablo Gerva?s, Raquel Herva?s, Carlos Leo?n
Natural Interaction based on Language (NIL)
Universidad Complutense de Madrid
c/ Profesor Jose? Garc??a Santesmases s/n, 28040 Madrid, Spain
pgervas@sip.ucm.es, raquelhb@fdi.ucm.es, cleon@fdi.ucm.es
1 Introduction
The NIL entry for the challenge has been con-
structed upon the general architecture for develop-
ing Natural Language Generation systems provided
by the TAP project (Gerva?s, 2007). TAP (Text Ar-
ranging Pipeline) is a set of interfaces that define
generic functionality for a pipeline of tasks oriented
toward natural language generation, from an initial
conceptual input to surface realization as a string,
with intervening stages of content planning and sen-
tence planning.
The TAP architecture considers three basic stages:
content planning, sentence planning and surface re-
alization. Of these, the first stage is not relevant to
the challenge tasks. The configuration choices ap-
plied to the other two stages to adapt them to the
challenge tasks are described below.
2 NIL-UCM-MFVF Entry for Task 1
The NIL-UCM-MFVF for Task 1 applies a Most-
Frequent-Value-First method for Attribute Selec-
tion. Of the five evaluation dimensions considered
in this challenge (Dice, MASI, accuracy, minimal-
ity and uniqueness), this method has been designed
to address explicitly only three: Dice, MASI and
uniqueness. Minimality was abandoned in view of
results in previous challenges (Herva?s and Gerva?s,
2007) that showed good minimality results tended to
produce low Dice scores. We have also opted for not
using accuracy evaluation to fit the performance of
our system, since the corpus contains a wide range
of style of reference and we are interested in pro-
viding our system with only a subset of these that
ensure correct identification.
2.1 Most-Frequent-Value-First Attribute
Selection
The selection algorithm employed is an adapta-
tion of the algorithm described in (Reiter and Dale,
1992). The original algorithm has been modified to
allow for a dynamically changing list of preferred
attributes, which determine the particular order in
which attributes are considered to generate the dis-
tinguishing expression. This list is constructed dy-
namically for each reference by computing the prob-
ability of occurrence in the corpus of the particu-
lar attribute-value pairs associated with the referent,
and using those probabilities to rank them into a spe-
cific list of preferred attributes. The idea is that at-
tributes should be considered in a particular order
depending highly on their values. For example, in
the people domain we have observed that almost
the 100% of the target entities that have beard (at-
tribute has value 1) are referred using the attribute
hasBeard, but when this attribute has value 0 it is
never used. For the hasHair attribute, the opposite
seems to be the case (mentioned only when lacking).
The training data was studied to obtain the prob-
ability of occurrence of an attribute given a certain
value for it. This probability was calculated using
Formula 1:
probvali =
?
appsV alueInAttSet
?
appsV alueInTarget (1)
For each possible value of each of the attributes
of the domains, the sum of the appearances of this
value in the ATTRIBUTE-SET elements (appsVal-
ueInAttSet) and the sum of the appearances of this
value in the attributes of all targets (appsValueInTar-
get) are calculated. The division of these two values
is the probability of mentioning an attribute when it
has a specific value.
215
Dice MASI Accuracy Uniqueness Minimality
Train. Furniture 79,18% 56,95% 41,69% 100% 0%
People 69,71% 42,41% 22,99% 100% 0%
Both 74,80% 50,23% 34,81% 100% 0%
Dev. Furniture 77,55% 53,97% 41,25% 100% 0%
People 70,86% 42,59% 22,06% 100% 0%
Both 74,48% 48,75% 32,43% 100% 0%
Table 1:Task 1 results for training and development data
Some examples of the results obtained are that the
attribute hasGlasses is mentioned in the 60% of
the situations when its value is 1, and in the 0% of
the situations when its value is 0. On the contrary,
the attribute hasShirt is almost never mentioned
(0.8% when its value is 1 and 0% with value 0).
The only exception in the algorithm is the type
attribute for the people domain. As every entity in
this domain is of type person, the attribute selector
does not choose this attribute because no distractor
is discarded by it. However, the experiments have
shown us that in the corpus a lot of descriptions in-
clude the type person even when it is redundant.
Following this idea, our algorithm always includes
the type in the list of chosen attributes for the peo-
ple domain.1
2.2 Obtained Results
Results obtained over the training and development
data are shown in Table 1. As can be seen com-
paring both tables there are no surprises in the final
results: the system gets similar results with both do-
mains and with both the training and development
data. These results confirm that the probability of
appearance of an attribute depending on its value is
more or less the same in the whole corpus.
3 NIL-UCM-BSC Entry for Task 2
The NIL-UCM-BSC for Task 2 applies a Best-
Scoring-Choice approach to Realization.
The realization tasks of the 2008 GRE challenge
required specific instantiations of the Referring Ex-
1We have only recently discovered that the surprising differ-
ence between NIL-UCM results for the people and the furniture
domains in the 2007 GRE challenge was the mostly due to our
not having taken this issue into account at the time. The effect
is noticeable only when the type attribute is redundant, as it is
in the people domain.
pression Generation, Syntactic Choice, and Lexical-
ization stages of the Sentence Planning module of
TAP, and it draws on the SurReal (Gerva?s, 2006) sur-
face realization module. SurReal provides a Java im-
plementation of the surface realization mechanisms
of FUF described in Elhadad (Elhadad, 1993), op-
erating over a grammar which follows the notational
conventions of the SURGE grammar in Elhadad (El-
hadad and Robin, 1996), but it is not systemic in na-
ture. It currently has much smaller coverage than the
original, but quite sufficient to deal with the kind of
realizations required for the challenge tasks.
3.1 Realization Choices in the Corpus
An analysis of the domain was carried out to ascer-
tain what the various alternatives required for real-
ization were for the given corpus, both in terms of
how to realize syntactically the different concepts
and what alternative lexicalizations should be con-
sidered. With respect to linguistic variation in the
form of expression we have distinguished between
choices that give rise to different syntactic struc-
tures (which we consider as syntactic choices) and
choices which give rise to the same syntactic struc-
tures but with different lexical items (which we con-
sider as lexical choices).
With respect to the Referring Expression Genera-
tion stage, the following issues required specific de-
cisions. The use of determiners is erratic. Some
examples in the corpus use indefinite article, some
use definite articles, and some omit the determin-
ers altogether. The corpus shows many cases where
spatial expressions describing the location of refer-
ents are used, many using different systems of refer-
ence (north-south vs. top-bottom). The use of par-
ticular features of the object in its description, as
in ?the desk with the drawers facing the viewer? or
?the chair with the seat facing away?. Comparison
216
with all or some of the distractors are also used, ei-
ther as adjuncts describing their position relative to
other distractors, as in ?the blue fan next to the green
fan?, or as comparative adjectives used for particu-
lar attributes, as in ?the largest red couch? (and even
combinations of the two as in ?the smaller of the two
blue fans?). Finally, there are samples in the corpus
of use of ellipsis and ungrammatical expressions.
The mention of particular features and the use of
comparison would involve operating on more data
than are generated in task 1, and the current sub-
mission is aimed to interconnection with task 2 for
addressing task 3. The issue of ungrammaticality is
important since it implies that there is an upper limit
to the possible scores that the system may achieve
over the corpus under the circumstances, totally un-
related with the correctness of the generated expres-
sions.
With respect to Syntactic Choice, some attributes
show more than one possible option for syntac-
tic realization. The number of alternatives varies
from color (?grey chair - chair that is gray?), through
beards (?with beard - with the beard - with whiskers
- the bearded man - with a beard - with facial hair?)
to orientation (12 different syntactic alternatives for
expressing orientation: back).
There are slight variations of Lexical Choice over
the corpus, as in ?sofa - couch - settee - loveseat?,
?ventilator - fan - windmill? or ?man - guy - bloke?
(for nouns) and ?large - big? or ?small - little? (for
adjectives). Because it has a significant impact on
the edit distance measure, it is also important to con-
sider the existence of a large number of misspellings
in the corpus. Finally, there are some conceptual
mismatches in annotation, between the attribute
set and the given realization in some cases (?purple
- blue?, ?black and white - grey?,...).
3.2 Best Scoring Choice Solution
The solution employed in the present submission
for selecting among the features described above
implements straight forward realization rather than
choice, in the sense in which (Cahill, 1998) uses the
terms for lexicalization. To implement real choice
the system would have to consider more than one al-
ternative for a specific feature and to select one of
them based on some criteria. This has not been done
in the present submission. Instead, a single alterna-
tive has been implemented for each feature, using
it consistently across all samples. The selection of
which particular alternative to implement has been
done empirically to ensure the best possible score
over the training corpus.
3.3 Results and Discussion
Results obtained over the training and development
data are shown in Table 2.
SE distance Accuracy
Train. Furniture 4,26 14,15%
People 5,43 9,12%
Both 4,8 11,82%
Dev. Furniture 4,21 15%
People 4,94 7,35%
Both 4,54 11,48%
Table 2:Task 2 results for training and development data
An important point to consider with respect to
the current submission is whether a solution im-
plementing real choice would have obtained bet-
ter results. Such a solution might have benefited
from the information that can be extracted from the
ANNOTATED-WORD-STRING to train a decision
procedure on the various features. This has not been
addressed in the present submission more for lack of
time than lack of conviction on its merit.
Addressing explicitly some of the possible con-
structions that are described in section 3.1 may also
have a positive effect on the results.
4 NIL-UCM-FVBS Entry for Task 3
The NIL-UCM-FVBS entry for Task 3 applies
a combination of the Most-Frequent-Value-First
method for Attribute Selection and the Best-
Scoring-Choice approach to Realization.
The modular architecture of TAP has allowed
easy integration for Task 3 of the solution for at-
tribute selection described in section 2, and the so-
lution for realization described in section 3.
4.1 Results and Discussion
Results obtained over the training and development
data are shown in Table 3. Comparing both sets of
results there are no surprises in the final results: the
system gets similar results with both domains and
217
with both the training and development data. These
results confirm that the probability of appearance of
an attribute depending on its value is more or less
the same in the whole corpus.
SE distance Accuracy
Train. Furniture 5,03 5,03%
People 6,11 5,47%
Both 5,53 5,24%
Dev. Furniture 5,06 3,75%
People 6,24 1,47%
Both 5,60 2,70%
Table 3:Task 3 results for training and development data
The results obtained are a bit lower than the ones
obtained by both the attribute selection and realiza-
tion submodules separately. This is not an unex-
pected result. Bad choices produced in the attribute
selection are propagated through the realization, re-
sulting in accumulated errors in the final evaluation.
However, there are additional shortcomings that
arise from considering the general goal of task 3
as a composition of task 2 over task 1. The re-
duction of the types of expression produced by hu-
man subjects to a set of attributes involves in some
cases a certain loss of information. This is par-
ticularly the case when the human-produced ex-
pressions involve attributes for which additional in-
formation is provided. This can be seen if the
ANNOTATED-WORD-STRING is compared with
the actual attribute set generated for some of the
human-produced expressions. For instance, the cor-
pus contains examples in which the hasBeard at-
tribute has a nested attribute that indicates the beard
is white. Other examples provide color information
on pieces of clothing worn. This information is lost
to the realization stage if the data have to go through
task 1, which reduces the available format to a set of
individual unstructured attributes.
Considering a version of task 3 that allowed full
realization directly from input data as considered for
task 1, with no requirements on the stages of inter-
mediate representation to be employed in the pro-
cess, may result in a richer range of realizations, and
possibly in improved performance with respect to
human evaluation.
In more general terms, it seems that the corpus
does contain adequate data for informing system
performance at the level of sentence planning sub-
tasks such as lexical choice or syntactic choice. Nev-
ertheless, some of the variations in the corpus, such
as the free use of determiners or the flexibility that
subjects exhibit in the way they refer to the images
do introduce a certain ?noise?. Instances of these oc-
cur when human-produced descriptions involve in-
tense forms of ellipsis, and agrammatical ordering
of attributes. Some of these might be reduced if a re-
fined version of the corpus were produced with more
control on the experimental settings, to ensure that
subjects either described the elements as images or
as the things represented in the images, for instance.
Acknowledgments
This research is funded by the Spanish Ministry
of Education and Science (TIN2006-14433-C02-01
project) and the UCM and the Direccio?n General de
Universidades e Investigacio?n of the CAM (CCG07-
UCM/TIC-2803).
References
Cahill, Lyne. 1998. Lexicalisation in applied NLG sys-
tems. Technical Report ITRI-99-04.
Elhadad, Michael. 1993. Technical Report CUCS-038-
91. Columbia University.
Elhadad, Michael and Robin, Jacques. 1996. Technical
Report 96-03. Department of Computer Science, Ben
Gurion University.
Gerva?s, Pablo. 2006. SurReal: a Surface Realiza-
tion module. Natural Interaction based on Language
Group Technical Report, Universidad Complutense de
Madrid, Spain.
Gerva?s, Pablo. 2007. TAP: a Text Arranging Pipeline.
Natural Interaction based on Language Group Tech-
nical Report, Universidad Complutense de Madrid,
Spain.
Herva?s, Raquel and Gerva?s, Pablo. 2007. NIL: Attribute
Selection for Matching the Task Corpus Using Rela-
tive Attribute Groupings Obtained from the Test Data.
First NLG Challenge on Attribute Selection for Gener-
ating Referring Expressions (ASGRE), UCNLG+MT
Workshop, Machine Translation Summit XI, Copen-
hagen.
Reiter, Ehud and Dale, Robert. 1992. A fast algorithm
for the generation of referring expressions. Proceed-
ings of the 14th conference on Computational Linguis-
tics, pp. 232-238. Association for Computational Lin-
guistics.
218
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 53?56
Manchester, August 2008
Concept-graph based Biomedical Automatic Summarization using
Ontologies
Laura Plaza Morales
Alberto D??az Esteban
Pablo Gerv
?
as
Universidad Complutense de Madrid
C/Profesor Jos?e Garc??a Santesmases, s/n, Madrid 28040, Spain
lplazam@pas.ucm.es, albertodiaz@fdi.ucm.es,pgervas@sip.ucm.es
Abstract
One of the main problems in research on
automatic summarization is the inaccu-
rate semantic interpretation of the source.
Using specific domain knowledge can con-
siderably alleviate the problem. In this pa-
per, we introduce an ontology-based ex-
tractive method for summarization. It is
based on mapping the text to concepts
and representing the document and its sen-
tences as graphs. We have applied our
approach to summarize biomedical litera-
ture, taking advantages of free resources as
UMLS. Preliminary empirical results are
presented and pending problems are iden-
tified.
1 Introduction
In recent years, the amount of electronic biomedi-
cal literature has increased explosively. Physicians
and researchers constantly have to consult up-to
date information according to their needs, but the
process is time-consuming. In order to tackle this
overload of information, text summarization can
undoubtedly play a role.
Simultaneously, a big deal of resources, such
as biomedical terminologies and ontologies, have
emerged. They can significantly benefit the deve-
lopment of NLP systems, and in particular, when
used in automatic summarization, they can in-
crease the quality of summaries.
In this paper, we present an ontology-based ex-
tractive method for the summarization of biomed-
ical literature, based on mapping the text to con-
cepts in UMLS and representing the document and
its sentences as graphs. To assess the importance
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of the sentences, we compute the centrality of their
concepts in the document graph.
2 Previous Work
Traditionally, automatic summarization methods
have been classified in those which generate ex-
tracts and those which generate abstracts. Al-
though human summaries are typically abstracts,
most of existing systems produce extracts.
Extractive methods build summaries on a super-
ficial analysis of the source. Early summariza-
tion systems are based on simple heuristic fea-
tures, as the position of sentences in the docu-
ment (Brandow et al, 1995), the frequency of
the words they contain (Luhn, 1958; Edmundson,
1969), or the presence of certain cue words or in-
dicative phrases (Edmundson, 1969). Some ad-
vanced approaches also employ machine learning
techniques to determine the best set of attributes
for extraction (Kupiec et al, 1995). Recently,
several graph-based methods have been proposed
to rank sentences for extraction. LexRank (Erkan
and Radev, 2004) is an example of a centroid-
based method to multi-document summarization
that assess sentence importance based on the con-
cept of eigenvector centrality. It represents the
sentences in each document by its tf*idf vectors
and computes sentence connectivity using the co-
sine similarity. Even if results are promising, most
of these approaches exhibit important deficiencies
which are consequences of not capturing the se-
mantic relations between terms (synonymy, hyper-
onymy, homonymy, and co-occurs and associated-
with relations).
We present an extractive method for summariza-
tion which attempts to solve this deficiencies. Un-
like researches conducted by (Yoo et al, 2007;
Erkan and Radev, 2004), which cluster sentences
to identify shared topics in multiple documents, in
this work we apply clustering to identify groups
53
of concepts closely related. We hypothesize that
each cluster represents a theme or topic in the do-
cument, and we evaluate three different heuristics
to ranking sentences.
3 Biomedical Ontologies. UMLS
Biomedical ontologies organize domain concepts
and knowledge in a system of hierarchical and as-
sociative relations. One of the most widespread
in NLP applications is UMLS
1
(Unified Medi-
cal Language System). UMLS consists of three
components: the Metathesaurus, a collection of
concepts and terms from various vocabularies and
their relationships; the Semantic Network, a set of
categories and relations used to classify and relate
the entries in the Metathesaurus; and the Special-
ist Lexicon, a database of lexicographic informa-
tion for use in NLP. In this work, we have se-
lected UMLS for several reasons. First, it pro-
vides a mapping structure between different ter-
minologies, including MeSH or SNOMED, and
thus allows to translate between them. Secondly, it
contains vocabularies in various languages, which
allows to process multilingual information.
4 Summarization Method
The method proposed consists of three steps. Each
step is discussed in detail below. A preliminary
system has been implemented and tested on several
documents from the corpus developed by BioMed
Central
2
.
As the preprocessing, text is split into sentences
using GATE
3
, and generic words and high fre-
quency terms are removed, as they are not useful
in discriminating between relevant and irrelevant
sentences.
4.1 Graph-based Document Representation
This step consists in representing each document
as a graph, where the vertices are the concepts in
UMLS associated to the terms, and the edges indi-
cate the relations between them. Firstly, each sen-
tence is mapped to the UMLSMetathesaurus using
MetaMap (Aronson, 2001). MetaMap allows
to map terms to UMLS concepts, using n-grams
for indexing in the ULMS Metathesaurus, and
performing disambiguation to identify the correct
1
NLM Unified Medical Language System (UMLS). URL:
http://www.nlm.nih.gov/research/umls
2
BioMed Central: http://www.biomedcentral.com/
3
GATE (Generic Architecture for Text Engineering):
http://gate.ac.uk/
concept for a term. Secondly, the UMLS concepts
are extended with their hyperonyms. Figure 1
shows the graph for sentence ?The goal of the trial
was to assess cardiovascular mortality and mor-
bidity for stroke, coronary heart disease and con-
gestive heart failure, as an evidence-based guide
for clinicians who treat hypertension.?
Next, each edge is assigned a weight, which is
directly proportional to the deep in the hierarchy at
which the concepts lies (Figure 1). That is to say,
the more specific the concepts connected are, the
more weight is assigned to them. Expression (1)
shows how these values are computed.
|? ? ?|
|? ? ?|
=
|?|
|?|
(1)
where ? is the set of all the parents of a con-
cept, including the concept, and ? is the set of all
the parents of its immediate higher-level concept,
including the concept.
Finally, the sentence graphs are merged into
a document graph, enriched with the associated-
with relations between the semantic types in
UMLS corresponding to the concepts (Figure 1).
Weights for the new edges are computed using ex-
pression (1).
4.2 Concept Clustering and Theme
Recognition
The second step consists of clustering concepts in
the document graph, using a degree-based method
(Erkan and Radev, 2004). Each cluster is com-
posed by a set of concepts that are closely related
in meaning, and can be seen as a theme in the do-
cument. The most central concepts in the cluster
give the sufficient and necessary information re-
lated to its theme. We hypothesize that the docu-
ment graph is an instance of a scale-free network
(Barabasi, 1999). Following (Yoo et al, 2007),
we introduce the salience of vertices. Mathemati-
cally, the salience of a vertex (v
i
) is calculated as
follows.
salience(v
i
) =
?
e
j
|?v
k
?e
j
conecta(v
i
,v
k
)
weight(e
j
)
(2)
Within the set of vertices, we select the n
that present the higher salience and iteratively
group them in Hub Vertex Sets (HVS). A HVS
represents a group of vertices that are strongly
related to each other. The remaining vertices are
54
Figure 1: Sentence graph
assigned to that cluster to which they are more
connected.
Finally, we assign each sentence to a cluster. To
measure the similarity between a cluster and a sen-
tence graph, we use a vote mechanism (Yoo et al,
2007). Each vertex (v
k
) of a sentence (O
j
) gives to
each cluster (C
i
) a different number of votes (p
i,j
)
depending on whether the vertex belongs to HVS
or non-HVS (3).
similarity(C
i
, O
j
) =
?
v
k
|v
k
?O
j
w
k,j
(3)
where
{
w
k,j
=0 si v
k
6?C
i
w
k,j
=1.0,si v
k
?HV S(C
i
)
w
k,j
=0.5,si v
k
6?HV S(C
i
)
4.3 Sentence Selection
The last step consists of selecting significant sen-
tences for the summary, based on the similarity
between sentences and clusters. We investigated
three alternatives for this step.
? Heuristic 1: For each cluster, the top n
i
sen-
tences are selected, where n
i
is proportional
to its size.
? Heuristic 2: We accept the hypothesis that
the cluster with more concepts represents the
main theme in the document, and select the
top N sentences from this cluster.
? Heuristic 3: We compute a single score for
each sentence, as the sum of the votes as-
signed to each cluster adjusted to their sizes,
and select theN sentences with higher scores.
5 Results and Evaluation
In order to evaluate the method, we analyze the
summaries generated by the three heuristics over
a document
4
from the BioMed Central Corpus,
using a compression rate of 20%. Table 1 shows
the sentences selected along with their scores.
Although results are not statistically significant,
they show some aspects in which our method be-
haves satisfactorily. Heuristics 1 and 3 extract sen-
tence 0, and assign to it the higher score. This
supports the positional criterion of selecting the
first sentence in the document, as the one that con-
tains the most significant information. Sentence 58
represents an example of sentence, situated at the
end, which gathers the conclusions of the author.
In general, these sentences are highly informative.
Sentence 19, in turn, evidences how the method
systematically gives preference to long sentences.
Moreover, while summaries by heuristics 1 and 3
have a lot of sentences in common (9 out of 12),
heuristic 2 generates a summary considerably dif-
ferent and ignores important topics in the docu-
ment. Finally, we have compared these summaries
with the author?s abstract. It can be observed that
heuristics 1 and 3 cover all topics in the author?s
abstract (see sentences 0, 4, 15, 17, 19, 20 and 25).
4
BioMed Central: www.biomedcentral.com/content/
download/xml/cvm-2-6-254.xml
55
Sentences 0 4 19 58 7 28 25 20 21 8 43 15
Heuristic 1 99.0 20.0 19.0 18.5 17.0 16.5 16.0 15.5 15.5 13.5 13.5 12.0
Heuristic 2 19.0 16.5 15.5 12.5 12.0 10.5 9.0 9.0 7.5 7.0 7.0 7.0
Heuristic 3 98.8 18.7 17.9 16.3 15.3 14.5 13.4 13.0 13.0 12.7 12.7 12.2
Table 1: Results
As far as heuristic 2 is concerned, it does not cover
adequately the information in the abstract.
6 Conclusions and Future Work
In this paper we introduce a method for summa-
rizing biomedical literature. We represent the do-
cument as an ontology-enriched scale-free graph,
using UMLS concepts and relations. This way we
get a richer representation than the one provided by
a vector space model. In section 5 we have evalu-
ated several heuristics for sentence extraction. We
have determined that heuristic 2 does not cover all
relevant topics and selects sentences with a low rel-
ative significance. Conversely, heuristics 1 and 3,
present very similar results and cover all important
topics.
Nonetheless, we have identified several prob-
lems and some possible improvements. Firstly, as
our method extracts whole sentences, long ones
have higher probability to be selected, because
they contain more concepts. The alternative could
be to normalise the sentences scores by the number
of concepts. Secondly, concepts associated with
general semantic types in UMLS, as functional
concept, temporal concept, entity and language,
could be ignored, since they do not contribute to
distinguish what sentences are significant.
Finally, in order to formally evaluate the method
and the different heuristics, a large-scale evalua-
tion on the BioMed Corpus is under way, based on
computing the ROUGE measures (Lin, 2004).
Acknowledgements
This research is funded by the Ministerio de Edu-
caci?on y Ciencia (TIN2006-14433-C02-01), Uni-
versidad Complutense de Madrid and Direcci?on
General de Universidades e Investigaci?on de la Co-
munidad de Madrid (CCG07-UCM/TIC 2803).
References
Aronson A. R. Effective Mapping of Biomedical Text
to the UMLS Metathesaurus: The MetaMap Pro-
gram. 2001. In Proceedings of American Medical
Informatics Association.
Barabasi A.L. and Albert R. Emergence of scaling in
random networks. 1999. Science,286?509.
Brandow R. and Mitze K. and Rau L. F. Automatic
Condensation of Electronic Publications by Sen-
tence Selection. 1995. Information Processing and
Management,5(31):675?685.
Edmundson H.P. New Methods in Automatic Extract-
ing. 1969. Journal of the Association for Computing
Machinery,2(16):264?285.
Erkan G. and Radev D. R. LexRank: Graph-based
Lexical Centrality as Salience in Text Summariza-
tion. 2004. Journal of Artificial Intelligence Re-
search (JAIR),22:457?479.
Kupiec J. and Pedersen J.O. and Chen F. A Trainable
Document Summarizer. 1995. In Proceedings of
the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval,68?73.
Lin C-Y. ROUGE: A Package for Automatic Eval-
uation of Summaries. 2004. In Proceedings of
Workshop on Text Summarization Branches Out,
Post-Conference Workshop of ACL 2004, Barcelona,
Spain.
Luhn H.P. The Automatic Creation of Literature
Abstracts. 1958. IBM Journal of Research
Development,2(2):159?165.
Sparck-Jones K. Automatic Summarizing: Factors and
Directions. 1999. I. Mani y M.T. Maybury, Advances
in Automatic Text Summarization. The MIT Press.
Yoo I. and Hu X. and Song I.Y. A coherent graph-based
semantic clustering and summarization approach for
biomedical literature and a new summarization eval-
uation method. 2007. BMC Bioinformatics,8(9).
56
Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 23?30,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Engineering Linguistic Creativity: Bird Flight and Jet Planes
Pablo Gerva?s
Universidad Complutense de Madrid
c/ Profesor Garc??a Santesmases s/n
Madrid, 28040, Spain
pgervas@sip.ucm.es
Abstract
Man achieved flight by studying how birds fly,
and yet the solution that engineers came up
with (jet planes) is very different from the one
birds apply. In this paper I review a number of
efforts in automated story telling and poetry
generation, identifying which human abilities
are being modelled in each case. In an anal-
ogy to the classic example of bird-flight and
jet planes, I explore how the computational
models relate to (the little we know about) hu-
man performance, what the similarities are be-
tween the case for linguistic creativity and the
case for flight, and what the analogy might
have to say about artificial linguistic creativ-
ity if it were valid.
1 Introduction
The achievement of flight by man is often used as
an example of how engineering practice may lead
to the successful emulation of behaviours observed
in nature. It is also used to illustrate the idea that a
successful engineering solution (such as a jet plane)
need not always mirror faithfully the natural phe-
nomenon which inspired it (the flight of birds).
The task of engineering solutions for linguistic
creativity is made difficult by an incomplete under-
standing of how we manage language and how we
achieve creativity. Nevertheless, over the past few
years a large research effort has been devoted to ex-
ploring issues such as computational creativity, au-
tomated story telling, or poetry generation. In these
cases, there is also a combination of a naturally oc-
curring source phenomenon and a set of engineering
techniques that provide an emulation of it.
In this paper I review a number of such research
and development efforts that I have been involved
in or studied in detail, paying particular attention to
identifying which traits of human activity are being
modelled in each case. In an analogy to the clas-
sic example of bird-flight and jet planes, I explore
how the computational models of linguistic creativ-
ity relate to (the little we know about) human per-
formance, what the similarities are between the case
for linguistic creativity and the case for flight, and
what the analogy might have to say about artificial
linguistic creativity if it were valid.
2 Creativity at Different Levels of
Linguistic Decision
Creativity is a tricky word because it can mean dif-
ferent things to different people. There seems to be
a historical reason for this, in as much as the actual
word we now use seems to have been invented in
the 19th century in an attempt to cover the differ-
ent concepts of innovation that were accepted in art
and science (Weiner, 2000). As it is very difficult
to make progress without a minimal agreement on
what we are talking about, I will set off with an at-
tempt to clarify what I refer to when I use the word
in what follows. This is not intended to be prescrip-
tive of how it should be used or descriptive of what
other people may mean when they use it. And it is
not meant to be exhaustive.1 The goal here is to pro-
vide a brief sketch for readers to have a general idea
of what is being talked about.
1Interested readers can refer to (Gerva?s, 2009) for a more
detailed discussion of my personal view on creativity.
23
For me creativity suggests the idea of someone
(a creator) generating something (an output) that is
somehow new, but also somewhat unexpected or dif-
ferent from what others might have produced. This
output should satisfy some goal, though in many
cases the particular goal implied is not altogether
clear. The expectation of novelty implicitly brings in
a second agent (an audience which usually involves
more than one individual) that perceives or evaluates
the result.
When used in different contexts, the word cre-
ativity acquires different meanings by virtue of in-
volving different concepts of author, product, goal,
or audience. The assumption that there is a generic
framework common to all the different cases should
be taken with a pinch of salt, as commonalities may
not go far beyond this basic sketch.
It may seem that restricting the study to linguis-
tic creativity simplifies the issue. Surely once the
domain is constrained to linguistic outputs, the de-
scription of creativity should indeed boil down to a
simple common framework. This assumption may
also be risky, as I discuss below.
There are several possible levels of decision at
which the final form of a sentence is shaped. At any
(or all) of these it is possible to exercise creativity in
the sense described above. At the level of phonetics,
the way letters are put together to make sounds can
be explored in search of pleasing uses of rhyme, in-
ternal rhyme or alliteration, as done in sound poetry
(Hultberg, 1993). If one considers rhythm, the stress
patterns of words may shape the stress pattern of a
sentence or a text into rhythms that are uncommon
in the language, or in existing poetry, as Poe claims
to have done in ?The Raven? (Poe, 1846). With re-
spect to lexical choice, the actual words chosen for
the text may be words that the user does not know
but which actually convey a certain meaning to the
reader, as done by Lewis Carrol in the poem ?Jab-
berwocky? (Carrol, 1872).
For other levels of decisions, such as syntax, se-
mantics or narrative, it is more difficult to pinpoint
specific examples of creative uses, because instances
occur in larger contexts and because they occur
much more frequently. They can be considered of
two different kinds: those in which the main ob-
jective is the communication of a certain message
or information, and those geared towards obtaining
a pleasing effect of some sort. The first kind oc-
curs for instance when a speaker in a hurry waives
the rules of correct syntax in his effort to get his
message accross briefly. In going beyond the ac-
cepted rules, such a speaker may be deemed to be
behaving creatively. This type of linguistic creativ-
ity (say, corner-cutting creative communication) is
worth exploring in detail, but it would require ac-
cess to enough samples of specific instances of the
phenomenon to provide starting material. The sec-
ond kind, in contrast, tend to get explicitly recorded
for this pleasing effect to be available at later times,
and they provide an easier starting point for a study
of this sort.
A number of examples of linguistic creativity of
the second kind were reviewed in (Gerva?s, 2002).
This study showed that creative behaviour does not
occur in the same degree across all levels. Creativ-
ity applied simultaneously at several linguistic lev-
els can be counterproductive for communication if
abused. Instead, a conservative approach in some
levels is required for a successful interpretation of
creative innovations at other levels. An additional
problem that would have to be tackled is the extent
to which the interaction between the theories for the
different levels complicates the picture significantly.
Intuition suggests that it will to a considerable ex-
tent. Creativity may operate at each of the levels of
decision involved in linguistic production, but it may
interact between different levels in ways that are not
evident.
Under this light, we can see that even within the
realm of linguistic creativity we seem to be faced
with a broad range of different types of creativity,
with different concepts of product and goal, giving
shape to widely differing phenomena. In the hope
of reducing even further the scope of the problem,
I will concentrate more specifically on instances
where a computer program is written to generate
pieces of text that, when produced by a human au-
thor, would be interpreted to have deliberate aspira-
tions of creativity.
3 Some Automatic Creators in the
Literary Field
An exhaustive study of existing automatic creators
of this kind would take more space than I have avail-
24
able here. The selection below must not be under-
stood to be exhaustive. It is not even intended to
indicate that the particular creators mentioned con-
stitute the most significant efforts in the field. I have
selected only a few for purposes of illustration, and I
have chosen examples where relevant features of the
corresponding human processes have been consid-
ered. There are two main fields where computer pro-
grams attempt to generate literary material: story-
telling programs and poetry generators. Again, a
difference in genre introduces differences in prod-
uct, goal and evaluation criteria, which leads to the
application of different construction processes, so I
will review each field separately.
3.1 Automatic Story Tellers
Research on storytelling systems has experienced
considerable growth over the years. Although it has
never been a popular research topic, nonetheless it
has received sustained attention over the years by a
dedicated community of researchers. In recent years
the number of systems developed has increased sig-
nificantly. The body of work resulting from these ef-
forts has identified a significant number of relevant
issues in storytelling. Successive systems have iden-
tified particular elements in stories that play a role in
the process of generation. Only a few illustrative ex-
amples will be mentioned here.
It is clear that planning has been central to ef-
forts of modelling storytelling for a long time. Most
of the existing storytelling systems feature a plan-
ning component of some kind, whether as a main
module or as an auxiliary one. TALESPIN (Mee-
han, 1977), AUTHOR (Dehn, 1981), UNIVERSE
(Lebowitz, 1983), MINSTREL (Turner, 1993) and
Fabulist (Riedl, 2004), all include some representa-
tion of goals and/or causality, though each of them
uses it differently in the task of generating stories.
An important insight resulting from this work (orig-
inally formulated by (Dehn, 1981) but later taken up
by others) was the distinction between goals of the
characters in the story or goals of the author. This
showed that planning is a highly relevant tool for
storytelling, both at the level of how the coherence
of stories can be represented and how the process of
generating them is related to goals and causality.
Another aspect that is obviously relevant for sto-
rytelling is emotion. This has been less frequently
addressed in automatic storytellers, but has an out-
standing champion in the MEXICA system. MEX-
ICA (Pe?rez y Pe?rez, 1999) was a computer model
designed to study the creative process in writing
in terms of the cycle of engagement and reflec-
tion (Sharples, 1999), which presents a description
of writing understood as a problem-solving process
where the writer is both a creative thinker and a de-
signer of text. MEXICA was designed to generate
short stories about the Mexicas (also wrongly known
as Aztecs), and it is a flexible tool where the user
can set the value of different parameters to constrain
the writing process and explore different aspects of
story generation. An important aspect of MEXICA
is that it takes into account emotional links and ten-
sions between the characters as means for driving
and evaluating ongoing stories. The internal repre-
sentation that MEXICA uses for its stories (a Story
World Context) is built incrementally as a story is
either read or produced (the system can do both,
as it learns its craft from a set of previous stories).
This representation keeps track of emotional links
and emotional tensions between characters. These
elements are represented as preconditions and post-
conditions of the set of available actions. The system
evaluates the quality of a partial draft for a story in
terms of the the rising and falling shape of the arc of
emotional tensions that can be computed from this
information.
In general, most storytelling systems, being AI-
style programs, can be said to operate by searching
a space of solutions, guided by a traversal function
that leads to new points in the space and an evalu-
ation function that rates each point of the space in
terms of quality. In general, most systems concen-
trate on the development and innovation efforts in
the function for generating new stories (the traversal
function), hoping that the candidates generated will
progressively get better. However, human authors
seem to learn their craft mostly by learning to distin-
guish good stories from bad stories (which would in-
volve focusing more on the evaluation function). A
fairly recent proposal (Gerva?s and Leo?n, ) describes
a story generation system that outputs new stories
obtained by exploring a restricted conceptual space
under the guidance of a set of evaluation rules. The
interesting feature in this system is that it uses ex-
haustive enumeration of the search space as its only
25
exploration procedure, and relies solely on its eval-
uation rules to identify good stories. This is a di-
rect application of the generate & test paradigm of
problem solving. This system also models the way
in which the evaluation rules can evolve over time,
leading to the production of new results.
3.2 Automatic Poetry Generators
Automatic poetry generators differ significantly
from storytellers in two aspects: they are expected to
satisfy very specific metric restrictions (in terms of
number of syllables per line, and position of stressed
syllables within the line) on the form of the out-
put text (which story tellers do not usually take into
account), and they are allowed a certain poetic li-
cence which boils down to relaxing, sometimes quite
dramatically, any expectations of meaning or coher-
ence in the output (which are fundamental for story-
tellers). As a result, there is a larger sample of poetry
generators. The review presented below attempts to
cover some of the basic techniques that have been
used as underlying technologies.
The generate & test paradigm of problem solving
has also been widely applied in poetry generators.
Because metric restrictions are reasonably easy to
model computationally, very simple generation so-
lutions coupled with an evaluation function for met-
ric constraints are likely to produce acceptable re-
sults (given an assumption of poetic licence as re-
gards to the content). An example of this approach
is the early version of the WASP system (Gerva?s,
2000). Initial work by Manurung (Manurung, 1999)
also applied a generate & test approach based on
chart generation, but added an important restriction:
that poems to be generated must aim for some spe-
cific semantic content, however vaguely defined at
the start of the composition process. This consti-
tutes a significant restriction on the extent of poetic
licence allowed.
Manurung went on to develop in his Phd thesis
(Manurung, 2003) an evolutionary solution for this
problem. Evolutionary solutions seem particularly
apt to model this process as they bear certain sim-
ilarities with the way human authors may explore
several possible drafts in parallel, progressively edit-
ing them while they are equally valuable, focusing
on one of them when it becomes better valued than
others, but returning to others if later modifications
prove them more interesting.
Another important tactic that human authors are
known to use is that of reusing ideas, structures, or
phrasings from previous work in new results. This is
very similar to the AI technique of Case-Based Rea-
soning (CBR). Some poetry generators have indeed
explored the use of this technique as a basic genera-
tion mechanism. An evolution of the WASP system
(Gerva?s, 2001) used CBR to build verses for an input
sentence by relying on a case base of matched pairs
of prose and verse versions of the same sentence.
Each case was a set of verses associated with a prose
paraphrase of their content. An input sentence was
used to query the case base and the structure of the
verses of the best-matching result was adapted into
a verse rendition of the input. This constituted a dif-
ferent approach to hardening the degree of poetic li-
cence required to deem the outputs acceptable (the
resulting verses should have a certain relation to the
input sentence).
Another important mechanism that has been em-
ployed by automatic poets is grammar-based gen-
eration. By using a grammar to produce gram-
matically correct combinations of words, the re-
sults obtained start to resemble understandable sen-
tences. As Chomsky mentioned in 1957, the fact
that a sentence is grammatically correct does not
imply that it will be interpretable. However, in the
context of automatically generated poetry, sentences
like Chomsky?s classic counterexample (?Colorless
green ideas sleep furiously?) acquire a special inter-
est, as they provide both a sense of validity (due to
their syntactic correctness) and a sense of adventure
(due to the impossibility of pinpointing a specific
meaning for them). On reading such sentences, the
human mind comes up with a number of conflicting
interpretations, none fully compatible with its literal
meaning. This multiplicity of shifting meanings is
very atractive in the light of modern theories about
the role of reader interpretation in the reading pro-
cess.
In 1984 William Chamberlain published a book
of poems called ?The Policeman?s Beard is Half
Constructed? (Chamberlain, 1981). In the preface,
Chamberlain claimed that all the book (but the pref-
ace) had been written by a computer program. The
program, called RACTER, managed verb conjuga-
tion and noun declension, and it could assign cer-
26
tain elements to variables in order to reuse them
periodically (which gave an impression of thematic
continuity). Although few details are provided re-
garding the implementation, it is generally assumed
that RACTER employed grammar-based generation.
The poems in Chamberlain?s book showed a degree
of sophistication that many claim would be impos-
sible to obtain using only grammars, and it has been
suggested that a savvy combination of grammars,
carefully-crafted templates and heavy filtering of a
very large number of results may have been em-
ployed.
The use of n-grams to model the probability of
certain words following on from others has proven
to be another useful tecnique. An example of poetry
generation based on this is the cybernetic poet devel-
oped by Ray Kurtzweil. RKCP (Ray Kurtzweils Cy-
bernetic Poet)2 is trained on a selection of poems by
an author or authors and it creates from them a lan-
guage model of the work of those authors. From this
model, RKCP can produce original poems which
will have a style similar to the author on which they
were trained. The generation process is controlled
by a series of additional parameters, for instance,
the type of stanza employed. RKCP includes an al-
gorithm to avoid generating poems too close to the
originals used during its training, and certain algo-
rithms to maintain thematic coherence over a given
poem. Over specific examples, it could be seen that
the internal coherence of given verses was good, but
coherence within sentences that spanned more than
one verse was not so impressive.
4 Discussion
The selection of automatic creators reviewed above
provides a significant sample of human abilities re-
lated with linguistic creativity that have been mod-
elled with reasonable success. These include: the
ability to recognise causality and use plans as skele-
tons for the backbone of a text, the ability to identify
emotional reactions and evaluate a story in terms of
emotional arcs, the ability to relax restrictions at the
time of building and delay evaluation until fuller re-
sults have been produced, the ability to iterate over
a draft applying successive modifications in search
of a best fit, the ability to measure metric forms, the
2http://www.kurzweilcyberart.com/poetry/rkcp overview.php3
ability to reuse the structures of texts we liked in the
past, the ability to rely on grammars for generating
valid text, and the ability to use n-grams to produce
a stream of text with surface form in a certain style.
This list of abilities is doubtless not exhaustive, but it
covers a broad range of aspects. The important idea
is that although existing systems have identified and
modelled these abilities, very few have considered
more than one or two of them simultaneously. And
yet intuition suggests that human authors are likely
to apply a combination of all of these (and proba-
bly many more additional ones that have not been
modelled yet) even in their simplest efforts.
It may pay to look in more detail at the set of tools
that we have identified, with a view to considering
how they might be put together in a single system
if we felt so inclined. The engagement and reflec-
tion model (Sharples, 1999) may provide a useful
framework for this purpose. Sharples? concept of
engagement seems to correspond with the ability to
generate a new instance of a given artefact, without
excessive concern to the quality or fitness for pur-
pose of the partial result at any intermediate stage of
the process. According to this view, planners, case-
based reasoning, grammars, or n-gram models can
provide reasonable implementations of procedures
for engagement. The concept of reflection captures
the need to evaluate the material generated during
engagement. Abilities like identifying emotional re-
actions and evaluating a story in terms of emotional
arcs, or measuring metric forms would clearly have
a role to play during reflection. However, it is im-
portant to consider that we are looking at a number
of possible mechanisms for use in engagement, to-
gether with a number of possible mechanisms for
use in reflection. This does indeed have a place in
the general scheme proposed by Sharples. Sharples
proposes a cyclic process moving through two dif-
ferent phases: engagement and reflection. During
the reflection phase, the generated material is revised
in a three step process of reviewing, contemplating
and planning the result. During reviewing the re-
sult is read, minor edits may be carried out, but most
important it is interpreted to represent ?the proce-
dures enacted during composition as explicit knowl-
edge?. Contemplation involves the process of oper-
ating on the results of this interpretation. Planning
uses the results of contemplation to create plans or
27
intentions to guide the next phase of engagement.
The evidence that we have presented so far suggests
that a specific mechanism (or maybe more than one)
may have been chosen to be used during a partic-
ular cycle of engagement. The process of review-
ing mentioned by Sharples might simply be one of
explicitly considering the choice of mechanism to
use in engagement. The process of contemplating
might be an application of the full set of evaluation
mechanisms particular to reflection. The process of
planning could be a complex process which would
include among other things a decision of whether
to change the engagement mechanism in use (or the
configuration of any parameters it may need), and
which mechanism to apply in each situation.
But we should not only study how closely auto-
matic creators resemble human ones, assuming any
divergence is a negative factor. Particular attention
must be paid to the question of whether certain char-
acteristics of human creativity are necessary condi-
tions for creativity or simply the ingenious solution
that makes it possible for the human mind while re-
maining within its limitations. This is particularly
important if one is to consider modelling creativity
in computer systems, which have different limita-
tions, but also different advantages.
Humans have limited memory. Many of the so-
lutions they seem to apply (such as providing con-
straints over a generative system so that it generates
only ?appropriate? solutions) are intended to avoid
problems arising from the large amount of memory
that would be required to consider all possible solu-
tions provided by the generative system. But com-
puters do not usually have the same problem. Com-
puters can store and consider a much large num-
ber of solutions. This has in the past been the big
advantage presented by computers over people. It
is such a significant advantage that, for some tasks
such as chess playing, computers can perform bet-
ter by computing all options and evaluating them
all (very fast) than people can by using intelligent
heuristics.
Though little definite is known about how the
brain works, it seems to follow a highly parallel
approach to computation. This is not true of most
modern day computers. The most widely extended
model for modern computers is the Von Neumann
architecture, a computer design model that uses a
single processing unit and a single separate storage
structure to hold both instructions and data. Over
this simple model, subsequent layers of abstraction
may be built, resulting in very complex models of
performance as perceived by a human user running
the computer. Many of these complex behaviours
(such as, for instance, evolutionary problem solv-
ing techniques) have often been considered prime
candidates for simulating creative behaviour in com-
puters on the grounds that they implement a par-
allel search method, but they are reckoned to be
slow, taking a long time to produce results. The
lack of speed is highy influenced by the fact that,
when run on computers with a Von Neumann ar-
chitecture, each possible solution must be built and
evaluated sequentially by the underlying single pro-
cessing unit. If any computational solution based
on parallel search methods shows merit for emulat-
ing creativity, it should not be discounted until it has
been tested over hardware that allows it to operate
in a really parallel manner, and instances of these
are becoming more and more popular. Nowadays it
has become more difficult to buy a new computer
without finding it has at least two cores. For gam-
ing consoles, this trend has gone even further, with
the new generations sporting up to nine processing
units.
5 Our Latest Efforts
Although the aim of the paper is not to report orig-
inal work, a brief description of my ongoing work
constitutes an example of the type of system that can
be considered along the lines described above. The
WASP poetry generator is still going strong. Only
recently a selection of 10 poems produced by WASP
has been published in a book about the possibilities
of computers writing love poems (Gerva?s, 2010).
The version of WASP used here is more advanced
than previous ones, and some of the ideas outlined
in the discussion have been introduced as modifica-
tions on earlier designs.
This version of WASP operates as a set of fami-
lies of automatic experts: one family of content gen-
erators (which generate a flow of text that is taken
as a starting point by the poets), one family of po-
ets (which try to convert flows of text into poems in
given strophic forms), one family of judges (which
28
evaluate different aspects that are considered impor-
tant), and one family of revisers (which apply modi-
fications to the drafts they receive, each one oriented
to correct a type of problem, or to modify the draft
in a specific way). These families work in a coor-
dinated manner like a cooperative society of read-
ers/critics/editors/writers. All together they generate
a population of drafts over which they all operate,
modifying it and pruning it in an evolutionary man-
ner over a pre-established number of generations of
drafts, until a final version, the best valued effort of
the lot, is chosen.
The overall style of the resulting poems is
strongly determined by the accumulated sources
used to train the content generators, which are
mostly n-gram based. The poems presented in
the book were produced with content generators
trained on collections of texts by Federico Garc??a
Lorca, Miguel Herna?ndez and a selection of Six-
teenth Century Spanish poets. Readers familiar with
the sources can detect similarities in vocabulary,
syntax and theme. A specific judge is in charge of
penalising instances of excessive similarity with the
sources, which then get pushed down in the ranking
and tend not to emerge as final solutions.
The various judges assign scores on specific pa-
rameters (on poem length, on verse length, on
rhyme, on stress patterns of each line, on similar-
ity to the sources, fitness against particular strophic
forms...) and an overall score for each draft is ob-
tained by combining all individual scores received
by the draft.
Poets operate mainly by deciding on the introduc-
tion of line breaks over the text they receive as input.
Revisers rely on scores assigned by judges to in-
troduce changes to drafts. Modifications can be of
several types: deletion of spans of text, substitution
of spans for newly generated ones, word substitu-
tion, sentence elimination, and simple cross-over of
fragments of poems to obtain new ones.
Because an initial draft produced by an n-gram
based content generator is then processed many
times over by poets and revisers, final results oscil-
late between surprising faithfulness to the sources
and very radical surreal compositions.
6 Conclusions
In view of the material presented, and taking up the
analogy between linguistic creativity and bird flight,
we can say we are still trying to model birds. So
far, we have only achieved small models of parts of
birds. The various features of automatic creators that
have been vaguely related to human abilities in sec-
tion 4 are clearly tools that human writers apply in
their daily task. Having systems that model these
techniques, and testing how far each technique goes
towards modelling human activity are steps forward.
Bird?s wings or bird?s feathers do not fly, but hav-
ing good models of them is crucial to understanding
what makes flight possible.
Yet humans do not apply any of them in isola-
tion, but rather rely on them as a set of tools and
combine them at need to produce new material, us-
ing different combinations at different times. In the
same way as research into flight considered how the
parts of birds interact to achieve flight, in the realm
of linguistic creativity more effort should be made to
model the way in which humans combine different
techniques and tools to achieve results. This could
not have been done a few years back for lack of a
valid set of tools to start from, but it is feasible now.
Aside from this positive optimistic analysis, there
is a darker thought to keep in mind. Because we
recognise that the models we are building at the
current stage are only reproductions of parts of the
whole mechanism, it would be unrealistic to demand
of them that they exhibit right now creativity at the
level of humans. As long as they focus on one as-
pect and leave out others, they are likely to perform
poorly in the overall task when compared with their
human couterparts. Yet even if they do not they are
still worthy pursuits as initial and fundamental steps
on which to build better solutions.
Once the various elements that contribute to the
task have been identified and modelled with reason-
able success, and the way in which they interact
when humans apply them, a new universe of pos-
sibilities opens up. Future research should address
the way in which humans apply these various ele-
ments, especially the way in which they combine
some with others to achieve better results. In do-
ing this, researchers should inform themselves with
existing research on this subject in the fields of psy-
29
chology, but also in the study of poetry, narratology
and literary theory in general.
By doing this, it will become more likely that
computer programs ever produce output compara-
ble to that of human authors. Yet the overall goal
should not just be to obtain a pastiche of specific
human artifacts, indistinguishable from the corre-
sponding human-produced versions. Jet planes are
perfectly distinguishable from birds. Which does
not mean they are worthless. Jet planes are differ-
ent from birds because the engineering solutions that
scientists found for achieving flight required differ-
ent materials (metal rather than bone and feathers),
different applications of the basic principles (static
rather than flapping wings) and different means of
propulsion (jet engines rather than muscle power).
However, these departures from the original model
have made the current solution capable of feats that
are impossible for birds. Jet planes can fly much
faster, much higher, and carrying much more weight
than any bird known. Yet al this was made possi-
ble by trying to emulate birds. If we carry the anal-
ogy to its full extent, we should generally consider
departures from human models of linguistic creativ-
ity wherever they result in methods better suited for
computers. This is already being done. However,
we should at some stage also start considering de-
partures from the models for the output as generated
by humans. I would say a second, more idealistic,
purpose of computational creativity might be to look
for things that machines can do that people cannot
do, but which people might yet learn to appreciate.
Acknowledgments
The work reported in this paper was partially
supported by the Ministerio de Educacio?n y
Ciencia (TIN2006-14433-C02-01, TIN2009-14659-
C03-01).
References
L. Carrol. 1872. Through the Looking-Glass and What
Alice Found There. Bo Ejeby Edition, Sweden.
W. Chamberlain. 1981. The Policeman?s Beard is Half
Constructed. Warner Books, New york.
Natalie Dehn. 1981. Story generation after tale-spin. In
In Proceedings of the International Joint Conference
on Artificial Intelligence, pages 16?18.
P. Gerva?s and Leo?n. Story generation driven by system-
modified evaluation validated by human judges. In
Proc. of the First International Conference on Com-
putational Creativity.
P. Gerva?s. 2000. WASP: Evaluation of different strate-
gies for the automatic generation of spanish verse. In
Proceedings of the AISB-00 Symposium on Creative &
Cultural Aspects of AI, pages 93?100.
P. Gerva?s. 2001. An expert system for the composition of
formal spanish poetry. Journal of Knowledge-Based
Systems, 14(3-4):181?188.
P. Gerva?s. 2002. Linguistic creativity at different levels
of decision in sentence production. In Proceedings of
the AISB 02 Symposium on AI and Creativity in Arts
and Science, pages 79?88.
P. Gerva?s. 2009. Computational approaches to story-
telling and creativity. AI Magazine, 30(3):49?62.
P. Gerva?s. 2010. Diez poemas emocionales gen-
erados por un computador. In D. Can?as and
C. Gonza?lez Tardo?n, editors, ?Puede un computador
escribir un poema de amor?, pages 189?196. Editorial
Devenir.
T. Hultberg. 1993. Literally Speaking: sound poetry &
text-sound composition. Bo Ejeby Edition, Sweden.
M. Lebowitz. 1983. Story-telling as planning and learn-
ing. In International Joint Conference on Artificial In-
telligence, volume 1.
H. M. Manurung. 1999. Chart generation of rhythm-
patterned text. In Proc. of the First International
Workshop on Literature in Cognition and Computers.
H. M. Manurung. 2003. An evolutionary algorithm ap-
proach to poetry generation. Ph.D. thesis, University
of Edimburgh, Edimburgh, UK.
James R. Meehan. 1977. TALE-SPIN, an interactive
program that writes stories. In In Proceedings of the
Fifth International Joint Conference on Artificial Intel-
ligence, pages 91?98.
R. Pe?rez y Pe?rez. 1999. MEXICA: A Computer Model of
Creativity in Writing. Ph.D. thesis, The University of
Sussex.
Edgar Allan Poe. 1846. The philosophy of composition.
Graham?s Magazine, XXVIII(28):163?167.
M. Riedl. 2004. Narrative Planning: Balancing Plot
and Character. Ph.D. thesis, Department of Computer
Science, North Carolina State University.
Mike Sharples. 1999. How We Write: Writing As Cre-
ative Design. Routledge, June.
Scott R. Turner. 1993. Minstrel: a computer model of
creativity and storytelling. Ph.D. thesis, University of
California at Los Angeles, Los Angeles, CA, USA.
R. Weiner. 2000. Creativity & beyond : cultures, val-
ues, and change. State University of New York Press,
Albany, NY.
30
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 153?161,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Hybrid Approach to Emotional Sentence Polarity and 
Intensity Classification  
Jorge Carrillo de Albornoz, Laura Plaza, Pablo Gerv?s 
Universidad Complutense de Madrid 
Madrid, Spain 
{jcalbornoz,lplazam}@fdi.ucm.es, pgervas@sip.ucm.es 
  
 
 
 
 
 
Abstract 
In this paper, the authors present a new ap-
proach to sentence level sentiment analysis. 
The aim is to determine whether a sentence 
expresses a positive, negative or neutral sen-
timent, as well as its intensity. The method 
performs WSD over the words in the sentence 
in order to work with concepts rather than 
terms, and makes use of the knowledge in an 
affective lexicon to label these concepts with 
emotional categories.  It also deals with the ef-
fect of negations and quantifiers on polarity 
and intensity analysis. An extensive evaluation 
in two different domains is performed in order 
to determine how the method behaves in 2-
classes (positive and negative), 3-classes (posi-
tive, negative and neutral) and 5-classes 
(strongly negative, weakly negative, neutral, 
weakly positive and strongly positive) classifi-
cation tasks. The results obtained compare fa-
vorably with those achieved by other systems 
addressing similar evaluations. 
1 Introduction 
Sentiment analysis has gained much attention 
from the research community in recent years. It 
is concerned with the problem of discovering 
emotional meanings in text, and most common 
tasks usually include emotion labeling (assigning 
a text its main emotion), polarity recognition 
(classifying a statement into positive or negative) 
and subjectivity identification (determining 
whether a text is subjective or objective). The 
growing research interest is mainly due to the 
practical applications of sentiment analysis. 
Companies and organizations are interested in 
finding out costumer sentiments and opinions, 
while individuals are interested in others? opi-
nions when purchasing a product or deciding 
whether or not watching a movie. 
Many approaches have dealt with sentiment 
analysis as the problem of classifying product or 
service reviews (Pang et al, 2002; Turney, 
2002), while others have attempted to classify 
news items (Devitt and Ahmad, 2007). The task 
is usually addressed as a 2-classes classification 
problem (positive vs. negative). Recent works 
have included the neutral class, trying to detect 
not only the polarity but also the absence of emo-
tional meaning (Wilson et al, 2005; Esuli and 
Sebastiani, 2006). However, few approaches try 
to face a more fine-grained prediction of the in-
tensity (e.g. classifying the polarity into strongly 
negative, weakly negative, neutral, weakly posi-
tive and strongly positive). 
Another important problem of most of these 
approximations is that they usually work with 
terms, and so disregard the contextual meaning 
of those terms in the sentence (Martineau and 
Finin, 2009; Moilanen and Pulman, 2007). The 
use of word disambiguation is not usual in this 
task, due to the fact that most approaches use 
lexical resources created to work with terms. 
However, it is essential to correctly capture the 
meaning of these terms within the text. 
In this paper, we present a hybrid approach 
based on machine learning techniques and lexical 
rules to classify sentences according to their po-
larity and intensity. Thus, given an input text, the 
method is able to determine the polarity of each 
sentence (i.e. if it is negative or positive), as well 
as its intensity. The system tackles the effect of 
negations and quantifiers in sentiment analysis, 
and addresses the problem of word ambiguity, 
taken into account the contextual meaning of the 
terms in the text by using a word sense disam-
biguation algorithm. 
The paper is organized as follows. Section 2 
exposes some background and related work on 
sentiment analysis. Section 3 presents the lexical 
resources and corpora used by the system. Sec-
153
tion 4 describes the method proposed for polarity 
and intensity classification. Section 5 presents 
the evaluation framework and discusses the ex-
perimental results. Finally, section 6 provides 
concluding remarks and future lines of work. 
2 Related work 
The sentiment analysis discipline in computa-
tional linguistic is mainly focused on identify-
ing/classifying different emotional contents with-
in a phrase, sentence or document. This field 
usually encloses tasks such as emotion identifica-
tion, subjectivity classification and polarity rec-
ognition. Sentiment analysis has obtained great 
popularity in the last years mostly due to its suc-
cessful application to different business domains, 
such as the evaluation of products and services, 
where the goal is to discern whether the opinion 
expressed by a user about a product or service is 
favorable or unfavorable. 
Focusing on polarity recognition, the aim of 
this task is the classification of texts into positive 
or negative according to their emotional mean-
ing. Most of the approaches rely on machine 
learning techniques or rule based methods. Sta-
tistical approaches based on term frequencies and 
bags of words are frequently used in machine 
learning approximations. Pang et al (2002) 
present a comparison between three different 
machine learning algorithms trained with bags of 
features computed over term frequencies, and 
conclude that SVM classifiers can be efficiently 
used in polarity identification. Martineau and 
Finin (2009) use a similar approach where the 
words are scored using a Delta TF-IDF function 
before classifying the documents. On the other 
hand, Meena and Prabhakar (2007) study the ef-
fect of conjuncts in polarity recognition using 
rule based methods over the syntax tree of the 
sentence. Whitelaw et al (2005) introduce the 
concept of ?appraisal groups? which are com-
bined with bags of word features to automatical-
ly classify movie reviews. To this aim, they use a 
semi-automated method to generate a lexicon of 
appraising adjectives and modifiers. 
During the past few years, the problem of po-
larity recognition has been usually faced as a step 
beyond the identification of the subjectivity or 
objectivity of texts (Wiebe et al, 1999). Differ-
ent approximations have been proposed to deal 
with this problem. Pang and Lee (2004) propose 
a graph-based method which finds minimum cuts 
in a document graph to classify the sentences 
into subjective or objective. After that, they use a 
bag of words approximation to classify the sub-
jective sentences into positive or negative. Kim 
and Hovy (2004) also introduce a previous step 
to identify the subjectivity of sentences regarding 
a certain topic, and later classify these sentences 
into positives or negatives. 
Most recent approaches do not only deal with 
the 2-classes classification problem, but also in-
troduce a new class representing neutrality. Thus, 
the aim of these works is to classify the text into 
positive, negative or neutral. Wilson et al (2005) 
present a double subjectivity classifier based on 
features such as syntactic classes and sentence 
position, and more semantic features such as ad-
jective graduation. The first classifier determines 
the subjectivity or neutrality of the phrases in the 
text, while the second determines its polarity (in-
cluding neutrality). Esuli and Sebastiani (2006) 
also address this problem testing three different 
variants of a semi-supervised method, and classi-
fy the input into positive, negative or neutral. 
The method proposed yields good results in the 
2-classes polarity classification, while the results 
decrease when dealing with 3-classes. A more 
ambitious classification task is proposed by 
Brooke (2009), where the goal is to measure the 
intensity of polarity. To this aim, the author clas-
sifies the input into 3-classes (strongly-negative, 
ambivalent, and strongly-positive), 4 classes 
(strongly-negative, weakly-negative, weakly-
positive and strongly-positive) and 5-classes 
(strongly-negative, weakly-negative, ambivalent, 
weakly-positive and strongly-positive). The re-
sults decrease considerably with the number of 
classes, from 62% of accuracy for 3-classes to 
38% of accuracy for 5-classes. 
3 Corpora and resources 
The evaluation of the system has been carried out 
using two corpora from two very distinct do-
mains: the Sentence Polarity Movie Review Da-
taset1 and the one used in the SemEval 2007 Af-
fective Text task 2
                                                 
1 http://www.cs.cornell.edu/People/pabo/movie-
review-data/  
. The first one consists of 
10.662 sentences selected from different movie 
review websites. These sentences are labeled as 
positive or negative depending on whether they 
express a positive or negative opinion within the 
movie review. The second one consists of a 
training set and a test set of 250 and 1000 news 
headlines respectively, extracted from different 
news sites. Each sentence is labeled with a value 
2 http://www.cse.unt.edu/~rada/affectivetext/ 
154
between -100 and 100, where -100 means highly 
negative emotional intensity, 100 means highly 
positive and 0 means neutral. To the purpose of 
this work, the test set from the SemEval corpus 
and 1000 sentences randomly extracted from the 
Sentence Polarity Movie Review corpus (500 
positive and 500 negative) were used as evalua-
tion datasets.  
In order to identify the emotional categories 
associated to the concepts in the sentences, an 
affective lexical database based on semantic 
senses, instead of terms, is needed. To this aim, 
the authors have tested different resources and 
finally selected the WordNet Affect affective 
database (Strapparava and Valitutti, 2004). This 
affective lexicon has the particularity of assign-
ing emotional categories to synsets of the Word-
Net lexical database (Miller et al, 1990), allow-
ing the system to correctly disambiguate the 
terms using one of the many WordNet-based 
word sense disambiguation algorithms. The emo-
tional categories in WordNet Affect are orga-
nized hierarchically, and its first level distin-
guishes between positive-emotion, negative-
emotion, neutral-emotion and ambiguous-
emotion. The second level encloses the emotion-
al categories themselves, and consists of a set of 
32 categories. For this work, a subset of 16 emo-
tional categories from this level has been se-
lected, since the hierarchy proposed in WordNet 
Affect is considerably broader than those com-
monly used in sentiment analysis. On the other 
hand, the first level of emotional categories may 
be useful to predict the polarity, but it is clearly 
not enough to predict the intensity of this polari-
ty. To be precise, the subset of emotional catego-
ries used in this work is: {joy, love, liking, calm-
ness, positive-expectation, hope, fear, sadness, 
dislike, shame, compassion, despair, anxiety, 
surprise, ambiguous-agitation and ambiguous-
expectation}. The authors consider this subset to 
be a good representation of the human feeling.  
Since the WordNet Affect hierarchy does not 
provide an antonym relationship, the authors has 
created that relation for the previous set of emo-
tional categories.  Only relationships between 
emotional categories with a strongly opposite 
meaning are created, such as liking-disliking and 
joy-sadness. The purpose of this antonym rela-
tionship is twofold: first, it contributes to handle 
negation forms; and second, it can be used to 
automatically expand the affective lexicon. Both 
issues are discussed in detail later in the docu-
ment. 
On the other hand, since a good amount of 
words with a highly emotional meaning, such as 
dead, cancer and violent, are not labeled in 
WordNet Affect, these words have been manual-
ly labeled by the authors and have been later ex-
tended with their synonyms, antonyms and de-
rived adjectives using the corresponding seman-
tic and lexical relations in WordNet. This process 
has been done in two steps in order to measure 
the effect of the number of synsets labeled on the 
classification accuracy, as described in section 5.  
The WordNet Affect 1.1 lexicon consists of a 
set of 911 synsets. However, the authors have 
detected that a good number of these synsets 
have been labeled more than once, and with dif-
ferent emotional categories (e.g. the synset 
?a#00117872 {angered, enraged, furious, infu-
riated, maddened}? is labeled with three different 
categories: anger, fury and infuriation). Thus, 
after removing these synsets and those labeled 
with an emotional category not included in the 
16-categories subset used in this work, the affec-
tive lexicon presents 798 synsets. After the first 
step of semi-automatic labeling, the affective 
lexicon increased the number of synsets in 372, 
of which 100 synsets were manually labeled, and 
272 were automatically derived throughout the 
WordNet relations. The second and last step of 
semi-automatic labeling added 603 synsets to the 
lexicon, of which 200 synsets were manually 
labeled, and 403 were automatically derived.  
The final lexicon presents 1773 synsets and 4521 
words labeled with an emotional category. Table 
1 shows the distribution of the affective lexicon 
in grammatical categories. 
 
Grammatical  
Category 
WNAffect 
 
WNAffect + 
1st  step 
WNAffect + 
 2nd step 
Nouns 280 440 699 
Verbs 122 200 309 
Adjectives 273 394 600 
Adverbs 123 136 165 
 
Table 1: Distribution in grammatical categories of the syn-
sets in the affective lexicon. 
4 The method 
In this section, the method for automatically 
labeling sentences with an emotional intensity 
and polarity is presented. The problem is faced 
as a text classification task, which is accomplish-
es throughout four steps. Each step is explained 
in detail in the following subsections.  
155
4.1 Pre-processing: POS tagging and con-
cept identification 
In order to determine the appropriate emotional 
category for each word in its context, a pre-
processing step is accomplished to translate each 
term in the sentence to its adequate sense in 
WordNet. To this aim, the system analyzes the 
text, splits it into sentences and tags the tokens 
with their part of speech. The Gate architecture3 
and the Stanford Parser4
Once the sentences have been split and tagged, 
the method maps each word of each sentence 
into its sense in WordNet according to its con-
text. To this end, the lesk WSD algorithm im-
plemented in the WordNet Sense-Relate perl 
package is used (Patwardhan et al, 2005). The 
disambiguation is carried out only over the 
words belonging to the grammatical categories 
noun, verb, adjective and adverb, as only these 
categories can present an emotional meaning. As 
a result, we get the stem and sense in WordNet 
of each word, and this information is used to re-
trieve its synset.  
 were selected to carry 
out this process. In particular the Annie English 
Tokeniser, Hash Gazetter, RegEx Sentence Split-
ter and the Stanford Parser modules in Gate are 
used to analyze the input. In this step also the 
syntax tree and dependencies are retrieved from 
the Stanford Parser. These features will be used 
in the post-processing step in order to identify 
the negations and the quantifiers, as well as their 
scope. 
A good example of the importance of perform-
ing word disambiguation can be shown in the 
sentence ?Test to predict breast cancer relapse is 
approved? from the SemEval news corpus. The 
noun cancer has five possible entries in WordNet 
and only one refers to a ?malignant growth or 
tumor?, while the others are related with ?astrol-
ogy? and the ?cancer zodiacal constellation?. 
Obviously, without a WSD algorithm, the wrong 
synset will be considered, and a wrong emotion 
will be assigned to the concept. 
Besides, to enrich the emotion identification 
step, the hypernyms of each concept are also re-
trieved from WordNet. 
4.2 Emotion identification 
The aim of the emotion identification step is to 
map the WordNet concepts previously identified 
to those present in the affective lexicon, as well 
                                                 
3 http://gate.ac.uk/ 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
as to retrieve from this lexicon the corresponding 
emotional category of each concept.  
We hypothesize that the hypernyms of a con-
cept entail the same emotions than the concept 
itself, but the intensity of such emotions decreas-
es as we move up the hierarchy (i.e. the more 
general the hypernym becomes, the less its emo-
tional intensity is).  Following this hypothesis, 
when no entry is found in the affective lexicon 
for a given concept, the emotional category asso-
ciated to its nearest hypernym, if any, is used to 
label the concept. However, only a certain level 
of hypernymy is accepted, since an excessive 
generalization introduces some noise in the emo-
tion identification. This parameter has been em-
pirically set to 3 (Carrillo de Albornoz et al, 
2010). Previous experiments have shown that, 
upper this level, the working hypothesis becomes 
unreliable. 
The sentence ?Siesta cuts risk of heart disease 
death study finds? clearly illustrates the process 
described above. In this sentence, the concepts 
risk, death and disease are labeled with an emo-
tional category: in particular, the categories as-
signed to them are fear, fear and dislike respec-
tively. However, while the two firsts are re-
trieved from the affective lexicon by their own 
synsets, the last one is labeled through its hyper-
nym: since no matching is found for disease in 
the lexicon, the analysis over its hypernyms de-
tects the category dislike assigned to the synset 
of its first hypernym, which contains words such 
as illness and sickness, and the same emotion 
(dislike) is assigned to disease. 
It must be noted that, to perform this analysis, 
a previous mapping between 2.1 and 1.6 Word-
Net versions was needed, since the method and 
the affective lexicon work on different versions 
of the database.  
4.3 Post-processing: Negation and quantifi-
ers detection 
Once the concepts of the sentence have been la-
beled with their emotional categories, the next 
step aims to detect and solve the effect of the 
negations and the quantifiers on the emotional 
categories identified in the previous step.  
The effect of negation has been broadly stu-
died in NLP (Morante and Daelemans, 2009) and 
sentiment analysis (Jia et al, 2009). Two main 
considerations must be taken into account when 
dealing with negation. First, the negation scope 
may affect only a word (no reason), a proposi-
tion (Beckham does not want to play again for 
Real) or even a subject (No one would like to do 
156
this). Different approximations have been pro-
posed to delimit the scope of negation. Some 
assume the scope to be those words between the 
negation token and the first punctuation mark 
(Pang et al, 2002), others consider a fixed num-
ber of words after the negation token (Hu and 
Liu, 2004). Second, the impact of negation is 
usually neutralized by reversing the polarity of 
the sentence (Polanyi and Zaenen, 2006) or using 
contextual valence shifters which increase or 
dismiss the final value of negativity or positivity 
of the sentence (Kennedy and Inkpen, 2006). 
In this work, the negation scope is detected us-
ing the syntax tree and dependencies generated 
by the Stanford Parser. The dependency neg al-
lows us to easily determine the presence of sev-
eral simple types of negation, such as those pre-
ceded by don?t, didn?t, not, never, etc. Other 
words not identified with this dependency, but 
also with a negation meaning, such as no, none? 
nor or nobody, are identified using a negation 
token list. To determine the negation scope, we 
find in the syntax tree the first common ancestor 
that encloses the negation token and the word 
immediately after it, and assume all descendant 
leaf nodes to be affected by the negation.  
For each concept in the sentence that falls into 
the scope of a negation, the system retrieves its 
antonym emotional category, if any, and assigns 
this category to the concept. If no antonym emo-
tion is obtained, the concept is labeled with no 
emotion, according to the premise that the nega-
tion may change or neutralize the emotional po-
larity. An example of this process can be shown 
in the sentence ?Children and adults enamored 
of all things pokemon won't be disappointed?. In 
this sentence, the Stanford Parser discovers a 
negation and the system, through the syntax tree, 
determines that the scope of the negation enclos-
es the words ?won?t be disappointed?. As the 
synset of ?disappointed? has been labeled with 
the emotional category despair, its antonym is 
retrieved, and the emotional category of the an-
tonym, hope, is used to label the concept.  
On the other hand, the quantifiers are words 
considered in sentiment analysis as amplifiers or 
downtoners (Quirk et al, 1985). That is to say, 
the word very in the sentence ?That is a very 
good idea? amplifies the intensity of the emo-
tional meaning and the positivity of the sentence, 
while the word less in the sentence ?It is less 
handsome than I was expecting? dismisses its 
intensity and polarity. The most common ap-
proach to identify quantifiers is the use of lists of 
words which play specific grammatical roles in 
the sentence. These lists normally contain a fixed 
value for all positive words and another value for 
all negative words (Polanyi and Zaenen, 2006). 
By contrast, Brooke (2009) proposes a novel ap-
proach where each quantifier is assigned its own 
polarity and weight.  
The quantifiers are usually represented as sen-
tence modifiers, assuming their scope to be the 
whole sentence and modifying its overall polari-
ty. However, when dealing with sentences like 
?The house is really nice and the neighborhood 
is not bad?, these approaches assume that the 
quantifier really amplifies the intensity of both 
conjunctives, when it only should amplify the 
intensity of the first one. By contrast, our ap-
proach determines the scope of the quantifiers by 
the syntax tree and the dependencies over them. 
Thus, when a quantifier is detected in a sentence, 
the dependencies are checked and only those that 
play certain roles, such as adverbial or adjectival 
modifiers, are considered. All concepts affected 
by a quantifier are marked with the weight cor-
responding to that quantifier, which will serve to 
amplify/dismiss the emotions of these concepts 
in the classification step.  The quantifier list used 
here is the one proposed in Brooke (2009). 
The sentence ?Stale first act, scrooge story, 
blatant product placement, some very good com-
edic songs? illustrates the analysis of the quan-
tifiers. The system detects two tokens which are 
in the quantifier list and play the appropriate 
grammatical roles. The first quantifier some af-
fects to the words ?very good comedic songs?, 
while the second quantifier very only affects to 
?good?. So these concepts are marked with the 
specific weight of each quantifier. Note that the 
concept ?good? is marked twice. 
4.4 Sentence classification 
Up to this point, the sentence has been labeled 
with a set of emotional categories, negations and 
their scope have been detected and the quantifi-
ers and the concepts affected by them have been 
identified. In this step, this information is used to 
translate the sentence into a Vector of Emotional 
Occurrences (VEO), which will be the input to 
the machine learning classification algorithm. 
Thus, each sentence is represented as a vector of 
16 values, each of one representing an emotional 
category. The VEO vector is generated as fol-
lows: 
? If the concept has been labeled with an 
emotional category, the position of the 
vector for this category is increased in 1. 
157
? If no emotional category has been found 
for the concept, then the category of its 
first hypernym labeled is used. As the 
hypernym generalizes the meaning of the 
concept, the value assigned to the position 
of the emotional category in the VEO is 
weighted as follows: 
[ ] [ ]
1.
1
+
+=
DepthHyper
iVEOiVEO  
? If a negation scope encloses the concept, 
then the antonym emotion is used, as de-
scribed in the previous step. The emotion-
al category position of this antonym in the 
VEO is increased in 0.9. Different tests 
have been carried out to set this parameter, 
and the 0.9 value got the best results. The 
reason for using a lower value for the 
emotional categories derived from nega-
tions is that the authors consider that a ne-
gation changes the emotional meaning of a 
concept but usually in a lower percentage. 
For example, the sentence ?The neighbor-
hood is not bad? does not necessarily 
mean that it is a good neighborhood, but it 
is a quite acceptable one.  
? If a concept is affected by a quantifier, 
then the weight of that quantifier is added 
to the position in the VEO of the emotion-
al category assigned to the concept. 
Thus, a sentence like ?This movie?. isn?t 
worth the energy it takes to describe how really 
bad it is? will be represented by the VEO [1.0, 0, 
0.0, 0, 0, 0.0, 0, 0, 2.95, 0, 0, 0, 0, 0, 0, 0].  In 
this sentence, the concept movie is labeled with 
the emotional category joy, the concept worth is 
labeled with positive-expectation, the concept 
energy is labeled with liking, and the concept 
bad is labeled with dislike. Since the concepts 
worth and energy fall into the negation scope, 
they both change their emotional category to dis-
like. Besides, since the concept bad is amplified 
by the quantifier really, the weight of this con-
cept in the VEO is increased in 0.15. 
5 Evaluation framework and results 
In this work, two different corpora have been 
used for evaluation (see Section 3): a movie re-
view corpus containing 1000 sentences labeled 
with either a positive or negative polarity; and a 
news headlines corpus composed of 1000 sen-
tences labeled with an emotional intensity value 
between -100 and 100. 
To determine the best machine learning algo-
rithm for the task, 20 classifiers currently imple-
mented in Weka5
5.1 Evaluating polarity classification 
 were compared. We only show 
the results of the best performance classifiers: a 
logistic regression model (Logistic), a C4.5 deci-
sion tree (J48Graph) and a support vector ma-
chine (LibSVM). The best outcomes for the three 
algorithms were reported when using their de-
fault parameters, except for J48Graph, where the 
confidence factor was set to 0.2. The evaluation 
is accomplishes using 10-fold cross validation. 
Therefore, 100 instances of each dataset are held 
back for testing in each fold, and the additional 
900 instances are used for training. 
We first analyze the effect of expanding the cov-
erage of the emotional lexicon by semi-
automatically adding to WordNet Affect more 
synsets labeled with emotional categories, as ex-
plained in Section 3. To this end, we compare the 
results of the method using three different affec-
tive lexical databases: WordNet Affect and 
WordNet Affect extended with 372 and 603 syn-
sets, respectively. For the sake of comparing the 
results in both corpora, the news dataset has been 
mapped to a -100/100 classification (-100 = [-
100, 0), 100 = [0,100]). 
Table 2 shows the results as average precision 
and accuracy of these experiments. Note that, as 
the weight of mislabeling for both classes is the 
same and the classes are balanced, accuracy is 
equal to recall in all cases. Labeling 975 new 
synsets significantly improves the performance 
of our system in both datasets and for all ML 
techniques. In particular, the best improvement is 
achieved by the Logistic classifier: from 52.7% 
to 72.4% of accuracy in the news dataset, and 
from 50.5% to 61.5% of accuracy in the movies 
dataset.  
 
Emotional  
Lexicon 
Method 
News Corpus Movie Reviews 
Pr. Ac. Pr. Ac. 
WNAffect 
Logistic 52.8 52.7 51.3 50.5 
J48Graph 27.7 52.6 50 50 
LibSVM 27.7 52.6 53.2 50.6 
WNAffect + 
372 synsets 
Logistic 69.9 65.2 53.9 53.8 
J48Graph 70.1 64.8 55.3 55.1 
LibSVM 68.9 63.9 52 51.8 
WNAffect + 
603 synsets 
Logistic 73.8 72.4 61.6 61.5 
J48Graph 73.6 70.9 60.9 60.9 
LibSVM 71.6 70.3 62.5 59.4 
 
Table 2: Precision and accuracy percentages achieved by 
our system using different affective databases. 
 
                                                 
5 http://www.cs.waikato.ac.nz/ml/weka/ 
158
We have observed that, especially in the news 
dataset, an important number of sentences that 
are labeled with a low positive or negative emo-
tional intensity could be perfectly considered as 
neutral. The intensity of these sentences highly 
depends on the previous knowledge and particu-
lar interpretation of the reader. For instance, the 
sentence ?Looking beyond the iPhone? does not 
express any emotion itself, unless you are fan or 
detractor of Apple. However, this sentence is 
labeled in the corpus with a 15 intensity value. It 
is likely that these kinds of sentences introduce 
noise into the dataset. In order to estimate the 
influence of such sentences in the experimental 
results, we conducted a test removing from the 
news dataset those instances with an intensity 
value in the range [-25, 25]. As expected, the 
accuracy of the method increases substantially, 
i.e. from 72.4% to 76.3% for logistic regression. 
The second group of experiments is directed to 
evaluate if dealing with negations and quantifiers 
improves the performance of the method. To this 
end, the approach described in Section 4.3 was 
applied to both datasets. Table 3 shows that 
processing negations consistently improves the 
accuracy of all algorithms in both datasets; while 
the effect of the quantifiers is not straightforward. 
Even if we expected that using quantifiers would 
lead to better results, the performance in both 
datasets decreases in 2 out of the 3 ML algo-
rithms. However, combining both features im-
proves the results in both datasets. The reason 
seems to be that, when no previous negation de-
tection is performed, if the emotional category 
assigned to certain concepts are wrong (because 
these concepts are affected by negations), the 
quantifiers will be weighting the wrong emotions.  
 
Features Method 
News Corpus Movie Reviews 
Pr. Ac. Pr. Ac. 
Negations  
Logistic 74.2 72.5 61.7 61.6 
J48Graph 74.1 71.2 62.8 62.6 
LibSVM 72.7 71.1 62.4 60.1 
Quantifiers 
Logistic 73.7 72.2 61.9 61.9 
J48Graph 73.6 70.9 59.5 59.5 
LibSVM 72.1 70.6 61.1 59 
Negations + 
Quantifiers 
Logistic 74.4 72.7 62.4 62.4 
J48Graph 74.1 71.2 62.5 62.1 
LibSVM 72.8 71.2 62.6 60.5 
 
Table 3: Precision and accuracy of the system improved 
with negation and quantifier detection. 
 
The comparison with related work is difficult 
due to the different datasets and methods used in 
the evaluations. For instance, Pang et al (2002) 
use the Movie Review Polarity Dataset, achiev-
ing an accuracy of 82.9% training a SVM over a 
bag of words. However, their aim was to deter-
mine the polarity of documents (i.e. the whole 
movie reviews) instead of sentences. When 
working at the sentence level, the information 
from the context is missed, and the results are 
expected to be considerably lower. As a matter 
of fact, it happens that many sentences in the 
Sentence Polarity Movie Review Dataset are la-
beled as positive or negative, but do not express 
any polarity when taken out of the context of the 
overall movie review. This conclusion is also 
drawn by Meena and Prabhakar (2007), who 
achieve an accuracy of 39% over a movie review 
corpus (not specified) working at the sentence 
level, using a rule based method to analyze the 
effect of conjuncts. This accuracy is well below 
that of our method (62.6%).  
Molianen and Pulman (2007) present a senti-
ment composition model where the polarity of a 
sentence is calculated as a complex function of 
the polarity of its parts. They evaluate their sys-
tem over the SemEval 2007 news corpus, and 
achieve an accuracy of 65.6%, under our same 
experimental conditions, which is also signifi-
cantly lower than the accuracy obtained by our 
method.  
5.2 Evaluating intensity classification 
Apart from identifying of polarity, we also want 
to examine the ability of our system to determine 
the emotional intensity in the sentences. To this 
aim, we define two intensity distributions: the 3-
classes and the 5-classes distribution. For the 
first distribution, we map the news dataset to 3-
classes: negative [-100, -50), neutral [-50, 50) 
and positive [50, 100]. For the second distribu-
tion, we map the dataset to 5-classes: strongly 
negative [-100, -60), negative [-60, -20), neutral 
[-20, 20), positive [20, 60) and strongly positive 
[60, 100]. We can see in Table 4 that, as the 
number of intensity classes increases, the results 
are progressively worse, since the task is pro-
gressively more difficult. 
 
Intensity 
classes 
Method 
News Corpus 
Pr. Ac. 
2-classes 
Logistic 74.4 72.7 
J48Graph 74.1 71.2 
LibSVM 72.8 71.2 
3-classes 
Logistic 60.2 63.8 
J48Graph 66 64.8 
LibSVM 54.8 64.6 
5-classes 
Logistic 48.3 55.4 
J48Graph 47.3 54.8 
LibSVM 43.1 53.1 
 
Table 4: Precision and accuracy in three different intensity 
classification tasks. 
159
The 3-classes distribution coincides exactly 
with that used in one of the SemEval 2007 Af-
fective task, so that we can easily compare our 
results with those of the systems that participated 
in the task. The CLaC and CLaC-NB systems 
(Andreevskaia and Bergler, 2007) achieved, re-
spectively, the best precision and recall. CLaC 
reported a precision of 61.42 % and a recall of 
9.20%; while CLaC-NB reported a precision of 
31.18% and a recall of 66.38%. Our method 
clearly outperforms both systems in precision, 
while provides a recall (which is equal to the ac-
curacy) near to that of the best system. Besides, 
our results for both metrics are well-balanced, 
which does not occur in the other systems. 
Regarding the 5-classes distribution evalua-
tion, to the authors? knowledge no other work 
has been evaluated under these conditions. How-
ever, our system reports promising results: using 
5 classes it achieves better results than other par-
ticipant in the SemEval task using just 3 classes 
(Chaumartin, 2007; Katz et al, 2007). 
5.3 Evaluating the effect of  word ambiguity 
on sentiment analysis 
A further test has been conducted to examine the 
effect of word ambiguity on the classification 
results. To this aim, we repeated the experiments 
above without using WSD. First, we simply as-
signed to each word its first sense in WordNet. 
Second, we selected these senses randomly.  The 
results are presented in Table 5. We only show 
those of the best algorithm for each intensity dis-
tribution.  
 
Intensity classes Method 
News Corpus 
Pr. Ac. 
2-classes (Logistic) 
WSD 74.4 72.6 
1st Sense 71.6 69.3 
Random Sense 69.1 64.1 
3-classes (J48Graph) 
WSD 66 64.8 
1st Sense 59 62.9 
Random Sense 50.8 61 
5-classes  (Logistic) 
WSD 48.3 55.4 
1st Sense 43.7 53.8 
Random Sense 46.8 51.6 
 
Table 5: Precision and accuracy for three different word 
disambiguation strategies. 
 
It can be observed that, even though the use of 
word disambiguation improves the classification 
precision and accuracy, the improvement with 
respect to the first sense heuristic is less than ex-
pected. This may be due to the fact that the 
senses of the words in WordNet are ranked ac-
cording to their frequency, and so the first sense 
of a word is also the most frequent one. Besides, 
the Most Frequent Sense (MFS) heuristic in 
WSD is usually regarded as a difficult competi-
tor. On the contrary, the improvement with re-
spect to the random sense heuristic is quite re-
markable. 
 
6 Conclusions and future work 
In this paper, a novel approach to sentence level 
sentiment analysis has been described. The sys-
tem has resulted in a good method for sentence 
polarity classification, as well as for intensity 
identification. The results obtained outperform 
those achieved by other systems which aim to 
solve the same task.  
Nonetheless, some considerations must be 
noted. Even with the extended affective lexicon, 
around 1 in 4 sentences of each corpus has not 
been assigned any emotional category, some-
times because their concepts are not labeled in 
the lexicon, but mostly because their concepts do 
not have any emotional meaning per se. A test on 
the news corpus removing those sentences not 
labeled with any emotional meaning has been 
performed for the 2-classes classification prob-
lem, allowing the method to obtain an accuracy 
of 81.7%. However, to correctly classify these 
sentences, it would be necessary to have addi-
tional information about their contexts (i.e. the 
body of the news item, its section in the newspa-
per, etc.).   
Finally, the authors plan to extend the method 
to deal with modal and conditional operators, 
which will allow us to distinguish among situa-
tions that have happened, situations that are hap-
pening, situations that could, might or possibly 
happen or will happen, situations that are wished 
to happen, etc. 
 
Acknowledgments 
This research is funded by the Spanish Ministry 
of Science and Innovation (TIN2009-14659-
C03-01), the Comunidad Autonoma de Madrid 
and the European Social Fund through the IV 
PRICIT program, and the Spanish Ministry of 
Education through the FPU program. 
References  
Julian Brooke. 2009. A Semantic Approach to Auto-
mated Text Sentiment Analysis. Simon Fraser 
University. Ph. D. Thesis. 
Jorge Carrillo de Albornoz, Laura Plaza and Pablo 
Gerv?s. 2010. Improving Emotional Intensity Clas-
160
sification using Word Sense Disambiguation. Re-
search in Computing Science 46 :131-142. 
Fran?ois-R?gis Chaumartin. 2007. UPAR7: A Know-
ledge-based System for Headline Sentiment Tag-
ging. In Proceedings of the 4th Workshop on Se-
mantic Evaluations (SemEval 2007), pages 422-
425. 
Ann Devitt and Khurshid Ahmad. 2007. Sentiment 
Polarity Identification in Financial News: A Cohe-
sion-based Approach. In Proceedings of the 45th 
Annual Meeting of the ACL, pages 984-991. 
Andrea Esuli and Fabrizio Sebastiani. 2006. Deter-
mining Term Subjectivity and Term Orientation for 
Opinion Mining. In Proceedings of the 11th Confe-
rence of the EACL, pages 193-200. 
Minging Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the 
10th ACM SIGKDD Conference on Knowledge 
Discovery and Data Mining, pages 168-177. 
Lifeng Jia, Clement Yu and Weiji Meng. 2009. The 
Effect of Negation on Sentiment Analysis and Re-
trieval Effectiveness. In Proceeding of the 18th  
ACM Conference on Information and Knowledge 
Management, pages 1827-1830. 
Phil Katz, Matthew Singleton and Richard Wicen-
towski. 2007. SWAT-MP: the SemEval-2007 Sys-
tems for Task 5 and Task 14. In Proceedings of the 
4th Workshop on Semantic Evaluations (SemEval 
2007), pages 308-313. 
Alistair Kennedy and Diana Inkpen. 2006. Sentiment 
Classification of Movie Reviews Using Contextual 
Valence Shifters. Computational Intelligence 
22(2): 110-125. 
Soo-Min Kim and Eduard Hovy. 2004. Determining 
the Sentiment of Opinions. In Proceedings of COL-
ING 2004, pages 1367-1373. 
Justin Martineau and Tim Finin. 2009. Delta TFIDF: 
An Improved Feature Space for Sentiment Analy-
sis. In Proceedings of the 3rd AAAI International 
Conference on Weblogs and Social Media. 
Arun Meena and T.V. Prabhakar. 2007. Sentence 
Level Sentiment Analysis in the Presence of Con-
juncts Using Linguistic Analysis. In Proceedings 
of ECIR 2007, pages 573-580. 
George A. Miller, Richard Beckwith, Christiane Fell-
baum Derek Gross and Katherine Miller. 1990. In-
troduction to WordNet: An On-Line Lexical Data-
base. International Journal of Lexicography 
3(4):235-244. 
Karo Moilanen and Stephen Pulman. 2007. Sentiment 
Composition. In Proceedings of RANLP 2007, 
pages 378-382. 
Roser Morante and Walter Daelemans. 2009. A Meta-
learning Approach to Processing the Scope of Ne-
gation. In Proceedings of the CONLL 2009, pages 
21-29. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. In Proceedings of 
CoRR 2002. 
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis using Subjectivity Sum-
marization based on Minimum Cuts. In Proceed-
ings of the 42nd  Annual Meeting of the ACL, pages 
271-278. 
Siddharth Patwardhan, Satanjeev Banerjee and Ted 
Pedersen. 2005. SenseRelate::TargetWord - A Ge-
neralized Framework for Word Sense Disambigua-
tion. In Proceedings of the ACL 2005 on Interac-
tive Poster and Demonstration Sessions, pages 73-
76. 
Livia Polanyi and Annie Zaenen. 2006. Contextual 
Valence Shifters. Computing Attitude and Affect in 
Text: Theory and Applications. In The Information 
Retrieval Series 20, pages 1-10. 
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech 
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman. 
Carlo Strapparava and Alessandro Valitutti. 2004. 
Wordnet-Affect: an Affective Extension of Word-
Net. In Proceedings of the LREC 2004, pages 
1083-1086.  
Peter D. Turney. 2002. Thumbs up or Thumbs 
down?: Semantic Orientation applied to Unsuper-
vised Classification of Reviews. In Proceedings of 
the 40th Annual Meeting of the ACL, pages 417-
424. 
Casey Whitelaw, Navendu Garg and Shlomo Arga-
mon. 2005. Using Appraisal Groups for Sentiment 
Analysis. In Proceedings of the 14th ACM Confe-
rence on Information and Knowledge Manage-
ment, pages 625-631. 
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P. 
O?Hara. 1999. Development and Use of a Gold-
standard Data Set for Subjectivity Classification. In 
Proceedings of the 37th Annual Meeting of the 
ACL, pages 246-253. 
Theresa Wilson, Janyce Wiebe and Paul Hoffman. 
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the 
HLT-EMNLP 2005, pages 347-354. 
161
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 128?136,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Experimental Identification of the Use of Hedges in the Simplification of
Numerical Expressions
Susana Bautista and Raquel Herva?s and Pablo Gerva?s
Universidad Complutense de Madrid, Spain
{raquelhb,subautis}@fdi.ucm.es, pgervas@sip.ucm.es
Richard Power and Sandra Williams
Department of Computing, The Open University, Milton Keynes MK76AA, UK
{r.power,s.h.williams}@open.ac.uk
Abstract
Numerical information is very common in
all kinds of documents from newspapers and
magazines to household bills and wage slips.
However, many people find it difficult to un-
derstand, particularly people with poor educa-
tion and disabilities. Sometimes numerical in-
formation is presented with hedges that mod-
ify the meaning. A numerical hedge is a word
or phrase employed to indicate explicitly that
some loss of precision has taken place (e.g.,
?around?) and it may also indicate the di-
rection of approximation (e.g., ?more than?).
This paper presents a study of the use of nu-
merical hedges that is part of research inves-
tigating the process of rewriting difficult nu-
merical expressions in simpler ways. We car-
ried out a survey in which experts in numer-
acy were asked to simplify a range of pro-
portion expressions and analysed the results to
obtain guidelines for automating the simplifi-
cation task.
1 Introduction
All public information services and documents
should be accessible in such a way that makes them
easily understood by everybody, according to the
United Nations (1994). Nowadays, a large percent-
age of information expressed in daily news comes
in the form of numerical expressions (statistics of
economy, demography data, etc). But many people
have problems with understanding such expressions
-e.g., people with limited education or some kind of
mental disability.
Lack of ability to understand numerical informa-
tion is an even greater problem than poor literacy.
A U.K. Government Survey in 2003 estimated that
6.8 million adults had insufficient numeracy skills
to perform simple everyday tasks such as paying
house-hold bills and understanding wage slips, and
23.8 million adults would be unable to achieve grade
C in the GCSE maths examination for 16 year-old
school children (Williams et al, 2003).
A first possible approach to solve this impor-
tant social problem is making numerical informa-
tion accessible by rewriting difficult numerical ex-
pressions using alternative wordings that are easier
to understand. Some loss of precision could have
positive advantages for numerate people as well as
less numerate. Such an approach would require a
set of rewriting strategies yielding expressions that
are linguistically correct, easier to understand than
the original, and as close as possible to the original
meaning.
In rewriting, hedges play an important role. For
example,?50.9%? could be rewritten as ?just over
half? using the hedge ?just over?. In this kind of
simplification, hedges indicate that the original num-
ber has been approximated and, in some cases, also
the direction of approximation.
This paper presents a preliminary study of the use
of hedges when numerical expressions are simplified
to make them more accessible. We have carried out
a survey in which experts in numeracy were asked to
simplify a range of proportion expressions to obtain
guidelines for developing the numerical expressions
simplification task automatically. As a first step to-
wards more complex simplification strategies, we
128
are trying to simplify numerical expressions without
losing substantial information. Our study does not
have a particular kind of disability in mind. Rather,
we aim to simplify according to levels of difficulty
defined in the Mathematics Curriculum of the Quali-
fications and Curriculum Authority (1999). Adapta-
tion to particular types of users is beyond the scope
of this paper.
2 Background
Text simplification, a relative new task in Natu-
ral Language Processing, has been directed mainly
at syntactic constructions and lexical choices that
some readers find difficult, such as long sentences,
passives, coordinate and subordinate clauses, ab-
stract words, low frequency words, and abbrevia-
tions. Chandrasekar et al (1996) introduced a two-
stage process, first transforming from sentence to
syntactic tree, then from syntactic tree to new sen-
tence; Siddharthan (2002) instead proposed a three-
stage process comprising analysis, transformation
and generation. In 1998, the project PSET (Car-
roll et al, 1998) employed lexical as well as syn-
tactic simplifications. Other researchers have fo-
cused on the generation of readable texts for readers
with low basic skills (Williams and Reiter, 2005),
and for teaching foreign languages (Petersen and
Ostendorf, 2007). There has been some previous
work on numerical expressions but more for experts
than for people who have difficulties with numer-
acy (Ellen Peters and Dieckmann, 2007), (Nathan
F. Dieckmann and Peters, 2009), (Ann M. Bisantz
and Munch, 2005), (Mishra H, 2011). However,
to our knowledge, there have been no previous at-
tempts to automatically simplify numerical informa-
tion in texts.
A corpus of numerical expressions was collected
for the NUMGEN project (Williams and Power,
2009). The corpus contains 10 sets of newspaper ar-
ticles and scientific papers (110 texts in total). Each
set is a collection of articles on the same topic ?
e.g., the increased risk of breast cancer in red meat
eaters, and the decline in the puffin population on
the Isle of May. Within each set, identical numeri-
cal facts are presented in a variety of linguistic and
mathematical forms.
3 Experiment
Our survey took the form of a questionnaire in
which participants were shown a sentence contain-
ing one or more numerical expressions which they
were asked to simplify using hedges if necessary.
3.1 Materials
Our simplification strategies are focused at two lev-
els: decimal percentages and whole-number per-
centages. For the survey we chose three sets of can-
didate sentences from the NUMGEN corpus: eight
sentences containing only decimal percentages and
two sets of eight sentences containing mixed whole-
number and decimal percentages. The number of
numerical expressions are more than eight because
some sentences contained more than one proportion
expression.
A wide spread of proportion values was present in
each set, including the two end points at nearly 0.0
and almost 1.0. We also included some numerical
expressions with hedges and sentences from differ-
ent topics in the corpus. In short, we included as
many variations in context, precision and different
wordings as possible.
3.2 Participants
We carried out the survey with primary or secondary
school mathematics teachers or adult basic numer-
acy tutors, all native English speakers. We found
them through personal contacts and posts to Inter-
net forums. The task of simplifying numerical ex-
pressions is difficult, but it is a task that this group
seemed well qualified to tackle since they are highly
numerate and accustomed to talking to people who
do not understand mathematical concepts very well.
Our experimental evaluation involved 34 partici-
pants who answered at least one question in our sur-
vey (some participants did not complete it).
3.3 Survey Design and Implementation
The survey was divided into three parts as follows:
1. Simplification of numerical expressions for a
person who can not understand percentages
2. Simplification of numerical expressions for a
person who can not understand decimals
129
3. Free simplification of numerical expressions
for a person with poor numeracy
Each part of the survey is considered as a differ-
ent kind of simplification: (1) simplification with no
percentages, (2) simplification with no decimals and
(3) free simplification.
For part (2), the set of sentences containing only
decimal percentages was used. One of the two
mixed sets of sentences with whole-number and
decimal percentages was used for part (1) and the
other for part (3). The experiment was presented on
SurveyMonkey1, a commonly-used provider of web
surveys. The survey was configured so that partic-
ipants could leave the questionnaire and later con-
tinue with it.
We asked participants to provide simplifications
for numerical expressions that were marked by
square brackets in each sentence. Below the sen-
tence, each bracketed number was shown beside a
text box in which the participant was asked to type
the simplified version. Our instructions said that nu-
merical expressions could be simplified using any
format: number words, digits, fractions, ratios, etc.
and that hedges such as ?more than?, ?almost? and
so on could be introduced if necessary. Participants
were also told that the meaning of the simplified ex-
pression should be as close to the original expres-
sion as possible and that, if necessary, they could
rewrite part of the original sentence. Figure 1 shows
a screenshot of part of the questionnaire.
3.4 Underlying assumptions
A numerical expression (NE) is considered to be a
phrase that represents a quantity, sometimes modi-
fied by a numerical hedge as in ?less than a quarter?
or ?about 20%?. We have restricted coverage to pro-
portions -i.e., fractions, ratios and percentages. We
had five hypotheses:
? H1: The use of hedges to accompany the sim-
plified numerical expression is influenced by
the simplification strategy selected. We con-
sider the use of fractions, ratios and percent-
ages like simplification strategies.
? H2: The use of hedges to simplify the numeri-
cal expression is influenced by the value of the
1www.surveymonkey.com
proportion, with values in the central range (say
0.2 to 0.8) and values at the extreme ranges (say
0.0-0.2 and 0.8-1.0) having a different use of
hedges.
? H3: The loss of precision allowed for the sim-
plified numerical expression is influenced by
the simplification strategy selected.
? H4: There is some kind of correlation between
the loss of precision and the use of hedges, in
such a way that the increase or decrease in the
former influences changes in the latter.
? H5: As an specific case of H4, when writers
choose numerical expressions for readers with
low numeracy, they do not tend to use hedges if
they are not losing precision.
4 Results
The results of the survey were carefully analyzed as
follows. First, within each block of questions, a set
of simplification strategies was identified for each
specific numerical expression. These strategies were
then grouped together according to the mathematical
forms and/or linguistic expressions employed (frac-
tions, ratios, percentages).
With a view to using these data to design an au-
tomated simplification system, these data have to be
analyzed in terms of pairs of a given input numeri-
cal expression and the simplified expression result-
ing from applying a specific simplification strategy.
For such pairings, three important features must be
considered as relevant to choosing a realization:
? Whether any numbers in the expression are re-
alized as one of the different types of available
expressions (fractions, ratios, percentages).
? The loss of precision involved in the simplifi-
cation.
? The possible use of a hedge to cover this loss
of precision explicitly in the simplified expres-
sion.
To calculate the loss of precision, we defined
Equation 1.
error =
(simplifiedNE ? originalNE)
originalNE
(1)
130
Figure 1: Screenshot of part of the questionnaire.
The set of pairings of input expression and ob-
served simplification strategies, loss of precision and
use of hedges as found in the results of the survey is
given in Tables 1, 2 and 3. For each input numer-
ical expression, the set of available simplification
strategies is represented as three lines in the table.
For each pairing, three columns are shown in the
table. Empty cells represent that the strategy was
not used. The first column presents the relative fre-
quency of usage with respect to the total set of possi-
ble simplification strategies used for that expression.
The second column captures the loss of precision in-
volved, represented in terms of the ratio between the
value of the difference between the original numer-
ical value in the input expression and the numerical
value that is conveyed by the corresponding simpli-
fied expression (using Equation 1). This ratio is also
expressed as a percentage. The third column indi-
cates the percentage of simplified numerical expres-
sions that contained a hedge. All of them are mean
values.
Each line represents one kind of simplification
strategy used to simplify the original numerical ex-
pression. Another point to explain is that frequen-
cies that belong to the same expression do not al-
ways add up to 100%. This is because a small num-
ber of others kinds of simplification strategies, like
deletions or rewriting of the whole sentence, are not
shown in the table. Moreover, we must keep in mind
that not all participants answered each question of
the survey.
Table 1 presents the relationships identified be-
tween the original numerical expressions and the
simplification strategies (presented as lines) for the
results of the first part of the survey (simplification
of numerical expressions for a person who can not
understand percentages). All the values are repre-
sented in percentages. Table 2 represents the same
data for the second part of the survey (simplification
of numerical expressions for a person who can not
understand decimals) and Table 3 for the third part
(free simplification of numerical expressions for a
person with poor numeracy).
In the three parts of the survey, the percentage of
simplifications that use hedges is slightly higher than
that of those not using hedges especially in the sec-
ond and third part of the survey. Adapting original
numerical expressions by inserting hedges accounts
for more than the 50% of cases. This reinforces
our assumption that simplifications involving loss of
precision may be better understood if an appropriate
hedge is used.
4.1 Analysis of the Use of Hedges in the
Simplified Numerical Expressions
In order to test hypothesis H1 (the use of hedges
in the simplified numerical expression is influenced
by the simplification strategy selected), we carried
out a series of two sample t-tests where statistical
significance was adjusted for multiple comparisons
by using the Bonferroni correction. Results are pre-
sented in Table 4. When considering the entire sur-
vey (Whole column), there is no significant differ-
ence in the use of hedges in fractions and percent-
ages. When analyzing the survey by parts we find
similar results. There is no significant difference in
the use of hedges in any strategy in the second (no
decimals) and the third (free simplification) parts of
131
Num. Exp. Frequency (%) Error (%) Hedge (%)
Fractions 18 0 67
more than 1% Ratios 6 0 100
Percentages 18 17 50
Fractions 6 0 50
2% Ratios 18 -1 17
Percentages 12 0 0
Fractions 26 1 67
16.8% Ratios 65 5 45
Percentages 9 -3 0
Fractions 82 -4 86
27% Ratios 12 8 75
Percentages 6 6 50
Fractions 41 0 93
at least 30% Ratios 35 13 67
Percentages 3 0 100
Fractions 53 12 50
40% Ratios 29 0 10
Percentages 6 0 0
Fractions 82 -13 82
56% Ratios
Percentages 6 -5 50
Fractions 74 -3 84
63% Ratios 24 0 75
Percentages 3 0 0
Fractions 32 0 0
75% Ratios 29 0 0
Percentages
Fractions 3 0 0
97.2% Ratios 38 -8 23
Percentages 18 1 50
Fractions 6 0 0
98% Ratios 12 0 0
Percentages 3 0 0
Fractions 39 -1 53
Average Ratios 24 2 41
Percentages 7 1 30
Table 1: Analysis of the data for 34 participants from the
first part of the survey (simplifications intended for peo-
ple who do not understand percentages). All values are
percentages. The first column represents the frequencies
of use for each simplification strategy. The second col-
umn shows the error as the loss of precision involved in
the simplification. And the last column displays the use
of hedges in the simplifications.
the survey, but in the first part (no percentages) we
find significant difference between fractions and ra-
tios (p<0.0006). These results do not support the
hypothesis, as there is not a direct relation between
the use of hedges and the selected strategy.
We performed another t-test adjusted by using the
Bonferroni correction on the simplification strate-
gies and central and peripheral values to test hypoth-
esis H2 (the use of hedges to simplify the numerical
expression is influenced by the value of the propor-
tion, with values in the central range (say 0.2 to 0.8)
and values at the extreme ranges (say 0.0-0.2 and
0.8-1.0) having a different use of hedges). In this
case there is also no significant difference. The re-
sults show that the use of hedges is not influenced by
central and peripheral values, rejecting our hypoth-
esis H2 with a p-value p=0.77 in the worst case for
the percentages strategy.
A new t-test adjusted by using the Bonferroni cor-
Num. Exp. Frequency (%) Error (%) Hedge (%)
Fractions 6 25 50
0.6% Ratios 9 22 33
Percentages 47 21 100
Fractions 3 -29 0
2.8% Ratios 24 6 63
Percentages 47 7 63
Fractions
6.1% Ratios 18 -4 50
Percentages 50 -3 82
Fractions 12 9 75
7.5% Ratios 12 -10 0
Percentages 50 7 41
Fractions 15 -1 80
15.5% Ratios 12 6 50
Percentages 44 2 33
Fractions 15 -3 100
25.9% Ratios 12 -3 75
Percentages 38 5 62
Fractions 3 0 0
29.1% Ratios 15 3 60
Percentages 50 2 71
Fractions 12 -5 100
35.4% Ratios 15 -4 60
Percentages 41 -1 71
Fractions 44 -2 93
50.8% Ratios 3 0 0
Percentages 21 0 43
Fractions 44 1 93
73.9% Ratios 6 1 50
Percentages 18 0 50
Fractions 3 0 0
87.8% Ratios 15 -1 60
Percentages 47 1 88
Fractions 3 0 0
96.9% Ratios 12 -2 75
Percentages 29 0 80
Fractions 6 0 50
96.9% Ratios 18 -1 67
Percentages 21 0 86
Fractions 3 0 0
97.2% Ratios 18 -1 67
Percentages 41 0 93
Fractions 3 0 0
97.2% Ratios 18 -1 83
Percentages 32 0 91
Fractions 3 0 0
98.2% Ratios 15 -2 40
Percentages 44 0 67
Fractions 11 0 43
Average Ratios 14 1 52
Percentages 39 2 70
Table 2: Analysis of the data for 34 participants from
the second part of the survey (simplifications intended for
people who do not understand decimals). All values are
percentages. The first column represents the frequencies
of use for each simplification strategy. The second col-
umn shows the error as the loss of precision involved in
the simplification. And the last column displays the use
of hedges in the simplifications.
rection was done to test hypothesis H3 (the loss of
precision allowed for the simplified numerical ex-
pression is influenced by the simplification strategy
selected). Table 5 shows significant differences be-
tween each simplification strategy and each kind of
simplification. In the Whole column we can observe
that the loss of precision in fractions is significantly
different to the one in ratios and percentages. In the
first part (no percentages) there is a significant dif-
ference between ratios and the rest of simplification
strategies. In the second part (no decimals) there is
132
Num. Exp. Frequency (%) Error (%) Hedge (%)
Fractions
0.7% Ratios 6 43 100
Percentages 9 43 100
Fractions 6 -17 100
12% Ratios 21 -8 71
Percentages 21 -17 100
Fractions 41 -4 57
26% Ratios 12 -4 50
Percentages
Fractions 41 -8 86
36% Ratios 9 -2 67
Percentages
Fractions 41 -6 50
53% Ratios
Percentages 6 -6 50
Fractions 21 -5 100
65% Ratios 18 -1 33
Percentages 3 0 0
Fractions 15 0 20
75% Ratios 9 0 33
Percentages 3 0 0
Fractions
91% Ratios 29 -1 50
Percentages 6 -1 50
Fractions
above 97% Ratios 32 0 64
Percentages 6 2 100
Fractions 18 -7 69
Average Ratios 15 3 59
Percentages 6 3 57
Table 3: Analysis of the data for 34 participants from the
third part of the survey (free simplification intended for
people with poor literacy). All values are percentages.
The first column represents the frequencies of use for
each simplification strategy. The second column shows
the error as the loss of precision involved in the simplifi-
cation. And the last column displays the use of hedges in
the simplifications.
no significant difference between any strategy. And
in the last part (free simplification) there is only a
significant difference between fractions and ratios.
These results seem not to support the hypothesis,
as there is not a direct relation between the use of
hedges and the loss of precision in the simplified nu-
merical expression.
For hypothesis H4 (there is some kind of corre-
lation between the loss of precision and the use of
hedges), we looked for correlations between each
part of the survey and each kind of simplification
strategy. We carried out a non-parametric measure
of statistical dependence between the two variables
(loss of precision and use of hedges) calculated by
the Spearman?s rank correlation coefficient.
In general, the results show no correlation, so
there is no linear dependence between the loss of
precision in the strategy and use of hedges, rejecting
our hypothesis. For example, there are cases with
a weak correlation (e.g. in the second part of the
survey for fractions with r=0.49, N=17 and p=0.03),
and cases where there is a strong correlation (e.g.
in the third part of the survey, with r=1, N=18 and
p<.0001).
Finally, when we analyzed hypothesis H5 (when
writers choose numerical expressions for readers
with low numeracy, they do not tend to use hedges if
they are not losing precision), we worked with each
part of the survey to study the cases where the loss
of precision is zero and what is the tendency of use
of hedges.
? In the first part of the survey (simplification
of numerical expressions for a person who can
not understand percentages), considering our
34 participants, in a 46% of responses the loss
of precision is zero, and for these cases only
11% used hedges.
? For the second part (simplification of numeri-
cal expressions for a person who can not un-
derstand decimals), considering our 34 partici-
pants, in a 16% of responses the loss of preci-
sion is zero and for these cases only 7% used
hedges.
? And finally, in the last part (simplification of
numerical expressions for a person with poor
numeracy), considering the same participants,
in a 23% of cases the loss of precision is zero
in the simplification and for these cases only
6% used hedges.
With this data, it seems that we can accept hypoth-
esis H5, that is, we found evidence for our assump-
tion that when writers choose numerical expressions
for readers with poor numeracy, they tend to use
hedges when they round the original numerical ex-
pression, i.e when the loss of precision is not zero.
4.2 Original Numerical Expressions with
Hedges
In our survey there were a few cases where the orig-
inal numerical expression had a hedge. We have
observed that if the original numerical expression
has hedge almost always the simplified numerical
expression contained a hedge. There is a special
case, ?above 97%? where we do not count the use
of hedges because in this case the participants chose
non-numeric options mostly and they rewrote the
numerical expression with phrases like ?around all?.
133
Strategy No Pct. No Dec. Free Simp. Whole
Fractions A A A A
Percentages A B A A A
Ratios B A A B
Table 4: Results of t-test adjusted by Bonferroni correction for H1 (the use of hedges in simplified numerical ex-
pressions is influenced by the simplification strategy selected). Strategies which do not share a letter are significantly
different.
Strategy No Pct. No Dec. Free Simp. Whole
Fractions A A A A
Percentages A A A B B
Ratios B A B B
Table 5: Results of t-test adjusted by Bonferroni correction for H3 (the loss of precision allowed for the simplified
numerical expression is influenced by the simplification strategy selected). Strategies which do not share a letter are
significantly different.
In the remaining cases, the same hedge is nearly al-
way chosen to simplify the numerical expression.
4.3 Kinds of Hedges
With respect to the actual hedges used, we have
identified two different possible roles of hedge in-
gredients in a numerical expression. In some cases,
hedges are used to indicate that the actual numeri-
cal value given is an approximation to the intended
value. Uses of about or around are instances of this.
This kind of hedge is employed to indicate explic-
itly that some loss of precision has taken place dur-
ing simplification. In other cases, hedges are used to
indicate the direction in which the simplified value
diverges from the original value. Uses of under or
over are instances of this. In some cases more than
one hedge may be added to an expression to indi-
cate both approximation and direction, or to some-
how specify the precision involved in the simplifica-
tion, as in just under or a little less than.
In our analysis we studied which hedges were
the most frequent in each part of the survey. Only
hedges with more than ten appearances in total (in-
cluding simplification strategies not present in the
table) have been considered in Table 6. We observed
that the three parts of the survey have three hedges
in common: about, just over and over. They are
used in different strategies for each kind of simpli-
fication. In the second part of the survey, where
simplifications of numerical expressions for a per-
son who can not understand decimals are done, is
where more hedges are used, in special for percent-
ages strategy. In the last part of the survey, where
there is more freedom to decide how simplify the
original numerical expression, participants used less
hedges compare to the others parts.
No Percentages
Hedge Fractions Ratios Percent.
about 15 9 0
at least 8 5 1
just over 21 1 0
more than 9 3 0
over 6 3 2
Total 59 21 3
No Decimals
Hedges Fractions Ratios Percent.
about 8 12 6
almost 4 1 8
just over 13 3 39
just under 3 2 27
nearly 7 5 24
over 7 5 9
Total 42 28 113
Free Simplification
Hedges Fractions Ratios Percent.
about 6 5 1
just over 6 0 5
more than 4 5 0
nearly 4 0 2
over 11 2 3
Total 31 12 11
Table 6: Use of the most frequent hedges in each part of
the survey
134
5 Discussion
As can be seen in the results, the use of hedges to
simplify numerical expressions can be influenced by
three parameters. The first is the kind of simplifica-
tion. Our survey was divided in three parts depend-
ing on the mathematical knowledge of the final user.
The second is the simplification strategy for choos-
ing mathematical form (fractions, ratios, or percent-
ages). In our data we observed some differences in
the usage of hedges with ratios and their usage with
fractions and percentages (see Table 4). The last pa-
rameter is the loss of precision that occurs when the
numerical expression is rounded. We investigated
the use of hedges vs. loss of precision with different
tests hoping to define some dependencies, but there
was no clear correlation between them, and it was
only when we tried a deeper analysis of strategies
and kind of simplifications that we found some cor-
relations such as those we presented in Section 4.1.
When asked to simplify for people who do not
understand percentages, or for people with poor nu-
meracy, the participants use different simplification
strategies and sometimes they use hedges to simplify
the original numerical expression. As some partic-
ipants commented, not only are percentages mathe-
matically sophisticated forms, but they may be used
in sophisticated ways in the text, often for example
describing rising and falling values, for which in-
creases or decreases can themselves be described in
percentages terms. Such complex relationships are
likely to pose problems for people with poor numer-
acy even if a suitable strategy can be found for sim-
plifying the individual percentages. In some of the
examples with more than one numerical expression
being compared, some of the evaluators reported a
tendency to phrase them both according to a com-
parable base. Thus we should consider the role of
context (the set of numerical expressions in a given
sentence as a whole, and the meaning of the text) in
establishing what simplifications must be used.
6 Conclusions and Future Work
Through a survey administered to experts on nu-
meracy, we have collected a wide range of exam-
ples of appropriate simplifications of percentage ex-
pressions. These examples of simplified expressions
give us information about the use of hedges that our
participants carry out to adapt the original numer-
ical expression to be understood by the final user.
We investigated the loss of precision that occurs with
each hedge and the relation between the simplifica-
tion strategy and the use of hedges.
Our aim is to use this data to guide the develop-
ment of a system for automatically simplifying per-
centages in texts. With the knowledge acquired from
our study we will improve our algorithm to simplify
numerical expressions. We could determinate from
the simplification strategy, kind of simplification and
the loss of precision allowed, which will be the best
option to adapt the original numerical expression to
the final user and if that option uses hedges to under-
stand better the original numerical expression. As a
part of our algorithm, we will have to look at inter-
rater agreements for identifying appropriate hedges.
As future work, we plan to carry out another study
to determine a ranking of simplification strategies
from collecting a repertoire of rewriting strategies
used to simplify. This data should allow us to deter-
mine whether common values are considered sim-
pler and whether the value of the original expression
influences the chosen simplification strategy. So,
given a numerical expression, we could choose what
simplification strategy to apply and whether to insert
a hedge. We could investigate whether the value of
the original proportion also influences choices, de-
pending on its correspondence with central or pe-
ripheral values.
We have also collected a parallel corpus of numer-
ical expressions (original vs. simplified version).
This corpus will be shared with other researches so
it can be used in different applications to improve
the readability of text. This could be a very use-
ful resource because simplification of percentages
remains an interesting and non-trivial problem.
Acknowledgments
This research is funded by the Spanish Ministry
of Education and Science (TIN2009-14659-C03-01
Project), Universidad Complutense de Madrid and
Banco Santander Central Hispano (GR58/08 Re-
search Group Grant), and the FPI grant program.
135
References
Stephanie Schinzing Marsiglio Ann M. Bisantz and Jes-
sica Munch. 2005. Displaying uncertainty: Inves-
tigating the effects of display format and specificity.
Human Factors: The Journal of the Human Factors
and Ergonomics Society, 47(4):777.
J. Carroll, G. Minnen, Y. Canning, S. Devlin, and J. Tait.
1998. Practical simplification of English newspaper
text to assist aphasic readers. In AAAI-98 Workshop on
Integrating Artificial Intelligence and Assistive Tech-
nology, Madison, Wisconsin.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and Methods for Text
Simplification. In COLING, pages 1041?1044.
Paul Slovic Ellen Peters, Judith Hibbard and Nathan
Dieckmann. 2007. Numeracy skill and the commu-
nication, comprehension, and use of risk-benefit infor-
mation. Health Affairs, 26(3):741?748.
Shiv B. Mishra H, Mishra A. 2011. In praise of vague-
ness: malleability of vague information as a perfor-
mance booster. Psychological Science, 22(6):733?8,
April.
Paul Slovic Nathan F. Dieckmann and Ellen M. Peters.
2009. The use of narrative evidence and explicit like-
lihood by decisionmakers varying in numeracy. Risk
Analysis, 29(10).
The United Nations. 1994. Normas uniformes sobre la
igualdad de oportunidades para las personas con dis-
capacidad. Technical report.
Sarah E. Petersen and Mari Ostendorf. 2007. Text Sim-
plification for Language Learners: A Corpus Analy-
sis. Speech and Language Technology for Education
(SLaTE).
Qualification and Curriculum Authority. 1999. Mathe-
matics: the national curriculum for england. Depart-
ment for Education and Employment, London.
Advaith Siddharthan. 2002. Resolving Attachment and
Clause Boundary Amgiguities for Simplifying Rela-
tive Clause Constructs. In Proceedings of the Student
Research Workshop, 40th Meeting of the Association
for Computacional Linguistics.
Sandra Williams and Richard Power. 2009. Precision
and mathematical form in first and subsequent men-
tions of numerical facts and their relation to document
structure. In Proceedings of the 12th European Work-
shop on Natural Language Generation, Athens.
Sandra Williams and Ehud Reiter. 2005. Generating
readable texts for readers with low basic skills. In
Proceeding of the 10th European Workshop on Natu-
ral Language Generation, pages 140?147, Aberdeen,
Scotland.
Joel Williams, Sam Clemens, Karin Oleinikova, and
Karen Tarvin. 2003. The Skills for Life survey: A
national needs and impact survey of literacy, numer-
acy and ICT skills. Technical Report Research Report
490, Department for Education and Skills.
136
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 39?48,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
A System for the Simplification of Numerical Expressions at Different Levels
of Understandability
Susana Bautista, Raquel Herva?s,
Pablo Gerva?s
Universidad Complutense de Madrid
Prof. Jose? Garc??a Santesmases
Madrid, Spain
{subautis,raquelhb}@fdi.ucm.es
pgervas@sip.ucm.es
Richard Power, Sandra Williams
Department of Computing,
The Open University
Milton Keynes,
MK76AA, UK
r.power@open.ac.uk
s.h.williams@open.ac.uk
Abstract
The purpose of this paper is to motivate and
describe a system that simplifies numerical
expression in texts, along with an evaluation
study in which experts in numeracy and liter-
acy assessed the outputs of this system. We
have worked with a collection of newspaper
articles with a significant number of numerical
expressions. The results are discussed in com-
parison to conclusions obtained from a prior
empirical survey.
1 Introduction
A surprisingly large number of people have limited
access to information because of poor literacy. The
most recent surveys of literacy in the United King-
dom reveal that 7 million adults in England can-
not locate the reference page for plumbers if given
the Yellow Pages alphabetical index. This means
that one in five adults has less literacy than the ex-
pected literacy in an 11-year-old child (Jama and
Dugdale, 2010; Williams et al, 2003a; Christina and
Jonathan, 2010). Additionally, almost 24 million
adults in the U.K. have insufficient numeracy skills
to perform simple everyday tasks such as paying
household bills and understanding wage slips. They
would be unable to achieve grade C in the GCSE
maths examination for 16-year-old school children
(Williams et al, 2003a).
?The Standard Rules on the Equalization of Op-
portunities for Persons with Disabilities? by United
Nations (1994) state that all public information ser-
vices and documents should be accessible in such
a way that they could be easily understood. If we
focus on numerical information, nowadays, a large
percentage of information expressed in daily news
or reports comes in the form of numerical expres-
sions (economic statistics, demography data, etc)
but many people have problems understanding the
more complex expressions. In the text simplification
process, different tasks are carried out: replacing
difficult words, splitting sentences, etc., and the sim-
plification of numerical expressions is one of them.
A possible approach to solve this important social
problem of making numerical information accessi-
ble is to rewrite difficult numerical expressions using
alternative wordings that are easier to understand.
For example, the original sentence, ?25.9% scored A
grades? could be rewritten by ?Around 26% scored
A grades?. In our study we define a ?numerical ex-
pression? as a phrase that presents a quantity, some-
times modified by a numerical hedge as in these ex-
amples: ?less than a quarter? or ?about 98%?. Such
an approach would require a set of rewriting strate-
gies yielding expressions that are linguistically cor-
rect, easier to understand than the original, and as
close as possible to the original meaning. Some loss
of precision could have positive advantages for nu-
merate people as well as less numerate. In rewrit-
ing, hedges play also an important role. For exam-
ple, ?50.9%? could be rewritten as ?about a half? us-
ing the hedge ?about?. In this kind of simplification,
hedges indicate that the original number has been
approximated and, in some cases, also the direction
of the approximation.
This paper presents a system developed for auto-
mated simplification of numerical expressions. Ex-
perts in simplification tasks are asked to validate the
39
simplifications done automatically. The system is
evaluated and the results are discussed against con-
clusions obtained from previous empirical survey.
2 Previous work
Text simplification, a relative new task in Natural
Language Processing, has been directed mainly at
syntactic constructions and lexical choices that some
readers find difficult, such as long sentences, pas-
sives, coordinate and subordinate clauses, abstract
words, low frequency words, and abbreviations.
The rule-based paradigm has been used in the
implementation of some systems for text simpli-
fication, each one focusing on a variety of read-
ers (with poor literacy, aphasia, etc) (Chandrasekar
et al, 1996; Siddharthan, 2003; Jr. et al, 2009;
Bautista et al, 2009).
The transformation of texts into easy-to-read ver-
sions can also be phrased as a translation problem
between two different subsets of language: the orig-
inal and the easy-to-read version. Corpus-based sys-
tems can learn from corpora the simplification oper-
ations and also the required degree of simplification
for a given task (Daelemans et al, 2004; Petersen
and Ostendorf, 2007; Gasperin et al, 2009).
A variety of simplification techniques have been
used, substituting common words for uncommon
words (Devlin and Tait, 1998), activating passive
sentences and resolving references (Canning, 2000),
reducing multiple-clause sentences to single-clause
sentences (Chandrasekar and Srinivas, 1997; Can-
ning, 2000; Siddharthan, 2002) and making appro-
priate choices at the discourse level (Williams et al,
2003b). Khan et at. (2008) studied the tradeoff be-
tween brevity and clarity in the context of generat-
ing referring expressions. Other researchers have fo-
cused on the generation of readable texts for readers
with low basic skills (Williams and Reiter, 2005),
and for teaching foreign languages (Petersen and
Ostendorf, 2007).
Previous work on numerical expressions has stud-
ied the treatment of numerical information in differ-
ent areas like health (Peters et al, 2007), forecast
(Dieckmann et al, 2009), representation of proba-
bilistic information (Bisantz et al, 2005) or vague
information (Mishra et al, 2011). In the NUM-
GEN project (Williams and Power, 2009), a corpus
of numerical expressions was collected and a for-
mal model for planning specifications for propor-
tions (numbers between 0 and 1) was developed.
The underlying theory and the design of the work-
ing program are described in (Power and Williams,
2012).
3 Experimental identification of
simplification strategies for numerical
information
In order to analyze different simplification strategies
for numerical expressions, first we have to study the
mathematical complexity of the expressions. Ex-
pressions can be classified and a level of difficulty
can be assigned. A study about the simplification
strategies selected by experts to simplify numerical
expressions expressed as decimal percentages in a
corpus was carried out in Bautista et al (2011b).
Other important aspect of the simplification task is
the use of hedges to simplify numerical expressions
in the text. A study was performed in Bautista et
al. (2011a) to analyze the use of hedges in the sim-
plification process. This study was done with ex-
perts in simplification tasks. A set of sentences with
numerical expressions were presented and they had
to rewrite the numerical expressions following some
rules. Several hypotheses were expressed and an-
alyzed to understand experts? preferences on sim-
plification strategies and use of hedges to simplify
numerical expressions in the text. The main conclu-
sions from the study were:
Conclusion 1: When experts choose expressions
for readers with low numeracy, they tend to prefer
round or common values to precise values. For ex-
ample, halves, thirds and quarters are usually pre-
ferred to eighths or similar, and expressions like N
in 10 or N in 100 are chosen instead of N in 36.
Conclusion 2: The value of the original propor-
tion influences the choice of simplification strategies
(fractions, ratios, percentages). With values in the
central range (say 0.2 to 0.8 in a 0.0 to 1.0 scale)
and values at the extreme ranges (say 0.0-0.2 and
0.8-1.0) favoring different strategies.
Conclusion 3: When writers choose numerical
expressions for readers with low numeracy, they
only use hedges if they are losing precision.
40
4 A system for adapting numerical
expressions
In this first prototype, only numerical expressions
defined as percentages are adapted. From an in-
put text, the percentage numerical expressions are
detected, a target level of difficulty is chosen and
the simplified version of the text is generated by re-
placing the original numerical expression with the
adapted expression.
4.1 Numerical expression
A numerical expression consists of: (1) a numerical
value, a quantity which may be expressed with dig-
its or with words; (2) an optional unit accompanying
the quantity (euro, miles, . . . ); and (3) an optional
numerical hedge modifier (around, less than, . . . ).
Some examples of numerical expressions used in
our experiments are: ?more than a quarter?, ?around
98.2%?, ?just over 25 per cent? or ?less than 100 kilo-
metres?.
4.2 Levels of difficulty
The Mathematics Curriculum of the Qualifications
and Curriculum Authority (1999) describes a num-
ber of teaching levels and we assume that concepts
to be taught at lower levels will be simpler than ones
taught at higher levels. Following this idea a Scale of
Mathematic Concepts is defined to identify the dif-
ferent levels of difficulty to understand mathematic
concepts. The scale defined from less to greater dif-
ficulty is: numerical expression in numbers (600),
words (six), fractions (1/4), ratios (1 in 4), percent-
ages (25%) and decimal percentages (33.8%).
From the Scale of Mathematic Concepts defined,
different levels of difficulty are considered in our
system. There are three different levels (from eas-
iest to hardest):
1. Fractions Level: each percentage in the text is
adapted using fractions as mathematical form
for the quantity, and sometimes a hedge is used.
2. Percentages without decimals Level (PWD):
the system rounds the original percentage with
decimals and uses hedges if they are needed.
3. Percentages with decimals Level: This is the
most difficult level where no adaptation is per-
formed.
The system operates only on numerical expres-
sions at the highest levels of the scale (the most dif-
ficult levels), that is, numerical expression given in
percentages or decimal percentages, adapting them
to other levels of less difficulty. So, the user can
select the level to which adapt the original numeri-
cal expression from the text. Using the interface of
the system, the level of difficulty is chosen by the fi-
nal user and the numerical expressions from the text
with higher level of difficulty than the level chosen
are adapted following the rules defined.
4.3 Set of strategies
A set of strategies is defined so they can be applied to
adapt the original numerical expression. The quan-
tity of the expression is replaced with another ex-
pression and sometimes numerical hedges are added
to create the simplified numerical expression.
The use of hedges to simplify numerical expres-
sion can be influenced by three parameters. The first
is the type of simplification depending on the math-
ematical knowledge of the final user. The second is
the simplification strategy for the choice of the final
mathematical form. And the last is the loss of preci-
sion that occurs when the expression is simplified.
Out of the European Guidelines for the Produc-
tion of Easy-to-Read Information for People with
Learning Disability (Freyhoff et al, 1998), only one
involves the treatment of numbers: ?Be careful with
numbers. If you use small numbers, always use the
number and not the word?. For example, if the texts
says ?four?, the system adapts it by ?4? following this
European Guideline. This strategy is applied by the
system at all levels.
There are other strategies to adapt numerical ex-
pressions in the form of percentage to other levels of
difficulty: (1) replace decimal percentages with per-
centages without decimals; (2) replace decimal per-
centages with ratios; (3) replace percentages with ra-
tios; (4) replace decimal percentages with fractions;
(5) replace percentages with fractions; (6) replace
ratios with fractions; (7) replace numerical expres-
sions in words with numerical expressions in digits.
At each level of difficulty, a subset of the strate-
gies is applied to simplify the numerical expression.
For the Fractions Level the strategies 4, 5 and 7
are used. For the Percentages with decimals Level
the strategies 1 and 7 are applied. And for the last
41
level, Percentages without decimals Level only the
last strategy, number 7, is used.
4.4 System operation
The system takes as input the original text. The user
of the system has to choose the level of difficulty. A
set of numerical expressions are selected and a set
of transformations is applied to adapt them, generat-
ing as output of the system a text with the numerical
expressions simplified at the chosen level.
The system works through several phases to adapt
the numerical expressions in the input text. Some of
them are internal working phases (2, 4 and 5). The
rest of them (1, 3 and 6) are phases where the user
of the system plays a role. The phases considered in
the system are:
1. Input text: an original text is selected to adapt
its numerical expressions.
2. Mark Numerical Expressions: the numerical
expressions that can be adapted are marked.
3. Choose the level of difficulty: the user chooses
the desired level of difficulty for the numerical
expressions in the text.
4. Adapt the numerical expression from the
text: each numerical expression is adapted if
the level of the numerical expression is higher
than the level of difficulty chosen.
5. Replace numerical expression in the text:
adapted numerical expressions replace the orig-
inals in the text.
6. Output text: the final adapted version of the
text is presented to the user.
The next subsections presents how the system acts
in each phase and what kind of tools are used to
achieve the final text.
4.4.1 Phase 1: Input text
In this first phase, a plain text is chosen as input to
the system to adapt its numerical expressions. Using
a Graphical User Interface (GUI) in Java, the user
can upload an original text.
4.4.2 Phase 2: Mark numerical expressions
For the text chosen, the system executes the Nu-
merical Expression Parser1. Using this parser the
numerical quantities are annotated with their type
(cardinal, fraction, percentage, decimal percentage,
etc.), their format (words, digits), their value (Vg),
their units, and hedging phrases, such as ?more
than?. The input to the program is the plain text file
and the output is the text with sentences and numer-
ical expressions annotated in XML format. In the
following code we can see how a numerical quantity
is annotated in the parser.
Overall figures showed the national pass
rate soared
<numex hedge=?above? hedge-
sem=?greaterthan? type=?percentage?
format=?digits? Vg=?0.97?>
above 97% </numex>
The XML file is treated by the system and numer-
ical expressions are marked in the original text. So,
the user can see which numerical expressions are go-
ing to be adapted by the system (in the next phase)
depending on the level of difficulty chosen.
4.4.3 Phase 3: Choose the level of difficulty
The user of the system chooses the level of dif-
ficulty to adapt the original numerical expressions.
There are three levels: fractions, percentages with-
out decimals and percentages with decimals.
4.4.4 Phase 4: Adapt the Numerical
Expressions
After deciding the level of difficulty, the system
has to adapt each numerical expression to generate
the final version. The process of simplification has
two stages: obtaining the candidate and applying the
adaptation and hedge choice rules.
From the XML file produced by the parser the fol-
lowing information for a numerical expression is ob-
tained: (1) if there is or not hedge and the kind of
hedge; (2) the type (cardinal, fraction, percentage,
decimal percentage) and format (digits or words)
of the original numerical expression; (3) the given
value (Vg) translated from the original numerical ex-
pression value of the text; and (4) the units from the
1For more details see (Williams, 2010)
42
O
rig
in
al
 
Ex
pr
es
sio
n
Pa
rs
er
Vm
g
Pr
op
or
tio
n
Ap
pr
ox
.
Pr
og
ra
m
Vr
M
or
e 
th
an
 2
8%
0.
28
0.
28
1/
3
0.
33
Vg
Vc
[0
...
1]
[0
...
1]
1/
3
30
%
28
%
Figure 1: Obtaining the candidate for simplification. The original expression is annotated by the parser (Vg), and this
value is normalized (Vmg). A candidate substitute value (Vc) is chosen from the proportion approximation program
and normalized (Vr).
original expression (M, ins, grams). For example,
if in the text the original numerical expression is a
percentage like ?25.9%?, there is no hedge, the type
is ?decimal percentage?, the format is ?digits?, Vg is
0.259 and there are no units. In the expression, ?20
grams?, there is no hedge, the type is ?cardinal?, the
format is ?digits?, Vg is 20 and the parser annotates
the units with ?g?.
The given value Vg annotated by the parser is
transformed into a value between 0 to 1, referred
to as mapping given value (Vmg), which represents
the proportion under consideration. This value is
given as input to the proportion approximation pro-
gram (Power and Williams, 2012), which returns a
list of candidates for substitution. From this list,
the first option is taken as candidate substitute value
(Vc), because the program returns them in decreas-
ing order of precision. This means that the most
precise candidate at the required level of difficulty
is chosen. The program also might return the val-
ues ?none? and ?all? if the input value is close to
0 or 1, respectively. From the Vc we calculate the
rounded value (Vr) corresponding to the normaliza-
tion of the candidate value between 0 to 1. For ex-
ample, if Fraction level is chosen, for the original
expression ?more than 28%? with Vmg=0.28, the
system chooses Vc=1/3 with Vr=0.33. The whole
process can be seen in Figure 1.
An additional level of adaptation is required be-
yond simple replacement with the candidate substi-
tute value. If the original numerical expressions in
the text are difficult to understand, the system must
adapt them to the desired level of difficulty. For each
numerical expression, the system only applies the
adaptation rules if the difficulty level of the numer-
ical expression is higher than the level of difficulty
chosen by the user. This is captured by a set of three
adaptation rules:
? If the type of the numerical expression is ?car-
dinal? and the format is ?words? then the candi-
date to be used in the simplification is Vg. For
example, if the original numerical expression is
?six?, it will be replaced by ?6?.
? In a similar way, if the type is ?fraction? (the
lowest possible level of difficulty) and the for-
mat is also ?words? then the candidate is ob-
tained by applying the proportion approxima-
tion program. For example, if the original nu-
merical expression is ?a quarter?, it would be
replaced by ?1/4?.
? If the type is ?percentages? or ?decimal percent-
ages? and the format is ?digits? then the can-
didate is calculated by the proportion approxi-
mation program provided that the level of dif-
ficulty chosen in the GUI was lower than the
level of the calculated numerical expression.
In order to complete the simplification, the system
has to decide if a hedge should be used to achieve
the final version of the adapted numerical expres-
sion. This decision is taken based on the difference
in value between the value of the original expression
in the text (Vg) and the value of the candidate substi-
tute (Vc) (as given by the relative difference between
the normalized values Vr and Vmg calculated in the
first stage). The actual hedge used in the original
expression (if any) is also considered. The various
possible combinations of these values, and the corre-
sponding choice of final hedge, are described in Ta-
ble 1, which presents all possible options to decide
in each case, the hedge and the value corresponding
to the final numerical expression. For example, if
the original expression is ?more than 28%?, we have
Vc=1/3, Vmg=0.28 and Vr=0.33. Then Vr>Vmg so
the corresponding choice of the final hedge is in the
43
OriginalNumExp if Vr>Vmg if Vr=Vmg if Vr<Vmg
more than OrigValue around Vc more than Vc more than Vc
exactly OrigValue less than Vc exactly Vc more than Vc
less than OrigValue less than Vc less than Vc around Vc
OrigValue around Vc Vc around Vc
Table 1: Hedge Choice Rules. For each original expression (OrigValue), the normalized values (Vmg, Vr) are used to
determinate the hedge chosen for the simplified expression. The final version is composed by the hedge chosen and
the candidate value (Vc)
first column of Table 1 (?around?) and the simplified
expression is ?around 1/3?.
When the user chooses the Fraction Level in the
system, every numerical expression with difficulty
level greater than fraction level will be replaced by
a numerical expression expressed in fraction form.
Depending on the values Vr and Vmg, the appropri-
ate hedge will be chosen.
4.4.5 Phase 5: Replace numerical expressions
Once the system has applied its rules, an adapted
version is available for each original numerical ex-
pression which was more difficult than the target dif-
ficulty level. The output text is obtained by replac-
ing these difficult expressions with the correspond-
ing simplified version.
5 Evaluation of the system
This section presents the evaluation of the system,
describing the materials, experiment, participants
and results of the evaluation.
5.1 Materials
We selected for the experiment a set of eight can-
didate sentences from the NUMGEN corpus, but the
number of numerical expressions was larger as some
sentences contained more than one proportion ex-
pression. In total we had 13 numerical expressions.
We selected sentences with as many variations in
context, precision and different wordings as possi-
ble. The range of proportions values was from points
nearly 0.0 to almost 1.0, to give coverage to a wide
spread of proportion values. We considered values
in the central range (say 0.2 to 0.8) and values at the
extreme ranges (say 0.0-0.2 and 0.8-1.0). We also
classified as common values the well-known per-
centages and fractions like 25%, 50%, 1/4 and 1/2,
and as uncommon values the rest like 15% or 6/7.
5.2 Experiment
To evaluate the system a questionnaire was pre-
sented to a set of human evaluators. The experi-
ment was created and presented on SurveyMonkey2,
a commonly-used provider of web surveys. For each
original sentence, we presented two possible simpli-
fications generated by the system. Participants were
asked to use their judgement to decide whether they
agreed that the simplified sentences were acceptable
for the original sentence. A Likert scale of four val-
ues (Strongly Disagree, Disagree, Agree, Strongly
Agree) was used to collect the answers.
In the survey only two levels of adaptation from
the original sentence were presented. The first op-
tion generated by the system was for the Fractions
level. The second option generated by the system
was for the Percentages without decimals (PWD).
5.3 Participants
The task of simplifying numerical expressions is dif-
ficult, so we selected a group of 34 experts made up
of primary or secondary school mathematics teach-
ers or adult basic numeracy tutors, all native English
speakers. This group is well qualified to tackle the
task since they are highly numerate and accustomed
to talking to people who do not understand mathe-
matical concepts very well. We found participants
through personal contacts and posts to Internet fo-
rums for mathematics teachers and numeracy tutors.
5.4 Results
The answers from the participants were evaluated.
In total we collected 377 responses, 191 responses
for the Fraction level and 186 responses for the Per-
centage without decimals (PWD). Table 2 shows the
average from the collected responses, considering 1
2http://www.surveymonkey.com/s/WJ69L86
44
Level Total average Values Average Values Average
Fraction 2,44
Central 2,87 Common 2,59
Extreme 2,14 Uncommon 1,21
PWD 2,96
Central 3,00 Common 2,80
Extreme 2,96 Uncommon 3,22
Table 2: System Evaluation: Fraction Level and Percentages Without Decimals (PWD)
Opinion Fraction PWD
Level Level
Strongly Disagree 19% 6%
Disagree 27% 15%
Agree 43% 56%
Strongly Agree 11% 23%
Table 3: Opinion of the experts in percentages
to 4 for strongly disagree to strongly agree. In ad-
dition, Table 3 shows the distribution in percentages
of the opinion of the experts. At the Fraction level,
there is not too much difference between the average
of the answers of the experts that agree with the sys-
tem and those that disagree. Most experts are neu-
tral. But for the PWD level the average shows that
most experts agree with the simplification done.
We have also analyzed the answers considering
two different criteria from the original numerical ex-
pressions: when they are central (20% to 80%) or
extreme values (0% to 20% and 80% to 100%), and
when the original numerical expressions are com-
mon or uncommon values. In general terms, the ex-
perts think that the simplification done by the sys-
tem in the PWD level is better than the simplification
done in the Fraction level. They disagree specially
with the simplification using fractions in two cases.
One is the treatment of the extreme values where the
system obtains as possible candidates ?none? and
?all?3. Another case is when uncommon fractions
are used to simplify the numerical expression, like
for example 9/10. In these two cases the average is
lower than the rest of the average achieved.
5.5 Discussion
The system combines syntactic transformations (via
the introduction of hedges) and lexical substitu-
3See (Power and Williams, 2012) for a discussion of appro-
priate hedges for values near the extreme points of 0 and 1.
tions (by replacing actual values with substitution
candidates and transforming quantities expressed as
words into digits) to simplify the original numerical
expression. These kinds of transformations are dif-
ferent from those used by other systems, which rely
only on syntactic transformations or only on lexi-
cal substitutions. Rules are purpose-specific and fo-
cused on numerical expressions. With this kind of
transformations the readability of the text improves
in spite of the fact that the resulting syntactic struc-
ture of the numerical expression is more compli-
cated, due to the possible presence of hedges. For
example, for a original numerical expression like
?25.9%? the system generates the simplified ?more
than a quarter? which is easier to understand even
though longer and syntactically more complex.
With respect to coverage of different types of nu-
merical expressions, this system does not consider
ratios as a possible simplification strategy because
the proportion approximation program does not use
them as candidates to simplify a proportion. This
possibility should be explored in the future.
Another observation is that the system does not
consider the context of the sentence in which the
numerical expression occurs. For example, if the
sentence makes a comparison between two numer-
ical expressions that the system rounded to the same
value, the original meaning is lost. One example
of this case is the following sentence from the cor-
pus: ?One in four children were awarded A grades
(25.9%, up from 25.3% last year)?. Both percent-
ages ?25.9%? and ?25.3%? are simplified by the sys-
tem using ?around 1/4? and the meaning of the sen-
tence is lost. Thus we should consider the role of
context (the set of numerical expressions in a given
sentence as a whole and the meaning of the text) in
establishing what simplifications must be used.
45
6 Conforming with conclusions of prior
surveys
The results presented for the system are evaluated
in this section for conformance with the conclusions
resulting from the empirical studies described in
(Bautista et al, 2011b) and (Bautista et al, 2011a).
With respect to the preference for round or com-
mon values in simplification (Conclusion 1), the sys-
tem presented conforms to this preference by virtue
of the way in which the list of candidate substitu-
tions is produced by the program. The candidates re-
turned by the program are already restricted to com-
mon values of percentages (rounded up) and frac-
tions, so the decision to consider as preferred candi-
date the one listed first implicitly applies the criteria
that leads to this behavior.
With respect to the need to treat differently values
in the extreme or central ranges of proportion (Con-
clusion 2), the system addresses this need by virtue
of the actual set of candidates produced by the pro-
gram in each case. For example, if the original ex-
pression is a extreme value like ?0.972?, the program
produces a different candidate substitution (?almost
all?) that in the central ranges is not considered.
With respect to restricting the use of hedges to
situations where loss of precision is incurred (Con-
clusion 3), the hedge choice rules applied by the
system (see Table 1) satisfy this restriction. When
Vr=Vmg hedges are included in the simplified ex-
pression only if they were already present in the
original expression.
In addition, the system rounds up any quantities
with decimal positions to the nearest whole num-
ber whenever the decimal positions are lost during
simplification. This functionality is provided im-
plicitly by the program, which presents the rounded
up version as the next option immediately follow-
ing the alternative which includes the decimal posi-
tions. For example, if the input proportion is ?0.198?,
some rounded candidate substitutions are calculated
as ?almost 20%? or ?less than 20%?.
Finally, the system follows the European guide-
lines for the production of easy to read information
in that it automatically replaces numerical quantities
expressed in words with the corresponding quantity
expressed in digits.
7 Conclusions and future work
The system described in this paper constitutes a first
approximation to the task of simplifying numerical
expressions in a text to varying degrees of difficulty.
The definition of an scale of difficulty of numeri-
cal expressions, the identification of rules governing
the selection of candidate substitution and the appli-
cation of hedges constitute important contributions.
The empirical evaluation of the system with human
experts results in acceptable rates of agreement. The
behavior of the system conforms to the conclusions
on simplification strategies as applied by humans re-
sulting from previous empirical surveys.
There are different aspects to improve the actual
system from the data collected, with a special atten-
tion to cases in which the experts disagree. As future
work, the syntactic context should be considered to
simplify numerical expression, extending the kind
of proportion to simplify and treating special cases
analyzed in this first version. At the syntactic level,
some transformation rules can be implemented from
a syntactic analysis. It is important that the meaning
of the sentences be preserved regardless of whether
part of the sentence is deleted or rewritten by the
adaptation rules. In addition, the numerical expres-
sion parser and the proportion approximation pro-
gram could also be studied in order to evaluate the
impact of their errors in the final performance.
Our final aim is to develop an automatic simplifi-
cation system in a broader sense, possibly including
more complex operations like syntactic transforma-
tions of the structure of the input text, or lexical sub-
stitution to reduce the complexity of the vocabulary
employed in the text. Additionally we hope to de-
velop versions of the simplification system for other
languages, starting with Spanish. Probably the sim-
plification strategies for numbers would be the same
but the use of hedge modifiers may be different.
Acknowledgments
This research is funded by the Spanish Ministry
of Education and Science (TIN2009-14659-C03-01
Project), Universidad Complutense de Madrid and
Banco Santander Central Hispano (GR58/08 Re-
search Group Grant), and the FPI grant program.
46
References
Susana Bautista, Pablo Gerva?s, and Ignacio Madrid.
2009. Feasibility Analysis for SemiAutomatic Con-
version of Text to Improve Readability. In Proceed-
ings of The Second International Conference on Infor-
mation and Communication Technologies and Acces-
sibility, Hammamet, Tunusia, May.
Susana Bautista, Raquel Herva?s, Pablo Gerva?s, Richard
Power, and Sandra Williams. 2011a. Experimental
identification of the use of hedges in the simplifica-
tion of numerical expressions. In Proceedings of the
Second Workshop on Speech and Language Process-
ing for Assistive Technologies, pages 128?136, Edin-
burgh, Scotland, UK, July. Association for Computa-
tional Linguistics.
Susana Bautista, Raquel Herva?s, Pablo Gerva?s, Richard
Power, and Sandra Williams. 2011b. How to
Make Numerical Information Accessible: Experimen-
tal Identification of Simplification Strategies. In Cam-
pos, Pedro and Graham, Nicholas and Jorge, Joaquim
and Nunes, Nuno and Palanque, Philippe and Winck-
ler, Marco, editor, Human-Computer Interaction IN-
TERACT 2011, volume 6946 of Lecture Notes in Com-
puter Science, pages 57?64. Springer Berlin / Heidel-
berg.
Ann M. Bisantz, Stephanie Schinzing, and Jessica
Munch. 2005. Displaying uncertainty: Investigating
the effects of display format and specificity. Human
Factors: The Journal of the Human Factors and Er-
gonomics Society, 47(4):777.
Yvonne Canning. 2000. Cohesive simplification of
newspaper text for aphasic readers. In 3rd annual
CLUK Doctoral Research Colloquium.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
Knowledge-Based Systems, 10.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text
simplification. In In Proceedings of the Sixteenth In-
ternational Conference on Computational Linguistics
(COLING ?96), pages 1041?1044.
Clark Christina and Douglas Jonathan. 2010. Young
people reading and writing today: Whether, what and
why. Technical report, London: National Literacy
Trust.
Walter Daelemans, Anja Hothker, and Erik Tjong Kim
Sang. 2004. Automatic Sentence Simplification for
Subtitling in Dutch and English. In Proceedings of the
4th Conference on Language Resources and Evalua-
tion, pages 1045?1048, Lisbon, Portugal.
Siobhan Devlin and John Tait. 1998. The use of a
Psycholinguistic database in the Simplification of Text
for Aphasic Readers. Lecture Notes. Stanford, USA:
CSLI.
Nathan Dieckmann, Paul Slovic, and Ellen Peters. 2009.
The use of narrative evidence and explicit likelihood
by decision makers varying in numeracy. Risk Analy-
sis, 29(10).
Geert Freyhoff, Gerhard Hess, Linda Kerr, Elizabeth
Menzel, Bror Tronbacke, and Kathy Van Der Veken.
1998. European guidelines for the production of easy-
to-read information.
Caroline Gasperin, Lucia Specia, Tiago F. Pereira, and
Sandra M. Aluisio. 2009. Learning when to simplify
sentences for natural text simplification. In Proceed-
ings of the Encontro Nacional de Inteligencia Artificial
(ENIA), pages 809?818, Bento Gonalves, Brazil.
Deeqa Jama and George Dugdale. 2010. Literacy: State
of the nation. Technical report, National Literacy
Trust.
Arnaldo Candido Jr., Erick Maziero, Caroline Gasperin,
Thiago A. S. Pardo, Lucia Specia, and Sandra M.
Aluisio. 2009. Supporting the Adaptation of Texts
for Poor Literacy Readers: a Text Simplification Ed-
itor for Brazilian Portuguese. In Proceedings of the
NAACL/HLT Workshop on Innovative Use of NLP
for Building Educational Applications, pages 34?42,
Boulder, Colorado.
Imtiaz Hussain Khan, Kees Deemter, and Graeme
Ritchie. 2008. Generation of refering expressions:
managing structural ambiguities. In Proceedings of
the 22nd International Conference on Computational
Linguistics(COLING), pages 433?440, Manchester.
Himanshu Mishra, Arul Mishra, and Baba Shiv. 2011.
In praise of vagueness: malleability of vague informa-
tion as a performance booster. Psychological Science,
22(6):733?8, April.
Ellen Peters, Judith Hibbard, Paul Slovic, and Nathan
Dieckmann. 2007. Numeracy skill and the commu-
nication, comprehension, and use of risk-benefit infor-
mation. Health Affairs, 26(3):741?748.
Sarah E. Petersen and Mari Ostendorf. 2007. Text Sim-
plification for Language Learners: A Corpus Analysis.
In Proceedings of Workshop on Speech and Language
Technology for Education (SLaTE).
Richard Power and Sandra Williams. 2012. Generating
numerical approximations. Computational Linguis-
tics, 38(1).
Qualification and Curriculum Authority. 1999. Mathe-
matics: the National Curriculum for England. Depart-
ment for Education and Employment, London.
Advaith Siddharthan. 2002. Resolving attachment and
clause boundary amgiguities for simplifying relative
clause constructs. In Proceedings of the Student Re-
search Workshop, 40th Meeting of the Association for
Computacional Linguistics.
47
Advaith Siddharthan. 2003. Syntactic Simplification and
Text Cohesion. Ph.D. thesis, University of Cambridge.
United Nations. 1994. Standard Rules on the Equal-
ization of Opportunities for Persons with Disabilities.
Technical report.
Sandra Williams and Richard Power. 2009. Precision
and mathematical form in first and subsequent men-
tions of numerical facts and their relation to document
structure. In Proc. of the 12th European Workshop on
Natural Language Generation, Athens.
Sandra Williams and Ehud Reiter. 2005. Generating
readable texts for readers with low basic skills. In
Proceeding of the 10th European Workshop on Natu-
ral Language Generation, pages 140?147, Aberdeen,
Scotland.
Joel Williams, Sam Clemens, Karin Oleinikova, and
Karen Tarvin. 2003a. The Skills for Life survey: A
national needs and impact survey of literacy, numer-
acy and ICT skills. Technical Report Research Report
490, Department for Education and Skills.
Sandra Williams, Ehud Reiter, and Liesl Osman. 2003b.
Experiments with discourse-level choices and read-
ability. In In Proceedings of the European Natu-
ral Language Generation Workshop (ENLG) and 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL03), pages
127?134.
Sandra Williams. 2010. A Parser and Information
Extraction System for English Numerical Expres-
sions. Technical report, The Open University, Milton
Keynes, MK7 6AA, U.K.
48
Proceedings of the 14th European Workshop on Natural Language Generation, pages 103?104,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Narrative Composition: Achieving the Perceived Linearity of Narrative
Pablo Gerva?s
Universidad Complutense de Madrid, Ciudad Universitaria, 28040 Madrid, Spain
pgervas@sip.ucm.es
The last few years have seen an increased in-
terest in narrative within the field of Natural Lan-
guage Generation (Reiter et al, 2008; Elson and
McKeown, 2010; Siddharthan et al, 2012; Lester,
2012). Narrative is generally acknowledged as a
fundamental mode of presenting and communicat-
ing information between humans, with different
manifestations across media but with a very signif-
icant presence in textual form. Yet efforts in Nat-
ural Language Generation research have generally
side stepped the issue. Aside from the pioneer-
ing work of (Callaway, 2002) and an early attempt
to bridge the gap between narratology and natu-
ral language generation (Lo?nneker, 2005), the field
had mostly avoided narrative until recent times.
Two possible arguments may be considered as an
explanation of this: one based on the need to re-
strict initial work within a field to the simpler chal-
lenges before tackling the difficult ones, and an-
other based on an assumption that the peculiarities
of narrative have already been covered by existing
work. Both arguments can be shown to be inap-
propriate.
With respect to the first argument, the field of
natural language generation has for many years
operated under the tacit assumption that state of
the art technology can only aspire to generating
texts within a limited range of domains and genres.
These have over the years been defined in different
ways, but in spite of changes, literary texts have
usually been considered to be outside the range
of possible candidates. From an engineering point
of view, this kind of restriction made sense when
the field was starting, for two important reasons.
One, the technological solutions available at the
time for the various tasks involved in natural lan-
guage generation were in their infancy, and the lin-
guistic complexity of literary text might have been
beyond their scope. Two, natural language gener-
ation arose from a desire to extend the studies that
had been carried out for computational analysis of
language to the task of generation, and what was
known about language from a computational point
of view concerned simple texts. Most of the stud-
ies on language and computation had applied sim-
ilar simplifying assumptions. However, such re-
stricting assumptions are no longer necessary and
may be inhibiting progress. In terms of technol-
ogy, the field has matured significantly over the
intervening years. The current state of the art pro-
vides a wide range of solutions that may be well
suited to address some of the more complex phe-
nomena involved in literary text. Additional ob-
jections may be made on the grounds that we do
not know enough about these phenomena. Such
objections, however valid they might have been
originally, are no longer valid either. Many of
the phenomena that were considered beyond com-
putational treatment (metaphor, emotion, tempo-
ral reasoning, dialogue...) have been the subject
of serious and sustained study over the same time
period. Many approaches to their computational
modelling and treatment have become available.
More to the point, the last few years have seen
a rise of interest on literary text within the natu-
ral language processing community. This is ev-
idenced by the number of workshops addressing
topics related to literature: Workshop on Com-
putational Approaches to Linguistic Creativity at
NAACL HLT 2009 and 2010, Computational Lin-
guistics for Literature Workshop at NAACL HLT
2012 and 2013, Computational Models of Narra-
tive events held as AAAI Fall symposium in 2010,
as LREC workshop in 2012, and as satellite work-
shop of CogSci 2013, just to name a few.
With respect to the second argument, the recent
reappearance of narrative as a research topic for
NLG should be enough to dispel the notion that
all its problems have already been solved. Narra-
tive has many peculiarities that set it apart from
other kinds of text, and the body of work address-
ing narrative as a research topic within NLG has
103
at most uncovered and staked out a set of prob-
lems and challenges that area waiting further ex-
ploration. Of these various open problems in the
treatment of narrative, my talk will focus on the
problem of narrative composition.
Research on narrative is plagued by the diffi-
culty of establishing a definition of the term that
is both sufficiently formal to act as foundation for
scientific rigour, and sufficiently rich to cover the
fundamental aspects that people associate with the
term. At the present stage of development, tenta-
tive definition need to be established, to be later
confirmed on the basis of empirical work and suc-
cesful evaluation of results. The talk will out-
line some of the possibilities that must be con-
sidered (arising from established definitions in the
field of narratology) and some of the restrictions
that arise from the computational nature of the
task. From the combination of these constraints,
a working model of narrative structure will be out-
lined. However, it is clear that such a model must
document the relation between a semantic descrip-
tion of the content of the narrative (what is usually
termed the fabula) and its rendition as a sequential
discourse. The task of narrative composition will
be specified as the task of constructing such a dis-
course (or discourse plan) for a given semantic de-
scription of fabula. This discourse should be sus-
ceptible of being converted into text and it should
appropriately conveys the set of events in the fab-
ula in such a way that satifies a number of tra-
ditionally accepted requirements (like having an
identifiable theme, a certain temporal and causal
coherence, a recognisable set of characters...). A
number of basic narratological concepts will be
described where they provide tools for breaking
down the task into computationally tractable sub-
problems. Of particular interest is the concept
of focalization, which is used by narratologists to
describe the way certain segments of a narrative
follow a particular character, and which provides
a useful computational representation of both the
granularity and the shift in focus employed during
the process of converting the semantics of the fab-
ula into a linear discourse.
As part of the talk, narrative composition will be
framed in terms of the accepted task breakdown
for natural language generation, considering that
it may involve a combination of content determi-
nation and discourse planning that cannot be seg-
regated into separate subtasks. The talk will also
discuss the relation of the task of narrative compo-
sition with a number of existing research problems
such as story generation (which could correspond
to the construction of fabula but is sometimes sim-
plified down to construction of a discourse di-
rectly) and creativity (which has been addressed
with respect to story generation but may also con-
stitute a fundamental ingredient of the composi-
tion task).
Acknowledgments
The work on which this talk is based was partially
supported by the Ministerio de Educacio?n y Cien-
cia (TIN2009-14659-C03-01).
References
Charles B. Callaway. 2002. Narrative prose genera-
tion. Artificial Intelligence, 139(2):213?252.
David K. Elson and Kathleen R. McKeown. 2010.
Tense and aspect assignment in narrative discourse.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference, INLG ?10, pages 47?
56, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
James Lester. 2012. Expressive nlg for next-
generation learning environments: language, affect,
and narrative. In Proceedings of the Seventh Inter-
national Natural Language Generation Conference,
INLG ?12, pages 2?2, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Birte Lo?nneker. 2005. Narratological Knowledge for
Natural Language Generation. In Graham Wilcock,
Kristiina Jokinen, Chris Mellish, and Ehud Reiter,
editors, Proceedings of the 10th European Workshop
on Natural Language Generation (= ENLG 2005),
pages 91?100, Aberdeen, Scotland, August.
Ehud Reiter, Albert Gatt, Franc?ois Portet, and Marian
van der Meulen. 2008. The importance of narra-
tive and other lessons from an evaluation of an nlg
system that summarises clinical data. In Proceed-
ings of the Fifth International Natural Language
Generation Conference, INLG ?08, pages 147?156,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Advaith Siddharthan, Matthew Green, Kees van
Deemter, Chris Mellish, and Rene? van der Wal.
2012. Blogging birds: generating narratives about
reintroduced species to promote public engagement.
In Proceedings of the Seventh International Natural
Language Generation Conference, INLG ?12, pages
120?124, Stroudsburg, PA, USA. Association for
Computational Linguistics.
104
