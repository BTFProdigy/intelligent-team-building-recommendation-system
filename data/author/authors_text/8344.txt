Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 644?651, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Robust Combination Strategy for Semantic Role Labeling
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and Jordi Turmo
Technical University of Catalunya
Barcelona, Spain
{lluism,surdeanu,pcomas,turmo}@lsi.upc.edu
Abstract
This paper focuses on semantic role la-
beling using automatically-generated syn-
tactic information. A simple and robust
strategy for system combination is pre-
sented, which allows to partially recover
from input parsing errors and to signif-
icantly boost results of individual sys-
tems. This combination scheme is also
very flexible since the individual systems
are not required to provide any informa-
tion other than their solution. Extensive
experimental evaluation in the CoNLL-
2005 shared task framework supports our
previous claims. The proposed architec-
ture outperforms the best results reported
in that evaluation exercise.
1 Introduction
The task of Semantic Role Labeling (SRL), i.e.
the process of detecting basic event structures
such as who did what to whom, when and where,
has received considerable interest in the past few
years (Gildea and Jurafsky, 2002; Surdeanu et al,
2003; Xue and Palmer, 2004; Pradhan et al, 2005a;
Carreras and Ma`rquez, 2005). It was shown that
the identification of such event frames has a signif-
icant contribution for many Natural Language Pro-
cessing (NLP) applications such as Information Ex-
traction (Surdeanu et al, 2003) and Question An-
swering (Narayanan and Harabagiu, 2004).
Most current SRL approaches can be classified
in one of two classes: approaches that take ad-
vantage of complete syntactic analysis of text, pi-
oneered by Gildea and Jurafsky (2002), and ap-
proaches that use partial syntactic analysis, cham-
pioned by previous evaluations performed within
the Conference on Computational Natural Language
Learning (CoNLL) (Carreras and Ma`rquez, 2004).
The wisdom extracted from this volume of work in-
dicates that full syntactic analysis has a significant
contribution to the SRL performance, when using
hand-corrected syntactic information.
On the other hand, when only automatically-
generated syntax is available, the quality of the in-
formation provided through full syntax decreases
because the state-of-the-art of full parsing is less
robust and performs worse than the tools used for
partial syntactic analysis. Under such real-world
conditions, the difference between the two SRL ap-
proaches (with full or partial syntax) is not that high.
More interestingly, the two SRL strategies perform
better for different semantic roles. For example,
models that use full syntax recognize better agent
and theme roles, whereas models based on partial
syntax are better at recognizing explicit patient roles,
which tend to be farther from the predicate and accu-
mulate more parsing errors (Ma`rquez et al, 2005).
The above observations motivate the work pre-
sented in this paper. We introduce a novel semantic
role labeling approach that combines several indi-
vidual SRL systems. Intuitively, our approach can
be separated in two stages: a candidate generation
phase, where the solutions provided by several indi-
vidual models are combined into a pool of candidate
arguments, and an inference phase, where the candi-
dates are filtered using a binary classifier, and possi-
644
The luxury auto maker last year sold 1,214 cars in the U.S.
PPNP
VPNPNP
S
ARG0 ARGM?TMP P ARG1 ARGM?LOC
Figure 1: Sample PropBank sentence.
ble conflicts with domain knowledge constraints are
resolved to obtain the final solution.
For robustness, the inference model uses only
global attributes extracted from the solutions pro-
vided by the individual systems, e.g., the sequence
of role labels generated by each system for the cur-
rent predicate. We do not use any attributes spe-
cific to the individual models, not even the confi-
dence assigned by the individual classifiers. Besides
simplicity, the consequence of this decision is that
our approach does not impose any restrictions on the
individual SRL strategies, as long as one solution
is provided for each predicate. On the other hand,
probabilistic inference processes, which have been
successfully used for SRL (Koomen et al, 2005),
mandate that each individual candidate argument be
associated with its raw activation, or confidence, in
the given model. However, this information is not
directly available in two out of three of our individ-
ual models, which classify argument chunks and not
entire arguments.
Despite its simplicity, our approach obtains en-
couraging results: the combined system outperforms
any of the individual systems and, using exactly the
same data, it is also competitive with the best SRL
systems that participated in the latest CoNLL shared
task evaluation (Carreras and Ma`rquez, 2005).
2 Semantic Corpora
In this paper we report results using PropBank, an
approximately one-million-word corpus annotated
with predicate-argument structures (Kingsbury et
al., 2002). To date, PropBank addresses mainly
predicates lexicalized by verbs and a small num-
ber of predicates lexicalized by verb nominalizations
and adjectives.
The arguments of each predicate are numbered se-
quentially from ARG0 to ARG5. Generally, ARG0
stands for agent, ARG1 for theme or direct ob-
ject, and ARG2 for indirect object, benefactive or
instrument, but mnemonics tend to be verb spe-
cific. Additionally, predicates might have ?adjunc-
tive arguments?, referred to as ARGMs. For example,
ARGM-LOC indicates a locative and ARGM-TMP in-
dicates a temporal. Figure 1 shows a sample sen-
tence where one predicate (?sold?) has 4 arguments.
In a departure from ?traditional? SRL approaches
that train on the hand-corrected syntactic trees as-
sociated with PropBank, we do not use any syn-
tactic information from PropBank. Instead, we
develop our models using automatically-generated
syntax and named-entity (NE) labels, made avail-
able by the CoNLL shared task evaluation (Carreras
and Ma`rquez, 2005). From the CoNLL data, our
individual models based on full syntactic analysis
use the trees generated by the Charniak parser. The
partial-syntax model uses the chunk? i.e. basic syn-
tactic phrase ? labels and clause boundaries. All in-
dividual models make use of the provided NE labels.
Following the CoNLL-2005 setting we evaluated
our system also on a fresh test set, derived from the
Brown corpus. This second evaluation allows us to
re-enforce our robustness claim.
3 Approach Overview
The proposed architecture, summarized in Figure 2,
consists of two stages: a candidate generation phase
and an inference stage.
In the candidate generation step, we merge the so-
lutions of three individual SRL models into a unique
pool of candidate arguments. The proposed models
range from complete reliance on full parsing to us-
ing only partial syntactic information. The first two
models, Model 1 and 2, are developed as sequential
taggers (using the BIO tagging scheme) on a shared
framework. The major difference between the two
models is that Model 1 uses only partial syntactic
information (basic phrases and clause boundaries),
whereas Model 2 uses complete syntactic informa-
tion. To maximize diversity, Model 3 implements
a different strategy: it models only arguments that
map into exactly one syntactic constituent. Section 4
details all three individual models.
The inference stage starts with candidate filtering,
645
Candidate Filtering
Reliance on full syntax
Model 1 Model 2 Model 3
Conflict Resolution
Inference
Candidate
Generation
Figure 2: Architecture of the proposed system.
which reduces the number of candidate arguments
in the pool using a single binary classifier. Using
this classifier?s confidence values and a number of
domain-specific constraints, e.g. no two arguments
can overlap, the conflict resolution component en-
forces the consistency of the final solution using a
straightforward greedy strategy. The complete in-
ference model is detailed in Section 5.
4 Individual SRL Models
Models 1 and 2. These models approach SRL as
a sequential tagging task. In a pre-process step, the
input syntactic structures are traversed in order to
select a subset of constituents organized sequentially
(i.e. non embedding). Model 1 makes use only of
the partial tree defined by base chunks and clause
boundaries, while Model 2 explores full parse trees.
Precisely, the sequential tokens are selected as fol-
lows. First, the input sentence is splitted into dis-
joint segments by considering the clause boundaries
given by the syntactic structure. Second, for each
segment, the set of top-most non-overlapping syn-
tactic constituents completely falling inside the seg-
ment are selected as tokens. Note that this strategy
provides a set of sequential tokens covering the com-
plete sentence. Also, it is independent of the syn-
tactic annotation explored, given it provides clause
boundaries ? see (Ma`rquez et al, 2005) for more
details.
Due to this pre-processing stage, the upper-bound
recall figures are 95.67% for Model 1 and 90.32%
for Model 2 using the datasets defined in Section 6.
The nodes selected are labeled with B-I-O tags
(depending if they are at the beginning, inside, or
outside of a predicate argument) and they are con-
verted into training examples by considering a rich
set of features, mainly borrowed from state-of-the-
art systems. These features codify properties from:
(a) the argument constituent, (b) the target predicate,
Constituent type and head: extracted using common head-
word rules. If the first element is a PP chunk, then the
head of the first NP is extracted.
First and last words and POS tags of the constituent.
POS sequence: if it is less than 5 tags long.
2/3/4-grams of the POS sequence.
Bag-of-words of nouns, adjectives, and adverbs.
TOP sequence: sequence of types of the top-most syntactic
elements in the constituent (if it is less than 5 elements long).
In the case of full parsing this corresponds to the right-hand
side of the rule expanding the constituent node.
2/3/4-grams of the TOP sequence.
Governing category as in (Gildea and Jurafsky, 2002).
NamedEnt, indicating if the constituent embeds or
strictly matches a named entity along with its type.
TMP, indicating if the constituent embeds or strictly matches
a temporal keyword (extracted from AM-TMP arguments of
the training set).
Previous and following words and POS of the constituent.
The same features characterizing focus constituents are
extracted for the two previous and following tokens, provided
they are inside the clause boundaries of the codified region.
Table 1: Constituent structure features: Models 1/2
Predicate form, lemma, and POS tag.
Chunk type and type of verb phrase in which verb is
included: single-word or multi-word.
The predicate voice. We currently distinguish five voice
types: active, passive, copulative, infinitive, and progressive.
Binary flag indicating if the verb is a start/end of a clause.
Sub-categorization rule, i.e. the phrase structure rule that
expands the predicate immediate parent.
Table 2: Predicate structure features: Models 1/2
and (c) the distance between the argument and pred-
icate. The three feature sets are listed in Tables 1, 2,
and 3, respectively.1
Regarding the learning algorithm, we used gener-
alized AdaBoost with real-valued weak classifiers,
which constructs an ensemble of decision trees of
fixed depth (Schapire and Singer, 1999). We con-
sidered a one-vs-all decomposition into binary prob-
lems to address multi-class classification. AdaBoost
binary classifiers are then used for labeling test se-
quences, from left to right, using a recurrent sliding
window approach with information about the tag as-
signed to the preceding token. This tagging module
enforces some basic constraints, e.g., BIO correct
structure, arguments cannot overlap with clause nor
chunk boundaries, discard ARG0-5 arguments not
present in PropBank frames for a certain verb, etc.
1Features extracted from partial parsing and Named Entities
are common to Model 1 and 2, while features coming from full
parse trees only apply to Model 2.
646
Relative position, distance in words and chunks, and level of
embedding (in #clause-levels) with respect to the constituent.
Constituent path as described in (Gildea and Jurafsky, 2002)
and all 3/4/5-grams of path constituents beginning at the
verb predicate or ending at the constituent.
Partial parsing path as described in (Carreras et al, 2004)
and all 3/4/5-grams of path elements beginning at the
verb predicate or ending at the constituent.
Syntactic frame as described by Xue and Palmer (2004)
Table 3: Predicate?constituent features: Models 1/2
The syntactic label of the candidate constituent.
The constituent head word, suffixes of length 2, 3, and 4,
lemma, and POS tag.
The constituent content word, suffixes of length 2, 3, and
4, lemma, POS tag, and NE label. Content words, which
add informative lexicalized information different from
the head word, were detected using the heuristics
of (Surdeanu et al, 2003).
The first and last constituent words and their POS tags.
NE labels included in the candidate phrase.
Binary features to indicate the presence of temporal cue
words, i.e. words that appear often in AM-TMP phrases
in training.
For each TreeBank syntactic label we added a feature to
indicate the number of such labels included in the
candidate phrase.
The sequence of syntactic labels of the constituent
immediate children.
The phrase label, head word and POS tag of the
constituent parent, left sibling, and right sibling.
Table 4: Constituent structure features: Model 3
Model 3. The third individual SRL model makes
the strong assumption that each predicate argument
maps to one syntactic constituent. For example, in
Figure 1 ARG0 maps to a noun phrase, ARGM-LOC
maps to a prepositional phrase etcetera. This as-
sumption holds well on hand-corrected parse trees
and simplifies significantly the SRL process because
only one syntactic constituent has to be correctly
classified in order to recognize one semantic argu-
ment. On the other hand, this approach is limited
when using automatically-generated syntactic trees.
For example, only slightly over 91% of the argu-
ments can be mapped to one of the syntactic con-
stituents produced by the Charniak parser.
Using a bottom-up approach, Model 3 maps each
argument to the first syntactic constituent that has
the exact same boundaries and then climbs as high as
possible in the tree across unary production chains.
We currently ignore all arguments that do not map
to a single syntactic constituent.
The predicate word and lemma.
The predicate voice. Same definition as Models 1 and 2.
A binary feature to indicate if the predicate is frequent
(i.e., it appears more than twice in the training data) or not.
Sub-categorization rule. Same def. as Models 1 and 2.
Table 5: Predicate structure features: Model 3
The path in the syntactic tree between the argument phrase
and the predicate as a chain of syntactic labels along with
the traversal direction (up or down).
The length of the above syntactic path.
The number of clauses (S* phrases) in the path.
The number of verb phrases (VP) in the path.
The subsumption count, i.e. the difference between the
depths in the syntactic tree of the argument and predicate
constituents. This value is 0 if the two phrases share the
same parent.
The governing category, which indicates if NP
arguments are dominated by a sentence (typical for
subjects) or a verb phrase (typical for objects).
We generalize syntactic paths with more than 3
elements using two templates:
(a) Arg ? Ancestor ? Ni ? Pred, where Arg is the
argument label, Pred is the predicate label, Ancestor
is the label of the common ancestor, and Ni is instantiated
with all the labels between Pred and Ancestor in
the full path; and
(b) Arg ? Ni ? Ancestor ? Pred, where Ni is
instantiated with all the labels between Arg and
Ancestor in the full path.
The surface distance between the predicate and the
argument phrases encoded as: the number of tokens, verb
terminals (VB*), commas, and coordinations (CC) between
the argument and predicate phrases, and a binary feature to
indicate if the two constituents are adjacent.
A binary feature to indicate if the argument starts with a
predicate particle, i.e. a token seen with the RP* POS
tag and directly attached to the predicate in training.
Table 6: Predicate?constituent features: Model 3
Once the mapping process completes, Model 3
extracts a rich set of lexical, syntactic, and seman-
tic features. Tables 4, 5, and 6 present these features
organized in the same three categories as the previ-
ous Models 1 and 2 ? see (Surdeanu and Turmo,
2005) for more details.
Similarly with Models 1 and 2, Model 3 trains
one-vs-all classifiers using AdaBoost for the most
common argument labels. To reduce the sample
space, Model 3 selects training examples (both posi-
tive and negative) only from: (a) the first clause that
includes the predicate, or (b) from phrases that ap-
pear to the left of the predicate in the sentence. More
than 98% of the argument constituents fall into one
of these classes.
At prediction time the classifiers are combined us-
ing a simple greedy technique that iteratively assigns
647
to each predicate the argument classified with the
highest confidence. For each predicate we consider
as candidates all AM attributes, but only numbered
attributes indicated in the corresponding PropBank
frame. Additionally, this greedy strategy enforces a
limited number of domain knowledge constraints in
the generated solution: (a) arguments can not over-
lap in any form, and (b) no duplicate arguments are
allowed for ARG0-5.
5 The Inference Model
The most important component of our inference
model is candidate filtering, which decides if a can-
didate argument should be maintained in the global
solution or not. Candidate filtering is implemented
as a single binary classifier that uses only features
extracted from the solutions provided by the individ-
ual systems. For robustness, we do not use any fea-
tures that are specific to any of the individual mod-
els, nor the confidence value of their classifiers.
Table 7 lists the features extracted from each can-
didate argument by the filtering classifier. For sim-
plicity we have focused only on attributes that: (a)
are readily available in the solutions proposed by the
individual classifiers, and (b) allow the gathering of
simple and robust statistics. For example, the fil-
tering classifier might learn that a candidate is to be
trusted if: (a) two individual systems proposed it, (b)
if its label is ARG2 and it was generated by Model 1,
or (c) if it was proposed by Model 2 within a certain
argument sequence.
The candidate arguments that pass the filtering
stage are incorporated in the global solution by the
conflict resolution module, which enforces several
domain specific constraints. We have currently im-
plemented two constraints: (a) arguments can not
overlap or embed other arguments; and (b) no du-
plicate arguments are allowed for the numbered ar-
guments ARG0-5. Theoretically, the set of con-
straints can be extended with any other rules, but in
our particular case, we know that some constraints,
e.g. providing only arguments indicated in the cor-
responding PropBank frame, are already guaranteed
by the individual models. Conflicts are solved with
a straightforward greedy strategy: the pool of candi-
date arguments is inspected in descending order of
the confidence values assigned by the filtering clas-
The label of the candidate argument.
The number of systems that generated an argument with
this label and span.
The unique ids, e.g. M1 and M2, of all the systems that
generated an argument with this label and span.
The argument sequence for this predicate for all the systems
that generated an argument with this label and span. For
example, the argument sequence for the proposition
illustrated in Figure 1 is: ARG0 - ARGM-TMP - P -
ARG1 - ARGM-LOC.
The number and unique ids of all the systems that generated
an argument with the same span but different label.
The number and unique ids of all the systems that generated
an argument included in the current argument.
The number and unique ids of all the systems that generated
an argument that contains the current argument.
The number and unique ids of all the systems that generated
an argument that overlaps the current argument.
Table 7: Features used by the candidate filtering
classifier.
sifier, and candidates are appended to the global so-
lution only if they do not violate any of the domain
constraints with the arguments already selected. Our
inference system currently has a sequential architec-
ture, i.e. no feedback is sent from the conflict reso-
lution module to candidate filtering.
6 Experimental Results
We trained the individual models using the complete
CoNLL-2005 training set (PropBank/TreeBank sec-
tions 2 to 21). All models were developed using
AdaBoost with decision trees of depth 4 (i.e. each
branch may represent a conjunction of at most 4 ba-
sic features). Each classification model was trained
for up to 2,000 rounds.
We applied some simplifications to keep training
times and memory requirements inside admissible
bounds: (a) we have limited the number of nega-
tive examples in Model 3 to the first 500,000; (b)
we have trained only the most frequent argument la-
bels: top 41 for Model 1, top 35 for Model 2, and
top 24 for Model 3; and (c) we discarded all features
occurring less than 15 times in the training set.
The models were tuned on a separate develop-
ment partition (TreeBank section 24) and evaluated
on two corpora: (a) TreeBank section 23, which
consists of Wall Street Journal (WSJ) documents,
and (b) on three sections of the Brown corpus, se-
mantically annotated by the PropBank team for the
CoNLL 2005 shared task evaluation. Table 8 sum-
648
WSJ PProps Precision Recall F?=1
Model 1 48.45% 78.76% 72.44% 75.47 ?0.8
Model 2 52.04% 79.65% 74.92% 77.21 ?0.8
Model 3 45.28% 80.32% 72.95% 76.46 ?0.6
Brown PProps Precision Recall F?=1
Model 1 30.85% 67.72% 58.29% 62.65 ?2.1
Model 2 36.44% 71.82% 64.03% 67.70 ?1.9
Model 3 29.48% 72.41% 59.67% 65.42 ?2.1
Table 8: Overall results of the individual models on
the WSJ and Brown test sets.
marizes the results of the three models on the WSJ
and Brown corpora. In that table we include the
percentage of perfect propositions detected by each
model (?PProps?), i.e. predicates recognized with
all their arguments, the overall precision, recall, and
F?=1 measure2.
The results summarized in Table 8 indicate that
all individual systems have a solid performance. Al-
though none of them would rank in the top 3 in this
year?s CoNLL evaluation (Carreras and Ma`rquez,
2005), their performance is comparable to the best
individual systems presented at that evaluation exer-
cise3. As expected, the models based on full parsing
(2 and 3) perform better than the model based on
partial syntax. But, interestingly, the difference is
not large (e.g., less than 2 points in F?=1 in the WSJ
corpus), evincing that having base syntactic chunks
and clause boundaries is enough to obtain a compet-
itive performance with a simple system.
Consistently with other systems evaluated on the
Brown corpus, all our models experience a severe
performance drop in this corpus, due to the lower
performance of the linguistic processors.
6.1 Performance of Combination Systems
We have trained the candidate filtering binary classi-
fier on one third of the training partition. Its training
data was generated using individual models trained
on the other two thirds of the training partition. The
classifier was developed using Support Vector Ma-
chines (SVM) with a polynomial kernel of degree 2.
We trained combined models for all 4 possible com-
binations of our 3 individual models.
2The significance intervals for the F1 measure have been ob-
tained using bootstrap resampling (Noreen, 1989). F1 rates out-
side of these intervals are assumed to be significantly different
from the related F1 rate (p < 0.05).
3The best performing SRL systems at CoNLL were a com-
bination of several subsystems. See section 7 for details.
Table 9 summarizes the performance of the com-
bined systems on the WSJ and Brown corpora.4
The combined systems are compared against a base-
line combination system, which merges all the argu-
ments generated by the individual systems. For con-
flict resolution, the baseline uses the greedy strategy
introduced in Section 5, but using as argument or-
dering criterion a radix sort that orders the candidate
arguments in descending order of: number of mod-
els that agreed on this argument; argument length in
tokens; and performance of the individual system5.
Table 9 indicates that our combination strategy is
always successful: the results of all combined sys-
tems improve upon their individual models and they
are better the baseline when using the same num-
ber of individual models. As expected, the highest
scoring combined system includes all three individ-
ual models. Its F?=1 measure is 2.35 points higher
than the best individual model (Model 2) in the WSJ
test set and 1.30 points higher in the Brown test
set. Somewhat surprisingly, the highest percentage
of perfect propositions is not obtained by the over-
all best combination, but by the system that com-
bines the two models based on full parsing (Models
2 and 3). This happens because Model 1 is the weak-
est performing of the bunch, hence its arguments,
while providing useful information to the filtering
classifier, decrease the number of perfect proposi-
tions when selected.
We consider these results encouraging given the
simplicity of our inference model and the limited
amount of training data used to train the candidate
filtering classifier. Additionally, they compare fa-
vorably with respect to the best performing systems
at CoNLL-2005 shared task (see Section 7).
6.2 Upper Limit of the Combination Strategy
To explore the potential of our approach we have
constructed a hypothetical system where our candi-
date filtering module is replaced with a perfect clas-
sifier that selects only correct arguments and dis-
cards all others. Table 10 lists the results obtained
on the WSJ and Brown corpora by this hypothetical
system using all three individual models.
4For conciseness, in Table 9 we introduced the notation
M1+2+3 to indicate the combination of Models 1, 2, and 3
5This combination produced the highest-scoring baseline
model.
649
WSJ PProps Prec. Recall F?=1
M1+2 51.30% 81.30% 74.13% 77.55 ?0.7
M1+3 47.26% 81.21% 73.36% 77.08 ?0.8
M2+3 52.65% 81.55% 75.30% 78.30 ?0.7
M1+2+3 51.64% 84.89% 74.87% 79.56 ?0.7
baseline 51.09% 77.29% 78.67% 77.98 ?0.7
Brown PProps Prec. Recall F?=1
M1+2 35.95% 73.70% 62.93% 67.89 ?2.0
M1+3 28.98% 72.83% 58.84% 65.09 ?2.2
M2+3 37.06% 73.89% 63.30% 68.18 ?2.2
M1+2+3 34.20% 78.66% 61.46% 69.00 ?2.1
baseline 33.58% 67.66% 66.01% 66.82 ?1.8
Table 9: Overall results of the combination models
on the WSJ and Brown test sets.
Perfect props Precision Recall F?=1
WSJ 70.76% 99.12% 85.22% 91.64
Brown 51.87% 99.63% 74.32% 85.14
Table 10: Performance upper limit on the test sets.
Table 10 indicates that the upper limit of proposed
approach is relatively high: the F?=1 of this hy-
pothetical system is over 12 points higher than our
best combined system in the WSJ test set, and over
16 points higher in the Brown corpus. These re-
sults indicate that the potential of our combination
strategy is high, especially when compared with re-
ranking strategies, which are limited to the perfor-
mance of the best complete solution in the candidate
pool. By allowing the re-combination of arguments
from the individual candidate solutions we raise this
threshold significantly. Table 11 lists the contribu-
tion of the individual models to this upper limit on
the WSJ corpus. For conciseness, we list only the
?core? numbered arguments. ?? of 3? indicates the
percentage of correct arguments where all 3 mod-
els agreed, ?? of 2? indicates the percentage of cor-
rect arguments where any 2 models agreed, and the
other columns indicate the percentage of correct ar-
guments detected by a single model. Table 11 indi-
cates that, as expected, two or more individual mod-
els agreed on a large percentage of the correct argu-
ments. Nevertheless, a significant number of correct
arguments, e.g. over 22% of ARG3, come from a
single individual system. This proves that, in order
to achieve maximum performance, one has to look
beyond simple voting strategies that favor arguments
with high agreement between individual systems.
? of 3 ? of 2 M1 M2 M3
ARG0 80.45% 12.10% 3.47% 2.14% 1.84%
ARG1 69.82% 17.83% 7.45% 2.77% 2.13%
ARG2 56.04% 22.32% 12.20% 4.95% 4.49%
ARG3 56.03% 21.55% 12.93% 5.17% 4.31%
ARG4 65.85% 20.73% 6.10% 2.44% 4.88%
Table 11: Contribution of the individual systems to
the upper limit, for ARG0?ARG4 in the WSJ test set.
WSJ Brown
PProps F?=1 PProps F?=1
koomen 53.79% 79.44 ?0.8 32.34% 67.75 ?1.8
haghighi 56.52% 78.45 ?0.8 37.06% 67.71 ?2.0
pradhan 50.14% 77.37 ?0.7 36.44% 67.07 ?2.0
Table 12: Results of the best combined systems at
CoNLL-2005.
7 Related Work
The best performing systems at the CoNLL-2005
shared task included a combination of different base
subsystems to increase robustness and to gain cover-
age and independence from parse errors. Therefore,
they are closely related to the work of this paper.
Table 12 summarizes their results under exactly the
same experimental setting.
Koomen et al (2005) used a 2 layer architecture
similar to ours. The pool of candidates is generated
by running a full syntax SRL system on alternative
input information (Collins parsing, and 5-best trees
from Charniak?s parser). The combination of can-
didates is performed in an elegant global inference
procedure as constraint satisfaction, which, formu-
lated as Integer Linear Programming, can be solved
efficiently. Interestingly, the generalized inference
layer allows to include in the objective function,
jointly with the candidate argument scores, a num-
ber of linguistically-motivated constraints to obtain
a coherent solution. Differing from the strategy pre-
sented in this paper, their inference layer does not
include learning. Also, they require confidence val-
ues from individual classifiers. This is the best per-
forming system at CoNLL-2005.
Haghighi et al (2005) implemented a double re-
ranking model on top of the base SRL models to se-
lect the most probable solution among a set of can-
didates. The re-ranking is performed, first, on a set
of n-best solutions obtained by the base system run
on a single parse tree, and, then, on the set of best-
candidates coming from the n-best parse trees. The
650
re-ranking approach allows to define global complex
features applying to complete candidate solutions to
train the rankers. The main drawback, compared to
our approach, is that re-ranking does not permit to
combine different solutions since it is forced to se-
lect a complete candidate solution. This fact implies
that the performance upper limit strongly depends
on the ability of the base model to generate the com-
plete correct solution in the set of n-best candidates.
Finally, Pradhan et al (2005b) followed a stack-
ing approach by learning two individual systems
based on full syntax, whose outputs are used to
generate features to feed the training stage of a fi-
nal chunk-by-chunk SRL system. Although the fine
granularity of the chunking-based system allows to
recover from parsing errors, we find this combina-
tion scheme quite ad-hoc because it forces to break
argument candidates into chunks in the last stage.
8 Conclusions
This paper introduces a novel, robust combination
strategy for semantic role labeling. Our approach
is separated in two stages: a candidate generation
phase, which combines the solutions generated by
several individual models into a pool of candidate ar-
guments, followed by a simple inference model that
filters the candidate arguments using a single binary
classifier and then enforces an arbitrary number of
domain-specific constraints.
The proposed approach has several advantages.
First, because it combines the solutions provided by
the individual models, the inference model can re-
cover from errors produced in the generation phase.
Second, due to the diversity of the individual models
employed, the candidate pool contains a high per-
centage of the correct arguments. And lastly, our
approach is flexible and robust: it can incorporate
any SRL model in the candidate generation stage
because it does not require that the individual SRL
models provide any information, e.g. classification
confidence values, other than an argument solution.
Our results are better than the state of the art us-
ing automatically-generated syntactic information.
These results are encouraging considering the sim-
plicity of the proposed approach.
Acknowledgments
This research has been partially supported by the
European Commission (CHIL project, IP-506909).
Mihai Surdeanu is a research fellow within the
Ramo?n y Cajal program of the Spanish Ministry of
Education and Science.
References
X. Carreras and L. Ma`rquez. 2004. Introduction to the CoNLL-
2004 shared task: Semantic role labeling. In Proceedings of
CoNLL 2004.
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 Shared Task: Semantic Role Labeling. In Proceedings
of CoNLL-2005.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL 2004 shared task.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3).
A. Haghighi, K. Toutanova, and C. Manning. 2005. A joint
model for semantic role labeling. In Proceedings of CoNLL-
2005 shared task.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding se-
mantic annotation to the Penn Treebank. In Proceedings of
the Human Language Technology Conference.
P. Koomen, V. Punyakanok, D. Roth, and W. Yih. 2005. Gen-
eralized inference with multiple semantic role labeling sys-
tems. In Proceedings of CoNLL-2005 shared task.
L. Ma`rquez, P. Comas, J. Gime?nez, and N. Catala`. 2005. Se-
mantic role labeling as sequential tagging. In Proceedings of
CoNLL-2005 shared task.
S. Narayanan and S. Harabagiu. 2004. Question answering
based on semantic structures. In International Conference
on Computational Linguistics (COLING 2004).
E. W. Noreen. 1989. Computer-Intensive Methods for Testing
Hypotheses. John Wiley & Sons.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin, and
D. Jurafsky. 2005a. Support vector learning for semantic
argument classification. Machine Learning, to appear.
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Juraf-
sky. 2005b. Semantic role chunking combining complemen-
tary syntactic views. In Proceedings of CoNLL-2005.
R. E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
M. Surdeanu and J. Turmo. 2005. Semantic role labeling using
complete syntactic analysis. In Proceedings of CoNLL-2005
shared task.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proceedings of ACL 2003.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of EMNLP-2004.
651
In: Proceedings of CoNLL-2000 and LLL-2000, pages 115-118, Lisbon, Portugal, 2000. 
Learning IE Rules for a Set of Related Concepts 
J. Turmo and H. Rodr iguez  
TALP Research Center. Universitat Polit~cnica de Catalunya 
Jordi Girona Salgado, 1-3 
E-08034 Barcelona - Spain 
1 In t roduct ion  
The growing availability of on-line text has led 
to an increase in the use of automatic knowledge 
acquisition approaches from textual data. In 
fact, a number of Information Extraction (IE) 
systems has emerged in the past few years in 
relation to the MUC conferences 1. The aim of 
an IE system consists in automatically extract- 
ing pieces of information from text, being this 
information relevant for a set of prescribed con- 
cepts (scenario). One of the main drawbacks of 
applying IE systems is the high cost involved in 
manually adapting them to new domains and 
text styles. 
In recent years, a variety of Machine Learn- 
ing (ML) techniques has been used to improve 
the portability of IE systems to new domains, 
as in SRV (Freitag, 1998), RAPIER (Califf 
and Mooney, 1997), LIEP (Huffman, 1996), 
CRYSTAL (Soderland et al, 1995) and WHISK 
(Soderland, 1999) . However, some drawbacks 
remain in the portability of these systems: a) 
existing systems generally depend on the sup- 
ported text style and learn IE-rules either for 
structured texts, semi-structured texts or free 
text , b) IE systems are mostly single-concept 
learning systems, c) consequently, an extrac- 
tor (e.g., a rule set) is learned for each con- 
cept within the scenario in an independent man- 
ner, d) the order of execution of the learners 
is set manually, and so are the scheduling and 
way of combination of the resulting extractors, 
and e) focusing on the training data, the size of 
available training corpora can be inadequate to 
accurately learn extractors for all the concepts 
within the scenario 2.
1 http://www.muc.saic.com/ 
~This is so when dealing with some combinations of
text style and domain. 
This paper describes EVIUS, a multi-concept 
learning system for free text that follows a 
multi-strategy constructive learning approach 
(MCL) (Michalshi, 1993) and supports insuffi- 
cient amounts of training corpora. EVIUS is 
a component of a multilingual IE system, M- 
TURBIO (Turmo et al, 1999). 
2 EV IUS .  Learn ing  ru le  sets  for a 
set  o f  re la ted  concepts  
The input of EVIUS is both a partially-parsed 
semantically-tagged 3 training corpus and a de- 
scription of the desired target structure. This 
description is provided as a set of concepts C 
related to a set of asymmetric binary relations, 
T~. 
In order to learn set S of IE rule sets for the 
whole C, EVIUS uses an MCL approach inte- 
grating constructive l arning, closed-loop learn- 
ing and deductive restructuring (Ko, 1998). 
In this multi-concept situation, the system 
determines which concepts to learn and, later, 
incrementally updates S. This can be relatively 
straightforward when using knowledge about 
the target structure in a closed-loop learning 
approach. Starting with C, EVIUS reduces et 
b/of  unlearned concepts iteratively by selecting 
subset P C/g formed by the primitive concepts 
in/.4 and learning a rule set for each c E P 4 
For instance, the single colour scenario 5 in fig- 
3With EuroWordNet (http://www.hum.uva.nl/-ewn/) 
synsets. No attempt has been made to disambiguate 
such tags. 
4No cyclic scenarios are allowed so that a topological 
sort of C is possible, which starts with a set of primitive 
concepts. 
5Our testing domain is mycology. Texts consists of 
Spanish descriptions ofspecimens. There is a rich variety 
of colour descriptions including basic colours, intervals, 
changes, etc. 
115 
ure 1 is provided to learn from instances of the 
following three related concepts: colour, such 
as in instance "azul ligeramente claro" (slightly 
pale blue), colour_interval, as in "entre rosa 
y rojo sangre" (between pink and blood red), 
and to_change, as in "rojo vira a marr6n" (red 
changes to brown). 
Initially, Lt = C = { colour, colour_interval, 
to_change}. Then, EVIUS calculates 
7 9 ={colour} and once a rule set has been 
learned for colour, the new L/={colour_interval, 
to_change} is studied identifying 79 = L/. 
to to 
from from 
Figure 1: A single scenario for the colour do- 
main 
In order to learn a rule set for a concept, 
EVIUS uses the relational learning method ex- 
plained in section 3, and defines the learn- 
ing space by means of a dynamic predicate 
model. As a pre-process of the system, the 
training corpus is translated into predicates 
using the following initial predicate model: 
a) attributive meta-predicates: pos_X(A), 
isa_X(A), has_hypernym_X(A), word_X(A) 
and lemma_X(A), where X is instantiated with 
closed categories, b) relational meta-predicates: 
distance_le._X(A,B), stating that there are X 
terminal nodes, at most, between A and B, and 
c) relational predicates: ancestor(A,B), where B 
is the syntactic ancestor of A, and brother(A,B), 
where B is the right brother node of A sharing 
the syntactic ancestor. 
Once a rule set for concept c is learned, 
new examples are added for further learning by 
means of a deductive restructuring approach: 
training examples are reduced to generate a
more compact and useful knowledge of the 
learned concept. This is achieved by using 
the induced rule set and a syntactico-semantic 
transformational grammar. Further to all this, 
a new predicate isa_c is added to the model. 
For instance, in figure 2 6 , the Spanish sen- 
tence "su color rojo vira a marrSn oscuro" 
(its red colour changes to dark brown) has 
6Which is presented here as a partially-parsed tree 
for simplicity. 
S (n12) 
spec n a v prep/ n a 
sucolorro~vira {lmarrdnloscurc ~ } 
(nl) (n2) (n3) (n4)(n5)~(n6) . (n7) /  
( n ~ e d u c t i o n  
spec n a v prep/( gnom . \  
~ ' r a  a marr6n oscur~ ) 
(nl) (n2) (n3) (n4) (n5)k _ ~  j 
Figure 2: Restructuring training examples 
two examples of colour, n3 and n6+n7, be- 
ing these "rojo" (red) and "marr6n'+"oscuro" 
(dark brown). No reduction is required by the 
former. However, the latter example is reduced 
to node n6'. As a consequence, two new at- 
tributes are added to the model: isa_colour(n3) 
and isa_colour(n6'). This new knowledge will 
be used to learn the concepts to_change and 
colour_interval. 
3 Ru le  set learn ing  
EVIUS uses FOIL (First-order Induction Learn- 
ing) (Quinlan, 1990) to build an initial rule set 
7~0 from a set of positive and negative xamples. 
Positive examples C+ can be selected using a 
friendly environment either as: 
? text relations: c(A:,A2) where both A: and 
A2 are terminal nodes that exactly delimit 
a text value for c. For instance, both text 
relations colour(n3,n3) or colour(n6,nT) in 
figure 2, or as: 
? ontology relations: c(A:,A2,...,An) where 
all Ai are terminal nodes which are in- 
stances of already learned concepts related 
to c in the scenario. For instance, the on- 
tology relation to_change(n3,n6') 7, in the 
same figure, means that the colour repre- 
sented by instance n3 changes to that rep- 
resented by n6'. 
Negative examples $ -  are automatically se- 
lected as explained in section 3.1. 
7Note that, after the deductive restructuring step, 
both n3 and n6' are instances of the concept colour. 
116 
If any uncovered examples et, g~-, remains 
after FOIL's performance, this is due to the lack 
of sufficient examples. Thus, the system tries 
to improve recall by growing set g+ with arti- 
ficial examples (pseudo-examples), as explained 
in 3.2. A new execution of FOIL is done by 
using the new g+. The resulting rule set 7~ 
is combined with T~0 in order to create 7?1 by 
appending the new rules from T?~ to 7?0. Conse- 
quently, the recall value of 7~1 is forced to be at 
least equal to that of 7~0, although the accuracy 
can decrease. A better method seems to be the 
merging of rules from 7~ and TO0 by studying 
empirical subsumptions. This last combination 
allows to create more compact and accurate rule 
sets. 
EVIUS uses an incremental learning approach 
to learn rule sets for each concept. This is done 
by iterating the process above while uncovered 
examples remain and the F1 score increment 
(AF1) is greater than pre-defined constant a: 
select g+ and generate g -  
7~0 = FOIL(g+,g -)  
$u + = uncover ed_ f r om ( 7~o ) 
= (7?o) 
while $u + ~ 0 and AF1 > a do 
g+ = g+ U pseudo-examples($u +) 
T?~ = FOIL(E+,g -) 
T~i+ l = combine_rules(7~i,T?~) 
gu + = uncovered_f rom( TQ+ l ) 
= E l (h i+ l )  - E l (h i )  
endwhile 
if AF1 > a then return "~i+1 
else return 7~i 
endi/ 
3.1 Generat ing  re levant  negat ive  
examples  
Negative examples can be defined as any com- 
bination of terminal nodes out of g+. However, 
this approach produces an extremely large num- 
ber of examples, out of which only a small sub- 
set is relevant o learn the concept. Related to 
this, (Freitag, 1998) uses words to learn only 
slot rules (learned from text-relation examples) 
, selecting as negative those non-positive word 
pairs that define a string as neither longer than 
the maximum length in positive examples, nor 
shorter than the minimum. 
A more general approach is adopted to define 
the distance between possible examples in the 
learning Space, applying a clustering method us- 
ing positive examples as medoids s. The N near- 
est non-positive examples to each medoid can be 
selected as negative ones. Distance, in our case, 
must be defined as multidimensional due to the 
typology of occurring features. It is relatively 
easy to define distances between examples for 
word_X and lemma_X predicates, being 1 when 
X values are equal, and 0 otherwise. For isa_X 
predicates, the minimum of all possible concep- 
tual distances (Agirre and Rigau, 1995) between 
X values in EWN has been used. Greater dif- 
ficulty is encountered when defining a distance 
from a morpho-syntactic point of view (e.g., a 
pronoun seems to be closer to a noun than a 
verb). In (Turmo et al, 1999), the concept of 
5-set has been presented as a syntactic relation 
generalization, and a distance measure has been 
based on this concept. 
3.2 Creat ing  pseudo-examples  
A method has been used inspired by the gen- 
eration of convex pseudo data (Breiman, 1998), 
in which a similar process to gene-combination 
in genetic algorithms is used. 
For each positive example c(A1,. . .  ,An) 9 of 
concept c to be dealt with, an attribute vector 
is defined as 
( word--X Bl ,. . . ,word._X B~ , lemma-X sl , . . . ,  
lemma_X B~ ,sem-X B1 ,... ,sem_X B~ ,context) 
where B1, . . . ,  Bn are the unrepeated terminal 
nodes from A1, . . . ,  An, context is the set of all 
predicates subsumed by the syntactico-semantic 
structure between the nearest positive exam- 
ple on the left and the nearest one on the 
right, and sem_XB~ is the list of isa_X and 
has_hypernym_X predicates for Bi. 
Then, for each example uncovered by the rule 
set learned by FOIL, a set of pseudo-examples is 
generated. A pseudo-example is built by com- 
bining both the uncovered example vector and 
a randomly selected covered one. This is done 
as follows: for each dimension, one of both pos- 
sible values is randomly selected as value for the 
pseudo-example. 
SA medoid is an actual data point representing a clus- 
ter. 
9As defined in section 3. 
117 
T. Set* $+ 
150 105 
25o 206 
35o 270 
45o 328 
55o 398 
Reca l l \ ]P rec .  F1 
56.86 100 0.725 
62.74 98.45 0.766 
73.53 97.40 0.838 
75.49 98.72 0.856 
75.49 98.7210.856 
Table 1: Results for the colour concept for dif- 
ferent training set sizes (* subscript 0 means 
only one FOIL iteration) 
4 Eva luat ion  
EVIUS has been tested on the mycological do- 
main. A set of 68 Spanish mycological docu- 
ments (covering 9800 words corresponding to 
1360 lemmas) has been used. 13 of them have 
been kept for testing and the others for train- 
ing. The target ontology consisted of 14 con- 
cepts and 24 relations. 
Several experiments have been carried out 
with different raining sets. Results of the initial 
rule set for the colour concept 1? are presented 
in table 1. 
Out of 34 in the 350 initial rule set, one of the 
most relevant learned rules is11: 
Col our ( A, B ) :-has_h ypern ym_OOO17586n ( B ) , 
has_hypernym_O3464624n (A), brother (A, B). 
Table 2 shows the results of adding pseudo- 
examples to the 35012 training set and using the 
algorithm in section 3. This was tested with 
a = 0.01 (two iterations are enough, 351 and 
352) and 5 pseudo-examples for each uncovered 
case. The algorithm returns the rule set pro- 
duced in the first iteration due to the fact that 
~F1T13> 0.01 between the first and the sec- 
ond iterations. Higher results can be generated 
when using lower values for a. 
Although no direct comparison with other 
systems is possible due to the domain and lan- 
guage used, our results can be considered state- 
1?This concept appears to be the most difficult to be 
learned. 
11A chromatic colour (03464624n) that is the left syn- 
tactic brother of an attribute (00017586n) such as lumi- 
nosity or another chromatic colour. 
12This size has been selected to allow a better com- 
parison with the results in table 1. 
laF1T means the F1 value for training sets 
T. Set E + F i r  Recall Prec. F1 
351 415 0.981 76.47 97.50 0.857 
352 465 0.987 79.41 97.50 0.875 
Table 2: Results from adding pseudo-examples 
to the initial training set with 35 documents. 
of-the-art regarding similar MUC competition 
tasks. 
Re ferences  
Eneko Agirre and German Rigau. 1995. A Proposal 
for Word Sense Disambiguation using Concep- 
tual Distance. In Proceedings of the International 
Conference RANLP, Tzigov Chark, Bulgaria. 
L. Breiman. 1998. Arcing Classifiers. The Annals 
of Statistics, 26(3):801-849. 
M.E. Califf and R. Mooney. 1997. Relational learn- 
ing of pattern-match rules for information extrac- 
tion. In Workshop on Natural Language Learning, 
pages 9-15. ACL. 
D. Freitag. 1998. Machine Learning for Informa- 
tion Extraction in Informal Domains. Ph.D. the- 
sis, Computer Science Department. Carnegie Mel- 
lon University. 
S. Huffman. 1996. Learning information extraction 
patterns from examples. In S. Wermter, E. Riloff, 
and G. Sheller, editors, Connectionist, statistical 
and symbolic approaches to learning for natural 
language processing. Springer-Verlag. 
H. Ko. 1998. Empirical assembly sequence planning: 
A multistrategy constructive l arning approach. 
In I. Bratko R. S. Michalsky and M. Kubat, ed- 
itors, Machine Learning and Data Mining. John 
Wiley & Sons LTD. 
R.S. Michalshi. 1993. Towards a unified theory of 
learning: Multistrategy task-adaptive l arning. 
In B.G. Buchanan and D. Wilkins, editors, Read- 
ings in Knowledge Acquisition and Learning. Mor- 
gan Kauffman. 
J.R. Quinlan. 1990. Learning logical definitions 
from relations. Machine Learning, 5:239-266. 
S. Soderland, D. Fisher, J. Aseltine, and W. Lehn- 
ert. 1995. Crystal: Inducing a conceptual dictio- 
nary. In XIV International Joint Conference on 
Artificial Intelligence, pages 1314-1321. 
S. Soderland. 1999. Learning information extraction 
rules for semi-structured and free text. Machine 
Learning, 34:233-272. 
J. Turmo, N. Catalk, and H. Rodrlguez. 1999. An 
adaptable i  system to new domains. Applied In- 
telligence, 10(2/3):225-246. 
118 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 221?224, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Complete Syntactic Analysis
Mihai Surdeanu
Technical University of Catalunya
surdeanu@lsi.upc.edu
Jordi Turmo
Technical University of Catalunya
turmo@lsi.upc.edu
Abstract
In this paper we introduce a semantic role
labeling system constructed on top of the
full syntactic analysis of text. The la-
beling problem is modeled using a rich
set of lexical, syntactic, and semantic at-
tributes and learned using one-versus-all
AdaBoost classifiers.
Our results indicate that even a simple ap-
proach that assumes that each semantic ar-
gument maps into exactly one syntactic
phrase obtains encouraging performance,
surpassing the best system that uses par-
tial syntax by almost 6%.
1 Introduction
Most current semantic role labeling (SRL) ap-
proaches can be classified in one of two classes:
approaches that take advantage of complete syntac-
tic analysis of text, pioneered by (Gildea and Juraf-
sky, 2002), and approaches that use partial syntac-
tic analysis, championed by the previous CoNLL
shared task evaluations (Carreras and Ma`rquez,
2004).
However, to the authors? knowledge, a clear anal-
ysis of the benefits of using full syntactic analysis
versus partial analysis is not yet available. On one
hand, the additional information provided by com-
plete syntax should intuitively be useful. But, on
the other hand, the state-of-the-art of full parsing
is known to be less robust and perform worse than
the tools used for partial syntactic analysis, which
would decrease the quality of the information pro-
vided. The work presented in this paper contributes
to this analysis by introducing a model that is en-
tirely based on the full syntactic analysis of text,
generated by a real-world parser.
2 System Description
2.1 Mapping Arguments to Syntactic
Constituents
Our approach maps each argument label to one syn-
tactic constituent, using a strategy similar to (Sur-
deanu et al, 2003). Using a bottom-up approach,
we map each argument to the first phrase that has the
exact same boundaries and climb as high as possible
in the syntactic tree across unary production chains.
Unfortunately, this one-to-one mapping between
semantic arguments and syntactic constituents is not
always possible. One semantic argument may be
mapped to many syntactic constituents due to: (a)
intrinsic differences between the syntactic and se-
mantic representations, and (b) incorrect syntactic
structure. Figure 1 illustrates each one of these sit-
uations: Figure 1 (a) shows a sentence where each
semantic argument correctly maps to one syntac-
tic constituent; Figure 1 (b) illustrates the situation
where one semantic argument correctly maps to two
syntactic constituents; and Figure 1 (c) shows a one-
to-many mapping caused by an incorrect syntactic
structure: argument A0 maps to two phrases, the ter-
minal ?by? and the noun phrase ?Robert Goldberg?,
due to the incorrect attachment of the last preposi-
tional phrase, ?at the University of California?.
Using the above observations, we separate one-
221
rising consumer prices
VBG NN NNS
NP
P A1
developed by Robert Goldberg at the University of California
NP
PPNP
NP
PP
VP
P A0 AM?LOC
The luxury auto maker last year sold 1,214 cars in the U.S.
PPNP
VPNPNP
S
A0 A1PAM?TMP AM?LOC
(b)(a) (c)
Figure 1: Mapping semantic arguments to syntactic constituents: (a) correct one?to-one mapping; (b) correct
one-to-many mapping; (c) one-to-many mapping due to incorrect syntax.
(a) (b) (c)
Training 96.06% 2.49% 1.45%
Development 91.36% 4.83% 3.81%
Table 1: Distribution of semantic arguments accord-
ing to their mapping to syntactic constituents ob-
tained with the Charniak parser: (a) one-to-one, (b)
one-to-many, all syntactic constituents have same
parent, (c) one-to-many, syntactic constituents have
different parents.
to-many mappings in two classes: (a) when the syn-
tactic constituents mapped to the semantic argument
have the same parent (Figure 1 (b)) the mapping is
correct and/or could theoretically be learned by a
sequential SRL strategy, and (b) when the syntac-
tic constituents mapped to the same argument have
different parents, the mapping is generally caused
by incorrect syntax. Such cases are very hard to be
learned due to the irregularities of the parser errors.
Table 1 shows the distribution of semantic argu-
ments into one of the above classes, using the syn-
tactic trees provided by the Charniak parser. For the
results reported in this paper, we model only one-
to-one mappings between semantic arguments and
syntactic constituents. A subset of the one-to-many
mappings are addressed with a simple heuristic, de-
scribed in Section 2.4.
2.2 Features
The features incorporated in the proposed model
are inspired from the work of (Gildea and Juraf-
sky, 2002; Surdeanu et al, 2003; Pradhan et al,
2005; Collins, 1999) and can be classified into five
classes: (a) features that capture the internal struc-
ture of the candidate argument, (b) features extracted
The syntactic label of the candidate constituent.
The constituent head word, suffixes of length 2, 3, and 4,
lemma, and POS tag.
The constituent content word, suffixes of length 2, 3, and
4, lemma, POS tag, and NE label. Content words, which
add informative lexicalized information different from
the head word, were detected using the heuristics
of (Surdeanu et al, 2003).
The first and last constituent words and their POS tags.
NE labels included in the candidate phrase.
Binary features to indicate the presence of temporal cue
words, i.e. words that appear often in AM-TMP phrases
in training.
For each TreeBank syntactic label we added a feature to
indicate the number of such labels included in the
candidate phrase.
The sequence of syntactic labels of the constituent
immediate children.
Table 2: Argument structure features
The phrase label, head word and POS tag of the
constituent parent, left sibling, and right sibling.
Table 3: Argument context features
from the argument context, (c) features that describe
properties of the target predicate, (d) features gener-
ated from the predicate context, and (e) features that
model the distance between the predicate and the ar-
gument. These five feature sets are listed in Tables 2,
3, 4, 5, and 6.
2.3 Classifier
The classifiers used in this paper were devel-
oped using AdaBoost with confidence rated predic-
tions (Schapire and Singer, 1999). AdaBoost com-
bines many simple base classifiers or rules (in our
case decision trees of depth 3) into a single strong
classifier using a weighted-voted scheme. Each base
classifier is learned sequentially from weighted ex-
amples and the weights are dynamically adjusted ev-
ery learning iteration based on the behavior of the
222
The predicate word and lemma.
The predicate voice. We currently distinguish five voice
types: active, passive, copulative, infinitive, and progressive.
A binary feature to indicate if the predicate is frequent - i.e.
it appears more than twice in the training partition - or not.
Table 4: Predicate structure features
Sub-categorization rule, i.e. the phrase structure rule that
expands the predicate immediate parent, e.g.
NP? VBG NN NNS for the predicate in Figure 1 (b).
Table 5: Predicate context features
The path in the syntactic tree between the argument phrase
and the predicate as a chain of syntactic labels along with
the traversal direction (up or down).
The length of the above syntactic path.
The number of clauses (S* phrases) in the path.
The number of verb phrases (VP) in the path.
The subsumption count, i.e. the difference between the
depths in the syntactic tree of the argument and predicate
constituents. This value is 0 if the two phrases share the
same parent.
The governing category, which indicates if NP
arguments are dominated by a sentence (typical for
subjects) or a verb phrase (typical for objects).
We generalize syntactic paths with more than 3
elements using two templates:
(a) Arg ? Ancestor ? Ni ? Pred, where Arg is the
argument label, Pred is the predicate label, Ancestor
is the label of the common ancestor, and Ni is instantiated
with all the labels between Pred and Ancestor in
the full path; and
(b) Arg ? Ni ? Ancestor ? Pred, where Ni is
instantiated with all the labels between Arg and
Ancestor in the full path.
The surface distance between the predicate and the
argument phrases encoded as: the number of tokens, verb
terminals (VB*), commas, and coordinations (CC) between
the argument and predicate phrases, and a binary feature to
indicate if the two constituents are adjacent.
A binary feature to indicate if the argument starts with a
predicate particle, i.e. a token seen with the RP* POS
tag and directly attached to the predicate in training.
Table 6: Predicate-argument distance features
previously learned rules.
We trained one-vs-all classifiers for the top 24
most common arguments in training (including
R-A* and C-A*). For simplicity we do not la-
bel predicates. Following the strategy proposed
by (Carreras et al, 2004) we select training exam-
ples (both positive and negative) only from: (a) the
first S* phrase that includes the predicate, or (b)
from phrases that appear to the left of the predicate
in the sentence. More than 98% of the arguments
fall into one of these classes.
At prediction time the classifiers are combined us-
ing a simple greedy technique that iteratively assigns
to each predicate the argument classified with the
highest confidence. For each predicate we consider
as candidates all AM attributes, but only numbered
attributes indicated in the corresponding PropBank
frame.
2.4 Argument Expansion Heuristics
We address arguments that should map to more
than one terminal phrase with the following post-
processing heuristic: if an argument is mapped to
one terminal phrase, its boundaries are extended
to the right to include all terminal phrases that are
not already labeled as other arguments for the same
predicate. For example, after the system tags ?con-
sumer? as the beginning of an A1 argument in Fig-
ure 1, this heuristic extends the right boundary of
the A1 argument to include the following terminal,
?prices?.
To handle inconsistencies in the treatment of
quotes in parsing we added a second heuristic: argu-
ments are expanded to include preceding/following
quotes if the corresponding pairing quote is already
included in the argument constituent.
3 Evaluation
3.1 Data
We trained our system using positive examples ex-
tracted from all training data available. Due to mem-
ory limitations on our development machines we
used only the first 500,000 negative examples. In the
experiments reported in this paper we used the syn-
tactic trees generated by the Charniak parser. The
results were evaluated for precision, recall, and F1
using the scoring script provided by the task orga-
nizers.
3.2 Results and Discussion
Table 7 presents the results obtained by our system.
On the WSJ data, our results surpass with almost 6%
the results obtained by the best SRL system that used
partial syntax in the CoNLL 2004 shared task eval-
uation (Hacioglu et al, 2004). Even though these
numbers are not directly comparable (this year?s
shared task offers more training data), we consider
these results encouraging given the simplicity of
our system (we essentially model only one-to-one
223
Precision Recall F?=1
Development 79.14% 71.57% 75.17
Test WSJ 80.32% 72.95% 76.46
Test Brown 72.41% 59.67% 65.42
Test WSJ+Brown 79.35% 71.17% 75.04
Test WSJ Precision Recall F?=1
Overall 80.32% 72.95% 76.46
A0 87.09% 85.21% 86.14
A1 79.80% 72.23% 75.83
A2 74.74% 58.38% 65.55
A3 83.04% 53.76% 65.26
A4 77.42% 70.59% 73.85
A5 0.00% 0.00% 0.00
AM-ADV 57.82% 46.05% 51.27
AM-CAU 49.38% 54.79% 51.95
AM-DIR 62.96% 40.00% 48.92
AM-DIS 72.19% 76.25% 74.16
AM-EXT 60.87% 43.75% 50.91
AM-LOC 64.19% 52.34% 57.66
AM-MNR 63.90% 44.77% 52.65
AM-MOD 98.09% 93.28% 95.63
AM-NEG 96.15% 97.83% 96.98
AM-PNC 55.22% 32.17% 40.66
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 79.17% 73.41% 76.18
R-A0 84.85% 87.50% 86.15
R-A1 75.00% 71.15% 73.03
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 68.00% 80.95% 73.91
R-AM-MNR 30.00% 50.00% 37.50
R-AM-TMP 60.81% 86.54% 71.43
V 0.00% 0.00% 0.00
Table 7: Overall results (top) and detailed results on
the WSJ test (bottom).
mappings between semantic arguments and syntac-
tic constituents). Only 0.14% out of the 75.17% F
measure obtained on the development partition are
attributed to the argument expansion heuristics in-
troduced in Section 2.4.
4 Conclusions
This paper describes a semantic role labeling sys-
tem constructed on top of the complete syntactic
analysis of text. We model semantic arguments that
map into exactly one syntactic phrase (about 90%
of all semantic arguments in the development set)
using a rich set of lexical, syntactic, and semantic
attributes. We trained AdaBoost one-versus-all clas-
sifiers for the 24 most common argument types. Ar-
guments that map to more than one syntactic con-
stituent are expanded with a simple heuristic in a
post-processing step.
Our results surpass with almost 6% the results ob-
tained by best SRL system that used partial syntax in
the CoNLL 2004 shared task evaluation. Although
the two evaluations are not directly comparable due
to differences in training set size, the current results
are encouraging given the simplicity of our proposed
system.
5 Acknowledgements
This research has been partially funded by the Euro-
pean Union project ?Computers in the Human Inter-
action Loop? (CHIL - IP506909). Mihai Surdeanu is
a research fellow within the Ramo?n y Cajal program
of the Spanish Ministry of Education and Science.
We would also like to thank Llu??s Ma`rquez and
Xavi Carreras for the help with the AdaBoost classi-
fier, for providing the set of temporal cue words, and
for the many motivating discussions.
References
X. Carreras and L. Ma`rquez. 2004. Introduction to the CoNLL-
2004 shared task: Semantic role labeling. In Proceedings of
CoNLL 2004 Shared Task.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL 2004 Shared Task.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. PhD Dissertation, University of Penn-
sylvania.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3).
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic
chunks. In Proceedings of CoNLL 2004 Shared Task.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin,
and D. Jurafsky. 2005. Support vector learning for semantic
argument classification. To appear in Journal of Machine
Learning.
R. E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proceedings of ACL 2003.
224
A Hybrid Approach for the Acquisition of
Information Extraction Patterns
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno
Technical University of Catalunya
Barcelona, Spain
{surdeanu,turmo,ageno}@lsi.upc.edu
Abstract
In this paper we present a hybrid ap-
proach for the acquisition of syntactico-
semantic patterns from raw text. Our
approach co-trains a decision list learner
whose feature space covers the set of all
syntactico-semantic patterns with an Ex-
pectation Maximization clustering algo-
rithm that uses the text words as attributes.
We show that the combination of the two
methods always outperforms the decision
list learner alone. Furthermore, using a
modular architecture we investigate sev-
eral algorithms for pattern ranking, the
most important component of the decision
list learner.
1 Introduction
Traditionally, Information Extraction (IE) identi-
fies domain-specific events, entities, and relations
among entities and/or events with the goals of:
populating relational databases, providing event-
level indexing in news stories, feeding link discov-
ery applications, etcetera.
By and large the identification and selective ex-
traction of relevant information is built around a
set of domain-specific linguistic patterns. For ex-
ample, for a ?financial market change? domain
one relevant pattern is <NOUN fall MONEY
to MONEY>. When this pattern is matched on
the text ?London gold fell $4.70 to $308.35?, a
change of $4.70 is detected for the financial in-
strument ?London gold?.
Domain-specific patterns are either hand-
crafted or acquired automatically (Riloff, 1996;
Yangarber et al, 2000; Yangarber, 2003; Steven-
son and Greenwood, 2005). To minimize annota-
tion costs, some of the latter approaches use lightly
supervised bootstrapping algorithms that require
as input only a small set of documents annotated
with their corresponding category label. The focus
of this paper is to improve such lightly supervised
pattern acquisition methods. Moreover, we focus
on robust bootstrapping algorithms that can han-
dle real-world document collections, which con-
tain many domains.
Although a rich literature covers bootstrap-
ping methods applied to natural language prob-
lems (Yarowsky, 1995; Riloff, 1996; Collins and
Singer, 1999; Yangarber et al, 2000; Yangar-
ber, 2003; Abney, 2004) several questions remain
unanswered when these methods are applied to
syntactic or semantic pattern acquisition. In this
paper we answer two of these questions:
(1) Can pattern acquisition be improved with
text categorization techniques?
Bootstrapping-based pattern acquisition algo-
rithms can also be regarded as incremental text
categorization (TC), since in each iteration docu-
ments containing certain patterns are assigned the
corresponding category label. Although TC is ob-
viously not the main goal of pattern acquisition
methodologies, it is nevertheless an integral part of
the learning algorithm: each iteration of the acqui-
sition algorithm depends on the previous assign-
ments of category labels to documents. Hence, if
the quality of the TC solution proposed is bad, the
quality of the acquired patterns will suffer.
Motivated by this observation, we introduce a
co-training-based algorithm (Blum and Mitchell,
1998) that uses a text categorization algorithm as
reinforcement for pattern acquisition. We show,
using both a direct and an indirect evaluation, that
the combination of the two methodologies always
improves the quality of the acquired patterns.
48
(2) Which pattern selection strategy is best?
While most bootstrapping-based algorithms fol-
low the same framework, they vary significantly
in what they consider the most relevant patterns in
each bootstrapping iteration. Several approaches
have been proposed in the context of word sense
disambiguation (Yarowsky, 1995), named entity
(NE) classification (Collins and Singer, 1999),
pattern acquisition for IE (Riloff, 1996; Yangarber,
2003), or dimensionality reduction for text catego-
rization (TC) (Yang and Pedersen, 1997). How-
ever, it is not clear which selection approach is
the best for the acquisition of syntactico-semantic
patterns. To answer this question, we have im-
plemented a modular pattern acquisition architec-
ture where several of these ranking strategies are
implemented and evaluated. The empirical study
presented in this paper shows that a strategy previ-
ously proposed for feature ranking for NE recogni-
tion outperforms algorithms designed specifically
for pattern acquisition.
The paper is organized as follows: Sec-
tion 2 introduces the bootstrapping framework
used throughout the paper. Section 3 introduces
the data collections. Section 4 describes the di-
rect and indirect evaluation procedures. Section 5
introduces a detailed empirical evaluation of the
proposed system. Section 6 concludes the paper.
2 The Pattern Acquisition Framework
In this section we introduce a modular pattern ac-
quisition framework that co-trains two different
views of the document collection: the first view
uses the collection words to train a text categoriza-
tion algorithm, while the second view bootstraps
a decision list learner that uses all syntactico-
semantic patterns as features. The rules acquired
by the latter algorithm, of the form p ? y, where
p is a pattern and y is a domain label, are the out-
put of the overall system. The system can be cus-
tomized with several pattern selection strategies
that dramatically influence the quality and order
of the acquired rules.
2.1 Co-training Text Categorization and
Pattern Acquisition
Given two views of a classification task, co-
training (Blum and Mitchell, 1998) bootstraps a
separate classifier for each view as follows: (1)
it initializes both classifiers with the same small
amount of labeled data (i.e. seed documents in our
case); (2) it repeatedly trains both classifiers us-
ing the currently labeled data; and (3) after each
learning iteration, the two classifiers share all or a
subset of the newly labeled examples (documents
in our particular case).
The intuition is that each classifier provides
new, informative labeled data to the other classi-
fier. If the two views are conditional independent
and the two classifiers generally agree on unla-
beled data they will have low generalization error.
In this paper we focus on a ?naive? co-training ap-
proach, which trains a different classifier in each
iteration and feeds its newly labeled examples to
the other classifier. This approach was shown to
perform well on real-world natural language prob-
lems (Collins and Singer, 1999).
Figure 1 illustrates the co-training framework
used in this paper. The feature space of the
first view contains only lexical information, i.e.
the collection words, and uses as classifier Ex-
pectation Maximization (EM) (Dempster et al,
1977). EM is actually a class of iterative algo-
rithms that find maximum likelihood estimates of
parameters using probabilistic models over incom-
plete data (e.g. both labeled and unlabeled docu-
ments) (Dempster et al, 1977). EM was theoret-
ically proven to converge to a local maximum of
the parameters? log likelihood. Furthermore, em-
pirical experiments showed that EM has excellent
performance for lightly-supervised text classifica-
tion (Nigam et al, 2000). The EM algorithm used
in this paper estimates its model parameters us-
ing the Naive Bayes (NB) assumptions, similarly
to (Nigam et al, 2000). From this point further,
we refer to this instance of the EM algorithm as
NB-EM.
The feature space of the second view contains
the syntactico-semantic patterns, generated using
the procedure detailed in Section 3.2. The second
learner is the actual pattern acquisition algorithm
implemented as a bootstrapped decision list clas-
sifier.
The co-training algorithm introduced in this pa-
per interleaves one iteration of the NB-EM algo-
rithm with one iteration of the pattern acquisition
algorithm. If one classifier converges faster (e.g.
NB-EM typically converges in under 20 iterations,
whereas the acquisition algorithms learns new pat-
terns for hundreds of iterations) we continue boot-
strapping the other classifier alone.
2.2 The Text Categorization Algorithm
The parameters of the generative NB model, ??, in-
clude the probability of seeing a given category,
49
pattern
Initialize
acquisition
Labeled seed documents
Unlabeled documents Iteration
NB?EM Patternacquisition
iteration
Pattern
acquisition
terminated?
NB?EM
converged?
Ranking
method
Initialize
NB?EM
No
Yes
Patterns
Yes
No
Figure 1: Co-training framework for pattern acquisition.
1. Initialization:
? Initialize the set of labeled examples with n la-
beled seed documents of the form (di, yi). yi is
the label of the ith document di. Each docu-
ment di contains a set of patterns {pi1, pi2, ..., pim}.
? Initialize the list of learned rules R = {}.
2. Loop:
? For each label y, select a small set of pattern
rules r = p ? y, r /? R.
? Append all selected rules r to R.
? For all non-seed documents d that contain a
pattern in R, set label(d) = argmaxp,y strength(p, y).
3. Termination condition:
? Stop if no rules selected or maximum number
of iterations reached.
Figure 2: Pattern acquisition meta algorithm
P (c|??), and the probability of seeing a word given
a category, P (w|c; ??). We calculate both simi-
larly to Nigam (2000). Using these parameters,
the word independence assumption typical to the
Naive Bayes model, and the Bayes rule, the prob-
ability that a document d has a given category c is
calculated as:
P (c|d; ??) = P (c|??)P (d|c; ??)
P (d|??)
(1)
= P (c|??)?
|d|
i=1P (wi|c; ??)
?q
j=1 P (cj |??)?
|d|
i=1P (wi|cj ; ??)
(2)
2.3 The Pattern Acquisition Algorithm
The lightly-supervised pattern acquisition algo-
rithm iteratively learns domain-specific IE pat-
terns from a small set of labeled documents and
a much larger set of unlabeled documents. Dur-
ing each learning iteration, the algorithm acquires
a new set of patterns and labels more documents
based on the new evidence. The algorithm output
is a list R of rules p ? y, where p is a pattern
in the set of patterns P , and y a category label in
Y = {1...k}, k being the number of categories in
the document collection. The list of acquired rules
R is sorted in descending order of rule importance
to guarantee that the most relevant rules are ac-
cessed first. This generic bootstrapping algorithm
is formalized in Figure 2.
Previous studies called the class of algorithms
illustrated in Figure 2 ?cautious? or ?sequential?
because in each iteration they acquire 1 or a small
set of rules (Abney, 2004; Collins and Singer,
1999). This strategy stops the algorithm from be-
ing over-confident, an important restriction for an
algorithm that learns from large amounts of unla-
beled data. This approach was empirically shown
to perform better than a method that in each itera-
tion acquires all rules that match a certain criterion
(e.g. the corresponding rule has a strength over a
certain threshold).
The key element where most instances of this
algorithm vary is the select procedure, which de-
cides which rules are acquired in each iteration.
Although several selection strategies have been
previously proposed for various NLP problems, to
our knowledge no existing study performs an em-
pirical analysis of such strategies in the context of
acquisition of IE patterns. For this reason, we im-
plement several selection methods in our system
(described in Section 2.4) and evaluate their per-
formance in Section 5.
The label of each collection document is given
by the strength of its patterns. Similarly to (Collins
and Singer, 1999; Yarowsky, 1995), we define the
strength of a pattern p in a category y as the pre-
cision of p in the set of documents labeled with
category y, estimated using Laplace smoothing:
strength(p, y) = count(p, y) + count(p) + k (3)
where count(p, y) is the number of documents la-
beled y containing pattern p, count(p) is the over-
all number of labeled documents containing p, and
k is the number of domains. For all experiments
presented here we used  = 1.
Another point where acquisition algorithms dif-
fer is the initialization procedure: some start with a
small number of hand-labeled documents (Riloff,
1996), as illustrated in Figure 2, while others start
with a set of seed rules (Yangarber et al, 2000;
Yangarber, 2003). However, these approaches are
conceptually similar: the seed rules are simply
used to generate the seed documents.
This paper focuses on the framework introduced
in Figure 2 for two reasons: (a) ?cautious? al-
50
gorithms were shown to perform best for several
NLP problems (including acquisition of IE pat-
terns), and (b) it has nice theoretical properties:
Abney (2004) showed that, regardless of the selec-
tion procedure, ?sequential? bootstrapping algo-
rithms converge to a local minimum of K, where
K is an upper bound of the negative log likelihood
of the data. Obviously, the quality of the local
minimum discovered is highly dependent of the
selection procedure, which is why we believe an
evaluation of several pattern selection strategies is
important.
2.4 Selection Criteria
The pattern selection component, i.e. the select
procedure of the algorithm in Figure 2, consists of
the following: (a) for each category y all patterns
p are sorted in descending order of their scores in
the current category, score(p, y), and (b) for each
category the top k patterns are selected. For all
experiments in this paper we have used k = 3.
We provide four different implementations for the
pattern scoring function score(p, y) according to
four different selection criteria.
Criterion 1: Riloff
This selection criterion was developed specifically
for the pattern acquisition task (Riloff, 1996) and
has been used in several other pattern acquisition
systems (Yangarber et al, 2000; Yangarber, 2003;
Stevenson and Greenwood, 2005). The intuition
behind it is that a qualitative pattern is yielded by a
compromise between pattern precision (which is a
good indicator of relevance) and pattern frequency
(which is a good indicator of coverage). Further-
more, the criterion considers only patterns that are
positively correlated with the corresponding cate-
gory, i.e. their precision is higher than 50%. The
Riloff score of a pattern p in a category y is for-
malized as:
score(p, y) =
{
prec(p, y) log(count(p, y)),
if prec(p, y) > 0.5;
0, otherwise.
(4)
prec(p, y) = count(p, y)count(p) (5)
where prec(p, y) is the raw precision of pattern p
in the set of documents labeled with category y.
Criterion 2: Collins
This criterion was used in a lightly-supervised NE
recognizer (Collins and Singer, 1999). Unlike the
previous criterion, which combines relevance and
frequency in the same scoring function, Collins
considers only patterns whose raw precision is
over a hard threshold T and ranks them by their
global coverage:
score(p, y) =
{
count(p), if prec(p, y) > T ;
0, otherwise. (6)
Similarly to (Collins and Singer, 1999) we used
T = 0.95 for all experiments reported here.
Criterion 3: ?2 (Chi)
The ?2 score measures the lack of independence
between a pattern p and a category y. It is com-
puted using a two-way contingency table of p and
y, where a is the number of times p and y co-occur,
b is the number of times p occurs without y, c is
the number of times y occurs without p, and d is
the number of times neither p nor y occur. The
number of documents in the collection is n. Sim-
ilarly to the first criterion, we consider only pat-
terns positively correlated with the corresponding
category:
score(p, y) =
{
?2(p, y), if prec(p, y) > 0.5;
0, otherwise. (7)
?2(p, y) = n(ad? cb)
2
(a + c)(b + d)(a + b)(c + d) (8)
The ?2 statistic was previously reported to be
the best feature selection strategy for text catego-
rization (Yang and Pedersen, 1997).
Criterion 4: Mutual Information (MI)
Mutual information is a well known information
theory criterion that measures the independence of
two variables, in our case a pattern p and a cate-
gory y (Yang and Pedersen, 1997). Using the same
contingency table introduced above, the MI crite-
rion is estimated as:
score(p, y) =
{
MI(p, y), if prec(p, y) > 0.5;
0, otherwise. (9)
MI(p, y) = log P (p ? y)P (p)? P (y) (10)
? log na(a + c)(a + b) (11)
3 The Data
3.1 The Document Collections
For all experiments reported in this paper we used
the following three document collections: (a) the
AP collection is the Associated Press (year 1999)
subset of the AQUAINT collection (LDC catalog
number LDC2002T31); (b) the LATIMES collec-
tion is the Los Angeles Times subset of the TREC-
5 collection1; and (c) the REUTERS collection is
the by now classic Reuters-21578 text categoriza-
tion collection2.
1http://trec.nist.gov/data/docs eng.html
2http://trec.nist.gov/data/reuters/reuters.html
51
Collection # of docs # of categories # of words # of patterns
AP 5000 7 24812 140852
LATIMES 5000 8 29659 69429
REUTERS 9035 10 12905 36608
Table 1: Document collections used in the evaluation
Similarly to previous work, for the REUTERS
collection we used the ModApte split and selected
the ten most frequent categories (Nigam et al,
2000). Due to memory limitations on our test ma-
chines, we reduced the size of the AP and LA-
TIMES collections to their first 5,000 documents
(the complete collections contain over 100,000
documents).
The collection words were pre-processed as fol-
lows: (i) stop words and numbers were discarded;
(ii) all words were converted to lower case; and
(iii) terms that appear in a single document were
removed. Table 1 lists the collection characteris-
tics after pre-processing.
3.2 Pattern Generation
In order to extract the set of patterns available in
a document, each collection document undergoes
the following processing steps: (a) we recognize
and classify named entities3, and (b) we generate
full parse trees of all document sentences using a
probabilistic context-free parser.
Following the above processing steps, we ex-
tract Subject-Verb-Object (SVO) tuples using a se-
ries of heuristics, e.g.: (a) nouns preceding active
verbs are subjects, (b) nouns directly attached to a
verb phrase are objects, (c) nouns attached to the
verb phrase through a prepositional attachment are
indirect objects. Each tuple element is replaced
with either its head word, if its head word is not
included in a NE, or with the NE category oth-
erwise. For indirect objects we additionally store
the accompanying preposition. Lastly, each tuple
containing more than two elements is generalized
by maintaining only subsets of two and three of its
elements and replacing the others with a wildcard.
Table 2 lists the patterns extracted from one
sample sentence. As Table 2 hints, the system
generates a large number of candidate patterns. It
is the task of the pattern acquisition algorithm to
extract only the relevant ones from this complex
search space.
4 The Evaluation Procedures
4.1 The Indirect Evaluation Procedure
The goal of our evaluation procedure is to measure
the quality of the acquired patterns. Intuitively,
3We identify six categories: persons, locations, organiza-
tions, other names, temporal and numerical expressions.
Text The Minnesota Vikings beat the Arizona
Cardinals in yesterday?s game.
Patterns s(ORG) v(beat)
v(beat) o(ORG)
s(ORG) o(ORG)
v(beat) io(in game)
s(ORG) io(in game)
o(ORG) io(in game)
s(ORG) v(beat) o(ORG)
s(ORG) v(beat) io(in game)
v(beat) o(ORG) io(in game)
Table 2: Patterns extracted from one sample sentence. s
stands for subject, v for verb, o for object, and io for indirect
object.
the learned patterns should have high coverage and
low ambiguity. We indirectly measure the quality
of the acquired patterns using a text categorization
strategy: we feed the acquired rules to a decision-
list classifier, which is then used to classify a new
set of documents. The classifier assigns to each
document the category label given by the first rule
whose pattern matches. Since we expect higher-
quality patterns to appear higher in the rule list,
the decision-list classifier never changes the cate-
gory of an already-labeled document.
The quality of the generated classification is
measured using micro-averaged precision and re-
call:
P =
?q
i=1 TruePositivesi
?q
i=1(TruePositivesi + FalsePositivesi)
(12)
R =
?q
i=1 TruePositivesi
?q
i=1(TruePositivesi + FalseNegativesi)
(13)
where q is the number of categories in the docu-
ment collection.
For all experiments and all collections with the
exception of REUTERS, which has a standard
document split for training and testing, we used 5-
fold cross validation: we randomly partitioned the
collections into 5 sets of equal sizes, and reserved
a different one for testing in each fold.
We have chosen this evaluation strategy because
this indirect approach was shown to correlate well
with a direct evaluation, where the learned patterns
were used to customize an IE system (Yangarber
et al, 2000). For this reason, much of the fol-
lowing work on pattern acquisition has used this
approach as a de facto evaluation standard (Yan-
garber, 2003; Stevenson and Greenwood, 2005).
Furthermore, given the high number of domains
and patterns (we evaluate on 25 domains), an eval-
uation by human experts is extremely costly. Nev-
ertheless, to show that the proposed indirect eval-
uation correlates well with a direct evaluation, two
human experts have evaluated the patterns in sev-
eral domains. The direct evaluation procedure is
described next.
52
4.2 The Direct Evaluation Procedure
The task of manually deciding whether an ac-
quired pattern is relevant or not for a given domain
is not trivial, mainly due to the ambiguity of the
patterns. Thus, this process should be carried out
by more than one expert, so that the relevance of
the ambiguous patterns can be agreed upon. For
example, the patterns s(ORG) v(score) o(goal) and
s(PER) v(lead) io(with point) are clearly relevant
only for the sports domain, whereas the patterns
v(sign) io(as agent) and o(title) io(in DATE) might
be regarded as relevant for other domains as well.
The specific procedure to manually evaluate the
patterns is the following: (1) two experts sepa-
rately evaluate the acquired patterns for the con-
sidered domains and collections; and (2) the re-
sults of both evaluations are compared. For any
disagreement, we have opted for a strict evalua-
tion: all the occurrences of the corresponding pat-
tern are looked up in the collection and, whenever
at least one pattern occurrence belongs to a docu-
ment assigned to a different domain than the do-
main in question, the pattern will be considered as
not relevant.
Both the ambiguity and the high number of
the extracted patterns have prevented us from per-
forming an exhaustive direct evaluation. For this
reason, only the top (most relevant) 100 patterns
have been evaluated for one domain per collection.
The results are detailed in Section 5.2.
5 Experimental Evaluation
5.1 Indirect Evaluation
For a better understanding of the proposed ap-
proach we perform an incremental evaluation:
first, we evaluate only the various pattern selection
criteria described in Section 2.4 by disabling the
NB-EM component. Second, using the best selec-
tion criteria, we evaluate the complete co-training
system.
In both experiments we initialize the system
with high-precision manually-selected seed rules
which yield seed documents with a coverage of
10% of the training partitions. The remaining 90%
of the training documents are maintained unla-
beled. For all experiments we used a maximum of
400 bootstrapping iterations. The acquired rules
are fed to the decision list classifier which assigns
category labels to the documents in the test parti-
tions.
Evaluation of the pattern selection criteria
Figure 3 illustrates the precision/recall charts
of the four algorithms as the number of patterns
made available to the decision list classifier in-
creases. All charts show precision/recall points
starting after 100 learning iterations with 100-
iteration increments. It is immediately obvious
that the Collins selection criterion performs sig-
nificantly better than the other three criteria. For
the same recall point, Collins yields a classifica-
tion model with much higher precision, with dif-
ferences ranging from 5% in the REUTERS col-
lection to 20% in the AP collection.
Theorem 5 in (Abney, 2002) provides a theo-
retical explanation for these results: if certain in-
dependence conditions between the classifier rules
are satisfied and the precision of each rule is larger
than a threshold T , then the precision of the final
classifier is larger than T . Although the rule inde-
pendence conditions are certainly not satisfied in
our real-world evaluation, the above theorem in-
dicates that there is a strong relation between the
precision of the classifier rules on labeled data and
the precision of the final classifier. Our results pro-
vide the empirical proof that controling the preci-
sion of the acquired rules (i.e. the Collins crite-
rion) is important.
The Collins criterion controls the recall of the
learned model by favoring rules with high fre-
quency in the collection. However, since the other
two criteria do not use a high precision thresh-
old, they will acquire more rules, which translates
in better recall. For two out of the three collec-
tions, Riloff and Chi obtain a slightly better recall,
about 2% higher than Collins?, albeit with a much
lower precision. We do not consider this an im-
portant advantage: in the next section we show
that co-training with the NB-EM component fur-
ther boosts the precision and recall of the Collins-
based acquisition algorithm.
The MI criterion performs the worst of the four
evaluated criteria. A clue for this behavior lies in
the following equivalent form for MI: MI(p, y) =
logP (p|y)?logP (p). This formula indicates that,
for patterns with equal conditional probabilities
P (p|y), MI assigns higher scores to patterns with
lower frequency. This is not the desired behavior
in a TC-oriented system.
Evaluation of the co-training system
Figure 4 compares the performance of the
stand-alone pattern acquisition algorithm (?boot-
strapping?) with the performance of the acquisi-
tion algorithm trained in the co-training environ-
53
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55
Pr
ec
is
io
n
Recall
collins
riloff
chi
mi
(a)
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.1  0.15  0.2  0.25  0.3  0.35  0.4
Pr
ec
is
io
n
Recall
collins
riloff
chi
mi
(b)
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45
Pr
ec
is
io
n
Recall
collins
riloff
chi
mi
(c)
Figure 3: Performance of the pattern acquisition algorithm for various pattern selection strategies and multiple collections:
(a) AP, (b) LATIMES, and (c) REUTERS
ment (?co-training?). For both setups we used the
best pattern selection criterion for pattern acqui-
sition, i.e. the Collins criterion. To put things in
perspective, we also depict the performance ob-
tained with a baseline system, i.e. the system con-
figured to use the Riloff pattern selection criterion
and without the NB-EM algorithm (?baseline?).
To our knowledge, this system, or a variation of
it, is the current state-of-the-art in pattern acqui-
sition (Riloff, 1996; Yangarber et al, 2000; Yan-
garber, 2003; Stevenson and Greenwood, 2005).
All algorithms were initialized with the same seed
rules and had access to all documents.
Figure 4 shows that the quality of the learned
patterns always improves if the pattern acquisi-
tion algorithm is ?reinforced? with EM. For the
same recall point, the patterns acquired in the
co-training environment yield classification mod-
els with precision (generally) much larger than
the models generated by the pattern acquisition
algorithm alone. When using the same pat-
tern acquisition criterion, e.g. Collins, the dif-
ferences between the co-training approach and
the stand-alone pattern acquisition method (?boot-
strapping?) range from 2-3% in the REUTERS
collection to 20% in the LATIMES collection.
These results support our intuition that the sparse
pattern space is insufficient to generate good clas-
sification models, which directly influences the
quality of all acquired patterns.
Furthermore, due to the increased coverage of
the lexicalized collection views, the patterns ac-
quired in the co-training setup generally have bet-
ter recall, up to 11% higher in the LATIMES col-
lection.
Lastly, the comparison of our best system (?co-
training?) against the current state-of-the-art (our
?baseline?) draws an even more dramatic picture:
Collection Domain Relevant Relevant Initial
patterns patterns inter-expert
baseline co-training agreement
AP Sports 22% 68% 84%
LATIMES Financial 67% 76% 70%
REUTERS Corporate 38% 46% 66%
Acquisitions
Table 3: Percentage of relevant patterns for one domain per
collection by the baseline system (Riloff) and the co-training
system.
for the same recall point, the co-training system
obtains a precision up to 35% higher for AP and
LATIMES, and up to 10% higher for REUTERS.
5.2 Direct Evaluation
As stated in Section 4.2, two experts have man-
ually evaluated the top 100 acquired patterns for
one different domain in each of the three collec-
tions. The three corresponding domains have been
selected intending to deal with different degrees of
ambiguity, which are reflected in the initial inter-
expert agreement. Any disagreement between ex-
perts is solved using the algorithm introduced in
Section 4.2. Table 3 shows the results of this di-
rect evaluation. The co-training approach outper-
forms the baseline for all three collections. Con-
cretely, improvements of 9% and 8% are achieved
for the Financial and the Corporate Acquisitions
domains, and 46%, by far the largest difference, is
found for the Sports domain in AP. Table 4 lists
the top 20 patterns extracted by both approaches
in the latter domain. It can be observed that for
the baseline, only the top 4 patterns are relevant,
the rest being extremely general patterns. On the
other hand, the quality of the patterns acquired by
our approach is much higher: all the patterns are
relevant to the domain, although 7 out of the 20
might be considered ambiguous and according to
the criterion defined in Section 4.2 have been eval-
uated as not relevant.
54
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.3  0.35  0.4  0.45  0.5  0.55  0.6
Pr
ec
is
io
n
Recall
co-training
bootstrapping
baseline
(a)
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.2  0.25  0.3  0.35  0.4  0.45
Pr
ec
is
io
n
Recall
co-training
bootstrapping
baseline
(b)
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.15  0.2  0.25  0.3  0.35  0.4  0.45
Pr
ec
is
io
n
Recall
co-training
bootstrapping
baseline
(c)
Figure 4: Comparison of the bootstrapping pattern acquisition algorithm with the co-training approach: (a) AP, (b) LATIMES,
and (c) REUTERS
Baseline Co-training
s(he) o(game) v(win) o(title)
v(miss) o(game) s(I) v(play)
v(play) o(game) s(he) v(game)
v(play) io(in LOC) s(we) v(play)
v(go) o(be) v(miss) o(game)
s(he) v(be) s(he) v(coach)
s(that) v(be) v(lose) o(game)
s(I) v(be) s(I) o(play)
s(it) v(go) o(be) v(make) o(play)
s(it) v(be) v(play) io(in game)
s(I) v(think) v(want) o(play)
s(I) v(know) v(win) o(MISC)
s(I) v(want) s(he) o(player)
s(there) v(be) v(start) o(game)
s(we) v(do) s(PER) o(contract)
v(do) o(it) s(we) o(play)
s(it) o(be) s(team) v(win)
s(we) v(are) v(rush) io(for yard)
s(we) v(go) s(we) o(team)
s(PER) o(DATE) v(win) o(Bowl)
Table 4: Top 20 patterns acquired from the Sports domain
by the baseline system (Riloff) and the co-training system for
the AP collection. The correct patterns are in bold.
6 Conclusions
This paper introduces a hybrid, lightly-supervised
method for the acquisition of syntactico-semantic
patterns for Information Extraction. Our approach
co-trains a decision list learner whose feature
space covers the set of all syntactico-semantic
patterns with an Expectation Maximization clus-
tering algorithm that uses the text words as at-
tributes. Furthermore, we customize the decision
list learner with up to four criteria for pattern se-
lection, which is the most important component of
the acquisition algorithm.
For the evaluation of the proposed approach we
have used both an indirect evaluation based on
Text Categorization and a direct evaluation where
human experts evaluated the quality of the gener-
ated patterns. Our results indicate that co-training
the Expectation Maximization algorithm with the
decision list learner tailored to acquire only high
precision patterns is by far the best solution. For
the same recall point, the proposed method in-
creases the precision of the generated models up
to 35% from the previous state of the art. Further-
more, the combination of the two feature spaces
(words and patterns) also increases the coverage
of the acquired patterns. The direct evaluation of
the acquired patterns by the human experts vali-
dates these results.
References
S. Abney. 2002. Bootstrapping. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics.
S. Abney. 2004. Understanding the Yarowsky algorithm.
Computational Linguistics, 30(3).
A. Blum and T. Mitchell. 1998. Combining labeled and un-
labeled data with co-training. In Proceedings of the 11th
Annual Conference on Computational Learning Theory.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of EMNLP.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series B,
39(1).
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000.
Text classification from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3).
E. Riloff. 1996. Automatically generating extraction patterns
from untagged text. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-96).
M. Stevenson and M. Greenwood. 2005. A semantic ap-
proach to ie pattern induction. In Proceedings of the 43rd
Meeting of the Association for Computational Linguistics.
Y. Yang and J. O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proceed-
ings of the Fourteenth International Conference on Ma-
chine Learning.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hutunen.
2000. Automatic acquisition of domain knowledge for in-
formation extraction. In Proceedings of the 18th Interna-
tional Conference of Computational Linguistics (COLING
2000).
R. Yangarber. 2003. Counter-training in discovery of se-
mantic patterns. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics (ACL
2003).
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proceedings of the
33rd Annual Meeting of the Association for Computa-
tional Linguistics.
55
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49?57,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Analysis of Bootstrapping for the Recognition of Temporal Expressions
Jordi Poveda
TALP Research Center
Technical University of Catalonia (UPC)
Barcelona, Spain
jpoveda@lsi.upc.edu
Mihai Surdeanu
NLP Group
Stanford University
Stanford, CA
mihais@stanford.edu
Jordi Turmo
TALP Research Center
Technical University of Catalonia (UPC)
Barcelona, Spain
turmo@lsi.upc.edu
Abstract
We present a semi-supervised (bootstrapping)
approach to the extraction of time expression
mentions in large unlabelled corpora. Because
the only supervision is in the form of seed
examples, it becomes necessary to resort to
heuristics to rank and filter out spurious pat-
terns and candidate time expressions. The
application of bootstrapping to time expres-
sion recognition is, to the best of our knowl-
edge, novel. In this paper, we describe one
such architecture for bootstrapping Informa-
tion Extraction (IE) patterns ?suited to the
extraction of entities, as opposed to events or
relations? and summarize our experimental
findings. These point out to the fact that a
pattern set with a good increase in recall with
respect to the seeds is achievable within our
framework while, on the other side, the de-
crease in precision in successive iterations is
succesfully controlled through the use of rank-
ing and selection heuristics. Experiments are
still underway to achieve the best use of these
heuristics and other parameters of the boot-
strapping algorithm.
1 Introduction
The problem of time expression recognition refers
to the identification in free-format natural language
text of the occurrences of expressions that denote
time. Time-denoting expressions appear in a great
diversity of forms, beyond the most obvious ab-
solute time or date references (e.g. 11pm, Febru-
ary 14th, 2005): time references that anchor on an-
other time (three hours after midnight, two weeks be-
fore Christmas), expressions denoting durations (a
few months), expressions denoting recurring times
(every third month, twice in the hour), context-
dependent times (today, last year), vague references
(somewhere in the middle of June, the near future)
or times that are indicated by an event (the day G.
Bush was reelected). This problem is a subpart of
a task called TERN (Temporal Expression Recog-
nition and Normalization), where temporal expres-
sions are first identified in text and then its intended
temporal meaning is represented in a canonical for-
mat. TERN was first proposed as an independent
task in the 2004 edition of the ACE conferences1.
The most widely used standard for the annotation of
temporal expressions is TIMEX (Ferro et al, 2005).
The most common approach to temporal expres-
sion recognition in the past has been the use of
hand-made grammars to capture the expressions (see
(Wiebe et al, 1998; Filatova and Hovy, 2001; Sa-
quete et al, 2004) for examples), which can then
be easily expanded with additional attributes for the
normalization task, based on computing distance
and direction (past or future) with respect to a ref-
erence time. This approach achieves an F1-measureof approximately 85% for recognition and normal-
ization. The use of machine learning techniques ?
mainly statistical? for this task is a more recent
development, either alongside the traditional hand-
grammar approach to learn to distinguish specific
difficult cases (Mani and Wilson, 2000), or on its
own (Hacioglu et al, 2005). The latter apply SVMs
to the recognition task alone, using the output of sev-
eral human-made taggers as additional features for
the classifier, and report an F1-measure of 87.8%.
1http://www.nist.gov/speech/tests/ace/
49
Bootstrapping techniques have been used for such
diverse NLP problems as: word sense disambigua-
tion (Yarowsky, 1995), named entity classification
(Collins and Singer, 1999), IE pattern acquisition
(Riloff, 1996; Yangarber et al, 2000; Yangarber,
2003; Stevenson and Greenwood, 2005), document
classification (Surdeanu et al, 2006), fact extraction
from the web (Pas?ca et al, 2006) and hyponymy re-
lation extraction (Kozareva et al, 2008).
(Yarowsky, 1995) used bootstrapping to train de-
cision list classifiers to disambiguate between two
senses of a word, achieving impressive classification
accuracy. (Collins and Singer, 1999) applied boot-
strapping to extract rules for named entity (NE) clas-
sification, seeding the sytem with a few handcrafted
rules. Their main innovation was to split training
in two alternate stages: during one step, only con-
textual rules are sought; during the second step, the
new contextual rules are used to tag further NEs and
these are used to produce new spelling rules.
Bootstrapping approaches are employed in
(Riloff, 1996), (Yangarber et al, 2000), (Yangarber,
2003), and (Stevenson and Greenwood, 2005)
in order to find IE patterns for domain-specific
event extraction. (Pas?ca et al, 2006) employ a
bootstrapping process to extract general facts from
the Web, viewed as two-term relationships (e.g
[Donald Knuth, 1938] could be an instance of
a ?born in year? relationship). (Surdeanu et al,
2006) used bootstrapping co-trained with an EM
classifier in order to perform topic classification
of documents based on the presence of certain
learned syntactic-semantic patterns. In (Kozareva
et al, 2008), bootstrapping is applied to finding
new members of certain class of objects (i.e. an
?is-a? relationship), by providing a member of the
required class as seed and using a ?such as? type of
textual pattern to locate new instances.
The recognition of temporal expressions is cru-
cial for many applications in NLP, among them: IE,
Question Answering (QA) and Automatic Summa-
rization (for the temporal ordering of events). Work
on slightly supervised approaches such as bootstrap-
ping is justified by the large availability of unla-
belled corpora, as opposed to tagged ones, from
which to learn models for recognition.
2 Architecture
Figure 1 illustrates the building blocks of the algo-
rithm and their interactions, along with input and
output data.
The inputs to the bootstrapping algorithm are the
unlabelled training corpus and a file of seed ex-
amples. The unlabelled corpus is a large collec-
tion of documents which has been tokenized, POS
tagged, lemmatized, and syntactically analyzed for
basic syntactic constituents (shallow parsing) and
headwords. The second input is a set of seed exam-
ples, consisting of a series of token sequences which
we assume to be correct time expressions. The seeds
are supplied without additional features, and without
context information.
Our bootstrapping algorithm works with two al-
ternative views of the same target data (time expres-
sions), that is: patterns and examples (i.e. an in-
stance of a pattern in the corpus). A pattern is a gen-
eralized representation that can match any sequence
of tokens meeting the conditions expressed in the
pattern (these can be morphological, semantic, syn-
tactic and contextual). An example is an actual can-
didate occurrence of a time expression. Patterns are
generated from examples found in the corpus and,
in its turn, new examples are found by searching
for matches of new patterns. Both patterns and ex-
amples may carry contextual information, that is, a
window of tokens left and right of the candidate time
expression.
Output examples and output patterns are the out-
puts of the bootstrapping process. Both the set of
output examples and the set of output patterns are
increased with each new iteration, by adding the new
candidate examples (respectively, patterns) that have
been ?accepted? during the last iteration (i.e. those
that have passed the ranking and selection step).
Initially, a single pass through the corpus is per-
formed in order to find occurrences of the seeds in
the text. Thus, we bootstrap an initial set of exam-
ples. From then on, the bootstrapping process con-
sists of a succession of iterations with the following
steps:
1. Ranking and selection of examples: Each ex-
ample produced during any of the previous it-
erations, 0 to i ? 1, is assigned a score (rank-
ing). The top n examples are selected to grow
the set of output examples (selection) and will
50
Figure 1: Block diagram of bootstrapping algorithm
be used for the next step. The details are given
in Section 4.2.
2. Generation of candidate patterns: Candidate
patterns for the current iteration are generated
from the selected examples of the previous step
(discussed in Section 3).
3. Ranking and selection of candidate patterns:
Each pattern from the current iteration is as-
signed a score and the top m patterns are se-
lected to grow the set of output patterns and to
be used in the next step (discussed in Section
4.1). This step also involves a process of analy-
sis of subsumptions, performed simultaneously
with selection, in which the set of selected pat-
terns is examined and those that are subsumed
by other patterns are discarded.
4. Search for instances of the selected patterns:
The training corpus is traversed, in order to
search for instances (matches) of the selected
patterns, which, together with the accepted ex-
amples from all previous iterations, will form
the set of candidate examples for iteration i+1.
Also, in order to relax the matching of pat-
terns to corpus tokens and of token forms among
themselves, the matching of token forms is case-
insensitive, and all the digits in a token are gen-
eralized to a generic digit marker (for instance,
?12-23-2006? is internally rewritten as ?@@-@@-
@@@@?).
Even though our architecture is built on a tradi-
tional boostrapping approach, there are several ele-
ments that are novel, at least in the context of tem-
poral expression recognition: a) our pattern repre-
sentation incorporates full syntax and distributional
semantics in a unified model (see Section 3); b) our
pattern ranking/selection approach includes a sub-
sumption model to limit redundancy; c) the formu-
lae in our example ranking/selection approach are
designed to work with variable-length expressions
that incorporate a context.
3 Pattern representation
Patterns capture both the sequence of tokens that
integrate a potential time expression (i.e. a time
expression mention), and information from the left
and right context where it occurs (up to a bounded
length). Let us call prefix the part of the pattern that
represents the left context, infix the part that repre-
sents a potential time expression mention and postfix
the part that represents the right context.
The EBNF grammar that encodes our pattern rep-
resentation is given in Figure 2. Patterns are com-
posed of multiple pattern elements (PEs). A pattern
element is the minimal unit that is matched against
the tokens in the text, and a single pattern element
can match to one or several tokens, depending on
the pattern element type. A pattern is considered to
match a sequence of tokens in the text when: first,
all the PEs from the infix are matched (this gives the
potential time expression mention) and, second, all
the PEs from the prefix and the postfix are matched
(this gives the left and right context information for
the new candidate example, respectively). There-
fore, patterns with a larger context window are more
restrictive, because all of the PEs in the prefix and
the postfix have to be matched (on top of the infix)
for the pattern to yield a match.
We distinguish among token-level generalizations
51
pattern ::= prefix SEP infix SEP postfix SEP
(modifiers)*
prefix ::= (pattern-elem)*
infix ::= (pattern-elem)+
postfix ::= (pattern-elem)*
pattern-elem ::= FORM "(" token-form ")" |
SEMCLASS "(" token-form ")" |
POS "(" pos-tag ")" | LEMMA "(" lemma-form ")" |
SYN "(" syn-type "," head ")" |
SYN-SEM "(" syn-type "," head ")"
modifiers ::= COMPLETE-PHRASE
Figure 2: The EBNF Grammar for Patterns
(i.e. PEs) and chunk-level generalizations. The for-
mer have been generated from the features of a sin-
gle token and will match to a single token in the text.
The latter have been generated from and match to a
sequence of tokens in the text (e.g. a basic syntactic
chunk). Patterns are built from the following types
of PEs (which can be seen in the grammar from Fig-
ure 2):
1. Token form PEs: The more restrictive, only
match a given token form.
2. Semantic class PEs: Match tokens (sometimes
multiwords) that belong to a given semantic
similarity class. This concept is defined below.
3. POS tag PEs: Match tokens with a given POS.
4. Lemma PEs: Match tokens with a given
lemma.
5. Syntactic chunk PEs: Match a sequence of to-
kens that is a syntactic chunk of a given type
(e.g. NP) and whose headword has the same
lemma as indicated.
6. Generalized syntactic PEs: Same as the previ-
ous, but the lemma of the headword may be any
in a given semantic similarity class.
The semantic similarity class of a word is defined
as the word itself plus a group of other semanti-
cally similar words. For computing these, we em-
ploy Lin?s corpus of pairwise distributional similari-
ties among words (nouns, verbs and adjectives) (Lin,
1998), filtered to include only those words whose
similarity value is above both an absolute (highest
n) and relative (to the highest similarity value in the
class) threshold. Even after filtering, Lin?s similari-
ties can be ?noisy?, since the corpus has been con-
structed relying on purely statistical means. There-
fore, we are employing in addition a set of manu-
ally defined semantic classes (hardcoded lists) sen-
sitive to our domain of temporal expressions, such
that these lists ?override? the Lin?s similarity cor-
pus whenever the semantic class of a word present
in them is involved. The manually defined semantic
classes include: the written form of cardinals; ordi-
nals; days of the week (plus today, tomorrow and
yesterday); months of the year; date trigger words
(e.g. day, week); time trigger words (e.g. hour, sec-
ond); frequency adverbs (e.g. hourly, monthly); date
adjectives (e.g. two- day, @@-week-long); and time
adjectives (e.g. three-hour, @@-minute-long).
We use a dynamic window for the amount of con-
text that is encoded into a pattern, that is, we gen-
erate all the possible patterns with the same infix,
and anything between 0 and the specified length of
the context window PEs in the prefix and the postfix,
and let the selection step decide which variations get
accepted into the next iteration.
The modifiers field in the pattern representa-
tion has been devised as an extension mecha-
nism. Currently the only implemented mod-
ifier is COMPLETE-PHRASE, which when at-
tached to a pattern, ?rounds? the instance (i.e.
candidate time expression) captured by its infix
to include the closest complete basic syntactic
chunk (e.g. ?LEMMA(end) LEMMA(of) SEM-
CLASS(January)? would match ?the end of De-
cember 2009? instead of only ?end of December?
against the text ?. . . By the end of December 2009,
. . . ?). This modifier was implemented in view of the
fact that most temporal expressions correspond with
whole noun phrases or adverbial phrases.
From the above types of PEs, we have built the
following types of patterns:
1. All-lemma patterns (including the prefix and
postfix).
2. All-semantic class patterns.
3. Combinations of token form with sem. class.
4. Combinations of lemma with sem. class.
5. All-POS tag patterns.
6. Combinations of token form with POS tag.
7. Combinations of lemma with POS tag.
8. All-syntactic chunk patterns.
9. All-generalized syntactic patterns.
4 Ranking and selection of patterns and
learning examples
4.1 Patterns
For the purposes of this section, let us define the
control set C as being formed by the seed examples
plus all the selected examples over the previous it-
erations (only the infix considered, not the context).
52
Note that, except for the seed examples, this is only
assumed correct, but cannot be guaranteed to be cor-
rect (unsupervised). In addition, let us define the in-
stance set Ip of a candidate pattern p as the set of
all the instances of the pattern found in a fraction of
the unlabelled corpus (only infix of the instance con-
sidered). Each candidate pattern pat is assigned two
partial scores:
1. A frequency-based score freq sc(p) that mea-
sures the coverage of the pattern in (a section
of) the unsupervised corpus:
freq sc(p) = Card(Ip ? C)
2. A precision score prec sc(p) that evaluates the
precision of the pattern in (a section of) the un-
supervised corpus, measured against the con-
trol set:
prec sc(p) = Card(Ip?C)Card(Ip)
These two scores are computed only against a
fraction of the unlabelled corpus for time effi-
ciency. There remains an issue with whether multi-
sets (counting each repeated instance several times)
or normal sets (counting them only once) should be
used for the instance sets Ip. Our experiments indi-
cate that the best results are obtained by employing
multisets for the frequency-based score and normal
sets for the precision score.
Given the two partial scores above, we have tried
three different strategies for combining them:
? Multiplicative combination: ?1 log(1 +
freq sc(p)) + ?2 log(2 + prec sc(p))
? The strategy suggested in (Collins and Singer,
1999): Patterns are first filtered by imposing
a threshold on their precision score. Only for
those patterns that pass this first filter, their final
score is considered to be their frequency-based
score.
? The strategy suggested in (Riloff, 1996):{ prec sc(p) ? log(freq sc(p)) if prec sc(p) ? thr
0 otherwise
4.1.1 Analysis of subsumptions
Intertwined with the selection step, an analysis of
subsumptions is performed among the selected pat-
terns, and the patterns found to be subsumed by oth-
ers in the set are discarded. This is repeated until ei-
ther a maximum ofm patterns with no subsumptions
among them are selected, or the list of candidate pat-
terns is exhausted, whichever happens first. The pur-
pose of this analysis of subsumptions is twofold: on
the one hand, it results in a cleaner output pattern
set by getting rid of redundant patterns; on the other
hand, it improves temporal efficiency by reducing
the number of patterns being handled in the last step
of the algorithm (i.e. searching for new candidate
examples).
In our scenario, a pattern p1 with instance set Ip1
is subsumed by a pattern p2 with instance set Ip2
if Ip1 ? Ip2 . We make a distinction among ?theo-
retical? and ?empirical? subsumptions. Theoretical
subsumptions are those that can be justified based on
theoretical grounds alone, from observing the form
of the patterns. Empirical subsumptions are those
cases where in fact one pattern subsumes another ac-
cording to the former definition, but this could only
be detected by having calculated their respective in-
stance sets a priori, which beats one of the purposes
of the analysis of subsumptions ?namely, tempo-
ral efficiency?. We are only dealing with theoreti-
cal subsumptions here. A pattern theoretically sub-
sumes another pattern when either of these condi-
tions occur:
? The first pattern is identical to the second, ex-
cept that the first has fewer contextual PEs in
the prefix and/or the postfix.
? Part or all of the PEs of the first pattern are
identical to the corresponding PEs in the sec-
ond pattern, except for the fact that they are
of a more general type (element-wise); the re-
maining PEs are identical. To this end, we have
defined a partial order of generality in the PE
types (see section 3), as follows:
FORM ? LEMMA ? SEMCLASS; FORM ? POS;
SYN ? SYN-SEMC
? Both the above conditions (fewer contextual
PEs and of a more general type) happen at the
same time.
4.2 Learning Examples
An example is composed of the tokens which have
been identified as a potential time expression (which
we shall call the infix) plus a certain amount of left
and right context (from now on, the context) en-
coded alongside the infix. For ranking and selecting
53
examples, we first assign a score and select a num-
ber n of distinct infixes and, in a second stage, we
assign a score to each context of appearance of an
infix and select (at most) m contexts per infix. Our
scoring system for the infixes is adapted from (Pas?ca
et al, 2006). Each distinct infix receives three par-
tial scores and the final score for the infix is a linear
combination of these, with the ?i being parameters:
?1sim sc(ex) + ?2pc sc(ex) + ?3ctxt sc(ex)
1. A similarity-based score (sim sc(ex)), which
measures the semantic similarity (as per the
Lin?s similarity corpus (Lin, 1998)) of the
infix with respect to set of ?accepted? output
examples from all previous iterations plus the
initial seeds. If w1, . . . , wn are the tokens in
the infix (excluding stopwords); ej,1, . . . , ej,mj
are the tokens in the j-th example of the set
E of seed plus output examples; and sv(x, y)
represents a similarity value, the similarity
Sim(wi) of the i-th word of the infix wrt
the seeds and output is given by Sim(wi) =?|E|
j=1 max(sv(wi, ej,1), . . . , sv(wi, ej,mj )),
and the similarity-based score of an in-
fix containing n words is given by?n
i=1 log(1+Sim(wi))
n .
2. A phrase-completeness score (pc sc(ex)),
which measures the likelihood that the infix
is a complete time expression and not merely
a part of one, over the entire set of candidate
example: count(INFIX)count(?INFIX?)
3. A context-based score (ctxt sc(ex)), intended
as a measure of the infix?s relevance. For each
context (up to a length) where this infix appears
in the corpus, the frequency of the word with
maximum relative frequency (over the words
in all the infix?s contexts) is taken. The sum
is then scaled by the relative frequency of this
particular infix.
Apart from the score associated with the infix,
each example (i.e. infix plus a context) receives
two additional frequency scores for the left and right
context part of the example respectively. Each of
these is given by the relative frequency of the token
with maximum frequency of that context, computed
over all the tokens that appear in all the contexts of
all the candidate examples. For each selected infix,
the m contexts with best score are selected.
5 Experiments
5.1 Experimental setup
As unsupervised data for our experiments, we use
the NW (newswire) category of LDC?s ACE 2005
Unsupervised Data Pool, containing 456 Mbytes of
data in 204K documents for a total of over 82 mil-
lion tokens. Simultaneously, we use a much smaller
labelled corpus (where the correct time expressions
are tagged) to measure the precision, recall and F1-measure of the pattern set learned by the bootstrap-
ping process. This is the ACE 2005 corpus, contain-
ing 550 documents with 257K tokens and approx.
4650 time expression mentions. The labelled corpus
is split in two halves: one half is used to obtain the
initial seed examples from among the time expres-
sions found therein; the other half is used for eval-
uation. We are requiring that a pattern captures the
target time expression mention exactly (no misalign-
ment allowed at the boundaries), in order to count it
as a precision or recall hit.
We will also be interested in measuring the gain
in recall, that is, the difference between the recall
in the best iteration and the initial recall given by
the seeds. Also important is the number of iter-
ations after which the bootstrapping process con-
verges. In the case where the same F1- measuremark is achieved in two experimental settings, ear-
lier convergence of the algorithm will be prefered.
Otherwise, better F1 and gain in recall are the pri-mary goals.
In order to start with a set of seeds with high pre-
cision, we select them automatically, imposing that
a seed time expression must have precision above a
certain value (understood as the percentage, of all
the appearances of the sequence of tokens in the su-
pervised corpus, those in which it is tagged as a cor-
rect time expression). In the experiments presented
below, this threshold for precision of the seeds is
90% ?in the half of the supervised corpus reserved
for extraction of seeds?. From those that pass this
filter, the ones that appear with greater frequency are
selected. For time expressions that have an identi-
cal digit pattern (e.g. two dates ?@@ December?
or two years ?@@@@?, where @ stands for any
digit), only one seed is taken. This approach sim-
ulates the human domain expert, which typically is
the first step in bootstrapping IE models
54
Unless specifically stated otherwise, all the exper-
iments presented below share the following default
settings:
? Only the first 2.36 Mbytes of the unsupervised
corpus are used (10 Mbytes after tokenization
and feature extraction), that is 0.5% of the
available data. This is to keep the execution
time of experiments low, where multiple exper-
iments need to be run to optimize a certain pa-
rameter.
? We use the Collins and Singer strategy (see
section 4.1) with a precision threshold of 0.50
for sub-score combination in pattern selection.
This strategy favours patterns with slightly
higher precision.
? The maximum length of prefix and postfix is 1
and 0 elements, respectively. This was deter-
mined experimentally.
? 100 seed examples are used (out of a maximum
of 605 available).
? In the ranking of examples, the ?i weights for
the three sub- scores for infixes are 0.5 for
the ?similarity-based score?, 0.25 for ?phrase-
completeness? and 0.25 for ?context-based
score?.
? In the selection of examples, the maximum
number of new infixes accepted per iteration is
200, with a maximum of 50 different contexts
per infix. In the selection of patterns, the max-
imum number of new accepted patterns per it-
eration is 5000 (although this number is never
reached due to the analysis of subsumptions).
? In the selection of patterns, multisets are used
for computing the instance set of a pattern
for the frequency-based score and normal sets
for the precision score (determined experimen-
tally).
? The POS tag type of generalization (pattern el-
ement) has been deactivated, that is, neither all-
POS patterns, nor patterns that are combina-
tions of POS PEs with another are generated.
After an analysis of errors, it was observed that
POS generalizations (because of the fact that
they are not lexicalized like, for instance, the
syntactic PEs with a given headword) give rise
to a considerable number of precision errors.
? All patterns are generated with COMPLETE-
PHRASE modifier automatically attached. It
was determined experimentally that it was best
to use this heuristic in all cases (see section 3).
5.2 Variation of the number of seeds
We have performed experiments using 1, 5, 10, 20,
50, 100, 200 and 500 seeds. The general trends ob-
served were as follows. The final precision (when
the bootstrapping converges) decreases more or less
monotonically as the number of seeds increases, al-
though there are slight fluctuations; besides, the dif-
ference in this respect between using few seeds (20
to 50) or more (100 to 200) is of only around 3%.
However, a big leap can be observed in moving from
200 to 500 seeds, where both the initial precision
(of the seeds) and final precision (at point of con-
vergence) drop by 10% wrt to using 200 seeds. The
final recall increases monotonically as the number
of seeds increases?since more supervised informa-
tion is provided?. The final F1-measure first in-creases and then decreases with an increasing num-
ber of seeds, with an optimum value being reached
somewhere between the 50 and 100 seeds.
The largest gain in recall (difference between re-
call of the seeds and recall at the point of con-
vergence) is achieved with 20 seeds, for a gain
of 16.38% (initial recall is 20.08% and final is
36.46%). The best mark in F1-measure is achievedwith 100 seeds, after 6 iterations: 60.43% (the final
precision is 69.29% and the final recall is 53.58%;
the drop in precision is 6.5% and the gain in recall is
14.28%). Figure 3 shows a line plot of precision vs
recall for these experiments. This experiment sug-
gests that the problem of temporal expression recog-
nition can be captured with minimal supervised in-
formation (100 seeds) and larger amounts of unsu-
pervised information.
Figure 3: Effect of varying the number of seeds
55
5.3 Variation of the type of generalizations
used in patterns
In these experiments, we have defined four differ-
ents sets of generalizations (i.e. types of pattern ele-
ments among those specified in section 3) to evalu-
ate how semantic and syntactic generalizations con-
tribute to performance of the algorithm. These four
experiments are labelled as follows: NONE includes
only PEs of the LEMMA type; SYN includes PEs
of the lemma type and of the not-generalized syn-
tactic chunk (SYN) type; SEM includes PEs of the
lemma type and of the semantic class (SEMCLASS)
type, as well as combinations of lemma with SEM-
CLASS PEs; and lastly, SYN+SEM includes every-
thing that both SYN and SEM experiments include,
plus PEs of the generalized syntactic chunk (SYN-
SEMC) type.
One can observe than neither type of generaliza-
tion, syntactic or semantic, is specially ?effective?
when used in isolation (only a 3.5% gain in recall in
both cases). It is only the combination of both types
that gives a good gain in recall (14.28% in the case
of this experiment). Figure 4 shows a line plot of this
experiment. The figure indicates that the problem of
temporal expression recognition, even though appar-
ently simple, requires both syntactic and semantic
information for efficient modeling.
Figure 4: Effect of using syntactic and/or semantic gen-
eralizations
5.4 Variation of the size of unsupervised data
used
We performed experiments using increasing
amounts of unsupervised data for training in the
bootstrapping: 1, 5, 10, 50 and 100 Mbytes of
preprocessed corpus (tokenized and with feature
extraction). The amounts of plain text data are
roughly a fifth part, respectively. The objective
of these experiments is to determine whether
performance improves as the amount of training
data is increased. The number of seeds passed to
the bootstrapping is 68. The maximum number of
new infixes (the part of an example that contains a
candidate time expression) accepted per iteration
has been increased from 200 to 1000, because it
was observed that larger amounts of unsupervised
training data need a greater number of selection
?slots? in order to render an improvement (that is, a
more ?reckless? bootstrapping), otherwise they will
fill up all the allowed selection slots.
The observed effect is that both the drop in preci-
sion (from the initial iteration to the point of conver-
gence) and the gain in recall improve more or less
consistently as a larger amount of training data is
taken, or otherwise the same recall point is achieved
in an earlier iteration. These improvements are nev-
ertheless slight, in the order of between 0.5% and
2%. The biggest improvement is observed in the 100
Mbytes experiment, where recall after 5 iterations is
6% better than in the 50 Mbytes experiment after 7
iterations. The drop in precision in the 100 Mbytes
experiment is 13.05%, for a gain in recall of 21.36%
(final precision is 71.02%, final recall 52.84% and
final F1 60.59%). Figure 5 shows a line plot of thisexperiment. This experiment indicates that increas-
ing amounts of unsupervised data can be used to im-
prove the performance of our model, but the task is
not trivial.
Figure 5: Effect of varying the amount of unsupervised
training data
6 Conclusions and future research
We have presented a slightly supervised algorithm
for the extraction of IE patterns for the recognition
56
of time expressions, based on bootstrapping, which
introduces a novel representation of patterns suited
to this task. Our experiments show that with a rel-
atively small amount of supervision (50 to 100 ini-
tial correct examples or seeds) and using a combina-
tion of syntactic and semantic generalizations, it is
possible to obtain an improvement of around 15%-
20% in recall (with regard to the seeds) and F1-measure over 60% learning exclusively from unla-
belled data. Furthermore, using increasing amounts
of unlabelled training data (of which there is plenty
available) is a workable way to obtain small im-
provements in performance, at the expense of train-
ing time. Our current focus is on addressing specific
problems that appear on inspection of the precision
errors in test, which can improve both precision and
recall to a degree. Future planned lines of research
include using WordNet for improving the semantic
aspects of the algorithm (semantic classes and simi-
larity), and studying forms of combining the patterns
obtained in this semi-supervised approach with su-
pervised learning.
References
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings of
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 100?110, College Park, MD. ACL.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. Tides 2005 standard for the annotation of
temporal expressions. Technical report, MITRE Cor-
poration.
E. Filatova and E. Hovy. 2001. Assigning time-stamps to
event-clauses. In Proceedings of the 2001 ACL Work-
shop on Temporal and Spatial Information Processing,
pages 88?95.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Automatic
time expression labelling for english and chinese text.
In Proc. of the 6th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing), pages 548?559. Springer.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pattern
linkage graphs. In Proc. of the Association for Com-
putational Linguistics 2008 (ACL-2008:HLT), pages
1048?1056.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th International
Conference on Computational Linguistics and the 36th
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL-98), pages 768?774, Mon-
treal, Quebec. ACL.
I. Mani and G. Wilson. 2000. Robust temporal process-
ing of news. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
pages 69?76, Morristown, NJ, USA. ACL.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Proceedings of the 21th In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 809?816.
ACL.
E. Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence
(AAAI-96), pages 1044?1049. AAAI/MIT Press.
E. Saquete, R. Mun?oz, and P. Mart??nez-Barco. 2004.
Event ordering using terseo system. In Proc. of the
9th International Conference on Application of Natu-
ral Language to Information Systems (NLDB), pages
39?50. Springer.
M. Stevenson and M. Greenwood. 2005. A semantic
approach to IE pattern induction. In Proceedings of
the 43rd Meeting of the Association for Computational
Linguistics, pages 379?386. ACL.
M. Surdeanu, J. Turmo, and A. Ageno. 2006. A hybrid
approach for the acquisition of information extraction
patterns. In Proceedings of the EACL 2006 Workshop
on Adaptive Text Extraction and Mining (ATEM 2006).
ACL.
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren, and
K. J. McKeever. 1998. An empirical approach to tem-
poral reference resolution. Journal of Artificial Intelli-
gence Research, 9:247?293.
R. Yangarber, R. Grishman, P. Tapanainen, and
S. Hutunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In Proceedings
of the 18th International Conference of Computational
Linguistics, pages 940?946.
R. Yangarber. 2003. Counter-training in discovery of
semantic patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics. ACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA. ACL.
57
Coling 2010: Poster Volume, pages 1086?1094,
Beijing, August 2010
A Global Relaxation Labeling Approach to Coreference Resolution
Emili Sapena, Llu??s Padro? and Jordi Turmo?
TALP Research Center
Universitat Polite`cnica de Catalunya
{esapena, padro, turmo}@lsi.upc.edu
Abstract
This paper presents a constraint-based
graph partitioning approach to corefer-
ence resolution solved by relaxation label-
ing. The approach combines the strengths
of groupwise classifiers and chain forma-
tion methods in one global method. Ex-
periments show that our approach signifi-
cantly outperforms systems based on sep-
arate classification and chain formation
steps, and that it achieves the best results
in the state of the art for the same dataset
and metrics.
1 Introduction
Coreference resolution is a natural language pro-
cessing task which consists of determining the
mentions that refer to the same entity in a text
or discourse. A mention is a noun phrase refer-
ring to an entity and includes named entities, def-
inite noun phrases, and pronouns. For instance,
?Michael Jackson? and ?the youngest of Jackson
5? are two mentions referring to the same entity.
A typical machine learning-based coreference
resolution system usually consists of two steps:
(i) classification, where the system evaluates the
coreferentiality of each pair or group of mentions,
and (ii) formation of chains, where given the con-
fidence values of the previous classifications the
system forms the coreference chains.
?Research supported by the Spanish Science and In-
novation Ministry, via the KNOW2 project (TIN2009-
14715-C04-04) and from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) under Grant
Agreement number 247762 (FAUST)
Regarding the classification step, pioneer sys-
tems developed were based on pairwise classi-
fiers. Given a pair of mentions, the process gen-
erates a feature vector and feeds it to a classi-
fier. The resolution is done by considering each
mention of the document as anaphor1 and look-
ing backward until the antecedent is found or
the beginning of the document is reached (Aone
and Bennett, 1995; McCarthy and Lehnert, 1995;
Soon et al, 2001).
A first approach towards groupwise classifiers
is the twin-candidate model (Yang et al, 2003).
The model faces the problem as a competition be-
tween two candidates to be the antecedent of the
anaphor into account. Each candidate mention is
compared with all the others in a round robin con-
test. Following the groupwise approach, rankers
consider all the possible antecedent mentions at
once (Denis and Baldridge, 2008). Rankers can
obtain more accurate results due to a more in-
formed context where all candidate mentions are
considered at the same time.
Coreference chains are formed after classifi-
cation. Many systems form the chains by join-
ing each positively-classified pair (i.e. single-
link) or with simple improvements such as linking
an anaphor only to its antecedent with maximum
confidence value (Ng and Cardie, 2002).
Some works propose more elaborated methods
than single-link for chain formation. The ap-
proaches used are Integer Linear Programming
1Typically a pair of coreferential mentions mi and mj
(i < j) are called antecedent and anaphor respectively,
though mj may not be anaphoric.
1086
(ILP) (Denis and Baldridge, 2007; Klenner and
Ailloud, 2009; Finkel and Manning, 2008), graph
partitioning (Nicolae and Nicolae, 2006), and
clustering (Klenner and Ailloud, 2008). The main
advantage of these types of post-processes is the
enforcement of transitivity sorting out the con-
tradictions that the previous classification process
may introduce.
Although chain formation processes search for
global consistency, the lack of contextual infor-
mation in the classification step is propagated for-
ward. Few works try to overcome the limita-
tions of keeping classification and chain formation
apart. Luo et al (2004) search the most proba-
ble path comparing each mention with the partial-
entities formed so far using a Bell tree struc-
ture. McCallum and Wellner (2005) propose a
graph partitioning cutting by distances, with the
peculiarity that distances are learned considering
coreferential chains of the labeled data instead of
pairs. Culotta et al (2007) combine a groupwise
classifier with a clustering process in a First-Order
probabilistic model.
The approach presented in this paper follows
the same research line of joining group classifi-
cation and chain formation in the same step. Con-
cretely, we propose a graph representation of the
problem solved by a relaxation labeling process,
reducing coreference resolution to a graph par-
titioning problem given a set of constraints. In
this manner, decisions are taken considering the
whole set of mentions, ensuring consistency and
avoiding that classification decisions are indepen-
dently taken. Our experimental results on the
ACE dataset show that our approach outperforms
systems based on separate classification and chain
formation steps, and that it achieves the best re-
sults in the state of the art for the same dataset and
metrics.
The paper is organized as follows. Section 2 de-
scribes the graph representation of the task. Sec-
tion 3 explains the use of relaxation labeling algo-
rithm and the machine learning process. Finally,
experiments and results are explained in Section 4
before paper is concluded.
2 Graph Representation
Let G = G(V,E) be an undirected graph where
V is a set of vertices and E a set of edges. Let
m = (m1, ...,mn) be the set of mentions of a
document with n mentions to resolve. Each men-
tion mi in the document is represented as a vertex
vi ? V . An edge eij ? E is added to the graph for
pairs of vertices (vi, vj) representing the possibil-
ity that both mentions corefer. The list of adjacent
vertices of a vertex vi is A(vi).
Let C be our set of constraints. Given a pair of
mentions (mi, mj), a subset of constraints Cij ?
C restrict the compatibility of both mentions. Cij
is used to compute the weight value of the edge
connecting vi and vj . Let wij ? W be the weight
of the edge eij :
wij =
?
k?Cij
?kfk(mi,mj) (1)
where fk(?) is a function that evaluates the con-
straint k. And ?k is the weight associated to the
constraint k (?k and wij can be negative).
In our approach, each vertex (vi) in the graph
is a variable (vi) for the algorithm. Let Li be the
number of different values (labels) that are pos-
sible for vi. The possible labels of each variable
are the partitions that the vertex can be assigned.
Note that the number of partitions (entities) in a
document is unknown, but it is at most the num-
ber of vertices (mentions), because in a extreme
case, each mention in a document could be refer-
ring to a different entity. A vertex with index i can
be in the first i partitions (i.e. Li = i).
Each combination of labelings for the graph
vertices is a partitioning (?). The resolution pro-
cess searches the partitioning ?? which optimizes
the goodness function F (?,W ), which depends
on the edge weights W. In this manner, ?? is opti-
mal if:
F (??,W ) ? F (?,W ),?? (2)
The next section describes the algorithm used
in the resolution process.
3 Relaxation Labeling
Relaxation labeling (Relax) is a generic name for
a family of iterative algorithms which perform
1087
function optimization, based on local information.
The algorithm has been widely used to solve NLP
problems such as PoS-tagging (Ma`rquez et al,
2000), chunking, knowledge integration, and Se-
mantic Parsing (Atserias, 2006).
Relaxation labeling solves our weighted con-
straint satisfaction problem dealing with the edge
weights. In this manner, each vertex is assigned to
a partition satisfying as many constraints as pos-
sible. To do that, the algorithm assigns a pro-
bability for each possible label of each variable.
Let H = (h1,h2, . . . ,hn) be the weighted label-
ing to optimize, where each hi is a vector con-
taining the probability distribution of vi, that is:
hi = (hi1, hi2, . . . , hiLi). Given that the resolutionprocess is iterative, the probability for label l of
variable vi at time step t is hil(t), or simply hil
when the time step is not relevant.
The support for a pair variable-label (Sil) ex-
presses how compatible is the assignment of label
l to variable vi considering the labels of adjacent
variables and the edge weights. Although several
support functions may be used (Torras, 1989), we
chose the following one, which defines the sup-
port as the sum of the edge weights that relate
variable vi with each adjacent variable vj multi-
plied by the weight for the same label l of vj :
Sil =
?
j?A(vi)
wij ? hjl (3)
where wij is the edge weight obtained in Equa-
tion 1. In our version of the algorithm, A(vi) is
the list of adjacent vertices of vi but only includ-
ing the ones with an index k < i. Consequently,
the weights only have influence in one direction
which is equivalent to using a directed graph. Al-
though the proposed representation is based on
a general undirected graph, preliminary experi-
ments showed that using directed edges yields
higher perfomance in this particular problem.
The aim of the algorithm is to find a weighted
labeling such that global consistency is maxi-
mized. Maximizing global consistency is defined
as maximizing the average support for each vari-
able. Formally, H? is a consistent labeling if:
Initialize:
H := H0,
Main loop:
repeat
For each variable vi
For each possible label l for vi
Sil =
?
j?A(vi) wij ? h
j
l
End for
For each possible label l for vi
hil(t + 1) =
hil(t)?(1+Sil)?Li
k=1
hik(t)?(1+Sik)End for
End for
Until no more significant changes
Figure 1: Relaxation labeling algorithm
Li?
l=1
h?il ? Sil ?
Li?
l=1
hil ? Sil ?h,?i (4)
A partitioning ? is directly obtained from the
weighted labeling H assigning to each variable
the label with maximum probability. The sup-
ports and the weighted labeling depend on the
edge weights (Equation 3). To satisfy Equation
4 is equivalent to satisfy Equation 2. Many stud-
ies have been done towards the demonstration of
the consistency, convergence and cost reduction
advantages of the relaxation algorithm (Rosenfeld
et al, 1976; Hummel and Zucker, 1987; Pelillo,
1997). Although some of the conditions required
by the formal demonstrations are not fulfilled in
our case, the presented algorithm ?that forces a
stop after a number of iterations? has proven use-
ful for practical purposes.
Figure 1 shows the pseudo-code of the relax-
ation algorithm. The process updates the weights
of the labels in each step until convergence. The
convergence is met when no more significant
changes are done in an iteration. Specifically,
when the maximum change in an update step
(maxi,l(|hil(t+1)?hil(t)|)) is lower than a param-
eter , a small value (0.001 in our experiments),
or a fixed number of iterations is reached (2000 in
our experiments). Finally, the assigned label for a
variable is the one with the highest weight. Figure
2 shows a representation.
1088
Figure 2: Representation of Relax. The vertices represent-
ing mentions are connected by weighted edges eij . Each ver-
tex has a vector hi of probabilities to belong to different par-
titions. The figure shows h2, h3 and h4.
3.1 Constraints
The performance of the resolution process de-
pends on the edge weights obtained by a set of
weighted constraints (Equation 1). Any method
or combination of methods to generate constraints
can be used. For example, a set of constraints
handwritten by linguist experts can be added to
another automatically obtained set.
This section explains the automatic constraint
generation process carried out in this work, using
a set of feature functions and a training corpus.
Ma`rquez et al (2000) have successfully used sim-
ilar processes to acquire constraints for constraint
satisfaction algorithms.
Each pair of mentions (mi, mj) in a training
document is evaluated by a set of feature functions
(Figure 3). The values returned by these functions
form a positive example when the pair of men-
tions corefer, and a negative one otherwise. Three
specialized models are constructed depending on
the type of anaphor mention (mj) of the pair: pro-
noun, named entity or nominal.
For each specialized model, a decision tree
(DT) is generated and a set of rules is ex-
tracted with C4.5 rule-learning algorithm (Quin-
lan, 1993). These rules are our set of constraints.
The C4.5rules algorithm generates a set of rules
for each path from the learnt tree. It then general-
izes the rules by dropping conditions.
The weight assigned to a constraint (?k) is its
DIST: Distance betweenmi andmj in sentences: number
DIST MEN: Distance betweenmi andmj in mentions: number
APPOSITIVE: One mention is in apposition with the other: y,n
I/J IN QUOTES:mi/j is in quotes or inside a NP or a sentence
in quotes: y,n
I/J FIRST:mi/j is the first mention in the sentence: y,n
I/J DEF NP:mi/j is a definitive NP: y,n
I/J DEM NP:mi/j is a demonstrative NP: y,n
I/J INDEF NP:mi/j is an indefinite NP: y,n
STR MATCH: String matching ofmi andmj : y,n
PRO STR: Both are pronouns and their strings match: y,n
PN STR: Both are proper names and their strings match: y,n
NONPRO STR: String matching like in Soon et al (2001)
and mentions are not pronouns: y,n
HEAD MATCH: String matching of NP heads: y,n
NUMBER: The number of both mentions match: y,n,u
GENDER: The gender of both mentions match: y,n,u
AGREEMENT: Gender and number of both
mentions match: y,n,u
I/J THIRD PERSON:mi/j is 3rd person: y,n
PROPER NAME: Both mentions are proper names: y,n,u
I/J PERSON:mi/j is a person (pronoun or
proper name in a list): y,n
ANIMACY: Animacy of both mentions match
(persons, objects): y,n
I/J REFLEXIVE:mi/j is a reflexive pronoun: y,n
I/J TYPE:mi/j is a pronoun (p), entity (e) or nominal (n)
NESTED: One mention is included in the other: y,n
MAXIMALNP: Both mentions have the same NP parent
or they are nested: y,n
I/J MAXIMALNP:mi/j is not included in any
other mention: y,n
I/J EMBEDDED:mi/j is a noun and is not a maximal NP: y,n
BINDING: Conditions B and C of binding theory: y,n
SEMCLASS: Semantic class of both mentions match: y,n,u
(the same as Soon et al (2001))
ALIAS: One mention is an alias of the other: y,n,u
(only entities, else unknown)
Figure 3: Feature functions used
precision over the training data (Pk), but shifted
to be zero-centered: ?k = Pk ? 0.5.
3.2 Pruning
Analyzing the errors of development experiments,
we have found two main error patterns that can be
solved by a pruning process. First, the contribu-
tion of the edge weights for the resolution depends
on the size of the document. And second, many
weak edge weights may sum up to produce a bias
in the wrong direction.
The weight of an edge depends on the weights
assigned for the constraints which apply to a pair
of mentions according to Equation 1. Each ver-
tex is adjacent to all the other vertices. This pro-
duces that the larger the number of adjacencies,
the smaller the influence of a constraint is. A con-
sequence is that resolution for large and short do-
cuments has different results.
Many works have to deal with similar prob-
lems, specially the ones looking backward for an-
tecedents. The larger the document, the more pos-
1089
sible antecedents the system has to classify. This
problem is usually solved looking for antecedents
in a window of few sentences, which entails an
evident limitation of recall.
Regarding the weak edge weights, it is notable
that some kind of mention pairs are very weakly
informative. For example, the pairs (pronoun,
pronoun). Many stories have a few main charac-
ters which monopolize the pronouns of the doc-
ument. This produces many positive training ex-
amples for pairs of pronouns matching in gender
and person, which may lead the algorithm to pro-
duce large coreferential chains joining all these
mentions even for stories where there are many
different characters. For example, we have found
in the results of some documents a huge corefer-
ence chain including every pronoun ?he?. This
is because a pair of mentions (?he?, ?he?) is usu-
ally linked with a small positive weight. Although
the highest adjacent edge weight of a ?he? men-
tion may link with the correct antecedent, the sum
of several edge weights linking the mention with
other ?he? causes the problem.
A pruning process is perfomed solving both
problems and reducing computational costs from
O(n3) to O(n2). For each vertex?s adjacency list
A(vi), only a maximum of N edges remain and the
others are pruned. Concretely, the N/2 edges with
largest positive weight and the N/2 with largest
negative weight. The value of N is empirically
chosen by maximizing performances over training
data. On the one hand, the pruning forces the max-
imum adjacency to be constant and the contribu-
tion of the edge weights does not depend on the
size of the document. On the other hand, most
edges of the less informative pairs are discarded
avoiding further confusion. There are no limita-
tions in distance or other restrictions which may
cause a loss of recall.
3.3 Initial State
The initial state of the vertices define the a pri-
ori probabilities for each vertex to be in each par-
tition. There are several possible initial states.
In the case where no prior information is avail-
able, a random or uniformly distributed state is
commonly used. However, a well-informed initial
state should drive faster the relaxation process to
a better solution. This section describes the well-
informed initial state chosen in our approach and
the random one. Both are compared in the exper-
iments (Section 4.2).
The well-informed initial state favors the cre-
ation of new chains. Variable vi has Li = i pos-
sible values while variable vi+1 has Li + 1. The
probability distribution of vi+1 is equiprobable for
values from 1 to Li but it is the double for the pro-
bability to start a new chain Li + 1.
hil = 1Li+1 , ?l = 1..Li ? 1
hiLi =
2
Li+1
Pronouns do not follow this distribution but a
totally equiprobable one, given that they are usu-
ally anaphoric.
hil = 1Li , ?l = 1..Li
This configuration enables the resolution pro-
cess to determine as singletons the mentions for
which little evidence is available. This small dif-
ference between initial probability weights is also
introduced in order to avoid exceptional cases
where all support values contribute with the same
value.
The random initial state is also used in our
experiments to test that our proposed configura-
tion is better-informed than random. Given the
equiprobability state, we add a random value to
each probability to be in a partition:
hil = 1Li + il, ?l = 0..Li
where il is a random value ?12Li ? il ? 12Li .These little random differences may help the algo-
rithm to avoid local minima.
3.4 Reordering
The vertices of the graph would usually be placed
in the same order as the mentions are found in the
document (chronological). In this manner, vi cor-
responds to mi. However, as suggested by Luo
(2007), there is no need to generate the model
following that order. In our approach, the first
variables have a lower number of possible labels.
Moreover, an error in the first variables has more
influence on the performance than an error in the
later ones. Placing named entities at the beginning
is reasonably to expect that is helpful for the al-
gorithm, given that named entities are usually the
most informative mentions.
1090
Tokens Mentions Entities
bnews train 66627 9937 4408
bnews test 17463 2579 1040
npaper train 68970 11283 4163
npaper test 17404 2483 942
nwire train 70832 10693 4297
nwire test 16772 2608 1137
Figure 4: Statistics about ACE-phase02
Suppose we have three mentions appearing in
this order somewhere in a document: ?A. Smith?,
?he?, ?Alice Smith?. For proximity, mention ?he?
may tend to link with ?A. Smith?. Then, the third
mention ?Alice Smith? clearly is the whole name
of ?A. Smith? but the gender with ?he? does not
agree. Given that our implementation acts like a
directed graph only looking backward (see Sec-
tion 3), mention ?he? won?t change its tendency
and it may cause a split in the ?Alice Smith? coref-
erence chain. However, having named entities in
first place and pronouns at the end, enables the
mention ?he? to determine that ?A. Smith? and
?Alice Smith? having the same label are not good
antecedents.
Reordering only affects on the number of pos-
sible labels of the variables and the list of adjacen-
cies A(vi). The chronological order of the docu-
ment is taken into account by the constraints re-
gardless of the graph representation. Our experi-
ments confirm (Section 4) that placing first named
entity mentions, then nominal mentions and fi-
nally the pronouns, the precision increases consid-
erably. Inside of each of these groups, the order is
the same order of the document.
4 Experiments and Results
We evaluate our approach to coreference res-
olution using ACE-phase02 corpus, which is
composed of three sections: Broadcast News
(BNEWS), Newswire (NWIRE) and Newspaper
(NPAPER). Each section is in turn composed of a
training set and a test set. Figure 4 shows some
statistics about this corpus.
In our experiments, we consider the true men-
tions of ACE. This is because our focus is on
evaluating pairwise approach versus the graph
partitioning approach and also comparing them
to some state-of-the-art approaches which also
use true mentions. Moreover, details on men-
tion identifier systems and their performances are
rarely published by the systems based on auto-
matic identification of mentions and it difficults
the comparison.
To evaluate our system we use CEAF (Luo,
2005) and B3 (Bagga and Baldwin, 1998). CEAF
is computed based on the best one-to-one map be-
tween key coreference chains and response ones.
We use the mention-based similarity metric which
counts the number of common mentions shared
by key coreference chains and response ones. As
we are using true mentions for the experiments,
precision, recall and F1 are the same value and
only F1 is shown. B3 scorer is used for com-
parison reasons. B3 algorithm looks at the pres-
ence/absence of mentions for each entity in the
system output. Precision and recall numbers are
computed for each mention, and the average gives
the final precision and recall numbers.
MUC scorer (Vilain et al, 1995) is not used
in our experiments. Although it has been widely
used in the state of the art, we consider the newer
metrics have overcome some MUC limitations
(Bagga and Baldwin, 1998; Luo, 2005; Klenner
and Ailloud, 2008; Denis and Baldridge, 2008).
Our preprocessing pipeline consists of
FreeLing (Atserias et al, 2006) for sentence
splitting and tokenization, SVMTool (Gimenez
and Marquez, 2004) for part of speech tagging
and BIO (Surdeanu et al, 2005) for named entity
recognition and classification. No lemmatization
neither syntactic analysis are used.
4.1 Baselines
4.1.1 DT with automatic feature selection
The baseline developed in our work is based on
Soon et al (2001) with the improvements of Ng
and Cardie (2002), which uses a Decision Tree
(DT). Many research works use the same refe-
rences in order to evaluate possible improvements
done by their new models or by the incorporation
of new features.
The features used in the baseline are the same
than those used in our proposed system (Figure
3). However, some features are noisy and many
others have redundancy which causes low perfor-
mances using DTs. In order to select the best set
1091
bnews npaper nwire Global
Metric: CEAF CEAF B3
Model F1 F1 F1 F1 P R F1
DT 60.6 57.8 60.5 59.7 61.0 74.1 66.9
DT Hill 67.8 61.6 65.0 64.8 74.7 69.8 72.2
Table 1: Results ACE-phase02. Comparing baselines based on Decision Trees.
bnews npaper nwire Global
Metric: CEAF CEAF B3
Model F1 F1 F1 F1 P R F1
DT 60.6 59.5 64.7 61.7 63.3 74.7 68.5
DT + ILP 62.8 60.3 63.7 62.5 72.4 69.2 70.7
DT Hill 67.8 63.2 67.2 66.5 76.8 71.0 73.8
DT Hill + ILP 67.6 63.5 66.7 66.3 80.0 68.3 73.7
Relax 69.5 68.3 73.0 70.4 86.5 67.9 76.1
Table 2: Results on documents shorter than 200 mentions of ACE-phase02
of features a Hill Climbing process has been per-
formed doing a five-fold cross-validation over the
training corpus. A similar feature selection pro-
cess has been done by Hoste (2005).
The Hill Climbing process starts using the
whole set of features. A cross-validation is done
(un)masking each feature. The (un)masked fea-
ture with more improvement is (added to) re-
moved from the set. The process is repeated until
an iteration without improvements is reached.
Note that this optimization process is biased by
the metric used to evaluate each feature combi-
nation. We use CEAF in our experiments, which
encourages precision and consistency.
4.1.2 Integer Linear Programming
The second baseline developed forms the coref-
erence chains given the output of the pair classi-
fication of the first baseline. A set of binary vari-
ables (xij) symbolize whether pairs of mentions
(mi,mj) corefer (xij = 1) or not (xij = 0). An
objective function is defined as follows:
min
?
i<j ?log(Pcij)xij ? log(1? Pcij)(1? xij)
where Pcij is the confidence value of mentions
mi and mj to corefer obtained by the pair clas-
sifier. The minimization of the objective func-
tion is done by Integer Linear Programming (ILP)
in a similar way to (Klenner, 2007; Denis and
Baldridge, 2007; Finkel and Manning, 2008). In
order to keep consistency in the results, which is
the goal of this post-process, a set of triangular
constraints is required. For each three mentions
with indexes i < j < k the corresponding vari-
ables have to satisfy three constraints:
? xik ? xij + xjk ? 1
? xij ? xik + xjk ? 1
? xjk ? xij + xik ? 1
This implies that this model needs, for a doc-
ument with n mentions, 12n(n ? 1) variables and1
2n(n ? 1)(n ? 2) constraints to assure consis-tency2. This is an important limitation with a view
to scalability. In our experiments only documents
shorter than 200 mentions can be solved by this
baseline due to its computational cost.
4.2 Experiments
Four experiments have been done in order to eval-
uate our proposed approach. This section de-
scribes and analyzes the results of each experi-
ment. Finally, our performances are compared
with the state of the art.
The first experiment compares the perfor-
mances of our baselines (Table 1). ?DT? is the
system based on Decision Tree using all the fea-
tures of Figure 3 and ?DT+Hill? is a DT using
the features selected by the Hill Climbing process
(Section 4.1.1). There is a significant improve-
ment in the performances (5.1 points with CEAF,
5.3 with B3) after the automatic feature selection
process is done.
2 1
6n(n ? 1)(n ? 2) for each one of the three triangularconstraints
1092
bnews npaper nwire Global
Metric: CEAF CEAF B3
Model F1 F1 F1 F1 P R F1
Relax 67.3 64.4 69.5 67.2 88.4 62.7 73.3
Relax pruning 68.6 65.2 70.1 68.0 82.3 66.9 73.8
Relax pruning & reorder 69.5 67.3 72.1 69.7 85.3 66.8 74.9
Relax random IS 68.2 66.1 71.0 68.5 83.5 66.7 74.2
MaxEnt+ILP (Denis, 2007) - - - 66.2 81.4 65.6 72.7
Rankers (Denis, 2007) 65.7 65.3 68.1 67.0 79.8 66.8 72.7
Table 3: Results ACE-phase02.
In the second experiment the ILP chain forma-
tion process is applied using the output of both
DTs. Results are shown in Table 2. Note that ILP
only applies to documents shorter than 200 men-
tions due to its excessive computational cost (Sec-
tion 4.1.2). Results for Relax applied to the same
documents are also included for comparison. ILP
forces consistency of the results producing an in-
crease in precision score with B3 metric in both
cases. However, ?DT+Hill? has been optimized
for CEAF metric which encourages precision and
consistency. For this, a post-process forcing con-
sistency seems unnecessary for a classifier already
optimized. Relax significantly outperforms all the
baselines.
The third experiment shows the improvements
achieved by the use of pruning and reordering
techniques (Sections 3.2 and 3.4). Table 3 shows
the results. Pruning improves performances with
both metrics. B3 precision is decreased but the
global F1 is increased due to a considerably im-
provement of recall. Reordering recovers the pre-
cision lost by the pruning without loosing recall,
which achieves the best performances of 69.7 with
CEAF and 74.9 with B3.
The fourth experiment evaluates the influence
of the initial state. A comparison is done with
the proposed initial state (Section 3.3) and the
random one. The results shown in Table 3
for random initial state are the average of 3
executions. The system called ?Relax random
IS? is using the same values for pruning and
reordering techniques than the best result of
previous experiment: ?Relax pruning & reorder?.
As expected, results with a well-informed initial
state outperform the random ones.
Finally, Relax performances are compared with
the best scores we have found using the same cor-
pora and metrics. We compare our approach with
specialized Rankers ?groupwise classifier?, and
a system using ILP not only forcing consistency
but also using information about anaphoricity and
named entities. Relax outperforms both systems
with both metrics (Table 3).
5 Conclusion
The approach for coreference resolution presented
in this paper is a constraint-based graph partition-
ing solved by relaxation labeling.
The decision to join or not a set of mentions
in the same entity is taken considering always the
whole set of previous mentions like in groupwise
classifiers. Contrarily to the approaches where
variables are the linkage of each pair of mentions,
in this model consistency is implicitly forced.
Moreover, the influence of the partial results of
the other mentions at the same time avoids that
decisions are independently taken.
The capacity to easily incorporate constraints
from different sources and using different know-
ledge is also remarkable. This flexibility gives
a great potencial to the approach. Anaphoricity
filtering is not needed given that the necessary
knowledge can be also introduced by constraints.
In addition, three tecniques to improve results
have been presented: reordering, pruning and fea-
ture selection by Hill Climbing. The experiments
confirm their utility.
The experimental results clearly outperform the
baselines with separate classification and chain
formaiton. The approach also outperforms oth-
ers in the state of the art using same corpora and
metrics.
1093
References
Aone, C. and S.W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proceedings of the 33rd annual meeting on ACL, pages
122?129.
Atserias, J., B. Casas, E. Comelles, M. Gonza?lez, L. Padro?,
and M. Padro?. 2006. Freeling 1.3: Syntactic and semantic
services in an open-source nlp library. In Proceedings of
the fifth international conference on Language Resources
and Evaluation (LREC 2006), ELRA. Genoa, Italy.
Atserias, J. 2006. Towards Robustness in Natural Lan-
guage Understanding. Ph.D. Thesis, Dept. Lenguajes
y Sistemas Informa?ticos. Euskal Herriko Unibertsitatea.
Donosti. Spain.
Bagga, A. and B. Baldwin. 1998. Algorithms for scoring
coreference chains. Proceedings of the Linguistic Coref-
erence Workshop at LREC, pages 563?566.
Culotta, A., M. Wick, and A. McCallum. 2007. First-Order
Probabilistic Models for Coreference Resolution. Pro-
ceedings of NAACL HLT, pages 81?88.
Denis, P. and J. Baldridge. 2007. Joint Determination of
Anaphoricity and Coreference Resolution using Integer
Programming. Proceedings of NAACL HLT, pages 236?
243.
Denis, P. and J. Baldridge. 2008. Specialized models and
ranking for coreference resolution. Proceedings of the
EMNLP, Hawaii, USA.
Denis, P. 2007. New Learning Models for Robust Refer-
ence Resolution. Ph.D. dissertation, University of Texas
at Austin.
Finkel, J.R. and C.D. Manning. 2008. Enforcing transitivity
in coreference resolution. In Proceedings of the 46th An-
nual Meeting of the ACL HLT: Short Papers, pages 45?48.
Association for Computational Linguistics.
Gimenez, J. and L. Marquez. 2004. Svmtool: A general pos
tagger generator based on support vector machines. In
Proceedings of the 4th International Conference on Lan-
guage Resources and Evaluation, pages 43?46.
Hoste, V. 2005. Optimization Issues in Machine Learning of
Coreference Resolution. PhD thesis.
Hummel, R. A. and S. W. Zucker. 1987. On the foundations
of relaxation labeling processes. pages 585?605.
Klenner, M. and E?. Ailloud. 2008. Enhancing Coreference
Clustering. In Proceedings of the Second Workshop on
Anaphora Resolution. WAR II.
Klenner, M. and E. Ailloud. 2009. Optimization in Corefer-
ence Resolution Is Not Needed: A Nearly-Optimal Algo-
rithm with Intensional Constraints. In Proceedings of the
12th Conference of the EACL.
Klenner, M. 2007. Enforcing consistency on coreference
sets. In Recent Advances in Natural Language Processing
(RANLP), pages 323?328.
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proceed-
ings of 42nd ACL, page 135.
Luo, X. 2005. On coreference resolution performance met-
rics. Proc. of HLT-EMNLP, pages 25?32.
Luo, X. 2007. Coreference or not: A twin model for coref-
erence resolution. In Proceedings of NAACL HLT, pages
73?80.
Ma`rquez, L., L. Padro?, and H. Rodr??guez. 2000. A ma-
chine learning approach for pos tagging. Machine Learn-
ing Journal, 39(1):59?91.
McCallum, A. and B. Wellner. 2005. Conditional models
of identity uncertainty with application to noun corefer-
ence. Advances in Neural Information Processing Sys-
tems, 17:905?912.
McCarthy, J.F. and W.G. Lehnert. 1995. Using decision
trees for coreference resolution. Proceedings of the Four-
teenth International Conference on Artificial Intelligence,
pages 1050?1055.
Ng, V. and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. Proceedings of the
40th Annual Meeting on Association for Computational
Linguistics, pages 104?111.
Nicolae, C. and G. Nicolae. 2006. Best Cut: A Graph Al-
gorithm for Coreference Resolution. Proceedings of the
2006 Conference on EMNLP, pages 275?283.
Pelillo, M. 1997. The dynamics of nonlinear relaxation la-
beling processes. Journal of Mathematical Imaging and
Vision, 7(4):309?323.
Quinlan, J.R. 1993. C4.5: Programs for Machine Learning.
Morgan Kaufmann.
Rosenfeld, R., R. A. Hummel, and S. W. Zucker. 1976.
Scene labelling by relaxation operations. IEEE Transac-
tions on Systems, Man and Cybernetics, 6(6):420?433.
Soon, W.M., H.T. Ng, and D.C.Y. Lim. 2001. A Machine
Learning Approach to Coreference Resolution of Noun
Phrases. Computational Linguistics, 27(4):521?544.
Surdeanu, M., J. Turmo, and E. Comelles. 2005. Named
Entity Recognition from Spontaneous Open-Domain
Speech. In Ninth European Conference on Speech Com-
munication and Technology. ISCA.
Torras, C. 1989. Relaxation and neural learning: Points
of convergence and divergence. Journal of Parallel and
Distributed Computing, 6:217?244.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scor-
ing scheme. Proceedings of the 6th conference on Mes-
sage understanding, pages 45?52.
Yang, X., G. Zhou, J. Su, and C.L. Tan. 2003. Coreference
resolution using competition learning approach. In ACL
?03: Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 176?183.
1094
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 88?91,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
RelaxCor: A Global Relaxation Labeling Approach to Coreference
Resolution
Emili Sapena, Llu??s Padr
?
o and Jordi Turmo
TALP Research Center
Universitat Polit`ecnica de Catalunya
Barcelona, Spain
{esapena, padro, turmo}@lsi.upc.edu
Abstract
This paper describes the participation
of RelaxCor in the Semeval-2010 task
number 1: ?Coreference Resolution in
Multiple Languages?. RelaxCor is a
constraint-based graph partitioning ap-
proach to coreference resolution solved by
relaxation labeling. The approach com-
bines the strengths of groupwise classifiers
and chain formation methods in one global
method.
1 Introduction
The Semeval-2010 task is concerned with intra-
document coreference resolution for six different
languages: Catalan, Dutch, English, German, Ital-
ian and Spanish. The core of the task is to iden-
tify which noun phrases (NPs) in a text refer to the
same discourse entity (Recasens et al, 2010).
RelaxCor (Sapena et al, 2010) is a graph rep-
resentation of the problem solved by a relaxation
labeling process, reducing coreference resolution
to a graph partitioning problem given a set of con-
straints. In this manner, decisions are taken con-
sidering the whole set of mentions, ensuring con-
sistency and avoiding that classification decisions
are independently taken.
The paper is organized as follows. Section 2 de-
scribes RelaxCor, the system used in the Semeval
task. Next, Section 3 describes the tuning needed
by the system to adapt it to different languages and
other task issues. The same section also analyzes
the obtained results. Finally, Section 4 concludes
the paper.
2 System Description
This section briefly describes RelaxCor. First, the
graph representation is presented. Next, there is
an explanation of the methodology used to learn
constraints and train the system. Finally, the algo-
rithm used for resolution is described.
2.1 Problem Representation
LetG = G(V,E) be an undirected graph where V
is a set of vertices and E a set of edges. Let m =
(m
1
, ...,m
n
) be the set of mentions of a document
with n mentions to resolve. Each mention m
i
in
the document is represented as a vertex v
i
? V
in the graph. An edge e
ij
? E is added to the
graph for pairs of vertices (v
i
, v
j
) representing the
possibility that both mentions corefer.
Let C be our set of constraints. Given a pair of
mentions (m
i
, m
j
), a subset of constraints C
ij
?
C restrict the compatibility of both mentions. C
ij
is used to compute the weight value of the edge
connecting v
i
and v
j
. Let w
ij
? W be the weight
of the edge e
ij
:
w
ij
=
?
k?C
ij
?
k
f
k
(m
i
,m
j
) (1)
where f
k
(?) is a function that evaluates the con-
straint k and ?
k
is the weight associated to the
constraint. Note that ?
k
and w
ij
can be negative.
In our approach, each vertex (v
i
) in the graph
is a variable (v
i
) for the algorithm. Let L
i
be the
number of different values (labels) that are possi-
ble for v
i
. The possible labels of each variable are
the partitions that the vertex can be assigned. A
vertex with index i can be in the first i partitions
(i.e. L
i
= i).
88
Distance and position:
DIST: Distance betweenm
i
andm
j
in sentences: number
DIST MEN: Distance betweenm
i
andm
j
in mentions: number
APPOSITIVE: One mention is in apposition with the other: y,n
I/J IN QUOTES:m
i/j
is in quotes or inside a NP or a sentence
in quotes: y,n
I/J FIRST:m
i/j
is the first mention in the sentence: y,n
Lexical:
I/J DEF NP:m
i/j
is a definitive NP: y,n
I/J DEM NP:m
i/j
is a demonstrative NP: y,n
I/J INDEF NP:m
i/j
is an indefinite NP: y,n
STR MATCH: String matching ofm
i
andm
j
: y,n
PRO STR: Both are pronouns and their strings match: y,n
PN STR: Both are proper names and their strings match: y,n
NONPRO STR: String matching like in Soon et al (2001)
and mentions are not pronouns: y,n
HEAD MATCH: String matching of NP heads: y,n
Morphological:
NUMBER: The number of both mentions match: y,n,u
GENDER: The gender of both mentions match: y,n,u
AGREEMENT: Gender and number of both
mentions match: y,n,u
I/J THIRD PERSON:m
i/j
is 3rd person: y,n
PROPER NAME: Both mentions are proper names: y,n,u
I/J PERSON:m
i/j
is a person (pronoun or
proper name in a list): y,n
ANIMACY: Animacy of both mentions match
(persons, objects): y,n
I/J REFLEXIVE:m
i/j
is a reflexive pronoun: y,n
I/J TYPE:m
i/j
is a pronoun (p), entity (e) or nominal (n)
Syntactic:
NESTED: One mention is included in the other: y,n
MAXIMALNP: Both mentions have the same NP parent
or they are nested: y,n
I/J MAXIMALNP:m
i/j
is not included in any
other mention: y,n
I/J EMBEDDED:m
i/j
is a noun and is not a maximal NP: y,n
BINDING: Conditions B and C of binding theory: y,n
Semantic:
SEMCLASS: Semantic class of both mentions match: y,n,u
(the same as (Soon et al, 2001))
ALIAS: One mention is an alias of the other: y,n,u
(only entities, else unknown)
I/J SRL ARG: Semantic role ofm
i/j
: N,0,1,2,3,4,M,L
SRL SAMEVERB: Both mentions have a semantic role
for the same verb: y,n
Figure 1: Feature functions used.
2.2 Training Process
Each pair of mentions (m
i
, m
j
) in a training doc-
ument is evaluated by the set of feature functions
shown in Figure 1. The values returned by these
functions form a positive example when the pair
of mentions corefer, and a negative one otherwise.
Three specialized models are constructed depend-
ing on the type of anaphor mention (m
j
) of the
pair: pronoun, named entity or nominal.
A decision tree is generated for each specialized
model and a set of rules is extracted with C4.5
rule-learning algorithm (Quinlan, 1993). These
rules are our set of constraints. The C4.5rules al-
gorithm generates a set of rules for each path from
the learned tree. It then checks if the rules can be
generalized by dropping conditions.
Given the training corpus, the weight of a con-
straint C
k
is related with the number of exam-
ples where the constraint applies A
C
k
and how
many of them corefer C
C
k
. We define ?
k
as
the weight of constraint C
k
calculated as follows:
?
k
=
C
C
k
A
C
k
? 0.5
2.3 Resolution Algorithm
Relaxation labeling (Relax) is a generic name for
a family of iterative algorithms which perform
function optimization, based on local informa-
tion (Hummel and Zucker, 1987). The algorithm
solves our weighted constraint satisfaction prob-
lem dealing with the edge weights. In this manner,
each vertex is assigned to a partition satisfying as
many constraints as possible. To do that, the al-
gorithm assigns a probability for each possible la-
bel of each variable. Let H = (h
1
,h
2
, . . . ,h
n
) be
the weighted labeling to optimize, where each h
i
is a vector containing the probability distribution
of v
i
, that is: h
i
= (h
i
1
, h
i
2
, . . . , h
i
L
i
). Given that
the resolution process is iterative, the probability
for label l of variable v
i
at time step t is h
i
l
(t), or
simply h
i
l
when the time step is not relevant.
Initialize:
H := H
0
,
Main loop:
repeat
For each variable v
i
For each possible label l for v
i
S
il
=
?
j?A(v
i
)
w
ij
? h
j
l
End for
For each possible label l for v
i
h
i
l
(t + 1) =
h
i
l
(t)?(1+S
il
)
?
L
i
k=1
h
i
k
(t)?(1+S
ik
)
End for
End for
Until no more significant changes
Figure 2: Relaxation labeling algorithm
The support for a pair variable-label (S
il
) ex-
presses how compatible is the assignment of la-
bel l to variable v
i
taking into account the labels
of adjacent variables and the edge weights. The
support is defined as the sum of the edge weights
that relate variable v
i
with each adjacent variable
v
j
multiplied by the weight for the same label l of
variable v
j
: S
il
=
?
j?A(v
i
)
w
ij
? h
j
l
where w
ij
is
the edge weight obtained in Equation 1 and vertex
v
i
has |A(v
i
)| adjacent vertices. In our version of
the algorithm for coreference resolution A(v
i
) is
the list of adjacent vertices of v
i
but only consid-
ering the ones with an index k < i.
The aim of the algorithm is to find a weighted
labeling such that global consistency is maxi-
mized. Maximizing global consistency is defined
89
Figure 3: Representation of Relax. The vertices represent-
ing mentions are connected by weighted edges e
ij
. Each ver-
tex has a vector h
i
of probabilities to belong to different par-
titions. The figure shows h
2
, h
3
and h
4
.
as maximizing the average support for each vari-
able. The final partitioning is directly obtained
from the weighted labeling H assigning to each
variable the label with maximum probability.
The pseudo-code of the relaxation algorithm
can be found in Figure 2. The process updates
the weights of the labels in each step until con-
vergence, i.e. when no more significant changes
are done in an iteration. Finally, the assigned label
for a variable is the one with the highest weight.
Figure 3 shows an example of the process.
3 Semeval task participation
RelaxCor have participated in the Semeval task for
English, Catalan and Spanish. The system does
not detect the mentions of the text by itself. Thus,
the participation has been restricted to the gold-
standard evaluation, which includes the manual
annotated information and also provides the men-
tion boundaries.
All the knowledge required by the feature func-
tions (Figure 1) is obtained from the annota-
tions of the corpora and no external resources
have been used, with the exception of WordNet
(Miller, 1995) for English. In this case, the sys-
tem has been run two times for English: English-
open, using WordNet, and English-closed, without
WordNet.
3.1 Language and format adaptation
The whole methodology of RelaxCor including
the resolution algorithm and the training process
is totally independent of the language of the docu-
ment. The only parts that need few adjustments are
the preprocess and the set of feature functions. In
most cases, the modifications in the feature func-
tions are just for the different format of the data
for different languages rather than for specific lan-
guage issues. Moreover, given that the task in-
cludes many information about the mentions of the
documents such as part of speech, syntactic depen-
dency, head and semantic role, no preprocess has
been needed.
One of the problems we have found adapting the
system to the task corpora was the large amount
of available data. As described in Section 2.2,
the training process generates a feature vector for
each pair of mentions into a document for all
the documents of the training data set. However,
the great number of training documents and their
length overwhelmed the software that learns the
constraints. In order to reduce the amount of pair
examples, we run a clustering process to reduce
the number of negative examples using the posi-
tive examples as the centroids. Note that negative
examples are near 94% of the training examples,
and many of them are repeated. For each positive
example (a corefering pair of mentions), only the
negative examples with distance less than a thresh-
old d are included in the final training data. The
distance is computed as the number of different
values inside the feature vector. After some exper-
iments over development data, the value of d was
assigned to 3. Thus, the negative examples were
discarded when they have more than three features
different than any positive example.
Our results for the development data set are
shown in Table 1.
3.2 Results analysis
Results of RelaxCor for the test data set are shown
in Table 2. One of the characteristics of the sys-
tem is that the resolution process always takes
into account the whole set of mentions and avoids
any possible pair-linkage contradiction as well as
forces transitivity. Therefore, the system favors
the precision, which results on high scores with
metrics CEAF and B
3
. However, the system is
penalized with the metrics based on pair-linkage,
specially with MUC. Although RelaxCor has the
highest precision scores even for MUC, the recall
is low enough to finally obtain low scores for F
1
.
Regarding the test scores of the task comparing
with the other participants (Recasens et al, 2010),
RelaxCor obtains the best performances for Cata-
90
- CEAF MUC B
3
language R P F
1
R P F
1
R P F
1
ca 69.7 69.7 69.7 27.4 77.9 40.6 67.9 96.1 79.6
es 70.8 70.8 70.8 30.3 76.2 43.4 68.9 95.0 79.8
en-closed 74.8 74.8 74.8 21.4 67.8 32.6 74.1 96.0 83.7
en-open 75.0 75.0 75.0 22.0 66.6 33.0 74.2 95.9 83.7
Table 1: Results on the development data set
- CEAF MUC B
3
BLANC
language R P F
1
R P F
1
R P F
1
R P Blanc
Information: closed Annotation: gold
ca 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7
es 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6
en 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3
Information: open Annotation: gold
en 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
Table 2: Results of the task
lan (CEAF and B
3
), English (closed: CEAF and
B
3
; open: B
3
) and Spanish (B
3
). Moreover, Relax-
Cor is the most precise system for all the metrics
in all the languages except for CEAF in English-
open and Spanish. This confirms the robustness of
the results of RelaxCor but also remarks that more
knowledge or more information is needed to in-
crease the recall of the system without loosing this
precision
The incorporation of WordNet to the English
run is the only difference between English-open
and English-closed. The scores are slightly higher
when using WordNet but not significant. Analyz-
ing the MUC scores, note that the recall is im-
proved, while precision decreases a little which
corresponds with the information and the noise
that WordNet typically provides.
The results for the test and development are
very similar as expected, except the Spanish (es)
ones. The recall considerably falls from develop-
ment to test. It is clearly shown in the MUC recall
and also is indirectly affecting on the other scores.
4 Conclusion
The participation of RelaxCor to the Semeval
coreference resolution task has been useful to eval-
uate the system in multiple languages using data
never seen before. Many published systems typi-
cally use the same data sets (ACE and MUC) and
it is easy to unintentionally adapt the system to the
corpora and not just to the problem. This kind of
tasks favor comparisons between systems with the
same framework and initial conditions.
The results obtained confirm the robustness of
the RelaxCor, and the performance is considerably
good in the state of the art. The system avoids con-
tradictions in the results which causes a high pre-
cision. However, more knowledge is needed about
the mentions in order to increase the recall without
loosing that precision. A further error analysis is
needed, but one of the main problem is the lack of
semantic information and world knowledge spe-
cially for the nominal mentions ? the mentions that
are NPs but not including named entities neither
pronouns?.
Acknowledgments
The research leading to these results has received funding
from the European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement number
247762 (FAUST), and from the Spanish Science and Inno-
vation Ministry, via the KNOW2 project (TIN2009-14715-
C04-04).
References
R. A. Hummel and S. W. Zucker. 1987. On the foundations
of relaxation labeling processes. pages 585?605.
G.A. Miller. 1995. WordNet: a lexical database for English.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learning.
Morgan Kaufmann.
M. Recasens, L. M`arquez, E. Sapena, M.A. Mart??, M. Taul?e,
V. Hoste, M. Poesio, and Y. Versley. 2010. SemEval-2010
Task 1: Coreference resolution in multiple languages. In
Proceedings of the 5th International Workshop on Seman-
tic Evaluations (SemEval-2010), Uppsala, Sweden.
E. Sapena, L. Padr?o, and J. Turmo. 2010. A Global Relax-
ation Labeling Approach to Coreference Resolution. Sub-
mitted.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A Machine
Learning Approach to Coreference Resolution of Noun
Phrases. Computational Linguistics, 27(4):521?544.
91
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 143?147, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UPC-CORE: What Can Machine Translation Evaluation Metrics and
Wikipedia Do for Estimating Semantic Textual Similarity?
Alberto Barro?n-Ceden?o1,2 Llu??s Ma`rquez1 Maria Fuentes1 Horacio Rodr??guez1 Jordi Turmo1
1 TALP Research Center, Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, 08034, Barcelona, Spain
2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid
Boadilla del Monte, 28660 Madrid, Spain
albarron, lluism, mfuentes, horacio, turmo @lsi.upc.edu
Abstract
In this paper we discuss our participation to
the 2013 Semeval Semantic Textual Similarity
task. Our core features include (i) a set of met-
rics borrowed from automatic machine trans-
lation, originally intended to evaluate auto-
matic against reference translations and (ii) an
instance of explicit semantic analysis, built
upon opening paragraphs of Wikipedia 2010
articles. Our similarity estimator relies on a
support vector regressor with RBF kernel. Our
best approach required 13 machine transla-
tion metrics + explicit semantic analysis and
ranked 65 in the competition. Our post-
competition analysis shows that the features
have a good expression level, but overfitting
and ?mainly? normalization issues caused
our correlation values to decrease.
1 Introduction
Our participation to the 2013 Semantic Textual Sim-
ilarity task (STS) (Agirre et al, 2013)1 was focused
on the CORE problem: GIVEN TWO SENTENCES,
s1 AND s2, QUANTIFIABLY INFORM ON HOW SIMI-
LAR s1 AND s2 ARE. We considered real-valued fea-
tures from four different sources: (i) a set of linguis-
tic measures computed with the Asiya Toolkit for
Automatic MT Evaluation (Gime?nez and Ma`rquez,
2010b), (ii) an instance of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), built on top
of Wikipedia articles, (iii) a dataset predictor, and
(iv) a subset of the features available in Takelab?s
Semantic Text Similarity system (?Saric? et al, 2012).
1http://ixa2.si.ehu.es/sts/
Our approaches obtained an overall modest result
compared to other participants (best position: 65 out
of 89). Nevertheless, our post-competition analysis
shows that the low correlation was caused mainly by
a deficient data normalization strategy.
The paper distribution is as follows. Section 2 of-
fers a brief overview of the task. Section 3 describes
our approach. Section 4 discuss our experiments and
obtained results. Section 5 provides conclusions.
2 Task Overview
Detecting two similar text fragments is a difficult
task in cases where the similarity occurs at seman-
tic level, independently of the implied lexicon (e.g
in cases of dense paraphrasing). As a result, simi-
larity estimation models must involve features other
than surface aspects. The STS task is proposed as
a challenge focused in short English texts of dif-
ferent nature: from automatic machine translation
alternatives to human descriptions of short videos.
The test partition also included texts extracted from
news headlines and FrameNet?Wordnet pairs.
The range of similarity was defined between 0
(no relation) up to 5 (semantic equivalence). The
gold standard values were averaged from different
human-made annotations. The expected system?s
output was composed of a real similarity value, to-
gether with an optional confidence level (our confi-
dence level was set constant).
Table 1 gives an overview of the development
(2012 training and test) and test datasets. Note
that both collections extracted from SMT data are
highly biased towards the maximum similarity val-
ues (more than 75% of the instances have a similar-
143
Table 1: Overview of sub-collections in the development and test datasets, including number of instances and distri-
bution of similarity values (in percentage) as well as mean, minimum, and maximum lengths.
similarity distribution length
dataset instances [0, 1) [1, 2) [2, 3) [3, 4) [4, 5] mean min max
dev-[train + test]
MSRpar 1,500 1.20 8.13 17.13 48.73 24.80 17.84 5 30
MSRvid 1,500 31.00 14.13 15.47 20.87 18.53 6.66 2 24
SMTEuroparl 1,193 0.67 0.42 1.17 12.32 85.4 21.13 1 72
OnWN 750 2.13 2.67 10.40 25.47 59.33 7.57 1 34
SMTnews 399 1.00 0.75 5.51 13.03 79.70 11.72 2 28
test
headlines 750 15.47 22.00 16.27 24.67 21.60 7.21 3 22
OnWN 561 36.54 9.80 7.49 17.11 29.05 7.17 5 22
FNWN 189 34.39 29.63 28.57 6.88 0.53 19.90 3 71
SMT 750 0.00 0.27 3.47 20.40 75.87 26.40 1 96
ity higher than 4) and include the longest instances.
On the other hand, the FNWN instances are shifted
towards low similarity levels (more than 60% have a
similarity lower than 2).
3 Approach
Our similarity assessment model relies upon
SVMlight?s support vector regressor, with RBF ker-
nel (Joachims, 1999).2 Our model estimation pro-
cedure consisted of two steps: parameter defini-
tion and backward elimination-based feature selec-
tion. The considered features belong to four fami-
lies, briefly described in the following subsections.
3.1 Machine Translation Evaluation Metrics
We consider a set of linguistic measures originally
intended to evaluate the quality of automatic trans-
lation systems. These measures compute the quality
of a translation by comparing it against one or sev-
eral reference translations, considered as gold stan-
dard. A straightforward application of these mea-
sures to the problem at hand is to consider s1 as the
reference and s2 as the automatic translation, or vice
versa. Some of the metrics are not symmetric so we
compute similarity between s1 and s2 in both direc-
tions and average the resulting scores.
The measures are computed with the Asiya
Toolkit for Automatic MT Evaluation (Gime?nez and
Ma`rquez, 2010b). The only pre-processing carried
out was tokenization (Asiya performs additional in-
box pre-processing operations, though). We consid-
2We also tried with linear kernels, but RBF always obtained
better results.
ered a sample from three similarity families, which
was proposed in (Gime?nez and Ma`rquez, 2010a) as
a varied and robust metric set, showing good corre-
lation with human assessments.3
Lexical Similarity Two metrics of Translation
Error Rate (Snover et al, 2006) (i.e. the esti-
mated human effort to convert s1 into s2): -TER
and -TERpA. Two measures of lexical precision:
BLEU (Papineni et al, 2002) and NIST (Dod-
dington, 2002). One measure of lexical recall:
ROUGEW (Lin and Och, 2004). Finally, four vari-
ants of METEOR (Banerjee and Lavie, 2005) (exact,
stemming, synonyms, and paraphrasing), a lexical
metric accounting for F -Measure.
Syntactic Similarity Three metrics that estimate
the similarity of the sentences over dependency
parse trees (Liu and Gildea, 2005): DP-HWCMic-4
for grammatical categories chains, DP-HWCMir-4
over grammatical relations, and DP-Or(?) over
words ruled by non-terminal nodes. Also, one mea-
sure that estimates the similarity over constituent
parse trees: CP-STM4 (Liu and Gildea, 2005).
Semantic Similarity Three measures that esti-
mate the similarities over semantic roles (i.e. ar-
guments and adjuncts): SR-Or, SR-Mr(?), and
SR-Or(?). Additionally, two metrics that es-
timate similarities over discourse representations:
DR-Or(?) and DR-Orp(?).
3Asiya is available at http://asiya.lsi.upc.edu.
Full descriptions of the metrics are available in the Asiya Tech-
nical Manual v2.0, pp. 15?21.
144
3.2 Explicit Semantic Analysis
We built an instance of Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007) with
the first paragraph of 100k Wikipedia articles (dump
from 2010).Pre-processing consisted of tokenization
and lemmatization.
3.3 Dataset Prediction
Given the similarity shifts in the different datasets
(cf. Table 1), we tried to predict what dataset an in-
stance belonged to on the basis of its vocabulary. We
built binary maxent classifiers for each dataset in the
development set, resulting in five dataset likelihood
features: dMSRpar, dSMTeuroparl, dMSRvid,
dOnWN, and dSMTnews.4 Pre-processing consisted
of tokenization and lemmatization.
3.4 Baseline
We considered the features included in the Takelab
Semantic Text Similarity system (?Saric? et al, 2012),
one of the top-systems in last year competition. This
system is used as a black box. The resulting features
are named tklab n, where n = [1, 21].
Our runs departed from three increasing subsets
of features: AE machine translation evaluation met-
rics and explicit semantic analysis, AED the pre-
vious set plus dataset prediction, and AED T the
previous set plus Takelab?s baseline features (cf. Ta-
ble 3). We performed a feature normalization, which
relied on the different feature?s distribution over the
entire dataset. Firstly, features were bounded in the
range ??3??2 in order to reduce the potentially neg-
ative impact of outliers. Secondly, we normalized
according to the z-score (Nardo et al, 2008, pp. 28,
84); i.e. x = (x ? ?)/?. As a result, each real-
valued feature distribution in the dataset has ? = 0
and ? = 1. During the model tuning stage we tried
with other numerous normalization options: normal-
izing each dataset independently, together with the
training set, and without normalization at all. Nor-
malizing according to the entire dev-test dataset led
to the best results
4We used the Stanford classifier; http://nlp.
stanford.edu/software/classifier.shtml
Table 2: Tuning process: parameter definition and feature
selection. Number of features at the beginning and end
of the feature selection step included.
run parameter def. feature sel.
c ? ? corr b e corr
AE 3.7 0.06 0.3 0.8257 19 14 0.8299
AED 3.8 0.03 0.2 0.8413 24 19 0.8425
AED T 2.9 0.02 0.3 0.8761 45 33 0.8803
4 Experiments and Results
Section 4.1 describes our model tuning strategy.
Sections 4.2 and 4.3 discuss the official and post-
competition results.
4.1 Model Tuning
We used only the dev-train partition (2012 training)
for tuning. By means of a 10-fold cross validation
process, we defined the trade-off (c), gamma (?),
and tube width (?) parameters for the regressor and
performed a backward-elimination feature selection
process (Witten and Frank, 2005, p. 294), indepen-
dently for the three experiments.
The results for the cross-validation process are
summarized in Table 2. The three runs allow for cor-
relations higher than 0.8. On the one hand, the best
regressor parameters obtain better results as more
features are considered, still with very small differ-
ences. On the other hand, the low correlation in-
crease after the feature selection step shows that a
few features are indeed irrelevant.
A summary of the features considered in each ex-
periment (also after feature selection) is displayed in
Table 3. The correlation obtained over the dev-test
partition are corrAE = 0.7269, corrAED = 0.7638,
and corrAEDT = 0.8044 ?it would have appeared
in the top-10 ranking of the 2012 competition.
4.2 Official Results
We trained three new regressors with the features
considered relevant by the tuning process, but using
the entire development dataset. The test 2013 parti-
tion was normalized again by means of z-score, con-
sidering the means and standard deviations of the en-
tire test dataset. Table 4 displays the official results.
Our best approach ?AE?, was positioned in rank
65. The worst results of run AED can be explained
by the difference in the nature of the test respect to
145
Table 3: Features considered at the beginning of each run, represented as empty squares (). Filled squares ()
represent features considered relevant after feature selection.
Feature AE AED AED T Feature AE AED AED T Feature AED T
DP-HWCM c-4    METEOR-pa    tklab 7 
DP-HWCM r-4    METEOR-st    tklab 8 
DP-Or(*)    METEOR-sy    tklab 9 
CP-STM-4    ESA    tklab 10 
SR-Or(*)    dMSRpar   tklab 11 
SR-Mr(*)    dSMTeuroparl   tklab 12 
SR-Or    dMSRvid   tklab 13 
DR-Or(*)    dOnWN   tklab 14 
DR-Orp(*)    dSMTnews   tklab 15 
BLEU    tklab 1  tklab 16 
NIST    tklab 2  tklab 17 
-TER    tklab 3  tklab 18 
-TERp-A    tklab 4  tklab 19 
ROUGE-W    tklab 5  tklab 20 
METEOR-ex    tklab 6  tklab 21 
Table 4: Official results for the three runs (rank included).
run headlines OnWN FNWN SMT mean
AE (65) 0.6092 0.5679 -0.1268 0.2090 0.4037
AED (83) 0.4136 0.4770 -0.0852 0.1662 0.3050
AED T (72) 0.5119 0.6386 -0.0464 0.1235 0.3671
the development dataset. AED T obtains worst re-
sults than AE on the headlines and SMT datasets.
The reason behind this behavior can be in the dif-
ference of vocabularies respect to that stored in the
Takelab system (it includes only the vocabulary of
the development partition). This could be the same
reason behind the drop in performance with respect
to the results previously obtained on the dev-test par-
tition (cf. Section 4.1).
4.3 Post-Competition Results
Our analysis of the official results showed the main
issue was normalization. Thus, we performed a
manifold of new experiments, using the same con-
figuration as in run AE, but applying other normal-
ization strategies: (a) z-score normalization, but ig-
noring the FNWN dataset (given its shift through
low values); (b) z-score normalization, but consid-
ering independent means and standard deviations for
each test dataset; and (c) without normalizing any of
dataset (including the regressor one).
Table 5 includes the results. (a) makes evident
that the instances in FNWN represent ?anomalies?
that harm the normalized values of the rest of sub-
sets. Run (b) shows that normalizing the test sets
Table 5: Post-competition experiments results
run headlines OnWN FNWN SMT mean
AE (a) 0.6210 0.5905 -0.0987 0.2990 0.4456
AE (b) 0.6072 0.4767 -0.0113 0.3236 0.4282
AE (c) 0.6590 0.6973 0.1547 0.3429 0.5208
independently is not a good option, as the regressor
is trained considering overall normalizations, which
explains the correlation decrease. Run (c) is com-
pletely different: not normalizing any dataset ?
both in development and test? reduces the influ-
ence of the datasets to each other and allows for the
best results. Indeed, this configuration would have
advanced practically forty positions at competition
time, locating us in rank 27.
Estimating the adequate similarities over FNWN
seems particularly difficult for our systems. We ob-
serve two main factors. (i) FNWN presents an im-
portant similarity shift respect to the other datasets:
nearly 90% of the instances similarity is lower than
2.5 and (ii) the average lengths of s1 and s2 are very
different: 30 vs 9 words. These characteristics made
it difficult for our MT evaluation metrics to estimate
proper similarity values (be normalized or not).
We performed two more experiments over
FNWN: training regressors with ESA as the only
feature, before and after normalization. The correla-
tion was 0.16017 and 0.3113, respectively. That is,
the normalization mainly affects the MT features.
146
5 Conclusions
In this paper we discussed on our participation to the
2013 Semeval Semantic Textual Similarity task. Our
approach relied mainly upon a combination of au-
tomatic machine translation evaluation metrics and
explicit semantic analysis. Building an RBF support
vector regressor with these features allowed us for a
modest result in the competition (our best run was
ranked 65 out of 89).
Acknowledgments
We would like to thank the organizers of this chal-
lenging task for their efforts.
This research work was partially carried out dur-
ing the tenure of an ERCIM ?Alain Bensoussan?
Fellowship. The research leading to these results re-
ceived funding from the EU FP7 Programme 2007-
2013 (grants 246016 and 247762). Our research
work is partially supported by the Spanish research
projects OpenMT-2 and SKATER (TIN2009-14675-
C03, TIN2012-38584-C06-01).
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pilot on
Typed-Similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Gold-
stein et al (Goldstein et al, 2005), pages 65?72.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-Gram Co-
occurrence Statistics. In Proceedings of the Second
International Conference on Human Language Tech-
nology Research, pages 138?145, San Francisco, CA.
Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial Intel-
ligence, pages 1606?1611, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94).
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
Measures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4):209?240.
Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare
Voss, editors. 2005. Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization. Asso-
ciation for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods ?
Support Vector Learning, chapter Making large-Scale
SVM Learning Practical. MIT Press.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Stroudsburg, PA. Association for Com-
putational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Goldstein
et al (Goldstein et al, 2005), pages 25?32.
Michela Nardo, Michaela Saisana, Andrea Saltelli, Ste-
fano Tarantola, Anders Hoffmann, and Enrico Giovan-
nini. 2008. Handbook on Constructing Composite In-
dicators: Methodology and User Guide. OECD Pub-
lishing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2002), pages 311?318,
Philadelphia, PA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,
and Bojana Dalbelo Bas?ic?. 2012. TakeLab: Sys-
tems for Measuring Semantic Text. In First Joint
Conference on Lexical and Computational Semantics
(*SEM), pages 441?448, Montre?al, Canada. Associa-
tion for Computational Linguistics.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, CA, 2 edition.
147
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 35?39,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
RelaxCor Participation in CoNLL Shared Task on Coreference Resolution
Emili Sapena, Llu??s Padro? and Jordi Turmo?
TALP Research Center
Universitat Polite`cnica de Catalunya
Barcelona, Spain
{esapena, padro, turmo}@lsi.upc.edu
Abstract
This paper describes the participation of
RELAXCOR in the CoNLL-2011 shared
task: ?Modeling Unrestricted Coreference in
Ontonotes?. RELAXCOR is a constraint-based
graph partitioning approach to coreference
resolution solved by relaxation labeling. The
approach combines the strengths of groupwise
classifiers and chain formation methods in one
global method.
1 Introduction
The CoNLL-2011 shared task (Pradhan et al, 2011)
is concerned with intra-document coreference reso-
lution in English, using Ontonotes corpora. The core
of the task is to identify which expressions (usually
NPs) in a text refer to the same discourse entity.
This paper describes the participation of RELAX-
COR and is organized as follows. Section 2 de-
scribes RELAXCOR, the system used in the task.
Next, Section 3 describes the tuning needed by the
system to adapt it to the task issues. The same sec-
tion also analyzes the obtained results. Finally, Sec-
tion 4 concludes the paper.
2 System description
RELAXCOR (Sapena et al, 2010a) is a coreference
resolution system based on constraint satisfaction.
It represents the problem as a graph connecting any
?Research supported by the Spanish Science and Innova-
tion Ministry, via the KNOW2 project (TIN2009-14715-C04-
04) and from the European Community?s Seventh Framework
Programme (FP7/2007-2013) under Grant Agreement number
247762 (FAUST)
pair of candidate coreferent mentions and applies re-
laxation labeling, over a set of constraints, to decide
the set of most compatible coreference relations.
This approach combines classification and cluster-
ing in one step. Thus, decisions are taken consider-
ing the entire set of mentions, which ensures consis-
tency and avoids local classification decisions. The
RELAXCOR implementation used in this task is an
improved version of the system that participated in
the SemEval-2010 Task 1 (Recasens et al, 2010).
The knowledge of the system is represented as a
set of weighted constraints. Each constraint has an
associated weight reflecting its confidence. The sign
of the weight indicates that a pair or a group of men-
tions corefer (positive) or not (negative). Only con-
straints over pairs of mentions were used in the cur-
rent version of RELAXCOR. However, RELAXCOR
can handle higher-order constraints. Constraints can
be obtained from any source, including a training
data set from which they can be manually or auto-
matically acquired.
The coreference resolution problem is represented
as a graph with mentions in the vertices. Mentions
are connected to each other by edges. Edges are as-
signed a weight that indicates the confidence that the
mention pair corefers or not. More specifically, an
edge weight is the sum of the weights of the con-
straints that apply to that mention pair. The larger
the edge weight in absolute terms, the more reliable.
RELAXCOR uses relaxation labeling for the res-
olution process. Relaxation labeling is an iterative
algorithm that performs function optimization based
on local information. It has been widely used to
solve NLP problems. An array of probability values
35
is maintained for each vertex/mention. Each value
corresponds to the probability that the mention be-
longs to a specific entity given all the possible enti-
ties in the document. During the resolution process,
the probability arrays are updated according to the
edge weights and probability arrays of the neighbor-
ing vertices. The larger the edge weight, the stronger
the influence exerted by the neighboring probability
array. The process stops when there are no more
changes in the probability arrays or the maximum
change does not exceed an epsilon parameter.
2.1 Attributes and Constraints
For the present study, all constraints were learned
automatically using more than a hundred attributes
over the mention pairs in the training sets. Usual at-
tributes were used for each pair of mentions (mi,mj)
?where i < j following the order of the document?
, like those in (Sapena et al, 2010b), but bina-
rized for each possible value. In addition, a set
of new mention attributes were included such as
SAME SPEAKER when both mentions have the
same speaker1 (Figures 1 and 2).
A decision tree was generated from the train-
ing data set, and a set of constraints was extracted
with the C4.5 rule-learning algorithm (Quinlan,
1993). The so-learned constraints are conjunctions
of attribute-value pairs. The weight associated with
each constraint is the constraint precision minus a
balance value, which is determined using the devel-
opment set. Figure 3 is an example of a constraint.
2.2 Training data selection
Generating an example for each possible pair of
mentions produces an unbalanced dataset where
more than 99% of the examples are negative (not
coreferent), even more considering that the mention
detection system has a low precision (see Section
3.1). So, it generates large amounts of not coref-
erent mentions. In order to reduce the amount of
negative pair examples, a clustering process is run
using the positive examples as the centroids. For
each positive example, only the negative examples
with distance equal or less than a threshold d are
included in the final training data. The distance is
computed as the number of different attribute values
1This information is available in the column ?speaker? of
the corpora.
Distance and position:
Distance between mi and mj in sentences:
DIST SEN 0: same sentence
DIST SEN 1: consecutive sentences
DIST SEN L3: less than 3 sentences
Distance between mi and mj in phrases:
DIST PHR 0, DIST PHR 1, DIST PHR L3
Distance between mi and mj in mentions:
DIST MEN 0, DIST MEN L3, DIST MEN L10
APPOSITIVE: One mention is in apposition with the other.
I/J IN QUOTES: mi/j is in quotes or inside a NP
or a sentence in quotes.
I/J FIRST: mi/j is the first mention in the sentence.
Lexical:
STR MATCH: String matching of mi and mj
PRO STR: Both are pronouns and their strings match
PN STR: Both are proper names and their strings match
NONPRO STR: String matching like in Soon et al (2001)
and mentions are not pronouns.
HEAD MATCH: String matching of NP heads
TERM MATCH: String matching of NP terms
I/J HEAD TERM: mi/j head matches with the term
Morphological:
The number of both mentions match:
NUMBER YES, NUMBER NO, NUMBER UN
The gender of both mentions match:
GENDER YES, GENDER NO, GENDER UN
Agreement: Gender and number of both mentions match:
AGREEMENT YES, AGREEMENT NO, AGREEMENT UN
Closest Agreement: mi is the first agreement found
looking backward from mj : C AGREEMENT YES,
C AGREEMENT NO, C AGREEMENT UN
I/J THIRD PERSON: mi/j is 3rd person
I/J PROPER NAME: mi/j is a proper name
I/J NOUN: mi/j is a common noun
ANIMACY: Animacy of both mentions match (person, object)
I/J REFLEXIVE: mi/j is a reflexive pronoun
I/J POSSESSIVE: mi/j is a possessive pronoun
I/J TYPE P/E/N: mi/j is a pronoun (p), NE (e) or nominal (n)
Figure 1: Mention-pair attributes (1/2).
inside the feature vector. After some experiments
over development data, the value of d was assigned
to 5. Thus, the negative examples were discarded
when they have more than five attribute values dif-
ferent than any positive example. So, in the end,
22.8% of the negative examples are discarded. Also,
both positive and negative examples with distance
zero (contradictions) are discarded.
2.3 Development process
The current version of RELAXCOR includes a pa-
rameter optimization process using the development
data sets. The optimized parameters are balance and
pruning. The former adjusts the constraint weights
to improve the balance between precision and re-
call as shown in Figure 4; the latter limits the num-
ber of neighbors that a vertex can have. Limiting
36
Syntactic:
I/J DEF NP: mi/j is a definite NP.
I/J DEM NP: mi/j is a demonstrative NP.
I/J INDEF NP: mi/j is an indefinite NP.
NESTED: One mention is included in the other.
MAXIMALNP: Both mentions have the same NP parent
or they are nested.
I/J MAXIMALNP: mi/j is not included in any other NP.
I/J EMBEDDED: mi/j is a noun and is not a maximal NP.
C COMMANDS IJ/JI: mi/j C-Commands mj/i.
BINDING POS: Condition A of binding theory.
BINDING NEG: Conditions B and C of binding theory.
I/J SRL ARG N/0/1/2/X/M/L/Z: Syntactic argument of mi/j .
SAME SRL ARG: Both mentions are the same argument.
I/J COORDINATE: mi/j is a coordinate NP
Semantic:
Semantic class of both mentions match
(the same as (Soon et al, 2001))
SEMCLASS YES, SEMCLASS NO, SEMCLASS UN
One mention is an alias of the other:
ALIAS YES, ALIAS NO, ALIAS UN
I/J PERSON: mi/j is a person.
I/J ORGANIZATION: mi/j is an organization.
I/J LOCATION: mi/j is a location.
SRL SAMEVERB: Both mentions have a semantic role
for the same verb.
SRL SAME ROLE: The same semantic role.
SAME SPEAKER: The same speaker for both mentions.
Figure 2: Mention-pair attributes (2/2).
DIST SEN 1 & GENDER YES & I FIRST &
I MAXIMALNP & J MAXIMALNP &
I SRL ARG 0 & J SRL ARG 0 &
I TYPE P & J TYPE P
Precision: 0.9581
Training examples: 501
Figure 3: Example of a constraint. It applies when the distance
between mi and mj is exactly 1 sentence, their gender match,
both are maximal NPs, both are argument 0 (subject) of their
respective sentences, both are pronouns, and mi is not the first
mention of its sentence. The final weight will be weight =
precision? balance.
the number of neighbors reduces the computational
cost significantly and improves overall performance
too. Optimizing this parameter depends on proper-
ties like document size and the quality of the infor-
mation given by the constraints.
The development process calculates a grid given
the possible values of both parameters: from 0 to 1
for balance with a step of 0.05, and from 2 to 14
for pruning with a step of 2. Both parameters were
empirically adjusted on the development set for the
evaluation measure used in this shared task: the un-
weighted average of MUC (Vilain et al, 1995), B3
(Bagga and Baldwin, 1998) and entity-based CEAF
(Luo, 2005).
Figure 4: Development process. The figure shows MUC?s pre-
cision (red), recall (green), and F1 (blue) for each balance value
with pruning adjusted to 6.
3 CoNLL shared task participation
RELAXCOR has participated in the CoNLL task in
the Closed mode. All the knowledge required by the
feature functions is obtained from the annotations
of the corpora and no external resources have been
used with the exception of WordNet (Miller, 1995),
gender and number information (Bergsma and Lin,
2006) and sense inventories. All of them are allowed
by the task organization and available in their web-
site.
There are many remarkable features that make
this task different and more difficult but realistic
than previous ones. About mention annotation, it
is important to emphasize that singletons are not an-
notated, mentions must be detected by the system
and the mapping between system and true mentions
is limited to exact matching of boundaries. More-
over, some verbs have been annotated as corefering
mentions. Regarding the evaluation, the scorer uses
the modification of (Cai and Strube, 2010), unprece-
dented so far, and the corpora was published very re-
cently and there are no published results yet to use as
reference. Finally, all the preprocessed information
is automatic for the test dataset, carring out some
noisy errors which is a handicap from the point of
view of machine learning.
Following there is a description of the mention de-
tection system developed for the task and an analysis
of the obtained results in the development dataset.
37
3.1 Mention detection system
The mention detection system extracts one mention
for every NP found in the syntactic tree, one for ev-
ery pronoun and one for every named entity. Then,
the head of every NP is determined using part-of-
speech tags and a set of rules from (Collins, 1999).
In case that some NPs share the same head, the
larger NP is selected and the rest discarded. Also the
mention repetitions with exactly the same bound-
aries are discarded. In addition, nouns with capital
letters and proper names not included yet, that ap-
pear two or more times in the document, are also in-
cluded. For instance, the NP ?an Internet business?
is added as a mention, but also ?Internet? itself is
added in the case that the word is found once again
in the document.
As a result, taking into account that just exact
boundary matching is accepted, the mention detec-
tion achieves an acceptable recall, higher than 90%,
but a low precision (see Table 1). The most typ-
ical error made by the system is to include ex-
tracted NPs that are not referential (e.g., predicative
and appositive phrases) and mentions with incorrect
boundaries. The incorrect boundaries are mainly
due to errors in the predicted syntactic column and
some mention annotation discrepancies. Further-
more, verbs are not detected by this algorithm, so
most of the missing mentions are verbs.
3.2 Results analysis
The results obtained by RELAXCOR can be found
in Tables 1 and 2. Due to the lack of annotated sin-
gletons, mention-based metrics B3 and CEAF pro-
duce lower scores ?near 60% and 50% respectively?
than the ones typically achieved with different anno-
tations and mapping policies ?usually near 80% and
70%. Moreover, the requirement that systems use
automatic preprocessing and do their own mention
detection increase the difficulty of the task which ob-
viously decreases the scores in general.
The measure which remains more stable on its
scores is MUC given that it is link-based and not
takes singletons into account anyway. Thus, it is the
only one comparable with the state of the art right
now. The results obtained with MUC scorer show an
improvement of RELAXCOR?s recall, a feature that
needed improvement given the previous published
Measure Recall Precision F1
Mention detection 92.45 27.34 42.20
mention-based CEAF 55.27 55.27 55.27
entity-based CEAF 47.20 40.01 43.31
MUC 54.53 62.25 58.13
B3 63.72 73.83 68.40
(CEAFe+MUC+B3)/3 - - 56.61
Table 1: Results on the development data set
Measure Recall Precision F1
mention-based CEAF 53.51 53.51 53.51
entity-based CEAF 44.75 38.38 41.32
MUC 56.32 63.16 59.55
B3 62.16 72.08 67.09
BLANC 69.50 73.07 71.10
(CEAFe+MUC+B3)/3 - - 55.99
Table 2: Official test results
results with a MUCs recall remarkably low (Sapena
et al, 2010b).
4 Conclusion
The participation of RELAXCOR to the CoNLL
shared task has been useful to evaluate the system
using data never seen before in a totally automatic
context: predicted preprocessing and system men-
tions. Many published systems typically use the
same data sets (ACE and MUC) and it is easy to un-
intentionally adapt the system to the corpora and not
just to the problem. This kind of tasks favor com-
parisons between systems with the same framework
and initial conditions.
The obtained performances confirm the robust-
ness of RELAXCOR and a recall improvement. And
the overall performance seems considerably good
taking into account the unprecedented scenario.
However, a deeper error analysis is needed, specially
in the mention detection system with a low precision
and the training data selection process which may
be discarding positive examples that could help im-
proving recall.
Acknowledgments
The research leading to these results has received funding from the
European Community?s Seventh Framework Programme (FP7/2007-
2013) under Grant Agreement number 247762 (FAUST), and from
the Spanish Science and Innovation Ministry, via the KNOW2 project
(TIN2009-14715-C04-04).
38
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC 98, pages 563?
566, Granada, Spain.
S. Bergsma and D. Lin. 2006. Bootstrapping path-based
pronoun resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 33?40. Association
for Computational Linguistics.
Jie Cai and Michael Strube. 2010. Evaluation met-
rics for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28?36, University of
Tokyo, Japan.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Joint Con-
ference on Human Language Technology and Empir-
ical Methods in Natural Language Processing (HLT-
EMNLP 2005, pages 25?32, Vancouver, B.C., Canada.
G.A. Miller. 1995. WordNet: a lexical database for En-
glish.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1?8, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010a. A
Global Relaxation Labeling Approach to Coreference
Resolution. In Proceedings of 23rd International Con-
ference on Computational Linguistics, COLING, Bei-
jing, China, August.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010b. Re-
laxCor: A Global Relaxation Labeling Approach to
Coreference Resolution. In Proceedings of the ACL
Workshop on Semantic Evaluations (SemEval-2010),
Uppsala, Sweden, July.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A
Machine Learning Approach to Coreference Resolu-
tion of Noun Phrases. Computational Linguistics,
27(4):521?544.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Conference
(MUC-6), pages 45?52.
39
