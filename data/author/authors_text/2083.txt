Directional Constraint Evaluation in Optimality Theory* 
J ason  E isner  
Dei)art\]nellt of Computer  Scien(:e / University of Rochester  
l~,ochester, NY  14607-0226 (U.S.A. )  / j ason@cs  . rochester .edu  
Weighted finite-state constraints that can count 1111- 
boundedly many violations make Optimality Theory 
more powerful than tinite-state transduction (Frank 
and Satta, 1998). This result is empirically mM (:om- 
t)ul, a.tionally awkwm'd. We 1)rol)ose replacing l;h(:se 
mlbounded constra in l ;s ,  ;Is well as non-filfite-state 
Gtmeralized Aligmnent constraillts, with a new class 
of finite-state dirc'ctional constraints. We give lin- 
guistic ai)plications , l'esult, s on  generative, power, and 
algorithms to comt)ile grmmna.rs into tl.'allSdllc(Ws. 
1 In t roduct ion  
()l)tinmlity Theory is a gl'aanlnitr framework 
thnt directly ext)resses constraints on 1)honolog- 
ical fbrms. I/,oughly, tim grmnlnm" t)l'et?~rs Ji)rlns 
thnt violate ea(:h constraint as little as I)ossil)le. 
Most (:onstrainl;s used in t)l"a(:tic(~ des(:ril)e 
disfavored local eontigurntions in the l)honolog- 
ieM tbrm (Eisner, 1997; 0. It is thel"etbre l)ossi- 
l)le tbr a, given tbrm to ottimd a single constraint 
at st',vcral locations in the fbrm. (l or example, 
a constraint against syllable codas will l)e ()t'- 
MMed by every syllM)le, that has n (:oda.) 
When eomt)m'ing tbrms, then, how do we ~Lg- 
greg~te ~ tbrm's multil)le lo(:al offenses into a,n 
overall v io la t ion  level? 
A (:onstraint (:ould answer this question ill nt 
least three, wi~ys, the, third being our proposM: 
? Unbounded eva luat ion  (l'rim:e mM 
Smolensky, 1993). A tbrnl's viob~tion le, vel 
is given 1)y the munber of ott'enses. Forms 
with fewer ot\[balses m'e t)reti~rre(1. 
? Bounded eva luat ion  (Frank and Satta, 
1998; Karttmmn~ 1998). A tbrm's viola- 
lion level is lnill(k, nunlber of offenses) fbr 
8Ollle \]i;. This is like mfl)ounded evalu~U;ion 
excet)t l;h~t the COllStrMnt does not; disl;in- 
guish among tbrms wil;h > ti: ofl'enses. 1
? D i rec t iona l  eva luat ion .  A tbrm's vio- 
lation level coxisiders the locntion of of- 
fimses, not their totM nunll)er. Under left -  
* l am grateful to the 3 mmnymous referees tbr tbx,xtback. 
1Nol;e that k = 1 gives "l)inary" constraints that can 
1)e des(:ril)ed siml)ly as languages. Any/,:-tramMed con- 
straint ca.n easily be simulated by k binary constraints. 
to - r ight  ewduation, the constraint pret~rs 
tbrms whose ofl~nses are as late as possible. 
To (:omp;~re two tbrms, it aligns tlmm (ac- 
cording to l;heir (:omnlon underlying repre- 
senl;ation), and scans theln in 1)arMlel from 
M'I; to right, stol)ping at the first loe~tion 
where one form hns ml offense and the ()tiler 
does not ("sudden death"); it; pret~rs the 
1;~tter. R ight - toqef t  evMmttion is similar. 
~2 of this paper gives linguistic and conqmta- 
tional motivation tbr the 1)rol)osal. ?3 tbrmMizes 
the idea trod shows thai; composin 9 a transducer 
'with a directional (:on,straint yields a transducer. 
\[Phus direc, l;ional constraints, like bouuded ones, 
kee t) (YI' within the, (:lass of regular relntions. 
(\]iut we Mso show them to be more exl)ressive. ) 
2 Mot ivat ion  
2.1 In tu i t ions  
RecM1 thnt ()T's constraint rmlking mechanism 
is ml answer to tim question: Ilow (:;m a gl'gd311- 
mar ewthml;e ;t ti)rnl \])y aggreg;d;ing its viola- 
lions of severM COllst;ra,ints'? Above we asked 
the Salne question at a liner some: How cm~ a 
(:onsl, raint evaluate a torm by aggregating its o5 
timses ~tl; several oe~tions? Figure I illustrates 
I;h;~l; Ollr itllSWel: is j l ls l ;  eonstrMnt rm~king l 'e(hlx.  
l)ireetional ewdual;ion sl;riel;ly ranks the im- 
1)ortnnt:e of the locations within ~ tbrm, e.g., 
from left; to right. This exeml)lilies ()T's "do 
only when necessary" strategy: l;he constraint 
preti~rs to postpone oItimses until the, y t)ecome 
strictly necessary toward tile right of the tbrm, 
even at the, cost of having more of them. 
One might l;hink from Figure 1 that each di- 
rect|ohm constraint could be decomposed into 
severnl binary or other bounded constrMnts, 
yielding ;t grmnnmr using only bounded con- 
straints. However, no single such grammar is 
general enough to handle M1 inputs: the nun> 
her of constraints needed tbr the decomposition 
corresponds to the length (i.e., the number of 
locntions) of the mMerlying represelltation. 
257 
ban.to.di.bo 
ban.ton.di.bo 
ban.to.dim.boa 
ban.ton.dim.bon 
(a) II INOCODA 
.1 
g~ ** 
***\[ 
***I* 
? ,. ,,. .......... 
? , ,  ~ . . . . .  
1~,5" * *1  
? * v * u * 
Figure 1: Directional evaluation as subconstraint ranking. All candidates lmve 4 syllables; we simplify here 
by regarding these as the locations. C1 is some high-rm~ked constraint that eliminates ban.to.di.bo; No CoDA 
is oflbnded by syllable codas. (a) %'aditional unbounded evaluation of NoCoDA. (b) Left-to-right evahlation 
of NOCODA, shown as if it were split into 4 constraints evaluating the syllables eparately. (c) I/.ight-to-left. 
2.2 I te rat ive  and f loat ing phenomena 
The main empMcal motiw~tion fbr direction- 
ally evaluated constraints is the existence of "it- 
erative" t)henomena such as metrical footing. 
(Derivational theories described these with pro- 
cedures that scanned a fbrm fi'om one end to 
the other and modified it; see (Johnson, 1972).) 
For most other phenomena, directional con- 
straints are indistinguishable fl'om traditional 
unbounded constraints. Usually, the candidates 
with the tbwest ofiimses are still the ones that 
sm'vive. (Since their competitors offend at ex- 
actly the same locations, and more.) This is 
precisely 1)ecause most phonology is locah sat- 
is(ring a constraint at one location does not usu- 
ally block satisi)ing it at another. 
Distinguishing cases, like the artificial Fig. 
1 where the constraint can only trade offenses 
at one location tbr oflbnses 21; anothm .... arise 
only under special conditions involving non- 
local t)henomena. Just as directional evaluation 
would predict, such a tbrced tr~(teoff is always 
resolved (to our knowledge) by placing offenses 
as late, or as em'ly, as higher constraints allow: 
? Prosodic groupings tbrce each segment or 
syllable 1;o choose which constituent (if 
any) to associate with. So-called left-to- 
right directional syllabification (Mester and 
Padgett, 1994) will syllabit~ /CVCCCV/  
greedily as CVC.CV.CV rather than 
CV.CVC.CV, postponing epenthetic ma- 
terial mltil as late as possible. Simi- 
larly, left-to-right binary tholing (Hayes, 
1995) prefers (aa) (~a)a  over a(acr)(arr)or 
(cra)a(~ra), postponing mffboted syllables. 
? Floating lexical material must surface 
somewhere in the ibrm. Floating features 
(e.g., tone) tend to dock at the let*most 
or rightmost available site, postponing the 
apt)earance of these marked t\[~atures. In- 
fixed morphemes tend to be infixed as little 
as possible (McCarthy and Prince, 1995), 
postI)oning the appearance of an affix edge 
or other aflIix material within the stem. 2 
? Floating non-lexical material must also ap- 
pear somewhere. If a high-ranked con- 
straint, CULMINATIVITY~ requires that a 
primary stress mark appear on each word, 
then a directional constraint against pri- 
mary stress will not only prevent additional 
marks but also trash the single mark to the 
first or last available syllable the tradi- 
tional "End Rule" (Prince, 1983). 
? I larmony must decide how fin" to spread 
features, and OCP ~ccts  such as Grass- 
man's Law must decide which copies of 
a feature to eliminate. Again, directional 
ewfluation seems to capture the facts. 
2.3 Why not  Genera l i zed  A l ignment?  
In OT, ibllowing a remark by Robert Kirch- 
her, it has been traditional to analyze such t)he- 
nolnena using highly non-local Genera l i zed  
A l ignment  (GA) constraints (McCarthy and 
Prince, 1993). For example, left-to-right foot- 
ing is thvored by A LIGN-LEF'ro~(Foot, Stem), 
which requires every ibot to be left-aligned 
with a morphological stem. Not only does 
each misaligned tbot offend the constraint, but 
the seriousness of its offense is given by the 
2 "Available site" and "l)ossible '' amount  of infixation 
are defined here by h igher-ranked constraints.  These 
might restr ict the allowed tone-bear ing units and the 
allowed CV shape aider infixation, lint do not flflly de- 
termine where the floating material will surface. 
A referee asks why codas do not also float (to postpone 
NOCODA offenses). Answer: Flotation requires unusual, 
non-local mechanisnls, gen or a constraint ntltst ellstlre 
that an mlchored tone sequence r sembles the mlderlying 
floating tone sequence, which may be represented on an 
auxiliary intmt tape or (if bounded) as an inlmt pretix. 
But ordinary ihithflflness constraints check only whether 
mMerlying material surfaces locally; they would l)enalize 
coda flotation as a local deletion plus a local insertion. 
258 
mmfl)er ()f syllal)les t)y which i(; is misaligned. 
Tlmse mmfl)ers nre sumnmd over nil otlhnding 
tk'.el; to ol)l;~ili tim violation level. \ ] ' i ) r  (:x~m> 
t)le, \[(r(cro)(o(y)c~(o-o)\],s,t,,,, has 1+3+6=10 vio- 
lations, a,nd \[crcmo(c.ro-)(o~cr)\] st,~m is (;q,,ally 1)ad 
at 4+6=10 violntions. Shifting thor l(;ftward or  
(;limin~fing th(;m r(;du(:cs t;he violation hwel. 
(Sl;(;mt)(,,rgcr, \]99(i) argued l;h;d; CA (:on- 
st;mints were l;()o pow(M'ul. (Ellison, 1995) 
s\]lowc(\] l;h~d; no singh; tinil;(>sl;nt(., ,ml)o,md('A 
constraint (:ould deline the smnc violation lev- 
els as a CA (:onsl;raint. (Eisner, 1997a) showed 
more strongly l;h;tl; sin(:(; GA (:nn t)e ma(te to 
(:(ml;(n: a floatil~g ton(', on ;t t)hras(',, :~ n() hierar- 
ch, y of tinil;(;-st;d;(; md)omMed (:onstrMnts could 
(wen (t(;tin(; the S&ilI(} optimal candidatc,s a.s a 
GA (:onstr;dnt. Thus GA (:retool; l)e simulated 
i,, (1994) ({}a. 
For this reason, as w(;ll as the awkwm'(hmss 
~md non-lo(:Mil;y of (:wdu;~ting CA otl'(ms(;s, w(; 
t)rol)ose t() r(q)bme (-IA with (tir(~(:tiotml c(m- 
straints. \])ir(,x:l;ionM (:onsl;ra.inl;s apt)ear l;() m()re 
dir(~(:tly (:;q)l;ur(: the ol)serv('xl t)h(mom(m:~. 
We (to nol;e th;d; m,)th(n', l;ri(:lder t)ossibility is 
to eliminalx; CA ill favor of ()l'(li~t;u'y '?mbo'und,d 
(:onsl;rainl;s thai; are in(tifl'('a'(ml: to th(', h)(:ation 
of oflimses. (Ellison, \]99d) noted that GA con- 
straint;s that (wahmte(t the 1)ln(:(mmnt of only 
one (:l(;m(mt (e.g., I)rinmry stress) could 1)(: r(> 
1)l:med by simt)hn: N() \ ]NTEl lVENING (;OliSi;\]';tinLs. 
(\]'\]isn(;r, ii 9971)) gives ;~ (IA-fr(',('. tr(~a.tm(',nt ()f th(', 
metri(:al sl;r(,,ss tyl)()l()gy of (llayes, 1995). 
2.4 Generat ive  power  
li|; has r(~(:ently t)(xm 1)ropos(M th:g; tbr (:Oml)U- 
tal, ionM r(',a,sons, (Yl' should (;liminnt(; not only 
GA t)ut all unbomMed constraints (Frnnk ~tlltl 
Satt~, 1998; Karttunen,  1998). As with GA, 
we oflbr the less extreme npl)roach of ret)lacing 
them with dircctionnl (:onsl;rainl;s insi;(;n(1. 
l/,c(:all that a t)honological grammar,  as usu- 
ally (:onc(',ive(t, is a (tescrit)tion of t)(',rmissil)l(', 
CUll,, SR) 1)Mrs. 'I It has long 1)e(m 1)eli(w(M 
t;h;h; natural ly o(:curring t)honologicnl grammnrs 
are r(:.qular relations (Johnson, 1972; Kal)lm~ 
and Kay, 1.994). This 111(2~1.118 |;hat they can 
1)e implemented as f in i te -s ta te  t ransducers  
(FSTs)  that accel)t exactly the granlmatical 
pairs. FSTs are immensely useflfl in l)ertbrm- 
3This is inde('xl too powerful: cent(Mng is ulmtt(~sl;cd. 
dUR = un(hMying rel)rcscntation , SIt = surta(:(; relm. 
ing mmly rt;h;vm~l; tasks r~q)idly: g(;ner~d;ion (Oil- 
raining all possible S\]{.s tSr a UI{.), COml)rehcn- 
sion (conversely), chm'act(Mzing I;\]m sol; of fin'ms 
on wlfi(:h two grmmnars (pertmI)S from diflk~rent 
descriI)tiv(', fi:am(,,works) would difl'er, etc. More- 
over, FS'i~s can 1)e al)t)lied in parallel to regular 
sets of tbrms. For example, one cml obtMn n 
w(fighted set of l)ossible Sl{.s (a l)hon(mm lat- 
l;i(:(~) ti'OlU & s\])(}(;(:\]l rtx:ognizer, pass it; through 
the invers(; l;rmlsdu(:(;r, int(,xsect the r(;sulting 
weight(~d set of U\]/.s with the lexicon, and I;hen 
(;xi;ra(:t th(; \])(;st surviving U1Rs. 
(Ellison, 199d; Eisner, 1997a) frmne ()T 
wit\]fin this tradition, t)y modeling Gen mM the 
(:onsl;ra.illtS :ts w(;ig\]lt(;d tinit(>sl;~t(; m~mhin(;s 
(see {}3.2). Bui; all;hough thos(; t)ai)ers showed 
hOW I;O gt}ll(}l';I,LO, I;ll(} S(}l; of  SI{,s t()l; ;L single given 
UI{., t\]my did not (:Oml)ih~ tim OT gr;umnar into 
;ul FST, or ol)tnin the other l)(mcfits th('d'(~of. 
In fact, (Frank and Satta, 11!)98) showed 
Idmt such (:onq)il~ti;ion is iml)ossil)h', in tlm g(m- 
er;tl (:;is(', ()\[ un\])otln(t(}(t :onsl;raints. 'l.i) see 
why, consi(hn: the grammar MAX, \])EI)~ ltAR- 
M()NY\[h(~ight,\] >> ll)l,',NT-l()\[h(;ight\]. This gra.m- 
mar insists on ll(:ight lmrmony ~m~()ng sm:li~(:(; 
vowels, but (lislik(;s (:h;mg(;s fl:om tim UR. The 
result is tlm ml~-g3;(}sl;e(t 1)h(momelion of "ma- 
jority nssimilntion" (l{akovi(:, 1999; Lomt)~r(ti, 
1999): a UR with more high vowels thml low 
will surt'a(:(; with all vowels high, ;uM vi(:(>v(n'sa. 
So ()T may comp~u'c 'unbo'u',,dcd count.s" in a. w~ty 
th;tt ;m \]i'S3' (:mmot a.n(l phonology (loes not. 
This suggests that ()T with unl)ounded (:on- 
stmints is i;oo l)owcrlul, lhmce (Fr;mk mM 
S~t;I;~L, 1998; Km:t tmmn,  1998) l)rOl)OS(: using 
only 1)(mnde(t (:onstrainl;s. q'hcy show this re- 
du(:es OT's  l)owcr to finite-state transduction. 
The worry is l;h~t 1)oundcd constraints italy 
1101; b(', (;xI)rcssiv(; enough,. A 2-bounded version 
of NOCODA would not distinguish among the 
tinM thre(; tbrms in Figure \]: it is agnostic wh(m 
l;lm intmt tbr(:es multii)l(,, codas in all c;mdidat(~s. 
To t)e sure, ~/~:-l)oun(h~d approximation may 
work well fi)r large t~:. 5 l lut its automaton (!}3.2) 
will tyl)ically h~we k times as m~my st;td;es ~s I;he 
mlb(mnded originM, since it mlrolls loops: the 
5Using the al)t)roximat(: grmnlnar tbr generation, an 
out,put is guarant(:(:d corrc(:t unless it achieves h vio- 
lations for some k-l)oundcd (:onstraint. One can t;hen 
raise k, recoml)ile tlw. grammar, and try again. But 1,: 
may grow (tuil;c large for long inputs like phonoh)gical 
1)hrases. 
259 
state must keep track of the offense count. In- 
tersecting many such large constraints cast pro- 
duct very large FSTs--whi le still failing to cap- 
tare simple generalizations, e.g., that all codas 
are dispreferred. 
In ?3, we will show that directional con- 
straints are more powerful than bounded 
constraints, as they can express such 
generalizations--yet hey keel) us within 
the world of regular relations and FSTs. 
2.5 Re la ted  Work  
Walther (1999), working with intersective con- 
straints, defines a similar notion of Bounded 
Local  Opt imizat ion  (BLO) .  Tronnner (1998; 
1999) applies a variant of Walther's idea to OT. 
The motivation in both eases is linguistic. 
We sketch how our idea differs via 3 examples: 
UR uuuuu uu uuu uuuuu 
candidate X vvvbb vv vbb vvvbb 
candidate Y vvbaa vvvvbaa vzbaa 
Consider *b, a left-to-right constraint hat is o f  
fended by each instance of b. On our proposal, 
candidate X wins in each column, because Y 
always offends ,b first, at position 3 in the UR. 
But under BLO, this offense is not fatal. Y 
can survive *b by inserting epenthetic material 
(colunm 2: Y wins by postt)oning b relative to 
tits SR), or by changing v to z (cohmm 3: Y ties 
X, since vv ? vz and BLO merely requires the 
cheapest choice 9iven the sur:face output so far). 
In the same way, NoCoDA under BLO would 
trigger many changes unrelated to codas. Our 
definition avoids these apparent inconveniences. 
Walther and Trommer do not consider the ex- 
pressive power of BLO (cf. ga.3) or whether 
grammars can be compiled into UR-to-SR FSTs 
(our main result; see discussion in ?3.4). 
3 Formal  Resu l ts  
3.1 Def in i t ion  o f  OT  
An OT grammar  is a pair (Gen, C) where 
? the cand idate  generator  Gen is a relation 
that maps eaeh input  to a nonempty set of 
candidate outputs ;  
? the h ie rarchy  C = (C1, C2,. . .)  is a finite 
tnple of const ra in t  functions that evaluate 
outputs. 
We write d(5) for the tuple (C~(5), C2(5),. . .) .  
Given a UR, or, as input, the grammar adnfits 
as its SRs all the outtmts 5 such that C(5) is lex- 
ieographicalty minimal in {C(5) : 5 ~ Gen(~)}. 
The values taken by 6'/ are called its viola- 
t ion levels. Conventionally these are natural 
mnnbers, trot any ordered set will do. 
Our d i rect iona l  const ra in ts  require the fol- 
lowing immvations. Each input a is a string as 
usual, but the outputs are not strings. Rather, 
each candidate 5 C Gen(cr) is a tuple of I~l + 1 
strings. We write 5 for the concatenation of 
these strings (the "real" SR). So 5 specifies an 
aliflnmcnt of 5 with a. The directional con- 
straint Ci maps the tuple 5 to a tuple of nat- 
ural numbers ("offense levels") also of length 
I~1 + \]. Its violation levels {6~(5) : 5 < Gen(~)} 
are compared lexicographically. 
3.2 F in i te -s ta te  assumpt ions  
We now confine our attention to tinite-state OT 
grammars, following (Ellison, 1994; Tesar, 1995; 
Eisner, 1997a; Frank and Satta, 1998; Kart- 
tunen, 1998). Gen C_ E* x A* is a regular 
relation, ~ and may be implemented as an uu- 
weighted FST. Each constraint is implemented 7 
as a possibly nondeterministic, weighted finite- 
state automaton (WFSA)  that accepts A* and 
whose ares are weighted with natural nulnbers. 
An FST, T, is a tinite-state automaton in 
which each arc is labeled with a string pair (t : 3'- 
Without loss of generality, we require \[eel < 1. 
This lets us define an a l igned t ransduct ion  
that maps strings to tuples: If ~r = al . . .a~, 
we define T(a) as the set of (n + 1)-tui)les 
5 = (50, 31,... 5n) such that T has a path trails- 
ducing a : g along which 50""  5i-1 is the con> 
plete output before ai is read fronl the input. 
We now describe how to evaluate C(d) where 
C is a WFSA. Consider the path in C that ac- 
cepts a. 8 In (nn)bounded evaluation, C(5) is 
the total weight of this path. In left>to-right 
evaluation, C(5) is the n + 1 tuI)le giving the re- 
spective total weights of the subpaths that con- 
sume a0,.. ,  at. In right-to-left evaluation, C(5) 
is the reverse of the previous tuple. "~ 
GEllison required only that Gen(c,) be regular (Vcr). 
rSpace prevents giving the equivalent characteriza- 
tion as a locally weighted language (Walther, 1999). 
Slf there art multiple accepting paths (nondetermin- 
ism), take the one that gives the least vahm of C(5). 
9This is equivalent to CR(5.~t,...,~) where ~ de- 
notes reversal of the automaton or string as apt)ropriate. 
260 
3.3 Express ive power  
Thanks to Gen, finite-state ()T can l;rivially iln- 
1)lement any regular inl)ut-outtmt relation with 
no coustrmnts at all! And {i3.4 below shows that 
whether we allow directional or houri(led con- 
straints does not affect this generative power. 
But in another sense, directional constraints 
are strictly more expressive than bounded ones. 
If Gen is fixed, then any hierarchy of hemmed 
constrMnts can be simulated by some hierarchy 
of directionM constraints 1? -but not vice-versa. 
Indeed, we show even more strongly that di- 
rectional constraild;S cannot always be simu- 
lated even by mflmmMed constraints. 11 \])trine 
? b as in ?2.5. This ranks the set (alb) '~ in lexico- 
graphic order, so it; makes 2u distinctions. Let 
Gen be the regular relation 
(a :(,,Ib:b)*(c:,((,,:-lb: b)* I (,:t,(-:(,F,: hi,,: t, lb:-)*) 
We claim that the grammar (Gen,*b) is not 
equivalent to (Gen, C1,. . . ,  C~) tTor any bounded 
or mfl)ounded constraints C~,... C.~. There is 
some k; such l,hat tbr all d 5 A", each Ci(5) < 
t~:'n. 12 So candidates 5 of length n have at most 
(h,n,) 's ditt'erenl; violation profiles (~(5). Choos, n 
such that 2 '~ > (k'n) ~. Then the set of 2 ~ strings 
(alb) n must contain two distinct strings, 6 = 
: , . , . . . : , : , ,  a' = > . - . . v , , , ,  w i l l ,  = 
Let i be nfinima,1 such that xi ~ ?,Ji, all(l with- 
oul; loss of generality ~ssume xi = o,, yi = b. Put 
O- = J;1 " ' 'Z i - - lC :C i+l  "'':g~t" Now 5, 51 C Gen(o)  
and 5 is lexicographicMly minimM in Gen(c,). 
So the granunar (Gen,*b) 1naps cr to 5 ouly,  
whe, reas (Gen, C) emmet distinguish between 5 
and (\[', st) it; maps cr to neither or both. 
3.4 Grammar  compi la t ion :  OT  ---- FST 
It is triviM to translate an arbitrary FST gram- 
mar into ()T: let Gen be the FST, aim C = (). 
The rest of this section shows, conversely~ how 
to compile a tinite-state OT grammar (Gen, C) 
into an FST, provided that the grammar uses 
only bounded and/or directional constraints. 
1?How? By using states to count, a bounded COil- 
straint's WIPSA can bc transtbrmed sothat all the weight 
of each path falls on its final arc. This defines the same 
optimal candidates, even when interpreted irectionally. 
,1Nor vice-versa, since only unlmunded constraints can 
implement non-regular relations (~2.4,{i3.4). 
12 Apply !i3.4.4 to elinfinate any e's froIn the constraint 
WDFAs (regarded as outlmtless transducers), then take 
1,: to exceed all arc weights in the result. 
3.4.1 The outer  loop of  compi la t ion  
Let 5/~ = Gen. For i > 0, we will construct 
an FST 77,/ that iml)lements the i)artial grmn- 
mar (Gen, C1, C2,... Ci). We construct Ti from 
T i_  1 al ld  C i only: Ti('/;) col t ta i l ls  the  forll lS 
y E Ti_l(X) tot" whieh G(Y) is minimal. 
If C i is L;-txmnded, we use the construction of 
(Frank and Satta, 1998; Karttulmn, 1998). 
If Ci is a left-to-right constraint, we compose 
Ti-1 with the WFSA that l'epresents Ci, obtain- 
ing a weigh, ted finite-state transducer (WFST) ,  
Ti ? This transducer may be regarded as assign- 
ing a Ci-violation level (an (1~1 + 1)-tuple) to 
each cr : (~ it accepts. We must now 1)rulm away 
the subol)timM candidates: using the DBP al- 
gorithm below, we construct a new unweighted 
FST 7) that transduces a : ~ ill" the weighted 9~ 
can transduce (r : 5 as cheaply as any a : 5 ~. 
If Ci is right-to-left;, we do just the same, ex- 
cept DBP is used to construct; T/t ti'om 7)\]". 
3.4.2 Di rect ional  Best  Paths" The idea 
All that remains is to give the construction of 
Ti from 7~i, which we call D i rec t iona l  Best  
Paths  (DBP) .  Recall standard bestq)aths or 
shortest-t)aths algorithms that pare a WFSA 
d(}wn to its 1)aths of minimmn total weight (Di- 
jkstra, 1959; Ellison, 1994). Our greedier ver- 
sion (toes llot SllUl along Imths trot always im- 
me(liately takes the lightest "availal)le" at('. 
Cru{:iMly, available ar{:s are define{t r(;lativc to 
the int)ut string, l)ecause we must retain one or 
more ot)timal output candidates for each inlmt. 
So availal}ility requires "lookahead": we must 
take a heavier are (b :z  beh)w) just when the 
rest; of the intmt (e.g., abd) emmet otherwise be 
ac{:et}ted on any t)ath. ~ c:c _ 
,,:,, 2 2 
Ti(abd) = {a:,'c, vc} k~_~.  c ~__~ 
i (ab ) = 
(a,c ,fl)bIev,att,s (e, a,, c)) su~mptimal ~ )  
On this example, DBP would simply make state 
6 non-tinal (tbrcing abe to take the light are un- 
availal)le to abd), but often it; must add states! 
This relativization is what lets us compile a 
hierarchy of directionM constraints, once and tbr 
all, into an single Fsq_' that can find the optimal 
OUtl)ut for aTzy of the infinitely many t)ossible in- 
puts. We saw in ?2.4 why this is so desirable. By 
261 
contrast, Ell ison's (1994) best-paths construc- 
tion tbr unbounded constraints, and previously 
proposed constructions tbr directional-style con- 
straints (set ?2.5) only find the optimal outt)ut 
for a single input, or at best a finite lexicon. 
a.4.a Dir .  Best  Paths :  A spec ia l  case  
?3.2 restricted our FSTs such that for every arc 
label o~ : 7, I ~t\] -< 1. In this section we construct 
^ 
~) ti'om Ti under the stronger assumption that 
% 
Ioe\[= 1, i.e., ~i is e-flee on the intmt side. 
If Q is the stateset of Ti, then let; the stateset 
of be S\]: c_ S c_ 0,  q c- S -  
This has size IQ\[" 31QI-*. However, most of 
these states are typicMly unreachable from the 
start state. Lazy "on-the-fly" construct ion tech- 
niques (Mohri, 1997) can be used to avoid allo- 
cating states or arcs until they arc discovered 
during exploration from the start State. 
For a E E*,q G Q, define V(G,q) as the 
minimmn cost (~ \[al-tut)le of weights) of' any 
^ 
or-reading 1)ath from Ti's start state q0 to q. 
The start state ot'fl) is \[q0; 0; {q0}\]. The intent 
is that Ti have a path from its start state to 
\[q; R.; S\] that  transduces cr :5  \]a itf 
? Ti has a q0 to q, o':a path of cost V(er, q); 
? = {q 'c  Q :  < 
? s = {4  c O :  <_ vO, q)}. 
So as Ti reads c,, it "Ibllows" Ti. cheapc.st cy- 
reading paths to q, while calculating R, to which 
yet cheaper (but l)erhaI)S dead-end) paths exist. 
Let \[q; R; S\] be a final state (in Ti) itf q is final 
and no q' E R is final (in 5~?i). So an accepting 
path in ~) survives into Ti ifl' there is no lower- 
cost accepting path in Ti for the same int)ut. 
The arcs fl'om \[q;R;S\]  correspond to arcs 
from q. For each arc fl'om q to q' labeled a : -y 
and with weight W, add an unweighted a : 7 
arc from \[q;R; S\] to \[q'; R'; S'\], provided that 
the latter state exists (i.e., unless q' E R', indi- 
cating that there is a cheaper path to q'). Here 
R'  is the set of states that art  either reachable 
from R by a (single) a-reading arc, or reachable 
from S by an a-reading arc of weight < W. S t 
is the union of R' and all states reachat)le from 
S by an a-reading arc of weight W. 
3.4.4 Dir .  Best  Paths :  The  genera l  case 
To apply the above construction, we nnlsl; firsl; 
transtbrm Ti so it is e-flee on the int)ut side. Of 
laa is a tuple of \]~r\]+l strings, but 50 = e by e-fl'eeuess. 
course int)ut c's are cruciM if Gen is to be allowed 
to insert unbounded alilOllllt8 of surface mate- 
rim (to be pruned back by the constraints). 14 
To eliminate e's while preserving these seman- 
tics, we are tbrced to introduce FST  arc labels 
of the tbrm a : F where F is actual ly a regular 
set of strings, represented as an FSA or regu- 
lar expression. Following e-elimination, we can 
apply the construct ion of ?3.4.3 to get Ti, and 
finMly convert Ti back to a normal transducer 
by expanding each a :F  into a subgraph. 
When we eli lninate an arc labeled c. : 7, we 
must Imsh 7 and the arcs  weight back onto 
a previous non< arc (but no further; contrast 
(Mohri, 1997)). The resulting machine will ira- 
plement the same Migncd transduct ion as ~ 1)ut 
more transparently:  in the notat ion of .~3.2, the 
arc reading ai will t ransduce it directly to 5i.1.5 
Concretely, suppose G~ can gel; from state q 
to q" via a t)ath of total weight W that 1)egins 
with a : 7~ on its first arc followed 1)y e : "T2~ 
e : 7a, . . .  on its remaining arcs. \?e would like 
to subst i tute an arc from q to q" with label 
a : 7172Ta-.. and weight I/V. But there may 
be infinitely many such q q" t)~ths, of varying 
weight, so we actual ly write a : F, where \]? de- 
scribes .just those q-q" paths with minimmn W. 
The exact procedure is as follows. Let G be 
the possibly discommcted subgraph of 5hi \]brined 
by e-reading arcs. Run ml nil-pairs shortest- 
paths algorithm Is on G. This finds, for each 
state pair (qt, q,) connected by an c-readiug 
path, the subgral)h Gq,,q,, of G formed by the 
minimmn-weight e-reading t)aths froln q' to q", 
as well as the common weight Wq,,q,, of these 
paths. So tbr each arc in 2Pi from q to q', with 
weig~ht W and label a : 7, we now add an arc 
to Ti from q to q" with weight W + l/Vq, q,, and 
label a : 7Gq,,q,,(e). (G(e) denotes the regular 
language to which G transduces e.) Having done 
this, we can delete all e-reading arcs. 
The modified e-free ~) is equivalent to 
14As is conventional. Besides epenthetic material, Gen 
often introduces COlfiOUS prosodic structure. 
lSThat arc is labeled ai : P where & E F. But what is 
ao? A special symbol E E E that we introduce so that 
5o can be pushed back onto it: Before e-dimination, we 
modify Ti by giving it a new start state, commcted to 
the old start state with an arc E : e. After e-elimination, 
we apply DBP and replace E with e in the result Ti. 
lS(Cormen et al, 1990) cites several, including fast 
algorithms for when edge weights are small integers. 
262 
tlm origim~l (;xcet)l; for ('Jilnim~ting some 
of tim sul)ol)timal sul)l);tths. H('a'c is a 
gri~l)h ti'agment heft)r(; and after c-climim~tion: 
iI:iI ~(2~') 
a :b 
Note: Right-to-left; evaluation applies \])BP t;o 
~ so consistency with our t)revious definitions 
mGms it lllltSt; trash c.'s forward, not backward. 
4 Conc lus ions  
This t)atmr has 1)rol)oscd anc, w notion in OT: 
"dirccl;ional ev;fluation," where UlM(;rlying loca- 
tions ~Lr(', sl;rictly rmlk('xt t)y I, hcir ilnporl,mm(;. 
Tra(lil;ionld finitc-stnt(; OT constraints lu~vc 
enough power to compare arbitrari ly \]figh 
(:(rants; Oen('.ralizcd Al igmncnt is even worse. 
Dirct,l;ion~fl constraints SC(}lll I;O cD~\])|;llr( ~, |;\]1(I p ros  
of these consla'ainl;s: |;hey al)prot)riately mili- 
l;~tl;e a,g~finsl; cvc, ry instan(:(', of a disfavored con- 
figured;ion in a (:andid~l;c forln, no ln~d;l;cr how 
many, and they natural ly (:at)t;ur(', il;erativc and 
(',(lgt;most ellbx:ts. Y(;t they do not hnve the ex- 
(:(',ss power: w(', ha,vc, showl~ l;ha, l; iH~ g~r}~liml{rr o\[' 
(tirc(:tional and/or 1)omMed (:onstrainl;s can 1)e 
COml)iled into n tinitc-sl;~m', t\]'~msduc(;r. '.l?h;~t is 
both empirically and ('Oml)ut~tion~dly dcsir;fl)h;. 
The most ohvious ful;m'e work comes t'rOll l  lin- 
guistics. Can dir(;cl;ionnl constraints do MI l;h('. 
work of mfl)ounded and CA cons\]mints? Itow 
do they change the style, of mlalysis'? (E.g., di- 
rectional versions of markedness consl;rai\]lts pin 
down the locations of mm:kcd ol)jc,('ts, leaving 
lower-ranked constr~fints no s~G.) \]?in~flly, direc- 
tional (:onstrainl;s ca, n t)c, vm'iously formul~t('xt 
(is *CLUSTEll, offended at tim start; or (rod of 
each duster? or of its enclosing sy\]\]{fl)le?). St) 
what conventions or restrictions shoukt apply? 
References  
Eric Bakovid. 1999. Assimilation to the unmarked. 
Ms., Rutgers Optimality Archive ROA-340. 
T. H. Cormen, C. E. Lciserson, and 11. L. Rivest. 
1990. Introduction to Algorith, ms. MIT Press. 
Edsger W. l)ijkstra. 1959. A note on two problems 
in colmexion with grat)hs. Numcrisch, e Math.c- 
matik, 1:269 27\]. 
Jason \]t;isner. \]997a. Efficient gener&tion i  t)rimi- 
l,iv(; O\])l;imality Titeory. hi I~'oc. of the ,TDth, An- 
wual ACL and 8th, EAUL, Madrid, July, 31.3 320. 
Jason Eisner. 1997b. FooTlPOltM decomposed: Us- 
ing primitive constraints in QT. In Benjmnin Bru- 
ening, editor, Pro< of SCIL VIII, MIT Working 
Papers in Linguistics 31, Cambridge, MA. 
T. Mark Ell\]son. 1994. Phonological d('aivation in 
optimality tlmory. In Proc. of COLING. 
~\]'. Mark Ell\]son. 1995. ()T, tin\]\]e-state repres('ma- 
tions and l)rocedurality. In Proc. of fit,(: Col@r- 
e'nee on I, brmal (trammar, Barcelona. 
Robert Frank and Giorgio Sa.tta. 1998. ()ptimal- 
ity Theory a.nd the generative complexity of con- 
straint violability. Comp. Ling., 24(2):307 315. 
Bruce ttayes. 1.995. Metrical Stress Theory: Princi- 
ples and Uasc Studies. U. of Chicago Press. 
C. Douglas Johnson. 1972. Formal Aspects of 
Phonological Description. Mouton. 
l{olmhl M. Kaplan mid Martin Kay. 1994. Regular 
models of lflmnological rule systems. Computa- 
l, ioual IAngv, istics, 20(3):331 378. 
l,mn'i Karttunen. 1998. The prol)cr treatment of 
optilmdity in COmlmta.tiona\] lfllonology. In P?'oc. 
ofFSMNLP'98, 1 12, Bilkent I.-., Ankara, Turkey. 
lfinda Lombard\]. 1999. Positional faithflflness and 
voicing assimilation in ()ptimality Theory. Nat',,- 
ral La',,g'uage and Linguistic Theory, 17:267 302. 
.Iohn McCarthy and Alan Prince. 1993. Generalized 
aligmnent. In G(!(;rt Booij and ,Iaap van Math',, 
c'ditors, D:a'rbook of Morph.ology, 79 \] 53. Kluwer. 
John McCarthy and Alan Prince.. 1995. Ntithfltlncss 
and reduplicative identity. In Jill Beckman ct al., 
editor, Papers in Optimality ~JT~,c.ory, 259 384. U. 
of Massachusetts, Amherst: GLSA. 
Armin Mester mid ,laS,e \]~adgc'tt. \]!)94. \])irectional 
syllalfification i Genca'alized Aligmnent. Phonol- 
ogy ~t|; Sa l l | ;a  Cr l lZ  3, ()(:t;ot)o.r. 
Mehryar Mohri. 1997. Finite-state tamlsducers in 
language & speech processing. Comp. Lin 9. 23(2). 
Ala.n Prince and l?mfl Smolensky. 1993. Optimal- 
it;.3, Theory: Consl;raint interaction in generative 
g;ralnnlar. \]k/Is., l{utgers U. and U. of Colorado. 
Alan Prince. 1983. Relating to the, grid. Linguistic 
Inq',.iry, 14:\].9 100. 
3. P. Stemberger. \]996. The. scope of the theory: 
Where does beyond lie? In L. MoNa\]r, K. Singer, 
L. M. Dobrin, and M. M. Auto\]n, eds. Papc~w f i rm 
the Parascssion on ~17~,cory and Data in Linguis- 
tics, CLS 23, \]39 -164. Chicago Linguistic Society. 
Bruce Tesar. 1995. Computational Optimality The- 
ory. Ph.D. thesis, U. of Colorado, Boulder. 
Jochen Trommer. 1998. ()primal morphology. In 
T. Mark Ell\]son, editor, Proc. of the /tth, ACL 
SIGPI ION Worksh, op, Quebec, July. 
Jochen Trommer. 1999. Mende ton(; lmtterns revis- 
ited: Tone mal)t)ing its local constraint evaluation. 
In Linguistics in Potsdam Working Papc~w. 
Markus Walthe.r. 1999. One-level prosodic morphol- 
ogy. Arl)eiten zur Linguistik 1. U. of Mart)urg. 
263 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 667?677, Prague, June 2007. c?2007 Association for Computational Linguistics
Bootstrapping Feature-Rich Dependency Parsers with Entropic Priors
David A. Smith and Jason Eisner
Department of Computer Science
Johns Hopkins University
Balitmore, MD 21218, USA
{dasmith,eisner}@jhu.edu
Abstract
One may need to build a statistical parser for a new language,
using only a very small labeled treebank together with raw
text. We argue that bootstrapping a parser is most promising
when the model uses a rich set of redundant features, as in re-
cent models for scoring dependency parses (McDonald et al,
2005). Drawing on Abney?s (2004) analysis of the Yarowsky
algorithm, we perform bootstrapping by entropy regulariza-
tion: we maximize a linear combination of conditional likeli-
hood on labeled data and confidence (negative Re?nyi entropy)
on unlabeled data. In initial experiments, this surpassed EM
for training a simple feature-poor generative model, and also
improved the performance of a feature-rich, conditionally esti-
mated model where EM could not easily have been applied. For
our models and training sets, more peaked measures of con-
fidence, measured by Re?nyi entropy, outperformed smoother
ones. We discuss how our feature set could be extended with
cross-lingual or cross-domain features, to incorporate knowl-
edge from parallel or comparable corpora during bootstrapping.
1 Motivation
In this paper, we address the problem of bootstrap-
ping new statistical parsers for new languages, gen-
res, or domains.
Why is this problem important? Many applica-
tions of multilingual NLP require parsing in order
to extract information, opinions, and answers from
text, and to produce improved translations. Yet
an adequate labeled training corpus?a large tree-
bank of manually constructed parse trees of typi-
cal sentences?is rarely available and would be pro-
hibitively expensive to develop.
We show how it is possible to train instead from
a small hand-labeled treebank in the target domain,
together with a large unannotated collection of in-
domain sentences. Additional resources such as
parsers for other domains or languages can be in-
tegrated naturally.
Dependency parsing is important as a key com-
ponent in leading systems for information extrac-
tion (Weischedel, 2004)1 and question answering
(Peng et al, 2005). These systems rely on edges
or paths in dependency parse trees to define their ex-
traction patterns and classification features. Parsing
is also key to the latest advances in machine transla-
tion, which translate syntactic phrases (Galley et al,
2006; Marcu et al, 2006; Cowan et al, 2006).
2 Our Approach
Our approach rests on three observations:
? Recent ?feature-based? parsing models are an
excellent fit for bootstrapping, because the
parse is often overdetermined by many redun-
dant features.
? The feature-based framework is flexible
enough to incorporate other sources of guid-
ance during training or testing?such as the
knowledge contained in a parser for another
language or domain.
? Maximizing a combination of likelihood on la-
beled data and confidence on unlabeled data is
a principled approach to bootstrapping.
2.1 Feature-Based Parsing
McDonald et al (2005) introduced a simple, flexi-
ble framework for scoring dependency parses. Each
directed edge e in the dependency tree is described
with a high-dimensional feature vector f(e). The
edge?s score is the dot product f(e) ? ?, where ? is a
learned weight vector. The overall score of a depen-
dency tree is the sum of the scores of all edges in the
tree.
1Ralph Weischedel (p.c.) reports that this system?s perfor-
mance degrades considerably when only phrase chunking is
available rather than full parsing.
667
Given an n-word input sentence, the parser begins
by scoring each of the O(n2) possible edges, and
then seeks the highest-scoring legal dependency tree
formed by any n? 1 of these edges, using an O(n3)
dynamic programming algorithm (Eisner, 1996) for
projective trees. For non-projective parsing, O(n3),
or with some trickery O(n2), greedy algorithms ex-
ist (Chu and Liu, 1965; Edmonds, 1967; Gabow et
al., 1986).
The feature function f may pay attention to many
properties of the directed edge e. Of course, features
may consider the parent and child words connected
by e, and their parts of speech.2 But some features
used by McDonald et al (2005) also consider the
parts of speech of words adjacent to the parent and
child, or between the parent and child, as well as the
number of words between the parent and child. In
general, these features are not available in a genera-
tive model such as a PCFG.
Although feature-based models are often trained
purely discriminatively, we will see in ?2.6 how to
train them to model conditional probabilities.
2.2 Feature-Based Parsing and Bootstrapping
The above parsing model is robust, thanks to its
many features. On the Penn Treebank WSJ sections
02?21, for example, McDonald?s parser extracts 5.5
million feature types from supervised edges alone,
with about 120 feature tokens firing per edge. The
highest-scoring parse tree represents a consensus
among all features on all prospective edges. Even if
a prospective edge has some discouraging features
(i.e., with negative or zero weights), it may still have
a relatively high score thanks to its other features.
Furthermore, even if the edge has a low total score,
it may still appear in the consensus parse if the al-
ternatives are even worse or are incompatible with
other high-scoring edges.
Put another way, the parser is not able to include
high-scoring features or edges independently of one
another. Selecting a good feature means accepting
all other features on that edge. It also means reject-
ing various other edges, because of the global con-
straints that a legal parse tree must give each word
only one parent and must be free of cycles and, in
2Note that since we are not trying to predict parts of speech,
we treat the output of one or more automatic taggers as yet more
inputs to edge feature functions.
the projective case, crossings.
Our observation is that this situation is ideal for
so-called ?bootstrapping,? ?co-training,? or ?min-
imally supervised? learning methods (Yarowsky,
1995; Blum and Mitchell, 1998; Yarowsky and Wi-
centowski, 2000). Such methods should thrive when
the right answer is overdetermined owing to redun-
dant features and/or global constraints.
Concretely, suppose we start by training a super-
vised parser on only 100 examples, using some reg-
ularization method to prevent overfitting to this set.
While many features might truly be relevant to the
task, only a few appear often enough in this small
training set to acquire significantly positive or nega-
tive weights.
Even this lightly trained parser may be quite sure
of itself on some test sentences in a large unanno-
tated corpus, when one parse scores far higher than
all others. More generally, the parser may be sure
about part of a sentence: it may be certain that a par-
ticular edge is present (or absent), because that edge
tends to be present (or absent) in all high-scoring
parses.
Retraining the feature weights ? on these high-
confidence edges can learn about additional features
that are correlated with an edge?s success or failure.
For example, it may now learn strong weights for
lexically specific features that were never observed
in the supervised training set. The retrained parser
may now be able to confidently parse even more of
the unannotated examples; so we can iterate the pro-
cess.
Our hope is that the model identifies new good
and bad edges at each step, and does so correctly.
The more features and global constraints the model
has,
? the more power it will have to discriminate
among edges even when ? is insufficiently
trained. (Some feature weights may be too
weak (i.e., too close to zero) because the initial
labeled set is small.)
? the more robust it will be against errors even
when ? is incorrectly trained. (Some feature
weights may be too strong or have the wrong
sign, because of overfitting or mistaken parses
during bootstrapping.)
668
In the former case, strong features lend their strength
to weak ones. In the latter case, a conflict among
strong features weakens the ones that depart from
the consensus, or discounts the example sentence if
there is no consensus.
Previous work on parser bootstrapping has not
been able to exploit this redundancy among features,
because it has used PCFG-like models with far fewer
features (Steedman et al, 2003).
2.3 Adaptation and Projection via Features
The previous section assumed that we had a small
supervised treebank in the target language and do-
main (plus a large unsupervised corpus). We now
consider other, more dubious, knowledge sources
that might supplement or replace this small tree-
bank. In each case, we can use these knowledge
sources to derive features that may?or may not?
prove trustworthy during bootstrapping.
Parses from a different domain. One might have
a treebank for a different domain or genre of the tar-
get language.
One could simply include these trees in the ini-
tial supervised training, and hope that bootstrapping
corrects any learned weights that are inappropriate
to the target domain, as discussed above. In fact,
McClosky et al (2006) found a similar technique to
be effective?though only in a model with a large
feature space (?PCFG + reranking?), as we would
predict.
However, another approach is to train a separate
out-of-domain parser, and use this to generate addi-
tional features on the supervised and unsupervised
in-domain data (Blitzer et al, 2006). Bootstrapping
now teaches us where to trust the out-of-domain
parser. If our basic model has 100 features, we could
add features 101 through 200, where for example
f123(e) = f23 ? log P?r(e) and P?r(e) is the poste-
rior edge probability according to the out-of-domain
parser. Learning that this feature has a high weight
means learning to trust the out-of-domain parser?s
decision on edges where in-domain feature 23 fires.
Even more sensibly, we could add features such as
f201(e) =
?10
i=1 f?i(e) ? ??i, where f? and ?? are the fea-
ture and weight vectors for the out-of-domain parser.
Learning that this feature has a high weight means
learning to trust the out-of-domain parser?s feature
weights for a particular class of features (those num-
bered 1 through 10). This addresses the intuition that
some linguistic phenomena remain stable across do-
mains.
Parses of translations. Suppose we have transla-
tions into English of some of our supervised or unsu-
pervised sentences. Good probabilistic dependency
parsers already exist for English, so we run one over
the English translation. We can now derive many
additional features on candidate edges on the tar-
get sentence. For example, dependency edges in the
target language of the form c
poss
?? p (this denotes
a child-to-parent dependency with label possessor)
might often correspond to dependency paths in the
English translation of the form p?
prep
?? of
pobj
?? c?. To
discover whether this is so, we define a feature i by
fi(c
poss
?? p) def= log
?
c?,p?
(Pr(c aligns with c?)
?Pr(p aligns with p?)
?Pr(p?
prep
?? of
pobj
?? c?))
(1)
where c?, p? range over word tokens in the English
translation, ?of? is a literal English word, and the
probabilities are posteriors provided by a probabilis-
tic aligner and a probabilistic English parser. Note
that this is a single feature (not a feature family pa-
rameterized by c, p). It scores any candidate edge on
whether it is a
poss
?? edge that seems to align to an
English
prep
?? of
pobj
?? path.
This method is inspired by Hwa et al (2005),
who bootstrapped parsers for Spanish and Chinese
by projecting dependencies from English transla-
tions and training a new parser on the resulting noisy
treebank. They used only 1-best translations, 1-best
alignments, dependency paths of length 1, and no
labeled data in Spanish or Chinese.
Hwa et al (2005) used a manually written post-
processor to correct some of the many incorrect pro-
jections. By contrast, our framework uses the pro-
jected dependencies only as one source of features.
They may be overridden by other features in particu-
lar cases, and will be given a high weight only if they
tend to agree with other features during bootstrap-
ping. A similar soft projection of dependencies was
used in supervised machine translation by Smith and
Eisner (2006), who used a source sentence?s depen-
dency paths to bias the generation of its translation.
669
Note that these bilingual features will only fire
on those supervised or unsupervised sentences for
which we have an English translation. In particu-
lar, they will usually be unavailable on the test set.
However, we hope that they will seed and facilitate
the bootstrapping process, by helping us confidently
parse some unsupervised sentences that we would
not be able to confidently parse without an English
translation.
Parses of comparable English sentences. World
knowledge can be useful in parsing. Suppose
you see a French sentence that contains mangeons
and pommes, and you know that manger=eat and
pomme=apple. You might reasonably guess that
pommes is the direct object of mangeons, because
you know that apple is a plausible direct object for
eat. We can discover this last bit of world knowl-
edge from comparable English text. Translation dic-
tionaries can themselves be induced from compara-
ble corpora (Schafer and Yarowsky, 2002; Schafer,
2006; Klementiev and Roth, 2006), or extracted
from bitext or digitized versions of human-readable
dictionaries if these are available.
The above inference pattern can be captured by
features similar to those in equation (1). For exam-
ple, one can define a feature j by
fi(c
poss
?? p) def= log Pr (p?
prep
?? of
pobj
?? c?
| p? translates p, c? translates c)
(2)
where each event in the event space is a pair (c?, p?)
of same-sentence tokens in comparable English text,
all pairs being equally likely. Thus, to estimate
Pr(? | ?), the denominator counts same-sentence
token pairs (c?, p?) in the comparable English cor-
pus that translate into the types (c, p), and the nu-
merator counts such pairs that are also related by
a
prep
?? of
pobj
?? path. Since the lexical transla-
tions and dependency paths are typically not labeled
in the English corpus, a given pair must be counted
fractionally according to its posterior probability of
satisfying these conditions, given models of contex-
tual translation and English parsing.3
3Similarly, Jansche (2005) imputes ?missing? trees by using
comparable corpora.
2.4 Bootstrapping as Optimization
Section 2.2 assumed a relatively conventional kind
of bootstrapping, where each iteration retrains the
model on the examples where it is currently most
confident. This kind of ?confidence thresholding?
has been popular in previous bootstrapping work (as
cited in ?2.2). It attempts to maintain high accu-
racy while gradually expanding coverage. The as-
sumption is that throughout the training procedure,
the parser?s confidence is a trustworthy guide to its
correctness. Different bootstrapping procedures use
different learners, smoothing methods, confidence
measures, and procedures for ?forgetting? the label-
ings from previous iterations.
In his analysis of Yarowsky (1995), Abney (2004)
formulates several variants of bootstrapping. These
are shown to increase either the likelihood of the
training data, or a lower bound on that likelihood. In
particular, Abney defines a function K that is an up-
per bound on the negative log-likelihood, and shows
his bootstrapping algorithms locally minimize K.
We now present a generalization of Abney?s K
function and relate it to another semi-supervised
learning technique, entropy regularization (Brand,
1999; Grandvalet and Bengio, 2005; Jiao et al,
2006). Our experiments will tune the feature weight
vector, ?, to minimize our function. We will do so
simply by applying a generic function minimization
method (stochastic gradient descent), rather than by
crafting a new Yarowsky-style or Abney-style itera-
tive procedure for our specific function.
Suppose we have examples xi and correspond-
ing possible labelings yi,k. We are trying to learn
a parametric model p?(yi,k | xi). If p?(yi,k | xi) is
a ?labeling distribution? that reflects our uncertainty
about the true labels, then our expected negative log-
likelihood of the model is
K def= ?
?
i
?
k
p?(yi,k | xi) log p?(yi,k | xi)
=
?
i
?
k
p?(yi,k|xi) log
p?(yi,k|xi)
p?(yi,k|xi)p?(yi,k|xi)
=
?
i
D(p?i?p?,i) + H(p?i) (3)
where p?i(?)
def= p?(? | xi) and p?,i(?)
def= p?(? | xi).
Note that K is a function not only of ? but also
670
of the labeling distribution p?; a learner might be al-
lowed to manipulate either in order to decrease K.
The summands of K in equation (3) can be di-
vided into two cases, according to whether xi is la-
beled or not. For the labeled examples {xi : i ? L},
the labeling distribution p?i is a point distribution that
assigns all probability to the true, known label y?i .
Then H(p?i) = 0. The total contribution of these ex-
amples to K simplifies to
?
i?L? log p?(y
?
i | xi),
i.e., just the negative log-likelihood on the labeled
data.
But what is the labeling distribution for the unla-
beled examples {xi : i 6? L}? Abney simply uses
a uniform distribution over labels (e.g., parses), to
reflect that the label is unknown. If his bootstrap-
ping algorithm ?labels? xi, then i moves into L and
H(p?i) is thereby reduced from maximal to 0. As a
result, a method that labels the most confident ex-
amples may reduce K, and Abney shows that his
method does so.
Our approach is different: we will take the label-
ing distribution p?i to be our actual current belief
p?,i, and manipulate it through changing ? rather
than L. L remains the original set of supervised ex-
amples. The total contribution of the unsupervised
examples to K then simplifies to
?
i6?L H(p?,i).
We have no reason to believe that these two con-
tributions (supervised and unsupervised) should be
weighted equally. We thus introduce a multiplier ?
to form the actual objective function that we mini-
mize with respect to ?:4
?
?
i?L
log p?,i(y
?
i ) + ?
N?
i6?L
H(p?,i) (4)
One may regard ? as a Lagrange multiplier that is
used to constrain the classifier?s uncertainty H to
be low, as presented in the work on entropy regular-
ization (Brand, 1999; Grandvalet and Bengio, 2005;
Jiao et al, 2006).
Conventional bootstrapping retrains on the most
confident unsupervised examples, making them
4This function is not necessarily convex in ?, because of the
addition of the entropy term (Jiao et al, 2006). One might try an
annealing strategy: start ? at zero (where the function is convex)
and gradually increase it, hoping to ?ride? the global maximum.
Although we could increase ? until the entropy term dominates
the minimizations and we approach a completely deterministic
classifier, it is preferable to use some labeled heldout data to
evaluate a stopping criterion.
more confident. Gradient descent on equation (4)
essentially does the same, since unsupervised exam-
ples contribute to (4) only through H , and the shape
of the H function means that it is most rapidly de-
creased by making the most confident unsupervised
examples more confident.
Besides favoring models that are self-confident on
the unlabeled data, the objective function (4) also ex-
plicitly asks the model to continue to get the correct
answers on the initial supervised corpus. 1/? con-
trols the strength of this request. One could obtain
a similar effect in conventional bootstrapping by up-
weighting the initial labeled corpus when retraining.
2.5 Online Learning
Minimizing equation (4) for parsing is more com-
putationally intensive than in many other applica-
tions of bootstrapping, such as word sense disam-
biguation or document classification. With millions
of features, our objective could take many iterations
to converge to a local optimum, if we were only to
update our parameter vector ? after each iteration
through a large unsupervised corpus.
For many machine learning problems over large
datasets, online learning methods such as stochas-
tic gradient descent (SGD) have been empirically
observed to converge in fewer iterations (Bottou,
2003). In SGD, instead of taking an optimiza-
tion step in the direction of the gradient calculated
over all unsupervised training examples, we parse
each example, calculate the gradient of the objective
function evaluated on that example alone, and then
take a small step downhill. The update rule is thus
?(t+1) ? ?(t) ? ? ? ?F (t)(?(t)) (5)
where ?(t) is the parameter vector at time t, F (t)(?)
is the objective function specialized to the time-t ex-
ample, and ? > 0 is a learning rate that we choose.
We check for convergence after each pass through
the example set.
2.6 Algorithms and Complexity
To evaluate equation (4), we need a conditional
model of trees given a sentence xi. We define one
by exponentiating and normalizing the tree scores:
p?,i(yi,k)
def= exp(
?
e?yi,k f(e) ? ?)/Zi.
With exponentially many parses of xi, does our
objective function (4) now have prohibitive com-
671
putational complexity? The complexity is actually
similar to that of the inside algorithm for parsing.
In fact, the first term of (4) for projective parsing
is found by running the O(n3) inside algorithm on
supervised data,5 and its gradient is found by the
corresponding O(n3) outside algorithm. For non-
projective parsing, the analogy to the inside algo-
rithm is the O(n3) ?matrix-tree algorithm,? which is
dominated asymptotically by a matrix determinant
(Smith and Smith, 2007; Koo et al, 2007; McDon-
ald and Satta, 2007). The gradient of a determinant
may be computed by matrix inversion, so evaluating
the gradient again has the same O(n3) complexity
as evaluating the function.
The second term of (4) is the Shannon entropy
of the posterior distribution over parses. Computing
this for projective parsing takes O(n3) time, using a
dynamic programming algorithm that is closely re-
lated to the inside algorithm (Hwa, 2000).6 For non-
projective parsing, unfortunately, the runtime rises
to O(n4), since it requires determinants of n distinct
matrices (each incorporating a log factor in a dif-
ferent column; we omit the details). The gradient
evaluation in both cases is again about as expensive
as the function evaluation.
A convenient speedup is to replace Shannon en-
tropy with Re?nyi entropy. The family of Re?nyi en-
tropy measures is parameterized by ?:
R?(p) =
1
1? ?
log
(
?
y
p(y)?
)
(6)
In our setting, where p = p?,i, the events y are the
possible parses yi,k of xi. Observe that under our
definition of p,
?
y p(y)
? = {
?
y exp[
?
e?y f(e) ?
(??)]}/Z?i . We already have Zi from running the
inside algorithm, and we can find the numerator by
running the inside algorithm again with ? scaled
by ?. Thus with Re?nyi entropy, all computations
and their gradients are O(n3)?even in the non-
projective case.
Re?nyi entropy is also a theoretically attractive
generalization. It can be shown that lim??1 R?(p)
5The numerator of p?,i(y?i ) (see definition above) is trivial
since y?i is a single known parse. But the denominator Zi is a
normalizing constant that sums over all parses; it is found by a
dependency-parsing variant of the inside algorithm, following
(Eisner, 1996).
6See also (Mann and McCallum, 2007) for similar results on
conditional random fields.
is in fact the Shannon entropy H(p) and that
lim???R?(p) = ? logmaxy p(y), i.e. the nega-
tive log probability of the modal or ?Viterbi? label
(Arndt, 2001; Karakos et al, 2007). The ? = 2
case, widely used as a measure of purity in decision
tree learning, is often called the ?Gini index.? Fi-
nally, when ? = 0, we get the log of the number
of labels, which equals the H(uniform distribution)
that Abney used in equation (3).
3 Evaluation
For this paper, we performed some initial bootstrap-
ping experiments on small corpora, using the fea-
tures from (McDonald et al, 2005). After discussing
experimental setup (?3.1), we look at the correlation
of confidence with accuracy and with oracle likeli-
hood, and at the fine-grained behaviour of models?
dependency edge posteriors (?3.2). We then com-
pare our confidence-maximizing bootstrapping to
EM, which has been widely used in semi-supervised
learning (?3.4). Section 3.3 presents overall boot-
strapping accuracy.
3.1 Experimental Design
We bootstrapped non-projective parsers for lan-
guages assembled for the CoNLL dependency pars-
ing competitions (Buchholz and Marsi, 2006). We
selected German, Spanish, and Czech (Brants et
al., 2002; Civit Torruella and Mart?? Anton??n, 2002;
Bo?hmova? et al, 2003). After removing sentences
more than 60 words long, we randomly divided each
corpus into small seed sets of 100 and 1000 trees;
development and test sets of 200 trees each; and an
unlabeled training set from the rest.
These treebanks contain strict dependency trees,
in the sense that their only nodes are the words and
a distinguished root node. In the Czech dataset,
more than one word can attach to the root; also, the
trees in German, Spanish, and Czech may be non-
projective. We use the MSTParser implementa-
tion described in McDonald et al (2005) for fea-
ture extraction. Since our seed sets are so small, we
extracted features from all edges in both the seed
and the unlabeled parts of our training data, not just
the edges annotated as correct. Since this produced
many more features, we pruned our features to those
with at least 10 occurrences over all edges.
672
Correlation of
100-tree model 1000-tree model
Re?nyi ? Acc. Xent. Acc. Xent.
(uniform, Abney) 0 -0.254 0.980 -0.180 0.937
.5 -0.256 0.981 -0.203 0.955
(Shannon) 1 -0.260 0.983 -0.220 0.964
(Gini) 2 -0.266 0.985 -0.250 0.977
5 -0.291 0.992 -0.304 0.990
7 -0.301 0.993 -0.341 0.991
(Viterbi) ? -0.317 0.995 -0.326 0.992
Xent. -0.391 1.000 -0.410 1.000
Table 1: Correlation, on development sentences, of Re?nyi en-
tropy with model accuracy and with cross-entropy (?Xent.?).
Since these are measures of uncertainty, we see a negative cor-
relation. As ? increases, we place more confidence in high-
probability parses and correlate better with accuracy.
We used stochastic gradient descent first to min-
imize equation (4) on the labeled seed sets. Then
we continued to optimize over the labeled and unla-
beled data together. We tested for convergence using
accuracy on development data.
3.2 Empirically Evaluating Entropy
Bootstrapping assumes that where the parser is con-
fident, it tends to be correct. Standard bootstrapping
methods retrain directly on confident links; simi-
larly, our approach tries to make the parser even
more confident on those links.
Is this assumption really true empirically? Yes:
not only does confidence on unlabeled data correlate
with cross-entropy, but both confidence and cross-
entropy correlate well with accuracy. As we will
see, some confidence measures correlate better than
others. In particular, measures that are more peaked
around the one-best prediction of the parser, as in
Viterbi re-estimation, perform well.
If we train a non-projective German parser on
small seed sets of 100 and 1000 trees, only, how well
does its own confidence predict its performance?
For 200 points?labeled development sentences?
we measured the linear correlation of various Re?nyi
entropies (6), normalized by sentence length, with
tree accuracy (Table 1). We also measured how these
normalized Re?nyi entropies correlate with the pos-
terior log-probability the model assigns to the true
parse (the cross-entropy).
Since Re?nyi entropy is a measure of uncertainty,
we see a negative correlation with accuracy. This
correlation strengthens as we raise ? to ?, so we
might expect Viterbi re-estimation, or a differen-
Figure 1: Posterior probability of correct and incorrect edges
in German test data under various models. We show the distri-
bution of posterior probabilities for correct edges, known from
an oracle, in black and incorrect edges in gray. In the upper
row, learning on an initial supervised set raises the posterior
probability of correct edges while dragging along some incor-
rect edges. In the lower row, we see that adding unlabeled data
with R2 entropy continues the pattern of the supervised learner.
R? (Viterbi) training induces a second mode in correct pos-
terior probabilities near 1 although it does shift more incorrect
edges closer to 1.
Figure 2: Precision-recall curves for selecting edges according
to their posterior probabilities: better bootstrapping puts more
area under the curve.
tiable objective function with a very high ?, to per-
form best on held-out data. Note also that the cross-
entropy, which looks at the true labels on the held-
out data, does not itself correlate very much bet-
ter with accuracy than the best unsupervised confi-
dence measures. Finally, we see that Re?nyi entropies
with higher ? are more stable: when calculated for a
model trained on more data, they improve their cor-
relation with accuracy.
From tree confidence, we now turn to edge confi-
dence: what is the posterior probability that a model
assigns to each of the n2 edges in the dependency
graph? Figure 1 shows smoothed histograms of true
edges (black) and false edges (gray) in held-out data,
according to the posterior probabilities we assign to
673
them. Since there are many more false edges, the
figures are cropped to zoom in on the distribution of
true edges. As we start training on the labeled seed
set, the posterior probabilities of true edges move to-
wards one; many false edges also get greater mass,
but not to the same extent. As we add unlabeled
data, we can see the different learning strategies of
different confidence measures. R2 gradually moves
a few true and many fewer false edges towards 1,
while R? (Viterbi) learning is so confident as to in-
duce a bimodal distribution in the posteriors of true
edges. Figure 2 visualizes the same data as four
precision-recall curves, which show how noisy the
highest-confidence edges are, across a range of con-
fidence thresholds. Although the very high precision
end stays stable after 10 iterations on the seed set,
the addition of unlabeled data puts more area under
the curve. Again, R? dominates R2.
3.3 Bootstrapping Results
We performed bootstrapping experiments on the full
CoNLL sets for Czech, German, and Spanish us-
ing the non-projective model from McDonald et al
(2005). Performance confirms the results of our
analysis above (Table 2). Adding unlabeled data im-
proves performance over that of the seed set, with
the exception of the Czech data with R2 bootstrap-
ping. As we saw in ?3.2, bootstrapping with R?
dominates bootstrapping with R2 confidence. For
comparison, we also show the results obtained by
supervised training on the combined seed and unla-
beled sets. Recall that we did not use the tree anno-
tations to perform feature selection; models trained
with only supported features ought to perform better.
Although we see statistically significant improve-
ments (at the .05 level on a paired permutation test),
the quality of the parsers is still quite poor, in con-
trast to other applications of bootstrapping which
?rival supervised methods? (Yarowsky, 1995). Al-
most certainly, the CoNLL datasets, comprising at
most some tens of thousands of sentences per lan-
guage, are too small to afford qualitative improve-
ments. Also, at these relatively small training sizes,
our preliminary attempts to leverage comparable En-
glish corpora did not improve performance.
What features were learned, and how dependent
is performance on the seed set? We analyzed the
performance of German bootstrapping on a develop-
% accuracy
Seed trees ? = 0 2 ?
Czech 100 56.1 54.8 58.3
1000 68.1 68.2 68.2
71468 77.9 ? ?
German 100 60.9 62.4 65.3
1000 74.6 74.5 75.0
37745 86.0 ? ?
Spanish 100 63.6 64.1 64.4
2786 76.6 ? ?
Table 2: Dependency accuracy of the McDonald model on 200
test sentences. When ? = 0, training only occurs on the super-
vised seed data. As ? increases, we train based on confidence
in our model?s analysis of the unlabeled data. Boldface results
are the best in their rows in a permutation test at the .05 level.
ment set (Table 3). Using the parameters at the last
iteration of supervised training on the seed set as a
baseline, we tried updating to their bootstrapped val-
ues the weights of only those features that occurred
in the seed set. This achieved nearly the same ac-
curacy as updating all the features. As one would
expect, using only the non-seed features? weights
performs abysmally. This might be the case sim-
ply because the seed set is likely to contain fre-
quently occurring features. If, however, we use only
the features occurring in an alternate training set of
the same size (100 sentences), we get much worse
performance. These results indicate that our boot-
strapped parser is still heavily dependent on the fea-
tures that happened to fire in the seed set; we have
not ?forgotten? our initial conditions. Similar exper-
iments show that unlexicalized features contribute
the most to bootstrapping performance. Since in
our log-linear models features have been trained to
work together, we must not put too much weight on
these ablation results. These experiments do, how-
ever, suggest that bootstrapping improved our results
by refining the values of known, non-lexicalized fea-
tures.
3.4 Comparison with EM
Perhaps the most popular statistical method for
learning from incomplete data is the EM algorithm
(Dempster et al, 1977). Since we cannot try EM on
McDonald?s conditional model, we ran some pilot
experiments using the generative dependency model
with valence (DMV) of Klein and Manning (2004).
As in their experiments, and unlike the other exper-
iments in the current paper, we restricted ourselves
674
Updated M feat. acc. Updated M feat. acc.
all 15.5 64.3 none 0 60.9
seed 1.4 64.1 non-seed 14.1 44.7
non-lexical 3.5 64.4 lexical 12.0 59.9
non-bilex. 12.6 64.4 bilexical 2.9 61.0
Table 3: Using all features, dependency accuracy on German
development data rose to 64.3% on bootstrapping. We show the
contribution of different feature splits to the performance of this
final model. For example, although this model was trained by
updating all 15.5M feature weights, it performs as well if we
then keep only the 1.4M features that appeared at least once in
the seed set, zeroing out the weights of the others. We do as well
as the full feature set if we keep only the 3.5M non-lexicalized
features.
% accuracy
train Bulg. German Spanish
supervised ML 74.2 80.0 71.3
CL 77.5 79.3 75.0
semi- EM 58.6 58.8 68.4
supervised Conf. 80.0 80.5 76.7
Table 4: Dependency accuracy of the DMV model (Klein and
Manning, 2004). Maximizing confidence using R1 (Shannon)
entropy improved performance over its own conditional like-
lihood (CL) baseline and over maximum likelihood (ML). EM
degraded its ML baseline. Since these models were only trained
and tested on sentences of 10 words or fewer, accuracy is much
higher than the full results in Table 2.
to sentences of ten words or fewer and to part-of-
speech sequences alone, without any lexical infor-
mation. Since the DMV models projective trees, we
ran experiments on three CoNLL corpora that had
augmented their primary non-projective parses with
alternate projective annotations: Bulgarian (Simov
et al, 2005), German, and Spanish.
We performed supervised maximum likelihood
and conditional likelihood estimation on a seed set
of 100 sentences for each language. These models
respectively initialized EM and confidence training
on unlabeled data. We see (Table 4) that EM de-
grades the performance of its ML baseline. Meri-
aldo (1994) saw a similar degradation over small
(and large) seed sets in HMM POS tagging. We
tried fixing and not fixing the feature expectations on
the seed set during EM and show the former, better
numbers. Confidence maximization improved over
both its own conditional likelihood initializer and
also over ML. We selected optimal smoothing pa-
rameters for all models and optimal ? (equation (6))
and ? (equation (4)) for the confidence model on la-
beled held-out data.
4 Future Work
We hypothesize that qualitatively better bootstrap-
ping results will require much larger unlabeled data
sets. In scaling up bootstrapping to larger unla-
beled training sets, we must carefully weight trade-
offs between expanding coverage and introducing
noise from out-of-domain data. We could also bet-
ter exploit the data we have with richer models of
syntax. In supervised dependency parsing, second-
order edge features provide improvements (McDon-
ald and Pereira, 2006; Riedel and Clarke, 2006);
moreover, the feature-based approach is not limited
to dependency parsing. Similar techniques could
score parses in other formalisms, such as CFG or
TAG. In this case, f extracts features from each
of the derivation tree?s rewrite rules (CFG) or ele-
mentary trees (TAG). In lexicalized formalisms, f
will still be able to score lexical dependencies that
are implicitly represented in the parse. Finally, we
want to investigate whether larger training sets will
provide traction for sparser cross-lingual and cross-
domain features.
5 Conclusions
Feature-rich dependency models promise to help
bootstrapping by providing many redundant features
for the learner, and they can also cleanly incorporate
cross-domain and cross-language information.
We explored bootstrapping feature-rich non-
projective dependency parsers for Czech, German,
and Spanish. Our bootstrapping method maximizes
a linear combination of likelihood and confidence.
In initial experiments on small datasets, this sur-
passed EM for training a simple feature-poor gener-
ative model, and also improved the performance of
a feature-rich, conditionally estimated model where
EM could not easily have been applied. For our
models and training sets, more peaked measures
of confidence, measured by Re?nyi entropy, outper-
formed smoother ones.
Acknowledgments
The authors thank the anonymous reviewers, Noah
A. Smith, and Keith Hall for helpful comments, and
Ryan McDonald for making his parsing code pub-
licly available. This work was supported in part by
NSF ITR grant IIS-0313193.
675
References
Steven Abney. 2004. Understanding the Yarowsky algo-
rithm. CL, 30(3):365?395.
Cristoph Arndt. 2001. Information Measures. Springer.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP, pages 120?128.
A. Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, volume 20 of Text, Speech and Language Tech-
nology, chapter 7. Kluwer.
Le?on Bottou. 2003. Stochastic learning. In Ad-
vanced Lectures in Machine Learning, pages 146?168.
Springer.
Matthew E. Brand. 1999. Structure learning in condi-
tional probability models via an entropic prior and pa-
rameter extinction. Neural Computation, 11(5):1155?
1182.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In TLT.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Civit Torruella and M. A. Mart?? Anton??n. 2002. De-
sign principles for a Spanish treebank. In TLT.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In EMNLP, pages 232?241.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum
likelihood estimation from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society B,
39:1?38.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In COLING,
pages 340?345.
H. N. Gabow, Z. Galil, T. H. Spencer, and R. E. Tarjan.
1986. Efficient algorithms for finding minimum span-
ning trees in undirected and directed graphs. Combi-
natorica, 6(2):109?122.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Yves Grandvalet and Yoshua Bengio. 2005. Semi-
supervised learning by entropy minimization. In
NIPS.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11:311?325.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In EMNLP, pages 45?52.
Martin Jansche. 2005. Treebank transfer. In IWPT.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved se-
quence segmentation and labeling. In COLING/ACL,
pages 209?216.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Carey E. Priebe. 2007. Cross-instance tuning
of unsupervised document clustering algorithms. In
HLT-NAACL, pages 252?259.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In ACL, pages 479?486.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In COLING-
ACL, pages 817?824.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the Matrix-Tree The-
orem. In EMNLP-CoNLL.
Gideon S. Mann and Andrew McCallum. 2007. Efficient
computation of entropy gradient for semi-supervised
conditional random fields. In HLT-NAACL.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In EMNLP, pages 44?52, July.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL, pages 337?344.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In EACL.
676
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In IWPT.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL, pages 91?98.
Bernardo Merialdo. 1994. Tagging English text with a
probabilistic model. CL, 20(2):155?72.
Fuchun Peng, Ralph Weischedel, Ana Licuanan, and
Jinxi Xu. 2005. Combining deep linguistics analy-
sis and surface pattern learning: A hybrid approach
to Chinese definitional question answering. In HLT-
EMNLP, pages 307?314.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In EMNLP, pages 129?137.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In CoNLL.
Charles Schafer. 2006. Translation Discovery Using Di-
verse Smilarity Measures. Ph.D. thesis, Johns Hop-
kins University.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2005. Design and implementation of the Bulgarian
HPSG-based treebank. In Journal of Research on Lan-
guage and Computation ? Special Issue. Kluwer.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 23?30.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees. In
EMNLP-CoNLL.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In EACL.
Ralph Weischedel. 2004. Extracting dynamic evidence
networks. Technical Report AFRL-IF-RS-TR-2004-
246, BBN Technologies, Cambridge, MA, December.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL, pages 207?216.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL, pages
189?196.
677
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 31?40,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Modeling Annotators:
A Generative Approach to Learning from Annotator Rationales?
Omar F. Zaidan and Jason Eisner
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,jason}@cs.jhu.edu
Abstract
A human annotator can provide hints to a machine learner
by highlighting contextual ?rationales? for each of his
or her annotations (Zaidan et al, 2007). How can one
exploit this side information to better learn the desired
parameters ?? We present a generative model of how
a given annotator, knowing the true ?, stochastically
chooses rationales. Thus, observing the rationales helps
us infer the true ?. We collect substring rationales for
a sentiment classification task (Pang and Lee, 2004) and
use them to obtain significant accuracy improvements for
each annotator. Our new generative approach exploits the
rationales more effectively than our previous ?masking
SVM? approach. It is also more principled, and could be
adapted to help learn other kinds of probabilistic classi-
fiers for quite different tasks.
1 Background
Many recent papers aim to reduce the amount of an-
notated data needed to train the parameters of a sta-
tistical model. Well-known paradigms include ac-
tive learning, semi-supervised learning, and either
domain adaptation or cross-lingual transfer from ex-
isting annotated data.
A rather different paradigm is to change the ac-
tual task that is given to annotators, giving them a
greater hand in shaping the learned classifier. Af-
ter all, human annotators themselves are more than
just black-box classifiers to be run on training data.
They possess some introspective knowledge about
their own classification procedure. The hope is to
mine this knowledge rapidly via appropriate ques-
tions and use it to help train a machine classifier.
How to do this, however, is still being explored.
1.1 Hand-crafted rules
An obvious option is to have the annotators directly
express their knowledge by hand-crafting rules. This
?This work was supported by National Science Foundation
grant No. 0347822 and the JHU WSE/APL Partnership Fund.
Special thanks to Christine Piatko for many useful discussions.
approach remains ?data-driven? if the annotators re-
peatedly refine their system against a corpus of la-
beled or unlabeled examples. This achieves high
performance in some domains, such as NP chunk-
ing (Brill and Ngai, 1999), but requires more analyt-
ical skill from the annotators. One empirical study
(Ngai and Yarowsky, 2000) found that it also re-
quired more annotation time than active learning.
1.2 Feature selection by humans
More recent work has focused on statistical classi-
fiers. Training such classifiers faces the ?credit as-
signment problem.? Given a training example xwith
many features, which features are responsible for its
annotated class y? It may take many training exam-
ples to distinguish useful vs. irrelevant features.1
To reduce the number of training examples
needed, one can ask annotators to examine or pro-
pose some candidate features. This is possible even
for the very large feature sets that are typically used
in NLP. In document classification, Raghavan et al
(2006) show that feature selection by an oracle could
be helpful, and that humans are both rapid and rea-
sonably good at distinguishing highly useful n-gram
features from randomly chosen ones, even when
viewing these n-grams out of context.
Druck et al (2008) show annotators some features
f from a fixed feature set, and ask them to choose a
class label y such that p(y | f) is as high as possible.
Haghighi and Klein (2006) do the reverse: for each
class label y, they ask the annotators to propose a
few ?prototypical? features f such that p(y | f) is as
high as possible.
1.3 Feature selection in context
The above methods consider features out of context.
An annotator might have an easier time examining
1Most NLP systems use thousands or millions of features,
because it is helpful to include lexical features over a large vo-
cabulary, often conjoined with lexical or non-lexical context.
31
features in context to recognize whether they appear
relevant. This is particularly true for features that
are only modestly or only sometimes helpful, which
may be abundant in NLP tasks.
Thus, Raghavan et al (2006) propose an active
learning method in which, while classifying a train-
ing document, the annotator also identifies some fea-
tures of that document as particularly relevant. E.g.,
the annotator might highlight particular unigrams as
he or she reads the document. In their proposal, a
feature that is highlighted in any document is as-
sumed to be globally more relevant. Its dimension
in feature space is scaled by a factor of 10 so that
this feature has more influence on distances or inner
products, and hence on the learned classifier.
1.4 Concerns about marking features
Despite the success of the above work, we have
several concerns about asking annotators to identify
globally relevant features.
First, a feature in isolation really does not have a
well-defined worth. A feature may be useful only in
conjunction with other features,2 or be useful only
to the extent that other correlated features are not
selected to do the same work.
Second, it is not clear how an annotator would
easily view and highlight features in context, ex-
cept for the simplest feature sets. In the phrase
Apple shares up 3%, there may be several fea-
tures that fire on the substring Apple?responding
to the string Apple, its case-invariant form apple,
its lemma apple- (which would also respond to ap-
ples), its context-dependent sense Apple2, its part
of speech noun, etc. How does the annotator indi-
cate which of these features are relevant?
Third, annotating features is only appropriate
when the feature set can be easily understood by a
human. This is not always the case. It would be hard
for annotators to read, write, or evaluate a descrip-
tion of a complex syntactic configuration in NLP or
a convolution filter in machine vision.
Fourth, traditional annotation efforts usually try to
remain agnostic about the machine learning methods
2For example, a linear classifier can learn that most training
examples satisfyA? B by setting ?A = ?5 and ?A?B = +5,
but this solution requires selecting bothA andA?B as features.
More simply, a polynomial kernel can consider the conjunction
A ?B only if both A and B are selected as features.
and features to be used. The project?s cost is justi-
fied by saying that the annotations will be reused by
many researchers (perhaps in a ?shared task?), who
are free to compete on how they tackle the learning
problem. Unfortunately, feature annotation commits
to a particular feature set at annotation time. Subse-
quent research cannot easily adjust the definition of
the features, or obtain annotation of new features.
2 Annotating Rationales
To solve these problems, we propose that annotators
should not select features but rather mark relevant
portions of the example. In earlier work (Zaidan et
al., 2007), we called these markings ?rationales.?
For example, when classifying a movie review as
positive or negative, the annotator would also high-
light phrases that supported that judgment. Figure 1
shows two such rationales.
A multi-annotator timing study (Zaidan et al,
2007) found that highlighting rationale phrases
while reading movie reviews only doubled annota-
tion time, although annotators marked 5?11 ratio-
nale substrings in addition to the simple binary class.
The benefit justified the extra time. Furthermore,
much of the benefit could have been obtained by giv-
ing rationales for only a fraction of the reviews.
In the visual domain, when classifying an im-
age as containing a zoo, the annotator might circle
some animals or cages and the sign reading ?Zoo.?
The Peekaboom game (von Ahn et al, 2006) was in
fact built to elicit such approximate yet relevant re-
gions of images. Further scenarios were discussed in
(Zaidan et al, 2007): rationale annotation for named
entities, linguistic relations, or handwritten digits.
Annotating rationales does not require the anno-
tator to think about the feature space, nor even to
know anything about it. Arguably this makes an-
notation easier and more flexible. It also preserves
the reusability of the annotated data. Anyone is free
to reuse our collected rationales (section 4) to aid
in learning a classifier with richer features, or a dif-
ferent kind of classifier altogether, using either our
procedures or novel procedures.
3 Modeling Rationale Annotations
As rationales are more indirect than explicit features,
they present a trickier machine learning problem.
32
We wish to learn the parameters ? of some classi-
fier. How can the annotator?s rationales help us to
do this without many training examples? We will
have to exploit a presumed relationship between the
rationales and the optimal value of ? (i.e., the value
that we would learn on an infinite training set).
This paper exploits an explicit, parametric model
of that relationship. The model?s parameters ? are
intended to capture what that annotator is doing
when he or she marks rationales. Most importantly,
they capture how he or she is influenced by the true
?. Given this, our learning method will prefer values
of ? that would adequately explain the rationales (as
well as the training classifications).
3.1 A generative approach
For concreteness, we will assume that the task is
document classification. Our training data consists
of n triples {(x1, y1, r1), ..., (xn, yn, rn)}), where xi
is a document, yi is its annotated class, and ri is its
rationale markup. At test time we will have to pre-
dict yn+1 from xn+1, without any rn+1.
We propose to jointly choose parameter vectors ?
and ? to maximize the following regularized condi-
tional likelihood:3
n?
i=1
p(yi, ri | xi, ?, ?) ? pprior(?, ?) (1)
def
=
n?
i=1
p?(yi | xi) ? p?(ri | xi, yi, ?) ? pprior(?, ?)
Here we are trying to model all the annotations, both
yi and ri. The first factor predicts yi using an ordi-
nary probabilistic classifier p?, while the novel sec-
ond factor predicts ri using a model p? of how an-
notators generate the rationale annotations.
The crucial point is that the second factor depends
on ? (since ri is supposed to reflect the relation be-
tween xi and yi that is modeled by ?). As a result,
the learner has an incentive to modify ? in a way
that increases the second factor, even if this some-
what decreases the first factor on training data.4
3It would be preferable to integrate out ? (and even ?), but
more difficult.
4Interestingly, even examples where the annotation yi is
wrong or unhelpful can provide useful information about ? via
the pair (yi, ri). Two annotators marking the same movie re-
view might disagree on whether it is overall a positive or nega-
After training, one should simply use the first fac-
tor p?(y | x) to classify test documents x. The sec-
ond factor is irrelevant for test documents, since they
have not been annotated with rationales r.
The second factor may likewise be omitted for any
training documents i that have not been annotated
with rationales, as there is no ri to predict in those
cases. In the extreme case where no documents are
annotated with rationales, equation (1) reduces to
the standard training procedure.
3.2 Noisy channel design of rationale models
Like ordinary class annotations, rationale annota-
tions present us with a ?credit assignment problem,?
albeit a smaller one that is limited to features that fire
?in the vicinity? of the rationale r. Some of these
?-features were likely responsible for the classifica-
tion y and hence triggered the rationale. Other such
?-features were just innocent bystanders.
Thus, the interesting part of our model is p?(r |
x, y, ?), which models the rationale annotation pro-
cess. The rationales r reflect ?, but in noisy ways.
Taking this noisy channel idea seriously, p?(r |
x, y, ?) should consider two questions when assess-
ing whether r is a plausible set of rationales given
?. First, it needs a ?language model? of rationales:
does r consist of rationales that are well-formed a
priori, i.e., before ? is considered? Second, it needs
a ?channel model?: does r faithfully signal the fea-
tures of ? that strongly support classifying x as y?
If a feature contributes heavily to the classification
of document x as class y, then the channel model
should tell us which parts of document x tend to be
highlighted as a result.
The channel model must know about the partic-
ular kinds of features that are extracted by f and
scored by ?. Suppose the feature not . . . gripping,5
with weight ?h, is predictive of the annotated class y.
This raises the probabilities of the annotator?s high-
lighting each of various words, or combinations of
words, in a phrase like not the most gripping ban-
quet on film. The channel model parameters in ?
tive review?but the second factor still allows learning positive
features from the first annotator?s positive rationales, and nega-
tive features from the second annotator?s negative rationales.
5Our current experiments use only unigram features, to
match past work, but we use this example to outline how our
approach generalizes to complex linguistic (or visual) features.
33
should specify how much each of these probabilities
is raised, based on the magnitude of ?h ? R, the
class y, and the fact that the feature is an instance
of the template <Neg> . . .<Adjective>. (Thus, ?
has no parameters specific to the word gripping; it
is a low-dimensional vector that only describes the
annotator?s general style in translating ? into r.)
The language model, however, is independent of
the feature set ?. It models what rationales tend to
look like in the input domain?e.g., documents or
images. In the document case, ? should describe:
How frequent and how long are typical rationales?
Do their edges tend to align with punctuation or ma-
jor syntactic boundaries in x? Are they rarer in the
middle of a document, or in certain documents?6
Thanks to the language model, we do not need to
posit high ? features to explain every word in a ratio-
nale. The language model can ?explain away? some
words as having been highlighted only because this
annotator prefers not to end a rationale in mid-
phrase, or prefers to sweep up close-together fea-
tures with a single long rationale rather than many
short ones. Similarly, the language model can help
explain why some words, though important, might
not have been included in any rationale of r.
If there are multiple annotators, one can learn dif-
ferent ? parameters for each annotator, reflecting
their different annotation styles.7 We found this to
be useful (section 8.2).
We remark that our generative modeling approach
(equation (1)) would also apply if r were not ratio-
nale markup, but some other kind of so-called ?side
information,? such as the feature annotations dis-
cussed in section 1. For example, Raghavan et al
(2006) assume that if feature h is relevant?a bi-
6Our current experiments do not model this last point. How-
ever, we imagine that if the document only has a few ?-features
that support the classification, the annotator will probably mark
most of them, whereas if such features are abundant, the anno-
tator may lazily mark only a few of the strongest ones. A simple
approach would equip ? with a different ?bias? or ?threshold?
parameter ?x for each rationale training document x, to mod-
ulate the a priori probability of marking a rationale in x. By
fitting this bias parameter, we deduce how lazy the annotator
was (for whatever reason) on document x. If desired, a prior
on ?x could consider whether x has many strong ?-features,
whether the annotator has recently had a coffee break, etc.
7Given insufficient rationale data to recover some annota-
tor?s ?well, one could smooth using data from other annotators.
But in our situation, ? had relatively few parameters to learn.
nary distinction?iff it was selected in at least one
document. But it might be more informative to ob-
serve that h was selected in 3 of the 10 documents
where it appeared, and to predict this via a model
p?(3 of 10 | ?h), where ? describes (e.g.) how to de-
rive a binomial parameter nonlinearly from ?h. This
approach would not how often h was marked and in-
fer how relevant is feature h (i.e., infer ?h). In this
case, p? is a simple channel that transforms relevant
features into direct indicators of the feature. Our
side information merely requires a more complex
transformation?from relevant features into well-
formed rationales, modulated by documents.
4 Experimental Data: Movie Reviews
In Zaidan et al (2007), we introduced the ?Movie
Review Polarity Dataset Enriched with Annotator
Rationales.?8 It is based on the dataset of Pang and
Lee (2004),9 which consists of 1000 positive and
1000 negative movie reviews, tokenized and divided
into 10 folds (F0?F9). All our experiments use F9
as their final blind test set.
The enriched dataset adds rationale annotations
produced by an annotator A0, who annotated folds
F0?F8 of the movie review set with rationales (in the
form of textual substrings) that supported the gold-
standard classifications. We will use A0?s data to
determine the improvement of our method over a
(log-linear) baseline model without rationales. We
also use A0 to compare against the ?masking SVM?
method and SVM baseline of Zaidan et al (2007).
Since ? can be tuned to a particular annotator, we
would also like to know how well this works with
data from annotators other than A0. We randomly
selected 100 reviews (50 positive and 50 negative)
and collected both class and rationale annotation
data from each of six new annotators A3?A8,10 fol-
lowing the same procedures as (Zaidan et al, 2007).
We report results using only data from A3?A5, since
we used the data from A6?A8 as development data
in the early stages of our work.
We use this new rationale-enriched dataset8 to de-
termine if our method works well across annotators.
We will only be able to carry out that comparison
8Available at http://cs.jhu.edu/?ozaidan/rationales.
9Polarity dataset version 2.0.
10We avoid annotator names A1?A2, which were already
used in (Zaidan et al, 2007).
34
Figure 1: Rationales as sequence an-
notation: the annotator highlighted
two textual segments as rationales for
a positive class. Highlighted words in
~x are tagged I in ~r, and other words
are tagged O. The figure also shows
some ?-features. For instance, gO(,)-I
is a count of O-I transitions that occur
with a comma as the left word. Notice
also that grel is the sum of the under-
lined values.
at small training set sizes, due to limited data from
A3?A8. The larger A0 dataset will still allow us to
evaluate our method on a range of training set sizes.
5 Detailed Models
5.1 Modeling class annotations with p?
We define the basic classifier p? in equation (1) to be
a standard conditional log-linear model:
p?(y | x)
def
=
exp(~? ? ~f(x, y))
Z?(x)
def
=
u(x, y)
Z?(x)
(2)
where ~f(?) extracts a feature vector from a classified
document, ~? are the corresponding weights of those
features, and Z?(x)
def
=
?
y u(x, y) is a normalizer.
We use the same set of binary features as in pre-
vious work on this dataset (Pang et al, 2002; Pang
and Lee, 2004; Zaidan et al, 2007). Specifically, let
V = {v1, ..., v17744} be the set of word types with
count ? 4 in the full 2000-document corpus. Define
fh(x, y) to be y if vh appears at least once in x, and
0 otherwise. Thus ? ? R17744, and positive weights
in ? favor class label y = +1 and equally discourage
y = ?1, while negative weights do the opposite.
This standard unigram feature set is linguistically
impoverished, but serves as a good starting point for
studying rationales. Future work should consider
more complex features and how they are signaled by
rationales, as discussed in section 3.2.
5.2 Modeling rationale annotations with p?
The rationales collected in this task are textual seg-
ments of a document to be classified. The docu-
ment itself is a word token sequence ~x = x1, ..., xM .
We encode its rationales as a corresponding tag se-
quence ~r = r1, ..., rM , as illustrated in Figure 1.
Here rm ? {I, O} according to whether the token
xm is in a rationale (i.e., xm was at least partly high-
lighted) or outside all rationales. x1 and xM are
special boundary symbols, tagged with O.
We predict the full tag sequence ~r at once using
a conditional random field (Lafferty et al, 2001). A
CRF is just another conditional log-linear model:
p?(r |x, y, ~?)
def
=
exp(~? ? ~g(r, x, y, ~?))
Z?(x, y, ~?)
def
=
u(r, x, y, ~?)
Z?(x, y, ~?)
where ~g(?) extracts a feature vector, ~? are the
corresponding weights of those features, and
Z?(x, y, ~?)
def
=
?
r u(r, x, y,
~?) is a normalizer.
As usual for linear-chain CRFs, ~g(?) extracts two
kinds of features: first-order ?emission? features that
relate rm to (xm, y, ?), and second-order ?transi-
tion? features that relate rm to rm?1 (although some
of these also look at x).
These two kinds of features respectively capture
the ?channel model? and ?language model? of sec-
tion 3.2. The former says rm is I because xm is
associated with a relevant ?-feature. The latter says
rm is I simply because it is next to another I.
5.3 Emission ?-features (?channel model?)
Recall that our ?-features (at present) correspond to
unigrams. Given (~x, y, ~?), let us say that a unigram
w ? ~x is relevant, irrelevant, or anti-relevant if
y ? ?w is respectively 0, ? 0, or 0. That is, w
is relevant if its presence in x strongly supports the
annotated class y, and anti-relevant if its presence
strongly supports the opposite class ?y.
35
Figure 2: The
function family Bs
in equation (3),
shown for s ?
{10, 2,?2,?10}.
We would like to learn the extent ?rel to which
annotators try to include relevant unigrams in their
rationales, and the (usually lesser) extent ?antirel to
which they try to exclude anti-relevant unigrams.
This will help us infer ~? from the rationales.
The details are as follows. ?rel and ?antirel are the
weights of two emission features extracted by ~g:
grel(~x, y, ~r, ~?)
def
=
M?
m=1
I(rm = I) ?B10(y ? ?xm)
gantirel(~x, y, ~r, ~?)
def
=
M?
m=1
I(rm = I) ?B?10(y ? ?xm)
Here I(?) denotes the indicator function, returning
1 or 0 according to whether its argument is true or
false. Relevance and negated anti-relevance are re-
spectively measured by the differentiable nonlinear
functions B10 and B?10, which are defined by
Bs(a) = (log(1 + exp(a ? s))? log(2))/s (3)
and graphed in Figure 2. Sample values of B10 and
grel are shown in Figure 1.
How does this work? The grel feature is a sum
over all unigrams in the document ~x. It does not fire
strongly on the irrelevant or anti-relevant unigrams,
since B10 is close to zero there.11 But it fires posi-
tively on relevant unigrams w if they are tagged with
I, and the strength of such firing increases approxi-
mately linearly with ?w. Since the weight ?rel > 0 in
practice, this means that raising a relevant unigram?s
?w (if y = +1) will proportionately raise its log-
odds of being tagged with I. Symmetrically, since
?antirel > 0 in practice, lowering an anti-relevant un-
igram?s ?w (if y = +1) will proportionately lower
11B10 sets the threshold for relevance to be about 0. One
could also include versions of the grel feature that set a higher
threshold, using B10(y ? ?xm ? threshold).
its log-odds of being tagged with I, though not nec-
essarily at the same rate as for relevant unigrams.12
Should ? also include traditional CRF emis-
sion features, which would recognize that particular
words like great tend to be tagged as I? No! Such
features would undoubtedly do a better job predict-
ing the rationales and hence increasing equation (1).
However, crucially, our true goal is not to predict
the rationales but to recover the classifier parame-
ters ?. Thus, if great tends to be highlighted, then
the model should not be permitted to explain this
directly by increasing some feature ?great, but only
indirectly by increasing ?great. We therefore permit
our rationale prediction model to consider only the
two emission features grel and gantirel, which see the
words in ~x only through their ?-values.
5.4 Transition ?-features (?language model?)
Annotators highlight more than just the relevant un-
igrams. (After all, they aren?t told that our current
?-features are unigrams.) They tend to mark full
phrases, though perhaps taking care to exclude anti-
relevant portions. ?models these phrases? shape, via
weights for several ?language model? features.
Most important are the 4 traditional CRF tag tran-
sition features gO-O, gO-I, gI-I, gI-O. For example,
gO-I counts the number of O-to-I transitions in ~r
(see Figure 1). Other things equal, an annotator with
high ?O-I is predicted to have many rationales per
1000 words. And if ?I-I is high, rationales are pre-
dicted to be long phrases (including more irrelevant
unigrams around or between the relevant ones).
We also learn more refined versions of these fea-
tures, which consider how the transition probabil-
ities are influenced by the punctuation and syntax
of the document ~x (independent of ~?). These re-
fined features are more specific and hence more
sparsely trained. Their weights reflect deviations
from the simpler, ?backed-off? transition features
such as gO-I. (Again, see Figure 1 for examples.)
Conditioning on left word. A feature of the form
gt1(v)-t2 is specified by a pair of tag types t1, t2 ?
{I,O} and a vocabulary word type v. It counts the
12If the two rates are equal (?rel = ?antirel), we get a simpler
model in which the log-odds change exactly linearly with ?w for
each w, regardless of w?s relevance/irrelevance/anti-relevance.
This follows from the fact thatBs(a)+B?s(a) simplifies to a.
36
number of times an t1?t2 transition occurs in ~r con-
ditioned on v appearing as the first of the two word
tokens where the transition occurs. Our experiments
include gt1(v)-t2 features that tie I-O and O-I tran-
sitions to the 4 most frequent punctuation marks v
(comma, period, ?, !).
Conditioning on right word. A feature gt1-t2(v)
is similar, but v must appear as the second of the
two word tokens where the transition occurs. Again
here, we use gt1-t2(v) features that tie I-O and O-I
transitions to the four punctuation marks mentioned
above. We also include five features that tie O-I
transitions to the words no, not, so, very, and quite,
since in our development data, those words were
more likely than others to start rationales.13
Conditioning on syntactic boundary. We parsed
each rationale-annotated training document (no
parsing is needed at test time).14 We then marked
each word bigram x1-x2 with three nonterminals:
NEnd is the nonterminal of the largest constituent
that contains x1 and not x2, NStart is the nontermi-
nal of the largest constituent that contains x2 and
not x1, and NCross is the nonterminal of the smallest
constituent that contains both x1 and x2.
For a nonterminalN and pair of tag types (t1, t2),
we define three features, gt1-t2/E=N , gt1-t2/S=N ,
and gt1-t2/C=N , which count the number of times
a t1-t2 transition occurs in ~r with N matching the
NEnd, NStart, or NCross nonterminal, respectively.
Our experiments include these features for 11 com-
mon nonterminal types N (DOC, TOP, S, SBAR,
FRAG, PRN, NP, VP, PP, ADJP, QP).
6 Training: Joint Optimization of ? and ?
To train our model, we use L-BFGS to locally max-
imize the log of the objective function (1):15
13These are the function words with count ? 40 in a random
sample of 100 documents, and which were associated with the
O-I tag transition at more than twice the average rate. We do
not use any other lexical ?-features that reference ~x, for fear that
they would enable the learner to explain the rationales without
changing ? as desired (see the end of section 5.3).
14We parse each sentence with the Collins parser (Collins,
1999). Then the document has one big parse tree, whose root is
DOC, with each sentence being a child of DOC.
15One might expect this function to be convex because p? and
p? are both log-linear models with no hidden variables. How-
ever, log p?(ri | xi, yi, ?) is not necessarily convex in ?.
n?
i=1
log p?(yi | xi)?
1
2?2?
???2
+C(
n?
i=1
log p?(ri | xi, yi, ?))?
1
2?2?
???2 (4)
This defines pprior from (1) to be a standard diago-
nal Gaussian prior, with variances ?2? and ?
2
? for the
two sets of parameters. We optimize ?2? in our ex-
periments. As for ?2?, different values did not affect
the results, since we have a large number of {I,O}
rationale tags to train relatively few ? weights; so
we simply use ?2? = 1 in all of our experiments.
Note the new C factor in equation (4). Our ini-
tial experiments showed that optimizing equation (4)
without C led to an increase in the likelihood of the
rationale data at the expense of classification accu-
racy, which degraded noticeably. This is because
the second sum in (4) has a much larger magnitude
than the first: in a set of 100 documents, it predicts
around 74,000 binary {I,O} tags, versus the one
hundred binary class labels. While we are willing
to reduce the log-likelihood of the training classifi-
cations (the first sum) to a certain extent, focusing
too much on modeling rationales (the second sum)
is clearly not our ultimate goal, and so we optimize
C on development data to achieve some balance be-
tween the two terms of equation (4). Typical values
of C range from 1300 to
1
50 .
16
We perform alternating optimization on ? and ?:
1. Initialize ? to maximize equation (4) but with
C = 0 (i.e. based only on class data).
2. Fix ?, and find ? that maximizes equation (4).
3. Fix ?, and find ? that maximizes equation (4).
4. Repeat 2 and 3 until convergence.
The L-BFGS method requires calculating the gra-
dient of the objective function (4). The partial
derivatives with respect to components of ? and ?
involve calculating expectations of the feature func-
tions, which can be computed in linear time (with
respect to the size of the training set) using the
forward-backward algorithm for CRFs. The par-
tial derivatives also involve the derivative of (3),
to determine how changing ? will affect the firing
strength of the emission features grel and gantirel.
16C also balances our confidence in the classifications y
against our confidence in the rationales r; either may be noisy.
37
7 Experimental Procedures
We report on two sets of experiments. In the first
set, we use the annotation data that A3?A5 provided
for the small set of 100 documents (as well as the
data from A0 on those same 100 documents). In
the second set, we used A0?s abundant annotation
data to evaluate our method with training set sizes up
to 1600 documents, and compare it with three other
methods: log-linear baseline, SVM baseline, and the
SVM masking method of (Zaidan et al, 2007).
7.1 Learning curves
The learning curves reported in section 8.1 are gen-
erated exactly as in (Zaidan et al, 2007). Each curve
shows classification accuracy at training set sizes
T = 1, 2, ..., 9 folds (i.e. 200, 400, ..., 1600 training
documents). For a given size T , the reported accu-
racy is an average of 9 experiments with different
subsets of the entire training set, each of size T :
1
9
8?
i=0
acc(F9 | Fi+1 ? . . . ? Fi+T ) (5)
where Fj denotes the fold numbered j mod 9, and
acc(F9 | Y ) means classification accuracy on the
held-out test set F9 after training on set Y .
We use an appropriate paired permutation test, de-
tailed in (Zaidan et al, 2007), to test differences in
(5). We call a difference significant at p < 0.05.
7.2 Comparison to ?masking SVM? method
We compare our method to the ?masking SVM?
method of (Zaidan et al, 2007). Briefly, that method
used rationales to construct several so-called con-
trast examples from every training example. A con-
trast example is obtained by ?masking out? one of
the rationales highlighted to support the training ex-
ample?s class. A good classifier should have more
trouble on this modified example. Hence, Zaidan et
al. (2007) required the learned SVM to classify each
contrast example with a smaller margin than the cor-
responding original example (and did not require it
to be classified correctly).
The masking SVM learner relies on a simple geo-
metric principle; is trivial to implement on top of an
existing SVM learner; and works well. However, we
believe that the generative method we present here is
more interesting and should apply more broadly.
Figure 3: Classification accuracy curves for the 4 meth-
ods: the two baseline learners that only utilize class data,
and the two learners that also utilize rationale annota-
tions. The SVM curves are from (Zaidan et al, 2007).
First, the masking method is specific to improving
an SVM learner, whereas our method can be used to
improve any classifier by adding a rationale-based
regularizer (the second half of equation (4)) to its
objective function during training.
More important, there are tasks where it is unclear
how to generate contrast examples. For the movie
review task, it was natural to mask out a rationale
by pretending its words never occurred in the doc-
ument. After all, most word types do not appear in
most documents, so it is natural to consider the non-
presence of a word as a ?default? state to which we
can revert. But in an image classification task, how
should one modify the image?s features to ignore
some spatial region marked as a rationale? There is
usually no natural ?default? value to which we could
set the pixels. Our method, on the other hand, elim-
inates contrast examples altogether.
8 Experimental Results and Analysis
8.1 The added benefit of rationales
Fig. 3 shows learning curves for four methods. A
log-linear model shows large and significant im-
provements, at all training sizes, when we incor-
porate rationales into its training via equation (4).
Moreover, the resulting classifier consistently out-
performs17 prior work, the masking SVM, which
starts with a slightly better baseline classifier (an
SVM) but incorporates the rationales more crudely.
17Differences are not significant at sizes 200, 1000, and 1600.
38
size A0 A3 A4 A5
SVM baseline 100 72.0 72.0 72.0 70.0
SVM+contrasts 100 75.0 73.0 74.0 72.0
Log-linear baseline 100 71.0 73.0 71.0 70.0
Log-linear+rats 100 76.0 76.0 77.0 74.0
SVM baseline 20 63.4 62.2 60.4 62.6
SVM+contrasts 20 65.4 63.4 62.4 64.8
Log-linear baseline 20 63.0 62.2 60.2 62.4
Log-linear+rats 20 65.8 63.6 63.4 64.8
Table 1: Accuracy rates using each annotator?s data. In a
given column, a value in italics is not significantly differ-
ent from the highest value in that column, which is bold-
faced. The size=20 results average over 5 experiments.
To confirm that we could successfully model an-
notators other than A0, we performed the same
comparison for annotators A3?A5; each had pro-
vided class and rationale annotations on a small 100-
document training set. We trained a separate ? for
each annotator. Table 1 shows improvements over
baseline, usually significant, at 2 training set sizes.
8.2 Analysis
Examining the learned weights ~? gives insight into
annotator behavior. High weights include I-O and
O-I transitions conditioned on punctuation, e.g.,
?I(.)-O = 3.55,
18 as well as rationales ending at the
end of a major phrase, e.g., ?I-O/E=VP = 1.88.
The large emission feature weights, e.g., ?rel =
14.68 and ?antirel = 15.30, tie rationales closely to
? values, as hoped. For example, in Figure 1, the
word w = succeeds, with ?w = 0.13, drives up
p(I)/p(O) by a factor of 7 (in a positive document)
relative to a word with ?w = 0.
In fact, feature ablation experiments showed that
almost all the classification benefit from rationales
can be obtained by using only these 2 emission
?-features and the 4 unconditioned transition ?-
features. Our full ? (115 features) merely improves
our ability to predict the rationales (whose likeli-
hood does increase significantly with more features).
We also checked that annotators? styles differ
enough that it helps to tune ? to the ?target? annota-
torAwho gave the rationales. Table 3 shows that a ?
model trained onA?s own rationales does best at pre-
dicting new rationales fromA. Table 2 shows that as
18When trained on folds F4?F8 with A0?s rationales.
?A0 ?A3 ?A4 ?A5 Baseline
?A0 76.0 73.0 74.0 73.0 71.0
?A3 73.0 76.0 74.0 73.0 73.0
?A4 75.0 73.0 77.0 74.0 71.0
?A5 74.0 71.0 72.0 74.0 70.0
Table 2: Accuracy rate for an annotator?s ? (rows) ob-
tained when using some other annotator?s ? (columns).
Notice that the diagonal entries and the baseline column
are taken from rows of Table 1 (size=100).
Trivial
?A0 ?A3 ?A4 ?A5 model
?L(rA0) 0.073 0.086 0.077 0.088 0.135
?L(rA3) 0.084 0.068 0.071 0.068 0.130
?L(rA4) 0.088 0.084 0.075 0.085 0.153
?L(rA5) 0.058 0.044 0.047 0.044 0.111
Table 3: Cross-entropy per tag of rationale annotations
~r for each annotator (rows), when predicted from that
annotator?s ~x and ~? via a possibly different annotator?s
? (columns). For comparison, the trivial model is a bi-
gram model of ~r, which is trained on the target annotator
but ignores ~x and ~?. 5-fold cross-validation on the 100-
document set was used to prevent testing on training data.
a result, classification performance on the test set is
usually best if it wasA?s own ? that was used to help
learn ? from A?s rationales. In both cases, however,
a different annotator?s ? is better than nothing.
9 Conclusions
We have demonstrated a effective method for elic-
iting extra knowledge from naive annotators, in
the form of lightweight ?rationales? for their an-
notations. By explicitly modeling the annotator?s
rationale-marking process, we are able to infer a bet-
ter model of the original annotations.
We showed that our method performs signifi-
cantly better than two strong baseline classifiers,
and also outperforms our previous discriminative
method for exploiting rationales (Zaidan et al,
2007). We also saw that it worked across four anno-
tators who have different rationale-marking styles.
In future, we are interested in new domains that
can adaptively solicit rationales for some or all
training examples. Our new method, being essen-
tially Bayesian inference, is potentially extensible to
many other situations?other tasks, classifier archi-
tectures, and more complex features.
39
References
Eric Brill and Grace Ngai. 1999. Man [and woman] vs.
machine: A case study in base noun phrase learning.
In Proceedings of the 37th ACL Conference.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of ACM Special Interest
Group on Information Retrieval, (SIGIR).
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 320?327, New York City,
USA, June. Association for Computational Linguis-
tics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 117?125, Hong Kong.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL, pages 271?
278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP, pages 79?86.
Hema Raghavan and James Allan. 2007. An interactive
algorithm for asking and incorporating feature feed-
back into support vector machines. In Proceedings of
SIGIR.
Hema Raghavan, Omid Madani, and Rosie Jones. 2006.
Active learning on both features and instances. Jour-
nal of Machine Learning Research, 7:1655?1686,
Aug.
Luis von Ahn, Ruoran Liu, and Manuel Blum. 2006.
Peekaboom: A game for locating objects. In CHI
?06: Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems, pages 55?64.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In NAACL HLT 2007;
Proceedings of the Main Conference, pages 260?267,
April.
40
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 145?156,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Dependency Parsing by Belief Propagation?
David A. Smith and Jason Eisner
Dept. of Computer Science, Johns Hopkins University
Balitmore, MD 21218, USA
{dasmith,eisner}@jhu.edu
Abstract
We formulate dependency parsing as a graphical model
with the novel ingredient of global constraints. We show
how to apply loopy belief propagation (BP), a simple and
effective tool for approximate learning and inference. As
a parsing algorithm, BP is both asymptotically and em-
pirically efficient. Even with second-order features or la-
tent variables, which would make exact parsing consider-
ably slower or NP-hard, BP needs only O(n3) time with
a small constant factor. Furthermore, such features sig-
nificantly improve parse accuracy over exact first-order
methods. Incorporating additional features would in-
crease the runtime additively rather than multiplicatively.
1 Introduction
Computational linguists worry constantly about run-
time. Sometimes we oversimplify our models, trad-
ing linguistic nuance for fast dynamic programming.
Alternatively, we write down a better but intractable
model and then use approximations. The CL com-
munity has often approximated using heavy pruning
or reranking, but is beginning to adopt other meth-
ods from the machine learning community, such
as Gibbs sampling, rejection sampling, and certain
variational approximations.
We propose borrowing a different approximation
technique from machine learning, namely, loopy be-
lief propagation (BP). In this paper, we show that
BP can be used to train and decode complex pars-
ing models. Our approach calls a simpler parser as a
subroutine, so it still exploits the useful, well-studied
combinatorial structure of the parsing problem.1
2 Overview and Related Work
We wish to make a dependency parse?s score de-
pend on higher-order features, which consider ar-
?This work was supported by the Human Language Tech-
nology Center of Excellence.
1As do constraint relaxation (Tromble and Eisner, 2006) and
forest reranking (Huang, 2008). In contrast, generic NP-hard
solution techniques like Integer Linear Programming (Riedel
and Clarke, 2006) know nothing about optimal substructure.
bitrary interactions among two or more edges in the
parse (and perhaps also other latent variables such
as part-of-speech tags or edge labels). Such features
can help accuracy?as we show. Alas, they raise the
polynomial runtime of projective parsing, and ren-
der non-projective parsing NP-hard. Hence we seek
approximations.
We will show how BP?s ?message-passing? disci-
pline offers a principled way for higher-order fea-
tures to incrementally adjust the numerical edge
weights that are fed to a fast first-order parser. Thus
the first-order parser is influenced by higher-order
interactions among edges?but not asymptotically
slowed down by considering the interactions itself.
BP?s behavior in our setup can be understood intu-
itively as follows. Inasmuch as the first-order parser
finds that edge e is probable, the higher-order fea-
tures will kick in and discourage other edges e? to the
extent that they prefer not to coexist with e.2 Thus,
the next call to the first-order parser assigns lower
probabilities to parses that contain these e?. (The
method is approximate because a first-order parser
must equally penalize all parses containing e?, even
those that do not in fact contain e.)
This behavior is somewhat similar to parser stack-
ing (Nivre and McDonald, 2008; Martins et al,
2008), in which a first-order parser derives some of
its input features from the full 1-best output of an-
other parser. In our method, a first-order parser de-
rives such input features from its own previous full
output (but probabilistic output rather than just 1-
best). This circular process is iterated to conver-
gence. Our method also permits the parse to in-
teract cheaply with other variables. Thus first-order
parsing, part-of-speech tagging, and other tasks on a
common input could mutually influence one another.
Our method and its numerical details emerge nat-
urally as an instance of the well-studied loopy BP
algorithm, suggesting several potential future im-
2This may be reminiscent of adjusting a Lagrange multiplier
on e? until some (hard) constraint is satisfied.
145
provements to accuracy (Yedidia et al, 2004; Braun-
stein et al, 2005) and efficiency (Sutton and McCal-
lum, 2007).
Loopy BP has occasionally been used before in
NLP, with good results, to handle non-local fea-
tures (Sutton and McCallum, 2004) or joint decod-
ing (Sutton et al, 2004). However, our application
to parsing requires an innovation to BP that we ex-
plain in ?5?a global constraint to enforce that the
parse is a tree. The tractability of some such global
constraints points the way toward applying BP to
other computationally intensive NLP problems, such
as syntax-based alignment of parallel text.
3 Graphical Models of Dependency Trees
3.1 Observed and hidden variables
To apply BP, we must formulate dependency parsing
as a search for an optimal assignment to the vari-
ables of a graphical model. We encode a parse using
the following variables:
Sentence. The n-word input sentence W is fully
observed (not a lattice). Let W = W0W1 ? ? ?Wn,
where W0 is always the special symbol ROOT.
Tags. If desired, the variables T = T1T2 ? ? ?Tn may
specify tags on the nwords, drawn from some tagset
T (e.g., parts of speech). These variables are needed
iff the tags are to be inferred jointly with the parse.
Links. The O(n2) boolean variables {Lij : 0 ?
i ? n, 1 ? j ? n, i 6= j} correspond to the possible
links in the dependency parse.3 Lij = true is in-
terpreted as meaning that there exists a dependency
link from parent i? child j.4
Link roles, etc. It would be straightforward to add
other variables, such as a binary variable Lirj that is
true iff there is a link i
r
? j labeled with role r (e.g.,
AGENT, PATIENT, TEMPORAL ADJUNCT).
3.2 Markov random fields
We wish to define a probability distribution over all
configurations, i.e., all joint assignments A to these
3?Links? are conventionally called edges, but we reserve the
term ?edge? for describing the graphical model?s factor graph.
4We could have chosen a different representation withO(n)
integer variables {Pj : 1 ? j ? n}, writing Pj = i instead of
Lij = true. This representation can achieve the same asymp-
totic runtime for BP by using sparse messages, but some con-
straints and algorithms would be somewhat harder to explain.
variables. Our distribution is simply an undirected
graphical model, or Markov random field (MRF):5
p(A)
def
=
1
Z
?
m
Fm(A) (1)
specified by the collection of factors Fm : A 7?
R?0. Each factor is a function that consults only a
subset of A. We say that the factor has degree d
if it depends on the values of d variables in A, and
that it is unary, binary, ternary, or global if d is
respectively 1, 2, 3, or unbounded (grows with n).
A factor function Fm(A) may also depend freely
on the observed variables?the input sentence W
and a known (learned) parameter vector ?. For no-
tational simplicity, we suppress these extra argu-
ments when writing and drawing factor functions,
and when computing their degree. In this treatment,
these observed variables are not specified by A, but
instead are absorbed into the very definition of Fm.
In defining a factor Fm, we often define the cir-
cumstances under which it fires. These are the only
circumstances that allow Fm(A) 6= 1. When Fm
does not fire, Fm(A) = 1 and does not affect the
product in equation (1).
3.3 Hard constraints
A hard factor Fm fires only on parsesA that violate
some specified condition. It has value 0 on those
parses, acting as a hard constraint to rule them out.
TREE. A hard global constraint on all the Lij vari-
ables at once. It requires that exactly n of these vari-
ables be true, and that the corresponding links form
a directed tree rooted at position 0.
PTREE. This stronger version of TREE requires
further that the tree be projective. That is, it pro-
hibits Lij and Lk` from both being true if i ? j
crosses k ? `. (These links are said to cross if one
of k, ` is strictly between i and j while the other is
strictly outside that range.)
EXACTLY1. A family of O(n) hard global con-
straints, indexed by 1 ? j ? n. EXACTLY1j re-
quires that j have exactly one parent, i.e., exactly
one of the Lij variables must be true. Note that EX-
ACTLY1 is implied by TREE or PTREE.
5Our overall model is properly called a dynamic MRF, since
we must construct different-size MRFs for input sentences of
different lengths. Parameters are shared both across and within
these MRFs, so that only finitely many parameters are needed.
146
ATMOST1. A weaker version. ATMOST1j re-
quires j to have one or zero parents.
NAND. A family of hard binary constraints.
NANDij,k` requires that Lij and Lk` may not both be
true. We will be interested in certain subfamilies.
NOT2. Shorthand for the family of O(n3) bi-
nary constraints {NANDij,kj}. These are collectively
equivalent to ATMOST1, but expressed via a larger
number of simpler constraints, which can make the
BP approximation less effective (footnote 30).
NO2CYCLE. Shorthand for the family of O(n2)
binary constraints {NANDij,ji}.
3.4 Soft constraints
A soft factor Fm acts as a soft constraint that prefers
some parses to others. In our experiments, it is al-
ways a log-linear function returning positive values:
Fm(A)
def
= exp
?
h?features(Fm)
?hfh(A,W,m) (2)
where ? is a learned, finite collection of weights and
f is a corresponding collection of feature functions,
some of which are used by Fm. (Note that fh is
permitted to consult the observed input W . It also
sees which factor Fm it is scoring, to support reuse
of a single feature function fh and its weight ?h by
unboundedly many factors in a model.)
LINK. A family of unary soft factors that judge
the links in a parse A individually. LINKij fires iff
Lij = true, and then its value depends on (i, j),
W , and ?. Our experiments use the same features as
McDonald et al (2005).
A first-order (or ?edge-factored?) parsing model
(McDonald et al, 2005) contains only LINK factors,
along with a global TREE or PTREE factor. Though
there are O(n2) link factors (one per Lij), only n
of them fire on any particular parse, since the global
factor ensures that exactly n are true.
We?ll consider various higher-order soft factors:
PAIR. The binary factor PAIRij,k` fires with some
value iff Lij and Lk` are both true. Thus, it penal-
izes or rewards a pair of links for being simultane-
ously present. This is a soft version of NAND.
GRAND. Shorthand for the family of O(n3) binary
factors {PAIRij,jk}, which evaluate grandparent-
parent-child configurations, i ? j ? k. For exam-
ple, whether preposition j attaches to verb i might
depend on its object k. In non-projective parsing,
we might prefer (but not require) that a parent and
child be on the same side of the grandparent.
SIB. Shorthand for the family of O(n3) binary fac-
tors {PAIRij,ik}, which judge whether two children
of the same parent are compatible. E.g., a given verb
may not like to have two noun children both to its
left.6 The children do not need to be adjacent.
CHILDSEQ. A family of O(n) global factors.
CHILDSEQi scores i?s sequence of children; hence
it consults all variables of the form Lij . The scor-
ing follows the parametrization of a weighted split
head-automaton grammar (Eisner and Satta, 1999).
If 5 has children 2, 7, 9 under A, then CHILDSEQi
is a product of subfactors of the form PAIR5#,57,
PAIR57,59, PAIR59,5# (right child sequence) and
PAIR5#,52, PAIR52,5# (left child sequence).
NOCROSS. A family of O(n2) global constraints.
If the parent-to-j link crosses the parent-to-` link,
then NOCROSSj` fires with a value that depends
only on j and `. (If j and ` do not each have ex-
actly one parent, NOCROSSj` fires with value 0; i.e.,
it incorporates EXACTLY1j and EXACTLY1`.)7
TAGi is a unary factor that evaluates whether Ti?s
value is consistent with W (especially Wi).
TAGLINKij is a ternary version of the LINKij fac-
tor whose value depends on Lij , Ti and Tj (i.e., its
feature functions consult the tag variables to decide
whether a link is likely). One could similarly enrich
the other features above to depend on tags and/or
link roles; TAGLINK is just an illustrative example.
TRIGRAM is a global factor that evaluates the tag
sequence T according to a trigram model. It is a
product of subfactors, each of which scores a tri-
gram of adjacent tags Ti?2, Ti?1, Ti, possibly also
considering the word sequence W (as in CRFs).
4 A Sketch of Belief Propagation
MacKay (2003, chapters 16 and 26) provides an
excellent introduction to belief propagation, a gen-
6A similar binary factor could directly discourage giving the
verb two SUBJECTs, if the model has variables for link roles.
7In effect, we have combined the O(n4) binary factors
PAIRij,k` into O(n2) groups, and made them more precise
by multiplying in EXACTLYONE constraints (see footnote 30).
This will permit O(n3) total computation if we are willing to
sacrifice the ability of the PAIR weights to depend on i and k.
147
LINK
L[2,5]
GRAND
L[5,6]
LINK
TREE
Figure 1: A fragment of a factor graph, illustrating a few
of the unary, binary, and global factors that affect vari-
ables L25 and L56. The GRAND factor induces a loop.
eralization of the forward-backward algorithm that
is deeply studied in the graphical models literature
(Yedidia et al, 2004, for example). We briefly
sketch the method in terms of our parsing task.
4.1 Where BP comes from
The basic BP idea is simple. Variable L34 main-
tains a distribution over values true and false?a
?belief??that is periodically recalculated based on
the current distributions at other variables.8
Readers familiar with Gibbs sampling can regard
this as a kind of deterministic approximation. In
Gibbs sampling, L34?s value is periodically resam-
pled based on the current values of other variables.
Loopy BP works not with random samples but their
expectations. Hence it is approximate but tends to
converge much faster than Gibbs sampling will mix.
It is convenient to visualize an undirected factor
graph (Fig. 1), in which each factor is connected
to the variables it depends on. Many factors may
connect to?and hence influence?a given variable
such as L34. If X is a variable or a factor, N (X)
denotes its set of neighbors.
4.2 What BP accomplishes
Given an input sentence W and a parameter vector
?, the collection of factors Fm defines a probabil-
ity distribution (1). The parser should determine the
values of the individual variables. In other words,
we would like to marginalize equation (1) to obtain
the distribution p(L34) over L34 = true vs. false,
the distribution p(T4) over tags, etc.
If the factor graph is acyclic, then BP com-
putes these marginal distributions exactly. Given
8Or, more precisely?this is the tricky part?based on ver-
sions of those other distributions that do not factor in L34?s re-
ciprocal influence on them. This prevents (e.g.) L34 and T3
from mutually reinforcing each other?s existing beliefs.
an HMM, for example, BP reduces to the forward-
backward algorithm.
BP?s estimates of these distributions are called be-
liefs about the variables. BP also computes be-
liefs about the factors, which are useful in learn-
ing ? (see ?7). E.g., if the model includes the factor
TAGLINKij , which is connected to variables Lij , Ti,
Tj , then BP will estimate the marginal joint distribu-
tion p(Lij , Ti, Tj) over (boolean, tag, tag) triples.
When the factor graph has loops, BP?s beliefs are
usually not the true marginals of equation (1) (which
are in general intractable to compute). Indeed, BP?s
beliefs may not be the true marginals of any distribu-
tion p(A) over assignments, i.e., they may be glob-
ally inconsistent. All BP does is to incrementally
adjust the beliefs till they are at least locally con-
sistent: e.g., the beliefs at factors TAGLINKij and
TAGLINKik must both imply9 the same belief about
variable Ti, their common neighbor.
4.3 The BP algorithm
This iterated negotiation among the factors is han-
dled by message passing along the edges of the fac-
tor graph. A message to or from a variable is a (pos-
sibly unnormalized) probability distribution over the
values of that variable.
The variable V sends a message to factor F , say-
ing ?My other neighboring factors G jointly suggest
that I have posterior distribution qV?F (assuming
that they are sending me independent evidence).?
Meanwhile, factor F sends messages to V , saying,
?Based on my factor function and the messages re-
ceived from my other neighboring variables U about
their values (and assuming that those messages are
independent), I suggest you have posterior distribu-
tion rF?V over your values.?
To be more precise, BP at each iteration k (until
convergence) updates two kinds of messages:
q(k+1)V?F (v) = ?
?
G?N (V )\F
r(k)G?V (v) (3)
from variables to factors, and
r(k+1)F?V (v) = ?
?
A s.t. A[V ]=v
F (A)
?
U?N (F )\V
q(k)U?F (A[U ])
(4)
9In the sense that marginalizing the belief p(Lij , Ti, Tj) at
the factor yields the belief p(Ti) at the variable.
148
from factors to variables. Each message is a proba-
bility distribution over values v of V , normalized by
a scaling constant ?. Alternatively, messages may be
left as unnormalized distributions, choosing ? 6= 1
only as needed to prevent over- or underflow. Mes-
sages are initialized to uniform distributions.
Whenever we wish, we may compute the beliefs
at V and F :
b(k+1)V? (v)
def
= ?
?
G?N (V )
r(k)G?V (v) (5)
b(k+1)F? (A)
def
= ? F (A)
?
U?N (F )
q(k)U?F (A[U ]) (6)
These beliefs do not truly characterize the ex-
pected behavior of Gibbs sampling (?4.1), since the
products in (5)?(6) make conditional independence
assumptions that are valid only if the factor graph
is acyclic. Furthermore, on cyclic (?loopy?) graphs,
BP might only converge to a local optimum (Weiss
and Freedman, 2001), or it might not converge at all.
Still, BP often leads to good, fast approximations.
5 Achieving Low Asymptotic Runtime
One iteration of standard BP simply updates all the
messages as in equations (3)?(4): one message per
edge of the factor graph.
Therefore, adding new factors to the model in-
creases the runtime per iteration additively, by in-
creasing the number of messages to update. We
believe this is a compelling advantage over dy-
namic programming?in which new factors usually
increase the runtime and space multiplicatively by
exploding the number of distinct items.10
5.1 Propagators for local constraints
But how long does updating each message take? The
runtime of summing over all assignments
?
A in
10For example, with unknown tags T , a model with
PTREE+TAGLINK will take only O(n3 + n2g2) time for BP,
compared to O(n3g2) time for dynamic programming (Eisner
& Satta 1999). Adding TRIGRAM, which is string-local rather
than tree-local, will increase this only to O(n3 + n2g2 + ng3),
compared to O(n3g6) for dynamic programming.
Even more dramatic, adding the SIB family of O(n3)
PAIRij,ik factors will add only O(n3) to the runtime of BP
(Table 1). By contrast, the runtime of dynamic programming
becomes exponential, because each item must record its head-
word?s full set of current children.
equation (4) may appear prohibitive. Crucially, how-
ever, F (A) only depends on the values in A of F ?s
its neighboring variables N (F ). So this sum is pro-
portional to a sum over restricted assignments to just
those variables.11
For example, computing a message from
TAGLINKij ? Ti only requires iterating over all
(boolean, tag, tag) triples.12 The runtime to update
that message is therefore O(2 ? |T | ? |T |).
5.2 Propagators for global constraints
The above may be tolerable for a ternary factor. But
how about global factors? EXACTLY1j has n neigh-
boring boolean variables: surely we cannot iterate
over all 2n assignments to these! TREE is even
worse, with 2O(n
2) assignments to consider. We will
give specialized algorithms for handling these sum-
mations more efficiently.
A historical note is in order. Traditional constraint
satisfaction corresponds to the special case of (1)
where all factors Fm are hard constraints (with val-
ues in {0, 1}). In that case, loopy BP reduces to
an algorithm for generalized arc consistency (Mack-
worth, 1977; Bessie`re and Re?gin, 1997; Dechter,
2003), and updating a factor?s outgoing messages is
known as constraint propagation. Re?gin (1994)
famously introduced an efficient propagator for
a global constraint, ALLDIFFERENT, by adapting
combinatorial bipartite matching algorithms.
In the same spirit, we will demonstrate efficient
propagators for our global constraints, e.g. by adapt-
ing combinatorial algorithms for weighted parsing.
We are unaware of any previous work on global fac-
tors in sum-product BP, although for max-product
BP,13 Duchi et al (2007) independently showed
that a global 1-to-1 alignment constraint?a kind
of weighted ALLDIFFERENT?permits an efficient
propagator based on weighted bipartite matching.
5.3 Constraint propagators for parsing
Table 1 shows our asymptotic runtimes for all fac-
tors in ??3.3?3.4. Remember that if several of these
11The constant of proportionality may be folded into ?; it is
the number of assignments to the other variables.
12Separately for each value v of Ti, get v?s probability by
summing over assignments to (Lij , Ti, Tj) s.t. Ti = v.
13Max-product replaces the sums in equations (3)?(6) with
maximizations. This replaces the forward-backward algorithm
with its Viterbi approximation.
149
factor degree runtime count runtime
family (each) (each) (total)
TREE O(n2) O(n3) 1 O(n3)
PTREE O(n2) O(n3) 1 O(n3)
EXACTLY1 O(n) O(n) n O(n2)
ATMOST1 O(n) O(n) n O(n2)
NOT2 2 O(1) O(n3) O(n3)
NO2CYCLE 2 O(1) O(n2) O(n2)
LINK 1 O(1) O(n2) O(n2)
GRAND 2 O(1) O(n3) O(n3)
SIB 2 O(1) O(n3) O(n3)
CHILDSEQ O(n) O(n2) O(n) O(n3)
NOCROSS O(n) O(n) O(n2) O(n3)
TAG 1 O(g) O(n) O(ng)
TAGLINK 3 O(g2) O(n2) O(n2g2)
TRIGRAM O(n) O(ng3) 1 O(ng3)
Table 1: Asymptotic runtimes of the propagators for var-
ious factors (where n is the sentence length and g is the
size of the tag set T ). An iteration of standard BP propa-
gates through each factor once. Running a factor?s prop-
agator will update all of its outgoing messages, based on
its current incoming messages.
factors are included, the total runtime is additive.14
Propagating the local factors is straightforward
(?5.1). We now explain how to handle the global
factors. Our main trick is to work backwards from
marginal beliefs. Let F be a factor and V be one
of its neighboring variables. At any time, F has a
marginal belief about V (see footnote 9),
b(k+1)F? (V = v) =
?
A s.t. A[V ]=v
b(k+1)F? (A) (7)
a sum over (6)?s products of incoming messages. By
the definition of rF?V in (4), and distributivity, we
can also express the marginal belief (7) as a point-
wise product of outgoing and incoming messages15
b(k+1)F? (V = v) = r
(k+1)
F?V (v) ? q
(k)
V?F (v) (8)
up to a constant. If we can quickly sum up the
marginal belief (7), then (8) says we can divide out
each particular incoming message q(k)V?F to obtain
its corresponding outgoing message r(k+1)F?V .
14We may ignore the cost of propagators at the variables.
Each outgoing message from a variable can be computed in
time proportional to its size, which may be amortized against
the cost of generating the corresponding incoming message.
15E.g., the familiar product of forward and backward mes-
sages that is used to extract posterior marginals from an HMM.
Note that the marginal belief and both messages
are unnormalized distributions over values v of V .
F and k are clear from context below, so we simplify
the notation so that (7)?(8) become
b(V = v) =
?
A s.t. A[V ]=v
b(A) = rV (v) ? qV (v)
TRIGRAM must sum over assignments to the tag
sequence T . The belief (6) in a given assignment
is a product of trigram scores (which play the role
of transition weights) and incoming messages qTj
(playing the role of emission weights). The marginal
belief (7) needed above, b(Ti = t), is found by sum-
ming over assignments where Ti = t. All marginal
beliefs are computed together in O(ng3) total time
by the forward-backward algorithm.16
EXACTLY1j is a sparse hard constraint. Even
though there are 2n assignments to its n neighboring
variables {Lij}, the factor function returns 1 on only
n assignments and 0 on the rest. In fact, for a given i,
b(Lij = true) in (7) is defined by (6) to have exactly
one non-zero summand, in whichA puts Lij = true
and all other Li?j = false. We compute the marginal
beliefs for all i together in O(n) total time:
1. Pre-compute pi
def
=
?
i qLij (false).
17
2. For each i, compute the marginal belief
b(Lij = true) as pi ? q?Lij , where q?Lij ? R de-
notes the odds ratio qLij (true)/qLij (false).
18
3. The partition function b() denotes
?
A b(A);
compute it in this case as
?
i b(Lij = true).
4. For each i, compute b(Lij = false) by subtrac-
tion, as b()? b(Lij = true).
TREE and PTREE must sum over assignments to
the O(n2) neighboring variables {Lij}. There are
now exponentially many non-zero summands, those
in whichA corresponds to a valid tree. Nonetheless,
16Which is itself an exact BP algorithm, but on a different
graph?a junction tree formed from the graph of TRIGRAM sub-
factors. Each variable in the junction tree is a bigram. If we had
simply replaced the global TRIGRAM factor with its subfactors
in the full factor graph, we would have had to resort to General-
ized BP (Yedidia et al, 2004) to obtain the same exact results.
17But taking pi = 1 gives the same results, up to a constant.
18As a matter of implementation, this odds ratio q?Lij can be
used to represent the incoming message qLij everywhere.
150
we can follow the same approach as for EXACTLY1.
Steps 1 and 4 are modified to iterate over all i, j such
that Lij is a variable. In step 3, the partition function?
A b(A) is now pi times the total weight of all trees,
where the weight of a given tree is the product of the
q?Lij values of its n edges. In step 2, the marginal
belief b(Lij = true) is now pi times the total weight
of all trees having edge i? j.
We perform these combinatorial sums by calling a
first-order parsing algorithm, with edge weights q?ij .
Thus, as outlined in ?2, a first-order parser is called
each time we propagate through the global TREE or
PTREE constraint, using edge weights that include
the first-order LINK factors but also multiply in any
current messages from higher-order factors.
The parsing algorithm simultaneously computes
the partition function b(), and all O(n2) marginal
beliefs b(Lij = true). For PTREE (projective), it
is the inside-outside version of a dynamic program-
ming algorithm (Eisner, 1996). For TREE (non-
projective), Koo et al (2007) and Smith and Smith
(2007) show how to employ the matrix-tree theorem.
In both cases, the total time is O(n3).19
NOCROSSj` must sum over assignments to O(n)
neighboring variables {Lij} and {Lk`}. The non-
zero summands are assignments where j and `
each have exactly one parent. At step 1, pi
def
=
?
i qLij (false) ?
?
k qLk`(false). At step 2, the
marginal belief b(Lij = true) sums over the n non-
zero assignments containing i ? j. It is pi ? q?Lij ??
k q?Lk` ? PAIRij,k`, where PAIRij,k` is xj` if i ? j
crosses k ? ` and is 1 otherwise. xj` is some factor
value defined by equation (2) to penalize or reward
the crossing. Steps 3?4 are just as in EXACTLY1j .
The question is how to compute b(Lij = true) for
each i in only O(1) time,20 so that we can propagate
each of the O(n2) NOCROSSj` in O(n) time. This
is why we allowed xj` to depend only on j, `. We
can rewrite the sum b(Lij = true) as
pi ? q?Lij ? (xj` ?
?
crossing k
q?Lk` + 1 ?
?
noncrossing k
q?Lk`) (9)
19A dynamic algorithm could incrementally update the out-
going messages if only a few incoming messages have changed
(as in asynchronous BP). In the case of TREE, dynamic matrix
inverse allows us to update any row or column (i.e., messages
from all parents or children of a given word) and find the new
inverse in O(n2) time (Sherman and Morrison, 1950).
20Symmetrically, we compute b(Lk` = true) for each k.
To find this in O(1) time, we precompute for each
` an array of partial sums Q`[s, t]
def
=
?
s?k?t q?Lk` .
SinceQ`[s, t] = Q`[s, t?1]+ q?Lt` , we can compute
each entry in O(1) time. The total precomputation
time over all `, s, t is then O(n3), with the array Q`
shared across all factors NOCROSSj?`. The crossing
sum is respectivelyQ`[0, i?1]+Q`[j+1, n],Q`[i+
1, j ? 1], or 0 according to whether ` ? (i, j), ` /?
[i, j], or ` = i.21 The non-crossing sum is Q`[0, n]
minus the crossing sum.
CHILDSEQi , like TRIGRAM, is propagated by a
forward-backward algorithm. In this case, the al-
gorithm is easiest to describe by replacing CHILD-
SEQi in the factor graph by a collection of local
subfactors, which pass messages in the ordinary
way.22 Roughly speaking,23 at each j ? [1, n],
we introduce a new variable Cij?a hidden state
whose value is the position of i?s previous child,
if any (so 0 ? Cij < j). So the ternary sub-
factor on (Cij , Lij , Ci,j+1) has value 1 if Lij =
false and Ci,j+1 = Ci,j ; a sibling-bigram score
(PAIRiCij ,iCi,j+1) if Lij = true and Ci,j+1 = j; and
0 otherwise. The sparsity of this factor, which is 0
almost everywhere, is what gives CHILDSEQi a total
runtime of O(n2) rather than O(n3). It is equivalent
to forward-backward on an HMM with n observa-
tions (the Lij) and n states per observation (the Cj),
with a deterministic (thus sparse) transition function.
6 Decoding Trees
BP computes local beliefs, e.g. the conditional prob-
ability that a link Lij is present. But if we wish
to output a single well-formed dependency tree, we
need to find a single assignment to all the {Lij} that
satisfies the TREE (or PTREE) constraint.
Our final belief about the TREE factor is a distri-
bution over such assignments, in which a tree?s prob-
ability is proportional to the probability of its edge
weights q?Lij (incoming messages). We could simply
return the mode of this distribution (found by using
a 1-best first-order parser) or the k-best trees, or take
samples.
21There are no NOCROSSj` factors with ` = j.
22We still treat CHILDSEQi as a global factor and compute all
its correct outgoing messages on a single BP iteration, via serial
forward and backward sweeps through the subfactors. Handling
the subfactors in parallel, (3)?(4), would need O(n) iterations.
23Ignoring the treatment of boundary symbols ?#? (see ?3.4).
151
In our experiments, we actually take the edge
weights to be not the messages q?Lij from the links,
but the full beliefs b?Lij at the links (where b?Lij
def
=
log bLij (true)/bLij (false)). These are passed into a
fast algorithm for maximum spanning tree (Tarjan,
1977) or maximum projective spanning tree (Eis-
ner, 1996). This procedure is equivalent to minimum
Bayes risk (MBR) parsing (Goodman, 1996) with a
dependency accuracy loss function.
Notice that the above decoding approaches do not
enforce any hard constraints other than TREE in the
final output. In addition, they only recover values
of the Lij variables. They marginalize over other
variables such as tags and link roles. This solves
the problem of ?nuisance? variables (which merely
fragment probability mass among refinements of a
parse). On the other hand, it may be undesirable for
variables whose values we desire to recover.24
7 Training
Our training method also uses beliefs computed by
BP, but at the factors. We choose the weight vector
? by maximizing the log-probability of training data
24An alternative is to attempt to find the most probable
(?MAP?) assignment to all variables?using the max-product
algorithm (footnote 13) or one of its recent variants. The esti-
mated marginal beliefs become ?max marginals,? which assess
the 1-best assignment consistent with each value of the variable.
We can indeed build max-product propagators for our global
constraints. PTREE still propagates in O(n3) time: simply
change the first-order parser?s semiring (Goodman, 1999) to use
max instead of sum. TREE requires O(n4) time: it seems that
the O(n2) max marginals must be computed separately, each
requiring a separate call to an O(n2) maximum spanning tree
algorithm (Tarjan, 1977).
If max-product BP converges, we may simply output each
variable?s favorite value (according to its belief), if unique.
However, max-product BP tends to be unstable on loopy graphs,
and we may not wish to wait for full convergence in any case. A
more robust technique for extracting an assignment is to mimic
Viterbi decoding, and ?follow backpointers? of the max-product
computation along some spanning subtree of the factor graph.
A slower but potentially more stable alternative is determin-
istic annealing. Replace each factor Fm(A) with Fm(A)1/T ,
where T > 0 is a temperature. As T ? 0 (?quenches?), the
distribution (1) retains the same mode (the MAP assignment),
but becomes more sharply peaked at the mode, and sum-product
BP approaches max-product BP. Deterministic annealing runs
sum-product BP while gradually reducing T toward 0 as it it-
erates. By starting at a high T and reducing T slowly, it often
manages in practice to find a good local optimum. We may then
extract an assignment just as we do for max-product.
under equation (1), regularizing only by early stop-
ping. If all variables are observed in training, this
objective function is convex (as for any log-linear
model).
The difficult step in computing the gradient of
our objective is finding ?? logZ, where Z in equa-
tion (1) is the normalizing constant (partition func-
tion) that sums over all assignments A. (Recall that
Z, like each Fm, depends implicitly on W and ?.)
As usual for log-linear models,
?? logZ =
?
m
Ep(A)[??Fm(A)] (10)
Since ??Fm(A) only depends on the assignment
A?s values for variables that are connected to Fm
in the factor graph, its expectation under p(A) de-
pends only on the marginalization of p(A) to those
variables jointly. Fortunately, BP provides an esti-
mate of that marginal distribution, namely, its belief
about the factor Fm, given W and ? (?4.2).25
Note that the hard constraints do not depend on ?
at all; so their summands in equation (10) will be 0.
We employ stochastic gradient descent (Bottou,
2003), since this does not require us to compute
the objective function itself but only to (approxi-
mately) estimate its gradient as explained above. Al-
ternatively, given any of the MAP decoding proce-
dures from ?6, we could use an error-driven learning
method such as the perceptron or MIRA.26
8 Experiments
We asked: (1) For projective parsing, where higher-
order factors have traditionally been incorporated
into slow but exact dynamic programming (DP),
what are the comparative speed and quality of the
BP approximation? (2) How helpful are such higher-
order factors?particularly for non-projective pars-
ing, where BP is needed to make them tractable?
(3) Do our global constraints (e.g., TREE) contribute
to the goodness of BP?s approximation?
25One could use coarser estimates at earlier stages of training,
by running fewer iterations of BP.
26The BP framework makes it tempting to extend an MRF
model with various sorts of latent variables, whose values are
not specified in training data. It is straightforward to train under
these conditions. When counting which features fire on a train-
ing parse or (for error-driven training) on an current erroneous
parse, we can find expected counts if these parses are not fully
observed, by using BP to sum over latent variables.
152
0 10 20 30 40 50 60 70
0
20
40
60
Sentence length
Av
era
ge
 pa
rsi
ng
 tim
e i
n s
ec
on
ds
MBR by DP
Viterbi DP
2 iterations of BP
3 iterations of BP
5 iterations of BP
10 iterations of BP
Figure 2: Runtime of BP parser on various sentence
lengths compared to O(n4) dynamic programming.
8.1 Data
We trained and tested on three languages from the
CoNLL Dependency Parsing Shared Task (Nivre et
al., 2007). The English data for that task were
converted from the Penn Treebank to dependen-
cies using a trace-recovery algorithm that induced
some very slight non-projectivity?about 1% of
links crossed other links. Danish is a slightly more
non-projective language (3% crossing links). Dutch
is the most non-projective language in the corpus
(11%). In all cases, the test input W consists of
part-of-speech-tagged words, so T variables were
not used.
8.2 Features
Although BP makes it cheap to incorporate many
non-local features and latent variables at once, we
kept our models relatively simple in this paper.
Our first-order LINKij factors replicate McDon-
ald et al (2005). Following equation (2), they are
defined using binary features that look at words i
and j, the distance j ? i, and the tags (provided in
W ) of words at, around, and between i and j.
Our second-order features are similar. In the
GRAND factors, features fire for particular triples
of tags and of coarse tags. A feature also fires if
the grandparent falls between the child and parent,
inducing crossing dependency links. The CHILD-
SEQ factors included features for tags, and like-
wise coarse tags, on adjacent sibling pairs and
0 10 20 30 40
0
20
40
60
80
10
0
12
0
14
0
Sentence length
Av
era
ge
 pa
rsi
ng
 tim
e i
n s
ec
on
ds
MBR by DP
Viterbi DP
2 iterations of BP
3 iterations of BP
5 iterations of BP
10 iterations of BP
Figure 3: Runtime of BP parser on various sentence
lengths compared to O(n5) dynamic programming. DP
is so slow for length > 45 that we do not even show it.
parent-sibling-sibling triples. Each of these fea-
tures also have versions that were conjoined with
link direction?pairs of directions in the grandpar-
ent case?or with signed link length of the child or
farther sibling. Lengths were binned per McDonald
et al (2005). The NOCROSSj` factors consider the
tag and coarse tag attributes of the two child words
j and `, separately or jointly.
8.3 Experimental procedures
We trained all models using stochastic gradient de-
scent (?7). SGD initialized ~? = 0 and ran for 10 con-
secutive passes over the data; we picked the stopping
point that performed best on held-out data.
When comparing runtimes for projective parsers,
we took care to produce comparable implementa-
tions. All beliefs and dynamic programming items
were stored and indexed using the high-level Dyna
language,27 while all inference and propagation was
written in C++. The BP parser averaged 1.8 seconds
per sentence for non-projective parsing and 1.5 sec-
onds per sentence for projective parsing (1.2 and 0.9
seconds/sentence for ? 40 words), using our stan-
dard setup, which included five iterations of BP and
the final MBR tree decoding pass.
In our tables, we boldface the best result in each
column along with any results that are not signifi-
cantly worse (paired permutation test, p < .05).
27This dominates runtime, and probably slows down all our
parsers by a factor of 4?11 owing to known inefficiencies in the
Dyna prototype we used (Eisner et al, 2005).
153
0 20 40 60
0.0
0.1
0.2
0.3
0.4
Parsing time in seconds
Err
or 
rel
ati
ve
 to
 ex
ac
t M
BR
Input length
40 words
50 words
60 words
70 words
2 iterations of BP
3 iterations of BP
5 iterations of BP
10 iterations of BP
MBR DP
Figure 4: Runtime vs. search error after different num-
bers of BP iterations. This shows the simpler model of
Fig. 2, where DP is still relatively fast.
8.4 Faster higher-order projective parsing
We built a first-order projective parser?one that
uses only factors PTREE and LINK?and then com-
pared the cost of incorporating second-order factors,
GRAND and CHILDSEQ, by BP versus DP.28
Under DP, the first-order runtime of O(n3) is in-
creased to O(n4) with GRAND, and to O(n5) when
we add CHILDSEQ as well. BP keeps runtime down
to O(n3)?although with a higher constant factor,
since it takes several rounds to converge, and since
it computes more than just the best parse.29
Figures 2?3 compare the empirical runtimes for
various input sentence lengths. With only the
GRAND factor, exact DP can still find the Viterbi
parse (though not the MBR parse29) faster than ten
iterations of the asymptotically better BP (Fig. 2),
at least for sentences with n ? 75. However, once
we add the CHILDSEQ factor, BP is always faster?
dramatically so for longer sentences (Fig. 3). More
complex models would widen BP?s advantage.
Fig. 4 shows the tradeoff between runtime and
search error of BP in the former case (GRAND only).
To determine BP?s search error at finding the MBR
parse, we measured its dependency accuracy not
28We trained these parsers using exact DP, using the inside-
outside algorithm to compute equation (10). The training and
test data were English, and for this section we filtered out sen-
tences with non-projective links.
29Viterbi parsing in the log domain only needs the (max,+)
semiring, whereas both BP and any MBR parsing must use the
slower (+, log+) so that they can compute marginals.
Danish Dutch English
(a) TREE+LINK 85.5 87.3 88.6
+NOCROSS 86.1 88.3 89.1
+GRAND 86.1 88.6 89.4
+CHILDSEQ 86.5 88.5 90.1
(b) Proj. DP 86.0 84.5 90.2
+hill-climbing 86.1 87.6 90.2
Table 2: (a) Percent unlabeled dependency accuracy for
various non-projective BP parsers (5 iterations only),
showing the cumulative contribution of different features.
(b) Accuracy for an projective DP parser with all features.
For relatively non-projective languages (Danish and espe-
cially Dutch), the exact projective parses can be improved
by non-projective hill-climbing?but in those cases, just
running our non-projective BP is better and faster.
against the gold standard, but against the optimal
MBR parse under the model, which DP is able to
find. After 10 iterations, the overall macro-averaged
search error compared to O(n4) DP MBR is 0.4%;
compared to O(n5) (not shown), 2.4%. More BP
iterations may help accuracy. In future work, we
plan to compare BP?s speed-accuracy curve on more
complex projective models with the speed-accuracy
curve of pruned or reranked DP.
8.5 Higher-order non-projective parsing
The BP approximation can be used to improve
the accuracy of non-projective parsing by adding
higher-order features. These would be NP-hard to
incorporate exactly; DP cannot be used.
We used BP with a non-projective TREE factor
to train conditional log-linear parsing models of two
highly non-projective languages, Danish and Dutch,
as well as slightly non-projective English (?8.1).
In all three languages, the first-order non-projective
parser greatly overpredicts the number of crossing
links. We thus added NOCROSS factors, as well
as GRAND and CHILDSEQ as before. All of these
significantly improve the first-order baseline, though
not necessarily cumulatively (Table 2).
Finally, Table 2 compares loopy BP to a previ-
ously proposed ?hill-climbing? method for approx-
imate inference in non-projective parsing McDon-
ald and Pereira (2006). Hill-climbing decodes our
richest non-projective model by finding the best pro-
jective parse under that model?using slow, higher-
order DP?and then greedily modifies words? par-
ents until the parse score (1) stops improving.
154
Decoding Danish Dutch English
NOT2 81.8 (76.7) 83.3 (75.0) 87.5 (66.4)
ATMOST1 85.4 (82.2) 87.3 (86.3) 88.5 (84.6)
EXACTLY1 85.7 (85.0) 87.0 (86.7) 88.6 (86.0)
+ NO2CYCLE 85.0 (85.2) 86.2 (86.7) 88.5 (86.2)
TREE 85.5 (85.5) 87.3 (87.3) 88.6 (88.6)
PTREE 85.8 83.9 88.8
Table 3: After training a non-projective first-order model
with TREE, decoding it with weaker constraints is asymp-
totically faster (except for NOT2) but usually harm-
ful. (Parenthetical numbers show that the harm is com-
pounded if the weaker constraints are used in training
as well; even though this matches training to test con-
ditions, it may suffer more from BP?s approximate gradi-
ents.) Decoding the TREE model with the even stronger
PTREE constraint can actually be helpful for a more pro-
jective language. All results use 5 iterations of BP.
BP for non-projective languages is much faster
and more accurate than the hill-climbing method.
Also, hill-climbing only produces an (approximate)
1-best parse, but BP also obtains (approximate)
marginals of the distribution over all parses.
8.6 Importance of global hard constraints
Given the BP architecture, do we even need the hard
TREE constraint? Or would it suffice for more local
hard constraints to negotiate locally via BP?
We investigated this for non-projective first-order
parsing. Table 3 shows that global constraints are
indeed important, and that it is essential to use TREE
during training. At test time, the weaker but still
global EXACTLY1 may suffice (followed by MBR
decoding to eliminate cycles), for total time O(n2).
Table 3 includes NOT2, which takes O(n3) time,
merely to demonstrate how the BP approximation
becomes more accurate for training and decoding
when we join the simple NOT2 constraints into more
global ATMOST1 constraints. This does not change
the distribution (1), but makes BP enforce stronger
local consistency requirements at the factors, rely-
ing less on independence assumptions. In general,
one can get better BP approximations by replacing a
group of factors Fm(A) with their product.30
The above experiments concern gold-standard
30In the limit, one could replace the product (1) with a sin-
gle all-purpose factor; then BP would be exact?but slow. (In
constraint satisfaction, joining constraints similarly makes arc
consistency slower but better at eliminating impossible values.)
accuracy under a given first-order, non-projective
model. Flipping all three of these parameters for
Danish, we confirmed the pattern by instead mea-
suring search error under a higher-order, projective
model (PTREE+LINK+GRAND), when PTREE was
weakened during decoding. Compared to the MBR
parse under that model, the search errors from de-
coding with weaker hard constraints were 2.2% for
NOT2, 2.1% for EXACTLY1, 1.7% for EXACTLY1
+ NO2CYCLE, and 0.0% for PTREE.
9 Conclusions and Future Work
Belief propagation improves non-projective depen-
dency parsing with features that would make ex-
act inference intractable. For projective parsing, it
is significantly faster than exact dynamic program-
ming, at the cost of small amounts of search error,
We are interested in extending these ideas to
phrase-structure and lattice parsing, and in try-
ing other higher-order features, such as those used
in parse reranking (Charniak and Johnson, 2005;
Huang, 2008) and history-based parsing (Nivre and
McDonald, 2008). We could also introduce new
variables, e.g., nonterminal refinements (Matsuzaki
et al, 2005), or secondary linksMij (not constrained
by TREE/PTREE) that augment the parse with repre-
sentations of control, binding, etc. (Sleator and Tem-
perley, 1993; Buch-Kromann, 2006).
Other parsing-like problems that could be at-
tacked with BP appear in syntax-based machine
translation. Decoding is very expensive with a syn-
chronous grammar composed with an n-gram lan-
guage model (Chiang, 2007)?but our footnote 10
suggests that BP might incorporate a language
model rapidly. String alignment with synchronous
grammars is quite expensive even for simple syn-
chronous formalisms like ITG (Wu, 1997)?but
Duchi et al (2007) show how to incorporate bipar-
tite matching into max-product BP.
Finally, we can take advantage of improvements
to BP proposed in the context of other applications.
For example, instead of updating all messages in
parallel at every iteration, it is empirically faster to
serialize updates using a priority queue (Elidan et
al., 2006; Sutton and McCallum, 2007).31
31These methods need alteration to handle our global propa-
gators, which do update all their outgoing messages at once.
155
References
C. Bessie`re and J.-C. Re?gin. 1997. Arc consistency for
general constraint networks: preliminary results. In
IJCAI, pages 398?404.
L. Bottou. 2003. Stochastic learning. In Advanced Lec-
tures in Machine Learning, pages 146?168. Springer.
A. Braunstein, M. Mezard, and R. Zecchina. 2005. Sur-
vey propagation: An algorithm for satisfiability. Ran-
dom Structures and Algorithms, 27:201?226.
M. Buch-Kromann. 2006. Discontinuous Grammar.
A Model of Human Parsing and Language Acquisi-
tion?. Dr.ling.merc. dissertation, Copenhagen Busi-
ness School.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
ACL, pages 173?180.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
R. Dechter. 2003. Constraint Processing. Morgan Kauf-
mann.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. In NIPS 2006, pages 369?376.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In ACL, pages 457?480.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Compiling
comp ling: Weighted dynamic programming and the
dyna language. In HLT-EMNLP, pages 281?290.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In COLING.
G. Elidan, I. McGraw, and D. Koller. 2006. Resid-
ual belief propagation: Informed scheduling for asyn-
chronous message passing. In UAI.
J. T. Goodman. 1996. Parsing algorithms and metrics.
In ACL, pages 177?183.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25(4):573?605.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In ACL, pages 586?594.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the Matrix-Tree The-
orem. In EMNLP-CoNLL.
D. MacKay. 2003. Information Theory, Inference, and
Learning Algorithms. Cambridge.
A. Mackworth. 1977. Consistency in networks of rela-
tions. Artificial Intelligence, 8(1):99?118.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL, pages 75?82.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL.
J.-C. Re?gin. 1994. A filtering algorithm for constraints
of difference in csps. In AAAI, pages 362?367.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP, pages 129?137.
J. Sherman and W. J. Morrison. 1950. Adjustment of an
inverse matrix corresponding to a change in one ele-
ment of a given matrix. Ann. Math. Stat., 21:124?127.
D. Sleator and D. Temperley. 1993. Parsing English with
a link grammar. In IWPT, pages 277?291, August.
D. A. Smith and N. A. Smith. 2007. Probabilistic mod-
els of nonprojective dependency trees. In EMNLP-
CoNLL.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information ex-
traction. In ICML Workshop on Statistical Relational
Learning.
C. Sutton and A. McCallum. 2007. Improved dynamic
schedules for belief propagation. In UAI.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. In ICML.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7:25?35.
R. W. Tromble and J. Eisner. 2006. A fast finite-state
relaxation method for enforcing global constraints on
sequence decoding. In HLT-NAACL, pages 423?430.
Y. Weiss and W. T. Freedman. 2001. On the optimal-
ity of solutions of the max-product belief propagation
algorithm in arbitrary graphs. IEEE Transactions on
Information Theory, 47.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. CL,
23(3):377?404.
J. S. Yedidia, W. T. Freeman, and Y. Weiss. 2004. Con-
structing free-energy approximations and generalized
belief approximation algorithms. MERL TR2004-
040, Mitsubishi Electric Research Laboratories.
156
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1080?1089,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Latent-Variable Modeling of String Transductions
with Finite-State Methods?
Markus Dreyer and Jason R. Smith and Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{markus,jsmith,jason}@cs.jhu.edu
Abstract
String-to-string transduction is a central prob-
lem in computational linguistics and natural
language processing. It occurs in tasks as di-
verse as name transliteration, spelling correc-
tion, pronunciation modeling and inflectional
morphology. We present a conditional log-
linear model for string-to-string transduction,
which employs overlapping features over la-
tent alignment sequences, and which learns la-
tent classes and latent string pair regions from
incomplete training data. We evaluate our ap-
proach on morphological tasks and demon-
strate that latent variables can dramatically
improve results, even when trained on small
data sets. On the task of generating mor-
phological forms, we outperform a baseline
method reducing the error rate by up to 48%.
On a lemmatization task, we reduce the error
rates in Wicentowski (2002) by 38?92%.
1 Introduction
A recurring problem in computational linguistics
and language processing is transduction of charac-
ter strings, e.g., words. That is, one wishes to model
some systematic mapping from an input string x to
an output string y. Applications include:
? phonology: underlying representation ? surface
representation
? orthography: pronunciation? spelling
? morphology: inflected form ? lemma, or differ-
ently inflected form
? fuzzy name matching (duplicate detection) and
spelling correction: spelling? variant spelling
?This work was supported by the Human Language Tech-
nology Center of Excellence and by National Science Founda-
tion grant No. 0347822 to the final author. We would also like
to thank Richard Wicentowski for providing us with datasets for
lemmatization, and the anonymous reviewers for their valuable
feedback.
? lexical translation (cognates, loanwords, translit-
erated names): English word? foreign word
We present a configurable and robust framework
for solving such word transduction problems. Our
results in morphology generation show that the pre-
sented approach improves upon the state of the art.
2 Model Structure
A weighted edit distance model (Ristad and Yian-
ilos, 1998) would consider each character in isola-
tion. To consider more context, we pursue a very
natural generalization. Given an input x, we evalu-
ate a candidate output y by moving a sliding window
over the aligned (x, y) pair. More precisely, since
many alignments are possible, we sum over all these
possibilities, evaluating each alignment separately.1
At each window position, we accumulate log-
probability based on the material that appears within
the current window. The window is a few charac-
ters wide, and successive window positions over-
lap. This stands in contrast to a competing approach
(Sherif and Kondrak, 2007; Zhao et al, 2007)
that is inspired by phrase-based machine translation
(Koehn et al, 2007), which segments the input string
into substrings that are transduced independently, ig-
noring context.2
1At the other extreme, Freitag and Khadivi (2007) use no
alignment; each feature takes its own view of how (x, y) relate.
2We feel that this independence is inappropriate. By anal-
ogy, it would be a poor idea for a language model to score a
string highly if it could be segmented into independently fre-
quent n-grams. Rather, language models use overlapping n-
grams (indeed, it is the language model that rescues phrase-
based MT from producing disjointed translations). We believe
phrase-based MT avoids overlapping phrases in the channel
model only because these would complicate the modeling of
reordering (though see, e.g., Schwenk et al (2007) and Casacu-
berta (2000)). But in the problems of section 1, letter reordering
is rare and we may assume it is local to a window.
1080
Figure 1: One of many possible alignment strings A for
the observed pair breaking/broke, enriched with latent
strings `1 and `2. Observed letters are shown in bold. The
box marks a trigram to be scored. See Fig. 2 for features
that fire on this trigram.
Joint n-gram models over the input and output di-
mensions have been used before, but not for mor-
phology, where we will apply them.3 Most notable
is the local log-linear grapheme-to-phoneme model
of Chen (2003), as well as generative models for
that task (Deligne et al (1995), Galescu and Allen
(2001), Bisani and Ney (2002)).
We advance that approach by adding new latent
dimensions to the (input, output) tuples (see Fig. 1).4
This enables us to use certain linguistically inspired
features and discover unannotated information. Our
features consider less or more than a literal n-gram.
On the one hand, we generalize with features that
abstract away from the n-gram window contents; on
the other, we specialize the n-gram with features that
make use of the added latent linguistic structure.
In section 5, we briefly sketch our framework for
concisely expressing and efficiently implementing
models of this form. Our framework uses familiar
log-linear techniques for stochastic modeling, and
weighted finite-state methods both for implementa-
tion and for specifying features. It appears general
enough to cover most prior work on word transduc-
tion. We imagine that it will be useful for future
work as well: one might easily add new, linguisti-
cally interesting classes of features, each class de-
fined by a regular expression.
2.1 Basic notation
We use an input alphabet ?x and output alphabet
?y. We conventionally use x ? ??x to denote the
input string and y ? ??y to denote the output string.
3Clark (2001) does use pair HMMs for morphology.
4Demberg et al (2007) similarly added extra dimensions.
However, their added dimensions were supervised, not latent,
and their model was a standard generative n-gram model whose
generalization was limited to standard n-gram smoothing.
There are many possible alignments between x
and y. We represent each as an alignment string
A ? ??
xy
, over an alignment alphabet of ordered
pairs, ?xy
def
= ((?x ? {})? (?y ? {}))? {(, )}.
For example, one alignment of x = breaking
with y = broke is the 9-character string A =
(b,b)(r,r)(e,o)(a, )(k,k)(,e)(i, )(n, )(g, ).
It is pictured in the first two lines of Fig. 1.
The remainder of Fig. 1 shows how we intro-
duce latent variables, by enriching the alignment
characters to be tuples rather than pairs. Let ?
def
=
(?xy ? ?`1 ? ?`2 ? ? ? ? ? ?`K ), where ?`i are al-
phabets used for the latent variables `i.
FSA and FST stand for ?finite-state acceptor? and
?finite-state transducer,? while WFSA and WFST
are their weighted variants. The ? symbol denotes
composition.
Let T be a relation and w a string. We write T [w]
to denote the image of w under T (i.e., range(w ?
T )), a set of 0 or more strings. Similarly, if W is a
weighted language (typically encoded by a WFSA),
we write W [w] to denote the weight of w in W .
Let pix ? ?? ? ??x denote the deterministic reg-
ular relation that projects an alignment string to its
corresponding input string, so that pix[A] = x. Sim-
ilarly, define piy ? ?? ? ??y so that piy[A] = y. Let
Axy be the set of alignment strings A compatible
with x and y; formally, Axy
def
= {A ? ?? : pix[A] =
x?piy[A] = y}. This set will range over all possible
alignments between x and y, and also all possible
configurations of the latent variables.
2.2 Log-linear modeling
We use a standard log-linear model whose features
are defined on alignment strings A ? Axy, allow-
ing them to be sensitive to the alignment of x and y.
Given a collection of features fi : ?? ? R with as-
sociated weights ?i ? R, the conditional likelihood
of the training data is
p?(y | x) =
?
A?Axy exp
?
i ?ifi(A)
?
y?
?
A?Axy?
exp
?
i ?ifi(A)
(1)
Given a parameter vector ?, we compute equa-
tion (1) using a finite-state machine. We define a
WFSA,U?, such thatU?[A] yields the unnormalized
probability u?(A)
def
= exp
?
i ?ifi(A) for any A ?
??. (See section 5 for the construction.) To obtain
1081
the numerator of equation (1), with its
?
A?Axy , we
sum over all paths in U? that are compatible with x
and y. That is, we build x ? pi?1x ? U? ? piy ? y and
sum over all paths. For the denominator we build the
larger machine x ? pi?1x ? U? and again compute the
pathsum. We use standard algorithms (Eisner, 2002)
to compute the pathsums as well as their gradients
with respect to ? for optimization (section 4.1).
Below, we will restrict our notion of valid align-
ment strings in ??. U? is constructed not to accept
invalid ones, thus assigning them probability 0.
Note that the possible output strings y? in the de-
nominator in equation (1) may have arbitrary length,
leading to an infinite summation over alignment
strings. Thus, for some values of ?, the sum in
the denominator diverges and the probability dis-
tribution is undefined. There exist principled ways
to avoid such ? during training. However, in our
current work, we simply restrict to finitely many
alignment strings (given x), by prohibiting as invalid
those with > k consecutive insertions (i.e., charac-
ters like (,a)).5 Finkel et al (2008) and others have
similarly bounded unary rule cycles in PCFGs.
2.3 Latent variables
The alignment between x and y is a latent ex-
planatory variable that helps model the distribution
p(y | x) but is not observed in training. Other latent
variables can also be useful. Morphophonological
changes are often sensitive to phonemes (whereas x
and y may consist of graphemes); syllable bound-
aries; a conjugation class; morpheme boundaries;
and the position of the change within the form.
Thus, as mentioned in section 2.1, we enrich the
alignment string A so that it specifies additional la-
tent variables to which features may wish to refer.
In Fig. 1, two latent strings are added, enabling the
features in Fig. 2(a)?(h). The first character is not
5We set k to a value between 1 and 3, depending on the tasks,
always ensuring that no input/output pairs observed in training
are excluded. The insertion restriction does slightly enlarge the
FSA U?: a state must keep track of the number of consecutive
 symbols in the immediately preceding x input, and for a few
states, this cannot be determined just from the immediately pre-
ceding (n ? 1)-gram. Despite this, we found empirically that
our approximation is at least as fast as the exact method of Eis-
ner (2002), who sums around cyclic subnetworks to numerical
convergence. Furthermore, our approximation does not require
us to detect divergence during training.
Figure 2: The boxes (a)-(h) represent some of the features
that fire on the trigram shown in Fig. 1. These features are
explained in detail in section 3.
just an input/output pair, but the 4-tuple (b,b,2,1).
Here, `1 indicates that this form pair (breaking /
broke) as a whole is in a particular cluster, or word
class, labeled with the arbitrary number 2. Notice in
Fig. 1 that the class 2 is visible in all local windows
throughout the string. It allows us to model how cer-
tain phenomena, e.g. the vowel change from ea to
o, are more likely in one class than in another. Form
pairs in the same class as the breaking / broke ex-
ample might include the following Germanic verbs:
speak, break, steal, tear, and bear.
Of course, word classes are latent (not labeled in
our training data). Given x and y, Axy will include
alignment strings that specify class 1, and others
that are identical except that they specify class 2;
equation (1) sums over both possibilities.6 In a valid
alignment stringA, `1 must be a constant string such
as 111... or 222..., as in Fig. 1, so that it spec-
ifies a single class for the entire form pair. See sec-
tions 4.2 and 4.3 for examples of what classes were
learned in our experiments.
The latent string `2 splits the string pair into num-
bered regions. In a valid alignment string, the re-
gion numbers must increase throughout `2, although
numbers may be skipped to permit omitted regions.
To guide the model to make a useful division into
regions, we also require that identity characters such
as (b,b) fall in even regions while change charac-
ters such as (e,o) (substitutions, deletions, or inser-
6The latent class is comparable to the latent variable on the
tree root symbol S in Matsuzaki et al (2005).
1082
tions) fall in odd regions.7 Region numbers must not
increase within a sequence of consecutive changes
or consecutive identities.8 In Fig. 1, the start of re-
gion 1 is triggered by e:o, the start of region 2 by
the identity k:k, region 3 by :e.
Allowing region numbers to be skipped makes it
possible to consistently assign similar labels to sim-
ilar regions across different training examples. Ta-
ble 2, for example, shows pairs that contain a vowel
change in the middle, some of which contain an ad-
ditional insertion of ge in the begining (verbinden
/ verbunden, reibt / gerieben). We expect the model
to learn to label the ge insertion with a 1 and vowel
change with a 3, skipping region 1 in the examples
where the ge insertion is not present (see section
4.2, Analysis).
In the next section we describe features over these
enriched alignment strings.
3 Features
One of the simplest ways of scoring a string is an n-
gram model. In our log-linear model (1), we include
ngram features fi(A), each of which counts the oc-
currences in A of a particular n-gram of alignment
characters. The log-linear framework lets us include
ngram features of different lengths, a form of back-
off smoothing (Wu and Khudanpur, 2000).
We use additional backoff features on alignment
strings to capture phonological, morphological, and
orthographic generalizations. Examples are found in
features (b)-(h) in Fig. 2. Feature (b) matches vowel
and consonant character classes in the input and
output dimensions. In the id/subst ngram feature,
we have a similar abstraction, where the character
classes ins, del, id, and subst are defined over in-
put/output pairs, to match insertions, deletions, iden-
tities (matches), and substitutions.
In string transduction tasks, it is helpful to in-
clude a language model of the target. While this
can be done by mixing the transduction model with
a separate language model, it is desirable to in-
clude a target language model within the transduc-
7This strict requirement means, perhaps unfortunately, that a
single region cannot accommodate the change ayc:xyz unless
the two y?s are not aligned to each other. It could be relaxed,
however, to a prior or an initialization or learning bias.
8The two boundary characters #, numbered 0 and max
(max=6 in our experiments), are neither changes nor identities.
tion model. We accomplish this by creating target
language model features, such as (c) and (g) from
Fig. 2, which ignore the input dimension. We also
have features which mirror features (a)-(d) but ig-
nore the latent classes and/or regions (e.g. features
(e)-(h)).
Notice that our choice of ? only permits mono-
tonic, 1-to-1 alignments, following Chen (2003).
We may nonetheless favor the 2-to-1 alignment
(ea,o) with bigram features such as (e,o)(a,). A
?collapsed? version of a feature will back off from
the specific alignment of the characters within a win-
dow: thus, (ea,o) is itself a feature. Currently, we
only include collapsed target language model fea-
tures. These ignore epsilons introduced by deletions
in the alignment, so that collapsed ok fires in a win-
dow that contains ok.
4 Experiments
We evaluate our model on two tasks of morphol-
ogy generation. Predicting morphological forms has
been shown to be useful for machine translation and
other tasks.9 Here we describe two sets of exper-
iments: an inflectional morphology task in which
models are trained to transduce verbs from one form
into another (section 4.2), and a lemmatization task
(section 4.3), in which any inflected verb is to be re-
duced to its root form.
4.1 Training and decoding
We train ? to maximize the regularized10 conditional
log-likelihood11
?
(x,y?)?C
log p?(y
? | x) + ||?||2/2?2, (2)
where C is a supervised training corpus. To max-
imize (2) during training, we apply the gradient-
based optimization method L-BFGS (Liu and No-
cedal, 1989).12
9E.g., Toutanova et al (2008) improve MT performance
by selecting correct morphological forms from a knowledge
source. We instead focus on generalizing from observed forms
and generating new forms (but see with rootlist in Table 3).
10The variance ?2 of the L2 prior is chosen by optimizing on
development data. We are also interested in trying an L1 prior.
11Alternatives would include faster error-driven methods
(perceptron, MIRA) and slower max-margin Markov networks.
12This worked a bit better than stochastic gradient descent.
1083
To decode a test example x, we wish to find
y? = argmaxy???y p?(y | x). Constructively, y? is the
highest-probability string in the WFSA T [x], where
T = pi?1x ?U??piy is the trained transducer that maps
x nondeterministically to y. Alas, it is NP-hard to
find the highest-probability string in a WFSA, even
an acyclic one (Casacuberta and Higuera, 2000).
The problem is that the probability of each string y
is a sum over many paths in T [x] that reflect differ-
ent alignments of y to x. Although it is straightfor-
ward to use a determinization construction (Mohri,
1997)13 to collapse these down to a single path per
y (so that y? is easily read off the single best path),
determinization can increase the WFSA?s size expo-
nentially. We approximate by pruning T [x] back to
its 1000-best paths before we determinize.14
Since the alignments, classes and regions are not
observed in C, we do not enjoy the convex objec-
tive function of fully-supervised log-linear models.
Training equation (2) therefore converges only to
some local maximum that depends on the starting
point in parameter space. To find a good starting
point we employ staged training, a technique in
which several models of ascending complexity are
trained consecutively. The parameters of each more
complex model are initialized with the trained pa-
rameters of the previous simpler model.
Our training is done in four stages. All weights
are initialized to zero. ? We first train only fea-
tures that fire on unigrams of alignment charac-
ters, ignoring features that examine the latent strings
or backed-off versions of the alignment characters
(such as vowel/consonant or target language model
features). The resulting model is equivalent to
weighted edit distance (Ristad and Yianilos, 1998).
? Next,15 we train all n-grams of alignment charac-
ters, including higher-order n-grams, but no backed-
off features or features that refer to latent strings.
13Weighted determinization is not always possible, but it is
in our case because our limit to k consecutive insertions guar-
antees that T [x] is acyclic.
14This value is high enough; we see no degradations in per-
formance if we use only 100 or even 10 best paths. Below that,
performance starts to drop slightly. In both of our tasks, our
conditional distributions are usually peaked: the 5 best output
candidates amass > 99% of the probability mass on average.
Entropy is reduced by latent classes and/or regions.
15When unclamping a feature at the start of stages ???, we
initialize it to a random value from [?0.01, 0.01].
13SIA. liebte, pickte, redete, rieb, trieb, zuzog
13SKE. liebe, picke, rede, reibe, treibe, zuziehe
2PIE. liebt, pickt, redet, reibt, treibt, zuzieht
13PKE.lieben, picken, reden, reiben, treiben, zuziehen
2PKE. abbrechet, entgegentretet, zuziehet
z. abzubrechen, entgegenzutreten, zuzuziehen
rP. redet, reibt, treibt, verbindet, u?berfischt
pA.geredet, gerieben, getrieben, verbunden, u?berfischt
Table 2: CELEX forms used in our experiments. Changes
from one form to the other are in bold (information not
given in training). The changes from rP to pA are very
complex. Note also the differing positions of zu in z.
? Next, we add backed-off features as well as all
collapsed features. ? Finally, we train all features.
In our experiments, we permitted latent classes 1?
2 and, where regions are used, regions 0?6. For
speed, stages ??? used a pruned ? that included
only ?plausible? alignment characters: a may not
align to b unless it did so in the trained stage-(1)
model?s optimal alignment of at least one training
pair (x, y?).
4.2 Inflectional morphology
We conducted several experiments on the CELEX
morphological database. We arbitrarily consid-
ered mapping the following German verb forms:16
13SIA ? 13SKE, 2PIE ? 13PKE, 2PKE ? z,
and rP ? pA.17 We refer to these tasks as 13SIA,
2PIE, 2PKE and rP. Table 2 shows some examples
of regular and irregular forms. Common phenomena
include stem changes (ei:ie), prefixes inserted af-
ter other morphemes (abzubrechen) and circumfixes
(gerieben).
We compile lists of form pairs from CELEX. For
each task, we sample 2500 data pairs without re-
placement, of which 500 are used for training, 1000
as development and the remaining 1000 as test data.
We train and evaluate models on this data and repeat
16From the available languages in CELEX (German, Dutch,
and English), we selected German as the language with the
most interesting morphological phenomena, leaving the mul-
tilingual comparison for the lemmatization task (section 4.3),
where there were previous results to compare with. The 4 Ger-
man datasets were picked arbitrarily.
17A key to these names: 13SIA=1st/3rd sg. ind. past;
13SKE=1st/3rd sg. subjunct. pres.; 2PIE=2nd pl. ind. pres.;
13PKE=1st/3rd pl. subjunct. pres.; 2PKE=2nd pl. subjunct.
pres.; z=infinitive; rP=imperative pl.; pA=past part.
1084
Features Task
ng vc tlm tlm-coll id lat.cl. lat.reg. 13SIA 2PIE 2PKE rP
ngrams x 82.3 (.23) 88.6 (.11) 74.1 (.52) 70.1 (.66)
ngrams+x
x x 82.8 (.21) 88.9 (.11) 74.3 (.52) 70.0 (.68)
x x 82.0 (.23) 88.7 (.11) 74.8 (.50) 69.8 (.67)
x x x 82.5 (.22) 88.6 (.11) 74.9 (.50) 70.0 (.67)
x x x 81.2 (.24) 88.7 (.11) 74.5 (.50) 68.6 (.69)
x x x x 82.5 (.22) 88.8 (.11) 74.5 (.50) 69.2 (.69)
x x 82.4 (.22) 88.9 (.11) 74.8 (.51) 69.9 (.68)
x x x 83.0 (.21) 88.9 (.11) 74.9 (.50) 70.3 (.67)
x x x 82.2 (.22) 88.8 (.11) 74.8 (.50) 70.0 (.67)
x x x x 82.9 (.21) 88.6 (.11) 75.2 (.50) 69.7 (.68)
x x x x 81.9 (.23) 88.6 (.11) 74.4 (.51) 69.1 (.68)
x x x x x 82.8 (.21) 88.7 (.11) 74.7 (.50) 69.9 (.67)
ngrams+x
+latent
x x x x x x 84.8 (.19) 93.6 (.06) 75.7 (.48) 81.8 (.43)
x x x x x x 87.4 (.16) 93.8 (.06) 88.0 (.28) 83.7 (.42)
x x x x x x x 87.5 (.16) 93.4 (.07) 87.4 (.28) 84.9 (.39)
Moses3 73.9 (.40) 92.0 (.09) 67.1 (.70) 67.6 (.77)
Moses9 85.0 (.21) 94.0 (.06) 82.3 (.31) 70.8 (.67)
Moses15 85.3 (.21) 94.0 (.06) 82.8 (.30) 70.8 (.67)
Table 1: Exact-match accuracy and average edit distance (the latter in parentheses) versus the correct answer on the
German inflection task, using different combinations of feature classes. The label ngrams corresponds to the second
stage of training, ngrams+x to the third where backoff features may fire (vc = vowel/consonant, tlm = target LM, tlm-
coll = collapsed tlm, id = identity/substitution/deletion features), and ngrams+x+latent to the fourth where features
sensitive to latent classes and latent regions are allowed to fire. The highest n-gram order used is 3, except for Moses9
and Moses15 which examine windows of up to 9 and 15 characters, respectively. We mark in bold the best result for
each dataset, along with all results that are statistically indistinguishable (paired permutation test, p < 0.05).
the process 5 times. All results are averaged over
these 5 runs.
Table 1 and Fig. 3 report separate results after
stages ?, ?, and ? of training, which include suc-
cessively larger feature sets. These are respectively
labeled ngrams, ngrams+x, and ngrams+x+latent.
In Table 1, the last row in each section shows the
full feature set at that stage (cf. Fig. 3), while earlier
rows test feature subsets.18
Our baseline is the SMT toolkit Moses (Koehn et
al., 2007) run over letter strings rather than word
strings. It is trained (on the same data splits) to
find substring-to-substring phrase pairs and translate
from one form into another (with phrase reordering
turned off). Results reported as moses3 are obtained
from Moses runs that are constrained to the same
context windows that our models use, so the maxi-
mum phrase length and the order of the target lan-
guage model were set to 3. We also report results
using much larger windows, moses9 and moses15.
18The number k of consecutive insertions was set to 3.
Results. The results in Table 1 show that including
latent classes and/or regions improves the results
dramatically. Compare the last line in ngrams+x
to the last line in ngrams+x+latent. The accuracy
numbers improve from 82.8 to 87.5 (13SIA), from
88.7 to 93.4 (2PIE), from 74.7 to 87.4 (2PKE), and
from 69.9 to 84.9 (rP).19 This shows that error re-
ductions between 27% and 50% were reached. On
3 of 4 tasks, even our simplest ngrams method beats
the moses3 method that looks at the same amount of
context.20 With our full model, in particular using
latent features, we always outperform moses3?and
even outperform moses15 on 3 of the 4 datasets, re-
ducing the error rate by up to 48.3% (rP). On the
fourth task (2PIE), our method and moses15 are sta-
tistically tied. Moses15 has access to context win-
dows of five times the size than we allowed our
methods in our experiments.
19All claims in the text are statistically significant under a
paired permutation test (p < .05).
20This bears out our contention in footnote 2 that a ?segment-
ing? channel model is damaging. Moses cannot fully recover by
using overlapping windows in the language model.
1085
While the gains from backoff features in Table 1
were modest (significant gains only on 13SIA), the
learning curve in Fig. 3 suggests that they were help-
ful for smaller training sets on 2PKE (see ngrams vs
ngrams+x on 50 and 100) and helped consistently
over different amounts of training data for 13SIA.
Analysis. The types of errors that our system (and
the moses baseline) make differ from task to task.
Due to lack of space, we mainly focus on the com-
plex rP task. Here, most errors come from wrongly
copying the input to the output, without making a
change (40-50% of the errors in all models, except
for our model with latent classes and no regions,
where it accounts for only 30% of the errors). This
is so common because about half of the training ex-
amples contain identical inputs and outputs (as in
the imperative berechnet and the participle (ihr habt)
berechnet). Another common error is to wrongly as-
sume a regular conjugation (just insert the prefix ge-
at the beginning). Interestingly, this error by sim-
plification is more common in the Moses models
(44% of moses3 errors, down to 40% for moses15)
than in our models, where it accounts for 37% of
the errors of our ngrams model and only 19% if la-
tent classes or latent regions are used; however, it
goes up to 27% if both latent classes and regions
are used.21 All models for rP contain errors where
wrong analogies to observed words are made (ver-
schweisst/verschwissen in analogy to the observed
durchweicht/durchwichen, or bebt/geboben in anal-
ogy to hebt/gehoben). In the 2PKE task, most errors
result from inserting the zu morpheme at a wrong
place or inserting two of them, which is always
wrong. This error type was greatly reduced by la-
tent regions, which can discover different parame-
ters for different positions, making it easier to iden-
tify where to insert the zu.
Analysis of the 2 latent classes (when used) shows
that a split into regular and irregular conjugations
has been learned. For the rP task we compute,
for each data pair in development data, the poste-
rior probabilities of membership in one or the other
class. 98% of the regular forms, in which the past
participle is built with ge- . . . -t, fall into one class,
21We suspect that training of the models that use classes and
regions together was hurt by the increased non-convexity; an-
nealing or better initialization might help.
Figure 3: Learning curves for German inflection tasks,
13SIA (left) and 2PKE (right), as a function of the num-
ber of training pairs. ngrams+x means all backoff fea-
tures were used, ngrams+x+latent means all latent fea-
tures were used in addition. Moses15 examines windows
of up to 15 characters.
which in turn consists nearly exclusively (96%) of
these forms. Different irregular forms are lumped
into the other class.
The learned regions are consistent across different
pairs. On development data for the rP task, 94.3%
of all regions that are labeled 1 are the insertion se-
quence (,ge), region 3 consists of vowel changes
93.7% of the time; region 5 represents the typical
suffixes (t,en), (et,en), (t,n) (92.7%). In the
2PKE task, region 0 contains different prefixes (e.g.
entgegen in entgegenzutreten), regions 1 and 2 are
empty, region 3 contains the zu affix, region 4 the
stem, and region 5 contains the suffix.
The pruned alignment alphabet excluded a few
gold standard outputs so that the model contains
paths for 98.9%?99.9% of the test examples. We
verified that the insertion limit did not hurt oracle
accuracy.
4.3 Lemmatization
We apply our models to the task of lemmatization,
where the goal is to generate the lemma given an in-
flected word form. We compare our model to Wicen-
towski (2002, chapter 3), an alternative supervised
approach. Wicentowski?s Base model simply learns
how to replace an arbitrarily long suffix string of an
input word, choosing some previously observed suf-
fix? suffix replacement based on the input word?s
1086
Without rootlist (generation) With rootlist (selection)
Wicentowski (2002) This paper Wicentowski (2002) This paper
Lang. Base Af. WFA. n n+x n+x+l Base Af. WFA. n n+x n+x+l
Basque 85.3 81.2 80.1 91.0 (.20) 91.1 (.20) 93.6 (.14) 94.5 94.0 95.0 90.9 (.29) 90.8 (.31) 90.9 (.30)
English 91.0 94.7 93.1 92.4 (.09) 93.4 (.08) 96.9 (.05) 98.3 98.6 98.6 98.7 (.04) 98.7(.04) 98.7(.04)
Irish 43.3 - 70.8 96.8 (.07) 97.0 (.06) 97.8 (.04) 43.9 - 89.1 99.6 (.02) 99.6 (.02) 99.5 (.03)
Tagalog 0.3 80.3 81.7 80.5 (.32) 83.0 (.29) 88.6 (.19) 0.8 91.8 96.0 97.0 (.07) 97.2 (.07) 97.7 (.05)
Table 3: Exact-match accuracy and average edit distance (the latter in parentheses) on the 8 lemmatization tasks (2
tasks ? 4 languages). The numbers from Wicentowski (2002) are for his Base, Affix and WFAffix models. The
numbers for our models are for the feature sets ngrams, ngrams+x, ngrams+x+latent. The best result per task is in
bold (as are statistically indistinguishable results when we can do the comparison, i.e., for our own models). Corpus
sizes: Basque 5,842, English 4,915, Irish 1,376, Tagalog 9,479.
final n characters (interpolating across different val-
ues of n). His Affix model essentially applies the
Base model after stripping canonical prefixes and
suffixes (given by a user-supplied list) from the input
and output. Finally, his WFAffix uses similar meth-
ods to also learn substring replacements for a stem
vowel cluster and other linguistically significant re-
gions in the form (identified by a deterministic align-
ment and segmentation of training pairs). This ap-
proach is a bit like our change regions combined
with Moses?s region-independent phrase pairs.
We compare against all three models. Note that
Affix and WFAffix have an advantage that our mod-
els do not, namely, user-supplied lists of canonical
affixes for each language. It is interesting to see
how our models with their more non-committal tri-
gram structure compare to this. Table 3 reports re-
sults on the data sets used in Wicentowski (2002),
for Basque, English, Irish, and Tagalog. Follow-
ing Wicentowski, 10-fold cross-validation was used.
The columns n+x and n+x+l mean ngram+x and
ngram+x+latent, respectively. As latent variables,
we include 2 word classes but no change regions.22
For completeness, Table 3 also compares with Wi-
centowski (2002) on a selection (rather than genera-
tion) task. Here, at test time, the lemma is selected
from a candidate list of known lemmas, namely, all
the output forms that appeared in training data.23
These additional results are labeled with rootlist in
the right half of Table 3.
On the supervised generation task without rootlist,
22The insertion limit k was set to 2 for Basque and 1 for the
other languages.
23Though test data contained no (input, output) pairs from
training data, it reused many of the output forms, since many
inflected inputs are to be mapped to the same output lemma.
our models outperform Wicentowski (2002) by a
large margin. Comparing our results that use la-
tent classes (n+x+l) with Wicentowski?s best mod-
els we observe error reductions ranging from about
38% (Tagalog) to 92% (Irish). On the selection task
with rootlist, we outperform Wicentowski (2002) in
English, Irish, and Tagalog.
Analysis. We examined the classes learned on En-
glish lemmatization by our ngrams+x+latent model.
For each of the input/output pairs in development
data, we found the most probable latent class. For
the most part, the 2 classes are separated based on
whether or not the correct output ends in e. This
use of latent classes helped address many errors like
wronging / wronge or owed / ow). Such missing or
surplus final e?s account for 72.5% of the errors for
ngrams and 70.6% of the errors for ngrams+x, but
only 34.0% of the errors for ngrams+x+latent.
The test oracles are between 99.8% ? 99.9%, due
to the pruned alignment alphabet. As on the inflec-
tion task, the insertion limit does not exclude any
gold standard paths.
5 Finite-State Feature Implementation
We used the OpenFST library (Allauzen et al, 2007)
to implement all finite-state computations, using the
expectation semiring (Eisner, 2002) for training.
Our model is defined by the WFSA U?, which is
used to score alignment strings in ?? (section 2.2).
We now sketch how to construct U? from features.
n-gram construction The construction that we
currently use is quite simple. All of our current
features fire on windows of width ? 3. We build
a WFSA with the structure of a 3-gram language
1087
model over ??. Each of the |?|2 states remembers
two previous alignment characters ab of history; for
each c ? ?, it has an outgoing arc that accepts c (and
leads to state bc). The weight of this arc is the total
weight (from ?) of the small set of features that fire
when the trigram window includes abc. By conven-
tion, these also include features on bc and c (which
may be regarded as backoff features ?bc and ??c).
Since each character in ? is actually a 4-tuple, this
trigram machine is fairly large. We build it lazily
(?on the fly?), constructing arcs only as needed to
deal with training or test data.
Feature templates Our experiments use over
50,000 features. How do we specify these features
to the above construction? Rather than writing ordi-
nary code to extract features from a window, we find
it convenient to harness FSTs as a ?little language?
(Bentley, 1986) for specifying entire sets of features.
A feature template T is an nondeterministic FST
that maps the contents of the sliding window, such
as abc, to one or more features, which are also
described as strings.24 The n-gram machine de-
scribed above can compute T [((a?b)?c)?] to find
out what features fire on abc and its suffixes. One
simple feature template performs ?vowel/consonant
backoff?; e.g., it maps abc to the feature named
VCC. Fig. 2 showed the result of applying several
actual feature templates to the window shown in
Fig. 1. The extended regular expression calculus
provides a flexible and concise notation for writ-
ing down these FSTs. As a trivial example, the tri-
gram ?vowel/consonant backoff? transducer can be
described as T = V V V , where V is a transducer
that performs backoff on a single alignment charac-
ter. Feature templates should make it easy to experi-
ment with adding various kinds of linguistic knowl-
edge. We have additional algorithms for compiling
U? from a set of arbitrary feature templates,25 in-
cluding templates whose features consider windows
of variable or even unbounded width. The details are
beyond the scope of this paper, but it is worth point-
ing out that they exploit the fact that feature tem-
plates are FSTs and not arbitrary code.
24Formally, if i is a string naming a feature, then fi(A)
counts the number of positions in A that are immediately pre-
ceded by some string in T?1[i].
25Provided that the total number of features is finite.
6 Conclusions
The modeling framework we have presented here
is, we believe, an attractive solution to most string
transduction problems in NLP. Rather than learn the
topology of an arbitrary WFST, one specifies the
topology using a small set of feature templates, and
simply trains the weights.
We evaluated on two morphology generation
tasks. When inflecting German verbs we, even with
the simplest features, outperform the moses3 base-
line on 3 out of 4 tasks, which uses the same amount
of context as our models. Introducing more sophis-
ticated features that have access to latent classes and
regions improves our results dramatically, even on
small training data sizes. Using these we outper-
form moses9 and moses15, which use long context
windows, reducing error rates by up to 48%. On the
lemmatization task we were able to improve the re-
sults reported in Wicentowski (2002) on three out of
four tested languages and reduce the error rates by
38% to 92%. The model?s errors are often reason-
able misgeneralizations (e.g., assume regular con-
jugation where irregular would have been correct),
and it is able to use even a small number of latent
variables (including the latent alignment) to capture
useful linguistic properties.
In future work, we would like to identify a set of
features, latent variables, and training methods that
port well across languages and string-transduction
tasks. We would like to use features that look at
wide context on the input side, which is inexpen-
sive (Jiampojamarn et al, 2007). Latent variables
we wish to consider are an increased number of
word classes; more flexible regions?see Petrov et
al. (2007) on learning a state transition diagram for
acoustic regions in phone recognition?and phono-
logical features and syllable boundaries. Indeed, our
local log-linear features over several aligned latent
strings closely resemble the soft constraints used by
phonologists (Eisner, 1997). Finally, rather than de-
fine a fixed set of feature templates as in Fig. 2,
we would like to refine empirically useful features
during training, resulting in language-specific back-
off patterns and adaptively sized n-gram windows.
Many of these enhancements will increase the com-
putational burden, and we are interested in strategies
to mitigate this, including approximation methods.
1088
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proc. of CIAA, volume 4783.
Jon Bentley. 1986. Programming pearls [column]. Com-
munications of the ACM, 29(8), August.
Maximilian Bisani and Hermann Ney. 2002. Inves-
tigations on jointmultigram models for grapheme-to-
phoneme conversion.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In Proc. of the 5th Inter-
national Colloquium on Grammatical Inference: Al-
gorithms and Applications, volume 1891.
Francisco Casacuberta. 2000. Inference of finite-
state transducers by using regular grammars and mor-
phisms. In A.L. Oliveira, editor, Grammatical Infer-
ence: Algorithms and Applications, volume 1891.
Stanley F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proc. of Inter-
speech.
Alexander Clark. 2001. Learning morphology with Pair
Hidden Markov Models. In Proc. of the Student Work-
shop at the 39th Annual Meeting of the Association for
Computational Linguistics, Toulouse, France, July.
Sabine Deligne, Francois Yvon, and Fre?de?ric Bimbot.
1995. Variable-length sequence matching for phonetic
transcription using joint multigrams. In Eurospeech.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphological
preprocessing for grapheme-to-phoneme conversion.
In Proc. of ACL, Prague, Czech Republic, June.
Jason Eisner. 1997. Efficient generation in primitive Op-
timality Theory. In Proc. of ACL-EACL, Madrid, July.
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proc. of ACL.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Dayne Freitag and Shahram Khadivi. 2007. A sequence
alignment model based on the averaged perceptron. In
Proc. of EMNLP-CoNLL.
Lucian Galescu and James F. Allen. 2001. Bi-directional
conversion between graphemes and phonemes using a
joint N-gram model.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proc. of NAACL-HLT, Rochester, New
York, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
of ACL, Companion Volume, Prague, Czech Republic,
June.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45(3, (Ser. B)):503?528.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. of ACL, Ann Arbor, Michigan, June.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2).
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learning
structured models for phone recognition. In Proc. of
EMNLP-CoNLL.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20(5).
Holger Schwenk, Marta R. Costa-jussa, and Jose A.
R. Fonollosa. 2007. Smooth bilingual n-gram transla-
tion. In Proc. of EMNLP-CoNLL, pages 430?438.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. of ACL, Prague, Czech
Republic, June.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08: HLT,
Columbus, Ohio, June.
Richard Wicentowski. 2002. Modeling and Learning
Multilingual Inflectional Morphology in a Minimally
Supervised Framework. Ph.D. thesis, Johns-Hopkins
University.
Jun Wu and Sanjeev Khudanpur. 2000. Efficient training
methods for maximum entropy language modeling. In
Proc. of ICSLP, volume 3, Beijing, October.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Proc. of NAACL-HLT,
Rochester, New York, April.
1089
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40?51,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
First- and Second-Order Expectation Semirings
with Applications to Minimum-Risk Training on Translation Forests
?
Zhifei Li and Jason Eisner
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com, jason@cs.jhu.edu
Abstract
Many statistical translation models can be
regarded as weighted logical deduction.
Under this paradigm, we use weights from
the expectation semiring (Eisner, 2002), to
compute first-order statistics (e.g., the ex-
pected hypothesis length or feature counts)
over packed forests of translations (lat-
tices or hypergraphs). We then introduce
a novel second-order expectation semir-
ing, which computes second-order statis-
tics (e.g., the variance of the hypothe-
sis length or the gradient of entropy).
This second-order semiring is essential for
many interesting training paradigms such
as minimum risk, deterministic anneal-
ing, active learning, and semi-supervised
learning, where gradient descent optimiza-
tion requires computing the gradient of en-
tropy or risk. We use these semirings in an
open-source machine translation toolkit,
Joshua, enabling minimum-risk training
for a benefit of up to 1.0 BLEU point.
1 Introduction
A hypergraph or ?packed forest? (Gallo et al,
1993; Klein and Manning, 2004; Huang and Chi-
ang, 2005) is a compact data structure that uses
structure-sharing to represent exponentially many
trees in polynomial space. A weighted hypergraph
also defines a probability or other weight for each
tree, and can be used to represent the hypothesis
space considered (for a given input) by a mono-
lingual parser or a tree-based translation system,
e.g., tree to string (Quirk et al, 2005; Liu et al,
2006), string to tree (Galley et al, 2006), tree to
tree (Eisner, 2003), or string to string with latent
tree structures (Chiang, 2007).
?
This research was partially supported by the Defense
Advanced Research Projects Agency?s GALE program via
Contract No HR0011-06-2-0001. We are grateful to Sanjeev
Khudanpur for early guidance and regular discussions.
Given a hypergraph, we are often interested in
computing some quantities over it using dynamic
programming algorithms. For example, we may
want to run the Viterbi algorithm to find the most
probable derivation tree in the hypergraph, or the k
most probable trees. Semiring-weighted logic pro-
gramming is a general framework to specify these
algorithms (Pereira and Warren, 1983; Shieber et
al., 1994; Goodman, 1999; Eisner et al, 2005;
Lopez, 2009). Goodman (1999) describes many
useful semirings (e.g., Viterbi, inside, and Viterbi-
n-best). While most of these semirings are used in
?testing? (i.e., decoding), we are mainly interested
in the semirings that are useful for ?training? (i.e.,
parameter estimation). The expectation semiring
(Eisner, 2002), originally proposed for finite-state
machines, is one such ?training? semiring, and can
be used to compute feature expectations for the E-
step of the EM algorithm, or gradients of the like-
lihood function for gradient descent.
In this paper, we apply the expectation semir-
ing (Eisner, 2002) to a hypergraph (or packed for-
est) rather than just a lattice. We then propose
a novel second-order expectation semiring, nick-
named the ?variance semiring.?
The original first-order expectation semiring al-
lows us to efficiently compute a vector of first-
order statistics (expectations; first derivatives) on
the set of paths in a lattice or the set of trees in a
hypergraph. The second-order expectation semir-
ing additionally computes a matrix of second-
order statistics (expectations of products; second
derivatives (Hessian); derivatives of expectations).
We present details on how to compute many in-
teresting quantities over the hypergraph using the
expectation and variance semirings. These quan-
tities include expected hypothesis length, feature
expectation, entropy, cross-entropy, Kullback-
Leibler divergence, Bayes risk, variance of hy-
pothesis length, gradient of entropy and Bayes
risk, covariance and Hessian matrix, and so on.
The variance semiring is essential for many in-
teresting training paradigms such as deterministic
40
annealing (Rose, 1998), minimum risk (Smith and
Eisner, 2006), active and semi-supervised learning
(Grandvalet and Bengio, 2004; Jiao et al, 2006).
In these settings, we must compute the gradient of
entropy or risk. The semirings can also be used for
second-order gradient optimization algorithms.
We implement the expectation and variance
semirings in Joshua (Li et al, 2009a), and demon-
strate their practical benefit by using minimum-
risk training to improve Hiero (Chiang, 2007).
2 Semiring Parsing on Hypergraphs
We use a specific tree-based system called Hiero
(Chiang, 2007) as an example, although the dis-
cussion is general for any systems that use a hy-
pergraph to represent the hypothesis space.
2.1 Hierarchical Machine Translation
In Hiero, a synchronous context-free grammar
(SCFG) is extracted from automatically word-
aligned corpora. An illustrative grammar rule for
Chinese-to-English translation is
X ? ?X
0
{ X
1
, X
1
of X
0
? ,
where the Chinese word { means of, and the
alignment, encoded via subscripts on the nonter-
minals, causes the two phrases around { to be
reordered around of in the translation. Given
a source sentence, Hiero uses a CKY parser to
generate a hypergraph, encoding many derivation
trees along with the translation strings.
2.2 Hypergraphs
Formally, a hypergraph is a pair ?V,E?, where V
is a set of nodes (vertices) and E is a set of hy-
peredges, with each hyperedge connecting a set of
antecedent nodes to a single consequent node.
1
In
parsing parlance, a node corresponds to an item
in the chart (which specifies aligned spans of in-
put and output together with a nonterminal label).
The root node corresponds to the goal item. A
hyperedge represents an SCFG rule that has been
?instantiated? at a particular position, so that the
nonterminals on the right and left sides have been
replaced by particular antecedent and consequent
items; this corresponds to storage of backpointers
in the chart.
We write T (e) to denote the set of antecedent
nodes of a hyperedge e. We write I(v) for the
1
Strictly speaking, making each hyperedge designate a
single consequent defines a B-hypergraph (Gallo et al, 1993).
X 0,2 the mat NA X 3,4 a cat NA
X 0,4 a cat the mat
X 0,4 the mat a cat
goal item
?
0
 ?
1
                   ?
2
               ?
3
on the mat                of              a cat
X??X0 ?X1,X1 onX0?
X??X0 ?X1,X1 of X0?X??X0 ?X1,X0 ?s X1?X??X0 ?X1,X0 X1?
X?????, the mat?
S??X0,X0? S??X0,X0?
X???, a cat?
Figure 1: A toy hypergraph in Hiero. When generating the
hypergraph, a trigram language model is integrated. Rect-
angles represent items, where each item is identified by the
non-terminal symbol, source span, and left- and right-side
language model states. An item has one or more incoming
hyperedges. A hyperedge consists of a rule, and a pointer to
an antecedent item for each non-terminal symbol in the rule.
set of incoming hyperedges of node v (i.e., hyper-
edges of which v is the consequent), which repre-
sent different ways of deriving v. Figure 1 shows
a simple Hiero-style hypergraph. The hypergraph
encodes four different derivation trees that share
some of the same items. By exploiting this shar-
ing, a hypergraph can compactly represent expo-
nentially many trees.
We observe that any finite-state automaton can
also be encoded as a hypergraph (in which every
hyperedge is an ordinary edge that connects a sin-
gle antecedent to a consequent). Thus, the meth-
ods of this paper apply directly to the simpler case
of hypothesis lattices as well.
2.3 Semiring Parsing
We assume a hypergraph HG, which compactly
encodes many derivation trees d ? D. Given HG,
we wish to extract the best derivations?or other
aggregate properties of the forest of derivations.
Semiring parsing (Goodman, 1999) is a general
framework to describe such algorithms. To define
a particular algorithm, we choose a semiring K
and specify a ?weight? k
e
? K for each hyper-
edge e. The desired aggregate result then emerges
as the total weight of all derivations in the hyper-
graph. For example, to simply count derivations,
one can assign every hyperedge weight 1 in the
semiring of ordinary integers; then each deriva-
tion also has weight 1, and their total weight is the
number of derivations.
We write K = ?K,?,?, 0, 1? for a semiring
with elements K, additive operation ?, multi-
41
plicative operation?, additive identity 0, and mul-
tiplicative identity 1. The ? operation is used to
obtain the weight of each derivation d by multi-
plying the weights of its component hyperedges e,
that is, k
d
=
?
e?d
k
e
. The ? operation is used
to sum over all derivations d in the hypergraph
to obtain the total weight of the hypergraph HG,
which is
?
d?D
?
e?d
k
e
.
2
Figure 2 shows how to
compute the total weight of an acyclic hypergraph
HG.
3
In general, the total weight is a sum over
exponentially many derivations d. But Figure 2
sums over these derivations in time only linear on
the size of the hypergraph. Its correctness relies
on axiomatic properties of the semiring: namely,
? is associative and commutative with identity 0,
? is associative with two-sided identity 1, and
? distributes over ? from both sides. The dis-
tributive property is what makes Figure 2 work.
The other properties are necessary to ensure that
?
d?D
?
e?d
k
e
is well-defined.
4
The algorithm in Figure 2 is general and can be
applied with any semiring (e.g., Viterbi). Below,
we present our novel semirings.
3 Finding Expectations on Hypergraphs
We now introduce the computational problems of
this paper and the semirings we use to solve them.
3.1 Problem Definitions
We are given a function p : D ? R
?0
, which
decomposes multiplicatively over component hy-
peredges e of a derivation d ? D: that is, p(d)
def
=
?
e?d
p
e
. In practice, p(d) will specify a probabil-
ity distribution over the derivations in the hyper-
2
Eisner (2002) uses closed semirings that are also
equipped with a Kleene closure operator
?
. For example, in
the real semiring ?R,+,?, 0, 1?, we define p
?
= (1 ? p)
?1
(= 1 + p + p
2
+ . . .) for |p| < 1 and is undefined other-
wise. The closure operator enables exact summation over the
infinitely many paths in a cyclic FSM, or trees in a hyper-
graph with non-branching cycles, without the need to iterate
around cycles to numerical convergence. For completeness,
we specify the closure operator for our semirings, satisfying
the axioms k
?
= 1 ? k ? k
?
= 1 ? k
?
? k, but we do not
use it in our experiments since our hypergraphs are acyclic.
3
We assume that HG has already been built by deductive
inference (Shieber et al, 1994). But in practice, the nodes? in-
side weights ?(v) are usually accumulated as the hypergraph
is being built, so that pruning heuristics can consult them.
4
Actually, the notation
?
e?d
k
e
assumes that ? is com-
mutative as well, as does the notation ?for u ? T (e)? in our
algorithms; neither specifies a loop order. One could how-
ever use a non-commutative semiring by ordering each hyper-
edge?s antecedents and specifying that a derivation?s weight
is the product of the weights of its hyperedges when visited in
prefix order. Tables 1?2 will not assume any commutativity.
INSIDE(HG,K)
1 for
v
in topological order on HG  each node
2  find ?(v)?
?
e?I(v)
(k
e
? (
?
u?T (e)
?(u)))
3 ?(v)? 0
4 for
e
? I(v)  each incoming hyperedge
5 k ? k
e
 hyperedge weight
6 for
u
? T (e)  each antecedent node
7 k ? k ? ?(u)
8 ?(v)? ?(v)? k
9 return ?(root)
Figure 2: Inside algorithm for an acyclic hypergraph HG,
which provides hyperedge weights k
e
? K. This computes
all ?inside weights? ?(v) ? K, and returns ?(root), which is
total weight of the hypergraph, i.e.,
?
d?D
?
e?d
k
e
.
OUTSIDE(HG,K)
1 for
v
in HG
2 ?(v)? 0
3 ?(root)? 1
4 for
v
in reverse topological order on HG
5 for
e
? I(v)  each incoming hyperedge
6 for
u
? T (e)  each antecedent node
7 ?(u)? ?(u)? (?(v)? k
e
?
8
?
w?T (e),w 6=u
?(w))
Figure 3: Computes the ?outside weights? ?(v). Can only be
run after INSIDE(HG) of Figure 2 has already computed the
inside weights ?(v).
graph. It is often convenient to permit this prob-
ability distribution to be unnormalized, i.e., one
may have to divide it through by some Z to get a
proper distribution that sums to 1.
We are also given two functions of interest r, s :
D ? R, each of which decomposes additively
over its component hyperedges e: that is, r(d)
def
=
?
e?d
r
e
, and s(d)
def
=
?
e?d
s
e
.
We are now interested in computing the follow-
ing quantities on the hypergraph HG:
Z
def
=
?
d?D
p(d) (1)
r
def
=
?
d?D
p(d)r(d) (2)
s
def
=
?
d?D
p(d)s(d) (3)
t
def
=
?
d?D
p(d)r(d)s(d) (4)
Note that r/Z, s/Z, and t/Z are expectations un-
der p of r(d), s(d), and r(d)s(d), respectively.
More formally, the probabilistic interpretation
is that D is a discrete sample space (consisting
42
INSIDE-OUTSIDE(HG,K,X )
1  Run inside and outside on HG with only k
e
weights
2
?
k ? INSIDE(HG,K)  see Figure 2
3 OUTSIDE(HG,K)  see Figure 3
4  Do a single linear combination to get x?
5 x?? 0
6 for
v
in HG  each node
7 for
e
? I(v)  each incoming hyperedge
8 k
e
? ?(v)
9 for
u
? T (e)  each antecedent node
10 k
e
? k
e
?(u)
11 x?? x?+ (k
e
x
e
)
12 return ?
?
k, x??
Figure 4: If every hyperedge specifies a weight ?k
e
, x
e
? in
some expectation semiring E
K,X
, then this inside-outside al-
gorithm is a more efficient alternative to Figure 2 for comput-
ing the total weight ?
?
k, x?? of the hypergraph, especially if the
x
e
are vectors. First, at lines 2?3, the inside and outside al-
gorithms are run using only the k
e
weights, obtaining only
?
k
(without x?) but also obtaining all inside and outside weights
?, ? ? K as a side effect. Then the second component x? of
the total weight is accumulated in lines 5?11 as a linear com-
bination of all the x
e
values, namely x? =
?
e
k
e
x
e
, where
k
e
is computed at lines 8?10 using ? and ? weights. The lin-
ear coefficient k
e
is the ?exclusive weight? for hyperedge e,
meaning that the product k
e
k
e
is the total weight in K of all
derivations d ? D that include e.
of all derivations in the hypergraph), p is a mea-
sure over this space, and r, s : D ? R are ran-
dom variables. Then r/Z and s/Z give the expec-
tations of these random variables, and t/Z gives
the expectation of their product t = rs, so that
t/Z ? (r/Z)(s/Z) gives their covariance.
Example 1: r(d) is the length of the translation
corresponding to derivation d (arranged by setting
r
e
to the number of target-side terminal words in
the SCFG rule associated with e). Then r/Z is
the expected hypothesis length. Example 2: r(d)
evaluates the loss of d compared to a reference
translation, using some additively decomposable
loss function. Then r/Z is the risk (expected loss),
which is useful in minimum-risk training. Exam-
ple 3: r(d) is the number of times that a certain
feature fires on d. Then r/Z is the expected fea-
ture count, which is useful in maximum-likelihood
training. We will generalize later in Section 4 to
allow r(d) to be a vector of features. Example 4:
Suppose r(d) and s(d) are identical and both com-
pute hypothesis length. Then the second-order
statistic t/Z is the second moment of the length
distribution, so the variance of hypothesis length
can be found as t/Z ? (r/Z)
2
.
3.2 Computing the Quantities
We will use the semiring parsing framework to
compute the quantities (1)?(4). Although each is a
sum over exponentially many derivations, we will
compute it in O(|HG|) time using Figure 2.
In the simplest case, let K = ?R,+,?, 0, 1?,
and define k
e
= p
e
for each hyperedge e. Then
the algorithm of Figure 2 reduces to the classical
inside algorithm (Baker, 1979) and computes Z.
Next suppose K is the expectation semiring
(Eisner, 2002), shown in Table 1. Define k
e
=
?p
e
, p
e
r
e
?. Then Figure 2 will return ?Z, r?.
Finally, suppose K is our novel second-order
expectation semiring, which we introduce in Ta-
ble 2. Define k
e
= ?p
e
, p
e
r
e
, p
e
s
e
, p
e
r
e
s
e
?.
Then the algorithm of Figure 2 returns ?Z, r, s, t?.
Note that, to compute t, one cannot simply con-
struct a first-order expectation semiring by defin-
ing t(d)
def
= r(d)s(d) because t(d), unlike r(d)
and s(d), is not additively decomposable over the
hyperedges in d.
5
Also, when r(d) and s(d) are
identical, the second-order expectation semiring
allows us to compute variance as t/Z ? (r/Z)
2
,
which is why we may call our second-order ex-
pectation semiring the variance semiring.
3.3 Correctness of the Algorithms
To prove our claim about the first-order expecta-
tion semiring, we first observe that the definitions
in Table 1 satisfy the semiring axioms. The
reader can easily check these axioms (as well
as the closure axioms in footnote 2). With a
valid semiring, we then simply observe that Fig-
ure 2 returns the total weight
?
d?D
?
e?d
k
e
=
?
d?D
?p(d), p(d)r(d)? = ?Z, r?. It is easy to
verify the second equality from the definitions
of ?, Z, and r. The first equality requires
proving that
?
e?d
k
e
= ?p(d), p(d)r(d)?
from the definitions of ?, k
e
, p(d), and r(d).
The main intuition is that ? can be used to
build up ?p(d), p(d)r(d)? inductively from the
k
e
: if d decomposes into two disjoint sub-
derivations d
1
, d
2
, then ?p(d), p(d)r(d)? =
?p(d
1
)p(d
2
), p(d
1
)p(d
2
)(r(d
1
) + r(d
2
))? =
?p(d
1
), p(d
1
)r(d
1
)? ? ?p(d
2
), p(d
2
)r(d
2
)?. The
base cases are where d is a single hyperedge e, in
which case ?p(d), p(d)r(d)? = k
e
(thanks to our
choice of k
e
), and where d is empty, in which case
5
However, in a more tricky way, the second-order expec-
tation semiring can be constructed using the first-order ex-
pectation semiring, as will be seen in Section 4.3.
43
Element ?p, r?
?p
1
, r
1
?? ?p
2
, r
2
? ?p
1
p
2
, p
1
r
2
+ p
2
r
1
?
?p
1
, r
1
?? ?p
2
, r
2
? ?p
1
+ p
2
, r
1
+ r
2
?
?p, r?
?
?p
?
, p
?
p
?
r?
0 ?0, 0?
1 ?1, 0?
Table 1: Expectation semiring: Each element in the semir-
ing is a pair ?p, r?. The second and third rows define the
operations between two elements ?p
1
, r
1
? and ?p
2
, r
2
?, and
the last two rows define the identities. Note that the multi-
plicative identity 1 has an r component of 0.
s
a
s
b
a+ b a ? b
s
a+b
`
a+b
s
a?b
`
a?b
+ + + `
a
+ log(1 + e
`
b
?`
a
) + `
a
+ `
b
+ - + `
a
+ log(1? e
`
b
?`
a
) - `
a
+ `
b
- + - `
a
+ log(1? e
`
b
?`
a
) - `
a
+ `
b
- - - `
a
+ log(1 + e
`
b
?`
a
) + `
a
+ `
b
Table 3: Storing signed values in log domain: each value a
(= s
a
e
`
a
) is stored as a pair ?s
a
, `
a
? where s
a
and `
a
are the
sign bit of a and natural logarithm of |a|, respectively. This
table shows the operations between two values a = s
a
2
`
a
and b = s
b
2
`
b
, assuming `
a
? `
b
. Note: log(1 + x) (where
|x| < 1) should be computed by the Mercator series x ?
x
2
/2+x
3
/3?? ? ? , e.g., using the math library function log1p.
?p(d), p(d)r(d)? = 1. It follows by induction that
?p(d), p(d)r(d)? =
?
e?d
k
e
.
The proof for the second-order expec-
tation semiring is similar. In particular,
one mainly needs to show that
?
e?d
k
e
=
?p(d), p(d)r(d), p(d)s(d), p(d)r(d)s(d)?.
3.4 Preventing Underflow/Overflow
In Tables 1?2, we do not discuss how to store p, r,
s, and t. If p is a probability, it often suffers from
the underflow problem. r, s, and tmay suffer from
both underflow and overflow problems, depending
on their scales.
To address these, we could represent p in the
log domain as usual. However, r, s, and t can be
positive or negative, and we cannot directly take
the log of a negative number. Therefore, we repre-
sent real numbers as ordered pairs. Specifically, to
represent a = s
a
e
`
a
, we store ?s
a
, `
a
?, where the
s
a
? {+,?} is the sign bit of a and the floating-
point number `
a
is the natural logarithm of |a|.
6
Table 3 shows the ??? and ?+?operations.
6
An alternative that avoids log and exp is to store a =
f
a
2
e
a
as ?f
a
, e
a
?, where f
a
is a floating-point number and
e
a
is a sufficiently wide integer. E.g., combining a 32-bit
f
a
with a 32-bit e
a
will in effect extend f
a
?s 8-bit internal
exponent to 32 bits by adding e
a
to it. This gives much more
dynamic range than the 11-bit exponent of a 64-bit double-
precision floating-point number, if vastly less than in Table 3.
4 Generalizations and Speedups
In this section, we generalize beyond the above
case where p, r, s are R-valued. In general, p may
be an element of some other semiring, and r and s
may be vectors or other algebraic objects.
When r and s are vectors, especially high-
dimensional vectors, the basic ?inside algorithm?
of Figure 2 will be slow. We will show how to
speed it up with an ?inside-outside algorithm.?
4.1 Allowing Feature Vectors and More
In general, for P,R, S, T , we can define the
first-order expectation semiring E
P,R
= ?P ?
R,?,?, 0, 1? and the second-order expectation
semiring E
P,R,S,T
= ?P ?R?S?T,?,?, 0, 1?,
using the definitions from Tables 1?2. But do
those definitions remain meaningful, and do they
continue to satisfy the semiring axioms?
Indeed they do when P = R, R = R
n
, S =
R
m
, T = R
n?m
, with rs defined as the outer
product rs
T
(a matrix) where s
T
is the trans-
pose of s. In this way, the second-order semiring
E
P,R,S,T
lets us take expectations of vectors and
outer products of vectors. So we can find means
and covariances of any number of linearly decom-
posable quantities (e.g., feature counts) defined on
the hypergraph.
We will consider some other choices in Sec-
tions 4.3?4.4 below. Thus, for generality, we con-
clude this section by stating the precise technical
conditions needed to construct E
P,R
and E
P,R,S,T
:
? P is a semiring
? R is a P -module (e.g, a vector space), mean-
ing that it comes equipped with an associative
and commutative addition operation with an
identity element 0, and also a multiplication
operation P?R? R, such that p(r
1
+r
2
) =
pr
1
+pr
2
, (p
1
+p
2
)r = p
1
r+p
2
r, p
1
(p
2
r) =
(p
1
p
2
)r
? S and T are also P -modules
? there is a multiplication operation R ? S ?
T that is bilinear, i.e., (r
1
+ r
2
)s = r
1
s +
r
2
s, r(s
1
+ s
2
) = rs
1
+ rs
2
, (pr)s = p(rs),
r(ps) = p(rs)
As a matter of notation, note that above and in
Tables 1?2, we overload ?+? to denote any of
the addition operations within P,R, S, T ; over-
load ?0? to denote their respective additive iden-
tities; and overload concatenation to denote any
of the multiplication operations within or between
44
Element ?p, r, s, t?
?p
1
, r
1
, s
1
, t
1
?? ?p
2
, r
2
, s
2
, t
2
? ?p
1
p
2
, p
1
r
2
+ p
2
r
1
, p
1
s
2
+ p
2
s
1
,
p
1
t
2
+ p
2
t
1
+ r
1
s
2
+ r
2
s
1
?
?p
1
, r
1
, s
1
, t
1
?? ?p
2
, r
2
, s
2
, t
2
? ?p
1
+ p
2
, r
1
+ r
2
, s
1
+ s
2
, t
1
+ t
2
?
?p, r, s, t?
?
?p
?
, p
?
p
?
r, p
?
p
?
s, p
?
p
?
(p
?
rs+ p
?
rs+ t)?
0 ?0, 0, 0, 0?
1 ?1, 0, 0, 0?
Table 2: Second-order expectation semiring (variance semiring): Each element in the semiring is a 4-tuple ?p, r, s, t?. The
second and third rows define the operations between two elements ?p
1
, r
1
, s
1
, t
1
? and ?p
2
, r
2
, s
2
, t
2
?, while the last two rows
define the identities. Note that the multiplicative identity 1 has r,s and t components of 0.
P,R, S, T . ?1? refers to the multiplicative identity
of P . We continue to use distinguished symbols
?,?, 0, 1 for the operations and identities in our
?main semiring of interest,? E
P,R
or E
P,R,S,T
.
To compute equations (1)?(4) in this more gen-
eral setting, we must still require multiplicative
or additive decomposability, defining p(d)
def
=
?
e?d
p
e
, r(d)
def
=
?
e?d
r
e
, s(d)
def
=
?
e?d
s
e
as be-
fore. But the
?
and
?
operators here now denote
appropriate operations within P , R, and S respec-
tively (rather than the usual operations within R).
4.2 Inside-Outside Speedup for First-Order
Expectation Semirings
Under the first-order expectation semiring E
R,R
n
,
the inside algorithm of Figure 2 will return ?Z, r?
where r is a vector of n feature expectations.
However, Eisner (2002, section 5) observes that
this is inefficient when n is large. Why? The
inside algorithm takes the trouble to compute an
inside weight ?(v) ? R ? R
n
for each node v
in the hypergraph (or lattice). The second com-
ponent of ?(v) is a presumably dense vector of
all features that fire in all subderivations rooted at
node v. Moreover, as ?(v) is computed in lines
3?8, that vector is built up (via the ? and ? oper-
ations of Table 1) as a linear combination of other
dense vectors (the second components of the vari-
ous ?(u)). These vector operations can be slow.
A much more efficient approach (usually) is
the traditional inside-outside algorithm (Baker,
1979).
7
Figure 4 generalizes the inside-outside
algorithm to work with any expectation semiring
E
K,X
.
8
We are given a hypergraph HG whose
edges have weights ?k
e
, x
e
? in this semiring (so
7
Note, however, that the expectation semiring requires
only the forward/inside pass to compute expectations, and
thus it is more efficient than the traditional inside-outside al-
gorithm (which requires two passes) if we are interested in
computing only a small number of quantities.
8
This follows Eisner (2002), who similarly generalized
the forward-backward algorithm.
now k
e
? K denotes only part of the edge weight,
not all of it). INSIDE-OUTSIDE(HG,K, X) finds
?
d?D
?
e?d
?k
e
, x
e
?, which has the form ?
?
k, x??.
But, INSIDE(HG,E
K,X
) could accomplish the
same thing. So what makes the inside-outside al-
gorithm more efficient? It turns out that x? can
be found quickly as a single linear combination
?
e
k
e
x
e
of just the feature vectors x
e
that ap-
pear on individual hyperedges?typically a sum
of very sparse vectors! And the linear coefficients
k
e
, as well as
?
k, are computed entirely within the
cheap semiring K. They are based on ? and ? val-
ues obtained by first running INSIDE(HG,K) and
OUTSIDE(HG,K), which use only the k
e
part of
the weights and ignore the more expensive x
e
.
It is noteworthy that the expectation semiring is
not used at all by Figure 4. Although the return
value ?
?
k, x?? is in the expectation semiring, it is
built up not by ? and ? but rather by computing
?
k and x? separately. One might therefore wonder
why the expectation semiring and its operations
are still needed. One reason is that the input to
Figure 4 consists of hyperedge weights ?k
e
, x
e
? in
the expectation semiring?and these weights may
well have been constructed using ? and ?. For
example, Eisner (2002) uses finite-state operations
such as composition, which do combine weights
entirely within the expectation semiring before
their result is passed to the forward-backward al-
gorithm. A second reason is that when we work
with a second-order expectation semiring in Sec-
tion 4.4 below, the
?
k, ?, and ? values in Figure 4
will turn out to be elements of a first-order expec-
tation semiring, and they must still be constructed
by first-order ? and ?, via calls to Figures 2?3.
Why does inside-outside work? Whereas the
inside algorithm computes
?
d?D
?
e?d
in any
semiring, the inside-outside algorithm exploits
the special structure of an expectation semir-
ing. By that semiring?s definitions of ? and ?
(Table 1),
?
d?D
?
e?d
?k
e
, x
e
? can be found as
45
??
d?D
?
e?d
k
e
,
?
d?D
?
e?d
(
?
e
?
?d,e
?
6=e
k
e
?
)x
e
?.
The first component (giving
?
k) is found
by calling the inside algorithm on just the
k
e
part of the weights. The second com-
ponent (giving x?) can be rearranged into
?
e
?
d: e?d
(
?
e
?
?d,e
?
6=e
k
e
?
)x
e
=
?
e
k
e
x
e
, where
k
e
def
=
?
d: e?d
(
?
e
?
?d,e
?
6=e
k
e
?
) is found from ?, ?.
The application described at the start of this
subsection is the classical inside-outside algo-
rithm. Here ?k
e
, x
e
?
def
= ?p
e
, p
e
r
e
?, and the al-
gorithm returns ?
?
k, x?? = ?Z, r?. In fact, that
x? = r can be seen directly: r =
?
d
p(d)r(d) =
?
d
p(d)(
?
e?d
r
e
) =
?
e
?
d: e?d
p(d)r
e
=
?
e
(k
e
k
e
)r
e
=
?
e
k
e
x
e
= x?. This uses the fact
that k
e
k
e
=
?
d: e?d
p(d).
4.3 Lifting Trick for Second-Order Semirings
We now observe that the second-order expectation
semiring E
P,R,S,T
can be obtained indirectly by
nesting one first-order expectation semiring inside
another! First ?lift? P to obtain the first-order ex-
pectation semiring K
def
= E
P,R
. Then lift this a sec-
ond time to obtain the ?nested? first-order expec-
tation semiring E
K,X
= E
(E
P,R
),(S?T )
, where we
equip X
def
= S ? T with the operations ?s
1
, t
1
? +
?s
2
, t
2
?
def
= ?s
1
+ s
2
, t
1
+ t
2
? and ?p, r??s, t?
def
=
?ps, pt+ rs?. The resulting first-order expectation
semiring has elements of the form ??p, r?, ?s, t??.
Table 4 shows that it is indeed isomorphic to
E
P,R,S,T
, with corresponding elements ?p, r, s, t?.
This construction of the second-order semiring
as a first-order semiring is a useful bit of abstract
algebra, because it means that known properties
of first-order semirings will also apply to second-
order ones. First of all, we are immediately guar-
anteed that the second-order semiring satisfies the
semiring axioms. Second, we can directly apply
the inside-outside algorithm there, as we now see.
4.4 Inside-Outside Speedup for
Second-Order Expectation Semirings
Given a hypergraph weighted by a second-order
expectation semiring E
P,R,S,T
. By recasting this
as the first-order expectation semiringE
K,X
where
K = E
P,R
and X = (S ? T ), we can again ap-
ply INSIDE-OUTSIDE(HG,K, X) to find the total
weight of all derivations.
For example, to speed up Section 3.2, we
may define ?k
e
, x
e
? = ??p
e
, p
e
r
e
?, ?p
e
s
e
, p
e
r
e
s
e
??
for each hyperedge e. Then the inside-outside
algorithm of Figure 4 will compute ?
?
k, x?? =
??Z, r?, ?s, t??, more quickly than the inside algo-
rithm of Figure 2 computed ?Z, r, s, t?.
Figure 4 in this case will run the inside and
outside algorithms in the semiring E
P,R
, so that
k
e
,
?
k, ?, ?, and k
e
will now be elements of P ?R
(not just elements of P as in the first-order case).
Finally it finds x? =
?
e
k
e
x
e
, where x
e
? S?T .
9
This is a particularly effective speedup over
the inside algorithm when R consists of scalars
(or small vectors) whereas S, T are sparse high-
dimensional vectors. We will see exactly this case
in our experiments, where our weights ?p, r, s, t?
denote (probability, risk, gradient of probability,
gradient of risk), or (probability, entropy, gradient
of probability, gradient of entropy).
5 Finding Gradients on Hypergraphs
In Sections 3.2 and 4.1, we saw how our semirings
helped find the sum Z of all p(d), and compute
expectations r, s, t of r(d), s(d), and r(d)s(d).
It turns out that these semirings can also com-
pute first- and second-order partial derivatives of
all the above results, with respect to a parameter
vector ? ? R
m
. That is, we ask how they are
affected when ? changes slightly from its current
value. The elementary values p
e
, r
e
, s
e
are now
assumed to implicitly be functions of ?.
Case 1: Recall that Z
def
=
?
d
p(d) is com-
puted by INSIDE(HG,R) if each hyperedge e has
weight p
e
. ?Lift? this weight to ?p
e
,?p
e
?, where
?p
e
? R
m
is a gradient vector. Now ?Z,?Z? will
be returned by INSIDE(HG,E
R,R
m
)? or, more
efficiently, by INSIDE-OUTSIDE(HG,R,R
m
).
Case 2: To differentiate a second
time, ?lift? the above weights again
to obtain ??p
e
,?p
e
?,??p
e
,?p
e
?? =
??p
e
,?p
e
?, ??p
e
,?
2
p
e
??, where ?
2
p
e
? R
m?m
is the Hessian matrix of second-order mixed
partial derivatives. These weights are in a
second-order expectation semiring.
10
Now
9
Figure 4 was already proved generally correct in Sec-
tion 4.2. To understand more specifically how ?s, t? gets
computed, observe in analogy to the end of Section 4.2 that
?s, t? =
?
d
?p(d)s(d), p(d)r(d)s(d)?
=
?
d
?p(d), p(d)r(d)??s(d), 0?
=
?
d
?p(d), p(d)r(d)?
?
e?d
?s
e
, 0?
=
?
e
?
d: e?d
?p(d), p(d)r(d)??s
e
, 0?
=
?
e
(k
e
k
e
)?s
e
, 0? =
?
e
k
e
?p
e
, p
e
r
e
??s
e
, 0?
=
?
e
k
e
?p
e
s
e
, p
e
r
e
s
e
? =
?
e
k
e
x
e
= x?.
10
Modulo the trivial isomorphism from ??p, r?, ?s, t?? to
?p, r, s, t? (see Section 4.3), the intended semiring both here
and in Case 3 is the one that was defined at the start of Sec-
tion 4.1, in which r, s are vectors and their product is defined
46
??p
1
, r
1
?, ?s
1
, t
1
??? ??p
2
, r
2
?, ?s
2
, t
2
?? = ??p
1
, r
1
? + ?p
2
, r
2
?, ?s
1
, t
1
? + ?s
2
, t
2
??
= ??p
1
+ p
2
, r
1
+ r
2
?, ?s
1
+ s
2
, t
1
+ t
2
??
??p
1
, r
1
?, ?s
1
, t
1
??? ??p
2
, r
2
?, ?s
2
, t
2
?? = ??p
1
, r
1
??p
2
, r
2
?, ?p
1
, r
1
??s
2
, t
2
? + ?p
2
, r
2
??s
1
, t
1
??
= ??p
1
p
2
, p
1
r
2
+ p
2
r
1
?, ?p
1
s
2
+ p
2
s
1
, p
1
t
2
+ p
2
t
1
+ r
1
s
2
+ r
2
s
1
??
Table 4: Constructing second-order expectation semiring as first-order. Here we show that the operations in E
K,X
are
isomorphic to Table 2?s operations in E
P,R,S,T
, provided that K
def
= E
P,R
and X
def
= S ? T is a K-module, in which addition is
defined by?s
1
, t
1
? + ?s
2
, t
2
?
def
= ?s
1
+ s
2
, t
1
+ t
2
?, and left-multiplication by K is defined by ?p, r??s, t?
def
= ?ps, pt+ rs?.
?Z,?Z,?Z,?
2
Z? will be returned by
INSIDE(HG,E
R,R
m
,R
m
,R
m?m), or more effi-
ciently by INSIDE-OUTSIDE(HG,E
R,R
m
,R
m
?
R
m?m
).
Case 3: Our experiments will need to find ex-
pectations and their partial derivatives. Recall that
?Z, r? is computed by INSIDE(HG,E
R,R
n
) when
the edge weights are ?p
e
, p
e
r
e
? with r
e
? R
n
. Lift
these weights to ??p
e
, p
e
r
e
?,??p
e
, p
e
r
e
?? =
??p
e
, p
e
r
e
?, ??p
e
, (?p
e
)r
e
+ p
e
(?r
e
)??.
Now ?Z, r,?Z,?r? will be returned
by INSIDE(HG,E
R,R
n
,R
m
,R
n?m) or by
INSIDE-OUTSIDE(HG,E
R,R
n
,R
m
? R
n?m
).
11
5.1 What Connects Gradients to Expectations?
In Case 1, we claimed that the same algorithm
will compute either gradients ?Z,?Z? or expec-
tations ?Z, r?, if the hyperedge weights are set to
?p
e
,?p
e
? or ?p
e
, p
e
r
e
? respectively.
12
This may
seem wonderful and mysterious. We now show in
two distinct ways why this follows from our setup
of Section 3.1. At the end, we derive as a special
case the well-known relationship between gradi-
ents and expectations in log-linear models.
From Expectations to Gradients One perspec-
tive is that our semiring fundamentally finds ex-
pectations. Thus, we must be finding ?Z by for-
mulating it as a certain expectation r. Specif-
ically, ?Z = ?
?
d
p(d) =
?
d
?p(d) =
to be rs
T
, a matrix. However, when using this semiring to
compute second derivatives (Case 2) or covariances, one may
exploit the invariant that r = s, e.g., to avoid storing s and to
compute r
1
s
2
+ s
1
r
2
in multiplication simply as 2 ? r
1
r
2
.
11
Or, if n > m, it is faster to instead use
INSIDE-OUTSIDE(HG,E
R,R
m
,R
n
? R
m?n
), swapping the
second and third components of the 4-tuple and trans-
posing the matrix in the fourth component. Alge-
braically, this changes nothing because E
R,R
n
,R
m
?R
n?m
and
E
R,R
m
,R
n
?R
m?n
are isomorphic, thanks to symmetries in Ta-
ble 2. This method computes the expectation of the gradient
rather than the gradient of the expectation?they are equal.
12
Cases 2?3 relied on the fact that this relationship still
holds even when the scalars Z, p
e
? R are replaced by more
complex objects that we wish to differentiate. Our discus-
sion below sticks to the scalar case for simplicity, but would
generalize fairly straightforwardly. Pearlmutter and Siskind
(2007) give the relevant generalizations of dual numbers.
?
d
p(d)r(d) = r, provided that r(d) =
(?p(d))/p(d). That can be arranged by defining
r
e
def
= (?p
e
)/p
e
.
13
So that is why the input weights
?p
e
, p
e
r
e
? take the form ?p
e
,?p
e
?.
From Gradients to Expectations An alterna-
tive perspective is that our semiring fundamen-
tally finds gradients. Indeed, pairs like ?p,?p?
have long been used for this purpose (Clifford,
1873) under the name ?dual numbers.? Oper-
ations on dual numbers, including those in Ta-
ble 1, compute a result in R along with its gradi-
ent. For example, our ? multiplies dual numbers,
since ?p
1
,?p
1
? ? ?p
2
,?p
2
? = ?p
1
p
2
, p
1
(?p
2
) +
(?p
1
)p
2
? = ?p
1
p
2
,?(p
1
p
2
)?. The inside algo-
rithm thus computes both Z and ?Z in a single
?forward? or ?inside? pass?known as automatic
differentiation in the forward mode. The inside-
outside algorithm instead uses the reverse mode
(a.k.a. back-propagation), where a separate ?back-
ward? or ?outside? pass is used to compute?Z.
How can we modify this machinery to pro-
duce expectations r? given some arbitrary r
e
of interest? Automatic differentiation may
be used on any function (e.g., a neural net),
but for our simple sum-of-products function
Z, it happens that ?Z = ?(
?
d
?
e
p
e
) =
?
d
?
e?d
(
?
e
?
?d,e
?
6=e
p
e
?
)?p
e
. Our trick is to
surreptitiously replace the ?p
e
in the input
weights ?p
e
,?p
e
? with p
e
r
e
. Then the output
changes similarly: the algorithms will instead
find
?
d
?
e?d
(
?
e
?
?d,e
?
6=e
p
e
?
)p
e
r
e
, which re-
duces to
?
d
?
e?d
p(d)r
e
=
?
d
p(d)
?
e?d
r
e
=
?
d
p(d)r(d) = r?.
Log-linear Models as a Special Case Replac-
ing ?p
e
with p
e
r
e
is unnecessary if ?p
e
already
equals p
e
r
e
. That is the case in log-linear models,
where p
e
def
= exp(r
e
? ?) for some feature vector r
e
associated with e. So there, ?Z already equals
r??yielding a key useful property of log-linear
13
Proof: r(d) =
?
e?d
r
e
=
?
e?d
(?p
e
)/p
e
=
?
e?d
? log p
e
= ?
?
e?d
log p
e
= ? log
?
e?d
p
e
=
? log p(d) = (?p(d))/p(d).
47
models, that ? logZ = (?Z)/Z = r?/Z, the vec-
tor of feature expectations (Lau et al, 1993).
6 Practical Applications
Given a hypergraph HG whose hyperedges e are
annotated with values p
e
. Recall from Section 3.1
that this defines a probability distribution over all
derivations d in the hypergraph, namely p(d)/Z
where p(d)
def
=
?
e?d
p
e
.
6.1 First-Order Expectation Semiring E
R,R
In Section 3, we show how to compute the ex-
pected hypothesis length or expected feature
counts, using the algorithm of Figure 2 with a
first-order expectation semiring E
R,R
. In general,
given hyperedge weights ?p
e
, p
e
r
e
?, the algorithm
computes ?Z, r? and thus r/Z, the expectation of
r(d)
def
=
?
e?d
r
e
. We now show how to compute a
few other quantities by choosing r
e
appropriately.
Entropy on a Hypergraph The entropy of the
distribution of derivations in a hypergraph
14
is
H(p) = ?
?
d?D
(p(d)/Z) log(p(d)/Z) (5)
= logZ ?
1
Z
?
d?D
p(d) log p(d)
= logZ ?
1
Z
?
d?D
p(d)r(d) = logZ ?
r
Z
provided that we define r
e
def
= log p
e
(so that
r(d) =
?
e?d
r
e
= log p(d)). Of course, we can
compute ?Z, r? as explained in Section 3.2.
Cross-Entropy and KL Divergence We may
be interested in computing the cross-entropy or
KL divergence between two distributions p and q.
For example, in variational decoding for machine
translation (Li et al, 2009b), p is a distribution
represented by a hypergraph, while q, represented
by a finite state automaton, is an approximation to
p. The cross entropy between p and q is defined as
H(p, q) = ?
?
d?D
(p(d)/Z
p
) log(q(d)/Z
q
) (6)
= logZ
q
?
1
Z
p
?
d?D
p(d) log q(d)
= logZ
q
?
1
Z
p
?
d?D
p(d)r(d) = logZ
q
?
r
Z
p
14
Unfortunately, it is intractable to compute the entropy of
the distribution over strings (each string?s probability is a sum
over several derivations). But Li et al (2009b, section 5.4) do
estimate the gap between derivational and string entropies.
where the first term Z
q
can be computed using
the inside algorithm with hyperedge weights q
e
,
and the numerator and denominator of the sec-
ond term using an expectation semiring with hy-
peredge weights ?p
e
, p
e
r
e
? with r
e
def
= log q
e
.
The KL divergence to p from q can be computed
as KL(p ? q) = H(p, q)? H(p).
Expected Loss (Risk) Given a reference sen-
tence y
?
, the expected loss (i.e., Bayes risk) of the
hypotheses in the hypergraph is defined as,
R(p) =
?
d?D
(p(d)/Z)L(Y(d), y
?
) (7)
where Y(d) is the target yield of d and L(y, y
?
) is
the loss of the hypothesis y with respect to the ref-
erence y
?
. The popular machine translation met-
ric, BLEU (Papineni et al, 2001), is not additively
decomposable, and thus we are not able to com-
pute the expected loss for it. Tromble et al (2008)
develop the following loss function, of which a lin-
ear approximation to BLEU is a special case,
L(y, y
?
) = ?(?
0
|y|+
?
w?N
?
w
#
w
(y)?
w
(y?)) (8)
where w is an n-gram type, N is a set of n-gram
types with n ? [1, 4], #
w
(y) is the number of oc-
currence of the n-gramw in y, ?
w
(y
?
) is an indica-
tor to check if y
?
contains at least one occurrence
of w, and ?
n
is the weight indicating the relative
importance of an n-gram match. If the hypergraph
is already annotated with n-gram (n ? 4) lan-
guage model states, this loss function is additively
decomposable. Using r
e
def
= L
e
where L
e
is the
loss for a hyperedge e, we compute the expected
loss,
R(p) =
?
d?D
p(d)L(Y(d), y
?
)
Z
=
r
Z
(9)
6.2 Second-Order Expectation Semirings
With second-order expectation semirings, we can
compute from a hypergraph the expectation and
variance of hypothesis length; the feature expec-
tation vector and covariance matrix; the Hessian
(matrix of second derivatives) of Z; and the gradi-
ents of entropy and expected loss. The computa-
tions should be clear from earlier discussion. Be-
low we compute gradient of entropy or Bayes risk.
Gradient of Entropy or Risk It is easy to see
that the gradient of entropy (5) is
?H(p) =
?Z
Z
?
Z?r ? r?Z
Z
2
(10)
48
We may compute ?Z, r,?Z,?r? as ex-
plained in Case 3 of Section 5 by using
k
e
def
= ?p
e
, p
e
r
e
,?p
e
, (?p
e
)r
e
+ p
e
?r
e
?
def
=
?p
e
, p
e
log p
e
,?p
e
, (1 + log p
e
)?p
e
?, where ?p
e
depends on the particular parameterization of the
model (see Section 7.1 for an example).
Similarly, the gradient of risk of (9) is
?R(p) =
Z?r ? r?Z
Z
2
(11)
We may compute ?Z, r,?Z,?r? using k
e
def
=
?p
e
, p
e
L
e
,?p
e
, L
e
?p
e
?.
7 Minimum-Risk Training for MT
We now show how we improve the training of a
Hiero MT model by optimizing an objective func-
tion that includes entropy and risk. Our objective
function could be computed with a first-order ex-
pectation semiring, but computing it along with its
gradient requires a second-order one.
7.1 The Model p
We assume a globally normalized linear model
for its simplicity. Each derivation d is scored by
score(d)
def
= ?(d) ? ? =
?
i
?
i
(d) ?
i
(12)
where ?(d) ? R
m
is a vector of features of d. We
then define the unnormalized distribution p(d) as
p(d) = exp(? ? score(d)) (13)
where the scale factor ? adjusts how sharply the
distribution favors the highest-scoring hypotheses.
7.2 Minimum-Risk Training
Adjusting ? or ? changes the distribution p. Mini-
mum error rate training (MERT) (Och, 2003) tries
to tune ? to minimize the BLEU loss of a decoder
that chooses the most probable output according
to p. (? has no effect.) MERT?s specialized line-
search addresses the problem that this objective
function is piecewise constant, but it does not scale
to a large number of parameters.
Smith and Eisner (2006) instead propose a dif-
ferentiable objective that can be optimized by gra-
dient descent: the Bayes risk R(p) of (7). This is
the expected loss if one were (hypothetically) to
use a randomized decoder, which chooses a hy-
pothesis d in proportion to its probability p(d). If
entropy H(p) is large (e.g., small ?), the Bayes risk
is smooth and has few local minima. Thus, Smith
and Eisner (2006) try to avoid local minima by
starting with large H(p) and decreasing it gradu-
ally during optimization. This is called determin-
istic annealing (Rose, 1998). As H(p) ? 0 (e.g.,
large ?), the Bayes risk does approach the MERT
objective (i.e. minimizing 1-best error).The objec-
tive is
minimize R(p)? T ? H(p) (14)
where the ?temperature? T starts high and is ex-
plicitly decreased as optimization proceeds.
7.3 Gradient Descent Optimization
Solving (14) for a given T requires computing the
entropy H(p) and risk R(p) and their gradients
with respect to ? and ?. Smith and Eisner (2006)
followed MERT in constraining their decoder to
only an n-best list, so for them, computing these
quantities did not involve dynamic programming.
We compare those methods to training on a hy-
pergraph containing exponentially many hypothe-
ses. In this condition, we need our new second-
order semiring methods and must also approxi-
mate BLEU (during training only) by an additively
decomposable loss (Tromble et al, 2008).
15
Our algorithms require that p(d) of (13) is mul-
tiplicatively decomposable. It suffices to define
?(d)
def
=
?
e?d
?
e
, so that all features are local
to individual hyperedges; the vector ?
e
indicates
which features fire on hyperedge e. Then score(d)
of (12) is additively decomposable:
score(d) =
?
e?d
score
e
=
?
e?d
?
e
? ? (15)
We can then set p
e
= exp(? ? score
e
), and ?p
e
=
?p
e
?(e), and use the algorithms described in Sec-
tion 6 to compute H(p) and R(p) and their gradi-
ents with respect to ? and ?.
16
15
Pauls et al (2009) concurrently developed a method to
maximize the expected n-gram counts on a hypergraph using
gradient descent. Their objective is similar to the minimum
risk objective (though without annealing), and their gradient
descent optimization involves in algorithms in computing ex-
pected feature/n-gram counts as well as expected products of
features and n-gram counts, which can be viewed as instances
of our general algorithms with first- and second-order semir-
ings. They focused on tuning only a small number (i.e. nine)
of features as in a regular MERT setting, while our experi-
ments involve both a small and a large number of features.
16
It is easy to verify that the gradient of a function f (e.g.
entropy or risk) with respect to ? can be written as a weighted
sum of gradients with respect to the feature weights ?
i
, i.e.
?
f
?
?
=
1
?
?
i
?
i
?
?
f
?
?
i
(16)
49
7.4 Experimental Results
7.4.1 Experimental Setup
We built a translation model on a corpus for
IWSLT 2005 Chinese-to-English translation task
(Eck and Hori, 2005), which consists of 40k pairs
of sentences. We used a 5-gram language model
with modified Kneser-Ney smoothing, trained on
the bitext?s English using SRILM (Stolcke, 2002).
7.4.2 Tuning a Small Number of Features
We first investigate how minimum-risk training
(MR), with and without deterministic annealing
(DA), performs compared to regular MERT. MR
without DA just fixes T = 0 and ? = 1 in (14).
All MR or MR+DA uses an approximated BLEU
(Tromble et al, 2008) (for training only), while
MERT uses the exact corpus BLEU in training.
The first five rows in Table 5 present the results
by tuning the weights of five features (? ? R
5
). We
observe that MR or MR+DA performs worse than
MERT on the dev set. This may be mainly because
MR or MR+DA uses an approximated BLEU while
MERT doesn?t. On the test set, MR or MR+DA
on an n-best list is comparable to MERT. But our
new approach, MR or MR+DA on a hypergraph,
does consistently better (statistically significant)
than MERT, despite approximating BLEU.
17
Did DA help? For both n-best and hypergraph,
MR+DA did obtain a better BLEU score than plain
MR on the dev set.
18
This shows that DA helps
with the local minimum problem, as hoped. How-
ever, DA?s improvement on the dev set did not
transfer to the test set.
7.4.3 Tuning a Large Number of Features
MR (with or without DA) is scalable to tune a
large number of features, while MERT is not. To
achieve competitive performance, we adopt a for-
est reranking approach (Li and Khudanpur, 2009;
Huang, 2008). Specifically, our training has two
stages. In the first stage, we train a baseline system
as usual. We also find the optimal feature weights
for the five features mentioned before, using the
method of MR+DA operating on a hypergraph. In
the second stage, we generate a hypergraph for
each sentence in the training data (which consists
of about 40k sentence pairs), using the baseline
17
Pauls et al (2009) concurrently observed a similar pat-
tern (i.e., MR performs worse than MERT on the dev set, but
performs better on a test set).
18
We also verified that MR+DA found a better objective
value (i.e., expected loss on the dev set) than MR.
Training scheme dev test
MERT (Nbest, small) 42.6 47.7
MR (Nbest, small) 40.8 47.7
MR+DA (Nbest, small) 41.6 47.8
NEW! MR (hypergraph, small) 41.3 48.4
NEW! MR+DA (hypergraph, small) 41.9 48.3
NEW! MR (hypergraph, large) 42.3 48.7
Table 5: BLEU scores on the Dev and test sets under different
training scenarios. In the ?small? model, five features (i.e.,
one for the language model, three for the translation model,
and one for word penalty) are tuned. In the ?large? model,
21k additional unigram and bigram features are used.
system. In this stage, we add 21k additional uni-
gram and bigram target-side language model fea-
tures (cf. Li and Khudanpur (2008)). For example,
a specific bigram ?the cat? can be a feature. Note
that the total score by the baseline system is also
a feature in the second-stage model. With these
features and the 40k hypergraphs, we run the MR
training to obtain the optimal weights.
During test time, a similar procedure is fol-
lowed. For a given test sentence, the baseline sys-
tem first generates a hypergraph, and then the hy-
pergraph is reranked by the second-stage model.
The last row in Table 5 reports the BLEU scores.
Clearly, adding more features improves (statisti-
cally significant) the case with only five features.
We plan to incorporate more informative features
described by Chiang et al (2009).
19
8 Conclusions
We presented first-order expectation semirings
and inside-outside computation in more detail
than (Eisner, 2002), and developed extensions to
higher-order expectation semirings. This enables
efficient computation of many interesting quanti-
ties over the exponentially many derivations en-
coded in a hypergraph: second derivatives (Hes-
sians), expectations of products (covariances), and
expectations such as risk and entropy along with
their derivatives. To our knowledge, algorithms
for these problems have not been presented before.
Our approach is theoretically elegant, like other
work in this vein (Goodman, 1999; Lopez, 2009;
Gimpel and Smith, 2009). We used it practically to
enable a new form of minimum-risk training that
improved Chinese-English MT by 1.0 BLEU point.
Our implementation will be released within the
open-source MT toolkit Joshua (Li et al, 2009a).
19
Their MIRA training tries to favor a specific oracle
translation?indeed a specific tree?from the (pruned) hyper-
graph. MR does not commit to such an arbitrary choice.
50
References
J. K. Baker. 1979. Trainable grammars for speech
recognition. In Jared J. Wolf and Dennis H. Klatt,
editors, Speech Communication Papers Presented at
the 97th Meeting of the Acoustical Society of Amer-
ica, MIT, Cambridge, MA, June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
W. K. Clifford. 1873. Preliminary sketch of bi-
quaternions. Proceedings of the London Mathemat-
ical Society, 4:381?395.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In Proc. of the
International Workshop on Spoken Language Trans-
lation.
Jason Eisner, Eric Goldlust, and Noah A. Smith.
2005. Compiling comp ling: practical weighted
dynamic programming and the dyna language. In
HLT/EMNLP, pages 281?290.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL, pages 1?8.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In ACL, pages
205?208.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and ap-
plications. Discrete Appl. Math., 42(2-3):177?201.
Kevin Gimpel and Noah A. Smith. 2009. Cube
summing, approximate inference with non-local fea-
tures, and dynamic programming without semirings.
In EACL, pages 318?326.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Y Grandvalet and Y Bengio. 2004. Semi-supervised
learning by entropy minimization. In NIPS, pages
529?536.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT, pages 53?64.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In ACL, pages
209?216.
Dan Klein and Christopher D. Manning. 2004. Pars-
ing and hypergraphs. New developments in parsing
technology, pages 351?372.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modelling using the maxi-
mum entropy principle. In Proc. ARPA Human Lan-
guage Technologies Workshop, pages 81?86.
Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In AMTA, pages 133?142.
Zhifei Li and Sanjeev Khudanpur. 2009. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09, pages 26?30.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In ACL, pages 609?616.
Adam Lopez. 2009. Translation as weighted deduc-
tion. In EACL, pages 532?540.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL, pages
311?318.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In EMNLP.
B. A. Pearlmutter and J. M. Siskind. 2007. Lazy mul-
tivariate higher-order forward-mode ad. In Proceed-
ings of the 34th Annual Symposium on Principles of
Programming Languages (POPL), pages 155?160.
Fernando C. N. Pereira and David H. D. Warren. 1983.
Parsing as deduction. In ACL, pages 137?144.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In ACL, pages 271?279.
Kenneth Rose. 1998. Deterministic annealing for clus-
tering, compression, classification, regression, and
related optimization problems. In Proceedings of
the IEEE, pages 2210?2239.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1994. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In ACL,
pages 787?794.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum-Bayes-
risk decoding for statistical machine translation. In
EMNLP, pages 620?629.
51
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 101?110,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Graphical Models over Multiple Strings
?
Markus Dreyer and Jason Eisner
Department of Computer Science / Johns Hopkins University
Baltimore, MD 21218, USA
{markus,jason}@cs.jhu.edu
Abstract
We study graphical modeling in the case of string-
valued random variables. Whereas a weighted
finite-state transducer can model the probabilis-
tic relationship between two strings, we are inter-
ested in building up joint models of three or more
strings. This is needed for inflectional paradigms
in morphology, cognate modeling or language re-
construction, and multiple-string alignment. We
propose a Markov Random Field in which each
factor (potential function) is a weighted finite-state
machine, typically a transducer that evaluates the
relationship between just two of the strings. The
full joint distribution is then a product of these fac-
tors. Though decoding is actually undecidable in
general, we can still do efficient joint inference
using approximate belief propagation; the nec-
essary computations and messages are all finite-
state. We demonstrate the methods by jointly pre-
dicting morphological forms.
1 Overview
This paper considers what happens if a graphical
model?s variables can range over strings of un-
bounded length, rather than over the typical finite
domains such as booleans, words, or tags. Vari-
ables that are connected in the graphical model are
related by some weighted finite-state transduction.
Graphical models have become popular in ma-
chine learning as a principled way to work with
collections of interrelated random variables. Most
often they are used as follows:
1. Build: Manually specify the n variables of
interest; their domains; and the possible di-
rect interactions among them.
2. Train: Train this model?s parameters ? to
obtain a specific joint probability distribution
p(V
1
, . . . , V
n
) over the n variables.
3. Infer: Use this joint distribution to predict
the values of various unobserved variables
from observed ones.
?
Supported by the Human Language Technology Center
of Excellence at Johns Hopkins University, and by National
Science Foundation grant No. 0347822 to the second author.
Note that 1. requires intuitions about the domain;
2. requires some choice of training procedure; and
3. requires a choice of exact or approximate infer-
ence algorithm.
Our graphical models over strings are natural
objects to investigate. We motivate them with
some natural applications in computational lin-
guistics (section 2). We then give our formalism:
a Markov Random Field whose potential functions
are rational weighted languages and relations (sec-
tion 3). Next, we point out that inference is in gen-
eral undecidable, and explain how to do approxi-
mate inference using message-passing algorithms
such as belief propagation (section 4). The mes-
sages are represented as weighted finite-state ma-
chines.
Finally, we report on some initial experiments
using these methods (section 7). We use incom-
plete data to train a joint model of morphological
paradigms, then use the trained model to complete
the data by predicting unseen forms.
2 Motivation
The problem of mapping between different forms
and representations of strings is ubiquitous in nat-
ural language processing and computational lin-
guistics. This is typically done between string
pairs, where a pronunciation is mapped to its
spelling, an inflected form to its lemma, a spelling
variant to its canonical spelling, or a name is
transliterated from one alphabet into another.
However, many problems involve more than just
two strings:
? in morphology, the inflected forms of a (possi-
bly irregular) verb are naturally considered to-
gether as a whole morphological paradigm in
which different forms reinforce one another;
? mapping an English word to its foreign translit-
eration may be easier when one considers the
orthographic and phonological forms of both
words;
? similar cognates in multiple languages are nat-
urally described together, in orthographic or
phonological representations, or both;
101
? modern and ancestral word forms form a phylo-
genetic tree in historical linguistics;
? in bioinformatics and in system combination,
multiple sequences need to be aligned in order
to identify regions of similarity.
We propose a unified model for multiple strings
that is suitable for all the problems mentioned
above. It is robust and configurable and can
make use of task-specific overlapping features. It
learns from observed and unobserved, or latent, in-
formation, making it useful in supervised, semi-
supervised, and unsupervised settings.
3 Formal Modeling Approach
3.1 Variables
A Markov Random Field (MRF) is a joint model
of a set of random variables, V = {V
1
, . . . , V
n
}.
We assume that all variables are string-valued, i.e.
the value of V
i
may be any string ? ?
?
i
, where ?
i
is some finite alphabet.
We may use meaningful names for the integers
i, such as V
2SA
for the 2nd singular past form of a
verb.
The assumption that all variables are string-
valued is not crucial; it merely simplifies our
presentation. It is, however, sufficient for many
practical purposes, since most other discrete ob-
jects can be easily encoded as strings. For exam-
ple, if V
1
is a part of speech tag, it may be en-
coded as a length-1 string over the finite alphabet
?
1
def
= {Noun,Verb, . . .}.
3.2 Factors
A Markov Random Field defines a probability for
each assignment A of values to the variables in V:
p(A)
def
=
1
Z
m
?
j=1
F
j
(A) (1)
This distribution over assignments is specified by
the collection of factors F
j
: A 7? R
?0
. Each
factor (or potential function) is a function that de-
pends on only a subset of A.
Fig. 1 displays an undirected factor graph, in
which each factor is connected to the variables
that it depends on. F
1
, F
3
, F
5
in this example are
unary factors because each one scores the value
of a single variable, while F
2
, F
4
, F
6
are binary
factors.
F
2
F
6
F
5
F
3
F
1
F
4
V
inf
V
2SA
V
3SE
Figure 1: Example of a factor graph. Black boxes represent
factors, circles represent variables (infinitive, 2nd past, and
3rd present-tense forms of the same verb; different samples
from the MRF correspond to different verbs). Binary factors
evaluate how well one string can be transduced into another,
summing over all transducer paths (i.e., alignments, which
are not observed in training).
In our setting, we will assume that each unary
factor is specified by a weighted finite-state au-
tomaton (WFSA) whose weights fall in the semir-
ing (R
?0
,+,?). Thus the score F
3
(. . . , V
2SA
=
x, . . .) is the total weight of all paths in the F
3
?s
WFSA that accept the string x ? ?
?
2SA
. Each
path?s weight is the product of its component arcs?
weights, which are non-negative.
Similarly, we assume that each binary factor is
specified by a weighted finite-state transducer
(WFST). Such a model is essentially a generaliza-
tion of stochastic edit distance (Ristad and Yian-
ilos, 1996) in which the edit probabilities can be
made sensitive to a finite summary of context.
Formally, a WFST is an automaton that resem-
bles a weighted FSA, but it nondeterministically
reads two strings x, y in parallel from left to right.
The score of (x, y) is given by the total weight of
all accepting paths in the WFST that map x to y.
For example, different paths may consider various
monotonic alignments of x with y, and we sum
over these mutually exclusive possibilities.
1
A factor might depend on k > 2 variables. This
requires a k-tape weighted finite-state machine
(WFSM), an obvious generalization where each
path reads k strings in some alignment.
2
To ensure that Z is finite in equation (1), we can
require each factor to be a ?proper? WFSM, i.e.,
its accepting paths have finite total weight (even if
the WFSM is cyclic, with infinitely many paths).
1
Each string is said to be on a different ?tape,? which has
its own ?read head,? allowing the WFSM to maintain a sep-
arate position in each string. Thus, a path in a WFST may
consume any number of characters from x before consuming
the next character from y.
2
Weighted acceptors and transducers are the cases k = 1
and k = 2, which are said to define rational languages and
rational relations.
102
3.3 Parameters
Our probability model has trainable parameters: a
vector of feature weights ? ? R. Each arc in each
WFSM has a real-valued weight that depends on ?.
Thus, tuning ? during training will change the arc
weights, hence the path weights, the factor func-
tions, and the whole probability distribution p(A).
Designing the probability model includes spec-
ifying the topology and weights of each WFSM.
Eisner (2002) explains how to specify and train
such parameterized WFSMs. Typically, the
weight of an arc is a simple sum like ?
12
+ ?
55
+
?
72
, where ?
12
is included on all arcs that share
feature 12. However, more interesting parameter-
izations arise if the WFSM is constructed by op-
erations such as transducer composition, or from a
weighted regular expression.
3.4 Power of the formalism
Factored finite-state string models (1) were orig-
inally suggested by the second author, in Kempe
et al (2004). That paper showed that even in the
unweighted case, such models could be used to en-
code relations that could not be recognized by any
k-tape FSM. We offer a more linguistic example
as a small puzzle. We invite the reader to spec-
ify a factored model (consisting of three FSTs as
in Fig. 1) that assigns positive probability to just
those triples of character strings (x, y, z) that have
the form (red ball, ball red, red), (white house,
house white, white), etc. This uses the auxiliary
variable Z to help encode a relation between X
and Y that swaps words of unbounded length. By
contrast, no FSM can accomplish such unbounded
swapping, even with 3 or more tapes.
Such extra power might be linguistically useful.
Troublingly, however, Kempe et al (2004) also
observed that the framework is powerful enough to
express computationally undecidable problems.
3
This implies that to work with arbitrary models,
we will need approximate methods.
4
Fortunately,
the graphical models community has already de-
3
Consider a simple model with two variables and two bi-
nary factors: p(V
1
, V
2
)
def
=
1
Z
? F
1
(V
1
, V
2
) ? F
2
(V
1
, V
2
). Sup-
pose F
1
is 1 or 0 according to whether its arguments are
equal. Under this model, p() < 1 iff there exists a string
x 6=  that can be transduced to itself by the unweighted
transducer F
2
. This question can be used to encode any in-
stance of Post?s Correspondence Problem, so is undecidable.
4
Notice that the simplest approximation to cure undecid-
ability would be to impose an arbitrary maximum on string
length, so that the random variables have a finite domain, just
as in most discrete graphical models.
V
F
U
?V?F ?F?U
Figure 2: Illustration of messages being passed from variable
to factor and factor to variable. Each message is represented
by a finite-state acceptor.
veloped many such methods, to deal with the com-
putational intractability (if not undecidability) of
exact inference.
4 Approximate Inference
In this paper, we focus on how belief propagation
(BP)?a simple well-known method for approxi-
mate inference in MRFs (Bishop, 2006)?can be
used in our setting. BP in its general form has
not yet been widely used in the NLP community.
5
However, it is just a generalization to arbitrary
factor graphs of the familiar forward-backward al-
gorithm (which operates only on chain-structured
factor graphs). The algorithm becomes approxi-
mate (and may not even converge) when the factor
graphs have cycles. (In that case it is more prop-
erly called ?loopy belief propagation.?)
4.1 Belief propagation
We first sketch how BP works in general. Each
variable V in the graphical model maintains a be-
lief about its value, in the form of a marginal dis-
tribution p?
V
over the possible values of V . The
final beliefs are the output of the algorithm.
Beliefs arise from messages that are sent be-
tween the variables and factors along the edges of
the factor graph. Variable V sends factor F a mes-
sage ?
V?F
, which is an (unnormalized) probabil-
ity distribution over V ?s values v, computed by
?
V?F
(v) :=
?
F
?
?N (V ),F
?
6=F
?
F
?
?V
(v) (2)
where N is the set of neighbors of V in the graph-
ical model. This message represents a consensus
of V ?s other neighboring factors concerning V ?s
value. It is how V tells F what its belief p?
V
would
be if F were absent. Informally, it communicates
to F : Here is what my value would be if it were up
to my other neighboring factors F
?
to determine.
5
Notable exceptions are Sutton et al (2004) for chunking
and tagging, Sutton and McCallum (2004) for information
extraction, Smith and Eisner (2008) for dependency parsing,
and Cromier`es and Kurohashi (2009) for alignment.
103
The factor F can then collect such incoming
messages from neighboring variables and send its
own message on to another neighbor U . Such a
message ?
F?U
suggests good values for U , in the
form of an (unnormalized) distribution over U ?s
values u, computed by
?
F?U
(u) :=
?
A s.t.A[U ]=u
F (A)
?
U
?
?N (F ),U
?
6=U
?
U
?
?F
(A[U
?
])
(3)
where A is an assignment to all variables, and
A[U ] is the value of variable U in that assign-
ment. This message represents F ?s prediction of
U ?s value based on its other neighboring variables
U
?
. Informally, via this message, F tells U : Here
is what I would like your value to be, based on
the messages that my other neighboring variables
have sent me about their values, and how I would
prefer you to relate to them.
Thus, each edge of the factor graph maintains
two messages ?
V?F
, ?
F?V
. All messages are
updated repeatedly, in some order, using the two
equations above, until some stopping criterion is
reached.
6
The beliefs are then computed:
p?
V
(v)
def
=
?
F?N (V )
?
F?V
(v) (4)
If variable V is observed, then the right-hand
sides of equations (2) and (4) are modified to tell
V that it must have the observed value v. This is
done by multiplying in an extra message ?
obs?V
that puts probability 1 on v
7
and 0 on other val-
ues. That affects other messages and beliefs. The
final belief at each variable estimates its posterior
marginal under the MRF (1), given all observa-
tions.
4.2 Finite-state messages in BP
Both ?
V?F
and ?
F?V
are unnormalized distribu-
tions over the possible values of V ?in our case,
strings. A distribution over strings is naturally
represented by a WFSA. Thus, belief propagation
translates to our setting as follows:
? Each message is a WFSA.
? Messages are typically initialized to a one-state
WFSA that accepts all strings in ?
?
, each with
6
Preferably when the beliefs converge to some fixed point
(a local minimum of the Bethe free energy). However, con-
vergence is not guaranteed.
7
More generally, on all possible observed variables.
weight 1.
8
? Taking a pointwise product of messages to V in
equation (2) corresponds to WFSA intersection.
? If F in equation (3) is binary,
9
then there is only
one U
?
. Then the outgoing message ?
F?U
, a
WFSA, is computed as domain(F ? ?
U
?
?F
).
Here ? composes the factor WFST with the in-
coming message WFSA, yielding a WFST that
gives a joint distribution over (U,U
?
). The
domain operator projects this WFST onto the U
side to obtain a WFSA, which corresponds to
marginalizing to obtain a distribution over U .
? In general, F is a k-tape WFSM. Equation (3)
?composes? k ? 1 of its tapes with k ? 1 in-
coming messages ?
U
?
?F
, to construct a joint
distribution over the k variables in N (F ), then
projects onto the k
th
tape to marginalize over the
k?1 U
?
variables and get a distribution over U .
All this can be accomplished by theWFSM gen-
eralized composition operator  (Kempe et al,
2004).
After projecting, it is desirable to determinize
the WFSA. Otherwise, the summation in (3) is
only implicit?the summands remain as distinct
paths in theWFSA
10
?and thus theWFSAs would
get larger and larger as BP proceeds. Unfortu-
nately, determinizing a WFSA still does not guar-
antee a small result. In fact it can lead to expo-
nential blowup, or even infinite blowup.
11
Thus,
in practice we recommend against determinizing
the messages, which may be inherently complex.
To shrink a message, it is safer to approximate it
with a small deterministic WFSA, as discussed in
the next section.
4.3 Approximation of messages
In our domain, it is possible for the finite-state
messages to grow unboundedly in size as they flow
around a cycle. After all, our messages are not
just multinomial distributions over a fixed finite
8
This is an (improper) uniform distribution over ?
?
. Al-
though is not a proper WFSA (see section 3.2), there is an
upper bound on the weights it assigns to strings. That guar-
antees that all the messages and beliefs computed by (2)?(4)
will be proper FSMs, provided that all the factors are proper
WFSMs.
9
If it is unary, (3) trivially reduces to ?
F?U
= F .
10
The usual implementation of projection does not change
the topology of the WFST, but only deletes the U
?
part of its
arc labels. Thus, multiple paths that accept the same value of
U remain distinct according to the distinct values of U
?
that
they were paired with before projection.
11
If there is no deterministic equivalent (Mohri, 1997).
104
set. They are distributions over the infinite set ?
?
.
A WFSA represents this in finite space, but more
complex distributions require bigger WFSAs, with
more distinct states and arc weights.
Facing the same problem for distributions over
the infinite set R, Sudderth et al (2002) simplified
each message ?
V?F
, approximating a complex
Gaussian mixture by using fewer components.
We could act similarly, variationally approxi-
mating a large WFSA P with a smaller one Q.
Choose a family of message approximations (such
as bigram models) by specifying the topology for
a (small) deterministic WFSA Q. Then choose
Q?s edge weights to minimize the KL divergence
KL(P ?Q). This can be done in closed form.
12
Another possible procedure?used in the ex-
periments of this paper?approximates ?
V?F
by
pruning it back to a finite set of most plausible
strings.
13
Equation (2) requests an intersection
of several WFSAs, e.g., ?
F
1
?V
? ?
F
2
?V
? ? ? ? .
List all strings that appear on any of the 1000-
best paths in any of these WFSAs, removing du-
plicates. Let
?
Q be a uniform distribution over this
combined list of plausible strings, represented as
a determinized, minimized, acyclic WFSA. Now
approximate the intersection of equation (2) as
((
?
Q ? ?
F
1
?V
) ? ?
F
2
?V
) ? ? ? ? . This is efficient
to compute and has the same topology as
?
Q.
5 Training the Model Parameters
Any standard training method for MRFs will
transfer naturally to our setting. In all cases we
draw on Eisner (2002), who showed how to train
the parameters ? of a single WFST, F , to (locally)
maximize the joint or conditional probability of
fully or partially observed training data. This in-
volves computing the gradient of that likelihood
function with respect to ?.
14
12
See Li et al (2009, footnote 9) for a sketch of the con-
struction, which finds locally normalized edge weights. Or
if Q is large but parameterized by some compact parameter
vector ?, so we are only allowed to control its edge weights
via ?, then Li and Eisner (2009, section 6) explain how to
minimize KL(P ?Q) by gradient descent. In both cases Q
must be deterministic.
We remark that if a factor F were specified by a syn-
chronous grammar rather than a WFSM, then its outgoing
messages would be weighted context-free languages. Exact
intersection of these is undecidable, but they too can be ap-
proximated variationally by WFSAs, with the same methods.
13
We are also considering other ways of adaptively choos-
ing the topology of WFSA approximations at runtime, partic-
ularly in conjunction with expectation propagation.
14
The likelihood is usually non-convex; even when the
two strings are observed (supervised training), their accepting
We must generalize this to train a product of
WFSMs. Typically, training data for an MRF (1)
consists of some fully or partially observed IID
samples of the joint distribution p(V
1
, . . . V
n
). It
is well-known how to tune an MRF?s parameters ?
by stochastic gradient descent to locally maximize
the probability of this training set, even though
both the probability and its gradient are in general
intractable to compute in an MRF. The gradient is
a sum of quantities, one for each factor F
j
. While
the summand for F
j
cannot be computed exactly,
it can be estimated using the BP messages to F
j
.
Roughly speaking, the gradient for F
j
is computed
much as in supervised training (see above), but
treating any message ?
V
i
?F
j
as an uncertain ob-
servation of V
i
?a form of noisy supervision.
15
Our concerns about training are the same as
for any MRF. First of all, BP is approximate.
Kulesza and Pereira (2008) warn that its estimates
of the gradient can be misleading. Second, semi-
supervised training (which we will attempt below)
is always difficult and prone to local optima. As
in EM, a small number of supervised examples for
some variable may be drowned out by many nois-
ily reconstructed examples.
Faster and potentially more stable approaches
include the piecewise training methods of Sut-
ton and McCallum (2008), which train the factors
independently or in small groups. In the semi-
supervised case, each factor can be trained on only
the supervised forms available for it. It might be
useful to reweight the trained factors (cf. Smith et
al. (2005)), or train the factors consecutively (cf.
Fahlman and Lebiere (1990)), in a way that mini-
mizes the loss of BP decoding on held-out data.
6 Comparison With Other Approaches
6.1 Multi-tape WFSMs
In principle, one could use a 100-tape WFSM to
jointly model the 100 distinct forms of a typical
Polish verb. In other words, the WFSM would de-
scribe the distribution of a random variable
~
V =
?V
1
, . . . , V
100
?, where each V
i
is a string. One
would train the parameters of the WFSM on a
sample of
~
V , each sample being a fully or partially
observed paradigm for some Polish verb. The re-
sulting distribution could be used to infer missing
forms for these or other verbs.
path through the WFST may be ambiguous and unobserved.
15
See Bishop (2006), or consult Smith and Eisner (2008)
for notation close to that of this paper.
105
As a simple example, either a morphological
generator or a morphological analyzer might need
the probability that krzycza?oby is the neuter third-
person singular conditional imperfective of krzy-
cze?c, despite never having observed it in training.
The model determines this probability based on
other observed and hypothesized forms of krzy-
cze?c, using its knowledge of how neuter third-
person singular conditional imperfectives are re-
lated to these other forms in other verbs.
Unfortunately, such a 100-tape WFSM would
be huge, with an astronomical number of arcs
(each representing a possible 100-way edit opera-
tion). Our approach is to factor the problem into a
number of (e.g.) pairwise relationships among the
verb forms. Using a factored distribution has sev-
eral benefits over the k-tape WFSM: (1) a smaller
representation in memory, (2) a small number
of parameters to learn, (3) efficient approximate
computation that takes advantage of the factored
structure, (4) the ability to reuse WFSAs and WF-
STs previously developed for smaller problems,
(5) additional modeling power.
6.2 Simpler graphical models on strings
Some previous researchers have used factored
joint models of several strings. To our knowledge,
they have all chosen acyclic, directed graphical
models. The acyclicity meant that exact inference
was at least possible for them, if not necessarily ef-
ficient. The factors in these past models have been
WFSTs (though typically simpler than the ones we
will use).
Many papers have used cascades of probabilis-
tic finite-state transducers. Such a cascade may
be regarded as a directed graphical model with a
linear-chain structure. Pereira and Riley (1997)
built a speech recognizer in this way, relating
acoustic to phonetic to lexical strings. Simi-
larly, Knight and Graehl (1997) presented a gen-
erative cascade using 4 variables and 5 factors:
p(w, e, j, k, o)
def
= p(w) ?p(e | w) ?p(j | e) ?p(k | j)
?p(o | k) where e is an English word sequence, w
its pronunciation, j a Japanese version of the pro-
nunciation, k a katakana rendering of the Japanese
pronunciation, and o an OCR-corrupted version of
the katakana. Knight and Graehl used finite-state
operations to perform inference at test time, ob-
serving o and recovering the most likely w, while
marginalizing out e, j, and k.
Bouchard-C?ot?e et al (2009) reconstructed an-
cient word forms given modern equivalents. They
used a directed graphical model, whose tree struc-
ture reflected the evolutionary development of the
modern languages, and which included latent vari-
ables for historical intermediate forms that were
never observed in training data. They used Gibbs
sampling rather than an exact solution (possible on
trees) or a variational approximation (like our BP).
Our work seeks to be general in terms of the
graphical model structures used, as well as effi-
cient through the use of BP with approximate mes-
sages. We also seek to avoid local normalization,
using a globally normalized model.
16
6.3 Unbounded objects in graphical models
We distinguish our work from ?dynamic? graph-
ical models such as Dynamic Bayesian Networks
and Conditional Random Fields, where the string
brechen would be represented by creating 7 letter-
valued variables. Those methods can represent
strings (or paths) of any length?but the length for
each training or test string must be specified in ad-
vance, not inferred. Furthermore, it is awkward
and costly to model unknown alignments, since
the variables are position-specific, and any posi-
tion in brechen could in principle align with any
position in brichst. WFSTs are a much more natu-
ral and flexible model of string pairs.
We also distinguish our work from current non-
parametric Bayesian models, which sometimes
generate unbounded strings, trees, or grammars. If
they generate two unbounded objects, they model
their relationship by a single synchronous genera-
tion process (akin to Section 6.1), rather than by
a globally normalized product of overlapping fac-
tors.
7 Experiments
To study our approach, we conducted initial ex-
periments that reconstruct missing word forms in
morphological paradigms. In inflectional mor-
phology, each uninflected verb form (lemma) is
associated with a vector of forms that are inflected
for tense, person, number, etc. Some inflected
forms may be observed frequently in natural text,
others rarely. Two variables that are usually pre-
dictable from each other may or may not keep this
relationship in the case of an irregular verb.
16
Although we do normalize locally during piecewise
training (see section 7.3).
106
(a) # paradigms 9,393
(b) # finite forms per paradigm 9
(c) # hidden finite forms per paradigm (avg.) 8.3
(d) # paradigms with some finite form(s) observed 2,176
(e) In (d), # of finite forms observed (avg.) 3.4
Table 1: Statistics of our training data.
Our task is to reconstruct (generate) specific un-
observed morphological forms in a paradigm by
learning from observed ones. This is a particu-
larly interesting semisupervised scenario, because
different subsets of the variables are observed on
different examples.
7.1 Experimental data
We used orthographic rather than phonological
forms. We extracted morphological paradigms for
all 9393 German verbs in the CELEX morpholog-
ical database. Each paradigm lists 5 present-tense
and 4 past-tense indicative forms, as well as the
verb?s lemma, for a total of 10 string-valued vari-
ables.
17
In each paradigm, we removed, or hid,
verb forms that occur only rarely in natural text,
i.e, verb forms with a small frequency figure pro-
vided by CELEX.
18
All paradigms other than sein
(?to be?) were now incompletely observed. Table 1
gives some statistics.
7.2 Model factors and parameters
Our current MRF uses only binary factors. Each
factor is aWFST that is trained to relate 2 of the 10
variables (morphological forms). Each WFST can
score an aligned pair using a log-linear model that
counts features in a sliding 3-character window.
To score an unaligned pair, it sums over all pos-
sible alignments. Specifically, our WFST topol-
ogy and parameterization follow the state-of-the-
art approach to supervised morphology in Dreyer
et al (2008), although we dropped some of their
features to speed up these early experiments.
19
We
17
Some pairs of forms are always identical in German,
hence are treated as a single form by CELEX. We likewise
use a single variable?these are the ?1,3? variables in Fig. 3.
Occasionally a form is listed as UNKNOWN. We neither
train nor evaluate on such forms, although the model will still
predict them.
18
The frequency figure for each word form is based on
counts in the Mannheim News corpus. We hide forms with
frequency < 10.
19
We dropped their latent classes and regions as well as
features that detected which characters were orthographic
vowels. Also, we retained their ?target language model fea-
tures? only in the baseline ?U? model, since elsewhere they
implemented and manipulated all WFSMs using
the OpenFST library (Allauzen et al, 2007).
7.3 Training in the experiments
We trained ? on the incompletely observed
paradigms. As suggested in section 5, we used
a variant of piecewise pseudolikelihood training
(Sutton and McCallum, 2008). Suppose there is
a binary factor F attached to forms U and V . For
any value of ?, we can define p
UV
(U | V ) from
the tiny MRF consisting only of U , V , and F .
We can therefore compute the goodness L
UV
def
=
log p
UV
(u
i
| v
i
)+log
V U
(v
i
| u
i
),
20
summed over
all observed (U, V ) pairs in training data. We at-
tempted to tune ? to maximize the total L
UV
over
all U, V pairs,
21
regularized by subtracting ||?||
2
.
Note that different factors thus enjoyed different
amounts of observed training data, but training
was fully supervised (except for the unobserved
alignments between u
i
and v
i
).
7.4 Inference in the experiments
At test time, we are given each lemma (e.g.
brechen) and all its observed (frequent) inflected
forms (e.g., brachen, bricht,. . . ), and are asked to
predict the remaining (rarer) forms (e.g., breche,
brichst, . . . ).
We run approximate joint inference using be-
lief propagation.
22
We extract our output from the
final beliefs: for each unseen variable V , we pre-
seemed to hurt in our current training setup.
We followed Dreyer et al (2008) in slightly pruning the
space of possible alignments. We compensated by replacing
their WFST, F , with the union F ? 10
?12
(0.999? ? ?)
?
.
This ensured that the factor could still map any string to any
other string (though perhaps with very low weight), guaran-
teeing that the intersection at the end of section 4.3 would be
non-empty.
20
The second term is omitted if V is the lemma. We do
not train the model to predict the lemma since it is always
observed in test data.
21
Unfortunately, just before press time we discovered that
this was not quite what we had done. A shortcut in our im-
plementation trained p
UV
(U | V ) and p
V U
(V | U) sepa-
rately. This let them make different use of the (unobserved)
alignments?so that even if each individually liked the pair
(u, v), they might not have been able to agree on the same
accepting path for it at test time. This could have slightly
harmed our joint inference results, though not our baselines.
22
To derive the update order for message passing, we take
an arbitrary spanning tree over the factor graph, and let O be
a list of all factors and variables that is topologically sorted
according to the spanning tree, with the leaves of the tree
coming first. We then discard the spanning tree. A single it-
eration visits all factors and variables in order of O, updating
each one?s messages to later variables and factors, and then
visits all factors and variables in reverse order, updating each
one?s messages to earlier variables and factors.
107
dict its value to be argmax
v
p?
V
(v). This predic-
tion considers the values of all other unseen vari-
ables but sums over their possibilities. This is the
Bayes-optimal decoder for our scoring function,
since that function reports the fraction of individ-
ual forms that were predicted perfectly.
23
7.5 Model selection of MRF topology
It is hard to know a priori what the causal relation-
ships might be in a morphological paradigm. In
principle, one would like to automatically choose
which factors to have in the MRF. Or one could
start with many factors, but use methods such as
those suggested in section 5 to learn that certain
less useful factors should be left weak to avoid
confusing loopy BP.
For our present experiments, we simply com-
pared several fixed model topologies (Fig. 3).
These were variously unconnected (U), chain
graphs (C1,. . . , C4), trees (T1, T2), or loopy
graphs (L1,. . . , L4). We used several factor graphs
that differ only by one or two added factors and
compared the results. The graphs were designed
by hand; they connect some forms with similar
morphological properties more or less densely.
We trained different models using the observed
forms in the 9393 paradigms as training data. The
first 100 paradigms were then used as develop-
ment data for model selection:
24
we were given
the answers to their hidden forms, enabling us to
compare the models. The best model was then
evaluated on the 9293 remaining paradigms.
7.6 Development data results
The models are compared on development data
in Table 2. Among the factor graphs we evalu-
ated, we find that L4 (see Fig. 3) performs best
overall (whole-word accuracy 82.1). Note that the
unconnected graph U does not perform very well
(69.0), but using factor graphs with more connect-
ing factors generally helps overall accuracy (see
C1?C3). Note, however, that in some cases the ad-
ditional structure hurts: The chain model C4 and
the loopy model L1 perform relatively badly. The
23
If we instead wished to maximize the fraction of entire
paradigms that were predicted perfectly, then we would have
approximated full MAP decoding over the paradigm (Viterbi
decoding) by using max-product BP. Other loss functions
(e.g., edit distance) would motivate other decoding methods.
24
Using these paradigms was simply a quick way to avoid
model selection by cross-validation. If data were really as
sparse as our training setup pretends (see Table 2), then 100
complete paradigms would be too valuable to squander as
mere development data.
(U)
1
Pres Past
S
i
n
g
u
l
a
r
P
l
u
r
a
l
2
3
1,3
2
1,3
2
2
1,3
1
2
3
1,3
2
1,3
2
2
1,3
1
2
3
1,3
2
1,3
2
2
1,3
(C1) (C2)
(C3)
1
2
3
1,3
2
1,3
2
2
1,3
1
2
3
1,3
2
1,3
2
2
1,3
(C4)
1
2
3
1,3
2
1,3
2
2
1,3
(T1)
1
2
3
1,3
2
1,3
2
2
1,3
(T2) (L1)
1
2
3
1,3
2
1,3
2
2
1,3
(L2)
1
2
3
1,3
2
1,3
2
2
1,3
1
2
3
1,3
2
1,3
2
2
1,3
(L3)
Pres Past Pres Past Pres Past Pres Past
S
i
n
g
u
l
a
r
P
l
u
r
a
l
1
2
3
1,3
2
1,3
2
2
1,3
(L4)
Figure 3: The graphs that we evaluate on development data.
The nodes represent morphological forms, e.g. the first node
in the left of each graph represents the first person singular
present. Each variable is also connected to the lemma (not
shown). See results in Table 2.
reason for such a performance degradation is that
undertrained factors were used: The factors relat-
ing second-person to second-person forms, for ex-
ample, are trained from only 8 available examples.
Non-loopy models always converge (exactly) in
one iteration (see footnote 22). But even our loopy
models appeared to converge in accuracy within
two iterations. Only L3 and L4 required the sec-
ond iteration, which made tiny improvements.
7.7 Test data results
Based on the development results, we selected
model L4 and tested on the remaining 9293
paradigms.
We regard the unconnected model U as a base-
line to improve upon. We also tried a rather differ-
ent baseline as in (Dreyer et al, 2008). We trained
the machine translation toolkit Moses (Koehn et
al., 2007) to translate groups of letters rather than
groups of words (?phrases?). For each form f
to be predicted, we trained a Moses model on
all supervised form pairs (l, f) available in the
data, to learn a prediction for the form given the
lemma l. The M,3 condition restricted Moses use
?phrases? no longer than 3 letters, comparable to
our own trigram-based factors (see section 7.2).
M,15 could use up to 15 letters.
Again, our novel L4 model far outperformed
the others overall. Breaking the results down by
form, we find that this advantage mainly comes
from the 3 forms with the fewest observed train-
ing examples (Table 3, first 3 rows). The M and
U models are barely able to predict these forms at
all from the lemma, but L4 can predict them bet-
108
Unconn. Chains Trees Loops
U C1 C2 C3 C4 T1 T2 L1 L2 L3 L4
69.0 72.9 73.4 74.8 65.2 78.1 78.7 62.3 79.6 78.9 82.1
Table 2: Whole-word accuracies of the different models in reconstructing the missing forms in morphological paradigms, here
on 100 verbs (development data). The names refer to the graphs in Fig. 3. We selected L4 as final model (Table 3).
Form # obs. M,3 M,15 U L4
2.Sg.Pa. 4 0.0 0.2 0.8 69.7
2.Pl.Pa. 9 0.9 1.1 1.4 45.6
2.Sg.Pr. 166 49.4 62.6 74.7 90.5
1.Sg.Pr. 285 99.6 98.8 99.3 97.2
1,3.Pl.Pa. 673 46.5 78.3 75.0 75.6
1,3.Sg.Pa. 1124 65.0 88.8 84.0 74.8
2.Pl.Pr. 1274 98.3 99.2 99.0 96.4
3.Sg.Pr. 1410 91.0 95.9 95.2 88.2
1,3.Pl.Pr. 1688 99.8 98.9 99.8 98.0
All 6633 59.2 67.3 68.0 81.2
Table 3: Whole-word accuracies on the missing forms from
9293 test paradigms. The Moses baselines and our un-
connected model (U) predict each form separately from the
lemma, which is always observed. L4 uses all observations
jointly, running belief propagation for decoding. Moses,15
memorizes phrases of length up to 15, all other models use
max length 3. The table is sorted by the column ?# obs.?,
which reports the numbers of observations for a given form.
ter by exploiting other observed or latent forms.
By contrast, well-trained forms were already easy
enough for the M and U models that L4 had little
new to offer and in fact suffered from its approxi-
mate training and/or inference.
Leaving aside the comparisons, it was useful to
confirm that loopy BP could be used in this set-
ting at all. 8014 of the 9293 test paradigms had
? 2 observed forms (in addition to the lemma)
but ? 7 missing forms. One might have expected
that loopy BP would have failed to converge, or
converged to the wrong thing. Nonetheless, it
achieved quite respectable success at exactly pre-
dicting various inflected forms.
For the curious, Table 4 shows accuracies
grouped by different categories of paradigms,
where the category is determined by the number
of missing forms to predict. Most paradigms fall
in the category where 7 to 9 forms are missing, so
the accuracies in that line are similar to the overall
accuracies in Table 3.
8 Conclusions
We have proposed that one can jointly model sev-
eral multiple strings by using Markov Random
Fields. We described this formally as an undi-
# missing # paradig. M,3 M,15 U L4
1?3 205 20.3 20.8 26.8 74.4
4?6 1037 44.2 50.5 52.7 82.8
7?9 8014 60.6 68.8 69.4 81.1
Table 4: Accuracy on test data, reported separately for
paradigms in which 1?3, 4?6, or 7?9 forms are missing.
Missing words have CELEX frequency count< 10; these are
the ones to predict. (The numbers in col. 2 add up to 9256,
not 9293, since some paradigms are incomplete in CELEX to
begin with, with no forms to be removed or evaluated.)
rected graphical model with string-valued vari-
ables and whose factors (potential functions) are
defined by weighted finite-state transducers. Each
factor evaluates some subset of the strings.
Approximate inference can be done by loopy
belief propagation. The messages take the form
of weighted finite-state acceptors, and are con-
structed by standard operations. We explained
why the messages might become large, and gave
methods for approximating them with smaller
messages. We also discussed training methods.
We presented some pilot experiments on the
task of jointly predicting multiple missing verb
forms in morphological paradigms. The factors
were simplified versions of statistical finite-state
models for supervised morphology. Our MRF
for this task might be used not only to conjugate
verbs (e.g., in MT), but to guide further learning
of morphology?either active learning from a hu-
man or semi-supervised learning from the distri-
butional properties of a raw text corpus.
Our modeling approach is potentially applicable
to a wide range of other tasks, including translit-
eration, phonology, cognate modeling, multiple-
sequence alignment and system combination.
Our work ties into a broader vision of using al-
gorithms like belief propagation to coordinate the
work of several NLP models and algorithms. Each
individual factor considers some portion of a joint
problem, using classical statistical NLP methods
(weighted grammars, transducers, dynamic pro-
gramming). The factors coordinate their work by
passing marginal probabilities. Smith and Eisner
(2008) reported complementary work in this vein.
109
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proc. of CIAA, volume 4783 of Lecture
Notes in Computer Science, pages 11?23.
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer.
Alexandre Bouchard-C?ot?e, Thomas L. Griffiths, and
Dan Klein. 2009. Improved reconstruction of pro-
tolanguage word forms. In Proc. of HLT-NAACL,
pages 65?73, Boulder, Colorado, June. Association
for Computational Linguistics.
Fabien Cromier`es and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proc. of EACL,
pages 166?174, Athens, Greece, March. Association
for Computational Linguistics.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Proc. of EMNLP, Hon-
olulu, Hawaii, October.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proc. of ACL,
pages 1?8, Philadelphia, July.
Scott E. Fahlman and Christian Lebiere. 1990. The
cascade-correlation learning architecture. Technical
Report CMU-CS-90-100, School of Computer Sci-
ence, Carnegie Mellon University.
Andr?e Kempe, Jean-Marc Champarnaud, and Jason
Eisner. 2004. A note on join and auto-intersection
of n-ary rational relations. In Loek Cleophas and
Bruce Watson, editors, Proceedings of the Eind-
hoven FASTAR Days (Computer Science Techni-
cal Report 04-40). Department of Mathematics and
Computer Science, Technische Universiteit Eind-
hoven, Netherlands.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proc. of ACL, pages 128?135.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL, Companion Volume, pages 177?180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Alex Kulesza and Fernando Pereira. 2008. Structured
learning with approximate inference. In Proc. of
NIPS.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proc. of EMNLP.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. of ACL.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2).
Fernando C. N. Pereira and Michael Riley. 1997.
Speech recognition by composition of weighted fi-
nite automata. In Emmanuel Roche and Yves
Schabes, editors, Finite-State Language Processing.
MIT Press, Cambridge, MA.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learn-
ing string edit distance. Technical Report CS-TR-
532-96, Princeton University, Department of Com-
puter Science, October.
David Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proc. of EMNLP.
Andrew Smith, Trevor Cohn, and Miles Osborne.
2005. Logarithmic opinion pools for conditional
random fields. In Proc. of ACL, pages 18?25, June.
Erik B. Sudderth, Alexander T. Ihler, Er T. Ihler,
William T. Freeman, and Alan S. Willsky. 2002.
Nonparametric belief propagation. In Proc. of
CVPR, pages 605?612.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. In ICML Workshop on Sta-
tistical Relational Learning and Its Connections to
Other Fields.
Charles Sutton and Andrew McCallum. 2008. Piece-
wise training for structured prediction. Machine
Learning. In submission.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proc. of
ICML.
110
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 822?831,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Parser Adaptation and Projection
with Quasi-Synchronous Grammar Features
?
David A. Smith
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
dasmith@cs.umass.edu
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Abstract
We connect two scenarios in structured
learning: adapting a parser trained on
one corpus to another annotation style, and
projecting syntactic annotations from one
language to another. We propose quasi-
synchronous grammar (QG) features for
these structured learning tasks. That is, we
score a aligned pair of source and target
trees based on local features of the trees
and the alignment. Our quasi-synchronous
model assigns positive probability to any
alignment of any trees, in contrast to a syn-
chronous grammar, which would insist on
some form of structural parallelism.
In monolingual dependency parser adap-
tation, we achieve high accuracy in trans-
lating among multiple annotation styles
for the same sentence. On the more
difficult problem of cross-lingual parser
projection, we learn a dependency parser
for a target language by using bilin-
gual text, an English parser, and auto-
matic word alignments. Our experiments
show that unsupervised QG projection im-
proves on parses trained using only high-
precision projected annotations and far
outperforms, by more than 35% absolute
dependency accuracy, learning an unsu-
pervised parser from raw target-language
text alone. When a few target-language
parse trees are available, projection gives
a boost equivalent to doubling the number
of target-language trees.
?
The first author would like to thank the Center for Intel-
ligent Information Retrieval at UMass Amherst. We would
also like to thank Noah Smith and Rebecca Hwa for helpful
discussions and the anonymous reviewers for their sugges-
tions for improving the paper.
1 Introduction
1.1 Parser Adaptation
Consider the problem of learning a dependency
parser, which must produce a directed tree whose
vertices are the words of a given sentence. There
are many differing conventions for representing
syntactic relations in dependency trees. Say that
we wish to output parses in the Prague style and
so have annotated a small target corpus?e.g.,
100 sentences?with those conventions. A parser
trained on those hundred sentences will achieve
mediocre dependency accuracy (the proportion of
words that attach to their correct parent).
But what if we also had a large number of trees
in the CoNLL style (the source corpus)? Ide-
ally they should help train our parser. But unfor-
tunately, a parser that learned to produce perfect
CoNLL-style trees would, for example, get both
links ?wrong? when its coordination constructions
were evaluated against a Prague-style gold stan-
dard (Figure 1).
If it were just a matter of this one construction,
the obvious solution would be to write a few rules
by hand to transform the large source training cor-
pus into the target style. Suppose, however, that
there were many more ways that our corpora dif-
fered. Then we would like to learn a statistical
model to transform one style of tree into another.
We may not possess hand-annotated training
data for this tree-to-tree transformation task. That
would require the two corpora to annotate some of
the same sentences in different styles.
But fortunately, we can automatically obtain a
noisy form of the necessary paired-tree training
data. A parser trained on the source corpus can
parse the sentences in our target corpus, yielding
trees (or more generally, probability distributions
over trees) in the source style. We will then learn
a tree transformation model relating these noisy
source trees to our known trees in the target style.
822
now o

r

never now

or

never
Prague Mel??cuk
no

w

or never now or ne

ver

CoNLL MALT
Figure 1: Four of the five logically possible schemes for
annotating coordination show up in human-produced depen-
dency treebanks. (The other possibility is a reverse Mel??cuk
scheme.) These treebanks also differ on other conventions.
This model should enable us to convert the orig-
inal large source corpus to target style, giving us
additional training data in the target style.
1.2 Parser Projection
For many target languages, however, we do not
have the luxury of a large parsed ?source cor-
pus? in the language, even one in a different style
or domain as above. Thus, we may seek other
forms of data to augment our small target corpus.
One option would be to leverage unannotated text
(McClosky et al, 2006; Smith and Eisner, 2007).
But we can also try to transfer syntactic informa-
tion from a parsed source corpus in another lan-
guage. This is an extreme case of out-of-domain
data. This leads to the second task of this paper:
learning a statistical model to transform a syntac-
tic analysis of a sentence in one language into an
analysis of its translation.
Tree transformations are often modeled with
synchronous grammars. Suppose we are given a
sentence w
?
in the ?source? language and its trans-
lation w into the ?target? language. Their syn-
tactic parses t
?
and t are presumably not indepen-
dent, but will tend to have some parallel or at least
correlated structure. So we could jointly model
the parses t
?
, t and the alignment a between them,
with a model of the form p(t, a, t
?
| w,w
?
).
Such a joint model captures how t, a, t
?
mu-
tually constrain each other, so that even partial
knowledge of some of these three variables can
help us to recover the others when training or de-
coding on bilingual text. This idea underlies a
number of recent papers on syntax-based align-
ment (using t and t
?
to better recover a), grammar
induction from bitext (using a to better recover t
and t
?
), parser projection (using t
?
and a to better
Figure 2: With the English tree and alignment provided by
a parser and aligner at test time, the Chinese parser finds the
correct dependencies (see ?6). A monolingual parser?s incor-
rect edges are shown with dashed lines.
recover t), as well as full joint parsing (Smith and
Smith, 2004; Burkett and Klein, 2008).
In this paper, we condition on the 1-best source
tree t
?
. As for the alignment a, our models ei-
ther condition on the 1-best alignment or integrate
the alignment out. Our models are thus of the
form p(t | w,w
?
, t
?
, a) or, in the generative case,
p(w, t, a | w
?
, t
?
). We intend to consider other for-
mulations in future work.
So far, this is very similar to the monolingual
parser adaptation scenario, but there are a few key
differences. Since the source and target sentences
in the bitext are in different languages, there is
no longer a trivial alignment between the words
of the source and target trees. Given word align-
ments, we could simply try to project dependency
links in the source tree onto the target text. A
link-by-link projection, however, could result in
invalid trees on the target side, with cycles or dis-
connected words. Instead, our models learn the
necessary transformations that align and transform
a source tree into a target tree by means of quasi-
synchronous grammar (QG) features.
Figure 2 shows an example of bitext helping
disambiguation when a parser is trained with only
a small number of Chinese trees. With the help
of the English tree and alignment, the parser is
able to recover the correct Chinese dependen-
cies using QG features. Incorrect edges from
the monolingual parser are shown with dashed
lines. (The bilingual parser corrects additional er-
rors in the second half of this sentence, which has
been removed to improve legibility.) The parser
is able to recover the long-distance dependency
from the first Chinese word (China) to the last
(begun), while skipping over the intervening noun
823
phrase that confused the undertrained monolin-
gual parser. Although, due to the auxiliary verb,
?China? and ?begun? are siblings in English and
not in direct dependency, the QG features still
leverage this indirect projection.
1.3 Plan of the Paper
We start by describing the features we use to
augment conditional and generative parsers when
scoring pairs of trees (?2). Then we discuss in turn
monolingual (?3) and cross-lingual (?4) parser
adaptation. Finally, we present experiments on
cross-lingual parser projection in conditions when
no target language trees are available for training
(?5) and when some trees are available (?6).
2 Form of the Model
What should our model of source and target trees
look like? In our view, traditional approaches
based on synchronous grammar are problematic
both computationally and linguistically. Full in-
ference takes O(n
6
) time or worse (depending on
the grammar formalism). Yet synchronous mod-
els only consider a limited hypothesis space: e.g.,
parses must be projective, and alignments must de-
compose according to the recursive parse struc-
ture. (For example, two nodes can be aligned
only if their respective parents are also aligned.)
The synchronous model?s probability mass func-
tion is also restricted to decompose in this way,
so it makes certain conditional independence as-
sumptions; put another way, it can evaluate only
certain properties of the triple (t, a, t
?
).
We instead model (t, a, t
?
) as an arbitrary graph
that includes dependency links among the words
of each sentence as well as arbitrary alignment
links between the words of the two sentences.
This permits non-synchronous and many-to-many
alignments. The only hard constraint we impose
is that the dependency links within each sentence
must constitute a valid monolingual parse?a di-
rected projective spanning tree.
1
Given the two sentences w,w
?
, our probabil-
ity distribution over possible graphs considers lo-
cal features of the parses, the alignment, and both
jointly. Thus, we learn what local syntactic con-
figurations tend to occur in each language and how
they correspond across languages. As a result, we
might learn that parses are ?mostly synchronous,?
but that there are some systematic cross-linguistic
1
Non-projective parsing would also be possible.
divergences and some instances of sloppy (non-
parallel or inexact) translation. Our model is thus a
form of quasi-synchronous grammar (QG) (Smith
and Eisner, 2006a). In that paper, QG was applied
to word alignment and has since found applica-
tions in question answering (Wang et al, 2007),
paraphrase detection (Das and Smith, 2009), and
machine translation (Gimpel and Smith, 2009).
All the models in this paper are conditioned on
the source tree t
?
. Conditionally-trained models
of adaptation and projection also condition on the
target string w and its alignment a to w
?
and thus
have the form p(t | w,w
?
, t
?
, a); the unsupervised,
generative projection models in ?5 have the form
p(w, t, a | w
?
, t
?
).
The score s of a given tuple of trees, words, and
alignment can thus be written as a dot product of
weights w with features f and g:
s(t, t
?
, a, w,w
?
) =
?
i
w
i
f
i
(t, w)
+
?
j
w
j
g
j
(t, t
?
, a, w,w
?
)
The features f look only at target words and de-
pendencies. In the conditional models of ?3 and
?6, these features are those of an edge-factored
dependency parser (McDonald et al, 2005). In
the generative models of ?5, f has the form of a
dependency model with valence (Klein and Man-
ning, 2004). All models, for instance, have a fea-
ture template that considers the parts of speech of
a potential parent-child relation.
In order to benefit from the source language, we
also need to include bilingual features g. When
scoring a candidate target dependency link from
word x ? y, these features consider the relation-
ship of their corresponding source words x
?
and
y
?
. (The correspondences are determined by the
alignment a.) For instance, the source tree t
?
may
contain the link x
?
? y
?
, which would cause a fea-
ture for monotonic projection to fire for the x ? y
edge. If, on the other hand, y
?
? x
?
? t
?
, a
head-swapping feature fires. If x
?
= y
?
, i.e. x
and y align to the same word, the same-word fea-
ture fires. Similar features fire when x
?
and y
?
are
in grandparent-grandchild, sibling, c-command, or
none-of-the above relationships, or when y aligns
to NULL. These alignment classes are called con-
figurations (Smith and Eisner, 2006a, and follow-
ing). When training is conditioned on the target
words (see ?3 and ?6 below), we conjoin these
824
configuration features with the part of speech and
coarse part of speech of one or both of the source
and target words, i.e. the feature template has from
one to four tags.
In conditional training, the exponentiated
scores s are normalized by a constant: Z =
?
t
exp[s(t, t
?
, a, w,w
?
)]. For the generative
model, the locally normalized generative process
is explained in ?5.3.4.
Previous researchers have written fix-up rules
to massage the projected links after the fact and
learned a parser from the resulting trees (Hwa et
al., 2005). Instead, our models learn the necessary
transformations that align and transform a source
tree into a target tree. Other researchers have tack-
led the interesting task of learning parsers from
unparsed bitext alone (Kuhn, 2004; Snyder et al,
2009); our methods take advantage of investments
in high-resource languages such as English. In
work most closely related to this paper, Ganchev et
al. (2009) constrain the posterior distribution over
target-language dependencies to align to source
dependencies some ?reasonable? proportion of the
time (? 70%, cf. Table 2 in this paper). This
approach performs well but cannot directly learn
regular cross-language non-isomorphisms; for in-
stance, some fixup rules for auxiliary verbs need
to be introduced. Finally, Huang et al (2009)
use features, somewhat like QG configurations, on
the shift-reduce actions in a monolingual, target-
language parser.
3 Adaptation
As discussed in ?1, the adaptation scenario is a
special case of parser projection where the word
alignments are one-to-one and observed. To test
our handling of QG features, we performed ex-
periments in which training saw the correct parse
trees in both source and target domains, and the
mapping between them was simple and regular.
We also performed experiments where the source
trees were replaced by the noisy output of a trained
parser, making the mapping more complex and
harder to learn.
We used the subset of the Penn Treebank from
the CoNLL 2007 shared task and converted it to
dependency representation while varying two pa-
rameters: (1) CoNLL vs. Prague coordination
style (Figure 1), and (2) preposition the head vs.
the child of its nominal object.
We trained an edge-factored dependency parser
(McDonald et al, 2005) on ?source? domain data
that followed one set of dependency conventions.
We then trained an edge-factored parser with QG
features on a small amount of ?target? domain
data. The source parser outputs were produced for
all target data, both training and test, so that fea-
tures for the target parser could refer to them.
In this task, we know what the gold-standard
source language parses are for any given text,
since we can produce them from the original Penn
Treebank. We can thus measure the contribution
of adaptation loss alone, and the combined loss
of imperfect source-domain parsing with adapta-
tion (Table 1). When no target domain trees are
available, we simply have the performance of the
source domain parser on this out-of-domain data.
Training a target-domain parser on as few as 10
sentences shows substantial improvements in ac-
curacy. In the ?gold? conditions, where the target
parser starts with perfect source trees, accuracy
approaches 100%; in the realistic ?parse? condi-
tions, where the target-domain parser gets noisy
source-domain parses, the improvements are quite
significant but approach a lower ceiling imposed
by the performance of the source parser.
2
The adaptation problem in this section is a sim-
ple proof of concept of the QG approach; however,
more complex and realistic adaptation problems
exist. Monolingual adaptation is perhaps most ob-
viously useful when the source parser is a black-
box or rule-based system or is trained on unavail-
able data. One might still want to use such a parser
in some new context, which might require new
data or a new annotation standard.
We are also interested in scenarios where we
want to avoid expensive retraining on large rean-
notated treebanks. We would like a linguist to be
able to annotate a few trees according to a hy-
pothesized theory and then quickly use QG adap-
tation to get a parser for that theory. One example
would be adapting a constituency parser to pro-
duce dependency parses. We have concentrated
here on adapting between two dependency parse
styles, in order to line up with the cross-lingual
tasks to which we now turn.
2
In the diagonal cells, source and target styles match, so
training the QG parser amounts to a ?stacking? technique
(Martins et al, 2008). The small training size and overreg-
ularization of the QG parser mildly hurts in-domain parsing
performance.
825
% Dependency Accuracy on Target
CoNLL-PrepHead CoNLL-PrepChild Prague-PrepHead Prague-PrepChild
Source 0 10 100 0 10 100 0 10 100 0 10 100
Gold CoNLL-PrepHead 100 99.6 99.6 79.5 96.9 97.8 90.5 95.0 98.1 71.0 92.7 95.4
Parse CoNLL-PrepHead 89.5 88.9 89.0 71.4 85.9 87.9 82.5 84.3 87.8 65.2 82.2 86.1
Gold CoNLL-PrepChild 79.5 96.6 97.3 100 99.6 99.6 71.0 91.3 95.5 89.9 94.5 97.9
Parse CoNLL-PrepChild 71.0 84.2 86.8 88.1 87.5 88.0 64.9 80.7 84.9 80.9 83.5 86.1
Gold Prague-PrepHead 90.5 95.5 96.7 71.0 92.0 94.2 100 99.6 99.6 79.6 97.4 98.1
Parse Prague-PrepHead 83.0 87.1 87.4 65.6 84.2 85.9 88.5 88.3 88.0 70.7 86.4 86.8
Gold Prague-PrepChild 71.0 91.6 93.8 89.9 95.6 96.4 79.6 96.0 97.1 100 99.6 99.6
Parse Prague-PrepChild 65.3 81.7 84.6 81.2 84.5 86.1 70.4 83.2 85.3 86.9 86.1 86.8
Table 1: Adapting a parser to a new annotation style. We learn to parse in a ?target? style (wide column label) given some
number (narrow column label) of supervised target-style training sentences. As a font of additional features, all training and
test sentences have already been augmented with parses in some ?source? style (row label): either gold-standard parses (an
oracle experiment) or else the output of a parser trained on 18k source trees (more realistic). If we have 0 training sentences, we
simply output the source-style parse. But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt,
mostly closing the gap with the diagonal block in the same column. In the diagonal blocks, source and target styles match, and
the QG parser degrades performance when acting as a ?stacked? parser.
4 Cross-Lingual Projection: Background
As in the adaptation scenario above, many syn-
tactic structures can be transferred from one lan-
guage to another. In this section, we evaluate the
extent of this direct projection on a small hand-
annotated corpus. In ?5, we will use a QG genera-
tive model to learn dependency parsers from bitext
when there are no annotations in the target lan-
guage. Finally, in ?6,we show how QG features
can augment a target-language parser trained on a
small set of labeled trees.
For syntactic annotation projection to work at
all, we must hypothesize, or observe, that at least
some syntactic structures are preserved in transla-
tion. Hwa et al (2005) have called this intuition
the Direct Correspondence Assumption (DCA,
with slight notational changes):
Given a pair of sentences w and w
?
that
are translations of each other with syn-
tactic structure t and t
?
, if nodes x
?
and
y
?
of t
?
are aligned with nodes x and y of
t, respectively, and if syntactic relation-
ship R(x
?
, y
?
) holds in t
?
, then R(x, y)
holds in t.
The validity of this assumption clearly depends
on the node-to-node alignment of the two trees.
We again work in a dependency framework, where
syntactic nodes are simply lexical items. This al-
lows us to use existing work on word alignment.
Hwa et al (2005) tested the DCA under ide-
alized conditions by obtaining hand-corrected de-
pendency parse trees of a few hundred sentences
of Spanish-English and Chinese-English bitext.
They also used human-produced word alignments.
Corpus Prec.[%] Rec.[%]
Spanish 64.3 28.4
(no punc.) 72.0 30.8
Chinese 65.1 11.1
(no punc.) 68.2 11.5
Table 2: Precision and recall of direct dependency projection
via one-to-one links alone.
Since their word alignments could be many-to-
many, they gave a heuristic Direct Projection Al-
gorithm (DPA) for resolving them into component
dependency relations. It should be noted that this
process introduced empty words into the projected
target language tree and left words that are un-
aligned to English detached from the tree; as a re-
sult, they measured performance in dependency F-
score rather than accuracy. With manual English
parses and word alignments, this DPA achieved
36.8% F-score in Spanish and 38.1% in Chinese.
With Collins-model English parses and GIZA++
word alignments, F-score was 33.9% for Spanish
and 26.3% for Chinese. Compare this to the Span-
ish attach-left baseline of 31.0% and the Chinese
attach-right baselines of 35.9%. These discour-
agingly low numbers led them to write language-
specific transformation rules to fix up the projected
trees. After these rules were applied to the pro-
jections of automatic English parses, F-score was
65.7% for English and 52.4% for Chinese.
While these F-scores were low, it is useful to
look at a subset of the alignment: dependencies
projected across one-to-one alignments before the
heuristic fix-ups had a much higher precision, if
lower recall, than Hwa et al?s final results. Us-
826
ing Hwa et al?s data, we calculated that the preci-
sion of projection to Spanish and Chinese via these
one-to-one links was ? 65% (Table 2). There is
clearly more information in these direct links than
one would think from the F-scores. To exploit this
information, however, we need to overcome the
problems of (1) learning from partial trees, when
not all target words are attached, and (2) learning
in the presence of the still considerable noise in the
projected one-to-one dependencies?e.g., at least
28% error for Spanish non-punctuation dependen-
cies.
What does this noise consist of? Some errors
reflect fairly arbitrary annotation conventions in
treebanks, e.g. should the auxiliary verb gov-
ern the main verb or vice versa. (Examples like
this suggest that the projection problem contains
the adaptation problem above.) Other errors arise
from divergences in the complements required of
certain head words. In the German-English trans-
lation pair, with co-indexed words aligned,
[an [den Libanon
1
]] denken
2
? remember
2
Lebanon
1
we would prefer that the preposition an attach
to denken, even though the preposition?s object
Libanon aligns to a direct child of remember.
In other words, we would like the grandparent-
parent-child chain of denken ? an ? Libanon
to align to the parent-child pair of remember ?
Lebanon. Finally, naturally occurring bitexts con-
tain some number of free or erroneous transla-
tions. Machine translation researchers often seek
to strike these examples from their training cor-
pora; ?free? translations are not usually welcome
from an MT system.
5 Unsupervised Cross-Lingual Projection
First, we consider the problem of parser projection
when there are zero target-language trees avail-
able. As in much other work on unsupervised
parsing, we try to learn a generative model that
can predict target-language sentences. Our novel
contribution is to condition the probabilities of the
generative actions on the dependency parse of a
source-language translation. Thus, our generative
model is a quasi-synchronous grammar, exactly as
in (Smith and Eisner, 2006a).
3
When training on target sentences w, there-
fore, we tune the model parameters to maxi-
mize not
?
t
p(t, w) as in ordinary EM, but rather
3
Our task here is new; they used it for alignment.
?
t
p(t, w, a | t
?
, w
?
). We hope that this condi-
tional EM training will drive the model to posit ap-
propriate syntactic relationships in the latent vari-
able t, because?thanks to the structure of the QG
model?that is the easiest way for it to exploit the
extra information in t
?
, w
?
to help predict w.
4
At
test time, t
?
, w
?
are not made available, so we just
use the trained model to find argmax
t
p(t | w),
backing off from the conditioning on t
?
, w
?
and
summing over a.
Below, we present the specific generative model
(?5.1) and some details of training (?5.2). We will
then compare three approaches (?5.3):
?5.3.2 a straight EM baseline (which does not
condition on t
?
, w
?
at all)
?5.3.3 a ?hard? projection baseline (which naively
projects t
?
, w
?
to derive direct supervision in
the target language)
?5.3.4 our conditional EM approach above (which
makes t
?
, w
?
available to the learner for ?soft?
indirect supervision via QG)
5.1 Generative Models
Our base models of target-language syntax are
generative dependency models that have achieved
state-of-the art results in unsupervised dependency
structure induction. The simplest version, called
Dependency Model with Valence (DMV), has been
used in isolation and in combination with other
models (Klein and Manning, 2004; Smith and Eis-
ner, 2006b). The DMV generates the right chil-
dren, and then independently the left children, for
each node in the dependency tree. Nodes corre-
spond to words, which are represented by their
part-of-speech tags. At each step of generation,
the DMV stochastically chooses whether to stop
generating, conditioned on the currently generat-
ing head; whether it is generating to the right or
left; and whether it has yet generated any chil-
dren on that side. If it chooses to continue, it then
4
The contrastive estimation of Smith and Eisner (2005)
also used a form of conditional EM, with similar motiva-
tion. They suggested that EM grammar induction, which
learns to predictw, unfortunately learns mostly to predict lex-
ical topic or other properties of the training sentences that do
not strongly require syntactic latent variables. To focus EM
on modeling the syntactic relationships, they conditioned the
prediction of w on almost complete knowledge of the lexi-
cal items. Similarly, we condition on a source translation of
w. Furthermore, our QG model structure makes it easy for
EM to learn to exploit the (explicitly represented) syntactic
properties of that translation when predicting w.
827
stochastically generates the tag of a new child,
conditioned on the head. The parameters of the
model are thus of the form
p(stop | head, dir, adj) (1)
p(child | head, dir) (2)
where head and child are part-of-speech tags,
dir ? {left, right}, and adj, stop ? {true, false}.
ROOT is stipulated to generate a single right child.
Bilingual configurations that condition on t
?
, w
?
(?2) are incorporated into the generative process
as in Smith and Eisner (2006a). When the model
is generating a new child for word x, aligned to x
?
,
it first chooses a configuration and then chooses a
source word y
?
in that configuration. The child y is
then generated, conditioned on its parent x, most
recent sibling a, and its source analogue y
?
.
5.2 Details of EM Training
As in previous work on grammar induction, we
learn the DMV from part-of-speech-tagged target-
language text. We use expectation maximization
(EM) to maximize the likelihood of the data. Since
the likelihood function is nonconvex in the unsu-
pervised case, our choice of initial parameters can
have a significant effect on the outcome. Although
we could also try many random starting points, the
initializer in Klein and Manning (2004) performs
quite well.
The base dependency parser generates the right
dependents of a head separately from the left de-
pendents, which allows O(n
3
) dynamic program-
ming for an n-word target sentence. Since the QG
annotates nonterminals of the grammar with sin-
gle nodes of t
?
, and we consider two nodes of t
?
when evaluating the above dependency configura-
tions, QG parsing runs inO(n
3
m
2
) for anm-word
source sentence. If, however, we restrict candidate
senses for a target child c to come from links in
an IBM Model 4 Viterbi alignment, we achieve
O(n
3
k
2
), where k is the maximum number of
possible words aligned to a given target language
word. In practice, k  m, and parsing is not ap-
preciably slower than in the monolingual setting.
If all configurations were equiprobable, the
source sentence would provide no information to
the target. In our QG experiments, therefore,
we started with a bias towards direct parent?child
links and a very small probability for breakages
of locality. The values of other configuration pa-
rameters seem, experimentally, less important for
insuring accurate learning.
5.3 Experiments
Our experiments compare learning on target lan-
guage text to learning on parallel text. In the lat-
ter case, we compare learning from high-precision
one-to-one alignments alone, to learning from all
alignments using a QG.
5.3.1 Corpora
Our development and test data were drawn from
the German TIGER and Spanish Cast3LB tree-
banks as converted to projective dependencies for
the CoNLL 2007 Shared Task (Brants et al, 2002;
Civit Torruella and Mart?? Anton??n, 2002).
5
Our training data were subsets of the 2006
Statistical Machine Translation Workshop Shared
Task, in particular from the German-English
and Spanish-English Europarl parallel corpora
(Koehn, 2002). The Shared Task provided pre-
built automatic GIZA++ word alignments, which
we used to facilitate replicability. Since these
word alignments do not contain posterior proba-
bilities or null links, nor do they distinguish which
links are in the IBMModel intersection, we treated
all links as equally likely when learning the QG.
Target language words unaligned to any source
language words were the only nodes allowed to
align to NULL in QG derivations.
We parsed the English side of the bitext with the
projective dependency parser described by Mc-
Donald et al (2005) trained on the Penn Treebank
??2?20. Much previous work on unsupervised
grammar induction has used gold-standard part-
of-speech tags (Smith and Eisner, 2006b; Klein
and Manning, 2004; Klein and Manning, 2002).
While there are no gold-standard tags for the Eu-
roparl bitext, we did train a conditional Markov
5
We made one change to the annotation conventions in
German: in the dependencies provided, words in a noun
phrase governed by a preposition were all attached to that
preposition. This meant that in the phrase das Kind (?the
child?) in, say, subject position, das was the child of Kind;
but, in f?ur das Kind (?for the child?), das was the child of
f?ur. This seems to be a strange choice in converting from the
TIGER constituency format, which does in fact annotate NPs
inside PPs; we have standardized prepositions to govern only
the head of the noun phrase. We did not change any other
annotation conventions to make them more like English. In
the Spanish treebank, for instance, control verbs are the chil-
dren of their verbal complements: in quiero decir (?I want to
say?=?I mean?), quiero is the child of decir. In German co-
ordinations, the coordinands all attach to the first, but in En-
glish, they all attach to the last. These particular divergences
in annotation style hurt all of our models equally (since none
of them have access to labeled trees). These annotation diver-
gences are one motivation for experiments below that include
some target trees.
828
Dependency accuracy [%]
Baselines German Spanish
Modify prev. 18.2 28.5
Modify next 27.5 21.4
EM 30.2 25.6
Hard proj. 66.2 59.1
Hard proj. w/EM 58.6 53.0
QG w/EM 68.5 64.8
Table 3: Test accuracy with unsupervised training methods
model tagger on a few thousand tagged sentences.
This is the only supervised data we used in the tar-
get. We created versions of each training corpus
with the first thousand, ten thousand, and hundred
thousand sentence pairs, each a prefix of the next.
Since the target-language-only baseline converged
much more slowly, we used a version of the cor-
pora with sentences 15 target words or fewer.
5.3.2 Fully Unsupervised EM
Using the target side of the bitext as training data,
we initialized our model parameters as described
in ?5.2 and ran EM. We checked convergence on
a development set and measured unlabeled depen-
dency accuracy on held-out test data. We com-
pare performance to simple attach-right and at-
tach left baselines (Table 3). For mostly head-
final German, the ?modify next? baseline is bet-
ter; for mostly head-initial Spanish, ?modify pre-
vious? wins. Even after several hundred iterations,
performance was slightly, but not significantly bet-
ter than the baseline for German. EM training did
not beat the baseline for Spanish.
6
5.3.3 Hard Projection, Semi-Supervised EM
The simplest approach to using the high-precision
one-to-one word alignments is labeled ?hard pro-
jection? in the table. We filtered the training cor-
pus to find sentences where enough links were
projected to completely determine a target lan-
guage tree. Of course, we needed to filter more
than 1000 sentences of bitext to output 1000
training sentences in this way. We simply per-
form supervised training with this subset, which
is still quite noisy (?4), and performance quickly
6
While these results are worse than those obtained previ-
ously for this model, the experiments in Klein and Manning
(2004) and only used sentences of 10 words or fewer, without
punctuation, and with gold-standard tags. Punctuation in par-
ticular seems to trip up the initializer: since a sentence-final
periods appear in most sentences, EM often decides to make
it the head.
plateaus. Still, this method substantially improves
over the baselines and unsupervised EM.
Restricting ourselves to fully projected trees
seems a waste of information. We can also sim-
ply take all one-to-one projected links, impute ex-
pected counts for the remaining dependencies with
EM, and update our models. This approach (?hard
projection with EM?), however, performed worse
than using only the fully projected trees. In fact,
only the first iteration of EM with this method
made any improvement; afterwards, EM degraded
accuracy further from the numbers in Table 3.
5.3.4 Soft Projection: QG & Conditional EM
The quasi-synchronous model used all of the
alignments in re-estimating its parameters and per-
formed significantly better than hard projection.
Unlike EM on the target language alone, the QG?s
performance does not depend on a clever initial-
izer for initial model weights?all parameters of
the generative model except for the QG configura-
tion features were initialized to zero. Setting the
prior to prefer direct correspondence provides the
necessary bias to initialize learning.
Error analysis showed that certain types of de-
pendencies eluded the QG?s ability to learn from
bitext. The Spanish treebank treats some verbal
complements as the heads of main verbs and aux-
iliary verbs as the children of participles; the QG,
following the English, learned the opposite de-
pendency direction. Spanish treebank conventions
for punctuation were also a common source of er-
rors. In both German and Spanish, coordinations
(a common bugbear for dependency grammars)
were often mishandled: both treebanks attach the
later coordinands and any conjunctions to the first
coordinand; the reverse is true in English. Finally,
in both German and Spanish, preposition attach-
ments often led to errors, which is not surprising
given the unlexicalized target-language grammars.
Rather than trying to adjudicate which dependen-
cies are ?mere? annotation conventions, it would
be useful to test learned dependency models on
some extrinsic task such as relation extraction or
machine translation.
6 Supervised Cross-Lingual Projection
Finally, we consider the problem of parser projec-
tion when some target language trees are available.
As in the adaptation case (?3), we train a condi-
tional model (not a generative DMV) of the target
829
tree given the target sentence, using the monolin-
gual and bilingual QG features, including config-
urations conjoined with tags, outlined above (?2).
For these experiments, we used the LDC?s
English-Chinese Parallel Treebank (ECTB). Since
manual word alignments also exist for a part of
this corpus, we were able to measure the loss in
accuracy (if any) from the use of an automatic
English parser and word aligner. The source-
language English dependency parser was trained
on the Wall Street Journal, where it achieved 91%
dependency accuracy on development data. How-
ever, it was only 80.3% accurate when applied to
our task, the English side of the ECTB.
7
After parsing the source side of the bitext, we
train a parser on the annotated target side, using
QG features described above (?2). Both the mono-
lingual target-language parser and the projected
parsers are trained to optimize conditional likeli-
hood of the target trees t
?
with ten iterations of
stochastic gradient ascent.
In Figure 3, we plot the performance of the
target-language parser on held-out bitext. Al-
though projection performance is, not surprisingly,
better if we know the true source trees at training
and test time, even with the 1-best output of the
source parser, QG features help produce a parser
as accurate asq one trained on twice the amount
of monolingual data. In ablation experiments, we
included bilingual features only for directly pro-
jected links, with no features for head-swapping,
grandparents, etc. When using 1-best English
parses, parsers trained only with direct-projection
and monolingual features performed worse; when
using gold English parses, parsers with direct-
projection-only features performed better when
trained with more Chinese trees.
7 Discussion
The two related problems of parser adaptation and
projection are often approached in different ways.
Many adaptation methods operate by simple aug-
mentations of the target feature space, as we have
done here (Daume III, 2007). Parser projection, on
the other hand, often uses a multi-stage pipeline
7
It would be useful to explore whether the techniques of
?3 above could be used to improve English accuracy by do-
main adaptation. In theory a model with QG features trained
to perform well on Chinese should not suffer from an inaccu-
rate, but consistent, English parser, but the results in Figure 3
indicate a significant benefit to be had from better English
parsing or from joint Chinese-English inference.
10 20 50 100 200 500 1000 2000
0.6
0
0.6
5
0.7
0
0.7
5
0.8
0
0.8
5
Training examples
Un
lab
ele
d a
ccu
rac
y
Target only
+Gold alignments
+Source text
+Gold parses, alignments
+Gold parses
Figure 3: Parser projection with target trees. Using the true
or 1-best parse trees in the source language is equivalent to
having twice as much data in the target language. Note that
the penalty for using automatic alignments instead of gold
alignments is negligible; in fact, using Source text alone is
often higher than +Gold alignments. Using gold source trees,
however, significantly outperforms using 1-best source trees.
(Hwa et al, 2005). The methods presented here
move parser projection much closer in efficiency
and simplicity to monolingual parsing.
We showed that augmenting a target parser with
quasi-synchronous features can lead to significant
improvements?first in experiments with adapt-
ing to different dependency representations in En-
glish, and then in cross-language parser projec-
tion. As with many domain adaptation problems,
it is quite helpful to have some annotated tar-
get data, especially when annotation styles vary
(Dredze et al, 2007). Our experiments show that
unsupervised QG projection improves on parsers
trained using only high-precision projected anno-
tations and far outperforms, by more than 35%
absolute dependency accuracy, unsupervised EM.
When a small number of target-language parse
trees is available, projection gives a boost equiv-
alent to doubling the number of target trees.
The loss in performance from conditioning only
on noisy 1-best source parses points to some nat-
ural avenues for improvement. We are explor-
ing methods that incorporate a packed parse for-
est on the source side and similar representations
of uncertainty about alignments. Building on our
recent belief propagation work (Smith and Eis-
ner, 2008), we can jointly infer two dependency
trees and their alignment, under a joint distribu-
tion p(t, a, t
?
| w,w
?
) that evaluates the full graph
of dependency and alignment edges.
830
References
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In TLT.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
M. Civit Torruella and M. A. Mart?? Anton??n. 2002.
Design principles for a Spanish treebank. In TLT.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In ACL-IJCNLP.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL, pages 256?263.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Jo?ao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adap-
tation for dependency parsing. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 1051?1055.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In ACL-IJCNLP.
Kevin Gimpel and Noah A. Smith. 2009. Feature-rich
translation by quasi-synchronous lattice parsing. In
EMNLP.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11:311?325.
Dan Klein and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In ACL, pages 128?135.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In ACL, pages
479?486.
Philipp Koehn. 2002. Europarl: A multilingual
corpus for evaluation of machine translation.
http://www.iccs.informatics.ed.ac.uk/?pkoehn/-
publications/europarl.ps.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In ACL, pages 470?477.
Andr?e F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In EMNLP.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In ACL, pages 337?344.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL, pages 91?98.
Noah A. Smith and Jason Eisner. 2005. Guiding unsu-
pervised grammar induction using contrastive esti-
mation. In International Joint Conference on Artifi-
cial Intelligence (IJCAI) Workshop on Grammatical
Inference Applications, Edinburgh, July.
David A. Smith and Jason Eisner. 2006a. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proceedings of
the HLT-NAACL Workshop on Statistical Machine
Translation, pages 23?30.
Noah A. Smith and Jason Eisner. 2006b. Annealing
structural bias in multilingual weighted grammar in-
duction. In ACL-COLING, pages 569?576.
David A. Smith and Jason Eisner. 2007. Bootstrap-
ping feature-rich dependency parsers with entropic
priors. In EMNLP-CoNLL, pages 667?677.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages
145?156.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In EMNLP, pages 49?56.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In ACL-IJCNLP.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In EMNLP-CoNLL,
pages 22?32.
831
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1007?1016,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Learning Linear Ordering Problems for Better Translation
?
Roy Tromble
Google, Inc.
4720 Forbes Ave.
Pittsburgh, PA 15213
royt@google.com
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
jason@cs.jhu.edu
Abstract
We apply machine learning to the Lin-
ear Ordering Problem in order to learn
sentence-specific reordering models for
machine translation. We demonstrate that
even when these models are used as a mere
preprocessing step for German-English
translation, they significantly outperform
Moses? integrated lexicalized reordering
model.
Our models are trained on automatically
aligned bitext. Their form is simple but
novel. They assess, based on features of
the input sentence, how strongly each pair
of input word tokens w
i
, w
j
would like
to reverse their relative order. Combining
all these pairwise preferences to find the
best global reordering is NP-hard. How-
ever, we present a non-trivial O(n
3
) al-
gorithm, based on chart parsing, that at
least finds the best reordering within a cer-
tain exponentially large neighborhood. We
show how to iterate this reordering process
within a local search algorithm, which we
use in training.
1 Introduction
Machine translation is an important but difficult
problem. One of the properties that makes it dif-
ficult is the fact that different languages express
the same concepts in different orders. A ma-
chine translation system must therefore rearrange
the source language concepts to produce a fluent
translation in the target language.
1
This work is excerpted and adapted from the first au-
thor?s Ph.D. thesis (Tromble, 2009). Some of the ideas here
appeared in (Eisner and Tromble, 2006) without empirical
validation. The material is based in part upon work sup-
ported by the National Science Foundation under Grant No.
0347822.
Phrase-based translation systems rely heavily
on the target language model to ensure a fluent
output order. However, a target n-gram language
model alone is known to be inadequate. Thus,
translation systems should also look at how the
source sentence prefers to reorder. Yet past sys-
tems have traditionally used rather weak models of
the reordering process. They may look only at the
distance between neighboring phrases, or depend
only on phrase unigrams. The decoders also rely
on search error, in the form of limited reordering
windows, for both efficiency and translation qual-
ity.
Demonstrating the inadequacy of such ap-
proaches, Al-Onaizan and Papineni (2006)
showed that even given the words in the reference
translation, and their alignment to the source
words, a decoder of this sort charged with merely
rearranging them into the correct target-language
order could achieve a BLEU score (Papineni et
al., 2002) of at best 69%?and that only when
restricted to keep most words very close to their
source positions.
This paper introduces a more sophisticated
model of reordering based on the Linear Order-
ing Problem (LOP), itself an NP-hard permutation
problem. We apply machine learning, in the form
of a modified perceptron algorithm, to learn pa-
rameters of a linear model that constructs a matrix
of weights from each source language sentence.
We train the parameters on orderings derived from
automatic word alignments of parallel sentences.
The LOP model of reordering is a complete
ordering model, capable of assigning a different
score to every possible permutation of the source-
language sentence. Unlike the target language
model, it uses information about the relative posi-
tions of the words in the source language, as well
as the source words themselves and their parts of
speech and contexts. It is therefore a language-pair
specific model.
1007
We apply the learned LOP model as a prepro-
cessing step before both training and evaluation of
a phrase-based translation system, namely Moses.
Our methods for finding a good reordering un-
der the NP-hard LOP are themselves of interest,
adapting algorithms from natural language parsing
and developing novel dynamic programs.
Our results demonstrate a significant improve-
ment over translation using unreordered German.
Using Moses with only distance-based reordering
and a distortion limit of 6, our preprocessing im-
proves BLEU from 25.27 to 26.40. Furthermore,
that improvement is significantly greater than the
improvement Moses achieves with its lexicalized
reordering model, 25.55.
Collins et al (2005) improved German-English
translation using a statistical parser and several
hand-written rules for preprocessing the German
sentences. This paper presents a similar improve-
ment using fully automatic methods.
2 A Linear Ordering Model
This section introduces a model of word reorder-
ing for machine translation based on the Linear
Ordering Problem.
2.1 Formalization
The input sentence is w = w
1
w
2
. . . w
n
. To dis-
tinguish duplicate tokens of the same word, we as-
sume that each token is superscripted by its input
position, e.g., w = die
1
Katze
2
hat
3
die
4
Frau
5
gekauft
6
(gloss: ?the cat has the woman bought?).
For a fixedw, a permutation pi = pi
1
pi
2
. . . pi
n
is
any reordering of the tokens in w. The set ?
n
of
all such permutations has size n!. We would like to
define a scoring model that assigns a high score to
the permutationpi = die
4
Frau
5
hat
3
gekauft
6
die
1
Katze
2
(gloss: ?the woman has bought the cat?),
since that corresponds well to the desired English
order.
To construct a function that scores permutations
of w, we first construct a pairwise preference ma-
trix B
w
? R
n?n
, whose entries are
B
w
[`, r]
def
= ? ? ?(w, `, r), (1)
Here ? is a vector of weights. ? is a vector of
feature functions, each considering the entire word
sequencew, as well as any functions thereof, such
as part of speech tags.
We will hereafter abbreviate B
w
as B. Its inte-
ger indices ` and r are identified with the input to-
kensw
`
andw
r
, and it can be helpful to write them
that way; e.g., we will sometimes write B[2, 5] as
B[Katze
2
,Frau
5
].
The idea behind our reordering model is
that B[Katze
2
,Frau
5
] > B[Katze
5
,Frau
2
] ex-
presses a preference to keep Katze
2
before Frau
5
,
whereas the opposite inequality would express a
preference?other things equal?for permutations
in which their order is reversed. Thus, we define
1
score(pi)
def
=
?
i,j: 1?i<j?n
B[pi
i
, pi
j
] (2)
p(pi)
def
=
1
Z
exp(? ? score(pi)) (3)
?
pi
def
= argmax
pi??
n
score(pi) (4)
Note that i and j denote positions in pi, whereas
pi
i
, pi
j
, `, and r denote particular input tokens such
as Katze
2
and Frau
5
.
2.2 Discussion
To the extent that the costs B generally discour-
age reordering, they will particularly discourage
long-distance movement, as it swaps more pairs
of words.
We point out that our model is somewhat pecu-
liar, since it does not directly consider whether the
permutation pi keeps die
4
and Frau
5
adjacent or
even close together, but only whether their order
is reversed.
Of course, the model could be extended to con-
sider adjacency, or more generally, the three-way
cost of interposing k between i and j. See (Eis-
ner and Tromble, 2006; Tromble, 2009) for such
extensions and associated algorithms.
However, in the present paper we focus on the
model in the simple form (2) that only considers
pairwise reordering costs for all pairs in the sen-
tence. Our goal is to show that these unfamiliar
pairwise reordering costs are useful, when mod-
eled with a rich feature set via equation (1). Even
in isolation (as a preprocessing step), without con-
sidering any other kinds of reordering costs or lan-
guage model, they can achieve useful reorderings
1
For any ` < r, we may assume without loss of gener-
ality that B[r, `] = 0, since if not, subtracting B[r, `] from
bothB[`, r] andB[r, `] (exactly one of which appears in each
score(pi)) will merely reduce the scores of all permutations
by this amount, leaving equations (3) and (4) unchanged.
Thus, in practice, we take B to be an upper triangular ma-
trix. We use equation (1) only to defineB[`, r] for ` < r, and
train ? accordingly. However, we will ignore this point in our
exposition.
1008
of German that complement existing techniques
and thus improve state-of-the-art systems. Our
positive results in even this situation suggest that
in future, pairwise reordering costs should proba-
bly be integrated into MT systems.
The probabilistic interpretation (3) of the
score (2) may be useful when thus integrating our
model with language models or other reordering
models during translation, or simply when train-
ing our model to maximize likelihood or minimize
expected error. However, in the present paper we
will stick to purely discriminative training and de-
coding methods that simply try to maximize (2).
2.3 The Linear Ordering Problem
In the combinatorial optimization literature, the
maximization problem (4) (with inputB) is known
as the Linear Ordering Problem. It has numer-
ous practical applications in fields including eco-
nomics, sociology, graph theory, graph drawing,
archaeology, and task scheduling (Gr?otschel et
al., 1984). Computational studies on real data
have often used ?input-output? matrices represent-
ing resource flows among economic sectors (Schi-
avinotto and St?utzle, 2004).
Unfortunately, the problem is NP-hard. Further-
more, it is known to be APX-complete, meaning
that there is no polynomial time approximation
scheme unless P=NP (Mishra and Sikdar, 2004).
However, there are various heuristic procedures
for approximating it (Tromble, 2009). We now
give an attractive, novel procedure, which uses a
CKY-parsing-like algorithm to search various sub-
sets of ?
n
in polynomial time.
3 Local Search
?Local search? refers to any hill-climbing proce-
dure that iteratively improves a solution by mak-
ing an optimal ?local? change at each iteration.
2
In this case, we start with the identity permutation,
find a ?nearby? permutation with a better score (2),
and repeat until we have reached a local maximum
of the scoring objective.
This section describes a local search procedure
that uses a very generous definition of ?local.? At
each iteration, it finds the optimal permutation in
a certain exponentially large neighborhood N(pi)
of the current permutation pi.
2
One can introduce randomness to obtain MCMC sam-
pling or simulated annealing algorithms. Our algorithms ex-
tend naturally to allow this (cf. Tromble (2009)).
S ? S
0,n
S
i,k
? S
i,j
S
j,k
S
i?1,i
? pi
i
Figure 1: A grammar for a large neighborhood of
permutations, given one permutation pi of length
n. The S
i,k
rules are instantiated for each 0 ?
i < j < k ? n, and the S
i?1,i
rules for each
0 < i ? n.
We say that two permutations are neighbors iff
they can be aligned by an Inversion Transduction
Grammar (ITG) (Wu, 1997), which is a familiar
reordering device in machine translation. Equiva-
lently, pi
?
? N(pi) iff pi can be transformed into
pi
?
by swapping various adjacent substrings of pi,
as long as these swaps are properly nested. Zens
and Ney (2003) used a normal form to show that
the size of the ITG neighborhood N(pi) is a large
Schr?oder number, which grows exponentially in
n. Asymptotically, the ratio between the size of
the neighborhood for n + 1 and the size for n ap-
proaches 3 + 2
?
2 ? 5.8.
We show that equation (2) can be optimized
within N(pi) in O(n
3
) time, using dynamic pro-
gramming. The algorithm is based on CKY pars-
ing. However, a novelty is that the grammar
weights must themselves be computed by O(n
3
)
dynamic programming.
Our grammar is shown in Figure 1. Parsing
the ?input sentence? pi with this grammar simply
constructs all binary trees that yield the string pi.
There is essentially only one nonterminal, S, but
we split it into O(n
2
) position-specific nontermi-
nals such as S
i,j
, which can only yield the span
pi
i+1
pi
i+2
. . . pi
j
. An example parse is shown in
Figure 2.
The important point is that we will place a
score on each binary grammar rule. The score
of the rule S
i,k
? S
i,j
S
j,k
is max(0,?
i,j,k
),
where ?
i,j,k
is the benefit to swapping the sub-
strings pi
i+1
pi
i+2
. . . pi
j
and pi
j+1
pi
j+2
. . . pi
k
. The
rule is considered to be a ?swap rule? if its
score is positive, showing that a swap will be
beneficial (independent of the rest of the tree).
If the parse in Figure 2 is the parse with
the highest total score, and its swap rules are
S
0,5
? S
0,1
S
1,5
and S
3,5
? S
3,4
S
4,5
, then
our best permutation in the neighborhood of pi
must be the (linguistically desirable) permutation
die
4
Frau
5
hat
3
gekauft
6
die
1
Katze
2
, obtained from
1009
SS
0,6






H
H
H
H
H
H
S
0,5




H
H
H
H
S
0,1
die
1
S
1,5




H
H
H
H
S
1,3


H
H
S
1,2
die
4
S
2,3
Frau
5
S
3,5


H
H
S
3,4
gekauft
6
S
4,5
hat
3
S
5,6
Katze
2
Figure 2: One parse of the current permutation pi.
In this example, pi has somehow gotten the input
words into alphabetical order (owing to previous
hill-climbing steps). We are now trying to further
improve this order.
pi by two swaps.
How do we find this solution? Clearly
the benefit (positive or negative) to swapping
pi
i+1
pi
i+2
. . . pi
j
with pi
j+1
pi
j+2
. . . pi
k
is
?
i,j,k
=
j
?
`=i+1
k
?
r=j+1
B[pi
r
, pi
`
]?B[pi
`
, pi
r
] (5)
We can evaluate all O(n
3
) possible swaps in to-
tal time O(n
3
), using the dynamic programming
recurrence
?
i,j,k
= ?
i,j,k?1
+ ?
i+1,j,k
??
i+1,j,k?1
(6)
+B[pi
k
, pi
i+1
]?B[pi
i+1
, pi
k
]
with the base case ?
i,j,k
= 0 if i = j or j = k.
This gives us the weights for the grammar rules,
and then we can use weighted CKY parsing to
find the highest-scoring (Viterbi) parse in O(n
3
)
time. Extracting our new and improved permuta-
tion pi
?
? N(pi) from this parse is a simple O(n)-
time algorithm.
Figure 3 gives pseudocode for our local search
algorithm, showing how to compute the quan-
tities (6) during parsing rather than beforehand.
?[i, k] holds the weight of the best permuta-
tion (in the neighborhood) of the subsequence
pi
i+1
pi
i+1
. . . pi
k
.
3
3
The use of ? is intended to suggest an analogy to inside
probability?or more precisely, the Viterbi approximation to
inside probability (since we are maximizing rather than sum-
ming over parses).
The next two sections describe how to use our
local search algorithm to discriminatively learn the
weights of the parameters from Section 2, equa-
tion (1).
4 Features
Our objective function (2) works only to the extent
that we can derive a good pairwise preference ma-
trix B
w
. We do this by using a rich feature set in
equation (1).
We adapt the features of McDonald et al
(2005), introduced there for dependency parsing,
to the task of machine translation reordering. Be-
cause both models construct features for pairs of
words given the entire sentence, there is a close
correspondence between the two tasks, although
the output is quite different.
Each feature ?(w, `, r) in equation (1) is a bi-
nary feature that fires when (w, `, r) has some
conjunction of properties. The properties that are
considered include the words w
`
and w
r
, the parts
of speech of {w
`?1
, . . . , w
r+1
}, and the distance
r ? `. Table 1 shows the feature templates.
We also tried features based on a dependency
parse of the German, with the idea of using LOP
features to reorder the dependents of each word,
and thus model syntactic movement. This did
produce better monolingual reorderings (as in Ta-
ble 2), but it did not help final translation into En-
glish (Table 3), so we do not report the details here.
5 Learning to Reorder
Ideally, we would have a large corpus of desir-
able reorderings of source sentences?in our case,
German sentences permuted into target English
word order?from which to train the parameters of
our model. Unfortunately, the alignments between
German and English sentences are only infre-
quently one-to-one. Furthermore, human-aligned
parallel sentences are hard to come by, and never
in the quantity we would like.
Instead, we make do with automatically-
generated word alignments, and we heuristi-
cally derive an English-like word order for
the German sentence based on the alignment.
We used GIZA++ (Och and Ney, 2003) to
align approximately 751,000 sentences from the
German-English portion of the Europarl corpus
(Koehn, 2005), in both the German-to-English and
English-to-German directions. We combined the
1010
1: procedure LOCALSEARCHSTEP(B,pi, n)
2: for i? 0 to n? 1 do
3: ?[i, i+ 1]? 0
4: for k ? i+ 1 to n do
5: ?[i, i, k]? ?[i, k, k]? 0
6: end for
7: end for
8: for w ? 2 to n do
9: for i? 0 to n? w do
10: k ? i+ w
11: ?[i, k]? ??
12: for j ? i+ 1 to k ? 1 do
13: ?[i, j, k]? ?[i, j, k ? 1] + ?[i+ 1, j, k]??[i+ 1, j, k ? 1] +B[pi
k
, pi
i+1
]?B[pi
i+1
, pi
k
]
14: ?[i, k]? max(?[i, k], ?[i, j] + ?[j, k] + max(0, ?[i, j, k]))
15: end for
16: end for
17: end for
18: return ?[0, n]
19: end procedure
Figure 3: Pseudocode for computing the score of the best permutation in the neighborhood of pi under
the Linear Ordering Problem specified by the matrix B. Computing the best neighbor is a simple matter
of keeping back pointers to the choices of max and ordering them as implied.
alignments using the ?grow-diag-final-and? proce-
dure provided with Moses (Koehn et al, 2007).
For each of these German sentences, we derived
the English-like reordering of it, which we call
German
?
, by the following procedure. Each Ger-
man token was assigned an integer key, namely
the position of the leftmost of the English tokens
to which it was aligned, or 0 if it was not aligned
to any English tokens. We then did a stable sort of
the German tokens based on these keys, meaning
that if two German tokens had the same key, their
order was preserved.
This is similar to the oracle ordering used by
Al-Onaizan and Papineni (2006), but differs in the
handling of unaligned words. They kept unaligned
words with the closest preceding aligned word.
4
Having found the German
?
corresponding to
each German sentence, we randomly divided
the sentences into 2,000 each for development
and evaluation, and the remaining approximately
747,000 for training.
We used the averaged perceptron algorithm
(Freund and Schapire, 1998; Collins, 2002) to
train the parameters of the model. We ran the al-
gorithm multiple times over the training sentences,
4
We tried two other methods for deriving English word
order from word alignments. The first alternative was to
align only in one direction, from English to German, with
null alignments disallowed, so that every German word was
aligned to a single English word. The second alternative
used BerkeleyAligner (Liang et al, 2006; DeNero and Klein,
2007), which shares information between the two alignment
directions to improve alignment quality. Neither alternative
produced improvements in our ultimate translation quality.
measuring the quality of the learned parameters by
reordering the held-out development set after each
iteration. We stopped when the BLEU score on
the development set failed to improve for two con-
secutive iterations, which occurred after fourteen
passes over the data.
Each perceptron update should compare the true
German
?
to the German
?
that would be predicted
by the model (2). As the latter is NP-hard to find,
we instead substitute the local maximum found by
local search as described in Section 3, starting at
the identity permutation, which corresponds to the
original German word order.
During training, we iterate the local search as
described earlier. However, for decoding, we only
do a single step of local search, thus restricting re-
orderings to the ITG neighborhood of the origi-
nal German. This restriction turns out to improve
performance slightly, even though it reduces the
quality of our approximation to the LOP prob-
lem (4). In other words, it turns out that reorder-
ings found outside the ITG neighborhood tend to
be poor German
?
even if our LOP-based objective
function thinks that they are good German
?
.
This is not to say that the gold standard German
?
is always in the ITG neighborhood of the original
German?often it is not. Thus, it might be bet-
ter in future work to still allow the local search to
take more than one step, but to penalize the second
step. In effect, score(pi) would then include a fea-
ture indicating whether pi is in the neighborhood
of the original German.
1011
t`?1
w
`
t
`
t
`+1
t
b
t
r?1
w
r
t
r
t
r+1
? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
? ?
? ?
? ?
? ?
?
?
?
?
? ? ?
? ? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ? ?
? ? ?
? ? ? ?
? ? ?
Table 1: Feature templates forB[`, r] (w
`
is the `th
word, t
`
its part of speech tag, and b matches any
index such that ` < b < r). Each of the above
is also conjoined with the distance between the
words, r ? `, to form an additional feature tem-
plate. Distances are binned into 1, 2, 3, 4, 5, > 5,
and > 10.
The model is initialized at the start of train-
ing using log-odds of the parameters. Let ?
m
=
{(w, `, r) | ?
m
(w, `, r) = 1} be the set of word
pairs in the training data for which feature m fires.
Let
?
?
m
be the subset of ?
m
for which the words
stay in order, and
?
?
m
the subset for which the
words reverse order. Then in this model,
?
m
= log
(
?
?
?
?
?
m
?
?
?
+
1
2
)
?log
(
?
?
?
?
?
m
?
?
?
+
1
2
)
. (7)
This model is equivalent to smoothed na??ve Bayes
if converted to probabilities. The learned model
significantly outperforms it on the monolingual re-
ordering task.
Table 2 compares the model after perceptron
training to the model at the start of training,
measuring BLEU score of the predicted German
?
against the observed German
?
. In addition to these
BLEU scores, we can measure precision and re-
call of pairs of reordered words against the ob-
Ordering p
2
p
3
p
4
BLEU
German 57.4 38.3 27.7 49.65
Log-odds 57.4 38.4 27.8 49.75
Perceptron 58.6 40.3 29.8 51.51
Table 2: Monolingual BLEU score on develop-
ment data, measured against the ?true? German
?
ordering that was derived from automatic align-
ments to known English translations. The table
evaluates three candidate orderings: the original
German, German reordered using the log-odds
initialized model, and German reordered using
the perceptron-learned model. In addition to the
BLEU score, the table shows bigram, trigram, and
4-gram precisions. The unigram precisions are al-
ways 100%, because the correct words are given.
served German
?
. On the held out test set, the pre-
dicted German
?
achieves a recall of only 21%, but
a precision of 64%. Thus, the learned model is
too conservative, but makes moderately good de-
cisions when it does reorder.
6 Reordering as Preprocessing
This section describes experiments using the
model introduced in Section 2 and learned in Sec-
tion 5 to preprocess German sentences for trans-
lation into English. These experiments are similar
to those of Collins et al (2005).
We used the model learned in Section 5 to gen-
erate a German
?
ordering of the training, develop-
ment, and test sets. The training sentences are the
same that the model was trained on, and the devel-
opment set is the same that was used as the stop-
ping criterion for the perceptron. The test set was
unused in training.
We used the resulting German
?
as the input to
the Moses training pipeline. That is, Moses re-
computed alignments of the German
?
training data
to the English sentences using GIZA++, then con-
structed a phrase table. Moses used the develop-
ment data for minimum error-rate training (Och,
2003) of its small number of parameters. Finally,
Moses translated the test sentences, and we mea-
sured performance against the English reference
sentences. This is the standard Moses pipeline, ex-
cept German has been replaced by German
?
.
Table 3 shows the results of translation, both
starting with unreordered German, and starting
with German
?
, reordered using the learned Linear
Ordering Problems. Note that Moses may itself re-
1012
System Input Moses Reord. p
1
p
2
p
3
p
4
BLEU METEOR TER
baseline German Distance 59.6 31.4 18.8 11.6 25.27 54.03 60.60
(a) German Lexical 60.0 32.0 19.3 12.1 25.55 54.18 59.76
(b) German
?
Distance 60.4 32.7 20.2 12.8 26.40 54.91 58.63
(a)+(b) German
?
Lexical 59.9 32.4 20.0 12.8 26.44 54.61 59.23
Table 3: Machine translation performance of several systems, measured against a single English refer-
ence translation. The results vary both the preprocessing?either none, or reordered using the learned
Linear Ordering Problems?and the reordering model used in Moses. Performance is measured using
BLEU, METEOR (Lavie et al, 2004), and TER (Snover et al, 2006). (For TER, smaller values are
better.)
order whatever input that it receives, during trans-
lation into English. Thus, the results in the table
also vary the reordering model used in Moses, set
to either a single-parameter distance-based model,
or to the lexicalized bidirectional msd model. The
latter model has six parameters for each phrase
in the phrase table, corresponding to monotone,
swapped, or discontinuous ordering relative to the
previous phrase in either the source or target lan-
guage.
How should we understand the results? The
baseline system is Moses phrase-based translation
with no preprocessing and only a simple distance-
based reordering model. There are two ways to
improve this: (a) ask Moses to use the lexicalized
bidirectional msd reordering model that is pro-
vided with Moses and is integrated with the rest of
translation, or (b) keep the simple distance-based
model within Moses, but preprocess its training
and test data with our linear reordering model.
Note that the preprocessing in (b) will obviously
change the phrasal substrings that are learned by
Moses, for better or for worse.
First, remarkably, (b) is significantly better than
(a) on BLEU, with p < 0.0001 according to a
paired permutation test.
Second, combining (a) with (b) produced no im-
provement over (b) in BLEU score (the difference
between 26.40 and 26.44 is not significant, even
at p < 0.2, according to the same paired per-
mutation test). Lexicalized reordering in Moses
even degraded translation performance according
to METEOR and TER. The TER change is sig-
nificant according to the paired permutation test at
p < 0.001. (We did not perform a significance test
for METEOR.)
Our word-based model surpasses the lexical-
ized reordering in Moses largely because of long-
distance movement. The 518 sentences (26%) in
ll
llll
l
l
ll
l
lll
l
lll
ll
ll
lll
lllllllll
llllllllll l l ll
llllll
l
l
ll
l
l
ll
l
lll
ll
ll
lll
lllllllll
lllllllll
l l l ll
0 10 20 30 40 50
?
0.00
20
.000
0.00
2
0.00
4
0.00
6
0.00
8
0.01
0
Word Pairs Reordered
Cum
ulat
ive 
BLE
U C
han
ge
BLEU Improvement Aggregated by Amount of Reordering
vs. baseline
vs. (a)
Figure 4: Cumulative change in BLEU score of
(b) relative to the baseline and (a), aggregated by
the number of reordered word pairs in each sen-
tence. For those sentences where our model re-
orders fewer than five word pairs, the BLEU score
of translation degrades.
the test set for which our model moves a word
more than six words away from its starting posi-
tion account for more than 67% of the improve-
ment in BLEU from (a) to (b).
Figure 4 shows another view of the BLEU im-
provement. It shows that, compared to the base-
line, our preprocessing has basically no effect for
sentences where it does only a little reordering,
changing the relative order of fewer than five pairs
of words. Compared to Moses with lexicalized re-
ordering, these same sentences actually hurt per-
formance. This more than accounts for the differ-
ence between the BLEU scores of (b) and (a)+(b).
Going beyond preprocessing, our model could
also be integrated into a phrase-based decoder. We
briefly sketch that possibility here.
1013
Phrase-based decoders keep a source coverage
vector with every partial translation hypothesis.
That coverage vector allows us to incorporate the
scores from a LOP matrix B directly. Whenever
the decoder extends the hypothesis with a new
source phrase, covering w
i+1
w
i+2
. . . w
j
, it adds
j?1
?
`=i+1
j
?
r=`+1
B[`, r] +
j
?
`=i+1
?
r?U
B[`, r].
The first term represents the phrase-internal score,
and the second the score of putting the words in the
phrase before all the remaining uncovered words
U .
7 Comparison to Prior Work
Preprocessing the source language to improve
translation is a common technique. Xia and Mc-
Cord (2004) improved English-French translation
using syntactic rewrite rules derived from Slot
Grammar parses. Collins et al (2005) reported
an improvement from 25.2% to 26.8% BLEU
on German-English translation using six hand-
written rules to reorder the German sentences
based on automatically-generated phrase-structure
trees. Our work differs from these approaches in
providing an explicit model that scores all pos-
sible reorderings. In this paper, our model was
trained and used only for 1-best preprocessing, but
it could potentially be integrated into decoding as
well, where it would work together with the trans-
lation model and target language model to find a
congenial translation.
Costa-juss`a and Fonollosa (2006) improved
Spanish-English and Chinese-English translation
using a two-step process, first reordering the
source language, then translating it, both using dif-
ferent versions of a phrase-based translation sys-
tem. Many others have proposed more explicit
reordering models (Tillmann, 2004; Kumar and
Byrne, 2005; Koehn et al, 2005; Al-Onaizan and
Papineni, 2006). The primary advantage of our
model is that it directly accounts for interactions
between distant words, leading to better treatment
of long-distance movement.
Xiong et al (2006) proposed a constituent
reordering model for a bracketing transduction
grammar (BTG) (Wu, 1995), which predicts the
probability that a pair of subconstituents will re-
order when combined to form a new constituent.
The features of their model look only at the first
source and target word of each constituent, mak-
ing it something like a sparse version of our model.
However, because of the target word features, their
reordering model cannot be separated from their
translation model.
8 Conclusions and Future Work
We have presented an entirely new model of re-
ordering for statistical machine translation, based
on the Linear Ordering Problem, and shown that
it can substantially improve translation from Ger-
man to English.
The model is demonstrably useful in this pre-
processing setting?which means that it can be
very simply added as a preprocessing step to any
MT system. German-to-English is a particularly
attractive use case, because the word orders are
sufficiently different as to require a good reorder-
ing model that requires long-distance reordering.
Our preprocessing here gave us a BLEU gain
of 0.9 point over the best Moses-based result.
English-to-German would obviously be another
potential win, as would translating between En-
glish and Japanese, for example.
As mentioned in Section 6, our model could
also be integrated into a phrase-based, or a syntax-
based decoder. That possibility remains future
work, but it is likely to lead to further improve-
ments, because it allows the translation system to
consider multiple possible reorderings under the
model, as well as to tune the weight of the model
relative to the other parts of the system during
MERT.
Tromble (2009) covers this integration in more
detail, and proposes several other ways of integrat-
ing our reordering model into machine translation.
It also experiments with numerous other param-
eter estimation procedures, including some that
use the probabilistic interpretation of our model
from (3). It presents numerous additional neigh-
borhoods for search in the Linear Ordering Prob-
lem.
We mentioned several possible extensions to the
model, such as going beyond the scoring model
of equation (2), or considering syntax-based fea-
tures. Another extension would try to reorder not
words but phrases, following (Xiong et al, 2006),
or segment choice models (Kuhn et al, 2006),
which assume a single segmentation of the words
into phrases. We would have to define the pair-
wise preference matrix B over phrases rather than
1014
words (Eisner and Tromble, 2006). This would
have the disadvantage of complicating the feature
space, but might be a better fit for integration with
a phrase-based decoder.
Finally, we gave a novel algorithm for ap-
proximately solving the Linear Ordering Prob-
lem, interestingly combining dynamic program-
ming with local search. Another novel contri-
bution is that we showed how to parameterize a
function that constructs a specific Linear Order-
ing Problem instance from an input sentence w,
and showed how to learn those parameters from
a corpus of parallel sentences, using the percep-
tron algorithm. Likelihood-based training using
equation (3) would also be possible, with modifi-
cations to our algorithm, notably the use of normal
forms to avoid counting some permutations multi-
ple times (Tromble, 2009).
It would be interesting to compare the speed
and accuracy of our dynamic-programming local-
search method with an exact algorithm for solving
the LOP, such as integer linear programming with
branch and bound (cf. Charon and Hudry (2006)).
Exact solutions can generally be found in practice
for n ? 100.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
COLING-ACL, pages 529?536, Sydney, July.
Ir`ene Charon and Olivier Hudry. 2006. A branch-and-
bound algorithm to solve the linear ordering problem
for weighted tournaments. Discrete Applied Mathe-
matics, 154(15):2097?2116, October.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In ACL, pages 531?540, Ann Arbor,
Michigan, June.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8, Philadelphia, July.
Marta R. Costa-juss`a and Jos?e A. R. Fonollosa. 2006.
Statistical machine reordering. In EMNLP, pages
70?76, Sydney, July.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL,
pages 17?24, Prague, June.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal per-
mutations in machine translation. In Workshop on
computationally hard problems and joint inference
in speech and language processing, New York, June.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In COLT, pages 209?217, New York. ACM Press.
Martin Gr?otschel, Michael J?unger, and Gerhard
Reinelt. 1984. A cutting plane algorithm for
the linear ordering problem. Operations Research,
32(6):1195?1220, November?December.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In IWSLT, Pittsburgh, October.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In ACL Demo and Poster Sessions, pages 177?
180, Prague, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit
X, pages 79?86, Phuket, Thailand, September.
Roland Kuhn, Denis Yuen, Michel Simard, Patrick
Paul, George Foster, Eric Joanis, and Howard John-
son. 2006. Segment choice models: Feature-rich
models for global distortion in statistical machine
translation. In HLT-NAACL, pages 25?32, New
York, June.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In HLT-EMNLP, pages 161?168, Van-
couver, October.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The signicance of recall in automatic
metrics for MT evaluation. In Robert E. Frederking
and Kathryn B. Taylor, editors, Machine Transla-
tion: From Real Users to Research, pages 134?143.
AMTA, Springer, September?October.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL, pages 104?
111, New York, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Spanning tree methods for discrim-
inative training of dependency parsers. Technical
Report MS-CIS-05-11, UPenn CIS.
Sounaka Mishra and Kripasindhu Sikdar. 2004. On
approximability of linear ordering and related NP-
optimization problems on graphs. Discrete Applied
Mathematics, 136(2?3):249?269, February.
1015
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167, Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, pages
311?318, Philadelphia, July.
Tommaso Schiavinotto and Thomas St?utzle. 2004.
The linear ordering problem: Instances, search
space analysis and algorithms. Journal of Math-
ematical Modelling and Algorithms, 3(4):367?402,
December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In HLT-
NAACL Short Papers, pages 101?104, Boston, May.
Roy Wesley Tromble. 2009. Search and Learning for
the Linear Ordering Problem with an Application
to Machine Translation. Ph.D. thesis, Johns Hop-
kins University, Baltimore, April. http://nlp.
cs.jhu.edu/
?
royt/
Dekai Wu. 1995. An algorithm for simultaneously
bracketing parallel texts by aligning words. In ACL,
pages 244?251, Cambridge, Massachusetts, June.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Septem-
ber.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING, pages 508?514,
Geneva, August.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In COLING-ACL,
pages 521?528, Sydney, July.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL, pages 144?151, Sapporo, July.
1016
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 281?290, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Compiling Comp Ling:
Practical Weighted Dynamic Programming and the Dyna Language?
Jason Eisner and Eric Goldlust and Noah A. Smith
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{jason,goldlust,nasmith}@cs.jhu.edu
Abstract
Weighted deduction with aggregation is a powerful theoretical
formalism that encompasses many NLP algorithms. This pa-
per proposes a declarative specification language, Dyna; gives
general agenda-based algorithms for computing weights and
gradients; briefly discusses Dyna-to-Dyna program transforma-
tions; and shows that a first implementation of a Dyna-to-C++
compiler produces code that is efficient enough for real NLP re-
search, though still several times slower than hand-crafted code.
1 Introduction
In this paper, we generalize some modern prob-
abilistic parsing techniques to a broader class of
weighted deductive algorithms. Our implemented
system encapsulates these implementation tech-
niques behind a clean interface?a small high-level
specification language, Dyna, which compiles into
C++ classes. This system should help the HLT com-
munity to experiment more easily with new models
and algorithms.
1.1 Dynamic programming as deduction
The ?parsing as deduction? framework (Pereira and
Warren, 1983) is now over 20 years old. It provides
an elegant notation for specifying a variety of pars-
ing algorithms (Shieber et al, 1995), including algo-
rithms for probabilistic or other semiring-weighted
parsing (Goodman, 1999). In the parsing commu-
nity, new algorithms are often stated simply as a set
of deductive inference rules (Sikkel, 1997; Eisner
and Satta, 1999).
It is also straightforward to specify other NLP al-
gorithms this way. Syntactic MT models, language
models, and stack decoders can be easily described
using deductive rules. So can operations on finite-
state and infinite-state machines.
?We thank Joshua Goodman, David McAllester, and Paul
Ruhlen for useful early discussions; pioneer users Markus
Dreyer, David Smith, and Roy Tromble for their feedback and
input; John Blatz for discussion of program transformations;
and several reviewers for useful criticism. This work was
supported by NSF ITR grant IIS-0313193, ONR MURI grant
N00014-01-1-0685, and a Hertz Foundation fellowship to the
third author. The views expressed are not necessarily endorsed
by the sponsors.
1.2 The role of toolkits
One might regard deductive inference as merely a
helpful perspective for teaching old algorithms and
thinking about new ones, linking NLP to logic and
classical AI. Real implementations would then be
carefully hand-coded in a traditional language.
That was the view ten years ago of finite-state
machines?that FSMs were part of the theoretical
backbone of CL, linking the field to the theory
of computation. Starting in the mid-1990?s, how-
ever, finite-state methods came to the center of ap-
plied NLP as researchers at Xerox, AT&T, Gronin-
gen and elsewhere improved the expressive power
of FSMs by moving from automata to transduc-
ers, adding semiring weights, and developing pow-
erful new regular-expression operators and algo-
rithms for these cases. They also developed soft-
ware. Karttunen et al (1996) built an FSM toolkit
that allowed construction of morphological ana-
lyzers for many languages. Mohri et al (1998)
built a weighted toolkit that implemented novel
algorithms (e.g., weighted minimization, on-the-
fly composition) and scaled up to handle large-
vocabulary continuous ASR. At the same time, re-
newed community-wide interest in shallow methods
for information extraction, chunking, MT, and di-
alogue processing meant that such off-the-shelf FS
toolkits became the core of diverse systems used in
cutting-edge research.
The weakness of FSMs, of course, is that they
are only finite-state. One would like something like
AT&T?s FSM toolkit that also handles the various
formalisms now under consideration for lexicalized
grammars, non-context-free grammars, and syntax-
based MT?and hold the promise of extending to
other formalisms and applications not yet imagined.
We believe that deductive inference should play
the role of regular expressions and FSMs, providing
the theoretical foundation for such an effort. Many
engineering ideas in the field can be regarded, we
281
1. :- double item=0. % declares that all item values are doubles, default is 0
2. constit(X,I,K) += rewrite(X,W) * word(W,I,K). % a constituent is either a word . . .
3. constit(X,I,K) += rewrite(X,Y,Z) * constit(Y,I,J) * constit(Z,J,K). % . . . or a combination of two adjacent subconstituents
4. goal += constit(?s?,0,N) whenever ?ends at(N). % a parse is any s constituent that covers the input string
Figure 1: A probabilistic CKY parser written in Dyna. Axioms are in boldface.
believe, as ideas for how to specify, transform, or
compile systems of inference rules.
2 A Language for Deductive Systems
Any toolkit needs an interface. For example, FS
toolkits offer a regular expression language. We
propose a simple but Turing-complete language,
Dyna, for specifying weighted deductive-inference
algorithms. We illustrate it here by example; see
http://dyna.org for more details and a tutorial.
The short Dyna program in Fig. 1 expresses the
inside algorithm for PCFGs (i.e., the probabilistic
generalization of CKY recognition). Its 3 inference
rules schematically specify many equations, over
an arbitrary number of unknowns. This is possible
bcause the unknowns (items) have structured names
(terms) such as constit(?s?,0,3). They resemble typed
variables in a C program, but we use variable in-
stead to refer to the capitalized identifiers X, I, K,
. . . in lines 2?4. Each rule gives a consequent on
the left-hand side of the +=, which can be built by
combining the antecedents on the right-hand side.1
Lines 2?4 are equational schemas that spec-
ify how to compute the value of items such as
constit(?s?,0,3) from the values of other items.
Using the summation operator +=, lines 2?
3 say that for any X, I, and K, constit(X,I,K)
is defined by summing over the remaining
variables, as
?
W rewrite(X,W)*word(W,I,K) +?
Y,Z,J rewrite(X,Y,Z)*constit(Y,I,J)*constit(Z,J,K). For
example, constit(?s?,0,3) is a sum of quantities such as
rewrite(?s?, ?np?, ?vp?)*constit(?np?,0,1)*constit(?vp?,1,3).
The whenever operator in line 4 specifies a side
condition that restricts the set of expressions in the
sum (i.e., only when N is the sentence length).
To fully define the system of equations, non-
default values (in this case, non-zero values) should
be asserted for some axioms at runtime. (Axioms,
shown in bold in Fig. 1, are items that never appear
1Much of our notation and terminology comes from
logic programming: term, variable, inference rule, an-
tecedent/consequent, assert/retract, axiom/theorem.
as a consequent.) If the PCFG contains a rewrite rule
np ? Mary with probability p(Mary | np)=0.005,
the user should assert that rewrite(?np?, ?Mary?) has
value 0.005. If the input is John loves Mary, val-
ues of 1 should be asserted for word(?John?,0,1),
word(?loves?,1,2), word(?Mary?,2,3), and ends at(3).
Given the axioms as base cases, the equations in
Fig. 1 enable deduction of values for other items.
The value of the theorem constit(?s?,0,3) will be the
inside probability ?s(0, 3),2 and the value of goal
will be the total probability of all parses.
If one replaces += by max= throughout, then con-
stit(?s?,0,3) will accumulate the maximum rather than
the sum of these quantities, and goal will accumulate
the probability of the best parse.
With different input, the same program car-
ries out lattice parsing. Simply assert axioms
that correspond to (weighted) lattice arcs, such as
word(?John?,17,50), where 17 and 50 are arbitrary
terms denoting states in the lattice. It is also quite
straightforward to lexicalize the nonterminals or ex-
tend to synchronous grammars.
A related context-free parsing strategy, shown in
Fig. 2, is Earley?s algorithm. These equations illus-
trate nested terms such as lists. The side condition
in line 2 prevents building any constituent until one
has built a left context that calls for it.
3 Relation to Previous Work
There is a large relevant literature. Some of the well-
known CL papers, notably Goodman (1999), were
already mentioned in section 1.1. Our project has
three main points of difference from these.
First, we provide an efficient, scalable, open-
source implementation, in the form of a compiler
from Dyna to C++ classes. (Related work is in ?7.2.)
The C++ classes are efficient and easy to use, with
statements such as c[rewrite(?np?,2,3)]=0.005 to assert
axiom values into a chart named c (i.e., a deduc-
2That is, the probability that s would stochastically rewrite
to the first three words of the input. If this can happen in more
than one way, the probability sums over multiple derivations.
282
1. need(??s??,0) = 1. % begin by looking for an s that starts at position 0
2. constit(Nonterm/Needed,I,I) += rewrite(Nonterm,Needed) whenever ?need(Nonterm, I). % traditional predict step
3. constit(Nonterm/Needed,I,K) += constit(Nonterm/cons(W,Needed),I,J) * word(W,J,K). % traditional scan step
4. constit(Nonterm/Needed,I,K) += constit(Nonterm,cons(X,Needed),I,J) * constit(X/nil,J,K). % traditional complete step
5. goal += constit(?s?/nil,0,N) whenever ?ends at(N). % we want a complete s constituent covering the sentence
6. need(Nonterm,J) += constit( /cons(Nonterm, ), ,J). % Note: underscore matches anything (anonymous wildcard)
Figure 2: An Earley parser that recovers inside probabilities (Earley, 1970; Stolcke, 1995). The rule np ? det n should be encoded
as the axiom rewrite(?np?,cons(?det?,cons(?n?,nil))), a nested term. ?np?/Needed is the label of a partial np constituent that is
still missing the list of subconstituents in Needed. need(?np?,3) is derived if some partial constituent seeks an np subconstituent
starting at position 3. As in Fig. 1, lattice parsing comes for free, as does training.
tive database) and expressions like c[goal] to extract
the values of the resulting theorems, which are com-
puted as needed. The C++ classes also give access to
the proof forest (e.g., the forest of parse trees), and
integrate with parameter optimization code.
Second, we fully generalize the agenda-based
strategy of Shieber et al (1995) to the weighted
case?in particular supporting a prioritized agenda.
That allows probabilities to guide the search for
the best parse(s), a crucial technique in state-of-the-
art context-free parsers.3 We also give a ?reverse?
agenda algorithm to compute gradients or outside
probabilities for parameter estimation.
Third, regarding weights, the Dyna language is
designed to express systems of arbitrary, hetero-
geneous equations over item values. In previous
work such as (Goodman, 1999; Nederhof, 2003),
one only specifies the inference rules as unweighted
Horn clauses, and then weights are added automat-
ically in a standard way: all values have the same
type W, and all rules transform to equations of the
form c ?= a1 ? a2 ? ? ? ? ? ak, where ? and ?
give W the structure of a semiring.4 In Dyna one
writes these equations explicitly in place of Horn
clauses (Fig. 1). Accordingly, heterogeneous Dyna
programs, to be supported soon by our compiler,
will allow items of different types to have values
of different types, computed by different aggrega-
tion operations over arbitrary right-hand-side ex-
3Previous treatments of weighted deduction have used an
agenda only for an unweighted parsing phase (Goodman, 1999)
or for finding the single best parse (Nederhof, 2003). Our algo-
rithm works in arbitrary semirings, including non-idempotent
ones, taking care to avoid double-counting of weights and to
handle side conditions.
4E.g., the inside algorithm in Fig. 1 falls into Goodman?s
framework, with ?W,?,?? = ?R?0,+, ???the PLUSTIMES
semiring. Because ? distributes over ? in a semiring, com-
puting goal is equivalent to an aggregation over many separate
parse trees. That is not the case for heterogeneous programs.
pressions. This allows specification of a wider class
of algorithms from NLP and elsewhere (e.g., mini-
mum expected loss decoding, smoothing formulas,
neural networks, game tree analysis, and constraint
programming). Although ?4 and ?5 have space to
present only techniques for the semiring case, these
can be generalized.
Our approach may be most closely related to de-
ductive databases, which even in their heyday were
apparently ignored by the CL community (except for
Minnen, 1996). Deductive database systems per-
mit inference rules that can derive new database
facts from old ones.5 They are essentially declara-
tive logic programming languages (with restrictions
or extensions) that are?or could be?implemented
using efficient database techniques. Some imple-
mented deductive databases such as CORAL (Ra-
makrishnan et al, 1994) and LOLA (Zukowski and
Freitag, 1997) support aggregation (as in Dyna?s
+=, log+=, max=, . . . ), although only ?stratified?
forms of it that exclude unary CFG rule cycles.6
Ross and Sagiv (1992) (and in a more restricted
way, Kifer and Subrahmanian, 1992) come closest to
our notion of attaching aggregable values to terms.
Among deductive or other database systems,
Dyna is perhaps unusual in that its goal is not to sup-
port transactional databases or ad hoc queries, but
rather to serve as an abstract layer for specifying an
algorithm, such as a dynamic programming (DP) al-
gorithm. Thus, the Dyna program already implicitly
or explicitly specifies all queries that will be needed.
This allows compilation into a hard-coded C++ im-
plementation. The compiler?s job is to support these
queries by laying out and indexing the database re-
5Often they use some variant of the unweighted agenda-
based algorithm, which is known in that community as ?semi-
naive bottom-up evaluation.?
6An unweighted parser was implemented in an earlier ver-
sion of LOLA (Specht and Freitag, 1995).
283
lations in memory7 in a way that resembles hand-
designed data structures for the algorithm in ques-
tion. The compiler has many choices to make here;
we ultimately hope to implement feedback-directed
optimization, using profiled sample runs on typical
data. For example, a sparse grammar should lead to
different strategies than a dense one.
4 Computing Theorem Values
Fig. 1 specifies a set of equations but not how to
solve them. Any declarative specification language
must be backed up by a solver for the class of speci-
fiable problems. In our continuing work to develop a
range of compiler strategies for arbitrary Dyna pro-
grams, we have been inspired by the CL commu-
nity?s experience in building efficient parsers.
In this paper and in our current implementa-
tion, we give only the algorithms for what we call
weighted dynamic programs, in which all axioms
and theorems are variable-free. This means that
a consequent may only contain variables that al-
ready appear elsewhere in the rule. We further re-
strict to semiring-weighted programs as in (Good-
man, 1999). But with a few more tricks not given
here, the algorithms can be generalized to a wider
class of heterogeneous weighted logic programs.8
4.1 Desired properties
Computation is triggered when the user requests the
value of one or more particular items, such as goal.
Our algorithm must have several properties in order
to substitute for manually written code.
Soundness. The algorithm cannot be guaranteed
to terminate (since it is possible to write arbitrary
Turing machines in Dyna). However, if it does ter-
minate, it should return values from a valid model of
the program, i.e., values that simultaneously satisfy
all the equations expressed by the program.
Reasonable completeness. The computation
should indeed terminate for programs of interest
to the NLP community, such as parsing under a
probabilistic grammar?even if the grammar has
7Some relations might be left unmaterialized and computed
on demand, with optional memoization and flushing of memos.
8Heterogeneous programs may propagate non-additive up-
dates, which arbitrarily modify one of the inputs to an aggrega-
tion. Non-dynamic programs require non-ground items in the
chart, complicating both storage and queries against the chart.
1. for each axiom a, set agenda[a] := value of axiom a
2. while there is an item a with agenda[a] 6= 0
3. (* remove an item from the agenda and move its value to the chart *)
4. choose such an a
5. ? := agenda[a]; agenda[a] := 0
6. old := chart[a]; chart[a] := chart[a]??
7. if chart[a] 6= old (* only propagate actual changes *)
8. (* compute new resulting updates and place them on the agenda *)
9. for each inference rule ?c ?= a1 ? a2 ? ? ? ? ? ak?
10. for i from 1 to k
11. for each way of instantiating the rule?s variables
such that ai = a
12. agenda[c] ?=
kO
j=1
8
><
>:
old if j < i and
aj = a
? if j = i
chart[aj ] otherwise
(* can skip this line if any multiplicand is 0 *)
Figure 3: Weighted agenda-based deduction in a semiring, with-
out side conditions (see text).
left recursion, unary rule cycles, or ?-productions.
This appears to rule out pure top-down (?backward-
chaining?) approaches.
Efficiency. Returning the value of goal should
do only as much computation as necessary. To re-
turn goal, one may not need to compute the values
of all items.9 In particular, finding the best parse
should not require finding all parses (in contrast to
Goodman (1999) and Zhou and Sato (2003)). Ap-
proximation techniques such as pruning and best-
first search must also be supported for practicality.
4.2 The agenda algorithm
Our basic algorithm (Fig. 3) is a weighted agenda-
based algorithm that works only with rules of the
form c ?= a1?a2?? ? ??ak. ? must distribute over ?.
Further, the default value for items (line 1 of Fig. 1)
must be the semiring?s zero element, denoted 0.10
Agenda-based deduction maintains two indexed
data structures: the agenda and the chart. chart[a]
stores the current value of item a. The agenda holds
future work that arises from assertions or from pre-
vious changes to the chart: agenda[a] stores an in-
cremental update to be added (using ?) to chart[a]
in future. If chart[a] or agenda[a] is not stored, it is
9This also affects completeness, as it sometimes enables the
computation of goal to terminate even if the program as a whole
contains some irrelevant non-terminating computation. Even
in practical cases, the runtime of computing all items is often
prohibitive, e.g., proportional to n6 or worse for a dense tree-
adjoining grammar or synchronous grammar.
10It satisfies x ? 0 = x, x ? 0 = 0 for all x. Also, this
algorithm requires ? to distribute over ?. Dyna?s semantics
requires ? to be associative and commutative.
284
taken to be the default 0.
When item a is removed from the agenda, its
chart weight is updated by the increment value. This
change is then propagated to other items c, via rules
of the form c ?= ? ? ? with a on the right-hand-side.
The resulting changes to c are placed back on the
agenda and carried out only later.
The unweighted agenda-based algorithm (Shieber
et al, 1995) may be regarded as the case where
?W,?,?? = ?{T, F},?,??. It has previously
been generalized (Nederhof, 2003) to the case
?W,?,?? = ?R?0,max,+?. In Fig. 3, we make
the natural further generalization to any semiring.
How is this a further generalization? Since ? (un-
like ? and max) might not be idempotent, we must
take care to avoid erroneous double-counting if the
antecedent a combines with, or produces, another
copy of itself.11 For instance, if the input contains ?
words, line 2 of Fig. 1 may get instantiated as con-
stit(?np?,5,5) += rewrite(?np?,?np?,?np?) * constit(?np?,5,5) *
constit(?np?,5,5). This is why we save the old values
of agenda[a] and chart[a] as ? and old, and why
line 12 is complex.
4.3 Side conditions
We now extend Fig. 3 to handle Dyna?s
side conditions, i.e., rules of the form
c ?= expression whenever boolean-expression.
We discuss only the simple side conditions
treated in previous literature, which we write as
c ?= a1?a2?? ? ??ak? whenever ?bk?+1 & ? ? ? & ?bk.
Here, ?bj is true or false according to whether there
exists an unweighted proof of bj .
Again, what is new here? Nederhof (2003) con-
siders only max= with a uniform-cost agenda disci-
pline (see ?4.5), which guarantees that no item will
be removed more than once from the agenda. We
wish to support other cases, so we must take care
that a second update to ai will not retrigger rules of
which ai is a side condition.
For simplicity, let us reformulate the above rule
as c ?= a1 ? a2 ? ? ? ? ? ak? ? ?bk?+1 ? ? ? ? ? ?bk,
where ?bi is now treated as having value 0 or 1 (the
identity for ?) rather than false or true respectively.
11An agenda update that increases x by 0.3 will increase r ?
x ? x by r ? (0.6x+0.09). Hence, the rule x += r ? x ? x must
propagate a new increase of that size to x, via the agenda.
We may now use Fig. 3, but now any aj might
have the form ?bj . Then in line 12, chart[aj ] will be
chart[?bj ], which is defined as 1 or 0 according to
whether chart[bj ] is stored (i.e., whether bj has been
derived). Also, if ai = ?a at line 11 (rather than
ai = a), then ? in line 12 is replaced by ??, where
we have set ?? := chart[?a] at line 5.
4.4 Convergence
Whether the agenda algorithm halts depends on the
Dyna program and the input. Like any other Turing-
complete language, Dyna gives you enough freedom
to write undesirable programs.
Most NLP algorithms do terminate, of course,
and this remains true under the agenda algorithm.
For typical algorithms, only finitely many differ-
ent items (theorems) can be derived from a given
finite input (set of axioms).12 This ensures termi-
nation if one is doing unweighted deduction with
?W,?,?? = ?{T, F},?,??, since the test at line 7
ensures that no item is processed more than once.13
The same test ensures termination if one is
searching for the best proof or parse with (say)
?W,?,?? = ?R?0,min,+?, where values are
negated log probabilities. Positive-weight cycles
will not affect the min. (Negative-weight cycles,
however, would correctly cause the computation to
diverge; these do not arise with probabilities.)
If one is using ?W,?,?? = ?R?0,+, ?? to com-
pute the total weight of all proofs or parses, as in
the inside algorithm, then Dyna must solve a sys-
tem of nonlinear equations. The agenda algorithm
does this by iterative approximation (propagating
updates around any cycles in the proof graph until
numerical convergence), essentially as suggested by
Stolcke (1995) for the case of Earley?s algorithm.14
Again, the computation may diverge.
12This holds for all Datalog programs, for instance.
13This argument does not hold if Dyna is used to express
programs outside the semiring. In particular, one can write in-
stances of SAT and other NP-hard constraint satisfaction prob-
lems by using cyclic rules with negation over finitely many
boolean-valued items (Niemela?, 1998). Here the agenda algo-
rithm can end up flipping values forever between false and true;
a more general solver would have to be called in order to find a
stable model of a SAT problem?s equations.
14Still assuming the number of items is finite, one could in
principle materialize the system of equations and call a ded-
icated numerical solver. In some special cases only a linear
solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or
?-cycles in FSMs (Eisner, 2002).
285
One can declare the conditions under which items
of a particular type (constit or goal) should be treated
as having converged. Then asking for the value
of goal will run the agenda algorithm not until the
agenda is empty, but only until chart[goal] has con-
verged by this criterion.
4.5 Prioritization
The order in which items are chosen at line 4 does
not affect the soundness of the agenda algorithm,
but can greatly affect its speed. We implement the
agenda as a priority queue whose priority function
may be specified by the user.15
Charniak et al (1998) and Caraballo and Char-
niak (1998) showed that, when seeking the best
parse (using min= or max=), best-first parsing can
be extremely effective. Klein and Manning (2003a)
went on to describe admissible heuristics and an A*
framework for parsing. For A* in our general frame-
work, the priority of item a should be an estimate of
the value of the best proof of goal that uses a. (This
non-standard formulation is carefully chosen.16) If
so, goal is guaranteed to converge the very first time
it is selected from the priority-queue agenda.
Prioritizing ?good? items first can also be useful
in other circumstances. The inside-outside training
algorithm requires one to find all parses, but finding
the high-probability parses first allows one to ignore
the rest by ?early stopping.?
In all these schemes (even A*), processing
promising items as soon as possible risks having to
reprocess them if their values change later. Thus,
this strategy should be balanced against the ?topo-
logical sort? strategy of waiting to process an item
until its value has (probably) converged.17 Ulti-
15At present by writing a C++ function; ultimately within
Dyna, by defining items such as priority(constit(?s?,0,3)).
16It is correct for proofs that incorporate two copies of a?s
value, or?more important?no copies of a?s value because a is
a side condition. Thus, it recognizes that a low-probability item
must have high priority if it could be used as a side condition
in a higher-probability parse (though this cannot happen for the
side conditions derived by the magic templates transformation
(?6)). Note also that a?s own value (Nederhof, 2003) might not
be an optimistic estimate, if negative weights are present.
17In parsing, for example, one often processes narrower con-
stituents before wider ones. But such strategies do not always
exist, or break down in the presence of unary rule cycles, or
cannot be automatically found. Goodman?s (1999) strategy
of building all items and sorting them before computing any
weights is wise only if one genuinely wants to build all items.
mately we hope to learn priority functions that ef-
fectively balance these two strategies (especially in
the context of early stopping).
4.6 Matching, indexing, and interning
The crucial work in Fig. 3 occurs in the iteration over
instantiated rules at lines 9?11. In practice, we re-
structure this triply nested loop as follows, where
each line retains the variable bindings that result
from the unification in the previous line:
9. for each antecedent pattern ai that appears in some
program rule r and unifies with a
10. for each way of simultaneously unifying r?s remain-
ing antecedent patterns a1, . . . ai?1, ai+1, . . . ak
with items that may have non-0 value in the chart
11. construct r?s consequent c (* all vars are bound *)
Our implementation of line 9 tests a against all of the
antecedent patterns at once, using a tree of simple
?if? tests (generated by the Dyna-to-C++ compiler)
to share work across patterns. As an example, a =
constit(?np?,3,8) will match two antecedents at line 3
of Fig. 1, but will fail to match in line 4. Because a is
variable-free (for DPs), a full unification algorithm
is not necessary, even though an antecedent pattern
can contain repeated variables and nested subterms.
Line 10 rapidly looks up the rule?s other an-
tecedents using indices that are automatically main-
tained on the chart. For example, once con-
stit(?np?,4,8) has matched antecedent 2 of line 3 of
Fig. 1, the compiled code consults a maintained
list of the chart constituents that start at position 8
(i.e., items of the form constit(Z,8,K) that have al-
ready been derived). Suppose one of these is con-
stit(?vp?,8,15): then the code finds the rule?s remain-
ing antecedent by consulting a list of items of the
form rewrite(X,?np?,?vp?). That leads it to construct
consequents such as constit(?s?,4,15) at line 11.
By default, equal terms are represented by equal
pointers. While this means terms must be ?interned?
when constructed (requiring hash lookup), it en-
forces structure-sharing and allows any term to be
rapidly copied, hashed, or equality-tested without
dereferencing the pointer.18
Each of the above paragraphs conceals many deci-
sions that affect runtime. This presents future oppor-
tunities for feedback-directed optimization, where
profiled runs on typical data influence the compiler.
18The compiled code provides garbage collection on the
terms; this is important when running over large datasets.
286
5 Computing Gradients
The value of goal is a function of the axioms? values.
If the function is differentiable, we may want to get
its gradient with respect to its parameters (the axiom
values), to aid in numerically optimizing it.
5.1 Gradients by symbolic differentiation
The gradient computation can be derived from the
original by a program transformation. For each item
a in the original program?in particular, for each
axiom?the new program will also compute a new
item g(a), whose value is ?goal/?a.
Thus, given weighted axioms, the new program
computes both goal and ?goal. An optimization al-
gorithm such as conjugate gradient can use this in-
formation to tune the axiom weights to maximize
goal. An alternative is the EM algorithm (Dempster
et al, 1977) for probabilistic generative models such
as PCFGs. Luckily the same program serves, since
for such models, the E count (expected count) of an
item a can be found as a ? g(a)/goal. In other words,
the inside-outside algorithm has the same structure
as computing the function and its gradient.
The GRADIENT transformation is simple. For
example,19 given a rule c += a1 ? a2 ? ? ? ? ?
ak? whenever ?bk?+1 & ? ? ? & ?bk, we add a new rule
g(ai) += g(c) ? a1 ? ? ? ? ? ai?1 ? ai+1 ? ? ? ? ?
ak? whenever ?ai, for each i = 1, 2, ..., k?. (The orig-
inal rule remains, since we need inside values to
compute outside values.) This strategy for comput-
ing the gradient ?goal/?a via the chain rule is an
example of automatic differentiation in the reverse
mode (Griewank and Corliss, 1991), known in the
neural network community as back-propagation.
5.2 Gradients by back-propagation
However, what if goal might be computed only ap-
proximately, by early stopping before convergence
(?4.5)? To avoid confusing the optimizer, we want
the exact gradient of the approximate function.
To do this, we ?unwind? the computation of
goal, undoing the value updates while building up
the gradient values. The idea is to differentiate
an ?unrolled? version of the original computation
(Williams and Zipser, 1989), in which an item at
19More generally, g(ai) = ?goal/?ai = Pc ?goal/?c ?
?c/?ai =
P
c g(c) ? ?c/?ai by the chain rule.
1. for each a, gchart[a] := 0 and gagenda[a] := 0
(* respectively hold ?goal/?chart[a] and ?goal/?agenda[a] *)
2. gchart[goal] := 1
3. for each ?a,?, old? triple that was considered at line 8
of Fig. 3, but in the reverse order (* ? is agenda[a] *)
4. ? := gchart[a] (* will accumulate gagenda[a] here *)
5. for each inference rule ?c += a1 ? a2 ? ? ? ? ? ak?
6. for i from 1 to k
7. for each way of instantiating the rule?s variables
such that ai = a
8. for h from 1 to k such that ah is not a side cond.
(* find ?goal/?agenda[c] ? ?agenda[c]/?(ah factor) *)
9. ? :=
kY
j=1
8
>
><
>
>:
gagenda[c] if j = h
old if j 6= h and j < i
and aj = a
? if j 6= h and j = i
chart[aj ] otherwise
10. if h 6= i then gchart[ah] += ?
11. if h ? i and ah = a then ? += ?
12. gagenda[a] := ?
13. chart[a] := old
14. return gagenda[a] for each axiom a
Figure 4: An efficient algorithm for computing ?goal (even
when goal is an early-stopping approximation), specialized to
the case ?W,?,?? = ?R,+, ??. The proof is suppressed for
lack of space.
time t is considered to be a different variable (possi-
bly with different value) than the same item at time
t + 1. The reverse pass must recover earlier values.
Our somewhat tricky algorithm is shown in Fig. 4.
At line 3, a stack is needed to remember the se-
quence of ?a, old,?? triples from the original com-
putation.20 It is a more efficient version of the ?tape?
usually used in automatic differentiation. For exam-
ple, it uses O(n2) rather than O(n3) space for the
CKY algorithm. The trick is that Fig. 3 does not
record all its computations, but only its sequence of
items. Fig. 4 then re-runs the inference rules to re-
construct the computations in an acceptable order.
This method is a generalization of Eisner?s (2001)
prioritized forward-backward algorithm for infinite-
state machines. As Eisner (2001) pointed out, the
tape created on the first forward pass can also be
used to speed up later passes (i.e., after the numeri-
cal optimizer has adjusted the axiom weights).21
20If one is willing to risk floating-point error, then one can
store only ?a, old? on the stack and recover ? as chart[a]?old.
Also, agenda[a] and gagenda[a] can be stored in the same loca-
tion, as they are only used during the forward and the backward
pass, respectively.
21In brief, a later forward pass that chooses a at Fig. 3, line 4
according to the recorded tape order (1) is faster than using a
priority queue, (2) avoids ordering-related discontinuities in the
objective function as the axiom weights change, (3) can prune
by skipping useless updates a that scarcely affected goal (e.g.,
287
5.3 Parameter estimation
To support parameter training using these gradi-
ents, our implementation of Dyna includes a train-
ing module, DynaMITE. DynaMITE supports the
EM algorithm (and many variants), supervised and
unsupervised training of log-linear (?maximum en-
tropy?) models using quasi-Newton methods, and
smoothing-parameter tuning on development data.
As an object-oriented C++ library, it also facilitates
rapid implementation of new estimation techniques
(Smith and Eisner, 2004; Smith and Eisner, 2005).
6 Program Transformations
Another interest of Dyna is that its high-level speci-
fications can be manipulated by mechanical source-
to-source program transformations. This makes it
possible to derive new algorithms from old ones.
?5.1 already sketched the gradient transformation
for finding ?goal. We note a few other examples.
Bounding transformations generate a new pro-
gram that computes upper or lower bounds on goal,
via generic bounding techniques (Prieditis, 1993;
Culberson and Schaeffer, 1998). The A* heuristics
explored by Klein and Manning (2003a) can be seen
as resulting from bounding transformations.
With John Blatz, we are also exploring trans-
formations that can result in asymptotically more
efficient computations of goal. Their unweighted
versions are well-known in the logic programming
community (Tamaki and Sato, 1984; Ramakrish-
nan, 1991). Folding introduces new intermediate
items, perhaps exploiting the distributive law; ap-
plications include parsing speedups such as (Eisner
and Satta, 1999), as well as well-known techniques
for speeding up multi-way database joins, constraint
programming, or marginalization of graphical mod-
els. Unfolding eliminates items; it can be used to
specialize a parser to a particular grammar and then
to eliminate unary rules. Magic templates introduce
top-down filtering into the search strategy and can be
used to derive Earley?s algorithm (Minnen, 1996), to
introduce left-corner filters, and to restrict FSM con-
structions to build only accessible states.
Finally, there are low-level optimizations. Term
constituents not in any good parse) by consulting gagenda[a]
values that the previous backward pass can have written onto
the tape (overwriting ? or old).
transformations restructure terms to change their
layout in memory. We are also exploring the intro-
duction of declarations that control which items use
the agenda or are memoized in the chart. This can
be used to support lazy or ?on-the-fly? computation
(Mohri et al, 1998) and asymptotic space-saving
tricks (Binder et al, 1997).
7 Usefulness of the Implementation
7.1 Applications
The current Dyna compiler has proved indispens-
able in our own recent projects, in the sense that we
would not have attempted many of them without it.
In some cases, we were experimenting with gen-
uinely new algorithms not supported by any ex-
isting tool, as in our work on dependency-length-
limited parsing (Eisner and Smith, 2005b) and
loosely syntax-based machine translation (Eisner
and D. Smith, 2005). (Dyna would have been
equally helpful in the first author?s earlier work on
new algorithms for lexicalized and CCG parsing,
syntactic MT, transformational syntax, trainable pa-
rameterized FSMs, and finite-state phonology.)
In other cases (Smith and Eisner, 2004; Smith and
Smith, 2004; Smith et al, 2005), Dyna let us quickly
replicate, tweak, and combine useful techniques
from the literature. These techniques included un-
weighted FS morphology, conditional random fields
(Lafferty et al, 2001), synchronous parsers (Wu,
1997; Melamed, 2003), lexicalized parsers (Eisner
and Satta, 1999),22 partially supervised training a` la
(Pereira and Schabes, 1992),23 and grammar induc-
tion (Klein and Manning, 2002). These replications
were easy to write and extend, and to train via ?5.2.
7.2 Experiments
We compared the current Dyna compiler to hand-
built systems on a variety of parsing tasks. These
problems were chosen not for their novelty or inter-
esting structure, but for the availability of existing
well-tuned implementations.
Best parse. We compared a Dyna CFG parser
to the Java parser of Klein and Manning (2003b),24
22Markus Dreyer?s reimplementation of the complex
Collins (1999) parser uses under 30 lines of Dyna.
23For example, lines 2?3 of Fig. 1 can be extended with
whenever permitted(X,I,K).
24Neither uses heuristics from Klein and Manning (2003a).
288
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0  1  2  3  4  5  6  7  8  9  10
Dy
na 
par
ser
 ru
ntim
e (s
eco
nds
)
Klein & Manning runtime (seconds)
y=3.99x
y=7.16x
y=9.94x
y=11.25x
10 words20 words30 words40 words
Figure 5: Dyna CKY parser vs. Klein & Manning hand-built
parser, comparing runtime.
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 0  5  10  15  20  25  30  35
Dy
na 
par
ser
 ru
ntim
e (s
eco
nds
)
Handwritten parser runtime (seconds)
y=5.4x
10 words20 words30 words
Figure 6: Dyna CKY parser vs. C++PARSE, a similar hand-
built parser. The implementation differences amount to storage
and indexing and give a consistent 5-fold speedup.
on the same grammar. Fig. 5 shows the re-
sults. Dyna?s disadvantage is greater on longer
sentences?probably because its greater memory
consumption results in worse cache behavior.25
We also compared a Dyna CKY parser to
our own hand-built implementation, C++PARSE.
C++PARSE is designed like the Dyna parser but
includes a few storage and indexing optimizations
that Dyna does not yet have. Fig. 6 shows the 5-
fold speedup from these optimizations on binarized-
Treebank parsing with a large 119K-rule grammar.
The sharp diagonal indicates that C++PARSE is sim-
ply a better-tuned version of the Dyna parser.
These optimizations and others are now being in-
corporated into the Dyna compiler, and are expected
25Unlike Java, Dyna does not yet decide automatically when
to perform garbage collection. In our experiment, garbage col-
lection was called explicitly after each sentence and counted
as part of the runtime (typically 0.25 seconds for 10-word sen-
tences, 5 seconds for 40-word sentences).
99% 99.99%
uniform 89.3 (4.5) 90.3 (4.6)
after 1 EM iteration 82.9 (6.8) 85.2 (6.9)
after 2 EM iterations 77.1 (8.4) 79.1 (8.3)
after 3 EM iterations 71.6 (9.4) 73.7 (9.5)
after 4 EM iterations 66.8 (10.0) 68.8 (10.2)
after 5 iterations 62.9 (10.3) 65.0 (10.5)
Table 1: Early stopping. Each row describes a PCFG at a differ-
ent stage of training; later PCFGs are sharper. The table shows
the percentage of agenda runtime (mean across 1409 sentences,
and standard deviation) required to get within 99% or 99.99%
of the true value of goal.
to provide similar speedups, putting Dyna?s parser
in the ballpark of the Klein & Manning parser. Im-
portantly, these improvements will speed up existing
Dyna programs through recompilation.
Inside parsing. Johnson (2000) provides a C im-
plementation of the inside-outside algorithm for EM
training of PCFGs. We ran five iterations of EM
on the WSJ10 corpus26 using the Treebank grammar
from that corpus. Dyna took 4.1 times longer.
Early stopping. An advantage of the weighted
agenda discipline (?4.2) is that, with a reasonable
priority function such as an item?s inside probabil-
ity, the inside algorithm can be stopped early with
an estimate of goal?s value. To measure the goodness
of this early estimate, we tracked the progression of
goal?s value as each sentence was being parsed. In
most instances, and especially after more EM itera-
tions, the estimate was very tight long before all the
weight had been accumulated (Table 1). This sug-
gests that early stopping is a useful training speedup.
PRISM. The implemented tool most similar to
Dyna that we have found is PRISM (Zhou and Sato,
2003), a probabilistic Prolog with efficient tabling
and compilation. PRISM inherits expressive power
from Prolog but handles only probabilities, not gen-
eral semirings (or even side conditions).27 In CKY
parsing tests, PRISM was able to handle only a small
fraction of the Penn Treebank ruleset (2,400 high-
probability rules) and tended to crash on long sen-
tences. Dyna is designed for real-world use: it con-
sistently parses over 10? faster than PRISM and
scales to full-sized problems.
IBAL (Pfeffer, 2001) is an elegant and power-
ful language for probabilistic modeling; it general-
izes Bayesian networks in interesting ways.28 Since
26Sentences with ?10 words, stripping punctuation.
27Thus it can handle a subset of the cases described by
Goodman (1999), again by building the whole parse forest.
28It might be possible to implement IBAL in Dyna (Pfeffer,
289
PCFGs and marginalization can be succinctly ex-
pressed in IBAL, we attempted a performance com-
parison on the task of the inside algorithm (Fig. 1).
Unfortunately, IBAL?s algorithm appears not to ter-
minate if the PCFG contains any kind of recursion
reachable from the start symbol.
8 Conclusions
Weighted deduction is a powerful theoretical for-
malism that encompasses many NLP algorithms
(Goodman, 1999). We have given a bottom-up ?in-
side? algorithm for general semiring-weighted de-
duction, based on a prioritized agenda, and a general
?outside? algorithm that correctly computes weight
gradients even when the inside algorithm is pruned.
We have also proposed a declarative language,
Dyna, that replaces Prolog?s Horn clauses with
?Horn equations? over terms with values. Dyna can
express more than the semiring-weighted dynamic
programs treated in this paper. Our ongoing work
concerns the full Dyna language, program transfor-
mations, and feedback-directed optimization.
Finally, we evaluated our first implementation of
a Dyna-to-C++ compiler (download and documen-
tation at http://dyna.org). We hope it will facili-
tate EMNLP research, just as FS toolkits have done
for the FS case. It produces code that is slower than
hand-crafted code but acceptably fast for our NLP
research, where it has been extremely helpful.
References
J. Binder, K. Murphy, and S. Russell. 1997. Space-efficient inference in dynamic
probabilistic networks. In Proc. of IJCAI.
S. A. Caraballo and E. Charniak. 1998. New figures of merit for best-first proba-
bilistic chart parsing. CL, 24(2):275?298.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-based best-first chart
parsing. In Proc. of COLING-ACL.
M. J. Collins. 1999. Head-Driven Statistical Models for Natural Language Pars-
ing. Ph.D. thesis, U. of Pennsylvania.
J. C. Culberson and J. Schaeffer. 1998. Pattern databases. Computational Intelli-
gence.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood estimation
from incomplete data via the EM algorithm. Journal of the Royal Statistical
Society B, 39:1?38.
J. Earley. 1970. An efficient context-free parsing algorithm. Communications of
the ACM, 13(2):94?102.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical CFGs and head-
automaton grammars. In Proc. of ACL.
J. Eisner and D. A. Smith. 2005a. Quasi-synchronous grammars: Alignment by
soft projection of syntactic dependencies. Technical report, Johns Hopkins U.
J. Eisner and N. A. Smith. 2005b. Parsing with soft and hard constraints on
dependency length. In Proc. of IWPT.
p.c.). Dyna is a lower-level language that itself knows nothing
about the semantics of probability models, but whose inference
rules could be used to implement any kind of message passing.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declarative language for
implementing dynamic programs. In Proc. of ACL (companion vol.).
J. Eisner. 2001. Smoothing a Probabilistic Lexicon via Syntactic Transforma-
tions. Ph.D. thesis, U. of Pennsylvania.
J. Eisner. 2002. Parameter estimation for probabilistic FS transducers. In Proc.
of ACL.
J. Goodman. 1999. Semiring parsing. CL, 25(4):573?605.
A. Griewank and G. Corliss, editors. 1991. Automatic Differentiation of Algo-
rithms. SIAM.
M. Johnson. 2000. Inside-outside (computer program).
http://www.cog.brown.edu/?mj/Software.htm.
L. Karttunen, J.-P. Chanod, G. Grefenstette, and A. Schiller. 1996. Regular ex-
pressions for language engineering. JNLE, 2(4):305?328.
M. Kifer and V. S. Subrahmanian. 1992. Theory of generalized annotated
logic programming and its applications. Journal of Logic Programming,
12(4):335?368.
D. Klein and C. D. Manning. 2002. A generative constituent-context model for
grammar induction. In Proc. of ACL.
D. Klein and C. D. Manning. 2003a. A? parsing: Fast exact Viterbi parse selec-
tion. In Proc. of HLT-NAACL.
D. Klein and C. D. Manning. 2003b. Accurate unlexicalized parsing. In Proc. of
ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling sequence data. In Proc. of ICML.
I. D. Melamed. 2003. Multitext grammars and synchronous parsers. In Proc.
HLT-NAACL.
G. Minnen. 1996. Magic for filter optimization in dynamic bottom-up processing.
In Proc. of ACL.
M. Mohri, F. Pereira, and M. Riley. 1998. A rational design for a weighted FST
library. LNCS, 1436.
M.-J. Nederhof. 2003. Weighted deductive parsing and Knuth?s algorithm. CL,
29(1):135?143.
I. Niemela?. 1998. Logic programs with stable model semantics as a constraint
programming paradigm. In Proc. Workshop on Computational Aspects of
Nonmonotonic Reasoning.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially brack-
eted corpora. In Proc. of ACL.
F. Pereira and D. H. D. Warren. 1983. Parsing as deduction. In Proc. of ACL.
A. Pfeffer. 2001. IBAL: An integrated Bayesian agent language. In Proc. of
IJCAI.
A. Prieditis. 1993. Machine discovery of effective admissible heuristics. Ma-
chine Learning, 12:117?41.
R. Ramakrishnan, D. Srivastava, S. Sudarshan, and P. Seshadri. 1994. The
CORAL deductive system. The VLDB Journal, 3(2):161?210.
R. Ramakrishnan. 1991. Magic templates: a spellbinding approach to logic pro-
grams. J. Log. Program., 11(3-4):189?216.
K. A. Ross and Y. Sagiv. 1992. Monotonic aggregation in deductive databases.
In Proc. of the ACM SIGACT-SIGMOD-SIGART Symposium on Principles of
Database Systems.
S. M. Shieber, Y. Schabes, and F. Pereira. 1995. Principles and implementation
of deductive parsing. Journal of Logic Programming, 24(1?2):3?36.
K. Sikkel. 1997. Parsing Schemata: A Framework for Specification and Analysis
of Parsing Algorithms. Texts in Theoretical Computer Science. Springer.
N. A. Smith and J. Eisner. 2004. Annealing techniques for unsupervised statisti-
cal language learning. In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear
models on unlabeled data. In Proc. of ACL.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing with factored estimation:
Using English to parse Korean. In Proc. of EMNLP.
N. A. Smith, D. A. Smith, and R. W. Tromble. 2005. Context-based morphologi-
cal disambiguation with random fields. In Proc. of HLT-EMNLP.
G. Specht and B. Freitag. 1995. AMOS: A NL parser implemented as a deductive
database in LOLA. In Applications of Logic Databases. Kluwer.
A. Stolcke. 1995. An efficient probabilistic CF parsing algorithm that computes
prefix probabilities. CL, 21(2):165?201.
H. Tamaki and T. Sato. 1984. Unfold/fold transformation of logic programs.
In S. A?. Ta?rnlund, editor, Proceedings Second International Conference on
Logic Programming, pages 127?138, Uppsala University.
R. J. Williams and D. Zipser. 1989. A learning algorithm for continually running
fully recurrent neural networks. Neural Computation, 1(2):270?280.
D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing
of parallel corpora. CL, 23(3):377?404.
N.-F. Zhou and T. Sato. 2003. Toward a high-performance system for symbolic
and statistical modeling. In Proc. of Workshop on Learning Statistical Models
from Relational Data.
U. Zukowski and B. Freitag. 1997. The deductive database system LOLA. In
Logic Programming and Nonmonotonic Reasoning, LNAI 1265. Springer.
290
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 395?402, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Bootstrapping Without the Boot?
Jason Eisner and Damianos Karakos
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{eisner,damianos}@jhu.edu
Abstract
?Bootstrapping? methods for learning require a small amount
of supervision to seed the learning process. We show that it
is sometimes possible to eliminate this last bit of supervision,
by trying many candidate seeds and selecting the one with the
most plausible outcome. We discuss such ?strapping? methods
in general, and exhibit a particular method for strapping word-
sense classifiers for ambiguous words. Our experiments on the
Canadian Hansards show that our unsupervised technique is sig-
nificantly more effective than picking seeds by hand (Yarowsky,
1995), which in turn is known to rival supervised methods.
1 Introduction
Some of NLP?s most interesting problems have to do with
unsupervised learning. Human language learners are able
to discover word senses, grammatical genders, morpho-
logical systems, grammars, discourse registers, and so
forth. One would like to build systems that discover the
same linguistic patterns in raw text. For that matter, one
would also like to discover patterns in bilingual text (for
translation), in document collections (for categorization
and retrieval), and in other data that fall outside the scope
of humans? language learning.
There are relatively few successful methods for fully
unsupervised learning from raw text. For example,
the EM algorithm (Dempster et al, 1977) extracts the
?wrong? patterns or gets stuck in local maxima.
One of the most promising avenues in recent years has
been the use of ?minimally supervised? methods. Such
methods are initialized with some sort of ?seed? that
grows into a full classifier (or generative model). We
say that a seed is ?fertile? if it grows into a classifier (or
model) that performs well on some desired criterion.
Ordinarily, it is up to a human to choose a seed that
he or she intuitively expects to be fertile. While this may
be easy when building a single classifier, it is prohibitive
when building many classifiers. For example, we may
wish to build
? word-sense classifiers for all words of a language (e.g.,
to get sharper lexical translation probabilities in a ma-
chine translation system)
? named-entity extractors for many languages
? new clusters or classifiers every day (for an evolving
document collection)
?We thank David Yarowsky for advice on the choice of data
and for the plant/tank dataset.
? new clusters or classifiers every minute (for the docu-
ment sets retrieved by ad hoc queries)
? many distinct classifiers that correspond to different
views of the data1
Even when building a single classifier, a human may not
know how to pick a good seed when working with an
unfamiliar language or sublanguage, or when trying to
induce less intuitive hidden variables, such as grammar
rules or fine-grained senses. And there is no reason to
expect humans to have good intuitions about seeds for
mining non-linguistic data such as consumer purchasing
records.
This paper considers how to remove this last element
of supervision. Our idea is to guess a number of plausi-
ble seeds, build a classifier for each one, and then try to
determine which of the seeds have grown successfully.
For example, to discover the two senses of the En-
glish word drug, we grow 200 classifiers (from different
seeds) that attempt to partition instances of drug into two
classes. We have no direct supervision about which of
the resulting partitions corresponds to the true sense dis-
tinction. Instead, we rely on clues that tend to signal that
a seed was fertile and led to a good partition. The clues
are not specific to the word drug, but they may have been
demonstrated to be good clues in general for successfully
grown word sense disambiguators.
Demonstrated how? If we consider more than one clue,
we may need some data to learn which clues to trust, and
their relative weights. Our method is unsupervised in the
conventional sense, as it obtains a classifier for drug with
no supervision about drug. However, to learn what good
classifiers generally look like2 for this task, we first use
1A word token or document can be characterized by a 20-bit
vector, corresponding to its classifications by 20 different binary
classifiers. These vectors are detailed abstract representations of
the words or documents. They can be clustered, or all their bits
can be included as potentially relevant features in another task.
2Ando and Zhang (2005) independently used this phrase, for
a semi-supervised, cross-task learner that differs from our unsu-
pervised, cross-instance learner. Both their work and ours try
to transfer knowledge to a target problem from many artificial
supervised ?auxiliary problems,? which are generated from un-
labeled data (e.g., our pseudoword disambiguation problems).
However, in their ?structural learning,? the target problem is
supervised (if inadequately), and the auxiliary problems (super-
vised instances of a different task) are a source of useful hidden
features for the classifier. In our ?strapping,? the target task is
unsupervised, and the auxiliary problems (supervised instances
395
supervised data for a few other ambiguous words?or
ambiguous pseudowords, a kind of artificial data where
supervision comes for free. This supervision?s effect on
drug might be called cross-instance learning.
To take another metaphor, minimally supervised learn-
ing is often called ?bootstrapping.? Our goal is to allow a
method to pull itself up by its own bootstraps3 even when
it has none. It places its stocking feet in anything handy,
pulls on what it hopes to be sturdy straps, and checks to
see how high it got.
We dub this family of methods ?bootstrapping without
the boot,? or ?strapping? for short. The name is meant
to evoke ?bagging? and ?boosting??other methods that
train and combine multiple classifiers of the same form.
However, we are careful to point out that strapping, un-
like those theoretically motivated methods, is an unsuper-
vised learning technique (in the sense explained above).
The clusters or other hidden variables extracted by the
winning classifier may or may not be the ones that one
had hoped to find. Designing a strapping algorithm for a
particular task requires more art than designing a super-
vised learner: one must invent not only appropriate fea-
tures for classifying the data, but also appropriate clues
for identifying ?successful? classifiers.
2 Bootstrapping
To show where strapping might be useful, we briefly re-
view a range of successful bootstrapping work. We con-
sider different tasks. Given an instance of the task and a
seed s for that instance, one bootstraps a classifier Cs that
can classify examples of the task instance.
2.1 The Yarowsky algorithm
Yarowsky (1995) sparked considerable interest in boot-
strapping with his successful method for word sense dis-
ambiguation. An instance of this task involves a homony-
mous word such as drug. A seed for the instance is a pair
of words that are strongly associated, respectively, with
the two senses of drug, such as (trafficking, therapy). An
example is a token of drug.
For our purposes, a bootstrapping method can be re-
garded almost as a black box. However, we review
the details of the Yarowsky algorithm to illustrate how
bootstrapping is possible, and why some seeds are bet-
ter than others. We will use these intuitions later in de-
signing a method to strap the Yarowsky algorithm on a
of the same task) are a source of clues for a meta-classifier that
chooses among classifiers grown from different seeds. In short,
their auxiliary problems help train the target classifier directly,
while ours help train only a simple meta-classifier that chooses
among many unsupervised target classifiers. We use far fewer
auxiliary problems but ours must be instances of the target task.
3The reference is to Baron Munchausen, a fictional 18th-
century adventurer who rescued himself from a pit in this way.
It is distinct from the ?bootstrap? in non-parametric statistics.
new instance?i.e., a method for automatically choosing
seeds that discover a true sense distinction.
A learned classifier for the instance drug is an ordered
decision list of contextual features (such as the presence
of dealer nearby) that strongly indicate one or the other
sense of drug. Given a sample token of drug, the classi-
fier picks a sense according to the single highest-ranked
feature that is present in the token?s context.
To bootstrap a decision-list classifier from a seed,
Yarowsky starts with all examples of drug that can be
classified by using the seed words as the only features.
These few examples are used as supervised data to train
a longer decision list, which includes the seed words and
any other features that suffice to distinguish these exam-
ples with high confidence. This longer decision list can
now classify further examples, which are used to train a
new and even longer decision list, and so on.
Yarowsky?s method works if it can maintain high ac-
curacy as it gradually increases its coverage. A precise
classifier at iteration t tends to accurately classify new
examples. This tends to produce a still-accurate classifier
with greater coverage at iteration t + 1.
The method fails if the initial classifier is inaccurate
(i.e., if the two seed words do not accurately pick out ex-
amples of the two senses). It may also fail if at some
point, by bad luck on sparse data, the process learns some
inappropriate features. If the classifier at iteration t is
sufficiently polluted by bad features, the classifier at iter-
ation t + 1 will start trying to distinguish examples that
do not correspond to different senses, which may lead to
even worse classifiers on subsequent iterations. However,
some alternative seed may have escaped this bad luck by
sprouting a different set of examples.
2.2 A Few Other Applications of Bootstrapping
Inspired by Yarowsky, Blum and Mitchell (1998) built a
classifier for the task of web page classification.4 They
considered only one instance of this task, namely distin-
guishing course home pages from other web pages at a
computer science department. Their seed consisted of 3
positive and 9 negative examples. Strapping a web page
classifier would mean identifying seeds that lead to other
?natural classes? of web pages. Strapping may be useful
for unsupervised text categorization in general.
Riloff et al (2003) learned lists of subjective nouns
in English, seeding their method with 20 high-frequency,
strongly subjective words. This seed set was chosen man-
ually from an automatically generated list of 850 can-
4More precisely, they bootstrapped two Naive Bayes
classifiers?one that looked at page content and the other that
looked at links to the page. This ?co-training? approach has be-
come popular. It was also used by the Cucerzan and Yarowsky
papers below, which looked at ?internal? and ?external? features
of a phrase.
396
didate words. Strapping their method would identify
subjective nouns in other languages, or other ?natural
classes? of English words.
Query expansion in IR searches for more documents
?similar to? a designated relevant document. This prob-
lem too might be regarded as searching for a natural
class?a small subset of documents that share some prop-
erty of the original document?and approached using it-
erative bootstrapping. The seed would specify the origi-
nal document plus one or two additional words or docu-
ments initially associated with the ?relevant? and/or ?ir-
relevant? classes. Strapping would guess various differ-
ent seeds that extended the original document, then try to
determine which seeds found a cohesive ?relevant set.?
Collins and Singer (1999) bootstrapped a system for
classifying phrases in context. Again, they considered
only one instance of this task: classifying English proper
names as persons, organizations, or locations. Their seed
consisted of 7 simple rules (?that New York, California,
and U.S. are locations; that any name containing Incor-
porated is an organization; and that I.B.M. and Microsoft
are organizations?). Strapping such a classifier would au-
tomatically discover named-entity classes in a different
language, or other phrase classes in English.
Cucerzan and Yarowsky (1999) built a similar system
that identified proper names as well as classifying them.
Their seed consisted of a list of 40 to 300 names. Large
seeds were not necessary for precision but did help recall.
Cucerzan and Yarowsky (2003) classified masculine
vs. feminine nouns. They experimented with several task
instances, namely different Indo-European languages. In
each instance, their seed consisted of up to 30 feminine
and 30 masculine words (e.g., girl, princess, father).
Many more papers along these lines could be listed. A
rather different task is grammar induction, where a task
instance is a corpus of text in some language, and the
learned classifier is a parser. Following Chomsky (1981),
we suggest that it may be possible to seed a grammar
induction method with a small number of facts about the
word order of the language: the basic clause order (SVO,
SOV, etc.), whether pronominal subjects may be omitted
(Chomsky?s ?pro-drop? parameter), etc. These facts can
for example be used to construct a starting point for the
inside-outside algorithm (Baker, 1979), which like other
EM algorithms is highly sensitive to starting point. In a
strapping method, one would guess a number of different
seeds and evaluate the learned grammars on likelihood,
entropy (Wang et al, 2002), correlation with semantics,
or plausibility on other linguistic grounds that were not
considered by the likelihood or the prior.
3 Strapping
Given a seed s for some task instance, let Cs denote the
classifier grown from s. Let f(s) denote the true fertility
of a seed s, i.e., the performance of Cs measured against
some set of correct answers for this instance. In gen-
eral, we do not know the correct answers and hence do
not know f(s). That is why we are doing unsupervised
learning.
Strapping relies on two estimates of f(s). Let g(s) be
a quick estimate that considers only superficial features
of the seed s. h(s) is a more careful estimate that can be
computed once Cs has been grown.
The basic method for strapping a classifier for a new
task instance is very simple:
1. Quickly select a set S of candidate seeds such that
g(s) is high.
2. For each seed s ? S, learn a classifier Cs and mea-
sure h(s).
3. Choose the seed s? ? S that maximizes h(s?).
4. Return Cs?.
Variants on this method are obviously possible. For
example, instead of returning a single classifier Cs?, one
might use classifier combination to combine several clas-
sifiers Cs that have high h(s).
It is clearly important that g and h be good estimates
of f . Can data help us design g and h? Unfortunately,
f is not known in an unsupervised setting. However, if
one can get a few supervised instances of the same task,
then one can select g and h so g(s) and h(s) approximate
f(s) for various seeds s for those instances, where f(s)
can be measured directly. The same g and h can then be
used for unsupervised learning on all new task instances.
3.1 Selecting Candidate Seeds
The first step in strapping a classifier is to select a set S
of seeds to try. For strapping to work, it is crucial that
this set contain a fertile seed. How can this be arranged?
Different strategies are appropriate for different problems
and bootstrapping methods.
? Sometimes a simple heuristic g(s) can help identify
plausibly fertile seeds, as in the pseudocode above. In
strapping the Yarowsky algorithm, we hope to find seeds
s = (x, y) such that x and y are strongly associated
with different senses of the ambiguous target word. We
choose s = (x, y) such that x and y were never ob-
served in the same sentence, but each of x and y has
high pointwise mutual information with the ambiguous
target word and appeared with it at least 5 times.
? If the space of possible seeds is small, it may be pos-
sible to try many or all of them. In grammar induction,
for example, perhaps seeding with a few basic word or-
der facts is enough. There are not so many basic word
orders to try.
397
? Some methods have many fertile seeds?so many that
a small random sample (perhaps filtered by g(s)) is
likely to include at least one. We rely on this for
the Yarowsky algorithm. If the target word is a true
homonym, there exist many words x associated strongly
with the first sense, and many words y associated
strongly with the second sense. It is not difficult to stum-
ble into a fertile seed s = (x, y), just as it is not difficult
for a human to think of one.5
? If fertile seeds are few and far between, one could
abandon the use of a candidate set S selected by g(s),
and directly use general-purpose search methods to look
for a seed whose predicted fertility h(s) is high.
For example, one could use genetic algorithms to
breed a population of seeds with high h(s). Or
after evaluating several candidate seeds to obtain
h(s1), h(s2), . . . h(sk), one could perform a regression
analysis that predicts h(s) from superficial features of
s, and use this regression function (a kind of g(s) that is
specific to the task instance) to pick sk+1.
Strapping may be harder in cases like gender induc-
tion: it is hard to stumble into the kind of detailed seed
used by Cucerzan and Yarowsky (2003). However, we
suspect that fertile seeds exist that are much smaller than
their lists of 50?60 words. While their large hand-crafted
seed is sure to work, a handful of small seeds (each
consisting of a few supposedly masculine and feminine
words) might be likely to contain at least one that is fer-
tile.6 That would be sufficient, assuming we have a way
to guess which seed in the handful is most fertile. That
issue is at the core of strapping, and we now turn to it.
3.2 Clues for Evaluating Bootstrapped Classifiers
Once we have identified a candidate seed s and built the
classifier Cs, we must evaluate whether Cs ?looks like?
the kind of classifier that tends to do well on our task.
This evaluation function h(s) is task-specific. It may
consider features of Cs, the growth trajectory of Cs, or
the relation between Cs and other classifiers.
For concretness, we consider the Yarowsky method for
word-sense disambiguation (WSD). How can we tell if a
seed s = (x, y) was fertile, without using even a small
validation set to judge Cs? There are several types of
5Alignment methods in machine translation rely even more
heavily on this property. While they begin with a small trans-
lation lexicon, they are sufficiently robust to the choice of this
initial seed (lexicon) that it suffices to construct a single seed by
crude automatic means (Brown et al, 1990; Melamed, 1997).
Human supervision (or strapping) is unnecessary.
6This is particularly likely if one favors function words (in
particular determiners and pronouns), which are strong indica-
tors of gender. Cucerzan and Yarowsky used only content words
because they could be extracted from bilingual dictionaries.
clues to fertility, which may be combined into a meta-
classifier that identifies fertile seeds.
Judge the result of classification with Cs: Even with-
out a validation set, the result of running Cs on the train-
ing corpus can be validated in various ways, using inde-
pendent plausibility criteria that were not considered by
the bootstrapping learner.
? Is the classification reasonably balanced? (If virtu-
ally all examples of the target word are labeled with
the same sense, then Cs has not found a sense dis-
tinction.)
? When a document contains multiple tokens of the
target word, are all examples labeled with the same
sense? This property tends to hold for correct clas-
sifiers (Gale et al, 1992a), at least for homonyms.
? True word senses usually correlate with document
or passage topic. Thus, choose a measure of simi-
larity between documents (e.g., the cosine measure
in TF/IDF space). Does the target word tend to
have the same sense in a document and in its nearby
neighbors?
? True word senses may also improve performance on
some task. Is the perplexity of a language model
much reduced by knowing whether sense x or sense
y (according to Cs) appeared in the current con-
text? (This relates to the previous point.) Likewise,
given a small bilingual text that has been automati-
cally (and perhaps poorly) word-aligned, is it easier
to predict how the target word will translate when
we know its sense (according to Cs)?
Judge the internal structure of Cs: Does Cs look
like a typical supervised decision list for word-sense dis-
ambiguation? For instance, does it contain many features
with high log-likelihood ratios? (If a true sense distinc-
tion was discovered, we would expect many contextual
features to correlate strongly with the predicted sense.)
Look at the process whereby Cs was learned: Does
the bootstrapping run that starts from s look like a typical
bootstrapping run from a fertile seed? For example, did
it rapidly add many new examples with high confidence?
Once new examples were classified, did their classifica-
tions remain stable rather than switching back and forth?
Judge the robustness of learning with seed s: Train
several versions of Cs, as in ensemble methods (but un-
supervised), by restricting each to a random subset of the
data, or a subset of the available features. Do these ver-
sions tend to agree on how to classify the data? If not,
seed s does not reliably find true (or even false) classes.
Judge the agreement of Cs with other classifiers:
Are there several other classifiers Cs? that agree strongly
with Cs on examples that they both classify? If the sense
398
distinction is real, then many different seeds should be
able to find it.
3.3 Training the Evaluation Function h(s)
Many of the above clues are necessary but not sufficient.
For example, a learned classification may be robust with-
out being a sense distinction. We therefore define h(s)
from a combination of several clues.
In general, h(s) is a classifier or regression function
that attempts to distinguish fertile from infertile seeds,
given the clues. As mentioned earlier, we train its free
parameters (e.g., coefficients for linear regression) on a
few supervised instances of the task. These supervised
instances allow us to measure the fertility f(s) of various
seeds, and thus to model the behavior of fertile versus
infertile seeds. The presumption is that these behavior
patterns will generalize to new seeds.
3.4 Training h(s) on Artificial Data
Optionally, to avoid the need for any human annotation at
all, the supervised task instances used to train h(s) may
be artificial instances, whose correct classifications are
known without annotation.
In the case of word-sense disambiguation, one can au-
tomatically construct ambiguous pseudowords (Gale et
al., 1992c; Schu?tze, 1998) by replacing all occurences of
two words or phrases with their conflation. For example,
banana and wine are replaced everywhere by banana-
wine. The original, unconflated text serves as a super-
vised answer key for the artificial task of disambiguating
banana-wine.
Traditionally, pseudowords are used as cheap test data
to evaluate a disambiguation system. Our idea is to use
them as cheap development data to tune a system. In
our case, they tune a few free parameters of h(s), which
says what a good classifier for this task looks like. Pseu-
dowords should be plausible instances of the task (Gaus-
tad, 2001; Nakov and Hearst, 2003): so it is deliberate
that banana and wine share syntactic and semantic fea-
tures, as senses of real ambiguous words often do.
Cheap ?pseudo-supervised? data are also available in
some other strapping settings. For grammar induction,
one could construct an artificial probabilistic grammar at
random, and generate text from it. The task of recovering
the grammar from the text then has a known answer.
4 Experiments
4.1 Unsupervised Training/Test Data
Our experiments focused on the original Yarowsky algo-
rithm. We attempted to strap word-sense classifiers, us-
ing English data only, for English words whose French
translations are ambiguous. This has obvious benefits for
training an English-to-French MT system: separate pa-
rameters can be learned for the two senses of drug.7
Gale et al (1992b) identified six such words in the
Canadian Hansards, a parallel sentence-aligned corpus of
parliamentary debate in English and French: drug, duty,
land, language, position, sentence. We extracted all ex-
amples of each word from the 14-million-word English
portion of the Hansards.8 Note that this is considerably
smaller than Yarowsky?s (1995) corpus of 460 million
words, so bootstrapping will not perform as well, and
may be more sensitive to the choice of seed.
Because we are doing unsupervised learning, we both
trained and tested these 6 words on the English Hansards.
We used the French portion of the Hansards only to create
a gold standard for evaluating our results.9 If an English
sentence containing drug is paired with a French sentence
that contains exactly one of me?dicament or drogue, we
take that as an infallible indicator of its sense.
4.2 Comparing Classifiers
Suppose binary classifier 1 assigns class ?+? to a of n
examples; binary classifier 2 assigns class ?+? to b of the
same n examples. Let e be the number of examples where
the classifiers agree (both ?+? or both ???).
An unsupervised classifier?s polarity is arbitrary: clas-
sifier 1?s ?+? may correspond to classifier 2?s ???. So we
define the overlap as E = max(e, n ? e), to reflect the
best polarity.
To evaluate a learned classifier, we measure its over-
lap with the true classification. The statistical signifi-
cance is the probability that this level of overlap would
be reached by chance under independent classifications
given the values a, b, n:
p =
?
max(a+b?n,0) ? c ? ?(a+b?E)/2?
or
?(a+b?(n?E))/2? ? c ? min(a,b)
?
a
c
? ?
n ? a
b ? c
?
/
?
n
b
?
Also, we can measure the agreement between any two
learned classifiers as ?(log p)/n. Note that a classifier
that strongly favors one sense will have low agreement
with other classifiers.
7To hedge against the possibility of misclassification, one
could interpolate with non-sense-specific parameters.
8We are not certain that our version of the Hansards is iden-
tical to that in (Gale et al, 1992b).
9By contrast, Gale et al (1992b) used the French portion as
a source of training supervision. By contrast, we will assume
that we do not have a large bilingual text such as the Hansards.
We train only on the English portion of the Hansards, ignoring
the French. This mimics the situation where we must construct
an MT system with very little bilingual text. By first discov-
ering word senses in unsupervised monolingual data (for either
language), we can avoid incorrectly mixing up two senses of
drug in our translation model.
399
4.3 Generating Candidate Seeds (via g(s))
For each target word t, we chose candidate seeds s =
(x, y) with a high score g(s), where g(s) = MI(t, x) +
MI(t, y), provided that c(x, y) = 0 and c(t, x) ? 5 and
c(t, y) ? 5 and 1/9 < c(t, x)/c(t, y) < 9.10
The set S of 200 seeds for t was constructed by repeat-
edly adding the top-scoring unused seed to S, except that
to increase the variety of words, we disallowed a seed
s = (x, y) if x or y already appeared 60 times in S.
4.4 Hand-Picked Seeds
To compare, we chose two seeds by hand for each t.
The casually hand-picked seed was chosen by intuition
from the list of 200 automatically generated seeds. This
took about 2 minutes (per seed).
The carefully hand-picked seed was not limited to this
list, and took up to 10 minutes to choose, in a data-guided
fashion. We first looked at some supervised example sen-
tences to understand the desired translational sense dis-
tinction, and then for each sense chose the highest-MI
word that both met some stringent subjective criteria and
appeared to retrieve an appropriate initial set of examples.
4.5 The Bootstrapping Classifier
Our approximate replication of Yarowsky?s algorithm
used only a small set of features:
? Original and lemmatized form of the word immedi-
ately preceding the target word t.
? Original and lemmatized form of the word immedi-
ately following t.
? Original and lemmatized form of the content words
that appear in the same sentence as t.
We used the seed to provisionally classify any token of
the target word that appeared in a sentence with exactly
one of the two seed words. This formed our initial ?train-
ing set? of disambiguated tokens. At each iteration of the
algorithm, we trained a decision list on the current train-
ing set. We then used the decision list to reclassify all k
tokens in the current training set, and also to augment the
training set by classifying the additional max(50, k/10)
tokens on which the decision list was most confident.11
10c(x, y) counts the sentences containing both x and y. MI(t,
x) = log c(t, x)c()/c(t)c(x) is pointwise mutual information.
11Such a token has some feature with high log-likelihood ra-
tio, i.e., it strongly indicates one of the senses in the current
training set. We smoothed using the method of (Yarowsky,
1996): when a feature has been observed with only one sense,
its log-likelihood ratio is estimated as a linear function of the
number of occurrences of the seen sense. Function words are
smoothed with a different linear coefficient than content words,
in order to discount their importance. We borrowed the ac-
tual coefficients from (Yarowsky, 1996), though we could have
learned them.
4.6 Development Data (for tuning h(s))
Before turning to the unsupervised Hansards, we tuned
our fertility estimator h(s) to identify good seeds on de-
velopment data?i.e., on other, supervised task instances.
In the supervised condition, we used just 2 additional
task instances, plant and tank, each with 4000 hand-
annotated instances drawn from a large balanced corpus
(Yarowsky, 1995).
In the pseudo-supervised condition, we used no hand-
annotated data, instead constructing 10 artificial super-
vised task instances (section 3.4) from the English por-
tion of the Hansards. To facilitate cross-instance learn-
ing, we tried to construct these pseudowords to behave
something like our ambiguous test words.12 Given a test
word t, we randomly selected a seed (x, y) from its candi-
date list (section 4.3), excluding any that contained func-
tion words.13 Our basic idea was to conflate x and y
into a pseudoword x-y. However, to get a pseudoword
with only two senses, we tried to focus on the particular
senses of x and y that were selected by t. We constructed
about 500 pseudoword tokens by using only x and y to-
kens that appeared in sentences that contained t, or in
sentences resembling those under a TF-IDF measure. We
repeated this process twice per test word to obtain 12
pseudowords. We then discarded the 2 pseudowords for
which no seed beat baseline performance, reasoning that
they were ill-chosen and unlike real ambiguous words.14
4.7 Clues to Fertility
For each seed s for each development or test target word,
we measured a few clues h1(s), h2(s) . . . h6(s) that we
hoped might correlate with fertility. (In future work, we
plan to investigate more clues inspired by section 3.2.)
? The agreeability of Cs with (some of) the other 199
classifiers:
?
?
1
199
?
s? 6=s
agr(Cs, Cs?)?
?
?
1/?
The agreement agr(Cs, Cs? ) was defined in section 4.2.
We tried 4 values for ? (namely 1, 2, 5, 10), each result-
ing in a different feature.
12We used collocates of t. Perhaps better yet would be words
that are distributionally similar to t (appear in same contexts).
Such words tend to be syntactically and semantically like t.
13For an unknown language or domain, a lexicon of function
words could be constructed automatically (Katz, 1996).
14Thus we discarded alcohol-trafficking and addicts-alcohol;
note that these were indeed ill-chosen (difficult) since both
words unluckily corresponded to the same sense of drug.
This left us with bound-constituents, customs-pray, claims-
value, claims-veterans, culture-unparliamentary, english-learn,
competitive-party, financial-party, death-quote, death-page.
400
? The robustness of the seed, defined by the agreement
of Cs with 10 variant classifiers C(k)s that were trained
with the same seed but under different conditions:
1
10
10
?
k=1
agr(Cs, C(k)s )
We simply trained each classifier C(k)s on a random sub-
set of the n test examples, chosen by sampling n times
with replacement.15
? The confidence of Cs on its own training data: its av-
erage confidence over the n training tokens, minus the
classifier skew.
The decision list?s confidence on a token is the log-
likelihood ratio of the single feature used to classify that
token. It has the form | log(c/d)| (perhaps smoothed)
and was previously used to select data while bootstrap-
ping Cs. Subtracting the skew, | log(a/(n?a))|,16 gives
a measurement? 0. It corrects for confidence that arises
from the classifier?s overall bias, leaving only the added
value of the relevant contextual feature.
4.8 Tuning h(s) and Strapping New Classifiers
For each of the 2 words or 10 pseudowords t in our de-
velopment set (see section 4.6), we ranked its 200 seeds
s by their true fertility f(s). We then ran support vec-
tor regression17 to learn a single linear function, h(s) =
~w ? (clue vector for Cs), that predicts the fertilities of all
2 ? 200 or 10 ? 200 seeds.18
Then, for each of our 6 Hansards test instances (sec-
tion 4.1), we used h(s) to pick the top-ranked of 200
seeds.19 It took about 3 hours total to strap classifiers for
all 6 instances, using about 40 machines and unoptimized
Perl code on the 14-million-word Hansards. For each
of the 6 instances, this involved selecting 200 candidate
15We eliminated duplicates, perhaps unfortunately.
16As before, a and n ? a are the numbers of tokens that Cs
classifies as ?+? and ??? respectively. Thus the skew is the log-
likelihood ratio of the decision list?s ?baseline? feature.
17We used cross-validation among the 10 development pseu-
dowords to choose the options to SVMlight (Joachims, 1999): a
linear kernel, a regularization parameter of 0.3, and a dependent
variable of 10f(s) ? [1, 10] rather than f(s) ? [0, 1], which
placed somewhat more emphasis on modeling the better seeds.
Our development objective function was the average over the 10
pseudowords of the Spearman rank-order correlation between
h(s) and f(s).
18We augmented the clue vector with binary clues of the form
t = plant, t = tank, etc. The regression weight of such a clue
is a learned bias term that models the inherent difficulty of the
task instance t (which varies greatly by t). This allows the other
regression features to focus on the quality of the seed given t.
19We do not have a clue t = . . . for this test instance. The re-
sulting lack of a bias term may subtract a constant from the pre-
dicted fertilities?but that does not affect the ranking of seeds.
seeds, bootstrapping 11 classifiers Cs, C(1)s , . . . C(10)s
from each seed, and choosing a particular Cs to return.
4.9 Results
Our results are in Table 1. On both development and test
instances of the task, g(s) proposed seeds with a good
range of fertilities. The correlation of predicted with ac-
tual fertility on test data averaged an outstanding 85%.
Despite having no knowledge of the desired senses,
strapping significantly beat human selection in all 24 of
the possible comparisons between a hand-picked seed
(casual or careful) and a strapped seed (chosen by an h(s)
tuned on supervised or pseudo-supervised instances).
The h(s) tuned on annotated plant/tank actually chose
the very best of the 200 seeds in 4 of the 6 instances. The
h(s) tuned on artificial pseudowords did nearly as well,
in 2 of 6 instances identifying the very best seed, and in
5 of 6 instances ranking it among its top 3 choices.
We conclude that our unsupervised clues to fertility ac-
tually work. Furthermore, combining clues via regres-
sion was wise, as it tended to work better than any single
clue. Somewhat better regression weights for the WSD
task were learned from 2 out-of-domain hand-annotated
words than from 10 in-domain artificial pseudowords.
5 Open Questions
The work reported here raises many interesting questions
for future research.
In the WSD task, we have only considered word types
with two unrelated senses (homonyms). A more general
problem is to determine when a word type is ambiguous
at all, and if so, how many coarse-grained or fine-grained
senses it has. Strapping seems naturally suited to this
problem, since it aims to discover when a sense distinc-
tion grown from some seed is a true sense distinction.
Then we would like to know how well strapping gen-
eralizes to additional bootstrapping scenarios. Our WSD
strapping experiments were successful using only a sub-
set of the techniques proposed in section 3. Generalizing
to other tasks may require other techniques for selecting
and evaluating candidate seeds, and perhaps combining
the resulting classifiers.
An interesting question is whether strapping can be
used in an active learning context. Active learning is a
kind of bootstrapping method that periodically requires
new seeds: it turns to the user whenever it gets confused.
Perhaps some of these seeds can be guessed nondetermin-
istically and the guesses evaluated automatically, with or
without user confirmation.
Finally, there may be theoretical guarantees about
strapping when something is known about the data.
When h(s) is trained to estimate f(s) well on some su-
pervised instances, there may be guarantees about how
strapping will perform on unsupervised instances drawn
401
st
ra
pp
in
g
(un
su
pe
rv
ise
d)
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
drug duty land language position sentence
baseline / # examples 51.2 / 371 70.1 / 633 76.6 / 1379 87.5 / 1012 81.7 / 2949 50.8 / 501
worst seed (of 200) 50.1 (200) traffickers trafficking 50.0 (200) 50.1 (200) claims farming 50.3 (200) 56.1 (200) 50.1 (200) length life
casually selected (from 200) 56.5 (87) food trafficking 73.4? (40) 76.2 (24) farm veterans 86.4 (76) 81.7 (41) 80.6? (40) page prison
carefully constructed 62.1? (75) alcohol costs 82.1? (8.5) 76.6 (20) farm strong 87.9 (25.5) 81.4 (56.5) 86.8? (27) death quote
best/oracle seed (of 200) 76.1?? (1) alcohol medical 86.2?? (1) 81.3?? (1) acres courts 90.9?? (1) 88.3?? (1) 89.9?? (1) reads served
most agreeable seed (?=1) 72.6?? (5) abuse information 64.7 (47) 67.5 (36) claims production 86.4 (79) 82.4 (36) 88.7?? (10) life quote
most robust seed 76.1?? (1) alcohol medical 86.2?? (1) 71.7 (29) claims price 85.6 (93) 82.7 (21) 88.8?? (9) commuted next
most confident seed 66.9? (32) trafficking used 72.1? (42) 77.9?? (3) claims courts 89.8?? (10) 84.4?? (8) 89.9?? (1) reads served
h(s)-picked (plant/tank) 76.1?? (1) alcohol medical 86.2?? (1) 81.3?? (1) acres courts 90.3?? (7) 84.5?? (7) 89.9?? (1) reads served
h(s)-picked (10 pseudowd) 70.4?? (10) alcohol found 86.2?? (1) 78.9?? (2) children farm 89.7?? (17) 83.7?? (16) 89.9?? (1) reads served
h(s)-picked, 2nd place 69.1? (13) alcohol related 85.7?? (2) 77.8?? (4) aboriginal acres 90.9?? (1) 82.8 (19) 89.0?? (7) prison quote
h(s)-picked, 3rd place 76.1?? (1) alcohol medical 84.2? (4) 77.1?? (5) acres cities 87.5 (28) 88.3?? (1) 88.6?? (15) life reads
h(s) rank of oracle seed 3 1 14 2 3 1
Spearman rank-order corr. 0.863 0.905 0.718 0.825 0.842 0.937
Table 1: [See section 4.9 for highlights.] Accuracy (as percentage) and rank (in parentheses) of bootstrapped classifiers for variously
chosen seeds, some of which are shown. * denotes statistically significant agreement with the truth (section 4.2, p < 0.01).
? denotes a seed having significantly better agreement with the truth than does the better of the hand-picked seeds (McNemar?s test,
p < 0.03). In each column, the best performance for an automatic or manual seed appears in boldface. The ?most . . . ? lines use no
tuning, the ?plant/tank? line tunes h(s) on 2 supervised instances, and the subsequent lines tune h(s) on 10 pseudoword instances.
The last line gives the Spearman rank-order correlation between seeds? predicted fertilities h(s) and their actual fertilities f(s).
from the same source (cross-instance learning). Even in
the fully unsupervised case, it may be possible to prove
that if the data were generated from a particular kind of
process (e.g., a Gaussian mixture), then a certain strap-
ping algorithm can recover the hidden variables.
6 Conclusions
In this paper, we showed that it is sometimes possible?
indeed, preferable?to eliminate the initial bit of supervi-
sion in ?bootstrapping? algorithms such as the Yarowsky
(1995) algorithm for word sense disambiguation. Our
?strapping? approach tries many candidate seeds as start-
ing points and evaluates them automatically. The eval-
uation function can be tuned if desired on other task in-
stances, perhaps artificially constructed ones. It can then
be used wherever human guidance is impractical.
We applied the method to unsupervised disambigua-
tion of English words in the Canadian Hansards, as if for
English-French translation. Our results (see section 4.9
for several highlights) show that our automatic ?strapped?
classifiers consistently outperform the classifiers boot-
strapped from manually, knowledgeably chosen seeds.
References
R. K. Ando and T. Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In ACL.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Jared J. Wolf and Dennis H. Klatt, editors, Speech
Communication Papers Presented at the 97th meeting of the
Acoustical Society of America, MIT, Cambridge, MA, June.
A. Blum and Tom Mitchell. 1998. Combining labeled and un-
labeled data with co-training. In Proc. of COLT, July.
P. F. Brown, J. Cook, S.A. Della Pietra, V.G. Della Pietra, F. Je-
linek, J.D. Lafferty, R.L. Mercer, and P.S. Roossin. 1990. A
statistical approach to machine translation. CL, 16(2).
N. Chomsky. 1981. Lectures on Government and Binding.
Foris, Dordrecht.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proc. of EMNLP/VLC.
S. Cucerzan and D. Yarowsky. 1999. Language independent
named entity recognition combining morphological and con-
textual evidence. In Proc. of EMNLP/VLC.
S. Cucerzan and D. Yarowsky. 2003. Minimally supervised
induction of grammatical gender. In Proc. of HLT/NAACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum
likelihood from incomplete data via the EM algorithm. J.
Royal Statist. Soc. Ser. B, 39(1):1?38.
W. A. Gale, K. W. Church, and D. Yarowsky. 1992a. One sense
per discourse. In Proc. of the 4th DARPA Speech and Natural
Language Workshop, pages 233?237.
W. A. Gale, K. W. Church, and D. Yarowsky. 1992b. Us-
ing bilingual materials to develop word sense disambiguation
methods. In Proc. of the 4th International Conf. on Theoret-
ical and Methodological Issues in Machine Translation.
W. A. Gale, K. W. Church, and D. Yarowsky. 1992c. Work on
statistical methods for word sense disambiguation. In Work-
ing Notes of the AAAI Fall Symposium on Probabilistic Ap-
proaches to Natural Language, pages 54?60.
T. Gaustad. 2001. Statistical corpus-based word sense disam-
biguation: Pseudowords vs. real ambiguous words. In Proc.
of ACL-EACL.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods?Support Vector Learning. MIT Press.
S. M. Katz. 1996. Distribution of context words and phrases in
text and language modelling. NLE, 2(1):15?59.
I. Dan Melamed. 1997. A word-to-word model of translational
equivalence. In Proc. of ACL/EACL, page 490.
P. Nakov and M. Hearst. 2003. Category-based pseudowords.
In HLT-NAACL?03, pages 67?69, Edmonton, Canada.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning subjec-
tive nouns using extraction pattern bootstrapping. In Proc.
of CoNLL, pages 25?32, May?June.
H. Schu?tze. 1998. Automatic word sense discrimination. Com-
putational Linguistics, 23.
S. Wang, R. Rosenfeld, Y. Zhao, and D. Schuurmans. 2002.
The latent maximum entropy principle. In Proc. of ISIT.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. of ACL.
D. Yarowsky. 1996. Three Machine Learning Algorithms for
Lexical Ambiguity Resolution. Ph.D. thesis, U. of Penn.
402
Simpler and More General Minimization
for Weighted Finite-State Automata
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD, USA 21218-2691
jason@cs.jhu.edu
Abstract
Previous work on minimizing weighted finite-state automata
(including transducers) is limited to particular types of weights.
We present efficient new minimization algorithms that apply
much more generally, while being simpler and about as fast.
We also point out theoretical limits on minimization algo-
rithms. We characterize the kind of ?well-behaved? weight
semirings where our methods work. Outside these semirings,
minimization is not well-defined (in the sense of producing a
unique minimal automaton), and even finding the minimum
number of states is in general NP-complete and inapproximable.
1 Introduction
It is well known how to efficiently minimize a determin-
istic finite-state automaton (DFA), in the sense of con-
structing another DFA that recognizes the same language
as the original but with as few states as possible (Aho et
al., 1974). This DFA also has as few arcs as possible.
Minimization is useful for saving memory, as when
building very large automata or deploying NLP systems
on small hand-held devices. When automata are built up
through complex regular expressions, the savings from
minimization can be considerable, especially when ap-
plied at intermediate stages of the construction, since (for
example) smaller automata can be intersected faster.
Recently the computational linguistics community has
turned its attention to weighted automata that compute
interesting functions of their input strings. A traditional
automaton only returns an boolean from the set K =
{true, false}, which indicates whether it has accepted
the input. But a probabilistic automaton returns a prob-
ability in K = [0, 1], or equivalently, a negated log-
probability in K = [0,?]. A transducer returns an output
string from K = ?? (for some alphabet ?).
Celebrated algorithms by Mohri (1997; 2000) have
recently made it possible to minimize deterministic au-
tomata whose weights (outputs) are log-probabilities or
strings. These cases are of central interest in language
and speech processing.
However, automata with other kinds of weights can
also be defined. The general formulation of weighted
automata (Berstel and Reutenauer, 1988) permits any
weight set K, if appropriate operations ? and ? are pro-
vided for combining weights from the different arcs of
the automaton. The triple (K,?,?) is called a weight
semiring and will be explained below. K-valued func-
tions that can be computed by finite-state automata are
called rational functions.
How does minimization generalize to arbitrary weight
semirings? The question is of practical as well as theoret-
ical interest. Some NLP automata use the real semiring
(R,+,?), or its log equivalent, to compute unnormalized
probabilities or other scores outside the range [0, 1] (Laf-
ferty et al, 2001; Cortes et al, 2002). Expectation semir-
ings (Eisner, 2002) are used to handle bookkeeping when
training the parameters of a probabilistic transducer. A
byproduct of this paper is a minimization algorithm that
works fully with those semirings, a new result permitting
more efficient automaton processing in those situations.
Surprisingly, we will see that minimization is not
even well-defined for all weight semirings! We will
then (nearly) characterize the semirings where it is well-
defined, and give a recipe for constructing minimization
algorithms similar to Mohri?s in such semirings.
Finally, we follow this recipe to obtain a specific, sim-
ple and practical algorithm that works for all division
semirings. All the cases above either fall within this
framework or can be forced into it by adding multiplica-
tive inverses to the semiring. The new algorithm provides
arguably simpler minimization for the cases that Mohri
has already treated, and also handles additional cases.
2 Weights and Minimization
We introduce weighted automata by example. The trans-
ducer below describes a partial function from strings to
strings. It maps aab 7? xyz and bab 7? wwyz. Why?
Since the transducer is deterministic, each input (such as
aab) is accepted along at most one path; the correspond-
ing output (such as xyz) is found by concatenating the
output strings found along the path. ? denotes the empty
string.
0
1a:x
3b:     ?
2
a:y
b:zz
a:wwy
4
b:wwzzz 5
b:z
b:     ?
? and ? standardly denote the automaton?s transition and
output functions: ?(3, a) = 2 is the state reached by the
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 64-71
                                                         Proceedings of HLT-NAACL 2003
a arc from state 3, and ?(3, a) = wwy is that arc?s output.
In an automaton whose outputs (weights) were num-
bers rather than strings like wwy, concatenating them
would not be sensible; instead we would want to add or
multiply the weights along the path. In general ? denotes
the chosen operation for combining weights along a path.
The ? operation need not be commutative?indeed
concatenation is not?but it must be associative. K must
contain (necessarily unique) weights, denoted 1 and 0,
such that 1? k = k ? 1 = k and 0? k = k ? 0 = 0 for
all k ? K. An unaccepted input (e.g., aba) is assigned
the output 0. When ? is string concatenation, 1 = ?, and
0 is a special object ? defined to satisfy the axioms.
If an input such as aa were accepted along multiple
paths, we would have to use another operation ? to com-
bine those paths? weights into a single output for aa.
But that cannot happen with the deterministic automata
treated by this paper. So we omit discussion of the prop-
erties that ? should have, and do not trouble to spell out
its definition for the semirings (K,?,?) discussed in this
paper.1 We are only concerned with the monoid (K,?).
The following automaton is equivalent to the previous
one since it computes the same function:
0
1a:x
3b:ww
2
a:yz
b:zzz
a:yz
4
b:zzz 5
b:     ?
b:     ?
However, it distributes weights differently along the arcs,
and states 1 and 3 can now obviously be merged (as can 2
and 4, yielding the minimal equivalent automaton). For-
mally we know that states 1 and 3 are equivalent because
F1 = F3, where Fq denotes the suffix function of state
q?the function defined by the automaton if the start state
is taken to be q rather than 0. (Thus, F3(ab) = yz.)
Equivalent states can safely be merged, by deleting one
and rerouting its incoming arcs to the other.
We will follow Mohri?s minimization strategy:
1. Turn the first automaton above into the second. This
operation is called pushing (or quasi-determinization).
Here, for instance, it ?pushed ww back? through state 3.
2. Merge equivalent states of the second automaton, by
applying ordinary unweighted DFA minimization (Aho et
al., 1974, section 4.13) as if each weighted arc label such
as a:yz were simply a letter in a large alphabet.
3. Trim the result, removing useless states and arcs that
are not on any accepting path (defined as a path whose
weight is non-0 because it has no missing arcs and its last
state is final).
1Though appropriate definitions do exist for our examples.
For example, take the ? of two strings to be the shorter of the
two, breaking ties by a lexicographic ordering.
Mohri (2000) proves that this technique finds the minimal
automaton, which he shows to be unique up to placement
of weights along paths.2
We will only have to modify step 1, generalizing push-
ing to other semirings. Pushing makes heavy use of left
quotients: we adopt the notation k\m for an element of
K such that k ? (k\m) = m. This differs from the nota-
tion k?1?m (in which k?1 denotes an actual element of
K) because k\m need not exist nor be unique. For exam-
ple, ww\wwzzz = zzz (a fact used above) but wwy\wwzzz
does not exist since wwzzz does not begin with wwy.
If F is a function, ? is a string, and k is a weight, we
use some natural notation for functions related to F :
k ? F : (k ? F )(?)
def
= k ? (F (?))
k\F : a function (if one exists) with k ? (k\F ) = F
??1F : (??1F )(?)
def
= F (??) (standard notation)
In effect, k\F and ??1F drop output and input prefixes.
3 Pushing and Its Limitations
The intuition behind pushing is to canonicalize states?
suffix functions. This increases the chance that two states
will have the same suffix function. In the example of the
previous section, we were able to replace F3 with ww\F3
(pushing the ww backwards onto state 3?s incoming arc),
making it equal to F1 so {1, 3} could merge.
Since canonicalization was also performed at states 2
and 4, F1 and F3 ended up with identical representa-
tions: arc weights were distributed identically along cor-
responding paths from 1 and 3. Hence unweighted mini-
mization could discover that F1 = F3 and merge {1, 3}.
Mohri?s pushing strategy?we will see others?is al-
ways to extract some sort of ?maximum left factor? from
each suffix function Fq and push it backwards. That is,
he expresses Fq = k ? G for as ?large? a k ? K as
possible?a maximal common prefix?then pushes fac-
tor k back out of the suffix function so that it is counted
earlier on paths through q (i.e., before reaching q). q?s
suffix function now has canonical form G (i.e., k\Fq).
How does Mohri?s strategy reduce to practice? For
transducers, where (K,?) = (??, concat), the maxi-
mum left factor of Fq is the longest common prefix of
the strings in range(Fq).3 Thus we had range(F3) =
{wwyz, wwzzz} above with longest common prefix ww.
For the tropical semiring (R?0 ? {?},min,+), where
k\m = m ? k is defined only if k ? m, the maximum
left factor k is the minimum of range(Fq).
But ?maximum left factor? is not an obvious notion
for all semirings. If we extended the tropical semir-
2That is, any other solution is isomorphic to the one found
here if output weights are ignored.
3In general we treat Fq as a partial function, so that
range(Fq) excludes 0 (the weight of unaccepted strings). Left
factors are unaffected, as anything can divide 0.
ing with negative numbers, or substituted the semiring
(R?0,+,?), keeping the usual definition of ?maximum,?
then any function would have arbitrarily large left factors.
A more fundamentally problematic example is the
semiring Z[
?
?5]. It is defined as ({m+n
?
?5 : m,n ?
Z},+,?) where Z denotes the integers. It is a stan-
dard example of a commutative algebra in which fac-
torization is not unique. For example, 6 = 2 ? 3 =
(1 +
?
?5) ? (1 ?
?
?5) and these 4 factors cannot be
factored further. This makes it impossible to canonicalize
F2 below:
0
1
a:1
2b:1
3
c:1
4
a:3
b:(1+sqrt(-5))
a:6
b:(2+2*sqrt(-5))
a:(1-sqrt(-5))
b:2
What is the best left factor to extract from F2? We could
left-divide F2 by either 2 or 1+
?
?5. The former action
allows us to merge {1, 2} and the latter to merge {2, 3};
but we cannot have it both ways. So this automaton has
no unique minimization! The minimum of 4 states is
achieved by two distinct answers (contrast footnote 2).
It follows that known minimization techniques will not
work in general semirings, as they assume state merge-
ability to be transitive.4 In general the result of mini-
mization is not even well-defined (i.e., unique).
Of course, given a deterministic automaton M , one
may still seek an equivalent M? with as few states as pos-
sible. But we will now see that even finding the minimum
number of states is NP-complete, and inapproximable.
The NP-hardness proof [which may be skipped on a
first reading] is by reduction from Minimum Clique Par-
tition. Given a graph with vertex set V = {1, 2, . . . n}
and edge set E, we wish to partition V into as few cliques
as possible. (S ? V is a clique of the graph iff ij ? E
for all pairs i, j ? S.) Determining the minimum num-
ber of cliques is NP-complete and inapproximable: that
is, unless P=NP, we cannot even find it within a factor of
2 or 3 or any other constant factor in polynomial time.5
Given such a graph, we reduce the clique problem to
our problem. Consider the ?bitwise boolean? semiring
({0, 1}n, OR, AND). Each weight k is a string of n bits,
4A further wrinkle lies in deciding what and how to push; in
general semirings, it can be necessary to shift weights forward
as well as backward along paths. Modify the example above by
pushing a factor of 2 backwards through state 2. Making F2 =
F3 in this modified example now requires pushing 2 forward
and then 1 +
?
?5 backward through state 2.
5This problem is just the dual of Graph Coloring. For de-
tailed approximability results see (Crescenzi and Kann, 1998).
denoted k1, . . . kn. For each i ? V , define f i, ki,mi ?
K as follows: f ij = 0 iff ij ? E; kij = 1 iff i = j; mij =
0 iff either ij ? E or i = j. Now consider the following
automaton M over the alphabet ? = {a, b, c1, . . . cn}.
The states are {0, 1, . . . n, n+1}; 0 is the initial state and
n + 1 is the only final state. For each i ? V , there is an
arc 0 ci:1
n
???i and arcs i a:k
i
???(n + 1) and i b:m
i
???(n + 1).
A minimum-state automaton equivalent to M must
have a topology obtained by merging some states of V .
Other topologies that could accept the same language
(c1|c2| ? ? ? |cn)(a|b) are clearly not minimal (they can be
improved by merging final states or by trimming).
We claim that for S ? {1, 2, . . . n}, it is possible to
merge all states in S into a single state (in the automaton)
if and only if S is a clique (in the graph):
? If S is a clique, then define k,m ? K by ki = 1 iff
i ? S, and mi = 1 iff i 6? S. Observe that for every
i ? S, we have ki = f i ? k, mi = f i ? m. So by
pushing back a factor of f i at each i ? S, one can make
all i ? S share a suffix function and then merge them.
? If S is not a clique, then choose i, j ? S so that
ij 6? E. Considering only bit i, there exists no bit
pair (ki,mi) ? {0, 1}2 of which (kii,mii) = (1, 0)
and (kji ,m
j
i ) = (0, 1) are both left-multiples. So there
can exist no weight pair (k,m) of which (ki,mi) and
(kj ,mj) are both left-multiples. It is therefore not pos-
sible to equalize the suffix functions Fi and Fj by left-
dividing each of them.6 i and j cannot be merged.
Thus, the partitions of V into cliques are identical to
the partitions of V into sets of mergeable states, which are
in 1-1 correspondence with the topologies of automata
equivalent to M and derived from it by merging. There is
an N -clique partition of V iff there is an (N+2)-state au-
tomaton. It follows that finding the minimum number of
states is as hard, and as hard to approximate within a con-
stant factor, as finding the minimum number of cliques.
4 When Is Minimization Unique?
The previous section demonstrated the existence of
pathological weight semirings. We now partially charac-
terize the ?well-behaved? semirings (K,?,?) in which
all automata do have unique minimizations. Except when
otherwise stated, lowercase variables are weights ? K
and uppercase ones are K-valued rational functions.
[This section may be skipped, except the last paragraph.]
A crucial necessary condition is that (K,?) allow
what we will call greedy factorization, meaning that
given f?F = g?G 6= 0, it is always possible to express
6This argument only shows that pushing backward cannot
give them the same suffix function. But pushing forward cannot
help either, despite footnote 4, since 1n on the arc to i has no
right factors other than itself (the identity) to push forward.
F = f ? ?H and G = g? ?H . This condition holds for
many practically useful semirings, commutative or other-
wise. It says, roughly, that the order in which left factors
are removed from a suffix function does not matter. We
can reach the same canonical H regardless of whether we
left-divide first by f or g.
Given a counterexample to this condition, one can con-
struct an automaton with no unique minimization. Sim-
ply follow the plan of the Z[
?
?5] example, putting
F1 = F , F2 = f ? F = g ? G, F3 = G.7 For ex-
ample, in semiring (K,?) = ({xn : n 6= 1}, concat), put
F2 = x2 ?{(a,x3), (b,x4)} = x3 ?{(a,x2), (b,x3)}.
Some useful semirings do fail the condition. One
is the ?bitwise boolean? semiring that checks a string?s
membership in two languages at once: (K,?,?) =
({00, 01, 10, 11}, OR, AND). (Let F2 = 01 ?
{(a, 11), (b, 00)} = 01 ? {(a, 01), (b, 10)}.) R2 under
pointwise ? (which computes a string?s probability under
two models) fails similarly. So does (sets,?,?) (which
collects features found along the accepting path).
We call H a residue of F iff F = f ? ? H for some
f ?. Write F ' G iff F , G have a common residue. In
these terms, (K,?) allows greedy factorization iff F '
G when F , G are residues of the same nonzero function.
More perspicuously, one can show that this holds iff ' is
an equivalence relation on nonzero, K-valued functions.
So in semirings where minimization is uniquely de-
fined, ' is necessarily an equivalence relation. Given an
automaton M for function F , we may regard ' as an
equivalence relation on the states of a trimmed version
of M :8 q ' r iff Fq ' Fr. Let [r] = {r1, . . . , rm}
be the (finite) equivalence class of r: we can inductively
find at least one function F[r] that is a common residue
of Fr1 , . . . , Frm . The idea behind minimization is to
construct a machine M? whose states correspond to these
equivalence classes, and where each [r] has suffix func-
tion F[r]. The Appendix shows that M? is then minimal.
If M has an arc q a:k??r, M? needs an arc [q]a:k
?
??[r], where
k? is such that a?1F[q] = k? ? F[r].
The main difficulty in completing the construction of
M? is to ensure each weight k? exists. That is, F[r] must be
carefully chosen to be a residue not only of Fr1 , . . . , Frm
(which ultimately does not matter, as long as F[0] is a
residue of F0, where 0 is the start state) but also of
a?1F[q]. If M is cyclic, this imposes cyclic dependen-
cies on the choices of the various F[q] and F[r] functions.
We have found no simple necessary and sufficient con-
dition on (K,?) that guarantees a globally consistent set
of choices to exist. However, we have given a useful nec-
7Then factoring F2 allows state 2 to merge with either 1 or
3; but all three states cannot merge, since any suffix function
that could be shared by 1 and 3 could serve as H .
8Trimming ensures that suffix functions are nonzero.
essary condition (greedy factorization), and we now give
a useful sufficient condition. Say that H is a minimum
residue of G 6= 0 if it is a residue of every residue of G.
(If G has several minimum residues, they are all residues
of one another.) If (K,?) is such that every G has a min-
imum residue?a strictly stronger condition than greedy
factorization?then it can be shown that G has the same
minimum residues as any H ' G. In such a (K,?),
M? can be constructed by choosing the suffix functions
F[r] independently. Just let F[r] = F{r1,...,rm} be a mini-
mum residue of Fr1 . Now consider again M ?s arc q
a:k
??r:
since a?1F[q] ' a?1Fq ' Fr ' Fr1 , we see F[r] is a
(minimum) residue of a?1F[q], so that a weight k? can be
chosen for [q]a:k
?
??[r].
A final step ensures that M? defines the function F . To
describe it, we must augment the formalism to allow an
initial weight ?(0) ? K, and a final weight ?(r) ? K
for each final state r. The weight of an accepting path
from the start state 0 to a final state r is now defined to
be ?(0)? (weights of arcs along the path)? ?(r). In M? ,
we set ?([0]) to some k such that F0 = k ? F[0], and set
?([r]) = F[r](?). The mathematical construction is done.
5 A Simple Minimization Recipe
We now give an effective algorithm for minimization in
the semiring (K,?). The algorithmic recipe has one in-
gredient: along with (K,?), the user must give us a left-
factor functional ? that can choose a left factor ?(F ) of
any function F . Formally, if ? is the input alphabet, then
we require ? : (?? ? K) ? K to have the following
properties for any rational F : ?? ? K and any k ? K:
? Shifting: ?(k ? F ) = k ? ?(F ).
? Quotient: ?(F )\?(a?1F ) exists in K for any a ? ?.
? Final-quotient: ?(F )\F (?) exists in K.9
The algorithm generalizes Mohri?s strategy as outlined
in section 2. We just use ? to pick the left factors during
pushing. The ??s used by Mohri for two semirings were
mentioned in section 3. We will define another ? in sec-
tion 6. Naturally, it can be shown that no ? can exist in a
semiring that lacks greedy factorization, such as Z[
?
?5].
The 3 properties above are needed for the strategy to
work. The strategy also requires (K,?) to be left can-
cellative, i.e., k ? m = k ? m? implies m = m? (if
k 6= 0). In other words, left quotients by k are unique
when they exist (except for 0\0). This relieves us from
having to make arbitrary choices of weight during push-
ing. Incompatible choices might prevent arc labels from
matching as desired during the merging step of section 2.
9To show the final-quotient property given the other two, it
suffices to show that ?(G) ? K has a right inverse in K, where
G is the function mapping ? to 1 and everything else to 0.
Given an input DFA. At each state q, simultaneously,
we will push back ?(Fq). This pushing construction
is trivial once the ?(Fq) values are computed. An
arc q a:k??r should have its weight changed from k to
?(Fq)\?(a?1Fq) = ?(Fq)\?(k ? Fr), which is well-
defined (by the quotient property and left cancellativity)10
and can be computed as ?(Fq)\(k??(Fr)) (by the shift-
ing property). Thus a subpath q a:k??r b:`??s, with weight
k ? `, will become q a:k
?
??r
b:`?
??s, with weight k? ? `? =
(?(Fq)\(k ? ?(Fr))) ? (?(Fr)\(` ? ?(Fs))). In this
way the factor ?(Fr) is removed from the start of all paths
from r, and is pushed backwards through r onto the end
of all paths to r. It is possible for this factor (or part of
it) to travel back through multiple arcs and around cycles,
since k? is found by removing a ?(Fq) factor from all of
k ? ?(Fr) and not merely from k.
As it replaces the arc weights, pushing also replaces
the initial weight ?(0) with ?(0) ? ?(F0), and replaces
each final weight ?(r) with ?(Fr)\?(r) (which is well-
defined, by the final-quotient property). Altogether, push-
ing leaves path weights unchanged (by easy induction).11
After pushing, we finish with merging and trimming as
in section 2. While merging via unweighted DFA mini-
mization treats arc weights as part of the input symbols,
what should it do with any initial and final weights? The
start state?s initial weight should be preserved. The merg-
ing algorithm can and should be initialized with a multi-
way partition of states by final weight, instead of just a
2-way partition into final vs. non-final.12
The Appendix shows that this strategy indeed finds the
unique minimal automaton.
It is worth clarifying how this section?s effective al-
gorithm implements the mathematical construction from
the end of section 4. At each state q, pushing replaces the
suffix function Fq with ?(Fq)\Fq. The quotient proper-
ties of ? are designed to guarantee that this quotient is
defined,13 and the shifting property is designed to ensure
10Except in the case 0\0, which is not uniquely defined. This
arises only if Fq = 0, i.e., q is a dead state that will be trimmed
later, so any value will do for 0\0: arcs from q are irrelevant.
11One may prefer a formalism without initial or final weights.
If the original automaton is free of final weights (other than 1),
so is the pushed automaton?provided that ?(F ) = 1 whenever
F (?) = 1, as is true for all ??s in this paper. Initial weights can
be eliminated at the cost of duplicating state 0 (details omitted).
12Alternatively, Mohri (2000, ?4.5) explains how to tem-
porarily eliminate final weights before the merging step.
13That is, ?(Fq)\Fq(?) exists for each ? ? ??. One may
show by induction on |?| that the left quotients ?(F )\F (?) ex-
ist for all F . When |?| = 0 this is the final-quotient property.
For |?| > 0 we can write ? as a??, and then ?(F )\F (?) =
?(F )\F (a??) = ?(F )\(a?1F )(??) = (?(F )\?(a?1F )) ?
(?(a?1F )\(a?1F )(??)), where the first factor exists by the
quotient property and the second factor exists by inductive hy-
pothesis.
that it is a minimum residue of Fq.14 In short, if the con-
ditions of this section are satisfied, so are the conditions
of section 4, and the construction is the same.
The converse is true as well, at least for right cancella-
tive semirings. If such a semiring satisfies the conditions
of section 4 (every function has a minimum residue), then
the requirements of this section can be met to obtain an
effective algorithm: there exists a ? satisfying our three
properties,15 and the semiring is left cancellative.16
6 Minimization in Division Semirings
For the most important idea of this paper, we turn to a
common special case. Suppose the semiring (K,?,?)
defines k\m for all m, k 6= 0 ? K. Equivalently,17 sup-
pose every k 6= 0 ? K has a unique two-sided inverse
k?1 ? K. Useful cases of such division semirings in-
clude the real semiring (R,+,?), the tropical semiring
extended with negative numbers (R?{?},min,+), and
expectation semirings (Eisner, 2002). Minimization has
not previously been available in these.
We propose a new left-factor functional that is fast to
compute and works in arbitrary division semirings. We
avoid the temptation to define ?(F ) as
?
range(F ): this
definition has the right properties, but in some semirings
including (R?0,+,?) the infinite summation is quite ex-
pensive to compute and may even diverge. Instead (un-
like Mohri) we will permit our ?(F ) to depend on more
than just range(F ).
Order the space of input strings ?? by length, breaking
ties lexicographically. For example, ? < bb < aab <
aba < abb. Now define
14Suppose X is any residue of Fq , i.e., we can write Fq =
x ? X . Then we can rewrite the identity Fq = ?(Fq) ?
(?(Fq)\Fq), using the shifting property, as x ? X = x ?
?(X)?(?(Fq)\Fq). As we have separately required the semir-
ing to be left cancellative, this implies that X = ?(X) ?
(?(Fq)\Fq). So (?(Fq)\Fq) is a residue of any residue X of
Fq , as claimed.
15Define ?(0) = 0. From each equivalence class of nonzero
functions under ', pick a single minimum residue (axiom of
choice). Given F , let [F ] denote the minimum residue from its
class. Observe that F = f?[F ] for some f ; right cancellativity
implies f is unique. So define ?(F ) = f . Shifting property:
?(k ? F ) = ?(k ? f ? [F ]) = k ? f = k ? ?(f ? [F ]) =
k? ?(F ). Quotient property: ?(a?1F )? [a?1F ] = a?1F =
a?1(?(F )? [F ]) = ?(F )? a?1[F ] = ?(F )? ?(a?1[F ])?
[a?1[F ]] = ?(F ) ? ?(a?1[F ]) ? [a?1F ] (the last step since
a?1[F ] ' a?1F ). Applying right cancellativity, ?(a?1F ) =
?(F )??(a?1[F ]), showing that ?(F )\?(a?1F ) exists. Final-
quotient property: Quotient exists since F (?) = ?(F )?[F ](?).
16Let ?x, y? denote the function mapping a to x, b to y, and
everything else to 0. Given km = km?, we have k ? ?m, 1? =
k??m?, 1?. Since the minimum residue property implies greedy
factorization, we can write ?m, 1? = f ? ?a, b?, ?m?, 1? =
g ? ?a, b?. Then f ? b = g ? b, so by right cancellativity
f = g, whence m = f ? a = g ? a = m?.
17The equivalence is a standard exercise, though not obvious.
?(F )
def
=
{
F (min support(F )) ? K if F 6= 0
0 if F = 0
where support(F ) denotes the set of input strings to
which F assigns a non-0 weight. This ? clearly has the
shifting property needed by section 5. The quotient and
final-quotient properties come for free because we are in
a division semiring and because ?(F ) = 0 iff F = 0.
Under this definition, what is ?(Fq) for a suffix func-
tion Fq? Consider all paths of nonzero weight18 from
state q to a final state. If none exist, ?(Fq) = 0. Oth-
erwise, min support(Fq) is the input string on the short-
est such path, breaking ties lexicographically.19 ?(Fq) is
simply the weight of that shortest path.
To push, we must compute ?(Fq) for each state q. This
is easy because ?(Fq) is the weight of a single, minimum-
length and hence acyclic path from q. (Previous meth-
ods combined the weights of all paths from q, even if
infinitely many.) It also helps that the left factors at dif-
ferent states are related: if the minimum path from q be-
gins with a weight-k arc to r, then it continues along the
minimum path from r, so ?(Fq) = k ? ?(Fr).
Below is a trivial linear-time algorithm for computing
?(Fq) at every q. Each state and arc is considered once
in a breadth-first search back from the final states. len(q)
and first(q) store the string length and first letter of a run-
ning minimum of support(Fq) ? ??.
1. foreach state q
2. if q is final then
3. len(q) := 0 (* min support(Fq) is ? for final q *)
4. ?(Fq) := ?(q) (* Fq(?) is just the final weight, ?(q) *)
5. enqueue q on a FIFO queue
6. else
7. len(q) := ? (* not yet discovered *)
8. ?(Fq) := 0 (* assume Fq = 0 until we discover q *)
9. until the FIFO queue is empty
10. dequeue a state r
11. foreach arc q a:k??r entering r such that k 6= 0
12. if len(q) = ? then enqueue q (* breadth-first search *)
13. if len(q) = ? or (len(q) = len(r) + 1
and a < first(q)) then
14. first(q) := a (* reduce min support(Fq) *)
15. len(q) := len(r) + 1
16. ?(Fq) := k ? ?(Fr)
The runtime is O(|states|+t ? |arcs|) if ? has runtime t.
If ? is slow, this can be reduced to O(t ? |states|+ |arcs|)
by removing line 16 and waiting until the end, when the
minimum path from each non-final state q is fully known,
to compute the weight ?(Fq) of that path. Simply finish
up by calling FIND-? on each state q:
FIND-?(state q):
1. if ?(Fq) = 0 and len(q) < ? then
2. ?(Fq) := ?(q, first(q))? FIND-?(?(q, first(q)))
3. return ?(Fq)
18In a division semiring, these are paths free of 0-weight arcs.
19The min exists since < is a well-ordering. In a purely lex-
icographic ordering, a?b ? ?? would have no min.
After thus computing ?(Fq), we simply proceed with
pushing, merging, and trimming as in section 5.20 Push-
ing runs in time O(t ? |arcs|) and trimming in O(|states|+
|arcs|). Merging is worse, with time O(|arcs| log |states|).
7 A Bonus: Non-Division Semirings
The trouble with Z[
?
?5] was that it ?lacked? needed
quotients. The example on p. 3 can easily be minimized
(down to 3 states) if we regard it instead as defined over
(C,+,?)?letting us use any weights in C. Simply use
section 6?s algorithm.
This new change-of-semiring trick can be used for
other non-division semirings as well. One can extend the
original weight semiring (K,?,?) to a division semiring
by adding ?-inverses.21
In this way, the tropical semiring (R?0 ? {?},
min,+) can be augmented with the negative reals to ob-
tain (R ? {?},min,+). And the transducer semiring
(?? ? {?},min, concat)22 can be augmented by extend-
ing the alphabet ? = {x, y, . . .} with inverse letters
{x?1, y?1, . . .}.
The minimized DFA we obtain may have ?weird? arc
weights drawn from the extended semiring. But the arc
weights combine along paths to produce the original au-
tomaton?s outputs, which fall in the original semiring.
Let us apply this trick to the example of section 2,
yielding the following pushed automaton in which F1 =
F3 as desired. (x?1, y?1, . . . are written as X, Y, . . ., and
?(Fq) is displayed at each q.)
0xyz
1yza:     ?
3wwyz
b:ZYXwwyz 2z
a:     ?b:ZYzzz
a:     ?
4?b:ZYzzz
5?b:     ?b:     ?:xyz
For example, the z?1y?1zzz output on the 3?4 arc was
computed as ?(F3)?1 ? wwzzz ? ?(F4) = (wwyz)?1 ?
wwzzz? ? = z?1y?1w?1w?1wwzzz.
This trick yields new algorithms for the tropical semir-
ing and sequential transducers, which is interesting and
perhaps worthwhile. How do they compare with previ-
ous work?
Over the tropical semiring, our linear-time pushing al-
gorithm is simpler than (Mohri, 1997), and faster by a
20It is also permissible to trim the input automaton at the start,
or right after computing ? (note that ?(Fq) = 0 iff we should
trim q). This simplifies pushing and merging. No trimming is
then needed at the end, except to remove the one dead state that
the merging step may have added to complete the automaton.
21This is often possible but not always; the semiring must be
cancellative, and there are other conditions. Even disregarding
? because we are minimizing a deterministic automaton, it is
not simple to characterize when the monoid (K,?) can be em-
bedded into a group (Clifford and Preston, 1967, chapter 12).
22Where min can be defined as in section 6 and footnote 1.
log factor, because it does not require a priority queue.
(Though this does not help the overall complexity of min-
imization, which is dominated by the merging step.) We
also have no need to implement faster algorithms for spe-
cial cases, as Mohri proposes, because our basic algo-
rithm is already linear. Finally, our algorithm generalizes
better, as it can handle negative weight cycles in the input.
These are useful in (e.g.) conditional random fields.
On the other hand, Mohri?s algorithm guarantees a po-
tentially useful property that we do not: that the weight
of the prefix path reading ? ? ?? is the minimum weight
of all paths with prefix ?. Commonly this approximates
? log(p(most probable string with prefix ?)), perhaps a
useful value to look up for pruning.
As for transducers, how does our minimization algo-
rithm (above) compare with previous ones? Following
earlier work by Choffrut and others, Mohri (2000) de-
fines ?(Fq) as the longest common prefix of range(Fq).
He constrains these values with a set of simultaneous
equations, and solves them by repeated changes of vari-
able using a complex relaxation algorithm. His imple-
mentation uses various techniques (including a trie and
a graph decomposition) to make pushing run in time
O(|states| + |arcs| ? maxq |?(Fq)|).23 Breslauer (1996)
gives a different computation of the same result.
To implement our simpler algorithm, we represent
strings in ?? as pointers into a global trie that extends
upon lookup. The strings are actually stored reversed in
the trie so that it is fast to add and remove short pre-
fixes. Over the extended alphabet, we use the pointer
pair (k,m) to represent the string k?1m where k,m ?
?? have no common prefix. Such pointer pairs can
be equality-tested in O(1) time during merging. For
k,m ? ??, k ?m is computed in time O(|k|), and k\m
in time O(|LCP(k,m)|) or more loosely O(|k|) (where
LCP = longest common prefix).
The total time to compute our ?(Fq) values is therefore
O(|states|+ t ? |arcs|), where t is the maximum length of
any arc?s weight. For each arc we then compute a new
weight as a left-quotient by a ? value. So our total run-
time for pushing is O(|states| + |arcs| ? maxq |?(Fq)|).
This may appear identical to Mohri?s runtime, but in fact
our |?(Fq)| ? Mohri?s, though the two definitions share
a worst case of t ? |states|.24
Inverse letters must be eliminated from the minimized
transducer if one wishes to pass it to any specialized al-
gorithms (composition, inversion) that assume weights
23We define |?| = 1 to simplify the O(? ? ?) expressions.
24The |?(Fq)| term contributed by a given arc from q is a
bound on the length of the LCP of the outputs of certain paths
from q. Mohri uses all paths from q and we use just two, so our
LCP is sometimes longer. However, both LCPs probably tend to
be short in practice, especially if one bypasses LCP(k, k) with
special handling for k\k = ?.
in ??. Fortunately this is not hard. If state q of the
result was formed by merging states q1, . . . qj , define
?(q) = LCS{?(Fqi) : i = 1, . . . j} ? ?
? (where LCS =
longest common suffix). Now push the minimized trans-
ducer using ?(q)?1 in place of ?(Fq) for all q. This cor-
rects for ?overpushing?: any letters ?(q) that were unnec-
essarily pushed back before minimization are pushed for-
ward again, cancelling the inverse letters. In our running
example, state 0 will push (xyz)?1 back and the merged
state {1,3} will push (yz)?1 back. This is equivalent to
pushing ?(0) = xyz forward through state 0 and the yz
part of it forward through {1,3}, canceling the z?1y?1 at
the start of one of the next arcs.
We must show that the resulting labels really are free
of inverse letters. Their values are as if the original push-
ing had pushed back not ?(Fqi) ? ?? but only its shorter
prefix ??(qi)
def
= ?(Fqi)/?(qi) ? ?
? (note the right quo-
tient). In other words, an arc from qi to ri? with weight
k ? ?? was reweighted as ??(qi)\(k ? ??(ri?)). Any in-
verse letters in such new weights clearly fall at the left.
So suppose the new weight on the arc from q to r begins
with an inverse letter z?1. Then ??(qi) must have ended
with z for each i = 1, . . . j. But then ?(qi) was not the
longest common suffix: z?(qi) is a longer one, a contra-
diction (Q.E.D.).
Negative weights can be similarly eliminated after
minimization over the tropical semiring, if desired, by
substituting min for LCS.
The optional elimination of inverse letters or nega-
tive weights does not affect the asymptotic runtime. A
caveat here is that the resulting automaton no longer has
a canonical form. Consider a straight-line automaton:
pushing yields a canonical form as always, but inverse-
letter elimination completely undoes pushing (??(qi) =
?). This is not an issue in Mohri?s approach.
8 Conclusion and Final Remarks
We have characterized the semirings over which
weighted deterministic automata can be minimized (sec-
tion 4), and shown how to perform such minimization in
both general and specific cases (sections 5, 6, 7). Our
technique for division semirings and their subsemirings
pushes back, at each state q, the output of a single, easily
found, shortest accepting path from q. This is simpler and
more general than previous approaches that aggregate all
accepting paths from q.
Our new algorithm (section 6) is most important for
previously unminimizable, practically needed division
semirings: real (e.g., for probabilities), expectation (for
learning (Eisner, 2002)), and additive with negative
weights (for conditional random fields (Lafferty et al,
2001)). It can also be used in non-division semirings,
as for transducers. It is unpatented, easy to implement,
comparable or faster in asymptotic runtime, and perhaps
faster in practice (especially for the tropical semiring,
where it seems preferable in most respects).
Our approach applies also to R-weighted sequential
transducers as in (Cortes et al, 2002). Such automata
can be regarded as weighted by the product semiring
(R ? ??, (+,min), (?, concat)). Equivalently, one can
push the numeric and string components independently.
Our new pushing algorithm enables not only minimiza-
tion but also equivalence-testing in more weight semir-
ings. Equivalence is efficiently tested by pushing the (de-
terministic) automata to canonicalize their arc labels and
then testing unweighted equivalence (Mohri, 1997).
References
A. V. Aho, J. E. Hopcroft, and J. D. Ullman. 1974. The Design
and Analysis of Computer Algorithms. Addison-Wesley.
Jean Berstel and Christophe Reutenauer. 1988. Rational Series
and their Languages. Springer-Verlag.
Dany Breslauer. 1996. The suffix tree of a tree and minimizing
sequential transducers. Lecture Notes in Computer Science,
1075.
A. H. Clifford and G. B. Preston. 1967. The Algebraic Theory
of Semigroups.
Corinna Cortes, Patrick Haffner, and Mehryar Mohri. 2002.
Rational kernels. In Proceedings of NIPS, December.
Pierluigi Crescenzi and Viggo Kann. 1998. How to find the best
approximation results?a follow-up to Garey and Johnson.
ACM SIGACT News, 29(4):90?97, December.
Jason Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proc. of ACL, Philadelphia, July.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of the
International Conference on Machine Learning.
Mehryar Mohri. 1997. Finite-state transducers in language and
speech processing. Computational Linguistics, 23(2).
Mehryar Mohri. 2000. Minimization algorithms for sequential
transducers. Theoretical Computer Science, 324:177?201.
Appendix: Remaining Proofs
Let M be an automaton to minimize and F : ?? ? K be the
function it defines. We assume (K,?) allows greedy factoriza-
tion, so ' is an equivalence relation on nonzero functions. We
first prove that M? with the properties of section 4 is the minimal
automaton computing F . We will then show, following Mohri,
that the algorithm of section 5 finds such an M? . (Section 6 is a
special case of section 5.)
We chose in advance a desired suffix function F[r] for each
state [r] of M? , and used these to determine the weights of M? .
To show that the weights were determined correctly, let F?[r] be
the actual suffix function of [r]. Claim that for all ? and r,
F?[r](?) = F[r](?). This is easily proved by induction on |?|.
Our choice of initial weight then ensures that M? computes F .
We must now prove minimality. For ?, ? ? ??, say ? F? ?
iff ??1F ' ??1F . Note that F? is an equivalence relation on
D
def
= {? ? ?? : ??1F 6= 0}.25
25It is not an equivalence relation on all of ??, since ? 6? D is
Let M ? be any automaton that computes F . For ?, ? ? D,
we say ? M
?
? ? iff ?M?(0, ?) = ?M?<(0, ?), i.e., the prefixes
? and ? lead from the start state 0 to the same state q in M ?.
If ? M
?
? ?, then ? F? ?, since ??1F = ?(0, ?) ? Fq '
?(0, ?)? Fq = ??1F .
If ? F? ?, then ??1F ' ??1F , so F?M (0,?) ' ?
?1F '
??1F ' F?M (0,?), so ?M (0, ?) ' ?M (0, ?), so ?
M?
? ? by
construction of M? .
In short, ? M
?
? ? ? ?
F
? ? ? ?
M?
? ?. So each of the three
partitions of D into equivalence classes is a refinement of the
next. Hence nM? ? nF ? nM? , where these are the respective
numbers of equivalence classes.
Since M
?
? has one equivalence class per useful state of M ? (as
defined in section 2), nM? is the number of states in a trimmed
version of M ?. Similarly nM? is the number of states of M? (after
trimming). Since M ? was arbitrary, M? is minimal.
Uniqueness: If M ? has the same number of states as M? , then
the two partitions must be equal. So two prefixes reach the same
state in M ? iff they do so in M? . This gives a ?-preserving iso-
morphism between M ? and M? . It follows that the minimal ma-
chine is unique, except for the distribution of output labels along
paths (which may depend on arbitrary choices of residues F[r]).
Now we turn to section 5?s effective construction, using ?,
of a pushed machine M? and a merged version M? . The proof
of minimality is essentially the same as in (Mohri, 2000). We
know that M? computes the same function as M (since pushing,
merging, and trimming preserve this). So it suffices to show
?
F
? ? ? ?
M?
? ?. The above proof of minimality will then go
through as before.
M and M? have the same states and transition function ?;
denote their emission functions by ? and ??. Fq refers to suf-
fix functions in M . Given ? F? ? (so ?, ? ? D), use the
definition of F? to write ??1F = k? ? F ? and ??1F =
k? ? F ?. Let q = ?(0, ?), r = ?(0, ?), k = ?(0, ?).
For any a ? ?, write ??(q, a) = ?(Fq)\?(a?1Fq) = (k ?
?(Fq))\(k ? ?(a?1Fq)) = ?(k ? Fq)\?(k ? a?1Fq) =
?(??1F )\?(a?1(??1F )) = ?(k??F ?)\?(a?1(k??F ?)) =
?(F ?)\?(a?1F ?). By symmetry, ??(r, a) = ?(F ?)\?(a?1F ?)
as well. Thanks to left cancellativity, left quotients are unique,
so ??(q, a) = ??(r, a).26
So ? F? ? ? corresponding arcs from q and r in M? output
identical weights. Since ?a F? ?a as well, the same holds at
?(q, a) and ?(r, a). So by induction, regarding M? as an un-
weighted automaton, exactly the same strings in (??K)? are
accepted from q and from r. So merging will merge q and r,
and ? M?? ? as claimed.
related by F? to every ?. This corresponds to the fact that a dead
state can be made to merge with any state by pushing 0 back
from it, so that the arcs to it have weight 0 and the arcs from
it have arbitrary weights. Our construction of M? only creates
states for the equivalence classes of D; ?(0, ?) for ? 6? D is
undefined, not a dead state.
26We must check that we did not divide by 0 and obtain a
false equation. It suffices to show that k 6= 0 and ?(Fq) 6=
0. Fortunately, ? ? D implies both. (It implies Fq 6= 0, so
(??1Fq)(?) = Fq(?) 6= 0 for some ?. Hence ?(Fq) 6= 0
since otherwise ?(??1Fq) = 0 and ?(??1Fq)\(??1Fq)(?) is
undefined, contradicting the final-quotient property.)
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 423?430,
New York, June 2006. c?2006 Association for Computational Linguistics
A fast finite-state relaxation method for enforcing
global constraints on sequence decoding
Roy W. Tromble and Jason Eisner
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{royt,jason}@cs.jhu.edu
Abstract
We describe finite-state constraint relaxation, a method for ap-
plying global constraints, expressed as automata, to sequence
model decoding. We present algorithms for both hard con-
straints and binary soft constraints. On the CoNLL-2004 se-
mantic role labeling task, we report a speedup of at least 16x
over a previous method that used integer linear programming.
1 Introduction
Many tasks in natural language processing involve
sequence labeling. If one models long-distance or
global properties of labeled sequences, it can be-
come intractable to find (?decode?) the best labeling
of an unlabeled sequence.
Nonetheless, such global properties can improve
the accuracy of a model, so recent NLP papers
have considered practical techniques for decod-
ing with them. Such techniques include Gibbs
sampling (Finkel et al, 2005), a general-purpose
Monte Carlo method, and integer linear program-
ming (ILP), (Roth and Yih, 2005), a general-purpose
exact framework for NP-complete problems.
Under generative models such as hidden Markov
models, the probability of a labeled sequence de-
pends only on its local properties. The situation
improves with discriminatively trained models, such
as conditional random fields (Lafferty et al, 2001),
which do efficiently allow features that are functions
of the entire observation sequence. However, these
features can still only look locally at the label se-
quence. That is a significant shortcoming, because
in many domains, hard or soft global constraints on
the label sequence are motivated by common sense:
? For named entity recognition, a phrase that
appears multiple times should tend to get the
same label each time (Finkel et al, 2005).
? In bibliography entries (Peng and McCallum,
2004), a given field (author, title, etc.) should
be filled by at most one substring of the in-
put, and there are strong preferences on the co-
occurrence and order of certain fields.
? In seminar announcements, a given field
(speaker, start time, etc.) should appear with
at most one value in each announcement, al-
though the field and value may be repeated
(Finkel et al, 2005).
? For semantic role labeling, each argument
should be instantiated only once for a given
verb. There are several other constraints that
we will describe later (Roth and Yih, 2005).
A popular approximate technique is to hypothe-
size a list of possible answers by decoding without
any global constraints, and then rerank (or prune)
this n-best list using the full model with all con-
straints. Reranking relies on the local model being
?good enough? that the globally best answer appears
in its n-best list. Otherwise, reranking can?t find it.
In this paper, we propose ?constraint relaxation,?
a simple exact alternative to reranking. As in rerank-
ing, we start with a weighted lattice of hypotheses
proposed by the local model. But rather than restrict
to the n best of these according to the local model,
we aim to directly extract the one best according to
the global model. As in reranking, we hope that the
local constraints alone will work well, but if they do
not, the penalty is not incorrect decoding, but longer
runtime as we gradually fold the global constraints
into the lattice. Constraint relaxation can be used
whenever the global constraints can be expressed as
regular languages over the label sequence.
In the worst case, our runtime may be exponential
in the number of constraints, since we are consider-
ing an intractable class of problems. However, we
show that in practice, the method is quite effective
at rapid decoding under global hard constraints.
423
0O
1
?
O?
Figure 1: An automaton expressing the constraint that the label
sequence cannot be O?. Here ? matches any symbol except O.
The remainder of the paper is organized as fol-
lows: In ?2 we describe how finite-state automata
can be used to apply global constraints. We then
give a brute-force decoding algorithm (?3). In ?4,
we present a more efficient algorithm for the case of
hard constraints. We report results for the semantic
role labeling task in ?5. ?6 treats soft constraints.
2 Finite-state constraints
Previous approaches to global sequence labeling?
Gibbs sampling, ILP, and reranking?seem moti-
vated by the idea that standard sequence methods are
incapable of considering global constraints at all.
In fact, finite-state automata (FSAs) are powerful
enough to express many long-distance constraints.
Since all finite languages are regular, any constraint
over label sequences of bounded length is finite-
state. FSAs are more powerful than n-gram mod-
els. For example, the regular expression ??X??Y??
matches only sequences of labels that contain an X
before a Y. Similarly, the regular expression ?(O?)
requires at least one non-O label; it compiles into the
FSA of Figure 1.
Note that this FSA is in one or the other of its two
states according to whether it has encountered a non-
O label yet. In general, the current state of an FSA
records properties of the label sequence prefix read
so far. The FSA needs enough states to keep track of
whether the label sequence as a whole satisfies the
global constraint in question.
FSAs are a flexible approach to constraints be-
cause they are closed under logical operations such
as disjunction (union) and conjunction (intersec-
tion). They may be specified by regular expressions
(Karttunen et al, 1996), in a logical language (Vail-
lette, 2004), or directly as FSAs. They may also be
weighted to express soft constraints.
Formally, we pose the decoding problem in terms
of an observation sequence x ? X ? and possible la-
bel sequences y ? Y?. In many NLP tasks, X is the
set of words, and Y the tags. A lattice L: Y? 7? R
maps label sequences to weights, and is encoded as a
weighted FSA. Constraints are formally the same?
any function C: Y? 7? R is a constraint, includ-
ing weighted features from a classifier or probabilis-
tic model. In this paper we will consider only con-
straints that are weighted in particular ways.
Given a lattice L and constraints C, we seek
y?
def
= argmax
y
(
L(y) +
?
C?C
C(y)
)
. (1)
We assume the lattice L is generated by a model
M : X ? 7? (Y? 7? R). For a given observation se-
quence x, we put L = M(x). One possible model
is a finite-state transducer, where M(x) is an FSA
found by composing the transducer with x. Another
is a CRF, where M(x) is a lattice with sums of log-
potentials for arc weights.1
3 A brute-force finite-state decoder
To find the best constrained labeling in a lattice, y?,
according to (1), we could simply intersect the lat-
tice with all the constraints, then extract the best
path.
Weighted FSA intersection is a generalization of
ordinary unweighted FSA intersection (Mohri et al,
1996). It is customary in NLP to use the so-called
tropical semiring, where weights are represented by
their natural logarithms and summed rather than
multiplied. Then the intersected automaton L ? C
computes
(L ? C)(y)
def
= L(y) + C(y) (2)
To find y?, one would extract the best path in
L ? C1 ? C2 ? ? ? ? using the Viterbi algorithm, or
Dijkstra?s algorithm if the lattice is cyclic. This step
is fast if the intersected automaton is small.
The problem is that the multiple intersections in
L ? C1 ? C2 ? ? ? ? can quickly lead to an FSA with
an intractable number of states. The intersection
of two finite-state automata produces an automaton
1For example, if M is a simple linear-chain CRF, L(y) =Pn
i=1 f(yi?1, yi) + g(xi, yi). We build L = M(x) as an
acyclic FSA whose state set is Y ? {1, 2, . . . n}, with transi-
tions (y?, i? 1) ? (y, i) of weight f(y?, y) + g(xi, y).
424
with the cross product state set. That is, if F has m
states and G has n states, then F ?G has up to mn
states (fewer if some of the mn possible states do
not lie on any accepting path).
Intersection of many such constraints, even if they
have only a few states each, quickly leads to a com-
binatorial explosion. In the worst case, the size, in
states, of the resulting lattice is exponential in the
number of constraints. To deal with this, we present
a constraint relaxation algorithm.
4 Hard constraints
The simplest kind of constraint is the hard con-
straint. Hard constraints are necessarily binary?
either the labeling satisfies the constraint, or it vi-
olates it. Violation is fatal?the labeling produced
by decoding must satisfy each hard constraint.
Formally, a hard constraint is a mappingC: Y? 7?
{0,??}, encoded as an unweighted FSA. If a string
satisfies the constraint, recognition of the string will
lead to an accepting state. If it violates the con-
straint, recognition will end in a non-accepting state.
Here we give an algorithm for decoding with a set
of such constraints. Later (?6), we discuss the case
of binary soft constraints. In what follows, we will
assume that there is always at least one path in the
lattice that satisfies all of the constraints.
4.1 Decoding by constraint relaxation
Our decoding algorithm first relaxes the global con-
straints and solves a simpler problem. In particular,
we find the best labeling according to the model,
y?0
def
= argmax
y
L(y) (3)
ignoring all the constraints in C.
Next, we check whether y?0 satisifies the con-
straints. If so, then we are done?y?0 is also y?. If
not, then we reintroduce the constraints. However,
rather than include all at once, we introduce them
only as they are violated by successive solutions to
the relaxed problems: y?0, y?1, etc. We define
y?1
def
= argmax
y
(L(y) + C(y)) (4)
for some constraint C that y?0 violates. Similarly,
y?2 satisfies an additional constraint that y?1 violates,
HARD-CONSTRAIN-LATTICE(L, C):
1. y := Best-Path(L)
2. while ?C ? C such that C(y) = ??:
3. L := L ? C
4. C := C ? {C}
5. y := Best-Path(L)
6. return y
Figure 2: Hard constraints decoding algorithm.
and so on. Eventually, we find some k for which y?k
satisfies all constraints, and this path is returned.
To determine whether a labeling y satisfies a con-
straint C, we represent y as a straight-line automa-
ton and intersect with C, checking the result for non-
emptiness. This is equivalent to string recognition.
Our hope is that, although intractable in the worst
case, the constraint relaxation algorithm will operate
efficiently in practice. The success of traditional se-
quence models on NLP tasks suggests that, for nat-
ural language, much of the correct analysis can be
recovered from local features and constraints alone.
We suspect that, as a result, global constraints will
often be easy to satisfy.
Pseudocode for the algorithm appears in Figure 2.
Note that line 2 does not specify how to choose
C from among multiple violated constraints. This
is discussed in ?7. Our algorithm resembles the
method of Koskenniemi (1990) and later work. The
difference is that there lattices are unweighted and
may not contain a path that satisfies all constraints,
so that the order of constraint intersection matters.
5 Semantic role labeling
The semantic role labeling task (Carreras and
Ma`rques, 2004) involves choosing instantiations of
verb arguments from a sentence for a given verb.
The verb and its arguments form a proposition. We
use data from the CoNLL-2004 shared task?the
PropBank (Palmer et al, 2005) annotations of the
Penn Treebank (Marcus et al, 1993), with sections
15?18 as the training set and section 20 as the de-
velopment set. Unless otherwise specified, all mea-
surements are made on the development set.
We follow Roth and Yih (2005) exactly, in order
to compare system runtimes. They, in turn, follow
Hacioglu et al (2004) and others in labeling only
the heads of syntactic chunks rather than all words.
We label only the core arguments (A0?A5), treating
425
(a)
0?
1
A0
A0
2
?
?
(b) 0
1
O A0 A1 A2 A3 A4 A5
2
O
(verb p
osition
)A1A2A3A4A5 OA0
(c)
0OA0A1A2A3
Figure 4: Automata expressing NO DUPLICATE A0 (? matches
anything but A0), KNOWN VERB POSITION[2], and DISALLOW
ARGUMENTS[A4,A5].
adjuncts and references as O.
Figure 3 shows an example sentence from the
shared task. It is marked with an IOB phrase chunk-
ing, the heads of the phrases, and the correct seman-
tic role labeling. Heads are taken to be the rightmost
words of chunks. On average, there are 18.8 phrases
per proposition, vs. 23.5 words per sentence. Sen-
tences may contain multiple propositions. There are
4305 propositions in section 20.
5.1 Constraints
Roth and Yih use five global constraints on label se-
quences for the semantic role labeling task. We ex-
press these constraints as FSAs. The first two are
general, and the seven automata encoding them can
be constructed offline:
? NO DUPLICATE ARGUMENT LABELS
(Fig. 4(a)) requires that each verb have at
most one argument of each type in a given
sentence. We separate this into six individual
constraints, one for each core argument type.
Thus, we have constraints called NO DUPLI-
CATE A0, NO DUPLICATE A1, etc. Each of
these is represented as a three-state FSA.
? AT LEAST ONE ARGUMENT (Fig. 1) simply re-
quires that the label sequence is not O?. This is
a two-state automaton as described in ?2.
The last three constraints require information
about the example, and the automata must be con-
structed on a per-example basis:
? ARGUMENT CANDIDATES (Fig. 5) encodes a
set of position spans each of which must re-
ceive only a single label type. These spans were
proposed using a high-recall heuristic (Xue and
Palmer, 2004).
? KNOWN VERB POSITION (Fig. 4(b)) simply
encodes the position of the verb in question,
which must be labeled O.
? DISALLOW ARGUMENTS (Fig. 4(c)) specifies
argument types that are compatible with the
verb in question, according to PropBank.
5.2 Experiments
We implemented our hard constraint relaxation al-
gorithm, using the FSA toolkit (Kanthak and Ney,
2004) for finite-state operations. FSA is an open-
source C++ library providing a useful set of algo-
rithms on weighted finite-state acceptors and trans-
ducers. For each example we decoded, we chose a
random order in which to apply the constraints.
Lattices are generated from what amounts to a
unigram model?the voted perceptron classifier of
Roth and Yih. The features used are a subset of those
commonly applied to the task.
Our system produces output identical to that of
Roth and Yih. Table 1 shows F-measure on the core
arguments. Table 2 shows a runtime comparison.
The ILP runtime was provided by the authors (per-
sonal communication). Because the systems were
run under different conditions, the times are not di-
rectly comparable. However, constraint relaxation is
more than sixteen times faster than ILP despite run-
ning on a slower platform.
5.2.1 Comparison to an ILP solver
Roth and Yih?s linear program has two kinds of
numeric constraints. Some encode the shortest path
problem structure; the others encode the global con-
straints of ?5.1. The ILP solver works by relaxing
to a (real-valued) linear program, which may obtain
a fractional solution that represents a path mixture
instead of a path. It then uses branch-and-bound to
seek the optimal rounding of this fractional solution
to an integer solution (Gue?ret et al, 2002) that repre-
sents a single path satisfying the global constraints.
Our method avoids fractional solutions: a relaxed
solution is always a true single path, which either
426
Mr. Turner said the test will be shipped in 45 days to hospitals and clinical laboratories .
B-NP I-NP B-VP B-NP I-NP B-VP I-VP I-VP B-PP B-NP I-NP B-PP B-NP O B-NP I-NP O
Turner said test shipped in days to hospitals and laboratories .
A0 O A1 A1 A1 A1 A1 A1 A1 A1 O
Figure 3: Example sentence, with phrase tags and heads, and core argument labels. The A1 argument of ?said? is a long clause.
0
1
O A0 A1 A2 A3 A4 A5
2
A2 A3 A4 A5 O A0 A1
4
O
10
A0
16
A1
22
A2
28
A3
34
A4
40
A5
5
O
11
A0
17
A1
23
A2
29
A3
35
A4
41
A5
42
A5
43
A5
44
A5
45
A5
3
A5
46
O A0 A1 A2 A3 A4 A5
OA0A1A2A3A4A5
36
A4
37
A4
38
A4
39
A4
A4
30
A3
31
A3
32
A3
33
A3
A3
24
A2
25
A2
26
A2
27
A2
A2
18
A1
19
A1
20
A1
21
A1
A1
12
A0
13
A0
14
A0
15
A0
A0
6
O
7
O
8
O
9
O
O
Figure 5: An automaton expressing ARGUMENT CANDIDATES.
Argument Count F-measure
A0 2849 79.27
A1 4029 75.59
A2 943 55.68
A3 149 46.41
A4 147 81.82
A5 4 25.00
All 8121 74.51
Table 1: F-measure on core arguments.
satisfies or violates each global constraint. In effect,
we are using two kinds of domain knowledge. First,
we recognize that this is a graph problem, and insist
on true paths so we can use Viterbi decoding. Sec-
ond, we choose to relax only domain-specific con-
straints that are likely to be satisfied anyway (in our
domain), in contrast to the meta-constraint of inte-
grality relaxed by ILP. Thus it is cheaper on aver-
age for us to repair a relaxed solution. (Our repair
strategy?finite-state intersection in place of branch-
and-bound search?remains expensive in the worst
case, as the problem is NP-hard.)
5.2.2 Constraint violations
The y?0s, generated with only local information,
satisfy most of the global constraints most of the
time. Table 3 shows the violations by type.
The majority of best labelings according to the
local model don?t violate any global constraints?
a fact especially remarkable because there are no
label sequence features in Roth and Yih?s unigram
Constraint Violations Fraction
ARGUMENT CANDIDATES 1619 0.376
NO DUPLICATE A1 899 0.209
NO DUPLICATE A0 348 0.081
NO DUPLICATE A2 151 0.035
AT LEAST ONE ARGUMENT 108 0.025
DISALLOW ARGUMENTS 48 0.011
NO DUPLICATE A3 13 0.003
NO DUPLICATE A4 3 0.001
NO DUPLICATE A5 1 0.000
KNOWN VERB POSITION 0 0.000
Table 3: Violations of constraints by y?0 .
model. This confirms our intuition that natural lan-
guage structure is largely apparent locally. Table 4
shows the breakdown. The majority of examples are
very efficient to decode, because they don?t require
intersection of the lattice with any constraints?y?0
is extracted and is good enough. Those examples
where constraints are violated are still relatively effi-
cient because they only require a small number of in-
tersections. In total, the average number of intersec-
tions needed, even with the naive randomized con-
straint ordering, was only 0.65. The order doesn?t
matter very much, since 75% of examples have one
violation or fewer.
5.2.3 Effects on lattice size
Figure 6 shows the effect of intersection with vi-
olated constraints on the average size of lattices,
measured in arcs. The vertical bars at k = 0,
k = 1, . . . show the number of examples where con-
427
Method Total Time Per Example Platform
Brute Force Finite-State 37m25.290s 0.522s Pentium III, 1.0 GHz
ILP 11m39.220s 0.162s Xeon, 3.x GHz
Constraint Relaxation 39.700s 0.009s Pentium III, 1.0 GHz
Table 2: A comparison of runtimes for constrained decoding with ILP and FSA.
Violations Labelings Fraction Cumulative
0 2368 0.550 0.550
1 863 0.200 0.750
2 907 0.211 0.961
3 156 0.036 0.997
4 10 0.002 0.999
5 1 0.000 1.000
6?10 0 0.000 1.000
Table 4: Number of y?0 with each violation count.
 0
 500
 1000
 1500
 2000
 2500
 0  1  2  3  4  5
VerbsMean Arcs with RelaxationMean Arcs with Brute Force
Figure 6: Mean lattice size (measured in arcs) throughout de-
coding. Vertical bars show the number of examples over which
each mean is computed.
straint relaxation had to intersect k contraints (i.e.,
y? ? y?k). The trajectory ending at (for example)
k = 3 shows how the average lattice size for that
subset of examples evolved over the 3 intersections.
The X at k = 3 shows the final size of the brute-force
lattice on the same subset of examples.
For the most part, our lattices do stay much
smaller than those produced by the brute-force algo-
rithm. (The uppermost curve, k = 5, is an obvious
exception; however, that curve describes only the
seven hardest examples.) Note that plotting only the
final size of the brute-force lattice obscures the long
trajectory of its construction, which involves 10 in-
tersections and, like the trajectories shown, includes
larger intermediate automata.2 This explains the far
2The final brute-force lattice is especially shrunk by its in-
Constraint Violations Fraction
ARGUMENT CANDIDATES 90 0.0209
AT LEAST ONE ARGUMENT 27 0.0063
NO DUPLICATE A2 3 0.0007
NO DUPLICATE A0 2 0.0005
NO DUPLICATE A1 2 0.0005
NO DUPLICATE A3 1 0.0002
NO DUPLICATE A4 1 0.0002
Table 5: Violations of constraints by y?, measured over the de-
velopment set.
longer runtime of the brute-force method (Table 2).
Harder examples (corresponding to longer trajec-
tories) have larger lattices, on average. This is partly
just because it is disproportionately the longer sen-
tences that are hard: they have more opportunities
for a relaxed decoding to violate global constraints.
Hard examples are rare. The left three columns,
requiring only 0?2 intersections, constitute 96% of
examples. The vast majority can be decoded without
much more than doubling the local-lattice size.
6 Soft constraints
The gold standard labels y? occasionally violate the
hard global constraints that we are using. Counts
for the development set appear in Table 5. Counts
for violations of NO DUPLICATE A? do not include
discontinous arguments, of which there are 104 in-
stances, since we ignore them.
Because of the infrequency, the hard constraints
still help most of the time. However, on a small sub-
set of the examples, they preclude us from inferring
the correct labeling.
We can apply these constraints with weights,
rather than making them inviolable. This constitutes
a transition from hard to soft constraints. Formally,
a soft constraint C: Y? 7? R? is a mapping from a
label sequence to a non-positive penalty.
Soft constraints present new difficulty for decod-
clusion of, for example, DISALLOW ARGUMENTS, which can
only remove arcs. That constraint is rarely included in the re-
laxation lattices because it is rarely violated (see Table 3).
428
SOFT-CONSTRAIN-LATTICE(L, C):
1. (y?, Score(y?)) := (empty,??)
2. branches := [(L, C, 0)]
3. while (L, C, penalty) := Dequeue(branches):
4. L := Prune(L, Score(y?)? penalty)
5. unless Empty(L):
6. y := Best-Path(L)
7. for C ? C:
8. if C(y) < 0: (* so C(y) = wC *)
9. C := C ? {C}
10. Enqueue(branches, (L ? C, C, penalty))
11. penalty := penalty + C(y)
12. if Score(y?) < L(y) + penalty:
13. (y?, Score(y?)) := (y, L(y) + penalty)
14. return y?
Figure 7: Soft constraints decoding algorithm
ing, because instead of eliminating paths of L from
contention, they just reweight them.
In what follows, we consider only binary soft
constraints?they are either satisfied or violated, and
the same penalty is assessed whenever a violation
occurs. That is, ?C ? C,?wC < 0 such that
?y, C(y) ? {0, wC}.
6.1 Soft constraint relaxation
The decoding algorithm for soft constraints is a gen-
eralization of that for hard constraints. The differ-
ence is that, whereas with hard constraints a vio-
lation meant disqualification, here violation simply
means a penalty. We therefore must find and com-
pare two labelings: the best that satisfies the con-
straint, and the best that violates it.
We present a branch-and-bound algorithm
(Lawler and Wood, 1966), with pseudocode in
Figure 7. At line 9, we process and eliminate a
currently violated constraint C ? C by considering
two cases. On the first branch, we insist that C be
satisfied, enqueuing L ? C for later exploration. On
the second branch, we assume C is violated by all
paths, and so continue considering L unmodified,
but accept a penalty for doing so; we immediately
explore the second branch by returning to the start
of the for loop.3
Not every branch needs to be completely ex-
plored. Bounding is handled by the PRUNE func-
tion at line 4, which shrinks L by removing some
3It is possible that a future best path on the second branch
will not actually violate C, in which case we have overpenalized
it, but in that case we will also find it with correct penalty on the
first branch.
or all paths that cannot score better than Score(y?),
the score of the best path found on any branch so
far. Our experiments used almost the simplest possi-
ble PRUNE: replace L by the empty lattice if the best
path falls below the bound, else leave L unchanged.4
A similar bounding would be possible in the im-
plicit branches. If, during the for loop, we find that
the test at line 12 would fail, we can quit the for
loop and immediately move to the next branch in
the queue at line 3.
There are two factors in this algorithm that con-
tribute to avoiding consideration of all of the expo-
nential number of leaves corresponding to the power
set of constraints. First, bounding stops evaluation
of subtrees. Second, only violated constraints re-
quire branching. If a lattice?s best path satisifies a
constraint, then the best path that violates it can be
no better since, by assumption, ?y, C(y) ? 0.
6.2 Runtime experiments
Using the ten constraints from ?5.1, weighted
naively by their log odds of violation, the soft con-
straint relaxation algorithm runs in a time of 58.40
seconds. It is, as expected, slower than hard con-
straint relaxation, but only by a factor of about two.
As a side note, softening these particular con-
straints in this particular way did not improve de-
coding quality in this case. It might help to jointly
train the relative weights of these constraints and
the local model?e.g., using a perceptron algorithm
(Freund and Schapire, 1998), which repeatedly ex-
tracts the best global path (using our algorithm),
compares it to the gold standard, and adjusts the con-
straint weights. An obvious alternative is maximum-
entropy training, but the partition function would
have to be computed using the large brute-force lat-
tices, or else approximated by a sampling method.
7 Future work
For a given task, we may be able to obtain further
speedups by carefully choosing the order in which
to test and apply the constraints. We might treat this
as a reinforcement learning problem (Sutton, 1988),
4Partial pruning is also possible: by running the Viterbi ver-
sion of the forward-backward algorithm, one can discover for
each edge the weight of the best path on which it appears. One
can then remove all edges that do not appear on any sufficiently
good path.
429
where an agent will obtain rewards by finding y?
quickly. In the hard-constraint algorithm, for ex-
ample, the agent?s possible moves are to test some
constraint for violation by the current best path, or
to intersect some constraint with the current lattice.
Several features can help the agent choose the next
move. How large is the current lattice, which con-
straints does it already incorporate, and which re-
maining constraints are already known to be satis-
fied or violated by its best path? And what were the
answers to those questions at previous stages?
Our constraint relaxation method should be tested
on problems other than semantic role labeling. For
example, information extraction from bibliography
entries, as discussed in ?1, has about 13 fields to ex-
tract, and interesting hard and soft global constraints
on co-occurrence, order, and adjacency. The method
should also be evaluated on a task with longer se-
quences: though the finite-state operations we use
do scale up linearly with the sequence length, longer
sequences have more chance of violating a global
constraint somewhere in the sequence, requiring us
to apply that constraint explicitly.
8 Conclusion
Roth and Yih (2005) showed that global constraints
can improve the output of sequence labeling models
for semantic role labeling. In general, decoding un-
der such constraints is NP-complete. We exhibited
a practical approach, finite-state constraint relax-
ation, that greatly sped up decoding on this NLP task
by using familiar finite-state operations?weighted
FSA intersection and best-path extraction?rather
than integer linear programming.
We have also given a constraint relaxation algo-
rithm for binary soft constraints. This allows incor-
poration of constraints akin to reranking features, in
addition to inviolable constraints.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
0347822. We thank Scott Yih for kindly providing
both the voted-perceptron classifier and runtime re-
sults for decoding with ILP, and the reviewers for
helpful comments.
References
Xavier Carreras and Llu??s Ma`rques. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In Proc.
of CoNLL, pp. 89?97.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of ACL, pp.
363?370.
Yoav Freund and Robert E. Schapire. 1998. Large margin clas-
sification using the perceptron algorithm. In Proc. of COLT,
pp. 209?217, New York. ACM Press.
Christelle Gue?ret, Christian Prins, and Marc Sevaux. 2002. Ap-
plications of optimization with Xpress-MP. Dash Optimiza-
tion. Translated and revised by Susanne Heipcke.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H. Mar-
tin, and Daniel Jurafsky. 2004. Semantic role labeling by
tagging syntactic chunks. In Proc. of CoNLL, pp. 110?113.
Stephan Kanthak and Hermann Ney. 2004. FSA: An efficient
and flexible C++ toolkit for finite state automata using on-
demand computation. In Proc. of ACL, pp. 510?517.
Lauri Karttunen, Jean-Pierre Chanod, Gregory Grefenstette,
and Anne Schiller. 1996. Regular expressions for lan-
guage engineering. Journal of Natural Language Engineer-
ing, 2(4):305?328.
Kimmo Koskenniemi. 1990. Finite-state parsing and disam-
biguation. In Proc. of COLING, pp. 229?232.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML, pp.
282?289.
Eugene L. Lawler and David E. Wood. 1966. Branch-and-
bound methods: A survey. Operations Research, 14(4):699?
719.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics,
19:313?330.
Mehryar Mohri, Fernando Pereira, and Michael Riley. 1996.
Weighted automata in text and speech processing. In A. Ko-
rnai, editor, Proc. of the ECAI 96 Workshop, pp. 46?50.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Fuchun Peng and Andrew McCallum. 2004. Accurate informa-
tion extraction from research papers using conditional ran-
dom fields. In Proc. of HLT-NAACL, pp. 329?336.
Dan Roth and Wen-tau Yih. 2005. Integer linear programming
inference for conditional random fields. In Proc. of ICML,
pp. 737?744.
Richard S. Sutton. 1988. Learning to predict by the methods of
temporal differences. Machine Learning, 3(1):9?44.
Nathan Vaillette. 2004. Logical Specification of Finite-State
Transductions for Natural Language Processing. Ph.D. the-
sis, Ohio State University.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proc. of EMNLP, pp. 88?94.
430
Proceedings of NAACL HLT 2007, pages 252?259,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Cross-Instance Tuning of Unsupervised Document Clustering Algorithms?
Damianos Karakos, Jason Eisner
and Sanjeev Khudanpur
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{damianos,eisner,khudanpur}@jhu.edu
Carey E. Priebe
Dept. of Applied Mathematics
and Statistics
Johns Hopkins University
Baltimore, MD 21218
cep@jhu.edu
Abstract
In unsupervised learning, where no train-
ing takes place, one simply hopes that
the unsupervised learner will work well
on any unlabeled test collection. How-
ever, when the variability in the data is
large, such hope may be unrealistic; a
tuning of the unsupervised algorithm may
then be necessary in order to perform well
on new test collections. In this paper,
we show how to perform such a tuning
in the context of unsupervised document
clustering, by (i) introducing a degree of
freedom, ?, into two leading information-
theoretic clustering algorithms, through
the use of generalized mutual informa-
tion quantities; and (ii) selecting the value
of ? based on clusterings of similar, but
supervised document collections (cross-
instance tuning). One option is to perform
a tuning that directly minimizes the error
on the supervised data sets; another option
is to use ?strapping? (Eisner and Karakos,
2005), which builds a classifier that learns
to distinguish good from bad clusterings,
and then selects the ? with the best pre-
dicted clustering on the test set. Experi-
ments from the ?20 Newsgroups? corpus
show that, although both techniques im-
prove the performance of the baseline al-
gorithms, ?strapping? is clearly a better
choice for cross-instance tuning.
?This work was partially supported by the DARPA GALE
program (Contract No
?
HR0011-06-2-0001) and by the JHU
WSE/APL Partnership Fund.
1 Introduction
The problem of combining labeled and unlabeled
examples in a learning task (semi-supervised learn-
ing) has been studied in the literature under various
guises. A variety of algorithms (e.g., bootstrapping
(Yarowsky, 1995), co-training (Blum and Mitchell,
1998), alternating structure optimization (Ando and
Zhang, 2005), etc.) have been developed in order to
improve the performance of supervised algorithms,
by automatically extracting knowledge from lots of
unlabeled examples. Of special interest is the work
of Ando and Zhang (2005), where the goal is to build
many supervised auxiliary tasks from the unsuper-
vised data, by creating artificial labels; this proce-
dure helps learn a transformation of the input space
that captures the relatedness of the auxiliary prob-
lems to the task at hand. In essence, Ando and Zhang
(2005) transform the semi-supervised learning prob-
lem to a multi-task learning problem; in multi-task
learning, a (usually large) set of supervised tasks is
available for training, and the goal is to build mod-
els which can simultaneously do well on all of them
(Caruana, 1997; Ben-David and Schuller, 2003; Ev-
geniou and Pontil, 2004).
Little work, however, has been devoted to study
the situation where lots of labeled examples, of one
kind, are used to build a model which is tested on
unlabeled data of a ?different? kind. This problem,
which is the topic of this paper, cannot be cast as a
multi-task learning problem (since there are labeled
examples of only one kind), neither can be cast as a
semi-supervised problem (since there are no training
labels for the test task). Note that we are interested
in the case where the hidden test labels may have
no semantic relationship with the training labels; in
252
some cases, there may not even be any informa-
tion about the test labels?what they represent, how
many they are, or at what granularity they describe
the data. This situation can arise in the case of un-
supervised clustering of documents from a large and
diverse corpus: it may not be known in what way the
resulting clusters split the corpus (is it in terms of
topic? genre? style? authorship? a combination of
the above?), unless one inspects each resulting clus-
ter to determine its ?meaning.?
At this point, we would like to differentiate be-
tween two concepts: a target task refers to a class
of problems that have a common, high-level de-
scription (e.g., the text document clustering task, the
speech recognition task, etc.). On the other hand,
a task instance refers to a particular example from
the class. For instance, if the task is ?document
clustering,? a task instance could be ?clustering of
a set of scientific documents into particular fields?;
or, if the task is ?parsing,? a task instance could be
?parsing of English sentences from the Wall Street
Journal corpus?. For the purposes of this paper, we
further assume that there are task instances which
are unrelated, in the sense that that there are no
common labels between them. For example, if the
task is ?clustering from the 20 Newsgroups corpus,?
then ?clustering of the computer-related documents
into PC-related and Mac-related? and ?clustering
of the politics-related documents into Middle-East-
related and non-Middle-East-related? are two dis-
tinct, unrelated instances. In more mathematical
terms, if task instances T1, T2 take sets of observa-
tionsX1,X2 as input, and try to predict labels from
sets S1, S2, respectively, then they are called unre-
lated if X1 ?X2 = ? and S1 ? S2 = ?.
The focus of this paper is to study the problem
of cross-instance tuning of unsupervised algorithms:
how one can tune an algorithm, which is used to
solve a particular task instance, using knowledge
from an unrelated task instance. To the best of our
knowledge, this cross-instance learning problem has
only been tackled in (Eisner and Karakos, 2005),
whose ?strapping? procedure learns a meta-classifier
for distinguishing good from bad clusterings.
In this paper, we introduce a scalar parameter ?
(a new degree of freedom) into two basic unsuper-
vised clustering algorithms. We can tune ? to max-
imize unsupervised clustering performance on dif-
ferent task instances where the correct clustering is
known. The hope is that tuning the parameter learns
something about the task in general, which trans-
fers from the supervised task instances to the un-
supervised one. Alternatively, we can tune a meta-
classifier so as to select good values of ? on the su-
pervised task instances, and then use the same meta-
classifier to select a good (possibly different) value
of ? in the unsupervised case.
The paper is organized as follows: Section 2 gives
a background on text categorization, and briefly de-
scribes the algorithms that we use in our experi-
ments. Section 3 describes our parameterization of
the clustering algorithms using Jensen-Re?nyi diver-
gence and Csisza?r?s mutual information. Experi-
mental results from the ?20 Newsgroups? data set
are shown in Section 4, along with two techniques
for cross-instance learning: (i) ?strapping,? which, at
test time, picks a parameter based on various ?good-
ness? cues that were learned from the labeled data
set, and (ii) learning the parameter from a supervised
data set which is chosen to statistically match the test
set. Finally, concluding remarks appear in Section 5.
2 Document Categorization
Document categorization is the task of deciding
whether a piece of text belongs to any of a set of
prespecified categories. It is a generic text process-
ing task useful in indexing documents for later re-
trieval, as a stage in natural language processing
systems, for content analysis, and in many other
roles (Lewis and Hayes, 1994). Here, we deal
with the unsupervised version of document cate-
gorization, in which we are interested in cluster-
ing together documents which (hopefully) belong to
the same topic, without having any training exam-
ples.1 Supervised information-theoretic clustering
approaches (Torkkola, 2002; Dhillon et al, 2003)
have been shown to be very effective, even with a
small amount of labeled data, while unsupervised
methods (which are of particular interest to us) have
been shown to be competitive, matching the classifi-
cation accuracy of supervised methods.
Our focus in this paper is on document catego-
rization algorithms which use information-theoretic
1By this, we mean that training examples having the same
category labels as the test examples are not available.
253
criteria, since there are natural ways of generalizing
these criteria through the introduction of tunable pa-
rameters. We use two such algorithms in our exper-
iments, the sequential Information Bottleneck (sIB)
and Iterative Denoising Trees (IDTs); details about
these algorithms appear below.
A note on mathematical notation: We assume
that we have a collection A = {X(1), . . . , X(N)}
of N documents. Each document X(i) is essentially
a ?bag of words?, and induces an empirical distri-
bution P?X(i) on the vocabulary X . Given a sub-
set (cluster) C of documents, the conditional dis-
tribution on X , given the cluster, is just the cen-
troid: P?X|C = 1|C|
?
X(i)?C P?X(i). If a subcollec-
tion S ? A of documents is partitioned into clusters
C1, . . . , Cm, and each document X(i) ? S is as-
signed to a cluster CZ(i), where Z(i) ? {1, . . . ,m}
is the cluster index, then the mutual information be-
tween words and corresponding clusters is given by
I(X;Z|S) =
?
z?{1,...,m}
P (z|S)D(P?X|Cz?P?X|S),
where P (z|S) , |Cz|/|S| is the ?prior? distribution
on the clusters and D(???) is the Kullback-Leibler
divergence (Cover and Thomas, 1991).
2.1 The Information Bottleneck Method
The Information Bottleneck (IB) method (Tishby et
al., 1999; Slonim and Tishby, 2000; Slonim et al,
2002) is one popular approach to unsupervised cat-
egorization. The goal of the IB (with ?hard? clus-
tering) is to find clusters such that the mutual in-
formation I(X;Z) between words and clusters is as
large as possible, under a constraint on the number
of clusters. The procedure for finding the maximiz-
ing clustering in (Slonim and Tishby, 2000) is ag-
glomerative clustering, while in (Slonim et al, 2002)
it is based on many random clusterings, combined
with a sequential update algorithm, similar to K-
means. The update algorithm re-assigns each data
point (document) d to its most ?similar? cluster C,
in order to minimize I(X;Z|C ? {d}), i.e.,
?D(P?X|{d}?P?X|{d}?C)+(1??)D(P?X|C?P?X|{d}?C),
where ? = 1|C|+1 . This latter procedure is called
the sequential Information Bottleneck (sIB) method,
and is considered the state-of-the-art in unsuper-
vised document categorization.
2.2 Iterative Denoising Trees
Decision trees are a powerful technique for equiva-
lence classification, accomplished through a recur-
sive successive refinement (Jelinek, 1997). In the
context of unsupervised classification, the goal of
decision trees is to cluster empirical distributions
(bags of words) into a given number of classes, with
each class corresponding to a leaf in the tree. They
are built top-down (as opposed to the bottom-up
construction in IB) using maximization of mutual
information between words and clusters I(X;Z|t)
to drive the splitting of each node t; the hope is that
each leaf will contain data points which belong to
only one latent category.
Iterative Denoising Trees (also called Integrated
Sensing and Processing Decision Trees) were intro-
duced in (Priebe et al, 2004a), as an extension of
regular decision trees. Their main feature is that
they transform the data at each node, before split-
ting, by projecting into a low-dimensional space.
This transformation corresponds to feature extrac-
tion; different features are suppressed (or ampli-
fied) by each transformation, depending on what
data points fall into each node (corpus-dependent-
feature-extraction property (Priebe et al, 2004b)).
Thus, dimensionality reduction and clustering are
chosen so that they jointly optimize the local objec-
tive.
In (Karakos et al, 2005), IDTs were used for an
unsupervised hyperspectral image segmentation ap-
plication. The objective at each node t was to maxi-
mize the mutual information between spectral com-
ponents and clusters given the pixels at node t, com-
puted from the projected empirical distributions. At
each step of the tree-growing procedure, the node
which yielded the highest increase in the average,
per-node mutual information, was selected for split-
ting (until a desired number of leaves was reached).
In (Karakos et al, 2007b), the mutual information
objective was replaced with a parameterized form of
mutual information, namely the Jensen-Re?nyi diver-
gence (Hero et al, 2001; Hamza and Krim, 2003), of
which more details are provided in the next section.
3 Parameterizing Unsupervised Clustering
As mentioned above, the algorithms considered in
this paper (sIB and IDTs) are unsupervised, in the
254
sense that they can be applied to test data with-
out any need for tuning. Our procedure of adapt-
ing them, based on some supervision on a different
task instance, is by introducing a parameter into the
unsupervised algorithm. At least for simple cross-
instance tuning, this parameter represents the infor-
mation which is passed between the supervised and
the unsupervised instances.
The parameterizations that we focused on have
to do with the information-theoretic objectives in
the two unsupervised algorithms. Specifically, fol-
lowing (Karakos et al, 2007b), we replace the mu-
tual information quantities in IDTs as well as sIB
with the parameterized mutual information mea-
sures mentioned above. These two quantities pro-
vide estimates of the dependence between the ran-
dom quantities in their arguments, just as the usual
mutual information does, but also have a scalar pa-
rameter ? ? (0, 1] that controls the sensitivity of the
computed dependence on the details of the joint dis-
tribution of X and Z. As a result, the effect of data
sparseness on estimation of the joint distribution can
be mitigated when computing these measures.
3.1 Jensen-Re?nyi Divergence
The Jensen-Re?nyi divergence was used in (Hero et
al., 2001; Hamza and Krim, 2003) as a measure of
similarity for image classification and retrieval. For
two discrete random variables X,Z with distribu-
tions PX , PZ and conditional PX|Z , it is defined as
I?(X;Z) = H?(PX)?
?
z
PZ(z)H?(PX|Z(?|z)),
(1)
where H?(?) is the Re?nyi entropy, given by
H?(P ) =
1
1? ?
log
(
?
x?X
P (x)?
)
, ? 6= 1. (2)
If ? ? (0, 1), H? is a concave function, and hence
I?(X;Z) is non-negative (and it is equal to zero if
and only if X and Z are independent). In the limit
as ? ? 1, H?(?) approaches the Shannon entropy
(not an obvious fact), so I?(?) reduces to the regular
mutual information. Similarly, we define
I?(X;Z|W ) =
?
w
PW (w)I?(X;Z|W = w),
where I?(X;Z|W = w) is computed via (1) using
the conditional distribution of X and Z given W .
Except in trivial cases, H?(?) is strictly larger
than H(?) when 0 < ? < 1; this means that the ef-
fects of extreme sparsity (few words per document,
or too few occurrences of non-frequent words) on
the estimation of entropy and mutual information
can be dampened with an appropriate choice of ?.
This happens because extreme sparsity in the data
yields empirical distributions which lie at, or close
to, the boundary of the probability simplex. The
entropy of such distributions is usually underesti-
mated, compared to the smooth distributions which
generate the data. Re?nyi?s entropy is larger than
Shannon?s entropy, especially in those regions close
to the boundary, and can thus provide an estimate
which is closer to the true entropy.
3.2 Csisza?r?s Mutual Information
Csisza?r defined the mutual information of order ? as
IC? (X;Z) = min
Q
?
z
PZ(z)D?(PX|Z(?|z)?Q(?)),
(3)
where D?(???) is the Re?nyi divergence (Csisza?r,
1995). It was shown that IC? (X;Z) retains most
of the properties of I(X;Z)?it is a non-negative,
continuous, and concave function of PX , it is con-
vex in PX|Z for ? < 1, and converges to I(X;Z) as
? ? 1.
Notably, IC? (X;Z) ? I(X;Z) for 0 < ? < 1;
this means, as above, that ? regulates the overesti-
mation of mutual information that may result from
data sparseness.
There is no analytic form for the minimizer of the
right-hand-side of (3) (Csisza?r, 1995), but it may be
computed via an alternating minimization algorithm
(Karakos et al, 2007a).
4 Experimental Methods and Results
We demonstrate the feasibility of cross-instance tun-
ing with experiments on unsupervised document cat-
egorization from the 20 Newsgroups corpus (Lang,
1995); this corpus consists of roughly 20,000 news
articles, evenly divided among 20 Usenet groups.
Random samples of 500 articles each were chosen
by (Slonim et al, 2002) to create multiple test col-
lections: 250 each from 2 arbitrarily chosen Usenet
255
groups for the Binary test collection, 100 articles
each from 5 groups for the Multi5 test collection,
and 50 each from 10 groups for the Multi10 test col-
lection. Three independent test collections of each
kind (Binary, Multi5 and Multi10) were created, for
a total of 9 collections. The sIB method was used to
separately cluster each collection, given the correct
number of clusters.
A comparison of sIB and IDTs on the same 9 test
collections was reported in (Karakos et al, 2007b;
Karakos et al, 2007a). Matlab code from (Slonim,
2003) was used for the sIB experiments, while the
parameterized mutual information measures of Sec-
tion 3 were used for the IDTs. A comparison was
also made with the EM-based Gaussian mixtures
clustering tool mclust (Fraley and Raftery, 1999),
and with a simple K-means algorithm. Since the
two latter techniques gave uniformly worse cluster-
ings than those of sIB and IDTs, we omit them from
the following discussion.
To show that our methods work beyond the 9 par-
ticular 500-document collections described above,
in this paper we instead use five different randomly
sampled test collections for each of the Binary,
Multi5 and Multi10 cases, making for a total of 15
new test collections in this paper. For diversity, we
ensure that none of the five test collections (in each
case) contain any documents used in the three col-
lections of (Slonim et al, 2002) (for the same case).
We pre-process the documents of each test col-
lection using the procedure2 mentioned in (Karakos
et al, 2007b). The 15 test collections are then
converted to feature matrices?term-document fre-
quency matrices for sIB, and discounted tf/idf ma-
trices (according to the Okapi formula (Gatford et
al., 1995)) for IDTs?with each row of a matrix rep-
resenting one document in that test collection.
2Excluding the subject line, the header of each abstract is
removed. Stop-words such as a, the, is, etc. are removed, and
stemming is performed (e.g., common suffixes such as -ing, -
er, -ed, etc., are removed). Also, all numbers are collapsed
to one symbol, and non-alphanumeric sequences are converted
to whitespace. Moreover, as suggested in (Yang and Pedersen,
1997) as an effective method for reducing the dimensionality of
the feature space (number of distinct words), all words which
occur fewer than t times in the corpus are removed. For the
sIB experiments, we use t = 2 (as was done in (Slonim et al,
2002)), while for the IDT experiments we use t = 3; these
choices result in the best performance for each method, respec-
tively, on another dataset.
4.1 Selecting ? with ?Strapping?
In order to pick the value of the parameter ? for
each of the sIB and IDT test experiments, we use
?strapping? (Eisner and Karakos, 2005), which, as
we mentioned earlier, is a technique for training a
meta-classifier that chooses among possible cluster-
ings. The training is based on unrelated instances of
the same clustering task. The final choice of cluster-
ing is still unsupervised, since no labels (or ground
truth, in general) for the instance of interest are used.
Here, our collection of possible clusterings for
each test collection is generated by varying the ? pa-
rameter. Strapping does not care, however, how the
collection was generated. (In the original strapping
paper, for example, Eisner and Karakos (2005) gen-
erated their collection by bootstrapping word-sense
classifiers from 200 different seeds.)
Here is how we choose a particular unsupervised
?-clustering to output for a given test collection:
? We cluster the test collection (e.g., the first Multi5
collection) with various values of ?, namely ? =
0.1, 0.2, . . . , 1.0.
? We compute a feature vector from each of the
clusterings. Note that the features are computed
from only the clusterings and the data points,
since no labels are available.
? Based on the feature vectors, we predict the
?goodness? of each clustering, and return the
?best? one.
How do we predict the ?goodness? of a cluster-
ing? By first learning to distinguish good cluster-
ings from bad ones, by using unrelated instances of
the task on which we know the true labels:
? We cluster some unrelated datasets with various
values of ?, just as we will do in the test condi-
tion.
? We evaluate each of the resulting clusterings us-
ing the true labels on its dataset.3
? We train a ?meta-classifier? that predicts the true
rank (or accuracy) of each clustering based on the
feature vector of the clustering.
3To evaluate a clustering, one only really needs the true la-
bels on a sample of the dataset, although in our experiments we
did have true labels on the entire dataset.
256
Specifically, for each task (Binary, Multi5, and
Multi10) and each clustering method (sIB and IDT),
a meta-classifier is learned thus:
? We obtain 10 clusterings (? = 0.1, 0.2, . . . , 1.0)
for each of 5 unrelated task instances (datasets
whose construction is described below).
? For each of these 50 clusterings, we compute the
following 14 features: (i) One minus the aver-
age cosine of the angle (in tf/idf space) between
each example and the centroid of the cluster to
which it belongs. (ii) The average Re?nyi diver-
gence, computed for parameters 1.0, 0.5, 0.1, be-
tween the empirical distribution of each example
and the centroid of the cluster to which it belongs.
(iii) We create 10 more features, one per ?. For
the ? used in this clustering, the feature value is
equal to e?0.1r?, where r? is the average rank of the
clustering (i.e., the average of the 4 ranks result-
ing from sorting all 10 clusterings (per training
example) according to one of the 4 features in (i)
and (ii)). For all other ??s, the feature is set to
zero. Thus, only ??s which yield relatively good
rankings can have non-zero features in the model.
? We normalize each group of 10 feature vectors,
translating and scaling each of the 14 dimensions
to make it range from 0 to 1. (We will do the same
at test time.)
? We train ranking SVMs (Joachims, 2002), with
a Gaussian kernel, to learn how to rank these 50
clusterings given their respective normalized fea-
ture vectors. The values of c, ? (which control
regularization and the Gaussian kernel) were op-
timized through leave-one-out cross validation in
order to maximize the average accuracy of the
top-ranked clustering, over the 5 training sets.
Once a local maximum of the average accuracy
was obtained, further tuning of c, ? to maximize
the Spearman rank correlation between the pre-
dicted and true ranks was performed.
A model trained in this way knows something
about the task, and may work well for many new,
unseen instances of the task. However, we pre-
sume that it will work best on a given test instance
if trained on similar instances. The ideal would be
to match the test collection in every aspect: (i) the
number of training labels should be equal to the
number of desired clusters of the test collection; (ii)
the training clusters should be topically similar to
the desired test clusters.
In our scenario, we enjoy the luxury of plenty
of labeled data that can be used to create similar
instances. Thus, given a test collection A to be
clustered into L clusters, we create similar train-
ing sets by identifying the L training newsgroups
whose centroids in tf/idf space (using the Okapi for-
mula mentioned earlier) have the smallest angle to
the centroid of A.4 (Of course, we exclude news-
groups that appear in A.) We then form a supervised
500-document training set A? by randomly choosing
500/L documents from each of these L newsgroups;
we do this 5 times to obtain 5 supervised training
sets.
Table 1 shows averaged classification errors re-
sulting from strapping (?str? rows) for the Jensen-
Re?nyi divergence and Csisza?r?s mutual information,
used within IDTs and sIB, respectively. (We also
tried the reverse, using Jensen-Re?nyi in sIB and
Csisza?r?s in IDTs, but the results were uniformly
worse in the former case and no better in the latter
case.) The ?MI? rows show the classification errors
of the untuned algorithms (? = 1), which, in almost
all cases, are worse than the tuned ones.
4.2 Tuning ? on Statistically Similar Examples
We now show that strapping outperforms a simpler
and more obvious method for cross-instance tun-
ing. To cluster a test collection A, we could simply
tune the clustering algorithm by choosing the ? that
works best on a related task instance.
We again take care to construct a training instance
A? that is closely related to the test instance A. In
fact, we take even greater care this time. Given A,
4For each of the Binary collections, the closest training
newsgroups in our experiments were talk.politics.guns,
talk.religion.misc; for each of the Multi5 collections
the closest newsgroups were sci.electronics, rec.autos,
sci.med, talk.politics.misc, talk.religion.misc, and for
the Multi10 collections they were talk.politics.misc,
rec.motorcycles, talk.religion.misc, comp.graphics,
comp.sys.ibm.pc.hardware, rec.sport.baseball, comp.os.ms-
windows.misc, comp.windows.x, soc.religion.christian,
talk.politics.mideast. Note that each of the Binary test
collections happens to be closest to the same two training
newsgroups; a similar behavior was observed for the Multi5
and Multi10 newsgroups.
257
PPPPPPPPMethod
Set Binary Multi5 Multi10
ID
Ts
MI 11.3% 9.9% 42.2%
I? (str) 10.4% 9.2% 39.0%
I? (rls) 10.1% 10.4% 42.7%
sI
B
MI 12.0% 6.8% 38.5%
IC? (str) 11.2% 6.9% 35.8%
IC? (rls) 11.1% 7.4% 37.4%
Table 1: Average classification errors for IDTs and
sIB, using strapping (?str? rows) and regularized
least squares (?rls? rows) to pick ? in Jensen-Re?nyi
divergence and Csisza?r?s mutual information. Rows
?MI? show the errors resulting from the untuned al-
gorithms, which use the regular mutual information
objective (? = 1). Results which are better than the
corresponding ?MI? results are shown in bold.
we identify the same set of L closest newsgroups as
described above. This time, however, we carefully
select |A|/L documents from each newsgroup rather
than randomly choosing 500/L of them. Specifi-
cally, for each test example (document) X ? A, we
add a similar training example X ? into A?, chosen as
follows:
We associate each test example X to the most
similar of the L training newsgroups, under a con-
straint that only |A|/L training examples may be as-
sociated to each newsgroup. To do this, we iterate
through all pairs (X,G) where X is a test example
and G is a training newsgroup, in increasing order
by the angle between X and G. If X is not yet asso-
ciated and G is not yet ?full,? then we associate X
with G, and choose X ? to be the document in G with
the smallest angle to X .
We cluster A? 10 times, for ? = 0.1, . . . , 1.0,
and we collect supervised error results E(?), ? ?
{0.1, . . . , 1.0}. Now, instead of using the single best
?? = argmin? E(?) to cluster A (which may re-
sult in overfitting) we use regularized least-squares
(RLS) (Hastie et al, 2001), where we try to approx-
imate the probability that an ? is the best. The esti-
mated probabilities are given by
p? = K(?I+K)?1p,
where I is the unit matrix, p is the training prob-
ability of the best ? (i.e., it is 1 at the position of
?? and zero elsewhere), and K is the kernel matrix,
where K(i, j) = exp(?(E(?i) ? E(?j))2/?2) is
the value of the kernel which expresses the ?sim-
ilarity? between two clusterings of the same train-
ing dataset, in terms of their errors. The parame-
ters ?, ? are set to 0.5, 0.1, respectively, after per-
forming a (local) maximization of the Spearman cor-
relation between training accuracies and predicted
probabilities p?, for all 15 training instances. Af-
ter performing a linear normalization of p? to make
it a probability vector, the average predicted value
of ?, i.e., ?? =
?10
i=1 p?i ?i, (rounded-off to one of
{0.1, . . . , 1.0}) is used to cluster A.
Table 1 shows the average classification error re-
sults using RLS (?rls? rows). We can see that, on
average over the 15 test instances, the error rate of
the tuned IDTs and sIB algorithms is lower than that
of the untuned algorithms, so cross-instance tuning
was effective. On the other hand, the errors are
generally higher than that of the strapping method,
which examines the results of using different ? val-
ues on A.
5 Concluding Remarks
We have considered the problem of cross-instance
tuning of two unsupervised document clustering al-
gorithms, through the introduction of a degree of
freedom into their mutual information objective.
This degree of freedom is tuned using labeled doc-
ument collections (which are unrelated to the test
collections); we explored two approaches for per-
forming the tuning: (i) through a judicious sampling
of training data, to match the marginal statistics of
the test data, and (ii) via ?strapping?, which trains a
meta-classifier to distinguish between good and bad
clusterings. Our unsupervised categorization exper-
iments from the ?20 Newsgroups? corpus indicate
that, although both approaches improve the base-
line algorithms, ?strapping? is clearly a better choice
for knowledge transfer between unrelated task in-
stances.
References
R. K. Ando and T. Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6:1817?1853, Nov.
258
S. Ben-David and R. Schuller. 2003. Exploiting task
relatedness for multiple task learning. In Proc. of
the Sixteenth Annual Conference on Learning Theory
(COLT-03).
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of the Workshop on Computational Learning Theory
(COLT-98), pages 92?100.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
T. Cover and J. Thomas. 1991. Elements of Information
Theory. John Wiley and Sons.
I. Csisza?r. 1995. Generalized cutoff rates and Re?nyi?s
information measures. IEEE Trans. on Information
Theory, 41(1):26?34, January.
I. Dhillon, S. Mallela, and R. Kumar. 2003. A divisive
information-theoretic feature clustering algorithm
for text classification. Journal of Machine Learning
Research (JMLR), Special Issue on Variable and
Feature Selection, pages 1265?1287, March.
J. Eisner and D. Karakos. 2005. Bootstrapping without
the boot. In Proc. 2005 Conference on Human
Language Technology / Empirical Methods in Natural
Language Processing (HLT/EMNLP 2005), October.
T. Evgeniou and M. Pontil. 2004. Regularized multi-task
learning. In Proc. Knowledge Discovery and Data
Mining.
C. Fraley and A. E. Raftery. 1999. Mclust: Software for
model-based cluster analysis. Journal on Classifica-
tion, 16:297?306.
M. Gatford, M. M. Hancock-Beaulieu, S. Jones,
S. Walker, and S. E. Robertson. 1995. Okapi at
TREC-3. In The Third Text Retrieval Conference
(TREC-3), pages 109?126.
A. Ben Hamza and H. Krim. 2003. Jensen-Re?nyi
divergence measure: Theoretical and computational
perspectives. In Proc. IEEE Int. Symp. on Information
Theory, Yokohama, Japan, June.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer-Verlag.
A. O. Hero, B. Ma, O. Michel, and J. Gorman. 2001.
Alpha-divergence for classification, indexing and
retrieval. Technical Report CSPL-328, University of
Michigan Ann Arbor, Communications and Signal
Processing Laboratory, May.
F. Jelinek. 1997. Statistical Methods for Speech Recog-
nition. MIT Press.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM Conf. on Knowledge
Discovery and Data Mining (KDD).
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2005. Unsupervised classification via decision trees:
An information-theoretic perspective. In Proc. 2005
International Conference on Acoustics, Speech and
Signal Processing (ICASSP 2005), March.
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2007a. Information-theoretic aspects of iterative
denoising. Submitted to the Journal of Machine
Learning Research, February.
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2007b. Iterative denoising using Jensen-Re?nyi diver-
gences with an application to unsupervised document
categorization. In Proc. 2007 International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP 2007), April.
K. Lang. 1995. Learning to filter netnews. In Proc. 13th
Int. Conf. on Machine Learning, pages 331?339.
David D. Lewis and Philip J. Hayes. 1994. Guest
editorial. ACM Transactions on Information Systems,
12(3):231, July.
C. E. Priebe, D. J. Marchette, and D. M. Healy.
2004a. Integrated sensing and processing decision
trees. IEEE Trans. on Pat. Anal. and Mach. Intel.,
26(6):699?708, June.
C. E. Priebe, D. J. Marchette, Y. Park, E. Wegman,
J. Solka, D. Socolinsky, D. Karakos, K. Church,
R. Guglielmi, R. Coifman, D. Lin, D. Healy, M. Ja-
cobs, and A. Tsao. 2004b. Iterative denoising for
cross-corpus discovery. In Proc. 2004 International
Symposium on Computational Statistics (COMPSTAT
2004), August.
N. Slonim and N. Tishby. 2000. Document clustering
using word clusters via the information bottleneck
method. In Research and Development in Information
Retrieval, pages 208?215.
N. Slonim, N. Friedman, and N. Tishby. 2002. Un-
supervised document classification using sequential
information maximization. In Proc. SIGIR?02, 25th
ACM Int. Conf. on Research and Development of
Inform. Retrieval.
N. Slonim. 2003. IBA 1.0: Matlab code for information
bottleneck clustering algorithms. Available from
http://www.princeton.edu/?nslonim/IB Release1.0/
IB Release1 0.tar.
N. Tishby, F. Pereira, and W. Bialek. 1999. The informa-
tion bottleneck method. In 37th Allerton Conference
on Communication and Computation.
K. Torkkola. 2002. On feature extraction by mutual in-
formation maximization. In Proc. IEEE Int. Conf. on
Acoustics, Speech and Signal Proc. (ICASSP-2002),
May.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Intl. Conf.
on Machine Learning (ICML-97), pages 412?420.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. 33rd
Annual Meeting of the Association for Computational
Linguistics, pages 189?196, Cambridge, MA.
259
Proceedings of NAACL HLT 2007, pages 260?267,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Using ?Annotator Rationales? to Improve
Machine Learning for Text Categorization?
Omar F. Zaidan and Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,jason}@cs.jhu.edu
Christine D. Piatko
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723 USA
christine.piatko@jhuapl.edu
Abstract
We propose a new framework for supervised ma-
chine learning. Our goal is to learn from smaller
amounts of supervised training data, by collecting a
richer kind of training data: annotations with ?ra-
tionales.? When annotating an example, the hu-
man teacher will also highlight evidence support-
ing this annotation?thereby teaching the machine
learner why the example belongs to the category. We
provide some rationale-annotated data and present a
learning method that exploits the rationales during
training to boost performance significantly on a sam-
ple task, namely sentiment classification of movie
reviews. We hypothesize that in some situations,
providing rationales is a more fruitful use of an an-
notator?s time than annotating more examples.
1 Introduction
Annotation cost is a bottleneck for many natural lan-
guage processing applications. While supervised
machine learning systems are effective, it is labor-
intensive and expensive to construct the many train-
ing examples needed. Previous research has ex-
plored active or semi-supervised learning as possible
ways to lessen this burden.
We propose a new way of breaking this annotation
bottleneck. Annotators currently indicate what the
correct answers are on training data. We propose
that they should also indicate why, at least by coarse
hints. We suggest new machine learning approaches
that can benefit from this ?why? information.
For example, an annotator who is categorizing
phrases or documents might also be asked to high-
light a few substrings that significantly influenced
her judgment. We call such clues ?rationales.? They
need not correspond to machine learning features.
?This work was supported by the JHU WSE/APL Partner-
ship Fund; National Science Foundation grant No. 0347822 to
the second author; and an APL Hafstad Fellowship to the third.
In some circumstances, rationales should not be
too expensive or time-consuming to collect. As long
as the annotator is spending the time to study exam-
ple xi and classify it, it may not require much extra
effort for her to mark reasons for her classification.
2 Using Rationales to Aid Learning
We will not rely exclusively on the rationales, but
use them only as an added source of information.
The idea is to help direct the learning algorithm?s
attention?helping it tease apart signal from noise.
Machine learning algorithms face a well-known
?credit assignment? problem. Given a complex da-
tum xi and the desired response yi, many features of
xi could be responsible for the choice of yi. The
learning algorithm must tease out which features
were actually responsible. This requires a lot of
training data, and often a lot of computation as well.
Our rationales offer a shortcut to solving this
?credit assignment? problem, by providing the
learning algorithm with hints as to which features
of xi were relevant. Rationales should help guide
the learning algorithm toward the correct classifica-
tion function, by pushing it toward a function that
correctly pays attention to each example?s relevant
features. This should help the algorithm learn from
less data and avoid getting trapped in local maxima.1
In this paper, we demonstrate the ?annotator ra-
tionales? technique on a text categorization problem
previously studied by others.
1To understand the local maximum issue, consider the hard
problem of training a standard 3-layer feed-forward neural net-
work. If the activations of the ?hidden? layer?s features (nodes)
were observed at training time, then the network would de-
compose into a pair of independent 2-layer perceptrons. This
turns an NP-hard problem with local maxima (Blum and Rivest,
1992) to a polytime-solvable convex problem. Although ratio-
nales might only provide indirect evidence of the hidden layer,
this would still modify the objective function (see section 8) in
a way that tended to make the correct weights easier to discover.
260
3 Discriminative Approach
One popular approach for text categorization is to
use a discriminative model such as a Support Vec-
tor Machine (SVM) (e.g. (Joachims, 1998; Dumais,
1998)). We propose that SVM training can in gen-
eral incorporate annotator rationales as follows.
From the rationale annotations on a positive ex-
ample ??xi , we will construct one or more ?not-quite-
as-positive? contrast examples ??vij . In our text cat-
egorization experiments below, each contrast docu-
ment ??vij was obtained by starting with the original
and ?masking out? one or all of the several rationale
substrings that the annotator had highlighted (rij).
The intuition is that the correct model should be less
sure of a positive classification on the contrast exam-
ple ??vij than on the original example ~xi, because
??vij
lacks evidence that the annotator found significant.
We can translate this intuition into additional con-
straints on the correct model, i.e., on the weight vec-
tor ~w. In addition to the usual SVM constraint on
positive examples that ~w ? ??xi ? 1, we also want (for
each j) that ~w ? ~xi ? ~w ?
??vij ? ?, where ? ? 0 con-
trols the size of the desired margin between original
and contrast examples.
An ordinary soft-margin SVM chooses ~w and ~? to
minimize
1
2
?~w?2 + C(
?
i
?i) (1)
subject to the constraints
(?i) ~w ? ??xi ? yi ? 1? ?i (2)
(?i) ?i ? 0 (3)
where ??xi is a training example, yi ? {?1,+1} is
its desired classification, and ?i is a slack variable
that allows training example ??xi to miss satisfying
the margin constraint if necessary. The parameter
C > 0 controls the cost of taking such slack, and
should generally be lower for noisier or less linearly
separable datasets. We add the contrast constraints
(?i, j) ~w ? (??xi ?
??vij) ? yi ? ?(1? ?ij), (4)
where ??vij is one of the contrast examples con-
structed from example ??xi , and ?ij ? 0 is an asso-
ciated slack variable. Just as these extra constraints
have their own margin ?, their slack variables have
their own cost, so the objective function (1) becomes
1
2
?~w?2 + C(
?
i
?i) + Ccontrast(
?
i,j
?ij) (5)
The parameter Ccontrast ? 0 determines the impor-
tance of satisfying the contrast constraints. It should
generally be less than C if the contrasts are noisier
than the training examples.2
In practice, it is possible to solve this optimization
using a standard soft-margin SVM learner. Dividing
equation (4) through by ?, it becomes
(?i, j) ~w ? ??xij ? yi ? 1? ?ij , (6)
where ??xij
def
=
??xi?
??vij
? . Since equation (6) takes
the same form as equation (2), we simply add the
pairs (??xij , yi) to the training set as pseudoexam-
ples, weighted by Ccontrast rather than C so that the
learner will use the objective function (5).
There is one subtlety. To allow a biased hyper-
plane, we use the usual trick of prepending a 1 el-
ement to each training example. Thus we require
~w ? (1,??xi) ? 1 ? ?i (which makes w0 play the
role of a bias term). This means, however, that we
must prepend a 0 element to each pseudoexample:
~w ? (1,~xi)?(1,
??vij)
? = ~w ? (0,
??xij) ? 1? ?ij .
In our experiments, we optimize ?, C, and
Ccontrast on held-out data (see section 5.2).
4 Rationale Annotation for Movie Reviews
In order to demonstrate that annotator rationales
help machine learning, we needed annotated data
that included rationales for the annotations.
We chose a dataset that would be enjoyable to re-
annotate: the movie review dataset of (Pang et al,
2002; Pang and Lee, 2004).3 The dataset consists
of 1000 positive and 1000 negative movie reviews
obtained from the Internet Movie Database (IMDb)
review archive, all written before 2002 by a total of
312 authors, with a cap of 20 reviews per author per
2Taking Ccontrast to be constant means that all rationales
are equally valuable. One might instead choose, for example,
to reduce Ccontrast for examples xi that have many rationales,
to prevent xi?s contrast examples vij from together dominating
the optimization. However, in this paper we assume that an xi
with more rationales really does provide more evidence about
the true classifier ~w.
3Polarity dataset version 2.0.
261
category. Pang and Lee have divided the 2000 docu-
ments into 10 folds, each consisting of 100 positive
reviews and 100 negative reviews.
The dataset is arguably artificial in that it keeps
only reviews where the reviewer provided a rather
high or rather low numerical rating, allowing Pang
and Lee to designate the review as positive or neg-
ative. Nonetheless, most reviews contain a difficult
mix of praise, criticism, and factual description. In
fact, it is possible for a mostly critical review to give
a positive overall recommendation, or vice versa.
4.1 Annotation procedure
Rationale annotators were given guidelines4 that
read, in part:
Each review was intended to give either a positive or a neg-
ative overall recommendation. You will be asked to justify why
a review is positive or negative. To justify why a review is posi-
tive, highlight the most important words and phrases that would
tell someone to see the movie. To justify why a review is nega-
tive, highlight words and phrases that would tell someone not to
see the movie. These words and phrases are called rationales.
You can highlight the rationales as you notice them, which
should result in several rationales per review. Do your best to
mark enough rationales to provide convincing support for the
class of interest.
You do not need to go out of your way to mark everything.
You are probably doing too much work if you find yourself go-
ing back to a paragraph to look for even more rationales in it.
Furthermore, it is perfectly acceptable to skim through sections
that you feel would not contain many rationales, such as a re-
viewer?s plot summary, even if that might cause you to miss a
rationale here and there.
The last two paragraphs were intended to provide
some guidance on how many rationales to annotate.
Even so, as section 4.2 shows, some annotators were
considerably more thorough (and slower).
Annotators were also shown the following exam-
ples5 of positive rationales:
? you will enjoy the hell out of American Pie.
? fortunately, they managed to do it in an interesting and
funny way.
? he is one of the most exciting martial artists on the big
screen, continuing to perform his own stunts and daz-
zling audiences with his flashy kicks and punches.
? the romance was enchanting.
and the following examples5 of negative rationales:
4Available at http://cs.jhu.edu/?ozaidan/rationales.
5For our controlled study of annotation time (section 4.2),
different examples were given with full document context.
Figure 1: Histograms of rationale counts per document (A0?s
annotations). The overall mean of 8.55 is close to that of the
four annotators in Table 1. The median and mode are 8 and 7.
? A woman in peril. A confrontation. An explosion. The
end. Yawn. Yawn. Yawn.
? when a film makes watching Eddie Murphy a tedious ex-
perience, you know something is terribly wrong.
? the movie is so badly put together that even the most
casual viewer may notice themiserable pacing and stray
plot threads.
? don?t go see this movie
The annotation involves boldfacing the rationale
phrases using an HTML editor. Note that a fancier
annotation tool would be necessary for a task like
named entity tagging, where an annotator must mark
many named entities in a single document. At any
given moment, such a tool should allow the annota-
tor to highlight, view, and edit only the several ra-
tionales for the ?current? annotated entity (the one
most recently annotated or re-selected).
One of the authors (A0) annotated folds 0?8 of
the movie review set (1,800 documents) with ra-
tionales that supported the gold-standard classifica-
tions. This training/development set was used for
all of the learning experiments in sections 5?6. A
histogram of rationale counts is shown in Figure 1.
As mentioned in section 3, the rationale annotations
were just textual substrings. The annotator did not
require knowledge of the classifier features. Thus,
our rationale dataset is a new resource4 that could
also be used to study exploitation of rationales un-
der feature sets or learning methods other than those
considered here (see section 8).
4.2 Inter-annotator agreement
To study the annotation process, we randomly se-
lected 150 documents from the dataset. The doc-
262
Rationales % rationales also % rationales also % rationales also % rationales also % rationales also
per document annotated by A1 annotated by A2 annotated by AX annotated by AY ann. by anyone else
A1 5.02 (100) 69.6 63.0 80.1 91.4
A2 10.14 42.3 (100) 50.2 67.8 80.9
AX 6.52 49.0 68.0 (100) 79.9 90.9
AY 11.36 39.7 56.2 49.3 (100) 75.5
Table 1: Average number of rationales and inter-annotator agreement for Tasks 2 and 3. A rationale by Ai (?I think this is a great
movie!?) is considered to have been annotated also by Aj if at least one of Aj?s rationales overlaps it (?I think this is a great
movie!?). In computing pairwise agreement on rationales, we ignored documents where Ai and Aj disagreed on the class. Notice
that the most thorough annotatorAY caught most rationales marked by the others (exhibiting high ?recall?), and that most rationales
enjoyed some degree of consensus, especially those marked by the least thorough annotator A1 (exhibiting high ?precision?).
uments were split into three groups, each consisting
of 50 documents (25 positive and 25 negative). Each
subset was used for one of three tasks:6
? Task 1: Given the document, annotate only the
class (positive/negative).
? Task 2: Given the document and its class, an-
notate some rationales for that class.
? Task 3: Given the document, annotate both the
class and some rationales for it.
We carried out a pilot study (annotators AX and
AY: two of the authors) and a later, more controlled
study (annotators A1 and A2: paid students). The
latter was conducted in a more controlled environ-
ment where both annotators used the same annota-
tion tool and annotation setup as each other. Their
guidelines were also more detailed (see section 4.1).
In addition, the documents for the different tasks
were interleaved to avoid any practice effect.
The annotators? classification accuracies in Tasks
1 and 3 (against Pang & Lee?s labels) ranged from
92%?97%, with 4-way agreement on the class for
89% of the documents, and pairwise agreement also
ranging from 92%?97%. Table 1 shows how many
rationales the annotators provided and how well
their rationales agreed.
Interestingly, in Task 3, four of AX?s ratio-
nales for a positive class were also partially
highlighted by AY as support for AY?s (incorrect)
negative classifications, such as:
6Each task also had a ?warmup? set of 10 documents to be
annotated before that tasks?s 50 documents. Documents for
Tasks 2 and 3 would automatically open in an HTML editor
while Task 1 documents opened in an HTML viewer with no
editing option. The annotators recorded their classifications for
Tasks 1 and 3 on a spreadsheet.
min./KB A1 time A2 time AX time AY time
Task 1 0.252 0.112 0.150 0.422
Task 2 0.396 0.537 0.242 0.626
Task 3 0.399 0.505 0.288 1.01
min./doc. A1 time A2 time AX time AY time
Task 1 1.04 0.460 0.612 1.73
min./rat. A1 time A2 time AX time AY time
Task 2 0.340 0.239 0.179 0.298
Task 3 0.333 0.198 0.166 0.302
Table 2: Average annotation rates on each task.
? Even with its numerous flaws, the movie all comes to-
gether, if only for those who . . .
? ?Beloved? acts like an incredibly difficult chamber
drama paired with a ghost story.
4.3 Annotation time
Average annotation times are in Table 2. As hoped,
rationales did not take too much extra time for most
annotators to provide. For each annotator except
A2, providing rationales only took roughly twice the
time (Task 3 vs. Task 1), even though it meant mark-
ing an average of 5?11 rationales in addition to the
class.
Why this low overhead? Because marking the
class already required the Task 1 annotator to read
the document and find some rationales, even if s/he
did not mark them. The only extra work in Task 3
is in making them explicit. This synergy between
class annotation and rationale annotation is demon-
strated by the fact that doing both at once (Task 3)
was faster than doing them separately (Tasks 1+2).
We remark that this task?binary classification on
full documents?seems to be almost a worst-case
scenario for the annotation of rationales. At a purely
mechanical level, it was rather heroic of A0 to at-
tach 8?9 new rationale phrases rij to every bit yi
of ordinary annotation. Imagine by contrast a more
local task of identifying entities or relations. Each
263
lower-level annotation yi will tend to have fewer ra-
tionales rij , while yi itself will be more complex and
hence more difficult to mark. Thus, we expect that
the overhead of collecting rationales will be less in
many scenarios than the factor of 2 we measured.
Annotation overhead could be further reduced.
For a multi-class problem like relation detection, one
could ask the annotator to provide rationales only for
the rarer classes. This small amount of extra time
where the data is sparsest would provide extra guid-
ance where it was most needed. Another possibility
is passive collection of rationales via eye tracking.
5 Experimental Procedures
5.1 Feature extraction
Although this dataset seems to demand discourse-
level features that contextualize bits of praise and
criticism, we exactly follow Pang et al (2002) and
Pang and Lee (2004) in merely using binary uni-
gram features, corresponding to the 17,744 un-
stemmed word or punctuation types with count ? 4
in the full 2000-document corpus. Thus, each docu-
ment is reduced to a 0-1 vector with 17,744 dimen-
sions, which is then normalized to unit length.7
We used the method of section 3 to place addi-
tional constraints on a linear classifier. Given a train-
ing document, we create several contrast documents,
each by deleting exactly one rationale substring
from the training document. Converting documents
to feature vectors, we obtained an original exam-
ple ??xi and several contrast examples
??vi1,
??vi2, . . ..8
Again, our training method required each original
document to be classified more confidently (by a
margin ?) than its contrast documents.
If we were using more than unigram features, then
simply deleting a rationale substring would not al-
ways be the best way to create a contrast document,
as the resulting ungrammatical sentences might
cause deep feature extraction to behave strangely
(e.g., parse errors during preprocessing). The goal in
creating the contrast document is merely to suppress
7The vectors are normalized before prepending the 1 corre-
sponding to the bias term feature (mentioned in section 3).
8The contrast examples were not normalized to precisely
unit length, but instead were normalized by the same factor used
to normalize ??xi . This conveniently ensured that the pseudoex-
amples ??xij
def
=
~xi?
??vij
? were sparse vectors, with 0 coordinates
for all words not in the jth rationale.
features (n-grams, parts of speech, syntactic depen-
dencies . . . ) that depend in part on material in one
or more rationales. This could be done directly by
modifying the feature extractors, or if one prefers to
use existing feature extractors, by ?masking? rather
than deleting the rationale substring?e.g., replacing
each of its word tokens with a special MASK token
that is treated as an out-of-vocabulary word.
5.2 Training and testing procedures
We transformed this problem to an SVM problem
(see section 3) and applied SVMlight for training and
testing, using the default linear kernel. We used only
A0?s rationales and the true classifications.
Fold 9 was reserved as a test set. All accuracy
results reported in the paper are the result of testing
on fold 9, after training on subsets of folds 0?8.
Our learning curves show accuracy after training
on T < 9 folds (i.e., 200T documents), for various
T . To reduce the noise in these results, the accuracy
we report for training on T folds is actually the aver-
age of 9 different experiments with different (albeit
overlapping) training sets that cover folds 0?8:
1
9
8?
i=0
acc(F9 | ?
?, Fi+1 ? . . . ? Fi+T ) (7)
where Fj denotes the fold numbered j mod 9, and
acc(Z | ?, Y ) means classification accuracy on the
set Z after training on Y with hyperparameters ?.
To evaluate whether two different training meth-
ods A and B gave significantly different average-
accuracy values, we used a paired permutation test
(generalizing a sign test). The test assumes in-
dependence among the 200 test examples but not
among the 9 overlapping training sets. For each
of the 200 test examples in fold 9, we measured
(ai, bi), where ai (respectively bi) is the number
of the 9 training sets under which A (respectively
B) classified the example correctly. The p value
is the probability that the absolute difference be-
tween the average-accuracy values would reach or
exceed the observed absolute difference, namely
| 1200
?200
i=1
ai?bi
9 |, if each (ai, bi) had an independent
1/2 chance of being replaced with (bi, ai), as per the
null hypothesis that A and B are indistinguishable.
For any given value of T and any given train-
ing method, we chose hyperparameters ?? =
264
Figure 2: Classification accuracy under five different experi-
mental setups (S1?S5). At each training size, the 5 accura-
cies are pairwise significantly different (paired permutation test,
p < 0.02; see section 5.2), except for {S3,S4} or {S4,S5} at
some sizes.
(C, ?,Ccontrast) to maximize the following cross-
validation performance:9
?? = argmax
?
8?
i=0
acc(Fi | ?, Fi+1 ? . . . ? Fi+T )
(8)
We used a simple alternating optimization procedure
that begins at ?0 = (1.0, 1.0, 1.0) and cycles repeat-
edly through the three dimensions, optimizing along
each dimension by a local grid search with resolu-
tion 0.1.10 Of course, when training without ratio-
nales, we did not have to optimize ? or Ccontrast.
6 Experimental Results
6.1 The value of rationales
The top curve (S1) in Figure 2 shows that perfor-
mance does increase when we introduce rationales
for the training examples as contrast examples (sec-
tion 3). S1 is significantly higher than the baseline
curve (S2) immediately below it, which trains an or-
dinary SVM classifier without using rationales. At
the largest training set size, rationales raise the accu-
racy from 88.5% to 92.2%, a 32% error reduction.
9One might obtain better performance (across all methods
being compared) by choosing a separate ?? for each of the 9
training sets. However, to simulate real limited-data training
conditions, one should then find the ?? for each {i, ..., j} us-
ing a separate cross-validation within {i, ..., j} only; this would
slow down the experiments considerably.
10For optimizing along the C dimension, one could use the
efficient method of Beineke et al (2004), but not in SVMlight.
The lower three curves (S3?S5) show that learn-
ing is separately helped by the rationale and the
non-rationale portions of the documents. S3?S5
are degraded versions of the baseline S2: they are
ordinary SVM classifiers that perform significantly
worse than S2 (p < 0.001).
Removing the rationale phrases from the train-
ing documents (S3) made the test documents much
harder to discriminate (compared to S2). This sug-
gests that annotator A0?s rationales often covered
most of the usable evidence for the true class.
However, the pieces to solving the classification
puzzle cannot be found solely in the short rationale
phrases. Removing all non-rationale text from the
training documents (S5) was even worse than re-
moving the rationales (S3). In other words, we can-
not hope to do well simply by training on just the
rationales (S5), although that approach is improved
somewhat in S4 by treating each rationale (similarly
to S1) as a separate SVM training example.
This presents some insight into why our method
gives the best performance. The classifier in S1
is able to extract subtle patterns from the corpus,
like S2, S3, or any other standard machine learn-
ing method, but it is also able to learn from a human
annotator?s decision-making strategy.
6.2 Using fewer rationales
In practice, one might annotate rationales for only
some training documents?either when annotating a
new corpus or when adding rationales post hoc to
an existing corpus. Thus, a range of options can be
found between curves S2 and S1 of Figure 2.
Figure 3 explores this space, showing how far the
learning curve S2 moves upward if one has time to
annotate rationales for a fixed number of documents
R. The key useful discovery is that much of the ben-
efit can actually be obtained with relatively few ra-
tionales. For example, with 800 training documents,
annotating (0%, 50%, 100%) of themwith rationales
gives accuracies of (86.9%, 89.2%, 89.3%). With
the maximum of 1600 training documents, annotat-
ing (0%, 50%, 100%) with rationales gives (88.5%,
91.7%, 92.2%).
To make this point more broadly, we find that the
R = 200 curve is significantly above the R = 0
curve (p < 0.05) at all T ? 1200. By contrast, the
R = 800, R = 1000, . . . R = 1600 points at each T
265
Figure 3: Classification accuracy for T ? {200, 400, ..., 1600}
training documents (x-axis) when only R ? {0, 200, ..., T} of
them are annotated with rationales (different curves). The R =
0 curve above corresponds to the baseline S2 from Figure 2.
S1?s points are found above as the leftmost points on the other
curves, where R = T .
value are all-pairs statistically indistinguishable.
The figure also suggests that rationales and docu-
ments may be somewhat orthogonal in their benefit.
When one has many documents and few rationales,
there is no longer much benefit in adding more doc-
uments (the curve is flattening out), but adding more
rationales seems to provide a fresh benefit: ratio-
nales have not yet reached their point of diminishing
returns. (While this fresh benefit was often statisti-
cally significant, and greater than the benefit from
more documents, our experiments did not establish
that it was significantly greater.)
The above experiments keep all of A0?s rationales
on a fraction of training documents. We also exper-
imented with keeping a fraction of A0?s rationales
(chosen randomly with randomized rounding) on all
training documents. This yielded no noteworthy or
statistically significant differences from Figure 3.
These latter experiments simulate a ?lazy annota-
tor? who is less assiduous than A0. Such annotators
may be common in the real world. We also suspect
that they will be more desirable. First, they should
be able to add more rationales per hour than the A0-
style annotator from Figure 3: some rationales are
simply more noticeable than others, and a lazy anno-
tator will quickly find the most noticeable ones with-
out wasting time tracking down the rest. Second, the
?most noticeable? rationales that they mark may be
the most effective ones for learning, although our
random simulation of laziness could not test that.
7 Related Work
Our rationales resemble ?side information? in ma-
chine learning?supplementary information about
the target function that is available at training time.
Side information is sometimes encoded as ?virtual
examples? like our contrast examples or pseudoex-
amples. However, past work generates these by
automatically transforming the training examples
in ways that are expected to preserve or alter the
classification (Abu-Mostafa, 1995). In another for-
mulation, virtual examples are automatically gener-
ated but must be manually annotated (Kuusela and
Ocone, 2004). Our approach differs because a hu-
man helps to generate the virtual examples. Enforc-
ing a margin between ordinary examples and con-
trast examples also appears new.
Other researchers have considered how to reduce
annotation effort. In active learning, the annotator
classifies only documents where the system so far is
less confident (Lewis and Gale, 1994), or in an in-
formation extraction setting, incrementally corrects
details of the system?s less confident entity segmen-
tations and labelings (Culotta andMcCallum, 2005).
Raghavan et al (2005) asked annotators to iden-
tify globally ?relevant? features. In contrast, our ap-
proach does not force the annotator to evaluate the
importance of features individually, nor in a global
context outside any specific document, nor even to
know the learner?s feature space. Annotators only
mark text that supports their classification decision.
Our methods then consider the combined effect of
this text on the feature vector, which may include
complex features not known to the annotator.
8 Future Work: Generative models
Our SVM contrast method (section 3) is not the only
possible way to use rationales. We would like to ex-
plicitly model rationale annotation as a noisy pro-
cess that reflects, imperfectly and incompletely, the
annotator?s internal decision procedure.
A natural approach would start with log-linear
models in place of SVMs. We can define a proba-
bilistic classifier
p?(y | x)
def
=
1
Z(x)
exp
k?
h=1
?hfh(x, y) (9)
266
where ~f(?) extracts a feature vector from a classified
document.
A standard training method would be to choose ?
to maximize the conditional likelihood of the train-
ing classifications:
argmax
~?
n?
i=1
p?(yi | xi) (10)
When a rationale ri is also available for each
(xi, yi), we propose to maximize a likelihood that
tries to predict these rationale data as well:
argmax
~?
n?
i=1
p?(yi | xi) ? p??(ri | xi, yi, ?) (11)
Notice that a given guess of ? might make equa-
tion (10) large, yet accord badly with the annotator?s
rationales. In that case, the second term of equa-
tion (11) will exert pressure on ? to change to some-
thing that conforms more closely to the rationales.
If the annotator is correct, such a ? will generalize
better beyond the training data.
In equation (11), p?? models the stochastic process
of rationale annotation. What is an annotator actu-
ally doing when she annotates rationales? In par-
ticular, how do her rationales derive from the true
value of ? and thereby tell us about ?? Building a
good model p?? of rationale annotation will require
some exploratory data analysis. Roughly, we expect
that if ?hfh(xi, y) is much higher for y = yi than
for other values of y, then the annotator?s ri is corre-
spondingly more likely to indicate in some way that
feature fh strongly influenced annotation yi. How-
ever, we must also model the annotator?s limited pa-
tience (she may not annotate all important features),
sloppiness (she may indicate only indirectly that fh
is important), and bias (tendency to annotate some
kinds of features at the expense of others).
One advantage of this generative approach is that
it eliminates the need for contrast examples. Con-
sider a non-textual example in which an annotator
highlights the line crossing in a digital image of the
digit ?8? to mark the rationale that distinguishes it
from ?0.? In this case it is not clear how to mask out
that highlighted rationale to create a contrast exam-
ple in which relevant features would not fire.11
11One cannot simply flip those highlighted pixels to white
9 Conclusions
We have proposed a quite simple approach to im-
proving machine learning by exploiting the clever-
ness of annotators, asking them to provide enriched
annotations for training. We developed and tested
a particular discriminative method that can use ?an-
notator rationales??even on a fraction of the train-
ing set?to significantly improve sentiment classifi-
cation of movie reviews.
We found fairly good annotator agreement on the
rationales themselves. Most annotators provided
several rationales per classification without taking
too much extra time, even in our text classification
scenario, where the rationales greatly outweigh the
classifications in number and complexity. Greater
speed might be possible through an improved user
interface or passive feedback (e.g., eye tracking).
In principle, many machine learning methods
might be modified to exploit rationale data. While
our experiments in this paper used a discriminative
SVM, we plan to explore generative approaches.
References
Y. S. Abu-Mostafa. 1995. Hints. Neural Computation, 7:639?
671, July.
P. Beineke, T. Hastie, and S. Vaithyanathan. 2004. The sen-
timental factor: Improving review classification via human-
provided information. In Proc. of ACL, pages 263?270.
A. L. Blum and R. L. Rivest. 1992. Training a 3-node neural
network is NP-complete. Neural Networks, 5(1):117?127.
A. Culotta and A. McCallum. 2005. Reducing labeling effort
for structured prediction tasks. In AAAI, pages 746?751.
S. Dumais. 1998. Using SVMs for text categorization. IEEE
Intelligent Systems Magazine, 13(4), July/August.
T. Joachims. 1998. Text categorization with support vector
machines: Learning with many relevant features. In Proc. of
the European Conf. on Machine Learning, pages 137?142.
P. Kuusela and D. Ocone. 2004. Learning with side informa-
tion: PAC learning bounds. J. of Computer and System Sci-
ences, 68(3):521?545, May.
D. D. Lewis and W. A. Gale. 1994. A sequential algorithm for
training text classifiers. In Proc. of ACM-SIGIR, pages 3?12.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proc. of ACL, pages 271?278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine learning techniques.
In Proc. of EMNLP, pages 79?86.
H. Raghavan, O. Madani, and R. Jones. 2005. Interactive fea-
ture selection. In Proc. of IJCAI, pages 41?46.
or black, since that would cause new features to fire. Possibly
one could simply suppress any feature that depends in any way
on the highlighted pixels, but this would take away too many
important features, including global features.
267
Parameter Estimation for Probabilistic Finite-State Transducers?
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD, USA 21218-2691
jason@cs.jhu.edu
Abstract
Weighted finite-state transducers suffer from the lack of a train-
ing algorithm. Training is even harder for transducers that have
been assembled via finite-state operations such as composition,
minimization, union, concatenation, and closure, as this yields
tricky parameter tying. We formulate a ?parameterized FST?
paradigm and give training algorithms for it, including a gen-
eral bookkeeping trick (?expectation semirings?) that cleanly
and efficiently computes expectations and gradients.
1 Background and Motivation
Rational relations on strings have become wide-
spread in language and speech engineering (Roche
and Schabes, 1997). Despite bounded memory they
are well-suited to describe many linguistic and tex-
tual processes, either exactly or approximately.
A relation is a set of (input, output) pairs. Re-
lations are more general than functions because they
may pair a given input string with more or fewer than
one output string.
The class of so-called rational relations admits
a nice declarative programming paradigm. Source
code describing the relation (a regular expression)
is compiled into efficient object code (in the form
of a 2-tape automaton called a finite-state trans-
ducer). The object code can even be optimized for
runtime and code size (via algorithms such as deter-
minization and minimization of transducers).
This programming paradigm supports efficient
nondeterminism, including parallel processing over
infinite sets of input strings, and even allows ?re-
verse? computation from output to input. Its unusual
flexibility for the practiced programmer stems from
the many operations under which rational relations
are closed. It is common to define further useful
operations (as macros), which modify existing rela-
tions not by editing their source code but simply by
operating on them ?from outside.?
?A brief version of this work, with some additional mate-
rial, first appeared as (Eisner, 2001a). A leisurely journal-length
version with more details has been prepared and is available.
The entire paradigm has been generalized to
weighted relations, which assign a weight to each
(input, output) pair rather than simply including or
excluding it. If these weights represent probabili-
ties P (input, output) or P (output | input), the
weighted relation is called a joint or conditional
(probabilistic) relation and constitutes a statistical
model. Such models can be efficiently restricted,
manipulated or combined using rational operations
as before. An artificial example will appear in ?2.
The availability of toolkits for this weighted case
(Mohri et al, 1998; van Noord and Gerdemann,
2001) promises to unify much of statistical NLP.
Such tools make it easy to run most current ap-
proaches to statistical markup, chunking, normal-
ization, segmentation, alignment, and noisy-channel
decoding,1 including classic models for speech
recognition (Pereira and Riley, 1997) and machine
translation (Knight and Al-Onaizan, 1998). More-
over, once the models are expressed in the finite-
state framework, it is easy to use operators to tweak
them, to apply them to speech lattices or other sets,
and to combine them with linguistic resources.
Unfortunately, there is a stumbling block: Where
do the weights come from? After all, statistical mod-
els require supervised or unsupervised training. Cur-
rently, finite-state practitioners derive weights using
exogenous training methods, then patch them onto
transducer arcs. Not only do these methods require
additional programming outside the toolkit, but they
are limited to particular kinds of models and train-
ing regimens. For example, the forward-backward
algorithm (Baum, 1972) trains only Hidden Markov
Models, while (Ristad and Yianilos, 1996) trains
only stochastic edit distance.
In short, current finite-state toolkits include no
training algorithms, because none exist for the large
space of statistical models that the toolkits can in
principle describe and run.
1Given output, find input to maximize P (input, output).
                    Computational Linguistics (ACL), Philadelphia, July 2002, pp. 1-8.
                         Proceedings of the 40th Annual Meeting of the Association for
(a) (b)
0/.15
a:x/.63 1/.15a:  /.07?   
2/.5b:  /.003?     b:z/.12
3/.5
b:x/.027
a:  /.7?  b:  /.03?   
b:z/.12 b:  /.1?  b:z/.4
b:  /.01?   
b:z/.4b:x/.09
4/.15
a:p/.7
5/.5b:p/.03b:q/.12
b:p/.1b:q/.4
(c)
6/1
p:x/.9
7/1p:  /.1?  q:z/1
p:  /1? q:z/1
Figure 1: (a) A probabilistic FST defining a joint probability
distribution. (b) A smaller joint distribution. (c) A conditional
distribution. Defining (a)=(b)?(c) means that the weights in (a)
can be altered by adjusting the fewer weights in (b) and (c).
This paper aims to provide a remedy through a
new paradigm, which we call parameterized finite-
state machines. It lays out a fully general approach
for training the weights of weighted rational rela-
tions. First ?2 considers how to parameterize such
models, so that weights are defined in terms of un-
derlying parameters to be learned. ?3 asks what it
means to learn these parameters from training data
(what is to be optimized?), and notes the apparently
formidable bookkeeping involved. ?4 cuts through
the difficulty with a surprisingly simple trick. Fi-
nally, ?5 removes inefficiencies from the basic algo-
rithm, making it suitable for inclusion in an actual
toolkit. Such a toolkit could greatly shorten the de-
velopment cycle in natural language engineering.
2 Transducers and Parameters
Finite-state machines, including finite-state au-
tomata (FSAs) and transducers (FSTs), are a kind
of labeled directed multigraph. For ease and brevity,
we explain them by example. Fig. 1a shows a proba-
bilistic FST with input alphabet ? = {a, b}, output
alphabet ? = {x, z}, and all states final. It may
be regarded as a device for generating a string pair
in ?? ? ?? by a random walk from 0?. Two paths
exist that generate both input aabb and output xz:
0?a:x/.63?? 0?
a:/.07
?? 1?
b:/.03
?? 2?
b:z/.4
?? 2/.5?
0?a:x/.63?? 0?
a:/.07
?? 1?
b:z/.12
?? 2?
b:/.1
?? 2/.5?
Each of the paths has probability .0002646, so
the probability of somehow generating the pair
(aabb, xz) is .0002646 + .0002646 = .0005292.
Abstracting away from the idea of random walks,
arc weights need not be probabilities. Still, define a
path?s weight as the product of its arc weights and
the stopping weight of its final state. Thus Fig. 1a
defines a weighted relation f where f(aabb, xz) =
.0005292. This particular relation does happen to be
probabilistic (see ?1). It represents a joint distribu-
tion (since ?x,y f(x, y) = 1). Meanwhile, Fig. 1c
defines a conditional one (?x?y f(x, y) = 1).
This paper explains how to adjust probability dis-
tributions like that of Fig. 1a so as to model training
data better. The algorithm improves an FST?s nu-
meric weights while leaving its topology fixed.
How many parameters are there to adjust in
Fig. 1a? That is up to the user who built it! An
FST model with few parameters is more constrained,
making optimization easier. Some possibilities:
? Most simply, the algorithm can be asked to tune
the 17 numbers in Fig. 1a separately, subject to the
constraint that the paths retain total probability 1. A
more specific version of the constraint requires the
FST to remain Markovian: each of the 4 states must
present options with total probability 1 (at state 1?,
15+.7+.03.+.12=1). This preserves the random-walk
interpretation and (we will show) entails no loss of
generality. The 4 restrictions leave 13 free params.
? But perhaps Fig. 1a was actually obtained as
the composition of Fig. 1b?c, effectively defin-
ing P (input, output) =
?
mid P (input,mid) ?
P (output | mid). If Fig. 1b?c are required to re-
main Markovian, they have 5 and 1 degrees of free-
dom respectively, so now Fig. 1a has only 6 param-
eters total.2 In general, composing machines mul-
tiplies their arc counts but only adds their param-
eter counts. We wish to optimize just the few un-
derlying parameters, not independently optimize the
many arc weights of the composed machine.
? Perhaps Fig. 1b was itself obtained by the proba-
bilistic regular expression (a : p)??(b : (p +? q))??
with the 3 parameters (?, ?, ?) = (.7, .2, .5). With
? = .1 from footnote 2, the composed machine
2Why does Fig. 1c have only 1 degree of freedom? The
Markovian requirement means something different in Fig. 1c,
which defines a conditional relation P (output | mid) rather
than a joint one. A random walk on Fig. 1c chooses among arcs
with a given input label. So the arcs from state 6? with input
p must have total probability 1 (currently .9+.1). All other arc
choices are forced by the input label and so have probability 1.
The only tunable value is .1 (denote it by ?), with .9 = 1? ?.
(Fig. 1a) has now been described with a total of just
4 parameters!3 Here, probabilistic union E +? F
def
=
?E + (1 ? ?)F means ?flip a ?-weighted coin and
generateE if heads, F if tails.? E??
def
= (?E)?(1??)
means ?repeatedly flip an ?-weighted coin and keep
repeating E as long as it comes up heads.?
These 4 parameters have global effects on Fig. 1a,
thanks to complex parameter tying: arcs 4? b:p?? 5?,
5? b:q?? 5? in Fig. 1b get respective probabilities (1?
?)?? and (1 ? ?)?, which covary with ? and vary
oppositely with ?. Each of these probabilities in turn
affects multiple arcs in the composed FST of Fig. 1a.
We offer a theorem that highlights the broad
applicability of these modeling techniques.4 If
f(input, output) is a weighted regular relation,
then the following statements are equivalent: (1) f is
a joint probabilistic relation; (2) f can be computed
by a Markovian FST that halts with probability 1;
(3) f can be expressed as a probabilistic regexp,
i.e., a regexp built up from atomic expressions a : b
(for a ? ?? {}, b ? ?? {}) using concatenation,
probabilistic union +p, and probabilistic closure ?p.
For defining conditional relations, a good regexp
language is unknown to us, but they can be defined
in several other ways: (1) via FSTs as in Fig. 1c, (2)
by compilation of weighted rewrite rules (Mohri and
Sproat, 1996), (3) by compilation of decision trees
(Sproat and Riley, 1996), (4) as a relation that per-
forms contextual left-to-right replacement of input
substrings by a smaller conditional relation (Gerde-
mann and van Noord, 1999),5 (5) by conditionaliza-
tion of a joint relation as discussed below.
A central technique is to define a joint relation as a
noisy-channel model, by composing a joint relation
with a cascade of one or more conditional relations
as in Fig. 1 (Pereira and Riley, 1997; Knight and
Graehl, 1998). The general form is illustrated by
3Conceptually, the parameters represent the probabilities of
reading another a (?); reading another b (?); transducing b to p
rather than q (?); starting to transduce p to  rather than x (?).
4To prove (1)?(3), express f as an FST and apply the
well-known Kleene-Schu?tzenberger construction (Berstel and
Reutenauer, 1988), taking care to write each regexp in the con-
struction as a constant times a probabilistic regexp. A full proof
is straightforward, as are proofs of (3)?(2), (2)?(1).
5In (4), the randomness is in the smaller relation?s choice of
how to replace a match. One can also get randomness through
the choice of matches, ignoring match possibilities by randomly
deleting markers in Gerdemann and van Noord?s construction.
P (v, z)
def
=
?
w,x,y P (v|w)P (w, x)P (y|x)P (z|y),
implemented by composing 4 machines.6,7
There are also procedures for defining weighted
FSTs that are not probabilistic (Berstel and
Reutenauer, 1988). Arbitrary weights such as 2.7
may be assigned to arcs or sprinkled through a reg-
exp (to be compiled into :/2.7?? arcs). A more subtle
example is weighted FSAs that approximate PCFGs
(Nederhof, 2000; Mohri and Nederhof, 2001), or
to extend the idea, weighted FSTs that approximate
joint or conditional synchronous PCFGs built for
translation. These are parameterized by the PCFG?s
parameters, but add or remove strings of the PCFG
to leave an improper probability distribution.
Fortunately for those techniques, an FST with
positive arc weights can be normalized to make it
jointly or conditionally probabilistic:
? An easy approach is to normalize the options at
each state to make the FST Markovian. Unfortu-
nately, the result may differ for equivalent FSTs that
express the same weighted relation. Undesirable
consequences of this fact have been termed ?label
bias? (Lafferty et al, 2001). Also, in the conditional
case such per-state normalization is only correct if
all states accept all input suffixes (since ?dead ends?
leak probability mass).8
? A better-founded approach is global normal-
ization, which simply divides each f(x, y) by
?
x?,y? f(x
?, y?) (joint case) or by?y? f(x, y?) (con-
ditional case). To implement the joint case, just di-
vide stopping weights by the total weight of all paths
(which ?4 shows how to find), provided this is finite.
In the conditional case, let g be a copy of f with the
output labels removed, so that g(x) finds the desired
divisor; determinize g if possible (but this fails for
some weighted FSAs), replace all weights with their
reciprocals, and compose the result with f .9
6P (w, x) defines the source model, and is often an ?identity
FST? that requires w = x, really just an FSA.
7We propose also using n-tape automata to generalize to
?branching noisy channels? (a case of dendroid distributions).
In
?
w,x P (v|w)P (v
?|w)P (w, x)P (y|x), the true transcrip-
tion w can be triply constrained by observing speech y and two
errorful transcriptions v, v?, which independently depend on w.
8A corresponding problem exists in the joint case, but may
be easily avoided there by first pruning non-coaccessible states.
9It suffices to make g unambiguous (one accepting path per
string), a weaker condition than determinism. When this is not
possible (as in the inverse of Fig. 1b, whose conditionaliza-
Normalization is particularly important because it
enables the use of log-linear (maximum-entropy)
parameterizations. Here one defines each arc
weight, coin weight, or regexp weight in terms of
meaningful features associated by hand with that
arc, coin, etc. Each feature has a strength ? R>0,
and a weight is computed as the product of the
strengths of its features.10 It is now the strengths
that are the learnable parameters. This allows mean-
ingful parameter tying: if certain arcs such as u:i??,
o:e
??, and a:ae?? share a contextual ?vowel-fronting?
feature, then their weights rise and fall together with
the strength of that feature. The resulting machine
must be normalized, either per-state or globally, to
obtain a joint or a conditional distribution as de-
sired. Such approaches have been tried recently
in restricted cases (McCallum et al, 2000; Eisner,
2001b; Lafferty et al, 2001).
Normalization may be postponed and applied in-
stead to the result of combining the FST with other
FSTs by composition, union, concatenation, etc. A
simple example is a probabilistic FSA defined by
normalizing the intersection of other probabilistic
FSAs f1, f2, . . .. (This is in fact a log-linear model
in which the component FSAs define the features:
string x has log fi(x) occurrences of feature i.)
In short, weighted finite-state operators provide a
language for specifying a wide variety of parameter-
ized statistical models. Let us turn to their training.
3 Estimation in Parameterized FSTs
We are primarily concerned with the following train-
ing paradigm, novel in its generality. Let f? :
????? ? R?0 be a joint probabilistic relation that
is computed by a weighted FST. The FST was built
by some recipe that used the parameter vector ?.
Changing ? may require us to rebuild the FST to get
updated weights; this can involve composition, reg-
exp compilation, multiplication of feature strengths,
etc. (Lazy algorithms that compute arcs and states of
tion cannot be realized by any weighted FST), one can some-
times succeed by first intersecting g with a smaller regular set
in which the input being considered is known to fall. In the ex-
treme, if each input string is fully observed (not the case if the
input is bound by composition to the output of a one-to-many
FST), one can succeed by restricting g to each input string in
turn; this amounts to manually dividing f(x, y) by g(x).
10Traditionally log(strength) values are called weights, but
this paper uses ?weight? to mean something else.
8 9a:x/.63 10a:x/.63 11b:x/.027
a:  /.7? 
b:  /.0051?       12/.5b:z/.1284b:  /.1? b:z/.404
b:  /.1? 
Figure 2: The joint model of Fig. 1a constrained to generate
only input ? a(a + b)? and output = xxz.
f? on demand (Mohri et al, 1998) can pay off here,
since only part of f? may be needed subsequently.)
As training data we are given a set of observed
(input, output) pairs, (xi, yi). These are assumed
to be independent random samples from a joint dis-
tribution of the form f??(x, y); the goal is to recover
the true ??. Samples need not be fully observed
(partly supervised training): thus xi ? ??, yi ? ??
may be given as regular sets in which input and out-
put were observed to fall. For example, in ordinary
HMM training, xi = ?? and represents a completely
hidden state sequence (cf. Ristad (1998), who allows
any regular set), while yi is a single string represent-
ing a completely observed emission sequence.11
What to optimize? Maximum-likelihood es-
timation guesses ?? to be the ? maximizing
?
i f?(xi, yi). Maximum-posterior estimation
tries to maximize P (?) ?
?
i f?(xi, yi) where P (?) is
a prior probability. In a log-linear parameterization,
for example, a prior that penalizes feature strengths
far from 1 can be used to do feature selection and
avoid overfitting (Chen and Rosenfeld, 1999).
The EM algorithm (Dempster et al, 1977) can
maximize these functions. Roughly, the E step
guesses hidden information: if (xi, yi) was gener-
ated from the current f?, which FST paths stand a
chance of having been the path used? (Guessing the
path also guesses the exact input and output.) The
M step updates ? to make those paths more likely.
EM alternates these steps and converges to a local
optimum. The M step?s form depends on the param-
eterization and the E step serves the M step?s needs.
Let f? be Fig. 1a and suppose (xi, yi) = (a(a +
b)?, xxz). During the E step, we restrict to paths
compatible with this observation by computing xi ?
f? ? yi, shown in Fig. 2. To find each path?s pos-
terior probability given the observation (xi, yi), just
conditionalize: divide its raw probability by the total
probability (? 0.1003) of all paths in Fig. 2.
11To implement an HMM by an FST, compose a probabilistic
FSA that generates a state sequence of the HMM with a condi-
tional FST that transduces HMM states to emitted symbols.
But that is not the full E step. The M step uses
not individual path probabilities (Fig. 2 has infinitely
many) but expected counts derived from the paths.
Crucially, ?4 will show how the E step can accumu-
late these counts effortlessly. We first explain their
use by the M step, repeating the presentation of ?2:
? If the parameters are the 17 weights in Fig. 1a, the
M step reestimates the probabilities of the arcs from
each state to be proportional to the expected number
of traversals of each arc (normalizing at each state
to make the FST Markovian). So the E step must
count traversals. This requires mapping Fig. 2 back
onto Fig. 1a: to traverse either 8? a:x?? 9? or 9? a:x?? 10?
in Fig. 2 is ?really? to traverse 0? a:x?? 0? in Fig. 1a.
? If Fig. 1a was built by composition, the M step
is similar but needs the expected traversals of the
arcs in Fig. 1b?c. This requires further unwinding of
Fig. 1a?s 0? a:x?? 0?: to traverse that arc is ?really? to
traverse Fig. 1b?s 4? a:p?? 4? and Fig. 1c?s 6? p:x?? 6?.
? If Fig. 1b was defined by the regexp given earlier,
traversing 4? a:p?? 4? is in turn ?really? just evidence
that the ?-coin came up heads. To learn the weights
?, ?, ?, ?, count expected heads/tails for each coin.
? If arc probabilities (or even ?, ?, ?, ?) have log-
linear parameterization, then the E step must com-
pute c =
?
i ecf (xi, yi), where ec(x, y) denotes
the expected vector of total feature counts along a
random path in f? whose (input, output) matches
(x, y). The M step then treats c as fixed, observed
data and adjusts ? until the predicted vector of to-
tal feature counts equals c, using Improved Itera-
tive Scaling (Della Pietra et al, 1997; Chen and
Rosenfeld, 1999).12 For globally normalized, joint
models, the predicted vector is ecf (??,??). If the
log-linear probabilities are conditioned on the state
and/or the input, the predicted vector is harder to de-
scribe (though usually much easier to compute).13
12IIS is itself iterative; to avoid nested loops, run only one it-
eration at each M step, giving a GEM algorithm (Riezler, 1999).
Alternatively, discard EM and use gradient-based optimization.
13For per-state conditional normalization, let Dj,a be the set
of arcs from state j with input symbol a ? ?; their weights are
normalized to sum to 1. Besides computing c, the E step must
count the expected number dj,a of traversals of arcs in each
Dj,a. Then the predicted vector given ? is
?
j,a dj,a ?(expected
feature counts on a randomly chosen arc in Dj,a). Per-state
joint normalization (Eisner, 2001b, ?8.2) is similar but drops the
dependence on a. The difficult case is global conditional nor-
malization. It arises, for example, when training a joint model
of the form f? = ? ? ? (g? ? h?) ? ? ?, where h? is a conditional
It is also possible to use this EM approach for dis-
criminative training, where we wish to maximize
?
i P (yi | xi) and f?(x, y) is a conditional FST that
defines P (y | x). The trick is to instead train a joint
model g ? f?, where g(xi) defines P (xi), thereby
maximizing
?
i P (xi) ? P (yi | xi). (Of course,
the method of this paper can train such composi-
tions.) If x1, . . . xn are fully observed, just define
each g(xi) = 1/n. But by choosing a more gen-
eral model of g, we can also handle incompletely
observed xi: training g ? f? then forces g and f?
to cooperatively reconstruct a distribution over the
possible inputs and do discriminative training of f?
given those inputs. (Any parameters of g may be ei-
ther frozen before training or optimized along with
the parameters of f?.) A final possibility is that each
xi is defined by a probabilistic FSA that already sup-
plies a distribution over the inputs; then we consider
xi ? f? ? yi directly, just as in the joint model.
Finally, note that EM is not all-purpose. It only
maximizes probabilistic objective functions, and
even there it is not necessarily as fast as (say) conju-
gate gradient. For this reason, we will also show be-
low how to compute the gradient of f?(xi, yi) with
respect to ?, for an arbitrary parameterized FST f?.
We remark without elaboration that this can help
optimize task-related objective functions, such as
?
i
?
y(P (xi, y)
?/
?
y? P (xi, y
?)?) ? error(y, yi).
4 The E Step: Expectation Semirings
It remains to devise appropriate E steps, which looks
rather daunting. Each path in Fig. 2 weaves together
parameters from other machines, which we must un-
tangle and tally. In the 4-coin parameterization, path
8? a:x?? 9?
a:x
?? 10?
a:
?? 10?
a:
?? 10?
b:z
?? 12? must yield up a
vector ?H?, T?,H?, T?,H? , T? ,H?, T?? that counts
observed heads and tails of the 4 coins. This non-
trivially works out to ?4, 1, 0, 1, 1, 1, 1, 2?. For other
parameterizations, the path must instead yield a vec-
tor of arc traversal counts or feature counts.
Computing a count vector for one path is hard
enough, but it is the E step?s job to find the expected
value of this vector?an average over the infinitely
log-linear model of P (v | u) for u ? ???, v ? ???. Then the
predicted count vector contributed by h is
?
i
?
u???? P (u |
xi, yi) ? ech(u,???). The term
?
i P (u | xi, yi) computes the
expected count of each u ? ???. It may be found by a variant
of ?4 in which path values are regular expressions over ???.
many paths pi through Fig. 2 in proportion to their
posterior probabilities P (pi | xi, yi). The results for
all (xi, yi) are summed and passed to the M step.
Abstractly, let us say that each path pi has not only
a probability P (pi) ? [0, 1] but also a value val(pi)
in a vector space V , which counts the arcs, features,
or coin flips encountered along path pi. The value of
a path is the sum of the values assigned to its arcs.
The E step must return the expected value of the
unknown path that generated (xi, yi). For example,
if every arc had value 1, then expected value would
be expected path length. Letting ? denote the set of
paths in xi ? f? ? yi (Fig. 2), the expected value is14
E[val(pi) | xi, yi] =
?
pi?? P (pi) val(pi)?
pi?? P (pi)
(1)
The denominator of equation (1) is the total prob-
ability of all accepting paths in xi ?f ?yi. But while
computing this, we will also compute the numerator.
The idea is to augment the weight data structure with
expectation information, so each weight records a
probability and a vector counting the parameters
that contributed to that probability. We will enforce
an invariant: the weight of any pathset ? must
be (
?
pi?? P (pi),
?
pi?? P (pi) val(pi)) ? R?0 ? V ,
from which (1) is trivial to compute.
Berstel and Reutenauer (1988) give a sufficiently
general finite-state framework to allow this: weights
may fall in any set K (instead of R). Multiplica-
tion and addition are replaced by binary operations
? and ? on K. Thus ? is used to combine arc
weights into a path weight and ? is used to com-
bine the weights of alternative paths. To sum over
infinite sets of cyclic paths we also need a closure
operation ?, interpreted as k? =
??
i=0 k
i
. The usual
finite-state algorithms work if (K,?,?, ?) has the
structure of a closed semiring.15
Ordinary probabilities fall in the semiring
(R?0,+,?, ?).16 Our novel weights fall in a novel
14Formal derivation of (1): ?pi P (pi | xi, yi) val(pi) =
(
?
pi P (pi, xi, yi) val(pi))/P (xi, yi) = (
?
pi P (xi, yi |
pi)P (pi) val(pi))/
?
pi P (xi, yi | pi)P (pi); now observe that
P (xi, yi | pi) = 1 or 0 according to whether pi ? ?.
15That is: (K,?) is a monoid (i.e., ? : K ? K ? K is
associative) with identity 1. (K,?) is a commutative monoid
with identity 0. ? distributes over ? from both sides, 0 ? k =
k? 0 = 0, and k? = 1? k? k? = 1? k?? k. For finite-state
composition, commutativity of ? is needed as well.
16The closure operation is defined for p ? [0, 1) as p? =
1/(1? p), so cycles with weights in [0, 1) are allowed.
V -expectation semiring, (R?0 ? V,?,?, ?):
(p1, v1)? (p2, v2)
def
= (p1p2, p1v2 + v1p2) (2)
(p1, v1)? (p2, v2)
def
= (p1 + p2, v1 + v2) (3)
if p? defined, (p, v)? def= (p?, p?vp?) (4)
If an arc has probability p and value v, we give it
the weight (p, pv), so that our invariant (see above)
holds if ? consists of a single length-0 or length-1
path. The above definitions are designed to preserve
our invariant as we build up larger paths and path-
sets. ? lets us concatenate (e.g.) simple paths pi1, pi2
to get a longer path pi with P (pi) = P (pi1)P (pi2)
and val(pi) = val(pi1) + val(pi2). The defini-
tion of ? guarantees that path pi?s weight will be
(P (pi), P (pi) ? val(pi)). ? lets us take the union of
two disjoint pathsets, and ? computes infinite unions.
To compute (1) now, we only need the total
weight ti of accepting paths in xi ? f ? yi (Fig. 2).
This can be computed with finite-state methods: the
machine (?xi)?f?(yi?) is a version that replaces
all input:output labels with  : , so it maps (, ) to
the same total weight ti. Minimizing it yields a one-
state FST from which ti can be read directly!
The other ?magical? property of the expecta-
tion semiring is that it automatically keeps track of
the tangled parameter counts. For instance, recall
that traversing 0? a:x?? 0? should have the same ef-
fect as traversing both the underlying arcs 4? a:p?? 4?
and 6? p:x?? 6?. And indeed, if the underlying arcs
have values v1 and v2, then the composed arc
0? a:x?? 0? gets weight (p1, p1v1) ? (p2, p2v2) =
(p1p2, p1p2(v1 + v2)), just as if it had value v1 + v2.
Some concrete examples of values may be useful:
? To count traversals of the arcs of Figs. 1b?c, num-
ber these arcs and let arc ` have value e`, the `th basis
vector. Then the `th element of val(pi) counts the ap-
pearances of arc ` in path pi, or underlying path pi.
? A regexp of formE+?F = ?E+(1??)F should
be weighted as (?, ?ek)E + (1? ?, (1? ?)ek+1)F
in the new semiring. Then elements k and k + 1 of
val(pi) count the heads and tails of the ?-coin.
? For a global log-linear parameterization, an arc?s
value is a vector specifying the arc?s features. Then
val(pi) counts all the features encountered along pi.
Really we are manipulating weighted relations,
not FSTs. We may combine FSTs, or determinize
or minimize them, with any variant of the semiring-
weighted algorithms.17 As long as the resulting FST
computes the right weighted relation, the arrange-
ment of its states, arcs, and labels is unimportant.
The same semiring may be used to compute gradi-
ents. We would like to find f?(xi, yi) and its gradient
with respect to ?, where f? is real-valued but need
not be probabilistic. Whatever procedures are used
to evaluate f?(xi, yi) exactly or approximately?for
example, FST operations to compile f? followed by
minimization of (?xi) ? f? ? (yi? )?can simply
be applied over the expectation semiring, replacing
each weight p by (p,?p) and replacing the usual
arithmetic operations with ?, ?, etc.18 (2)?(4) pre-
serve the gradient ((2) is the derivative product rule),
so this computation yields (f?(xi, yi),?f?(xi, yi)).
5 Removing Inefficiencies
Now for some important remarks on efficiency:
? Computing ti is an instance of the well-known
algebraic path problem (Lehmann, 1977; Tarjan,
1981a). Let Ti = xi?f?yi. Then ti is the total semir-
ing weight w0n of paths in Ti from initial state 0 to
final state n (assumed WLOG to be unique and un-
weighted). It is wasteful to compute ti as suggested
earlier, by minimizing (?xi)?f?(yi?), since then
the real work is done by an -closure step (Mohri,
2002) that implements the all-pairs version of alge-
braic path, whereas all we need is the single-source
version. If n and m are the number of states and
edges,19 then both problems are O(n3) in the worst
case, but the single-source version can be solved in
essentially O(m) time for acyclic graphs and other
reducible flow graphs (Tarjan, 1981b). For a gen-
eral graph Ti, Tarjan (1981b) shows how to partition
into ?hard? subgraphs that localize the cyclicity or
irreducibility, then run the O(n3) algorithm on each
subgraph (thereby reducing n to as little as 1), and
recombine the results. The overhead of partitioning
and recombining is essentially only O(m).
? For speeding up theO(n3) problem on subgraphs,
one can use an approximate relaxation technique
17Eisner (submitted) develops fast minimization algorithms
that work for the real and V -expectation semirings.
18Division and subtraction are also possible: ?(p, v) =
(?p,?v) and (p, v)?1 = (p?1,?p?1vp?1). Division is com-
monly used in defining f? (for normalization).
19Multiple edges from j to k are summed into a single edge.
(Mohri, 2002). Efficient hardware implementation is
also possible via chip-level parallelism (Rote, 1985).
? In many cases of interest, Ti is an acyclic graph.20
Then Tarjan?s method computes w0j for each j in
topologically sorted order, thereby finding ti in a
linear number of ? and ? operations. For HMMs
(footnote 11), Ti is the familiar trellis, and we would
like this computation of ti to reduce to the forward-
backward algorithm (Baum, 1972). But notice that
it has no backward pass. In place of pushing cumu-
lative probabilities backward to the arcs, it pushes
cumulative arcs (more generally, values in V ) for-
ward to the probabilities. This is slower because
our ? and ? are vector operations, and the vec-
tors rapidly lose sparsity as they are added together.
We therefore reintroduce a backward pass that lets
us avoid ? and ? when computing ti (so they are
needed only to construct Ti). This speedup also
works for cyclic graphs and for any V . Write wjk
as (pjk, vjk), and let w1jk = (p1jk, v1jk) denote the
weight of the edge from j to k.19 Then it can be
shown that w0n = (p0n,
?
j,k p0jv
1
jkpkn). The for-
ward and backward probabilities, p0j and pkn, can
be computed using single-source algebraic path for
the simpler semiring (R,+,?, ?)?or equivalently,
by solving a sparse linear system of equations over
R, a much-studied problem at O(n) space, O(nm)
time, and faster approximations (Greenbaum, 1997).
? A Viterbi variant of the expectation semiring ex-
ists: replace (3) with if(p1 > p2, (p1, v1), (p2, v2)).
Here, the forward and backward probabilities can be
computed in time only O(m + n log n) (Fredman
and Tarjan, 1987). k-best variants are also possible.
6 Discussion
We have exhibited a training algorithm for param-
eterized finite-state machines. Some specific conse-
quences that we believe to be novel are (1) an EM al-
gorithm for FSTs with cycles and epsilons; (2) train-
ing algorithms for HMMs and weighted contextual
edit distance that work on incomplete data; (3) end-
to-end training of noisy channel cascades, so that it
is not necessary to have separate training data for
each machine in the cascade (cf. Knight and Graehl,
20If xi and yi are acyclic (e.g., fully observed strings), and
f (or rather its FST) has no  :  cycles, then composition will
?unroll? f into an acyclic machine. If only xi is acyclic, then
the composition is still acyclic if domain(f) has no  cycles.
1998), although such data could also be used; (4)
training of branching noisy channels (footnote 7);
(5) discriminative training with incomplete data; (6)
training of conditional MEMMs (McCallum et al,
2000) and conditional random fields (Lafferty et al,
2001) on unbounded sequences.
We are particularly interested in the potential for
quickly building statistical models that incorporate
linguistic and engineering insights. Many models of
interest can be constructed in our paradigm, without
having to write new code. Bringing diverse models
into the same declarative framework also allows one
to apply new optimization methods, objective func-
tions, and finite-state algorithms to all of them.
To avoid local maxima, one might try determinis-
tic annealing (Rao and Rose, 2001), or randomized
methods, or place a prior on ?. Another extension is
to adjust the machine topology, say by model merg-
ing (Stolcke and Omohundro, 1994). Such tech-
niques build on our parameter estimation method.
The key algorithmic ideas of this paper extend
from forward-backward-style to inside-outside-style
methods. For example, it should be possible to do
end-to-end training of a weighted relation defined
by an interestingly parameterized synchronous CFG
composed with tree transducers and then FSTs.
References
L. E. Baum. 1972. An inequality and associated max-
imization technique in statistical estimation of proba-
bilistic functions of a Markov process. Inequalities, 3.
Jean Berstel and Christophe Reutenauer. 1988. Rational
Series and their Languages. Springer-Verlag.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, Carnegie Mellon.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 19(4).
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. J. Royal Statist. Soc. Ser. B, 39(1):1?38.
Jason Eisner. 2001a. Expectation semirings: Flexible
EM for finite-state transducers. In G. van Noord, ed.,
Proc. of the ESSLLI Workshop on Finite-State Methods
in Natural Language Processing. Extended abstract.
Jason Eisner. 2001b. Smoothing a Probabilistic Lexicon
via Syntactic Transformations. Ph.D. thesis, Univer-
sity of Pennsylvania.
D. Gerdemann and G. van Noord. 1999. Transducers
from rewrite rules with backreferences. Proc. of EACL.
Anne Greenbaum. 1997. Iterative Methods for Solving
Linear Systems. Soc. for Industrial and Applied Math.
Kevin Knight and Yaser Al-Onaizan. 1998. Translation
with finite-state devices. In Proc. of AMTA.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proc. of ICML.
D. J. Lehmann. 1977. Algebraic structures for transitive
closure. Theoretical Computer Science, 4(1):59?76.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. Proc. of ICML, 591?598.
M. Mohri and M.-J. Nederhof. 2001. Regular approxi-
mation of context-free grammars through transforma-
tion. In J.-C. Junqua and G. van Noord, eds., Robust-
ness in Language and Speech Technology. Kluwer.
Mehryar Mohri and Richard Sproat. 1996. An efficient
compiler for weighted rewrite rules. In Proc. of ACL.
M. Mohri, F. Pereira, and M. Riley. 1998. A rational de-
sign for a weighted finite-state transducer library. Lec-
ture Notes in Computer Science, 1436.
M. Mohri. 2002. Generic epsilon-removal and input
epsilon-normalization algorithms for weighted trans-
ducers. Int. J. of Foundations of Comp. Sci., 1(13).
Mark-Jan Nederhof. 2000. Practical experiments
with regular approximation of context-free languages.
Computational Linguistics, 26(1).
Fernando C. N. Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In E. Roche and Y. Schabes, eds., Finite-State
Language Processing. MIT Press, Cambridge, MA.
A. Rao and K. Rose. 2001 Deterministically annealed
design of hidden Markov movel speech recognizers.
In IEEE Trans. on Speech and Audio Processing, 9(2).
Stefan Riezler. 1999. Probabilistic Constraint Logic
Programming. Ph.D. thesis, Universita?t Tu?bingen.
E. Ristad and P. Yianilos. 1996. Learning string edit
distance. Tech. Report CS-TR-532-96, Princeton.
E. Ristad. 1998. Hidden Markov models with finite state
supervision. In A. Kornai, ed., Extended Finite State
Models of Language. Cambridge University Press.
Emmanuel Roche and Yves Schabes, editors. 1997.
Finite-State Language Processing. MIT Press.
Gu?nter Rote. 1985. A systolic array algorithm for the
algebraic path problem (shortest paths; matrix inver-
sion). Computing, 34(3):191?219.
Richard Sproat and Michael Riley. 1996. Compilation of
weighted finite-state transducers from decision trees.
In Proceedings of the 34th Annual Meeting of the ACL.
Andreas Stolcke and Stephen M. Omohundro. 1994.
Best-first model merging for hidden Markov model in-
duction. Tech. Report ICSI TR-94-003, Berkeley, CA.
Robert Endre Tarjan. 1981a. A unified approach to path
problems. Journal of the ACM, 28(3):577?593, July.
Robert Endre Tarjan. 1981b. Fast algorithms for solving
path problems. J. of the ACM, 28(3):594?614, July.
G. van Noord and D. Gerdemann. 2001. An extendible
regular expression compiler for finite-state approaches
in natural language processing. In Automata Imple-
mentation, no. 22 in Springer Lecture Notes in CS.
Comprehension and Compilation in Optimality Theory?
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD, USA 21218-2691
jason@cs.jhu.edu
Abstract
This paper ties up some loose ends in finite-state Optimality
Theory. First, it discusses how to perform comprehension un-
der Optimality Theory grammars consisting of finite-state con-
straints. Comprehension has not been much studied in OT; we
show that unlike production, it does not always yield a regular
set, making finite-state methods inapplicable. However, after
giving a suitably flexible presentation of OT, we show care-
fully how to treat comprehension under recent variants of OT
in which grammars can be compiled into finite-state transduc-
ers. We then unify these variants, showing that compilation is
possible if all components of the grammar are regular relations,
including the harmony ordering on scored candidates. A side
benefit of our construction is a far simpler implementation of
directional OT (Eisner, 2000).
1 Introduction
To produce language is to convert utterances from
their underlying (?deep?) form to a surface form.
Optimality Theory or OT (Prince and Smolensky,
1993) proposes to describe phonological production
as an optimization process. For an underlying x,
a speaker purportedly chooses the surface form z
so as to maximize the harmony of the pair (x, z).
Broadly speaking, (x, z) is harmonic if z is ?easy?
to pronounce and ?similar? to x. But the precise har-
mony measure depends on the language; according
to OT, it can be specified by a grammar of ranked
desiderata known as constraints.
According to OT, then, production maps each un-
derlying form to its best possible surface pronuncia-
tion. It is akin to the function that maps each child x
to his or her most flattering outfit z. Different chil-
dren look best in different clothes, and for an oddly
shaped child x, even the best conceivable outfit z
may be an awkward compromise between style and
fit?that is, between ease of pronunciation and sim-
ilarity to x.
Language comprehension is production in re-
verse. In OT, it maps each outfit z to the set of chil-
?Thanks to Kie Zuraw for asking about comprehension; to
Ron Kaplan for demanding an algebraic construction before he
believed directional OT was finite-state; and to others whose
questions convinced me that this paper deserved to be written.
dren x for whom that outfit is optimal, i.e., is at least
as flattering as any other outfit z?:
PRODUCE(x) = {z : (@z?) (x, z?) > (x, z)}
COMPREHEND(z) = {x : z ? PRODUCE(x)}
= {x : (@z?) (x, z?) > (x, z)}
In general z and z? may range over infinitely many
possible pronunciations. While the formulas above
are almost identical, comprehension is in a sense
more complex because it varies both the underlying
and surface forms. While PRODUCE(x) considers
all pairs (x, z?), COMPREHEND(z) must for each x
consider all pairs (x, z?). Of course, this nested def-
inition does not preclude computational shortcuts.
This paper has three modest goals:
1. To show that OT comprehension does in fact
present a computational problem that production
does not. Even when the OT grammar is required to
be finite-state, so that production can be performed
with finite-state techniques, comprehension cannot
in general be performed with finite-state techniques.
2. To consider recent constructions that cut through
this problem (Frank and Satta, 1998; Karttunen,
1998; Eisner, 2000; Gerdemann and van Noord,
2000). By altering or approximating the OT
formalism?that is, by hook or by crook?these con-
structions manage to compile OT grammars into
finite-state transducers. Transducers may readily be
inverted to do comprehension as easily as produc-
tion. We carefully lay out how to use them for com-
prehension in realistic circumstances (in the pres-
ence of correspondence theory, lexical constraints,
hearer uncertainty, and phonetic postprocessing).
3. To give a unified treatment in the extended finite-
state calculus of the constructions referenced above.
This clarifies their meaning and makes them easy to
implement. For example, we obtain a transparent al-
gebraic version of Eisner?s (2000) unbearably tech-
nical automaton construction for his proposed for-
malism of ?directional OT.?
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 56-63.
                         Proceedings of the 40th Annual Meeting of the Association for
The treatment shows that all the constructions
emerge directly from a generalized presentation of
OT, in which the crucial fact is that the harmony or-
dering on scored candidates is a regular relation.
2 Previous Work on Comprehension
Work focusing on OT comprehension?or even
mentioning it?has been surprisingly sparse. While
the recent constructions mentioned in ?1 can easily
be applied to the comprehension problem, as we will
explain, they were motivated primarily by a desire to
pare back OT?s generative power to that of previous
rewrite-rule formalisms (Johnson, 1972).
Fosler (1996) noted the existence of the OT com-
prehension task and speculated that it might suc-
cumb to heuristic search. Smolensky (1996) pro-
posed to solve it by optimizing the underlying form,
COMPREHEND(z)
?
= {x : (@x?) (x?, z) > (x, z)}
Hale and Reiss (1998) pointed out in response that
any comprehension-by-optimization strategy would
have to arrange for multiple optima: after all, phono-
logical comprehension is a one-to-many mapping
(since phonological production is many-to-one).1
The correctness of Smolensky?s proposal (i.e.,
whether it really computes COMPREHEND) depends
on the particular harmony measure. It can be made
to work, multiple optima and all, if the harmony
measure is constructed with both production and
comprehension in mind. Indeed, for any phonology,
it is trivial to design a harmony measure that both
production and comprehension optimize. (Just de-
fine the harmony of (x, z) to be 1 or 0 according
to whether the mapping x 7? z is in the language!)
But we are really only interested in harmony mea-
sures that are defined by OT-style grammars (rank-
ings of ?simple? constraints). In this case Smolen-
sky?s proposal can be unworkable. In particular, ?4
will show that a finite-state production grammar in
classical OT need not be invertible by any finite-state
comprehension grammar.
1Hale & Reiss?s criticism may be specific to phonology
and syntax. For some phenomena in semantics, pragmatics,
and even morphology, Blutner (1999) argues for a one-to-one
form-meaning mapping in which marked forms express marked
meanings. He deliberately uses bidirectional optimization to
rule out many-to-one cases: roughly speaking, an (x, z) pair is
grammatical for him only if z is optimal given x and vice-versa.
3 A General Presentation of OT
This section (graphically summarized in Fig. 1) lays
out a generalized version of OT?s theory of produc-
tion, introducing some notational and representa-
tional conventions that may be useful to others and
will be important below. In particular, all objects
are represented as strings, or as functions that map
strings to strings. This will enable us to use finite-
state techniques later.
The underlying form x and surface form z are
represented as strings. We often refer to these strings
as input and output. Following Eisner (1997), each
candidate (x, z) is also represented as a string y.
The notation (x, z) that we have been using so far
for candidates is actually misleading, since in fact
the candidates y that are compared encode more than
just x and z. They also encode a particular alignment
or correspondence between x and z. For example,
if x = abdip and z = a[di][bu], then a typical
candidate would be encoded
y = aab0[ddii][pb0u]
which specifies that a corresponds to a, b was
deleted (has no surface correspondent), voiceless p
surfaces as voiced b, etc. The harmony of y might
depend on this alignment as well as on x and z (just
as an outfit might fit worse when worn backwards).
Because we are distinguishing underlying and
surface material by using disjoint alphabets ? =
{a,b, . . .} and ? = {[,],a,b, . . .},2 it is easy to
extract the underlying and surface forms (x and z)
from y.
Although the above example assumes that x and
z are simple strings of phonemes and brackets, noth-
ing herein depends on that assumption. Autoseg-
mental representations too can be encoded as strings
(Eisner, 1997).
In general, an OT grammar consists of 4 com-
ponents: a constraint ranking, a harmony ordering,
and generating and pronouncing functions. The con-
straint ranking is the language-specific part of the
grammar; the other components are often supposed
to be universal across languages.
The generating function GEN maps any x ? ??
to the (nonempty) set of candidates y whose under-
lying form is x. In other words, GEN just inserts
2An alternative would be to distinguish them by odd and
even positions in the string.
x
????
underlying form x???
GEN
?? Y0(x)
C1?? Y1(x)
C2?? Y2(x) ? ? ?
Cn?? Yn(x)
? ?? ?
sets of candidates y?(???)?
PRON
??Z(x)
? ?? ?
set of surface forms z???
where Yi?1(x)
Ci?? Yi(x) really means Yi?1(x)
? ?? ?
y?(???)?
Ci?? Y?i(x)
prune
?? optimal subset of Y?i(x)
? ?? ?
y??(????{?})?
delete ?
?? Yi(x)
? ?? ?
y?(???)?
Figure 1: This paper?s view of OT production. In the second line,Ci inserts ??s into candidates; then the candidates with suboptimal
starrings are pruned away, and finally the ??s are removed from the survivors.
arbitrary substrings from ?? amongst the charac-
ters of x, subject to any restrictions on what consti-
tutes a legitimate candidate y.3 (Legitimacy might
for instance demand that y?s surface material z have
matched, non-nested left and right brackets, or even
that z be similar to x in terms of edit distance.)
A constraint ranking is simply a sequence
C1, C2, . . . Cn of constraints. Let us take each
Ci to be a function that scores candidates y by
annotating them with violation marks ?. For ex-
ample, a NODELETE constraint would map y =
aab0c0[ddii][pb0u] to y? =NODELETE(y) =
aab?0c?0[ddii][pb0u], inserting a ? after each
underlying phoneme that does not correspond to any
surface phoneme. This unconventional formulation
is needed for new approaches that care about the ex-
act location of the ??s. In traditional OT only the
number of ??s is important, although the locations
are sometimes shown for readability.
Finally, OT requires a harmony ordering 
on scored candidates y? ? (? ? ? ? {?})?. In
traditional OT, y? is most harmonic when it con-
tains the fewest ??s. For example, among candi-
dates scored by NODELETE, the most harmonic
ones are the ones with the fewest deletions; many
candidates may tie for this honor. ?6 considers
other harmony orderings, a possibility recognized
by Prince and Smolensky (1993) ( corresponds to
their H-EVAL). In general  may be a partial or-
der: two competing candidates may be equally har-
monic or incomparable (in which case both can
survive), and candidates with different underlying
forms never compete at all.
Production under such a grammar is a matter of
successive filtering by the constraints C1, . . . Cn.
Given an underlying form x, let
Y0(x) = GEN(x) (1)
3It is never really necessary for GEN to enforce such restric-
tions, since they can equally well be enforced by the top-ranked
constraint C1 (see below).
Yi(x) = {y ? Yi?1(x) : (2)
(@y? ? Yi?1(x)) Ci(y?)  Ci(y)}
The set of optimal candidates is now Yn(x). Ex-
tracting z from each y ? Yn(x) gives the set Z(x)
or PRODUCE(x) of acceptable surface forms:
Z(x) = {PRON(y) : y ? Yn(x)} ? ?? (3)
PRON denotes the simple pronunciation function
that extracts z from y. It is the counterpart to GEN:
just as GEN fleshes out x ? ?? into y by inserting
symbols of ?, PRON slims y down to z ? ?? by
removing symbols of ?.
Notice that Yn ? Yn?1 ? . . . ? Y0. The only
candidates y ? Yi?1 that survive filtering by Ci are
the ones that Ci considers most harmonic.
The above notation is general enough to handle
some of the important variations of OT, such as
Paradigm Uniformity and Sympathy Theory. In par-
ticular, one can define GEN so that each candidate
y encodes not just an alignment between x and z,
but an alignment among x, z, and some other strings
that are neither underlying nor surface. These other
strings may represent the surface forms for other
members of the same morphological paradigm, or
intermediate throwaway candidates to which z is
sympathetic. Production still optimizes y, which
means that it simultaneously optimizes z and the
other strings.
4 Comprehension in Finite-State OT
This section assumes OT?s traditional harmony or-
dering, in which the candidates that survive filtering
by Ci are the ones into which Ci inserts fewest ??s.
Much computational work on OT has been con-
ducted within a finite-state framework (Ellison,
1994), in keeping with a tradition of finite-state
phonology (Johnson, 1972; Kaplan and Kay, 1994).4
4The tradition already included (inviolable) phonological
Finite-state OT is a restriction of the formal-
ism discussed above. It specifically assumes that
GEN, C1, . . . Cn, and PRON are all regular relations,
meaning that they can be described by finite-state
transducers. GEN is a nondeterministic transducer
that maps each x to multiple candidates y. The other
transducers map each y to a single y? or z.
These finite-state assumptions were proposed
(in a different and slightly weaker form) by
Ellison (1994). Their empirical adequacy has been
defended by Eisner (1997).
In addition to having the right kind of power lin-
guistically, regular relations are closed under vari-
ous relevant operations and allow (efficient) parallel
processing of regular sets of strings. Ellison (1994)
exploited such properties to give a production algo-
rithm for finite-state OT. Given x and a finite-state
OT grammar, he used finite-state operations to con-
struct the set Yn(x) of optimal candidates, repre-
sented as a finite-state automaton.
Ellison?s construction demonstrates that Yn is al-
ways a regular set. Since PRON is regular, it follows
that PRODUCE(x) = Z(x) is also a regular set.
We now show that COMPREHEND(z), in con-
strast, need not be a regular set. Let ? = {a,b},
? = {[,],a,b, . . .} and suppose that GEN allows
candidates like the ones in ?3, in which parts of the
string may be bracketed between [ and ]. The cru-
cial grammar consists of two finite-state constraints.
C2 penalizes a?s that fall between brackets (by in-
serting ? next to each one) and also penalizes b?s
that fall outside of brackets. It is dominated by C1,
which penalizes brackets that do not fall at either
edge of the string. Note that this grammar is com-
pletely permissive as to the number and location of
surface characters other than brackets.
If x contains more a?s than b?s, then PRODUCE(x)
is the set ??? of all unbracketed surface forms, where
?? is ? minus the bracket symbols. If x contains
fewer a?s than b?s, then PRODUCE(x) = [???].
And if a?s and b?s appear equally often in x, then
PRODUCE(x) is the union of the two sets.
Thus, while the x-to-z mapping is not a regular
relation under this grammar, at least PRODUCE(x)
is a regular set for each x?just as finite-state OT
constraints, notably Koskenniemi?s (1983) two-level model,
which like OT used finite-state constraints on candidates y that
encoded an alignment between underlying x and surface z.
guarantees. But for any unbracketed z ? ???, such
as z = abc, COMPREHEND(z) is not regular: it is
the set of underlying strings with # of a?s? # of b?s.
This result seems to eliminate any hope of han-
dling OT comprehension in a finite-state frame-
work. It is interesting to note that both OT and
current speech recognition systems construct finite-
state models of production and define comprehen-
sion as the inverse of production. Speech recog-
nizers do correctly implement comprehension via
finite-state optimization (Pereira and Riley, 1997).
But this is impossible in OT because OT has a more
complicated production model. (In speech recog-
nizers, the most probable phonetic or phonological
surface form is not presumed to have suppressed its
competitors.)
One might try to salvage the situation by barring
constraints like C1 or C2 from the theory as linguis-
tically implausible. Unfortunately this is unlikely
to succeed. Primitive OT (Eisner, 1997) already re-
stricts OT to something like a bare minimum of con-
straints, allowing just two simple constraint families
that are widely used by practitioners of OT. Yet even
these primitive constraints retain enough power to
simulate any finite-state constraint. In any case, C1
and C2 themselves are fairly similar to ?domain?
constraints used to describe tone systems (Cole and
Kisseberth, 1994). While C2 is somewhat odd in
that it penalizes two distinct configurations at once,
one would obtain the same effect by combining three
separately plausible constraints: C2 requires a?s be-
tween brackets (i.e., in a tone domain) to receive sur-
face high tones, C3 requires b?s outside brackets to
receive surface high tones, and C4 penalizes all sur-
face high tones.5
Another obvious if unsatisfying hack would im-
pose heuristic limits on the length of x, for exam-
ple by allowing the comprehension system to return
the approximation COMPREHEND(z) ? {x : |x| ?
2 ? |z|}. This set is finite and hence regular, so per-
5Since the surface tones indicate the total number of a?s and
b?s in the underlying form, COMPREHEND(z) is actually a finite
set in this version, hence regular. But the non-regularity argu-
ment does go through if the tonal information in z is not avail-
able to the comprehension system (as when reading text with-
out diacritics); we cover this case in ?5. (One can assume that
some lower-ranked constraints require a special suffix before ],
so that the bracket information need not be directly available to
the comprehension system either.)
haps it can be produced by some finite-state method,
although the automaton to describe the set might be
large in some cases.
Recent efforts to force OT into a fully finite-state
mold are more promising. As we will see, they iden-
tify the problem as the harmony ordering , rather
than the space of constraints or the potential infini-
tude of the answer set.
5 Regular-Relation Comprehension
Since COMPREHEND(z) need not be a regular set
in traditional OT, a corollary is that COMPREHEND
and its inverse PRODUCE are not regular relations.
That much was previously shown by Markus Hiller
and Paul Smolensky (Frank and Satta, 1998), using
similar examples.
However, at least some OT grammars ought to de-
scribe regular relations. It has long been hypothe-
sized that all human phonologies are regular rela-
tions, at least if one omits reduplication, and this is
necessarily true of phonologies that were success-
fully described with pre-OT formalisms (Johnson,
1972; Koskenniemi, 1983).
Regular relations are important for us because
they are computationally tractable. Any regular rela-
tion can be implemented as a finite-state transducer
T , which can be inverted and used for comprehen-
sion as well as production. PRODUCE(x) = T (x) =
range(x ? T ), and COMPREHEND(z) = T?1(z) =
domain(T ? z).
We are therefore interested in compiling OT
grammars into finite-state transducers?by hook or
by crook. ?6 discusses how; but first let us see how
such compilation is useful in realistic situations.
Any practical comprehension strategy must rec-
ognize that the hearer does not really perceive the
entire surface form. After all, the surface form con-
tains phonetically invisible material (e.g., syllable
and foot boundaries) and makes phonetically imper-
ceptible distinctions (e.g., two copies of a tone ver-
sus one doubly linked copy). How to comprehend in
this case?
The solution is to modify PRON to ?go all the
way??to delete not only underlying material but
also phonetically invisible material. Indeed, PRON
can also be made to perform any purely phonetic
processing. Each output z of PRODUCE is now not a
phonological surface form but a string of phonemes
or spectrogram segments. So long as PRON is a reg-
ular relation (perhaps a nondeterministic or prob-
abilistic one that takes phonetic variation into ac-
count), we will still be able to construct T and use it
for production and comprehension as above.6
How about the lexicon? When the phonology can
be represented as a transducer, COMPREHEND(z) is
a regular set. It contains all inputs x that could have
produced output z. In practice, many of these in-
puts are not in the lexicon, nor are they possible
novel words. One should restrict to inputs that ap-
pear in the lexicon (also a regular set) by intersecting
COMPREHEND(z) with the lexicon. For novel words
this intersection will be empty; but one can find the
possible underlying forms of the novel word, for
learning?s sake, by intersecting COMPREHEND(z)
with a larger (infinite) regular set representing all
forms satisfying the language?s lexical constraints.
There is an alternative treatment of the lexicon.
GEN can be extended ?backwards? to incorporate
morphology just as PRON was extended ?forwards?
to incorporate phonetics. On this view, the input
x is a sequence of abstract morphemes, and GEN
performs morphological preprocessing to turn x into
possible candidates y. GEN looks up each abstract
morpheme?s phonological string ? ?? from the lex-
icon,7 then combines these phonological strings by
concatenation or template merger, then nondeter-
ministically inserts surface material from ??. Such
a GEN can plausibly be built up (by composition)
as a regular relation from abstract morpheme se-
quences to phonological candidates. This regularity,
as for PRON, is all that is required.
Representing a phonology as a transducer T has
additional virtues. T can be applied efficiently
to any input string x, whereas Ellison (1994) or
Eisner (1997) requires a fresh automaton construc-
tion for each x. A nice trick is to build T without
6Pereira and Riley (1997) build a speech recognizer by com-
posing a probabilistic finite-state language model, a finite-state
pronouncing dictionary, and a probabilistic finite-state acoustic
model. These three components correspond precisely to the in-
put to GEN, the traditional OT grammar, and PRON, so we are
simply suggesting the same thing in different terminology.
7Nondeterministically in the case of phonologically condi-
tioned allomorphs: INDEFINITE APPLE 7? {??pl, ?n?pl} ?
??. This yields competing candidates that differ even in their
underlying phonological material.
PRON and apply it to all conceivable x?s in paral-
lel, yielding the complete set of all optimal candi-
dates Yn(??) =
?
x??? Yn(x). If Y and Y ? denote
the sets of optimal candidates under two grammars,
then (Y ? ?Y ?) ? (Y ? ? ?Y ) yields the candidates
that are optimal under only one grammar. Applying
GEN?1 or PRON to this set finds the regular set of
underlying or surface forms that the two grammars
would treat differently; one can then look for empir-
ical cases in this set, in order to distinguish between
the two grammars.
6 Theorem on Compiling OT
Why are OT phonologies not always regular re-
lations? The trouble is that inputs may be arbi-
trarily long, and so may accrue arbitrarily large
numbers of violations. Traditional OT (?4) is
supposed to distinguish all such numbers. Con-
sider syllabification in English, which prefers
to syllabify the long input bibambam . . .bam
? ?? ?
k copies
as [bi][bam][bam] . . .[bam] (with k codas)
rather than [bib][am][bam] . . .[bam] (with
k + 1 codas). NOCODA must therefore distinguish
annotated candidates y? with k ??s (which are opti-
mal) from those with k + 1 ??s (which are not). It
requires a (? k + 2)-state automaton to make this
distinction by looking only at the ??s in y?. And if k
can be arbitrarily large, then no finite-state automa-
ton will handle all cases.
Thus, constraints like NOCODA do not allow an
upper bound on k for all x ? ??. Of course, the min-
imal number of violations k of a constraint is fixed
given the underlying form x, which is useful in pro-
duction.8 But comprehension is less fortunate: we
cannot bound k given only the surface form z. In
the grammar of ?4, COMPREHEND(abc) included
underlying forms whose optimal candidates had ar-
bitrarily large numbers of violations k.
Now, in most cases, the effect of an OT gram-
mar can be achieved without actually counting any-
thing. (This is to be expected since rewrite-rule
8Ellison (1994) was able to construct PRODUCE(x) from x.
One can even build a transducer for PRODUCE that is correct on
all inputs that can achieve? K violations and returns ? on other
inputs (signalling that the transducer needs to be recompiled
with increased K). Simply use the construction of (Frank and
Satta, 1998; Karttunen, 1998), composed with a hard constraint
that the answer must have ? K violations.
grammars were previously written for the same
phonologies, and they did not use counting!) This
is possible despite the above arguments because
for some grammars, the distinction between opti-
mal and suboptimal y? can be made by looking at
the non-? symbols in y? rather than trying to count
the ??s. In our NOCODA example, a surface sub-
string such as . . .ib?][a. . . might signal that y? is
suboptimal because it contains an ?unnecessary?
coda. Of course, the validity of this conclusion
depends on the grammar and specifically the con-
straints C1, . . . Ci?1 ranked above NOCODA, since
whether that coda is really unnecessary depends on
whether Y?i?1 also contains the competing candidate
. . .i][ba . . . with fewer codas.
But as we have seen, some OT grammars do have
effects that overstep the finite-state boundary (?4).
Recent efforts to treat OT with transducers have
therefore tried to remove counting from the formal-
ism. We now unify such efforts by showing that they
all modify the harmony ordering .
?4 described finite-state OT grammars as ones
where GEN, PRON, and the constraints are regular
relations. We claim that if the harmony ordering 
is also a regular relation on strings of (????{?})?,
then the entire grammar (PRODUCE) is also regular.
We require harmony orderings to be compatible
with GEN: an ordering must treat y??, y? as incompa-
rable (neither is  the other) if they were produced
from different underlying forms.9
To make the notation readable let us denote the 
relation by the letter H . Thus, a transducer for H
accepts the pair (y??, y?) if y??  y?.
The construction is inductive. Y0 = GEN is reg-
ular by assumption. If Yi?1 is regular, then so is Yi
since (as we will show)
Yi = (Y?i ? ?range(Y?i ?H)) ?D (4)
where Y?i
def
= Yi?1 ? Ci and maps x to the set of
starred candidates that Ci will prune; ? denotes the
complement of a regular language; and D is a trans-
ducer that removes all ??s. Therefore PRODUCE =
Yn ? PRON is regular as claimed.
9For example, the harmony ordering of traditional OT is
{(y??, y?) : y?? has the same underlying form as, but contains
fewer ??s than, y?}. If we were allowed to drop the same-
underlying-form condition then the ordering would become reg-
ular, and then our claim would falsely imply that all traditional
finite-state OT grammars were regular relations.
It remains to derive (4). Equation (2) implies
Ci(Yi(x)) = {y? ? Y?i(x) : (@y?? ? Y?i(x)) y??  y?} (5)
= Y?i(x)? {y? : (?y?
? ? Y?i(x)) y?
?  y?} (6)
= Y?i(x)?H(Y?i(x)) (7)
One can read H(Y?i(x)) as ?starred candidates that
are worse than other starred candidates,? i.e., subop-
timal. The set difference (7) leaves only the optimal
candidates. We now see
(x, y?) ? Yi ? Ci ? y? ? Ci(Yi(x)) (8)
? y? ? Y?i(x), y? 6? H(Y?i(x)) [by (7)] (9)
? y? ? Y?i(x), (@z)y? ? H(Y?i(z)) [see below](10)
? (x, y?) ? Y?i, y? 6? range(Y?i ?H) (11)
? (x, y?) ? Y?i ? ?range(Y?i ?H) (12)
therefore Yi ? Ci = Y?i ? ?range(Y?i ?H) (13)
and composing both sides with D yields (4). To jus-
tify (9)? (10) we must show when y? ? Y?i(x) that
y? ? H(Y?i(x)) ? (?z)y? ? H(Y?i(z)). For the ?
direction, just take z = x. For ?, y? ? H(Y?i(z))
means that (?y?? ? Y?i(z))y??  y?; but then x = z
(giving y? ? H(Y?i(x))), since if not, our compatibil-
ity requirement on H would have made y?? ? Y?i(z)
incomparable with y? ? Y?i(x).
Extending the pretty notation of (Karttunen,
1998), we may use (4) to define a left-associative
generalized optimality operator ooH :
Y ooH C
def
= (Y ?C??range(Y ?C?H))?D (14)
Then for any regular OT grammar, PRODUCE =
GEN ooH C1 ooH C2 ? ? ? ooH Cn ? PRON
and can be inverted to get COMPREHEND. More
generally, different constraints can usefully be ap-
plied with different H?s (Eisner, 2000).
The algebraic construction above is inspired by a
version that Gerdemann and van Noord (2000) give
for a particular variant of OT. Their regular expres-
sions can be used to implement it, simply replacing
their add_violation by our H .
Typically, H ignores surface characters when
comparing starred candidates. So H can be written
as elim(?)?G?elim(?)?1 where elim(?) is a
transducer that removes all characters of ?. To sat-
isfy the compatibility requirement on H , G should
be a subset of the relation (?| ? |( : ?)|(? : ))?.10
10This transducer regexp says to map any symbol in ?? {?}
to itself, or insert or delete ??and then repeat.
We now summarize the main proposals from the
literature (see ?1), propose operator names, and cast
them in the general framework.
? Y o C: Inviolable constraint (Koskenniemi,
1983; Bird, 1995), implemented by composition.
? Y o+ C: Counting constraint (Prince and
Smolensky, 1993): more violations is more dishar-
monic. No finite-state implementation possible.
? Y oo C: Binary approximation (Karttunen,
1998; Frank and Satta, 1998). All candidates with
any violations are equally disharmonic. Imple-
mented by G = (??( : ?)??)+, which relates un-
derlying forms without violations to the same forms
with violations.
? Y oo3 C: 3-bounded approximation (Karttunen,
1998; Frank and Satta, 1998). Like o+ , but all
candidates with ? 3 violations are equally dishar-
monic. G is most easily described with a transducer
that keeps count of the input and output ??s so far, on
a scale of 0, 1, 2, ? 3. Final states are those whose
output count exceeds their input count on this scale.
? Y o? C: Matching or subset approximation
(Gerdemann and van Noord, 2000). A candidate is
more disharmonic than another if it has stars in all
the same locations and some more besides.11 Here
G = ((?|?)?( : ?)(?|?)?)+.
? Y o> C: Left-to-right directional evaluation (Eis-
ner, 2000). A candidate is more disharmonic than
another if in the leftmost position where they differ
(ignoring surface characters), it has a ?. This revises
OT?s ?do only when necessary? mantra to ?do only
when necessary and then as late as possible? (even
if delaying ??s means suffering more of them later).
Here G = (?|?)?(( : ?)|((? : ?)(?|?)?)). Unlike
the other proposals, here two forms can both be op-
timal only if they have exactly the same pattern of
violations with respect to their underlying material.
? Y <o C: Right-to-left directional evaluation.
?Do only when necessary and then as early as possi-
ble.? Here G is the reverse of the G used in o> .
The novelty of the matching and directional pro-
posals is their attention to where the violations fall.
Eisner?s directional proposal (o>, <o) is the only
11Many candidates are incomparable under this ordering, so
Gerdemann and van Noord also showed how to weaken the no-
tation of ?same location? in order to approximate o+ better.
(a) x =bantodibo
[ban][to][di][bo]
[ban][ton][di][bo]
[ban][to][dim][bon]
[ban][ton][dim][bon]
(b) NOCODA
ban?todibo
ban?to?dibo
ban?todi?bo?
ban?to?di?bo?
(c) C1 NOCODA
*! *
+ **
***!
***!*
(d) C1 ?1 ?2 ?3 ?4
*! *
* *!
+ * * *
* *! * *
Figure 2: Counting vs. directionality. [Adapted from (Eisner, 2000).] C1 is some high-ranked constraint that kills the most faithful
candidate; NOCODA dislikes syllable codas. (a) Surface material of the candidates. (b) Scored candidates for G to compare.
Surface characters but not ??s have been removed by elim(?). (c) In traditional evaluation o+ , G counts the ??s. (d) Directional
evaluation o> gets a different result, as if NOCODA were split into 4 constraints evaluating the syllables separately. More
accurately, it is as if NOCODA were split into one constraint per underlying letter, counting the number of ??s right after that letter.
one defended on linguistic as well as computational
grounds. He argues that violation counting (o+) is
a bug in OT rather than a feature worth approximat-
ing, since it predicts unattested phenomena such as
?majority assimilation? (Bakovic?, 1999; Lombardi,
1999). Conversely, he argues that comparing viola-
tions directionally is not a hack but a desirable fea-
ture, since it naturally predicts ?iterative phenom-
ena? whose description in traditional OT (via Gener-
alized Alignment) is awkward from both a linguistic
and a computational point of view. Fig. 2 contrasts
the traditional and directional harmony orderings.
Eisner (2000) proved that o> was a regular op-
erator for directional H , by making use of a rather
different insight, but that machine-level construction
was highly technical. The new algebraic construc-
tion is simple and can be implemented with a few
regular expressions, as for any other H .
7 Conclusion
See the itemized points in ?1 for a detailed summary.
In general, this paper has laid out a clear, general
framework for finite-state OT systems, and used it to
obtain positive and negative results about the under-
studied problem of comprehension. Perhaps these
results will have some bearing on the development
of realistic learning algorithms.
The paper has also established sufficient condi-
tions for a finite-state OT grammar to compile into a
finite-state transducer. It should be easy to imagine
new variants of OT that meet these conditions.
References
Eric Bakovic?. 1999. Assimilation to the unmarked. Rut-
gers Optimality Archive ROA-340., August.
Steven Bird. 1995. Computational Phonology: A
Constraint-Based Approach. Cambridge.
Reinhard Blutner. 1999. Some aspects of optimality in
natural language interpretation. In Papers on Optimal-
ity Theoretic Semantics. Utrecht.
J. Cole and C. Kisseberth. 1994. An optimal domains
theory of harmony. Studies in the Linguistic Sciences,
24(2).
Jason Eisner. 1997. Efficient generation in primitive Op-
timality Theory. In Proc. of ACL/EACL.
Jason Eisner. 2000. Directional constraint evaluation in
Optimality Theory. In Proc. of COLING.
T. Mark Ellison. 1994. Phonological derivation in Opti-
mality Theory. In Proc. of COLING
J. Eric Fosler. 1996. On reversing the generation process
in Optimality Theory. Proc. of ACL Student Session.
R. Frank and G. Satta. 1998. Optimality Theory and the
generative complexity of constraint violability. Com-
putational Linguistics, 24(2):307?315.
D. Gerdemann and G. van Noord. 2000. Approxima-
tion and exactness in finite-state Optimality Theory. In
Proc. of ACL SIGPHON Workshop.
Mark Hale and Charles Reiss. 1998. Formal and empir-
ical arguments concerning phonological acquisition.
Linguistic Inquiry, 29:656?683.
C. Douglas Johnson. 1972. Formal Aspects of Phonolog-
ical Description. Mouton.
R. Kaplan and M. Kay. 1994. Regular models of phono-
logical rule systems. Comp. Ling., 20(3).
L. Karttunen. 1998. The proper treatment of optimality
in computational phonology. In Proc. of FSMNLP.
Kimmo Koskenniemi. 1983. Two-level morphology: A
general computational model for word-form recogni-
tion and production. Publication 11, Dept. of General
Linguistics, University of Helsinki.
Linda Lombardi. 1999. Positional faithfulness and voic-
ing assimilation in Optimality Theory. Natural Lan-
guage and Linguistic Theory, 17:267?302.
Fernando C. N. Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In E. Roche and Y. Schabes, eds., Finite-State
Language Processing. MIT Press.
A. Prince and P. Smolensky. 1993. Optimality Theory:
Constraint interaction in generative grammar. Ms.,
Rutgers and U. of Colorado (Boulder).
Paul Smolensky. 1996. On the comprehen-
sion/production dilemma in child language. Linguistic
Inquiry, 27:720?731.
Learning Non-Isomorphic Tree Mappings for Machine Translation
Jason Eisner, Computer Science Dept., Johns Hopkins Univ. <jason@cs.jhu.edu>
Abstract
Often one may wish to learn a tree-to-tree mapping, training it
on unaligned pairs of trees, or on a mixture of trees and strings.
Unlike previous statistical formalisms (limited to isomorphic
trees), synchronous TSG allows local distortion of the tree topol-
ogy. We reformulate it to permit dependency trees, and sketch
EM/Viterbi algorithms for alignment, training, and decoding.
1 Introduction: Tree-to-Tree Mappings
Statistical machine translation systems are trained on
pairs of sentences that are mutual translations. For exam-
ple, (beaucoup d?enfants donnent un baiser a` Sam, kids
kiss Sam quite often). This translation is somewhat free,
as is common in naturally occurring data. The first sen-
tence is literally Lots of?children give a kiss to Sam.
This short paper outlines ?natural? formalisms and al-
gorithms for training on pairs of trees. Our methods work
on either dependency trees (as shown) or phrase-structure
trees. Note that the depicted trees are not isomorphic.
a
kiss
baiser
donnent
Sam often
quite
beaucoup un Sam
d?
enfants
kids
Our main concern is to develop models that can align
and learn from these tree pairs despite the ?mismatches?
in tree structure. Many ?mismatches? are characteristic
of a language pair: e.g., preposition insertion (of ? ),
multiword locutions (kiss ? give a kiss to; misinform
? wrongly inform), and head-swapping (float down ?
descend by floating). Such systematic mismatches should
be learned by the model, and used during translation.
It is even helpful to learn mismatches that merely tend
to arise during free translation. Knowing that beaucoup
d? is often deleted will help in aligning the rest of the tree.
When would learned tree-to-tree mappings be useful?
Obviously, in MT, when one has parsers for both the
source and target language. Systems for ?deep? anal-
ysis and generation might wish to learn mappings be-
tween deep and surface trees (Bo?hmova? et al, 2001)
or between syntax and semantics (Shieber and Schabes,
1990). Systems for summarization or paraphrase could
also be trained on tree pairs (Knight and Marcu, 2000).
Non-NLP applications might include comparing student-
written programs to one another or to the correct solution.
Our methods can naturally extend to train on pairs of
forests (including packed forests obtained by chart pars-
ing). The correct tree is presumed to be an element of
the forest. This makes it possible to train even when the
correct parse is not fully known, or not known at all.
2 A Natural Proposal: Synchronous TSG
We make the quite natural proposal of using a syn-
chronous tree substitution grammar (STSG). An STSG
is a collection of (ordered) pairs of aligned elementary
trees. These may be combined into a derived pair of
trees. Both the elementary tree pairs and the operation to
combine them will be formalized in later sections.
As an example, the tree pair shown in the introduction
might have been derived by ?vertically? assembling the
6 elementary tree pairs below. The _ symbol denotes
a frontier node of an elementary tree, which must be
replaced by the circled root of another elementary tree.
If two frontier nodes are linked by a dashed line labeled
with the state X , then they must be replaced by two roots
that are also linked by a dashed line labeled with X .
a
kiss
null (0,Adv)
Start
un
baiser
NP
donnent
NP
NP
beaucoup
NPd?
(0,Adv)null
null often(0,Adv)
(0,Adv)
null quite
enfants kidsNP
Sam SamNP
The elementary trees represent idiomatic translation
?chunks.? The frontier nodes represent unfilled roles in
the chunks, and the states are effectively nonterminals
that specify the type of filler that is required. Thus, don-
nent un baiser a` (?give a kiss to?) corresponds to kiss,
with the French subject matched to the English subject,
and the French indirect object matched to the English
direct object. The states could be more refined than
those shown above: the state for the subject, for exam-
ple, should probably be not NP but a pair (Npl, NP3s).
STSG is simply a version of synchronous tree-
adjoining grammar or STAG (Shieber and Schabes, 1990)
that lacks the adjunction operation. (It is also equivalent
to top-down tree transducers.) What, then, is new here?
First, we know of no previous attempt to learn the
?chunk-to-chunk? mappings. That is, we do not know at
training time how the tree pair of section 1 was derived,
or even what it was derived from. Our approach is to
reconstruct all possible derivations, using dynamic pro-
gramming to decompose the tree pair into aligned pairs
of elementary trees in all possible ways. This produces
a packed forest of derivations, some more probable than
others. We use an efficient inside-outside algorithm to
do Expectation-Maximization, reestimating the model by
training on all derivations in proportion to their probabil-
ities. The runtime is quite low when the training trees are
fully specified and elementary trees are bounded in size.1
Second, it is not a priori obvious that one can reason-
ably use STSG instead of the slower but more powerful
STAG. TSG can be parsed as fast as CFG. But without
an adjunction operation,2, one cannot break the training
trees into linguistically minimal units. An elementary
tree pair A = (elle est finalement partie, finally she left)
cannot be further decomposed into B = (elle est partie,
she left) and C = (finalement, finally). This appears to
miss a generalization. Our perspective is that the gener-
alization should be picked up by the statistical model that
defines the probability of elementary tree pairs. p(A) can
be defined using mainly the same parameters that define
p(B) and p(C), with the result that p(A) ? p(B) ? p(C).
The balance between the STSG and the statistical model
is summarized in the last paragraph of this paper.
Third, our version of the STSG formalism is more
flexible than previous versions. We carefully address the
case of empty trees, which are needed to handle free-
translation ?mismatches.? In the example, an STSG can-
not replace beaucoup d? (?lots of?) in the NP by quite
often in the VP; instead it must delete the former and in-
sert the latter. Thus we have the alignments (beaucoup
d?, ) and (, quite often). These require innovations. The
tree-internal deletion of beaucoup d? is handled by an
empty elementary tree in which the root is itself a fron-
tier node. (The subject frontier node of kiss is replaced
with this frontier node, which is then replaced with kids.)
The tree-peripheral insertion of quite often requires an
English frontier node that is paired with a French null.
We also formulate STSGs flexibly enough that they can
handle both phrase-structure trees and dependency trees.
The latter are small and simple (Alshawi et al, 2000):
tree nodes are words, and there need be no other structure
to recover or align. Selectional preferences and other in-
teractions can be accommodated by enriching the states.
Any STSG has a weakly equivalent SCFG that gen-
erates the same string pairs. So STSG (unlike STAG)
has no real advantage for modeling string pairs.3 But
STSGs can generate a wider variety of tree pairs, e.g.,
non-isomorphic ones. So when actual trees are provided
for training, STSG can be more flexible in aligning them.
1Goodman (2002) presents efficient TSG parsing with un-
bounded elementary trees. Unfortunately, that clever method
does not permit arbitrary models of elementary tree probabili-
ties, nor does it appear to generalize to our synchronous case.
(It would need exponentially many nonterminals to keep track
of an matching of unboundedly many frontier nodes.)
2Or a sister-adjunction operation, for dependency trees.
3However, the binary-branching SCFGs used by Wu (1997)
and Alshawi et al (2000) are strictly less powerful than STSG.
3 Past Work
Most statistical MT derives from IBM-style models
(Brown et al, 1993), which ignore syntax and allow ar-
bitrary word-to-word translation. Hence they are able to
align any sentence pair, however mismatched. However,
they have a tendency to translate long sentences into word
salad. Their alignment and translation accuracy improves
when they are forced to translate shallow phrases as con-
tiguous, potentially idiomatic units (Och et al, 1999).
Several researchers have tried putting ?more syntax?
into translation models: like us, they use statistical ver-
sions of synchronous grammars, which generate source
and target sentences in parallel and so describe their cor-
respondence.4 This approach offers four features absent
from IBM-style models: (1) a recursive phrase-based
translation, (2) a syntax-based language model, (3) the
ability to condition a word?s translation on the translation
of syntactically related words, and (4) polynomial-time
optimal alignment and decoding (Knight, 1999).
Previous work in statistical synchronous grammars
has been limited to forms of synchronous context-free
grammar (Wu, 1997; Alshawi et al, 2000; Yamada and
Knight, 2001). This means that a sentence and its trans-
lation must have isomorphic syntax trees, although they
may have different numbers of surface words if null
words  are allowed in one or both languages. This rigid-
ity does not fully describe real data.
The one exception is the synchronous DOP approach
of (Poutsma, 2000), which obtains an STSG by decom-
posing aligned training trees in all possible ways (and us-
ing ?naive? count-based probability estimates). However,
we would like to estimate a model from unaligned data.
4 A Probabilistic TSG Formalism
For expository reasons (and to fill a gap in the literature),
first we formally present non-synchronous TSG. Let Q be
a set of states. Let L be a set of labels that may decorate
nodes or edges. Node labels might be words or nontermi-
nals. Edge labels might include grammatical roles such
as Subject. In many trees, each node?s children have an
order, recorded in labels on the node?s outgoing edges.
An elementary tree is a a tuple ?V, V i, E, `, q, s?
where V is a set of nodes; V i ? V is the set of internal
nodes, and we write V f = V ?V i for the set of frontier
nodes; E ? V i ? V is a set of directed edges (thus all
frontier nodes are leaves). The graph ?V,E? must be con-
nected and acyclic, and there must be exactly one node
r ? V (the root) that has no incoming edges. The func-
tion ` : (V i ?E) ? L labels each internal node or edge;
q ? Q is the root state, and s : V f ? Q assigns a fron-
tier state to each frontier node (perhaps including r).
4The joint probability model can be formulated, if desired,
as a language model times a channel model.
A TSG is a set of elementary trees. The generation
process builds up a derived tree T that has the same form
as an elementary tree, and for which V f = ?. Initially,
T is chosen to be any elementary tree whose root state
T.q = Start. As long as T has any frontier nodes, T.V f ,
the process expands each frontier node d ? T.V f by sub-
stituting at d an elementary tree t whose root state, t.q,
equals d?s frontier state, T.s(d). This operation replaces
T with ?T.V ? t.V ?{d}, T.V i? t.V i, T.E?? t.E, T.`?
t.`, T.q, T.s ? t.s ? {d, t.q}?. Note that a function is re-
garded here as a set of ?input, output? pairs. T.E? is a
version of T.E in which d has been been replaced by t.r.
A probabilistic TSG also includes a function p(t | q),
which, for each state q, gives a conditional probability
distribution over the elementary trees t with root state q.
The generation process uses this distribution to randomly
choose which tree t to substitute at a frontier node of T
having state q. The initial value of T is chosen from p(t |
Start). Thus, the probability of a given derivation is a
product of p(t | q) terms, one per chosen elementary tree.
There is a natural analogy between (probabilistic)
TSGs and (probabilistic) CFGs. An elementary tree t
with root state q and frontier states q1 . . . qk (for k ? 0) is
analogous to a CFG rule q ? t q1 . . . qk. (By including t
as a terminal symbol in this rule, we ensure that distinct
elementary trees t with the same states correspond to dis-
tinct rules.) Indeed, an equivalent definition of the gener-
ation process first generates a derivation tree from this
derivation CFG, and then combines its terminal nodes t
(which are elementary trees) into the derived tree T .
5 Tree Parsing Algorithms for TSG
Given a a grammar G and a derived tree T , we may be in-
terested in constructing the forest of T ?s possible deriva-
tion trees (as defined above). We call this tree parsing,
as it finds ways of decomposing T into elementary trees.
Given a node c ? T.v, we would like to find all the
potential elementary subtrees t of T whose root t.r could
have contributed c during the derivation of T . Such an
elementary tree is said to fit c, in the sense that it is iso-
morphic to some subgraph of T rooted at c.
The following procedure finds an elementary tree t that
fits c. Freely choose a connected subgraph U of T such
that U is rooted at c (or is empty). Let t.V i be the vertex
set of U . Let t.E be the set of outgoing edges from nodes
in t.V i to their children, that is, t.E = T.E ? (t.V i ?
T.V ). Let t.` be the restriction of T.` to t.V i ? t.E, that
is, t.` = T.` ? ((t.V i ? t.E) ? L). Let t.V be the set
of nodes mentioned in t.E, or put t.V = {c} if t.V i =
t.E = ?. Finally, choose t.q freely from Q, and choose
s : t.V f ? Q to associate states with the frontier nodes
of t; the free choice is because the nodes of the derived
tree T do not specify the states used during the derivation.
How many elementary trees can we find that fit c? Let
us impose an upper bound k on |t.V i| and hence on |U |.
Then in an m-ary tree T , the above procedure considers at
most m
k?1
m?1 connected subgraphs U of order ? k rooted
at c. For dependency grammars, limiting to m ? 6 and
k = 3 is quite reasonable, leaving at most 43 subgraphs
U rooted at each node c, of which the biggest contain
only c, a child c? of c, and a child or sibling of c?. These
will constitute the internal nodes of t, and their remaining
children will be t?s frontier nodes.
However, for each of these 43 subgraphs, we must
jointly hypothesize states for all frontier nodes and the
root node. For |Q| > 1, there are exponentially many
ways to do this. To avoid having exponentially many hy-
potheses, one may restrict the form of possible elemen-
tary trees so that the possible states of each node of t
can be determined somehow from the labels on the corre-
sponding nodes in T . As a simple but useful example, a
node labeled NP might be required to have state NP. Rich
labels on the derived tree essentially provide supervision
as to what the states must have been during the derivation.
The tree parsing algorithm resembles bottom-up chart
parsing under the derivation CFG. But the input is a tree
rather than a string, and the chart is indexed by nodes of
the input tree rather than spans of the input string:5
1. for each node c of T , in bottom-up order
2. for each q ? Q, let ?c(q) = 0
3. for each elementary tree t that fits c
4. increment ?c(t.q) by p(t | t.q) ?
?
d?t.V f ?d(t.s(d))
The ? values are inside probabilities. After running the
algorithm, if r is the root of T , then ?r(Start) is the prob-
ability that the grammar generates T .
p(t | q) in line 4 may be found by hash lookup if the
grammar is stored explicitly, or else by some probabilistic
model that analyzes the structure, labels, and states of the
elementary tree t to compute its probability.
One can mechanically transform this algorithm to
compute outside probabilities, the Viterbi parse, the parse
forest, and other quantities (Goodman, 1999). One can
also apply agenda-based parsing strategies.
For a fixed grammar, the runtime and space are only
O(n) for a tree of n nodes. The grammar constant is the
number of possible fits to a node c of a fixed tree. As
noted above, there usually not many of these (unless the
states are uncertain) and they are simple to enumerate.
As discussed above, an inside-outside algorithm may
be used to compute the expected number of times each
elementary tree t appeared in the derivation of T . That is
the E step of the EM algorithm. In the M step, these ex-
pected counts (collected over a corpus of trees) are used
to reestimate the parameters ~? of p(t | q). One alternates
E and M steps till p(corpus | ~?) ?p(~?) converges to a local
maximum. The prior p(~?) can discourage overfitting.
5We gloss over the standard difficulty that the derivation
CFG may contain a unary rule cycle. For us, such a cycle is
a problem only when it arises solely from single-node trees.
6 Extending to Synchronous TSG
We are now prepared to discuss the synchronous case.
A synchronous TSG consists of a set of elementary tree
pairs. An elementary tree pair t is a tuple ?t1, t2, q,m, s?.
Here t1 and t2 are elementary trees without state la-
bels: we write tj = ?Vj , V ij , Ej , `j?. q ? Q is the
root state as before. m ? V f1 ? V
f
2 is a matching
between t1?s and t2?s frontier nodes,6. Let m? denote
m ? {(d1,null) : d1 is unmatched in m} ? {(null, d2) :
d2 is unmatched in m}. Finally, s : m? ? Q assigns a
state to each frontier node pair or unpaired frontier node.
In the figure of section 2, donnent un baiser a` has 2
frontier nodes and kiss has 3, yielding 13 possible match-
ings. Note that least one English node must remain un-
matched; it still generates a full subtree, aligned with null.
As before, a derived tree pair T has the same form as
an elementary tree pair. The generation process is similar
to before. As long as T.m? 6= ?, the process expands some
node pair (d1, d2) ? T.m?. It chooses an elementary tree
pair t such that t.q = T.s(d1, d2). Then for each j = 1, 2,
it substitutes tj at dj if non-null. (If dj is null, then t.q
must guarantee that tj is the special null tree.)
In the probabilistic case, we have a distribution p(t | q)
just as before, but this time t is an elementary tree pair.
Several natural algorithms are now available to us:
? Training. Given an unaligned tree pair (T1, T2), we
can again find the forest of all possible derivations, with
expected inside-outside counts of the elementary tree
pairs. This allows EM training of the p(t | q) model.
The algorithm is almost as before. The outer loop iter-
ates bottom-up over nodes c1 of T1; an inner loop iter-
ates bottom-up over c2 of T2. Inside probabilities (for
example) now have the form ?c1,c2(q). Although this
brings the complexity up to O(n2), the real complica-
tion is that there can be many fits to (c1, c2). There are
still not too many elementary trees t1 and t2 rooted at c1
and c2; but each (t1, t2) pair may be used in many ele-
mentary tree pairs t, since there are exponentially many
matchings of their frontier nodes. Fortunately, most
pairs of frontier nodes have low ? values that indicate
that their subtrees cannot be aligned well; pairing such
nodes in a matching would result in poor global proba-
bility. This observation can be used to prune the space
of matchings greatly.
? 1-best Alignment (if desired). This is just like train-
ing, except that we use the Viterbi algorithm to find the
single best derivation of the input tree pair. This deriva-
tion can be regarded as the optimal syntactic alignment.7
6A matching between A and B is a 1-to-1 correspondence
between a subset of A and a subset of B.
7As free-translation post-processing, one could try to match
pairs of stray subtrees that could have aligned well, according to
the chart, but were forced to align with null for global reasons.
? Decoding. We create a forest of possible synchronous
derivations (cf. (Langkilde, 2000)). We chart-parse T1
as much as in section 5, but fitting the left side of an
elementary tree pair to each node. Roughly speaking:
1. for c1 = null and then c1 ? T1.V , in bottom-up order
2. for each q ? Q, let ?c1(q) = ??
3. for each probable t = (t1, t2, q,m, s) whose t1 fits c1
4. max p(t | q) ?
?
(d1,d2)?m?
?d1(s(d1, d2)) into ?c1(q)
We then extract the max-probability synchronous
derivation and return the T2 that it derives. This algo-
rithm is essentially alignment to an unknown tree T2;
we do not loop over its nodes c2, but choose t2 freely.
7 Status of the Implementation
We have sketched an EM algorithm to learn the probabil-
ities of elementary tree pairs by training on pairs of full
trees, and a Viterbi decoder to find optimal translations.
We developed and implemented these methods at the
2002 CLSP Summer Workshop at Johns Hopkins Univer-
sity, as part of a team effort (led by Jan Hajic?) to translate
dependency trees from surface Czech, to deep Czech, to
deep English, to surface English. For the within-language
translations, it sufficed to use a simplistic, fixed model of
p(t | q) that relied entirely on morpheme identity.
Team members are now developing real, trainable
models of p(t | q), such as log-linear models on meaning-
ful features of the tree pair t. Cross-language translation
results await the plugging-in of these interesting models.
The algorithms we have presented serve only to ?shrink?
the modeling, training and decoding problems from full
trees to bounded, but still complex, elementary trees.
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning
dependency translation models as collections of finite state
head transducers. Computational Linguistics, 26(1):45?60.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2001. The
Prague dependency treebank. In A. Abeille?, ed., Treebanks:
Building & Using Syntactically Annotated Corpora. Kluwer.
Joshua Goodman. 1999. Semiring parsing. Computational
Linguistics, 25(4):573?605, December.
Joshua Goodman. 2002. Efficient parsing of DOP with PCFG-
reductions. In Rens Bod, Khalil Sima?an, and Remko Scha,
editors, Data Oriented Parsing. CSLI.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization?step 1: Sentence compression. Proc. AAAI.
Kevin Knight. 1999. Decoding complexity in word-replace-
ment translation models. Computational Linguistics, 25(4).
Irene Langkilde. 2000. Forest-based statistical sentence gener-
ation. In Proceedings of NAACL.
F. Och, C. Tillmann, and H. Ney. 1999. Improved alignment
models for statistical machine translation. Proc. of EMNLP.
A. Poutsma. 2000. Data-oriented translation. Proc. COLING.
Stuart Shieber and Yves Schabes. 1990. Synchronous tree ad-
joining grammars. In Proc. of COLING.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Comp. Ling., 23(3).
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of ACL.
This work was supported by ONR grant N00014-01-1-0685,
?Improving Statistical Models Via Text Analyzers Trained
from Parallel Corpora.? The views expressed are the author?s.
Annealing Techniques for Unsupervised Statistical Language Learning
Noah A. Smith and Jason Eisner
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,jason}@cs.jhu.edu
Abstract
Exploiting unannotated natural language data is hard
largely because unsupervised parameter estimation is
hard. We describe deterministic annealing (Rose et al,
1990) as an appealing alternative to the Expectation-
Maximization algorithm (Dempster et al, 1977). Seek-
ing to avoid search error, DA begins by globally maxi-
mizing an easy concave function and maintains a local
maximum as it gradually morphs the function into the
desired non-concave likelihood function. Applying DA
to parsing and tagging models is shown to be straight-
forward; significant improvements over EM are shown
on a part-of-speech tagging task. We describe a vari-
ant, skewed DA, which can incorporate a good initializer
when it is available, and show significant improvements
over EM on a grammar induction task.
1 Introduction
Unlabeled data remains a tantalizing potential re-
source for NLP researchers. Some tasks can thrive
on a nearly pure diet of unlabeled data (Yarowsky,
1995; Collins and Singer, 1999; Cucerzan and
Yarowsky, 2003). But for other tasks, such as ma-
chine translation (Brown et al, 1990), the chief
merit of unlabeled data is simply that nothing else
is available; unsupervised parameter estimation is
notorious for achieving mediocre results.
The standard starting point is the Expectation-
Maximization (EM) algorithm (Dempster et al,
1977). EM iteratively adjusts a model?s parame-
ters from an initial guess until it converges to a lo-
cal maximum. Unfortunately, likelihood functions
in practice are riddled with suboptimal local max-
ima (e.g., Charniak, 1993, ch. 7). Moreover, max-
imizing likelihood is not equivalent to maximizing
task-defined accuracy (e.g., Merialdo, 1994).
Here we focus on the search error problem. As-
sume that one has a model for which improving
likelihood really will improve accuracy (e.g., at pre-
dicting hidden part-of-speech (POS) tags or parse
trees). Hence, we seek methods that tend to locate
mountaintops rather than hilltops of the likelihood
function. Alternatively, we might want methods that
find hilltops with other desirable properties.1
1Wang et al (2003) suggest that one should seek a high-
In ?2 we review deterministic annealing (DA)
and show how it generalizes the EM algorithm. ?3
shows how DA can be used for parameter estimation
for models of language structure that use dynamic
programming to compute posteriors over hidden
structure, such as hidden Markov models (HMMs)
and stochastic context-free grammars (SCFGs). In
?4 we apply DA to the problem of learning a tri-
gram POS tagger without labeled data. We then de-
scribe how one of the received strengths of DA?
its robustness to the initializing model parameters?
can be a shortcoming in situations where the ini-
tial parameters carry a helpful bias. We present
a solution to this problem in the form of a new
algorithm, skewed deterministic annealing (SDA;
?5). Finally we apply SDA to a grammar induc-
tion model and demonstrate significantly improved
performance over EM (?6). ?7 highlights future di-
rections for this work.
2 Deterministic annealing
Suppose our data consist of a pairs of random vari-
ables X and Y , where the value of X is observed
and Y is hidden. For example, X might range
over sentences in English and Y over POS tag se-
quences. We use X and Y to denote the sets of
possible values of X and Y , respectively. We seek
to build a model that assigns probabilities to each
(x, y) ? X?Y. Let ~x = {x1, x2, ..., xn} be a corpus
of unlabeled examples. Assume the class of models
is fixed (for example, we might consider only first-
order HMMs with s states, corresponding notion-
ally to POS tags). Then the task is to find good pa-
rameters ~? ? RN for the model. The criterion most
commonly used in building such models from un-
labeled data is maximum likelihood (ML); we seek
the parameters ~??:
argmax
~?
Pr(~x | ~?) = argmax
~?
n?
i=1
?
y?Y
Pr(xi, y | ~?) (1)
entropy hilltop. They argue that to account for partially-
observed (unlabeled) data, one should choose the distribution
with the highest Shannon entropy, subject to certain data-driven
constraints. They show that this desirable distribution is one of
the local maxima of likelihood. Whether high-entropy local
maxima really predict test data better is an empirical question.
Input: ~x, ~?(0) Output: ~??
i? 0
do:
(E) p?(~y)?
Pr(~x,~y|~?(i))
?
~y??Yn Pr(~x,~y?|~?(i))
,?~y
(M) ~?(i+1) ? argmax~? Ep?(~Y )
[
log Pr(~x, ~Y | ~?)
]
i? i+ 1
until ~?(i) ? ~?(i?1)
~?? ? ~?(i)
Fig. 1: The EM algorithm.
Each parameter ?j corresponds to the conditional
probability of a single model event, e.g., a state tran-
sition in an HMM or a rewrite in a PCFG. Many
NLP models make it easy to maximize the likeli-
hood of supervised training data: simply count the
model events in the observed (xi, yi) pairs, and set
the conditional probabilities ?i to be proportional to
the counts. In our unsupervised setting, the yi are
unknown, but solving (1) is almost as easy provided
that we can obtain the posterior distribution of Y
given each xi (that is, Pr(y | xi) for each y ? Y
and each xi). The only difference is that we must
now count the model events fractionally, using the
expected number of occurrences of each (xi, y) pair.
This intuition leads to the EM algorithm in Fig. 1.
It is guaranteed that Pr(~x | ~?(i+1)) ? Pr(~x | ~?(i)).
For language-structure models like HMMs and
SCFGs, efficient dynamic programming algorithms
(forward-backward, inside-outside) are available to
compute the distribution p? at the E step of Fig. 1
and use it at the M step. These algorithms run in
polynomial time and space by structure-sharing the
possible y (tag sequences or parse trees) for each
xi, of which there may be exponentially many in
the length of xi. Even so, the majority of time spent
by EM for such models is on the E steps. In this pa-
per, we can fairly compare the runtime of EM and
other training procedures by counting the number of
E steps they take on a given training set and model.
2.1 Generalizing EM
Figure 2 shows the deterministic annealing (DA) al-
gorithm derived from the framework of Rose et al
(1990). It is quite similar to EM.2 However, DA
adds an outer loop that iteratively increases a value
?, and computation of the posterior in the E step is
modified to involve this ?.
2Other expositions of DA abound; we have couched ours in
data-modeling language. Readers interested in the Lagrangian-
based derivations and analogies to statistical physics (including
phase transitions and the role of ? as the inverse of temperature
in free-energy minimization) are referred to Rose (1998) for a
thorough discussion.
Input: ~x, ~?(0), ?max>?min>0, ?>1 Output: ~??
i? 0; ? ? ?min
while ? ? ?max:
do:
(E) p?(~y)?
Pr(~x,~y|~?(i))
?
?
~y??Yn Pr(~x,~y?|~?(i))
? ,?~y
(M) ~?(i+1) ? argmax~? Ep?(~Y )
[
log Pr(~x, ~Y | ~?)
]
i? i+ 1
until ~?(i) ? ~?(i?1)
? ? ? ? ?
end while
~?? ? ~?(i)
Fig. 2: The DA algorithm: a generalization of EM.
When ? = 1, DA?s inner loop will behave exactly
like EM, computing p? at the E step by the same for-
mula that EM uses. When ? ? 0, p? will be close
to a uniform distribution over the hidden variable ~y,
since each numerator Pr(~x, ~y | ~?)? ? 1. At such
?-values, DA effectively ignores the current param-
eters ? when choosing the posterior p? and the new
parameters. Finally, as ? ? +?, p? tends to place
nearly all of the probability mass on the single most
likely ~y. This winner-take-all situation is equivalent
to the ?Viterbi? variant of the EM algorithm.
2.2 Gradated difficulty
In both the EM and DA algorithms, the E step se-
lects a posterior p? over the hidden variable ~Y and
the M step selects parameters ~?. Neal and Hinton
(1998) show how the EM algorithm can be viewed
as optimizing a single objective function over both ~?
and p?. DA can also be seen this way; DA?s objective
function at a given ? is
F
(
~?, p?, ?
)
=
1
?
H(p?) +Ep?(~Y )
[
log Pr(~x, ~Y | ~?)
]
(2)
The EM version simply sets ? = 1. A complete
derivation is not difficult but is too lengthy to give
here; it is a straightforward extension of that given
by Neal and Hinton for EM.
It is clear that the value of ? allows us to manip-
ulate the relative importance of the two terms when
maximizing F. When ? is close to 0, only the H
term matters. The H term is the Shannon entropy
of the posterior distribution p?, which is known to be
concave in p?. Maximizing it is simple: set al x to be
equiprobable (the uniform distribution). Therefore
a sufficiently small ? drives up the importance of
H relative to the other term, and the entire problem
becomes concave with a single global maximum to
which we expect to converge.
In gradually increasing ? from near 0 to 1, we
start out by solving an easy concave maximization
problem and use the result to initialize the next max-
imization problem, which is slightly more difficult
(i.e., less concave). This continues, with the solu-
tion to each problem in the series being used to ini-
tialize the subsequent problem. When ? reaches 1,
DA behaves just like EM. Since the objective func-
tion is continuous in ? where ? > 0, we can vi-
sualize DA as gradually morphing the easy concave
objective function into the one we really care about
(likelihood); we hope to ?ride the maximum? as ?
moves toward 1.
DA guarantees iterative improvement of the ob-
jective function (see Ueda and Nakano (1998) for
proofs). But it does not guarantee convergence to
a global maximum, or even to a better local maxi-
mum than EM will find, even with extremely slow
?-raising. A new mountain on the surface of the
objective function could arise at any stage that is
preferable to the one that we will ultimately find.
To run DA, we must choose a few control param-
eters. In this paper we set ?max = 1 so that DA
will approach EM and finish at a local maximum of
likelihood. ?min and the ?-increase factor ? can be
set high for speed, but at a risk of introducing lo-
cal maxima too quickly for DA to work as intended.
(Note that a ?fast? schedule that tries only a few ?
values is not as fast as one might expect, since it will
generally take longer to converge at each ? value.)
To conclude the theoretical discussion of DA, we
review its desirable properties. DA is robust to ini-
tial parameters, since when ? is close to 0 the ob-
jective hardly depends on ~?. DA gradually increases
the difficulty of search, which may lead to the avoid-
ance of some local optima. By modifying the an-
nealing schedule, we can change the runtime of the
DA algorithm. DA is almost exactly like EM in im-
plementation, requiring only a slight modification to
the E step (see ?3) and an additional outer loop.
2.3 Prior work
DA was originally described as an algorithm for
clustering data in RN (Rose et al, 1990). Its pre-
decessor, simulated annealing, modifies the objec-
tive function during search by applying random per-
turbations of gradually decreasing size (Kirkpatrick
et al, 1983). Deterministic annealing moves the
randomness ?inside? the objective function by tak-
ing expectations. DA has since been applied to
many problems (Rose, 1998); we describe two key
applications in language and speech processing.
Pereira, Tishby, and Lee (1993) used DA for soft
hierarchical clustering of English nouns, based on
the verbs that select them as direct objects. In their
case, when ? is close to 0, each noun is fuzzily
placed in each cluster so that Pr(cluster | noun)
is nearly uniform. On the M step, this results in
clusters that are almost exactly identical; there is
one effective cluster. As ? is increased, it becomes
increasingly attractive for the cluster centroids to
move apart, or ?split? into two groups (two effective
clusters), and eventually they do so. Continuing to
increase ? yields a hierarchical clustering through
repeated splits. Pereira et al describe the tradeoff
given through ? as a control on the locality of influ-
ence of each noun on the cluster centroids, so that as
? is raised, each noun exerts less influence on more
distant centroids and more on the nearest centroids.
DA has also been applied in speech recognition.
Rao and Rose (2001) used DA for supervised dis-
criminative training of HMMs. Their goal was
to optimize not likelihood but classification error
rate, a difficult objective function that is piecewise-
constant (hence not differentiable everywhere) and
riddled with shallow local minima. Rao and Rose
applied DA,3 moving from training a nearly uni-
form classifier with a concave cost surface (? ? 0)
toward the desired deterministic classifier (? ?
+?). They reported substantial gains in spoken
letter recognition accuracy over both a ML-trained
classifier and a localized error-rate optimizer.
Brown et al (1990) gradually increased learn-
ing difficulty using a series of increasingly complex
models for machine translation. Their training al-
gorithm began by running an EM approximation on
the simplest model, then used the result to initialize
the next, more complex model (which had greater
predictive power and many more parameters), and
so on. Whereas DA provides gradated difficulty
in parameter search, their learning method involves
gradated difficulty among classes of models. The
two are orthogonal and could be used together.
3 DA with dynamic programming
We turn now to the practical use of determinis-
tic annealing in NLP. Readers familiar with the
EM algorithm will note that, for typical stochas-
tic models of language structure (e.g., HMMs and
SCFGs), the bulk of the computational effort is re-
quired by the E step, which is accomplished by
a two-pass dynamic programming (DP) algorithm
(like the forward-backward algorithm). The M step
for these models normalizes the posterior expected
counts from the E step to get probabilities.4
3With an M step modified for their objective function: it im-
proved expected accuracy under p?, not expected log-likelihood.
4That is, assuming the usual generative parameterization of
such models; if we generalize to Markov random fields (also
known as log-linear or maximum entropy models) the M step,
while still concave, might entail an auxiliary optimization rou-
tine such as iterative scaling or a gradient-based method.
Running DA for such models is quite simple and
requires no modifications to the usual DP algo-
rithms. The only change to make is in the values
of the parameters passed to the DP algorithm: sim-
ply replace each ?j by ??j . For a given x, the forward
pass of the DP computes (in a dense representation)
Pr(y | x, ~?) for all y. Each Pr(y | x, ~?) is a product
of some of the ?j (each ?j is multiplied in once for
each time its corresponding model event is present
in (x, y)). Raising the ?j to a power will also raise
their product to that power, so the forward pass will
compute Pr(y | x, ~?)? when given ~?? as parameter
values. The backward pass normalizes to the sum;
in this case it is the sum of the Pr(y | x, ~?)? , and
we have the E step described in Figure 2. We there-
fore expect an EM iteration of DA to take the same
amount of time as a normal EM iteration.5
4 Part-of-speech tagging
We turn now to the task of inducing a trigram POS
tagging model (second-order HMM) from an unla-
beled corpus. This experiment is inspired by the
experiments in Merialdo (1994). As in that work,
complete knowledge of the tagging dictionary is as-
sumed. The task is to find the trigram transition
probabilities Pr(tag i | tag i?1, tag i?2) and emis-
sion probabilities Pr(word i | tag i). Merialdo?s key
result:6 If some labeled data were used to initialize
the parameters (by taking the ML estimate), then it
was not helpful to improve the model?s likelihood
through EM iterations, because this almost always
hurt the accuracy of the model?s Viterbi tagging on
a held-out test set. If only a small amount of labeled
data was used (200 sentences), then some accuracy
improvement was possible using EM, but only for
a few iterations. When no labeled data were used,
EM was able to improve the accuracy of the tagger,
and this improvement continued in the long term.
Our replication of Merialdo?s experiment used
the Wall Street Journal portion of the Penn Tree-
bank corpus, reserving a randomly selected 2,000
sentences (48,526 words) for testing. The remain-
ing 47,208 sentences (1,125,240 words) were used
in training, without any tags. The tagging dictionary
was constructed using the entire corpus (as done by
Merialdo). To initialize, the conditional transition
and emission distributions in the HMM were set to
uniform with slight perturbation. Every distribution
was smoothed using add-0.1 smoothing (at every M
5With one caveat: less pruning may be appropriate because
probability mass is spread more uniformly over different recon-
structions of the hidden data. This paper uses no pruning.
6Similar results were found by Elworthy (1994).
Fig. 3:  Learning curves for
EM and DA.  Steps in DA?s curve
correspond to   ?changes.  The shape of
the DA curve is partly a function of the an?
nealing schedule, which only gradually (and
away from the uniform distribution.
in steps) allows the parameters to move 
?
 40
 45
 50
 55
 60
 65
 70
 75
 0  200  400  600  800  1000  1200%
 co
rre
ct a
mb
igu
ous
 tes
t ta
gs
EM iterations
DAEM
step). The criterion for convergence is that the rela-
tive increase in the objective function between two
iterations fall below 10?9.
4.1 Experiment
In the DA condition, we set ?min = 0.0001, ?max =
1, and ? = 1.2. Results for the completely unsuper-
vised condition (no labeled data) are shown in Fig-
ure 3 and Table 1. Accuracy was nearly monotonic:
the final model is approximately the most accurate.
DA happily obtained a 10% reduction in tag er-
ror rate on training data, and an 11% reduction on
test data. On the other hand, it did not manage to
improve likelihood over EM. So was the accuracy
gain mere luck? Perhaps not. DA may be more re-
sistant to overfitting, because it may favor models
whose posteriors p? have high entropy. At least in
this experiment, its initial bias toward such models
carried over to the final learned model.7
In other words, the higher-entropy local maxi-
mum found by DA, in this case, explained the ob-
served data almost as well without overcommit-
ting to particular tag sequences. The maximum en-
tropy and latent maximum entropy principles (Wang
et al, 2003, discussed in footnote 1) are best justi-
fied as ways to avoid overfitting.
For a supervised tagger, the maximum entropy
principle prefers a conditional model Pr(~y | ~x) that
is maximally unsure about what tag sequence ~y to
apply to the training word sequence ~x (but expects
the same feature counts as the true ~y). Such a model
is hoped to generalize better to unsupervised data.
We can make the same argument. But in our case,
the split between supervised/unsupervised data is
not the split between training/test data. Our super-
vised data are, roughly, the fragments of the training
corpus that are unambiguously tagged thanks to the
tag dictionary.8 The EM model may overfit some
7We computed the entropy over possible tags for each word
in the test corpus, given the sentence the word occurs in. On
average, the DA model had 0.082 bits per tag, while EM had
only 0.057 bits per tag, a statistically significant difference (p <
10?6) under a binomial sign test on word tokens.
8Without the tag dictionary, our learners would treat the tag
final training cross- final test cross- % correct training tags % correct test tags
E steps entropy (bits/word) entropy (bits/word) (all) (ambiguous) (all) (ambiguous)
EM 279 9.136 9.321 82.04 66.61 82.08 66.63
DA 1200 9.138 9.325 83.85 70.02 84.00 70.25
Table 1: EM vs. DA on unsupervised trigram POS tagging, using a tag dictionary. Each of the accuracy results is significant when
accuracy is compared at either the word-level or sentence-level. (Significance at p < 10?6 under a binomial sign test in each
case. E.g., on the test set, the DA model correctly tagged 1,652 words that EM?s model missed while EM correctly tagged 726
words that DA missed. Similarly, the DA model had higher accuracy on 850 sentences, while EM had higher accuracy on only 287.
These differences are extremely unlikely to occur due to chance.) The differences in cross-entropy, compared by sentence, were
significant in the training set but not the test set (p < 0.01 under a binomial sign test). Recall that lower cross entropy means higher
likelihood.
parameters to these fragments. The higher-entropy
DA model may be less likely to overfit, allowing it
to do better on the unsupervised data?i.e., the rest
of the training corpus and the entire test corpus.
We conclude that DA has settled on a local maxi-
mum of the likelihood function that (unsurprisingly)
corresponds well with the entropy criterion, and per-
haps as a result, does better on accuracy.
4.2 Significance
Seeking to determine how well this result general-
ized, we randomly split the corpus into ten equally-
sized, nonoverlapping parts. EM and DA were run
on each portion;9 the results were inconclusive. DA
achieved better test accuracy than EM on three of
ten trials, better training likelihood on five trials,
and better test likelihood on all ten trials.10 Cer-
tainly decreasing the amount of data by an order of
magnitude results in increased variance of the per-
formance of any algorithm?so ten small corpora
were not enough to determine whether to expect an
improvement from DA more often than not.
4.3 Mixing labeled and unlabeled data (I)
In the other conditions described by Merialdo, vary-
ing amounts of labeled data (ranging from 100 sen-
tences to nearly half of the corpus) were used to
initialize the parameters ~?, which were then trained
using EM on the remaining unlabeled data. Only
in the case where 100 labeled examples were used,
and only for a few iterations, did EM improve the
names as interchangeable and could not reasonably be evalu-
ated on gold-standard accuracy.
9The smoothing parameters were scaled down so as to be
proportional to the corpus size.
10It is also worth noting that runtimes were longer with the
10%-sized corpora than the full corpus (EM took 1.5 times as
many E steps; DA, 1.3 times). Perhaps the algorithms traveled
farther to find a local maximum. We know of no study of the
effect of unlabeled training set size on the likelihood surface,
but suggest two issues for future exploration. Larger datasets
contain more idiosyncrasies but provide a stronger overall sig-
nal. Hence, we might expect them to yield a bumpier likelihood
surface whose local maxima are more numerous but also dif-
fer more noticeably in height. Both these tendencies of larger
datasets would in theory increase DA?s advantage over EM.
accuracy of this model. We replicated these experi-
ments and compared EM with DA; DA damaged the
models even more than EM. This is unsurprising; as
noted before, DA effectively ignores the initial pa-
rameters ~?(0). Therefore, even if initializing with a
model trained on small amounts of labeled data had
helped EM, DA would have missed out on this ben-
efit. In the next section we address this issue.
5 Skewed deterministic annealing
The EM algorithm is quite sensitive to the initial pa-
rameters ~?(0). We touted DA?s insensitivity to those
parameters as an advantage, but in scenarios where
well-chosen initial parameters can be provided (as
in ?4.3), we wish for DA to be able exploit them.
In particular, there are at least two cases where
?good? initializers might be known. One is the
case explored by Merialdo, where some labeled data
were available to build an initial model. The other is
a situation where a good distribution is known over
the labels y; we will see an example of this in ?6.
We wish to find a way to incorporate an initializer
into DA and still reap the benefit of gradated diffi-
culty. To see how this will come about, consider
again the E step for DA, which for all y:
p?(y)?
Pr(x, y | ~?)?
Z ?(~?, ?)
=
Pr(x, y | ~?)?u(y)1??
Z(~?, ?)
where u is the uniform distribution over Y and
Z ?(~?, ?) and Z(~?, ?) = Z ?(~?, ?) ? u(y)1?? are nor-
malizing terms. (Note that Z(~?, ?) does not depend
on y because u(y) is constant with respect to y.) Of
course, when ? is close to 0, DA chooses the uni-
form posterior because it has the highest entropy.
Seen this way, DA is interpolating in the log do-
main between two posteriors: the one given by y
and ~? and the uniform one u; the interpolation coef-
ficient is ?. To generalize DA, we will replace the
uniform u with another posterior, the ?skew? pos-
terior p?, which is an input to the algorithm. This
posterior might be specified directly, as it will be in
?6, or it might be computed using an M step from
some good initial ~?(0).
The skewed DA (SDA) E step is given by:
p?(y)?
1
Z(?)
Pr(x, y | ?)? p?(y)1?? (3)
When ? is close to 0, the E step will choose p? to
be very close to p?. With small ?, SDA is a ?cau-
tious? EM variant that is wary of moving too far
from the initializing posterior p? (or, equivalently, the
initial parameters ~?(0)). As ? approaches 1, the ef-
fect of p? will diminish, and when ? = 1, the algo-
rithm becomes identical to EM. The overall objec-
tive (matching (2) except for the boxed term) is:
F?
(
~?, p?, ?
)
=
1
?
H(p?) +Ep?(~Y )
[
log Pr
(
~x, ~Y | ~?
)]
+ 1? ?
?
Ep?(~Y )
[
log p?
(
~Y
)]
Mixing labeled and unlabeled data (II) Return-
ing to Merialdo?s mixed conditions (?4.3), we found
that SDA repaired the damage done by DA but did
not offer any benefit over EM. Its behavior in the
100-labeled sentence condition was similar to that
of EM?s, with a slightly but not significantly higher
peak in training set accuracy. In the other condi-
tions, SDA behaved like EM, with steady degrada-
tion of accuracy as training proceeded. It ultimately
damaged performance only as much as EM did or
did slightly better than EM (but still hurt).
This is unsurprising: Merialdo?s result demon-
strated that ML and maximizing accuracy are gener-
ally not the same; the EM algorithm consistently de-
graded the accuracy of his supervised models. SDA
is simply another search algorithm with the same
criterion as EM. SDA did do what it was expected
to do?it used the initializer, repairing DA damage.
6 Grammar induction
We turn next to the problem of statistical grammar
induction: inducing parse trees over unlabeled text.
An excellent recent result is by Klein and Manning
(2002). The constituent-context model (CCM) they
present is a generative, deficient channel model of
POS tag strings given binary tree bracketings. We
first review the model and describe a small mod-
ification that reduces the deficiency, then compare
both models under EM and DA.
6.1 Constituent-context model
Let (x, y) be a (tag sequence, binary tree) pair. xji
denotes the subsequence of x from the ith to the
jth word. Let yi,j be 1 if the yield from i to j is a
constituent in the tree y and 0 if it is not. The CCM
gives to a pair (x, y) the following probability:
Pr(x, y) = Pr(y) ?
?
1?i?j?|x|
[
?
(
xji
?
?
? yi,j
)
? ? (xi?1, xj+1| yi,j)
]
where ? is a conditional distribution over possi-
ble tag-sequence yields (given whether the yield is
a constituent or not) and ? is a conditional distribu-
tion over possible contexts of one tag on either side
of the yield (given whether the yield is a constituent
or not). There are therefore four distributions to be
estimated; Pr(y) is taken to be uniform.
The model is initialized using expected counts of
the constituent and context features given that all
the trees are generated according to a random-split
model.11
The CCM generates each tag not once but O(n2)
times, once by every constituent or non-constituent
span that dominates it. We suggest the following
modification to alleviate some of the deficiency:
Pr(x, y) = Pr(y) ?
?
1?i?j?|x|
[
?
(
xji
?
?
? yi,j , j ? i+ 1
)
?? (xi?1, xj+1| yi,j)
]
The change is to condition the yield feature ? on
the length of the yield. This decreases deficiency by
disallowing, for example, a constituent over a four-
tag yield to generate a seven-tag sequence. It also
decreases inter-parameter dependence by breaking
the constituent (and non-constituent) distributions
into a separate bin for each possible constituent
length. We will refer to Klein and Manning?s CCM
and our version as models 1 and 2, respectively.
6.2 Experiment
We ran experiments using both CCM models on
the tag sequences of length ten or less in the Wall
Street Journal Penn Treebank corpus, after extract-
ing punctuation. This corpus consists of 7,519 sen-
tences (52,837 tag tokens, 38 types). We report
PARSEVAL scores averaged by constituent (rather
than by sentence), and do not give the learner credit
for getting full sentences or single tags as con-
stituents.12 Because the E step for this model is
computationally intensive, we set the DA parame-
ters at ?min = 0.01, ? = 1.5 so that fewer E steps
would be necessary.13 The convergence criterion
was relative improvement < 10?9 in the objective.
The results are shown in Table 2. The first point
to notice is that a uniform initializer is a bad idea,
as Klein and Manning predicted. All conditions but
11We refer readers to Klein and Manning (2002) or Cover
and Thomas (1991, p. 72) for details; computing expected
counts for a sentence is a closed form operation. Klein and
Manning?s argument for this initialization step is that it is less
biased toward balanced trees than the uniform model used dur-
ing learning; we also found that it works far better in practice.
12This is why the CCM 1 performance reported here differs
from Klein and Manning?s; our implementation of the EM con-
dition gave virtually identical results under either evaluation
scheme (D. Klein, personal communication).
13A pilot study got very similar results for ?min = 10?6.
E steps cross-entropy (bits/tag) UR UP F CB
CCM 1 EM (uniform) 146 103.1654 61.20 45.62 52.27 1.69
DA 403 103.1542 55.13 41.10 47.09 1.91
EM (split) 124 103.1951 78.14 58.24 66.74 0.98
SDA (split) 339 103.1651 62.71 46.75 53.57 1.62
CCM 2 EM (uniform) 26 84.8106 57.60 42.94 49.20 1.86
DA 331 84.7899 40.81 30.42 34.86 2.66
EM (split) 44 84.8049 78.56 58.56 67.10 0.98
SDA (split) 290 84.7940 79.64 59.37 68.03 0.93
Table 2: The two CCM models, trained with two unsupervised algorithms, each with two initializers. Note that DA is equivalent
to SDA initialized with a uniform distribution. The third line corresponds to the setup reported by Klein and Manning (2002).
UR is unlabeled recall, UP is unlabeled precision, F is their harmonic mean, and CB is the average number of crossing brackets
per sentence. All evaluation is on the same data used for unsupervised learning (i.e., there is no training/test split). The high
cross-entropy values arise from the deficiency of models 1 and 2, and are not comparable across models.
one find better structure when initialized with Klein
and Manning?s random-split model. (The exception
is SDA on model 1; possibly the high deficiency of
model 1 interacts poorly with SDA?s search in some
way.)
Next we note that with the random-split initial-
izer, our model 2 is a bit better than model 1 on
PARSEVAL measures and converges more quickly.
Every instance of DA or SDA achieved higher
log-likelihood than the corresponding EM condi-
tion. This is what we hoped to gain from annealing:
better local maxima. In the case of model 2 with
the random-split initializer, SDA significantly out-
performed EM (comparing both matches and cross-
ing brackets per sentence under a binomial sign test,
p < 10?6); we see a > 5% reduction in average
crossing brackets per sentence. Thus, our strategy
of using DA but modifying it to accept an initial-
izer worked as desired in this case, yielding our best
overall performance.
The systematic results we describe next suggest
that these patterns persist across different training
sets in this domain.
6.3 Significance
The difficulty we experienced in finding generaliza-
tion to small datasets, discussed in ?4.2, was appar-
ent here as well. For 10-way and 3-way random,
nonoverlapping splits of the dataset, we did not have
consistent results in favor of either EM or SDA. In-
terestingly, we found that training model 2 (using
EM or SDA) on 10% of the corpus resulted on av-
erage in models that performed nearly as well on
their respective training sets as the full corpus con-
dition did on its training set; see Table 3. In ad-
dition, SDA sometimes performed as well as EM
under model 1. For a random two-way split, EM
and SDA converged to almost identical solutions on
one of the sub-corpora, and SDA outperformed EM
significantly on the other (on model 2).
In order to get multiple points of comparison of
EM and SDA on this task with a larger amount of
data, we jack-knifed the WSJ-10 corpus by split-
ting it randomly into ten equally-sized nonoverlap-
ping parts then training models on the corpus with
each of the ten sub-corpora excluded.14 These trials
are not independent of each other; any two of the
sub-corpora have 89 of their training data in com-
mon. Aggregate results are shown in Table 3. Using
model 2, SDA always outperformed EM, and in 8 of
10 cases the difference was significant when com-
paring matching constituents per sentence (7 of 10
when comparing crossing constituents).15 The vari-
ance of SDA was far less than that of EM; SDA not
only always performed better with model 2, but its
performance was more consistent over the trials.
We conclude this experimental discussion by cau-
tioning that both CCM models are highly deficient
models, and it is unknown how well they generalize
to corpora of longer sentences, other languages, or
corpora of words (rather than POS tags).
7 Future work
There are a number of interesting directions for fu-
ture work. Noting the simplicity of the DA algo-
rithm, we hope that current devotees of EM will
run comparisons of their models with DA (or SDA).
Not only might this improve performance of exist-
14Note that this is not a cross-validation experiment; results
are reported on the unlabeled training set, and the excluded sub-
corpus remains unused.
15Binomial sign test, with significance defined as p < 0.05,
though all significant results had p < 0.001.
10% corpus 90% corpus
?F ?F ?F ?F
CCM 1 EM 65.00 1.091 66.12 0.6643
SDA 63.00 4.689 53.53 0.2135
CCM 2 EM 66.74 1.402 67.24 0.7077
SDA 66.77 1.034 68.07 0.1193
Table 3: The mean ? and standard deviation ? of F -measure
performance for 10 trials using 10% of the corpus and 10 jack-
knifed trials using 90% of the corpus.
ing systems, it will contribute to the general under-
standing of the likelihood surface for a variety of
problems (e.g., this paper has raised the question of
how factors like dataset size and model deficiency
affect the likelihood surface).
DA provides a very natural way to gradually
introduce complexity to clustering models (Rose
et al, 1990; Pereira et al, 1993). This comes about
by manipulating the ? parameter; as it rises, the
number of effective clusters is allowed to increase.
An open question is whether the analogues of ?clus-
ters? in tagging and parsing models?tag symbols
and grammatical categories, respectively?might be
treated in a similar manner under DA. For instance,
we might begin with the CCM, the original formula-
tion of which posits only one distinction about con-
stituency (whether a span is a constituent or not) and
gradually allow splits in constituent-label space, re-
sulting in multiple grammatical categories that, we
hope, arise naturally from the data.
In this paper, we used ?max = 1. It would
be interesting to explore the effect on accuracy of
?quenching,? a phase at the end of optimization
that rapidly raises ? from 1 to the winner-take-all
(Viterbi) variant at ? = +?.
Finally, certain practical speedups may be possi-
ble. For instance, increasing ?min and ?, as noted
in ?2.2, will vary the number of E steps required for
convergence. We suggested that the change might
result in slower or faster convergence; optimizing
the schedule using an online algorithm (or deter-
mining precisely how these parameters affect the
schedule in practice) may prove beneficial. Another
possibility is to relax the convergence criterion for
earlier ? values, requiring fewer E steps before in-
creasing ?, or even raising ? slightly after every E
step (collapsing the outer and inner loops).
8 Conclusion
We have reviewed the DA algorithm, describing
it as a generalization of EM with certain desir-
able properties, most notably the gradual increase
of difficulty of learning and the ease of imple-
mentation for NLP models. We have shown how
DA can be used to improve the accuracy of a tri-
gram POS tagger learned from an unlabeled cor-
pus. We described a potential shortcoming of DA
for NLP applications?its failure to exploit good
initializers?and then described a novel algorithm,
skewed DA, that solves this problem. Finally, we re-
ported significant improvements to a state-of-the-art
grammar induction model using SDA and a slight
modification to the parameterization of that model.
These results support the case that annealing tech-
niques in some cases offer performance gains over
the standard EM approach to learning from unla-
beled corpora, particularly with large corpora.
Acknowledgements
This work was supported by a fellowship to the first au-
thor from the Fannie and John Hertz Foundation, and
by an NSF ITR grant to the second author. The views
expressed are not necessarily endorsed by the sponsors.
The authors thank Shankar Kumar, Charles Schafer,
David Smith, and Roy Tromble for helpful comments
and discussions; three ACL reviewers for advice that im-
proved the paper; Eric Goldlust for keeping the Dyna
compiler (Eisner et al, 2004) up to date with the de-
mands made by this work; and Dan Klein for sharing
details of his CCM implementation.
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Je-
linek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990.
A statistical approach to machine translation. Computational
Linguistics, 16(2):79?85.
E. Charniak. 1993. Statistical Language Learning. MIT Press.
M. Collins and Y. Singer. 1999. Unsupervised models for
named-entity classification. In Proc. of EMNLP.
T. M. Cover and J. A. Thomas. 1991. Elements of Information
Theory. John Wiley and Sons.
S. Cucerzan and D. Yarowsky. 2003. Minimally supervised
induction of grammatical gender. In Proc. of HLT/NAACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likeli-
hood estimation from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1?38.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declar-
ative language for implementing dynamic programs. In Proc.
of ACL (companion volume).
D. Elworthy. 1994. Does Baum-Welch re-estimation help tag-
gers? In Proc. of ANLP.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimiza-
tion by simulated annealing. Science, 220:671?680.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for grammar induction. In Proc. of ACL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?72.
R. Neal and G. Hinton. 1998. A view of the EM algorithm
that justifies incremental, sparse, and other variants. In M. I.
Jordan, editor, Learning in Graphical Models. Kluwer.
F. C. N. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proc. of ACL.
A. Rao and K. Rose. 2001. Deterministically annealed design
of Hidden Markov Model speech recognizers. IEEE Transac-
tions on Speech and Audio Processing, 9(2):111?126.
K. Rose, E. Gurewitz, and G. C. Fox. 1990. Statistical me-
chanics and phase transitions in clustering. Physical Review
Letters, 65(8):945?948.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimization
problems. Proc. of the IEEE, 86(11):2210?2239.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271?282.
S. Wang, D. Schuurmans, and Y. Zhao. 2003. The latent maxi-
mum entropy principle. In review.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. of ACL.
Dyna: A Declarative Language for Implementing Dynamic Programs?
Jason Eisner and Eric Goldlust and Noah A. Smith
Department of Computer Science, Johns Hopkins University
Baltimore, MD 21218 U.S.A.
{jason,eerat,nasmith}@cs.jhu.edu
Abstract
We present the first version of a new declarative pro-
gramming language. Dyna has many uses but was de-
signed especially for rapid development of new statis-
tical NLP systems. A Dyna program is a small set of
equations, resembling Prolog inference rules, that spec-
ify the abstract structure of a dynamic programming al-
gorithm. It compiles into efficient, portable, C++ classes
that can be easily invoked from a larger application. By
default, these classes run a generalization of agenda-
based parsing, prioritizing the partial parses by some
figure of merit. The classes can also perform an exact
backward (outside) pass in the service of parameter train-
ing. The compiler already knows several implementation
tricks, algorithmic transforms, and numerical optimiza-
tion techniques. It will acquire more over time: we in-
tend for it to generalize and encapsulate best practices,
and serve as a testbed for new practices. Dyna is now be-
ing used for parsing, machine translation, morphological
analysis, grammar induction, and finite-state modeling.
1 Introduction
Computational linguistics has become a more experi-
mental science. One often uses real-world data to test
one?s formal models (grammatical, statistical, or both).
Unfortunately, as in other experimental sciences, test-
ing each new hypothesis requires much tedious lab
work: writing and tuning code until parameter estimation
(?training?) and inference over unknown variables (?de-
coding?) are bug-free and tolerably fast. This is intensive
work, given complex models or a large search space (as
in modern statistical parsing and machine translation). It
is a major effort to break into the field with a new system,
and modifying existing systems?even in a conceptually
simple way?can require significant reengineering.
Such ?lab work? mainly consists of reusing or rein-
venting various dynamic programming architectures. We
propose that it is time to jump up a level of abstraction.
We offer a new programming language, Dyna, that al-
lows one to quickly and easily specify a model?s com-
binatorial structure. We also offer a compiler, dynac,
that translates from Dyna into C++ classes. The com-
piler does all the tedious work of writing the training and
decoding code. It is intended to do as good a job as a
clever graduate student who already knows the tricks of
the trade (and is willing to maintain hand-tuned C++).
? We would like to thank Joshua Goodman, David McAllester, and
Paul Ruhlen for useful early discussions, and pioneer users Markus
Dreyer, David Smith, and Roy Tromble for their feedback and input.
This work was supported by NSF ITR grant IIS-0313193 to the first
author, by a Fannie & John Hertz Foundation fellowship to the third
author, and by ONR MURI grant N00014-01-1-0685. The views ex-
pressed are not necessarily endorsed by the sponsors.
2 A Basic Example: PCFG Parsing
We believe Dyna is a flexible and intuitive specification
language for dynamic programs. Such a program spec-
ifies how to combine partial solutions until a complete
solution is reached.
2.1 The Inside Algorithm, in Dyna
Fig. 1 shows a simple Dyna program that corresponds
to the inside algorithm for PCFGs (i.e., the probabilis-
tic generalization of CKY parsing). It may be regarded
as a system of equations over an arbitrary number of
unknowns, which have structured names such as con-
stit(s,0,3). These unknowns are called items. They re-
semble variables in a C program, but we use variable
instead to refer to the capitalized identifiers X, I, K, . . . in
lines 2?4.1
At runtime, a user must provide an input sentence and
grammar by asserting values for certain items. If the
input is John loves Mary, the user should assert values
of 1 for word(John,0,1), word(loves,1,2), word(Mary,2,3),
and end(3). If the PCFG contains a rewrite rule np ?
Mary with probability p(Mary | np) = 0.003, the user
should assert that rewrite(np,Mary) has value 0.003.
Given these base cases, the equations in Fig. 1 en-
able Dyna to deduce values for other items. The de-
duced value of constit(s,0,3) will be the inside probability
?s(0, 3),2 and the deduced value of goal will be the total
probability of all parses of the input.
Lines 2?4 are equational schemas that specify how to
compute the value of items such as constit(s,0,3) from
the values of other items. By using the summation op-
erator +=, lines 2?3 jointly say that for any X, I, and
K, constit(X,I,K) is defined by summation over the re-
maining variables, as
?
W rewrite(X,W)*word(W,I,K) +?
Y,Z,J rewrite(X,Y,Z)*constit(Y,I,J)*constit(Z,J,K). For
example, constit(s,0,3) is a sum of quantities such as
rewrite(s,np,vp)*constit(np,0,1)*constit(vp,1,3).
2.2 The Execution Model
Dyna?s declarative semantics state only that it will find
values such that all the equations hold.3 Our implemen-
tation?s default strategy is to propagate updates from an
equation?s right-hand to its left-hand side, until the sys-
tem converges. Thus, by default, Fig. 1 yields a bottom-
up or data-driven parser.
1Much of our terminology (item, chart, agenda) is inherited from
the parsing literature. Other terminology (variable, term, inference rule,
antecedent/consequent, assert/retract, chaining) comes from logic pro-
gramming. Dyna?s syntax borrows from both Prolog and C.
2That is, the probability that s would stochastically rewrite to the
first three words of the input. If this can happen in more than one way,
the probability sums over multiple derivations.
3Thus, future versions of the compiler are free to mix any efficient
strategies, even calling numerical equation solvers.
1. :- valtype(term, real). % declares that all item values are real numbers
2. constit(X,I,K) += rewrite(X,W) * word(W,I,K). % a constituent is either a word . . .
3. constit(X,I,K) += rewrite(X,Y,Z) * constit(Y,I,J) * constit(Z,J,K). % . . . or a combination of two adjacent subconstituents
4. goal += constit(s,0,N) * end(N). % a parse is any s constituent that covers the input string
Figure 1: A probabilistic CKY parser written in Dyna.
Dyna may be seen as a new kind of tabled logic
programming language in which theorems are not just
proved, but carry values. This suggests some terminol-
ogy. Lines 2?4 of Fig. 1 are called inference rules. The
items on the right-hand side are antecedents, and the
item on the left-hand side is their consequent. Asser-
tions can be regarded as axioms. And the default strategy
(unlike Prolog?s) is forward chaining from the axioms,
as in some theorem provers.
Suppose constit(verb,1,2) increases by ?. Then
the program in Fig. 1 must find all the instantiated
rules that have constit(verb,1,2) as an antecedent,
and must update their consequents. For example,
since line 3 can be instantiated as constit(vp,1,3) +=
rewrite(vp,verb,np)*constit(verb,1,2)*constit(np,2,3),
then constit(vp,1,3) must be increased by
rewrite(vp,verb,np) * ? * constit(np,2,3).
Line 3 actually requires infinitely many such up-
dates, corresponding to all rule instantiations of
the form constit(X,1,K) += rewrite(X,verb,Z)*con-
stit(verb,1,2)*constit(Z,2,K).4 However, most of these
updates would have no effect. We only need to consider
the finitely many instantiations where rewrite(X,verb,Z)
and constit(Z,2,K) have nonzero values (because they
have been asserted or updated in the past).
The compiled Dyna program rapidly computes this set
of needed updates and adds them to a worklist of pend-
ing updates, the agenda. Updates from the agenda are
processed in some prioritized order (which can strongly
affect the speed of the program). When an update is car-
ried out (e.g., constit(vp,1,3) is increased), any further
updates that it triggers (e.g., to constit(s,0,3)) are placed
back on the agenda in the same way. Multiple updates
to the same item are consolidated on the agenda. This
cascading update process begins with axiom assertions,
which are treated like other updates.
2.3 Closely Related Algorithms
We now give some examples of variant algorithms.
Fig. 1 provides lattice parsing for free. Instead of
being integer positions in an string, I, J and K can be
symbols denoting states in a finite-state automaton. The
code does not have to change, only the input. Axioms
should now correspond to weighted lattice arcs, e.g.,
word(loves,q,r) with value p(portion of speech signal |
loves).
To find the probability of the best parse instead of the
total probability of all parses, simply change the value
type: replace real with viterbi in line 1. If a and b are
viterbi values, a+b is implemented as max(a, b).5
4As well as instantiations constit(X,I,2) += rewrite(X,Y,
verb)*constit(Y,I,1)*constit(verb,1,2).
5Also, a*b is implemented as a+ b, as viterbi values actually rep-
resent log probabilities (for speed and dynamic range).
Similarly, replacing real with boolean obtains an un-
weighted parser, in which a constituent is either derived
(true value) or not (false value) Then a*b is implemented
as a ? b, and a+b as a ? b.
The Dyna programmer can declare the agenda disci-
pline?i.e., the order in which updates are processed?to
obtain variant algorithms. Although Dyna supports stack
and queue (LIFO and FIFO) disciplines, its default is to
use a priority queue prioritized by the size of the update.
When parsing with real values, this quickly accumulates
a good approximation of the inside probabilities, which
permits heuristic early stopping before the agenda is
empty. With viterbi values, it amounts to uniform-cost
search for the best parse, and an item?s value is guaran-
teed not to change once it is nonzero. Dyna will soon al-
low user-defined priority functions (themselves dynamic
programs), which can greatly speed up parsing (Cara-
ballo and Charniak, 1998; Klein and Manning, 2003).
2.4 Parameter Training
Dyna provides facilities for training parameters. For ex-
ample, from Fig. 1, it automatically derives the inside-
outside (EM) algorithm for training PCFGs.
How is this possible? Once the program of Fig. 1 has
run, goal?s value is the probability of the input sentence
under the grammar. This is a continuous function of
the axiom values, which correspond to PCFG parame-
ters (e.g., the weight of rewrite(np,Mary)). The function
could be written out explicitly as a sum of products of
sums of products of . . . of axiom values, with the details
depending on the sentence and grammar.
Thus, Dyna can be regarded as computing a function
F (~?), where ~? is a vector of axiom values and F (~?) is an
objective function such as the probability of one?s train-
ing data. In learning, one wishes to repeatedly adjust ~?
so as to increase F (~?).
Dyna can be told to evaluate the gradient of the func-
tion with respect to the current parameters ~?: e.g., if
rewrite(vp,verb,np) were increased by , what would hap-
pen to goal? Then any gradient-based optimization
method can be applied, using Dyna to evaluate both F (~?)
and its gradient vector. Also, EM can be applied where
appropriate, since it can be shown that EM?s E counts can
be derived from the gradient. Dyna?s strategy for com-
puting the gradient is automatic differentiation in the re-
verse mode (Griewank and Corliss, 1991), known in the
neural network community as back-propagation.
Dyna comes with a constrained optimization module,
DynaMITE,6 that can locally optimize F (~?). At present,
DynaMITE provides the conjugate gradient and variable
metric methods, using the Toolkit for Advanced Opti-
mization (Benson et al, 2000) together with a softmax
6DynaMITE = Dyna Module for Iterative Training and Estimation.
technique to enforce sum-to-one constraints. It supports
maximum-entropy training and the EM algorithm.7
DynaMITE provides an object-oriented API that al-
lows independent variation of such diverse elements of
training as the model parameterization, optimization al-
gorithm, smoothing techniques, priors, and datasets.
How about supervised or partly supervised training?
The role of supervision is to permit some constituents
to be built but not others (Pereira and Schabes, 1992).
Lines 2?3 of Fig. 1 can simply be extended with an addi-
tional antecedent permitted(X,I,K), which must be either
asserted or derived for constit(X,I,K) to be derived. In
?soft? supervision, the permitted axioms may have val-
ues between 0 and 1.8
3 C++ Interface and Implementation
A Dyna program compiles to a set of portable C++
classes that manage the items and perform inference.
These classes can be used in a larger C++ application.9
This strategy keeps Dyna both small and convenient.
A C++ chart object supports the computation of item
values and gradients. It keeps track of built items, their
values, and their derivations, which form a proof for-
est. It also holds an ordered agenda of pending updates.
Some built items may be ?transient,? meaning that they
are not actually stored in the chart at the moment but will
be transparently recomputed upon demand.
The Dyna compiler generates a hard-coded decision
tree that analyzes the structure of each item popped from
the agenda to decide which inference rules apply to it.
To enable fast lookup of the other items that participate
in these inference rules, it generates code to maintain ap-
propriate indices on the chart.
Objects such as constit(vp,1,3) are called terms and
may be recursively nested to any depth. (Items are just
terms with values.) Dyna has a full first-order type sys-
tem for terms, including primitive and disjunctive types,
and permitting compile-time type inference. These types
are compiled into C++ classes that support construc-
tors and accessors, garbage-collection, subterm sharing
(which may lead to asymptotic speedups, as in CCG pars-
ing (Vijay-Shanker and Weir, 1990)), and interning.10
Dyna can import new primitive term types and value
types from C++, as well as C++ functions to combine
values and to user-define the weights of certain terms.
In the current implementation, every rule must have
the restricted form c += a1*a2* ? ? ? *ak (where each ai
is an item or side condition and (X, +, *) is a semiring
of values). The design for Dyna?s next version lifts this
restriction to allow arbitrary, type-heterogeneous expres-
sions on the right-hand side of an inference rule.11
7It will eventually offer additional methods, such as deterministic
annealing, simulated annealing, and iterative scaling.
8Such item values are not probabilities. We are generally interested
in log-linear models for parsing (Riezler et al, 2000) and other tasks.
9We are also now developing a default application: a visual debug-
ger that allows a user to assert axioms and explore the proof forest
created during inference.
10Interned values are hashed so that equal values are represented by
equal pointers. It is very fast to compare and hash such representations.
11That will make Dyna useful for a wider variety of non-NLP algo-
4 Some Further Applications
Dyna is useful for any problem where partial hypothe-
ses are assembled, or where consistency has to be main-
tained. It is already being used for parsing, syntax-based
machine translation, morphological analysis, grammar
induction, and finite-state operations.
It is well known that various parsing algorithms for
CFG and other formalisms can be simply written in terms
of inference rules. Fig. 2 renders one such example
in Dyna, namely Earley?s algorithm. Two features are
worth noting: the use of recursively nested subterms such
as lists, and the SIDE function, which evaluates to 1 or 0
according to whether its argument has a defined value
yet. These side conditions are used here to prevent hy-
pothesizing a constituent until there is a possible left con-
text that calls for it.
Several recent syntax-directed statistical machine
translation models are easy to build in Dyna. The sim-
plest (Wu, 1997) uses constit(np,3,5,np,4,8) to denote a
NP spanning positions 3?5 in the English string that is
aligned with an NP spanning positions 4?8 in the Chi-
nese string. When training or decoding, the hypotheses
of better-trained monolingual parsers can provide either
hard or soft partial supervision (section 2.4).
Dyna can manipulate finite-state transducers. For in-
stance, the weighted arcs of the composed FST M1 ?M2
can be deduced from the arcs of M1 and M2. Training
M1 ?M2 back-propagates to train the original weights in
M1 and M2, as in (Eisner, 2002).
5 Speed and Code Size
One of our future priorities is speed. Comparing infor-
mally to the best hand-written C++ code we found online
for inside-outside and Dijkstra?s algorithms, Dyna (like
Java) currently runs up to 5 times slower. We mainly un-
derstand the reasons (memory layout and overreliance on
hashing) and are working actively to close the gap.12
Programmer time is also worth considering. Our
inside-outside and Dijkstra?s algorithms are each about
5 lines of Dyna code (plus a short C driver program),
but were compared in the previous paragraph against ef-
ficient C++ implementations of 5500 and 900 lines.13
Our colleague Markus Dreyer, as his first Dyna pro-
gram, decided to replicate the Collins parser (3400 lines
of C). His implementation used under 40 lines of Dyna
code, plus a 300-line C++ driver program that mostly
dealt with I/O. One of us (Smith) has written substan-
tially more complex Dyna programs (e.g., 56 types +
46 inference rules), enabling research that he would not
have been willing to undertake in another language.
6 Related Work
This project tries to synthesize much folk wisdom. For
NLP algorithms, three excellent longer papers have at-
rithms (e.g., neural networks, constraint programming, clustering, and
dynamic graph algorithms). However, it introduces several interesting
design complications in the Dyna language and the implementation.
12Dyna spends most of its time manipulating hash tables and the
priority queue. Inference is very fast because it is compiled.
13The code size comparisons are rough ones, because of mismatches
between the programs being compared.
1. need(s,0) = 1. % begin by looking for an s that starts at position 0
2. constit(Nonterm/Needed,I,I) += SIDE(need(Nonterm,I)) * rewrite(Nonterm,Needed). % traditional predict step
3. constit(Nonterm/Needed,I,K) += constit(Nonterm/cons(W,Needed),I,J) * word(W,J,K). % traditional scan step
4. constit(Nonterm/Needed,I,K) += constit(Nonterm,cons(X,Needed),I,J) * constit(X/nil,J,K). % traditional complete step
5. goal += constit(s/nil,0,N) * end(N). % we want a complete s constituent covering the sentence
6. need(Nonterm,J) += constit( /cons(Nonterm, ), ,J). % Note: underscore matches anything (anonymous wildcard)
Figure 2: An Earley parser in Dyna. np/Needed is syntactic sugar for slash(np,Needed), which is the label of a partial
np constituent that is still missing the list of subconstituents in Needed. In particular, np/nil is a complete np. (A list
[n,pp] is encoded here as cons(n,cons(pp,nil)), although syntactic sugar for lists is also available.) need(np,3) is derived
if some partial constituent seeks an np subconstituent starting at position 3. As usual, probabilistic, agenda-based
lattice parsing comes for free, as does training.
tempted similar syntheses (though without covering vari-
ant search and storage strategies, which Dyna handles).
Shieber et al (1995) (already noting that ?many of the
ideas we present are not new?) showed that several un-
weighted parsing algorithms can be specified in terms of
inference rules, and used Prolog to implement an agenda-
based interpreter for such rules. McAllester (1999) made
a similar case for static analysis algorithms, with a more
rigorous discussion of indexing the chart.
Goodman (1999) generalized this line of work
to weighted parsing, using rules of the form
c += a1*a2* ? ? ? *ak (with side conditions allowed);
he permitted values to fall in any semiring, and gen-
eralized the inside-outside algorithm. Our approach
extends this to a wider variety of processing orders, and
in particular shows how to use a prioritized agenda in
the general case, using novel algorithms. We also extend
to a wider class of formulas (e.g., neural networks).
The closest implemented work we have found is
PRISM (Zhou and Sato, 2003), a kind of probabilis-
tic Prolog that claims to be efficient (thanks to tabling,
compilation, and years of development) and can handle
a subset of the cases described by Goodman. It is in-
teresting because it inherits expressive power from Pro-
log. On the other hand, its rigid probabilistic framework
does not permit side conditions (Fig. 2), general semir-
ings (Goodman), or general formulas (Dyna). PRISM
does not currently seem practical for statistical NLP re-
search: in CKY parsing tests, it was only able to handle
a small fraction of the Penn Treebank ruleset (2400 high-
probability rules) and tended to crash on sentences over
50 words. Dyna, by contrast, is designed for real-world
use: it consistently parses over 10x faster than PRISM,
scales to full-sized problems, and attempts to cover real-
world necessities such as prioritization, bottom-up infer-
ence, pruning, smoothing, underflow avoidance, maxent,
non-EM optimization techniques, etc.
7 Conclusions
Dyna is a declarative programming language for building
efficient systems quickly. As a language, it is inspired
by previous work in deductive parsing, adding weights
in a particularly general way. Dyna?s compiler has been
designed with an eye toward low-level issues (indexing,
structure-sharing, garbage collection, etc.) so that the
cost of this abstraction is minimized.
The goal of Dyna is to facilitate experimentation: a
new model or algorithm automatically gets a new mem-
ory layout, indexing, and training code. We hope this
will lower the barrier to entry in the field, in both research
and education. In Dyna we seek to exploit as many al-
gorithmic tricks as we can, generalizing them to as many
problems as possible on behalf of future Dyna programs.
In turn the body of old programs can provide a unified
testbed for new training and decoding techniques.
Our broader vision is to unify a problem?s possible al-
gorithms by automatically deriving all of them and their
possible training procedures from a single high-level
Dyna program, using source-to-source program transfor-
mations and compiler directives. We plan to choose auto-
matically among these variants by machine learning over
runs on typical data. This involves, for example, auto-
matically learning a figure of merit to guide decoding.
The Dyna compiler, documentation, and examples can
be found at www.dyna.org. The compiler is available
under an open-source license. The commented C++ code
that it generates is free to modify.
References
S. Benson, L. C. McInnes, and J. J. More?. 2000. TAO users
manual. Tech Rpt ANL/MCS-TM-242, Argonne Nat. Lab.
S. A. Caraballo, E. Charniak. 1998. New figures of merit for
best-first probabilistic chart parsing. Comp. Ling., 24(2).
Jason Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proc. of ACL.
Joshua Goodman. 1999. Semiring parsing. Comp. Ling, 25(4).
Andreas Griewank and George Corliss, editors. 1991. Auto-
matic Differentiation of Algorithms. SIAM.
Dan Klein and Christopher D. Manning. 2003. A? parsing:
Fast exact Viterbi parse selection. Proc. of HLT-NAACL.
David McAllester. 1999. On the complexity analysis of static
analyses. 6th Intl. Static Analysis Symposium.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. Proc. of ACL.
S. Riezler, D. Prescher, J. Kuhn, M. Johnson. 2000. Lexical-
ized stochastic modeling of constraint-based grammars us-
ing log-linear measures and EM training. Proc. of ACL.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira. 1995.
Principles and implementation of deductive parsing. Jour-
nal of Logic Programming.
K. Vijay-Shanker and D. Weir. 1990. Polynomial-time parsing
of combinatory categorial grammars. Proc. of ACL.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
N.-F. Zhou and T. Sato. 2003. Toward a high-performance sys-
tem for symbolic and statistical modeling. IJCAI-03 Work-
shop on Learning Statistical Models from Relational Data.
Proceedings of the 43rd Annual Meeting of the ACL, pages 354?362,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Contrastive Estimation: Training Log-Linear Models on Unlabeled Data?
Noah A. Smith and Jason Eisner
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,jason}@cs.jhu.edu
Abstract
Conditional random fields (Lafferty et al, 2001) are
quite effective at sequence labeling tasks like shal-
low parsing (Sha and Pereira, 2003) and named-
entity extraction (McCallum and Li, 2003). CRFs
are log-linear, allowing the incorporation of arbi-
trary features into the model. To train on unlabeled
data, we require unsupervised estimation methods
for log-linear models; few exist. We describe a novel
approach, contrastive estimation. We show that the
new technique can be intuitively understood as ex-
ploiting implicit negative evidence and is computa-
tionally efficient. Applied to a sequence labeling
problem?POS tagging given a tagging dictionary
and unlabeled text?contrastive estimation outper-
forms EM (with the same feature set), is more robust
to degradations of the dictionary, and can largely re-
cover by modeling additional features.
1 Introduction
Finding linguistic structure in raw text is not easy.
The classical forward-backward and inside-outside
algorithms try to guide probabilistic models to dis-
cover structure in text, but they tend to get stuck in
local maxima (Charniak, 1993). Even when they
avoid local maxima (e.g., through clever initializa-
tion) they typically deviate from human ideas of
what the ?right? structure is (Merialdo, 1994).
One strategy is to incorporate domain knowledge
into the model?s structure. Instead of blind HMMs
or PCFGs, one could use models whose features
?This work was supported by a Fannie and John Hertz
Foundation fellowship to the first author and NSF ITR grant IIS-
0313193 to the second author. The views expressed are not nec-
essarily endorsed by the sponsors. The authors also thank three
anonymous ACL reviewers for helpful comments, colleagues
at JHU CLSP (especially David Smith and Roy Tromble) and
Miles Osborne for insightful feedback, and Eric Goldlust and
Markus Dreyer for Dyna language support.
are crafted to pay attention to a range of domain-
specific linguistic cues. Log-linear models can be so
crafted and have already achieved excellent perfor-
mance when trained on annotated data, where they
are known as ?maximum entropy? models (Ratna-
parkhi et al, 1994; Rosenfeld, 1994).
Our goal is to learn log-linear models from
unannotated data. Since the forward-backward
and inside-outside algorithms are instances of
Expectation-Maximization (EM) (Dempster et al,
1977), a natural approach is to construct EM algo-
rithms that handle log-linear models. Riezler (1999)
did so, then resorted to an approximation because
the true objective function was hard to normalize.
Stepping back from EM, we may generally en-
vision parameter estimation for probabilistic mod-
eling as pushing probability mass toward the train-
ing examples. We must consider not only where
the learner pushes the mass, but also from where the
mass is taken. In this paper, we describe an alterna-
tive to EM: contrastive estimation (CE), which (un-
like EM) explicitly states the source of the probabil-
ity mass that is to be given to an example.1
One reason is to make normalization efficient. In-
deed, CE generalizes EM and other practical tech-
niques used to train log-linear models, including
conditional estimation (for the supervised case) and
Riezler?s approximation (for the unsupervised case).
The other reason to use CE is to improve accu-
racy. CE offers an additional way to inject domain
knowledge into unsupervised learning (Smith and
Eisner, 2005). CE hypothesizes that each positive
example in training implies a domain-specific set
of examples which are (for the most part) degraded
(?2). This class of implicit negative evidence pro-
vides the source of probability mass for the observed
example. We discuss the application of CE to log-
linear models in ?3.
1Not to be confused with contrastive divergence minimiza-
tion (Hinton, 2003), a technique for training products of experts.
354
We are particularly interested in log-linear models
over sequences, like the conditional random fields
(CRFs) of Lafferty et al (2001) and weighted CFGs
(Miyao and Tsujii, 2002). For a given sequence, im-
plicit negative evidence can be represented as a lat-
tice derived by finite-state operations (?4). Effec-
tiveness of the approach on POS tagging using un-
labeled data is demonstrated (?5). We discuss future
work (?6) and conclude (?7).
2 Implicit Negative Evidence
Natural language is a delicate thing. For any plausi-
ble sentence, there are many slight perturbations of
it that will make it implausible. Consider, for ex-
ample, the first sentence of this section. Suppose
we choose one of its six words at random and re-
move it; on this example, odds are two to one that
the resulting sentence will be ungrammatical. Or,
we could randomly choose two adjacent words and
transpose them; none of the results are valid conver-
sational English. The learner we describe here takes
into account not only the observed positive exam-
ple, but also a set of similar but deprecated negative
examples.
2.1 Learning setting
Let ~x = ?x1, x2, ...?, be our observed example sen-
tences, where each xi ? X, and let y?i ? Y be the
unobserved correct hidden structure for xi (e.g., a
POS sequence). We seek a model, parameterized by
~?, such that the (unknown) correct analysis y?i is the
best analysis for xi (under the model). If y?i were ob-
served, a variety of training criteria would be avail-
able (see Tab. 1), but y?i is unknown, so none apply.
Typically one turns to the EM algorithm (Dempster
et al, 1977), which locally maximizes
?
i
p
(
X = xi | ~?
)
=
?
i
?
y?Y
p
(
X = xi, Y = y | ~?
)
(1)
where X is a random variable over sentences and
Y a random variable over analyses (notation is of-
ten abbreviated, eliminating the random variables).
An often-used alternative to EM is a class of so-
called Viterbi approximations, which iteratively find
the probabilistically-best y? and then, on each itera-
tion, solve a supervised problem (see Tab. 1).
joint likelihood (JL) ?
i
p
(
xi, y
?
i | ~?
)
conditional
likelihood (CL)
?
i
p
(
y?i | xi, ~?
)
classification
accuracy (Juang
and Katagiri, 1992)
?
i
?(y?i , y?(xi))
expected
classification
accuracy (Klein and
Manning, 2002)
?
i
p
(
y?i | xi, ~?
)
negated boosting
loss (Collins, 2000) ?
?
i
p
(
y?i | xi, ~?
)?1
margin (Crammer
and Singer, 2001) ? s.t. ?
~?? ? 1;?i,?y 6= y?i ,
~? ? (~f(xi, y?i ) ? ~f(xi, y)) ? ?
expected local
accuracy (Altun et
al., 2003)
?
i
?
j
p
(
`j(Y ) = `j(y
?
i ) | xi, ~?
)
Table 1: Various supervised training criteria. All functions are
written so as to be maximized. None of these criteria are avail-
able for unsupervised estimation because they all depend on the
correct label, y?.
2.2 A new approach: contrastive estimation
Our approach instead maximizes
?
i
p
(
Xi = xi | Xi ? N(xi), ~?
)
(2)
where the ?neighborhood? N(xi) ? X is a set of
implicit negative examples plus the example xi it-
self. As in EM, p(xi | ..., ~?) is found by marginal-
izing over hidden variables (Eq. 1). Note that the
x? ? N(xi) are not treated as hard negative exam-
ples; we merely seek to move probability mass from
them to the observed x.
The neighborhood of x, N(x), contains examples
that are perturbations of x. We refer to the mapping
N : X ? 2X as the neighborhood function, and the
optimization of Eq. 2 as contrastive estimation (CE).
CE seeks to move probability mass from the
neighborhood of an observed xi to xi itself. The
learner hypothesizes that good models are those
which discriminate an observed example from its
neighborhood. Put another way, the learner assumes
not only that xi is good, but that xi is locally op-
timal in example space (X), and that alternative,
similar examples (from the neighborhood) are infe-
rior. Rather than explain all of the data, the model
must only explain (using hidden variables) why the
355
observed sentence is better than its neighbors. Of
course, the validity of this hypothesis will depend
on the form of the neighborhood function.
Consider, as a concrete example, learning nat-
ural language syntax. In Smith and Eisner (2005),
we define a sentence?s neighborhood to be a set of
slightly-altered sentences that use the same lexemes,
as suggested at the start of this section. While their
syntax is degraded, the inferred meaning of any of
these altered sentences is typically close to the in-
tended meaning, yet the speaker chose x and not
one of the other x? ? N(x). Why? Deletions
are likely to violate subcategorization requirements,
and transpositions are likely to violate word order
requirements?both of which have something to do
with syntax. x was the most grammatical option that
conveyed the speaker?s meaning, hence (we hope)
roughly the most grammatical option in the neigh-
borhood N(x), and the syntactic model should make
it so.
3 Log-Linear Models
We have not yet specified the form of our probabilis-
tic model, only that it is parameterized by ~? ? Rn.
Log-linear models, which we will show are a natural
fit for CE, assign probability to an (example, label)
pair (x, y) according to
p
(
x, y | ~?
)
def
=
1
Z
(
~?
)u
(
x, y | ~?
)
(3)
where the ?unnormalized score? u(x, y | ~?) is
u
(
x, y | ~?
)
def
= exp
(
~? ? ~f(x, y)
)
(4)
The notation above is defined as follows. ~f : X ?
Y ? Rn?0 is a nonnegative vector feature function,
and ~? ? Rn are the corresponding feature weights
(the model?s parameters). Because the features can
take any form and need not be orthogonal, log-linear
models can capture arbitrary dependencies in the
data and cleanly incorporate them into a model.
Z(~?) (the partition function) is chosen so that
?
(x,y) p(x, y |
~?) = 1; i.e., Z(~?) =
?
(x,y) u(x, y |
~?). u is typically easy to compute for a given (x, y),
but Z may be much harder to compute. All the ob-
jective functions in this paper take the form
?
i
?
(x,y)?Ai
p
(
x, y | ~?
)
?
(x,y)?Bi
p
(
x, y | ~?
) (5)
likelihood criterion Ai Bi
joint {(xi, y?i )} X? Y
conditional {(xi, y?i )} {xi} ? Y
marginal (a la` EM) {xi} ? Y X? Y
contrastive {xi} ? Y N(xi) ? Y
Table 2: Supervised (upper box) and unsupervised (lower box)
estimation with log-linear models in terms of Eq. 5.
where Ai ? Bi (for each i). For log-linear models
this is simply
?
i
?
(x,y)?Ai
u
(
x, y | ~?
)
?
(x,y)?Bi
u
(
x, y | ~?
) (6)
So there is no need to compute Z(~?), but we do need
to compute sums over A and B. Tab. 2 summarizes
some concrete examples; see also ?3.1?3.2.
We would prefer to choose an objective function
such that these sums are easy. CE focuses on choos-
ing appropriate small contrast sets Bi, both for effi-
ciency and to guide the learner. The natural choice
for Ai (which is usually easier to sum over) is the set
of (x, y) that are consistent with what was observed
(partially or completely) about the ith training ex-
ample, i.e., the numerator
?
(x,y)?Ai p(x, y |
~?) is
designed to find p(observation i | ~?). The idea is to
focus the probability mass within Bi on the subset
Ai where the i the training example is known to be.
It is possible to build log-linear models where
each xi is a sequence.2 In this paper, each model
is a weighted finite-state automaton (WFSA) where
states correspond to POS tags. The parameter vector
~? ? Rn specifies a weight for each of the n transi-
tions in the automaton. y is a hidden path through
the automaton (determining a POS sequence), and x
is the string it emits. u(x, y | ~?) is defined by ap-
plying exp to the total weight of all transitions in y.
This is an example of Eqs. 4 and 6 where fj(x, y) is
the number of times the path y takes the jth transi-
tion.
The partition function Z(~?) of the WFSA is found
by adding up the u-scores of all paths through the
WFSA. For a k-state WFSA, this equates to solving
a linear system of k equations in k variables (Tarjan,
1981). But if the WFSA contains cycles this infi-
nite sum may diverge. Alternatives to exact com-
2These are exemplified by CRFs (Lafferty et al, 2001),
which can be viewed alternately as undirected dynamic graph-
ical models with a chain topology, as log-linear models over
entire sequences with local features, or as WFSAs. Because
?CRF? implies CL estimation, we use the term ?WFSA.?
356
putation, like random sampling (see, e.g., Abney,
1997), will not help to avoid this difficulty; in addi-
tion, convergence rates are in general unknown and
bounds difficult to prove. We would prefer to sum
over finitely many paths in Bi.
3.1 Parameter estimation (supervised)
For log-linear models, both CL and JL estimation
(Tab. 1) are available. In terms of Eq. 5, both
set Ai = {(xi, y?i )}. The difference is in B: for
JL, Bi = X ? Y, so summing over Bi is equiva-
lent to computing the partition function Z(~?). Be-
cause that sum is typically difficult, CL is preferred;
Bi = {xi} ? Y for xi, which is often tractable.
For sequence models like WFSAs it is computed us-
ing a dynamic programming algorithm (the forward
algorithm for WFSAs). Klein and Manning (2002)
argue for CL on grounds of accuracy, but see also
Johnson (2001). See Tab. 2; other contrast sets Bi
are also possible.
When Bi contains only xi paired with the current
best competitor (y?) to y?i , we have a technique that
resembles maximum margin training (Crammer and
Singer, 2001). Note that y? will then change across
training iterations, making Bi dynamic.
3.2 Parameter estimation (unsupervised)
The difference between supervised and unsuper-
vised learning is that in the latter case, Ai is forced
to sum over label sequences y because they weren?t
observed. In the unsupervised case, CE maximizes
LN
(
~?
)
= log
?
i
?
y?Y
u
(
xi, y | ~?
)
?
(x,y)?N(xi)?Y
u
(
x, y | ~?
) (7)
In terms of Eq. 5, A = {xi}?Y and B = N(xi)?Y.
EM?s objective function (Eq. 1) is a special case
where N(xi) = X, for all i, and the denomina-
tor becomes Z(~?). An alternative is to restrict the
neighborhood to the set of observed training exam-
ples rather than all possible examples (Riezler, 1999;
Johnson et al, 1999; Riezler et al, 2000):
?
i
[
u
(
xi | ~?
)
/
?
j
u
(
xj | ~?
)
]
(8)
Viewed as a CE method, this approach (though ef-
fective when there are few hypotheses) seems mis-
guided; the objective says to move mass to each ex-
ample at the expense of all other training examples.
Another variant is conditional EM. Let xi be a
pair (xi,1, xi,2) and define the neighborhood to be
N(xi) = {x? = (x?1, xi,2)}. This approach has
been applied to conditional densities (Jebara and
Pentland, 1998) and conditional training of acoustic
models with hidden variables (Valtchev et al, 1997).
Generally speaking, CE is equivalent to some
kind of EM when N(?) is an equivalence relation
on examples, so that the neighborhoods partition X.
Then if q is any fixed (untrained) distribution over
neighborhoods, CE equates to running EM on the
model defined by
p?
(
x, y | ~?
)
def
= q (N(x)) ? p
(
x, y | N(x), ~?
)
(9)
CE may also be viewed as an importance sam-
pling approximation to EM, where the sample space
X is replaced by N(xi). We will demonstrate ex-
perimentally that CE is not just an approximation to
EM; it makes sense from a modeling perspective.
In ?4, we will describe neighborhoods of se-
quences that can be represented as acyclic lattices
built directly from an observed sequence. The sum
over Bi is then the total u-score in our model of all
paths in the neighborhood lattice. To compute this,
intersect the WFSA and the lattice, obtaining a new
acyclic WFSA, and sum the u-scores of all its paths
(Eisner, 2002) using a simple dynamic programming
algorithm akin to the forward algorithm. The sum
over Ai may be computed similarly.
CE with lattice neighborhoods is not confined to
the WFSAs of this paper; when estimating weighted
CFGs, the key algorithm is the inside algorithm for
lattice parsing (Smith and Eisner, 2005).
3.3 Numerical optimization
To maximize the neighborhood likelihood (Eq. 7),
we apply a standard numerical optimization method
(L-BFGS) that iteratively climbs the function using
knowledge of its value and gradient (Liu and No-
cedal, 1989). The partial derivative of LN with re-
spect to the jth feature weight ?j is
?LN
??j
=
?
i
E~? [fj | xi] ?E~? [fj | N(xi)] (10)
This looks similar to the gradient of log-linear like-
lihood functions on complete data, though the ex-
pectation on the left is in those cases replaced by an
observed feature value fj(xi, y?i ). In this paper, the
357
natural language is a delicate thing
a. DEL1WORD:
natural language is a delicate thing
language is a delicate thing
is a
delicate
thing ?:?
? ?
b. TRANS1:
natural language a delicate thingis
delicate
is
is
a
natural
a
is a delicate thing
language
language
delicate
thing
: xx2 1x2x1 :
:x x2 3 :x x3 2
:x xm m?1 xm?1:xm
? ?
.
.
.
(Each bigram xi+1i in the sentence has an
arc pair (xi : xi+1, xi+1 : xi).)
c. DEL1SUBSEQ:
natural language is a delicate thing
language
is
is
a
a
a delicate thing
?:?
?:?
?:?
?
?
?
?
?
?
Figure 1: A sentence and three lattices representing some of its neighborhoods. The transducer used to generate each neighborhood
lattice (via composition with the sentence, followed by determinization and minimization) is shown to its right.
expectations in Eq. 10 are computed by the forward-
backward algorithm generalized to lattices.
We emphasize that the function LN is not glob-
ally concave; our search will lead only to a local op-
timum.3 Therefore, as with all unsupervised statisti-
cal learning, the bias in the initialization of ~? will af-
fect the quality of the estimate and the performance
of the method. In future we might wish to apply
techniques for avoiding local optima, such as deter-
ministic annealing (Smith and Eisner, 2004).
4 Lattice Neighborhoods
We next consider some non-classical neighborhood
functions for sequences. When X = ?+ for some
symbol alphabet ?, certain kinds of neighborhoods
have natural, compact representations. Given an in-
put string x = ?x1, x2, ..., xm?, we write xji for
the substring ?xi, xi+1, ..., xj? and xm1 for the whole
string. Consider first the neighborhood consisting of
all sequences generated by deleting a single symbol
from the m-length sequence xm1 :
DEL1WORD(xm1 ) =
{
x`?11 x
m
`+1 | 1 ? ` ? m
}
? {xm1 }
This set consists of m + 1 strings and can be com-
pactly represented as a lattice (see Fig. 1a). Another
3Without any hidden variables, LN is globally concave.
neighborhood involves transposing any pair of adja-
cent words:
TRANS1(xm1 ) =
{
x`?11 x`+1x`x
m
`+2 | 1 ? ` < m
}
? {xm1 }
This set can also be compactly represented as a lat-
tice (Fig. 1b). We can combine DEL1WORD and
TRANS1 by taking their union; this gives a larger
neighborhood, DELORTRANS1.4
The DEL1SUBSEQ neighborhood allows the dele-
tion of any contiguous subsequence of words that is
strictly smaller than the whole sequence. This lattice
is similar to that of DEL1WORD, but adds some arcs
(Fig. 1c); the size of this neighborhood is O(m2).
A final neighborhood we will consider is
LENGTH, which consists of ?m. CE with the
LENGTH neighborhood is very similar to EM; it is
equivalent to using EM to estimate the parameters
of a model defined by Eq. 9 where q is any fixed
(untrained) distribution over lengths.
When the vocabulary ? is the set of words in a
natural language, it is never fully known; approx-
imations for defining LENGTH = ?m include us-
ing observed ? from the training set (as we do) or
adding a special OOV symbol.
4In general, the lattices are obtained by composing the ob-
served sequence with a small FST and determinizing and mini-
mizing the result; the relevant transducers are shown in Fig. 1.
358
 30
 40
 50
 60
 70
 80
 90
 100
 0.1  1  10
% c
orre
ct ta
gs
smoothing parameter0 8
 
 
 
 
 
 
 
 
 
12K 24K 48K 96K
sel. oracle sel. oracle sel. oracle sel. oracle
CRF (supervised) 100.0 99.8 99.8 99.5
HMM (supervised) 99.3 98.5 97.9 97.2
LENGTH 74.9 77.4 78.7 81.5 78.3 81.3 78.9 79.3
DELORTR1 70.8 70.8 78.6 78.6 78.3 79.1 75.2 78.8
TRANS1 72.7 72.7 77.2 77.2 78.1 79.4 74.7 79.0
EM 49.5 52.9 55.5 58.0 59.4 60.9 60.9 62.1
DEL1WORD 55.4 55.6 58.6 60.3 59.9 60.2 59.9 60.4
DEL1SSQ 53.0 53.3 55.0 56.7 55.3 55.4 57.3 58.7
random expected 35.2 35.1 35.1 35.1
ambiguous words 6,244 12,923 25,879 51,521
Figure 2: Percent ambiguous words tagged correctly in the 96K dataset, as the smoothing parameter (? in the case of EM, ?2 in the
CE cases) varies. The model selected from each criterion using unlabeled development data is circled in the plot. Dataset size is
varied in the table at right, which shows models selected using unlabeled development data (?sel.?) and using an oracle (?oracle,?
the highest point on a curve). Across conditions, some neighborhood roughly splits the difference between supervised models and
EM.
5 Experiments
We compare CE (using neighborhoods from ?4)
with EM on POS tagging using unlabeled data.
5.1 Comparison with EM
Our experiments are inspired by those in
Merialdo (1994); we train a trigram tagger using
only unlabeled data, assuming complete knowledge
of the tagging dictionary.5 In our experiments,
we varied the amount of data available (12K?96K
words of WSJ), the heaviness of smoothing, and the
estimation criterion. In all cases, training stopped
when the relative change in the criterion fell below
10?4 between steps (typically ? 100 steps). For this
corpus and tag set, on average, a tagger must decide
between 2.3 tags for a given token.
The generative model trained by EM was identical
to Merialdo?s: a second-order HMM. We smoothed
using a flat Dirichlet prior with single parameter ?
for all distributions (?-values from 0 to 10 were
tested).6 The model was initialized uniformly.
The log-linear models trained by CE used the
same feature set, though the feature weights are no
longer log-probabilities and there are no sum-to-one
constraints. In addition to an unsmoothed trial, we
tried diagonal Gaussian priors (quadratic penalty)
with ?2 ranging from 0.1 to 10. The models were
initialized with all ?j = 0.
Unsupervised model selection. For each (crite-
5Without a tagging dictionary, tag names are interchange-
able and cannot be evaluated on gold-standard accuracy. We
address the tagging dictionary assumption in ?5.2.
6This is equivalent to add-? smoothing within every M step.
rion, dataset) pair, we selected the smoothing trial
that gave the highest estimation criterion score on a
5K-word development set (also unlabeled).
Results. The plot in Fig. 2 shows the Viterbi ac-
curacy of each criterion trained on the 96K-word
dataset as smoothing was varied; the table shows,
for each (criterion, dataset) pair the performance of
the selected ? or ?2 and the one chosen by an oracle.
LENGTH, TRANS1, and DELORTRANS1 are con-
sistently the best, far out-stripping EM. These gains
dwarf the performance of EM on over 1.1M words
(66.6% as reported by Smith and Eisner (2004)),
even when the latter uses improved search (70.0%).
DEL1WORD and DEL1SUBSEQ, on the other hand,
are poor, even worse than EM on larger datasets.
An important result is that neighborhoods do not
succeed by virtue of approximating log-linear EM;
if that were so, we would expect larger neighbor-
hoods (like DEL1SUBSEQ) to out-perform smaller
ones (like TRANS1)?this is not so. DEL1SUBSEQ
and DEL1WORD are poor because they do not give
helpful classes of negative evidence: deleting a word
or a short subsequence often does very little dam-
age. Put another way, models that do a good job of
explaining why no word or subsequence should be
deleted do not do so using the familiar POS cate-
gories.
The LENGTH neighborhood is as close to log-
linear EM as it is practical to get. The inconsis-
tencies in the LENGTH curve (Fig. 2) are notable
and also appeared at the other training set sizes.
Believing this might be indicative of brittleness in
Viterbi label selection, we computed the expected
359
DELORTRANS1 TRANS1 LENGTH EM
words in trigram trigram+ spelling trigram
trigram
+ spelling trigram
trigram
+ spelling trigram
tagging dict. sel. oracle sel. oracle sel. oracle sel. oracle sel. oracle sel. oracle sel. oracle ran
do
m
ex
pe
ct
ed
am
bi
gu
ou
s w
or
ds
av
e.
ta
gs
/to
ke
n
all train & dev. 78.3 90.1 80.9 91.1 90.4 90.4 88.7 90.9 87.8 90.4 87.1 91.9 78.0 84.4 69.5 13,150 2.3
1st 500 sents. 72.3 84.8 80.2 90.8 80.8 82.9 88.1 90.1 68.1 78.3 76.9 83.2 77.2 80.5 60.5 13,841 3.7
count ? 2 69.5 81.3 79.5 90.3 77.0 78.6 78.7 90.1 65.3 75.2 73.3 73.8 70.1 70.9 56.6 14,780 4.4
count ? 3 65.0 77.2 78.3 89.8 71.7 73.4 78.4 89.5 62.8 72.3 73.2 73.6 66.5 66.5 51.0 15,996 5.5
Table 3: Percent of all words correctly tagged in the 24K dataset, as the tagging dictionary is diluted. Unsupervised model selection
(?sel.?) and oracle model selection (?oracle?) across smoothing parameters are shown. Note that we evaluated on all words (unlike
Fig. 3) and used 17 coarse tags, giving higher scores than in Fig. 2.
accuracy of the LENGTH models; the same ?dips?
were present. This could indicate that the learner
was trapped in a local maximum, suggesting that,
since other criteria did not exhibit this behavior,
LENGTH might be a bumpier objective surface. It
would be interesting to measure the bumpiness (sen-
sitivity to initial conditions) of different contrastive
objectives.7
5.2 Removing knowledge, adding features
The assumption that the tagging dictionary is com-
pletely known is difficult to justify. While a POS
lexicon might be available for a new language, cer-
tainly it will not give exhaustive information about
all word types in a corpus. We experimented with
removing knowledge from the tagging dictionary,
thereby increasing the difficulty of the task, to see
how well various objective functions could recover.
One means to recovery is the addition of features to
the model?this is easy with log-linear models but
not with classical generative models.
We compared the performance of the best
neighborhoods (LENGTH, DELORTRANS1, and
TRANS1) from the first experiment, plus EM, us-
ing three diluted dictionaries and the original one,
on the 24K dataset. A diluted dictionary adds (tag,
word) entries so that rare words are allowed with
any tag, simulating zero prior knowledge about the
word. ?Rare? might be defined in different ways;
we used three definitions: words unseen in the first
500 sentences (about half of the 24K training cor-
pus); singletons (words with count ? 1); and words
with count ? 2. To allow more trials, we projected
the original 45 tags onto a coarser set of 17 (e.g.,
7A reviewer suggested including a table comparing different
criterion values for each learned model (i.e., each neighborhood
evaluated on each other neighborhood). This table contained no
big surprises; we note only that most models were the best on
their own criterion, and among unsupervised models, LENGTH
performed best on the CL criterion.
RB? ?ADV).
To take better advantage of the power of log-
linear models?specifically, their ability to incorpo-
rate novel features?we also ran trials augmenting
the model with spelling features, allowing exploita-
tion of correlations between parts of the word and a
possible tag. Our spelling features included all ob-
served 1-, 2-, and 3-character suffixes, initial capital-
ization, containing a hyphen, and containing a digit.
Results. Fig. 3 plots tagging accuracy (on am-
biguous words) for each dictionary on the 24K
dataset. The x-axis is the smoothing parameter (?
for EM, ?2 for CE). Note that the different plots are
not comparable, because their y-axes are based on
different sets of ambiguous words.
So that models under different dilution conditions
could be compared, we computed accuracy on all
words; these are shown in Tab. 3. The reader will
notice that there is often a large gap between unsu-
pervised and oracle model selection; this draws at-
tention to a need for better unsupervised regulariza-
tion and model selection techniques.
Without spelling features, all models perform
worse as knowledge is removed. But LENGTH suf-
fers most substantially, relative to its initial perfor-
mance. Why is this? LENGTH (like EM) requires
the model to explain why a given sentence was seen
instead of some other sentence of the same length.
One way to make this explanation is to manipulate
emission weights (i.e., for (tag, word) features): the
learner can construct a good class-based unigram
model of the text (where classes are tags). This is
good for the LENGTH objective, but not for learning
good POS tag sequences.
In contrast, DELORTRANS1 and TRANS1 do not
allow the learner to manipulate emission weights for
words not in the sentence. The sentence?s good-
ness must be explained in a way other than by the
words it contains: namely through the POS tags. To
360
check this intuition, we built local normalized mod-
els p(word | tag) from the parameters learned by
TRANS1 and LENGTH. For each tag, these were
compared by KL divergence to the empirical lexical
distributions (from labeled data). For the ten tags
accounting for 95.6% of the data, LENGTH more
closely matched the empirical lexical distributions.
LENGTH is learning a correct distribution, but that
distribution is not helpful for the task.
The improvement from adding spelling features
is striking: DELORTRANS1 and TRANS1 recover
nearly completely (modulo the model selection
problem) from the diluted dictionaries. LENGTH
sees far less recovery. Hence even our improved fea-
ture sets cannot compensate for the choice of neigh-
borhood. This highlights our argument that a neigh-
borhood is not an approximation to log-linear EM;
LENGTH tries very hard to approximate log-linear
EM but requires a good dictionary to be on par with
the other criteria. Good neighborhoods, rather, per-
form well in their own right.
6 Future Work
Foremost for future work is the ?minimally super-
vised? paradigm in which a small amount of la-
beled data is available (see, e.g., Clark et al (2003)).
Unlike well-known ?bootstrapping? approaches
(Yarowsky, 1995), EM and CE have the possible ad-
vantage of maintaining posteriors over hidden labels
(or structure) throughout learning; bootstrapping ei-
ther chooses, for each example, a single label, or
remains completely agnostic. One can envision a
mixed objective function that tries to fit the labeled
examples while discriminating unlabeled examples
from their neighborhoods.8
Regardless of how much (if any) data are labeled,
the question of good smoothing techniques requires
more attention. Here we used a single zero-mean,
constant-variance Gaussian prior for all parameters.
Better performance might be achieved by allowing
different variances for different feature types. This
8Zhu and Ghahramani (2002) explored the semi-supervised
classification problem for spatially-distributed data, where
some data are labeled, using a Boltzmann machine to model
the dataset. For them, the Markov random field is over label-
ing configurations for all examples, not, as in our case, com-
plex structured labels for a particular example. Hence their B
(Eq. 5), though very large, was finite and could be sampled.
All train & development words are in the tagging dictionary:
 40
 45
 50 55
 60
 65
 70
 75
 80
 85
Tagging dictionary taken from the first 500 sentences:
 40
 45
 50 55
 60
 65
 70
 75
 80
 85
Tagging dictionary contains words with count ? 2:
 40
 45
 50 55
 60
 65
 70
 75
 80
 85
Tagging dictionary contains words with count ? 3:
 40
 45
 50 55
 60
 65
 70
 75
 80
 85
 40 45
 50 55
 60 65
 70 5
80 85
 0.1  1  10smoothing parameter0 8
50
DELORTRANS1  
TRANS1  
LENGTH 4 5
EM trig
ram
m
od
el
? trig
ram
+
spe
llin
g
Figure 3: Percent ambiguous words tagged correctly (with
coarse tags) on the 24K dataset, as the dictionary is diluted and
with spelling features. Each graph corresponds to a different
level of dilution. Models selected using unlabeled development
data are circled. These plots (unlike Tab. 3) are not compara-
ble to each other because each is measured on a different set of
ambiguous words.
361
leads to a need for more efficient tuning of the prior
parameters on development data.
The effectiveness of CE (and different neighbor-
hoods) for dependency grammar induction is ex-
plored in Smith and Eisner (2005) with considerable
success. We introduce there the notion of design-
ing neighborhoods to guide learning for particular
tasks. Instead of guiding an unsupervised learner to
match linguists? annotations, the choice of neighbor-
hood might be made to direct the learner toward hid-
den structure that is helpful for error-correction tasks
like spelling correction and punctuation restoration
that may benefit from a grammatical model.
Wang et al (2002) discuss the latent maximum
entropy principle. They advocate running EM many
times and selecting the local maximum that maxi-
mizes entropy. One might do the same for the local
maxima of any CE objective, though theoretical and
experimental support for this idea remain for future
work.
7 Conclusion
We have presented contrastive estimation, a new
probabilistic estimation criterion that forces a model
to explain why the given training data were better
than bad data implied by the positive examples. We
have shown that for unsupervised sequence model-
ing, this technique is efficient and drastically out-
performs EM; for POS tagging, the gain in accu-
racy over EM is twice what we would get from ten
times as much data and improved search, sticking
with EM?s criterion (Smith and Eisner, 2004). On
this task, with certain neighborhoods, contrastive
estimation suffers less than EM does from dimin-
ished prior knowledge and is able to exploit new
features?that EM can?t?to largely recover from
the loss of knowledge.
References
S. P. Abney. 1997. Stochastic attribute-value grammars. Com-
putational Linguistics, 23(4):597?617.
Y. Altun, M. Johnson, and T. Hofmann. 2003. Investigating
loss functions and optimization methods for discriminative
learning of label sequences. In Proc. of EMNLP.
E. Charniak. 1993. Statistical Language Learning. MIT Press.
S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrapping
POS taggers using unlabelled data. In Proc. of CoNLL.
M. Collins. 2000. Discriminative reranking for natural lan-
guage parsing. In Proc. of ICML.
K. Crammer and Y. Singer. 2001. On the algorithmic imple-
mentation of multiclass kernel-based vector machines. Jour-
nal of Machine Learning Research, 2(5):265?92.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likeli-
hood estimation from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1?38.
J. Eisner. 2002. Parameter estimation for probabilistic finite-
state transducers. In Proc. of ACL.
G. E. Hinton. 2003. Training products of experts by mini-
mizing contrastive divergence. Technical Report GCNU TR
2000-004, University College London.
T. Jebara and A. Pentland. 1998. Maximum conditional like-
lihood via bound maximization and the CEM algorithm. In
Proc. of NIPS.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999.
Estimators for stochastic ?unification-based? grammars. In
Proc. of ACL.
M. Johnson. 2001. Joint and conditional estimation of tagging
and parsing models. In Proc. of ACL.
B.-H. Juang and S. Katagiri. 1992. Discriminative learning for
minimum error classification. IEEE Trans. Signal Process-
ing, 40:3043?54.
D. Klein and C. D. Manning. 2002. Conditional structure vs.
conditional estimation in NLP models. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited memory method
for large scale optimization. Mathematical Programming B,
45(3):503?28.
A. McCallum and W. Li. 2003. Early results for named-
entity extraction with conditional random fields. In Proc.
of CoNLL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?72.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estimation for
feature forests. In Proc. of HLT.
A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A maximum
entropy model for parsing. In Proc. of ICSLP.
S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000. Lex-
icalized stochastic modeling of constraint-based grammars
using log-linear measures and EM training. In Proc. of ACL.
S. Riezler. 1999. Probabilistic Constraint Logic Programming.
Ph.D. thesis, Universita?t Tu?bingen.
R. Rosenfeld. 1994. Adaptive Statistical Language Modeling:
A Maximum Entropy Approach. Ph.D. thesis, CMU.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of HLT-NAACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Guiding unsupervised gram-
mar induction using contrastive estimation. In Proc. of IJ-
CAI Workshop on Grammatical Inference Applications.
R. E. Tarjan. 1981. A unified approach to path problems. Jour-
nal of the ACM, 28(3):577?93.
V. Valtchev, J. J. Odell, P. C. Woodland, and S. J. Young. 1997.
MMIE training of large vocabulary speech recognition sys-
tems. Speech Communication, 22(4):303?14.
S. Wang, R. Rosenfeld, Y. Zhao, and D. Schuurmans. 2002.
The latent maximum entropy principle. In Proc. of ISIT.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. of ACL.
X. Zhu and Z. Ghahramani. 2002. Towards semi-supervised
classification with Markov random fields. Technical Report
CMU-CALD-02-106, Carnegie Mellon University.
362
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 569?576,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Annealing Structural Bias in Multilingual Weighted Grammar Induction?
Noah A. Smith and Jason Eisner
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,jason}@cs.jhu.edu
Abstract
We first show how a structural locality bias can improve the
accuracy of state-of-the-art dependency grammar induction
models trained by EM from unannotated examples (Klein
and Manning, 2004). Next, by annealing the free parame-
ter that controls this bias, we achieve further improvements.
We then describe an alternative kind of structural bias, to-
ward ?broken? hypotheses consisting of partial structures
over segmented sentences, and show a similar pattern of im-
provement. We relate this approach to contrastive estimation
(Smith and Eisner, 2005a), apply the latter to grammar in-
duction in six languages, and show that our new approach
improves accuracy by 1?17% (absolute) over CE (and 8?30%
over EM), achieving to our knowledge the best results on this
task to date. Our method, structural annealing, is a gen-
eral technique with broad applicability to hidden-structure
discovery problems.
1 Introduction
Inducing a weighted context-free grammar from
flat text is a hard problem. A common start-
ing point for weighted grammar induction is
the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977; Baker, 1979). EM?s
mediocre performance (Table 1) reflects two prob-
lems. First, it seeks to maximize likelihood, but a
grammar that makes the training data likely does
not necessarily assign a linguistically defensible
syntactic structure. Second, the likelihood surface
is not globally concave, and learners such as the
EM algorithm can get trapped on local maxima
(Charniak, 1993).
We seek here to capitalize on the intuition that,
at least early in learning, the learner should search
primarily for string-local structure, because most
structure is local.1 By penalizing dependencies be-
tween two words that are farther apart in the string,
we obtain consistent improvements in accuracy of
the learned model (?3).
We then explore how gradually changing ? over
time affects learning (?4): we start out with a
?This work was supported by a Fannie and John Hertz
Foundation fellowship to the first author and NSF ITR grant
IIS-0313193 to the second author. The views expressed are
not necessarily endorsed by the sponsors. We thank three
anonymous COLING-ACL reviewers for comments.
1To be concrete, in the corpora tested here, 95% of de-
pendency links cover ? 4 words (English, Bulgarian, Por-
tuguese), ? 5 words (German, Turkish), ? 6 words (Man-
darin).
model selection among values of ? and ?(0)
worst unsup. sup. oracle
German 19.8 19.8 54.4 54.4
English 21.8 41.6 41.6 42.0
Bulgarian 24.7 44.6 45.6 45.6
Mandarin 31.8 37.2 50.0 50.0
Turkish 32.1 41.2 48.0 51.4
Portuguese 35.4 37.4 42.3 43.0
Table 1: Baseline performance of EM-trained dependency
parsing models: F1 on non-$ attachments in test data, with
various model selection conditions (3 initializers? 6 smooth-
ing values). The languages are listed in decreasing order by
the training set size. Experimental details can be found in the
appendix.
strong preference for short dependencies, then re-
lax the preference. The new approach, structural
annealing, often gives superior performance.
An alternative structural bias is explored in ?5.
This approach views a sentence as a sequence
of one or more yields of separate, independent
trees. The points of segmentation are a hidden
variable, and during learning all possible segmen-
tations are entertained probabilistically. This al-
lows the learner to accept hypotheses that explain
the sentences as independent pieces.
In ?6 we briefly review contrastive estimation
(Smith and Eisner, 2005a), relating it to the new
method, and show its performance alone and when
augmented with structural bias.
2 Task and Model
In this paper we use a simple unlexicalized depen-
dency model due to Klein and Manning (2004).
The model is a probabilistic head automaton gram-
mar (Alshawi, 1996) with a ?split? form that ren-
ders it parseable in cubic time (Eisner, 1997).
Let x = ?x1, x2, ..., xn? be the sentence. x0 is a
special ?wall? symbol, $, on the left of every sen-
tence. A tree y is defined by a pair of functions
yleft and yright (both {0, 1, 2, ..., n} ? 2{1,2,...,n})
that map each word to its sets of left and right de-
pendents, respectively. The graph is constrained
to be a projective tree rooted at $: each word ex-
cept $ has a single parent, and there are no cycles
569
or crossing dependencies.2 yleft(0) is taken to be
empty, and yright(0) contains the sentence?s single
head. Let yi denote the subtree rooted at position
i. The probability P (yi | xi) of generating this
subtree, given its head word xi, is defined recur-
sively:
?
D?{left ,right}
pstop(stop | xi,D , [yD(i) = ?]) (1)
?
?
j?yD (i)
pstop(?stop | xi,D ,firsty(j))
?pchild(xj | xi,D)? P (yj | xj)
where firsty(j) is a predicate defined to be true iff
xj is the closest child (on either side) to its parent
xi. The probability of the entire tree is given by
p?(x,y) = P (y0 | $). The parameters ? are the
conditional distributions pstop and pchild.
Experimental baseline: EM. Following com-
mon practice, we always replace words by part-of-
speech (POS) tags before training or testing. We
used the EM algorithm to train this model on POS
sequences in six languages. Complete experimen-
tal details are given in the appendix. Performance
with unsupervised and supervised model selec-
tion across different ? values in add-? smoothing
and three initializers ?(0) is reported in Table 1.
The supervised-selected model is in the 40?55%
F1-accuracy range on directed dependency attach-
ments. (Here F1 ? precision ? recall; see ap-
pendix.) Supervised model selection, which uses
a small annotated development set, performs al-
most as well as the oracle, but unsupervised model
selection, which selects the model that maximizes
likelihood on an unannotated development set, is
often much worse.
3 Locality Bias among Trees
Hidden-variable estimation algorithms?
including EM?typically work by iteratively
manipulating the model parameters ? to improve
an objective function F (?). EM explicitly
alternates between the computation of a posterior
distribution over hypotheses, p?(y | x) (where
y is any tree with yield x), and computing a new
parameter estimate ?.3
2A projective parser could achieve perfect accuracy on our
English and Mandarin datasets, > 99% on Bulgarian, Turk-
ish, and Portuguese, and > 98% on German.
3For weighted grammar-based models, the posterior does
not need to be explicitly represented; instead expectations un-
der p? are used to compute updates to ?.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-1 -0.8 -0.6 -0.4 -0.2  0  0.2
?
F (E
M b
ase
line
)
German
English
Bulgarian
Mandarin
Turkish
Portuguese
Figure 1: Test-set F1 performance of models trained by EM
with a locality bias at varying ?. Each curve corresponds
to a different language and shows performance of supervised
model selection within a given ?, across ? and ?(0) values.
(See Table 3 for performance of models selected across ?s.)
We decode with ? = 0, though we found that keeping the
training-time value of ? would have had almost no effect. The
EM baseline corresponds to ? = 0.
One way to bias a learner toward local expla-
nations is to penalize longer attachments. This
was done for supervised parsing in different ways
by Collins (1997), Klein and Manning (2003),
and McDonald et al (2005), all of whom con-
sidered intervening material or coarse distance
classes when predicting children in a tree. Eis-
ner and Smith (2005) achieved speed and accuracy
improvements by modeling distance directly in a
ML-estimated (deficient) generative model.
Here we use string distance to measure the
length of a dependency link and consider the inclu-
sion of a sum-of-lengths feature in the probabilis-
tic model, for learning only. Keeping our original
model, we will simply multiply into the probabil-
ity of each tree another factor that penalizes long
dependencies, giving:
p??(x,y) ? p?(x,y)?e
?
?
?
??
n?
i=1
?
j?y(i)
|i? j|
?
?
?
?
(2)
where y(i) = yleft(i) ? yright(i). Note that if
? = 0, we have the original model. As ? ? ??,
the new model p?? will favor parses with shorter
dependencies. The dynamic programming algo-
rithms remain the same as before, with the appro-
priate e?|i?j| factor multiplied in at each attach-
ment between xi and xj . Note that when ? = 0,
p?? ? p?.
Experiment. We applied a locality bias to the
same dependency model by setting ? to different
570
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-1 -0.5  0  0.5  1  1.5
?
F
German
Bulgarian
Turkish
Figure 2: Test-set F1 performance of models trained by EM
with structural annealing on the distance weight ?. Here
we show performance with add-10 smoothing, the all-zero
initializer, for three languages with three different initial val-
ues ?0. Time progresses from left to right. Note that it is
generally best to start at ?0  0; note also the importance of
picking the right point on the curve to stop. See Table 3 for
performance of models selected across smoothing, initializa-
tion, starting, and stopping choices, in all six languages.
values in [?1, 0.2] (see Eq. 2). The same initial-
izers ?(0) and smoothing conditions were tested.
Performance of supervised model selection among
models trained at different ? values is plotted in
Fig. 1. When a model is selected across all condi-
tions (3 initializers ? 6 smoothing values ? 7 ?s)
using annotated development data, performance is
notably better than the EM baseline using the same
selection procedure (see Table 3, second column).
4 Structural Annealing
The central idea of this paper is to gradually
change (anneal) the bias ?. Early in learning, local
dependencies are emphasized by setting ?  0.
Then ? is iteratively increased and training re-
peated, using the last learned model to initialize.
This idea bears a strong similarity to determin-
istic annealing (DA), a technique used in clus-
tering and classification to smooth out objective
functions that are piecewise constant (hence dis-
continuous) or bumpy (non-concave) (Rose, 1998;
Ueda and Nakano, 1998). In unsupervised learn-
ing, DA iteratively re-estimates parameters like
EM, but begins by requiring that the entropy of
the posterior p?(y | x) be maximal, then gradu-
ally relaxes this entropy constraint. Since entropy
is concave in ?, the initial task is easy (maximize
a concave, continuous function). At each step the
optimization task becomes more difficult, but the
initializer is given by the previous step and, in
practice, tends to be close to a good local max-
imum of the more difficult objective. By the last
iteration the objective is the same as in EM, but the
annealed search process has acted like a good ini-
tializer. This method was applied with some suc-
cess to grammar induction models by Smith and
Eisner (2004).
In this work, instead of imposing constraints on
the entropy of the model, we manipulate bias to-
ward local hypotheses. As ? increases, we penal-
ize long dependencies less. We call this structural
annealing, since we are varying the strength of a
soft constraint (bias) on structural hypotheses. In
structural annealing, the final objective would be
the same as EM if our final ?, ?f = 0, but we
found that annealing farther (?f > 0) works much
better.4
Experiment: Annealing ?. We experimented
with annealing schedules for ?. We initialized at
?0 ? {?1,?0.4,?0.2}, and increased ? by 0.1 (in
the first case) or 0.05 (in the others) up to ?f = 3.
Models were trained to convergence at each ?-
epoch. Model selection was applied over the same
initialization and regularization conditions as be-
fore, ?0, and also over the choice of ?f , with stop-
ping allowed at any stage along the ? trajectory.
Trajectories for three languages with three dif-
ferent ?0 values are plotted in Fig. 2. Generally
speaking, ?0  0 performs better. There is con-
sistently an early increase in performance as ? in-
creases, but the stopping ?f matters tremendously.
Selected annealed-? models surpass EM in all six
languages; see the third column of Table 3. Note
that structural annealing does not always outper-
form fixed-? training (English and Portuguese).
This is because we only tested a few values of ?0,
since annealing requires longer runtime.
5 Structural Bias via Segmentation
A related way to focus on local structure early
in learning is to broaden the set of hypothe-
ses to include partial parse structures. If x =
?x1, x2, ..., xn?, the standard approach assumes
that x corresponds to the vertices of a single de-
pendency tree. Instead, we entertain every hypoth-
esis in which x is a sequence of yields from sepa-
rate, independently-generated trees. For example,
?x1, x2, x3? is the yield of one tree, ?x4, x5? is the
4The reader may note that ?f > 0 actually corresponds to
a bias toward longer attachments. A more apt description in
the context of annealing is to say that during early stages the
learner starts liking local attachments too much, and we need
to exaggerate ? to ?coax? it to new hypotheses. See Fig. 2.
571
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-1.5-1-0.5 0 0.5
?
F
German
Bulgarian
Turkish
Figure 3: Test-set F1 performance of models trained by EM
with structural annealing on the breakage weight ?. Here
we show performance with add-10 smoothing, the all-zero
initializer, for three languages with three different initial val-
ues ?0. Time progresses from left (large ?) to right. See Ta-
ble 3 for performance of models selected across smoothing,
initialization, and stopping choices, in all six languages.
yield of a second, and ?x6, ..., xn? is the yield of a
third. One extreme hypothesis is that x is n single-
node trees. At the other end of the spectrum is the
original set of hypotheses?full trees on x. Each
has a nonzero probability.
Segmented analyses are intermediate represen-
tations that may be helpful for a learner to use
to formulate notions of probable local structure,
without committing to full trees.5 We only allow
unobserved breaks, never positing a hard segmen-
tation of the training sentences. Over time, we in-
crease the bias against broken structures, forcing
the learner to commit most of its probability mass
to full trees.
5.1 Vine Parsing
At first glance broadening the hypothesis space
to entertain all 2n?1 possible segmentations may
seem expensive. In fact the dynamic program-
ming computation is almost the same as sum-
ming or maximizing over connected dependency
trees. For the latter, we use an inside-outside al-
gorithm that computes a score for every parse tree
by computing the scores of items, or partial struc-
tures, through a bottom-up process. Smaller items
are built first, then assembled using a set of rules
defining how larger items can be built.6
Now note that any sequence of partial trees
over x can be constructed by combining the same
items into trees. The only difference is that we
5See also work on partial parsing as a task in its own right:
Hindle (1990) inter alia.
6See Eisner and Satta (1999) for the relevant algorithm
used in the experiments.
are willing to consider unassembled sequences of
these partial trees as hypotheses, in addition to
the fully connected trees. One way to accom-
plish this in terms of yright(0) is to say that the
root, $, is allowed to have multiple children, in-
stead of just one. Here, these children are inde-
pendent of each other (e.g., generated by a uni-
gram Markov model). In supervised dependency
parsing, Eisner and Smith (2005) showed that im-
posing a hard constraint on the whole structure?
specifically that each non-$ dependency arc cross
fewer than k words?can give guaranteed O(nk2)
runtime with little to no loss in accuracy (for sim-
ple models). This constraint could lead to highly
contrived parse trees, or none at all, for some
sentences?both are avoided by the allowance of
segmentation into a sequence of trees (each at-
tached to $). The construction of the ?vine? (se-
quence of $?s children) takes only O(n) time once
the chart has been assembled.
Our broadened hypothesis model is a proba-
bilistic vine grammar with a unigram model over
$?s children. We allow (but do not require) seg-
mentation of sentences, where each independent
child of $ is the root of one of the segments. We do
not impose any constraints on dependency length.
5.2 Modeling Segmentation
Now the total probability of an n-length sentence
x, marginalizing over its hidden structures, sums
up not only over trees, but over segmentations of
x. For completeness, we must include a proba-
bility model over the number of trees generated,
which could be anywhere from 1 to n. The model
over the number T of trees given a sentence of
length n will take the following log-linear form:
P (T = t | n) = et?
/
n?
i=1
ei?
where ? ? R is the sole parameter. When ? = 0,
every value of T is equally likely. For ?  0, the
model prefers larger structures with few breaks.
At the limit (? ? ??), we achieve the standard
learning setting, where the model must explain x
using a single tree. We start however at ?  0,
where the model prefers smaller trees with more
breaks, in the limit preferring each word in x to be
its own tree. We could describe ?brokenness? as a
feature in the model whose weight, ?, is chosen
extrinsically (and time-dependently), rather than
empirically?just as was done with ?.
572
model selection among values of ?2 and ?(0)
worst unsup. sup. oracle
DORT1 32.5 59.3 63.4 63.4
Ge
r.
LENGTH 30.5 56.4 57.3 57.8
DORT1 20.9 56.6 57.4 57.4
En
g.
LENGTH 29.1 37.2 46.2 46.2
DORT1 19.4 26.0 40.5 43.1
Bu
l.
LENGTH 25.1 35.3 38.3 38.3
DORT1 9.4 24.2 41.1 41.1
Ma
n.
LENGTH 13.7 17.9 26.2 26.2
DORT1 7.3 38.6 58.2 58.2
Tu
r.
LENGTH 21.5 34.1 55.5 55.5
DORT1 35.0 59.8 71.8 71.8
Po
r.
LENGTH 30.8 33.6 33.6 33.6
Table 2: Performance of CE on test data, for different neigh-
borhoods and with different levels of regularization. Bold-
face marks scores better than EM-trained models selected the
same way (Table 1). The score is the F1 measure on non-$
attachments.
Annealing ? resembles the popular bootstrap-
ping technique (Yarowsky, 1995), which starts out
aiming for high precision, and gradually improves
coverage over time. With strong bias (?  0), we
seek a model that maintains high dependency pre-
cision on (non-$) attachments by attaching most
tags to $. Over time, as this is iteratively weak-
ened (? ? ??), we hope to improve coverage
(dependency recall). Bootstrapping was applied
to syntax learning by Steedman et al (2003). Our
approach differs in being able to remain partly ag-
nostic about each tag?s true parent (e.g., by giving
50% probability to attaching to $), whereas Steed-
man et al make a hard decision to retrain on a
whole sentence fully or leave it out fully. In ear-
lier work, Brill and Marcus (1992) adopted a ?lo-
cal first? iterative merge strategy for discovering
phrase structure.
Experiment: Annealing ?. We experimented
with different annealing schedules for ?. The ini-
tial value of ?, ?0, was one of {?12 , 0,
1
2}. After
EM training, ? was diminished by 110 ; this was re-
peated down to a value of ?f = ?3. Performance
after training at each ? value is shown in Fig. 3.7
We see that, typically, there is a sharp increase
in performance somewhere during training, which
typically lessens as ? ? ??. Starting ? too high
can also damage performance. This method, then,
7Performance measures are given using a full parser that
finds the single best parse of the sentence with the learned
parsing parameters. Had we decoded with a vine parser, we
would see a precision?, recall? curve as ? decreased.
is not robust to the choice of ?, ?0, or ?f , nor does
it always do as well as annealing ?, although con-
siderable gains are possible; see the fifth column
of Table 3.
By testing models trained with a fixed value of ?
(for values in [?1, 1]), we ascertained that the per-
formance improvement is due largely to annealing,
not just the injection of segmentation bias (fourth
vs. fifth column of Table 3).8
6 Comparison and Combination with
Contrastive Estimation
Contrastive estimation (CE) was recently intro-
duced (Smith and Eisner, 2005a) as a class of alter-
natives to the likelihood objective function locally
maximized by EM. CE was found to outperform
EM on the task of focus in this paper, when ap-
plied to English data (Smith and Eisner, 2005b).
Here we review the method briefly, show how it
performs across languages, and demonstrate that
it can be combined effectively with structural bias.
Contrastive training defines for each example xi
a class of presumably poor, but similar, instances
called the ?neighborhood,? N(xi), and seeks to
maximize
CN(?) =
?
i
log p?(xi | N(xi))
=
?
i
log
?
y p?(xi,y)
?
x??N(xi)
?
y p?(x
?,y)
At this point we switch to a log-linear (rather
than stochastic) parameterization of the same
weighted grammar, for ease of numerical opti-
mization. All this means is that ? (specifically,
pstop and pchild in Eq. 1) is now a set of nonnega-
tive weights rather than probabilities.
Neighborhoods that can be expressed as finite-
state lattices built from xi were shown to give sig-
nificant improvements in dependency parser qual-
ity over EM. Performance of CE using two of
those neighborhoods on the current model and
datasets is shown in Table 2.9 0-mean diagonal
Gaussian smoothing was applied, with different
variances, and model selection was applied over
smoothing conditions and the same initializers as
8In principle, segmentation can be combined with the lo-
cality bias in ?3 (?). In practice, we found that this usually
under-performed the EM baseline.
9We experimented with DELETE1, TRANSPOSE1, DELE-
TEORTRANSPOSE1, and LENGTH. To conserve space we
show only the latter two, which tend to perform best.
573
EM fixed ? annealed ? fixed ? annealed ? CE fixed ? + CE
? ?0 ? ?f ? ?0 ? ?f N N, ?
German 54.4 61.3 0.2 70.0 -0.4 ? 0.4 66.2 0.4 68.9 0.5 ? -2.4 63.4 DORT1 63.8 DORT1, -0.2
English 41.6 61.8 -0.6 53.8 -0.4 ? 0.3 55.6 0.2 58.4 0.5 ? 0.0 57.4 DORT1 63.5 DORT1, -0.4
Bulgarian 45.6 49.2 -0.2 58.3 -0.4 ? 0.2 47.3 -0.2 56.5 0 ? -1.7 40.5 DORT1 ?
Mandarin 50.0 51.1 -0.4 58.0 -1.0 ? 0.2 38.0 0.2 57.2 0.5 ? -1.4 43.4 DEL1 ?
Turkish 48.0 62.3 -0.2 62.4 -0.2 ? -0.15 53.6 -0.2 59.4 0.5 ? -0.7 58.2 DORT1 61.8 DORT1, -0.6
Portuguese 42.3 50.4 -0.4 50.2 -0.4 ? -0.1 51.5 0.2 62.7 0.5 ? -0.5 71.8 DORT1 72.6 DORT1, -0.2
Table 3: Summary comparing models trained in a variety of ways with some relevant hyperparameters. Supervised model
selection was applied in all cases, including EM (see the appendix). Boldface marks the best performance overall and trials
that this performance did not significantly surpass under a sign test (i.e., p 6< 0.05). The score is the F1 measure on non-$
attachments. The fixed ? + CE condition was tested only for languages where CE improved over EM.
before. Four of the languages have at least one ef-
fective CE condition, supporting our previous En-
glish results (Smith and Eisner, 2005b), but CE
was harmful for Bulgarian and Mandarin. Perhaps
better neighborhoods exist for these languages, or
there is some ideal neighborhood that would per-
form well for all languages.
Our approach of allowing broken trees (?5) is
a natural extension of the CE framework. Con-
trastive estimation views learning as a process of
moving posterior probability mass from (implicit)
negative examples to (explicit) positive examples.
The positive evidence, as in MLE, is taken to be
the observed data. As originally proposed, CE al-
lowed a redefinition of the implicit negative ev-
idence from ?all other sentences? (as in MLE)
to ?sentences like xi, but perturbed.? Allowing
segmentation of the training sentences redefines
the positive and negative evidence. Rather than
moving probability mass only to full analyses of
the training example xi, we also allow probability
mass to go to partial analyses of xi.
By injecting a bias (? 6= 0 or ? > ??) among
tree hypotheses, however, we have gone beyond
the CE framework. We have added features to
the tree model (dependency length-sum, number
of breaks), whose weights we extrinsically manip-
ulate over time to impose locality bias CN and im-
prove search on CN. Another idea, not explored
here, is to change the contents of the neighborhood
N over time.
Experiment: Locality Bias within CE. We
combined CE with a fixed-? locality bias for
neighborhoods that were successful in the earlier
CE experiment, namely DELETEORTRANSPOSE1
for German, English, Turkish, and Portuguese.
Our results, shown in the seventh column of Ta-
ble 3, show that, in all cases except Turkish, the
combination improves over either technique on its
own. We leave exploration of structural annealing
with CE to future work.
Experiment: Segmentation Bias within CE.
For (language, N) pairs where CE was effec-
tive, we trained models using CE with a fixed-
? segmentation model. Across conditions (? ?
[?1, 1]), these models performed very badly, hy-
pothesizing extremely local parse trees: typically
over 90% of dependencies were length 1 and
pointed in the same direction, compared with the
60?70% length-1 rate seen in gold standards. To
understand why, consider that the CE goal is to
maximize the score of a sentence and all its seg-
mentations while minimizing the scores of neigh-
borhood sentences and their segmentations. An n-
gram model can accomplish this, since the same
n-grams are present in all segmentations of x,
and (some) different n-grams appear in N(x)
(for LENGTH and DELETEORTRANSPOSE1). A
bigram-like model that favors monotone branch-
ing, then, is not a bad choice for a CE learner that
must account for segmentations of x and N(x).
Why doesn?t CE without segmentation resort to
n-gram-like models? Inspection of models trained
using the standard CE method (no segmentation)
with transposition-based neighborhoods TRANS-
POSE1 and DELETEORTRANSPOSE1 did have
high rates of length-1 dependencies, while the
poorly-performing DELETE1 models found low
length-1 rates. This suggests that a bias toward
locality (?n-gram-ness?) is built into the former
neighborhoods, and may partly explain why CE
works when it does. We achieved a similar locality
bias in the likelihood framework when we broad-
ened the hypothesis space, but doing so under CE
over-focuses the model on local structures.
574
7 Error Analysis
We compared errors made by the selected EM con-
dition with the best overall condition, for each lan-
guage. We found that the number of corrected at-
tachments always outnumbered the number of new
errors by a factor of two or more.
Further, the new models are not getting better
by merely reversing the direction of links made
by EM; undirected accuracy also improved signif-
icantly under a sign test (p < 10?6), across all six
languages. While the most common corrections
were to nouns, these account for only 25?41% of
corrections, indicating that corrections are not ?all
of the same kind.?
Finally, since more than half of corrections in
every language involved reattachment to a noun
or a verb (content word), we believe the improved
models to be getting closer than EM to the deeper
semantic relations between words that, ideally,
syntactic models should uncover.
8 Future Work
One weakness of all recent weighted grammar
induction work?including Klein and Manning
(2004), Smith and Eisner (2005b), and the present
paper?is a sensitivity to hyperparameters, includ-
ing smoothing values, choice of N (for CE), and
annealing schedules?not to mention initializa-
tion. This is quite observable in the results we have
presented. An obstacle for unsupervised learn-
ing in general is the need for automatic, efficient
methods for model selection. For annealing, in-
spiration may be drawn from continuation meth-
ods; see, e.g., Elidan and Friedman (2005). Ideally
one would like to select values simultaneously for
many hyperparameters, perhaps using a small an-
notated corpus (as done here), extrinsic figures of
merit on successful learning trajectories, or plau-
sibility criteria (Eisner and Karakos, 2005).
Grammar induction serves as a tidy example
for structural annealing. In future work, we envi-
sion that other kinds of structural bias and anneal-
ing will be useful in other difficult learning prob-
lems where hidden structure is required, including
machine translation, where the structure can con-
sist of word correspondences or phrasal or recur-
sive syntax with correspondences. The technique
bears some similarity to the estimation methods
described by Brown et al (1993), which started
by estimating simple models, using each model to
seed the next.
9 Conclusion
We have presented a new unsupervised parameter
estimation method, structural annealing, for learn-
ing hidden structure that biases toward simplic-
ity and gradually weakens (anneals) the bias over
time. We applied the technique to weighted de-
pendency grammar induction and achieved a sig-
nificant gain in accuracy over EM and CE, raising
the state-of-the-art across six languages from 42?
54% to 58?73% accuracy.
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Floresta
sinta?(c)tica: a treebank for Portuguese. In Proc. of LREC.
H. Alshawi. 1996. Head automata and bilingual tiling:
Translation with minimal representations. In Proc. of
ACL.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation
process in the Turkish treebank. In Proc. of LINC.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Proc. of the Acoustical Society of America.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER Treebank. In Proc. of Workshop on
Treebanks and Linguistic Theories.
E. Brill and M. Marcus. 1992. Automatically acquiring
phrase structure using distributional analysis. In Proc. of
DARPA Workshop on Speech and Natural Language.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In Proc. of CoNLL.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
M. Collins. 1997. Three generative, lexicalised models for
statistical parsing. In Proc. of ACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum like-
lihood estimation from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society B, 39:1?38.
J. Eisner and D. Karakos. 2005. Bootstrapping without the
boot. In Proc. of HLT-EMNLP.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head automaton grammars. In
Proc. of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and hard
constraints on dependency length. In Proc. of IWPT.
J. Eisner. 1997. Bilexical grammars and a cubic-time proba-
bilistic parser. In Proc. of IWPT.
G. Elidan and N. Friedman. 2005. Learning hidden variable
networks: the information bottleneck approach. Journal
of Machine Learning Research, 6:81?127.
D. Hindle. 1990. Noun classification from predicate-
argument structure. In Proc. of ACL.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for improved grammar induction. In Proc.
of ACL.
D. Klein and C. D. Manning. 2003. Fast exact inference with
a factored model for natural language parsing. In NIPS 15.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. of ACL.
575
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc. of
ACL.
K. Oflazer, B. Say, D. Z. Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In A. Abeille, editor,
Building and Exploiting Syntactically-Annotated Cor-
pora. Kluwer.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimiza-
tion problems. Proc. of the IEEE, 86(11):2210?2239.
K. Simov and P. Osenova. 2003. Practical annotation scheme
for an HPSG treebank of Bulgarian. In Proc. of LINC.
K. Simov, G. Popova, and P. Osenova. 2002. HPSG-
based syntactic treebank of Bulgarian (BulTreeBank). In
A. Wilson, P. Rayson, and T. McEnery, editors, A Rain-
bow of Corpora: Corpus Linguistics and the Languages
of the World, pages 135?42. Lincom-Europa.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2004.
Design and implementation of the Bulgarian HPSG-based
Treebank. Journal of Research on Language and Compu-
tation, 2(4):495?522.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In Proc.
of ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc. of
ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In Proc.
of IJCAI Workshop on Grammatical Inference Applica-
tions.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003.
Bootstrapping statistical parsers from small datasets. In
Proc. of EACL.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271?282.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn
Chinese Treebank: Phrase structure annotation of a large
corpus. Natural Language Engineering, 10(4):1?30.
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proc. of ACL.
A Experimental Setup
Following the usual conventions (Klein and Man-
ning, 2002), our experiments use treebank POS
sequences of length ? 10, stripped of words and
punctuation. For smoothing, we apply add-?, with
six values of ? (in CE trials, we use a 0-mean di-
agonal Gaussian prior with five different values of
?2). Our training datasets are:
? 8,227 German sentences from the TIGER Tree-
bank (Brants et al, 2002),
? 5,301 English sentences from the WSJ Penn
Treebank (Marcus et al, 1993),
? 4,929 Bulgarian sentences from the BulTree-
Bank (Simov et al, 2002; Simov and Osenova,
2003; Simov et al, 2004),
? 2,775 Mandarin sentences from the Penn Chi-
nese Treebank (Xue et al, 2004),
? 2,576 Turkish sentences from the METU-
Sabanci Treebank (Atalay et al, 2003; Oflazer et
al., 2003), and
? 1,676 Portuguese sentences from the Bosque
portion of the Floresta Sinta?(c)tica Treebank
(Afonso et al, 2002).
The Bulgarian, Turkish, and Portuguese datasets
come from the CoNLL-X shared task (Buchholz
and Marsi, 2006); we thank the organizers.
When comparing a hypothesized tree y to a
gold standard y?, precision and recall measures
are available. If every tree in the gold standard and
every hypothesis tree is such that |yright(0)| = 1,
then precision = recall = F1, since |y| = |y?|.
|yright(0)| = 1 for all hypothesized trees in this
paper, but not all treebank trees; hence we report
the F1 measure. The test set consists of around
500 sentences (in each language).
Iterative training proceeds until either 100 it-
erations have passed, or the objective converges
within a relative tolerance of  = 10?5, whichever
occurs first.
Models trained at different hyperparameter set-
tings and with different initializers are selected
using a 500-sentence development set. Unsuper-
vised model selection means the model with the
highest training objective value on the develop-
ment set was chosen. Supervised model selection
chooses the model that performs best on the anno-
tated development set. (Oracle and worst model
selection are chosen based on performance on the
test data.)
We use three initialization methods. We run a
single special E step (to get expected counts of
model events) then a single M step that renormal-
izes to get a probabilistic model ?(0). In initializer
1, the E step scores each tree as follows (only con-
nected trees are scored):
u(x,yleft ,yright) =
n?
i=1
?
j?y(i)
(
1 +
1
|i? j|
)
(Proper) expectations under these scores are com-
puted using an inside-outside algorithm. Initial-
izer 2 computes expected counts directly, without
dynamic programming. For an n-length sentence,
p(yright(0) = {i}) = 1n and p(j ? y(i)) ?
1
|i?j| .
These are scaled by an appropriate constant for
each sentence, then summed across sentences to
compute expected event counts. Initializer 3 as-
sumes a uniform distribution over hidden struc-
tures in the special E step by setting all log proba-
bilities to zero.
576
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787?794,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Minimum Risk Annealing for Training Log-Linear Models?
David A. Smith and Jason Eisner
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{dasmith,eisner}@jhu.edu
Abstract
When training the parameters for a natural language system,
one would prefer to minimize 1-best loss (error) on an eval-
uation set. Since the error surface for many natural language
problems is piecewise constant and riddled with local min-
ima, many systems instead optimize log-likelihood, which is
conveniently differentiable and convex. We propose training
instead to minimize the expected loss, or risk. We define this
expectation using a probability distribution over hypotheses
that we gradually sharpen (anneal) to focus on the 1-best hy-
pothesis. Besides the linear loss functions used in previous
work, we also describe techniques for optimizing nonlinear
functions such as precision or the BLEU metric. We present
experiments training log-linear combinations of models for
dependency parsing and for machine translation. In machine
translation, annealed minimum risk training achieves signif-
icant improvements in BLEU over standard minimum error
training. We also show improvements in labeled dependency
parsing.
1 Direct Minimization of Error
Researchers in empirical natural language pro-
cessing have expended substantial ink and effort in
developing metrics to evaluate systems automati-
cally against gold-standard corpora. The ongoing
evaluation literature is perhaps most obvious in the
machine translation community?s efforts to better
BLEU (Papineni et al, 2002).
Despite this research, parsing or machine trans-
lation systems are often trained using the much
simpler and harsher metric of maximum likeli-
hood. One reason is that in supervised training,
the log-likelihood objective function is generally
convex, meaning that it has a single global max-
imum that can be easily found (indeed, for su-
pervised generative models, the parameters at this
maximum may even have a closed-form solution).
In contrast to the likelihood surface, the error sur-
face for discrete structured prediction is not only
riddled with local minima, but piecewise constant
?This work was supported by an NSF graduate research
fellowship for the first author and by NSF ITR grant IIS-
0313193 and ONR grant N00014-01-1-0685. The views ex-
pressed are not necessarily endorsed by the sponsors. We
thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and
the reviewers for helpful discussions and comments.
and not everywhere differentiable with respect to
the model parameters (Figure 1). Despite these
difficulties, some work has shown it worthwhile
to minimize error directly (Och, 2003; Bahl et al,
1988).
We show improvements over previous work on
error minimization by minimizing the risk or ex-
pected error?a continuous function that can be
derived by combining the likelihood with any eval-
uation metric (?2). Seeking to avoid local min-
ima, deterministic annealing (Rose, 1998) gradu-
ally changes the objective function from a convex
entropy surface to the more complex risk surface
(?3). We also discuss regularizing the objective
function to prevent overfitting (?4). We explain
how to compute expected loss under some evalu-
ation metrics common in natural language tasks
(?5). We then apply this machinery to training
log-linear combinations of models for dependency
parsing and for machine translation (?6). Finally,
we note the connections of minimum risk training
to max-margin training and minimum Bayes risk
decoding (?7), and recapitulate our results (?8).
2 Training Log-Linear Models
In this work, we focus on rescoring with log-
linear models. In particular, our experiments con-
sider log-linear combinations of a relatively small
number of features over entire complex structures,
such as trees or translations, known in some pre-
vious work as products of experts (Hinton, 1999)
or logarithmic opinion pools (Smith et al, 2005).
A feature in the combined model might thus be
a log probability from an entire submodel. Giv-
ing this feature a small or negative weight can
discount a submodel that is foolishly structured,
badly trained, or redundant with the other features.
For each sentence xi in our training corpus S,
we are given Ki possible analyses yi,1, . . . yi,Ki .
(These may be all of the possible translations or
parse trees; or only the Ki most probable under
787
Figure 1: The loss surface for a machine translation sys-
tem: while other parameters are held constant, we vary the
weights on the distortion and word penalty features. Note the
piecewise constant regions with several local maxima.
some other model; or only a random sample of
size Ki.) Each analysis has a vector of real-valued
features (i.e., factors, or experts) denoted fi,k. The
score of the analysis yi,k is ? ? fi,k, the dot prod-
uct of its features with a parameter vector ?. For
each sentence, we obtain a normalized probability
distribution over the Ki analyses as
p?(yi,k | xi) =
exp ? ? fi,k
?Ki
k?=1 exp ? ? fi,k?
(1)
We wish to adjust this model?s parameters ?
to minimize the severity of the errors we make
when using it to choose among analyses. A loss
function Ly?(y) assesses a penalty for choosing
y when y? is correct. We will usually write this
simply as L(y) since y? is fixed and clear from
context. For clearer exposition, we assume below
that the total loss over some test corpus is the sum
of the losses on individual sentences, although we
will revisit that assumption in ?5.
2.1 Minimizing Loss or Expected Loss
One training criterion directly mimics test condi-
tions. It looks at the loss incurred if we choose the
best analysis of each xi according to the model:
min
?
?
i
L(argmax
yi
p?(yi | xi)) (2)
Since small changes in ? either do not change
the best analysis or else push a different analy-
sis to the top, this objective function is piecewise
constant, hence not amenable to gradient descent.
Och (2003) observed, however, that the piecewise-
constant property could be exploited to character-
ize the function exhaustively along any line in pa-
rameter space, and hence to minimize it globally
along that line. By calling this global line mini-
mization as a subroutine of multidimensional opti-
mization, he was able to minimize (2) well enough
to improve over likelihood maximization for train-
ing factored machine translation systems.
Instead of considering only the best hypothesis
for any ?, we can minimize risk, i.e., the expected
loss under p? across all analyses yi:
min
?
Ep?L(yi,k)
def
= min
?
?
i
?
k
L(yi,k)p?(yi,k | xi)
(3)
This ?smoothed? objective is now continuous and
differentiable. However, it no longer exactly mim-
ics test conditions, and it typically remains non-
convex, so that gradient descent is still not guaran-
teed to find a global minimum. Och (2003) found
that such smoothing during training ?gives almost
identical results? on translation metrics.
The simplest possible loss function is 0/1 loss,
where L(y) is 0 if y is the true analysis y?i and
1 otherwise. This loss function does not at-
tempt to give partial credit. Even in this sim-
ple case, assuming P 6= NP, there exists no gen-
eral polynomial-time algorithm for even approx-
imating (2) to within any constant factor, even
for Ki = 2 (Hoffgen et al, 1995, from Theo-
rem 4.10.4).1 The same is true for for (3), since
for Ki = 2 it can be easily shown that the min 0/1
risk is between 50% and 100% of the min 0/1 loss.
2.2 Maximizing Likelihood
Rather than minimizing a loss function suited to
the task, many systems (especially for language
modeling) choose simply to maximize the prob-
ability of the gold standard. The log of this likeli-
hood is a convex function of the parameters ?:
max
?
?
i
log p?(y
?
i | xi) (4)
where y?i is the true analysis of sentence xi. The
only wrinkle is that p?(y?i | xi) may be left unde-
fined by equation (1) if y?i is not in our set of Ki
hypotheses. When maximizing likelihood, there-
fore, we will replace y?i with the min-loss analy-
sis in the hypothesis set; if multiple analyses tie
1Known algorithms are exponential but only in the dimen-
sionality of the feature space (Johnson and Preparata, 1978).
788
?10 ?5 0 5 10
17.
5
18.0
18.5
19.0
Translation model 1
Ble
u %
? = ?? = 0.1? = 1? = 10
Figure 2: Loss and expected loss as one translation model?s
weight varies: the gray line (? = ?) shows true BLEU (to be
optimized in equation (2)). The black lines show the expected
BLEU as ? in equation (5) increases from 0.1 toward?.
for this honor, we follow Charniak and Johnson
(2005) in summing their probabilities.2
Maximizing (4) is equivalent to minimizing an
upper bound on the expected 0/1 loss
?
i(1 ?
p?(y?i | xi)). Though the log makes it tractable,
this remains a 0/1 objective that does not give par-
tial credit to wrong answers, such as imperfect but
useful translations. Most systems should be eval-
uated and preferably trained on less harsh metrics.
3 Deterministic Annealing
To balance the advantages of direct loss minimiza-
tion, continuous risk minimization, and convex
optimization, deterministic annealing attempts
the solution of increasingly difficult optimization
problems (Rose, 1998). Adding a scale hyperpa-
rameter ? to equation (1), we have the following
family of distributions:
p?,?(yi,k | xi) =
(exp ? ? fi,k)
?
?Ki
k?=1
(
exp ? ? fi,k?
)? (5)
When ? = 0, all yi,k are equally likely, giving
the uniform distribution; when ? = 1, we recover
the model in equation (1); and as ? ? ?, we
approach the winner-take-all Viterbi function that
assigns probability 1 to the top-scoring analysis.
For a fixed ?, deterministic annealing solves
min
?
Ep?,? [L(yi,k)] (6)
2An alternative would be to artificially add y?i (e.g., the
reference translation(s)) to the hypothesis set during training.
We then increase ? according to some schedule
and optimize ? again. When ? is low, the smooth
objective might allow us to pass over local min-
ima that could open up at higher ?. Figure 3 shows
how the smoothing is gradually weakened to reach
the risk objective (3) as ? ? 1 and approach the
true error objective (2) as ? ? ?.
Our risk minimization most resembles the work
of Rao and Rose (2001), who trained an isolated-
word speech recognition system for expected
word-error rate. Deterministic annealing has also
been used to tackle non-convex likelihood sur-
faces in unsupervised learning with EM (Ueda and
Nakano, 1998; Smith and Eisner, 2004). Other
work on ?generalized probabilistic descent? mini-
mizes a similar objective function but with ? held
constant (Katagiri et al, 1998).
Although the entropy is generally higher at
lower values of ?, it varies as the optimization
changes ?. In particular, a pure unregularized log-
linear model such as (5) is really a function of ? ??,
so the optimizer could exactly compensate for in-
creased ? by decreasing the ? vector proportion-
ately!3 Most deterministic annealing procedures,
therefore, express a direct preference on the en-
tropy H , and choose ? and ? accordingly:
min
?,?
Ep?,? [L(yi,k)] ? T ?H(p?,?) (7)
In place of a schedule for raising ?, we now use
a cooling schedule to lower T from ? to ??,
thereby weakening the preference for high en-
tropy. The Lagrange multiplier T on entropy is
called ?temperature? due to a satisfying connec-
tion to statistical mechanics. Once T is quite cool,
it is common in practice to switch to raising ? di-
rectly and rapidly (quenching) until some conver-
gence criterion is met (Rao and Rose, 2001).
4 Regularization
Informally, high temperature or ? < 1 smooths
our model during training toward higher-entropy
conditional distributions that are not so peaked at
the desired analyses y?i . Another reason for such
smoothing is simply to prevent overfitting to these
training examples.
A typical way to control overfitting is to use a
quadratic regularizing term, ||?||2 or more gener-
ally
?
d ?
2
d/2?
2
d. Keeping this small keeps weights
3For such models, ? merely aids the nonlinear optimizer
in its search, by making it easier to scale all of ? at once.
789
low and entropy high. We may add this regularizer
to equation (6) or (7). In the maximum likelihood
framework, we may subtract it from equation (4),
which is equivalent to maximum a posteriori esti-
mation with a diagonal Gaussian prior (Chen and
Rosenfeld, 1999). The variance ?2d may reflect a
prior belief about the potential usefulness of fea-
ture d, or may be tuned on heldout data.
Another simple regularization method is to stop
cooling before T reaches 0 (cf. Elidan and Fried-
man (2005)). If loss on heldout data begins to
increase, we may be starting to overfit. This
technique can be used along with annealing or
quadratic regularization and can achieve addi-
tional accuracy gains, which we report elsewhere
(Dreyer et al, 2006).
5 Computing Expected Loss
At each temperature setting of deterministic an-
nealing, we need to minimize the expected loss on
the training corpus. We now discuss how this ex-
pectation is computed. When rescoring, we as-
sume that we simply wish to combine, in some
way, statistics of whole sentences4 to arrive at the
overall loss for the corpus. We consider evalua-
tion metrics for natural language tasks from two
broadly applicable classes: linear and nonlinear.
A linear metric is a sum (or other linear combi-
nation) of the loss or gain on individual sentences.
Accuracy?in dependency parsing, part-of-speech
tagging, and other labeling tasks?falls into this
class, as do recall, word error rate in ASR, and
the crossing-brackets metric in parsing. Thanks to
the linearity of expectation, we can easily compute
our expected loss in equation (6) by adding up the
expected loss on each sentence.
Some other metrics involve nonlinear combi-
nations over the sentences of the corpus. One
common example is precision, P
def
=
?
i ci/
?
i ai,
where ci is the number of correctly posited ele-
ments, and ai is the total number of posited ele-
ments, in the decoding of sentence i. (Depend-
ing on the task, the elements may be words, bi-
grams, labeled constituents, etc.) Our goal is to
maximize P , so during a step of deterministic an-
nealing, we need to maximize the expectation of
P when the sentences are decoded randomly ac-
cording to equation (5). Although this expectation
is continuous and differentiable as a function of
4Computing sentence xi?s statistics usually involves iter-
ating over hypotheses yi,1, . . . yi,Ki . If these share substruc-
ture in a hypothesis lattice, dynamic programming may help.
?, unfortunately it seems hard to compute for any
given ?. We observe however that an equivalent
goal is to minimize ? logP . Taking that as our
loss function instead, equation (6) now needs to
minimize the expectation of ? logP ,5 which de-
composes somewhat more nicely:
E[? logP ] = E[log
?
i
ai ? log
?
i
ci]
= E[logA] ? E[logC] (8)
where the integer random variables A =
?
i ai
and C =
?
i ci count the number of posited and
correctly posited elements over the whole corpus.
To approximate E[g(A)], where g is any twice-
differentiable function (here g = log), we can ap-
proximate g locally by a quadratic, given by the
Taylor expansion of g aboutA?s mean ?A = E[A]:
E[g(A)] ? E[g(?A) + (A? ?A)g?(?A)
+
1
2
(A? ?A)
2g??(?A)]
= g(?A) + E[A? ?A]g?(?A)
+
1
2
E[(A? ?A)2]g??(?A)
= g(?A) +
1
2
?2Ag
??(?A).
Here ?A =
?
i ?ai and ?
2
A =
?
i ?
2
ai , since A
is a sum of independent random variables ai (i.e.,
given the current model parameters ?, our ran-
domized decoder decodes each sentence indepen-
dently). In other words, given our quadratic ap-
proximation to g, E[g(A)] depends on the (true)
distribution of A only through the single-sentence
means ?ai and variances ?
2
ai , which can be found
by enumerating the Ki decodings of sentence i.
The approximation becomes arbitrarily good as
we anneal ? ? ?, since then ?2A ? 0 and
E[g(A)] focuses on g near ?A. For equation (8),
E[g(A)] = E[logA] ? log(?A) ?
?2A
2?2A
and E[logC] is found similarly.
Similar techniques can be used to compute the
expected logarithms of some other non-linear met-
rics, such as F-measure (the harmonic mean of
precision and recall)6 and Papineni et al (2002)?s
5This changes the trajectory that DA takes through pa-
rameter space, but ultimately the objective is the same: as
? ? ? over the course of DA, minimizing E[? logP ] be-
comes indistinguishable from maximizing E[P ].
6R
def
= C/B; the count B of correct elements is known.
So logF
def
= log 2PR/(P + R) = log 2R/(1 + R/P ) =
log 2C/B ? log(1 +A/B). Consider g(x) = log 1 + x/B.
790
BLEU translation metric (the geometric mean of
several precisions). In particular, the expectation
of log BLEU distributes over its N +1 summands:
log BLEU = min(1 ?
r
A1
, 0) +
N?
n=1
wn logPn
where Pn is the precision of the n-gram elements
in the decoding.7 As is standard in MT research,
we take wn = 1/N and N = 4. The first term in
the BLEU score is the log brevity penalty, a con-
tinuous function of A1 (the total number of uni-
gram tokens in the decoded corpus) that fires only
ifA1 < r (the average word count of the reference
corpus). We again use a Taylor series to approxi-
mate the expected log brevity penalty.
We mention an alternative way to compute (say)
the expected precisionC/A: integrate numerically
over the joint density of C and A. How can we
obtain this density? As (C,A) =
?
i(ci, ai) is a
sum of independent random length-2 vectors, its
mean vector and 2 ? 2 covariance matrix can be
respectively found by summing the means and co-
variance matrices of the (ci, ai), each exactly com-
puted from the distribution (5) over Ki hypothe-
ses. We can easily approximate (C,A) by the
(continuous) bivariate normal with that mean and
covariance matrix8?or else accumulate an exact
representation of its (discrete) probability mass
function by a sequence of numerical convolutions.
6 Experiments
We tested the above training methods on two
different tasks: dependency parsing and phrase-
based machine translation. Since the basic setup
was the same for both, we outline it here before
describing the tasks in detail.
In both cases, we start with 8 to 10 models
(the ?experts?) already trained on separate training
data. To find the optimal coefficients ? for a log-
linear combination of these experts, we use sepa-
rate development data, using the following proce-
dure due to Och (2003):
1. Initialization: Initialize ? to the 0 vector. For
each development sentence xi, set its Ki-best
list to ? (thus Ki = 0).
7BLEU is careful when measuring ci on a particular de-
coding yi,k. It only counts the first two copies of the (e.g.) as
correct if the occurs at most twice in any reference translation
of xi. This ?clipping? does not affect the rest of our method.
8Reasonable for a large corpus, by Lyapunov?s central
limit theorem (allows non-identically distributed summands).
2. Decoding: For each development sentence
xi, use the current ? to extract the 200 anal-
yses yi,k with the greatest scores exp ? ? fi,k.
Calcuate each analysis?s loss statistics (e.g.,
ci and ai), and add it to the Ki-best list if it is
not already there.
3. Convergence: If Ki has not increased for
any development sentence, or if we have
reached our limit of 20 iterations, stop: the
search has converged.
4. Optimization: Adjust ? to improve our ob-
jective function over the whole development
corpus. Return to step 2.
Our experiments simply compare three proce-
dures at step 4. We may either
? maximize log-likelihood (4), a convex func-
tion, at a given level of quadratic regulariza-
tion, by BFGS gradient descent;
? minimize error (2) by Och?s line search
method, which globally optimizes each com-
ponent of ? while holding the others con-
stant;9 or
? minimize the same error (2) more effectively,
by raising ? ? ? while minimizing the an-
nealed risk (6), that is, cooling T ? ?? (or
? ? ?) and at each value, locally minimiz-
ing equation (7) using BFGS.
Since these different optimization procedures
will usually find different ? at step 4, their K-best
lists will diverge after the first iteration.
For final testing, we selected among several
variants of each procedure using a separate small
heldout set. Final results are reported for a larger,
disjoint test set.
6.1 Machine Translation
For our machine translation experiments, we
trained phrase-based alignment template models
of Finnish-English, French-English, and German-
English, as follows. For each language pair, we
aligned 100,000 sentence pairs from European
Parliament transcripts using GIZA++. We then
used Philip Koehn?s phrase extraction software
to merge the GIZA++ alignments and to extract
9The component whose optimization achieved the lowest
loss is then updated. The process iterates until no lower loss
can be found. In contrast, Papineni (1999) proposed a linear
programming method that may search along diagonal lines.
791
and score the alignment template model?s phrases
(Koehn et al, 2003).
The Pharaoh phrase-based decoder uses pre-
cisely the setup of this paper. It scores a candidate
translation (including its phrasal alignment to the
original text) as ? ? f , where f is a vector of the
following 8 features:
1. the probability of the source phrase given the
target phrase
2. the probability of the target phrase given the
source phrase
3. the weighted lexical probability of the source
words given the target words
4. the weighted lexical probability of the target
words given the source words
5. a phrase penalty that fires for each template
in the translation
6. a distortion penalty that fires when phrases
translate out of order
7. a word penalty that fires for each English
word in the output
8. a trigram language model estimated on the
English side of the bitext
Our goal was to train the weights ? of these 8
features. We used the method described above,
employing the Pharaoh decoder at step 2 to gener-
ate the 200-best translations according to the cur-
rent ?. As explained above, we compared three
procedures at step 4: maximum log-likelihood by
gradient ascent; minimum error using Och?s line-
search method; and annealed minimum risk. As
our development data for training ?, we used 200
sentence pairs for each language pair.
Since our methods can be tuned with hyperpa-
rameters, we used performance on a separate 200-
sentence held-out set to choose the best hyper-
parameter values. The hyperparameter levels for
each method were
? maximum likelihood: a Gaussian prior with
all ?2d at 0.25, 0.5, 1, or ?
? minimum error: 1, 5, or 10 different ran-
dom starting points, drawn from a uniform
Optimization Finnish- French- German-
Procedure English English English
Max. like. 5.02 5.31 7.43
Min. error 10.27 26.16 20.94
Ann. min. risk 16.43 27.31 21.30
Table 1: BLEU 4n1 percentage on translating 2000-
sentence test corpora, after training the 8 experts on 100,000
sentence pairs and fitting their weights ? on 200 more, using
settings tuned on a further 200. The current minimum risk an-
nealing method achieved significant improvements over min-
imum error and maximum likelihood at or below the 0.001
level, using a permutation test with 1000 replications.
distribution on [?1, 1]? [?1, 1]? ? ? ? , when
optimizing ? at an iteration of step 4.10
? annealed minimum risk: with explicit en-
tropy constraints, starting temperature T ?
{100, 200, 1000}; stopping temperature T ?
{0.01, 0.001}. The temperature was cooled
by half at each step; then we quenched by
doubling ? at each step. (We also ran exper-
iments with quadratic regularization with all
?2d at 0.5, 1, or 2 (?4) in addition to the en-
tropy constraint. Also, instead of the entropy
constraint, we simply annealed on ? while
adding a quadratic regularization term. None
of these regularized models beat the best set-
ting of standard deterministic annealing on
heldout or test data.)
Final results on a separate 2000-sentence test set
are shown in table 1. We evaluated translation us-
ing BLEU with one reference translation and n-
grams up to 4. The minimum risk annealing pro-
cedure significantly outperformed maximum like-
lihood and minimum error training in all three lan-
guage pairs (p < 0.001, paired-sample permuta-
tion test with 1000 replications).
Minimum risk annealing generally outper-
formed minimum error training on the held-out
set, regardless of the starting temperature T . How-
ever, higher starting temperatures do give better
performance and a more monotonic learning curve
(Figure 3), a pattern that held up on test data.
(In the same way, for minimum error training,
10That is, we run step 4 from several starting points, finish-
ing at several different points; we pick the finishing point with
lowest development error (2). This reduces the sensitivity of
this method to the starting value of ?. Maximum likelihood
is not sensitive to the starting value of ? because it has only a
global optimum; annealed minimum risk is not sensitive to it
either, because initially ? ? 0, making equation (6) flat.
792
5 10 15 20
16
18
20
22
Iteration
Bleu
T=1000T=200T=100Min. error
Figure 3: Iterative change in BLEU on German-English de-
velopment (upper) and held-out (lower), under annealed min-
imum risk training with different starting temperatures, ver-
sus minimum error training with 10 random restarts.
5 10 15 20
5
10
15
20
Iteration
Bleu
10 restarts1 restart
Figure 4: Iterative change in BLEU on German-English
development (upper) and held-out (lower), using 10 random
restarts vs. only 1.
more random restarts give better performance and
a more monotonic learning curve?see Figure 4.)
Minimum risk annealing did not always win on
the training set, suggesting that its advantage is
not superior minimization but rather superior gen-
eralization: under the risk criterion, multiple low-
loss hypotheses per sentence can help guide the
learner to the right part of parameter space.
Although the components of the translation and
language models interact in complex ways, the im-
provement on Finnish-English may be due in part
to the higher weight that minimum risk annealing
found for the word penalty. That system is there-
fore more likely to produce shorter output like i
have taken note of your remarks and i also agree
with that . than like this longer output from the
minimum-error-trained system: i have taken note
of your remarks and i shall also agree with all that
the union .
We annealed using our novel expected-BLEU
approximation from ?5. We found this to perform
significantly better on BLEU evaluation than if we
trained with a ?linearized? BLEU that summed
per-sentence BLEU scores (as used in minimum
Bayes risk decoding by Kumar and Byrne (2004)).
6.2 Dependency Parsing
We trained dependency parsers for three different
languages: Bulgarian, Dutch, and Slovenian.11 In-
put sentences to the parser were already tagged for
parts of speech. Each parser employed 10 experts,
each parameterized as a globally normalized log-
linear model (Lafferty et al, 2001). For example,
the 9th component of the feature vector fi,k (which
described the kth parse of the ith sentence) was the
log of that parse?s normalized probability accord-
ing to the 9th expert.
Each expert was trained separately to maximize
the conditional probability of the correct parse
given the sentence. We used 10 iterations of gradi-
ent ascent. To speed training, for each of the first
9 iterations, the gradient was estimated on a (dif-
ferent) sample of only 1000 training sentences.
We then trained the vector ?, used to combine
the experts, to minimize the number of labeled de-
pendency attachment errors on a 200-sentence de-
velopment set. Optimization proceeded over lists
of the 200-best parses of each sentence produced
by a joint decoder using the 10 experts.
Evaluating on labeled dependency accuracy on
200 test sentences for each language, we see that
minimum error and annealed minimum risk train-
ing are much closer than for MT. For Bulgarian
and Dutch, they are statistically indistinguishable
using a paired-sample permutations test with 1000
replications. Indeed, on Dutch, all three opti-
mization procedures produce indistinguishable re-
sults. On Slovenian, annealed minimum risk train-
ing does show a significant improvement over the
other two methods. Overall, however, the results
for this task are mediocre. We are still working on
improving the underlying experts.
7 Related Work
We have seen that annealed minimum risk train-
ing provides a useful alternative to maximum like-
lihood and minimum error training. In our ex-
periments, it never performed significantly worse
11For information on these corpora, see the CoNLL-X
shared task on multilingual dependency parsing: http:
//nextens.uvt.nl/?conll/.
793
Optimization labeled dependency acc. [%]
Procedure Slovenian Bulgarian Dutch
Max. like. 27.78 47.23 36.78
Min. error 22.52 54.72 36.78
Ann. min. risk 31.16 54.66 36.71
Table 2: Labeled dependency accuracy on parsing 200-
sentence test corpora, after training 10 experts on 1000 sen-
tences and fitting their weights ? on 200 more. For Slove-
nian, minimum risk annealing is significantly better than the
other training methods, while minimum error is significantly
worse. For Bulgarian, both minimum error and annealed min-
imum risk training achieve significant gains over maximum
likelihood, but are indistinguishable from each other. For
Dutch, the three methods are indistinguishable.
than either and in some cases significantly helped.
Note, however, that annealed minimum risk train-
ing results in a deterministic classifier just as these
other training procedures do. The orthogonal
technique of minimum Bayes risk decoding has
achieved gains on parsing (Goodman, 1996) and
machine translation (Kumar and Byrne, 2004). In
speech recognition, researchers have improved de-
coding by smoothing probability estimates numer-
ically on heldout data in a manner reminiscent of
annealing (Goel and Byrne, 2000). We are inter-
ested in applying our techniques for approximat-
ing nonlinear loss functions to MBR by perform-
ing the risk minimization inside the dynamic pro-
gramming or other decoder.
Another training approach that incorporates ar-
bitrary loss functions is found in the structured
prediction literature in the margin-based-learning
community (Taskar et al, 2004; Crammer et al,
2004). Like other max-margin techniques, these
attempt to make the best hypothesis far away from
the inferior ones. The distinction is in using a loss
function to calculate the required margins.
8 Conclusions
Despite the challenging shape of the error sur-
face, we have seen that it is practical to opti-
mize task-specific error measures rather than op-
timizing likelihood?it produces lower-error sys-
tems. Different methods can be used to attempt
this global, non-convex optimization. We showed
that for MT, and sometimes for dependency pars-
ing, an annealed minimum risk approach to opti-
mization performs significantly better than a pre-
vious line-search method that does not smooth the
error surface. It never does significantly worse.
With such improved methods for minimizing er-
ror, we can hope to make better use of task-specific
training criteria in NLP.
References
L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mer-
cer. 1988. A new algorithm for the estimation of hidden
Markov model parameters. In ICASSP, pages 493?496.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In ACL,
pages 173?180.
S. F. Chen and R. Rosenfeld. 1999. A gaussian prior for
smoothing maximum entropy models. Technical report,
CS Dept., Carnegie Mellon University.
K. Crammer, R. McDonald, and F. Pereira. 2004. New large
margin algorithms for structured prediction. In Learning
with Structured Outputs (NIPS).
M. Dreyer, D. A. Smith, and N. A. Smith. 2006. Vine parsing
and minimum risk reranking for speed and precision. In
CoNLL.
G. Elidan and N. Friedman. 2005. Learning hidden variable
networks: The information bottleneck approach. JMLR,
6:81?127.
V. Goel and W. J. Byrne. 2000. Minimum Bayes-Risk au-
tomatic speech recognition. Computer Speech and Lan-
guage, 14(2):115?135.
J. T. Goodman. 1996. Parsing algorithms and metrics. In
ACL, pages 177?183.
G. Hinton. 1999. Products of experts. In Proc. of ICANN,
volume 1, pages 1?6.
K.-U. Hoffgen, H.-U. Simon, and K. S. Van Horn. 1995.
Robust trainability of single neurons. J. of Computer and
System Sciences, 50(1):114?125.
D. S. Johnson and F. P. Preparata. 1978. The densest hemi-
sphere problem. Theoretical Comp. Sci., 6(93?107).
S. Katagiri, B.-H. Juang, and C.-H. Lee. 1998. Pattern recog-
nition using a family of design algorithms based upon the
generalized probabilistic descent method. Proc. IEEE,
86(11):2345?2373, November.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, pages 48?54.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk decod-
ing for statistical machine translation. In HLT-NAACL.
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Condi-
tional random fields: Probabilistic models for segmenting
and labeling sequence data. In ICML.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In ACL, pages 311?318.
K. A. Papineni. 1999. Discriminative training via linear
programming. In ICASSP.
A. Rao and K. Rose. 2001. Deterministically annealed de-
sign of Hidden Markov Model speech recognizers. IEEE
Trans. on Speech and Audio Processing, 9(2):111?126.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimiza-
tion problems. Proc. IEEE, 86(11):2210?2239.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In ACL, pages
486?493.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL, pages
18?25.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In EMNLP, pages 1?8.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271?282.
794
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 81?84,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Machine Translation System Combination
using ITG-based Alignments?
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, Markus Dreyer
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218
{damianos,eisner,khudanpur,dreyer}@jhu.edu
Abstract
Given several systems? automatic translations
of the same sentence, we show how to com-
bine them into a confusion network, whose
various paths represent composite translations
that could be considered in a subsequent
rescoring step. We build our confusion net-
works using the method of Rosti et al (2007),
but, instead of forming alignments using the
tercom script (Snover et al, 2006), we create
alignments that minimize invWER (Leusch
et al, 2003), a form of edit distance that
permits properly nested block movements of
substrings. Oracle experiments with Chinese
newswire and weblog translations show that
our confusion networks contain paths which
are significantly better (in terms of BLEU and
TER) than those in tercom-based confusion
networks.
1 Introduction
Large improvements in machine translation (MT)
may result from combining different approaches
to MT with mutually complementary strengths.
System-level combination of translation outputs is
a promising path towards such improvements. Yet
there are some significant hurdles in this path. One
must somehow align the multiple outputs?to iden-
tify where different hypotheses reinforce each other
and where they offer alternatives. One must then
?This work was partially supported by the DARPA GALE
program (Contract No HR0011-06-2-0001). Also, we would
like to thank the IBM Rosetta team for the availability of several
MT system outputs.
use this alignment to hypothesize a set of new, com-
posite translations, and select the best composite hy-
pothesis from this set. The alignment step is difficult
because different MT approaches usually reorder the
translated words differently. Training the selection
step is difficult because identifying the best hypothe-
sis (relative to a known reference translation) means
scoring all the composite hypotheses, of which there
may be exponentially many.
Most MT combination methods do create an ex-
ponentially large hypothesis set, representing it as a
confusion network of strings in the target language
(e.g., English). (A confusion network is a lattice
where every node is on every path; i.e., each time
step presents an independent choice among several
phrases. Note that our contributions in this paper
could be applied to arbitrary lattice topologies.) For
example, Bangalore et al (2001) show how to build
a confusion network following a multistring align-
ment procedure of several MT outputs. The proce-
dure (used primarily in biology, (Thompson et al,
1994)) yields monotone alignments that minimize
the number of insertions, deletions, and substitu-
tions. Unfortunately, monotone alignments are often
poor, since machine translations (particularly from
different models) can vary significantly in their word
order. Thus, when Matusov et al (2006) use this
procedure, they deterministically reorder each trans-
lation prior to the monotone alignment.
The procedure described by Rosti et al (2007)
has been shown to yield significant improvements in
translation quality, and uses an estimate of Trans-
lation Error Rate (TER) to guide the alignment.
(TER is defined as the minimum number of inser-
81
tions, deletions, substitutions and block shifts be-
tween two strings.) A remarkable feature of that
procedure is that it performs the alignment of the
output translations (i) without any knowledge of the
translation model used to generate the translations,
and (ii) without any knowledge of how the target
words in each translation align back to the source
words. In fact, it only requires a procedure for cre-
ating pairwise alignments of translations that allow
appropriate re-orderings. For this, Rosti et al (2007)
use the tercom script (Snover et al, 2006), which
uses a number of heuristics (as well as dynamic pro-
gramming) for finding a sequence of edits (inser-
tions, deletions, substitutions and block shifts) that
convert an input string to another. In this paper, we
show that one can build better confusion networks
(in terms of the best translation possible from the
confusion network) when the pairwise alignments
are computed not by tercom, which approximately
minimizes TER, but instead by an exact minimiza-
tion of invWER (Leusch et al, 2003), which is a re-
stricted version of TER that permits only properly
nested sets of block shifts, and can be computed in
polynomial time.
The paper is organized as follows: a summary of
TER, tercom, and invWER, is presented in Section
2. The system combination procedure is summa-
rized in Section 3, while experimental (oracle) re-
sults are presented in Section 4. Conclusions are
given in Section 5.
2 Comparing tercom and invWER
The tercom script was created mainly in order to
measure translation quality based on TER. As is
proved by Shapira and Storer (2002), computation
of TER is an NP-complete problem. For this reason,
tercom uses some heuristics in order to compute an
approximation to TER in polynomial time. In the
rest of the paper, we will denote this approximation
as tercomTER, to distinguish it from (the intractable)
TER. The block shifts which are allowed in tercom
have to adhere to the following constraints: (i) A
block that has an exact match cannot be moved, and
(ii) for a block to be moved, it should have an exact
match in its new position. However, this sometimes
leads to counter-intuitive sequences of edits; for in-
stance, for the sentence pair
?thomas jefferson says eat your vegetables?
?eat your cereal thomas edison says?,
tercom finds an edit sequence of cost 5, instead of
the optimum 3. Furthermore, the block selection is
done in a greedy manner, and the final outcome is
dependent on the shift order, even when the above
constraints are imposed.
An alternative to tercom, considered in this pa-
per, is to use the Inversion Transduction Grammar
(ITG) formalism (Wu, 1997) which allows one to
view the problem of alignment as a problem of bilin-
gual parsing. Specifically, ITGs can be used to find
the optimal edit sequence under the restriction that
block moves must be properly nested, like paren-
theses. That is, if an edit sequence swaps adjacent
substrings A and B of the original string, then any
other block move that affects A (or B) must stay
completely within A (or B). An edit sequence with
this restriction corresponds to a synchronous parse
tree under a simple ITG that has one nonterminal
and whose terminal symbols allow insertion, dele-
tion, and substitution.
The minimum-cost ITG tree can be found by dy-
namic programming. This leads to invWER (Leusch
et al, 2003), which is defined as the minimum num-
ber of edits (insertions, deletions, substitutions and
block shifts allowed by the ITG) needed to convert
one string to another. In this paper, the minimum-
invWER alignments are used for generating confu-
sion networks. The alignments are found with a 11-
rule Dyna program (Dyna is an environment that fa-
cilitates the development of dynamic programs?see
(Eisner et al, 2005) for more details). This pro-
gram was further sped up (by about a factor of 2)
with an A? search heuristic computed by additional
code. Specifically, our admissible outside heuris-
tic for aligning two substrings estimated the cost of
aligning the words outside those substrings as if re-
ordering those words were free. This was compli-
cated somewhat by type/token issues and by the fact
that we were aligning (possibly weighted) lattices.
Moreover, the same Dyna program was used for the
computation of the minimum invWER path in these
confusion networks (oracle path), without having to
invoke tercom numerous times to compute the best
sentence in an N -best list.
The two competing alignment procedures were
82
Lang. / Genre tercomTER invWER
Arabic NW 15.1% 14.9%
Arabic WB 26.0% 25.8%
Chinese NW 26.1% 25.6%
Chinese WB 30.9% 30.4%
Table 1: Comparison of average per-document ter-
comTER with invWER on the EVAL07 GALE Newswire
(?NW?) and Weblogs (?WB?) data sets.
used to estimate the TER between machine transla-
tion system outputs and reference translations. Ta-
ble 1 shows the TER estimates using tercom and
invWER. These were computed on the translations
submitted by a system to NIST for the GALE eval-
uation in June 2007. The references used are the
post-edited translations for that system (i.e., these
are ?HTER? approximations). As can be seen from
the table, in all language and genre conditions, in-
vWER gives a better approximation to TER than
tercomTER. In fact, out of the roughly 2000 total
segments in all languages/genres, tercomTER gives
a lower number of edits in only 8 cases! This is a
clear indication that ITGs can explore the space of
string permutations more effectively than tercom.
3 The System Combination Approach
ITG-based alignments and tercom-based alignments
were also compared in oracle experiments involving
confusion networks created through the algorithm of
Rosti et al (2007). The algorithm entails the follow-
ing steps:
? Computation of all pairwise alignments be-
tween system hypotheses (either using ITGs or
tercom); for each pair, one of the hypotheses
plays the role of the ?reference?.
? Selection of a system output as the ?skele-
ton? of the confusion network, whose words
are used as anchors for aligning all other ma-
chine translation outputs together. Each arc has
a translation output word as its label, with the
special token ?NULL? used to denote an inser-
tion/deletion between the skeleton and another
system output.
? Multiple consecutive words which are inserted
relative to the skeleton form a phrase that gets
Genre CNs with tercom CNs with ITG
NW 50.1% (27.7%) 48.8% (28.3%)
WB 51.0% (25.5%) 50.5% (26.0%)
Table 2: TercomTERs of invWER-oracles and (in paren-
theses) oracle BLEU scores of confusion networks gen-
erated with tercom and ITG alignments. The best results
per row are shown in bold.
aligned with an epsilon arc of the confusion
network.
? Setting the weight of each arc equal to the
negative log (posterior) probability of its la-
bel; this probability is proportional to the num-
ber of systems which output the word that gets
aligned in that location. Note that the algo-
rithm of Rosti et al (2007) used N -best lists in
the combination. Instead, we used the single-
best output of each system; this was done be-
cause not all systems were providing N -best
lists, and an unbalanced inclusion would favor
some systems much more than others. Further-
more, for each genre, one of our MT systems
was significantly better than the others in terms
of word order, and it was chosen as the skele-
ton.
4 Experimental Results
Table 2 shows tercomTERs of invWER-oracles (as
computed by the aforementioned Dyna program)
and oracle BLEU scores of the confusion networks.
The confusion networks were generated using 9
MT systems applied to the Chinese GALE 2007
Dev set, which consists of roughly 550 Newswire
segments, and 650 Weblog segments. The confu-
sion networks which were generated with the ITG-
based alignments gave significantly better oracle ter-
comTERs (significance tested with a Fisher sign
test, p ? 0.02) and better oracle BLEU scores.
The BLEU oracle sentences were found using the
dynamic-programming algorithm given in Dreyer et
al. (2007) and measured using Philipp Koehn?s eval-
uation script. On the other hand, a comparison be-
tween the 1-best paths did not reveal significant dif-
ferences that would favor one approach or the other
(either in terms of tercomTER or BLEU).
83
We also tried to understand which alignment
method gives higher probability to paths ?close?
to the corresponding oracle. To do that, we com-
puted the probability that a random path from a
confusion network is within x edits from its ora-
cle. This computation was done efficiently using
finite-state-machine operations, and did not involve
any randomization. Preliminary experiments with
the invWER-oracles show that the probability of all
paths which are within x = 3 edits from the oracle
is roughly the same for ITG-based and tercom-based
confusion networks. We plan to report our findings
for a whole range of x-values in future work. Fi-
nally, a runtime comparison of the two techniques
shows that ITGs are much more computationally
intensive: on average, ITG-based alignments took
1.5 hours/sentence (owing to their O(n6) complex-
ity), while tercom-based alignments only took 0.4
sec/sentence.
5 Concluding Remarks
We compared alignments obtained using the widely
used program tercom with alignments obtained with
ITGs and we established that the ITG alignments are
superior in two ways. Specifically: (a) we showed
that invWER (computed using the ITG alignments)
gives a better approximation to TER between ma-
chine translation outputs and human references than
tercom; and (b) in an oracle system combination ex-
periment, we found that confusion networks gen-
erated with ITG alignments contain better oracles,
both in terms of tercomTER and in terms of BLEU.
Future work will include rescoring results with a
language model, as well as exploration of heuristics
(e.g., allowing only ?short? block moves) that can
reduce the ITG alignment complexity to O(n4).
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In Proceedings of ASRU, pages
351?354.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing reordering constraints for smt using efficient bleu
oracle computation. In Proceedings of SSST, NAACL-
HLT 2007 / AMTA Workshop on Syntax and Structure
in Statistical Translation, pages 103?110, Rochester,
New York, April. Association for Computational Lin-
guistics.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling comp ling: Weighted dynamic program-
ming and the Dyna language. In Proceedings of HLT-
EMNLP, pages 281?290. Association for Computa-
tional Linguistics, October.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications to
machine translation evaluation. In Proceedings of the
Machine Translation Summit 2003, pages 240?247,
September.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
Proceedings of EACL, pages 33?40.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. In Proceedings of the ACL, pages 312?
319, June.
D. Shapira and J. A. Storer. 2002. Edit distance with
move operations. In Proceedings of the 13th Annual
Symposium on Combinatorial Pattern Matching, vol-
ume 2373/2002, pages 85?98, Fukuoka, Japan, July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, Cam-
bridge, MA, August.
J. D. Thompson, D. G. Higgins, and T. J. Gibson.
1994. Clustalw: Improving the sensitivity of progres-
sive multiple sequence alignment through sequence
weighting, position-specific gap penalties and weight
matrix choice. Nucleic Acids Research, 22(22):4673?
4680.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
84
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593?601,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Variational Decoding for Statistical Machine Translation
Zhifei Li and Jason Eisner and Sanjeev Khudanpur
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com, jason@cs.jhu.edu, khudanpur@jhu.edu
Abstract
Statistical models in machine translation
exhibit spurious ambiguity. That is, the
probability of an output string is split
among many distinct derivations (e.g.,
trees or segmentations). In principle, the
goodness of a string is measured by the
total probability of its many derivations.
However, finding the best string (e.g., dur-
ing decoding) is then computationally in-
tractable. Therefore, most systems use
a simple Viterbi approximation that mea-
sures the goodness of a string using only
its most probable derivation. Instead,
we develop a variational approximation,
which considers all the derivations but still
allows tractable decoding. Our particular
variational distributions are parameterized
as n-gram models. We also analytically
show that interpolating these n-gram mod-
els for different n is similar to minimum-
risk decoding for BLEU (Tromble et al,
2008). Experiments show that our ap-
proach improves the state of the art.
1 Introduction
Ambiguity is a central issue in natural language
processing. Many systems try to resolve ambigu-
ities in the input, for example by tagging words
with their senses or choosing a particular syntax
tree for a sentence. These systems are designed to
recover the values of interesting latent variables,
such as word senses, syntax trees, or translations,
given the observed input.
However, some systems resolve too many ambi-
guities. They recover additional latent variables?
so-called nuisance variables?that are not of in-
terest to the user.1 For example, though machine
translation (MT) seeks to output a string, typical
MT systems (Koehn et al, 2003; Chiang, 2007)
1These nuisance variables may be annotated in training
data, but it is more common for them to be latent even there,
i.e., there is no supervision as to their ?correct? values.
will also recover a particular derivation of that out-
put string, which specifies a tree or segmentation
and its alignment to the input string. The compet-
ing derivations of a string are interchangeable for
a user who is only interested in the string itself, so
a system that unnecessarily tries to choose among
them is said to be resolving spurious ambiguity.
Of course, the nuisance variables are important
components of the system?s model. For example,
the translation process from one language to an-
other language may follow some hidden tree trans-
formation process, in a recursive fashion. Many
features of the model will crucially make reference
to such hidden structures or alignments.
However, collapsing the resulting spurious
ambiguity?i.e., marginalizing out the nuisance
variables?causes significant computational dif-
ficulties. The goodness of a possible MT out-
put string should be measured by summing up
the probabilities of all its derivations. Unfortu-
nately, finding the best string is then computation-
ally intractable (Sima?an, 1996; Casacuberta and
Higuera, 2000).2 Therefore, most systems merely
identify the single most probable derivation and
report the corresponding string. This corresponds
to a Viterbi approximation that measures the good-
ness of an output string using only its most proba-
ble derivation, ignoring all the others.
In this paper, we propose a variational method
that considers all the derivations but still allows
tractable decoding. Given an input string, the orig-
inal system produces a probability distribution p
over possible output strings and their derivations
(nuisance variables). Our method constructs a sec-
ond distribution q ? Q that approximates p as well
as possible, and then finds the best string accord-
ing to q. The last step is tractable because each
q ? Q is defined (unlike p) without reference to
nuisance variables. Notice that q here does not ap-
proximate the entire translation process, but only
2May and Knight (2006) have successfully used tree-
automaton determinization to exactly marginalize out some
of the nuisance variables, obtaining a distribution over parsed
translations. However, they do not marginalize over these
parse trees to obtain a distribution over translation strings.
593
the distribution over output strings for a particular
input. This is why it can be a fairly good approxi-
mation even without using the nuisance variables.
In practice, we approximate with several dif-
ferent variational families Q, corresponding to n-
gram (Markov) models of different orders. We
geometrically interpolate the resulting approxima-
tions q with one another (and with the original dis-
tribution p), justifying this interpolation as similar
to the minimum-risk decoding for BLEU proposed
by Tromble et al (2008). Experiments show that
our approach improves the state of the art.
The methods presented in this paper should be
applicable to collapsing spurious ambiguity for
other tasks as well. Such tasks include data-
oriented parsing (DOP), applications of Hidden
Markov Models (HMMs) and mixture models, and
other models with latent variables. Indeed, our
methods were inspired by past work on varia-
tional decoding for DOP (Goodman, 1996) and for
latent-variable parsing (Matsuzaki et al, 2005).
2 Background
2.1 Terminology
In MT, spurious ambiguity occurs both in regular
phrase-based systems (e.g., Koehn et al (2003)),
where different segmentations lead to the same
translation string (Figure 1), and in syntax-based
systems (e.g., Chiang (2007)), where different
derivation trees yield the same string (Figure 2).
In the Hiero system (Chiang, 2007) we are us-
ing, each string corresponds to about 115 distinct
derivations on average.
We use x to denote the input string, and D(x) to
consider the set of derivations then considered by
the system. Each derivation d ? D(x) yields some
translation string y = Y(d) in the target language.
We write D(x, y)
def
= {d ? D(x) : Y(d) = y} to
denote the set of all derivations that yield y. Thus,
the set of translations permitted by the model is
T(y)
def
= {y : D(x, y) 6= ?} (or equivalently,
T(y)
def
= {Y(d) : d ? D(x)}). We write y? for
the translation string that is actually output.
2.2 Maximum A Posterior (MAP) Decoding
For a given input sentence x, a decoding method
identifies a particular ?best? output string y?. The
maximum a posteriori (MAP) decision rule is
y? = argmax
y?T(x)
p(y | x) (1)
machine translation software
?  ?  ?  ?  ?  ?
machine translation software
?  ?  ?  ?  ?  ?
Figure 1: Segmentation ambiguity in phrase-based MT: two
different segmentations lead to the same translation string.
S->(? ?, machine) S->( ? , translation) S->( ? , software)
S->(? ?, machine)
?
S->( ? , software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(S0 ?  S1, S0 translation S1)
Figure 2: Tree ambiguity in syntax-based MT: two derivation
trees yield the same translation string.
(An alternative decision rule, minimum Bayes
risk (MBR), will be discussed in Section 4.)
To obtain p(y | x) above, we need to marginal-
ize over a nuisance variable, the derivation of y.
Therefore, the MAP decision rule becomes
y? = argmax
y?T(x)
?
d?D(x,y)
p(y, d | x) (2)
where p(y, d | x) is typically derived from a log-
linear model as follows,
p(y, d | x) =
e??s(x,y,d)
Z(x)
=
e??s(x,y,d)
?
y,d e
??s(x,y,d)
(3)
where ? is a scaling factor to adjust the sharp-
ness of the distribution, the score s(x, y, d) is a
learned linear combination of features of the triple
(x, y, d), and Z(x) is a normalization constant.
Note that p(y, d | x) = 0 if y 6= Y(d). Our deriva-
tion set D(x) is encoded in polynomial space, us-
ing a hypergraph or lattice.3 However, both |D(x)|
and |T(x)| may be exponential in |x|. Since the
marginalization needs to be carried out for each
member of T(x), the decoding problem of (2)
turns out to be NP-hard,4 as shown by Sima?an
(1996) for a similar problem.
3A hypergraph is analogous to a parse forest (Huang and
Chiang, 2007). (A finite-state lattice is a special case.) It can
be used to encode exponentially many hypotheses generated
by a phrase-based MT system (e.g., Koehn et al (2003)) or a
syntax-based MT system (e.g., Chiang (2007)).
4Note that the marginalization for a particular y would be
tractable; it is used at training time in certain training objec-
tive functions, e.g., maximizing the conditional likelihood of
a reference translation (Blunsom et al, 2008).
594
2.3 Viterbi Approximation
To approximate the intractable decoding problem
of (2), most MT systems (Koehn et al, 2003; Chi-
ang, 2007) use a simple Viterbi approximation,
y? = argmax
y?T(x)
pViterbi(y | x) (4)
= argmax
y?T(x)
max
d?D(x,y)
p(y, d | x) (5)
= Y
(
argmax
d?D(x)
p(y, d | x)
)
(6)
Clearly, (5) replaces the sum in (2) with a max.
In other words, it approximates the probability of
a translation string by the probability of its most-
probable derivation. (5) is found quickly via (6).
The Viterbi approximation is simple and tractable,
but it ignores most derivations.
2.4 N-best Approximation (or Crunching)
Another popular approximation enumerates the N
best derivations in D(x), a set that we call ND(x).
Modifying (2) to sum over only these derivations
is called crunching by May and Knight (2006):
y? = argmax
y?T(x)
pcrunch(y | x) (7)
= argmax
y?T(x)
?
d?D(x,y)?ND(x)
p(y, d | x)
3 Variational Approximate Decoding
The Viterbi and crunching methods above approx-
imate the intractable decoding of (2) by ignor-
ing most of the derivations. In this section, we
will present a novel variational approximation,
which considers all the derivations but still allows
tractable decoding.
3.1 Approximate Inference
There are several popular approaches to approxi-
mate inference when exact inference is intractable
(Bishop, 2006). Stochastic techniques such as
Markov Chain Monte Carlo are exact in the limit
of infinite runtime, but tend to be too slow for large
problems. By contrast, deterministic variational
methods (Jordan et al, 1999), including message-
passing (Minka, 2005), are inexact but scale up
well. They approximate the original intractable
distribution with one that factorizes better or has
a specific parametric form (e.g., Gaussian).
In our work, we use a fast variational method.
Variational methods generally work as follows.
When exact inference under a complex model p
is intractable, one can approximate the posterior
p(y | x) by a tractable model q(y), where q ? Q is
chosen to minimize some information loss such as
the KL divergence KL(p ? q). The simpler model
q can then act as a surrogate for p during inference.
3.2 Variational Decoding for MT
For each input sentence x, we assume that a base-
line MT system generates a hypergraph HG(x)
that compactly encodes the derivation set D(x)
along with a score for each d ? D(x),5 which we
interpret as p(y, d | x) (or proportional to it). For
any single y ? T(x), it would be tractable using
HG(x) to compute p(y | x) =
?
d p(y, d | x).
However, as mentioned, it is intractable to find
argmaxy p(y | x) as required by the MAP de-
coding (2), so we seek an approximate distribution
q(y) ? p(y | x).6
For a fixed x, we seek a distribution q ? Q that
minimizes the KL divergence from p to q (both
regarded as distributions over y):7
q? = argmin
q?Q
KL(p ? q) (8)
= argmin
q?Q
?
y?T(x)
(p log p? p log q) (9)
= argmax
q?Q
?
y?T(x)
p log q (10)
So far, in order to approximate the intractable
optimization problem (2), we have defined an-
other optimization problem (10). If computing
p(y | x) during decoding is computationally in-
tractable, one might wonder if the optimization
problem (10) is any simpler. We will show this is
the case. The trick is to parameterize q as a fac-
torized distribution such that the estimation of q?
and decoding using q? are both tractable through
efficient dynamic programs. In the next three sub-
sections, we will discuss the parameterization, es-
timation, and decoding, respectively.
3.2.1 Parameterization of q
In (10), Q is a family of distributions. If we se-
lect a large family Q, we can allow more com-
plex distributions, so that q? will better approxi-
mate p. If we select a smaller family Q, we can
5The baseline system may return a pruned hypergraph,
which has the effect of pruning D(x) and T(x) as well.
6Following the convention in describing variational infer-
ence, we write q(y) instead of q(y | x), even though q(y)
always depends on x implicitly.
7To avoid clutter, we denote p(y | x) by p, and q(y) by q.
We drop p log p from (9) because it is constant with respect
to q. We then flip the sign and change argmin to argmax.
595
guarantee that q? will have a simple form with
many conditional independencies, so that q?(y)
and y? = argmaxy q
?(y) are easier to compute.
Since each q(y) is a distribution over output
strings, a natural choice for Q is the family of
n-gram models. To obtain a small KL diver-
gence (8), we should make n as large as possible.
In fact, q? ? p as n ? ?. Of course, this last
point also means that our computation becomes
intractable as n??.8 However, if p(y | x) is de-
fined by a hypergraph HG(x) whose structure ex-
plicitly incorporates an m-gram language model,
both training and decoding will be efficient when
m ? n. We will give algorithms for this case that
are linear in the size of HG(x).9
Formally, each q ? Q takes the form
q(y) =
?
w?W
q(r(w) | h(w))cw(y) (11)
where W is a set of n-gram types. Each w ?W is
an n-gram, which occurs cw(y) times in the string
y, and w may be divided into an (n ? 1)-gram
prefix h(w) (the history) and a 1-gram suffix r(w)
(the rightmost or current word).
8Blunsom et al (2008) effectively do take n = ?, by
maintaining the whole translation string in the dynamic pro-
gramming state. They alleviate the computation cost some-
how by using aggressive beam pruning, which might be sen-
sible for their relatively small task (e.g., input sentences of
< 10 words). But, we are interested in improving the perfor-
mance for a large-scale system, and thus their method is not
a viable solution. Moreover, we observe in our experiments
that using a larger n does not improve much over n = 2.
9A reviewer asks about the interaction with backed-off
language models. The issue is that the most compact finite-
state representations of these (Allauzen et al, 2003), which
exploit backoff structure, are not purely m-gram for any
m. They yield more compact hypergraphs (Li and Khudan-
pur, 2008), but unfortunately those hypergraphs might not be
treatable by Fig. 4?since where they back off to less than an
n-gram, e is not informative enough for line 8 to find w.
We sketch a method that works for any language model
given by a weighted FSA, L. The variational family Q can
be specified by any deterministic weighted FSA, Q, with
weights parameterized by ?. One seeks ? to minimize (8).
Intersect HG(x) with an ?unweighted? version of Q in
which all arcs have weight 1, so that Q does not prefer
any string to another. By lifting weights into an expectation
semiring (Eisner, 2002), it is then possible to obtain expected
transition counts in Q (where the expectation is taken under
p), or other sufficient statistics needed to estimate ?.
This takes only time O(|HG(x)|) when L is a left-to-right
refinement of Q (meaning that any two prefix strings that
reach the same state in L also reach the same state in Q),
for then intersecting L or HG(x) with Q does not split any
states. That is the case when L and Q are respectively pure
m-gram and n-gram models withm ? n, as assumed in (12)
and Figure 4. It is also the case when Q is a pure n-gram
model and L is constructed not to back off beyond n-grams;
or when the variational family Q is defined by deliberately
taking the FSA Q to have the same topology as L.
The parameters that specify a particular q ? Q
are the (normalized) conditional probability distri-
butions q(r(w) | h(w)). We will now see how to
estimate these parameters to approximate p(? | x)
for a given x at test time.
3.2.2 Estimation of q?
Note that the objective function (8)?(10) asks us to
approximate p as closely as possible, without any
further smoothing. (It is assumed that p is already
smoothed appropriately, having been constructed
from channel and language models that were esti-
mated with smoothing from finite training data.)
In fact, if p were the empirical distribution over
strings in a training corpus, then q? of (10) is just
the maximum-likelihood n-gram model?whose
parameters, trivially, are just unsmoothed ratios of
the n-gram and (n?1)-gram counts in the training
corpus. That is, q?(r(w) | h(w)) = c(w)c(h(w)) .
Our actual job is exactly the same, except that p
is specified not by a corpus but by the hypergraph
HG(x). The only change is that the n-gram counts
c?(w) are no longer integers from a corpus, but are
expected counts under p:10
q?(r(w) | h(w)) =
c?(w)
c?(h(w))
= (12)
?
y cw(y)p(y | x)
?
y ch(w)(y)p(y | x)
=
?
y,d cw(y)p(y, d | x)
?
y,d ch(w)(y)p(y, d | x)
Now, the question is how to efficiently compute
(12) from the hypergraph HG(x). To develop the
intuition, we first present a brute-force algorithm
in Figure 3. The algorithm is brute-force since
it first needs to unpack the hypergraph and enu-
merate each possible derivation in the hypergraph
(see line 1), which is computationally intractable.
The algorithm then enumerates each n-gram and
(n ? 1)-gram in y and accumulates its soft count
into the expected count, and finally obtains the pa-
rameters of q? by taking count ratios via (12).
Figure 4 shows an efficient version that exploits
the packed-forest structure of HG(x) in com-
puting the expected counts. Specifically, it first
runs the inside-outside procedure, which annotates
each node (say v) with both an inside weight ?(v)
and an outside weight ?(v). The inside-outside
also finds Z(x), the total weight of all derivations.
With these weights, the algorithm then explores
the hypergraph once more to collect the expected
10One can prove (12) via Lagrange multipliers, with q?(? |
h) constrained to be a normalized distribution for each h.
596
Brute-Force-MLE(HG(x ))
1 for y , d in HG(x)  each derivation
2 forw in y  each n-gram type
3  accumulate soft count
4 c?(w) + = cw(y) ? p(y, d | x)
5 c?(h(w)) + = cw(y) ? p(y, d | x)
6 q? ? MLE using formula (12)
7 return q?
Figure 3: Brute-force estimation of q?.
Dynamic-Programming-MLE(HG(x ))
1 run inside-outside on the hypergraph HG(x)
2 for v in HG(x)  each node
3 for e ? B(v)  each incoming hyperedge
4 ce ? pe ? ?(v)/Z(x)
5 for u ? T (e)  each antecedent node
6 ce ? ce ? ?(u)
7  accumulate soft count
8 forw in e  each n-gram type
9 c?(w) + = cw(e) ? ce
10 c?(h(w)) + = cw(e) ? ce
11 q? ? MLE using formula (12)
12 return q?
Figure 4: Dynamic programming estimation of q?. B(v) rep-
resents the set of incoming hyperedges of node v; pe repre-
sents the weight of the hyperedge e itself; T (e) represents
the set of antecedent nodes of hyperedge e. Please refer to
the text for the meanings of other notations.
counts. For each hyperedge (say e), it first gets the
posterior weight ce (see lines 4-6). Then, for each
n-gram type (say w), it increments the expected
count by cw(e) ? ce, where cw(e) is the number of
copies of n-gram w that are added by hyperedge
e, i.e., that appear in the yield of e but not in the
yields of any of its antecedents u ? T (e).
While there may be exponentially many deriva-
tions, the hypergraph data structure represents
them in polynomial space by allowing multiple
derivations to share subderivations. The algorithm
of Figure 4 may be run over this packed forest
in time O(|HG(x)|) where |HG(x)| is the hyper-
graph?s size (number of hyperedges).
3.2.3 Decoding with q?
When translating x at runtime, the q? constructed
from HG(x) will be used as a surrogate for p dur-
ing decoding. We want its most probable string:
y? = argmax
y
q?(y) (13)
Since q? is an n-gram model, finding y? is equiv-
alent to a shortest-path problem in a certain graph
whose edges correspond to n-grams (weighted
with negative log-probabilities) and whose ver-
tices correspond to (n? 1)-grams.
However, because q? only approximates p, y? of
(13) may be locally appropriate but globally inade-
quate as a translation of x. Observe, e.g., that an n-
gram model q?(y) will tend to favor short strings
y, regardless of the length of x. Suppose x = le
chat chasse la souris (?the cat chases the mouse?)
and q? is a bigram approximation to p(y | x). Pre-
sumably q?(the | START), q?(mouse | the), and
q?(END | mouse) are all large in HG(x). So the
most probable string y? under q? may be simply
?the mouse,? which is short and has a high proba-
bility but fails to cover x.
Therefore, a better way of using q? is to restrict
the search space to the original hypergraph, i.e.:
y? = argmax
y?T(x)
q?(y) (14)
This ensures that y? is a valid string in the origi-
nal hypergraph HG(x), which will tend to rule out
inadequate translations like ?the mouse.?
If our sole objective is to get a good approxi-
mation to p(y | x), we should just use a single
n-gram model q? whose order n is as large as pos-
sible, given computational constraints. This may
be regarded as favoring n-grams that are likely to
appear in the reference translation (because they
are likely in the derivation forest). However, in or-
der to score well on the BLEU metric for MT eval-
uation (Papineni et al, 2001), which gives partial
credit, we would also like to favor lower-order n-
grams that are likely to appear in the reference,
even if this means picking some less-likely high-
order n-grams. For this reason, it is useful to in-
terpolate different orders of variational models,
y? = argmax
y?T(x)
?
n
?n ? log q
?
n(y) (15)
where n may include the value of zero, in which
case log q?0(y)
def
= |y|, corresponding to a conven-
tional word penalty feature. In the geometric inter-
polation above, the weight ?n controls the relative
veto power of the n-gram approximation and can
be tuned using MERT (Och, 2003) or a minimum
risk procedure (Smith and Eisner, 2006).
Lastly, note that Viterbi and variational approx-
imation are different ways to approximate the ex-
act probability p(y | x), and each of them has
pros and cons. Specifically, Viterbi approxima-
tion uses the correct probability of one complete
597
derivation, but ignores most of the derivations in
the hypergraph. In comparison, the variational ap-
proximation considers all the derivations in the hy-
pergraph, but uses only aggregate statistics of frag-
ments of derivations. Therefore, it is desirable to
interpolate further with the Viterbi approximation
when choosing the final translation output:11
y? = argmax
y?T(x)
?
n
?n ? log q
?
n(y)
+ ?v ? log pViterbi(y | x) (16)
where the first term corresponds to the interpolated
variational decoding of (15) and the second term
corresponds to the Viterbi decoding of (4).12 As-
suming ?v > 0, the second term penalizes transla-
tions with no good derivation in the hypergraph.13
For n ? m, any of these decoders (14)?
(16) may be implemented efficiently by using the
n-gram variational approximations q? to rescore
HG(x)?preserving its hypergraph topology, but
modifying the hyperedge weights.14 While the
original weights gave derivation d a score of
log p(d | x), the weights as modified for (16)
will give d a score of
?
n ?n ? log q
?
n(Y(d)) + ?v ?
log p(d | x). We then find the best-scoring deriva-
tion and output its target yield; that is, we find
argmaxy?T(x) via Y(argmaxd?D(x)).
4 Variational vs. Min-Risk Decoding
In place of the MAP decoding, another commonly
used decision rule is minimum Bayes risk (MBR):
y? = argmin
y
R(y) = argmin
y
?
y?
l(y, y?)p(y? | x)
(17)
11It would also be possible to interpolate with the N -best
approximations (see Section 2.4), with some complications.
12Zens and Ney (2006) use a similar decision rule as here
and they also use posterior n-gram probabilities as feature
functions, but their model estimation and decoding are over
an N -best, which is trivial in terms of computation.
13Already at (14), we explicitly ruled out translations y
having no derivation at all in the hypergraph. However,
suppose the hypergraph were very large (thanks to a large
or smoothed translation model and weak pruning). Then
(14)?s heuristic would fail to eliminate bad translations (?the
mouse?), since nearly every string y ? ?? would be derived
as a translation with at least a tiny probability. The ?soft? ver-
sion (16) solves this problem, since unlike the ?hard? (14), it
penalizes translations that appear only weakly in the hyper-
graph. As an extreme case, translations not in the hypergraph
at all are infinitely penalized (log pViterbi(y) = log 0 =
??), making it natural for the decoder not to consider them,
i.e., to do only argmaxy?T(x) rather than argmaxy??? .
14One might also want to use the q?n or smoothed versions
of them to rescore additional hypotheses, e.g., hypotheses
proposed by other systems or by system combination.
where l(y, y?) represents the loss of y if the true
answer is y?, and the risk of y is its expected
loss.15 Statistical decision theory shows MBR is
optimal if p(y? | x) is the true distribution, while
in practice p(y? | x) is given by a model at hand.
We now observe that our variational decoding
resembles the MBR decoding of Tromble et al
(2008). They use the following loss function, of
which a linear approximation to BLEU (Papineni
et al, 2001) is a special case,
l(y, y?) = ?(?0|y|+
?
w?N
?wcw(y)?w(y
?)) (18)
where w is an n-gram type, N is a set of n-gram
types with n ? [1, 4], cw(y) is the number of oc-
currence of the n-gram w in y, and ?w(y?) is an
indicator function to check if y? contains at least
one occurrence of w. With the above loss func-
tion, Tromble et al (2008) derive the MBR rule16
y? = argmax
y
(?0|y|+
?
w?N
?wcw(y)g(w | x))
(19)
where g(w | x) is a specialized ?posterior? proba-
bility of the n-gram w, and is defined as
g(w | x) =
?
y?
?w(y
?)p(y? | x) (20)
Now, let us divide N , which contains n-gram
types of different n, into several subsets Wn, each
of which contains only the n-grams with a given
length n. We can now rewrite (19) as follows,
y? = argmax
y
?
n
?n ? gn(y | x) (21)
by assuming ?w = ?|w| and,
gn(y | x)=
{
|y| if n = 0
?
w?Wn g(w | x)cw(y) if n > 0
(22)
Clearly, their rule (21) has a quite similar form
to our rule (15), and we can relate (20) to (12) and
(22) to (11). This justifies the use of interpolation
in Section 3.2.3. However, there are several im-
portant differences. First, the n-gram ?posterior?
of (20) is very expensive to compute. In fact, it re-
quires an intersection between each n-gram in the
lattice and the lattice itself, as is done by Tromble
15The MBR becomes the MAP decision rule of (1) if a so-
called zero-one loss function is used: l(y, y?) = 0 if y = y?;
otherwise l(y, y?) = 1.
16Note that Tromble et al (2008) only consider MBR for a
lattice without hidden structures, though their method can be
in principle applied in a hypergraph with spurious ambiguity.
598
et al (2008). In comparison, the optimal n-gram
probabilities of (12) can be computed using the
inside-outside algorithm, once and for all. Also,
g(w | x) of (20) is not normalized over the history
of w, while q?(r(w) | h(w)) of (12) is. Lastly, the
definition of the n-gram model is different. While
the model (11) is a proper probabilistic model, the
function of (22) is simply an approximation of the
average n-gram precisions of y.
A connection between variational decoding and
minimum-risk decoding has been noted before
(e.g., Matsuzaki et al (2005)), but the derivation
above makes the connection formal.
DeNero et al (2009) concurrently developed
an alternate to MBR, called consensus decoding,
which is similar to ours in practice although moti-
vated quite differently.
5 Experimental Results
We report results using an open source MT toolkit,
called Joshua (Li et al, 2009), which implements
Hiero (Chiang, 2007).
5.1 Experimental Setup
We work on a Chinese to English translation task.
Our translation model was trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora
distributed by LDC for the NIST MT evalua-
tion using a sampling method based on the n-
gram matches between training and test sets in
the foreign side. We also used a 5-gram lan-
guage model with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), trained on a data
set consisting of a 130M words in English Giga-
word (LDC2007T07) and the English side of the
parallel corpora. We use GIZA++ (Och and Ney,
2000), a suffix-array (Lopez, 2007), SRILM (Stol-
cke, 2002), and risk-based deterministic annealing
(Smith and Eisner, 2006)17 to obtain word align-
ments, translation models, language models, and
the optimal weights for combining these models,
respectively. We use standard beam-pruning and
cube-pruning parameter settings, following Chi-
ang (2007), when generating the hypergraphs.
The NIST MT?03 set is used to tune model
weights (e.g. those of (16)) and the scaling factor
17We have also experimented with MERT (Och, 2003), and
found that the deterministic annealing gave results that were
more consistent across runs and often better.
Decoding scheme MT?04 MT?05
Viterbi 35.4 32.6
MBR (K=1000) 35.8 32.7
Crunching (N=10000) 35.7 32.8
Crunching+MBR (N=10000) 35.8 32.7
Variational (1to4gram+wp+vt) 36.6 33.5
Table 1: BLEU scores for Viterbi, Crunching, MBR, and vari-
ational decoding. All the systems improve significantly over
the Viterbi baseline (paired permutation test, p < 0.05). In
each column, we boldface the best result as well as all results
that are statistically indistinguishable from it. In MBR, K is
the number of unique strings. For Crunching and Crunch-
ing+MBR, N represents the number of derivations. On av-
erage, each string has about 115 distinct derivations. The
variational method ?1to4gram+wp+vt? is our full interpola-
tion (16) of four variational n-gram models (?1to4gram?), the
Viterbi baseline (?vt?), and a word penalty feature (?wp?).
? of (3),18 and MT?04 and MT?05 are blind test-
sets. We will report results for lowercase BLEU-4,
using the shortest reference translation in comput-
ing brevity penalty.
5.2 Main Results
Table 1 presents the BLEU scores under Viterbi,
crunching, MBR, and variational decoding. Both
crunching and MBR show slight significant im-
provements over the Viterbi baseline; variational
decoding gives a substantial improvement.
The difference between MBR and Crunch-
ing+MBR lies in how we approximate the distri-
bution p(y? | x) in (17).19 For MBR, we take
p(y? | x) to be proportional to pViterbi(y? | x) if y?
is among the K best distinct strings on that mea-
sure, and 0 otherwise. For Crunching+MBR, we
take p(y? | x) to be proportional to pcrunch(y? | x),
which is based on the N best derivations.
5.3 Results of Different Variational Decoding
Table 2 presents the BLEU results under different
ways in using the variational models, as discussed
in Section 3.2.3. As shown in Table 2a, decod-
ing with a single variational n-gram model (VM)
as per (14) improves the Viterbi baseline (except
the case with a unigram VM), though often not
statistically significant. Moreover, a bigram (i.e.,
?2gram?) achieves the best BLEU scores among
the four different orders of VMs.
The interpolation between a VM and a word
penalty feature (?wp?) improves over the unigram
18We found the BLEU scores are not very sensitive to ?,
contrasting to the observations by Tromble et al (2008).
19We also restrict T(x) to {y : p(y | x) > 0}, using the
same approximation for p(y | x) as we did for p(y? | x).
599
(a) decoding with a single variational model
Decoding scheme MT?04 MT?05
Viterbi 35.4 32.6
1gram 25.9 24.5
2gram 36.1 33.4
3gram 36.0? 33.1
4gram 35.8? 32.9
(b) interpolation between a single variational
model and a word penalty feature
1gram+wp 29.7 27.7
2gram+wp 35.5 32.6
3gram+wp 36.1? 33.1
4gram+wp 35.7? 32.8?
(c) interpolation of a single variational model, the
Viterbi model, and a word penalty feature
1gram+wp+vt 35.6? 32.8?
2gram+wp+vt 36.5? 33.5?
3gram+wp+vt 35.8? 32.9?
4gram+wp+vt 35.6? 32.8?
(d) interpolation of several n-gram VMs, the
Viterbi model, and a word penalty feature
1to2gram+wp+vt 36.6? 33.6?
1to3gram+wp+vt 36.6? 33.5?
1to4gram+wp+vt 36.6? 33.5?
Table 2: BLEU scores under different variational decoders
discussed in Section 3.2.3. A star ? indicates a result that is
significantly better than Viterbi decoding (paired permutation
test, p < 0.05). We boldface the best system and all systems
that are not significantly worse than it. The brevity penalty
BP in BLEU is always 1, meaning that on average y? is no
shorter than the reference translation, except for the ?1gram?
systems in (a), which suffer from brevity penalties of 0.826
and 0.831.
VM dramatically, but does not improve higher-
order VMs (Table 2b). Adding the Viterbi fea-
ture (?vt?) into the interpolation further improves
the lower-order models (Table 2c), and all the im-
provements over the Viterbi baseline become sta-
tistically significant. At last, interpolation of sev-
eral variational models does not yield much fur-
ther improvement over the best previous model,
but makes the results more stable (Table 2d).
5.4 KL Divergence of Approximate Models
While the BLEU scores reported show the prac-
tical utility of the variational models, it is also
interesting to measure how well each individual
variational model q(y) approximates the distribu-
tion p(y | x). Ideally, the quality of approxima-
tion should be measured by the KL divergence
KL(p ? q)
def
= H(p, q) ? H(p), where the cross-
entropy H(p, q)
def
= ?
?
y p(y | x) log q(y), and
Measure H(p, ?) Hd(p) H(p)
bits/word q?1 q
?
2 q
?
3 q
?
4 ?
MT?04 2.33 1.68 1.57 1.53 1.36 1.03
MT?05 2.31 1.69 1.58 1.54 1.37 1.04
Table 3: Cross-entropies H(p, q) achieved by various ap-
proximations q. The notation H denotes the sum of cross-
entropies of all test sentences, divided by the total number
of test words. A perfect approximation would achieve H(p),
which we estimate using the true Hd(p) and a 10000-best list.
the entropy H(p)
def
= ?
?
y p(y | x) log p(y | x).
Unfortunately H(p) (and hence KL = H(p, q) ?
H(p)) is intractable to compute. But, since H(p)
is the same for all q, we can simply use H(p, q)
to compare different models q. Table 3 reports the
cross-entropies H(p, q) for various models q.
We also report the derivational entropy
Hd(p)
def
= ?
?
d p(d | x) log p(d | x).
20 From this,
we obtain an estimate of H(p) by observing that
the ?gap? Hd(p) ? H(p) equals Ep(y)[H(d | y)],
which we estimate from our 10000-best list.
Table 3 confirms that higher-order variational
models (drawn from a larger family Q) approxi-
mate p better. This is necessarily true, but it is
interesting to see that most of the improvement is
obtained just by moving from a unigram to a bi-
gram model. Indeed, although Table 3 shows that
better approximations can be obtained by using
higher-order models, the best BLEU score in Ta-
bles 2a and 2c was obtained by the bigram model.
After all, p cannot perfectly predict the reference
translation anyway, hence may not be worth ap-
proximating closely; but p may do a good job
of predicting bigrams of the reference translation,
and the BLEU score rewards us for those.
6 Conclusions and Future Work
We have successfully applied the general varia-
tional inference framework to a large-scale MT
task, to approximate the intractable problem of
MAP decoding in the presence of spurious am-
biguity. We also showed that interpolating vari-
ational models with the Viterbi approximation can
compensate for poor approximations, and that in-
terpolating them with one another can reduce the
Bayes risk and improve BLEU. Our empirical re-
sults improve the state of the art.
20Both H(p, q) and Hd(p) involve an expectation over ex-
ponentially many derivations, but they can be computed in
time only linear in the size of HG(x) using an expectation
semiring (Eisner, 2002). In particular, H(p, q) can be found
as ?
?
d?D(x) p(d | x) log q(Y(d)).
600
Many interesting research directions remain
open. To approximate the intractable MAP de-
coding problem of (2), we can use different vari-
ational distributions other than the n-gram model
of (11). Interpolation with other models is also
interesting, e.g., the constituent model in Zhang
and Gildea (2008). We might also attempt to min-
imize KL(q ? p) rather than KL(p ? q), in order
to approximate the mode (which may be prefer-
able since we care most about the 1-best transla-
tion under p) rather than the mean of p (Minka,
2005). One could also augment our n-gram mod-
els with non-local string features (Rosenfeld et al,
2001) provided that the expectations of these fea-
tures could be extracted from the hypergraph.
Variational inference can also be exploited to
solve many other intractable problems in MT (e.g.,
word/phrase alignment and system combination).
Finally, our method can be used for tasks beyond
MT. For example, it can be used to approximate
the intractable MAP decoding inherent in systems
using HMMs (e.g. speech recognition). It can also
be used to approximate a context-free grammar
with a finite state automaton (Nederhof, 2005).
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In ACL, pages 40?47.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning. Springer.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on proba-
bilistic grammars and transducers. In ICGI, pages
15?24.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
ACL-IJCNLP.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL, pages 1?8.
Joshua Goodman. 1996. Efficient algorithms for pars-
ing the DOP model. In EMNLP, pages 143?152.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL, pages 144?151.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational meth-
ods for graphical models. In Learning in Graphical
Models. MIT press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL, pages 48?54.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09, pages 135?
139.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL, pages
976?985.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351?358.
Tom Minka. 2005. Divergence measures and message
passing. In Microsoft Research Technical Report
(MSR-TR-2005-173). Microsoft Research.
Mark-Jan Nederhof. 2005. A general technique to
train language models on language models. Com-
put. Linguist., 31(2):173?186.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL, pages 440?
447.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: A vehicle for linguistic-statistical integration.
Computer Speech and Language, 15(1).
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In COLING, pages 1175?1180.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In ACL,
pages 787?794.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In ICSLP, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
EMNLP, pages 620?629.
Richard Zens and Hermann Ney. 2006. N-gram poste-
rior probabilities for statistical machine translation.
In WMT06, pages 72?77.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In ACL, pages 209?217.
601
An Interactive Spreadsheet for Teaching the Forward-Backward Algorithm
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD, USA 21218-2691
jason@cs.jhu.edu
Abstract
This paper offers a detailed lesson plan on the forward-
backward algorithm. The lesson is taught from a live, com-
mented spreadsheet that implements the algorithm and graphs
its behavior on a whimsical toy example. By experimenting
with different inputs, one can help students develop intuitions
about HMMs in particular and Expectation Maximization in
general. The spreadsheet and a coordinated follow-up assign-
ment are available.
1 Why Teach from a Spreadsheet?
Algorithm animations are a wonderful teaching tool.
They are concrete, visual, playful, sometimes inter-
active, and remain available to students after the lec-
ture ends. Unfortunately, they have mainly been lim-
ited to algorithms that manipulate easy-to-draw data
structures.
Numerical algorithms can be ?animated? by
spreadsheets. Although current spreadsheets do not
provide video, they can show ?all at once? how a
computation unfolds over time, displaying interme-
diate results in successive rows of a table and on
graphs. Like the best algorithm animations, they
let the user manipulate the input data to see what
changes. The user can instantly and graphically see
the effect on the whole course of the computation.
Spreadsheets are also transparent. In Figure 1, the
user has double-clicked on a cell to reveal its un-
derlying formula. The other cells that it depends on
are automatically highlighted, with colors keyed to
the references in the formula. There is no program-
ming language to learn: spreadsheet programs are
aimed at the mass market, with an intuitive design
and plenty of online help, and today?s undergrad-
uates already understand their basic operation. An
adventurous student can even experiment with mod-
ifying the formulas, or can instrument the spread-
sheet with additional graphs.
Finally, modern spreadsheet programs such as
Microsoft Excel support visually attractive layouts
with integrated comments, color, fonts, shading, and
Figure 1: User has double-clicked on cell D29.
drawings. This makes them effective for both class-
room presentation and individual study.
This paper describes a lesson plan that was cen-
tered around a live spreadsheet, as well as a subse-
quent programming assignment in which the spread-
sheet served as a debugging aid. The materials are
available for use by others.
Students were especially engaged in class, appar-
ently for the following reasons:
? Striking results (?It learned it!?) that could be im-
mediately apprehended from the graphs.
? Live interactive demo. The students were eager
to guess what the algorithm would do on partic-
ular inputs and test their guesses.
? A whimsical toy example.
? The departure from the usual class routine.
? Novel use of spreadsheets. Several students who
thought of them as mere bookkeeping tools were
awed by this, with one calling it ?the coolest-ass
spreadsheet ever.?
2 How to Teach from a Spreadsheet?
It is possible to teach from a live spreadsheet by us-
ing an RGB projector. The spreadsheet?s zoom fea-
ture can compensate for small type, although under-
graduate eyes prove sharp enough that it may be un-
necessary. (Invite the students to sit near the front.)
Of course, interesting spreadsheets are much too
big to fit on the screen, even with a ?View / Full
Screen? command. But scrolling is easy to follow
if it is not too fast and if the class has previously
                     July 2002, pp. 10-18.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
been given a tour of the overall spreadsheet layout
(by scrolling and/or zooming out). Split-screen fea-
tures such as hide rows/columns, split panes, and
freeze panes can be moderately helpful; so can com-
mands to jump around the spreadsheet, or switch be-
tween two windows that display different areas. It
is a good idea to memorize key sequences for such
commands rather than struggle with mouse menus
or dialog boxes during class.
3 The Subject Matter
Among topics in natural language processing,
the forward-backward or Baum-Welch algorithm
(Baum, 1972) is particularly difficult to teach.
The algorithm estimates the parameters of a
Hidden Markov Model (HMM) by Expectation-
Maximization (EM), using dynamic programming
to carry out the expectation steps efficiently.
HMMs have long been central in speech recog-
nition (Rabiner, 1989). Their application to part-
of-speech tagging (Church, 1988; DeRose, 1988)
kicked off the era of statistical NLP, and they have
found additional NLP applications to phrase chunk-
ing, text segmentation, word-sense disambiguation,
and information extraction.
The algorithm is also important to teach for ped-
agogical reasons, as the entry point to a family of
EM algorithms for unsupervised parameter estima-
tion. Indeed, it is an instructive special case of (1)
the inside-outside algorithm for estimation of prob-
abilistic context-free grammars; (2) belief propa-
gation for training singly-connected Bayesian net-
works and junction trees (Pearl, 1988; Lauritzen,
1995); (3) algorithms for learning alignment mod-
els such as weighted edit distance; (4) general finite-
state parameter estimation (Eisner, 2002).
Before studying the algorithm, students should
first have worked with some if not all of the key
ideas in simpler settings. Markov models can be
introduced through n-gram models or probabilistic
finite-state automata. EM can be introduced through
simpler tasks such as soft clustering. Global opti-
mization through dynamic programming can be in-
troduced in other contexts such as probabilistic CKY
parsing or edit distance. Finally, the students should
understand supervised training and Viterbi decod-
ing of HMMs, for example in the context of part-
of-speech tagging.
Even with such preparation, however, the
forward-backward algorithm can be difficult for be-
ginning students to apprehend. It requires them to
think about all of the above ideas at once, in com-
bination, and to relate them to the nitty-gritty of the
algorithm, namely
? the two-pass computation of mysterious ? and ?
probabilities
? the conversion of these prior path probabilities to
posterior expectations of transition and emission
counts
Just as important, students must develop an under-
standing of the algorithm?s qualitative properties,
which it shares with other EM algorithms:
? performs unsupervised learning (what is this and
why is it possible?)
? alternates expectation and maximization steps
? maximizes p(observed training data) (i.e., total
probability of all hidden paths that generate those
data)
? finds only a local maximum, so is sensitive to
initial conditions
? cannot escape zeroes or symmetries, so they
should be avoided in initial conditions
? uses the states as it sees fit, ignoring the sugges-
tive names that we may give them (e.g., part of
speech tags)
? may overfit the training data unless smoothing is
used
The spreadsheet lesson was deployed in two 50-
minute lectures at Johns Hopkins University, in an
introductory NLP course aimed at upper-level un-
dergraduates and first-year graduate students. A sin-
gle lecture might have sufficed for a less interactive
presentation.
The lesson appeared in week 10 of 13, by which
time the students had already been exposed to most
of the preparatory topics mentioned above, includ-
ing Viterbi decoding of a part-of-speech trigram tag-
ging model. However, the present lesson was their
first exposure to EM or indeed to any kind of unsu-
pervised learning.
Figure 2: Initial guesses of parameters.
Figure 3: Diary data and reconstructed weather.
4 The Ice Cream Climatology Data
[While the spreadsheet could be used in many ways,
the next several sections offer one detailed lesson
plan. Questions for the class are included; subse-
quent points often depend on the answers, which are
concealed here in footnotes. Some fragments of the
full spreadsheet are shown in the figures.]
The situation: You are climatologists in the year
2799, studying the history of global warming. You
can?t find any records of Baltimore weather, but you
do find my diary, in which I assiduously recorded
how much ice cream I ate each day (see Figure 3).
What can you figure out from this about the weather
that summer?
Let?s simplify and suppose there are only two
kinds of days: C (cold) and H (hot). And let?s sup-
pose you have guessed some probabilities as shown
on the spreadsheet (Figure 2).
Thus, you guess that on cold days, I usually ate
only 1 ice cream cone: my probabilities of 1, 2,
or 3 cones were 70%, 20% and 10%. That adds
up to 100%. On hot days, the probabilities were
reversed?I usually ate 3 ice creams. So other things
equal, if you know I ate 3 ice creams, the odds are
7 to 1 that it was a hot day, but if I ate 2 ice creams,
the odds are 1 to 1 (no information).
You also guess (still Figure 2) that if today is cold,
tomorrow is probably cold, and if today is hot, to-
morrow is probably hot. (Q: How does this setup
resemble part-of-speech tagging?1)
We also have some boundary conditions. I only
kept this diary for a while. If I was more likely to
start or stop the diary on a hot day, then that is use-
ful information and it should go in the table. (Q: Is
there an analogy in part-of-speech tagging?2) For
simplicity, let?s guess that I was equally likely to
start or stop on a hot or cold day. So the first day I
started writing was equally likely (50%) to be hot or
cold, and any given day had the same chance (10%)
of being the last recorded day, e.g., because on any
day I wrote (regardless of temperature), I had a 10%
chance of losing my diary.
5 The Trellis and ?? Decoding
[The notation pi(H) in this paper stands for the prob-
ability of H on day i, given all the observed ice cream
data. On the spreadsheet itself the subscript i is
clear from context and is dropped; thus in Figure 3,
p(H) denotes the conditional probability pi(H), not a
prior. The spreadsheet likewise omits subscripts on
?i(H) and ?i(H).]
Scroll down the spreadsheet and look at the lower
line of Figure 3, which shows a weather reconstruc-
tion under the above assumptions. It estimates the
relative hot and cold probabilities for each day. Ap-
parently, the summer was mostly hot with a cold
spell in the middle; we are unsure about the weather
on a few transitional days.
We will now see how the reconstruction was ac-
complished. Look at the trellis diagram on the
spreadsheet (Figure 4). Consistent with visual intu-
ition, arcs (lines) represent days and states (points)
represent the intervening midnights. A cold day is
represented by an arc that ends in a C state.3 (So
1A: This is a bigram tag generation model with tags C and H.
Each tag independently generates a word (1, 2, or 3); the word
choice is conditioned on the tag.
2A: A tagger should know that sentences tend to start with
determiners and end with periods. A tagging that ends with a
determiner should be penalized because p(Stop | Det) ? 0.
3These conventions are a compromise between a traditional
view of HMMs and a finite-state view used elsewhere in the
course. (The two views correspond to Moore vs. Mealy ma-
chines.) In the traditional view, states would represent days and
Figure 4: The ?-? trellis.
each arc effectively inherits the C or H label of its
terminal state.)
Q: According to the trellis, what is the a priori
probability that the first three days of summer are
H,H,C and I eat 2,3,3 cones respectively (as I did)?4
Q: Of the 8 ways to account for the 2,3,3 cones,
which is most probable?5 Q: Why do all 8 paths
have low probabilities?6
Recall that the Viterbi algorithm computes, at
each state of the trellis, the maximum probability
of any path from Start. Similarly, define ? at a
state to be the total probability of all paths to that
state from Start. Q: How would you compute it
by dynamic programming?7 Q: Symmetrically, how
would you compute ? at a state, which is defined to
be the total probability of all paths to Stop?
The ? and ? values are computed on the spread-
sheet (Figure 1). Q: Are there any patterns in the
values?8
Now for some important questions. Q: What is
the total probability of all paths from Start to
would bear emission probabilities such as p(3 | H). In Figure 4,
as in finite-state machines, this role is played by the arcs (which
also carry transition probabilities such as p(H | C)); this allows
? and ? to be described more simply as sums of path probabili-
ties. But we persist in a traditional labeling of the states as H or
C so that the ?? notation can refer to them.
4A: Consult the path Start ? H ? H ? C, which has
probability (0.5?0.2)?(0.8?0.7)?(0.1?0.1) = 0.1?0.56?0.01 =
0.00056. Note that the trellis is specialized to these data.
5A: H,H,H gives probability 0.1 ? 0.56 ? 0.56 = 0.03136.
(Starting with C would be as cheap as starting with H, but then
getting from C to H would be expensive.)
6A: It was a priori unlikely that I?d eat exactly this sequence
of ice creams. (A priori there were many more than 8 possible
paths, but this trellis only shows the paths generating the actual
data 2,3,3.) We?ll be interested in the relative probabilities of
these 8 paths.
7A: In terms of ? at the predecessor states: just replace
?max? with ?+? in the Viterbi algorithm.
8A: ? probabilities decrease going down the column, and ?
probabilities decrease going up, as they become responsible for
generating more and more ice cream data.
Figure 5: Computing expected counts and their totals.
Stop in which day 3 is hot?9 It is shown in col-
umn H of Figure 1. Q: Why is column I of Figure 1
constant at 9.13e-19 across rows?10 Q: What does
that column tell us about ice cream or weather?11
Now the class may be able to see how to complete
the reconstruction:
p(day 3 hot | 2, 3, 3, . . .) = p(day 3 hot,2,3,3,...)p(2,3,3,...)
= ?3(H)??3(H)?3(C)??3(C)+?3(H)??3(H) =
9.0e-19
9.8e-21+9.0e-19
which is 0.989, as shown in cell K29 of Figure 5.
Figure 3 simply graphs column K.
6 Understanding the Reconstruction
Notice that the lower line in Figure 3 has the same
general shape as the upper line (the original data),
but is smoother. For example, some 2-ice-cream
days were tagged as probably cold and some as
probably hot. Q: Why?12 Q: Since the first day has
2 ice creams and doesn?t follow a hot day, why was
it tagged as hot?13 Q: Why was day 11, which has
only 1 ice cream, tagged as hot?14
We can experiment with the spreadsheet (using
the Undo command after each experiment). Q: What
do you predict will happen to Figure 3 if we weaken
9A: By the distributive law, ?3(H) ? ?3(H).
10A: It is the total probability of all paths that go through ei-
ther C or H on a given day. But all paths do that, so this is simply
the total probability of all paths! The choice of day doesn?t mat-
ter.
11A: It is the probability of my actual ice cream consumption:
p(2, 3, 3, . . .) =
?
~w p(~w, 2, 3, 3, . . .) = 9.13e-19, where ~w
ranges over all 233 possible weather state sequences such as
H,H,C,. . . . Each summand is the probability of a trellis path.
12A: Figure 2 assumed a kind of ?weather inertia? in which
a hot day tends to be followed by another hot day, and likewise
for cold days.
13Because an apparently hot day follows it. (See footnote 5.)
It is the ? factors that consider this information from the future,
and make ?1(H) ? ?1(H) ?1(C) ? ?1(C).
14A: Switching from hot to cold and back (HCH) has proba-
bility 0.01, whereas staying hot (HHH) has probability 0.64. So
although the fact that I ate only one ice cream on day 11 favors
C by 7 to 1, the inferred ?fact? that days 10 and 12 are hot favors
H by 64 to 1.
(a)
(b)
Figure 6: With (a) no inertia, and (b) anti-inertia.
or remove the ?weather inertia? in Figure 2?15 Q:
What happens if we try ?anti-inertia??16
Even though the number of ice creams is not de-
cisive (consider day 11), it is influential. Q: What
do you predict will happen if the distribution of ice
creams is the same on hot and cold days?17 Q: What
if we also change p(H | Start) from 0.5 to 0?18
7 Reestimating Emission Probabilities
We originally assumed (Figure 2) that I had a 20%
chance of eating 2 cones on either a hot or a cold
day. But if our reconstruction is right (Figure 3), I
actually ate 2 cones on 20% of cold days but 40+%
of hot days.
15A: Changing p(C | C) = p(H | C) = p(C | H) = p(H |
H) = 0.45 cancels the smoothing effect (Figure 6a). The lower
line now tracks the upper line exactly.
16A: Setting p(C | H) = p(H | C) = 0.8 and p(C | C) =
p(H | H) = 0.1, rather than vice-versa, yields Figure 6b.
17A: The ice cream data now gives us no information about
the weather, so pi(H) = pi(C) = 0.5 on every day i.
18A: p1(H) = 0, but pi(H) increases toward an asymptote
of 0.5 (the ?limit distribution?). The weather is more likely to
switch to hot than to cold if it was more likely cold to begin
with; so pi(H) increases if it is < 0.5.
Figure 7: Parameters of Figure 2 updated by reestimation.
So now that we ?know? which days are hot and
which days are cold, we should really update our
probabilities to 0.2 and 0.4, not 0.2 and 0.2. After
all, our initial probabilities were just guesses.
Q: Where does the learning come from?isn?t this
circular? Since our reconstruction was based on the
guessed probabilities 0.2 and 0.2, why didn?t the re-
construction perfectly reflect those guesses?19
Scrolling rightward on the spreadsheet, we find
a table giving the updated probabilities (Figure 7).
This table feeds into a second copy of the forward-
backward calculation and graph. Q: The second
graph of pi(H) (not shown here) closely resembles
the first; why is it different on days 11 and 27?20
The updated probability table was computed by
the spreadsheet. Q: When it calculated how often I
ate 2 cones on a reconstructed hot day, do you think
it counted day 27 as a hot day or a cold day?21
8 Reestimating Transition Probabilities
Notice that Figure 7 also updated the transition prob-
abilities. This involved counting the 4 kinds of days
distinguished by Figure 8:22 e.g., what fraction of H
19A: The reconstruction of the weather underlying the ob-
served data was a compromise between the guessed probabili-
ties (Figure 2) and the demands of the actual data. The model
in Figure 2 disagreed with the data: it would not have predicted
that 2-cone days actually accounted for more than 20% of all
days, or that they were disproportionately likely to fall between
3-cone days.
20A: These days fall between hot and cold days, so smoothing
has little effect: their temperature is mainly reconstructed from
the number of ice creams. 1 ice cream is now better evidence of
a cold day, and 2 ice creams of a hot day. Interestingly, days 11
and 14 can now conspire to ?cool down? the intervening 3-ice-
cream days.
21A: Half of each, since p27(H) ? 0.5! The actual compu-
tation is performed in Figure 5 and should be discussed at this
point.
22Notice how p(H ? C) and p(C ? H) spike when the
weather changes, on day 14 and either day 27 or 28.
Figure 8: Second-order weather reconstruction.
days were followed by H? Again, fractional counts
were used to handle uncertainty.
Q: Does Figure 3 (first-order reconstruction)
contain enough information to construct Figure 8
(second-order reconstruction)?23
Continuing with the probabilities from the end of
footnote 23, suppose we increase p(H | Start) to
0.7. Q: What will happen to the first-order graph?24
Q: What if we switch from anti-inertia back to iner-
tia (Figure 9)?25
Q: In this last case, what do you predict will hap-
pen when we reestimate the probabilities?26
This reestimation (Figure 10) slightly improved
the reconstruction. [Defer discussion of what ?im-
proved? means: the class still assumes that good re-
constructions look like Figure 3.] Q: Now what?
A: Perhaps we should do it again. And again, and
again. . . Scrolling rightward past 10 successive rees-
timations, we see that this arrives at the intuitively
23A: No. A dramatic way to see this is to make the dis-
tribution of ice cream distribution the same on hot and cold
days. This makes the first-order graph constant at 0.5 as in foot-
note 17. But we can still get a range of behaviors in the second-
order graph; e.g., if we switch from inertia to anti-inertia as in
footnote 16, then we switch from thinking the weather is un-
known but constant to thinking it is unknown but oscillating.
24A: pi(H) alternates and converges to 0.5 from both sides.
25A: pi(H) converges to 0.5 from above (cf. footnote 18), as
shown in Figure 9.
26A: The first-order graph suggests that the early days of
summer were slightly more likely to be hot than cold. Since
we ate more ice cream on those days, the reestimated probabili-
ties (unlike the initial ones) slightly favor eating more ice cream
on hot days. So the new reconstruction based on these proba-
bilities has a very shallow ?U? shape (bottom of Figure 10), in
which the low-ice-cream middle of the summer is slightly less
likely to be hot.
Figure 9: An initial poor reconstruction that will be improved
by reestimation.
correct answer (Figure 11)!
Thus, starting from an uninformed probability ta-
ble, the spreadsheet learned sensible probabilities
(Figure 11) that enabled it to reconstruct the weather.
The 3-D graph shows how the reconstruction im-
proved over time.
The only remaining detail is how the transition
probabilities in Figure 8 were computed. Recall that
to get Figure 3, we asked what fraction of paths
passed through each state. This time we must ask
what fraction of paths traversed each arc. (Q: How
to compute this?27) Just as there were two possible
states each day, there are four possible arcs each day,
and the graph reflects their relative probabilities.
9 Reestimation Experiments
We can check whether the algorithm still learns from
other initial guesses. The following examples appear
on the spreadsheet and can be copied over the table
of initial probabilities. (Except for the pathologi-
cally symmetric third case, they all learn the same
structure.)
1. No weather inertia, but more ice cream on hot
days. The model initially behaves as in foot-
27A: The total probability of all paths traversing q ? r is
?(q) ? p(q ? r) ? ?(r).
Figure 10: The effect of reestimation on Figure 9.
note 15, but over time it learns that weather does
have inertia.
2. Inertia, but only a very slight preference for more
ice cream on hot days. The pi(H) graph is ini-
tially almost as flat as in footnote 17. But over
several passes the model learns that I eat a lot
more ice cream on hot days.
3. A completely symmetric initial state: no inertia,
and no preference at all for more ice cream on hot
days. Q: What do you expect to happen under
reestimation?28
4. Like the previous case, but break the symmetry
by giving cold days a slight preference to eat
more ice cream (Figure 12). This initial state is
almost perfectly symmetric. Q: Why doesn?t this
case appear to learn the same structure as the pre-
vious ones?29
The final case does not converge to quite the same
result as the others: C and H are reversed. (It is
28A: Nothing changes, since the situation is too symmetric.
As H and C behave identically, there is nothing to differentiate
them and allow them to specialize.
29A: Actually it does; it merely requires more iterations to
converge. (The spreadsheet is only wide enough to hold 10 iter-
ations; to run for 10 more, just copy the final probabilities back
over the initial ones. Repeat as necessary.) It learns both inertia
and a preference for more ice cream on cold days.
Figure 11: Nine more passes of forward-backward reestimation
on Figures 9?10. Note that the final graph is even smoother than
Figure 3.
Figure 12: Breaking symmetry toward the opposite solution.
Figure 13: If T is the sequence of 34 training observations
(33 days plus Stop), then p(T ) increases rapidly during rees-
timation. To compress the range of the graph, we don?t plot
p(T ) but rather perplexity per observation = 1/ 34
?
p(T ) =
2?(log2 p(T ))/34.
now H that is used for the low-ice-cream midsummer
days.) Should you care about this difference? As
climatologists, you might very well be upset that the
spreadsheet reversed cold and hot days. But since C
and H are ultimately just arbitrary labels, then per-
haps the outcome is equally good in some sense.
What does it mean for the outcome of this unsuper-
vised learning procedure to be ?good?? The dataset
is just the ice cream diary, which makes no reference
to weather. Without knowing the true weather, how
can we tell whether we did a good job learning it?
10 Local Maximization of Likelihood
The answer: A good model is one that predicts the
dataset as accurately as possible. The dataset actu-
ally has temporal structure, since I tended to have
long periods of high and low ice cream consump-
tion. That structure is what the algorithm discov-
ered, regardless of whether weather was the cause.
The state C or H distinguishes between the two kinds
of periods and tends to persist over time.
So did this learned model predict the dataset well?
It was not always sure about the state sequence,
but Figure 13 shows that the likelihood of the ob-
served dataset (summed over all possible state se-
quences) increased on every iteration. (Q: How is
this found?30)
That behavior is actually guaranteed: repeated
30It is the total probability of paths that explain the data, i.e.,
all paths in Figure 4, as given by column I of Figure 1; see
footnote 10.
forward-backward reestimation converges to a local
maximum of likelihood. We have already discov-
ered two symmetric local maxima, both with per-
plexity of 2.827 per day: the model might use C to
represent cold and H to represent hot, or vice versa.
Q: How much better is 2.827 than a model with no
temporal structure?31
Remember that maximizing the likelihood of the
training data can lead to overfitting. Q: Do you see
any evidence of this in the final probability table?32
Q: Is there a remedy?33
11 A Trick Ending
We get very different results if we slightly mod-
ify Figure 12 by putting p(1 | H) = 0.3 with
p(2 | H) = 0.4. The structure of the solution is
very different (Figure 14). In fact, the final param-
eters now show anti-inertia, giving a reconstruction
similar to Figure 6b. Q: What went wrong?34
In the two previous local maxima, H meant ?low
ice-cream day? or ?high ice-cream day.? Q: Accord-
ing to Figure 14, what does Hmean here?35 Q: What
does the low value of p(H | H) mean?36
So we see that there are actually two kinds of
structure coexisting in this dataset: days with a lot
(little) ice cream tend to repeat, and days with 2 ice
creams tend not to repeat. The first kind of structure
did a better job of lowering the perplexity, but both
31A: A model with no temporal structure is a unigram model.
A good guess is that it will have perplexity 3, since it will be
completely undecided between the 3 kinds of observations. (It
so happens that they were equally frequent in the dataset.) How-
ever, if we prevent the learning of temporal structure (by setting
the initial conditions so that the model is always in state C, or is
always equally likely to be in states C and H), we find that the
perplexity is 3.314, reflecting the four-way unigram distribution
p(1) = p(2) = p(3) = 11/34, p(Stop)=1/34.
32A: p(H | Start) ? 1 because we become increasingly
sure that the training diary started on a hot day. But this single
training observation, no matter how justifiably certain we are of
it, might not generalize to next summer?s diary.
33A: Smoothing the fractional counts. Note: If a prior is used
for smoothing, the algorithm is guaranteed to locally maximize
the posterior (in place of the likelihood).
34A: This is a third local maximum of likelihood, unrelated
to the others, with worse perplexity (3.059). Getting stuck in
poor local maxima is an occupational hazard.
35A: H usually emits 2 ice creams, whereas C never does. So
H stands for a 2-ice-cream day.
36A: That 2 ice creams are rarely followed by 2 ice creams.
Looking at the dataset, this is true. So even this local maximum
successfully discovered some structure: it discovered (to my
surprise) that when I make up data, I tend not to repeat 2?s!
Figure 14: A suboptimal local maximum.
are useful. Q: How could we get our model to dis-
cover both kinds of structure (thereby lowering the
perplexity further)?37
Q: We have now seen three locally optimal mod-
els in which the H state was used for 3 different
things?even though we named it H for ?Hot.? What
does this mean for the application of this algorithm
to part-of-speech tagging?38
12 Follow-Up Assignment
In a follow-up assignment, students applied Viterbi
decoding and forward-backward reestimation to
part-of-speech tagging.39
In the assignment, students were asked to test
their code first on the ice cream data (provided as
a small tagged corpus) before switching to real data.
This cemented the analogy between the ice cream
and tagging tasks, helping students connect the class
to the assignment.
37A: Use more states. Four states would suffice to distinguish
hot/2, cold/2, hot/not2, and cold/not2 days.
38A: There is no guarantee that N and V will continue to dis-
tinguish nouns and verbs after reestimation. They will evolve to
make whatever distinctions help to predict the word sequence.
39Advanced students might also want to read about a mod-
ern supervised trigram tagger (Brants, 2000), or the mixed re-
sults when one actually trains trigram taggers by EM (Merialdo,
1994).
Furthermore, students could check their ice cream
output against the spreadsheet, and track down basic
bugs by comparing their intermediate results to the
spreadsheet?s. They reported this to be very useful.
Presumably it helps learning when students actually
find their bugs before handing in the assignment, and
when they are able to isolate their misconceptions
on their own. It also made office hours and grading
much easier for the teaching assistant.
13 Availability
The spreadsheet (in Microsoft Excel) and assign-
ment are available at http://www.cs.jhu.
edu/?jason/papers/#tnlp02.
Also available is a second version of the spread-
sheet, which uses the Viterbi approximation for de-
coding and reestimation. The Viterbi algorithm is
implemented in an unconventional way so that the
two spreadsheets are almost identical; there is no
need to follow backpointers. The probability of the
best path through state H on day 3 is ?3(H) ? ?3(H),
where ? and ? are computed like ? and ? but maxi-
mizing rather than summing path probabilities. The
Viterbi approximation treats p3(H) as 1 or 0 accord-
ing to whether ?3(H) ? ?3(H) equals max(?3(C) ?
?3(C), ?3(H) ? ?3(H)).
Have fun! Comments are most welcome.
References
L. E. Baum. 1972. An inequality and associated max-
imization technique in statistical estimation of proba-
bilistic functions of a Markov process. Inequalities, 3.
Thorsten Brants. 2000. TnT: A statistical part-of-speech
tagger. In Proc. of ANLP, Seattle.
K. W. Church. 1988. A stochastic parts program and noun
phrase parser for unrestricted text. Proc. of ANLP.
Steven J. DeRose. 1988. Grammatical category disam-
biguation by statistical optimization. Computational
Linguistics, 14(1):31?39.
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proc. of ACL.
Steffen L. Lauritzen. 1995. The EM algorithm for graph-
ical association models with missing data. Computa-
tional Statistics and Data Analysis, 19:191?201.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Comp. Ling., 20(2):155?172.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo, California.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of IEEE, 77(2):257?285, February.
Transformational Priors Over Grammars
Jason Eisner <jason@cs.jhu.edu>
Johns Hopkins University, 3400 N. Charles St., NEB 224, Baltimore, MD
Abstract
This paper proposes a novel class of PCFG parameterizations
that support linguistically reasonable priors over PCFGs. To
estimate the parameters is to discover a notion of relatedness
among context-free rules such that related rules tend to have
related probabilities. The prior favors grammars in which the
relationships are simple to describe and have few major excep-
tions. A basic version that bases relatedness on weighted edit
distance yields superior smoothing of grammars learned from
the Penn Treebank (20% reduction of rule perplexity over the
best previous method).
1 A Sketch of the Concrete Problem
This paper uses a new kind of statistical model to smooth
the probabilities of PCFG rules. It focuses on ?flat? or
?dependency-style? rules. These resemble subcategoriza-
tion frames, but include adjuncts as well as arguments.
The verb put typically generates 3 dependents?a
subject NP at left, and an object NP and goal PP at right:
? S? NP put NP PP: Jim put [the pizza] [in the oven]
But put may also take other dependents, in other rules:
? S? NP Adv put NP PP: Jim often put [a pizza] [in the oven]
? S? NP put NP PP PP: Jim put soup [in an oven] [at home]
? S? NP put NP: Jim put [some shares of IBM stock]
? S? NP put Prt NP: Jim put away [the sauce]
? S? TO put NP PP: to put [the pizza] [in the oven]
? S? NP put NP PP SBAR: Jim put it [to me] [that . . . ]
These other rules arise if put can add, drop, reorder,
or retype its dependents. These edit operations on rules
are semantically motivated and quite common (Table 1).
We wish to learn contextual probabilities for the edit
operations, based on an observed sample of flat rules. In
English we should discover, for example, that it is quite
common to add or delete PP at the right edge of a rule.
These contextual edit probabilities will help us guess the
true probabilities of novel or little-observed rules.
However, rules are often idiosyncratic. Our smooth-
ing method should not keep us from noticing (given
enough evidence) that put takes a PP more often than
most verbs. Hence this paper?s proposal is a Bayesian
smoothing method that allows idiosyncrasy in the gram-
mar while presuming regularity to be more likely a priori.
The model will assign a positive probability to each
of the infinitely many formally possible rules. The fol-
lowing bizarre rule is not observed in training, and seems
very unlikely. But there is no formal reason to rule it out,
and it might help us parse an unlikely test sentence. So
the model will allow it some tiny probability:
? S? NP Adv PP put PP PP PP NP AdjP S
2 Background and Other Approaches
A PCFG is a conditional probability function p(RHS |
LHS).1 For example, p(V NP PP | VP) gives the proba-
bility of the rule VP? V NP PP. With lexicalized non-
terminals, it has form p(Vput NPpizza PPin | VPput).
Usually one makes an independence assumption and
defines this as p(Vput NP PP | VPput) times factors that
choose dependent headwords pizza and in according
to the selectional preferences of put. This paper is about
estimating the first factor, p(Vput NP PP | VPput).
In supervised learning, it is simplest to use a max-
imum likelihood estimate (perhaps with backoff from
put). Charniak (1997) calls this a ?Treebank grammar?
and gambles that assigning 0 probability to rules unseen
in training data will not hurt parsing accuracy too much.
However, there are four reasons not to use a Treebank
grammar. First, ignoring unseen rules necessarily sacri-
fices some accuracy. Second, we will show that it im-
proves accuracy to flatten the parse trees and use flat,
dependency-style rules like p(NP put NP PP | Sput);
this avoids overly strong independence assumptions, but
it increases the number of unseen rules and so makes
Treebank grammars less tenable. Third, backing off from
the word is a crude technique that does not distinguish
among words.2 Fourth, one would eventually like to re-
duce or eliminate supervision, and then generalization is
important to constrain the search to reasonable grammars.
To smooth the distribution p(RHS | LHS), one can de-
fine it in terms of a set of parameters and then estimate
those parameters. Most researchers have used an n-gram
model (Eisner, 1996; Charniak, 2000) or more general
Markov model (Alshawi, 1996) to model the sequence
of nonterminals in the RHS. The sequence Vput NP PP
in our example is then assumed to be emitted by some
Markov model of VPput rules (again with backoff from
put). Collins (1997, model 2) uses a more sophisticated
model in which all arguments in this sequence are gener-
ated jointly, as in a Treebank grammar, and then a Markov
process is used to insert adjuncts among the arguments.
While Treebank models overfit the training data,
Markov models underfit. A simple compromise (novel to
this paper) is a hybrid Treebank/Markov model, which
backs off from a Treebank model to a Markov. Like
this paper?s main proposal, it can learn well-observed id-
iosyncratic rules but generalizes when data are sparse.3
1Nonstandardly, this allows infinitely many rules with p>0.
2One might do better by backing off to word clusters, which
Charniak (1997) did find provided a small benefit.
3Carroll and Rooth (1998) used a similar hybrid technique
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 63-70.
                         Proceedings of the Conference on Empirical Methods in Natural
These models are beaten by our rather different model,
transformational smoothing, which learns common
rules and common edits to them. The comparison is a
direct one, based on the perplexity or cross-entropy of
the trained models on a test set of S? ? ? ? rules.4
A subtlety is that two annotation styles are possible. In
the Penn Treebank, put is the head of three constituents
(V, VP, and S, where underlining denotes a head child)
and joins with different dependents at different levels:
? [S [NP Jim] [VP [V put] [NP pizza] [PP in the oven]]]
In the flattened or dependency version that we prefer,
each word joins with all of its dependents at once:
? [S [NP Jim] put [NP pizza] [PP in the oven]]
A PCFG generating the flat structure must estimate
p(NP put NP PP | Sput). A non-flat PCFG adds
the dependents of put in 3 independent steps, so in ef-
fect it factors the flat rule?s probability into 3 suppos-
edly independent ?subrule probabilities,? p(NP VPput |
Sput) ? p(Vput NP PP | VPput) ? p(put | Vput).
Our evaluation judges the estimates of flat-rule prob-
abilities. Is it better to estimate these directly, or as a
product of estimated subrule probabilities?5 Transforma-
tional smoothing is best applied to the former, so that the
edit operations can freely rearrange all of a word?s depen-
dents. We will see that the Markov and Treebank/Markov
models also work much better this way?a useful finding.
3 The Abstract Problem: Designing Priors
This section outlines the Bayesian approach to learning
probabilistic grammars (for us, estimating a distribution
over flat CFG rules). By choosing among the many
grammars that could have generated the training data, the
learner is choosing how to generalize to novel sentences.
To guide the learner?s choice, one can explicitly spec-
ify a prior probability distribution p(?) over possible
grammars ?, which themselves specify probability dis-
tributions over strings, rules, or trees. A learner should
seek ? that maximizes p(?) ? p(D | ?), where D is the
set of strings, rules, or trees observed by the learner. The
first factor favors regularity (?pick an a priori plausible
grammar?), while the second favors fitting the idiosyn-
crasies of the data, especially the commonest data.6
to evaluate rule distributions that they acquired from an
automatically-parsed treebank.
4All the methods evaluated here apply also to full PCFGs,
but verb-headed rules S ? ? ? ? present the most varied, inter-
esting cases. Many researchers have tried to learn verb subcat-
egorization, though usually not probabilistic subcategorization.
5In testing the latter case, we sum over all possible internal
bracketings of the rule. We do train this case on the true internal
bracketing, but it loses even with this unfair advantage.
6This approach is called semi-Bayesian or Maximum A Pos-
Priors can help both unsupervised and supervised
learning. (In the semi-supervised experiments here, train-
ing data is not raw text but a sparse sample of flat rules.)
Indeed a good deal of syntax induction work has been
carried out in just this framework (Stolcke and Omohun-
dro, 1994; Chen, 1996; De Marcken, 1996; Gru?nwald,
1996; Osborne and Briscoe, 1997). However, all such
work to date has adopted rather simple prior distributions.
Typically, it has defined p(?) to favor PCFGs whose rules
are few, short, nearly equiprobable, and defined over a
small set of nonterminals. Such definitions are conve-
nient, especially when specifying an encoding for MDL,
but since they treat all rules alike, they may not be good
descriptions of linguistic plausibility. For example, they
will never penalize the absence of a predictable rule.
A prior distribution can, however, be used to encode
various kinds of linguistic notions. After all, a prior is
really a soft form of Universal Grammar: it gives the
learner enough prior knowledge of grammar to overcome
Chomsky?s ?poverty of the stimulus? (i.e., sparse data).
? A preference for small or simple grammars, as above.
? Substantive preferences, such as a preference for verbs
to take 2 nominal arguments, or to allow PP adjuncts.
? Preferences for systematicity, such as a preference for
the rules to be consistently head-initial or head-final.
This paper shows how to design a prior that favors a
certain kind of systematicity. Lexicalized grammars for
natural languages are very large?each word specifies a
distribution over all possible dependency rules it could
head?but they tend to have internal structure. The new
prior prefers grammars in which a rule?s probability can
be well-predicted from the probabilities of other rules, us-
ing linguistic transformations such as edit operations.
For example, p(NP Adv w put NP PP | Sw) cor-
relates with p(NP w NP PP | Sw). Both numbers are
high for w = put, medium for w = fund, and low for
w = sleep. The slope of the regression line has to do
with the rate of preverbal Adv-insertion in English.
The correlation is not perfect (some verbs are espe-
cially prone to adverbial modification), which is why we
will only model it with a prior. To just the extent that evi-
dence aboutw is sparse, the prior will cause the learner to
smooth the two probabilities toward the regression line.
4 Patterns Worth Modeling
Before spelling out our approach, let us do a sanity check.
A frame is a flat rule whose headword is replaced with
teriori learning, since it is equivalent to maximizing p(? | D).
It is also equivalent to Minimum Description Length (MDL)
learning, which minimizes the total number of bits `(?)+`(D |
?) needed to encode grammar and data, because one can choose
an encoding scheme where `(x) = ? log2 p(x), or conversely,
define probability distributions by p(x) = 2?`(x).
MI ? ? MI ? ? MI ? ?
9.01 [NP ADJP-PRD] [NP RB ADJP-PRD] 4.76 [TO S] [ S] 5.54 [TO NP PP] [NP TO NP]
8.65 [NP ADJP-PRD] [NP PP-LOC-PRD] 4.17 [TO S] [TO NP PP] 5.25 [TO NP PP] [NP MD NP .]
8.01 [NP ADJP-PRD] [NP NP-PRD] 2.77 [TO S] [TO NP] 4.67 [TO NP PP] [NP MD NP]
7.69 [NP ADJP-PRD] [NP ADJP-PRD .] 6.13 [TO NP] [TO NP SBAR-TMP] 4.62 [TO NP PP] [TO ]
8.49 [NP NP-PRD] [NP NP-PRD .] 5.72 [TO NP] [TO NP PP PP] 3.19 [TO NP PP] [TO NP]
7.91 [NP NP-PRD] [NP ADJP-PRD .] 5.36 [TO NP] [NP MD RB NP] 2.05 [TO NP PP] [ NP]
7.01 [NP NP-PRD] [NP ADJP-PRD] 5.16 [TO NP] [TO NP PP PP-TMP] 5.08 [ NP] [ADVP-TMP NP]
8.45 [NP ADJP-PRD .] [NP PP-LOC-PRD] 5.11 [TO NP] [TO NP ADVP] 4.86 [ NP] [ADVP NP]
8.30 [NP ADJP-PRD .] [NP NP-PRD .] 4.85 [TO NP] [TO NP PP-LOC] 4.53 [ NP] [ NP PP-LOC]
8.04 [NP ADJP-PRD .] [NP NP-PRD] 4.84 [TO NP] [MD NP] 3.50 [ NP] [ NP PP]
7.01 [NP ADJP-PRD .] [NP ADJP-PRD] 4.49 [TO NP] [NP TO NP] 3.17 [ NP] [ S]
7.01 [NP SBAR] [NP SBAR . ?] 4.36 [TO NP] [NP MD S] 2.28 [ NP] [NP NP]
4.75 [NP SBAR] [NP SBAR .] 4.36 [TO NP] [NP TO NP PP] 1.89 [ NP] [NP NP .]
6.94 [NP SBAR .] [? NP SBAR .] 4.26 [TO NP] [NP MD NP PP] 2.56 [NP NP] [NP NP .]
5.94 [NP SBAR .] [NP SBAR . ?] 4.26 [TO NP] [TO NP PP-TMP] 2.20 [NP NP] [ NP]
5.90 [NP SBAR .] [S , NP .] 4.21 [TO NP] [TO PRT NP] 4.89 [NP NP .] [NP ADVP-TMP NP .]
5.82 [NP SBAR .] [NP ADVP SBAR .] 4.20 [TO NP] [NP MD NP] 4.57 [NP NP .] [NP ADVP NP .]
4.68 [NP SBAR .] [ SBAR] 3.99 [TO NP] [TO NP PP] 4.51 [NP NP .] [NP NP PP-TMP]
4.50 [NP SBAR .] [NP SBAR] 3.69 [TO NP] [NP MD NP .] 3.35 [NP NP .] [NP S .]
3.23 [NP SBAR .] [NP S .] 3.60 [TO NP] [TO ] 2.99 [NP NP .] [NP NP]
2.07 [NP SBAR .] [NP ] 3.56 [TO NP] [TO PP] 2.96 [NP NP .] [NP NP PP .]
1.91 [NP SBAR .] [NP NP .] 2.56 [TO NP] [NP NP PP] 2.25 [NP NP .] [ NP PP]
1.63 [NP SBAR .] [NP NP] 2.04 [TO NP] [NP S] 2.20 [NP NP .] [ NP]
4.52 [NP S] [NP S .] 1.99 [TO NP] [NP NP] 4.82 [NP S .] [ S]
4.27 [NP S] [ S] 1.69 [TO NP] [NP NP .] 4.58 [NP S .] [NP S]
3.36 [NP S] [NP ] 1.68 [TO NP] [NP NP PP .] 3.30 [NP S .] [NP ]
2.66 [NP S] [NP NP .] 1.03 [TO NP] [ NP] 2.93 [NP S .] [NP NP .]
2.37 [NP S] [NP NP] 4.75 [S , NP .] [NP SBAR .] 2.28 [NP S .] [NP NP]
Table 1: The most predictive pairs of sentential frames. If S? ? occurs in training data at least 5 times with a given headword in
the position, then S? ? also tends to appear at least once with that headword. MI measures the mutual information of these
two events, computed over all words. When MI is large, as here, the edit distance between ? and ? tends to be strikingly small (1
or 2), and certain linguistically plausible edits are extremely common.
the variable ? ? (corresponding tow above). Table 1 il-
lustrates that in the Penn Treebank, if frequent rules with
frame ? imply matching rules with frame ?, there are
usually edit operations (section 1) to easily turn ? into ?.
How about rare rules, whose probabilities are most in
need of smoothing? Are the same edit transformations
that we can learn from frequent cases (Table 1) appropri-
ate for predicting the rare cases? The very rarity of these
rules makes it impossible to create a table like Table 1.
However, rare rules can be measured in the aggregate,
and the result suggests that the same kinds of transforma-
tions are indeed useful?perhaps even more useful?in
predicting them. Let us consider the set R of 2,809,545
possible flat rules that stand at edit distance 1 from the set
of S ? ? ? ? rules observed in our English training data.
That is, a rule such as Sput ? NP put NP is in R if it
did not appear in training data itself, but could be derived
by a single edit from some rule that did appear.
A bigram Markov model (section 2) was used to iden-
tify 2,714,763 rare rules in R?those that were predicted
to occur with probability < 0.0001 given their head-
words. 79 of these rare rules actually appeared in a
development-data set of 1423 rules. The bigram model
would have expected only 26.2 appearances, given the
lexical headwords in the test data set. The difference is
statistically significant (p < 0.001, bootstrap test).
In other words, the bigram model underpredicts the
edit-distance ?neighbors? of observed rules by a factor
of 3.7 One can therefore hope to use the edit transforma-
tions to improve on the bigram model. For example, the
7Similar results are obtained when we examine just one par-
ticular kind of edit operation, or rules of one particular length.
Delete Y transformation recognizes that if ? ? ? X Y Z ? ? ?
has been observed, then ? ? ? X Z ? ? ? is plausible even if
the bigram X Z has not previously been observed.
Presumably, edit operations are common because they
modify a rule in semantically useful ways, allowing the
filler of a semantic role to be expressed (Insert), sup-
pressed (Delete), retyped (Substitute), or heavy-shifted
(Swap). Such ?valency-affecting operations? have re-
peatedly been invoked by linguists; they are not confined
to English.8 So a learner of an unknown language can
reasonably expect a priori that flat rules related by edit
operations may have related probabilities.
However, which edit operations varies by language.
Each language defines its own weighted, contextual,
asymmetric edit distance. So the learner will have to dis-
cover how likely particular edits are in particular con-
texts. For example, it must learn the rates of prever-
bal Adv-insertion and right-edge PP-insertion. Evidence
about these rates comes mainly from the frequent rules.
5 A Transformation Model
The form of our new model is shown in Figure 1. The
vertices are flat context-free rules, and the arcs between
them represent edit transformations. The set of arcs leav-
8Carpenter (1991) writes that whenever linguists run into the
problem of systematic redundancy in the syntactic lexicon, they
design a scheme in which lexical entries can be derived from
one another by just these operations. We are doing the same
thing. The only twist that the lexical entries (in our case, flat
PCFG rules) have probabilities that must also be derived, so we
will assume that the speaker applies these operations (randomly
from the hearer?s viewpoint) at various rates to be learned.
exp ?1
Z1 exp ?2+?8
Z1To fund NP
exp ?3+?5+?8
Z2
exp ?0
Z4
exp ?0
Z8
exp ?3+?5+?9
Z6
exp ?2+?9
Z5
exp ?1
Z5
0.0002
START
HALT START(merge)
To merge NP
To merge PP NP
To merge NP PP
HALT
To fund NP PP
To fund PP NP
HALT
HALT START(fund)
exp ?7+?8
Z3exp ?0
Z3
0.0011
exp ?0
Z7
exp ?7+?9
Z7
exp ?3+?4
Z2
exp ?0
Z2
exp ?0
Z6
exp ?3+?4
Z6
exp ?6
Z3
exp ?6
Z7
?0 halts ?3 inserts PP ?6 deletes PP ?8 yields To fund NP PP
?1 chooses To NP ?4 inserts PP before NP ?7 moves NP right past PP ?9 yields To merge NP PP
?2 chooses To NP PP ?5 inserts PP before right edge
Figure 1: A fragment of a transformation model. Vertices are possible context-free rules (their left-hand sides, Sfund ? and
Smerge ? , are omitted to avoid visual clutter). Arc probabilities are determined log-linearly, as shown, from a real-valued vector ?
of feature weights. The Z values are chosen so that the arcs leaving each vertex have total probability 1. Dashed arrows represent
arcs not shown here (there are hundreds from each vertex, mainly insertions). Also, not all features are shown (see Table 2).
ing any given vertex has total probability 1. The learner?s
job is to discover the probabilities.
Fortunately, the learner does not have to learn a sep-
arate probability for each of the (infinitely) many arcs,
since many of the arcs represent identical or similar edits.
As shown in Figure 1, an arc?s probability is determined
from meaningful features of the arc, using a conditional
log-linear model of p(arc | source vertex). The learner
only has to learn the finite vector ? of feature weights.
Arcs that represent similar transformations have similar
features, so they tend to have similar probabilities.
This transformation model is really a PCFG with un-
usual parameterization. That is, for any value of ?, it
defines a language-specific probability distribution over
all possible context-free rules (graph vertices). To sam-
ple from this distribution, take a random walk from the
special vertex START to the special vertex HALT. The
rule at the last vertex reached before HALT is the sample.
This sampling procedure models a process where the
speaker chooses an initial rule and edits it repeatedly.
The random walk might reach Sfund ? To fund NP
in two steps and simply halt there. This happens
with probability 0.0011 ? exp ?1Z1 ?
exp ?0
Z2
. Or, having
arrived at Sfund ? To fund NP, it might transform
it into Sfund ? To fund PP NP and then further to
Sfund ? To fund NP PP before halting.
Thus, p?(Sfund ? To fund NP PP) denotes the
probability that the random walk somehow reaches
Sfund ? To fund NP PP and halts there. Condi-
tionalizing this probability gives p?(To NP PP |
Sfund), as needed for the PCFG.9
9The experiments of this paper do not allow transformations
Given ?, it is nontrivial to solve for the probability dis-
tribution over grammar rules e. Let I?(e) denote the flow
to vertex e. This is defined to be the total probability of
all paths from START to e. Equivalently, it is the expected
number of times e would be visited by a random walk
from START. The following recurrence defines p?(e):10
I?(e) = ?e,START +
?
e?I?(e
?) ? p(e? ? e) (1)
p?(e) = I?(e) ? p(e? HALT) (2)
Since solving the large linear system (1) would be pro-
hibitively expensive, in practice we use an approximate
relaxation algorithm (Eisner, 2001) that propagates flow
through the graph until near-convergence. In general this
may underestimate the true probabilities somewhat.
Now consider how the parameter vector ? affects the
distribution over rules, p?(e), in Figure 1:
? By raising the initial weight ?1, one can
increase the flow to Sfund ? To fund NP,
Smerge ? To merge NP, and the like. By equa-
tion (2), this also increases the probability of these rules.
But the effect also feeds through the graph to increase
the flow and probability at those rules? descendants in
the graph, such as Smerge ? To merge NP PP.
So a single parameter ?1 controls a whole complex of
rule probabilities (roughly speaking, the infinitival transi-
tives). The model thereby captures the fact that, although
that change the LHS or headword of a rule, so it is trivial to find
the divisor p?(Sfund): in Figure 1 it is 0.0011. But in general,
LHS-changing transformations can be useful (Eisner, 2001).
10Where ?x,y = 1 if x = y, else ?x,y = 0.
rules are mutually exclusive events whose probabilities
sum to 1, transformationally related rules have positively
correlated probabilities that rise and fall together.
? The exception weight ?9 appears on all and only the
arcs to Smerge ? To merge NP PP. That rule has
even higher probability than predicted by PP-insertion as
above (since merge, unlike fund, actually tends to sub-
categorize for PPwith). To model its idiosyncratic prob-
ability, one can raise ?9. This ?lists? the rule specially
in the grammar. Rules derived from it also increase in
probability (e.g., Smerge ? To Adv merge NP PP),
since again the effect feeds through the graph.
? The generalization weight ?3 models the strength of
the PP-insertion relationship. Equations (1) and (2) im-
ply that p?(Sfund ? To fund NP PP) is modeled as
a linear combination of the probabilities of that rule?s
parents in the graph. ?3 controls the coefficient of
p?(Sfund ? To fund NP) in this linear combination,
with the coefficient approaching zero as ?3 ? ??.
? Narrower generalization weights such as ?4 and ?5
control where PP is likely to be inserted. To learn the
feature weights is to learn which features of a transfor-
mation make it probable or improbable in the language.
Note that the vertex labels, graph topology, and arc
parameters are language independent. That is, Figure 1
is supposed to represent Universal Grammar: it tells a
learner what kinds of generalizations to look for. The
language-specific part is ?, which specifies which gener-
alizations and exceptions help to model the data.
6 The Prior
The model has more parameters than data. Why? Beyond
the initial weights and generalization weights, in practice
we allow one exception weight (e.g., ?8, ?9) for each rule
that appeared in training data. (This makes it possible to
learn arbitrary exceptions, as in a Treebank grammar.)
Parameter estimation is nonetheless possible, using a
prior to help choose among the many values of ? that do
a reasonable job of explaining the training data. The prior
constrains the degrees of freedom: while many parame-
ters are available in principle, the prior will ensure that
the data are described using as few of them as possible.
The point of reparameterizing a PCFG in terms of ?,
as in Figure 1, is precisely that only one parameter is
needed per linguistically salient property of the PCFG.
Making ?3 > 0 creates a broadly targeted transforma-
tion. Making ?9 6= 0 or ?1 6= 0 lists an idiosyncratic rule,
or class of rules, together with other rules derived from
them. But it takes more parameters to encode less sys-
tematic properties, such as narrowly targeted edit trans-
formations (?4, ?5) or families of unrelated exceptions.
A natural prior for the parameter vector ? ? Rk is
therefore specified in terms of a variance ?2. We simply
say that the weights ?1, ?2, . . . ?k are independent sam-
ples from the normal distribution with mean 0 and vari-
ance ?2 > 0 (Chen and Rosenfeld, 1999):
? ? N(0, ?2)?N(0, ?2)? ? ? ? ?N(0, ?2) (3)
or equivalently, that ? is drawn from a multivariate Gaus-
sian with mean ~0 and diagonal covariance matrix ?2I ,
i.e., ? ? N(~0, ?2I).
This says that a priori, the learner expects most fea-
tures in Figure 1 to have weights close to zero, i.e., to be
irrelevant. Maximizing p(?) ? p(D | ?) means finding
a relatively small set of features that adequately describe
the rules and exceptions of the grammar. Reducing the
variance ?2 strengthens this bias toward simplicity.
For example, if Sfund ? To fund NP PP and
Smerge ? To fund NP PP are both observed more
often than the current p? distribution predicts, then the
learner can follow either (or both) of two strategies: raise
?8 and ?9, or raise ?3. The former strategy fits the training
data only; the latter affects many disparate arcs and leads
to generalization. The latter strategy may harm p(D | ?)
but is preferred by the prior p(?) because it uses one pa-
rameter instead of two. If more than two words act like
merge and fund, the pressure to generalize is stronger.
7 Perturbation Parameters
In experiments, we have found that a slight variation on
this model gets slightly better results. Let ?e denote the
exception weight (if any) that allows one to tune the prob-
ability of rule e. We eliminate ?e and introduce a different
parameter pie, called a perturbation, which is used in the
following replacements for equations (1) and (2):
I?(e) = ?e,START +
?
e?
I?(e
?) ? exppie ? p(e
? ? e)(4)
p?(e) = I?(e) ? exppie ? p(e? HALT)/Z (5)
where Z is a global normalizing factor chosen so that
?
e p?(e) = 1. The new prior on pie is the same as the
old prior on ?e.
Increasing either ?e or pie will raise p?(e); the learner
may do this to account for observations of e in training
data. The probabilities of other rules consequently de-
crease so that
?
e p?(e) = 1. When pie is raised, all
rules? probabilities are scaled down slightly and equally
(because Z increases). When ?e is raised, e steals proba-
bility from its siblings,11 but these are similar to e so tend
to appear in test data if e is in training data. Raising ?e
without disproportionately harming e?s siblings requires
manipulation of many other parameters, which is discour-
aged by the prior and may also suffer from search error.
We speculate that this is why pie works better.
11Raising the probability of an arc from e? to e decreases the
probabilities of arcs from e? to siblings of e, as they sum to 1.
(Insert) (Insert, target)
(Insert, left) (Insert, target, left)
(Insert, right) (Insert, target, right)
(Insert, left, right)
(Insert, side) (Insert, side, target)
(Insert, side, left) (Insert, side, target, left)
(Insert, side, right) (Insert, side, target, right)
(Insert, side, left, right)
If the arc inserts
Adv after TO
in TO fund PP,
then
target=Adv
left=TO
right=??
side=left of head
Table 2: Each Insert arc has 14 features. The features of any
given arc are found by instantiating the tuples above, as shown.
Each instantiated tuple has a weight specified in ?.
S? ? ? ? rules only train dev test
Treebank sections 0?15 16 17
sentences 15554 1343 866
rule tokens 18836 1588 973
rule types 11565 1317 795
frame types 2722 564 365
headword types 3607 756 504
novel rule tokens 51.6% 47.8%
novel frame tokens 8.9% 6.3%
novel headword tokens 10.4% 10.2%
novel rule types 61.4% 57.5%
novel frame types 24.6% 16.4%
novel headword types 20.9% 18.8%
nonterminal types 78
# transformations applicable to 158n?1 158n?1 158n?1
rule with RHS length = n
Table 3: Properties of the experimental data. ?Novel? means
not observed in training. ?Frame? was defined in section 4.
8 Evaluation12
To evaluate the quality of generalization, we used pre-
parsed training data D and testing data E (Table 3).
Each dataset consisted of a collection of flat rules such as
Sput ? NP put NP PP extracted from the Penn Tree-
bank (Marcus et al, 1993). Thus, p(D | ?,pi) and
p(E | ?,pi) were each defined as a product of rule prob-
abilities of the form p?,pi(NP put NP PP | Sput).
The learner attempted to maximize p(?,pi) ? p(D |
?,pi) by gradient ascent. This amounts to learning the
generalizations and exceptions that related the training
rules D. The evaluation measure was then the perplex-
ity on test data, ? log2 p(E | ?,pi)/|E| . To get a good
(low) perplexity score, the model had to assign reason-
able probabilities to the many novel rules in E (Table 3).
For many of these rules, even the frame was novel.
Note that although the training data was preparsed into
rules, it was not annotated with the paths in Figure 1 that
generated those rules, so estimating ? and pi was still an
unsupervised learning problem.
The transformation graph had about 14 features per arc
(Table 2). In the finite part of the transformation graph
that was actually explored (including bad arcs that com-
pete with good ones), about 70000 distinct features were
encountered, though after training, only a few hundred
12See (Eisner, 2001) for full details of data preparation,
model structure, parameter initialization, backoff levels for the
comparison models, efficient techniques for computing the ob-
jective and its gradient, and more analysis of the results.
Treebank/Markov
basic Katz one-counta
flat non-flatb flat flat non-flat
(a) Treebank ? ?
1-gram 1774.9 86435.1 340.9 160.0 193.2
2-gram 135.2 199.3 127.2 116.2 174.7
3-gram 136.5 177.4 132.7 123.3 174.8
Collinsc 363.0 494.5 197.9
transformation 108.6
averagedd 102.3
(b) 1-gram 1991.2 96318.8 455.1 194.3 233.1
2-gram 162.2 236.6 153.2 138.8 205.6
3-gram 161.9 211.0 156.8 145.7 208.1
Collins 414.5 589.4 242.0
transformation 124.8
averaged 118.0
aBack off from Treebank grammar with Katz vs. one-count
backoff (Chen and Goodman, 1996) (Note: One-count was al-
ways used for backoff within the n-gram and Collins models.)
bSee section 2 for discussion
cCollins (1997, model 2)
dAverage of transformation model with best other model
Table 4: Perplexity of the test set under various models. (a) Full
training set. (b) Half training set (sections 0?7 only).
feature weights were substantial, and only a few thousand
were even far enough from zero to affect performance.
There was also a parameter pie for each observed rule e.
Results are given in Table 4a, which compares the
transformation model to various competing models dis-
cussed in section 2. The best (smallest) perplexities ap-
pear in boldface. The key results:
? The transformation model was the winner, reducing
perplexity by 20% over the best model replicated from
previous literature (a bigram model).
? Much of this improvement could be explained by
the transformation model?s ability to model exceptions.
Adding this ability more directly to the bigram model,
using the new Treebank/Markov approach of section 2,
also reduced perplexity from the bigram model, by 6%
or 14% depending on whether Katz or one-count backoff
was used, versus the transformation model?s 20%.
? Averaging the transformation model with the best com-
peting model (Treebank/bigram) improved it by an addi-
tional 6%. So using transformations yields a total per-
plexity reduction of 12% over Treebank/bigram, and 24%
over the best previous model from the literature (bigram).
? What would be the cost of achieving such a perplexity
improvement by additional annotation? Training the av-
eraged model on only the first half of the training set, with
no further tuning of any options (Table 4b), yielded a test
set perplexity of 118.0. So by using transformations, we
can achieve about the same perplexity as the best model
without transformations (Treebank/bigram, 116.2), using
only half as much training data.
? Furthermore, comparing Tables 4a and 4b shows that
the transformation model had the most graceful perfor-
mance degradation when the dataset was reduced in size.
1e?10 1e?07 1e?04 1e?01
1e
?1
0
1e
?0
7
1e
?0
4
1e
?0
1
p(r
ule
 | h
ead
wo
rd):
 av
era
ged
 tra
nsf
.
5e?04 5e?03 5e?02 5e?01
5e
?0
4
5e
?0
3
5e
?0
2
5e
?0
1
p(rule | headword): Treebank/bigram
0.001 0.010 0.100 1.000
0.0
01
0.0
10
0.1
00
1.0
00
Figure 2: Probabilities of test set flat rules under the averaged model, plotted against the corresponding probabilities under the
best transformation-free model. Improvements fall above the main diagonal; dashed diagonals indicate a factor of two. The three
log-log plots (at different scales!) partition the rules by the number of training observations: 0 (left graph), 1 (middle), ? 2 (right).
This is an encouraging result for the use of the method
in less supervised contexts (although results on a noisy
dataset would be more convincing in this regard).
? The competing models from the literature are best used
to predict flat rules directly, rather than by summing over
their possible non-flat internal structures, as has been
done in the past. This result is significant in itself. Ex-
tending Johnson (1998), it shows the inappropriateness of
the traditional independence assumptions that build up a
frame by several rule expansions (section 2).
Figure 2 shows that averaging the transformation
model with the Treebank/bigram model improves the lat-
ter not merely on balance, but across the board. In other
words, there is no evident class of phenomena for which
incorporating transformations would be a bad idea.
? Transformations particularly helped raise the estimates
of the low-probability novel rules in test data, as hoped.
? Transformations also helped on test rules that had
been observed once in training with relatively infrequent
words. (In other words, the transformation model does
not discount singletons too much.)
? Transformations hurt slightly on balance for rules ob-
served more than once in training, but the effect was tiny.
All these differences are slightly exaggerated if one com-
pares the transformation model directly with the Tree-
bank/bigram model, without averaging.
The transformation model was designed to use edit
operations in order to generalize appropriately from a
word?s observed frames to new frames that are likely to
appear with that word in test data. To directly test the
model?s success at such generalization, we compared it
to the bigram model on a pseudo-disambiguation task.
Each instance of the task consisted of a pair of rules
from test data, expressed as (word, frame) pairs (w1, f1)
and (w2, f2), such that f1 and f2 are ?novel? frames that
did not appear in training data (with any headword).
Each model was then asked: Does f1 go with w1 and
f2 with w2, or vice-versa? In other words, which is big-
ger, p(f1 | w1) ? p(f2 | w2) or p(f2 | w1) ? p(f1 | w2)?
Since the frames were novel, the model had to make
the choice according to whether f1 or f2 looked more
like the frames that had actually been observed with w1
in the past, and likewise w2. What this means depends
on the model. The bigram model takes two frames to
look alike if they contain many bigrams in common. The
transformation model takes two frames to look alike if
they are connected by a path of probable transformations.
The test data contained 62 distinct rules (w, f) in
which f was a novel frame. This yielded 62?612 = 1891
pairs of rules, leading to 1811 task instances after obvi-
ous ties were discarded.13
Baseline performance on this difficult task is 50% (ran-
dom guess). The bigram model chose correctly in 1595
of the 1811 instances (88.1%). Parameters for ?memo-
rizing? specific frames do not help on this task, which in-
volves only novel frames, so the Treebank/bigram model
had the same performance. By contrast, the transforma-
tion model got 1669 of 1811 correct (92.2%), for a more-
than-34% reduction in error rate. (The development set
showed similar results.) However, since the 1811 task
instances were derived non-independently from just 62
novel rules, this result is based on a rather small sample.
9 Discussion
This paper has presented a nontrivial way to reparameter-
ize a PCFG in terms of ?deep? parameters representing
transformations and exceptions. A linguistically sensible
prior was natural to define over these deep parameters.
Famous examples of ?deep reparameterization? are the
Fourier transform in speech recognition and the SVD
transform for Latent Semantic Analysis in IR. Like our
technique, they are intended to reveal significant structure
through the leading parameters while relegating noise and
exceptions to minor parameters. Such representations
13An obvious tie is an instance where f1 = f2, or where
both w1 and w2 were novel headwords. (The 62 rules included
11 with novel headwords.) In such cases, neither the bigram nor
the transformation model has any basis for making its decision:
the probabilities being compared will necessarily be equal.
make it easier to model the similarity or probability of the
objects at hand (waveforms, documents, or grammars).
Beyond the fact that it shows at least a good perplex-
ity improvement (it has not yet been applied to a real
task), an exciting ?big idea? aspect of this work is its
flexibility in defining linguistically sensible priors over
grammars. Our reparameterization is made with refer-
ence to a user-designed transformation graph (Figure 1).
The graph need not be confined to edit distance transfor-
mations, or to the simple features of Table 2 (used here
for comparability with the Markov models), which con-
dition a transformation?s probability on local context.
In principle, the approach could be used to capture
a great many linguistic phenomena. Figure 1 could be
extended with more ambitious transformations, such as
gapping, gap-threading, and passivization. The flat rules
could be annotated with internal structure (as in TAG) and
thematic roles. Finally, the arcs could bear further fea-
tures. For example, the probability of unaccusative move-
ment (someone sank the boat? the boat sank) should de-
pend on whether the headword is a change-of-state verb.
Indeed, Figure 1 can be converted to any lexicalized
theory of grammar, such as categorial grammar, TAG,
LFG, HPSG, or Minimalism. The vertices represent lex-
ical entries and the arcs represent probabilistic lexical re-
dundancy rules or metarules (see footnote 8). The trans-
formation model approach is therefore a full stochas-
tic treatment of lexicalized syntax? apparently the first
to treat lexical redundancy rules, although (Briscoe and
Copestake, 1999) give an ad hoc approach. See (Eisner,
2001; Eisner, 2002a) for more discussion.
It is worthwhile to compare the statistical approach
here with some other approaches:
? Transformation models are similar to graphical mod-
els: they allow similar patterns of deductive and abduc-
tive inference from observations. However, the vertices
of a transformation graph do not represent different ran-
dom variables, but rather mutually exclusive values of the
same random variable, whose probabilities sum to 1.
? Transformation models incorporate conditional log-
linear (maximum entropy) models. As an alternative,
one could directly build a conditional log-linear model
of p(RHS | LHS). However, such a model would learn
probabilities, not relationships. A feature weight would
not really model the strength of the relationship between
two frames e, e? that share that feature. It would only in-
fluence both frames? probabilities. If the probability of e
were altered by some unrelated factor (e.g., an exception
weight), then the probability of e? would not respond.
? A transformation model can be regarded as a proba-
bilistic FSA that consists mostly of -transitions. (Rules
are only emitted on the arcs to HALT.) This perspective
allows use of generic methods for finite-state parameter
estimation (Eisner, 2002b). We are strongly interested in
improving the speed of such methods and their ability to
avoid local maxima, which are currently the major diffi-
culty with our system, as they are for many unsupervised
learning techniques. We expect to further pursue trans-
formation models (and simpler variants that are easier to
estimate) within this flexible finite-state framework.
The interested reader is encouraged to look at (Eisner,
2001) for a much more careful and wide-ranging discus-
sion of transformation models, their algorithms, and their
relation to linguistic theory, statistics, and parsing. Chap-
ter 1 provides a good overview. For a brief article high-
lighting the connection to linguistics, see (Eisner, 2002a).
References
Hiyan Alshawi. 1996. Head automata for speech translation.
In Proceedings of ICSLP, Philadelphia, PA.
T. Briscoe and A. Copestake. 1999. Lexical rules in constraint-
based grammar. Computational Linguistics, 25(4):487?526.
Bob Carpenter. 1991. The generative power of categorial gram-
mars and head-driven phrase structure grammars with lexical
rules. Computational Linguistics, 17(3):301?313.
Glenn Carroll and Mats Rooth. 1998. Valence induction with a
head-lexicalized PCFG. In Proceedings of EMNLP.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In Proc. of AAAI, 598?603.
Eugene Charniak. 2000. A maximum-entropy inspired parser.
In Proceedings of NAACL.
Stanley Chen and Joshua Goodman. 1996. An empirical study
of smoothing techniques. In Proceedings of ACL.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical Report
CMU-CS-99-108, Carnegie Mellon University, February.
Stanley Chen. 1996. Building Probabilistic Models for Natural
Language. Ph.D. thesis, Harvard University.
Michael J. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL/EACL, 16?23.
Carl De Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, MIT.
Jason Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. Proc. of COLING, 340?345.
Jason Eisner. 2001. Smoothing a Probabilistic Lexicon via Syn-
tactic Transformations. Ph.D. thesis, Univ. of Pennsylvania.
Jason Eisner. 2002a. Discovering syntactic deep structure via
Bayesian statistics. Cognitive Science, 26(3), May.
Jason Eisner. 2002b. Parameter estimation for probabilistic
finite-state transducers. In Proceedings of the 40th ACL.
P. Gru?nwald. 1996. A minimum description length approach
to grammar inference. In S. Wermter et al, eds., Symbolic,
Connectionist and Statistical Approaches to Learning for
NLP, no. 1040 in Lecture Notes in AI, pages 203?216.
Mark Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24(4):613?632.
Beth Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313?330.
Miles Osborne and Ted Briscoe. 1997. Learning stochastic cat-
egorial grammars. In Proceedings of CoNLL, 80?87. ACL.
A. Stolcke and S.M. Omohundro. 1994. Inducing probabilistic
grammars by Bayesian model merging. In Proc. of ICGI.
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 30?41,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Parsing with Soft and Hard Constraints on Dependency Length?
Jason Eisner and Noah A. Smith
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{jason,nasmith}@cs.jhu.edu
Abstract
In lexicalized phrase-structure or dependency parses, a word?s
modifiers tend to fall near it in the string. We show that a crude
way to use dependency length as a parsing feature can sub-
stantially improve parsing speed and accuracy in English and
Chinese, with more mixed results on German. We then show
similar improvements by imposing hard bounds on dependency
length and (additionally) modeling the resulting sequence of
parse fragments. This simple ?vine grammar? formalism has
only finite-state power, but a context-free parameterization with
some extra parameters for stringing fragments together. We ex-
hibit a linear-time chart parsing algorithm with a low grammar
constant.
1 Introduction
Many modern parsers identify the head word of
each constituent they find. This makes it possible
to identify the word-to-word dependencies implicit
in a parse.1 (Some parsers, known as dependency
parsers, even return these dependencies as their pri-
mary output.)
Why bother to identify these dependencies? The
typical reason is to model the fact that some word
pairs are more likely than others to engage in a de-
pendency relationship.2 In this paper, we propose a
different reason to identify dependencies in candi-
date parses: to evaluate not the dependency?s word
pair but its length (i.e., the string distance between
the two words). Dependency lengths differ from
? This work was supported by NSF ITR grant IIS-0313193
to the first author and a fellowship from the Fannie and John
Hertz Foundation to the second author. The views expressed
are not necessarily endorsed by the sponsors. The authors thank
Mark Johnson, Eugene Charniak, Charles Schafer, Keith Hall,
and John Hale for helpful discussion and Elliott Dra?bek and
Markus Dreyer for insights on (respectively) Chinese and Ger-
man parsing. They also thank an anonymous reviewer for sug-
gesting the German experiments.
1In a phrase-structure parse, if phrase X headed by word
token x is a subconstituent of phrase Y headed by word token
y 6= x, then x is said to depend on y. In a more powerful
compositional formalism like LTAG or CCG, dependencies can
be extracted from the derivation tree.
2It has recently been questioned whether these ?bilexical?
features actually contribute much to parsing performance (Klein
and Manning, 2003; Bikel, 2004), at least when one has only a
million words of training.
typical parsing features in that they cannot be deter-
mined from tree-local information. Though lengths
are not usually considered, we will see that bilexical
dynamic-programming parsing algorithms can eas-
ily consider them as they build the parse.
Soft constraints. Like any other feature of trees,
dependency lengths can be explicitly used as fea-
tures in a probability model that chooses among
trees. Such a model will tend to disfavor long de-
pendencies (at least of some kinds), as these are em-
pirically rare. In the first part of the paper, we show
that such features improve a simple baseline depen-
dency parser.
Hard constraints. If the bias against long de-
pendencies is strengthened into a hard constraint
that absolutely prohibits long dependencies, then the
parser turns into a partial parser with only finite-state
power. In the second part of the paper, we show how
to perform chart parsing in asymptotic linear time
with a low grammar constant. Such a partial parser
does less work than a full parser in practice, and in
many cases recovers a more precise set of dependen-
cies (with little loss in recall).
2 Short Dependencies in Langugage
We assume that correct parses exhibit a ?short-
dependency preference?: a word?s dependents tend
to be close to it in the string.3 If the j th word of a sen-
tence depends on the ith word, then |i?j| tends to be
3 In this paper, we consider only a crude notion of ?close-
ness?: the number of intervening words. Other distance mea-
sures could be substituted or added (following the literature on
heavy-shift and sentence comprehension), including the phono-
logical, morphological, syntactic, or referential (given/new)
complexity of the intervening material (Gibson, 1998). In pars-
ing, the most relevant previous work is due to Collins (1997),
who considered three binary features of the intervening mate-
rial: did it contain (a) any word tokens at all, (b) any verbs,
(c) any commas or colons? Note that (b) is effective because
it measures the length of a dependency in terms of the number
of alternative attachment sites that the dependent skipped over,
a notion that could be generalized. Similarly, McDonald et al
(2005) separately considered each of the intervening POS tags.
30
small. This implies that neither i nor j is modified by
complex phrases that fall between i and j. In terms
of phrase structure, it implies that the phrases mod-
ifying word i from a given side tend to be (1) few
in number, (2) ordered so that the longer phrases fall
farther from i, and (3) internally structured so that
the bulk of each phrase falls on the side of j away
from i.
These principles can be blamed for several lin-
guistic phenomena. (1) helps explain the ?late clo-
sure? or ?attach low? heuristic (e.g., Frazier, 1979;
Hobbs and Bear, 1990): a modifier such as a PP is
more likely to attach to the closest appropriate head.
(2) helps account for heavy-shift: when an NP is
long and complex, take NP out, put NP on the ta-
ble, and give NP to Mary are likely to be rephrased
as take out NP, put on the table NP, and give Mary
NP. (3) explains certain non-canonical word orders:
in English, a noun?s left modifier must become a
right modifier if and only if it is right-heavy (a taller
politician vs. a politician taller than all her rivals4),
and a verb?s left modifier may extrapose its right-
heavy portion (An aardvark walked in who had cir-
cumnavigated the globe5).
Why should sentences prefer short dependencies?
Such sentences may be easier for humans to produce
and comprehend. Each word can quickly ?discharge
its responsibilities,? emitting or finding all its depen-
dents soon after it is uttered or heard; then it can
be dropped from working memory (Church, 1980;
Gibson, 1998). Such sentences also succumb nicely
to disambiguation heuristics that assume short de-
pendencies, such as low attachment. Thus, to im-
prove comprehensibility, a speaker can make stylis-
tic choices that shorten dependencies (e.g., heavy-
shift), and a language can categorically prohibit
some structures that lead to long dependencies (*a
taller-than-all-her-rivals politician; *the sentence
4Whereas *a politician taller and *a taller-than-all-her-
rivals politician are not allowed. The phenomenon is pervasive.
5This actually splits the heavy left dependent [an aardvark
who ...] into two non-adjacent pieces, moving the heavy second
piece. By slightly stretching the aardvark-who dependency in
this way, it greatly shortens aardvark-walked. The same is pos-
sible for heavy, non-final right dependents: I met an aardvark
yesterday who had circumnavigated the globe again stretches
aardvark-who, which greatly shortens met-yesterday. These ex-
amples illustrate (3) and (2) respectively. However, the resulting
non-contiguous constituents lead to non-projective parses that
are beyond the scope of this paper.
that another sentence that had center-embedding
was inside was incomprehensible).
Such functionalist pressures are not all-powerful.
For example, many languages use SOV basic word
order where SVO (or OVS) would give shorter de-
pendencies. However, where the data exhibit some
short-dependency preference, computer parsers as
well as human parsers can obtain speed and accu-
racy benefits by exploiting that fact.
3 Soft Constraints on Dependency Length
We now enhance simple baseline probabilistic
parsers for English, Chinese, and German so that
they consider dependency lengths. We confine our-
selves (throughout the paper) to parsing part-of-
speech (POS) tag sequences. This allows us to ig-
nore data sparseness, out-of-vocabulary, smoothing,
and pruning issues, but it means that our accuracy
measures are not state-of-the-art. Our techniques
could be straightforwardly adapted to (bi)lexicalized
parsers on actual word sequences, though not neces-
sarily with the same success.
3.1 Grammar Formalism
Throughout this paper we will use split bilexical
grammars, or SBGs (Eisner, 2000), a notationally
simpler variant of split head-automaton grammars,
or SHAGs (Eisner and Satta, 1999). The formalism
is context-free. We define here a probabilistic ver-
sion,6 which we use for the baseline models in our
experiments. They are only baselines because the
SBG generative process does not take note of de-
pendency length.
An SBG is an tuple G = (?, $, L,R). ? is an
alphabet of words. (In our experiments, we parse
only POS tag sequences, so ? is actually an alpha-
bet of tags.) $ 6? ? is a distinguished root symbol;
let ?? = ? ? {$}. L and R are functions from ??
to probabilistic ?-free finite-state automata over ?.
Thus, for each w ? ??, the SBG specifies ?left? and
?right? probabilistic FSAs, Lw and Rw.
We use Lw(G) : ??? ? [0, 1] to denote the prob-
abilistic context-free language of phrases headed by
w. Lw(G) is defined by the following simple top-
down stochastic process for sampling from it:
6There is a straightforward generalization to weighted
SBGs, which need not have a stochastic generative model.
31
1. Sample from the finite-state language L(Lw) a
sequence ? = w?1w?2 . . . w?` ? ?? of left
children, and from L(Rw) a sequence ? =
w1w2 . . . wr ? ?? of right children. Each se-
quence is found by a random walk on its proba-
bilistic FSA. We say the children depend on w.
2. For each i from ?` to r with i 6= 0, recursively
sample ?i ? ?? from the context-free language
Lwi(G). It is this step that indirectly determines
dependency lengths.
3. Return ??` . . . ??2??1w?1?2 . . . ?r ? ???, a
concatenation of strings.
Notice that w?s left children ? were generated in
reverse order, so w?1 and w1 are its closest children
while w?` and wr are the farthest.
Given an input sentence ? = w1w2 . . . wn ? ??,
a parser attempts to recover the highest-probability
derivation by which $? could have been generated
from L$(G). Thus, $ plays the role of w0. A sample
derivation is shown in Fig. 1a. Typically, L$ and
R$ are defined so that $ must have no left children
(` = 0) and at most one right child (r ? 1), the
latter serving as the conventional root of the parse.
3.2 Baseline Models
In the experiments reported here, we defined only
very simple automata for Lw and Rw (w ? ?).
However, we tried three automaton types, of vary-
ing quality, so as to evaluate the benefit of adding
length-sensitivity at three different levels of baseline
performance.
In model A (the worst), each automaton has topol-
ogy }  ff, with a single state q1, so token w?s left
dependents are conditionally independent of one an-
other given w. In model C (the best), each au-
tomaton }??}  ff has an extra state q0 that al-
lows the first (closest) dependent to be chosen dif-
ferently from the rest. Model B is a compromise:7
it is like model A, but each type w ? ? may
have an elevated or reduced probability of having
no dependents at all. This is accomplished by us-
ing automata }??}  ff as in model C, which al-
lows the stopping probabilities p(STOP | q0) and
p(STOP | q1) to differ, but tying the conditional dis-
7It is equivalent to the ?dependency model with valence? of
Klein and Manning (2004).
tributions p(q0 w??q1 | q0,?STOP) and p(q1 w??q1 |
q1,?STOP).
Finally, in ?3, L$ and R$ are restricted as above,
so R$ gives a probability distribution over ? only.
3.3 Length-Sensitive Models
None of the baseline models A?C explicitly model
the distance between a head and child. We enhanced
them by multiplying in some extra length-sensitive
factors when computing a tree?s probability. For
each dependency, an extra factor p(? | . . .) is mul-
tiplied in for the probability of the dependency?s
length ? = |i ? j|, where i and j are the positions
of the head and child in the surface string.8
Again we tried three variants. In one version, this
new probability p(?| . . .) is conditioned only on the
direction d = sign(i ? j) of the dependency. In
another version, it is conditioned only on the POS
tag h of the head. In a third version, it is conditioned
on d, h, and the POS tag c of the child.
3.4 Parsing Algorithm
Fig. 2a gives a variant of Eisner and Satta?s (1999)
SHAG parsing algorithm, adapted to SBGs, which
are easier to understand.9 (We will modify this al-
gorithm later in ?4.) The algorithm obtains O(n3)
runtime, despite the need to track the position of
head words, by exploiting the conditional indepen-
dence between a head?s left children and right chil-
dren. It builds ?half-constituents? denoted by @
(a head word together with some modifying phrases
on the right, i.e., w?1 . . . ?r) and   (a head word
together with some modifying phrases on the left,
i.e., ??` . . . ??1w). A new dependency is intro-
duced when @ +   are combined to get H
or
 (a pair of linked head words with all the
intervening phrases, i.e., w?1 . . . ?r???`? . . . ???1w?,
where w is respectively the parent or child of w?).
One can then combine H + @ = @ , or
8Since the ? values are fully determined by the tree but ev-
ery p(? | . . .) ? 1, this crude procedure simply reduces the
probability mass of every legal tree. The resulting model is de-
ficient (does not sum to 1); the remaining probability mass goes
to impossible trees whose putative dependency lengths ? are
inconsistent with the tree structure. We intend in future work
to explore non-deficient models (log-linear or generative), but
even the present crude approach helps.
9The SHAG notation was designed to highlight the connec-
tion to non-split HAGs.
32
  +  =   . Only O(n3) combinations
are possible in total when parsing a length-n sen-
tence.
3.5 A Note on Word Senses
[This section may be skipped by the casual reader.]
A remark is necessary about :w and :w? in Fig. 2a,
which represent senses of the words at positions
h and h?. Like past algorithms for SBGs (Eisner,
2000), Fig. 2a is designed to be a bit more general
and integrate sense disambiguation into parsing. It
formally runs on an input ? = W1 . . .Wn ? ??,
where each Wi ? ? is a ?confusion set? over pos-
sible values of the ith word wi. The algorithm re-
covers the highest-probability derivation that gener-
ates $? for some ? ? ? (i.e., ? = w1 . . . wn with
(?i)wi ?Wi).
This extra level of generality is not needed for any
of our experiments, but it is needed for SBG parsers
to be as flexible as SHAG parsers. We include it in
this paper to broaden the applicability of both Fig. 2a
and our extension of it in ?4.
The ?senses? can be used in an SBG to pass a
finite amount of information between the left and
right children of a word, just as SHAGs allow.10 For
example, to model the fronting of a direct object, an
SBG might use a special sense of a verb, whose au-
tomata tend to generate both one more noun in ? and
one fewer noun in ?.
Senses can also be used to pass information be-
tween parents and children. Important uses are
to encode lexical senses, or to enrich the de-
pendency parse with constituent labels or depen-
10Fig. 2a enhances the Eisner-Satta version with explicit
senses while matching its asymptotic performance. On this
point, see (Eisner and Satta, 1999, ?8 and footnote 6). How-
ever, it does have a practical slowdown, in that START-LEFT
nondeterministically guesses every possible sense of Wi, and
these senses are pursued separately. To match the Eisner-Satta
algorithm, we should not need to commit to a word?s sense un-
til we have seen all its left children. That is, left triangles and
left trapezoids should not carry a sense :w at all, except for the
completed left triangle (marked F) that is produced by FINISH-
LEFT. FINISH-LEFT should choose a sense w of Wh accord-
ing to the final state q, which reflects knowledge of Wh?s left
children. For this strategy to work, the transitions in Lw (used
by ATTACH-LEFT) must not depend on the particular sense w
but only on W . In other words, all Lw : w ? Wh are really
copies of a shared LWh , except that they may have different fi-
nal states. This requirement involves no loss of generality, since
the nondeterministic shared LWh is free to branch as soon as it
likes onto paths that commit to the various senses w.
dency labels (Eisner, 2000). For example, the in-
put token Wi = {bank1/N/NP , bank2/N/NP ,
bank3/V/VP , bank3/V/S} ? ? allows four
?senses? of bank, namely two nominal meanings,
and two syntactically different versions of the verbal
meaning, whose automata require them to expand
into VP and S phrases respectively.
The cubic runtime is proportional to the num-
ber of ways of instantiating the inference rules in
Fig. 2a: O(n2(n + t?)tg2), where n = |?| is the
input length, g = maxni=1 |Wi| bounds the size of
a confusion set, t bounds the number of states per
automaton, and t? ? t bounds the number of au-
tomaton transitions from a state that emit the same
word. For deterministic automata, t? = 1.11
3.6 Probabilistic Parsing
It is easy to make the algorithm of Fig. 2a length-
sensitive. When a new dependency is added by an
ATTACH rule that combines @ +   , the an-
notations on @ and   suffice to determine
the dependency?s length ? = |h ? h?|, direction
d = sign(h ? h?), head word w, and child word
w?.12 So the additional cost of such a dependency,
e.g. p(? | d,w,w?), can be included as the weight
of an extra antecedent to the rule, and so included in
the weight of the resulting  or H .
To execute the inference rules in Fig. 2a, we
use a prioritized agenda. Derived items such as
@ ,   ,

, and H are prioritized by
their Viterbi-inside probabilities. This is known
as uniform-cost search or shortest-hyperpath search
(Nederhof, 2003). We halt as soon as a full parse
(the accept item) pops from the agenda, since
uniform-cost search (as a special case of the A?
algorithm) guarantees this to be the maximum-
probability parse. No other pruning is done.
11Confusion-set parsing may be regarded as parsing a par-
ticular lattice with n states and ng arcs. The algorithm can
be generalized to lattice parsing, in which case it has runtime
O(m2(n + t?)t) for a lattice of n states and m arcs. Roughly,
h : w is replaced by an arc, while i is replaced by a state and
i? 1 is replaced by the same state.
12For general lattice parsing, it is not possible to determine ?
while applying this rule. There h and h? are arcs in the lattice,
not integers, and different paths from h to h? might cover dif-
ferent numbers of words. Thus, if one still wanted to measure
dependency length in words (rather than in, say, milliseconds
of speech), each item would have to record its width explicitly,
leading in general to more items and increased runtime.
33
With a prioritized agenda, a probability model
that more sharply discriminates among parses will
typically lead to a faster parser. (Low-probability
constituents languish at the back of the agenda and
are never pursued.) We will see that the length-
sensitive models do run faster for this reason.
3.7 Experiments with Soft Constraints
We trained models A?C, using unsmoothed maxi-
mum likelihood estimation, on three treebanks: the
Penn (English) Treebank (split in the standard way,
?2?21 train/?23 test, or 950K/57K words), the Penn
Chinese Treebank (80% train/10% test or 508K/55K
words), and the German TIGER corpus (80%/10%
or 539K/68K words).13 Estimation was a simple
matter of counting automaton events and normaliz-
ing counts into probabilities. For each model, we
also trained the three length-sensitive versions de-
scribed in ?3.3.
The German corpus contains non-projective trees.
None of our parsers can recover non-projective de-
pendencies (nor can our models produce them). This
fact was ignored when counting events for maxi-
mum likelihood estimation: in particular, we always
trained Lw and Rw on the sequence of w?s immedi-
ate children, even in non-projective trees.
Our results (Tab. 1) show that sharpening the
probabilities with the most sophisticated distance
factors p(? | d, h, c), consistently improved the
speed of all parsers.14 The change to the code is
trivial. The only overhead is the cost of looking up
and multiplying in the extra distance factors.
Accuracy also improved over the baseline mod-
els of English and Chinese, as well as the simpler
baseline models of German. Again, the most so-
phisticated distance factors helped most, but even
the simplest distance factor usually obtained most
of the accuracy benefit.
German model C fell slightly in accuracy. The
speedup here suggests that the probabilities were
sharpened, but often in favor of the wrong parses.
We did not analyze the errors on German; it may
13Heads were extracted for English using Michael Collins?
rules and Chinese using Fei Xia?s rules (defaulting in both cases
to right-most heads where the rules fail). German heads were
extracted using the TIGER Java API; we discarded all resulting
dependency structures that were cyclic or unconnected (6%).
14We measure speed abstractly by the number of items built
and pushed on the agenda.
be relevant that 25% of the German sentences con-
tained a non-projective dependency between non-
punctuation tokens.
Studying the parser output for English, we found
that the length-sensitive models preferred closer at-
tachments, with 19.7% of tags having a nearer parent
in the best parse under model C with p(? | d, h, c)
than in the original model C, 77.7% having a par-
ent at the same distance, and only 2.5% having a
farther parent. The surviving long dependencies (at
any length > 1) tended to be much more accurate,
while the (now more numerous) length-1 dependen-
cies were slightly less accurate than before.
We caution that length sensitivity?s most dramatic
improvements to accuracy were on the worse base-
line models, which had more room to improve. The
better baseline models (B and C) were already able
to indirectly capture some preference for short de-
pendencies, by learning that some parts of speech
were unlikely to have multiple left or multiple right
dependents. Enhancing B and C therefore con-
tributed less, and indeed may have had some harmful
effect by over-penalizing some structures that were
already appropriately penalized.15 It remains to
be seen, therefore, whether distance features would
help state-of-the art parsers that are already much
better than model C. Such parsers may already in-
corporate features that indirectly impose a good
model of distance, though perhaps not as cheaply.
4 Hard Dependency-Length Constraints
We have seen how an explicit model of distance can
improve the speed and accuracy of a simple proba-
bilistic dependency parser. Another way to capital-
ize on the fact that most dependencies are local is
to impose a hard constraint that simply forbids long
dependencies.
The dependency trees that satisfy this constraint
yield a regular string language.16 The constraint pre-
vents arbitrarily deep center-embedding, as well as
arbitrarily many direct dependents on a given head,
15Owing to our deficient model. A log-linear or discrimina-
tive model would be trained to correct for overlapping penalties
and would avoid this risk. Non-deficient generative models are
also possible to design, along lines similar to footnote 16.
16One proof is to construct a strongly equivalent CFG without
center-embedding (Nederhof, 2000). Each nonterminal has the
form ?w, q, i, j?, where w ? ?, q is a state of Lw or Rw, and
i, j ? {0, 1, . . . k?1,? k}. We leave the details as an exercise.
34
English (Penn Treebank) Chinese (Chinese Treebank) German (TIGER Corpus)
recall (%) runtime model recall (%) runtime model recall (%) runtime model
model train test test size train test test size train test test size
A (1 state) 62.0 62.2 93.6 1,878 50.7 49.3 146.7 782 70.9 72.0 53.4 1,598
+ p(? | d) 70.1 70.6 97.0 2,032 59.0 58.0 161.9 1,037 72.3 73.0 53.2 1,763
+ p(? | h) 70.5 71.0 94.7 3,091 60.5 59.1 148.3 1,759 73.1 74.0 48.3 2,575
+ p(? | d, h, c) 72.8 73.1 70.4 16,305 62.2 60.6 106.7 7,828 75.0 75.1 31.6 12,325
B (2 states, tied arcs) 69.7 70.4 93.5 2,106 56.7 56.2 151.4 928 73.7 75.1 52.9 1,845
+ p(? | d) 72.6 73.2 95.3 2,260 60.2 59.5 156.9 1,183 72.9 73.9 52.6 2,010
+ p(? | h) 73.1 73.7 92.1 3,319 61.6 60.7 144.2 1,905 74.1 75.3 47.6 2,822
+ p(? | d, h, c) 75.3 75.6 67.7 16,533 62.9 61.6 104.0 7,974 75.2 75.5 31.5 12,572
C (2 states) 72.7 73.1 90.3 3,233 61.8 61.0 148.3 1,314 75.6 76.9 48.5 2,638
+ p(? | d) 73.9 74.5 91.7 3,387 61.5 60.6 154.7 1,569 74.3 75.0 48.9 2,803
+ p(? | h) 74.3 75.0 88.6 4,446 63.1 61.9 141.9 2,291 75.2 76.3 44.3 3,615
+ p(? | d, h, c) 75.3 75.5 66.6 17,660 63.4 61.8 103.4 8,360 75.1 75.2 31.0 13,365
Table 1: Dependency parsing of POS tag sequences with simple probabilistic split bilexical grammars. The models differ only
in how they weight the same candidate parse trees. Length-sensitive models are larger but can improve dependency accuracy
and speed. (Recall is measured as the fraction of non-punctuation tags whose correct parent (if not the $ symbol) was correctly
recovered by the parser; it equals precision, unless the parser left some sentences unparsed (or incompletely parsed, as in ?4), in
which case precision is higher. Runtime is measured abstractly as the average number of items (i.e., @ ,   ,  , H )
built per word. Model size is measured as the number of nonzero parameters.)
either of which would allow the non-regular lan-
guage {anbcn : 0 < n < ?}. It does allow ar-
bitrarily deep right- or left-branching structures.
4.1 Vine Grammars
The tighter the bound on dependency length, the
fewer parse trees we allow and the faster we can find
them using the algorithm of Fig. 2a. If the bound
is too tight to allow the correct parse of some sen-
tence, we would still like to allow an accurate partial
parse: a sequence of accurate parse fragments (Hin-
dle, 1990; Abney, 1991; Appelt et al, 1993; Chen,
1995; Grefenstette, 1996). Furthermore, we would
like to use the fact that some fragment sequences are
presumably more likely than others.
Our partial parses will look like the one in Fig. 1b.
where 4 subtrees rather than 1 are dependent on $.
This is easy to arrange in the SBG formalism. We
merely need to construct our SBG so that the au-
tomaton R$ is now permitted to generate multiple
children?the roots of parse fragments.
This R$ is a probabilistic finite-state automaton
that describes legal or likely root sequences in ??.
In our experiments in this section, we will train it
to be a first-order (bigram) Markov model. (Thus
we construct R$ in the usual way to have |?| + 1
states, and train it on data like the other left and right
automata. During generation, its state remembers
the previously generated root, if any. Recall that we
are working with POS tag sequences, so the roots,
like all other words, are tags in ?.)
The 4 subtrees in Fig. 1b appear as so many
bunches of grapes hanging off a vine. We refer to
the dotted dependencies upon $ as vine dependen-
cies, and the remaining, bilexical dependencies as
tree dependencies.
One might informally use the term ?vine gram-
mar? (VG) for any generative formalism, intended
for partial parsing, in which a parse is a constrained
sequence of trees that cover the sentence. In gen-
eral, a VG might use a two-part generative process:
first generate a finite-state sequence of roots, then
expand the roots according to some more powerful
formalism. Conveniently, however, SBGs and other
dependency grammars can integrate these two steps
into a single formalism.
4.2 Feasible Parsing
Now, for both speed and accuracy, we will restrict
the trees that may hang from the vine. We define a
feasible parse under our SBG to be one in which all
tree dependencies are short, i.e., their length never
exceeds some hard bound k. The vine dependencies
may have unbounded length, of course, as in Fig. 1b.
Sentences with feasible parses form a regular lan-
guage. This would also be true under other defini-
tions of feasibility, e.g., we could have limited the
depth or width of each tree on the vine. However,
that would have ruled out deeply right-branching
trees, which are very common in language, and
35
(a) $ would
``````````````````
``````````````````
```
aaaaaaaaa
aaaaaaaaa
aa
eee
ee YYY
YY
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
According
YYY
YY
, changes
bbbbb
bbbbb
eee
ee
cut
\\\\\
\\\\\
]]]]]]]
]]]]]]]
]
to
\\\\\
\\\\\
the rule filings
eee
ee
by
YYY
YY
.
estimates
eee
ee
insider more
YYY
YY
some than
\\\\\
\\\\\
third
eee
ee
a
(b) $ According
WW
WW
, would
gg
gg WW
WW
to
[[[[
[[[[
changes
cccc
cccc
gg
gg
cut
[[[[
[[[[
\\\\\
\\\\\
\
estimates
gg
gg
the rule filings
gg
gg
by
WW
WW
.
some insider more
WW
WW
than
[[[[
[[[[
third
gg
gg
a
Figure 1: (a) A dependency tree on words. (Our experiments use only POS tags.) (b) A partial
parse for the same sentence retaining only tree dependencies of length ? k = 3. The roots of the
4 resulting parse fragments are now connected only by their dotted-line ?vine dependencies? on $.
Transforming (a) into (b) involves grafting subtrees rooted at ?According?, ?,?, and ?.? onto the vine.
are also the traditional way to describe finite-state
sublanguages within a context-free grammar. By
contrast, our limitation on dependency length en-
sures regularity while still allowing (for any bound
k ? 1) arbitrarily wide and deep trees, such as
a? b? . . .? root? . . .? y ? z.
Our goal is to find the best feasible parse (if
any). Rather than transform the grammar as in foot-
note 16, our strategy is to modify the parser so that it
only considers feasible parses. The interesting prob-
lem is to achieve linear-time parsing with a grammar
constant that is as small as for ordinary parsing.
We also correspondingly modify the training data
so that we only train on feasible parses. That is, we
break any long dependencies and thereby fragment
each training parse (a single tree) into a vine of one
or more restricted trees. When we break a child-
to-parent dependency, we reattach the child to $.17
This process, grafting, is illustrated in Fig. 1. Al-
though this new parse may score less than 100% re-
call of the original dependencies, it is the best feasi-
ble parse, so we would like to train the parser to find
it.18 By training on the modified data, we learn more
17Any dependency covering the child must also be broken to
preserve projectivity. This case arises later; see footnote 25.
18Although the parser will still not be able to find it if it is
non-projective (possible in German). Arguably we should have
defined ?feasible? to also require projectivity, but we did not.
appropriate statistics for both R$ and the other au-
tomata. If we trained on the original trees, we would
inaptly learn that R$ always generates a single root
rather than a certain kind of sequence of roots.
For evaluation, we score tree dependencies in our
feasible parses against the tree dependencies in the
unmodified gold standard parses, which are not nec-
essarily feasible. We also show oracle performance.
4.3 Approach #1: FSA Parsing
Since we are now dealing with a regular language,
it is possible in principle to use a weighted finite-
state automaton (FSA) to search for the best feasible
parse. The idea is to find the highest-weighted path
that accepts the input string ? = w1w2 . . . wn. Us-
ing the Viterbi algorithm, this takes time O(n).
The trouble is that this linear runtime hides a con-
stant factor, which depends on the size of the rele-
vant part of the FSA and may be enormous for any
correct FSA.19
Consider an example from Fig 1b. Af-
ter nondeterministically reading w1 . . . w11 =
According. . . insider along the correct path, the FSA
state must record (at least) that insider has no parent
yet and that R$ and Rcut are in particular states that
19The full runtime is O(nE), where E is the number of FSA
edges, or for a tighter estimate, the number of FSA edges that
can be traversed by reading ?.
36
may still accept more children. Else the FSA cannot
know whether to accept a continuation w12 . . . wn.
In general, after parsing a prefix w1 . . . wj , the
FSA state must somehow record information about
all incompletely linked words in the past. It must
record the sequence of past words wi (i ? j) that
still need a parent or child in the future; if wi still
needs a child, it must also record the state of Rwi .
Our restriction to dependency length ? k is what
allows us to build a finite-state machine (as opposed
to some kind of pushdown automaton with an un-
bounded number of configurations). We need only
build the finitely many states where the incompletely
linked words are limited to at most w0 = $ and the k
most recent words, wj?k+1 . . . wj . Other states can-
not extend into a feasible parse, and can be pruned.
However, this still allows the FSA to be in
O(2ktk+1) different states after reading w1 . . . wj .
Then the runtime of the Viterbi algorithm, though
linear in n, is exponential in k.
4.4 Approach #2: Ordinary Chart Parsing
A much better idea for most purposes is to use a
chart parser. This allows the usual dynamic pro-
gramming techniques for reusing computation. (The
FSA in the previous section failed to exploit many
such opportunities: exponentially many states would
have proceeded redundantly by building the same
wj+1wj+2wj+3 constituent.)
It is simple to restrict our algorithm of Fig. 2a to
find only feasible parses. It is the ATTACH rules
@ +   that add dependencies: simply use a
side condition to block them from applying unless
|h? h?| ? k (short tree dependency) or h = 0 (vine
dependency). This ensures that all H and 
will have width ? k or have their left edge at 0.
One might now incorrectly expect runtime linear
in n: the number of possible ATTACH combinations
is reduced from O(n3) to O(nk2), because i and h?
are now restricted to a narrow range given h.
Unfortunately, the half-constituents @ and
  may still be arbitrarily wide, thanks to arbi-
trary right- and left-branching: a feasible vine parse
may be a sequence of wide trees   @ . Thus there
are O(n2k) possible COMPLETE combinations, not
to mention O(n2) ATTACH-RIGHT combinations
for which h = 0. So the runtime remains quadratic.
4.5 Approach #3: Specialized Chart Parsing
How, then, do we get linear runtime and a rea-
sonable grammar constant? We give two ways to
achieve runtime of O(nk2).
First, we observe without details that we can eas-
ily achieve this by starting instead with the algo-
rithm of Eisner (2000),20 rather than Eisner and
Satta (1999), and again refusing to add long tree de-
pendencies. That algorithm effectively concatenates
only trapezoids, not triangles. Each is spanned by a
single dependency and so has width ? k. The vine
dependencies do lead to wide trapezoids, but these
are constrained to start at 0, where $ is. So the algo-
rithm tries at most O(nk2) combinations of the form
h i+ i j (like the ATTACH combinations above)
and O(nk) combinations of the form 0 i + i j,
where i? h ? k, j ? i ? k. The precise runtime is
O(nk(k + t?)tg3).
We now propose a hybrid linear-time algorithm
that further improves runtime to O(nk(k + t?)tg2),
saving a factor of g in the grammar constant.21 We
observe that since within-tree dependencies must
have length ? k, they can all be captured within
Eisner-Satta trapezoids of width ? k. So our VG
parse   @ ? can be assembled by simply concate-
nating a sequence (    ? H ? @ )? of these
narrow trapezoids interspersed with width-0 trian-
gles. As this is a regular sequence, we can assem-
ble it in linear time from left to right (rather than in
the order of Eisner and Satta (1999)), multiplying
the items? probabilities together. Whenever we start
adding the right half H ? @ of a tree along the
vine, we have discovered that tree?s root, so we mul-
tiply in the probability of a $? root dependency.
Formally, our hybrid parsing algorithm restricts
the original rules of Fig. 2a to build only trapezoids
of width ? k and triangles of width < k.22 The
additional inference rules in Fig. 2b then assemble
the final VG parse as just described.
20With a small change that when two items are combined, the
right item (rather than the left) must be simple.
21This savings comes from building the internal structure of
a trapezoid from both ends inward rather than from left to right.
The corresponding unrestricted algorithms (Eisner, 2000; Eis-
ner and Satta, 1999, respectively) have exactly the same run-
times with k replaced by n.
22For the experiments of ?4.7, where k varied by type, we
restricted these rules as tightly as possible given h and h?.
37
(a)
ST
A
R
T-
LE
FT
:
w
?
W
h
q
?
in
it(
L
w
)
  
q
h
h
:w
1
?
h
?
n
ST
A
R
T-
R
IG
H
T:
q
?
in
it(
R
w
)
@@q
h
:w
h
  
F
i
h
:w
ST
A
R
T-
V
IN
E:
q
?
in
it(
R
$)
@@q
0
:$
0
FI
N
IS
H
-
LE
FT
:
  
q
i
h
:w
q
?
fin
al(
L
w
)
  
F
i
h
:w
FI
N
IS
H
-
R
IG
H
T:
@@q
h
:w
i
q
?
fin
al(
R
w
)
@@F
h
:w
i
EN
D
-
V
IN
E:
@@F
0
:$
n
ac
ce
pt
AT
TA
C
H
-
LE
FT
:
? ?
@@F
h
? :
w
?
i?
1
  
q
i
h
:w
? ?
q
w
?
??
r
?
L
w

r
h
? :
w
?
h
:w
AT
TA
C
H
-
R
IG
H
T:
? ?
@@q
h
:w
i?
1
  
F
i
h
? :
w
?? ?
q
w
?
??
r
?
R
w
HHr
h
? :
w
?
h
:w
C
O
M
PL
E
T
E
-
LE
FT
:
  
F
i
h
? :
w
?

q
h
? :
w
?
h
:w
  
q
i
h
:w
C
O
M
PL
E
T
E
-
R
IG
H
T:
HHq
h
? :
w
?
h
:w
@@F
h
? :
w
?
i
@@q
h
:w
i
Fi
gu
re
2:
(a)
A
n
al
go
rit
hm
th
at
pa
rs
es
W
1
..
.W
n
in
cu
-
bi
ct
im
eO
(n
2
(n
+
t?
)t
g2
).
A
da
pt
ed
w
ith
im
pr
ov
e-
m
en
ts
fro
m
(E
isn
er
an
d
Sa
tta
,1
99
9,
Fi
g.
3).
Th
e
pa
re
nt
he
se
s
in
th
e
AT
TA
C
H
ru
le
s
in
di
ca
te
th
e
de
-
du
ct
io
n
o
fa
n
in
te
rm
ed
ia
te
ite
m
th
at
?
fo
rg
et
s?
i.
(b)
If
th
e
AT
TA
C
H
ru
le
sa
re
re
st
ric
te
d
to
ap
pl
y
o
n
ly
w
he
n
ca
se
|h
?
h
? |
?
k
,
an
d
th
e
C
O
M
PL
E
T
E
ru
le
s
o
n
ly
w
he
n
|h
?
i|
<
k
,
th
en
th
e
ad
di
tio
na
l
ru
le
s
in
(b)
w
ill
as
se
m
bl
e
th
e
re
su
lti
ng
fra
gm
en
ts
in
to
a
v
in
e
pa
rs
e.
In
th
is
ca
se
,
AT
TA
C
H
-
R
IG
H
T
sh
ou
ld
al
so
be
re
st
ric
te
d
to
h
>
0,
to
pr
ev
en
t
du
pl
ic
at
e
de
riv
at
io
ns
.
Th
e
ru
n
tim
e
is
O
(n
k
(k
+
t?
)t
g2
),
do
m
in
at
ed
by
th
e
AT
TA
C
H
ru
le
s;
th
e
ru
le
s
in
(b)
re
qu
ire
o
n
ly
O
(n
k
tg
2
+
n
gt
t?
)
tim
e.
Ea
ch
al
go
rit
hm
is
sp
ec
ifi
ed
as
a
co
lle
ct
io
n
o
f
de
du
ct
iv
e
in
fe
re
nc
er
u
le
s.
O
nc
eo
n
e
ha
sd
er
iv
ed
al
l
an
te
ce
de
nt
ite
m
sa
bo
v
e
th
eh
or
iz
on
ta
ll
in
ea
n
d
an
y
sid
ec
o
n
di
tio
ns
to
th
er
ig
ht
o
ft
he
lin
e,
o
n
e
m
ay
de
-
riv
e
th
e
co
n
se
qu
en
ti
te
m
be
lo
w
th
e
lin
e.
W
ei
gh
te
d
ag
en
da
-b
as
ed
de
du
ct
io
n
is
ha
nd
le
d
in
th
e
u
su
al
w
ay
(N
ed
erh
of,
20
03
;E
isn
er
et
al
.,
20
05
).
Th
ep
ro
ba
bi
lit
ie
sg
ov
er
n
in
g
th
ea
u
to
m
at
on
L
w
,
n
am
el
y
p(
st
ar
ta
tq
),
p(
q
w
?
?
?
r
|
q)
,
an
d
p(
st
op
|
q)
,
ar
e
re
sp
ec
tiv
el
y
as
so
ci
at
ed
w
ith
th
e
ax
io
m
at
ic
ite
m
s
q
?
in
it(
L
w
),
q
w
?
?
?
r
?
L
w
,
an
d
q
?
fin
al(
L
w
).
A
n
ac
o
u
st
ic
sc
o
re
p(
o
bs
er
va
tio
n
at
h
|
w
)
co
u
ld
be
as
so
ci
at
ed
w
ith
th
e
ite
m
w
?
W
h
.
(b)
TR
E
E
-
ST
A
R
T:
@@q
0
:$
i
?
1
  
F
i
i:
w
 
@@q
i:
w
0
:$
TR
E
E
-
LE
FT
:
 
@@q
i:
w
0
:$

F
i:
w
j
:x
 
@@q
j
:x
0
:$
G
R
A
FT
-
V
IN
E:
 
@@q
i:
w
0
:$
q
w ??
r
?
R
$
XXy
XXyr
i:
w
0
:$
TR
E
E
-
R
IG
H
T:
XXy
XXyq
i:
w
0
:$
HHF
j
:x
i:
w
XXy
XXyq
j
:x
0
:$
TR
E
E
-
EN
D
:
XXy
XXyq
i:
w
0
:$
@@F
i:
w
i
@@q
0
:$
i
SE
A
L
-
LE
FT
:

q
h
? :
w
?
h
:w
q
?
fin
al(
L
w
)

F
h
? :
w
?
h
:w
SE
A
L
-
R
IG
H
T:
HHq
h
? :
w
?
h
:w
q
?
fin
al(
R
w
)
HHF
h
? :
w
?
h
:w
38
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0.3  0.4  0.5  0.6  0.7  0.8  0.9
recall
pre
cis
ion E
C
G
k = 1
Model C, no bound
single bound (English)
 (Chinese)
 (German)
Figure 3: Trading precision and recall: Imposing bounds can
improve precision at the expense of recall, for English and Chi-
nese. German performance suffers more. Bounds shown are
k = {1, 2, ..., 10, 15, 20}. The dotted lines show constant F -
measure of the unbounded model.
4.6 Experiments with Hard Constraints
Our experiments used the asymptotically fast hybrid
parsing algorithm above. We used the same left and
right automata as in model C, the best-performing
model from ?3.2. However, we now define R$ to
be a first-order (bigram) Markov model (?4.1). We
trained and tested on the same headed treebanks as
before (?3.7), except that we modified the training
trees to make them feasible (?4.2).
Results are shown in Figures 3 (precision/recall
tradeoff) and 4 (accuracy/speed tradeoff), for k ?
{1, 2, ..., 10, 15, 20}. Dots correspond to different
values of k. On English and Chinese, some values of
k actually achieve better F -measure accuracy than
the unbounded parser, by eliminating errors.23
We observed that changing R$ from a bigram
to a unigram model significantly hurt performance,
showing that it is in fact useful to empirically model
likely sequences of parse fragments.
4.7 Finer-Grained Hard Constraints
The dependency length bound k need not be a sin-
gle value. Substantially better accuracy can be re-
tained if each dependency type?each (h, c, d) =
(head tag, child tag, direction) tuple?has its own
23Because our prototype implementation of each kind of
parser (baseline, soft constraints, single-bound, and type-
specific bounds) is known to suffer from different inefficiencies,
runtimes in milliseconds are not comparable across parsers. To
give a general idea, 60-word English sentences parsed in around
300ms with no bounds, but at around 200ms with either a dis-
tance model p(?|d, h, c) or a generous hard bound of k = 10.
bound k(h, c, d). We call these type-specific bounds:
they create a many-dimensional space of possible
parsers. We measured speed and accuracy along a
sensible path through this space, gradually tighten-
ing the bounds using the following process:
1. Initialize each bound k(h, c, d) to the maximum
distance observed in training (or 1 for unseen
triples).24
2. Greedily choose a bound k(h, c, d) such that, if
its value is decremented and trees that violate the
new bound are accordingly broken, the fewest de-
pendencies will be broken.25
3. Decrement the bound k(h, c, d) and modify the
training data to respect the bound by breaking de-
pendencies that violate the bound and ?grafting?
the loose portion onto the vine. Retrain the parser
on the training data.
4. If all bounds are not equal to 1, go to step 2.
The performance of every 200th model along the
trajectory of this search is plotted in Fig. 4.26 The
graph shows that type-specific bounds can speed up
the parser to a given level with less loss in accuracy.
5 Related Work
As discussed in footnote 3, Collins (1997) and Mc-
Donald et al (2005) considered the POS tags inter-
vening between a head and child. These soft con-
straints were very helpful, perhaps in part because
they helped capture the short dependency preference
(?2). Collins used them as conditioning variables
and McDonald et al as log-linear features, whereas
our ?3 predicted them directly in a deficient model.
As for hard constraints (?4), our limitation on de-
pendency length can be regarded as approximating
a context-free language by a subset that is a regular
24In the case of the German TIGER corpus, which contains
non-projective dependencies, we first make the training trees
into projective vines by raising all non-projective child nodes to
become heads on the vine.
25Not counting dependencies that must be broken indirectly
in order to maintain projectivity. (If word 4 depends on word
7 which depends on word 2, and the 4 ? 7 dependency is
broken, making 4 a root, then we must also break the 2 ? 7
dependency.)
26Note that k(h, c, right) = 7 bounds the width of @ +
  =

. For a finer-grained approach, we could in-
stead separately bound the widths of @ and   , say by
kr(h, c, right) = 4 and kl(h, c, right) = 2.
39
language. Our ?vines? then let us concatenate sev-
eral strings in this subset, which typically yields a
superset of the original context-free language. Sub-
set and superset approximations of (weighted) CFLs
by (weighted) regular languages, usually by pre-
venting center-embedding, have been widely ex-
plored; Nederhof (2000) gives a thorough review.
We limit all dependency lengths (not just center-
embedding).27 Further, we derive weights from a
modified treebank rather than by approximating the
true weights. And though regular grammar approxi-
mations are useful for other purposes, we argue that
for parsing it is more efficient to perform the approx-
imation in the parser, not in the grammar.
Brants (1999) described a parser that encoded the
grammar as a set of cascaded Markov models. The
decoder was applied iteratively, with each iteration
transforming the best (or n-best) output from the
previous one until only the root symbol remained.
This is a greedy variant of CFG parsing where the
grammar is in Backus-Naur form.
Bertsch and Nederhof (1999) gave a linear-time
recognition algorithm for the recognition of the reg-
ular closure of deterministic context-free languages.
Our result is related; instead of a closure of deter-
ministic CFLs, we deal in a closure of CFLs that are
assumed (by the parser) to obey some constraint on
trees (like a maximum dependency length).
6 Future Work
The simple POS-sequence models we used as an ex-
perimental baseline are certainly not among the best
parsers available today. They were chosen to illus-
trate how modeling and exploiting distance in syntax
can affect various performance measures. Our ap-
proach may be helpful for other kinds of parsers as
well. First, we hope that our results will generalize
to more expressive grammar formalisms such as lex-
icalized CFG, CCG, and TAG, and to more expres-
sively weighted grammars, such as log-linear mod-
els that can include head-child distance among other
rich features. The parsing algorithms we presented
also admit inside-outside variants, allowing iterative
estimation methods for log-linear models (see, e.g.,
Miyao and Tsujii, 2002).
27Of course, this still allows right-branching or left-
branching to unbounded depth.
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  20  40  60  80  100
runtime (items/word)
English
F
k = 1
2
3 15 20
Model C, baseline
soft constraint
single bound
type-specific bounds
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  20  40  60  80  100  120  140  160
runtime (items/word)
Chinese
F
k = 1
2 3
15 20
Model C, baseline
soft constraint
single bound
type-specific bounds
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  10  20  30  40  50  60
runtime (items/word)
German
F
k = 1
2
3
15 20
Model C, baseline
soft constraint
single bound
type-specific bounds
Figure 4: Trading off speed and accuracy by varying the set
of feasible parses: The baseline (no length bound) is shown
as +. Tighter bounds always improve speed, except for the
most lax bounds, for which vine construction overhead incurs
a slowdown. Type-specific bounds tend to maintain good F -
measure at higher speeds than the single-bound approach. The
vertical error bars show the ?oracle? accuracy for each experi-
ment (i.e., the F -measure if we had recovered the best feasible
parse, as constructed from the gold-standard parse by grafting:
see ?4.2). Runtime is measured as the number of items per word
(i.e., @ ,   ,  , H , @ , Xy
Xy
) built
by the agenda parser. The ?soft constraint? point marked with
? represents the p(? | d, h, c)-augmented model from ?3.
40
Second, fast approximate parsing may play a role
in more accurate parsing. It might be used to rapidly
compute approximate outside-probability estimates
to prioritize best-first search (e.g., Caraballo and
Charniak, 1998). It might also be used to speed up
the early iterations of training a weighted parsing
model, which for modern training methods tends to
require repeated parsing (either for the best parse, as
by Taskar et al, 2004, or all parses, as by Miyao and
Tsujii, 2002).
Third, it would be useful to investigate algorith-
mic techniques and empirical benefits for limiting
dependency length in more powerful grammar for-
malisms. Our runtime reduction from O(n3) ?
O(nk2) for a length-k bound applies only to a
?split? bilexical grammar.28 Various kinds of syn-
chronous grammars, in particular, are becoming im-
portant in statistical machine translation. Their high
runtime complexity might be reduced by limiting
monolingual dependency length (for a related idea
see Schafer and Yarowsky, 2003).
Finally, consider the possibility of limiting depen-
dency length during grammar induction. We reason
that a learner might start with simple structures that
focus on local relationships, and gradually relax this
restriction to allow more complex models.
7 Conclusion
We have described a novel reason for identifying
headword-to-headword dependencies while parsing:
to consider their length. We have demonstrated
that simple bilexical parsers of English, Chinese,
and German can exploit a ?short-dependency pref-
erence.? Notably, soft constraints on dependency
length can improve both speed and accuracy, and
hard constraints allow improved precision and speed
with some loss in recall (on English and Chinese,
remarkably little loss). Further, for the hard con-
straint ?length ? k,? we have given an O(nk2) par-
tial parsing algorithm for split bilexical grammars;
the grammar constant is no worse than for state-of-
the-art O(n3) algorithms. This algorithm strings to-
gether the partial trees? roots along a ?vine.?
28The obvious reduction for unsplit head automaton gram-
mars, say, is only O(n4) ? O(n3k), following (Eisner and
Satta, 1999). Alternatively, one can convert the unsplit HAG to
a split one that preserves the set of feasible (length ? k) parses,
but then g becomes prohibitively large in the worst case.
Our approach might be adapted to richer parsing
formalisms, including synchronous ones, and should
be helpful as an approximation to full parsing when
fast, high-precision recovery of syntactic informa-
tion is needed.
References
S. P. Abney. Parsing by chunks. In Principle-Based Parsing:
Computation and Psycholinguistics. Kluwer, 1991.
D. E. Appelt, J. R. Hobbs, J. Bear, D. Israel, and M. Tyson.
FASTUS: A finite-state processor for information extraction
from real-world text. In Proc. of IJCAI, 1993.
E. Bertsch and M.-J. Nederhof. Regular closure of deterministic
languages. SIAM J. on Computing, 29(1):81?102, 1999.
D. Bikel. A distributional analysis of a lexicalized statistical
parsing model. In Proc. of EMNLP, 2004.
T. Brants. Cascaded Markov models. In Proc. of EACL, 1999.
S. A. Caraballo and E. Charniak. New figures of merit for best-
first probabilistic chart parsing. Computational Linguistics,
24(2):275?98, 1998.
S. Chen. Bayesian grammar induction for language modeling.
In Proc. of ACL, 1995.
K. W. Church. On memory limitations in natural language pro-
cessing. Master?s thesis, MIT, 1980.
M. Collins. Three generative, lexicalised models for statistical
parsing. In Proc. of ACL, 1997.
J. Eisner. Bilexical grammars and their cubic-time parsing al-
gorithms. In Advances in Probabilistic and Other Parsing
Technologies. Kluwer, 2000.
J. Eisner, E. Goldlust, and N. A. Smith. Compiling Comp Ling:
Practical weighted dynamic programming and the Dyna lan-
guage. In Proc. of HLT-EMNLP, 2005.
J. Eisner and G. Satta. Efficient parsing for bilexical cfgs and
head automaton grammars. In Proc. of ACL, 1999.
L. Frazier. On Comprehending Sentences: Syntactic Parsing
Strategies. PhD thesis, University of Massachusetts, 1979.
E. Gibson. Linguistic complexity: Locality of syntactic depen-
dencies. Cognition, 68:1?76, 1998.
G. Grefenstette. Light parsing as finite-state filtering. In Proc.
of Workshop on Extended FS Models of Language, 1996.
D. Hindle. Noun classification from predicate-argument struc-
ture. In Proc. of ACL, 1990.
J. R. Hobbs and J. Bear. Two principles of parse preference. In
Proc. of COLING, 1990.
D. Klein and C. D. Manning. Accurate unlexicalized parsing.
In Proc. of ACL, 2003.
D. Klein and C. D. Manning. Corpus-based induction of syn-
tactic structure: Models of dependency and constituency. In
Proc. of ACL, 2004.
R. McDonald, K. Crammer, and F. Pereira. Online large-margin
training of dependency parsers. In Proc. of ACL, 2005.
Y. Miyao and J. Tsujii. Maximum entropy estimation for feature
forests. In Proc. of HLT, 2002.
M.-J. Nederhof. Practical experiments with regular approxima-
tion of context-free languages. CL, 26(1):17?44, 2000.
M.-J. Nederhof. Weighted deductive parsing and Knuth?s algo-
rithm. Computational Linguistics, 29(1):135?143, 2003.
C. Schafer and D. Yarowsky. A two-level syntax-based ap-
proach to Arabic-English statistical machine translation. In
Proc. of Workshop on MT for Semitic Languages, 2003.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
Max-margin parsing. In Proc. of EMNLP, 2004.
41
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317?326,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Better Informed Training of Latent Syntactic Features
Markus Dreyer and Jason Eisner
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University
3400 North Charles Street, Baltimore, MD 21218 USA
{markus,jason}@clsp.jhu.edu
Abstract
We study unsupervised methods for learn-
ing refinements of the nonterminals in
a treebank. Following Matsuzaki et al
(2005) and Prescher (2005), we may for
example split NP without supervision into
NP[0] and NP[1], which behave differently.
We first propose to learn a PCFG that adds
such features to nonterminals in such a
way that they respect patterns of linguis-
tic feature passing: each node?s nontermi-
nal features are either identical to, or inde-
pendent of, those of its parent. This lin-
guistic constraint reduces runtime and the
number of parameters to be learned. How-
ever, it did not yield improvements when
training on the Penn Treebank. An orthog-
onal strategy was more successful: to im-
prove the performance of the EM learner
by treebank preprocessing and by anneal-
ing methods that split nonterminals selec-
tively. Using these methods, we can main-
tain high parsing accuracy while dramati-
cally reducing the model size.
1 Introduction
Treebanks never contain enough information; thus
PCFGs estimated straightforwardly from the Penn
Treebank (Bies et al, 1995) work only moderately
well (Charniak, 1996). To address this problem,
researchers have used heuristics to add more infor-
mation. Eisner (1996), Charniak (1997), Collins
(1997), and many subsequent researchers1 anno-
tated every node with lexical features passed up
from its ?head child,? in order to more precisely re-
flect the node?s ?inside? contents. Charniak (1997)
and Johnson (1998) annotated each node with its
parent and grandparent nonterminals, to more pre-
cisely reflect its ?outside? context. Collins (1996)
split the sentence label S into two versions, repre-
senting sentences with and without subjects. He
1Not to mention earlier non-PCFG lexicalized statistical
parsers, notably Magerman (1995) for the Penn Treebank.
also modified the treebank to contain different la-
bels for standard and for base noun phrases. Klein
and Manning (2003) identified nonterminals that
could valuably be split into fine-grained ones us-
ing hand-written linguistic rules. Their unlexical-
ized parser combined several such heuristics with
rule markovization and reached a performance
similar to early lexicalized parsers.
In all these cases, choosing which nonterminals
to split, and how, was a matter of art. Ideally
such splits would be learned automatically from
the given treebank itself. This would be less costly
and more portable to treebanks for new domains
and languages. One might also hope that the auto-
matically learned splits would be more effective.
Matsuzaki et al (2005) introduced a model for
such learning: PCFG-LA.2 They used EM to in-
duce fine-grained versions of a given treebank?s
nonterminals and rules. We present models that
similarly learn to propagate fine-grained features
through the tree, but only in certain linguistically
motivated ways. Our models therefore allocate
a supply of free parameters differently, allow-
ing more fine-grained nonterminals but less fine-
grained control over the probabilities of rewriting
them. We also present simple methods for decid-
ing selectively (during training) which nontermi-
nals to split and how.
Section 2 describes previous work in finding
hidden information in treebanks. Section 3 de-
scribes automatically induced feature grammars.
We start by describing the PCFG-LA model, then
introduce new models that use specific agreement
patterns to propagate features through the tree.
Section 4 describes annealing-like procedures for
training latent-annotation models. Section 5 de-
scribes the motivation and results of our experi-
ments. We finish by discussing future work and
conclusions in sections 6?7.
2Probabilistic context-free grammar with latent annota-
tions.
317
Citation Observed data Hidden data
Collins (1997) Treebank tree with head child an-
notated on each nonterminal
No hidden data. Degenerate EM
case.
Lari and Young (1990) Words Parse tree
Pereira and Schabes (1992) Words and partial brackets Parse tree
Klein and Manning (2001) Part-of-speech tags Parse tree
Chiang and Bikel (2002) Treebank tree Head child on each nonterminal
Matsuzaki et al (2005) Treebank tree Integer feature on each nontermi-
nal
INHERIT model (this paper) Treebank tree and head child
heuristics
Integer feature on each nontermi-
nal
Table 1: Observed and hidden data in PCFG grammar learning.
2 Partially supervised EM learning
The parameters of a PCFG can be learned with
or without supervision. In the supervised case,
the complete tree is observed, and the rewrite rule
probabilities can be estimated directly from the
observed rule counts. In the unsupervised case,
only the words are observed, and the learning
method must induce the whole structure above
them. (See Table 1.)
In the partially supervised case we will con-
sider, some part of the tree is observed, and
the remaining information has to be induced.
Pereira and Schabes (1992) estimate PCFG pa-
rameters from partially bracketed sentences, using
the inside-outside algorithm to induce the miss-
ing brackets and the missing node labels. Some
authors define a complete tree as one that speci-
fies not only a label but also a ?head child? for
each node. Chiang and Bikel (2002) induces the
missing head-child information; Prescher (2005)
induces both the head-child information and the
latent annotations we will now discuss.
3 Feature Grammars
3.1 The PCFG-LA Model
Staying in the partially supervised paradigm, the
PCFG-LA model described in Matsuzaki et al
(2005) observe whole treebank trees, but learn
an ?annotation? on each nonterminal token?an
unspecified and uninterpreted integer that distin-
guishes otherwise identical nonterminals. Just as
Collins manually split the S nonterminal label into
S and SG for sentences with and without subjects,
Matsuzaki et al (2005) split S into S[1], S[2], . . . ,
S[L] where L is a predefined number?but they do
it automatically and systematically, and not only
for S but for every nonterminal. Their partially
supervised learning procedure observes trees that
are fully bracketed and fully labeled, except for
the integer subscript used to annotate each node.
After automatically inducing the annotations with
EM, their resulting parser performs just as well as
one learned from a treebank whose nonterminals
were manually refined through linguistic and error
analysis (Klein and Manning, 2003).
In Matsuzaki?s PCFG-LA model, rewrite rules
take the form
X[?] ? Y [?] Z[?] (1)
in the binary case, and
X[?] ? w (2)
in the lexical case. The probability of a tree con-
sisting of rules r1, r2, . . . is given by the probabil-
ity of its root symbol times the conditional prob-
abilities of the rules. The annotated tree T1 in
Fig. 1, for example, has the following probability:
P (T1) = P (ROOT ? S[2])
?P (S[2] ? NP[1] VP[3])
?P (NP[1] ?? He)
?P (VP[3] ?? loves cookies)
where, to simplify the notation, we use
P (X ? Y Z) to denote the conditional probabil-
ity P (Y Z | X) that a given node with label X
will have children Y Z.
Degrees of freedom. We will want to compare
models that have about the same size. Models with
more free parameters have an inherent advantage
on modeling copious data because of their greater
318
Figure 1: Treebank tree with annotations.
expressiveness. Models with fewer free parame-
ters are easier to train accurately on sparse data,
as well as being more efficient in space and often
in time. Our question is therefore what can be ac-
complished with a given number of parameters.
How many free parameters in a PCFG-LA
model? Such a model is created by annotating
the nonterminals of a standard PCFG (extracted
from the given treebank) with the various integers
from 1 to L. If the original, ?backbone? grammar
has R3 binary rules of the form X ? Y Z, then
the resulting PCFG-LA model has L3 ? R3 such
rules: X[1] ? Y [1] Z[1], X[1] ? Y [1] Z[2],
X[1] ? Y [2] Z[1], . . . , X[L] ? Y [L] Z[L]. Sim-
ilarly, if the backbone grammar has R2 rules of
the form X ? Y the PCFG-LA model has L2 ?
R2 such rules.3 The number of R1 terminal rules
X ? w is just multiplied by L.
The PCFG-LA has as many parameters to learn
as rules: one probability per rule. However, not
all these parameters are free, as there are L ? N
sum-to-one constraints, where N is the number of
backbone nonterminals. Thus we have
L3R3 + L
2R2 + LR1 ? LN (3)
degrees of freedom.
We note that Goodman (1997) mentioned possi-
ble ways to factor the probability 1, making inde-
pendence assumptions in order to reduce the num-
ber of parameters.
Runtime. Assuming there are no unary rule cy-
cles in the backbone grammar, bottom-up chart
parsing of a length-n sentence at test time takes
time proportional to n3L3R3 + n2L2R2 + nLR1,
by attempting to apply each rule everywhere in the
sentence. (The dominating term comes from equa-
tion (4) of Table 2: we must loop over all n3 triples
i, j, k and all R3 backbone rules X ? Y Z and all
3We use unary rules of this form (e.g. the Treebank?s S?
NP) in our reimplementation of Matsuzaki?s algorithm.
L3 triples ?, ?, ?.) As a function of n and L only,
this is O(n3L3).
At training time, to induce the annotations on
a given backbone tree with n nodes, one can run
a constrained version of this algorithm that loops
over only the n triples i, j, k that are consistent
with the given tree (and considers only the single
consistent backbone rule for each one). This takes
time O(nL3), as does the inside-outside version
we actually use to collect expected PCFG-LA rule
counts for EM training.
We now introduce a model that is smaller, and
has a lower runtime complexity, because it adheres
to specified ways of propagating features through
the tree.
3.2 Feature Passing: The INHERIT Model
Many linguistic theories assume that features get
passed from the mother node to their children or
some of their children. In many cases it is the
head child that gets passed its feature value from
its mother (e.g., Kaplan and Bresnan (1982), Pol-
lard and Sag (1994)). In some cases the feature is
passed to both the head and the non-head child, or
perhaps even to the non-head alone.
Figure 2: Features are passed to different children
at different positions in the tree.
In the example in Fig. 2, the tense feature (pres)
is always passed to the head child (underlined).
How the number feature (sg/pl) is passed depends
on the rewrite rule: S ? NP VP passes it to both
children, to enforce subject-verb agreement, while
VP ? V NP only passes it to the head child, since
the object NP is free not to agree with the verb.
A feature grammar can incorporate such pat-
terns of feature passing. We introduce additional
parameters that define the probability of passing a
feature to certain children. The head child of each
node is given deterministically by the head rules
of (Collins, 1996).
Under the INHERIT model that we propose, the
319
Model Runtime and d.f. Simplified equation for inside probabilities (ignores unary rules)
Matsuzaki
et al (2005)
test: O(n3L3)
train: O(nL3)
d.f.: L3R3 +
L2R2 +LR1?LN
BX[?](i, k) =
X
Y,?,Z,?,j
P (X[?] ? Y [?] Z[?]) (4)
?BY [?](i, j)?BZ[?](j, k)
INHERIT
model
(this paper)
test: O(n3L)
train: O(nL)
d.f.: L(R3 + R2 +
R1) + 3R3 ?N
BX[?](i, k) =
X
Y,Z,j
P (X[?] ? Y Z) (5)
?
0
B
@
P (neither | X,Y, Z) ? BY (i, j) ? BZ(j, k))
+ P (left | X,Y, Z) ? BY [?](i, j) ? BZ(j, k))
+ P (right | X,Y, Z) ? BY (i, j) ? BZ[?](j, k))
+ P (both | X,Y, Z) ? BY [?]Y (i, j) ? BZ[?](j, k))
1
C
A
BX(i, j) =
X
?
Pann(? | X)?BX[?](i, j) (6)
P (left | X,Y, Z) =
?
P (head | X,Y, Z) if Y heads X ? Y Z
P (nonhead | X,Y, Z) otherwise (7)
P (right | X,Y, Z) =
?
P (head | X,Y, Z) if Z heads X ? Y Z
P (nonhead | X,Y, Z) otherwise (8)
Table 2: Comparison of the PCFG-LA model with the INHERIT model proposed in this paper. ?d.f.?
stands for ?degrees of freedom? (i.e., free parameters). The B terms are inside probabilities; to compute
Viterbi parse probabilities instead, replace summation by maximization. Note the use of the intermediate
quantity BX(i, j) to improve runtime complexity by moving some summations out of the inner loop;
this is an instance of a ?folding transformation? (Blatz and Eisner, 2006).
Figure 3: Two passpatterns. Left: T2. The feature
is passed to the head child (underlined). Right: T3.
The feature is passed to both children.
probabilities of tree T2 in Fig. 3 are calculated as
follows, with Pann(1 | NP ) being the probability
of annotating an NP with feature 1 if it does not
inherit its parent?s feature. The VP is boldfaced to
indicate that it is the head child of this rule.
P (T2) = P (ROOT ? S[2])
?P (S[2] ? NP VP)
?P (pass to head | S ? NP VP)
?Pann(1 | NP) ? P (NP[1] ?? He)
?P (VP[2] ?? loves cookies)
Tree T3 in Fig. 3 has the following probability:
P (T3) = P (ROOT ? S[2])
?P (S[2] ? NP VP)
?P (pass to both | S ? NP VP)
?P (NP[2] ?? He)
?P (VP[2] ?? loves cookies)
In T2, the subject NP chose feature 1 or 2 indepen-
dent of its parent S, according to the distribution
Pann(? | NP). In T3, it was constrained to inherit
its parent?s feature 2.
Degrees of freedom. The INHERIT model may
be regarded as containing all the same rules
(see (1)) as the PCFG-LA model. However, these
rules? probabilities are now collectively deter-
mined by a smaller set of shared parameters.4 That
is because the distribution of the child features ?
and ? no longer depends arbitrarily on the rest of
the rule. ? is either equal to ?, or chosen indepen-
dently of everything but Y .
The model needs probabilities for L ? R3
binary-rule parameters like P (S[2] ? NP VP)
above, as well as L ? R2 unary-rule and L ? R1
lexical-rule parameters. None of these consider
the annotations on the children. They are subject
to L?N sum-to-one constraints.
The model also needs 4?R3 passpattern prob-
abilities like P (pass to head | X ? Y Z) above,
with R3 sum-to-one constraints, and L ? N non-
inherited annotation parameters Pann(?|X), with
N sum-to-one constraints.
Adding these up and canceling the two L ? N
4The reader may find it useful to write out the probability
P (X[?] ? Y [?] Z[?]) in terms of the parameters described
below. Like equation (5), it is P (X[?] ? Y Z) times a sum
of up to 4 products, corresponding to the 4 passpattern cases.
320
terms, the INHERIT model has
L(R3 +R2 +R1) + 3R3 ?N (9)
degrees of freedom. Thus for a typical grammar
where R3 dominates, we have reduced the number
of free parameters from about L3R3 to only about
LR3.
Runtime. We may likewise reduce an L3 factor
to L in the runtime. Table 2 shows dynamic pro-
gramming equations for the INHERIT model. By
exercising care, they are able to avoid summing
over all possible values of ? and ? within the in-
ner loop. This is possible because when they are
not inherited, they do not depend on X,Y, Z, or ?.
3.3 Multiple Features
The INHERIT model described above is linguisti-
cally naive in several ways. One problem (see sec-
tion 6 for others) is that each nonterminal has only
a single feature to pass. Linguists, however, usu-
ally annotate each phrase with multiple features.
Our example tree in Fig. 2 was annotated with both
tense and number features, with different inheri-
tance patterns.
As a step up from INHERIT, we propose an
INHERIT2 model where each nonterminal carries
two features. Thus, we will have L6R3 binary
rules instead of L3R3. However, we assume that
the two features choose their passpatterns inde-
pendently, and that when a feature is not inher-
ited, it is chosen independently of the other fea-
ture. This keeps the number of parameters down.
In effect, we are defining
P (X[?][?] ? Y [?][?] Z[?][? ])
= P (X[?][?] ? Y Z)
?P1(?, ? | X[?] ? Y Z)
?P2(?, ? | X[?] ? Y Z)
where P1 and P2 choose child features as if they
were separate single-feature INHERIT models.
We omit discussion of dynamic programming
speedups for INHERIT2. Empirically, the hope is
that the two features when learned with the EM
algorithm will pick out different linguistic proper-
ties of the constituents in the treebank tree.
4 Annealing-Like Training Approaches
Training latent PCFG models, like training most
other unsupervised models, requires non-convex
optimization. To find good parameter values, it
is often helpful to train a simpler model first and
use its parameters to derive a starting guess for the
harder optimization problem. A well-known ex-
ample is the training of the IBM models for statis-
tical machine translation (Berger et al, 1994).
In this vein, we did an experiment in which we
gradually increased L during EM training of the
PCFG-LA and INHERIT models. Whenever the
training likelihood began to converge, we man-
ually and globally increased L, simply doubling
or tripling it (see ?clone all? in Table 3 and
Fig. 5). The probability of X[?] ? Y [?]Z[?]
under the new model was initialized to be pro-
portional to the probability of X[? mod L] ?
Y [? mod L]Z[? mod L] (where L refers to the
old L),5 times a random ?jitter? to break symme-
try.
In a second annealing experiment (?clone
some?) we addressed a weakness of the PCFG-
LA and INHERIT models: They give every non-
terminal the same number of latent annotations.
It would seem that different coarse-grained non-
terminals in the original Penn Treebank have dif-
ferent degrees of impurity (Klein and Manning,
2003). There are linguistically many kinds of
NP, which are differentially selected for by vari-
ous contexts and hence are worth distinguishing.
By contrast, -LRB- is almost always realized as
a left parenthesis and may not need further refine-
ment. Our ?clone some? annealing starts by train-
ing a model with L=2 to convergence. Then, in-
stead of cloning all nonterminals as in the previ-
ous annealing experiments, we clone only those
that have seemed to benefit most from their previ-
ous refinement. This benefit is measured by the
Jensen-Shannon divergence of the two distribu-
tions P (X[0] ? ? ? ? ) and P (X[1] ? ? ? ? ). The
5Notice that as well as cloning X[?], this procedure mul-
tiplies by 4, 2, and 1 the number of binary, unary, and lex-
ical rules that rewrite X[?]. To leave the backbone gram-
mar unchanged, we should have scaled down the probabili-
ties of such rules by 1/4, 1/2, and 1 respectively. Instead, we
simply scaled them all down by the same proportion. While
this temporarily changes the balance of probability among the
three kinds of rules, EM immediately corrects this balance on
the next training iteration to match the observed balance on
the treebank trees?hence the one-iteration downtick in Fig-
ure 5).
321
Jensen-Shannon divergence is defined as
D(q, r) =
1
2
(
D
(
q ||
q + r
2
)
+D
(
r ||
q + r
2
))
These experiments are a kind of ?poor man?s
version? of the deterministic annealing cluster-
ing algorithm (Pereira et al, 1993; Rose, 1998),
which gradually increases the number of clus-
ters during the clustering process. In determinis-
tic annealing, one starts in principle with a very
large number of clusters, but maximizes likeli-
hood only under a constraint that the joint distri-
bution p(point , cluster) must have very high en-
tropy. This drives all of the cluster centroids to co-
incide exactly, redundantly representing just one
effective cluster. As the entropy is permitted to de-
crease, some of the cluster centroids find it worth-
while to drift apart.6 In future work, we would
like to apply this technique to split nonterminals
gradually, by initially requiring high-entropy parse
forests on the training data and slowly relaxing this
constraint.
5 Experiments
5.1 Setup
We ran several experiments to compare the IN-
HERIT with the PCFG-LA model and look into the
effect of different Treebank preprocessing and the
annealing-like procedures.
We used sections 2?20 of the Penn Treebank 2
Wall Street Journal corpus (Marcus et al, 1993)
for training, section 22 as development set and
section 23 for testing. Following Matsuzaki et al
(2005), words occurring fewer than 4 times in the
training corpus were replaced by unknown-word
symbols that encoded certain suffix and capitaliza-
tion information.
All experiments used simple add-lambda
smoothing (?=0.1) during the reestimation step
(M step) of training.
Binarization and Markovization. Before ex-
tracting the backbone PCFG and running the con-
strained inside-outside (EM) training algorithm,
we preprocessed the Treebank using center-parent
binarization Matsuzaki et al (2005). Besides mak-
ing the rules at most binary, this preprocessing also
helpfully enriched the backbone nonterminals. For
6In practice, each very large group of centroids (effective
cluster) is represented by just two, until such time as those
two drift apart to represent separate effective clusters?then
each is cloned.
all but the first (?Basic?) experiments, we also
enriched the nonterminals with order-1 horizon-
tal and order-2 vertical markovization (Klein and
Manning, 2003).7 Figure 4 shows what a multiple-
child structure X ? A B H C D looks like
after binarization and markovization. The bina-
rization process starts at the head of the sentence
and moves to the right, inserting an auxiliary node
for each picked up child, then moving to the left.
Each auxiliary node consists of the parent label,
the direction (L or R) and the label of the child
just picked up.
Figure 4: Horizontal and vertical markovization
and center-parent binarization of the rule X ?
A B H C D where H is the head child.
Initialization. The backbone PCFG grammar
was read off the altered Treebank, and the initial
annotated grammar was created by creating sev-
eral versions of every rewrite rule. The proba-
bilities of these newly created rules are uniform
and proportional to the original rule, multiplied by
a random epsilon factor uniformly sampled from
[.9999,1.0001] to break symmetry.
5.2 Decoding
To test the PCFG learned by a given method,
we attempted to recover the unannotated parse
of each sentence in the development set. We
then scored these parses by debinarizing or de-
markovizing them, then measuring their precision
and recall of the labeled constituents from the
gold-standard Treebank parses.
7The vertical markovization was applied before binariza-
tion. ? Matsuzaki et al (2005) used a markovized grammar
to get a better unannotated parse forest during decoding, but
they did not markovize the training data.
322
Figure 5: Loge-likelihood during training. The
two ?anneal? curves use the ?clone all? method.
We increased L after iteration 50 and, for the IN-
HERIT model, iteration 110. The downward spikes
in the two annealed cases are due to perturbation
of the model parameters (footnote 5).
An unannotated parse?s probability is the total
probability, under our learned PCFG, of all of its
annotated refinements. This total can be efficiently
computed by the constrained version of the inside
algorithm in Table 2.
How do we obtain the unannotated parse whose
total probability is greatest? It does not suffice to
find the single best annotated parse and then strip
off the annotations. Matsuzaki et al (2005) note
that the best annotated parse is in fact NP-hard to
find. We use their reranking approximation. A
1000-best list for each sentence in the decoding
set was created by parsing with our markovized
unannotated grammar and extracting the 1000 best
parses using the k-best algorithm 3 described in
Huang and Chiang (2005). Then we chose the
most probable of these 1000 unannotated parses
under our PCFG, first finding the total probability
of each by using the the constrained inside algo-
rithm as explained above.8
5.3 Results and Discussion
Table 3 summarizes the results on development
and test data. 9 Figure 5 shows the training log-
likelihoods.
First, markovization of the Treebank leads to
8For the first set of experiments, in which the models were
trained on a simple non-markovized grammar, the 1000-best
trees had to be ?demarkovized? before our PCFG was able to
rescore them.
9All results are reported on sentences of 40 words or less.
striking improvements. The ?Basic? block of ex-
periments in Table 3 used non-markovized gram-
mars, as in Matsuzaki et al (2005). The next block
of experiments, introducing markovized gram-
mars, shows a considerable improvement. This
is not simply because markovization increases the
number of parameters: markovization with L = 2
already beats basic models that have much higher
L and far more parameters.
Evidently, markovization pre-splits the labels
in the trees in a reasonable way, so EM has less
work to do. This is not to say that markovization
eliminates the need for hidden annotations: with
markovization, going from L=1 to L=2 increases
the parsing accuracy even more than without it.
Second, our ?clone all? training technique
(shown in the next block of Table 3) did not
help performance and may even have hurt slightly.
Here we initialized the L=2x2 model with the
trained L=2 model for PCFG-LA, and the L=3x3
model with the L=3 and the L=3x3x3 model with
the L=3x3 model.
Third, our ?clone some? training technique ap-
peared to work. On PCFG-LA, the L<2x2 con-
dition (i.e., train with L=2 and then clone some)
matched the performance of L=4 with 30% fewer
parameters. On INHERIT, L<2x2 beat L=4 with
8% fewer parameters. In these experiments, we
used the average divergence as a threshold: X[0]
and X[1] are split again if the divergence of their
rewrite distributions is higher than average.
Fourth, our INHERIT model was a disappoint-
ment. It generally performed slightly worse than
PCFG-LA when given about as many degrees
of freedom. This was also the case on some
cursory experiments on smaller training corpora.
It is tempting to conclude that INHERIT simply
adopted overly strong linguistic constraints, but
relaxing those constraints by moving to the IN-
HERIT2 model did not seem to help. In our
one experiment with INHERIT2 (not shown in Ta-
ble 3), using 2 features that can each take L=2
values (d.f.: 212,707) obtains an F1 score of only
83.67?worse than 1 feature taking L=4 values.
5.4 Analysis: What was learned by INHERIT?
INHERIT did seem to discover ?linguistic? fea-
tures, as intended, even though this did not im-
prove parse accuracy. We trained INHERIT and
PCFG-LA models (both L=2, non-markovized)
and noticed the following.
323
PCFG-LA INHERIT
L d.f. LP LR F1 L d.f. LP LR F1
B
as
ic 1 24,226 76.99 74.51 75.73 1 35,956 76.99 74.51 75.73
2 72,392 81.22 80.67 80.94 2 60,902 79.42 77.58 78.49
4 334,384 83.53 83.39 83.46 12 303,162 82.41 81.55 81.98
8 2,177,888 85.43 85.05 85.24 80 1,959,053 83.99 83.02 83.50
M
ar
ko
v. 1 41,027 79.95 78.43 79.18 1 88,385 79.95 78.43 79.18
2 132,371 83.85 82.23 83.03
2 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.31
4 220,343 85.30 84.06 84.68
3 506,427 86.44 85.19 85.81 9 440,273 86.16 85.12 85.64
4 1,120,232 87.09 85.71 86.39 26 1,188,035 86.55 85.55 86.05
Cl
on
e
al
l 2 178,264 85.70 84.37 85.03 3 176,357 85.04 83.60 84.31
3x3 440,273 85.99 84.88 85.43
2x2 1,120,232 87.06 85.49 86.27 3x3x3 1,232,021 86.65 85.70 86.17
Cl
.so
m
e 2 178,264 85.70 84.37 85.03 2 132,371 83.85 82.23 83.03
<2x2 789,279 87.17 85.71 86.43 <2x2 203,673 85.49 84.45 84.97
<2x2x2 314,999 85.57 84.60 85.08
Table 3: Results on the development set: labeled precision (LP), labeled recall (LR), and their harmonic
mean (F1). ?Basic? models are trained on a non-markovized treebank (as in Matsuzaki et al (2005)); all
others are trained on a markovized treebank. The best model (PCFG-LA with ?clone some? annealing,
F1=86.43) has also been decoded on the final test set, reaching P/R=86.94/85.40 (F1=86.17).
We used both models to assign the most-
probable annotations to the gold parses of the de-
velopment set. Under the INHERIT model, NP[0]
vs. NP[1] constituents were 21% plural vs. 41%
plural. Under PCFG-LA this effect was weaker
(30% vs. 39%), although it was significant in both
(Fisher?s exact test, p < 0.001). Strikingly, un-
der the INHERIT model, the NP?s were 10 times
more likely to pass this feature to both children
(Fisher?s, p < 0.001)?just as we would expect
for a number feature, since the determiner and
head noun of an NP must agree.
The INHERIT model also learned to use feature
value 1 for ?tensed auxiliary.? The VP[1] nonter-
minal was far more likely than VP[0] to expand as
V VP, where V represents any of the tensed verb
preterminals VBZ, VBG, VBN, VBD, VBP. Further-
more, these expansion rules had a very strong pref-
erence for ?pass to head,? so that the left child
would also be annotated as a tensed auxiliary, typ-
ically causing it to expand as a form of be, have,
or do. In short, the feature ensured that it was gen-
uine auxiliary verbs that subcategorized for VP?s.
(The PCFG-LA model actually arranged the
same behavior, e.g. similarly preferring VBZ[1] in
the auxiliary expansion rule VP ? VBZ VP. The
difference is that the PCFG-LA model was able
to express this preference directly without prop-
agating the [1] up to the VP parent. Hence neither
VP[0] nor VP[1] became strongly associated with
the auxiliary rule.)
Many things are equally learned by both mod-
els: They learn the difference between subordinat-
ing conjunctions (while, if ) and prepositions (un-
der, after), putting them in distinct groups of the
original IN tag, which typically combine with sen-
tences and noun phrases, respectively. Both mod-
els also split the conjunction CC into two distinct
groups: a group of conjunctions starting with an
upper-case letter at the beginning of the sentence
and a group containing all other conjunctions.
6 Future Work: Log-Linear Modeling
Our approach in the INHERIT model made certain
strict independence assumptions, with no backoff.
The choice of a particular passpattern, for exam-
ple, depends on all and only the three nontermi-
nals X,Y, Z. However, given sparse training data,
sometimes it is advantageous to back off to smaller
amounts of contextual information; the nontermi-
nal X or Y might alone be sufficient to predict the
passpattern.
324
A very reasonable framework for handling this
issue is to model P (X[?] ? Y [?] Z[?]) with
a log-linear model.10 Feature functions would
consider the values of variously sized, over-
lapping subsets of X,Y, Z, ?, ?, ?. For exam-
ple, a certain feature might fire when X[?] =
NP[1] and Z[?] = N[2]. This approach can be ex-
tended to the multi-feature case, as in INHERIT2.
Inheritance as in the INHERIT model can then
be expressed by features like ? = ?, or ? =
? and X = VP. During early iterations, we could
use a prior to encourage a strong positive weight
on these inheritance features, and gradually re-
lax this bias?akin to the ?structural annealing? of
(Smith and Eisner, 2006).
When modeling the lexical rule P (X[?] ? w),
we could use features that consider the spelling
of the word w in conjunction with the value of
?. Thus, we might learn that V [1] is particularly
likely to rewrite as a word ending in -s. Spelling
features that are predictable from string context
are important clues to the existence and behavior
of the hidden annotations we wish to induce.
A final remark is that ?inheritance? does not
necessarily have to mean that ? = ?. It is enough
that ? and ? should have high mutual informa-
tion, so that one can be predicted from the other;
they do not actually have to be represented by the
same integer. More broadly, we might like ? to
have high mutual information with the pair (?, ?).
One might try using this sort of intuition directly
in an unsupervised learning procedure (Elidan and
Friedman, 2003).
7 Conclusions
We have discussed ?informed? techniques for in-
ducing latent syntactic features. Our INHERIT
model tries to constrain the way in which features
are passed through the tree. The motivation for
this approach is twofold: First, we wanted to cap-
ture the linguistic insight that features follow cer-
tain patterns in propagating through the tree. Sec-
ond, we wanted to make it statistically feasible and
computationally tractable to increase L to higher
values than in the PCFG-LA model. The hope was
that the learning process could then make finer dis-
tinctions and learn more fine-grained information.
However, it turned out that the higher values of
L did not compensate for the perhaps overly con-
10This affects EM training only by requiring a convex op-
timization at the M step (Riezler, 1998).
strained model. The results on English parsing
rather suggest that it is the similarity in degrees of
freedom (e.g., INHERIT with L=3x3x3 and PCFG-
LA with L=2x2) that produces comparable results.
Substantial gains were achieved by using
markovization and splitting only selected nonter-
minals. With these techniques we reach a pars-
ing accuracy similar to Matsuzaki et al (2005),
but with an order of magnitude less parameters,
resulting in more efficient parsing. We hope to
get more wins in future by using more sophisti-
cated annealing techniques and log-linear model-
ing techniques.
Acknowledgments
This paper is based upon work supported by the
National Science Foundation under Grant No.
0313193. We are grateful to Takuya Matsuzaki
for providing details about his implementation of
PCFG-LA, and to Noah Smith and the anonymous
reviewers for helpful comments.
References
A. Berger, P. Brown, S. Pietra, V. Pietra, J. Lafferty,
H. Printz, and L. Ures. 1994. The CANDIDE sys-
tem for machine translation.
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre, Victoria Tredinnick, Grace Kim, Mary Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for Treebank II style: Penn
Treebank project. Technical Report MS-CIS-95-06,
University of Pennsylvania, January.
John Blatz and Jason Eisner. 2006. Transforming pars-
ing algorithms and other weighted logic programs.
In Proceedings of the 11th Conference on Formal
Grammar.
Eugene Charniak. 1996. Tree-bank grammars. In Pro-
ceedings of the 13th National Conference on Artifi-
cial Intelligence.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference on
Artificial Intelligence, pages 598?603.
David Chiang and Daniel M. Bikel. 2002. Recov-
ering latent information in treebanks. In COLING
2002: The 17th International Conference on Com-
putational Linguistics, Taipei.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In ACL-96,
pages 184?191, Santa Cruz, CA. ACL.
325
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and 8th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 16?23, Madrid. Association for
Computational Linguistics.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling comp ling: Weighted dynamic program-
ming and the Dyna language. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 281?290, Vancouver, October.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING96: Proceedings of the 16th International Con-
ference on Computational Linguistics, pages 340?
345, Copenhagen. Center for Sprogteknologi.
Gal Elidan and Nir Friedman. 2003. The information
bottleneck EM algorithm. In Proceedings of UAI.
Joshua Goodman. 1997. Probabilistic feature gram-
mars. In Proceedings of the 5th International Work-
shop on Parsing Technologies, pages 89?100, MIT,
Cambridge, MA, September.
L. Huang and D. Chiang. 2005. Parsing and k-best
algorithms. In Proc. of IWPT.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
pages 173?281. MIT Press, Cambridge, MA.
Dan Klein and Christopher D. Manning. 2001. Dis-
tributional phrase structure induction. In The Fifth
Conference on Natural Language Learning.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Erhard Hinrichs and
Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 423?430, Sapporo, Japan.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside
algorithm. Computer Speech and Language, 4:35?
56.
David M. Magerman. 1995. Statistical Decision-Tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, pages 276?283, Cambridge, Mass.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330, June.
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?05),
University of Michigan.
Fernando Pereira and Yves Schabes. 1992. Inside-
Outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Meeting of the As-
sociation for Computational Linguistics, pages 128?
135, Newark. University of Delaware.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of ACL, Ohio State University.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago.
Detlef Prescher. 2005. Head-driven PCFGs with
latent-head statistics. In Proceedings of the 9th
International Workshop on Parsing Technologies,
pages 115?124, Vancouver, BC, Canada, October.
Stefan Riezler. 1998. Statistical inference and prob-
abilistic modeling for constraint-based NLP. In
B. Schro?der, W. Lenders, W. Hess, and T. Portele,
editors, Computers, Linguistics, and Phonetics be-
tween Language and Speech: Proceedings of the 4th
Conference on Natural Language Processing (KON-
VENS?98), pages 111?124, Bonn. Lang.
Kenneth Rose. 1998. Deterministic annealing for clus-
tering, compression, classification, regression, and
related optimization problems. Proceedings of the
IEEE, 80:2210?2239, November.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of ACL.
326
Proceedings of the Workshop on Statistical Machine Translation, pages 23?30,
New York City, June 2006. c?2006 Association for Computational Linguistics
Quasi-Synchronous Grammars:
Alignment by Soft Projection of Syntactic Dependencies
David A. Smith and Jason Eisner
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{dasmith,eisner}@jhu.edu
Abstract
Many syntactic models in machine trans-
lation are channels that transform one
tree into another, or synchronous gram-
mars that generate trees in parallel. We
present a newmodel of the translation pro-
cess: quasi-synchronous grammar (QG).
Given a source-language parse tree T1, a
QG defines a monolingual grammar that
generates translations of T1. The trees
T2 allowed by this monolingual gram-
mar are inspired by pieces of substruc-
ture in T1 and aligned to T1 at those
points. We describe experiments learning
quasi-synchronous context-free grammars
from bitext. As with other monolingual
language models, we evaluate the cross-
entropy of QGs on unseen text and show
that a better fit to bilingual data is achieved
by allowing greater syntactic divergence.
When evaluated on a word alignment task,
QG matches standard baselines.
1 Motivation and Related Work
1.1 Sloppy Syntactic Alignment
This paper proposes a new type of syntax-based
model for machine translation and alignment. The
goal is to make use of syntactic formalisms, such as
context-free grammar or tree-substitution grammar,
without being overly constrained by them.
Let S1 and S2 denote the source and target sen-
tences. We seek to model the conditional probability
p(T2, A | T1) (1)
where T1 is a parse tree for S1, T2 is a parse tree
for S2, and A is a node-to-node alignment between
them. This model allows one to carry out a variety
of alignment and decoding tasks. Given T1, one can
translate it by finding the T2 and A that maximize
(1). Given T1 and T2, one can align them by finding
the A that maximizes (1) (equivalent to maximizing
p(A | T2, T1)). Similarly, one can align S1 and S2
by finding the parses T1 and T2, and alignment A,
that maximize p(T2, A | T1) ? p(T1 | S1), where
p(T1 | S1) is given by a monolingual parser. We
usually accomplish such maximizations by dynamic
programming.
Equation (1) does not assume that T1 and T2 are
isomorphic. For example, a model might judge T2
and A to be likely, given T1, provided that many?
but not necessarily all?of the syntactic dependen-
cies in T1 are aligned with corresponding depen-
dencies in T2. Hwa et al (2002) found that hu-
man translations from Chinese to English preserved
only 39?42% of the unlabeled Chinese dependen-
cies. They increased this figure to 67% by using
more involved heuristics for aligning dependencies
across these two languages. That suggests that (1)
should be defined to consider more than one depen-
dency at a time.
This inspires the key novel feature of our models:
A does not have to be a ?well-behaved? syntactic
alignment. Any portion of T2 can align to any por-
tion of T1, or to NULL. Nodes that are syntactically
related in T1 do not have to translate into nodes that
are syntactically related in T2?although (1) is usu-
ally higher if they do.
This property makes our approach especially
promising for aligning freely, or erroneously, trans-
lated sentences, and for coping with syntactic diver-
23
gences observed between even closely related lan-
guages (Dorr, 1994; Fox, 2002). We can patch to-
gether an alignment without accounting for all the
details of the translation process. For instance, per-
haps a source NP (figure 1) or PP (figure 2) appears
?out of place? in the target sentence. A linguist
might account for the position of the PP auf diese
Frage either syntactically (by invoking scrambling)
or semantically (by describing a deep analysis-
transfer-synthesis process in the translator?s head).
But an MT researcher may not have the wherewithal
to design, adequately train, and efficiently compute
with ?deep? accounts of this sort. Under our ap-
proach, it is possible to use a simple, tractable syn-
tactic model, but with some contextual probability
of ?sloppy? transfer.
1.2 From Synchronous to Quasi-Synchronous
Grammars
Because our approach will let anything align to
anything, it is reminiscent of IBM Models 1?5
(Brown et al, 1993). It differs from the many ap-
proaches where (1) is defined by a stochastic syn-
chronous grammar (Wu, 1997; Alshawi et al, 2000;
Yamada and Knight, 2001; Eisner, 2003; Gildea,
2003; Melamed, 2004) and from transfer-based sys-
tems defined by context-free grammars (Lavie et al,
2003).
The synchronous grammar approach, originally
due to Shieber and Schabes (1990), supposes that T2
is generated in lockstep to T1.1 When choosing how
to expand a certain VP node in T2, a synchronous
CFG process would observe that this node is aligned
to a node VP? in T1, which had been expanded in T1
by VP? ? NP? V?. This might bias it toward choos-
ing to expand the VP in T2 as VP ? V NP, with the
new children V aligned to V? and NP aligned to NP?.
The process then continues recursively by choosing
moves to expand these children.
One can regard this stochastic process as an in-
stance of analysis-transfer-synthesis MT. Analysis
chooses a parse T1 given S1. Transfer maps the
context-free rules in T1 to rules of T2. Synthesis
1The usual presentation describes a process that generates
T1 and T2 jointly, leading to a joint model p(T2, A, T1). Divid-
ing by the marginal p(T1) gives a conditional model p(T2, A |
T1) as in (1). In the text, we directly describe an equivalent
conditional process for generating T2, A given T1.
deterministically assembles the latter rules into an
actual tree T2 and reads off its yield S2.
What is worrisome about the synchronous pro-
cess is that it can only produce trees T2 that are
perfectly isomorphic to T1. It is possible to relax
this requirement by using synchronous grammar for-
malisms more sophisticated than CFG:2 one can per-
mit unaligned nodes (Yamada and Knight, 2001),
duplicated children (Gildea, 2003)3, or alignment
between elementary trees of differing sizes rather
than between single rules (Eisner, 2003; Ding and
Palmer, 2005; Quirk et al, 2005). However, one
would need rather powerful and slow grammar for-
malisms (Shieber and Schabes, 1990; Melamed et
al., 2004), often with discontiguous constituents, to
account for all the linguistic divergences that could
arise from different movement patterns (scrambling,
wh-in situ) or free translation. In particular, a syn-
chronous grammar cannot practically allow S2 to be
any permutation of S1, as IBM Models 1?5 do.
Our alternative is to define a ?quasi-synchronous?
stochastic process. It generates T2 in a way that is
not in thrall to T1 but is ?inspired by it.? (A human
translator might be imagined to behave similarly.)
When choosing how to expand nodes of T2, we are
influenced both by the structure of T1 and by mono-
lingual preferences about the structure of T2. Just as
conditional Markov models can more easily incor-
porate global features than HMMs, we can look at
the entire tree T1 at every stage in generating T2.
2 Quasi-Synchronous Grammar
Given an input S1 or its parse T1, a quasi-
synchronous grammar (QG) constructs a monolin-
gual grammar for parsing, or generating, the possi-
ble translations S2?that is, a grammar for finding
appropriate trees T2. What ties this target-language
grammar to the source-language input? The gram-
mar provides for target-language words to take on
2When one moves beyond CFG, the derived trees T1 and
T2 are still produced from a single derivation tree, but may be
shaped differently from the derivation tree and from each other.
3For tree-to-tree alignment, Gildea proposed a clone opera-
tion that allowed subtrees of the source tree to be reused in gen-
erating a target tree. In order to preserve dynamic programming
constraints, the identity of the cloned subtree is chosen indepen-
dently of its insertion point. This breakage of monotonic tree
alignment moves Gildea?s alignment model from synchronous
to quasi-synchronous.
24
Then:1 we:2
could:3
deal:4 .:10
with:5 later:9
Chernobyl:6
some:7
time:8
Tschernobyl/NE:6
koennte/VVFIN:3
dann/ADV:1 etwas/ADV:0 spaeter/ADJ:1 an/PREP:0 kommen/VVINF:0 ./S-SYMBOL:10
Reihe/NN:0
die/ART:0
Figure 1: German and English dependency parses and their alignments from our system where German
is the target language. Tschernobyl depends on ko?nnte even though their English analogues are not in a
dependency relationship. Note the parser?s error in not attaching etwas to spa?ter.
German: Tschernobyl ko?nnte dann etwas spa?ter an die Reihe kommen .
Literally: Chernobyl could then somewhat later on the queue come.
English: Then we could deal with Chernobyl some time later .
I:1
did:2
not:3 unfortunately:4 receive:5 .:11
answer:7
an:6 to:8
question:10
this:9
Auf/PREP:8
Frage/NN:10
diese/DEM:9
habe/VHFIN:2 ich/PPRO:1 leider/ADV:4
keine/INDEF:3
Antwort/NN:7
bekommen/VVpast:5
./S-SYMBOL:11
Figure 2: Here the German sentence exhibits scrambling of the phrase auf diese Frage and negates the object
of bekommen instead of the verb itself.
German: Auf diese Frage habe ich leider keine Antwort bekommen .
Literally: To this question have I unfortunately no answer received.
English: I did not unfortunately receive an answer to this question .
25
multiple hidden ?senses,? which correspond to (pos-
sibly empty sets of) word tokens in S1 or nodes in
T1. To take a familiar example, when parsing the
English side of a French-English bitext, the word
bank might have the sense banque (financial) in one
sentence and rive (littoral) in another.
The QG4 considers the ?sense? of the former bank
token to be a pointer to the particular banque token
to which it aligns. Thus, a particular assignment of
S1 ?senses? to word tokens in S2 encodes a word
alignment.
Now, selectional preferences in the monolingual
grammar can be influenced by these T1-specific
senses. So they can encode preferences for how T2
ought to copy the syntactic structure of T1. For ex-
ample, if T1 contains the phrase banque nationale,
then the QG for generating a corresponding T2 may
encourage any T2 English noun whose sense is
banque (more precisely, T1?s token of banque) to
generate an adjectival English modifier with sense
nationale. The exact probability of this, as well as
the likely identity and position of that English mod-
ifier (e.g., national bank), may also be influenced by
monolingual facts about English.
2.1 Definition
A quasi-synchronous grammar is a monolingual
grammar that generates translations of a source-
language sentence. Each state of this monolingual
grammar is annotated with a ?sense??a set of zero
or more nodes from the source tree or forest.
For example, consider a quasi-synchronous
context-free grammar (QCFG) for generating trans-
lations of a source tree T1. The QCFG generates the
target sentence using nonterminals from the cross
product U ? 2V1 , where U is the set of monolingual
target-language nonterminals such as NP, and V1 is
the set of nodes in T1.
Thus, a binarized QCFG has rules of the form
?A,?? ? ?B, ???C, ?? (2)
?A,?? ? w (3)
where A,B,C ? U are ordinary target-language
nonterminals, ?, ?, ? ? 2V1 are sets of source tree
4By abuse of terminology, we often use ?QG? to refer to the
T1-specific monolingual grammar, although the QG is properly
a recipe for constructing such a grammar from any input T1.
nodes to which A,B,C respectively align, and w is
a target-language terminal.
Similarly, a quasi-synchronous tree-substitution
grammar (QTSG) annotates the root and frontier
nodes of its elementary trees with sets of source
nodes from 2V1 .
2.2 Taming Source Nodes
This simple proposal, however, presents two main
difficulties. First, the number of possible senses for
each target node is exponential in the number of
source nodes. Second, note that the senses are sets
of source tree nodes, not word types or absolute sen-
tence positions as in some other translation models.
Except in the case of identical source trees, source
tree nodes will not recur between training and test.
To overcome the first problem, we want further re-
strictions on the set ? in a QG state such as ?A,??. It
should not be an arbitrary set of source nodes. In the
experiments of this paper, we adopt the simplest op-
tion of requiring |?| ? 1. Thus each node in the tar-
get tree is aligned to a single node in the source tree,
or to ? (the traditional NULL alignment). This allows
one-to-many but not many-to-one alignments.
To allow many-to-many alignments, one could
limit |?| to at most 2 or 3 source nodes, perhaps fur-
ther requiring the 2 or 3 source nodes to fall in a par-
ticular configuration within the source tree, such as
child-parent or child-parent-grandparent. With that
configurational requirement, the number of possi-
ble senses ? remains small?at most three times the
number of source nodes.
We must also deal with the menagerie of differ-
ent source tree nodes in different sentences. In other
words, how can we tie the parameters of the different
QGs that are used to generate translations of differ-
ent source sentences? The answer is that the proba-
bility or weight of a rule such as (2) should depend
on the specific nodes in ?, ?, and ? only through
their properties?e.g., their nonterminal labels, their
head words, and their grammatical relationship in
the source tree. Such properties do recur between
training and test.
For example, suppose for simplicity that |?| =
|?| = |?| = 1. Then the rewrite probabilities of (2)
and (3) could be log-linearly modeled using features
that ask whether the single node in ? has two chil-
dren in the source tree; whether its children in the
26
source are the nodes in ? and ?; whether its non-
terminal label in the source is A; whether its fringe
in the source translates as w; and so on. The model
should also consider monolingual features of (2) and
(3), evaluating in particular whether A ? BC is
likely in the target language.
Whether rule weights are given by factored gener-
ative models or by naive Bayes or log-linear models,
we want to score QG productions with a small set of
monolingual and bilingual features.
2.3 Synchronous Grammars Again
Finally, note that synchronous grammar is a special
case of quasi-synchronous grammar. In the context-
free case, a synchronous grammar restricts senses to
single nodes in the source tree and the NULL node.
Further, for any k-ary production
?X0, ?0? ? ?X1, ?1? . . . ?Xk, ?k?
a synchronous context-free grammar requires that
1. (?i 6= j) ?i 6= ?j unless ?i = NULL,
2. (?i > 0) ?i is a child of ?0 in the source tree,
unless ?i = NULL.
Since NULL has no children in the source tree, these
rules imply that the children of any node aligned to
NULL are themselves aligned to NULL. The con-
struction for synchronous tree-substitution and tree-
adjoining grammars goes through similarly but op-
erates on the derivation trees.
3 Parameterizing a QCFG
Recall that our goal is a conditional model of
p(T2, A | T1). For the remainder of this paper, we
adopt a dependency-tree representation of T1 and
T2. Each tree node represents a word of the sentence
together with a part-of-speech tag. Syntactic depen-
dencies in each tree are represented directly by the
parent-child relationships.
Why this representation? First, it helps us con-
cisely formulate a QG translation model where the
source dependencies influence the generation of tar-
get dependencies (see figure 3). Second, for evalu-
ation, it is trivial to obtain the word-to-word align-
ments from the node-to-node alignments. Third, the
part-of-speech tags are useful backoff features, and
in fact play a special role in our model below.
When stochastically generating a translation T2,
our quasi-synchronous generative process will be in-
fluenced by both fluency and adequacy. That is, it
considers both the local well-formedness of T2 (a
monolingual criterion) and T2?s local faithfulness
to T1 (a bilingual criterion). We combine these in
a simple generative model rather than a log-linear
model. When generating the children of a node in
T2, the process first generates their tags using mono-
lingual parameters (fluency), and then fills in in the
words using bilingual parameters (adequacy) that se-
lect and translate words from T1.5
Concretely, each node in T2 is labeled by a triple
(tag, word, aligned word). Given a parent node
(p, h, h?) in T2, we wish to generate sequences of
left and right child nodes, of the form (c, a, a?).
Our monolingual parameters come from a simple
generative model of syntax used for grammar induc-
tion: the Dependency Model with Valence (DMV) of
Klein and Manning (2004). In scoring dependency
attachments, DMV uses tags rather than words. The
parameters of the model are:
1. pchoose(c | p, dir): the probability of generat-
ing c as the next child tag in the sequence of
dir children, where dir ? {left, right}.
2. pstop(s | h, dir, adj): the probability of gener-
ating no more child tags in the sequence of dir
children. This is conditioned in part on the ?ad-
jacency? adj ? {true, false}, which indicates
whether the sequence of dir children is empty
so far.
Our bilingual parameters score word-to-word
translation and aligned dependency configurations.
We thus use the conditional probability ptrans(a |
a?) that source word a?, which may be NULL, trans-
lates as target word a. Finally, when a parent word
h aligned to h? generates a child, we stochastically
decide to align the child to a node a? in T1 with
one several possible relations to h?. A ?monotonic?
dependency alignment, for example, would have
h? and a? in a parent-child relationship like their
target-tree analogues. In different versions of the
model, we allowed various dependency alignment
configurations (figure 3). These configurations rep-
5This division of labor is somewhat artificial, and could be
remedied in a log-linear model, Naive Bayes model, or defi-
cient generative model that generates both tags and words con-
ditioned on both monolingual and bilingual context.
27
resent cases where the parent-child dependency be-
ing generated by the QG in the target language maps
onto source-language child-parent, for head swap-
ping; the same source node, for two-to-one align-
ment; nodes that are siblings or in a c-command re-
lationship, for scrambling and extraposition; or in
a grandparent-grandchild relationship, e.g. when a
preposition is inserted in the source language. We
also allowed a ?none-of-the-above? configuration, to
account for extremely mismatched sentences.
The probability of the target-language depen-
dency treelet rooted at h is thus:
P (D(h) | h, h?, p) =
?
dir?{l,r}
?
c?depsD(p,dir)
P (D(c) | a, a?, c) ? pstop(nostop | p, dir, adj)
?pchoose(c | p, dir)
?pconfig(config) ? ptrans(a | a
?)
pstop(stop | p, dir, adj)
4 Experiments
We claim that for modeling human-translated bitext,
it is better to project syntax only loosely. To evaluate
this claim, we train quasi-synchronous dependency
grammars that allow progressively more divergence
from monotonic tree alignment. We evaluate these
models on cross-entropy over held-out data and on
error rate in a word-alignment task.
One might doubt the use of dependency trees
for alignment, since Gildea (2004) found that con-
stituency trees aligned better. That experiment, how-
ever, aligned only the 1-best parse trees. We too will
consider only the 1-best source tree T1, but in con-
strast to Gildea, we will search for the target tree T2
that aligns best with T1. Finding T2 and the align-
ment is simply a matter of parsing S2 with the QG
derived from T1.
4.1 Data and Training
We performed our modeling experiments with the
German-English portion of the Europarl European
Parliament transcripts (Koehn, 2002). We obtained
monolingual parse trees from the Stanford German
and English parsers (Klein and Manning, 2003).
Initial estimates of lexical translation probabilities
came from the IBM Model 4 translation tables pro-
duced by GIZA++ (Brown et al, 1993; Och and
Ney, 2003).
All text was lowercased and numbers of two or
more digits were converted to an equal number of
hash signs. The bitext was divided into training
sets of 1K, 10K, and 100K sentence pairs. We held
out one thousand sentences for evaluating the cross-
entropy of the various models and hand-aligned
100 sentence pairs to evaluate alignment error rate
(AER).
We trained the model parameters on bitext using
the Expectation-Maximization (EM) algorithm. The
T1 tree is fully observed, but we parse the target lan-
guage. As noted, the initial lexical translation proba-
bilities came from IBM Model 4. We initialized the
monolingual DMV parameters in one of two ways:
using either simple tag co-occurrences as in (Klein
andManning, 2004) or ?supervised? counts from the
monolingual target-language parser. This latter ini-
tialization simulates the condition when one has a
small amount of bitext but a larger amount of tar-
get data for language modeling. As with any mono-
lingual grammar, we perform EM training with the
Inside-Outside algorithm, computing inside prob-
abilities with dynamic programming and outside
probabilities through backpropagation.
Searching the full space of target-language depen-
dency trees and alignments to the source tree con-
sumed several seconds per sentence. During train-
ing, therefore, we constrained alignments to come
from the union of GIZA++ Model 4 alignments.
These constraints were applied only during training
and not during evaluation of cross-entropy or AER.
4.2 Conditional Cross-Entropy of the Model
To test the explanatory power of our QCFG, we eval-
uated its conditional cross-entropy on held-out data
(table 1). In other words, we measured how well a
trained QCFG could predict the true translation of
novel source sentences by summing over all parses
of the target given the source. We trained QCFG
models under different conditions of bitext size and
parameter initialization. However, the principal in-
dependent variable was the set of dependency align-
ment configurations allowed.
From these cross-entropy results, it is clear that
strictly synchronous grammar is unwise. We ob-
28
(a) parent-child (b) child-parent (c) same node
sehe
ich
see
I
schwimmt
gern swimming
likes Voelkerrecht law
international
(d) siblings (e) grandparent-grandchild (f) c-commandbekommen
auf Antwort to
answer Wahlkampf
von
campaign
2003
2003
sagte
Was dass what
kaufte
bought
Figure 3: When a head h aligned to h? generates a new child a aligned to a? under the QCFG, h? and a? may be related in the
source tree as, among other things, (a) parent?child, (b) child?parent, (c) identical nodes, (d) siblings, (e) grandparent?grandchild,
(f) c-commander?c-commandee, (g) none of the above. Here German is the source and English is the target. Case (g), not pictured
above, can be seen in figure 1, in English-German order, where the child-parent pair Tschernobyl ko?nnte correspond to the words
Chernobyl and could, respectively. Since could dominates Chernobyl, they are not in a c-command relationship.
Permitted configurations CE CE CE
at 1k 10k 100k
? or parent-child (a) 43.82 22.40 13.44
+ child-parent (b) 41.27 21.73 12.62
+ same node (c) 41.01 21.50 12.38
+ all breakages (g) 35.63 18.72 11.27
+ siblings (d) 34.59 18.59 11.21
+ grandparent-grandchild (e) 34.52 18.55 11.17
+ c-command (f) 34.46 18.59 11.27
No alignments allowed 60.86 53.28 46.94
Table 1: Cross-entropy on held-out data with different depen-
dency configurations (figure 3) allowed, for 1k, 10k, and 100k
training sentences. The big error reductions arrive when we
allow arbitrary non-local alignments in condition (g). Distin-
guishing some common cases of non-local alignments improves
performance further. For comparison, we show cross-entropy
when every target language node is unaligned.
tain comparatively poor performance if we require
parent-child pairs in the target tree to align to parent-
child pairs in the source (or to parent-NULL or
NULL-NULL). Performance improves as we allow
and distinguish more alignment configurations.
4.3 Word Alignment
We computed standard measures of alignment preci-
sion, recall, and error rate on a test set of 100 hand-
aligned German sentence pairs with 1300 alignment
links. As with many word-alignment evaluations,
we do not score links to NULL. Just as for cross-
entropy, we see that more permissive alignments
lead to better performance (table 2).
Having selected the best system using the cross-
entropy measurement, we compare its alignment er-
ror rate against the standard GIZA++ Model 4 base-
lines. As Figure 4 shows, our QCFG for German ?
English consistently produces better alignments than
the Model 4 channel model for the same direction,
German ? English. This comparison is the appro-
priate one because both of these models are forced
to align each English word to at most one German
word. 6
5 Conclusions
With quasi-synchronous grammars, we have pre-
sented a new approach to syntactic MT: construct-
ing a monolingual target-language grammar that de-
scribes the aligned translations of a source-language
sentence. We described a simple parameterization
6For German ? English MT, one would use a German ?
English QCFG as above, but an English ? German channel
model. In this arguably inappropriate comparison, Figure 4
shows, the Model 4 channel model produces slightly better
word alignments than the QG.
29
Permitted configurations AER AER AER
at 1k 10k 100k
? or parent-child (a) 40.69 39.03 33.62
+ child-parent (b) 43.17 39.78 33.79
+ same node (c) 43.22 40.86 34.38
+ all breakages (g) 37.63 30.51 25.99
+ siblings (d) 37.87 33.36 29.27
+ grandparent-grandchild (e) 36.78 32.73 28.84
+ c-command (f) 37.04 33.51 27.45
Table 2: Alignment error rate (%) with different dependency
configurations allowed.
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 1000  10000  100000  1e+06
alig
nm
ent
 err
or r
ate
training sentence pairs
QCFGGiza4Giza4 bk
Figure 4: Alignment error rate with best model (all break-
ages). The QCFG consistently beat one GIZA++ model and
was close to the other.
with gradually increasing syntactic domains of lo-
cality, and estimated those parameters on German-
English bitext.
The QG formalism admits many more nuanced
options for features than we have exploited. In par-
ticular, we now are exploring log-linear QGs that
score overlapping elementary trees of T2 while con-
sidering the syntactic configuration and lexical con-
tent of the T1 nodes to which each elementary tree
aligns.
Even simple QGs, however, turned out to do quite
well. Our evaluation on a German-English word-
alignment task showed them to be competitive with
IBM model 4?consistently beating the German-
English direction by several percentage points of
alignment error rate and within 1% AER of the
English-German direction. In particular, alignment
accuracy benefited from allowing syntactic break-
ages between the two dependency structures.
We are also working on a translation decoding us-
ing QG. Our first system uses the QG to find optimal
T2 aligned to T1 and then extracts a synchronous
tree-substitution grammar from the aligned trees.
Our second system searches a target-language vo-
cabulary for the optimal T2 given the input T1.
Acknowledgements
This work was supported by a National Science
Foundation Graduate Research Fellowship for the
first author and by NSF Grant No. 0313193.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning
dependency translation models as collections of finite state
head transducers. CL, 26(1):45?60.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. CL, 19(2):263?311.
Y. Ding and M. Palmer. 2005. Machine translation using prob-
abilistic synchronous dependency insertion grammars. In
ACL, pages 541?548.
B. J. Dorr. 1994. Machine translation divergences: A formal
description and proposed solution. Computational Linguis-
tics, 20(4):597?633.
J. Eisner. 2003. Learning non-isomorphic tree mappings for
machine translation. In ACL Companion Vol.
H. J. Fox. 2002. Phrasal cohesion and statistical machine trans-
lation. In EMNLP, pages 392?399.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In ACL, pages 80?87.
D. Gildea. 2004. Dependencies vs. constituents for tree-based
alignment. In EMNLP, pages 214?221.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. Evalu-
ating translational correspondence using annotation projec-
tion. In ACL.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL, pages 423?430.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL, pages 479?486.
P. Koehn. 2002. Europarl: A multilingual
corpus for evaluation of machine translation.
http://www.iccs.informatics.ed.ac.uk/?pkoehn/-
publications/europarl.ps.
A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst, A. F.
Llitjo?s, R. Reynolds, J. Carbonell, and R. Cohen. 2003. Ex-
periments with a Hindi-to-English transfer-basedMT system
under a miserly data scenario. ACM Transactions on Asian
Language Information Processing, 2(2):143 ? 163.
I. D. Melamed, G. Satta, and B. Wellington. 2004. Generalized
multitext grammars. In ACL, pages 661?668.
I. D. Melamed. 2004. Statistical machine translation by pars-
ing. In ACL, pages 653?660.
F. J. Och and H. Ney. 2003. A systematic comparison of various
statistical alignment models. CL, 29(1):19?51.
C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency
treelet translation: Syntactically informed phrasal SMT. In
ACL, pages 271?279.
S. M. Shieber and Y. Schabes. 1990. Synchronous tree-
adjoining grammars. In ACL, pages 253?258.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. CL, 23(3):377?403.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In ACL.
30
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 97?105,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Competitive Grammar Writing?
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
Just as programming is the traditional introduction to
computer science, writing grammars by hand is an ex-
cellent introduction to many topics in computational lin-
guistics. We present and justify a well-tested introductory
activity in which teams of mixed background compete
to write probabilistic context-free grammars of English.
The exercise brings together symbolic, probabilistic, al-
gorithmic, and experimental issues in a way that is acces-
sible to novices and enjoyable.
1 Introduction
We describe a hands-on group activity for novices
that introduces several central topics in computa-
tional linguistics (CL). While the task is intellec-
tually challenging, it requires no background other
than linguistic intuitions, no programming,1 and
only a very basic understanding of probability.
The activity is especially appropriate for mixed
groups of linguists, computer scientists, and others,
letting them collaborate effectively on small teams
and learn from one another. A friendly competition
among the teams makes the activity intense and en-
joyable and introduces quantitative evaluation.
1.1 Task Overview
Each 3-person team is asked to write a generative
context-free grammar that generates as much of En-
? This work was supported by NSF award 0121285,
?ITR/IM+PE+SY: Summer Workshops on Human Language
Technology: Integrating Research and Education,? and by a
Fannie and John Hertz Foundation Fellowship to the second
author. We thank David A. Smith and Markus Dreyer for co-
leading the lab in 2004?2007 and for implementing various im-
provements in 2004?2007 and for providing us with data from
those years. The lab has benefited over the years from feedback
from the participants, many of whom attended the JHU sum-
mer school thanks to the generous support of NAACL. We also
thank the anonymous reviewers for helpful comments.
1In our setup, students do need the ability to invoke scripts
and edit files in a shared directory, e.g., on a Unix system.
glish as possible (over a small fixed vocabulary).
Obviously, writing a full English grammar would
take years even for experienced linguists. Thus each
team will only manage to cover a few phenomena,
and imperfectly.
To encourage precision but also recall and lin-
guistic creativity, teams are rewarded for generating
sentences that are (prescriptively) grammatical but
are not anticipated by other teams? grammars. This
somewhat resembles scoring in the Boggle word
game, where players are rewarded for finding valid
words in a grid that are not found by other players.
A final twist is that the exercise uses probabilistic
context-free grammars (PCFGs); the actual scoring
methods are based on sampling and cross-entropy.
Each team must therefore decide how to allocate
probability mass among sentences. To avoid assign-
ing probability of zero when attempting to parse an-
other team?s sentences, a team is allowed to ?back
off? (when parsing) to a simpler probability model,
such as a part-of-speech bigram model, also ex-
pressed as a PCFG.
1.2 Setting
We have run this activity for six consecutive years,
as a laboratory exercise on the very first afternoon
of an intensive 2-week summer school on various
topics in human language technology.2 We allot 3.5
hours in this setting, including about 15 minutes for
setup, 30 minutes for instructions, 15 minutes for
evaluation, and 30 minutes for final discussion.
The remaining 2 hours is barely enough time for
team members to get acquainted, understand the re-
quirements, plan a strategy, and make a small dent in
2This 2-week course is offered as a prelude to the Johns
Hopkins University summer research workshops, sponsored by
the National Science Foundation and the Department of De-
fense. In recent years the course has been co-sponsored by the
North American ACL.
97
the problem. Nonetheless, participants consistently
tell us that the exercise is enjoyable and pedagogi-
cally effective, almost always voting to stay an extra
hour to make further progress.
Our 3-person teams have consisted of approxi-
mately one undergraduate, one junior graduate stu-
dent, and one more senior graduate student. If pos-
sible, each team should include at least one member
who has basic familiarity with some syntactic phe-
nomena and phrasal categories. Teams that wholly
lack this experience have been at a disadvantage in
the time-limited setting.
1.3 Resources for Instructors
We will maintain teaching materials at http:
//www.clsp.jhu.edu/grammar-writing,
for both the laboratory exercise version and for
homework versions: scripts, data, instructions for
participants, and tips for instructors. While our
materials are designed for participants who are
fluent in English, we would gladly host translations
or adaptations into other languages, as well as other
variants and similar assignments.
2 Why Grammar Writing?
A computer science curriculum traditionally starts
with programming, because programming is acces-
sible, hands-on, and necessary to motivate or under-
stand most other topics in computer science. We be-
lieve that grammar writing should play the same role
in computational linguistics?as it often did before
the statistical revolution3?and for similar reasons.
Grammar writing remains central because many
theoretical and applied CL topics center around
grammar formalisms. Much of the field tries to de-
sign expressive formalisms (akin to programming
languages); solve linguistic problems within them
(akin to programming); enrich them with probabil-
ities; process them with efficient algorithms; learn
them from data; and connect them to other modules
in the linguistic processing pipeline.
3The first author was specifically inspired by his experience
writing a grammar in Bill Woods?s NLP course at Harvard in
1987. An anonymous reviewer remarks that such assignments
were common at the time. Our contributions are to introduce
statistical and finite-state elements, to make the exercise into a
game, and to provide reusable instructional materials.
Of course, there are interesting grammar for-
malisms at all levels of language processing. One
might ask why syntax is a good level at which to be-
gin education in computational linguistics.
First, starting with syntax establishes at the start
that there are formal and computational methods
specific to natural language. Computational linguis-
tics is not merely a set of applied tasks to be solved
with methods already standardly taught in courses
on machine learning, theory of computation,4 or
knowledge representation.
Second, we have found that syntax captures stu-
dents? interest rapidly. They quickly appreciate the
linguistic phenomena, see that they are non-trivial,
and have little trouble with the CFG formalism.
Third, beginning specifically with PCFGs pays
technical dividends in a CL course. Once one un-
derstands PCFG models, it is easy to understand the
simpler finite-state models (including n-gram mod-
els, HMMs, etc.) and their associated algorithms, ei-
ther by analogy or by explicit reduction to special
cases of PCFGs. CFGs are also a good starting point
for more complex syntactic formalisms (BNF, cate-
gorial grammars, TAG, LFG, HPSG, etc.) and for
compositional semantics. Indeed, our exercise mo-
tivates these more complex formalisms by forcing
students to work with the more impoverished PCFG
formalism and experience its limitations.
3 Educational Goals of the Exercise
Our grammar-writing exercise is intended to serve
as a touchstone for discussion of many subsequent
topics in NLP and CL (which are italicized below).
As an instructor, one can often refer back later to the
exercise to remind students of their concrete experi-
ence with a given concept.
Generative probabilistic models. The first set of
concepts concerns language models. These are eas-
iest to understand as processes for generating text.
Thus, we give our teams a script for generating ran-
dom sentences from their grammar and their backoff
4Courses on theory of computation do teach pushdown au-
tomata and CFGs, of course, but they rarely touch on parsing
or probabilistic grammars, as this exercise does. Courses on
compilers may cover parsing algorithms, but only for restricted
grammar families such as unambiguous LR(1) grammars.
98
model?a helpful way to observe the generative ca-
pacity and qualitative behavior of any model.
Of course, in practice a generative grammar is
most often run ?backwards? to parse an observed
sentence or score its inside probability, and we also
give the teams a script to do that. Most teams do ac-
tually run these scripts repeatedly to test their gram-
mars, since both scripts will be central in the evalua-
tion (where sentences are randomly generated from
one grammar and scored with another grammar).
It is common for instructors of NLP to show ex-
amples of randomly-generated text from an n-gram
model (e.g., Jurafsky and Martin, 2000, pp. 202?
203), yet this amusing demonstration may be misin-
terpreted as merely illustrating the inadequacy of n-
gram models. Our use of a hand-crafted PCFG com-
bined with a bigram-based (HMM) backoff grammar
demonstrates that although the HMM is much worse
at generating valid English sentences (precision), it
is much better at robustly assigning nonzero proba-
bility when analyzing English sentences (recall).
Finally, generative models do more than assign
probability. They often involve linguistically mean-
ingful latent variables, which can be recovered
given the observed data. Parsing with an appropri-
ate PCFG thus yields a intuitive and useful analy-
sis (a syntactic parse tree), although only for the
sentences that the PCFG covers. Even parsing with
the simple backoff grammar that we initially provide
yields some coarser analysis, in the form of a part-
of-speech tagging, since this backoff grammar is a
right-branching PCFG that captures part-of-speech
bigrams (for details see ?1.1, ?4.1, and Table 2). In
fact, parsing with the backoff PCFG is isomorphic to
Viterbi decoding in an HMM part-of-speech tagger,
a topic that is commonly covered in NLP courses.
Modeling grammaticality. The next set of con-
cepts concerns linguistic grammaticality. During the
evaluation phase of our exercise (see below), stu-
dents must make grammaticality judgments on other
teams? randomly generated sentences?which are
usually nonsensical, frequently hard for humans to
parse, and sometimes ungrammatical. This concrete
task usually prompts questions from students about
how grammaticality ought to be defined, both for
purposes of the task and in principle. It could also
be used to discuss why some of the sentences are
so hard for humans to understand (e.g., garden-path
and frequency effects) and what parsing strategies
humans or machines might use.
The exercise of modeling grammaticality with
the CFG formalism, a formalism that appears else-
where in the computer science curriculum, high-
lights some important differences between natural
languages and formal languages. A natural lan-
guage?s true grammar is unknown (and may not even
exist: perhaps the CFG formalism is inadequate).
Rather, a grammar must be induced or constructed
as an approximate model of corpus data and/or cer-
tain native-speaker intuitions. A natural language
also differs from a programming language in includ-
ing ambiguous sentences. Students observe that the
parser uses probabilities to resolve ambiguity.
Linguistic analysis. Grammar writing is an ex-
cellent way to get students thinking about linguis-
tic phenomena (e.g., adjuncts, embedded sentences,
wh-questions, clefts, point absorption of punctuation
marks). It also forces students to think about appro-
priate linguistic formalisms. Many phenomena are
tedious to describe within CFGs (e.g., agreement,
movement, subcategorization, selectional restric-
tions, morphological inflection, and phonologically-
conditioned allomorphy such as a vs. an). They
can be treated in CFGs only with a large number of
repetitive rules. Students appreciate these problems
by grappling with them, and become very receptive
to designing expressive improvements such as fea-
ture structures and slashed categories.
Parameter tuning. Students observe the effects
of changing the rule probabilities by running the
scripts. For example, teams often find themselves
generating unreasonably long (or even infinite) sen-
tences, and must damp down the probabilities of
their recursive rules. Adjusting the rule probabilities
can also change the score and optimal tree that are
returned by the parser, and can make a big difference
in the final evaluation (see ?5). This appreciation for
the role of numerical parameters helps motivate fu-
ture study of machine learning in NLP.
Quantitative evaluation. As an engineering pur-
suit, NLP research requires objective evaluation
measures to know how well systems work.
Our first measure is the precision of each team?s
99
probabilistic grammar: how much of its probability
mass is devoted to sentences that are truly grammat-
ical? Estimating this requires human grammaticality
judgments on a random sample C of sentences gen-
erated from all teams? grammars. These binary judg-
ments are provided by the participants themselves,
introducing the notion of linguistic annotation (al-
beit of a very simple kind). Details are in ?4.3.3.
Our second measure is an upper-bound approx-
imation to cross-entropy (or log-perplexity?in ef-
fect, the recall of a probability model): how well
does each team?s probabilistic model (this time in-
cluding the backoff model of ?1.1) anticipate unseen
data that are truly grammatical? (Details in ?4.3.3.)
Note that in contrast to parsing competitions, we
do not evaluate the quality of the parse trees (e.g.,
PARSEVAL). Our cross-entropy measure evaluates
only the grammars? ability to predict word strings
(language modeling). That is because we impose no
annotation standard for parse trees: each team is free
to develop its own theory of syntax. Furthermore,
many sentences will only be parsable by the backoff
grammar (e.g., a bigram model), which is not ex-
pected to produce a full syntactic analysis.
The lesson about cross-entropy evaluation is
slightly distorted by our peculiar choice of test data.
In principle, the instructors might prepare a batch
of grammatical sentences ahead of time and split
them into a test set (used to evaluate cross-entropy
at the end) and a development set (provided to the
students at the start, so that they know which gram-
matical phenomena are important to handle). The
activity could certainly be run in this way to demon-
strate proper experimental design for evaluating a
language model (discussed further in ?5 and ?6).
We have opted for the more entertaining ?Boggle-
style? evaluation described in ?1.1, where teams try
to stump one another by generating difficult test
data, using the fixed vocabulary. Thus, we evaluate
each team?s cross-entropy on all grammatical sen-
tences in the collection C, which was generated ex
post facto from all teams? grammars.
4 Important Details
4.1 Data
A few elements are provided to participants to get
them started on their grammars.
Vocabulary. The terminal vocabulary ? consists
of words from early scenes of the film Monty Python
and the Holy Grail along with some inflected forms
and function words, for a total vocabulary of 220
words. For simplicity, only 3rd-person pronouns,
nouns, and verbs are included. All words are case-
sensitive for readability (as are the grammar nonter-
minals), but we do not require or expect sentence-
initial capitalization.
All teams are restricted to this vocabulary, so that
the sentences that they generate will not frustrate
other teams? parsers with out-of-vocabulary words.
However, they are free to use words in unexpected
ways (e.g., using castle in its verbal sense from
chess, or building up unusual constructions with the
available function words).
Initial lexicon. The initial lexical rules take the
form T ? w, where w ? ?+ and T ? T , with
T being a set of six coarse part-of-speech tags:
Noun: 21 singular nouns starting with consonants
Det: 9 singular determiners
Prep: 14 prepositions
Proper: 8 singular proper nouns denoting people
(including multiwords such as Sir Lancelot)
VerbT: 6 3rd-person singular present transitive
verbs
Misc: 183 other words, divided into several com-
mented sections in the grammar file
Students are free to change this tagset. They
are especially encouraged to refine the Misc tag,
which includes 3rd-person plural nouns (including
some proper nouns), 3rd-person pronouns (nomina-
tive, accusative, and genitive), additional 3rd-person
verb forms (plural present, past, stem, and partici-
ples), verbs that cannot be used transitively, modals,
adverbs, numbers, adjectives (including some com-
parative and superlative forms), punctuation, coor-
dinating and subordinating conjunctions, wh-words,
and a few miscellaneous function words (to, not, ?s).
The initial lexicon is ambiguous: some words are
associated with more than one tag. Each rule has
weight 1, meaning that a tag T is equally likely to
rewrite as any of its allowed nonterminals.
Initial grammar. We provide the ?S1? rules in Ta-
ble 1, so that students can try generating and parsing
100
1 S1 ? NP VP .
1 VP ? VerbT NP
20 NP ? Det Nbar
1 NP ? Proper
20 Nbar ? Noun
1 Nbar ? Nbar PP
1 PP ? Prep NP
Table 1: The S1 rules: a starting point for building an En-
glish grammar. The start symbol is S1. The weights in
the first column will be normalized into generative proba-
bilities; for example, the probability of expanding a given
NP with NP ? Det Nbar is actually 20/(20 + 1).
1 S2 ?
1 S2 ? Noun
1 S2 ? Misc
1 Noun ? Noun
1 Noun ? Noun Noun
1 Noun ? Noun Misc
1 Misc ? Misc
1 Misc ? Misc Noun
1 Misc ? Misc Misc
Table 2: The S2 rules (simplified here where T =
{Noun,Misc}): a starting point for a backoff grammar.
The start symbol is S2. The Noun nonterminal gener-
ates those phrases that start with Nouns. Its 3 rules mean
that following a Noun, there is 1/3 probability each of
stopping, continuing with another Noun (via Noun), or
continuing with a Misc word (via Misc).
sentences right away. The S1 and lexical rules to-
gether implement a very small CFG. Note that no
Misc words can yet be generated. Indeed, this ini-
tial grammar will only generate some simple gram-
matical SVO sentences in singular present tense, al-
though they may be unboundedly long and ambigu-
ous because of recursion through Nbar and PP.
Initial backoff grammar. The provided ?S2?
grammar is designed to assign positive probability
to any string in ?? (see ?1.1). At least initially, this
PCFG generates only right-branching structures. Its
nonterminals correspond to the states of a weighted
finite-state machine, with start state S2 and one state
per element of T (the coarse parts of speech listed
above). Table 2 shows a simplified version.
From each state, transition into any state except
the start state S2 is permitted, and so is stopping.
These rules can be seen as specifying the transitions
Arthur is the king .
Arthur rides the horse near the castle .
riding to Camelot is hard .
do coconuts speak ?
what does Arthur ride ?
who does Arthur suggest she carry ?
are they suggesting Arthur ride to Camelot ?
Guinevere might have known .
it is Sir Lancelot who knows Zoot !
neither Sir Lancelot nor Guinevere will speak of it .
the Holy Grail was covered by a yellow fruit .
do not speak !
Arthur will have been riding for eight nights .
Arthur , sixty inches , is a tiny king .
Arthur and Guinevere migrate frequently .
he knows what they are covering with that story .
the king drank to the castle that was his home .
when the king drinks , Patsy drinks .
Table 3: Example sentences. Only the first two can be
parsed by the initial S1 and lexical rules.
in a bigram hidden Markov model (HMM) on part-
of-speech tags, whose emissions are specified by the
lexical rules. Since each rule initially has weight 1,
all part-of-speech sequences of a given length are
equally likely, but these weights could be changed
to arbitrary transition probabilities.
Start rules. The initial grammar S1 and the ini-
tial backoff grammar S2 are tied together by a single
symbol START, which has two production rules:
99 START ? S1
1 START ? S2
These two rules are obligatory, but their weights
may be changed. The resulting model, rooted at
START, is a mixture of the S1 and S2 grammars,
where the weights of these two rules implement
the mixture coefficients. This is a simple form of
backoff smoothing by linear interpolation (Jelinek
and Mercer, 1980). The teams are warned to pay
special attention to these rules. If the weight of
START ? S1 is decreased relative to START ?
S2, then the model relies more heavily on the back-
off model?perhaps a wise choice for keeping cross-
entropy small, if the team has little faith in S1?s abil-
ity to parse the forthcoming data.
Sample sentences. A set of 27 example sentences
in ?+ (subset shown in Table 3) is provided for lin-
guistic inspiration and as practice data on which to
101
run the parser. Since only 2 of these sentences can
be parsed with the initial S1 and lexical rules, there
is plenty of room for improvement. A further devel-
opment set is provided midway through the exercise
(?4.3.2).
4.2 Computing Environment
We now describe how the above data are made avail-
able to students along with some software.
4.2.1 Scripts
We provide scripts that implement two important
capabilities for PCFG development. Both scripts
are invoked with a set of grammar files specified on
the command line (typically all of them, ?*.gr?).
A PCFG is obtained by concatenating these files
and stripping their comments, then normalizing their
rule weights into probabilities (see Table 1), and
finally checking that all terminal symbols of this
PCFG are legal words of the vocabulary ?.
The random generation script prints a sample of
n sentences from this PCFG. The generator can op-
tionally print trees or flat word sequences. A start
symbol other than the default S1 may be specified
(e.g., NP, S2, START, etc.), to allow participants to
test subgrammars or the backoff grammar.5
The parsing script prints the most probable parse
tree for each sentence read from a file (or from the
standard input). A start symbol may again be speci-
fied; this time the default is START. The parser also
prints each sentence?s probability, total number of
parses, and the fraction of the probability that goes
to the best parse.
Tree outputs can be pretty-printed for readability.
4.2.2 Collaborative Setup
Teams of three or four sit at adjacent workstations
with a shared filesystem. The scripts above are pub-
licly installed; a handout gives brief usage instruc-
tions. The instructor and teaching assistant roam the
room and offer assistance as needed.
Each team works in its own shared directory. The
Emacs editor will warn users who are simultane-
ously editing the same file. Individual participants
tend to work on different sub-grammar files; all of
5For some PCFGs, the stochastic process implemented by
the script has a nonzero probability of failing to terminate. This
has not caused problems to date.
a team?s files can be concatenated (as *.gr) when
the scripts are run. (The directory initially includes
separate files for the S1 rules, S2 rules, and lexi-
cal rules.) To avoid unexpected interactions among
these grammar fragments, students are advised to di-
vide work based on nonterminals; e.g., one member
of a team may claim jurisdiction over all rules of the
form VP plural ? ? ? ?.
4.3 Activities
4.3.1 Introductory Lecture
Once students have formed themselves into teams
and managed to log in at adjacent computers, we
begin with an 30-minute introductory lecture. No
background is assumed. We explain PCFGs simply
by showing the S1 grammar and hand-simulating the
action of the random sentence generator.
We explain the goal of extending the S1 gram-
mar to cover more of English. We explain how each
team?s precision will be evaluated by human judg-
ments on a sample, but point out that this measure
gives no incentive to increase coverage (recall). This
motivates the ?Boggle? aspect of the game, where
teams must also be able to parse one another?s gram-
matical sentences, and indeed assign them as high
a probability as possible. We demonstrate how the
parser assigns a probability by running it on the sen-
tence that we earlier generated by hand.6
We describe how the parser?s probabilities are
turned into a cross-entropy measure, and discuss
strategy. Finally, we show that parsing a sentence
that is not covered by the S1 grammar will lead to
infinite cross-entropy, and we motivate the S2 back-
off grammar as an escape hatch.
4.3.2 Midpoint: Development data
Once or more during the course of the exercise,
we take a snapshot of all teams? S1 grammars and
sample 50 sentences from each. The resulting col-
lection of sentences, in random order, is made avail-
able to all teams as a kind of development data.
While we do not filter for grammaticality as in the
final evaluation, this gives all participants an idea
of what they will be up against when it comes time
6The probability will be tiny, as a product of many rule prob-
abilities. But it may be higher than expected, and students are
challenged to guess why: there are additional parses beyond the
one we hand-generated, and the parser sums over all of them.
102
to parse other teams? sentences. Teams are on their
honor not to disguise the true state of their grammar
at the time of the snapshot.
4.3.3 Evaluation procedure
Grammar development ends at an announced
deadline. The grammars are now evaluated on the
two measures discussed in ?3. The instructors run a
few scripts that handle most of this work.
First, we generate a collection C by sampling 20
sentences from each team?s probabilistic grammar,
using S1 as the start symbol. (Thus, the backoff S2
grammar is not used for generation.)
We now determine, for each team, what fraction
of its 20-sentence sample was grammatical. The par-
ticipants play the role of grammaticality judges. In
our randomized double-blind procedure, each indi-
vidual judge receives (in his or her team directory)
a file of about 20 sentences from C, with instruc-
tions to delete the ungrammatical ones and save the
file, implying coarse Boolean grammaticality judg-
ments.7 The files are constructed so that each sen-
tence in C is judged by 3 different participants; a
sentence is considered grammatical if ? 2 judges
thinks that it is.
We define the test corpus C? to consist of all sen-
tences in C that were judged grammatical. Each
team?s full grammar (using START as the start sym-
bol to allow backoff) is used to parse C?. This
gives us the log2-probability of each sentence in C?;
the cross-entropy score is the sum of these log2-
probabilities divided by the length of C?.
4.3.4 Group discussion
While the teaching assistant is running the evalua-
tion scripts and compiling the results, the instructor
leads a general discussion. Many topics are possi-
ble, according to the interests of the instructor and
participants. For example: What linguistic phenom-
ena did the teams handle, and how? Was the CFG
formalism adequately expressive? How well would
it work for languages other than English?
What strategies did the teams adopt, based on the
evaluation criteria? How were the weights chosen?
7Judges are on their honor to make fair judgments rather
than attempt to judge other teams? sentences ungrammatical.
Moreover, such an attempt might be self-defeating, as they
might unknowingly be judging some of their own team?s sen-
tences ungrammatical.
cross-entropy new rules
team precision (bits/sent.) lex. other
A 0.30 35.57 202 111
B 0.00 54.01 304 80
C 0.80 38.48 179 48
D 0.25 49.37 254 186
E 0.55 39.59 198 114
F 0.00 39.56 193 37
G 0.65 40.97 71 15
H 0.30 36.53 176 9
I 0.70 36.17 181 54
J 0.00 ? 193 29
Table 4: Teams? evaluation scores in one year, and the
number of new rules (not including weight changes) that
they wrote. Only teams A and H modified the relative
weights of the START rules (they used 80/20 and 75/25,
respectively), giving them competitive perplexity scores.
(Cross-entropy in this year was approximated by an upper
bound that uses only the probability of each sentence?s
single best parse.)
How would you build a better backoff grammar?8
How would you organize a real long-term effort
to build a full English grammar? What would such a
grammar be good for? Would you use any additional
tools, data, or evaluation criteria?
5 Outcomes
Table 4 shows scores achieved in one year (2002).
A valuable lesson for the students was the impor-
tance of backoff. None but the first two of the exam-
ple sentences (Table 3) are parseable with the small
S1 grammar. Thus, the best way to reduce perplexity
was to upweight the S2 grammar and perhaps spend
a little time improving its rules or weights. Teams
that spent all of their time on the S1 grammar may
have learned a lot about linguistics, but tended to
score poorly on perplexity.
Indeed, the winning team in a later year spent
nearly all of their effort on the S2 grammar. They
placed almost all their weight on the S2 grammar,
whose rules they edited and whose parameters they
estimated from the example sentences and develop-
ment data. As for their S1 grammar, it generated
only a small set of grammatical sentences with ob-
8E.g., training the model weights, extending it to trigrams,
or introducing syntax into the S2 model by allowing it to invoke
nonterminals of the S1 grammar.
103
scure constructions that other teams were unlikely to
model well in their S1 grammars. This gave them a
100% precision score on grammaticality while pre-
senting a difficult parsing challenge to other teams.
This team gamed our scoring system, exploiting the
idiosyncrasy that S2 would be used to parse but not
to generate. (See ?3 for an alternative system.)
We conducted a post hoc qualitative survey of the
grammars from teams in 2002. Teams were not
asked to provide comments, and nonterminal nam-
ing conventions often tend to be inscrutable, but the
intentions are mostly understandable. All 10 teams
developed more fine-grained parts of speech, includ-
ing coordinating conjunctions, modal verbs, number
words, adverbs. 9 teams implemented singular and
plural features on nouns and/or verbs, and 9 imple-
mented the distinction between base, past, present,
and gerund forms of verbs (or a subset of those). 7
teams brought in other features like comparative and
superlative adjectives and personal vs. possessive
pronouns. 4 teams modeled pronoun case. Team
C created a ?location? category.
7 teams explicitly tried to model questions, of-
ten including rules for do-support; 3 of those teams
also modeled negation with do-insertion. 2 teams
used gapped categories (team D used them exten-
sively), and 7 teams used explicit X? nonterminals,
most commonly within noun phrases (following the
initial grammar). Three teams used a rudimentary
subcategorization frame model, distinguishing be-
tween sentence-taking, transitive, and intransitive
verbs, with an exploded set of production rules as
a result. Team D modeled appositives.
The amount of effort teams put into weights var-
ied, as well. Team A used 11 distinct weight values
from 1 to 80, giving 79 rules weights> 1 (next clos-
est was team 10, with 7 weight values in [1, 99] and
only 43 up-weighted rules). Most teams set fewer
than 25 rules? weights to something other than 1.
6 Use as a Homework Assignment
Two hours is not enough time to complete a good
grammar. Our participants are ambitious but never
come close to finishing what they undertake; Table 4
reflects incomplete work. Nonetheless, we believe
that the experience still successfully fulfills many of
the goals of ?2?3 in a short time, and the participants
enjoy the energy in a roomful of peers racing toward
a deadline. The fact that the task is open-ended and
clearly impossible keeps the competition friendly.
An alternative would be to allot 2 weeks or more
as a homework assignment, allowing teams to go
more deeply into linguistic issues and/or backoff
modeling techniques. A team?s grade could be
linked to its performance. In this setting, we recom-
mend limiting the team size to 1 or 2 people each,
since larger teams may not be able to find time or
facilities to work side-by-side for long.
This homework version of our exercise might
helpfully be split into two assignments:
Part 1 (non-competitive, smaller vocabulary).
?Extend the initial S1 grammar to cover a certain
small set of linguistic phenomena, as illustrated by a
development set [e.g., Table 3]. You will be eval-
uated on the cross-entropy of your grammar on a
test set that closely resembles the development set
[see ?3], and perhaps also on the acceptability of
sentences sampled from your grammar (as judged
by you, your classmates, or the instructor). You
will also receive qualitative feedback on how cor-
rectly and elegantly your grammar solves the lin-
guistic problems posed by the development set.?
Part 2 (competitive, full 220-word vocabulary).
?Extend your S1 grammar from Part 1 to generate
phenomena that stump other teams, and add an S2
grammar to avoid being stumped by them. You will
be evaluated as follows . . . [see ?4.3.3].?
We have already experimented with simpler non-
competitive grammar-writing exercises (similar to
Part 1) in our undergraduate NLP courses. Given
two weeks, even without teammates, many students
do a fine job of covering several non-trivial syntactic
phenomena. These assignments are available for use
by others (see ?1.3). In some versions, students were
asked to write their own random generator, judge
their own sentences, explain how to evaluate per-
plexity, or guess why the S2 grammar was used.
7 Conclusion
We hope that other instructors can make use of these
materials or ideas. Our competitive PCFG-writing
game touches upon many core CL concepts, is chal-
lenging and enjoyable, allows collaboration, and is
suitable for cross-disciplinary and intro courses.
104
References
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data. In
Proc. of Workshop on Pattern Recognition in Practice.
D. Jurafsky and J.H. Martin. 2000. Speech and Lan-
guage Processing. Prentice Hall.
105
Coling 2010: Poster Volume, pages 656?664,
Beijing, August 2010
Unsupervised Discriminative Language Model Training
for Machine Translation using Simulated Confusion Sets
Zhifei Li and Ziyuan Wang and Sanjeev Khudanpur and Jason Eisner
Center for Language and Speech Processing
Johns Hopkins University
zhifei.work@gmail.com,{zwang40, khudanpur, eisner}@jhu.edu
Abstract
An unsupervised discriminative training
procedure is proposed for estimating a
language model (LM) for machine trans-
lation (MT). An English-to-English syn-
chronous context-free grammar is derived
from a baseline MT system to capture
translation alternatives: pairs of words,
phrases or other sentence fragments that
potentially compete to be the translation
of the same source-language fragment.
Using this grammar, a set of impostor
sentences is then created for each En-
glish sentence to simulate confusions that
would arise if the system were to process
an (unavailable) input whose correct En-
glish translation is that sentence. An LM
is then trained to discriminate between
the original sentences and the impostors.
The procedure is applied to the IWSLT
Chinese-to-English translation task, and
promising improvements on a state-of-
the-art MT system are demonstrated.
1 Discriminative Language Modeling
A language model (LM) constitutes a crucial com-
ponent in many tasks such as machine translation
(MT), speech recognition, information retrieval,
handwriting recognition, etc. It assigns a pri-
ori probabilities to word sequences. In general,
we expect a low probability for an ungrammat-
ical or implausible word sequence. The domi-
nant LM used in such systems is the so-called
n-gram model, which is typically derived from a
large corpus of target language text via maximum
likelihood estimation, mitigated by some smooth-
ing or regularization. Due to the Markovian as-
sumptions implicit in n-gram models, however,
richer linguistic and semantic dependencies are
not well captured. Rosenfeld (1996) and Khu-
danpur and Wu (2000) address such shortcom-
ing by using maximum entropy models with long-
span features, while still working with a locally
normalized left-to-right LM. The whole-sentence
maximum entropy LM of Rosenfeld et al (2001)
proposes a globally normalized log-linear LM in-
corporating several sentence-wide features.
The n-gram as well as the whole-sentence
model are generative or descriptive models of
text. However, in a task like Chinese-to-English
MT, the de facto role of the LM is to discriminate
among the alternative English translations being
contemplated by the MT system for a particular
Chinese input sentence. We call the set of such
alternative translations a confusion set. Since a
confusion set is typically a minuscule subset of
the set of all possible word sequences, it is ar-
guably better to train the LM parameters so as to
make the best candidate in the confusion set more
likely than its competitors, as done by Roark et al
(2004) for speech recognition and by Li and Khu-
danpur (2008) for MT. Note that identifying the
best candidate requires supervised training data?
bilingual text in case of MT?which is expensive
in many domains (e.g. weblog or newsgroup) and
for most language pairs (e.g. Urdu-English).
We propose a novel discriminative LM in this
paper: a globally normalized log-linear LM that
can be trained in an efficient and unsupervised
manner, using only monolingual (English) text.
The main idea is to exploit (translation) un-
certainties inherent in an MT system to de-
rive an English-to-English confusion grammar
(CG), illustrated in this paper for a Hiero sys-
tem (Chiang, 2007). From the bilingual syn-
chronous context-free grammar (SCFG) used in
Hiero, we extract a monolingual SCFG, with rules
of the kind, X ? ?strong tea, powerful tea? or
656
X ? ?in X1, in the X1?. Thus our CG is also an
SCFG that generates pairs of English sentences
that differ from each other in ways that alterna-
tive English hypothesis considered during transla-
tion would differ from each other. This CG is then
used to ?translate? each sentence in the LM train-
ing corpus into what we call its confusion set ? a
set of other ?sentences? with which that sentence
would likely be confused by the MT system, were
it to be the target translation of a source-language
sentence. Sentences in the training corpus, each
paired with its confusion set, are then used to train
a discriminative LM to prefer the training sen-
tences over the alternatives in their confusion sets.
Since the monolingual CG and the bilingual
Hiero grammar are both SCFGs, the confusion
sets are isomorphic with translation hypergraphs
that are used by supervised discriminative train-
ing. The confusion sets thus simulate the super-
vised case, with a key exception: lack of any
(Chinese) source-language information. There-
fore, only target-side ?language model? probabil-
ities may be estimated from confusion sets.
We carry out this discriminative training proce-
dure, and empirically demonstrate promising im-
provements in translation quality.
2 Discriminative LM Training
2.1 Whole-sentence Maximum Entropy LM
We aim to train a globally normalized log-linear
language model p?(y) of the form
p?(y) = Z?1 ef(y)?? (1)
where y is an English sentence, f(y) is a vector
of arbitrary features of y, ? is the (weight) vec-
tor of model parameters, and Z def= ?y? ef(y?)?? is
a normalization constant. Given a set of English
training sentences {yi}, the parameters ? may be
chosen to maximize likelihood, as
?? = argmax
?
?
i
p?(yi). (2)
This is the so called whole-sentence maximum
entropy (WSME) language model1 proposed by
1Note the contrast with the maximum entropy n-gram
LM (Rosenfeld, 1996; Khudanpur and Wu, 2000), where the
normalization is performed for each n-gram history.
Rosenfeld et al (2001). Training the model of
(2) requires computing Z, a sum over all possible
word sequences y? with any length, which is com-
putationally intractable. Rosenfeld et al (2001)
approximate Z by random sampling.
2.2 Supervised Discriminative LM Training
In addition to the computational disadvantage, (2)
also has a modeling limitation. In particular, in
a task like MT, the primary role of the LM is to
discriminate among alternative translations of a
given source-language sentence. This set of alter-
natives is typically a minuscule subset of all pos-
sible target-language word sequences. Therefore,
a better way to train the global log-linear LM,
given bilingual text {(xi, yi)}, is to generate the
real confusion set N (xi) for each input sentence
xi using a specific MT system, and to adjust ? to
discriminate between the reference translation yi
and y? ? N (xi) (Roark et al, 2004; Li and Khu-
danpur, 2008).
For example, one may maximize the condi-
tional likelihood of the bilingual training data as
?? = argmax
?
?
i
p?(yi |xi) (3)
= argmax
?
?
i
ef(xi,yi)???
y??N (xi) ef(xi,y
?)?? ,
which entails summing over only the candidate
translations y? of the given input xi. Furthermore,
if the features f(xi, y) are depend on only the out-
put y, i.e. on the English-side features of the bilin-
gual text, the resulting discriminative model may
be interpreted as a language model.
Finally, in a Hiero style MT system, if f(xi, y)
depends on the target-side(s) of the bilingual rules
used to construct y from xi, we essentially have a
syntactic LM.
2.3 Unsupervised Discriminative Training
using Simulated Confusion Sets
While the supervised discriminative LM training
has both computational and modeling advantages
over the WSME LM, it relies on bilingual data,
which is expensive to obtain for several domains
and language pairs. For such cases, we propose
a novel discriminative language model, which is
657
still a global log-linear LM with the modeling ad-
vantage and computational efficiency of (3) but re-
quires only monolingual text {yi} for training ?.
Specifically, we propose to modify (3) as
?? = argmax
?
?
i
p?(yi | N (yi)) (4)
= argmax
?
?
i
ef(yi)???
y??N (yi) ef(y
?)?? ,
where N (yi) is a simulated confusion set for yi
obtained by applying a confusion grammar to yi,
as detailed in Section 3. Our hope is that N (yi)
resembles the actual confusion set N (xi) that an
MT system would generate if it were given the in-
put sentence xi.
Like (3), the maximum likelihood training of
(4) does not entail the expensive computation of a
global normalization constant Z, and is therefore
very efficient. Unlike (3) however, where the input
xi for each output yi is needed to create N (xi),
the model of (4) can be trained in an unsupervised
manner with only {yi}.
3 Unsupervised Discriminative Training
of the Language Model for MT
The following is thus the proposed procedure for
unsupervised discriminative training of the LM.
1. Extract a confusion grammar (CG) from the
baseline MT system.
2. ?Translate? each English sentence in the LM
training corpus, using the CG as an English-
to-English translation model, to generate a
simulated confusion set.
3. Train a discriminative language model on the
simulated confusion sets, using the corre-
sponding original English sentences as the
training references.
The trained model may then be used for actual MT
decoding. We next describe each step in detail.
3.1 Extracting a Confusion Grammar
We assume a synchronous context free grammar
(SCFG) formalism for the confusion grammar
(CG). While the SCFG used by the MT system
is bilingual, the CG we extract will be monolin-
gual, with both the source and target sides being
English. Some example CG rules are:
X ? ? strong tea , powerful tea ? ,
X ? ?X0 at beijing , beijing ?s X0 ? ,
X ? ?X0 of X1 , X0 of the X1 ? ,
X ? ?X0 ?s X1 , X1 of X0 ? .
Like a regular SCFG, a CG contains rules with
different ?arities? and reordering of the nontermi-
nals (as shown in the last example) capturing the
confusions that the MT system encounters when
choosing word senses, reordering patterns, etc.
3.1.1 Extracting a Confusion Grammar from
the Bilingual Grammar
The confusion grammar is derived from the MT
system?s bilingual grammar. In Hiero, the bilin-
gual rules are of the form X ? ?c, e?, where
both c and e may contain (a matched number of)
nonterminal symbols. For every c which appears
on the source-side of two different Hiero rules
X ? ?c, e1? and X ? ?c, e2?, we extract two CG
rules, X ? ?e1, e2? and X ? ?e2, e1?, to capture
the confusion the MT system would face were it
to encounter c in its input. For each Hiero rule
X ? ?c, e?, we also extract X ? ?e, e?, the iden-
tity rule. Therefore, if a pattern c appears with |E|
different translation options, we extract |E|2 dif-
ferent CG rules from c. In our current work, the
rules of the CG are unweighted.
3.1.2 Test-set Specific Confusion Grammars
If the bilingual grammar contains all the rules
that are extractable from the bilingual training cor-
pus, the resulting confusion grammar is likely to
be huge. As a way of reducing computation, the
bilingual grammar can be restricted to a specific
test set, and only rules used by the MT system for
translating the test set used for extracting the CG.2
To economize further, one may extract a CG
from the translation hypergraphs that are gener-
ated for the test-set. Recall that a node in a hy-
pergraph corresponds to a specific source (Chi-
nese) span, and the node has many incident hy-
peredges, each associated with a different bilin-
2Test-set specific CGs are of course only practical for off-
line applications.
658
gual rule. Therefore, all the bilingual rules asso-
ciated with the incoming hyperedges of a given
node translate the same Chinese string. At each
hypergraph node, we extract CG rules to represent
the competing English sides as described above.
Note that even though different rules associated
with a node may have different ?arity,? we extract
CG rules only from pairs of bilingual rules that
have the same arity.
A CG extracted from only the bilingual rule
pairs incident on the same node in the test hy-
pergraphs is, of course, much smaller than a CG
extracted from the entire bilingual grammar. It
is also more suitable for our task, since the test
hypergraphs have already benefited from a base-
line n-gram LM and pruning, removing all confu-
sions that are easily resolved (rightly or wrongly)
by other system components.
3.2 Generating Simulated Confusion Sets
For each English sentence y in the training cor-
pus, we use the extracted CG to produce a simu-
lated confusion set N (y). This is done like a reg-
ular MT decoding pass, because we can treat the
CG as a Hiero style ?translation? grammar3 for an
English-to-English translation system.
Since the CG is an SCFG, the confusion set
N (y) generated for a sentence y is a hypergraph,
encoding not only the alternative sentences y? but
also the hierarchical derivation tree for each y?
from y (e.g., which phrase in y has been re-
placed with what in y?). As usual, many differ-
ent derivation trees d may correspond to the same
string/sentence y? due to spurious ambiguity. We
use D(y) to denote the set of derivations d, which
is a hypergraph representation of N (y).
Figure 1 presents an example confusion hy-
pergraph for the English sentence y =?a cat on
the mat,? containing four alternative hypotheses:
3To make sure that we produce at least one derivation tree
for each y, we need to add to the CG the following two glue
rules, as done in Hiero (Chiang, 2007).
S ? ?X0 , X0 ? ,
S ? ?S0 X1 , S0 X1 ? .
We also add an out of vocabulary rule X ? ?word, oov? for
each word in y and set the cost of this rule to a high value so
that the OOV rule will get used only when the CG does not
know how to ?translate? the word.
X ? ? a cat , the cat ?
X ? ? the mat , the mat ?
X ? ?X0 on X1 , X0 X1 ?
X ? ?X0 on X1 , X0 ?s X1 ?
X ? ?X0 on X1 , X1 on X0 ?
X ? ?X0 on X1 , X1 of X0 ?
S ? ?X0 , X0 ?
(a) An example confusion grammar.
a
0
  cat
1             
       on
2
         the
3
 mat
4
S??X0,X0?
X
0,5
X
0,2
X
3,5
X ? ? a cat , the cat ? X ? ? the mat , the mat ?
X ? ?X0 on X1 , X0 X1 ?
X ? ?X0 on X1 , X0 ?s X1 ? X ? ?X0 on X1 , X1 of X0 ?
X ? ?X0 on X1 , X1 on X0 ?
S
0,5
(b) An example hypergraph generated by the confusion
grammar of (a) for the input sentence ?a cat on the mat.?
Figure 1: Example confusion grammar and simulated
confusion hypergraph. Given an input sentence y = ?a cat
on the mat,? the confusion grammar of (a) generates a hyper-
graph D(y) shown in (b), which represents the confusion set
N (y) containing four alternative sentences y?.
N (y) = { ?the cat the mat,? ?the cat ?s the mat,?
?the mat of the cat,? ?the mat on the cat?}.
Notice that each competitor y? ? N (y) can be
regarded as the result of a ?round-trip? translation
y ? x ? y?, in which we reconstruct a possible
Chinese source sentence x that our Hiero bilin-
gual grammar could translate into both y and y?.4
We will train our LM to prefer y, which was ac-
tually observed. Our CG-based round-trip forces
x? y? to use the same hierarchical segmentation
of x as y ? x did. This constraint leads to effi-
cient training but artificially reduces the diversity
4This is because of the way we construct our CG from the
Hiero grammar. However, the identity and glue rules in our
CG allow almost any portion of y to be preserved untrans-
lated through the entire y ? x ? y? process. Much of y
will necessarily be preserved in the situation where the CG is
extracted from a small test set and hence has few non-identity
rules. See (Li, 2010) for further discussion.
659
ofN (y). In other recent work (Li et al, 2010), we
have taken the round-trip view more seriously, by
imputing likely source sentences x and translating
them back to separate, weighted confusion forests
N (y), without any same-segmentation constraint.
3.3 Confusion-based Discriminative Training
With the training sentences yi and their simulated
confusion sets N (yi) ? represented as hyper-
graphs D(yi)) ? we can perform the discrimi-
native training using any of a number of proce-
dures such as MERT (Och, 2003) or MIRA as
used by Chiang et al (2009). In our paper, we
use hypergraph-based minimum risk (Li and Eis-
ner, 2009),
?? = argmin
?
?
i
Risk?(yi) (5)
= argmin
?
?
i
?
d?D(yi)
L(Y(d), yi)p?(d |D(yi)),
where L(y?, yi) is the loss (e.g negated BLEU) in-
curred by producing y? when the true answer is yi,
Y(d) is the English yield of a derivation d, and
p?(d |D(yi)) is defined as,
p?(d |D(yi)) = e
f(d)??
?
d?D(yi) ef(d)??
, (6)
where f(d) is a feature vector over d. We will
specify the features in Section 5, but in general
they should be defined such that the training will
be efficient and the actual MT decoding can use
them conveniently.
The objective of (5) is differentiable and thus
we can optimize ? by a gradient-based method.
The risk and its gradient on a hypergraph can
be computed by using a second-order expectation
semiring (Li and Eisner, 2009).
3.3.1 Iterative Training
In practice, the full confusion set N (y) defined
by a confusion grammar may be too large and we
have to perform pruning when training our model.
But the pruning itself may depend on the model
that we aim to train. How do we solve this circu-
lar dependency problem? We adopt the following
procedure. Given an initial model ?, we generate a
hypergraph (with pruning) for each y, and train an
optimal ?? of (5) on these hypergraphs. Then, we
use the optimal ?? to regenerate a hypergraph for
each y, and do the training again. This iterates un-
til convergence. This procedure is quite similar to
the k-best MERT (Och, 2003) where the training
involves a few iterations, and each iteration uses a
new k-best list generated using the latest model.
3.4 Applying the Discriminative LM
First, we measure the goodness of our language
model in a simulated task. We generate simulated
confusion sets N (y) for some held out English
sentences y, and test how well p?(d |D(y)) can
recover y from N (y). This is merely a proof of
concept, and may be useful in deciding which fea-
tures f(d) to employ for discriminative training.
The intended use of our model is, of course, for
actual MT decoding (e.g., translating Chinese to
English). Specifically, we can add the discrimina-
tive model into an MT pipeline as a feature, and
tune its weight relative to other models in the MT
system, including the baseline n-gram LM.
4 Related and Similar Work
The detailed relation between the proposed pro-
cedure and other language modeling techniques
has been discussed in Sections 1 and 2. Here, we
review two other methods that are related to our
method in a broader context.
4.1 Unsupervised Training of Global
Log-linear Models
Our method is similar to the contrastive estimation
(CE) of Smith and Eisner (2005) and its succes-
sors (Poon et al, 2009). In particular, our confu-
sion grammar is like a neighborhood function in
CE. Also, our goal is to improve both efficiency
and accuracy, just as CE does. However, there
are two important differences. First, the neigh-
borhood function in CE is manually created based
on human insights about the particular task, while
our neighborhood function, generated by the CG,
is automatically learnt (e.g., from the bilingual
grammar) and specific to the MT system being
used. Therefore, our neighborhood function is
more likely to be informative and adaptive to the
task. Secondly, when tuning ?, CE uses the maxi-
mum likelihood training, but we use the minimum
660
risk training of (5). Since our training uses a task-
specific loss function, it is likely to perform better
than maximum likelihood training.
4.2 Paraphrasing Models
Our method is also related to methods for train-
ing paraphrasing models (Quirk et al, 2004; Ban-
nard and Callison-Burch, 2005; Callison-Burch et
al., 2006; Madnani et al, 2007). Specifically, the
form of our confusion grammar is similar to that
of the paraphrase model they use, and the ways
of extracting the grammar/model are also similar
as both employ a second language (e.g., Chinese
in our case) as a pivot. However, while a ?trans-
lation? rule in a paraphrase model is expected to
contain a pair of phrases that are good alterna-
tives for each other, a confusion rule in our CG
is based on an MT system processing unseen test
data and contains pairs of phrases that are typi-
cally bad (and only rarely good) alternatives for
each other.
The motivation and goal are also different. For
example, the goal of Bannard and Callison-Burch
(2005) is to extract paraphrases with the help of
parallel corpora. Callison-Burch et al (2006) aim
to improve MT quality by adding paraphrases in
the translation table, while Madnani et al (2007)
aim to improve the minimum error rate training by
adding the automatically generated paraphrases
into the English reference sets. In contrast, our
motivation is to train a discriminative language
model to improve MT (by using the confusion
grammar to decide what alternatives the model
should learn to discriminate).
5 Experimental Results
We have applied the confusion-based discrimina-
tive language model (CDLM) to the IWSLT 2005
Chinese-to-English text translation task5 (Eck and
Hori, 2005). We see promising improvements
over an n-gram LM for a solid Joshua-based
baseline system (Li et al, 2009).
5.1 Data Partitions for Training & Testing
Four kinds of data are used for CDLM training:
5This is a relatively small task compared to, say, the NIST
MT tasks. We worked on it for a proof-of-concept. Having
been successful, we are now investigating larger MT tasks.
# sentencesData Usage ZH EN
Set1 TM & LM training 40k 40k
Set2 Min-risk training 1006 1006?16
Set3 CDLM training ? 1006?16
Set4 Test 506 506?16
Table 1: Data sets used. Set1 contains translation-equivalent
Chinese-English sentence pairs, while for each Chinese sen-
tence in Set2 and Set4, there are 16 English translations. Set3
happens to be the English side of Set2 due to lack of ad-
ditional in-domain English text, but this is not noteworthy;
Set3 could be any in-domain target-language text corpus.
Set1 a bilingual training set on which 10 individ-
ual MT system components are trained,
Set2 a small bilingual, in-domain set for tuning
relative weights of the system components,
Set3 an in-domain monolingual target-language
corpus for CDLM training, and
Set4 a test set on which improvements in MT per-
formance is measured.
We partition the IWSLT data into four such sub-
sets as listed in Table 1.
5.2 Baseline MT System
Our baseline translation model components are
estimated from 40k pairs of utterances from the
travel domain, called Set1 in Table 1. We use a 5-
gram language model with modified Kneser-Ney
smoothing (Chen and Goodman, 1998), trained on
the English side of Set1, as our baseline LM.
The baseline MT system comprises 10 com-
ponent models (or ?features?) that are standard
in Hiero (Chiang, 2007), namely the baseline
language model (BLM) feature, three baseline
translation model features, one word-insertion
penalty (WP) feature, and five arity features ?
three to count how many rules with an arity of
zero/one/two are used in a derivation, and two
to count how many times the unary and binary
glue rules are used in a derivation. The rela-
tive weights of these 10 features are tuned via
hypergraph-based minimum risk training (Li and
Eisner, 2009) on the bilingual data Set2.
The resulting MT system gives a BLEU score of
48.5% on Set4, which is arguably a solid baseline.
661
5.3 Unsupervised Training of the CDLM
We extract a test-set specific CG from the hyper-
graphs obtained by decoding Set2 and Set4, as de-
scribed in Section 3.1.2. The number of rules in
the bilingual grammar and the CG are about 167k
and 1583k respectively. The CG is used as the
?translation? model to generate confusion hyper-
graphs for sentences in Set3.
Two CDLMs, corresponding to different fea-
ture sets f(d) in equation (6), were trained.
Only n-gram LM Features: We consider a
CDLM with only two features f(d): a base-
line LM feature (BLM) that equals the 5-
gram probability of Y(d) and a word penalty
feature (WP) equal to the length of Y(d).
Target-side Rule Bigram Features6: For each
CG rule used in d, we extract counts of bi-
grams that appear on the target-side of the
CG rule. For example, if the confusion rule
X ? ?X0 of X1 , X0 of the X1 ? is used in
d, the bigram features in f(d) whose counts
are incremented are: ?X of,? ?of the? and
?the X .?7 Note that the indices on the non-
terminals in the rule have been removed. To
avoid very rare features, we only consider
the 250 most freqent terminal symbol (En-
glish words) in the English of Set1 and map
all other terminal symbols into a single class.
Finally, we replace the identities of words
with their dominant POS tags. These restric-
tions result in 525 target-side rule bigram
(TsRB) features f(d) in the model of (6).
For each choice of the feature vector f(d), be it
2- or 527-dimensional, we use the training proce-
dure of Section 3.3.1 to iteratively minimize the
objective of (5) and get the CDLM parameter ??.
Note that each English sentence in Set3 has 15
other paraphrases. We generate a separate confu-
sion hypergraph D(y) for each English sentence
y, but for each such hypergraph we use both y
and its 15 paraphrases as ?reference translations?
when computing the risk L(Y(d), {y}) in (5).8
6Note that these features are novel in MT.
7With these target-side rule-based features, our LM is es-
sentially a syntactic LM, not just an LM on English strings.
8We take unfair advantage of this unusual dataset to com-
5.4 Results on Monolingual Simulation
We first probe how our novel CDLM performs as
a language model itself. One usually uses the per-
plexity of the LM on some unseen text to measure
its goodness. But since we did not optimize the
CDLM for likelihood, we instead examine how
it performs in discriminating between a good En-
glish sentence and sentences with which the MT
system may confuse that sentence. The test is per-
formed as follows. For each test English sentence
y of Set4, the confusion grammar defines a full
confusion set N (y) via a hypergraph D(y). We
use a LM to pick the most likely y? from N (y),
and then compute its BLEU score by using y and
its 15 paraphrase sentences as references. The
higher the BLEU, the better is the LM in picking
out a good translation from N (y).
Table 2 shows the results9 under a regular n-
gram LM and the two CDLMs described in Sec-
tion 5.3.
The baseline LM (BLM) entails no weight op-
timization a la (5) on Set3. The CDLM with the
BLM and word pentaly (WP) features improves
over the baseline LM. Compared to either of them,
the CDLM with the target-side rule bigram fea-
tures (TsRB) performs dramatically better.
5.5 Results on MT Test Data
We now examine how our CDLM performs during
actual MT decoding. To incorporate the CDLM
into MT decoding, we add the log-probability (6)
of a derivation d under the CDLM as an additional
bat an unrelated complication?a seemingly problematic in-
stability in the minimum risk training procedure.
As an illustration of this problem, we note that in super-
vised tuning of the baseline MT system (|f(d)|=10) with
500 sentences from Set2, the BLEU score on Set4 varies from
38.6% to 44.2% to 47.8% if we use 1, 4 and 16 reference
translations during the supervised training respectively. We
choose a system tuned on 16 references on Set2 as our base-
line. In order not to let the unsupervised CDLM training
suffer from this unrelated limitation of the tuning procedure,
we give it too the benefit of being able to compute risk on
Set3 using y plus its 15 paraphrases.
We wish to emphasize that this trait of Set3 having 15
paraphrases for each sentence is otherwise unnecessary, and
does not detract much from the main claim of this paper.
9Note that the scores in Table 2 are very low compared to
scores for actual translation from Chinese shown in Table 3.
This is mainly because in this monolingual simulation, the
LM is the only model used to rank the y? ? N (y). Said dif-
ferently, y? is being chosen in Table 2 entirely for its fluency
with no consideration whatsoever for its adequacy.
662
LM used for Features used BLEU
rescoring BLM WP TsRB on Set4
Baseline LM X 12.8
CDLM X X 14.2
CDLM X X X 25.3
Table 2: BLEU scores in monolingual simulations. Rescor-
ing the confusion sets of English sentences created using the
CG shows that the CDLM with TsRB features recovers hy-
potheses much closer to the sentence that generated the con-
fusion set than does the baseline n-gram LM.
Model used Features used BLEU
for rescoring 10 models TsRB on Set4
Joshua X 48.5
+ CDLM X X 49.5
Table 3: BLEU scores on the test set. The baseline MT sys-
tem has ten models/features, and the proposed system has
one additional model, the CDLM. Note that for the CDLM,
only the TsRB features are used during MT decoding.
feature, on top of the 10 features already present
in baseline MT system (see Section 5.2). We then
(re)tune relative weights for these 11 features on
the bilingual data Set2 of Table 1.
Note that the MT system also uses the BLM and
WP features whose weights are now retuned on
Set2. Therefore, when integrating a CDLM into
MT decoding, it is mathematically equivalent to
use only the TsRB features of the CDLM, with
the corresponding weights as estimated alongside
its ?own? BLM and WP features during unsuper-
vised discriminative training on Set3.
Table 3 reports the results. A BLEU score im-
provement of 1% is seen, reinforcing the claim
that the unsupervised CDLM helps select better
translations from among the system?s alternatives.
5.6 Goodness of Simulated Confusion Sets
The confusion set N (y) generated by applying
the CG to an English sentence y aims to simulate
the real confusion set that would be generated by
the MT system if the system?s input was the Chi-
nese sentence whose English translation is y. We
investigate, in closing, how much the simulated
confusion set resembles to the real one. Since
we know the actual input-output pairs (xi, yi) for
Set4, we generate two confusion sets: the simu-
lated set N (yi) and the real one N (xi).
One way to measure the goodness of N (yi) as
a proxy for N (xi), is to extract the n-gram types
n-gram Precision Recall
unigram 36.5% 48.2%
bigram 10.1% 12.8%
trigram 3.7% 4.6%
4-gram 2.0% 2.4%
Table 4: n-gram precision and recall of simulated con-
fusion sets relative to the true confusions when translating
Chinese sentences. The n-grams are collected from k-best
strings in both cases, with k = 100. The precision and recall
change little when varying k.
witnessed in the two sets, and compute the ratio of
the number of n-grams in the intersection to the
number in their union. Another is to measure the
precision and recall of N (yi) relative to N (xi).
Table 4 presents such precision and recall fig-
ures. For convenience, the n-grams are collected
from the 100-best strings, instead of the hyper-
graph D(yi) and D(xi). Observe that the sim-
ulated confusion set does a reasonably good job
on the real unigram confusions but the simulation
needs improving for higher order n-grams.
6 Conclusions
We proposed a novel procedure to discrimina-
tively train a globally normalized log-linear lan-
guage model for MT, in an efficient and unsu-
pervised manner. Our method relies on the con-
struction of a confusion grammar, an English-to-
English SCFG that captures translation alterna-
tives that an MT system may face when choosing
a translation for a given input. For each English
training sentence, we use this confusion gram-
mar to generate a simulated confusion set, from
which we train a discriminative language model
that will prefer the original English sentence over
sentences in the confusion set. Our experiments
show that the novel CDLM picks better alterna-
tives than a regular n-gram LM from simulated
confusion sets, and improves performance in a
real Chinese-to-English translation task.
7 Acknowledgements
This work was partially supported by the National
Science Foundation via grants No? SGER-0840112
and RI-0963898, and by the DARPA GALE pro-
gram. The authors thank Brian Roark and Dami-
anos Karakos for insightful discussions.
663
References
Bannard, Colin and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages
597?604, Morristown, NJ, USA. Association for
Computational Linguistics.
Callison-Burch, Chris, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 17?
24, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Chen, Stanley F. and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report.
Chiang, David, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Eck, Matthias and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In Proc. of the
International Workshop on Spoken Language Trans-
lation.
Khudanpur, Sanjeev and Jun Wu. 2000. Maximum en-
tropy techniques for exploiting syntactic, semantic
and collocational dependencies in language model-
ing. In Computer Speech and Language, number 4,
pages 355?372.
Li, Zhifei and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
40?51, Singapore, August. Association for Compu-
tational Linguistics.
Li, Zhifei and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In AMTA, pages 133?142.
Li, Zhifei, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In WMT09,
pages 26?30.
Li, Zhifei, Ziyuan Wang, Jason Eisner, and Sanjeev
Khudanpur. 2010. Minimum imputed risk training
for machine translation. In review.
Li, Zhifei. 2010. Discriminative training and varia-
tional decoding in machine translation via novel al-
gorithms for weighted hypergraphs. PHD Disserta-
tion, Johns Hopkins University.
Madnani, Nitin, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Workshop on Statistical Machine
Translation, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL, pages
160?167.
Poon, Hoifung, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, pages 209?217, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Quirk, Chris, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 142?149.
Roark, Brian, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 47?54,
Barcelona, Spain, July.
Rosenfeld, Roni, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: a vehicle for linguistic-statistical integration.
Computers Speech and Language, 15(1).
Rosenfeld, Roni. 1996. A maximum entropy approach
to adaptive statistical language modeling. In Com-
puter Speech and Language, number 3, pages 187?
228.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Association for Compu-
tational Linguistics (ACL 2005), Ann Arbor, Michi-
gan.
664
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 616?627,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Discovering Morphological Paradigms from Plain Text
Using a Dirichlet Process Mixture Model
Markus Dreyer?
SDL Language Weaver
Los Angeles, CA 90045, USA
mdreyer@sdl.com
Jason Eisner
Computer Science Dept., Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Abstract
We present an inference algorithm that orga-
nizes observed words (tokens) into structured
inflectional paradigms (types). It also natu-
rally predicts the spelling of unobserved forms
that are missing from these paradigms, and dis-
covers inflectional principles (grammar) that
generalize to wholly unobserved words.
Our Bayesian generative model of the data ex-
plicitly represents tokens, types, inflections,
paradigms, and locally conditioned string edits.
It assumes that inflected word tokens are gen-
erated from an infinite mixture of inflectional
paradigms (string tuples). Each paradigm is
sampled all at once from a graphical model,
whose potential functions are weighted finite-
state transducers with language-specific param-
eters to be learned. These assumptions natu-
rally lead to an elegant empirical Bayes infer-
ence procedure that exploits Monte Carlo EM,
belief propagation, and dynamic programming.
Given 50?100 seed paradigms, adding a 10-
million-word corpus reduces prediction error
for morphological inflections by up to 10%.
1 Introduction
1.1 Motivation
Statistical NLP can be difficult for morphologically
rich languages. Morphological transformations on
words increase the size of the observed vocabulary,
which unfortunately masks important generalizations.
In Polish, for example, each lexical verb has literally
100 inflected forms (Janecki, 2000). That is, a single
lexeme may be realized in a corpus as many different
word types, which are differently inflected for person,
number, gender, tense, mood, etc.
? This research was done at Johns Hopkins University as
part of the first author?s dissertation work. It was supported by
the Human Language Technology Center of Excellence and by
the National Science Foundation under Grant No. 0347822.
All this makes lexical features even sparser than
they would be otherwise. In machine translation
or text generation, it is difficult to learn separately
how to translate, or when to generate, each of these
many word types. In text analysis, it is difficult to
learn lexical features (as cues to predict topic, syntax,
semantics, or the next word), because one must learn
a separate feature for each word form, rather than
generalizing across inflections.
Our engineering goal is to address these problems
by mostly-unsupervised learning of morphology. Our
linguistic goal is to build a generative probabilistic
model that directly captures the basic representations
and relationships assumed by morphologists. This
model suffices to define a posterior distribution over
analyses of any given collection of type and/or token
data. Thus we obtain scientific data interpretation as
probabilistic inference (Jaynes, 2003). Our computa-
tional goal is to estimate this posterior distribution.
1.2 What is Estimated
Our inference algorithm jointly reconstructs token,
type, and grammar information about a language?s
morphology. This has not previously been attempted.
Tokens: We will tag each word token in a corpus
with (1) a part-of-speech (POS) tag,1 (2) an inflection,
and (3) a lexeme. A token of broken might be tagged
as (1) a VERB and more specifically as (2) the past
participle inflection of (3) the abstract lexeme b&r?a?k.2
Reconstructing the latent lexemes and inflections
allows the features of other statistical models to con-
sider them. A parser may care that broken is a
past participle; a search engine or question answer-
ing system may care that it is a form of b&r?a?k; and a
translation system may care about both facts.
1POS tagging may be done as part of our Bayesian model or
beforehand, as a preprocessing step. Our experiments chose the
latter option, and then analyzed only the verbs (see section 8).
2We use cursive font for abstract lexemes to emphasize that
they are atomic objects that do not decompose into letters.
616
singular plural
present 1st-person breche brechen
2nd-person brichst brecht
3rd-person bricht brechen
past 1st-person brach brachen
2nd-person brachst bracht
3rd-person brach brachen
Table 1: Part of a morphological paradigm in German,
showing the spellings of some inflections of the lexeme
b&r?a?k (whose lemma is brechen), organized in a grid.
Types: In carrying out the above, we will recon-
struct specific morphological paradigms of the lan-
guage. A paradigm is a grid of all the inflected forms
of some lexeme, as illustrated in Table 1. Our recon-
structed paradigms will include our predictions of
inflected forms that were never observed in the cor-
pus. This tabular information about the types (rather
than the tokens) of the language may be separately
useful, for example in translation and other genera-
tion tasks, and we will evaluate its accuracy.
Grammar: We estimate parameters ~? that de-
scribe general patterns in the language. We learn
a prior distribution over inflectional paradigms by
learning (e.g.) how a verb?s suffix or stem vowel
tends to change when it is pluralized. We also learn
(e.g.) whether singular or plural forms are more com-
mon. Our basic strategy is Monte Carlo EM, so these
parameters tell us how to guess the paradigms (Monte
Carlo E step), then these reconstructed paradigms tell
us how to reestimate the parameters (M step), and so
on iteratively. We use a few supervised paradigms to
initialize the parameters and help reestimate them.
2 Overview of the Model
We begin by sketching the main ideas of our model,
first reviewing components that we introduced in
earlier papers. Sections 5?7 will give more formal
details. Full details and more discussion can be found
in the first author?s dissertation (Dreyer, 2011).
2.1 Modeling Morphological Alternations
We begin with a family of joint distributions p(x, y)
over string pairs, parameterized by ~?. For example,
to model just the semi-systematic relation between a
German lemma and its 3rd-person singular present
form, one could train ~? to maximize the likelihood
of (x, y) pairs such as (brechen, bricht). Then,
given a lemma x, one could predict its inflected form
y via p(y | x), and vice-versa.
Dreyer et al (2008) define such a family via a
log-linear model with latent alignments,
p(x, y) =
?
a
p(x, y, a) ?
?
a
exp(~? ? ~f(x, y, a))
Here a ranges over monotonic 1-to-1 character align-
ments between x and y. ?means ?proportional to? (p
is normalized to sum to 1). ~f extracts a vector of local
features from the aligned pair by examining trigram
windows. Thus ~? can reward or penalize specific
features?e.g., insertions, deletions, or substitutions
in specific contexts, as well as trigram features of x
and y separately.3 Inference and training are done by
dynamic programming on finite-state transducers.
2.2 Modeling Morphological Paradigms
A paradigm such as Table 1 describes how some ab-
stract lexeme (b&r?a?k) is expressed in German.4 We
evaluate whole paradigms as linguistic objects, fol-
lowing word-and-paradigm or realizational morphol-
ogy (Matthews, 1972; Stump, 2001). That is, we pre-
sume that some language-specific distribution p(pi)
defines whether a paradigm pi is a grammatical?and
a priori likely?way for a lexeme to express itself
in the language. Learning p(pi) helps us reconstruct
paradigms, as described at the end of section 1.2.
Let pi = (x1, x2, . . .). In Dreyer and Eisner (2009),
we showed how to model p(pi) as a renormalized
product of many pairwise distributions prs(xr, xs),
each having the log-linear form of section 2.1:
p(pi) ?
?
r,s
prs(xr, xs) ? exp(
?
r,s
~????frs(xr, xs, ars))
This is an undirected graphical model (MRF) over
string-valued random variables xs; each factor prs
evaluates the relationship between some pair of
strings. Note that it is still a log-linear model, and pa-
rameters in ~? can be reused across different rs pairs.
To guess at unknown strings in the paradigm,
Dreyer and Eisner (2009) show how to perform ap-
proximate inference on such an MRF by loopy belief
3Dreyer et al (2008) devise additional helpful features based
on enriching the aligned pair with additional latent information,
but our present experiments drop those for speed.
4Our present experiments focus on orthographic forms, be-
cause we are learning from a written corpus. But it would be
natural to use phonological forms instead, or to include both in
the paradigm so as to model their interrelationships.
617
X1pl
X
2pl
X
3pl
X
Lem
X
1sg
X
2sg
X
3sg
brichen
brechen
 ...
?
brichen
brechen
 ...
?
bricht
brecht
 ...
?
briche
breche
 ...
?
brichst
brechst
 ...
?
bricht
brechen
Figure 1: A distribution over paradigms modeled as an
MRF over 7 strings. Random variables XLem, X1st, etc.,
are the lemma, the 1st person form, etc. Suppose two
forms are observed (denoted by the ?lock? icon). Given
these observations, belief propagation estimates the poste-
rior marginals over the other variables (denoted by ???).
propagation, using finite-state operations. It is not
necessary to include all rs pairs. For example, Fig. 1
illustrates the result of belief propagation on a simple
MRF whose factors relate all inflected forms to a
common (possibly unobserved) lemma, but not di-
rectly to one another.5
Our method could be used with any p(pi). To speed
up inference (see footnote 7), our present experiments
actually use the directed graphical model variant of
Fig. 1?that is, p(pi) = p1(x1) ??s>1 p1s(xs | x1),
where x1 denotes the lemma.
2.3 Modeling the Lexicon (types)
Dreyer and Eisner (2009) learned ~? by partially ob-
serving some paradigms (type data). That work,
while rather accurate at predicting inflected forms,
sometimes erred: it predicted spellings that never oc-
curred in text, even for forms that ?should? be com-
mon. To fix this, we shall incorporate an unlabeled
or POS-tagged corpus (token data) into learning.
We therefore need a model for generating tokens?
a probabilistic lexicon that specifies which inflections
of which lexemes are common, and how they are
spelled. We do not know our language?s probabilistic
lexicon, but we assume it was generated as follows:
1. Choose parameters ~? of the MRF. This defines
p(pi): which paradigms are likely a priori.
2. Choose a distribution over the abstract lexemes.
5This view is adopted by some morphological theorists (Al-
bright, 2002; Chan, 2006), although see Appendix E.2 for a
caution about syncretism. Note that when the lemma is unob-
served, the other forms do still influence one another indirectly.
3. For each lexeme, choose a distribution over its
inflections.
4. For each lexeme, choose a paradigm that will
be used to express the lexeme orthographically.
Details are given later. Briefly, step 1 samples ~?
from a Gaussian prior. Step 2 samples a distribution
from a Dirichlet process. This chooses a countable
number of lexemes to have positive probability in the
language, and decides which ones are most common.
Step 3 samples a distribution from a Dirichlet. For
the lexeme t?h?i?n?k, this might choose to make 1st-
person singular more common than for typical verbs.
Step 4 just samples IID from p(pi).
In our model, each part of speech generates its own
lexicon: VERBs are inflected differently from NOUNs
(different parameters and number of inflections). The
size and layout of (e.g.) VERB paradigms is language-
specific; we currently assume it is given by a linguist,
along with a few supervised VERB paradigms.
2.4 Modeling the Corpus (tokens)
At present, we use only a very simple exchangeable
model of the corpus. We assume that each word was
independently sampled from the lexicon given its
part of speech, with no other attention to context.
For example, a token of brechen may have been
chosen by choosing frequent lexeme b&r?a?k from the
VERB lexicon; then choosing 1st-person plural given
b&r?a?k; and finally looking up that inflection?s spelling
in b&r?a?k?s paradigm. This final lookup is determinis-
tic since the lexicon has already been generated.
3 A Sketch of Inference and Learning
3.1 Gibbs Sampling Over the Corpus
Our job in inference is to reconstruct the lexicon that
was used and how each token was generated from it
(i.e., which lexeme and inflection?). We use collapsed
Gibbs sampling, repeatedly guessing a reanalysis of
each token in the context of all others. Gradually, sim-
ilar tokens get ?clustered? into paradigms (section 4).
The state of the sampler is illustrated in Fig. 2.
The bottom half shows the current analyses of the
verb tokens. Each is associated with a particular slot
in some paradigm. We are now trying to reanalyze
brechen at position ?. The dashed arrows show
some possible analyses.
618
singular
plural
1st
2nd
3rd
bricht
brichst
brechst
...
?
briche
breche
...
?
brechen
bricht
brecht
...
?
brichen
brechen
...
?
springst
sprengst
...
?
springe
sprenge
...
?
springt
sprengt
...
?
springen
sprengen
...
springen
sprengen
...
1
3
Index i
2
PRON
4
NOUN
6
PREP
POS ti
Infl si
Spell wi
Lex ?i
1
VERB
3rd sg.
bricht
5
VERB
2nd pl.
springt
3
VERB
3rd pl.
brechen
?
?
5
Lexicon
Corpus
?
...
?
?
springt
7
VERB
1st pl.
...
brechen... ... ...
3rd pl.
Figure 2: A state of the Gibbs sampler (note that the
assumed generative process runs roughly top-to-bottom).
Each corpus token i has been tagged with part of speech ti,
lexeme `i and inflection si. Token ? has been tagged as
b&r?a?k and 3rd sg., which locked the corresponding type
spelling in the paradigm to the spelling w1 = bricht;
similarly for ? and ?. Now w7 is about to be reanalyzed.
The key intuition is that the current analyses of the
other verb tokens imply a posterior distribution over
the VERB lexicon, shown in the top half of the figure.
First, because of the current analyses of ? and ?,
the 3rd-person spellings of b&r?a?k are already con-
strained to match w1 and w3 (the ?lock? icon).
Second, belief propagation as in Fig. 1 tells us
which other inflections of b&r?a?k (the ??? icon) are
plausibly spelled as brechen, and how likely they
are to be spelled that way.
Finally, the fact that other tokens are associated
with b&r?a?k suggest that this is a popular lexeme, mak-
ing it a plausible explanation of ? as well. (This is
the ?rich get richer? property of the Chinese restau-
rant process; see section 6.6.) Furthermore, certain
inflections of b&r?a?k appear to be especially popular.
In short, given the other analyses, we know which
inflected lexemes in the lexicon are likely, and how
likely each one is to be spelled as brechen. This lets
us compute the relative probabilities of the possible
analyses of token ?, so that the Gibbs sampler can
accordingly choose one of these analyses at random.
3.2 Monte Carlo EM Training of ~?
For a given ~?, this Gibbs sampler converges to the
posterior distribution over analyses of the full corpus.
To improve our ~? estimate, we periodically adjust ~?
to maximize or increase the probability of the most
recent sample(s). For example, having tagged w5 =
springt as s5 = 2nd-person plural may strengthen
our estimated probability that 2nd-person spellings
tend to end in -t. That revision to ~?, in turn, will
influence future moves of the sampler.
If the sampler is run long enough between calls to
the ~? optimizer, this is a Monte Carlo EM procedure
(see end of section 1.2). It uses the data to optimize a
language-specific prior p(pi) over paradigms?an em-
pirical Bayes approach. (A fully Bayesian approach
would resample ~? as part of the Gibbs sampler.)
3.3 Collapsed Representation of the Lexicon
The lexicon is collapsed out of our sampler, in the
sense that we do not represent a single guess about the
infinitely many lexeme probabilities and paradigms.
What we store about the lexicon is information about
its full posterior distribution: the top half of Fig. 2.
Fig. 2 names its lexemes as b&r?a?k and ?j?u?m?p for ex-
pository purposes, but of course the sampler cannot
reconstruct such labels. Formally, these labels are col-
lapsed out, and we represent lexemes as anonymous
objects. Tokens ? and ? are tagged with the same
anonymous lexeme (which will correspond to sitting
at the same table in a Chinese restaurant process).
For each lexeme ` and inflection s, we maintain
pointers to any tokens currently tagged with the slot
(`, s). We also maintain an approximate marginal
distribution over the spelling of that slot:6
1. If (`, s) points to at least one token i, then we
know (`, s) is spelled as wi (with probability 1).
2. Otherwise, the spelling of (`, s) is not known.
But if some spellings in `?s paradigm are known,
store a truncated distribution that enumerates the
25 most likely spellings for (`, s), according to
loopy belief propagation within the paradigm.
3. Otherwise, we have observed nothing about `:
it is currently unused. All such ` share the same
marginal distribution over spellings of (`, s):
the marginal of the prior p(pi). Here a 25-best
list could not cover all plausible spellings. In-
stead we store a probabilistic finite-state lan-
guage model that approximates this marginal.7
6Cases 1 and 2 below must in general be updated whenever
a slot switches between having 0 and more than 0 tokens. Cases
2 and 3 must be updated when the parameters ~? change.
7This character trigram model is fast to build if p(pi) is de-
619
A hash table based on cases 1 and 2 can now be
used to rapidly map any word w to a list of slots of
existing lexemes that might plausibly have generated
w. To ask whether w might instead be an inflection s
of a novel lexeme, we score w using the probabilistic
finite-state automata from case 3, one for each s.
The Gibbs sampler randomly chooses one of these
analyses. If it chooses the ?novel lexeme? option,
we create an arbitrary new lexeme object in mem-
ory. The number of explicitly represented lexemes is
always finite (at most the number of corpus tokens).
4 Interpretation as a Mixture Model
It is common to cluster points in Rn by assuming
that they were generated from a mixture of Gaussians,
and trying to reconstruct which points were generated
from the same Gaussian.
We are similarly clustering word tokens by assum-
ing that they are generated from a mixture of weighted
paradigms. After all, each word token was obtained
by randomly sampling a weighted paradigm (i.e., a
cluster) and then randomly sampling a word from it.
Just as each Gaussian in a Gaussian mixture is
a distribution over all points Rn, each weighted
paradigm is a distribution over all spellings ?? (but
assigns probability > 0 to only a finite subset of ??).
Inference under our model clusters words together
by tagging them with the same lexeme. It tends to
group words that are ?similar? in the sense that the
base distribution p(pi) predicts that they would tend
to co-occur within a paradigm. Suppose a corpus
contains several unlikely but similar tokens, such
as discombobulated and discombobulating.
A language might have one probable lexeme from
whose paradigm all these words were sampled. It is
much less likely to have several probable lexemes that
all coincidentally chose spellings that started with
discombobulat-. Generating discombobulat-
only once is cheaper (especially for such a long pre-
fix), so the former explanation has higher probability.
This is like explaining nearby points in Rn as sam-
ples from the same Gaussian. Of course, our model
is sensitive to more than shared prefixes, and it does
not merely cluster words into a paradigm but assigns
them to particular inflectional slots in the paradigm.
fined as at the end of section 2.2. If not, one could still try belief
propagation; or one could approximate by estimating a language
model from the spellings associated with slot s by cases 1 and 2.
4.1 The Dirichlet Process Mixture Model
Our mixture model uses an infinite number of mix-
ture components. This avoids placing a prior bound
on the number of lexemes or paradigms in the lan-
guage. We assume that a natural language has an
infinite lexicon, although most lexemes have suffi-
ciently low probability that they have not been used
in our training corpus or even in human history (yet).
Our specific approach corresponds to a Bayesian
technique, the Dirichlet process mixture model. Ap-
pendix A (supplementary material) explains the
DPMM and discusses it in our context.
The DPMM would standardly be presented as gen-
erating a distribution over countably many Gaussians
or paradigms. Our variant in section 2.3 instead broke
this into two steps: it first generated a distribution
over countably many lexemes (step 2), and then gen-
erated a weighted paradigm for each lexeme (steps
3?4). This construction keeps distinct lexemes sepa-
rate even if they happen to have identical paradigms
(polysemy). See Appendix A for a full discussion.
5 Formal Notation
5.1 Value Types
We now describe our probability model in more for-
mal detail. It considers the following types of mathe-
matical objects. (We use consistent lowercase letters
for values of these types, and consistent fonts for
constants of these types.)
A word w, such as broken, is a finite string of
any length, over some finite, given alphabet ?.
A part-of-speech tag t, such as VERB, is an ele-
ment of a certain finite set T , which in this paper we
assume to be given.
An inflection s,8 such as past participle, is an ele-
ment of a finite set St. A token?s part-of-speech tag
t ? T determines its set St of possible inflections.
For tags that do not inflect, |St| = 1. The sets St
are language-specific, and we assume in this paper
that they are given by a linguist rather than learned.
A linguist also specifies features of the inflections:
the grid layout in Table 1 shows that 4 of the 12
inflections in SVERB share the ?2nd-person? feature.
8We denote inflections by s because they represent ?slots? in
paradigms (or, in the metaphor of section 6.7, ?seats? at tables in
a Chinese restaurant). These slots (or seats) are filled by words.
620
A paradigm for t ? T is a mapping pi : St ? ??,
specifying a spelling for each inflection in St. Table 1
shows one VERB paradigm.
A lexeme ` is an abstract element of some lexical
space L. Lexemes have no internal semantic struc-
ture: the only question we can ask about a lexeme is
whether it is equal to some other lexeme. There is no
upper bound on how many lexemes can be discovered
in a text corpus; L is infinite.
5.2 Random Quantities
Our generative model of the corpus is a joint probabil-
ity distribution over a collection of random variables.
We describe them in the same order as section 1.2.
Tokens: The corpus is represented by token vari-
ables. In our setting the sequence of words ~w =
w1, . . . , wn ? ?? is observed, along with n. We
must recover the corresponding part-of-speech tags
~t = t1, . . . , tn ? T , lexemes ~` = `1, . . . , `n ? L,
and inflections ~s = s1, . . . , sn, where (?i)si ? Sti .
Types: The lexicon is represented by type
variables. For each of the infinitely many lex-
emes ` ? L, and each t ? T , the paradigm
pit,` is a function St ? ??. For example,
Table 1 shows a possible value piVERB,b&r?a?k.The various spellings in the paradigm, such as
piVERB,b&r?a?k(1st-person sing. pres.)=breche, arestring-valued random variables that are correlated
with one another.
Since the lexicon is to be probabilistic (section 2.3),
Gt(`) denotes tag t?s distribution over lexemes ` ?
L, while Ht,`(s) denotes the tagged lexeme (t, `)?s
distribution over inflections s ? St.
Grammar: Global properties of the language are
captured by grammar variables that cut across lex-
ical entries: our parameters ~? that describe typical
inflectional alternations, plus parameters ~?t, ?t, ??t, ~?
(explained below). Their values control the overall
shape of the probabilistic lexicon that is generated.
6 The Formal Generative Model
We now fully describe the generative process that
was sketched in section 2. Step by step, it randomly
chooses an assignment to all the random variables of
section 5.2. Thus, a given assignment?s probability?
which section 3?s algorithms consult in order to re-
sample or improve the current assignment?is the
product of the probabilities of the individual choices,
as described in the sections below. (Appendix B
provides a drawing of this as a graphical model.)
6.1 Grammar Variables p(~?), p(???t), p(?t), p(??t)
First select the grammar variables from a prior. (We
will see below how these variables get used.) Our
experiments used fairly flat priors. Each weight in ~?
or ???t is drawn IID from N (0, 10), and each ?t or ??t
from a Gamma with mode 10 and variance 1000.
6.2 Paradigms p(pit,` | ~?)
For each t ? T , let Dt(pi) denote the distribution
over paradigms that was presented in section 2.2
(where it was called p(pi)). Dt is fully specified by
our graphical model for paradigms of part of speech
t, together with its parameters ~? as generated above.
This is the linguistic core of our model. It consid-
ers spellings: DVERB describes what verb paradigms
typically look like in the language (e.g., Table 1).
Parameters in ~? may be shared across parts of
speech t. These ?backoff? parameters capture gen-
eral phonotactics of the language, such as prohibited
letter bigrams or plausible vowel changes.
For each possible tagged lexeme (t, `), we now
draw a paradigm pit,` fromDt. Most of these lexemes
will end up having probability 0 in the language.
6.3 Lexical Distributions p(Gt | ?t)
We now formalize section 2.3. For each t ? T , the
language has a distribution Gt(`) over lexemes. We
draw Gt from a Dirichlet process DP(G,?t), where
G is the base distribution over L, and ?t > 0 is
a concentration parameter generated above. If ?t
is small, then Gt will tend to have the property that
most of its probability mass falls on relatively few
of the lexemes in Lt def= {` ? L : Gt(`) > 0}. A
closed-class tag is one whose ?t is especially small.
For G to be a uniform distribution over an infinite
lexeme set L, we need L to be uncountable.9 How-
ever, it turns out10 that with probability 1, each Lt
is countably infinite, and all the Lt are disjoint. So
each lexeme ` ? L is selected by at most one tag t.
9For example, L def= [0, 1], so that b&r?a?k is merely a sugges-
tive nickname for a lexeme such as 0.2538159.
10This can be seen by considering the stick-breaking construc-
tion of the Dirichlet process that (Sethuraman, 1994; Teh et al,
2006). A separate stick is broken for each Gt. See Appendix A.
621
6.4 Inflectional Distributions p(Ht,` | ???t, ??t)
For each tagged lexeme (t, `), the language specifies
some distribution Ht,` over its inflections.
First we construct backoff distributions Ht that are
independent of `. For each tag t ? T , let Ht be some
base distribution over St. As St could be large in
some languages, we exploit its grid structure (Table 1)
to reduce the number of parameters of Ht. We take
Ht to be a log-linear distribution with parameters ???t
that refer to features of inflections. E.g., the 2nd-
person inflections might be systematically rare.
Now we model each Ht,` as an independent draw
from a finite-dimensional Dirichlet distribution with
meanHt and concentration parameter ??t. E.g., t?h?i?n?k
might be biased toward 1st-person sing. present.
6.5 Part-of-Speech Tag Sequence p(~t | ~?)
In our current experiments, ~t is given. But in general,
to discover tags and inflections simultaneously, we
can suppose that the tag sequence ~t (and its length n)
are generated by a Markov model, with tag bigram or
trigram probabilities specified by some parameters ~? .
6.6 Lexemes p(`i | Gti)
We turn to section 2.4. A lexeme token depends on
its tag: draw `i from Gti , so p(`i | Gti) = Gti(`i).
6.7 Inflections p(si | Hti,`i)
An inflection slot depends on its tagged lexeme: we
draw si from Hti,`i , so p(si | Hti,`i) = Hti,`i(si).
6.8 Spell-out p(wi | piti,`i(si))
Finally, we generate the word wi through a determin-
istic spell-out step.11 Given the tag, lexeme, and in-
flection at position i, we generate the word wi simply
by looking up its spelling in the appropriate paradigm.
So p(wi | piti,`i(si)) is 1 if wi = piti,`i(si), else 0.
6.9 Collapsing the Assignment
Again, a full assignment?s probability is the product
of all the above factors (see drawing in Appendix B).
11To account for typographical errors in the corpus, the spell-
out process could easily be made nondeterministic, with the
observed word wi derived from the correct spelling piti,`i(si)
by a noisy channel model (e.g., (Toutanova and Moore, 2002))
represented as a WFST. This would make it possible to analyze
brkoen as a misspelling of a common or contextually likely
word, rather than treating it as an unpronounceable, irregularly
inflected neologism, which is presumably less likely.
But computationally, our sampler?s state leaves the
Gt unspecified. So its probability is the integral of
p(assignment) over all possible Gt. As Gt appears
only in the factors from headings 6.3 and 6.6, we can
just integrate it out of their product, to get a collapsed
sub-model that generates p(~` | ~t, ~?) directly:
?
GADJ
? ? ?
?
GVERB
dG
(?
t?T
p(Gt | ?t)
)( n?
i=1
p(`i | Gti)
)
= p(~` | ~t, ~?) =
n?
i=1
p(`i | `1, . . . `i?1 ~t, ~?)
where it turns out that the factor that generates `i is
proportional to |{j < i : `j = `i and tj = ti}| if that
integer is positive, else proportional to ?tiG(`i).
Metaphorically, each tag t is a Chinese restaurant
whose tables are labeled with lexemes. The tokens
are hungry customers. Each customer i = 1, 2, . . . , n
enters restaurant ti in turn, and `i denotes the label
of the table she joins. She picks an occupied table
with probability proportional to the number of pre-
vious customers already there, or with probability
proportional to ?ti she starts a new table whose label
is drawn from G (it is novel with probability 1, since
G gives infinitesimal probability to each old label).
Similarly, we integrate out the infinitely many
lexeme-specific distributionsHt,` from the product of
6.4 and 6.7, replacing it by the collapsed distribution
p(~s | ~`,~t,???t,
??
??) [recall that ???t determines Ht]
=
n?
i=1
p(si | s1, . . . si?1, ~`,~t,
???t,
??
??)
where the factor for si is proportional to |{j < i :
sj = si and (tj , `j) = (ti, `i)}|+ ??tiHti(si).Metaphorically, each table ` in Chinese restaurant
t has a fixed, finite set of seats corresponding to the
inflections s ? St. Each seat is really a bench that
can hold any number of customers (tokens). When
customer i chooses to sit at table `i, she also chooses
a seat si at that table (see Fig. 2), choosing either an
already occupied seat with probability proportional to
the number of customers already in that seat, or else
a random seat (sampled from Hti and not necessarily
empty) with probability proportional to ??ti .
7 Inference and Learning
As section 3 explained, the learner alternates between
a Monte Carlo E step that uses Gibbs sampling to
622
sample from the posterior of (~s, ~`,~t) given ~w and the
grammar variables, and an M step that adjusts the
grammar variables to maximize the probability of the
(~w,~s, ~`,~t) samples given those variables.
7.1 Block Gibbs Sampling
As in Gibbs sampling for the DPMM, our sampler?s
basic move is to reanalyze token i (see section 3).
This corresponds to making customer i invisible and
then guessing where she is probably sitting?which
restaurant t, table `, and seat s??given knowledge
of wi and the locations of all other customers.12
Concretely, the sampler guesses location (ti, `i, si)
with probability proportional to the product of
? p(ti | ti?1, ti+1, ~?) (from section 6.5)
? the probability (from section 6.9) that a new cus-
tomer in restaurant ti chooses table `i, given the
other customers in that restaurant (and ?ti)13
? the probability (from section 6.9) that a new
customer at table `i chooses seat si, given the
other customers at that table (and ???ti and ??ti)13
? the probability (from section 3.3?s belief propa-
gation) that piti,`i(si) = wi (given ~?).
We sample only from the (ti, `i, si) candidates for
which the last factor is non-negligible. These are
found with the hash tables and FSAs of section 3.3.
7.2 Semi-Supervised Sampling
Our experiments also consider the semi-supervised
case where a few seed paradigms?type data?are
fully or partially observed. Our samples should also
be conditioned on these observations. We assume
that our supervised list of observed paradigms was
generated by sampling from Gt.14 We can modify
our setup for this case: certain tables have a host
who dictates the spelling of some seats and attracts
appropriate customers to the table. See Appendix C.
7.3 Parameter Gradients
Appendix D gives formulas for the M step gradients.
12Actually, to improve mixing time, we choose a currently
active lexeme ` uniformly at random, make all customers {i :
`i = `} invisible, and sequentially guess where they are sitting.
13This is simple to find thanks to the exchangeability of the
CRP, which lets us pretend that i entered the restaurant last.
14Implying that they are assigned to lexemes with non-
negligible probability. We would learn nothing from a list of
merely possible paradigms, since Lt is infinite and every con-
ceivable paradigm is assigned to some ` ? Lt (in fact many!).
50 seed paradigms 100 seed paradigms
Corpus size 0 106 107 0 106 107
Accuracy 89.9 90.6 90.9 91.5 92.0 92.2
Edit dist. 0.20 0.19 0.18 0.18 0.17 0.17
Table 2: Whole-word accuracy and edit distance of pre-
dicted inflection forms given the lemma. Edit distance to
the correct form is measured in characters. Best numbers
per set of seed paradigms in bold (statistically signifi-
cant on our large test set under a paired permutation test,
p < 0.05). Appendix E breaks down these results per
inflection and gives an error analysis and other statistics.
8 Experiments
8.1 Experimental Design
We evaluated how well our model learns German
verbal morphology. As corpus we used the first 1
million or 10 million words from WaCky (Baroni
et al, 2009). For seed and test paradigms we used
verbal inflectional paradigms from the CELEX mor-
phological database (Baayen et al, 1995). We fully
observed the seed paradigms. For each test paradigm,
we observed the lemma type (Appendix C) and eval-
uated how well the system completed the other 21
forms (see Appendix E.2) in the paradigm.
We simplified inference by fixing the POS tag
sequence to the automatic tags delivered with the
WaCky corpus. The result that we evaluated for each
variable was the value whose probability, averaged
over the entire Monte Carlo EM run,15 was highest.
For more details, see (Dreyer, 2011).
All results are averaged over 10 different train-
ing/test splits of the CELEX data. Each split sampled
100 paradigms as seed data and used the remain-
ing 5,415 paradigms for evaluation.16 From the 100
paradigms, we also sampled 50 to obtain results with
smaller seed data.17
8.2 Results
Type-based Evaluation. Table 2 shows the results
of predicting verb inflections, when running with no
corpus, versus with an unannotated corpus of size 106
and 107 words. Just using 50 seed paradigms, but
15This includes samples from before ~? has converged, some-
what like the voted perceptron (Freund and Schapire, 1999).
16100 further paradigms were held out for future use.
17Since these seed paradigms are sampled uniformly from a
set of CELEX paradigms, most of them are regular. We actually
only used 90 and 40 for training, reserving 10 as development
data for sanity checks and for deciding when to stop.
623
Bin Frequency # Verb Forms
1 0?9 116,776
2 10?99 4,623
3 100?999 1,048
4 1,000?9,999 95
5 10,000? 10
all any 122,552
Table 3: The inflected verb forms from 5,615 inflectional
paradigms, split into 5 token frequency bins. The frequen-
cies are based on the 10-million word corpus.
no corpus, gives an accuracy of 89.9%. By adding
a corpus of 10 million words we reduce the error
rate by 10%, corresponding to a one-point increase
in absolute accuracy to 90.9%. A similar trend can
be seen when we use more seed paradigms. Sim-
ply training on 100 seed paradigms, but not using a
corpus, results in an accuracy of 91.5%. Adding a
corpus of 10 million words to these 100 paradigms re-
duces the error rate by 8.3%, increasing the absolute
accuracy to 92.2%. Compared to the large corpus,
the smaller corpus of 1 million words goes more than
half the way; it results in error reductions of 6.9%
(50 seed paradigms) and 5.8% (100 seed paradigms).
Larger unsupervised corpora should help by increas-
ing coverage even more, although Zipf?s law implies
a diminishing rate of return.18
We also tested a baseline that simply inflects each
morphological form according to the basic regular
German inflection pattern; this reaches an accuracy
of only 84.5%.
Token-based Evaluation. We now split our results
into different bins: how well do we predict the
spellings of frequently expressed (lexeme, inflection)
pairs as opposed to rare ones? For example, the third
person singular indicative of ?g?i?v (geben) is used
significantly more often than the second person plural
subjunctive of b$a?s?k (aalen);19 they are in different
frequency bins (Table 3). The more frequent a form
is in text, the more likely it is to be irregular (Jurafsky
et al, 2000, p. 49).
The results in Table 4 show: Adding a corpus of
either 1 or 10 million words increases our prediction
accuracy across all frequency bins, often dramati-
cally. All methods do best on the huge number of
18Considering the 63,778 distinct spellings from all of our
5,615 CELEX paradigms, we find that the smaller corpus con-
tains 7,376 spellings and the 10? larger corpus contains 13,572.
19See Appendix F for how this was estimated from text.
50 seed paradigms 100 seed paradigms
Bin 0 106 107 0 106 107
1 90.5 91.0 91.3 92.1 92.4 92.6
2 78.1 84.5 84.4 80.2 85.5 85.1
3 71.6 79.3 78.1 73.3 80.2 79.1
4 57.4 61.4 61.8 57.4 62.0 59.9
5 20.7 25.0 25.0 20.7 25.0 25.0
all 52.6 57.5 57.8 53.4 58.5 57.8
all (e.d.) 1.18 1.07 1.03 1.16 1.02 1.01
Table 4: Token-based analysis: Whole-word accuracy re-
sults split into different frequency bins. In the last two
rows, all predictions are included, weighted by the fre-
quency of the form to predict. Last row is edit distance.
rare forms (Bin 1), which are mostly regular, and
worst on on the 10 most frequent forms of the lan-
guage (Bin 5). However, adding a corpus helps most
in fixing the errors in bins with more frequent and
hence more irregular verbs: in Bins 2?5 we observe
improvements of up to almost 8% absolute percent-
age points. In Bin 1, the no-corpus baseline is already
relatively strong.
Surprisingly, while we always observe gains from
using a corpus, the gains from the 10-million-word
corpus are sometimes smaller than the gains from the
1-million-word corpus, except in edit distance. Why?
The larger corpus mostly adds new infrequent types,
biasing ~? toward regular morphology at the expense
of irregular types. A solution might be to model irreg-
ular classes with separate parameters, using the latent
conjugation-class model of Dreyer et al (2008).
Note that, by using a corpus, we even improve
our prediction accuracy for forms and spellings that
are not found in the corpus, i.e., novel words. This
is thanks to improved grammar parameters. In the
token-based analysis above we have already seen that
prediction accuracy increases for rare forms (Bin 1).
We add two more analyses that more explicitly show
our performance on novel words. (a) We find all
paradigms that consist of novel spellings only, i.e.
none of the correct spellings can be found in the
corpus.20 The whole-word prediction accuracies for
the models that use corpus size 0, 1 million, and
10 million words are, respectively, 94.0%, 94.2%,
94.4% using 50 seed paradigms, and 95.1%, 95.3%,
95.2% using 100 seed paradigms. (b) Another, sim-
20This is measured on the largest corpus used in inference, the
10-million-word corpus, so that we can evaluate all models on
the same set of paradigms.
624
pler measure is the prediction accuracy on all forms
whose correct spelling cannot be found in the 10-
million-word corpus. Here we measure accuracies
of 91.6%, 91.8% and 91.8%, respectively, using 50
seed paradigms. With 100 seed paradigms, we have
93.0%, 93.4% and 93.1%. The accuracies for the
models that use a corpus are higher, but do not al-
ways steadily increase as we increase the corpus size.
The token-based analysis we have conducted here
shows the strength of the corpus-based approach pre-
sented in this paper. While the integrated graphi-
cal models over strings (Dreyer and Eisner, 2009)
can learn some basic morphology from the seed
paradigms, the added corpus plays an important role
in correcting its mistakes, especially for the more fre-
quent, irregular verb forms. For examples of specific
errors that the models make, see Appendix E.3.
9 Related Work
Our word-and-paradigm model seamlessly handles
nonconcatenative and concatenative morphology
alike, whereas most previous work in morphological
knowledge discovery has modeled concatenative mor-
phology only, assuming that the orthographic form
of a word can be split neatly into stem and affixes?a
simplifying asssumption that is convenient but often
not entirely appropriate (Kay, 1987) (how should one
segment English stopping, hoping, or knives?).
In concatenative work, Harris (1955) finds mor-
pheme boundaries and segments words accordingly,
an approach that was later refined by Hafer and
Weiss (1974), D?jean (1998), and many others. The
unsupervised segmentation task is tackled in the
annual Morpho Challenge (Kurimo et al, 2010),
where ParaMor (Monson et al, 2007) and Morfessor
(Creutz and Lagus, 2005) are influential contenders.
The Bayesian methods that Goldwater et al (2006b,
et seq.) use to segment between words might also be
applied to segment within words, but have no notion
of paradigms. Goldsmith (2001) finds what he calls
signatures?sets of affixes that are used with a given
set of stems, for example (NULL, -er, -ing, -s).
Chan (2006) learns sets of morphologically related
words; he calls these sets paradigms but notes that
they are not substructured entities, in contrast to the
paradigms we model in this paper. His models are
restricted to concatenative and regular morphology.
Morphology discovery approaches that han-
dle nonconcatenative and irregular phenomena
are more closely related to our work; they are
rarer. Yarowsky and Wicentowski (2000) identify
inflection-root pairs in large corpora without supervi-
sion. Using similarity as well as distributional clues,
they identify even irregular pairs like take/took.
Schone and Jurafsky (2001) and Baroni et al (2002)
extract whole conflation sets, like ?abuse, abused,
abuses, abusive, abusively, . . . ,? which may
also be irregular. We advance this work by not only
extracting pairs or sets of related observed words,
but whole structured inflectional paradigms, in which
we can also predict forms that have never been ob-
served. On the other hand, our present model does
not yet use contextual information; we regard this as
a future opportunity (see Appendix G). Naradowsky
and Goldwater (2009) add simple spelling rules to
the Bayesian model of (Goldwater et al, 2006a), en-
abling it to handle some systematically nonconcate-
native cases. Our finite-state transducers can handle
more diverse morphological phenomena.
10 Conclusions and Future Work
We have formulated a principled framework for si-
multaneously obtaining morphological annotation,
an unbounded morphological lexicon that fills com-
plete structured morphological paradigms with ob-
served and predicted words, and parameters of a non-
concatenative generative morphology model.
We ran our sampler over a large corpus (10 million
words), inferring everything jointly and reducing the
prediction error for morphological inflections by up
to 10%. We observed that adding a corpus increases
the absolute prediction accuracy on frequently occur-
ring morphological forms by up to almost 8%. Future
extensions to the model could leverage token context
for further improvements (Appendix G).
We believe that a major goal of our field should be
to build full-scale explanatory probabilistic models
of language. While we focus here on inflectional
morphology and evaluate the results in isolation, we
regard the present work as a significant step toward
a larger generative model under which Bayesian
inference would reconstruct other relationships as
well (e.g., inflectional, derivational, and evolution-
ary) among the words in a family of languages.
625
References
A. C. Albright. 2002. The Identification of Bases in
Morphological Paradigms. Ph.D. thesis, University of
California, Los Angeles.
D. Aldous. 1985. Exchangeability and related topics.
?cole d??t? de probabilit?s de Saint-Flour XIII, pages
1?198.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric problems.
Annals of Statistics, 2(6):1152?1174.
R. H Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX lexical database (release 2)[cd-rom]. Philadel-
phia, PA: Linguistic Data Consortium, University of
Pennsylvania [Distributor].
M. Baroni, J. Matiasek, and H. Trost. 2002. Unsupervised
discovery of morphologically related words based on
orthographic and semantic similarity. In Proc. of the
ACL-02 Workshop on Morphological and Phonological
Learning, pages 48?57.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky Wide Web: A collection of very
large linguistically processed web-crawled corpora.
Language Resources and Evaluation, 43(3):209?226.
David Blackwell and James B. MacQueen. 1973. Fergu-
son distributions via P?lya urn schemes. The Annals of
Statistics, 1(2):353?355, March.
David M. Blei and Peter I. Frazier. 2010. Distance-
dependent Chinese restaurant processes. In Proc. of
ICML, pages 87?94.
E. Chan. 2006. Learning probabilistic paradigms for
morphology in a latent class model. In Proceedings of
the Eighth Meeting of the ACL Special Interest Group
on Computational Phonology at HLT-NAACL, pages
69?78.
M. Creutz and K. Lagus. 2005. Unsupervised morpheme
segmentation and morphology induction from text cor-
pora using Morfessor 1.0. Computer and Information
Science, Report A, 81.
H. D?jean. 1998. Morphemes as necessary concept
for structures discovery from untagged corpora. In
Proc. of the Joint Conferences on New Methods in
Language Processing and Computational Natural Lan-
guage Learning, pages 295?298.
Markus Dreyer and Jason Eisner. 2009. Graphical models
over multiple strings. In Proc. of EMNLP, Singapore,
August.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proc. of EMNLP, Honolulu,
Hawaii, October.
Markus Dreyer. 2011. A Non-Parametric Model for the
Discovery of Inflectional Paradigms from Plain Text
Using Graphical Models over Strings. Ph.D. thesis,
Johns Hopkins University.
T.S. Ferguson. 1973. A Bayesian analysis of some non-
parametric problems. The annals of statistics, 1(2):209?
230.
Y. Freund and R. Schapire. 1999. Large margin classifica-
tion using the perceptron algorithm. Machine Learning,
37(3):277?296.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Linguis-
tics, 27(2):153?198.
S. Goldwater, T. Griffiths, and M. Johnson. 2006a. In-
terpolating between types and tokens by estimating
power-law generators. In Proc. of NIPS, volume 18,
pages 459?466.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006b.
Contextual dependencies in unsupervised word seg-
mentation. In Proc. of COLING-ACL.
P.J. Green. 1995. Reversible jump Markov chain Monte
Carlo computation and Bayesian model determination.
Biometrika, 82(4):711.
M. A Hafer and S. F Weiss. 1974. Word segmentation
by letter successor varieties. Information Storage and
Retrieval, 10:371?385.
Z. S. Harris. 1955. From phoneme to morpheme. Lan-
guage, 31(2):190?222.
G.E. Hinton. 2002. Training products of experts by min-
imizing contrastive divergence. Neural Computation,
14(8):1771?1800.
Klara Janecki. 2000. 300 Polish Verbs. Barron?s Educa-
tional Series.
E. T. Jaynes. 2003. Probability Theory: The Logic of
Science. Cambridge Univ Press. Edited by Larry Bret-
thorst.
D. Jurafsky, J. H. Martin, A. Kehler, K. Vander Linden,
and N. Ward. 2000. Speech and Language Processing:
An Introduction to Natural Language Processing, Com-
putational Linguistics, and Speech Recognition. MIT
Press.
M. Kay. 1987. Nonconcatenative finite-state morphology.
In Proc. of EACL, pages 2?10.
M. Kurimo, S. Virpioja, V. Turunen, and K. Lagus. 2010.
Morpho Challenge competition 2005?2010: Evalua-
tions and results. In Proc. of ACL SIGMORPHON,
pages 87?95.
P. H. Matthews. 1972. Inflectional Morphology: A Theo-
retical Study Based on Aspects of Latin Verb Conjuga-
tion. Cambridge University Press.
Christian Monson, Jaime Carbonell, Alon Lavie, and Lori
Levin. 2007. ParaMor: Minimally supervised induc-
tion of paradigm structure and morphological analysis.
In Proc. of ACL SIGMORPHON, pages 117?125, June.
626
J. Naradowsky and S. Goldwater. 2009. Improving mor-
phology induction by learning spelling rules. In Proc.
of IJCAI, pages 1531?1536.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordinator.
Annals of Probability, 25:855?900.
P. Schone and D. Jurafsky. 2001. Knowledge-free induc-
tion of inflectional morphologies. In Proc. of NAACL,
volume 183, pages 183?191.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4(2):639?650.
N. A. Smith, D. A. Smith, and R. W. Tromble. 2005.
Context-based morphological disambiguation with ran-
dom fields. In Proceedings of HLT-EMNLP, pages
475?482, October.
G. T. Stump. 2001. Inflectional Morphology: A Theory of
Paradigm Structure. Cambridge University Press.
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101(476):1566?1581.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL.
K. Toutanova and R.C. Moore. 2002. Pronunciation
modeling for improved spelling correction. In Proc. of
ACL, pages 144?151.
D. Yarowsky and R. Wicentowski. 2000. Minimally su-
pervised morphological analysis by multimodal align-
ment. In Proc. of ACL, pages 207?216, October.
627
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 920?929,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Minimum Imputed Risk: Unsupervised Discriminative Training for
Machine Translation
Zhifei Li?
Google Research
Mountain View, CA 94043, USA
zhifei.work@gmail.com
Ziyuan Wang, Sanjeev Khudanpur
Johns Hopkins University
Baltimore, MD 21218, USA
zwang40,khudanpur@jhu.edu
Jason Eisner
Johns Hopkins University
Baltimore, MD 21218, USA
eisner@jhu.edu
Brian Roark
Oregon Health & Science University
Beaverton, Oregon 97006, USA
roark@cslu.ogi.edu
Abstract
Discriminative training for machine transla-
tion has been well studied in the recent past.
A limitation of the work to date is that it relies
on the availability of high-quality in-domain
bilingual text for supervised training. We
present an unsupervised discriminative train-
ing framework to incorporate the usually plen-
tiful target-language monolingual data by us-
ing a rough ?reverse? translation system. Intu-
itively, our method strives to ensure that prob-
abilistic ?round-trip? translation from a target-
language sentence to the source-language and
back will have low expected loss. Theoret-
ically, this may be justified as (discrimina-
tively) minimizing an imputed empirical risk.
Empirically, we demonstrate that augment-
ing supervised training with unsupervised data
improves translation performance over the su-
pervised case for both IWSLT and NIST tasks.
1 Introduction
Missing data is a common problem in statistics when
fitting the parameters ? of a model. A common strat-
egy is to attempt to impute, or ?fill in,? the missing
data (Little and Rubin, 1987), as typified by the EM
algorithm. In this paper we develop imputation tech-
niques when ? is to be trained discriminatively.
We focus on machine translation (MT) as our ex-
ample application. A Chinese-to-English machine
translation system is given a Chinese sentence x and
? Zhifei Li is currently working at Google Research, and
this work was done while he was a PHD student at Johns Hop-
kins University.
asked to predict its English translation y. This sys-
tem employs statistical models p?(y | x) whose pa-
rameters ? are discriminatively trained using bilin-
gual sentence pairs (x, y). But bilingual data for
such supervised training may be relatively scarce for
a particular language pair (e.g., Urdu-English), es-
pecially for some topics (e.g., technical manuals) or
genres (e.g., blogs). So systems seek to exploit ad-
ditional monolingual data, i.e., a corpus of English
sentences y with no corresponding source-language
sentences x, to improve estimation of ?. This is our
missing data scenario.1
Discriminative training of the parameters ? of
p?(y | x) using monolingual English data is a cu-
rious idea, since there is no Chinese input x to trans-
late. We propose an unsupervised training approach,
called minimum imputed risk training, which is con-
ceptually straightforward: First guess x (probabilis-
tically) from the observed y using a reverse English-
to-Chinese translation model p?(x | y). Then train
the discriminative Chinese-to-English model p?(y |
x) to do a good job at translating this imputed x
back to y, as measured by a given performance met-
ric. Intuitively, our method strives to ensure that
probabilistic ?round-trip? translation from a target-
language sentence to the source-language and back
again will have low expected loss.
Our approach can be applied in an application
scenario where we have (1) enough out-of-domain
bilingual data to build two baseline translation sys-
tems, with parameters ? for the forward direction,
and ? for the reverse direction; (2) a small amount
1Contrast this with traditional semi-supervised training that
looks to exploit ?unlabeled? inputs x, with missing outputs y.
920
of in-domain bilingual development data to discrim-
inatively tune a small number of parameters in ?;
and (3) a large amount of in-domain English mono-
lingual data.
The novelty here is to exploit (3) to discrimina-
tively tune the parameters ? of all translation model
components,2 p?(y|x) and p?(y), not merely train a
generative language model p?(y), as is the norm.
Following the theoretical development below, the
empirical effectiveness of our approach is demon-
strated by replacing a key supervised discriminative
training step in the development of large MT sys-
tems ? learning the log-linear combination of sev-
eral component model scores (viewed as features) to
optimize a performance metric (e.g. BLEU) on a set
of (x, y) pairs ? with our unsupervised discrimina-
tive training using only y. One may hence contrast
our approach with the traditional supervised meth-
ods applied to the MT task such as minimum error
rate training (Och, 2003; Macherey et al, 2008), the
averaged Perceptron (Liang et al, 2006), maximum
conditional likelihood (Blunsom et al, 2008), min-
imum risk (Smith and Eisner, 2006; Li and Eisner,
2009), and MIRA (Watanabe et al, 2007; Chiang et
al., 2009).
We perform experiments using the open-source
MT toolkit Joshua (Li et al, 2009a), and show that
adding unsupervised data to the traditional super-
vised training setup improves performance.
2 Supervised Discriminative Training via
Minimization of Empirical Risk
Let us first review discriminative training in the su-
pervised setting?as used in MERT (Och, 2003) and
subsequent work.
One wishes to tune the parameters ? of some
complex translation system ??(x). The function ??,
which translates Chinese x to English y = ??(x)
need not be probabilistic. For example, ? may be
the parameters of a scoring function used by ?, along
with pruning and decoding heuristics, for extracting
a high-scoring translation of x.
The goal of discriminative training is to mini-
mize the expected loss of ??(?), under a given task-
specific loss function L(y?, y) that measures how
2Note that the extra monolingual data is used only for tuning
the model weights, but not for inducing new phrases or rules.
bad it would be to output y? when the correct output
is y. For an MT system that is judged by the BLEU
metric (Papineni et al, 2001), for instance, L(y?, y)
may be the negated BLEU score of y? w.r.t. y. To be
precise, the goal3 is to find ? with low Bayes risk,
?? = argmin
?
?
x,y
p(x, y)L(??(x), y) (1)
where p(x, y) is the joint distribution of the input-
output pairs.4
The true p(x, y) is, of course, not known and,
in practice, one typically minimizes empirical risk
by replacing p(x, y) above with the empirical dis-
tribution p?(x, y) given by a supervised training set
{(xi, yi), i = 1, . . . , N}. Therefore,
?? = argmin
?
?
x,y
p?(x, y)L(??(x), y)
= argmin
?
1
N
N?
i=1
L(??(xi), yi). (2)
The search for ?? typically requires the use of nu-
merical methods and some regularization.5
3 Unsupervised Discriminative Training
with Missing Inputs
3.1 Minimization of Imputed Risk
We now turn to the unsupervised case, where we
have training examples {yi} but not their corre-
sponding inputs {xi}. We cannot compute the sum-
mand L(??(xi), yi) for such i in (2), since ??(xi)
requires to know xi. So we propose to replace
3This goal is different from the minimum risk training of
Li and Eisner (2009) in a subtle but important way. In both
cases, ?? minimizes risk or expected loss, but the expectation
is w.r.t. different distributions: the expectation in Li and Eisner
(2009) is under the conditional distribution p(y |x), while the
expectation in (1) is under the joint distribution p(x, y).
4In the terminology of statistical decision theory, p(x, y) is
a distribution over states of nature. We seek a decision rule
??(x) that will incur low expected loss on observations x that
are generated from unseen states of nature.
5To compensate for the shortcut of using the unsmoothed
empirical distribution rather than a posterior estimate of p(x, y)
(Minka, 2000), it is common to add a regularization term ||?||22
in the objective of (2). The regularization term can prevent over-
fitting to a training set that is not large enough to learn all pa-
rameters.
921
L(??(xi), yi) with the expectation
?
x
p?(x | yi)L(??(x), yi), (3)
where p?(? | ?) is a ?reverse prediction model? that
attempts to impute the missing xi data. We call the
resulting variant of (2) the minimization of imputed
empirical risk, and say that
?? = argmin
?
1
N
N?
i=1
?
x
p?(x | yi)L(??(x), yi) (4)
is the estimate with the minimum imputed risk6.
The minimum imputed risk objective of (4) could
be evaluated by brute force as follows.
1. For each unsupervised example yi, use the re-
verse prediction model p?(? | yi) to impute pos-
sible reverse translations Xi = {xi1, xi2, . . .},
and add each (xij , yi) pair, weighted by
p?(xij | yi) ? 1, to an imputed training set .
2. Perform the supervised training of (2) on the
imputed and weighted training data.
The second step means that we must use ?? to
forward-translate each imputed xij , evaluate the loss
of the translations y?ij against the corresponding true
translation yi, and choose the ? that minimizes the
weighted sum of these losses (i.e., the empirical risk
when the empirical distribution p?(x, y) is derived
from the imputed training set). Specific to our MT
task, this tries to ensure that probabilistic ?round-
trip? translation, from the target-language sentence
yi to the source-language and back again, will have
a low expected loss.7
The trouble with this method is that the reverse
model p? generates a weighted lattice or hyper-
graph Xi encoding exponentially many translations
of yi, and it is computationally infeasible to forward-
translate each xij ? Xi. We therefore investigate
several approximations to (4) in Section 3.4.
6One may exploit both supervised data {(xi, yi)} and unsu-
pervised data {yj} to perform semi-supervised training via an
interpolation of (2) and (4). We will do so in our experiments.
7Our approach may be applied to other tasks as well. For
example, in a speech recognition task, ?? is a speech recognizer
that produces text, whereas p? is a speech synthesizer that must
produce a distribution over audio (or at least over acoustic fea-
tures or phone sequences) (Huang et al, 2010).
3.2 The Reverse Prediction Model p?
A crucial ingredient in (4) is the reverse prediction
model p?(?|?) that attempts to impute the missing xi.
We will train this model in advance, doing the best
job we can from available data, including any out-
of-domain bilingual data as well as any in-domain
monolingual data8 x.
In the MT setting, ?? and p? may have similar pa-
rameterization. One translates Chinese to English;
the other translates English to Chinese.
Yet the setup is not quite symmetric. Whereas ??
is a translation system that aims to produce a single,
low-loss translation, the reverse version p? is rather
a probabilistic model. It is supposed to give an accu-
rate probability distribution over possible values xij
of the missing input sentence xi. All of these val-
ues are taken into account in (4), regardless of the
loss that they would incur if they were evaluated for
translation quality relative to the missing xi.
Thus, ? does not need to be trained to minimize
the risk itself (so there is no circularity). Ideally,
it should be trained to match the underlying condi-
tional distribution of x given y, by achieving a low
conditional cross-entropy
H(X |Y ) = ?
?
x,y
p(x, y) log p?(x | y). (5)
In practice, ? is trained by (empirically) minimiz-
ing ? 1M
?N
j=1 log p?(xj | yj) + 12?2 ???22 on some
bilingual data, with the regularization coefficient ?2
tuned on held out data.
It may be tolerable for p? to impute mediocre
translations xij . All that is necessary is that the (for-
ward) translations generated from the imputed xij
?simulate? the competing hypotheses that we would
see when translating the correct Chinese input xi.
3.3 The Forward Translation System ?? and
The Loss Function L(??(xi), yi)
The minimum empirical risk objective of (2) is
quite general and various popular supervised train-
ing methods (Lafferty et al, 2001; Collins, 2002;
Och, 2003; Crammer et al, 2006; Smith and Eisner,
8In a translation task from x to y, one usually does not make
use of in-domain monolingual data x. But we can exploit x to
train a language model p?(x) for the reverse translation system,
which will make the imputed xij look like true Chinese inputs.
922
2006) can be formalized in this framework by choos-
ing different functions for ?? and L(??(xi), yi). The
generality of (2) extends to our minimum imputed
risk objective of (4). Below, we specify the ?? and
L(??(xi), yi) we considered in our investigation.
3.3.1 Deterministic Decoding
A simple translation rule would define
??(x) = argmax
y
p?(y |x) (6)
If this ??(x) is used together with a loss function
L(??(xi), yi) that is the negated BLEU score9, our
minimum imputed risk objective of (4) is equivalent
to MERT (Och, 2003) on the imputed training data.
However, this would not yield a differentiable ob-
jective function. Infinitesimal changes to ? could re-
sult in discrete changes to the winning output string
??(x) in (6), and hence to the loss L(??(x), yi). Och
(2003) developed a specialized line search to per-
form the optimization, which is not scalable when
the number of model parameters ? is large.
3.3.2 Randomized Decoding
Instead of using the argmax of (6), we assume
during training that ??(x) is itself random, i.e. the
MT system randomly outputs a translation y with
probability p?(y |x). As a result, we will modify
our objective function of (4) to take yet another ex-
pectation over the unknown y. Specifically, we will
replace L(??(x), yi) in (4) with
?
y
p?(y |x)L(y, yi). (7)
Now, the minimum imputed empirical risk objective
of (4) becomes
?? = argmin
?
1
N
N?
i=1
?
x,y
p?(x | yi) p?(y |x)L(y, yi)
(8)
If the loss function L(y, yi) is a negated BLEU, this
is equivalent to performing minimum-risk training
described by (Smith and Eisner, 2006; Li and Eisner,
2009) on the imputed data.10
9One can manipulate the loss function to support other
methods that use deterministic decoding, such as Perceptron
(Collins, 2002) and MIRA (Crammer et al, 2006).
10Again, one may manipulate the loss function to support
other probabilistic methods that use randomized decoding, such
as CRFs (Lafferty et al, 2001).
The objective function in (8) is now differentiable,
since each coefficient p?(y |x) is a differentiable
function of ?, and thus amenable to optimization
by gradient-based methods; we use the L-BFGS al-
gorithm (Liu et al, 1989) in our experiments. We
perform experiments with the syntax-based MT sys-
tem Joshua (Li et al, 2009a), which implements
dynamic programming algorithms for second-order
expectation semirings (Li and Eisner, 2009) to effi-
ciently compute the gradients needed for optimizing
(8).
3.4 Approximating p?(x | yi)
As mentioned at the end of Section 3.1, it is com-
putationally infeasible to forward-translate each of
the imputed reverse translations xij . We propose
four approximations that are computationally feasi-
ble. Each may be regarded as a different approxima-
tion of p?(x | yi) in equations (4) or (8).
k-best. For each yi, add to the imputed training set
only the k most probable translations {xi1, . . . xik}
according to p?(x | yi). (These can be extracted
from Xi using standard algorithms (Huang and Chi-
ang, 2005).) Rescale their probabilities to sum to 1.
Sampling. For each yi, add to the training set k in-
dependent samples {xi1, . . . xik} from the distribu-
tion p?(x | yi), each with weight 1/k. (These can be
sampled from Xi using standard algorithms (John-
son et al, 2007).) This method is known in the liter-
ature as multiple imputation (Rubin, 1987).
Lattice. 11 Under certain special cases it is be pos-
sible to compute the expected loss in (3) exactly
via dynamic programming. Although Xi does con-
tain exponentially many translations, it may use a
?packed? representation in which these translations
share structure. This representation may further-
more enable sharing work in forward-translation, so
as to efficiently translate the entire set Xi and ob-
tain a distribution over translations y. Finally, the
expected loss under that distribution, as required by
equation (3), may also be efficiently computable.
All this turns out to be possible if (a) the poste-
rior distribution p?(x | yi) is represented by an un-
11The lattice approximation is presented here as a theoreti-
cal contribution, and we do not empirically evaluate it since its
implementation requires extensive engineering effort that is be-
yond the main scope of this paper.
923
ambiguous weighted finite-state automaton Xi, (b)
the forward translation system ?? is structured in a
certain way as a weighted synchronous context-free
grammar, and (c) the loss function decomposes in a
certain way. We omit the details of the construction
as beyond the scope of this paper.
In our experimental setting described below, (b) is
true (using Joshua), and (c) is true (since we use a
loss function presented by Tromble et al (2008) that
is an approximation to BLEU and is decomposable).
While (a) is not true in our setting because Xi is a
hypergraph (which is ambiguous), Li et al (2009b)
show how to approximate a hypergraph representa-
tion of p?(x | yi) by an unambiguous WFSA. One
could then apply the construction to this WFSA12,
obtaining an approximation to (3).
Rule-level Composition. Intuitively, the reason
why the structure-sharing in the hypergraphXi (gen-
erated by the reverse system) cannot be exploited
during forward translating is that when the forward
Hiero system translates a string xi ? Xi, it must
parse it into recursive phrases.
But the structure-sharing within the hypergraph of
Xi has already parsed xi into recursive phrases, in a
way determined by the reverse Hiero system; each
translation phrase (or rule) corresponding to a hy-
peredge. To exploit structure-sharing, we can use
a forward translation system that decomposes ac-
cording to that existing parse of xi. We can do that
by considering only forward translations that respect
the hypergraph structure of Xi. The simplest way to
do this is to require complete isomorphism of the
SCFG trees used for the reverse and forward trans-
lations. In other words, this does round-trip impu-
tation (i.e., from y to x, and then to y?) at the rule
level. This is essentially the approach taken by Li et
al. (2010).
3.5 The Log-Linear Model p?
We have not yet specified the form of p?. Following
much work in MT, we begin with a linear model
score(x, y) = ? ? f(x, y) =
?
k
?kfk(x, y) (9)
where f(x, y) is a feature vector indexed by k. Our
deterministic test-time translation system ?? simply
12Note that the forward translation of a WFSA is tractable by
using a lattice-based decoder such as that by Dyer et al (2008).
outputs the highest-scoring y for fixed x. At training
time, our randomized decoder (Section 3.3.2) uses
the Boltzmann distribution (here a log-linear model)
p?(y |x) =
e??score(x,y)
Z(x) =
e??score(x,y)?
y? e??score(x,y
?) (10)
The scaling factor ? controls the sharpness of the
training-time distribution, i.e., the degree to which
the randomized decoder favors the highest-scoring
y. For large ?, our training objective approaches
the imputed risk of the deterministic test-time sys-
tem while remaining differentiable.
In a task like MT, in addition to the input x and
output y, we often need to introduce a latent variable
d to represent the hidden derivation that relates x to
y. A derivation d represents a particular phrase seg-
mentation in a phrase-based MT system (Koehn et
al., 2003) and a derivation tree in a typical syntax-
based system (Galley et al, 2006; Chiang, 2007).
We change our model to assign scores not to an
(x, y) pair but to the detailed derivation d; in partic-
ular, now the function f that extracts a feature vector
can look at all of d. We replace y by d in (9)?(10),
and finally define p?(y|x) by marginalizing out d,
p?(y |x) =
?
d?D(x,y)
p?(d |x) (11)
where D(x, y) represents the set of derivations that
yield x and y.
4 Minimum Imputed Risk vs. EM
The notion of imputing missing data is familiar
from other settings (Little and Rubin, 1987), particu-
larly the expectation maximization (EM) algorithm,
a widely used generative approach. So it is instruc-
tive to compare EM with minimum imputed risk.
One can estimate ? by maximizing the log-
likelihood of the data {(xi, yi), i = 1, . . . , N} as
argmax
?
1
N
N?
i=1
log p?(xi, yi). (12)
If the xi?s are missing, EM tries to iteratively maxi-
mize the marginal probability:
argmax
?
1
N
N?
i=1
log
?
x
p?(x, yi). (13)
924
The E-step of each iteration comprises comput-
ing ?x p?t(x | yi) log p?(x, yi), the expected log-
likelihood of the complete data, where p?t(x | yi) is
the conditional part of p?t(x, yi) under the current
iterate ?t, and the M-step comprises maximizing it:
?t+1 = argmax
?
1
N
N?
i=1
?
x
p?t(x | yi) log p?(x, yi).
(14)
Notice that if we replace p?t(x|yi) with p?(x | yi)
in the equation above, and admit negated log-
likelihood as a loss function, then the EM update
(14) becomes identical to (4). In other words, the
minimum imputed risk approach of Section 3.1 dif-
fers from EM in (i) using an externally-provided and
static p?, instead of refining it at each iteration based
on the current p?t , and (ii) using a specific loss func-
tion, namely negated log-likelihood.
So why not simply use the maximum-likelihood
(EM) training procedure for MT? One reason is
that it is not discriminative: the loss function (e.g.
negated BLEU) is ignored during training.
A second reason is that training good joint models
p?(x, y) is computationally expensive. Contempo-
rary MT makes heavy use of log-linear probability
models, which allow the system designer to inject
phrase tables, linguistic intuitions, or prior knowl-
edge through a careful choice of features. Comput-
ing the objective function of (14) in closed form is
difficult if p? is an arbitrary log-linear model, be-
cause the joint probability p?(xi, yi) is then defined
as a ratio whose denominatorZ? involves a sum over
all possible sentence pairs (x, y) of any length.
By contrast, our discriminative framework will
only require us to work with conditional models.
While conditional probabilities such as p?(x | y) and
p?(y |x) are also ratios, computing their denomina-
tors only requires us to sum over a packed forest of
possible translations of a given y or x.13
In summary, EM would impute missing data us-
ing p?(x | y) and predict outputs using p?(y |x),
both being conditional forms of the same joint
model p?(x, y). Our minimum imputed risk train-
ing method is similar, but it instead uses a pair of
13Analogously, discriminative CRFs have become more pop-
ular than generative HMMs because they permit efficient train-
ing even with a wide variety of log-linear features (Lafferty et
al., 2001).
separately parameterized, separately trained mod-
els p?(x | y) and p?(y |x). By sticking to condi-
tional models, we can efficiently use more sophis-
ticated model features, and we can incorporate the
loss function when we train ?, which should improve
both efficiency and accuracy at test time.
5 Experimental Results
We report results on Chinese-to-English translation
tasks using Joshua (Li et al, 2009a), an open-source
implementation of Hiero (Chiang, 2007).
5.1 Baseline Systems
5.1.1 IWSLT Task
We train both reverse and forward baseline sys-
tems. The translation models are built using the cor-
pus for the IWSLT 2005 Chinese to English trans-
lation task (Eck and Hori, 2005), which comprises
40,000 pairs of transcribed utterances in the travel
domain. We use a 5-gram language model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998), trained on the English (resp. Chi-
nese) side of the bitext. We use a standard train-
ing pipeline and pruning settings recommended by
(Chiang, 2007).
5.1.2 NIST Task
For the NIST task, the TM is trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora dis-
tributed by LDC for the NIST MT evaluation using a
sampling method implemented in Joshua. We also
used a 5-gram language model, trained on a data set
consisting of a 130M words in English Gigaword
(LDC2007T07) and the bitext?s English side.
5.2 Feature Functions
We use two classes of features fk for discriminative
training of p? as defined in (9).
5.2.1 Regular Hiero Features
We include ten features that are standard in Hi-
ero (Chiang, 2007). In particular, these include
one baseline language model feature, three baseline
translation models, one word penalty feature, three
features to count how many rules with an arity of
925
zero/one/two are used in a derivation, and two fea-
tures to count how many times the unary and binary
glue rules in Hiero are used in a derivation.
5.2.2 Target-rule Bigram Features
In this paper, we do not attempt to discrimina-
tively tune a separate parameter for each bilingual
rule in the Hiero grammar. Instead, we train several
hundred features that generalize across these rules.
For each bilingual rule, we extract bigram fea-
tures over the target-side symbols (including non-
terminals and terminals). For example, if a bilingual
rule?s target-side is ?on the X1 issue of X2? where
X1 and X2 are non-terminals (with a position in-
dex), we extract the bigram features on the, the X ,
X issue, issue of, and of X . (Note that the posi-
tion index of a non-terminal is ignored in the fea-
ture.) Moreover, for the terminal symbols, we will
use their dominant POS tags (instead of the sym-
bol itself). For example, the feature the X becomes
DTX . We use 541 such bigram features for IWSLT
task (and 1023 such features for NIST task) that fire
frequently.
5.3 Data Sets for Discriminative Training
5.3.1 IWSLT Task
In addition to the 40,000 sentence pairs used to
train the baseline generative models (which are used
to compute the features fk), we use three bilingual
data sets listed in Table 1, also from IWSLT, for dis-
criminative training: one to train the reverse model
p? (which uses only the 10 standard Hiero features
as described in Section 5.2.1),14 one to train the for-
ward model ?? (which uses both classes of features
described in Section 5.2, i.e., 551 features in total),
and one for test.
Note that the reverse model ? is always trained us-
ing the supervised data of Dev ?, while the forward
model ? may be trained in a supervised or semi-
supervised manner, as we will show below.
In all three data sets, each Chinese sentence xi
has 16 English reference translations, so each yi is
actually a set of 16 translations. When we impute
data from yi (in the semi-supervised scenario), we
14Ideally, we should train ? to minimize the conditional
cross-entropy (5) as suggested in section 3.2. In the present
results, we trained ? discriminatively to minimize risk, purely
for ease of implementation using well versed steps.
Data set Purpose # of sentencesChinese English
Dev ? training ? 503 503?16
Dev ? training ? 503? 503?16
Eval ? testing 506 506?16
Table 1: IWSLT Data sets used for discriminative
training/test. Dev ? is used for discriminatively training
of the reverse model ?, Dev ? is for the forward model,
and Eval ? is for testing. The star ? for Dev ? empha-
sizes that some of its Chinese side will not be used in the
training (see Table 2 for details).
actually impute 16 different values of xi, by using
p? to separately reverse translate each sentence in
yi. This effectively adds 16 pairs of the form (xi, yi)
to the training set (see section 3.4), where each xi
is a different input sentence (imputed) in each case,
but yi is always the original set of 16 references.
5.3.2 NIST Task
For the NIST task, we use MT03 set (having 919
sentences) to tune the component parameters in both
the forward and reverse baseline systems. Addition-
ally, we use the English side of MT04 (having 1788
sentences) to perform semi-supervised tuning of the
forward model. The test sets are MT05 and MT06
(having 1082 and 1099 sentences, respectively). In
all the data sets, each source sentence has four refer-
ence translations.
5.4 Main Results
We compare two training scenarios: supervised and
semi-supervised. The supervised system (?Sup?)
carries out discriminative training on a bilingual data
set. The semi-supervised system (?+Unsup?) addi-
tionally uses some monolingual English text for dis-
criminative training (where we impute one Chinese
translation per English sentence).
Tables 2 and 3 report the results for the two tasks
under two training scenarios. Clearly, adding unsu-
pervised data improves over the supervised case, by
at least 1.3 BLEU points in IWSLT and 0.5 BLEU in
NIST.
5.5 Results for Analysis Purposes
Below, we will present more results on the IWSLT
data set to help us understand the behavior of the
926
Training scenario Test BLEU
Sup, (200, 200?16) 47.6
+Unsup, 101?16 Eng sentences 49.0
+Unsup, 202?16 Eng sentences 48.9
+Unsup, 303?16 Eng sentences 49.7?
Table 2: BLEU scores for semi-supervised training for
IWSLT task. The supervised system (?Sup?) is trained
on a subset of Dev ? containing 200 Chinese sentences
and 200?16 English translations. ?+Unsup? means that
we include additional (monolingual) English sentences
from Dev ? for semi-supervised training; for each En-
glish sentence, we impute the 1-best Chinese translation.
A star ? indicates a result that is signicantly better than
the ?Sup? baseline (paired permutation test, p < 0.05).
Training scenario Test BLEUMT05 MT06
Sup, (919, 919?4) 32.4 30.6
+Unsup, 1788 Eng sentences 33.0? 31.1?
Table 3: BLEU scores for semi-supervised training for
NIST task. The ?Sup? system is trained on MT03, while
the ?+Unsup? system is trained with additional 1788 En-
glish sentences from MT04. (Note that while MT04 has
1788?4 English sentences as it has four sets of refer-
ences, we only use one such set, for computational ef-
ficiency of discriminative training.) A star ? indicates a
result that is signicantly better than the ?Sup? baseline
(paired permutation test, p < 0.05).
methods proposed in this paper.
5.5.1 Imputation with Different Reverse
Models
A critical component of our unsupervised method
is the reverse translation model p?(x | y). We
wonder how the performance of our unsupervised
method changes when the quality of the reverse sys-
tem varies. To study this question, we used two dif-
ferent reverse translation systems, one with a lan-
guage model trained on the Chinese side of the bi-
text (?WLM?), and the other one without using such
a Chinese LM (?NLM?). Table 4 (in the fully unsu-
pervised case) shows that the imputed Chinese trans-
lations have a far lower BLEU score without the lan-
guage model,15 and that this costs us about 1 English
15The BLEU scores are low even with the language model
because only one Chinese reference is available for scoring.
Data size Imputed-CN BLEU Test-EN BLEUWLM NLM WLM NLM
101 11.8 3.0 48.5 46.7
202 11.7 3.2 48.9 47.6
303 13.4 3.5 48.8 47.9
Table 4: BLEU scores for unsupervised training
with/without using a language model in the reverse
system. A data size of 101 means that we use only
the English sentences from a subset of Dev ? containing
101 Chinese sentences and 101?16 English translations;
for each English sentence we impute the 1-best Chinese
translation. ?WLM? means a Chinese language model
is used in the reverse system, while ?NLM? means no
Chinese language model is used. In addition to reporting
the BLEU score on Eval ?, we also report ?Imputed-CN
BLEU?, the BLEU score of the imputed Chinese sentences
against their corresponding Chinese reference sentences.
BLEU point in the forward translations. Still, even
with the worse imputation (in the case of ?NLM?),
our forward translations improve as we add more
monolingual data.
5.5.2 Imputation with Different k-best Sizes
In all the experiments so far, we used the reverse
translation system to impute only a single Chinese
translation for each English monolingual sentence.
This is the 1-best approximation of section 3.4.
Table 5 shows (in the fully unsupervised case)
that the performance does not change much as k in-
creases.16 This may be because that the 5-best sen-
tences are likely to be quite similar to one another
(May and Knight, 2006). Imputing a longer k-best
list, a sample, or a lattice for xi (see section 3.4)
might achieve more diversity in the training inputs,
which might make the system more robust.
6 Conclusions
In this paper, we present an unsupervised discrimi-
native training method that works with missing in-
puts. The key idea in our method is to use a re-
verse model to impute the missing input from the ob-
served output. The training will then forward trans-
late the imputed input, and choose the parameters of
the forward model such that the imputed risk (i.e.,
16In the present experiments, however, we simply weighted
all k imputed translations equally, rather than in proportion to
their posterior probabilities as suggested in Section 3.4.
927
Training scenario Test BLEU
Unsup, k=1 48.5
Unsup, k=2 48.4
Unsup, k=3 48.9
Unsup, k=4 48.5
Unsup, k=5 48.4
Table 5: BLEU scores for unsupervised training with
different k-best sizes. We use 101?16 monolingual En-
glish sentences, and for each English sentence we impute
the k-best Chinese translations using the reverse system.
the expected loss of the forward translations with
respect to the observed output) is minimized. This
matches the intuition that the probabilistic ?round-
trip? translation from the target-language sentence
to the source-language and back should have low ex-
pected loss.
We applied our method to two Chinese to English
machine translation tasks (i.e. IWSLT and NIST).
We showed that augmenting supervised data with
unsupervised data improved performance over the
supervised case (for both tasks).
Our discriminative model used only a small
amount of training data and relatively few features.
In future work, we plan to test our method in settings
where there are large amounts of monolingual train-
ing data (enabling many discriminative features).
Also, our experiments here were performed on a lan-
guage pair (i.e., Chinese to English) that has quite
rich bilingual resources in the domain of the test
data. In future work, we plan to consider low-
resource test domains and language pairs like Urdu-
English, where bilingual data for novel domains is
sparse.
Acknowledgements
This work was partially supported by NSF Grants
No IIS-0963898 and No IIS-0964102 and the
DARPA GALE Program. The authors thank Markus
Dreyer, Damianos Karakos and Jason Smith for in-
sightful discussions.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In ACL,
pages 1012?1020.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In IWSLT.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT, pages 53?64.
Jui-Ting Huang, Xiao Li, and Alex Acero. 2010. Dis-
criminative training methods for language models us-
ing conditional entropy criteria. In ICASSP.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In NAACL, pages 139?146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL,
pages 48?54.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40?51.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar. Zaidan. 2009a.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In WMT09, pages 26?30.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593?601.
Zhifei Li, Ziyuan Wang, Sanjeev Khudanpur, and Jason
Eisner. 2010. Unsupervised discriminative language
928
model training for machine translation using simulated
confusion sets. In COLING, pages 556?664.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL, pages 761?
768.
R. J. A. Little and D. B. Rubin. 1987. Statistical Analysis
with Missing Data. J. Wiley & Sons, New York.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On the
limited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503?528.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725?734.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351?358.
Thomas Minka. 2000. Empirical risk minimization is
an incomplete inductive principle. In MIT Media Lab
note.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
D. B. Rubin. 1987. Multiple Imputation for Nonresponse
in Surveys. J. Wiley & Sons, New York.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In ACL,
pages 787?794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum-Bayes-risk de-
coding for statistical machine translation. In EMNLP,
pages 620?629.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764?773.
929
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 344?355, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Name Phylogeny: A Generative Model of String Variation
Nicholas Andrews and Jason Eisner and Mark Dredze
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA
{noa,eisner,mdredze}@jhu.edu
Abstract
Many linguistic and textual processes involve transduc-
tion of strings. We show how to learn a stochastic trans-
ducer from an unorganized collection of strings (rather
than string pairs). The role of the transducer is to orga-
nize the collection. Our generative model explains simi-
larities among the strings by supposing that some strings
in the collection were not generated ab initio, but were in-
stead derived by transduction from other, ?similar? strings
in the collection. Our variational EM learning algorithm
alternately reestimates this phylogeny and the transducer
parameters. The final learned transducer can quickly link
any test name into the final phylogeny, thereby locating
variants of the test name. We find that our method can
effectively find name variants in a corpus of web strings
used to refer to persons inWikipedia, improving over stan-
dard untrained distances such as Jaro-Winkler and Leven-
shtein distance.
1 Introduction
Systematic relationships between pairs of strings
are at the core of problems such as transliteration
(Knight and Graehl, 1998), morphology (Dreyer and
Eisner, 2011), cross-document coreference resolu-
tion (Bagga and Baldwin, 1998), canonicalization
(Culotta et al2007), and paraphrasing (Barzilay and
Lee, 2003). Stochastic transducers such as proba-
bilistic finite-state transducers are often used to cap-
ture such relationships. They model a conditional
distribution p(y | x), and are ordinarily trained on
input-output pairs of strings (Dreyer et al2008).
In this paper, we are interested in learning from
an unorganized collection of strings, some of which
might have been derived from others by transforma-
tive linguistic processes such as abbreviation, mor-
phological derivation, historical sound or spelling
change, loanword formation, translation, transliter-
ation, editing, or transcription error. We assume that
each string was derived from at most one parent, but
may give rise to any number of children.
The difficulty is that most or all of these parent-
child relationships are unobserved. We must recon-
struct this evolutionary phylogeny. At the same time,
we must fit the parameters of a model of the relevant
linguistic process p(y | x), which says what sort of
children y might plausibly be derived from parent x.
Learning this model of p(y | x) helps us organize the
training collection by reconstructing its phylogeny,
and also permits us to generalize to new forms.
We will focus on the problem of name varia-
tion. We observe a collection of person names?full
names, nicknames, abbreviated or misspelled names,
etc. Some of these names can refer to the same per-
son; we hope to detect this. It would be an unlikely
coincidence if two mentions of John Jacob Jingle-
heimer Schmidt referred to different people, since
this is a long and unusual name. Similarly, John Ja-
cob Jingelhimer Smith andDr. J. J. Jingleheimermay
also be related names for this person. That is, these
names may be derived from one another, via unseen
relationships, although we cannot be sure.
Readers may be reminded of unsupervised clus-
tering, in which ?suspiciously similar? points can be
explained as having been generated by the same clus-
ter. Since each name is linked to at most one parent,
our setting resembles single-link clustering?with a
learned, asymmetric distance measure p(y | x).
We will propose a generative process that makes
explicit assumptions about how strings are copied
with mutation. It is assumed to have generated all the
names in the collection, in an unknown order. Given
learned parameters, we can ask the model whether a
name Dr. J. J. Jingelheimer in the collection is more
likely to have been generated from scratch, or derived
from some previous name.
1.1 Related Work
Several previous papers have also considered learn-
ing transducers or other models of word pairs when
344
the pairing between inputs and outputs is not given.
Most commonly, one observes parallel or compa-
rable corpora in two languages, and must recon-
struct a matching from one language?s words to the
other?s before training on the resulting pairs (Schafer,
2006b; Klementiev and Roth, 2006; Haghighi et al
2008; Snyder et al2010; Sajjad et al2011).
Hall and Klein (2010) extend this setting to more
than two languages, where the phylogenetic tree is
known. A given lexeme (abstract word) can be re-
alized in each language by at most one word (string
type), derived from the parent language?s realization
of the same lexeme. The system must match words
that share an underlying lexeme (i.e., cognates), cre-
ating a matching of each language?s vocabulary to its
parent language?s vocabulary. A further challenge is
that the parent words are unobserved ancestral forms.
Similarly, Dreyer and Eisner (2011) organize
words into morphological paradigms of a given
structure. Again words with the same underlying lex-
eme (i.e., morphemes) must be identified. A lexeme
can be realized in each grammatical inflection (such
as ?first person plural present?) by exactly one word
type, related to other inflected forms of the same lex-
eme, which as above may be unobserved. Their in-
ference setting is closer to ours because the input is
an unorganized collection of words?input words are
not tagged with their grammatical inflections. This
contrasts with the usual multilingual setting where
each word is tagged with its true language.
In one way, our problem differs significantly from
the above problems. We are interested in random
variation that may occur within a language as well
as across languages. A person name may have un-
boundedly many different variants. This is unlike
the above problems, in which a lexeme has at most
K realizations, where K is the (small) number of
languages or inflections.1 We cannot assign the ob-
served strings to positions in an existing structure
that is shared across all lexemes, such as a given phy-
logenetic tree whose K nodes represent languages,
or a given inflectional grid whose K cells represent
grammatical inflections. Rather, we must organize
1In the above problems, one learns a set ofO(K) orO(K2)
specialized transducers that relate Latin to Italian, singular to
plural, etc. We instead use one global mutation model that ap-
plies to all names?but see footnote 14 on incorporating special-
ized transductions (Latin to Italian) within our mutation model.
them into a idiosyncratic phylogenetic tree whose
nodes are the string types or tokens themselves.
Names and words are not the only non-biological
objects that are copied with mutation. Documents,
database records, bibliographic entries, code, and
images can evolve in the same way. Reconstructing
these relationships has been considered by a number
of papers on authorship attribution, near-duplicate
detection, deduplication, record linkage, and plagia-
rism detection. A few such papers reconstruct a phy-
logeny, as in the case of chain letters (Bennett et
al., 2003), malware (Karim et al2005), or images
(Dias et al2012). In fact, the last of these uses the
same minimum spanning tree method that we apply
in ?5.3. However, these papers do not train a similar-
ity measure as we do. To our knowledge, these two
techniques have not been combined outside biology.
In molecular evolutionary analysis, phylogenetic
techniques have often been combined with estima-
tion of some parametric model of mutation (Tamura
et al2011). However, names mutate differently
from biological sequences, and our mutation model
for names (?4, ?8) reflects that. We also posit a spe-
cific process (?3) that generates the name phylogeny.
2 An Example
A fragment of a phylogeny for person names is
shown in Figure 1. Our procedure learned this auto-
matically from a collection of name tokens, without
observing any input/output pairs. The nodes of the
phylogeny are the observed name types,2 each one
associated with a count of observed tokens.
Each arrow corresponds to a hypothesized mu-
tation. These mutations reflect linguistic processes
such as misspelling, initialism, nicknaming, translit-
eration, etc. As an exception, however, each ar-
row from the distinguished root node ? generates
an initial name for a new entity. The descendants of
this initial name are other names that subsequently
evolved for that entity. Thus, the child subtrees of ?
give a partition of the name types into entities.
Thanks to the phylogeny, the seemingly disparate
names Ghareeb Nawaz and Muinuddin Chishti are
seen to refer to the same entity. They may be traced
back to their common ancestor Khawaja Gharib-
2We cannot currently hypothesize unobserved intermediate
forms, e.g., common ancestors of similar strings. See ?6.2.
345
Khawaja Gharibnawaz Muinuddin Hasan Chisty
Khwaja Gharib NawazKhwaja Muin al-Din ChishtiGhareeb Nawaz Khwaja Moinuddin ChishtiKhwaja gharibnawaz Muinuddin Chishti
Thomas Ruggles Pynchon, Jr.Thomas Ruggles Pynchon Jr.Thomas R. Pynchon, Jr.Thomas R. Pynchon Jr.Thomas R. Pynchon Thomas Pynchon, Jr.Thomas Pynchon Jr.
Figure 1: A portion of a spanning tree found by our model.
nawaz Muinuddin Hasan Chisty, from which both
were derived via successive mutations.
Not shown in Figure 1 is our learned family p of
conditional probability distributions, which models
the likely mutations in this corpus. Our EM learn-
ing procedure found p jointly with the phylogeny.
Specifically, it alternated between improving p and
improving the distribution over phylogenies. At the
end, we extracted the single best phylogeny.
Together, the learned p and the phylogeny in Fig-
ure 1 form an explanation of the observed collection
of names. What makes it more probable than other
explanations? Informally, two properties:
? Each node in the tree is plausibly derived from
its parent. More precisely, the product of
the edge probabilities under p is comparatively
high. A different p would have reduced the
probability of the events in this phylogeny. A
different phylogeny would have involved a more
improbable collection of events, such as replac-
ing Chishti with Pynchon, or generating many
unrelated copies of Pynchon directly from ?.
? In the phylogeny, the parent names tend to be
used often enough that it is plausible for variants
of these names to have emerged. Our model
says that new tokens are derived from previ-
ously generated tokens. Thus?other things
equal?Barack Obama is more plausibly a vari-
ant of Barack Obama, Jr. than of Barack
Obama, Sr. (which has fewer tokens).
3 A Generative Model of Tokens
Our model should reflect the reasons that name vari-
ation exists. A named entity has the form y = (e, w)
where w is a string being used to refer to entity e. A
single entity e may be referred to on different occa-
sions by different name strings w. We suppose that
this is the result of copying the entity with occasional
mutation of its name (as in asexual reproduction).
Thus, we assume the following simple generative
process that produces an ordered sequence of tokens
y1, y2, . . ., where yi = (ei, wi).
? After the first k tokens y1, . . . yk have been gen-
erated, the author responsible for generating yk+1
must choose whom to talk about next. She is likely
to think of someone she has heard about often in the
past. So to make this choice, she selects one of the
previous tokens yi uniformly at random, each having
probability 1/(k + ?); or else she selects ?, with
probability ?/(k + ?).
? If the author selected a previous token yi, then
with probability 1 ? ? she copies it faithfully, so
yk+1 = yi. But with probability ?, she instead draws
a mutated token yk+1 = (ek+1, wk+1) from the mu-
tation model p(? | yi). This preserves the entity
(ek+1 = ei with probability 1), but the new name
wk+1 is a stochastic transduction of wi drawn from
p(? | wi).3 For example, in referring to ei, the author
may shorten and respellwi = Khwaja Gharib Nawaz
into wk+1 = Ghareeb Nawaz (Figure 1).
? If the author selected?, she must choose a fresh
entity yk+1 = (ek+1, wk+1) to talk about. So she
sets ek+1 to a newly created entity, sampling its name
wk+1 from the distribution p(? | ?). For example,
wk+1 = Thomas Ruggles Pynchon, Jr. (Figure 1).
Nothing prevents wk+1 from being a name that is al-
ready in use for another entity (i.e., wk+1 may equal
wj for some j ? k).
3Straightforward extensions are to allow a variable mutation
rate ?(yi) that depends on properties of yi, and to allow wk+1
to depend on known properties of ei. See footnote 14 for further
discussion of enriched tokens.
346
3.1 Relationship to other models
If we ignore the name strings, we can see that the
sequence of entities e1, e2, . . . eN is being generated
from a Chinese restaurant process (CRP) with con-
centration parameter ?. To the extent that ? is low
(so that  is rarely used), a few randomly chosen en-
tities will dominate the corpus.
The CRP is equivalent to sampling e1, e2, . . . IID
from an unknown distribution that was itself drawn
from a Dirichlet process with concentration ?. This
is indeed a standard model of a distribution over en-
tities. For example, Hall et al2008) use it to model
venues in bibliographic entries.
From this characterization of the CRP, one can see
that any permutation of this entity sequence would
have the same probability. That is, our distribution
over sequences of entities e is exchangeable.
However, our distribution over sequences of
named entities y = (e, w) is non-exchangeable.
It assigns different probabilities to different order-
ings of the same tokens. This is because our model
posits that later authors are influenced by earlier au-
thors, copying entity names from them with muta-
tion. So ordering is important. The mutation process
is not symmetric?for example, Figure 1 reflects a
tendency to shorten rather than lengthen names.
Non-exchangeability is one way that our present
model differs from (parametric) transformationmod-
els (Eisner, 2002) and (non-parametric) transforma-
tion processes (Andrews and Eisner, 2011). These
too are defined using mutation of strings or other
types. From a transformation process, one can draw
a distribution over types, from which the tokens are
then sampled IID. This results in an exchangeable
sequence of tokens, just as in the Dirichlet process.
We avoid transformation models here for three
reasons. (1) Inference is more expensive. (2) A
transformation process seems less realistic as a
model of authorship. It constructs a distribution over
derivational paths, similar to the paths in Figure 1.
It effectively says that each token is generated by re-
capitulating some previously used path from ?, but
with some chance of deviating at each step. For an
author to generate a name token this way, she would
have to know the whole derivational history of the
previous name she was adapting. Our present model
instead allows an author simply to select a name she
previously saw and copy or mutate its surface form.
(3) One should presumably prefer to explain a novel
name y as a mutation of a frequent name x, other
things equal (?2). But surprisingly, inference under
the transformation process does not prefer this.4
Another view of our present model comes from
the literature on random graphs (e.g., for modeling
social networks or the link structure of the web). In
a preferential attachment model, a graph?s vertices
are added one by one, and each vertex selects some
previous vertices as its neighbors. Our phylogeny
is a preferential attachment tree, a random directed
graph in which each vertex selects a single previous
vertex as its parent. Specifically, it is a random recur-
sive tree (Smythe and Mahmoud, 1995) whose ver-
tices are the tokens.5 To this simple random topol-
ogy we have added a random labeling process with
mutation. The first ? vertices are labeled with ?.
4 A Mutation Model for Strings
Our model in ?3 samples the next token y, when it is
not simply a faithful copy, from p(y | x) or p(y | ?).
The key step there is to sample the name string wy
from p(wy | wx) or p(wy | ?).
Our model of these distributions could easily in-
corporate detailed linguistic knowledge of the muta-
tion process (see ?8). Here we describe the specific
model that we use in our experiments. Like many
such models, it can be regarded as a stochastic finite-
state string-to-string transducer parameterized by ?.
There is much prior work on stochastic models of
edit distance (Ristad andYianilos, 1998; Bilenko and
Mooney, 2003; Oncina and Sebban, 2006; Schafer,
2006a; Bouchard-C?t? et al2008; Dreyer et al
2008, among others). For the present experiments,
we designed a moderately simple one that employs
(1) conditioning on one character of right context,
(2) latent ?edit? and ?no-edit? regions to capture the
fact that groups of edits are often made in close prox-
imity, and (3) some simple special handling for the
distribution conditioned on the root p(wy | ?).
We assume a stochastic mutation process which,
when given an input string wx, edits it from left to
4The very fact that x has been frequently observed demon-
strates that it has often chosen to stop mutating. This implies
that it is likely to choose stop again rather than mutate into y.
5This is not the tree shown in Figure 1, whose vertices are
types rather than tokens.
347
right into an output string wy. Then p(wy | wx) is
the total probability of all operation sequences onwx
that would produce wy. This total can be computed
in time O(|wx| ? |wy|) by dynamic programming.
Our process has four character-level edit opera-
tions: copy, substitute, insert, delete. It also has a
distinguished no-edit operation that behaves exactly
like copy. At each step, the process first randomly
chooses whether to edit or no-edit, conditioned only
on whether the previous operation was an edit. If it
chooses to edit, it chooses a random edit type with
some probability conditioned on the next input char-
acter. In the case of insert or substitute, it then ran-
domly chooses an output character, conditioned on
the type of edit and the next input character.
It is common to mutate a name by editing con-
tiguous substrings (e.g., words). Contiguous regions
of copying versus editing can be modeled by a low
probability of transitioning between no-edit and edit
regions.6 Note that an edit region may include some
copy edits (or substitute edits that replace a charac-
ter with itself) without leaving the edit region. This
is why we distinguish copy from no-edit.
Input and output strings are augmented with a
trailing eos (?end-of-string?) symbol that is seen by
the single-character lookahead. If the next character
is eos, the only available edit is insert. Alternatively,
if the process selects no-edit, then eos is copied to
the output string and the process terminates.
In the case of p(wy | ?), the input string is empty,
and both input and output are augmented with a trail-
ing eos? character that behaves like eos. Then wy
is generated by a sequence of insertions followed by
a copy. These are conditioned as usual on the next
character, here eos?, so the model can learn to insert
more or different characters when the input is ?.
The parameters ? determining the conditional
probabilities of the different operations and charac-
ters are estimated with backoff smoothing.
5 Inference
The input to inference is a collection of named entity
tokens y. Most are untagged tokens of the form y =
(?, w). In a semi-supervised setting, however, some
6This somewhat resembles the traditional affine gap penalty
in computational biology (Gusfield, 1997), which makes dele-
tions or insertions cheaper if they are consecutive. We instead
make consecutive edits cheaper regardless of the edit type.
of the tokens may be tagged tokens of the form y =
(e, w), whose true entity is known. The entity tags
place a constraint on the phylogeny, since each child
subtree of ? must correspond to exactly one entity.
5.1 An unrealistically supervised setting
Suppose we were lucky enough to fully observe the
sequence of named entity tokens yi = (ei, wi) pro-
duced by our generative model. That is, suppose all
tokens were tagged and we knew their ordering.
Yet there would still be something to infer: which
tokens were derived from which previous tokens.
This phylogeny is described by a spanning tree over
the tokens. Let us see how to infer it.
For each potential edge x ? y between named
entity tokens, define ?(y | x) to be the probability of
choosing x and copying it (possibly with mutation)
to obtain y. So
?(yj | ?) = ? p(yj | ?) (1)
?(yj | yi) = ? p(yj | yi) + (1? ?)1(yj = yi) (2)
except that if i ? j or if ei 6= ej , then ?(yj | yi) = 0
(since yj can only be derived from an earlier token
yi with the same entity).
Now the prior probability of generating y1, . . . yN
with a given phylogenetic tree is easily seen to be a
product over all tree edges,
?
j ?(yj | pa(yj)) where
pa(yj) is the parent of yj . As a result, it is known
that the following are efficient to compute from the
(N + 1)? (N + 1) matrix of ? values (see ?5.3):
(a) the max-probability spanning tree
(b) the total probability of all spanning trees
(c) the marginal probability of each edge, under the
posterior distribution on spanning trees
(a) is our single best guess of the phylogeny. We use
this during evaluation. (b) gives the model likeli-
hood, i.e., the total probability of the observed data
y1, . . . yN . To locally maximize the model likeli-
hood, (c) can serve as the E step of our EM algorithm
(?6) for tuning our mutation model. The M step then
retrains the mutation model?s parameters ? on input-
output pairs wi ? wj , weighting each pair by its
edge?s posterior marginal probability (c), since that
is the expected count of a wi ? wj mutation. This
computation is iterated.
348
5.2 The unsupervised setting
Now we turn to a real setting?fully unsupervised
data. Two issues will force us to use an approximate
inference algorithm. First, we have an untagged cor-
pus: a token?s entity tag e is never observed. Second,
the order of the tokens is not observed, so we do not
know which other tokens are candidate parents.
Our first approximation is to consider only phylo-
genies over types rather than tokens.7 The type phy-
logeny in Figure 1 represents a set of possible token
phylogenies. Each node of Figure 1 represents an
untagged name type y = (?, w). By grouping all ny
tokens of this type into a single node, we mean that
the first token of y was derived by mutation from the
parent node, while each later token of y was derived
by copying an (unspecified) earlier token of y.
A token phylogeny cannot be represented in this
way if two or more tokens of y were created by mu-
tations. In that case, their name strings are equal only
by coincidence. They may have different parents
(perhaps of different entities), whereas the y node in
a type phylogeny can have only one parent.
We argue, however, that these unrepresentable to-
ken phylogenies are comparatively unlikely a poste-
riori and can be reasonably ignored during inference.
The first token of y is necessarily amutation, but later
tokens are much more likely to be copies. The prob-
ability of generating a later token y by copying some
previous token is at least
(1? ?)/(N + ?),
while the probability of generating it in some other
way is at most
max(? p(y | ?), ? max
x?Y
p(y | x))
where Y is the set of observed types. The second
probability is typically much smaller: an author is
unlikely to invent exactly the observed string y, cer-
tainly from ? but even by mutating a similar string
x (especially when the mutation rate ? is small).
How do we evaluate a type phylogeny? Con-
sider the probability of generating untagged tokens
7Working over types improves the quality of our second ap-
proximation, and also speeds up the spanning tree algorithms.
?6 explains how to regard this approximation as variational EM.
y1, . . . yN in that order and respecting the phylogeny:
(
N?
k=1
1
k + ?
)
?
y?Y
g(y | pa(y))
?
?
ny?1?
i=1
i (1? ?)
?
?
(3)
where g(y | pa(y)) is a factor for generating the first
token of y from its parent pa(y), defined by
g(y | ?) = ? ? p(y | ?) (4)
g(y | x) = ? ? (# tokens of x preceding
first token of y) ? p(y | x) (5)
But we do not actually know the token order: by
assumption, our input corpus is only an unordered
bag of tokens. So we must treat the hidden order-
ing like any other hidden variable and maximize the
marginal likelihood, which sums (3) over all possi-
ble orderings (permutations). This sum can be re-
garded as the number of permutations N ! (which is
fixed given the corpus) times the expectation of (3)
for a permutation chosen uniformly at random.
This leads to our second approximation. We ap-
proximate this expectation of the product (3) with a
product of expectations of its individual factors.8 To
find the expectation of (5), observe that the expected
number of tokens of x that precede the first token of
y is nx/(ny+1), since each of the nx tokens of x has
a 1/(ny + 1) chance of falling before all ny tokens
of y. It follows that the approximated probability of
generating all tokens in some order, with our given
type parentage, is proportional to
?
y?Y
?(y | pa(y)) (6)
where
?(y | ?) = ? ? p(y | ?) (7)
?(y | x) = ? ? p(y | x) ? nx/(ny + 1) (8)
and the constant of proportionality depends on the
corpus.
The above equations are analogous to those in
?5.1. Again, the approximate posterior probability
of a given type parentage tree is edge-factored?it is
the product of individual edge weights defined by ?.
Thus, we are again eligible to use the spanning tree
algorithms in ?5.3 below.
8In general this is an overestimate for each phylogeny.
349
Notice that the ratio ?/? controls the preference
for an entity to descend from ? versus an existing
entity. Thus, by tuning this ratio, we can control
the number of entities inferred by our method, where
each entity corresponds to one of the child subtrees
of ?.
Also note that nx in the numerator of (8) means
that y?s parent is more likely to be frequent. Also,
ny +1 in the denominator means that a frequent y is
not as likely to have any parent x 6= ?, because its
first token probably falls early in the sequence where
there are fewer available parents x 6= ?.
5.3 Spanning tree algorithms
Define a complete directed graphG over the vertices
Y ? {?}. The weight of an edge x ? y is defined
by ?(y | x). The (approximate) posterior probability
of a given phylogeny given our evidence, is propor-
tional to the product of the ? values of its edges.
Formally, let T?(G) denote the set of spanning
trees of G rooted at ?, and define the weight of a
particular spanning tree T ? T?(G) to be the prod-
uct of the weights of its edges:
w(T ) =
?
(x?y)?T
?(y | x) (9)
Then the posterior probability of spanning tree T is
p?(T ) =
w(T )
Z(G)
(10)
where Z(G) =
?
T?T?(G)
w(T ) is the partition
function, i.e. the total probability of generating the
dataG via any spanning tree of the formwe consider.
This distribution is determined by the parameters ?
of the transducer p?, along with the ratio ?/?.
There exist several algorithms to find the sin-
glemaximum-probability spanning tree, notably Tar-
jan?s implementation of the Chu-Liu-Edmonds algo-
rithm, which runs in O(m log n) for a sparse graph
or O(n2) for a dense graph (Tarjan, 1977). Figure 1
shows a spanning tree found by our model using Tar-
jan?s algorithm. Here n is the number of vertices
(in our case, types and ), whilem is the number of
edges (which we can keep small by pruning, ?6.1).
6 Training the Transducer with EM
Our inference algorithm assumes that we know the
transducer parameters ?. We now explain how to op-
timize ? to maximize the marginal likelihood of the
training data. This marginal likelihood sums over all
the other latent variables in the model?the spanning
tree, the alignments between strings, and the hidden
token ordering.
The EMprocedure repeats the following until con-
vergence:
E-step: Given ?, compute the posterior marginal
probabilities cxy of all possible phylogeny
edges.
M-step Given all cxy, retrain ? to assign a high
conditional probability to the mutations on the
probable edges.
We actually use a variational EM algorithm: our
E step approximates the true distribution q over all
phylogenies with the closest distribution p that as-
signs positive probability only to type-based phylo-
genies. This distribution is given by (10) and min-
imizes KL(p || q). We argued in section ?5.2 that
it should be a good approximation. The posterior
marginal probability of a directed edge from vertex
x to vertex y, according to (10), is
cxy =
?
T?T?(G):(x?y)?T
p?(T ) (11)
The probability cxy is a ?pseudocount? for the ex-
pected number of mutations from x to y. This is at
most 1 under our assumptions.
Calculating cxy requires summing over all span-
ning trees of G, of which there are nn?2 for a fully
connected graph with n vertices. Fortunately, Tutte
(1984) shows how to compute this sum by the fol-
lowing method, which extends Kirchhoff?s classi-
cal matrix-tree theorem to weighted directed graphs.
This result has previously been employed in non-
projective dependency parsing (Koo et al2007;
Smith and Smith, 2007).
Let L ? Rn?n denote the Laplacian ofG, namely
L =
{ ?
x? ?(y | x
?) if x = y
??(y | x) if x 6= y
(12)
Tutte?s theorem relates the determinant of the Lapla-
cian to the spanning trees in graph G. In particular,
the cofactor L0,0 is equal to the sum of the weights
350
of all directed spanning trees rooted at 0, which (sup-
posing? is indexed at 0) yields the partition function
Z(G).
The edge marginals of interest are related to the
log partition function by
cxy =
?Z(G)
??(y | x)
(13)
which has the closed-form solution
cxy =
{
?(y | ?)L?1yy if x = y
?(y | x)(L?1xx ? L
?1
xy ) if x 6= y
(14)
Thus, the problem of computing edge marginals re-
duces to that of computing a matrix inverse, which
may be done in O(n3) time.
At the M step, we retrain the mutation model pa-
rameters ? to maximize
?
xy cxy log p(wy | wx).
This is tantamount to maximum conditional likeli-
hood training on a supervised collection of (wx, wy)
pairs that are respectively weighted by cxy.
The M step is nontrivial because the term p(wy |
wx) sums over a hidden alignment between two
strings. It may be performed by an inner loop of EM,
where the E step uses dynamic programming to ef-
ficiently consider all possible alignments, as in (Ris-
tad and Yianilos, 1996). In practice, we have found it
effective to take only a single step of this inner loop.
Such a Generalized EM procedure enjoys the same
convergence properties as EM, but may reach a local
optimum faster (Dempster et al1977).
6.1 Pruning the graph
For large graphs, it is essential to prune the number
of edges to avoid considering all n(n ? 1) input-
output pairs. To prune the graph, we eliminate all
edges between strings that do not share any common
trigrams (case- and diacritic-insensitive), by setting
their matrix entries to 0. As a result, the graph Lapla-
cian is a sparse matrix, which often allows faster
matrix inversion using preconditioned iterative algo-
rithms. Furthermore, pruned edges do not appear in
any spanning tree, so the E step will find that their
posterior marginal probabilities are 0. This means
that the input-output pairs corresponding to these
edges can be ignored when re-estimating the trans-
ducer parameters in the M step. We found that prun-
ing significantly improves training time with no ap-
preciable loss in performance.9
6.2 Training with unobserved tokens?
A deficiency of our method is that it assumes that
authors of our corpus have only been exposed to pre-
vious tokens in our corpus. In principle, one could
also train with U additional tokens (e, w) where we
observe neither e nor w, for very large U . This is the
?universe of discourse? in which our authors oper-
ate.10 In this case, we would need (expensive) new
algorithms to reconstruct the strings w. However,
this model could infer a more realistic phylogeny by
positing unobserved ancestral or intermediate forms
that relate the observed tokens, as in transformation
models (Eisner, 2002; Andrews and Eisner, 2011).
7 Experimental Evaluation
7.1 Data preparation
Scraping Wikipedia. Wikipedia documents many
variant names for entities. As a result, it has fre-
quently been used as a source for mining name vari-
ations, both within and across languages (Parton et
al., 2008; Cucerzan, 2007). We used Wikipedia to
create a list of name aliases for different entities.
Specifically, we mined English Wikipedia11 for all
redirects: page names that lead directly to another
page. Redirects are created by Wikipedia users for
resolving common name variants to the correct page.
For example, the pages titled Barack Obama Ju-
nior and Barack Hussein Obama automatically redi-
rect to the page titled Barack Obama. This redirec-
tion implies that the first two are name variants of
the third. Collecting all such links within English
Wikipedia yields a large number of aliases for each
page. However, many redirects are for topics other
than individual people, and these would be poor ex-
amples of name variation. In addition, some phrases
9For instance, on a dataset of approximately 6000 distinct
names, pruning reduced the number of outgoing edges at each
vertex to fewer than 100 per vertex.
10Notice that theN observed tokens would be approximately
exchangeable in this setting: they are unlikely to depend on one
another when N  U , and hence their order no longer matters
much. In effect, generating theU hidden tokens constructs a rich
distribution (analogous to a sample from the Dirichlet process)
from which the N observed tokens are then sampled IID.
11Using a Wikipedia dump from February 2, 2011.
351
Ho Chi Minh, Ho chi mihn, Ho-Chi Minh, Ho Chih-minh
Guy Fawkes, Guy fawkes, Guy faux, Guy Falks, Guy Faukes, Guy Fawks, Guy foxe, Guy Falkes
Nicholas II of Russia, Nikolai Aleksandrovich Romanov, Nicholas Alexandrovich of Russia, Nicolas II
Bill Gates, Lord Billy, Bill Gates, BillGates, Billy Gates, William Gates III, William H. Gates
William Shakespeare, William shekspere, William shakspeare, Bill Shakespear
Bill Clinton, Billll Clinton, William Jefferson Blythe IV, Bill J. Clinton, William J Clinton
Figure 2: Sample alias lists scraped from Wikipedia. Note that only partial alias lists are shown for space reasons.
that redirect to an entity are descriptions rather than
names. For example, 44th President of the United
States also links to Barack Obama, but it is not a
name variant.
Freebase filtering. To improve data quality we used
Freebase, a structured knowledge base that incorpo-
rates information from Wikipedia. Among its struc-
tured information are entity types, including the type
?person.? We filtered the Wikipedia redirect col-
lection to remove pairs where the target page was
not listed as a person in Freebase. Additionally, to
remove redirects that were not proper names (44th
President of the United States), we applied a series
of rule based filters to remove bad aliases: removing
numerical names, parentheticals after names, quota-
tion marks, and names longer than 5 tokens, since
we found that these long names were rarely person
names (e.g. United States Ambassador to the Eu-
ropean Union, Success Through a Positive Mental
Attitude which links to the author Napoleon Hill.)
While not perfect, these modifications dramatically
improved quality. The result was a list of 78,079 dif-
ferent person entities, each with one or more known
names or aliases. Some typical names are shown in
Figure 2.
Estimating empirical type counts. Our method is
really intended to be run on a corpus of string to-
kens. However, for experimental purposes, we in-
stead use the above dataset of string types because
this allows us to use the ?ground truth? given by
the Wikipedia redirects. To synthesize token counts,
empirical token frequencies for each type were esti-
mated from the LDC Gigaword corpus,12 which is
a corpus of newswire text spanning several years.
Wikipedia name types that did not appear in Giga-
word were assigned a ?backoff count? of one. Note
that by virtue of the domain, many misspellings will
12LDC Catalog No. LDC2003T05.
not appear; however, edges ?popular? names (which
may be canonical names) will be assigned higher
weight.
7.2 Experiments
We begin by evaluating the generalization ability of a
transducer trained using a transformation model. To
do so, we measure log-likelihood on held-out entity
title and alias pairs. We then verify that the general-
ization ability according to log-likelihood translates
into gains for a name matching task. For the experi-
ments in this section, we use ? = 0.9 and ? = 0.1.13
Held-out log-likelihood. We construct pairs of en-
tity title (input) and alias (output) names from the
Wikipedia data. For different amounts of supervised
data, we trained the transformation model on the
training set, and plotted the log-likelihood of held-
out test data for the transducer parameters at each it-
eration of EM. The held-out test set is constructed
from a disjoint set of Wikipedia entities, the same
number of entities as in the training set. We used
different corpora of 1000 and 1500 entities for train
and test.
Name matching. For each alias a in a test set (not
seen at training time), we produce a ranking of test
entity titles t according to transducer probabilities
p?(a | t). A good transducer should assign high
probability to transformations from the correct ti-
tle for the alias. Mean reciprocal rank (MRR) is a
commonly used metric to estimate the quality of a
ranking, which we report in Figure 4. The reported
mean is over all aliases in the test data. In addition to
evaluating the ranking for different initializations of
our transducer, we compare to two baselines: Lev-
enshtein distance and Jaro-Winkler similarity. Jaro-
Winkler is a measure on strings that was specifically
designed for record linkage (Winkler, 1999). The
13We did not find these parameters to be sensitive.
352
0 1 2 3 4 5 6 7 8 9EM iteration150000
140000
130000
120000
110000
100000
90000
Held o
ut log
-likelih
ood
sup=0sup=5sup=25sup=100sup=250
(a) 1000 entities.
0 1 2 3 4 5 6 7 8 9EM iteration240000
230000
220000
210000
200000
190000
180000
170000
160000
150000
Held o
ut log
-likelih
ood
sup=0sup=5sup=25sup=100sup=250
(b) 1500 entities.
Figure 3: Learning curves for different initializations of the transducer parameters. Above, ?sup=100? (for instance)
means that 100 entities were used as training data to initialize the transducer parameters (constructing pairs between
all title-alias pairs for those Wikpedia entities).
15000.60
0.65
0.70
0.75
0.80
0.85
MRR
jwinklevsup10semi10unsupsup
Figure 4: Mean reciprocal rank (MRR) results for differ-
ent training conditions: ?sup10? means that 10 entities
(roughly 40 name pairs) were used as training data for
the transducer; ?semi10? means that the ?sup10? model
was used as initialization before re-estimating the param-
eters using our model; ?unsup? is the transducer trained
using our model without any initial supervision; ?sup? is
trained on all 1500 entities in the training set (an upper
bound on performance); ?jwink? and ?lev? correspond to
Jaro-Winkler and Levenshtein distance baselines.
matching experiments were performed on a corpus
of 1500 entities (with separate corpora of the same
size for training and test).
8 Conclusions and Future Work
We have presented a new unsupervised method for
learning string-to-string transducers. It learns from
a collection of related strings whose relationships are
unknown. The key idea is that some strings are mu-
tations of common strings that occurred earlier. We
compute a distribution over the unknown phyloge-
netic tree that relates these strings, and use it to rees-
timate the transducer parameters via EM.
One direction for future work would be more so-
phisticated transduction models than the one we de-
veloped in ?4. For names, this could include learn-
ing common nicknames (nonparametrically); explic-
itly modeling abbreviation processes such as initials;
conditioning on name components such as title and
middle name; and transliterating across languages.14
In other domains, one could model bibliographic en-
try propagation, derivational morphology, or histor-
ical sound change (again using language tags).
Another future direction would be to incorporate
the context of tokens in order to help reconstruct
which tokens are coreferent. For example, we might
extend the generative story to generate a context for
token (e, w) conditioned on e. Combining contex-
tual similarity with string similarity has previously
proved very useful for identifying cognates (Schafer
and Yarowsky, 2002; Schafer, 2006b; Bergsma and
Van Durme, 2011). In our setting it would help to
distinguish people with identical names, as well as
determining whether two people with similar names
are really the same.
14These last two points suggest that the mutation model
should operate not on simple (entity, string) pairs, but on richer
representations in which the name has been parsed into its com-
ponents (Eisenstein et al2011), labeled with a language ID,
and perhaps labeled with a phonological pronunciation. These
additional properties of a named entity may be either observed
or latent in training data. For example, if wy and `y denote the
string and language of name y, then define p(y | x) = p(`y |
`x) ? p(wy | `y, `x, wx). The second factor captures translitera-
tion from language `x to language `y , e.g., by using ?4?s model
with an (`x, `y)-specific parameter setting.
353
References
Nicholas Andrews and Jason Eisner. 2011. Transformation pro-
cess priors. In NIPS 2011 Workshop on Bayesian Nonpara-
metrics: Hope or Hype?, Sierra Nevada, Spain, December.
Extended abstract (3 pages).
A. Bagga and B. Baldwin. 1998. Algorithms for scoring coref-
erence chains. In LREC.
Regina Barzilay and Lillian Lee. 2003. Learning to para-
phrase: an unsupervised approach using multiple-sequence
alignment. In Proc. of NAACL-HLT, pages 16?23, Strouds-
burg, PA, USA.
C. H. Bennett, M. Li, , and B. Ma. 2003. Chain letters
and evolutionary histories. Scientific American, 288(3):76?
81, June. More mathematical version available at http:
//www.cs.uwaterloo.ca/~mli/chain.html.
Shane Bergsma and Benjamin Van Durme. 2011. Learning
bilingual lexicons using the visual similarity of labeled web
images. In Proc. of IJCAI, pages 1764?1769, Barcelona,
Spain.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive
duplicate detection using learnable string similarity mea-
sures. In Proc. of ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ?03, pages
39?48, New York, NY, USA. ACM.
Alexandre Bouchard-C?t?, Percy Liang, Thomas Griffiths, and
Dan Klein. 2008. A probabilistic approach to language
change. In Proc. of NIPS, pages 169?176.
S. Cucerzan. 2007. Large-scale named entity disambiguation
based on Wikipedia data. In Proc. of EMNLP.
Aron Culotta, Michael Wick, Robert Hall, Matthew Marzilli,
and Andrew McCallum. 2007. Canonicalization of database
records using adaptive similarity measures. In Proc. of ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ?07, pages 201?209.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society. Series B (Method-
ological), 39(1):1?38.
Z. Dias, A. Rocha, and S. Goldenstein. 2012. Image phy-
logeny by minimal spanning trees. IEEE Trans. on Informa-
tion Forensics and Security, 7(2):774?788, April.
Markus Dreyer and Jason Eisner. 2011. Discovering morpho-
logical paradigms from plain text using a Dirichlet process
mixture model. In Proc. of EMNLP, pages 616?627. Sup-
plementary material (9 pages) also available.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-
variable modeling of string transductions with finite-state
methods. In Proc. of EMNLP, pages 1080?1089, Honolulu,
Hawaii, October. Association for Computational Linguistics.
Jacob Eisenstein, Tae Yano, William Cohen, Noah Smith, and
Eric Xing. 2011. Structured databases of named entities
fromBayesian nonparametrics. InProc. of the First workshop
on Unsupervised Learning in NLP, pages 2?12, Edinburgh,
Scotland, July. Association for Computational Linguistics.
Jason Eisner. 2002. Transformational priors over grammars.
In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Philadelphia, July.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences?Computer Science and Computational Biology.
Cambridge University Press.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan
Klein. 2008. Learning bilingual lexicons from monolingual
corpora. In Proc. of ACL-08: HLT, pages 771?779.
David Hall and Dan Klein. 2010. Finding cognates using phylo-
genies. In Association for Computational Linguistics (ACL).
Rob Hall, Charles Sutton, and Andrew McCallum. 2008. Un-
supervised deduplication using cross-field dependencies. In
Proc. of the ACM SIGKDD International Conference On
Knowledge Discovery and Data Mining, KDD ?08, pages
310?317.
Md. Enamul. Karim, Andrew Walenstein, Arun Lakhotia, and
Laxmi Parida. 2005. Malware phylogeny generation using
permutations of code. Journal in Computer Virology, 1(1?
2):13?23.
Alexandre Klementiev and Dan Roth. 2006. Weakly supervised
named entity transliteration and discovery from multilingual
comparable corpora. In Proc. of COLING-ACL, pages 817?
824.
K. Knight and J. Graehl. 1998. Machine transliteration. Com-
putational Linguistics, 24:599?612.
Terry Koo, Amir Globerson, Xavier Carreras, and Michael
Collins. 2007. Structured prediction models via the matrix-
tree theorem. In Proc. of EMNLP-CoNLL, pages 141?150.
Jose Oncina and Marc Sebban. 2006. Using learned conditional
distributions as edit distance. In Proc. of the 2006 Joint IAPR
international Conference on Structural, Syntactic, and Statis-
tical Pattern Recognition, SSPR?06/SPR?06, pages 403?411.
Kristen Parton, Kathleen R. McKeown, James Allan, and En-
rique Henestroza. 2008. Simultaneous multilingual search
for translingual information retrieval. In Proceeding of the
ACM conference on Information and Knowledge Manage-
ment, CIKM ?08, pages 719?728.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learning string
edit distance. Technical Report CS-TR-532-96, Princeton
University, Department of Computer Science.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recognition and
Machine Intelligence, 20(5):522?532, May.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2011.
An algorithm for unsupervised transliteration mining with an
application to word alignment. In Proc. of ACL, pages 430?
439.
Charles Schafer and David Yarowsky. 2002. Inducing transla-
tion lexicons via diverse similarity measures and bridge lan-
guages. In Proc. of CONLL, pages 146?152.
Charles Schafer. 2006a. Novel probabilistic finite-state transduc-
ers for cognate and transliteration modeling. In 7th Biennial
Conference of the Association for Machine Translation in the
Americas (AMTA).
Charles Schafer. 2006b. Translation Discovery Using Diverse
Smilarity Measures. Ph.D. thesis, Johns Hopkins University.
David A. Smith and Noah A. Smith. 2007. Probabilistic mod-
els of nonprojective dependency trees. In Proc. of EMNLP-
CoNLL, pages 132?140.
354
R. T. Smythe and H. M. Mahmoud. 1995. A survey of recur-
sive trees. Theory of Probability andMathematical Statistics,
51(1?27).
Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A
statistical model for lost language decipherment. In Proc. of
ACL, pages 1048?1057.
Koichiro Tamura, Daniel Peterson, Nicholas Peterson, Glen
Stecher, Masatoshi Nei, and Sudhir Kumar. 2011. Mega5:
Molecular evolutionary genetics analysis using maximum
likelihood, evolutionary distance, and maximum parsimony
methods. Molecular Biology and Evolution, 28(10):2731?
2739.
R E Tarjan. 1977. Finding optimum branchings. Networks,
7(1):25?35.
W. Tutte. 1984. Graph Theory. Addison-Wesley.
William E. Winkler. 1999. The state of record linkage and cur-
rent research problems. Technical report, Statistical Research
Division, U.S. Census Bureau.
355
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455?1464,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Dynamic Feature Selection for Dependency Parsing
He He Hal Daume? III
Department of Computer Science
University of Maryland
College Park, MD 20740
{hhe,hal}@cs.umd.edu
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
jason@cs.jhu.edu
Abstract
Feature computation and exhaustive search
have significantly restricted the speed of
graph-based dependency parsing. We propose
a faster framework of dynamic feature selec-
tion, where features are added sequentially as
needed, edges are pruned early, and decisions
are made online for each sentence. We model
this as a sequential decision-making problem
and solve it by imitation learning techniques.
We test our method on 7 languages. Our dy-
namic parser can achieve accuracies compara-
ble or even superior to parsers using a full set
of features, while computing fewer than 30%
of the feature templates.
1 Introduction
Graph-based dependency parsing usually consists of
two stages. In the scoring stage, we score all pos-
sible edges (or other small substructures) using a
learned function; in the decoding stage, we use com-
binatorial optimization to find the dependency tree
with the highest total score.
Generally linear edge-scoring functions are used
for speed. But they use a large set of features, de-
rived from feature templates that consider different
conjunctions of the edge?s attributes. As a result,
parsing time is dominated by the scoring stage?
computing edge attributes, using them to instanti-
ate feature templates, and looking up the weights of
the resulting features in a hash table. For example,
McDonald et al (2005a) used on average about 120
first-order feature templates on each edge, built from
attributes such as the edge direction and length, the
two words connected by the edge, and the parts of
speech of these and nearby words.
We therefore ask the question: can we use fewer
features to score the edges, while maintaining the ef-
fect that the true dependency tree still gets a higher
score? Motivated by recent progress on dynamic
feature selection (Benbouzid et al, 2012; He et al,
2012), we propose to add features one group at a
time to the dependency graph, and to use these fea-
tures together with interactions among edges (as de-
termined by intermediate parsing results) to make
hard decisions on some edges before all their fea-
tures have been seen. Our approach has a similar
flavor to cascaded classifiers (Viola and Jones, 2004;
Weiss and Taskar, 2010) in that we make decisions
for each edge at every stage. However, in place of
relatively simple heuristics such as a global relative
pruning threshold, we learn a featurized decision-
making policy of a more complex form. Since each
decision can affect later stages, or later decisions in
the same stage, we model this problem as a sequen-
tial decision-making process and solve it by Dataset
Aggregation (DAgger) (Ross et al, 2011), a recent
iterative imitation learning technique for structured
prediction.
Previous work has made much progress on the
complementary problem: speeding up the decoding
stage by pruning the search space of tree structures.
In Roark and Hollingshead (2008) and Bergsma and
Cherry (2010), pruning decisions are made locally
as a preprocessing step. In the recent vine prun-
ing approach (Rush and Petrov, 2012), significant
speedup is gained by leveraging structured infor-
mation via a coarse-to-fine projective parsing cas-
1455
cade (Charniak et al, 2006). These approaches
do not directly tackle the feature selection problem.
Although pruned edges do not require further fea-
ture computation, the pruning step must itself com-
pute similar high-dimensional features just to de-
cide which edges to prune. For this reason, Rush
and Petrov (2012) restrict the pruning models to a
smaller feature set for time efficiency. We aim to do
feature selection and edge pruning dynamically, bal-
ancing speed and accuracy by using only as many
features as needed.
In this paper, we first explore standard static fea-
ture selection methods for dependency parsing, and
show that even a few feature templates can give de-
cent accuracy (Section 3.2). We then propose a
novel way to dynamically select features for each
edge while keeping the overhead of decision mak-
ing low (Section 4). Our present experiments use the
Maximum Spanning Tree (MST) parsing algorithm
(McDonald et al, 2005a; McDonald and Pereira,
2006). However, our approach applies to other
graph-based dependency parsers as well?including
non-projective parsing, higher-order parsing, or ap-
proximations to higher-order parsing that use stack-
ing (Martins et al, 2008), belief propagation (Smith
and Eisner, 2008), or structured boosting (Wang et
al., 2007).
2 Graph-based Dependency Parsing
In graph-based dependency parsing of an n-word in-
put sentence, we must construct a tree y whose ver-
tices 0, 1, . . . n correspond to the root node (namely
0) and the ordered words of the sentence. Each di-
rected edge of this tree points from a head (parent)
to one of its modifiers (child).
Following a common approach to structured pre-
diction problems, the score of a tree y is defined
as a sum of local scores. That is, s?(y) = ? ??
E?y ?(E) =
?
E?y ? ? ?(E), where E ranges
over small connected subgraphs of y that can be
scored individually. Here ?(E) extracts a high-
dimensional feature vector from E together with the
input sentence, and ? denotes a weight vector that
has typically been learned from data.
The first-order model decomposes the tree into
edges E of the form ?h,m?, where h ? [0, n] and
m ? [1, n] (with h 6= m) are a head token and one
of its modifiers. Finding the best tree requires first
computing ???(E) for each of the n2 possible edges.
Since scoring the edges independently in this way
restricts the parser to a local view of the depen-
dency structure, higher-order models can achieve
better accuracy. For example, in the second-order
model of McDonald and Pereira (2006), each local
subgraph E is a triple that includes the head and
two modifiers of the head, which are adjacent to
each other. Other methods that use triples include
grandparent-parent-child triples (Koo and Collins,
2010), or non-adjacent siblings (Carreras, 2007).
Third-order models (Koo and Collins, 2010) use
quadruples, employing grand-sibling and tri-sibling
information.
The usual inference problem is to find the high-
est scoring tree for the input sentence. Note that in
a valid tree, each token 1, . . . , n must be attached
to exactly one parent (either another token or the
root 0). We can further require the tree to be pro-
jective, meaning that edges are not allowed to cross
each other. It is well known that dynamic program-
ming can be used to find the best projective depen-
dency tree in O(n3) time, much as in CKY, for first-
order models and some higher-order models (Eis-
ner, 1996; McDonald and Pereira, 2006).1 When
the projectivity restriction is lifted, McDonald et al
(2005b) pointed out that the best tree can be found in
O(n2) time using a minimum directed spanning tree
algorithm (Chu and Liu, 1965; Edmonds, 1967; Tar-
jan, 1977), though only for first-order models.2 We
will make use of this fast non-projective algorithm
as a subroutine in early stages of our system.
3 Dynamic Feature Selection
Unlike typical feature selection methods that fix a
subset of selected features and use it throughout test-
ing, in dynamic feature selection we choose features
adaptively for each instance. We briefly introduce
this framework below and motivate our algorithm
from empirical results on MST dependency parsing.
1Although the third-order model of Koo and Collins (2010),
for example, takes O(n4) time.
2The non-projective parsing problem becomes NP-hard for
higher-order models. One approximate solution (McDonald
and Pereira, 2006) works by doing projective parsing and then
rearranging edges.
1456
  
.This time , the firms were ready$ .This time , the firms were ready$ $ This time , the firms were ready .
This time , were ready .$ the firms
add feat.group
projectivedecoding
(a) (b) (c)
.This time , were ready$ the firms(d)(e).This time , the firms were ready$(f)
add feat.group
add feat.group
add f eat.group
Figure 1: Dynamic feature selection for dependency parsing. (a) Start with all possible edges except those filtered
by the length dictionary. (b) ? (e) Add the next group of feature templates and parse using the non-projective parser.
Predicted trees are shown as blue and red edges, where red indicates the edges that we then decide to lock. Dashed
edges are pruned because of having the same child as a locked edge; 2-dot-3-dash edges are pruned because of crossing
with a locked edge; fine-dashed edges are pruned because of forming a cycle with a locked edge; and 2-dot-1-dash
edges are pruned since the root has already been locked with one child. (f) Final projective parsing.
3.1 Sequential Decision Making
Our work is motivated by recent progress on dy-
namic feature selection (Benbouzid et al, 2012; He
et al, 2012; Grubb and Bagnell, 2012), where fea-
tures are added sequentially to a test instance based
on previously acquired features and intermediate
prediction results. This requires sequential decision
making. Abstractly, when the system is in some state
s ? S, it chooses an action a = pi(s) from the ac-
tion setA using its policy pi, and transitions to a new
state s?, inducing some cost. In the specific case of
dynamic feature selection, when the system is in a
given state, it decides whether to add some more
features or to stop and make a prediction based on
the features added so far. Usually the sequential de-
cision making problem is solved by reinforcement
learning (Sutton and Barto, 1998) or imitation learn-
ing (Abbeel and Ng, 2004; Ratliff et al, 2004).
The dynamic feature selection framework has
been successfully applied to supervised classifica-
tion and ranking problems (Benbouzid et al, 2012;
He et al, 2012; Gao and Koller, 2010). Below, we
design a version that avoids overhead in our struc-
tured prediction setting. As there are n2 possible
edges on a sentence of length n, we wish to avoid
the overhead of making many individual decisions
about specific features on specific edges, with each
decision considering the current scores of all other
edges. Instead we will batch the work of dynamic
feature selection into a smaller number of coarse-
grained steps.
3.2 Strategy
To speed up graph-based dependency parsing, we
first investigate time usage in the parsing process
on our development set, section 22 of the Penn
Treebank (PTB) (Marcus et al, 1993). In Fig-
ure 2, we observe that (a) feature computation took
more than 80% of the total time; (b) even though
non-projective decoding time grows quadratically in
terms of the sentence length, in practice it is al-
most negligible compared to the projective decoding
time, with an average of 0.23 ms; (c) the second-
order projective model is significantly slower due
to higher asymptotic complexity in both the scoring
and decoding stages.
At each stage of our algorithm, we need to de-
cide whether to use additional features to refine the
edge scores. As making this decision separately for
each of the n2 possible edges is expensive, we in-
stead propose a version that reduces the number of
decisions needed. We show the process for one short
sentence in Figure 1. The first step is to parse us-
ing the current features. We use the fast first-order
non-projective parser for this purpose, since given
observations (b) and (c), we cannot afford to run
projective parsing multiple times. The single result-
ing tree (blue and red edges in Figure 1) has only
1457
0 10 20 30 40 50 60 70sentence length0
200
400
600
800
1000
1200
1400
mean
time(
ms)
1st-order scoring O(n2)2nd-order scoring O(n3)proj dec O(n3)non-proj dec O(n2)2nd-order proj dec O(n3)
Figure 2: Time comparison of scoring time and decoding
time on English PTB section 22.
n edges, and we use a classifier to decide which
of these edges are reliable enough that we should
?lock? them?i.e., commit to including them in the
final tree. This is the only decision that our policy
pi must make. Locked (red) edges are definitely in
the final tree. We also do constraint propagation: we
rule out all edges that conflict with the locked edges,
barring them from appearing in the final tree.3 Con-
flicts are defined as violation of the projective pars-
ing constraints:
? Each word has exactly one parent
? Edges cannot cross each other4
? The directed graph is non-cyclic
? Only one word is attached to the root
For example, in Figure 1(d), the dashed edges are
removed because they have the same child as one of
the locked (red) edges. The 2-dot-3-dash edge time
? firms is removed because it crosses the locked
edge (comma)? were (whereas we ultimately seek
a projective parse). The fine dashed edge were ?
(period) is removed because it forms a cycle with
were ? (period). In Figure 1(e), the 2-dot-1-dash
edge (root) ? time is removed since we allow the
root to have only one modifier.
3Constraint propagation also automatically locks an edge
when all other edges with the same child have been ruled out.
4A reviewer asks about the cost of finding edges that cross a
locked edge. Naively this is O(n2). But at most n edges will be
locked during the entire algorithm, for a total O(n3) runtime?
the same as one call to projective parsing, and far faster in prac-
tice. With cleverness this can even be reduced to O(n2 logn).
Once constraint propagation has finished, we visit
all edges (gray) whose fate is still unknown, and up-
date their scores in parallel by adding the next group
of features.
As a result, most edges will be locked in or ruled
out without needing to look up all of their features.
Some edges may still remain uncertain even after in-
cluding all features. If so, a final iteration (Figure 1
(f)) uses the slower projective parser to resolve the
status of these maximally uncertain edges. In our
example, the parser does not figure out the correct
parent of time until this final step. This final, accu-
rate parser can use its own set of weighted features,
including higher-order features, as well as the pro-
jectivity constraint. But since it only needs to re-
solve the few uncertain edges, both scoring and de-
coding are fast.
If we wanted our parser to be able to produce non-
projective trees, then we would skip this final step
or have it use a higher-order non-projective parser.
Also, at earlier steps we would not prune edges
crossing the locked edges.
4 Methods
Our goal is to produce a faster dependency parser by
reducing the feature computation time. We assume
that we are given three increasingly accurate but in-
creasingly slow parsers that can be called as sub-
routines: a first-order non-projective parser, a first-
order projective parser, and a second-order projec-
tive parser. In all cases, their feature weights have
already been trained using the full set of features,
and we will not change these weights. In general
we will return the output of one of the projective
parsers. But at early iterations, the non-projective
parser helps us rapidly consider interactions among
edges that may be relevant to our dynamic decisions.
4.1 Feature Template Ranking
We first rank the 268 first-order feature templates by
forward selection. We start with an empty list of fea-
ture templates, and at each step we greedily add the
one whose addition most improves the parsing ac-
curacy on a development set. Since some features
may be slower than others (for example, the ?be-
tween? feature templates require checking all tokens
in-between the head and the modifier), we could in-
1458
0 50 100 150 200 250 300 350 400Number of feature templates used0.5
0.6
0.7
0.8
0.9
Unlab
eleda
ttachm
entsc
ore(U
AS)
1st-order non-proj1st-order proj2nd-order
Figure 3: Forward feature selection result using the non-
projective model on English PTB section 22.
stead select the feature template with the highest ra-
tio of accuracy improvement to runtime. However,
for simplicity we do not consider this: after group-
ing (see below), minor changes of the ranks within a
group have no effect. The accuracy is evaluated by
running the first-order non-projective parser, since
we will use it to make most of the decisions. The
112 second-order feature templates are then ranked
by adding them in a similar greedy fashion (given
that all first-order features have already been added),
evaluating with the second-order projective parser.
We then divide this ordered list of feature tem-
plates into K groups: {T1, T2, . . . , TK}. Our parser
adds an entire group of feature templates at each
step, since adding one template at a time would re-
quire too many decisions and obviate speedups. The
simplest grouping method would be to put an equal
number of feature templates in each group. From
Figure 3 we can see that the accuracy increases sig-
nificantly with the first few templates and gradually
levels off as we add less valuable templates. Thus,
a more cost-efficient method is to split the ranked
list into several groups so that the accuracy increases
by roughly the same amount after each group is
added. In this case, earlier stages are fast because
they tend to have many fewer feature templates than
later stages. For example, for English, we use 7
groups of first-order feature templates and 4 groups
of second-order feature templates. The sequence of
group sizes is 1, 4, 10, 12, 47, 33, 161 and 35, 29, 31,
17 for first- and second-order parsing respectively.
4.2 Sequential Feature Selection
Similar to the length dictionary filter of Rush and
Petrov (2012), for each test sentence, we first de-
terministically remove edges longer than the maxi-
mum length of edges in the training set that have the
same head POS tag, modifier POS tag, and direction.
This simple step prunes around 40% of the non-gold
edges in our Penn Treebank development set (Sec-
tion 6.1) at a cost of less than 0.1% in accuracy.
Given a test sentence of length n, we start with
a complete directed graph G(V, E), where E =
{?h,m? : h ? [0, n], m ? [1, n]}. After the length
dictionary pruning step, we compute T1 for all re-
maining edges to obtain a pruned weighted directed
graph. We predict a parse tree using the features so
far (other features are treated as absent, with value
0). Then for each edge in this intermediate tree, we
use a binary linear classifier to choose between two
actions: A = {lock, add}. The lock action ensures
that ?h,m? appears in the final parse tree by prun-
ing edges that conflict with ?h,m?.5 If the classi-
fier is not confident enough about the parent of m,
it decides to add to gather more information. The
add action computes the next group of features for
?h,m? and all other competing edges with child m.
(Since we classify the edges one at a time, deci-
sions on one edge may affect later edges. To im-
prove efficiency and reduce cascaded error, we sort
the edges in the predicted tree and process them as
above in descending order of their scores.)
Now we can continue with the second iteration of
parsing. Overall, our method runs up to K = K1 +
K2 iterations on a given sentence, where we have
K1 groups of first-order features and K2 groups of
second-order features. We run K1 ? 1 iterations
of non-projective first-order parsing (adding groups
T1, . . . , TK1?1), then 1 iteration of projective first-
order parsing (adding group TK1), and finally K2 it-
erations of projective second-order parsing (adding
groups TK1+1, . . . TK).
Before each iteration, we use the result of the pre-
vious iteration (as explained above) to prune some
edges and add a new group of features to the rest. We
5If the conflicting edge is in the current predicted parse tree
(which can happen because of non-projectivity), we forbid the
model to prune it. Otherwise in rare cases the non-projective
parser at the next stage may fail to find a tree.
1459
then run the relevant parser. Each of the three parsers
has a different set of feature weights, so when we
switch parsers on rounds K1 and K1 + 1, we must
also change the weights of the previously added fea-
tures to those specified by the new parsing model.
In practice, we can stop as soon as the fate of all
edges is known. Also, if no projective parse tree
can be constructed at round K1 using the available
unpruned edges, then we immediately fall back to
returning the non-projective parse tree from round
K1 ? 1. This FAIL case rarely occurs in our experi-
ments (fewer than 1% of sentences).
We report results both for a first-order system
where K2 = 0 (shown in Figure 1 and Algorithm 1)
and for a second-order system where K2 > 0.
Algorithm 1 DynFS(G(V, E), pi)
E ? {?h,m? : |h?m| ? lenDict(h,m)}
Add T1 to all edges in E
y? ? non-projective decoding
for i = 2 to K do
Esort ? sort unlocked edges {E : E ? y?} in
descending order of their scores
for ?h,m? ? Esort do
if pi(?(?h,m?)) == lock then
E ? E \ {{?h?,m? ? E : h? 6= h}
?
{?h?,m?? ? E : crosses ?h,m?} ?
{?h?,m?? ? E : cycle with ?h,m?}}
if h == 0 then
E ? E \ {?0,m?? ? E : m? 6= m}
end if
else
Add Ti to {?h?,m?? ? E : m? == m}
end if
end for
if i == K then
y? ? projective decoding
else if i 6= K or FAIL then
y? ? non-projective decoding
end if
end for
return y?
5 Policy Training
We cast this problem as an imitation learning task
and use Dataset Aggregation (DAgger) Ross et al
(2011) to train the policy iteratively.
5.1 Imitation Learning
In imitation learning (also called apprenticeship
learning) (Abbeel and Ng, 2004; Ratliff et al, 2004),
instead of exploring the environment directed by its
feedback (reward) as in typical reinforcement learn-
ing problems, the learner observes expert demon-
strations and aims to mimic the expert?s behavior.
The expert demonstration can be represented as tra-
jectories of state-action pairs, {(st, at)} where t is
the time step. A typical approach to imitation learn-
ing is to collect supervised data from the expert?s
trajectories to learn a policy (multiclass classifier),
where the input is ?(s), a feature representation of
the current state (we call these policy features to
avoid confusion with the parsing features), and the
output is the predicted action (label) for that state.
In the sequential feature selection framework, it is
hard to directly apply standard reinforcement learn-
ing algorithms, as we cannot assign credit to certain
features until the policy decides to stop and let us
evaluate the prediction result. On the other hand,
knowing the gold parse tree makes it easy to ob-
tain expert demonstrations, which enables imitation
learning.
5.2 DAgger
Since the above approach collects training data only
from the expert?s trajectories, it ignores the fact that
the distribution of states at training time and that at
test time are different. If the learned policy can-
not mimic the expert perfectly, one wrong step may
lead to states never visited by the expert due to cu-
mulative errors. This problem of insufficient explo-
ration can be alleviated by iteratively learning a pol-
icy trained under states visited by both the expert
and the learner (Ross et al, 2011; Daume? III et al,
2009; Ka?a?ria?inen, 2006).
Ross et al (2011) proposed to train the policy iter-
atively and aggregate data collected from the previ-
ous learned policy. Let pi? denote the expert?s policy
and spii denote states visited by executing pii. In its
simplest parameter-free form, in each iteration, we
first run the most recently learned policy pii; then for
each state spii on the trajectory, we collect a training
example (?(spii), pi?(spii)) by labeling the state with
the expert?s action. Intuitively, this step intends to
correct the learner?s mistakes and pull it back to the
1460
expert?s trajectory. Thus we can obtain a policy that
performs well under its own induced state distribu-
tion.
5.3 DAgger for Feature Selection
In our case, the expert?s decision is rather straight-
forward. Replace the policy pi in Algorithm 1 by
an expert. If the edge under consideration is a gold
edge, it executes lock; otherwise, it executes add.
Basically the expert ?cheats? by knowing the true
tree and always making the right decision. On our
PTB dev set, it can get 96.47% accuracy6 with only
2.9% of the first-order features. This is an upper
bound on our performance.
We present the training procedure in Algorithm
2. We begin by partitioning the training set into
N folds. To simulate parsing results at test time,
when collecting examples on T i, similar to cross-
validation, we use parsers trained on T? i = T \ T i.
Also note that we show only one pass over training
sentences in Algorithm 2; however, multiple passes
are possible in practice, especially when the training
data is limited.
Algorithm 2 DAgger(T , pi?)
Split the training sentences T into N folds
T 1, T 2, . . . , T N
Initialize D ? ?, pi1 ? pi?
for i = 1 to N do
for G(V, E) ? T i do
Sample trajectories {(spii , pii(spii))} by
DynFS(G(V, E), pii)
D ? D
?
{(?(s), pi?(s)}
end for
end for
Train policy pii+1 on D
return Best pii evaluated on development set
5.4 Policy Features
Our linear edge classifier uses a feature vector ? that
concatenates all previously acquired parsing fea-
tures together with ?meta-features? that reflect con-
fidence in the edge. The classifier?s weights are fixed
6The imperfect performance is because the accuracy is mea-
sured with respect to the gold parse trees. The expert only
makes optimal pruning decisions but the performance depends
on the pre-trained parser as well.
across iterations, but ?(edge) changes per iteration.
We standardize the edge scores by a sigmoid func-
tion. Let s? denote the normalized score, defined
by s??(?h,m?) = 1/(1 + exp{?s?(?h,m?)}). Our
meta-features for ?h,m? include
? current normalized score, and normalized score
before adding the current feature group
? margin to the highest scoring competing edges,
i.e., s?(w, ?h,m?)?maxh? s?(w, ?h?,m?)
where h? ? [0, n] and h? 6= h
? index of the next feature group to be added
We also tried more complex meta-features, for ex-
ample, mean and variance of the scores of compet-
ing edges, and structured features such as whether
the head of e is locked and how many locked chil-
dren it currently has. It turns out that given all the
parsing features, the margin is the most discrimi-
native meta-feature. When it is present, other meta-
features we added do not help much, Thus we do not
include them in our experiments due to overhead.
6 Experiment
6.1 Setup
We generate dependency structures from the PTB
constituency trees using the head rules of Yamada
and Matsumoto (2003). Following convention, we
use sections 02?21 for training, section 22 for de-
velopment and section 23 for testing. We also re-
port results on six languages from the CoNLL-X
shared task (Buchholz and Marsi, 2006) as sug-
gested in (Rush and Petrov, 2012), which cover a
variety of language families. We follow the stan-
dard training/test split specified in the CoNLL-X
data and tune parameters by cross validation when
training the classifiers (policies). The PTB test data
is tagged by a Stanford part-of-speech (POS) tagger
(Toutanova et al, 2003) trained on sections 02?21.
We use the provided gold POS tags for the CoNLL
test data. All results are evaluated by the unlabeled
attachment score (UAS). For fair comparison with
previous work, punctuation is included when com-
puting parsing accuracy of all CoNLL-X languages
but not English (PTB).
For policy training, we train a linear SVM classi-
fier using Liblinear (Fan et al, 2008). For all lan-
guages, we run DAgger for 20 iterations and se-
1461
Language Method
First-order Second-order
Speedup Cost(%) UAS(D) UAS(F) Speedup Cost(%) UAS(D) UAS(F)
Bulgarian
DYNFS 3.44 34.6 91.1 91.3 4.73 16.3 91.6 92.0
VINEP 3.25 - 90.5 90.7 7.91 - 91.6 92.0
Chinese
DYNFS 2.12 42.7 91.0 91.3 2.36 31.6 91.6 91.9
VINEP 1.02 - 89.3 89.5 2.03 - 90.3 90.5
English
DYNFS 5.58 24.8 91.7 91.9 5.27 49.1 92.5 92.7
VINEP 5.23 - 91.0 91.2 11.88 - 92.2 92.4
German
DYNFS 4.71 21.0 89.2 89.3 6.02 36.6 89.7 89.9
VINEP 3.37 - 89.0 89.2 7.38 - 90.1 90.3
Japanese
DYNFS 4.80 15.6 93.7 93.6 8.49 7.53 93.9 93.9
VINEP 4.60 - 91.7 92.0 14.90 - 92.1 92.0
Portuguese
DYNFS 4.36 32.9 87.3 87.1 6.84 40.4 88.0 88.2
VINEP 4.47 - 90.0 90.1 12.32 - 90.9 91.2
Swedish
DYNFS 3.60 37.8 88.8 89.0 5.04 22.1 89.5 89.8
VINEP 4.64 - 88.3 88.5 13.89 - 89.4 89.7
Table 1: Comparison of speedup and accuracy with the vine pruning cascade approach for six languages. In the setup,
DYNFS means our dynamic feature selection model, VINEP means the vine pruning cascade model, UAS(D) and
UAS(F) refer to the unlabeled attachment score of the dynamic model (D) and the full-feature model (F) respectively.
For each language, the speedup is relative to its corresponding first- or second-order model using the full set of features.
Results for the vine pruning cascade model are taken from Rush and Petrov (2012). The cost is the percentage of
feature templates used per sentence on edges that are not pruned by the dictionary filter.
lect the best policy evaluated on the development set
among the 20 policies obtained from each iteration.
6.2 Baseline Models
We use the publicly available implementation of
MSTParser7 (with modifications to the feature com-
putation) and its default settings, so the feature
weights of the projective and non-projective parsers
are trained by the MIRA algorithm (Crammer and
Singer, 2003; Crammer et al, 2006).
Our feature set contains most features proposed
in the literature (McDonald et al, 2005a; Koo and
Collins, 2010). The basic feature components in-
clude lexical features (token, prefix, suffix), POS
features (coarse and fine), edge length and direction.
The feature templates consists of different conjunc-
tions of these components. Other than features on
the head word and the child word, we include fea-
tures on in-between words and surrounding words as
well. For PTB, our first-order model has 268 feature
templates and 76,287,848 features; the second-order
model has 380 feature templates and 95,796,140 fea-
tures. The accuracy of our full-feature models is
7http://www.seas.upenn.edu/?strctlrn/
MSTParser/MSTParser.html
comparable or superior to previous results.
6.3 Results
0 1 2 3 4 5 6Feature selection stage0.0
0.2
0.4
0.6
0.8
1.0
Time/
Accur
acy/E
dgeP
ercen
tage
runtime %UAS %remaining edge %locked edge %pruned edge %
Figure 4: System dynamics on English PTB section 23.
Time and accuracy are relative to those of the baseline
model using full features. Red (locked), gray (unde-
cided), dashed gray (pruned) lines correspond to edges
shown in Figure 1.
In Table 1, we compare the dynamic parsing mod-
els with the full-feature models and the vine prun-
ing cascade models for first-order and second-order
1462
0 10 20 30 40 50 60 70 80Runtime (s)0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
Unlab
eleda
ttachm
entsc
ore(U
AS)
staticdynamic
Figure 5: Pareto curves for the dynamic and static ap-
proaches on English PTB section 23.
parsing. The speedup for each language is defined as
the speed relative to its full-feature baseline model.
We take results reported by Rush and Petrov (2012)
for the vine pruning model. As speed comparison
for parsing largely relies on implementation, we also
report the percentage of feature templates chosen for
each sentence. The cost column shows the average
number of feature templates computed for each sen-
tence, expressed as a percentage of the number of
feature templates if we had only pruned using the
length dictionary filter.
From the table we notice that our first-order
model?s performance is comparable or superior to
the vine pruning model, both in terms of speedup
and accuracy. In some cases, the model with fewer
features even achieves higher accuracy than the
model with full features. The second-order model,
however, does not work as well. In our experi-
ments, the second-order model is more sensitive to
false negatives, i.e. pruning of gold edges, due to
larger error propagation than the first-order model.
Therefore, to maintain parsing accuracy, the policy
must make high-precision pruning decisions and be-
comes conservative. We could mitigate this by train-
ing the original parsing feature weights in conjunc-
tion with our policy feature weights. In addition,
there is larger overhead during when checking non-
projective edges and cycles.
We demonstrate the dynamics of our system in
Figure 4 on PTB section 23. We show how the run-
time, accuracy, number of locked edges and unde-
cided edges change over the iterations in our first-
order dynamic projective parsing. From iterations
1 to 6, we obtain parsing results from the non-
projective parser; in iteration 7, we run the projective
parser. The plot shows relative numbers (percent-
age) to the baseline model with full features. The
number of remaining edges drops quickly after the
second iteration. From Figure 3, however, we notice
that even with the first feature group which only con-
tains one feature template, the non-projective parser
can almost achieve 50% accuracy. Thus, ideally, our
policy should have locked that many edges after the
first iteration. The learned policy does not imitate
the expert perfectly, either because our policy fea-
tures are not discriminative enough, or because a lin-
ear classifier is not powerful enough for this task.
Finally, to show the advantage of making dynamic
decisions that consider the interaction among edges
on the given input sentence, we compare our results
with a static feature selection approach on PTB sec-
tion 23. The static algorithm does no pruning except
by the length dictionary at the start. In each iteration,
instead of running a fast parser and making deci-
sions online, it simply adds the next group of feature
templates to all edges. By forcing both algorithms
to stop after each stage, we get the Pareto curves
shown in Figure 5. For a given level of high accu-
racy, our dynamic approach (black) is much faster
than its static counterpart (blue).
7 Conclusion
In this paper we present a dynamic feature selec-
tion algorithm for graph-based dependency parsing.
We show that choosing feature templates adaptively
for each edge in the dependency graph greatly re-
duces feature computation time and in some cases
improves parsing accuracy. Our model also makes
it practical to use an even larger feature set, since
features are computed only when needed. In future,
we are interested in training parsers favoring the dy-
namic feature selection setting, for example, parsers
that are robust to missing features, or parsers opti-
mized for different stages.
Acknowledgements
This work was supported by the National Science
Foundation under Grant No. 0964681. We thank the
anonymous reviewers for very helpful comments.
1463
References
P. Abbeel and A. Y. Ng. 2004. Apprenticeship learning
via inverse reinforcement learning. In Proceedings of
ICML.
D. Benbouzid, R. Busa-Fekete, and B. Ke?gl. 2012. Fast
classification using space decision DAGs. In Proceed-
ings of ICML.
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proceedings of
COLING.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine Hill,
R. Shrivaths, Jeremy Moore, Michael Pozar, and
Theresa Vu. 2006. Multilevel coarse-to-fine PCFG
parsing. In Proceedings of ACL.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Hal Daume? III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing Journal (MLJ).
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
(71B):233?240.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: an exploration. In Proceedings
of COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Tianshi Gao and Daphne Koller. 2010. Active classifi-
cation based on value of classifier. In Proceedings of
NIPS.
Alexander Grubb and J. Andrew Bagnell. 2012.
SpeedBoost: Anytime prediction with uniform near-
optimality. In AISTATS.
He He, Hal Daume? III, and Jason Eisner. 2012. Cost-
sensitive dynamic feature selection. In ICML Infern-
ing Workshop.
Matti Ka?a?ria?inen. 2006. Lower bounds for reduc-
tions. Talk at the Atomic Learning Workshop (TTI-C),
March.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proceedings of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, K. Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency parsing
using spanning tree algorithms. In Proc. of EMNLP.
N. Ratliff, D. Bradley, J. A. Bagnell, and J. Chestnutt.
2004. Boosting structured prediction for imitation
learning. In Proceedings of ICML.
B. Roark and K. Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference.
In Proceedings of COLING.
Ste?phane. Ross, Geoffrey J. Gordon, and J. Andrew. Bag-
nell. 2011. A reduction of imitation learning and
structured prediction to no-regret online learning. In
Proceedings of AISTATS.
Alexander Rush and Slav Petrov. 2012. Vine pruning for
efficient multi-pass dependency parsing. In Proceed-
ings of NAACL.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning : An Introduction. MIT Press.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?35.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL.
Paul Viola and Michael Jones. 2004. Robust feal-time
face detection. International Journal of Computer Vi-
sion, 57:137?154.
Qin Iris Wang, Dekang Lin, and Dale Schuurmans. 2007.
Simple training of dependency parsers via structured
boosting. In Proceedings of IJCAI.
David Weiss and Ben Taskar. 2010. Structured predic-
tion cascades. In Proceedings of AISTATS.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with Support Vector Machines. In Pro-
ceedings of IWPT.
1464
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 120?130,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Minimum-Risk Training of Approximate CRF-Based NLP Systems
Veselin Stoyanov and Jason Eisner
HLTCOE and Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{ves, jason}@cs.jhu.edu
Abstract
Conditional Random Fields (CRFs) are a pop-
ular formalism for structured prediction in
NLP. It is well known how to train CRFs with
certain topologies that admit exact inference,
such as linear-chain CRFs. Some NLP phe-
nomena, however, suggest CRFs with more
complex topologies. Should such models be
used, considering that they make exact infer-
ence intractable? Stoyanov et al (2011) re-
cently argued for training parameters to min-
imize the task-specific loss of whatever ap-
proximate inference and decoding methods
will be used at test time. We apply their
method to three NLP problems, showing that
(i) using more complex CRFs leads to im-
proved performance, and that (ii) minimum-
risk training learns more accurate models.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are often used to model dependencies among
linguistic variables. CRF-based models have im-
proved the state of the art in a number of natural
language processing (NLP) tasks ranging from part-
of-speech tagging to information extraction and sen-
timent analysis (Lafferty et al, 2001; Peng and Mc-
Callum, 2006; Choi et al, 2005).
Robust and theoretically sound training proce-
dures have been developed for CRFs when the
model can be used with exact inference and de-
coding.1 However, some NLP problems seem to
1?Inference? typically refers to computing posterior
marginal or max-marginal probability distributions of output
random variables, given some evidence. ?Decoding? derives
a single structured output from the results of inference.
call for higher-treewidth graphical models in which
exact inference is expensive or intractable. These
?loopy? CRFs have cyclic connections among the
output and/or latent variables. Alas, standard learn-
ing procedures assume exact inference: they do not
compensate for approximations that will be used at
test time, and can go surprisingly awry if approxi-
mate inference is used at training time (Kulesza and
Pereira, 2008).
While NLP research has been consistently evolv-
ing toward more richly structured models, one may
hesitate to add dependencies to a graphical model if
there is a danger that this will end up hurting per-
formance through approximations. In this paper we
illustrate how to address this problem, even for ex-
tremely interconnected models in which every pair
of output variables is connected.
Wainwright (2006) showed that if approximate in-
ference will be used at test time, it may be beneficial
to use a learning procedure that does not converge to
the true model but to one that performs well under
the approximations. Stoyanov et al (2011) argue for
minimizing a certain non-convex training objective,
namely the empirical risk of the entire system com-
prising the CRF together with whatever approximate
inference and decoding procedures will be used at
test time. They regard this entire system as sim-
ply a complex decision rule, analogous to a neu-
ral network, and show how to use back-propagation
to tune its parameters to locally minimize the em-
pirical risk (i.e., the average task-specific loss on
training data). Stoyanov et al (2011) show that
on certain synthetic-data problems, this frequentist
training regimen significantly reduced test-data loss
120
compared to approximate maximum likelihood esti-
mation (MLE). However, this method has not been
evaluated on real-world problems until now.
We will refer to the Stoyanov et al (2011) ap-
proach as ?ERMA??Empirical Risk Minimization
under Approximations. ERMA is attractive for NLP
because the freedom to use arbitrarily structured
graphical models makes it possible to include latent
linguistic variables, predict complex structures such
as parses (Smith and Eisner, 2008), and do collec-
tive prediction in relational domains (Ji and Grish-
man, 2011; Benson et al, 2011; Dreyer and Eis-
ner, 2009). In training, ERMA considers not only
the approximation method but also the task-specific
loss function. This means that ERMA is careful to
use the additional variables and dependencies only
in ways that help training set performance. (Overfit-
ting on the enlarged parameter set should be avoided
through regularization.)
We have developed a simple syntax for specify-
ing CRFs with complex structures, and a software
package (available from http://www.clsp.
jhu.edu/?ves/software.html) that allows
ERMA training of these CRFs for several popular
loss functions (e.g., accuracy, mean-squared error,
F-measure). In this paper, we use these tools to re-
visit three previously studied NLP applications that
can be modeled naturally with approximate CRFs
(we will use approximate CRFs to refer to CRF-
based systems that are used with approximations in
inference or decoding). We show that (i) natural lan-
guage can be modeled more effectively with CRFs
that are not restricted to a linear structure and (ii)
that ERMA training represents an improvement over
previous learning methods.
The first application, predicting congressional
votes, has not been previously modeled with CRFs.
By using a more principled probabilistic approach,
we are able to improve the state-of-the-art accuracy
from 71.2% to 78.2% when training to maximize the
approximate log-likelihood of the training data. By
switching to ERMA training, we improve this result
further to 85.1%.
The second application, information extraction
from seminar announcements, has been modeled
previously with skip-chain CRFs (Sutton and Mc-
Callum, 2005; Finkel et al, 2005). The skip-chain
CRF introduces loops and requires approximate in-
ference, which motivates minimum risk training.
Our results show that ERMA training improves F-
measures from 89.5 to 90.9 (compared to 87.1 for
the model without skip-chains).
Finally, for our third application, we perform col-
lective multi-label text classification. We follow pre-
vious work (Ghamrawi and McCallum, 2005; Finley
and Joachims, 2008) and use a fully connected CRF
to model all pairwise dependencies between labels.
We observe similar trends for this task: switching
from a maximum entropy model that does not model
label dependencies to a loopy CRF leads to an im-
provement in F-measure from 81.6 to 84.0, and us-
ing ERMA leads to additional improvement (84.7).
2 Preliminaries
2.1 Conditional Random Fields
A conditional random field (CRF) is an undirected
graphical model defined by a tuple (X ,Y,F , f, ?).
X = (X1, X2, . . .) is a set of random variables and
Y = (Y1, Y2, . . .) is a set of output random vari-
ables.2 We use x = (x1, x2, . . .), to denote a possi-
ble assignment of values to X , and similarly for y,
with xy denoting the joint assignment. Each ? ? F
is a subset of the random variables, ? ? X ? Y ,
and we write xy? to denote the restriction of xy to
?. Finally, for each ? ? F , the CRF specifies a
function ~f? that extracts a feature vector ? Rd from
the restricted assignment xy?. We define the over-
all feature vector ~f(x,y) =
?
??F
~f?(xy?) ? R
d.
The model defines conditional probabilities
p?(y|x) =
exp ~? ? ~f(x,y)
?
y? exp
~? ? ~f(x,y?)
(1)
where ~? ? Rd is a global weight vector (to be
learned). This is a log-linear model; the denomina-
tor (traditionally denoted Zx) sums over all possible
output assignments to normalize the distribution.
Provided that all probabilities needed at training
or test time are conditioned on an observation of the
form X = x, CRFs can include arbitrary overlap-
ping features of the input without having to explic-
itly model input feature dependencies.
2Stoyanov et al (2011) distinguished some of the Y vari-
ables as latent (i.e., unsupervised and ignored by the loss func-
tion). We omit this possibility, to simplify the notation.
121
2.2 Inference in CRFs
Inference in general CRFs is intractable (Koller and
Friedman, 2009). Nevertheless, there exist several
approximate algorithms that have theoretical moti-
vation and tend to exhibit good performance in prac-
tice. Those include variational methods such as
loopy belief propagation (BP) (Murphy et al, 1999)
and mean-field, as well as Markov Chain Monte
Carlo methods.
ERMA training is applicable to any approxima-
tion that corresponds to a differentiable function,
even if the function has no simple closed form but is
computed by an iterative update algorithm. In this
paper we select BP, which is exact when the fac-
tor graph is a tree, such as a linear-chain CRF, but
whose results can be somewhat distorted by loops
in the factor graph, as in our settings. BP computes
beliefs about the marginal distribution of each ran-
dom variable using iterative updates. We standardly
approximate the posterior CRF marginals given the
input observations by running BP over a CRF that
enforces those observations.
2.3 Decoding
Conditional random fields are models of probabil-
ity. A decoder is a procedure for converting these
probabilities into system outputs. Given x, the de-
coder would ideally choose y to minimize the loss
`(y,y?), where ` compares a candidate assignment
y to the true assignment y?. But of course we do not
know the truth at test time. Instead we can average
over possible values y? of the truth:
argmin
y
?
y?
p(y? | x) ? `(y,y?) (2)
This is the minimum Bayes risk (MBR) principle
from statistical decision theory: choose y to mini-
mize the expected loss (i.e., the risk) according to
the CRF?s posterior beliefs given x.
In the NLP literature, CRFs are often decoded by
choosing y to be the maximum posterior probabil-
ity assignment (e.g., Sha and Pereira (2003), Sutton
et al (2007)). This is the MBR procedure for the
0-1 loss function that simply tests whether y = y?.
For other loss functions, however, the corresponding
MBR procedure is preferable. For some loss func-
tions it is tractable given the posterior marginals of
p, while in other cases approximations are needed.
In our experiments we use MBR decoding (or a
tractable approximation) but substitute the approx-
imate posterior marginals of p as computed by BP.
For example, if the loss of y is the number of incor-
rectly recovered output variables, MBR says to sep-
arately pick the most probable value for each output
variable, according to its (approximate) marginal.
3 Minimum-Risk CRF Training
This section briefly describes the ERMA training al-
gorithm from Stoyanov et al (2011) and compares it
to related structured learning methods. We assume
a standard ML setting, with a set of training inputs
xi and corresponding correct outputs yi?. All the
methods below are regularized in practice, but we
omit mention of regularizers for simplicity.
3.1 Related Structured Learning Methods
When inference and decoding can be performed ex-
actly, the CRF parameters ~? are often trained by
maximum likelihood estimation (MLE):
argmax
?
?
i
log p?(y
i? | xi) (3)
The gradient of each summand log p?(yi? | xi)
can be computed by performing inference in two set-
tings, one with xi,yi? observed and one with only
the conditioning events xi observed. The gradient
emerges as the difference between the feature ex-
pectations in the two cases. If exact inference is
intractable, one can compute approximate feature
expectations by loopy BP. Computing the approx-
imate gradient in this way, and training the CRF
with some gradient-based optimization method, has
been shown to work relatively well in practice (Vish-
wanathan et al, 2006; Sutton and McCallum, 2005).
The above method takes into account neither the
loss function that will be used for evaluation, nor
the approximate algorithms that have been selected
for inference and decoding at test time. Other struc-
ture learning methods do consider loss, though it is
not obvious how to make them consider approxima-
tions. Those include maximum margin (Taskar et
al., 2003; Finley and Joachims, 2008) and softmax-
margin (Gimpel and Smith, 2010). The idea of
margin-based methods is to choose weights ~? so that
the correct alternative yi? always gets a better score
122
than each possible alternative yi ? Y . The loss is
incorporated in these methods by requiring the mar-
gin (~? ? ~f(xi,yi?)? ~? ? ~f(xi,yi)) ? `(yi,yi?), with
penalized slack in these constraints. The softmax-
margin method uses a different criterion?it resem-
bles MLE but modifies the denominator of (1) to
Zx =
?
y??Y exp(
~? ? ~f(x,y?) + `(y?,y?)).
In our experiments we compare against MLE
training (which is common) and softmax-margin,
which incorporates loss and which Gimpel and
Smith (2010) show is either better or competitive
when compared to other margin methods on an NLP
task. We adapt these methods to the loopy case in
the obvious way, by replacing exact inference with
loopy BP and keeping everything else the same.
3.2 Minimum-Risk Training
We wish to consider the approximate inference and
decoding algorithms and the loss function that will
be used during testing. Thus, we want ? to minimize
the expected loss under the true data distribution P :
argmin
?
Exy?P [`(??(x),y)] (4)
where ?? is the decision rule (parameterized by ?),
which decodes the results of inference under p?.
In practice, we do not know the true data distri-
bution, but we can do empirical risk minimization
(ERM), instead averaging the loss over our sample
of (xi,yi) pairs. ERM for structured prediction was
first introduced in the speech community (Bahl et
al., 1988) and later used in NLP (Och, 2003; Kakade
et al, 2002; Suzuki et al, 2006; Li and Eisner, 2009,
etc.). Previous applications of risk minimization as-
sume exact inference, having defined the hypothe-
sis space by a precomputed n-best list, lattice, or
packed forest over which exact inference is possible.
The ERMA approach (Stoyanov et al, 2011)
works with approximate inference and computes ex-
act gradients of the output loss (or a differentiable
surrogate) in the context of the approximate infer-
ence and decoding algorithms. To determine the gra-
dient of `(??(xi),yi) with respect to ?, the method
relies on automatic differentiation in the reverse
mode (Griewank and Corliss, 1991), a general tech-
nique for sensitivity analysis in computations. The
intuition behind automatic differentiation is that the
entire computation is a sequence of elementary dif-
ferentiable operations. For each elementary opera-
tion, given that we know the input and result values,
and the partial derivative of the loss with respect to
the result, we can compute the partial derivative of
the loss with respect to the inputs to the step. Dif-
ferentiating the whole complicated computation can
be carried out in backward pass in this step-by-step
manner as long as we record intermediate results
during the computation of the function (the forward
pass). At the end, we accumulate the partials of the
loss with respect to each parameter ?i.
ERMA is similar to back-propagation used in re-
current neural networks, which involve cyclic up-
dates like those in belief propagation (Williams and
Zipser, 1989). It considers an ?unrolled? version of
the forward pass, in which ?snapshots? of a vari-
able at times t and t + 1 are treated as distinct vari-
ables, with one perhaps influencing the other. The
forward pass computes `(??(xi),yi) by performing
approximate inference, then decoding, then evalu-
ation. These steps convert (xi, ?) ? marginals ?
decision? loss. The backward pass rewinds the en-
tire computation, differentiating each phase in term.
The total time required by this algorithm is roughly
twice the time of the forward pass, so its complexity
is comparable to approximate inference.
In this paper, we do not advocate any particular
test-time inference or decoding procedures. It is rea-
sonable to experiment with several choices that may
produce faster or more accurate systems. We sim-
ply recommend doing ERMA training to match each
selected test-time condition. Stoyanov et al (2011)
specifically showed how to train a system that will
use sum-product BP for inference at test time (un-
like margin-based methods). This may be advanta-
geous for some tasks because it marginalizes over la-
tent variables. However, it is popular and sometimes
faster to do 1-best decoding, so we also include ex-
periments where the test-time system returns a 1-
best value of y (or an approximation to this if the
CRF is loopy), based on max-product BP inference.
Although 1-best systems are not differentiable func-
tions, we can approach their behavior during ERM
training by annealing the training objective (Smith
and Eisner, 2006). In the annealed case we evaluate
(4) and its gradient under sum-product BP, except
that we perform inference under p(?/T ) instead of p?.
123
We gradually reduce the temperature T ? R from 1
to 0 as training proceeds, which turns sum-product
inference into max-product by moving all the prob-
ability mass toward the highest-scoring assignment.
4 Modeling Natural Language with CRFs
This section describes three NLP problems that can
be naturally modeled with approximate CRFs. The
first problem, modeling congressional votes, has not
been previously modeled with a CRF. We show that
by switching to the principled CRF framework we
can learn models that are much more accurate when
evaluated on test data, though using the same (or less
expressive) features as previous work. The other
two problems, information extraction from semi-
structured text and collective multi-label classifica-
tion, have been modeled with loopy CRFs before.
For all three models, we show that ERMA training
results in better test set performance.3
4.1 Modeling Congressional Votes
The Congressional Vote (ConVote) corpus was cre-
ated by Thomas et al (2006) to study whether votes
of U.S. congressional representatives can be pre-
dicted from the speeches they gave when debating
a bill. The corpus consists of transcripts of con-
gressional floor debates split into speech segments.
Each speech segment is labeled with the represen-
tative who is speaking and the recorded vote of that
representative on the bill. We aim to predict a high
percentage of the recorded votes correctly.
Speakers often reference one another (e.g., ?I
thank the gentleman from Utah?), to indicate agree-
ment or disagreement. The ConVote corpus manu-
ally annotates each phrase such as ?the gentleman
from Utah? with the representative that it denotes.
Thomas et al (2006) show that classification us-
ing the agreement/disagreement information in the
local context of such references, together with the
rest of the language in the speeches, can lead to sig-
nificant improvement over using either of these two
3We also experimented with a fourth application, joint POS
tagging and shallow parsing (Sutton et al, 2007) and observed
the same overall trend (i.e., minimum risk training improved
performance significantly). We do not include those experi-
ments, however, because we were unable to make our baseline
results replicate (Sutton et al, 2007).
sources of information in isolation. The original ap-
proach of Thomas et al (2006) is based on training
two Support Vector Machine (SVM) classifiers?
one for classifying speeches as supporting/opposing
the legislation and another for classifying references
as agreement/disagreement. Both classifiers rely on
bag-of-word (unigram) features of the document and
the context surrounding the link respectively. The
scores produced by the two SVMs are used to weight
a global graph whose vertices are the representa-
tives; then the min-cut algorithm is applied to par-
tition the vertices into ?yea? and ?nay? voters.
While the approach of Thomas et al (2006)
leads to significant improvement over using the first
SVM alone, it does not admit a probabilistic in-
terpretation and the two classifiers are not trained
jointly. We also remark that the min-cut technique
would not generalize beyond binary random vari-
ables (yea/nay).
We observe that congressional votes together with
references between speakers can be naturally mod-
eled with a CRF. Figure 1 depicts the CRF con-
structed for one of the debates in the development
part of the ConVote corpus. It contains a random
variable for each representative?s vote. In addition,
each speech is an observed input random variable:
it is connected by a factor to its speaker?s vote and
encourages it to be ?yea? or ?nay? according to fea-
tures of the text of the speech. Finally, each ref-
erence in each speech is an observed input random
variable connected by a factor to two votes?those
of the speaker and the referent?which it encourages
to agree or disagree according to features of the text
surrounding the reference. Just as in (Thomas et al,
2006), the score of a global assignment to all votes is
defined by considering both kinds of factors. How-
ever, unlike min-cut, CRF inference finds a proba-
bility distribution over assignments, not just a sin-
gle best assignment. This fact allows us to train the
two kinds of factors jointly (on the set of training
debates where the votes are known) to predict the
correct votes accurately (as defined by accuracy).
As Figure 1 shows, the reference factors introduce
arbitrary loops, making exact inference intractable
and thus motivating ERMA. Our experiments de-
scribed in section 5.2 show that switching to a CRF
model (keeping the same features) leads to a sizable
improvement over the previous state of the art?
124
Figure 1: An example of a debate structure from the Con-
Vote corpus. Each black square node represents a factor
and is connected to the variables in that factor, shown
as round nodes. Unshaded variables correspond to the
representatives? votes and depict the output variables that
we learn to jointly predict. Shaded variables correspond
to the observed input data? the text of all speeches of a
representative (in dark gray) or all local contexts of refer-
ences between two representatives (in light gray).
and that ERMA further significantly improves per-
formance, particularly when it properly trains with
the same inference algorithm (max-product vs. sum-
product) to be used at test time.
Baseline. As an exact baseline, we compare
against the results of Thomas et al (2006). Their
test-time Min-Cut algorithm is exact in this case: bi-
nary variables and a two-way classification.
4.2 Information Extraction from
Semi-Structured Text
We utilize the CMU seminar announcement corpus
of Freitag (2000) consisting of emails with seminar
announcements. The task is to extract four fields that
describe each seminar: speaker, location, start time
and end time. The corpus annotates the document
with all mentions of these four fields.
Sequential CRFs have been used successfully for
semi-structured information extraction (Sutton and
McCallum, 2005; Finkel et al, 2005). However,
they cannot model non-local dependencies in the
data. For example, in the seminar announcements
corpus, if ?Sutner? is mentioned once in an email
in a context that identifies him as a speaker, it is
Sutner
S
Who:
O
Prof.
S
Klaus
S
will
O
Prof.
S
Sutner
S? ?? ?
Figure 2: Skip-chain CRF for semi-structured informa-
tion extraction.
likely that other occurrences of ?Sutner? in the same
email should be marked as speaker. Hence Finkel et
al. (2005) and Sutton and McCallum (2005) propose
adding non-local edges to a sequential CRF to repre-
sent soft consistency constraints. The model, called
a ?skip-chain CRF? and shown in Figure 2, contains
a factor linking each pair of capitalized words with
the same lexical form. The skip-chain CRF model
exhibits better empirical performance than its se-
quential counterpart (Sutton and McCallum, 2005;
Finkel et al, 2005).
The non-local skip links make exact inference
intractable. To train the full model, Finkel et al
(2005) estimate the parameters of a sequential CRF
and then manually select values for the weights of
the non-local edges. At test time, they use Gibbs
sampling to perform inference. Sutton and McCal-
lum (2005) use max-product loopy belief propaga-
tion for test-time inference, and compare a train-
ing procedure that uses a piecewise approximation
of the partition function against using sum-product
loopy belief propagation to compute output variable
marginals. They find that the two training regimens
perform similarly on the overall task. All of these
training procedures try to approximately maximize
conditional likelihood, whereas we will aim to mini-
mize the empirical loss of the approximate inference
and decoding procedures.
Baseline. As an exact (non-loopy) baseline, we
train a model without the skip chains. We give two
baseline numbers in Table 1?for training the exact
CRF with MLE and with ERM. The ERM setting re-
sulted in a statistically significant improvement even
in the exact case, thanks to the use of the loss func-
tion at training time.
4.3 Multi-Label Classification
Multi-label classification is the problem of assign-
ing multiple labels to a document. For example, a
news article can be about both ?Libya? and ?civil
125
war.? The most straightforward approach to multi-
label classification employs a binary classifier for
each class separately. However, previous work has
shown that incorporating information about label de-
pendencies can lead to improvement in performance
(Elisseeff and Weston, 2001; Ghamrawi and McCal-
lum, 2005; Finley and Joachims, 2008).
For this task we follow Ghamrawi and McCallum
(2005) and Finley and Joachims (2008) and model
the label interactions by constructing a fully con-
nected CRF between the output labels. That is, for
every document, we construct a CRF that contains
a binary random variable for each label (indicating
that the corresponding label is on/off for the doc-
ument) and one binary edge for every unique pair
of labels. This architecture can represent dependen-
cies between labels, but leads to a setting in which
the output variables form one massive clique. The
resulting intractability of inference (and decoding)
motivates the use of ERMA training.
Baseline. We train a model without any of the
pairwise edges (i.e., a separate logistic regression
model for each class). We report the single best
baseline number, since MLE and ERM training re-
sulted in statistically indistinguishable results.
5 Experiments
5.1 Learning Methodology
For all experiments we split the data into
train/development/test sets using the standard splits
when available. We tune optimization algorithm pa-
rameters (initial learning rate, batch size and meta-
parameters ? and ? for stochastic meta descent) on
the training set based on training objective conver-
gence rates. We tune the regularization parameter
? (below) on development data when available, oth-
erwise we use a default value of 0.1?performance
was generally robust for small changes in the value
of ?. All statistical significance testing is performed
using paired permutation tests (Good, 2000).
Gradient-based Optimization. Gradient infor-
mation from the back-propagation procedure can be
used in a local optimization method to minimize em-
pirical loss. In this paper we use stochastic meta
descent (SMD) (Schraudolph, 1999). SMD is a
second-order method that requires vector-Hessian
products. For computing those, we do not need to
maintain the full Hessian matrix. Instead, we apply
more automatic differentiation magic?this time in
the forward mode. Computing the vector-Hessian
product and utilizing it in SMD does not add to the
asymptotic runtime, it requires about twice as many
arithmetic operations, and leads to much faster con-
vergence of the learner in our experience. See Stoy-
anov et al (2011) for details.
Since the empirical risk objective could overfit
the training data, we add an L2 regularizer ?
?
j ?
2
j
that prefers parameter values close to 0. This im-
proves generalization, like the margin constraints in
margin-based methods.
Training Procedure Stoyanov et al (2011) ob-
served that the minimum-risk objective tends to be
highly non-convex in practice. The usual approx-
imate log likelihood training objective appeared to
be smoother over the parameter space, but exhibited
global maxima at parameter values that were rela-
tively good, but sub-optimal for other loss functions.
Mean-squared error (MSE) also gave a smoother ob-
jective than other loss functions. These observations
motivated Stoyanov et al (2011) to use a contin-
uation method. They optimized approximate log-
likelihood for a few iterations to get to a good part of
the parameter space, then switched to using the hy-
brid loss function ?`(y, y?)+(1??)`MSE(y, y?). The
coefficient ? changed gradually from 0 to 1 during
training, which morphs from optimizing a smoother
loss to optimizing the desired bumpy test loss. We
follow the same procedure.
Experiments in this paper use two evaluation met-
rics: percentage accuracy and F-measure. For both
of these losses we decode by selecting the most
probable value under the marginal distribution of
each random variable. This is an exact MBR de-
code for accuracy but an approximate one for the
F-measure; our ERMA training will try to compen-
sate for this approximate decoder. This decoding
procedure is not differentiable due to the use of the
argmax function. To make the decoder differen-
tiable, we replace argmax with a stochastic (soft-
max) version during training, averaging loss over all
possible values v in proportion to their exponenti-
ated probability p(yi = v | x)1/Tdecode . This de-
coder loses smoothness and approaches an argmax
126
Problem Congressional Vote Semi-structured IE Multi-label class.
Loss function Accuracy Token-wise F-score F-score
Non-loopy Baseline 71.2 86.2 (87.1) 81.6
Loopy CRF models INFERENCE:
T
R
A
IN
IN
G
: maxprod sumprod maxprod sumprod maxprod sumprod
MLE 78.2 78.2 89.0 89.5 84.2 84.0
Softmax-margin 79.0 79.0 90.1 90.2 84.3 83.8
Min-risk (maxprod) 85.1 80.1 90.9 90.7 84.5 84.4
Min-risk (sumprod) 83.6 84.5 90.3 90.9 84.7 84.6
Table 1: Results. The top of the table lists the loss function used for each problem and the score for the best exact
baseline. The bottom lists results for the full models used with loopy BP. Models are tested with either sum-product
BP (sumprod) or max-product BP (maxprod) and trained with MLE or the minimum risk criterion. Min-risk training
runs are either annealed (maxprod), which matches max-product test, or not (sumprod), which matches sum-product
test; grey cells in the table indicate matched training and test settings. In each column, we boldface the best result as
well as all results that are not significantly worse (paired permutation test, p < 0.05).
decoder as Tdecode decreases toward 0. For simplic-
ity, our experiments just use a single fixed value of
0.1 for Tdecode. Annealing the decoder slowly did not
lead to significant differences in early experiments
on development data.
5.2 Results
Table 1 lists results of our evaluation. For all three
of our problems, using approximate CRFs results
in statistically significant improvement over the ex-
act baselines, for any of the training procedures.
But among the training procedures for approximate
CRFs, our ERMA procedure?minimizing empiri-
cal risk with the training setting matched to the test
setting?improves over the two baselines, namely
MLE and softmax-margin. MLE and softmax-
margin training were statistically indistinguishable
in our experiments with the exception of semi-
structured IE. ERMA?s improvements over them are
statistically significant at the p < .05 level for the
Congressional Vote and Semi-Structured IE prob-
lems and at the p < .1 level for the Multi-label clas-
sification problem (comparing each matched min-
risk setting shown in a gray cell in Table 1 vs. MLE).
When minimizing risk, we also observe that
matching training and test-time procedures can re-
sult in improved performance in one of the three
problems, Congressional Vote. For this problem, the
matched training condition performs better than the
alternatives (accuracy of 85.1 vs. 83.6 for the an-
nealed max-product testing and 84.5 vs 80.1 for the
sum-product setting), significant at p < .01). We
observe the same effect for semi-structured IE when
testing using max-product inference. For the other
remaining three problem setting training with either
minimal risk training regiment.
Finally, we hypothesized that sum-product infer-
ence may produce more accurate results in certain
cases as it allows more information about differ-
ent parts of the model to be exchanged. How-
ever, our results show that for these three problems,
sum-product and max-product inference yield statis-
tically indistinguishable results. This may be be-
cause the particular CRFs we used included no la-
tent variables (in constrast to the synthetic CRFs
in Stoyanov et al (2011)). As expected, we found
that max-product BP converges in fewer iterations?
sum-product BP required as many as twice the num-
ber of iterations for some of the runs.
Results in this paper represent a new state-of-the-
art for the first two of the problems, Congressional
Vote and Semi-structured IE. For Multi-Label classi-
fication, comparing against the SVM-based method
of Finley and Joachims (2008) goes beyond the
scope of this paper.
6 Related Work
Minimum-risk training has been used in speech
recognition (Bahl et al, 1988), machine translation
(Och, 2003), and energy-based models generally
(LeCun et al, 2006). In graphical models, methods
have been proposed to directly minimize loss in tree-
127
shaped or linear chain MRFs and CRFs (Kakade et
al., 2002; Suzuki et al, 2006; Gross et al, 2007).
All of the above focus on exact inference. Our
approach can be seen as generalizing these methods
to arbitrary graph structures, arbitrary loss functions
and approximate inference.
Lacoste-Julien et al (2011) also consider the ef-
fects of approximate inference on loss. However,
they assume the parameters are given, and modify
the approximate inference algorithm at test time to
consider the loss function.
Using empirical risk minimization to train graph-
ical models was independently proposed by Domke
(2010; 2011). Just as in our own paper (Stoy-
anov et al, 2011), Domke took a decision-theoretic
stance and proposed ERM as a way of calibrating
the graphical model for use with approximate infer-
ence, or for use with data that do not quite match the
modeling assumptions.4
In particular, (Domke, 2011) is similar to (Stoy-
anov et al, 2011) in using ERMA to train model pa-
rameters to be used with ?truncated? inference that
will be run for only a fixed number of iterations. For
a common pixel-labeling benchmark in computer vi-
sion, Domke (2011) shows that this procedure im-
proves training time by orders of magnitude, and
slightly improves accuracy if the same number of
message-passing iterations is used at test time.
Stoyanov and Eisner (2011) extend the ERMA
objective function by adding an explicit runtime
term. This allows them to tune model parameters
and stopping criteria to learn models that obtain a
given speed-accuracy tradeoff. Their approach im-
proves this hybrid objective over a range of coeffi-
cients when compared to the traditional way of in-
ducing sparse structures through L1 regularization.
Eisner and Daume? III (2011) propose the same lin-
ear combination of speed and accuracy as a rein-
forcement learning objective. In general, our pro-
posed ERMA setting resembles the reinforcement
learning problem of trying to directly learn a policy
that minimizes loss or maximizes reward.
We have been concerned with the fact that ERMA
training objectives may suffer from local optima and
non-differentiability. Stoyanov et al (2011) studied
4However, he is less focused than we are on matching train-
ing conditions to test conditions (by including the decoder and
task loss in the ERMA objective).
several such settings, graphed the difficult objective,
and identified some practical workarounds that are
used in the present paper. Although these methods
have enabled us to get strong results by reducing the
empirical risk, we suspect that ERMA training ob-
jectives will benefit from more sophisticated opti-
mization methods. This is true even when the ap-
proximate inference itself is restricted to be some-
thing as simple as a convex minimization. While
that simplified setting can make it slightly more con-
venient to compute the gradient of the inference re-
sult with respect to the parameters (Domke, 2008;
Domke, 2012), there is still no guarantee that follow-
ing that gradient will minimize the empirical risk.
Convex inference does not imply convex training.
7 Conclusions
Motivated by the recently proposed method of Stoy-
anov et al (2011) for minimum-risk training of
CRF-based systems, we revisited three NLP do-
mains that can naturally be modeled with approx-
imate CRF-based systems. These include appli-
cations that have not been modeled with CRFs
before (the ConVote corpus), as well as applica-
tions that have been modeled with loopy CRFs
trained to minimize the approximate log-likelihood
(semi-structured information extraction and collec-
tive multi-label classification). We show that (i)
the NLP models are improved by moving to richer
CRFs that require approximate inference, and (ii)
empirical performance is always significantly im-
proved by training to reduce the loss that would be
achieved by approximate inference, even compared
to another state-of-the-art training method (softmax-
margin) that also considers loss and uses approxi-
mate inference. The general software package that
implements the algorithms in this paper is avail-
able at http://www.clsp.jhu.edu/?ves/
software.html.
Acknowledgments
This material is based upon work supported by the
National Science Foundation under Grant #0937060
to the Computing Research Association for the
CIFellows Project.
128
References
L. Bahl, P. Brown, P. de Souza, and R. Mercer. 1988.
A new algorithm for the estimation of hidden Markov
model parameters. In Proceedings of ICASSP, pages
493?496.
E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds. In Proceedings of
ACL-HLT, pages 389?398.
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005.
Identifying sources of opinions with conditional ran-
dom fields and extraction patterns. In Proceedings of
HLT/EMNLP, pages 355?362.
J. Domke. 2008. Learning convex inference of
marginals. In Proceedings of UAI.
J. Domke. 2010. Implicit differentiation by perturba-
tion. In Advances in Neural Information Processing
Systems, pages 523?531.
J. Domke. 2011. Parameter learning with truncated
message-passing. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR).
J. Domke. 2012. Generic methods for optimization-
based modeling. In Proceedings of AISTATS.
M. Dreyer and J. Eisner. 2009. Graphical models over
multiple strings. In Proceedings of EMNLP, pages
101?110.
J. Eisner and Hal Daume? III. 2011. Learning speed-
accuracy tradeoffs in nondeterministic inference al-
gorithms. In COST: NIPS 2011 Workshop on Com-
putational Trade-offs in Statistical Learning, Sierra
Nevada, Spain, December.
A. Elisseeff and J. Weston. 2001. Kernel methods for
multi-labelled classification and categorical regression
problems. In Advances in Neural Information Pro-
cessing Systems, pages 681?687.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In Proceedings of
ACL, pages 363?370.
T. Finley and T. Joachims. 2008. Training structural
SVMs when exact inference is intractable. In Proceed-
ings of ICML, pages 304?311.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine learning,
39(2).
N. Ghamrawi and A. McCallum. 2005. Collective multi-
label classification. In Proceedings of CIKM, pages
195?200.
K. Gimpel and N.A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost functions.
In Proceedings of ACL, pages 733?736.
P. I. Good. 2000. Permutation Tests. Springer.
A. Griewank and G. Corliss, editors. 1991. Automatic
Differentiation of Algorithms. SIAM, Philadelphia.
S. Gross, O. Russakovsky, C. Do, and S. Batzoglou.
2007. Training conditional random fields for maxi-
mum labelwise accuracy. Advances in Neural Infor-
mation Processing Systems, 19:529.
H. Ji and R. Grishman. 2011. Knowledge base popula-
tion: Successful approaches and challenges. In Pro-
ceedings of ACL-HLT, pages 1148?1158.
S. Kakade, Y.W. Teh, and S. Roweis. 2002. An alternate
objective function for Markovian fields. In Proceed-
ings of ICML, pages 275?282.
D. Koller and N. Friedman. 2009. Probabilistic Graph-
ical Models: Principles and Techniques. The MIT
Press.
A. Kulesza and F. Pereira. 2008. Structured learning
with approximate inference. In Advances in Neural
Information Processing Systems, pages 785?792.
S. Lacoste-Julien, F. Huszr, and Z. Ghahramani.
2011. Approximate inference for the loss-calibrated
Bayesian. In Proceedings of AISTATS.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML, pages 282?289.
Y. LeCun, S. Chopra, R. Hadsell, M.A. Ranzato, and F.-
J. Huang. 2006. A tutorial on energy-based learning.
In G. Bakir, T. Hofman, B. Schlkopf, A. Smola, and
B. Taskar, editors, Predicting Structured Data. MIT
Press.
Z. Li and J. Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
EMNLP, pages 40?51.
K. P. Murphy, Y. Weiss, and M. I. Jordan. 1999. Loopy
belief propagation for approximate inference: An em-
pirical study. In Proceedings of UAI.
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL, pages
160?167.
F. Peng and A. McCallum. 2006. Information extraction
from research papers using conditional random fields.
Information Processing & Management, 42(4):963?
979.
N.N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. In Proceedings of ANN,
pages 569?574.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of ACL/HLT,
pages 134?141.
D.A. Smith and J. Eisner. 2006. Minimum risk annealing
for training log-linear models. In Proceedings of the
COLING/ACL, pages 787?794.
129
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proceedings of EMNLP, pages
145?156.
V. Stoyanov and J. Eisner. 2011. Learning cost-aware,
loss-aware approximate inference policies for proba-
bilistic graphical models. In COST: NIPS 2011 Work-
shop on Computational Trade-offs in Statistical Learn-
ing, Sierra Nevada, Spain, December.
V. Stoyanov, A. Ropson, and J. Eisner. 2011. Empirical
risk minimization of graphical model parameters given
approximate inference, decoding, and model structure.
In Proceedings of AISTATS.
C. Sutton and A. McCallum. 2005. Piecewise training
of undirected models. In Proceedings of UAI, pages
568?575.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized proba-
bilistic models for labeling and segmenting sequence
data. The Journal of Machine Learning Research,
8:693?723.
J. Suzuki, E. McDermott, and H. Isozaki. 2006. Train-
ing conditional random fields with multivariate eval-
uation measures. In Proceedings of COLING/ACL,
pages 217?224.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. Proceedings of NIPS, pages 25?32.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote:
Determining support or opposition from congressional
floor-debate transcripts. In Proceedings of EMNLP,
pages 327?335.
S. Vishwanathan, N. Schraudolph, M. Schmidt, and
K. Murphy. 2006. Accelerated training of conditional
random fields with stochastic gradient methods. In
Proceedings of ICML, pages 969?976.
M. Wainwright. 2006. Estimating the ?wrong? graphi-
cal model: Benefits in the computation-limited setting.
Journal of Machine Learning Research, 7:1829?1859,
September.
R.J. Williams and D. Zipser. 1989. A learning algo-
rithm for continually running fully recurrent neural
networks. Neural Computation, 1(2):270?280.
130
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 131?141,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Learning on an Approximate Corpus?
Jason Smith and Jason Eisner
Center for Language and Speech Processing
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218, USA
{jsmith,jason}@cs.jhu.edu
Unsupervised learning techniques can take advan-
tage of large amounts of unannotated text, but the
largest text corpus (the Web) is not easy to use in
its full form. Instead, we have statistics about this
corpus in the form of n-gram counts (Brants and
Franz, 2006). While n-gram counts do not directly
provide sentences, a distribution over sentences can
be estimated from them in the same way that n-
gram language models are estimated. We treat this
distribution over sentences as an approximate cor-
pus and show how unsupervised learning can be
performed on such a corpus using variational infer-
ence. We compare hidden Markov model (HMM)
training on exact and approximate corpora of vari-
ous sizes, measuring speed and accuracy on unsu-
pervised part-of-speech tagging.
1 Introduction
We consider the problem of training generative mod-
els on very large datasets in sublinear time. It is well
known how to train an HMM to maximize the like-
lihood of a corpus of sentences. Here we show how
to train faster on a distribution over sentences that
compactly approximates the corpus. The distribu-
tion is given by an 5-gram backoff language model
that has been estimated from statistics of the corpus.
In this paper, we demonstrate our approach on
a traditional testbed for new structured-prediction
learning algorithms, namely HMMs. We focus on
unsupervised learning. This serves to elucidate the
structure of our variational training approach, which
stitches overlapping n-grams together rather than
treating them in isolation. It also confirms that at
least in this case, accuracy is not harmed by the
key approximations made by our method. In future,
we hope to scale up to the Google n-gram corpus
(Brants and Franz, 2006) and learn a more detailed,
explanatory joint model of tags, syntactic dependen-
cies, and topics. Our intuition here is that web-scale
data may be needed to learn the large number of lex-
ically and contextually specific parameters.
?Work was supported in part by NSF grant No. 0347822.
1.1 Formulation
Let w (?words?) denote an observation sequence,
and let t (?tags?) denote a hidden HMM state se-
quence that may explain w. This terminology is
taken from the literature on inducing part-of-speech
(POS) taggers using a first-order HMM (Merialdo,
1994), which we use as our experimental setting.
Maximum a posteriori (MAP) training of an
HMM p? seeks parameters ? to maximize
N ?
?
w
c(w) log
?
t
p?(w, t) + log Pr prior(?) (1)
where c is an empirical distribution that assigns
probability 1/N to each of the N sentences in a
training corpus. Our technical challenge is to gen-
eralize this MAP criterion to other, structured dis-
tributions c that compactly approximate the corpus.
Specifically, we address the case where c is given
by any probabilistic FSA, such as a backoff lan-
guage model?that is, a variable-order Markov
model estimated from corpus statistics. Similar sen-
tences w share subpaths in the FSA and cannot eas-
ily be disentangled. The support of c is typically infi-
nite (for a cyclic FSA) or at least exponential. Hence
it is no longer practical to compute the tagging distri-
bution p(t | w) for each sentence w separately, as in
traditional MAP-EM or gradient ascent approaches.
We will maximize our exact objective, or a cheaper
variational approximation to it, in a way that cru-
cially allows us to retain the structure-sharing.
1.2 Motivations
Why train from a distribution rather than a corpus?
First, the foundation of statistical NLP is distribu-
tions over strings that are specified by weighted au-
tomata and grammars. We regard parameter estima-
tion from such a distribution c (rather than from a
sample) as a natural question. Previous work on
modeling c with a distribution from another fam-
ily was motivated by approximating a grammar or
131
model rather than generalizing from a dataset, and
hence removed latent variables while adding param-
eters (Nederhof, 2000; Mohri and Nederhof, 2001;
Liang et al, 2008), whereas we do the reverse.
Second, in practice, one may want to incorporate
massive amounts of (possibly out-of-domain) data
in order to get better coverage of phenomena. Mas-
sive datasets usually require a simple model (given a
time budget). We propose that it may be possible to
use a lot of data and a good model by reducing the
accuracy of the data representation instead. While
training will become more complicated, it can still
result in an overall speedup, because a frequent 5-
gram collapses into a single parameter of the esti-
mated distribution that only needs to be processed
once per training iteration. By pruning low-count
n-grams or reducing the maximum n below 5, one
can further increase data volume for the fixed time
budget at the expense of approximation quality.
Third, one may not have access to the original
corpus. If one lacks the resources to harvest the
web, the Google n-gram corpus was derived from
over a trillion words of English web text. Privacy
or copyright issues may prevent access, but one may
still be able to work with n-gram statistics: Michel
et al (2010) used such statistics from 5 million
scanned books. Several systems use n-gram counts
(Bergsma et al, 2009; Lin et al, 2009) or other
web statistics (Lapata and Keller, 2005) as features
within a classifier. A large language model from n-
gram counts yields an effective prior over hypothe-
ses in tasks like machine translation (Brants et al,
2007). We similarly construct an n-gram model, but
treat it as the primary training data whose structure
is to be explained by the generative HMM. Thus our
criterion does not explain the n-grams in isolation,
but rather tries to explain the likely full sentences
w that the model reconstructed from overlapping n-
grams. This is something like shotgun sequencing,
in which likely DNA strings are reconstructed from
overlapping short reads (Staden, 1979); however, we
train an HMM on the resulting distribution rather
than merely trying to find its mode.
Finally, unsupervised HMM training discovers la-
tent structure by approximating an empirical distri-
bution c (the corpus) with a latent-variable distribu-
tion p (the trained HMM) that has fewer parameters.
We show how to do the same where the distribution
c is not a corpus but a finite-state distribution. In
general, this finite-state c could represent some so-
phisticated estimate of the population distribution,
using shrinkage, word classes, neural-net predictors,
etc. to generalize in some way beyond the training
sample before fitting p. For the sake of speed and
clear comparison, however, our present experiments
take c to be a compact approximation to the sample
distribution, requiring only n-grams.
Spectral learning of HMMs (Hsu et al, 2009)
also learns from a collection of n-grams. It has the
striking advantage of converging globally to the true
HMM parameters (under a certain reparameteriza-
tion), with enough data and under certain assump-
tions. However, it does not exploit context beyond
a trigram (it will not maximize, even locally, the
likelihood of a finite sample of sentences), and can-
not exploit priors or structure?e.g., that the emis-
sions are consistent with a tag dictionary or that the
transitions encode a higher-order or factorial HMM.
Our more general technique extends to other latent-
variable models, although it suffers from variational
EM?s usual local optima and approximation errors.
2 A variational lower bound
Our starting point is the variational EM algorithm
(Jordan et al, 1999). Recall that this maximizes a
lower bound on the MAP criterion of equation 1, by
bounding the log-likelihood subterm as follows:
log
?
t p?(w, t) (2)
= log
?
t q(t)(p?(w, t)/q(t))
?
?
t q(t) log(p?(w, t)/q(t))
= Eq(t)[log p?(w, t)? log q(t)] (3)
This use of Jensen?s inequality is valid for any distri-
bution q. As Neal and Hinton (1998) show, the EM
algorithm (Dempster et al, 1977) can be regarded
as locally maximizing the resulting lower bound by
alternating optimization, where q is a free parame-
ter. The E-step optimizes q for fixed ?, and the M-
step optimizes ? for fixed q. These computations are
tractable for HMMs, since the distribution q(t) =
p?(t | w) that is optimal at the E-step (which makes
the inequality tight) can be represented as a lattice
(a certain kind of weighted DFA), and this makes
the M-step tractable via the forward-backward algo-
rithm. However, there are many extensions such as
132
factorial HMMs and Bayesian HMMs in which an
expectation under p?(t | w) involves an intractable
sum. In this setting, one may use variational EM, in
which q is restricted to some parametric family q?
that will permit a tractable M-step. In this case the
E-step chooses the optimal values of the variational
parameters ?; the inequality is no longer tight.
There are two equivalent views of how this pro-
cedure is applied to a training corpus. One view is
that the corpus log-likelihood is just as in (2), where
w is taken to be the concatenation of all training
sentences. The other view is that the corpus log-
likelihood is a sum over many terms of the form (2),
one for each training sentence w, and we bound each
summand individually using a different q?.
However, neither view leads to a practical imple-
mentation in our setting. We can neither concatenate
all the relevant w nor loop over them, since we want
the expectation of (2) under some distribution c(w)
such that {w : c(w) > 0} is very large or infinite.
Our move is to make q be a conditional distribution
q(t | w) that applies to all w at once. The follow-
ing holds by applying Jensen?s inequality separately
to each w in the expectation (this is valid since for
each w, q(t | w) is a distribution):
Ec(w) log
?
t p?(w, t) (4)
= Ec(w) log
?
t q(t | w)(p?(w, t)/q(t | w))
? Ec(w)
?
t q(t | w) log(p?(w, t)/q(t | w))
= Ecq(w,t)[log p?(w, t)? log q(t | w)] (5)
where we use cq(w, t) to denote the joint distribu-
tion c(w) ? q(t | w). Thus, just as c is our approx-
imate corpus, cq is our approximate tagged corpus.
Our variational parameters ? will be used to param-
eterize cq directly. To ensure that cq? can indeed
be expressed as c(w) ? q(t | w), making the above
bound valid, it suffices to guarantee that our varia-
tional family preserves the marginals:
(?w)
?
t cq?(w, t) = c(w)
3 Finite-state encodings and algorithms
In the following, we will show how to maximize
(5) for particular families of p, c, and cq that can
be expressed using finite-state machines (FSMs)?
that is, finite-state acceptors (FSAs) and transducers
(FSTs). This general presentation of our method en-
ables variations using other FSMs.
A path in an FSA accepts a string. In an FST,
each arc is labeled with a ?word : tag? pair, so that a
path accepts a string pair (w, t) obtained by respec-
tively concatenating the words and the tags encoun-
tered along the path. Our FSMs are weighted in the
(+,?) semiring: the weight of any path is the prod-
uct (?) of its arc weights, while the weight assigned
to a string or string pair is the total weight (+) of all
its accepting paths. An FSM is unambiguous if each
string or string pair has at most one accepting path.
Figure 1 reviews how to represent an HMM POS
tagger as an FST (b), and how composing this with
an FSA that accepts a single sentence gives us the
familiar HMM tagging lattice as an FST (c). The
forward-backward algorithm sums over paths in the
lattice via dynamic programming (Rabiner, 1989).
In section 3.1, we replace the straight-line FSA
of Figure 1a with an FSA that defines a more gen-
eral distribution c(w) over many sentences. Note
that we cannot simply use this as a drop-in replace-
ment in the construction of Figure 1. That would
correspond to running EM on a single but uncer-
tain sentence (distributed as c(w)) rather than a col-
lection of observed sentences. For example, in the
case of an ordinary training corpus of N sentences,
the new FSA would be a parallel union (sum) of
N straight-line paths?rather than a serial concate-
nation (product) of those paths as in ordinary EM
(see above). Running the forward algorithm on the
resulting lattice would compute Ec(w)
?
t p(w, t),
whose log is logEc(w)
?
t p(w, t) rather than our
desired Ec(w) log
?
t p(w, t). Instead, we use c in
section 3.2 to construct a variational family cq?. We
then show in sections 3.3?3.5 how to compute and
locally maximize the variational lower bound (5).
3.1 Modeling a corpus with n-gram counts
n-gram backoff language models have been used for
decades in automatic speech recognition and statis-
tical machine translation. We follow the usual FSA
construction (Allauzen et al, 2003). The state of a 5-
gram FSA model c(w) must remember the previous
4-gram. For example, it would include an arc from
state defg (the previous 4-gram) to state efgh with
label h and weight c(h | defg). Then, with appro-
priate handling of boundary conditions, a sentence
w = . . . defghi . . . is accepted along a single path of
weight c(w) = ? ? ? c(h | defg) ? c(i | efgh) ? ? ? . Arcs
133
(a) w Time flies like an arrow
(b) p(w,t) Start Vw:V
StopNw:N
DT
w:DTw:V
w:V
(c) w o p(w,t) Start VTime : V NTime : N
Vflies : V
N
flies : N
flies : V
flies : N
Prep
like : Prep
V
like : V
like : Prep
like : V
DTan : DTan : DT Narrow : N
Figure 1: Ordinary HMM tagging with finite-state machines. An arc?s label may have up to three components:
?word:tag / weight.? (Weights are suppressed for space. State labels are not part of the machine but suggest the history
recorded by each state.) (a) w is an FSA that generates the sentence ?Time flies like an arrow?; all arcs have probability
1. (b) p(w, t) is an FST representing an HMM (many arcs are not shown and words are abbreviated as ?w?). Each arc
w : t is weighted by the product of transition and emission probabilities, p(t | previous t) ? p(w | t). Composing (a)
with (b) yields (c), an FST that encodes the joint probabilities p(w, t) of all possible taggings of the sentence w.
of weight 0 can be omitted from the FSA.1
To estimate a conditional probability like c(h |
defg) above, we simply take an unsmoothed ratio of
two n-gram counts. This ML estimation means that
c will approximate as closely as possible the train-
ing sample from which the counts were drawn. That
gives a fair comparison with ordinary EM, which
trains directly on that sample. (See discussion at the
end of section 1.2 for alternatives.)
Yet we decline to construct a full 5-gram model,
which would not be as compact as desired. A col-
lection of all web 5-grams would be nearly as large
as the web itself (by Zipf?s Law). We may not have
such a collection. For example, the Google n-gram
corpus version 2 contains counts only for 1-grams
that appear at least 40 times and 2-, 3-, 4-, and 5-
grams that appear at least 10 times (Lin et al, 2009).
1The FSA?s initial state is the unigram history #, and its final
states (which have no outgoing arcs) are the other states whose
n-gram labels end in #. Here # is a boundary symbol that falls
between sentences. To compute the weighted transitions, sen-
tence boundaries must be manually or automatically annotated,
either on the training corpus as in our present experiments, or
directly on the training n-grams if we have only those.
To automatically find boundaries in an n-gram collection,
one could apply a local classifier to each n-gram. But in princi-
ple, one could exploit more context and get a globally consistent
annotation by stitching the n-grams together and applying the
methods of this paper?replacing p? with an existing CRF sen-
tence boundary detector, replacing c with a document-level (not
sentence-level) language model, and optimizing cq? to be a ver-
sion of c that is probabilistically annotated with sentence bound-
aries, which yields our desired distribution over sentences.
Instead, we construct a backoff language model.
This FSA has one arc for each n-gram in the col-
lection. Our algorithm?s runtime (per iteration) will
be linear in the number of arcs. If the 5-gram defgh
is not in our collection, then there can be no h arc
leaving defg. When encountering h in state defg, the
automaton will instead take a failure arc (Allauzen
et al, 2003) to the ?backoff state? efg. It may be
able to consume the h from that state, on an arc with
weight c(h | efg); or it may have to back off further
to fg. Each state?s failure arc is weighted such that
the state?s outgoing arcs sum to 1. It is labeled with
the special symbol ?, which does not contribute to
the word string accepted along a path.
We take care never to allow backoff to the empty
state ,2 since we find that c(w) is otherwise too
coarse an approximation to English: sampled sen-
tences tend to be disjointed, with some words gener-
ated in complete ignorance of their left context.
3.2 The variational distribution cq(w, t)
The ?variational gap? between (4) and (5) is
Ec(w)KL(q(t | w) || p?(t | w)). That is, the bound
is good if q does a good job of approximating p??s
tagging distribution on a randomly drawn sentence.
Note that n?1 is the order of our n-gram Markov
2To prevent such backoff, it suffices to include all 2-grams
with count > 0. But where the full collection of 2-grams is
unavailable or too large, one can remove the empty state (and
recursively remove all states that transition only to removed
states), and then renormalize the model locally or globally.
134
model c(w) (i.e., each word is chosen given the pre-
vious n ? 1 words). Let np ? 1 be the order of the
HMM p?(w, t) that we are training: i.e., each tag is
chosen given the previous np ? 1 tags. Our experi-
ments take np = 2 (a bigram HMM) as in Figure 1.
We will take q?(t | w) to be a conditional Markov
model of order nq ? 1.3 It will predict the tag at po-
sition i using a multinomial conditioned on the pre-
ceding nq?1 tags and on the word n-gram ending at
position i (where n is as large as possible such that
this n-gram is in our training collection). ? is the
collection of all multinomial parameters.
If nq = np, then our variational gap can be made 0
as in ordinary non-variational EM (see section 3.5).
In our experiments, however, we save memory by
choosing nq = 1. Thus, our variational gap is tight
to the extent that a word?s POS tag under the model
p? is conditionally independent of previous tags and
the rest of the sentence, given an n-word window.4
This is the assumption made by local classification
models (Punyakanok et al, 2005; Toutanova and
Johnson, 2007). Note that it is milder than the ?one
tagging per n-gram? hypothesis (Dawborn and Cur-
ran, 2009; Lin et al, 2009), which claims that each
5-gram (and therefore each sentence!) is unambigu-
ous as to its full tagging. In contrast, we allow that
a tag may be ambiguous even given an n-word win-
dow; we merely suppose that there is no further dis-
ambiguating information accessible to p?.5
We can encode the resulting cq(w, t) as an FST.
With nq = 1, the states of cq are isomorphic to the
states of c. However, an arc in c from defg with
label h and weight 0.2 is replaced in cq by several
arcs?one per tag t?with label h : t and weight
0.2 ? q?(t | defgh).6 We remark that an encoding of
3A conditional Markov model is a simple case of a
maximum-entropy Markov model (McCallum et al, 2000).
4At present, the word being tagged is the last word in the
window. We do have an efficient modification in which the win-
dow is centered on the word, by using an FST cq that delays the
emission of a tag until up to 2 subsequent words have been seen.
5With difficulty, one can construct English examples that
violate our assumption. (1) ?Some monitor lizards from
Africa . . . ? versus ?Some monitor lizards from a distance . . . ?:
there are words far away from ?monitor? that help disambiguate
whether ?monitor? is a noun or a verb. (?Monitor lizards? are
a species, but some people like to monitor lizards.) (2) ?Time
flies?: ?flies? is more likely to be a noun if ?time? is a verb.
6In the case nq > 1, the states of c would need to be split
in order to remember nq ? 1 tags of history. For example, if
q(t | w) as an FST would be identical except for
dropping the c factor (e.g., 0.2) from each weight.
Composing c ? q would then recover cq.
This construction associates one variational pa-
rameter in ? with each arc in cq?that is, with each
pair (arc in c, tag t), if nq = 1. There would be lit-
tle point in sharing these parameters across arcs of
cq, as that would reduce the expressiveness of the
variational distribution without reducing runtime.7
Notice that maximizing equation (5) jointly learns
not only a compact slow HMM tagger p?, but also a
large fast tagger q? that simply memorizes the likely
tags in each n-gram context. This is reminiscent of
structure compilation (Liang et al, 2008).
3.3 Computing the variational objective
The expectation in equation (5) can now be com-
puted efficiently and elegantly by dynamic program-
ming over the FSMs, for a given ? and ?.
We exploit our representation of cq? as an FSM
over the (+,?) semiring. The path weights repre-
sent a probability distribution over the paths. In gen-
eral, it is efficient to compute the expected value of
a random FSM path, for any definition of value that
decomposes additively over the path?s arcs. The ap-
proach is to apply the forward algorithm to a version
of cq? where we now regard each arc as weighted
by an ordered pair of real numbers. The (+,?) op-
erations for combining weights (section 3) are re-
placed with the operations of an ?expectation semir-
ing? whose elements are such pairs (Eisner, 2002).
Suppose we want to find Ecq?(w,t) log q?(t | w).
To reduce this to an expected value problem, we
must assign a value to each arc of cq? such that the
c is Figure 1a, splitting its states with nq = 2 would yield a
cq with a topology like Figure 1c, but with each arc having an
independent variational parameter.
7One could increase the number of arcs and hence varia-
tional parameters by splitting the states of cq to remember more
history. In particular, one could increase the width nq of the tag
window, or one could increase the width of the word window by
splitting states of c (without changing the distribution c(w)).
Conversely, one could reduce the number of variational pa-
rameters by further restricting the variational family. For exam-
ple, requiring q(t | w) to have entropy 0 (analogous to ?hard
EM? or ?Viterbi EM?) would associate a single deterministic
tag with each arc of c. This is fast, makes cq as compact as c,
and is still milder than ?one tagging per n-gram.? More gener-
ously, one could allow up to 2 tags per arc of c, or use a low-
dimensional representation of the arc?s distribution over tags.
135
total value of a path accepting (w, t) is log q?(t |
w). Thus, let the value of each arc in cq? be the log
of its weight in the isomorphic FST q?(t | w).8
We introduce some notation to make this precise.
A state of cq? is a pair of the form [hc, hq], where hc
is a state of c (e.g., an (n? 1)-word history) and hq
is an (nq ? 1)-tag history. We saw in the previous
section that an arc a leaving this state, and labeled
with w : t where w is a word and t is a tag, will
have a weight of the form ka
def
= c(w | hc)?a where
?a
def
= q?(t | hcw, hq). We now let the value va
def
=
log ?a.9 Then, just as the weight of a path accepting
(w, t) is
?
a ka = cq?(w, t), the value of that path
is
?
a va = log q?(t | w), as desired.
To compute the expected value r? over all paths,
we follow a generalized forward-backward recipe
(Li and Eisner, 2009, section 4.2). First, run the for-
ward and backward algorithms over cq?.10 Now the
expected value is a sum over all arcs of cq?, namely
r? =
?
a ?akava?a, where ?a denotes the forward
probability of arc a?s source state and ?a denotes
the backward probability of arc a?s target state.
Now, in fact, the expectation we need to compute
is not Ecq?(w,t) log q?(t | w) but rather equation (5).
So the value va of arc a should not actually be
log ?a but rather log ?a ? log ?a where ?a
def
= p?(t |
8The total value is then the sum of the logs, i.e., the log
of the product. This works because q? is unambiguous, i.e., it
computes q?(t | w) as a product along a single accepting path,
rather than summing over multiple paths.
9The special case of a failure arc a goes from [hc, hq] to
[h?c, hq], where h
?
c is a backed-off version of hc. It is labeled
with ? : , which does not contribute to the word string or
tag string accepted along a path. Its weight ka is the weight
c(? | hc) of the corresponding failure arc in c from hc to h?c.
We define va
def
= 0, so it does not contribute to the total value.
10Recall that the forward probability of each state is defined
recursively from the forward probabilities of the states that have
arcs leading to it. As our FST is cyclic, it is not possible to visit
the states in topologically sorted order. We instead solve these
simultaneous equations by a relaxation algorithm (Eisner, 2002,
section 5): repeatedly sweep through all states, updating their
forward probability, until the total forward probability of all fi-
nal states is close to the correct total of 1 =
?
w,t cq?(w, t)
(showing that we have covered all high-prob paths). A corre-
sponding backward relaxation is actually not needed yet (we do
need it for ?? in section 3.4): backward probabilities are just 1,
since cq? is constructed with locally normalized probabilities.
When we rerun the forward-backward algorithm after a pa-
rameter update, we use the previous solution as a starting point
for the relaxation algorithm. This greatly speeds convergence.
hp) ? p?(w | t). This is a minor change?except that
va now depends on hp, which is the history of np?1
previous tags. If np > nq, then a?s start state does
not store such a long history. Thus, the value of a
actually depends on how one reaches a! It is prop-
erly written as vza, where za is a path ending with
a and z is sufficiently long to determine hp.11
Formally, let Za be a ?partitioning? set of paths to
a, such that any path in cq? from an initial state to
the start state of a must have exactly one z ? Za as
a suffix, and each z ? Za is sufficiently long so that
vza is well-defined. We can now find the expected
value as r? =
?
a
?
z?Za ?z
(?
z?z kz
)
kavza?a.
The above method permits p? to score the tag se-
quences of length np that are hypothesized by cq?.
One can regard it as implicitly running the general-
ized forward-backward algorithm over a larger FST
that marries the structure of cq? with the np-gram
HMM structure,12 so that each value is again local to
a single arc za. However, it saves space by working
directly on cq? (which has manageable size because
we deliberately kept nq small), rather than material-
izing the larger FST (as bad as increasing nq to np).
TheZa trick usesO(CTnq) rather thanO(CTnp)
space to store the FST, where C is the number of
arcs in c (= number of training n-grams) and T is
the number of tag types. With or without the trick,
runtime isO(CTnp+BCTnq), whereB is the num-
11By concatenating z?s start state?s hq with the tags along z.
Typically z has length np ? nq (and Za consists of the paths
of that length to a?s start state). However, z may be longer if it
contains ? arcs, or shorter if it begins with an initial state.
12Constructed by lazy finite-state intersection of cq? and p?
(Mohri et al, 2000). These do not have to be n-gram taggers,
but must be same-length FSTs (these are closed under inter-
section) and unambiguous. Define arc values in both FSTs such
that for any (w, t), cq? and p? accept (w, t) along unique paths
of total values v = ? log q?(t | w) and v? = log p?(w, t), re-
spectively. We now lift the weights into the expectation semir-
ing (Eisner, 2002) as follows. In cq?, replace arc a?s weight
ka with the semiring weight ?ka, kava?. In p? , replace arc a??s
weight with ?1, v?a??. Then if k = cq?(w, t), the intersected
FST accepts (w, t) with weight ?k, k(v + v?)?. The expecta-
tion of v+v? over all paths is then a sum
?
za ?zarza?za over
arcs za of the intersected FST?we are using za to denote the
arc in the intersected FST that corresponds to ?a in cq? when
reached via path z,? and rza to denote the second component
of its semiring weight. Here ?za and ?za denote the forward
and backward probabilities in the intersected FST, defined from
the first components of the semiring weights. We can get them
more efficiently from the results of running forward-backward
on the smaller cq?: ?za = ?z
?
z?z kz and ?za = ?a = 1.
136
ber of forward-backward sweeps (footnote 10). The
ordinary forward algorithm requires nq = np and
takesO(CTnp) time and space on a length-C string.
3.4 Computing the gradient as well
To maximize our objective (5), we compute its gra-
dient with respect to ? and ?. We follow an efficient
recipe from Li and Eisner (2009, section 5, case 3).
The runtime and space match those of section 3.3,
except that the runtime rises to O(BCTnp).13
First suppose that each va is local to a single arc.
We replace each weight ka with k?a = ?ka, kava?
in the so-called expectation semiring, whose sum
and product operations can be found in Li and Eis-
ner (2009, Table 1). Using these in the forward-
backward algorithm yields quantities ??a and ??a
that also fall in the expectation semiring.14 (Their
first components are the old ?a and ?a.) The
desired gradient15 ??k?,?r?? is
?
a ??a(?k?a)??a,
16
where?k?a = (?ka,?(kava)) = (?ka, (?ka)va+
ka(?va)). Here? gives the vector of partial deriva-
tives with respect to all ? and ? parameters. Yet each
?k?a is sparse, with only 3 nonzero components, be-
cause k?a depends on only one ? parameter (?a) and
two ? parameters (via ?a as defined in section 3.3).
When np > nq, we sum not over arcs a of cq? but
over arcs za of the larger FST (footnote 12). Again
we can do this implicitly, by using the short path za
in cq? in place of the arc za. Each state of cq? must
then store ?? and ?? values for each of the Tnp?nq
states of the larger FST that it corresponds to. (In the
case np ? nq = 1, as in our experiments, this fortu-
nately does not increase the total asymptotic space,
13An alternative would be to apply back-propagation
(reverse-mode automatic differentiation) to section 3.3?s com-
putation of the objective. This would achieve the same runtime
as in section 3.3, but would need as much space as time.
14This also computes our objective r?: summing the ???s of the
final states of cq? gives ?k?, r?? where k? = 1 is the total probabil-
ity of all paths. This alternative computation of the expectation
r?, using the forward algorithm (instead of forward-backward)
but over the expectation semiring, was given by Eisner (2002).
15We are interested in ?r?. ?k? is just a byproduct. We re-
mark that ?k? 6= 0, even though k? = 1 for any valid parameter
vector ? (footnote 14), as increasing ? invalidly can increase k?.
16By a product of pairs we always mean ?k, r??s, t? def=
?ks, kt+ rs?, just as in the expectation semiring, even though
the pair?k?a is not in that semiring (its components are vectors
rather than scalars). See (Li and Eisner, 2009, section 4.3). We
also define scalar-by-pair products as k?s, t? def= ?ks, kt?.
since each state of cq? already has to store T arcs.)
With more cleverness, one can eliminate this
extra storage while preserving asymptotic runtime
(still using sparse vectors). Find ??k?, (?r?)(1)? =
?
a ??a??ka, 0???a. Also find ?r?, (?r?)
(2)? =
?
a
?
z?Za?z
(?
z?z?kz,?kz?
)
?kavza,?(kavza)?
?a. Now our desired gradient ?r? emerges as
(?r?)(1) + (?r?)(2). The computation of (?r?)(1)
uses modified definitions of ??a and ??a that depend
only on (respectively) the source and target states of
a?not za.17 To compute them, initialize ?? (respec-
tively ??) at each state to ?1, 0? or ?0, 0? according to
whether the state is initial (respectively final). Now
iterate repeatedly (footnote 10) over all arcs a: Add
??a?ka, 0? +
?
z?Za ?z
(?
z?z kz
)
?0, kavza? to the
?? at a?s target state. Conversely, add ?ka, 0???a to
the ?? at a?s source state, and for each z ? Za, add(?
z?z kz
)
?0, kavza??a to the ?? at z?s source state.
3.5 Locally optimizing the objective
Recall that cq? associates with each [hc, hq, w] a
block of ? parameters that must be ? 0 and sum to
1. Our optimization method must enforce these con-
straints. A standard approach is to use a projected
gradient method, where after each gradient step on
?, the parameters are projected back onto the prob-
ability simplex. We implemented another standard
approach: reexpress each block of parameters {?a :
a ? A} as ?a
def
= exp ?a/
?
b?A exp ?b, as is possi-
ble iff the ?a parameters satisfy the constraints. We
then follow the gradient of r? with respect to the new
? parameters, given by ?r?/??a = ?a(?r?/??a?EA)
where EA =
?
b ?b(?r?/??b).
Another common approach is block coordinate
ascent on ? and ??this is ?variational EM.? M-
step: Given ?, we can easily find optimal esti-
mates of the emission and transition probabilities ?.
They are respectively proportional to the posterior
expected counts of arcs a and paths za under cq?,
namely N ? ?aka?a and N ? ?z
(?
z?z kz
)
ka?a.
E-step: Given ?, we cannot easily find the opti-
mal ? (even if nq = np).18 This was the rea-
17First components ?a and ?a remain as in cq?. ??a sums
paths to a. ??ka, 0???a can?t quite sum over paths starting with
a (their early weights depend on z), but (?r?)(2) corrects this.
18Recall that cq? must have locally normalized probabilities
(to ensure that its marginal is c). If nq = np, the optimal ?
is as follows: we can reduce the variational gap to 0 by setting
137
son for gradient ascent. However, for any single
sum-to-1 block of parameters {?a : a ? A}, it
is easy to find the optimal values if the others are
held fixed. We maximize LA
def
= r? + ?A
?
a?A ?a,
where ?A is a Lagrange multiplier chosen so that
the sum is 1. The partial derivative ?r?/??a can be
found using methods of section 3.4, restricting the
sums to za for the given a. For example, follow-
ing paragraphs 2?3 of section 3.4, let ??a, ra?
def
=
?
z?Za ??za, rza? where ??za, rza?
def
= ??za??za.19
Setting ?LA/??a = 0 implies that ?a is propor-
tional to exp((ra +
?
z?Za ?za log ?za)/?a).
20
Rather than doing block coordinate ascent by up-
dating one ? block at a time (and then recomputing
ra values for all blocks, which is slow), one can take
an approximate step by updating all blocks in paral-
lel. We find that replacing the E-step with a single
parallel step still tends to improve the objective, and
that this approximate variational EM is faster than
gradient ascent with comparable results.21
4 Experiments
4.1 Constrained unsupervised HMM learning
We follow the unsupervised POS tagging setup of
Merialdo (1994) and many others (Smith and Eis-
ner, 2005; Haghighi and Klein, 2006; Toutanova and
Johnson, 2007; Goldwater and Griffiths, 2007; John-
son, 2007). Given a corpus of sentences, one seeks
the maximum-likelihood or MAP parameters of a bi-
gram HMM (np = 2). The observed sentences, for
q?(t | hcw, hq) to the probability that t begins with t if we
randomly draw a suffix w ? c(? | hcw) and randomly tag ww
with t ? p?(? | ww, hq). This is equivalent to using p? with the
backward algorithm to conditionally tag each possible suffix.
19The first component of ??za??za is ?za?za = ?za ? 1.
20If a is an arc of cq? then ?r?/??a is the second component
of
?
z?Za
??za(?k?za/??a)??za. Then ?LA/??a works out to?
z?Za
ca(rza+?za(log ?za?log ?a?1))+?A. Set to 0 and
solve for ?a, noting that ca, ?a, ?A are constant over a ? A.
21In retrospect, an even faster strategy might be to do a series
of block ? and ?? updates, updating ?? at a state (footnote 10) im-
mediately after updating ? on the arcs leading from that state,
which allows a better block update at predecessor states. On an
acyclic machine, a single backward pass of this sort will reduce
the variational gap to 0 if nq = np (footnote 18). This is be-
cause, thanks to the up-to-date ??, each block of arcs gets new ?
weights in proportion to relative suffix path probabilities under
the new ?. After this backward pass, a single forward pass can
update the ? values and collect expected counts for the M-step
that will update ?. Standard EM is a special case of this strategy.
us, are replaced by the faux sentences extrapolated
from observed n-grams via the language model c.
The states of the HMM correspond to POS tags as
in Figure 1. All transitions are allowed, but not all
emissions. If a word is listed in a provided ?dictio-
nary? with its possible tags, then other tags are given
0 probability of emitting that word. The EM algo-
rithm uses the corpus to learn transition and emis-
sion probabilities that explain the data under this
constraint. The constraint ensures that the learned
states have something to do with true POS tags.
Merialdo (1994) spawned a long line of work
on this task. Ideas have included Bayesian learn-
ing methods (MacKay, 1997; Goldwater and Grif-
fiths, 2007; Johnson, 2007), better initial parame-
ters (Goldberg et al, 2008), and learning how to
constrain the possible parts of speech for a word
(Ravi and Knight, 2008), as well as non-HMM se-
quence models (Smith and Eisner, 2005; Haghighi
and Klein, 2006; Toutanova and Johnson, 2007).
Most of this work has used the Penn Treebank
(Marcus et al, 1993) as a dataset. While this
million-word Wall Street Journal (WSJ) corpus is
one of the largest that is manually annotated with
parts of speech, unsupervised learning methods
could take advantage of vast amounts of unannotated
text. In practice, runtime concerns have sometimes
led researchers to use small subsets of the Penn Tree-
bank (Goldwater and Griffiths, 2007; Smith and Eis-
ner, 2005; Haghighi and Klein, 2006). Our goal is
to point the way to using even larger datasets.
The reason for all this past research is that (Meri-
aldo, 1994) was a negative result: while EM is
guaranteed to improve the model?s likelihood, it de-
grades the match between the latent states and true
parts of speech (if the starting point is a good one
obtained with some supervision). Thus, for the task
of POS induction, there must be something wrong
with the HMM model, the likelihood objective, or
the search procedure. It is clear that the model is far
too weak: there are many latent variables in natural
language, so the HMM may be picking up on some-
thing other than POS tags. Ultimately, fixing this
will require richer models with many more param-
eters. But learning these (lexically specific) param-
eters will require large training datasets?hence our
present methodological exploration on whether it is
possible to scale up the original setting.
138
4.2 Setup
We investigate how much performance degrades
when we approximate the corpus and train approx-
imately with nq = 1. We examine two measures:
likelihood on a held-out corpus and accuracy in POS
tagging. We train on corpora of three different sizes:
?WSJ-big (910k words? 441k n-grams @ cutoff 3),
? Giga-20 (20M words? 2.9M n-grams @ cutoff 10),
? Giga-200 (200M wds? 14.4M n-grams @ cutoff 20).
These were drawn from the Penn Treebank (sections
2?23) and the English Gigaword corpus (Parker et
al., 2009). For held-out evaluation, we use WSJ-
small (Penn Treebank section 0) or WSJ-big.
We estimate backoff language models for these
corpora based on collections of n-grams with n ? 5.
In this work, we select the n-grams by simple count
cutoffs as shown above,22 taking care to keep all 2-
grams as mentioned in footnote 2.
Similar to Merialdo (1994), we use a tag dictio-
nary which limits the possible tags of a word to those
it was observed with in the WSJ, provided that the
word was observed at least 5 times in the WSJ. We
used the reduced tagset of Smith and Eisner (2005),
which collapses the original 45 fine-grained part-of-
speech tags into just 17 coarser tags.
4.3 Results
In all experiments, our method achieves similar ac-
curacy though slightly worse likelihood. Although
this method is meant to be a fast approximation of
EM, standard EM is faster on the smallest dataset
(WSJ-big). This is because this corpus is not much
bigger than the 5-gram language model built from it
(at our current pruning level), and so the overhead
of the more complex n-gram EM method is a net
disadvantage. However, when moving to larger cor-
pora, the iterations of n-gram EM become as fast as
standard EM and then faster. We expect this trend
to continue as one moves to much larger datasets, as
the compression ratio of the pruned language model
relative to the original corpus will only improve.
The Google n-gram corpus is based on 50? more
data than our largest but could be handled in RAM.
22Entropy-based pruning (Stolcke, 2000) may be a better se-
lection method when one is in a position to choose. However,
count cutoffs were already used in the creation of the Google
n-gram corpus, and more complex methods of pruning may not
be practical for very large datasets.
 72
 74
 76
 78
 80
 82
 84
 86
Acc
ura
cy
Time
EM (WSJ-big)N-gram EM (WSJ-big)EM (Giga-20)N-gram EM (Giga-20)EM (Giga-200)N-gram EM (Giga-200)
Like
liho
od
Time
EM (WSJ-big)N-gram EM (WSJ-big)EM (Giga-20)N-gram EM (Giga-20)EM (Giga-200)N-gram EM (Giga-200)
Figure 2: POS-tagging accuracy and log-likelihood af-
ter each iteration, measured on WSJ-big when training
on the Gigaword datasets, else on WSJ-small. Runtime
and log-likelihood are scaled differently for each dataset.
Replacing EM with our method changes runtime per it-
eration from 1.4s? 3.5s, 48s? 47s, and 506s? 321s.
5 Conclusions
We presented a general approach to training genera-
tive models on a distribution rather than on a training
sample. We gave several motivations for this novel
problem. We formulated an objective function simi-
lar to MAP, and presented a variational lower bound.
Algorithmically, we gave nontrivial general meth-
ods for computing and optimizing our variational
lower bound for arbitrary finite-state data distribu-
tions c, generative models p, and variational fami-
lies q, provided that p and q are unambiguous same-
length FSTs. We also gave details for specific useful
families for c, p, and q.
As proof of principle, we used a traditional HMM
POS tagging task to demonstrate that we can train
a model from n-grams almost as accurately as from
full sentences, and do so faster to the extent that the
n-gram dataset is smaller. More generally, we offer
our approach as an intriguing new tool to help semi-
supervised learning benefit from very large datasets.
139
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proc. of ACL, pages 40?47.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In Proc. of IJCAI.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Linguistic Data Consortium, Philadelphia.
LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proc. of EMNLP.
Tim Dawborn and James R. Curran. 2009. CCG
parsing with one syntactic structure per n-gram. In
Australasian Language Technology Association Work-
shop, pages 71?79.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proc. of ACL, pages 1?8.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, pages 746?754.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proc. of ACL, pages 744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proc. of NAACL,
pages 320?327.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A
spectral algorithm for learning hidden Markov models.
In Proc. of COLT.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL, pages
296?305.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. In M. I. Jordan, editor, Learning
in Graphical Models. Kluwer.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of
EMNLP, pages 40?51.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: Trading structure for features.
In International Conference on Machine Learning
(ICML), Helsinki, Finland.
D. Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2009. Unsupervised ac-
quisition of lexical knowledge from n-grams. Sum-
mer workshop technical report, Center for Language
and Speech Processing, Johns Hopkins University.
David J. C. MacKay. 1997. Ensemble learning for hid-
den Markov models. http://www.inference.
phy.cam.ac.uk/mackay/abstracts/
ensemblePaper.html.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov models for
information extraction and segmentation. In Proc. of
ICML, pages 591?598.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2):155?
171.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, W. Brockman, The Google Books Team, J. P.
Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant,
S. Pinker, M. A. Nowak, and E. L. Aiden. 2010.
Quantitative analysis of culture using millions of digi-
tized books. Science, 331(6014):176?182.
Mehryar Mohri and Mark-Jan Nederhof. 2001. Regu-
lar approximation of context-free grammars through
transformation. In Jean-Claude Junqua and Gert-
jan van Noord, editors, Robustness in Language and
Speech Technology, chapter 9, pages 153?163. Kluwer
Academic Publishers, The Netherlands, February.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17?32, January.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view
of the EM algorithm that justifies incremental, sparse,
and other variants. In M.I. Jordan, editor, Learning in
Graphical Models, pages 355?368. Kluwer.
Mark-Jan Nederhof. 2000. Practical experiments
with regular approximation of context-free languages.
Computational Linguistics, 26(1).
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth
edition. Linguistic Data Consortium, Philadelphia.
LDC2009T13.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In
Proc. of IJCAI, pages 1124?1129.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
140
recognition. Proc. of the IEEE, 77(2):257?286, Febru-
ary.
Sujith Ravi and Kevin Knight. 2008. Minimized models
for unsupervised part-of-speech tagging. In Proc. of
ACL, pages 504?512.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. of ACL, pages 354?362.
R. Staden. 1979. A strategy of DNA sequencing em-
ploying computer programs. Nucleic Acids Research,
6(7):2601?2610, June.
Andreas Stolcke. 2000. Entropy-based pruning of back-
off language models. In DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Proc. of NIPS, volume 20.
141
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 232?242,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Implicitly Intersecting Weighted Automata using Dual Decomposition?
Michael J. Paul and Jason Eisner
Department of Computer Science / Johns Hopkins University
Baltimore, MD 21218, USA
{mpaul,jason}@cs.jhu.edu
Abstract
We propose an algorithm to find the best path
through an intersection of arbitrarily many
weighted automata, without actually perform-
ing the intersection. The algorithm is based on
dual decomposition: the automata attempt to
agree on a string by communicating about fea-
tures of the string. We demonstrate the algo-
rithm on the Steiner consensus string problem,
both on synthetic data and on consensus de-
coding for speech recognition. This involves
implicitly intersecting up to 100 automata.
1 Introduction
Many tasks in natural language processing in-
volve functions that assign scores?such as log-
probabilities?to candidate strings or sequences.
Often such a function can be represented compactly
as a weighted finite state automaton (WFSA). Find-
ing the best-scoring string according to a WFSA is
straightforward using standard best-path algorithms.
It is common to construct a scoring WFSA by
combining two or more simpler WFSAs, taking ad-
vantage of the closure properties of WFSAs. For ex-
ample, consider noisy channel approaches to speech
recognition (Pereira and Riley, 1997) or machine
translation (Knight and Al-Onaizan, 1998). Given
an input f , the score of a possible English tran-
scription or translation e is the sum of its language
model score log p(e) and its channel model score
log p(f | e). If each of these functions of e is repre-
sented as a WFSA, then their sum is represented as
the intersection of those two WFSAs.
WFSA intersection corresponds to constraint con-
junction, and hence is often a mathematically natu-
ral way to specify a solution to a problem involving
?The authors are grateful to Damianos Karakos for provid-
ing tools and data for the ASR experiments. This work was
supported in part by an NSF Graduate Research Fellowship.
multiple soft constraints on a desired string. Unfor-
tunately, the intersection may be computationally in-
efficient in practice. The intersection of K WFSAs
having n1, n2, . . . , nK states may have n1?n2 ? ? ?nK
states in the worst case.1
In this paper, we propose a more efficient method
for finding the best path in an intersection without
actually computing the full intersection. Our ap-
proach is based on dual decomposition, a combina-
torial optimization technique that was recently intro-
duced to the vision (Komodakis et al, 2007) and lan-
guage processing communities (Rush et al, 2010;
Koo et al, 2010). Our idea is to interrogate the
several WFSAs separately, repeatedly visiting each
WFSA to seek a high-scoring path in each WFSA
that agrees with the current paths found in the other
WSFAs. This iterative negotiation is reminiscent of
message-passing algorithms (Sontag et al, 2008),
while the queries to the WFSAs are reminiscent of
loss-augmented inference (Taskar et al, 2005).
We remark that a general solution whose asymp-
totic worst-case runtime beat that of naive intersec-
tion would have important implications for com-
plexity theory (Karakostas et al, 2003). Our ap-
proach is not such a solution. We have no worst-case
bounds on how long dual decomposition will take to
converge in our setting, and indeed it can fail to con-
verge altogether.2 However, when it does converge,
we have a ?certificate? that the solution is optimal.
Dual decomposition is usually regarded as a
method for finding an optimal vector in Rd, sub-
ject to several constraints. However, it is not ob-
vious how best to represent strings as vectors?they
1Most regular expression operators combine WFSA sizes
additively. It is primarily intersection and its close relative,
composition, that do so multiplicatively, leading to inefficiency
when two large WFSAs are combined, and to exponential
blowup when many WFSAs are combined. Yet these operations
are crucially important in practice.
2An example that oscillates can be constructed along lines
similar to the one given by Rush et al (2010).
232
have unbounded length, and furthermore the abso-
lute position of a symbol is not usually significant in
evaluating its contribution to the score.3 One con-
tribution of this work is that we propose a general,
flexible scheme for converting strings to feature vec-
tors on which the WFSAs must agree. In principle
the number of features may be infinite, but the set
of ?active? features is expanded only as needed un-
til the algorithm converges. Our experiments use a
particular instantiation of our general scheme, based
on n-gram features.
We apply our method to a particular task: finding
the Steiner consensus string (Gusfield, 1997) that
has low total edit distance to a number of given, un-
aligned strings. As an illustration, we are pleased to
report that ?alia? and ?aian? are the consensus
popular names for girls and boys born in the U.S. in
2010. We use this technique for consensus decoding
from speech recognition lattices, and to reconstruct
the common source of up to 100 strings corrupted by
random noise. Explicit intersection would be astro-
nomically expensive in these cases. We demonstrate
that our approach tends to converge rather quickly,
and that it finds good solutions quickly in any case.
2 Preliminaries
2.1 Weighted Finite State Automata
A weighted finite state automaton (WFSA) over the
finite alphabet ? is an FSA that has a cost or weight
associated with each arc. We consider the case of
real-valued weights in the tropical semiring. This is
a fancy way of saying that the weight of a path is the
sum of its arc weights, and that the weight of a string
is the minimum weight of all its accepting paths (or
? if there are none).
When we intersect two WFSAs F and G, the ef-
fect is to add string weights: (F ?G)(x) = F (x) +
G(x). Our problem is to find the x that minimizes
this sum, but without constructing F ? G to run a
shortest-path algorithm on it.
2.2 Dual Decomposition
The trick in dual decomposition is to decompose
an intractable global problem into two or more
3Such difficulties are typical when trying to apply structured
prediction or optimization techniques to predict linguistic ob-
jects such as strings or trees, rather than vectors.
tractable subproblems that can be solved indepen-
dently. If we can somehow combine the solutions
from the subproblems into a ?valid? solution to the
global problem, then we can avoid optimizing the
joint problem directly. A valid solution is one in
which the individual solutions of each subproblem
all agree on the variables which are shared in the
joint problem. For example, if we are combining a
parser with a part-of-speech tagger, the tag assign-
ments from both models must agree in the final so-
lution (Rush et al, 2010); if we are intersecting a
translation model with a language model, then it is
the words that must agree (Rush and Collins, 2011).
More formally, suppose we want to find a global
solution that is jointly optimized among K sub-
problems: argminx
?K
k=1 fk(x). Suppose that x
ranges over vectors. Introducing an auxiliary vari-
able xk for each subproblem fk allows us to equiv-
alently formulate this as the following constrained
optimization problem:
argmin
{x,x1,...,xK}
K?
k=1
fk(xk) s.t. (?k)xk = x (1)
For any set of vectors ?k that sum to 0,
?K
k=1 ?k =
0, Komodakis et al (2007) show that the following
Lagrangian dual is a lower bound on (1):4
min
{x1,...,xK}
K?
k=1
fk(xk) + ?k ? xk (2)
where the Lagrange multiplier vectors ?k can be
used to penalize solutions that do not satisfy the
agreement constraints (?k)xk = x. Our goal is to
maximize this lower bound and hope that the result
does satisfy the constraints. The graphs in Fig. 2
illustrate how we increase the lower bound over
time, using a subgradient algorithm to adjust the ??s.
At each subgradient step, (2) can be computed by
choosing each xk = argminxk fk(xk) +?k ?xk sep-
arately. In effect, each subproblem makes an inde-
pendent prediction xk influenced by ?k, and if these
outputs do not yet satisfy the agreement constraints,
then the ?k are adjusted to encourage the subprob-
lems to agree on the next iteration. See Sontag et al
(2011) for a detailed tutorial on dual decomposition.
4The objective in (2) can always be made as small as in (1)
by choosing the vectors (x1, . . . xK) that minimize (1) (because
then
P
k ?k ? xk =
P
k ?k ? x = 0 ? x = 0). Hence (2) ? (1).
233
3 WFSAs and Dual Decomposition
Given K WFSAs, F1, . . . , FK , we are interested in
finding the string x which has the best score in the
intersection F1? . . .?FK . The lowest-cost string in
the intersection of all K machines is defined as:
argmin
x
?
k
Fk(x) (3)
As explained above, the trick in dual decomposi-
tion is to recast (3) as independent problems of the
form argminxk Fk(xk), subject to constraints that
all xk are the same. However, it is not so clear how
to define agreement constraints on strings. Perhaps
a natural formulation is that Fk should be urged to
favor strings xk that would be read by Fk? along a
similar path to that of xk? . But Fk cannot keep track
of the state of Fk? for all k? without solving the full
intersection?precisely what we are trying to avoid.
Instead of requiring the strings xk to be equal as
in (1), we will require their features to be equal:
(?k) ?(xk) = ?(x) (4)
Of course, we must define the features. We will use
an infinite feature vector ?(x) that completely char-
acterizes x, so that agreement of the feature vectors
implies agreement of the strings. At each subgradi-
ent step, however, we will only allow finitely many
elements of ?k to become nonzero, so only a finite
portion of ?(xk) needs to be computed.5
We will define these ?active? features of a string
x by constructing some unweighted deterministic
FSA, G (described in ?4). The active features of x
are determined by the collection of arcs on the ac-
cepting path of x in G. Thus, to satisfy the agree-
ment constraint, xi and xj must be accepted using
the same arcs ofG (or more generally, arcs that have
the same features).
We relax the constraints by introducing a col-
lection ? = ?1, . . . , ?K of Lagrange multipliers,
5The simplest scheme would define a binary feature for each
string in ??. Then the nonzero elements of ?k would spec-
ify punishments and rewards for outputting various strings that
had been encountered at earlier iterations: ?Try subproblem k
again, and try harder not to output michael this time, as it still
didn?t agree with other subproblems: try jason instead.? This
scheme would converge glacially if at all. We instead focus on
featurizations that let subproblems negotiate about substrings:
?Try again, avoiding mi if possible and favoring ja instead.?
and defining G?k(x) such that the features of G are
weighted by the vector ?k (all of whose nonzero el-
ements must correspond to features in G). As in (2),
we assume ? ? ?, where ? = {? :
?
k ?k = 0}.
This gives the objective:
h(?) = min
{x1,...,xK}
?
k
(Fk(xk) +G?k(xk)) (5)
This minimization fully decomposes into K sub-
problems that can be solved independently. The kth
subproblem is to find argminxk Fk(xk)+G?k(xk),
which is straightforward to solve with finite-state
methods. It is the string on the lowest-cost path
through Hk = Fk ? G?k , as found with standard
path algorithms (Mohri, 2002).
The dual problem we wish to solve is
max??? h(?), where h(?) itself is a min over
{x1, . . . , xK}. We optimize ? via projected subgra-
dient ascent (Komodakis et al, 2007). The update
equation for ?k at iteration t is then:
?(t+1)k = ?
(t)
k + ?t
(
?(x(t)k )?
?
k? ?(x
(t)
k? )
K
)
(6)
where ?t > 0 is the step size at iteration t. This up-
date is intuitive. It moves away from the current so-
lution and toward the average solution (where they
differ), by increasing the cost of the former?s fea-
tures and reducing the cost of the latter?s features.
This update may be very dense, however, since
?(x) is an infinite vector. So we usually only up-
date the elements of ?k that correspond to the small
finite set of active features (the other elements are
still ?frozen? at 0), denoted ?. This is still a valid
subgradient step. This strategy is incorrect only if
the updates for all active features are 0?in other
words, only if we have achieved equality of the cur-
rently active features and yet still the {xk} do not
agree. In that case, we must choose some inactive
features that are still unequal and allow the subgra-
dient step to update their ? coefficients to nonzero,
making them active. At the next step of optimiza-
tion, we must expand G to consider this enlarged set
of active features.
4 The Agreement Machine
The agreement machine (or constraint machine) G
can be thought of as a way of encoding features of
234
strings on which we enforce agreement. There are a
number of different topologies for G that might be
considered, with varying degrees of efficiency and
utility. Constructing G essentially amounts to fea-
ture engineering; as such, it is unlikely that there
is a universally optimal topology of G. Neverthe-
less, there are clearly bad ways to build G, as not
all topologies are guaranteed to lead to an optimal
solution. In this section, we lay out some abstract
guidelines for appropriateG construction, before we
describe specific topologies in the later subsections.
Most importantly, we should design G so that it
accepts all strings in F1? . . .?FK . This is to ensure
that it accepts the string that is the optimal solution
to the joint problem. If G did not accept that string,
then neither would Hk = Fk ?G, and our algorithm
would not be able to find it.
Even ifHk can accept the optimal string, it is pos-
sible that this string would never be the best path in
this machine, regardless of ?. For example, suppose
G is a single-state machine with self-loops accept-
ing each symbol in the alphabet (i.e. a unigram ma-
chine). Suppose Hk outputs the string aaa in the
current iteration, but we would like the machines to
converge to aaaaa. We would lower the weight of
?a to encourage Hk to output more of the symbol a.
However, if Hk has a cyclic topology, then it could
happen that a negative value of ?a could create a
negative-weight cycle, in which the lowest-cost path
throughHk is infinitely long. It might be that adjust-
ing ?a can change the best string to either aaa or
aaaaaaaaa. . . (depending on whether a cycle af-
ter the initial aaa has positive or negative weight),
but never the optimal aaaaa. On the other hand,
if G instead encoded 5-grams, this would not be a
problem because a path through a 5-gram machine
could accept aaaaa without traversing a cycle.
Finally, agreeing on (active) features does not
necessarily mean that all xk are the same string. For
example, if we again use a unigram G (that is, ? =
?, the set of unigrams), then ??(abc) = ??(cba),
where ?? returns a feature vector where all but the
active features are zeroed out. In this instance, we
satisfy the constraints imposed by G, even though
we have not satisfied the constraint we truly care
about: that the strings agree.
To summarize, we will aim to choose ? such that
G has the following characteristics:
1. The language L(Fk ?G) = L(Fk); i.e. G does
not restrict the set of strings accepted by Fk.
2. When ??(xi) = ??(xj), typically xi = xj .
3. ?? ? ? s.t. argminx Fk(x) +G?k(x) =
argminx
?
k? Fk?(x), i.e., the optimal string
can be the best path in Fk ? G.6 This may not
be the case if G is cyclic.
The first of these is required during every itera-
tion of the algorithm in order to maintain optimality
guarantees. However, even if we do not satisfy the
latter two points, we may get lucky and the strings
themselves will agree upon convergence, and no fur-
ther work is required. Furthermore, the unigram ma-
chine G used in the above examples, despite break-
ing these requirements, has the advantage of being
very efficient to intersect with F . This motivates
our ?active feature? strategy of using a simpleG ini-
tially, and incrementally altering it as needed, for ex-
ample if we satisfy the constraints but the strings do
not yet match. We discuss this in ?4.2.
4.1 N-Gram Construction of G
In principle, it is valid to use any G that satisfies the
guidelines above, but in practice, some topologies
will lead to faster convergence than others.
Perhaps the most obvious form is a simple vector
encoding of strings, e.g. ?a at position 1?, ?b at po-
sition 2?, and so on. As a WFSA, this would simply
have one state represent each position, with arcs for
each symbol going from position i to i + 1. This is
essentially a unigram machine where the loops have
been ?unrolled? to also keep track of position.
However, early experiments showed that with
this topology for G, our algorithm converged very
slowly, if at all. What goes wrong? The problem
stems from the fact that the strings are unaligned and
of varying length, and it is difficult to get the strings
to agree quickly at specific positions. For example,
if two subproblems have b at positions 6 and 8 in the
current iteration, they might agree at position 7?but
our features don?t encourage this. The Lagrangian
update would discourage accepting b at 6 and en-
courage b at 8 (and vice versa), without giving credit
6It is not always possible to construct a G to satisfy this
property, as the Lagrangian dual may not be a tight bound to the
original problem.
235
for meeting in the middle. Further, these features do
not encourage the subproblems to preserve the rela-
tive order of neighboring symbols, and strings which
are almost the same but slightly misaligned will be
penalized essentially everywhere. This is an ineffec-
tive way for the subproblems to communicate.
In this paper, we focus on the feature set we
found to work the best in our experiments: the
strings should agree on their n-gram features, such
as ?number of occurrences of the bigram ab.? Even
if we don?t yet know precisely where ab should ap-
pear in the string, we can still move toward conver-
gence if we try to force the subproblems to agree on
whether and how often ab appears at all.
To encode n-gram features in a WFSA, each state
represents the (n?1)-gram history, and all arcs leav-
ing the state represent the final symbol in the n-
gram, weighted by the score of that n-gram. The
machine will also contain start and end states, with
appropriate transitions to/from the n-gram states.
For example, if the trigram abc has weight ?abc,
then the trigram machine will encode this as an arc
with the symbol c leaving the state representing ab,
and this arc will have weight ?abc. If our feature
set alo contains 1- and 2-grams, then the arc in this
example would incorporate the weights of all of the
corresponding features: ?abc + ?bc + ?c.
A drawback is that these features give no infor-
mation about where in the string the n-grams should
occur. In a long string, we might want to encour-
age or discourage an n-gram in a certain ?region? of
the string. Our features can only encourage or dis-
courage it everywhere in the string, which may lead
to slow convergence. Nevertheless, in our particular
experimental settings, we find that this works better
than other topologies we have considered.
Sparse N-Gram Encoding A full n-gram lan-
guage model requires ? |?|n arcs to encode as a
WFSA. This could be quite expensive. Fortunately,
large n-gram models can be compacted by using
failure arcs (?-arcs) to encode backoff (Allauzen et
al., 2003). These arcs act as -transitions that can
be taken only when no other transition is available.
They allow us to encode the sparse subset of n-
grams that have nonzero Lagrangians. We encodeG
such that all features whose ? value is 0 will back off
to the next largest n-gram having nonzero weight.
This form of G still accepts ?? and has the same
weights as a dense representation, but could require
substantially fewer states.
4.2 Incrementally Expanding G
As mentioned above, we may need to alter G as we
go along. Intuitively, we may want to start with fea-
tures that are cheap to encode, to move the param-
eters ? to a good part of the solution space, then
incrementally bring in more expensive features as
needed. Shorter n-grams require a smaller G and
will require a shorter runtime per iteration, but if
they are too short to be informative, then they may
require many more iterations to reach convergence.
In an extreme case, we may reach a point where
the subproblems all agree on n-grams currently in
?, but the actual strings still do not match. Wait-
ing until we hit such a point may be unnecessarily
slow. We experimented with periodically increas-
ing n (e.g. adding trigrams to the feature set if we
haven?t converged with bigrams after a fixed num-
ber of iterations), but this is expensive, and it is not
clear how to define a schedule for increasing the or-
der of n. We instead present a simple and effective
heuristic for bringing in more features.
The idea is that if the subproblem solutions cur-
rently disagree on counts of the bigrams ab and
bc, then an abc feature may be unnecessary, since
the subproblems could still make progress with only
these bigram constraints. However, once the sub-
problems agree on these two bigrams, but disagree
on trigram abc, we bring this into the feature set ?.
More generally, we add an (n+ 1)-gram to the fea-
ture set if the current strings disagree on its counts
despite agreeing on its n-gram prefix and n-gram
suffix (which need not necessarily be ?). This se-
lectively brings in larger n-grams to target portions
of the strings that may require longer context, while
keeping the agreement machine small.
Algorithm 1 gives pseudocode for our complete
algorithm when using n-gram features with this in-
cremental strategy. To summarize, we solve for each
xk using the current ?k, and if all the strings agree,
we return them as the optimal solution. Otherwise,
we update ?k and repeat. At each iteration, we check
for n-gram agreement, and bring in select (n + 1)-
grams to the feature set as appropriate.
Finally, there is another instance where we might
236
Algorithm 1 The dual decomposition algorithm
with n-gram features.
Initialize ? to some initial set of n-gram features.
for t = 1 to T do
for k = 1 to K do
Solve xk = argminx(Fk ? G?k)(x) with a
shortest-path algorithm
end for
if (?i, j)xi = xj then
return {x1, . . . , xK}
else
? = ? ? {z ? ?? : all xk agree on the features
corresponding to the length-(|z| ? 1) prefix and
suffix of z, but not on z itself}
for k = 1 to K do
Update ?k according to equation (6)
Create G?k to encode the features ?
end for
end if
end for
need to expand G, which we omit from the pseu-
docode for conciseness. If both Fk and G are cyclic,
then there is a chance that there will be a negative-
weight cycle inFk?G?k . (If at least one of these ma-
chines is acyclic, then this is not a problem, because
their intersection yields a finite set.) In the case of
a negative-weight cycle, the best path is infinitely
long, and so the algorithm will either return an error
or fail to terminate. If this happens, then we need to
backtrack, and either decrease the subgradient step
size to avoid moving into this territory, or alter G to
expand the cycles. This can be done by unrolling
loops to keep track of more information?when en-
coding n-gram features with G, this amounts to ex-
panding G to encode higher order n-grams. When
using a sparse G with ?-arcs, it may also be neces-
sary to increase the minimum n-gram history that
is used for back-off. For example, instead of al-
lowing bigrams to back off to unigrams, we might
force G to encode the full set of bigrams (not just
bigrams with nonzero ?) in order to avoid cycles
in the lower order states. Our strategy for avoiding
negative-weight cycles is detailed in ?5.1.
5 Experiments with Consensus Decoding
To best highlight the utility of our approach, we con-
sider applications that must (implicitly) intersect a
large number of WFSAs. We will demonstrate that,
in many cases, our algorithm converges to an exact
solution on problems involving 10, 25, and even 100
machines, all of which would be hopeless to solve
by taking the full intersection.
We focus on the problem of solving for the Steiner
consensus string: given a set of K strings, find the
string in ?? that has minimal total edit distance to
all strings in the set. This is an NP-hard problem
that can be solved as an intersection of K machines,
as we will describe in ?5.2. The consensus string
also gives an implicit multiple sequence alignment,
as we discuss in ?6.
We begin with the application of minimum Bayes
risk decoding of speech lattices, which we show can
reduce to the consensus string problem. We then ex-
plore the consensus problem in depth by applying it
to a variety of different inputs.
5.1 Experimental Details
We initialize ? to include both unigrams and bi-
grams, as we find that unigrams alone are not pro-
ductive features in these experiments. As we expand
?, we allow it to include n-grams up to length five.
We run our algorithm for a maximum of 1000 iter-
ations, using a subgradient step size of ?/(t + 500)
at iteration t, which satisfies the general properties
to guarantee asymptotic convergence (Spall, 2003).
We initialize ? to 1 and 10 in the two subsections, re-
spectively. We halve ? whenever we hit a negative-
weight cycle and need to backtrack. If we still get
negative-weight cycles after ? ? 10?4 then we reset
? and increase the minimum order of n which is en-
coded in G. (If n is already at our maximum of five,
then we simply end without converging.) In the case
of non-convergence after 1000 iterations, we select
the best string (according to the objective) from the
set of strings that were solutions to any subproblem
at any point during optimization.
Our implementation uses OpenFST 1.2.8 (Al-
lauzen et al, 2007).
5.2 Minimum Bayes Risk Decoding for ASR
We first consider the task of automatic speech recog-
nition (ASR). Suppose x? is the true transcription
(a string) of an spoken utterance, and pi(w) is an
ASR system?s probability distribution over possi-
ble transcriptions w. The Bayes risk of an out-
put transcription x is defined as the expectation
237
?
w pi(w) `(x,w) for some loss function ` (Bickel
and Doksum, 2006). Minimum Bayes risk decoding
(Goel and Byrne, 2003) involves choosing the x that
minimizes the Bayes risk, rather than simply choos-
ing the x that maximizes pi(x) as in MAP decoding.
As a reasonable approximation, we will take the
expectation over just the strings w1, . . . , wK that are
most probable under pi. A common loss function
is the Levenshtein distance because this is generally
used to measure the word error rate of ASR output.
Thus, we seek a consensus transcription
argmin
x
K?
k=1
pik d(x,wk) (7)
that minimizes a weighted sum of edit distances to
all of the top-K strings, where high edit distance
to more probable strings is more strongly penal-
ized. Here d(x,w) is the unweighted Levenshtein
distance between two strings, and pik = pi(wk). If
each pik = 1/K, then argminx is known as the
Steiner consensus string, which is NP-hard to find
(Sim and Park, 2003). Equation (7) is a weighted
generalization of the Steiner problem.
Given an input string wk, it is straightforward
to define our WFSA Fk such that Fk(x) computes
pik d(x,wk). A direct construction of Fk is as fol-
lows. First, create a ?straight line? WFSA whose
single path accepts (only) wk; each each state corre-
sponds to a position in wk. These arcs all have cost
0. Now add various arcs with cost pik that permit
edit operations. For each arc labeled with a symbol
a ? ?, add competing ?substitution? arcs labeled
with the other symbols in ?, and a competing ?dele-
tion? arc labeled with ; these have the same source
and target as the original arc. Also, at each state, add
a self-loop labeled with each symbol in ?; these are
?insertion? arcs. Each arc that deviates from wk has
a cost of pik, and thus the lowest-cost path through
Fk accepting x has weight pik d(x,wk).
The consensus objective in Equation (7) can be
solved by finding the lowest-cost path in F1 ? . . . ?
FK , and we can solve this best-path problem using
the dual decomposition algorithm described above.
5.2.1 Experiments
We ran our algorithm on Broadcast News data, us-
ing 226 lattices produced by the IBM Attila decoder
0 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WE WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I DON?T WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WELL I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> THEY WANT TO BE TAKING A DEEP BREATH NOW </s>
300 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WE WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I DON?T WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WELL I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WELL WANT TO BE TAKING A DEEP BREATH NOW </s>
375 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I DON?T WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
472 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
Figure 1: Example run of the consensus problem on
K = 25 strings on a Broadcast News utterance, showing
x1, . . . , x5 at the 0th, 300th, 375th, and 472nd iterations.
(Chen et al, 2006; Soltau et al, 2010) on a subset
of the NIST dev04f data, using models trained by
Zweig et al (2011). For each lattice, we found the
consensus of the top K = 25 strings.
85% of the problems converged within 1000 it-
erations, with an average of 147.4 iterations. We
found that the true consensus was often the most
likely string under pi, but not always?this was true
70% of the time. In the Bayes risk objective we are
optimizing in equation (7)?the expected loss?our
approach averaged a score of 1.59, while always tak-
ing the top string gives only a slightly worse average
of 1.66. 8% of the problems encountered negative-
weight cycles, which were all resolved either by de-
creasing the step size or encoding larger n-grams.
5.3 Investigating Consensus Performance with
Synthetic Data
The above experiments demonstrate that we can ex-
actly find the best path in the intersection of 25
machines?an intersection that could not feasibly be
constructed in practice. However, these experiments
do not exhaustively explore how dual decomposition
behaves on the Steiner string problem in general.
Above, we experimented with only a fixed num-
ber of input strings, which were generally similar to
one another. There are a variety of other inputs to the
consensus problem which might lead to different be-
havior and convergence results, however. If we were
to instead run this experiment on DNA sequences
(for example, if we posit that the strings are all muta-
tions of the same ancestor), the alphabet {A,T,C,G}
238
0 5 10 15 20Runtime (s)0
10
20
30
40
50
60
70
80
90
Scor
e
K=50,`=10,|?|=10,?=0.2
PrimalDual
0 10 20 30 40 50 60Runtime (s)0
10
20
30
40
50
Scor
e
K=10,`=15,|?|=20,?=0.4
PrimalDual
0 5 10 15 20 25 30 35Runtime (s)0
5
10
15
20
25
30
35
40
Scor
e
K=5,`=50,|?|=5,?=0.1
PrimalDual
Figure 2: The algorithm?s behavior on three specific consensus problems. The curves show the current values of
the primal bound (based on the best string at the current iteration) and dual bound h(?). The horizontal axis shows
runtime. Red upper triangles are placed every 10 iterations, while blue lower triangles are placed for every 10%
increase in the size of the feature set ?.
K ` |?| ? Conv. Iters. Red.
5 100 5 0.1 68% 257 (?110) 24%
5 100 5 0.2 0% ? 8%
5 50 5 0.1 80% 123 (? 65) 20%
5 50 5 0.2 10% 436 (?195) 18%
10 50 5 0.1 69% 228 (?164) 18%
10 50 5 0.2 0% ? 8%
10 50 5 0.4 0% ? 3%
10 30 10 0.1 100% 50 (? 69) 13%
10 30 10 0.2 93% 146 (?142) 20%
10 30 10 0.4 0% ? 16%
10 15 20 0.1 100% 26 (? 6) 1%
10 15 20 0.2 98% 43 (? 18) 10%
10 15 20 0.4 63% 289 (?217) 18%
10 15 20 0.8 0% ? 11%
25 15 20 0.1 98% 30 (? 5) 0%
25 15 20 0.2 92% 69 (?112) 6%
25 15 20 0.4 55% 257 (?149) 16%
25 15 20 0.8 0% ? 12%
50 10 10 0.2 68% 84 (?141) 0%
50 10 10 0.4 21% 173 (? 94) 9%
100 10 10 0.2 44% 147 (?220) 0%
100 10 10 0.4 13% 201 (?138) 6%
Table 1: A summary of results for various consensus
problems, as described in ?5.3.
is so small that n-grams are likely to be repeated in
many parts of the strings, and the lack of position in-
formation in our features could make it hard to reach
agreement. Another interesting case is when the in-
put strings have little or nothing in common?can
we still converge to an optimal consensus in a rea-
sonable number of iterations?
We can investigate many different cases by cre-
ating synthetic data, where we tune the number of
input strings K, the length of the strings, the size of
the vocabulary |?|, as well as how similar the strings
are. We do this by randomly generating a base string
x? ? ?` of length `. We then generate K random
strings w1, . . . , wK , each by passing x? through a
noisy edit channel, where each position has inde-
pendent probability ? of making an edit. For each
position in x?, we uniformly sample once among
the three types of edits (substitution, insertion, dele-
tion), and in the case of the first two, we uniformly
sample from the vocabulary (excluding the current
symbol for substitution). The larger ?, the more mu-
tated the strings will be. For small ? or large K, the
optimal consensus ofw1, . . . , wK will usually be x?.
Table 1 shows results under various settings. Each
line presents the percentage of 100 examples that
converge within the iteration limit, the average num-
ber of iterations to convergence (? standard devi-
ation) for those that converged, and the reduction
in the objective value that is obtained over a sim-
ple baseline of choosing the best string in the input
set, to show how much progress the algorithm makes
between the 0th and final iteration.
As expected, a higher mutation probability slows
convergence in all cases, as does having longer in-
put strings. These results also confirm our hypothe-
sis that a small alphabet would lead to slow conver-
gence when using small n-gram features. For these
types of strings, which might show up in biological
data, one would likely need more informative con-
straints than position-agnostic n-grams.
Figure 2 shows example runs on problems gen-
erated at three different parameter settings. We plot
the objective value as a function of runtime, showing
both the primal objective (3) that we hope to mini-
mize, which we measure as the quality of the best
solution among the {xk} that are output at the cur-
239
rent iteration, and the dual objective (5) that our al-
gorithm is maximizing. The dual problem (which is
concave in ?) lower bounds the primal. If the two
functions ever touch, we know the solution to the
dual problem is in the set of feasible solutions to the
original primal problem we are attempting to solve,
and indeed must be optimal. The figure shows that
the dual function always has an initial value of 0,
since we initialize each ?k = 0, and then Fk will
simply return the input wk as its best solution (since
wk has zero distance to itself). As the algorithm be-
gins to enforce the agreement constraints, the value
of the relaxed dual problem gradually worsens, until
it fully satisfies the constraints.
These plots indicate the number of iterations that
have passed and the number of active features. We
see that the time per iteration increases as the num-
ber of features increases, as expected, because more
(and longer) n-grams are being encoded by G.
The three patterns shown are typical of almost all
the trials we examined. When the solution is in the
original input set (a likely occurrence for large K or
small ? ? `), the primal value will be optimal from
the start, and our algorithm only has to prove its op-
timality. For more challenging problems, the primal
solution may jump around in quality at each iteration
before settling into a stable part of the space.
To investigate how different n-gram sizes affect
convergence rates, we experiment with using the en-
tire set of n-grams (for a fixed n) for the duration
of the optimization procedure. Figure 3 shows con-
vergence rates (based on both iterations and run-
time) of different values of n for one set of param-
eters. While bigrams are very fast (average runtime
of 14s among those that converged), this converged
within 1000 iterations only 78% of the time, and
the remaining 22% end up bringing down the av-
erage speed (with an overall average runtime over a
minute). All larger n-grams converged every time;
trigrams had an average runtime of 32s. Our algo-
rithm, which begins with bigrams but brings in more
features (up to 5-grams) as needed, had an average
runtime of 19s (with 98% convergence).
6 Discussion and Future Work
An important (and motivating) property of La-
grangian relaxation methods is the certificate of op-
0 50 100 150 200Number of Iterations0
100
200
300
400
500
Runt
ime 
(s)
K=10,`=15,|?|=20,?=0.2
n=2n=3n=4n=5
Figure 3: Convergence rates for a fixed set of n-grams.
timality. Even in instances where approximate algo-
rithms perform well, it could be useful to have a true
optimality guarantee. For example, our algorithm
can be used to produce reference solutions, which
are important to have for research purposes.
Under a sum-of-pairs Levenshtein objective, the
exact multi-sequence alignment can be directly ob-
tained from the Steiner consensus string and vice
versa (Gusfield, 1997). This implies that our ex-
act algorithm could be also used to find exact multi-
sequence alignments, an important problem in nat-
ural language processing (Barzilay and Lee, 2003)
and computational biology (Durbin et al, 2006) that
is almost always solved with approximate methods.
We have noted that some constraints are more
useful than others. Position-specific information is
hard to agree on and leads to slow convergence,
while pure n-gram constraints do not work as well
for long strings where the position may be impor-
tant. One avenue we are investigating is the use
of a non-deterministic G, which would allow us to
encode latent variables (Dreyer et al, 2008), such
as loosely defined ?regions? within a string, and to
allow for the encoding of alignments between the
input strings. We would also like to extend these
methods to other combinatorial optimization prob-
lems involving strings, such as inference in graphi-
cal models over strings (Dreyer and Eisner, 2009).
To conclude, we have presented a general frame-
work for applying dual decomposition to implicit
WFSA intersection. This could be applied to a num-
ber of NLP problems such as language model and
lattice intersection. To demonstrate its utility on a
large number of automata, we applied it to consen-
sus decoding, determining the true optimum in a rea-
sonable amount of time on a large majority of cases.
240
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 40?47.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the 12th International Con-
ference on Implementation and Application of Au-
tomata, CIAA?07, pages 11?23.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 16?
23.
Peter J. Bickel and Kjell A. Doksum. 2006. Mathemat-
ical Statistics: Basic Ideas and Selected Topics, vol-
ume 1. Pearson Prentice Hall.
Stanley F. Chen, Brian Kingsbury, Lidia Mangu, Daniel
Povey, George Saon, Hagen Soltau, and Geoffrey
Zweig. 2006. Advances in speech transcription at
IBM under the DARPA EARS program. IEEE Trans-
actions on Audio, Speech & Language Processing,
14(5):1596?1608.
Markus Dreyer and Jason Eisner. 2009. Graphical mod-
els over multiple strings. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?09, pages 101?110. As-
sociation for Computational Linguistics.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1080?1089, Honolulu, October.
R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 2006.
Biological Sequence Analysis. Cambridge University
Press.
Vaibhava Goel and William J. Byrne. 2003. Mini-
mum Bayes risk methods in automatic speech recog-
nition. In Wu Chou and Biing-Hwang Juan, editors,
Pattern Recognition in Speech and Language Process-
ing. CRC Press.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational Bi-
ology. Cambridge University Press.
George Karakostas, Richard J Lipton, and Anastasios Vi-
glas. 2003. On the complexity of intersecting finite
state automata and NL versus NP. Theoretical Com-
puter Science, pages 257?274.
Kevin Knight and Yaser Al-Onaizan. 1998. Translation
with finite-state devices. In AMTA?98, pages 421?437.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
Passing revisited. In Computer Vision, 2007. ICCV
2007. IEEE 11th International Conference on, pages
1?8.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1288?1298.
Mehryar Mohri. 2002. Semiring frameworks and algo-
rithms for shortest-distance problems. J. Autom. Lang.
Comb., 7:321?350, January.
Fernando C. N. Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. CoRR.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through La-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 72?82.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 1?11.
Jeong Seop Sim and Kunsoo Park. 2003. The consen-
sus string problem for a metric is NP-complete. J. of
Discrete Algorithms, 1:111?117, February.
H. Soltau, G. Saon, and B. Kingsbury. 2010. The IBM
Attila speech recognition toolkit. In Proc. IEEE Work-
shop on Spoken Language Technology, pages 97?102.
David Sontag, Talya Meltzer, Amir Globerson, Yair
Weiss, and Tommi Jaakkola. 2008. Tightening LP
relaxations for MAP using message-passing. In 24th
Conference in Uncertainty in Artificial Intelligence,
pages 503?510. AUAI Press.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.
James C. Spall. 2003. Introduction to Stochastic Search
and Optimization. John Wiley & Sons, Inc., New
York, NY, USA, 1 edition.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured prediction
models: A large margin approach. In Proceedings of
241
the 22nd international conference on Machine learn-
ing, ICML ?05, pages 896?903.
Geoffrey Zweig, Patrick Nguyen, Dirk Van Compernolle,
Kris Demuynck, Les E. Atlas, Pascal Clark, Gregory
Sell, Meihong Wang, Fei Sha, Hynek Hermansky,
Damianos Karakos, Aren Jansen, Samuel Thomas,
Sivaram G. S. V. S., Sam Bowman, and Justine T. Kao.
2011. Speech recognition with segmental conditional
random fields: A summary of the JHU CLSP 2010
Summer Workshop. In ICASSP.
242
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 783?792,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Shared Components Topic Models
Matthew R. Gormley Mark Dredze Benjamin Van Durme Jason Eisner
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,mdredze,vandurme,jason}@cs.jhu.edu
Abstract
With a few exceptions, extensions to latent
Dirichlet alocation (LDA) have focused on
the distribution over topics for each document.
Much less attention has been given to the un-
derlying structure of the topics themselves. As
a result, most topic models generate topics in-
dependently from a single underlying distri-
bution and require millions of parameters, in
the form of multinomial distributions over the
vocabulary. In this paper, we introduce the
Shared Components Topic Model (SCTM), in
which each topic is a normalized product of a
smaller number of underlying component dis-
tributions. Our model learns these component
distributions and the structure of how to com-
bine subsets of them into topics. The SCTM
can represent topics in a much more compact
representation than LDA and achieves better
perplexity with fewer parameters.
1 Introduction
Topic models are probabilistic graphical models
meant to capture the semantic associations underly-
ing corpora. Since the introduction of latent Dirich-
let alocation (LDA) (Blei et al, 2003), these mod-
els have been extended to account for more complex
distributions over topics, such as adding supervision
(Blei and McAuliffe, 2007), non-parametric priors
(Blei et al, 2004; Teh et al, 2006), topic correla-
tions (Li and McCallum, 2006; Mimno et al, 2007;
Blei and Lafferty, 2006) and sparsity (Williamson et
al., 2010; Eisenstein et al, 2011).
While much research has focused on modeling
distributions over topics, less focus has been given to
the makeup of the topics themselves. This emphasis
leads us to find two problems with LDA and its vari-
ants mentioned above: (1) independently generated
topics and (2) overparameterized models.
Independent Topics In the models above, the top-
ics are modeled as independent draws from a single
underlying distribution, typically a Dirichlet. This
violates the topic modeling community?s intuition
that these distributions over words are often related.
As an example, consider a corpus that supports two
related topics, baseball and hockey. These topics
likely overlap in their allocation of mass to high
probability words (e.g. team, season, game, play-
ers), even though the two topics are unlikely to ap-
pear in the same documents. When topics are gen-
erated independently, the model does not provide a
way to capture this sharing between related topics.
Many extensions to LDA have addressed a related
issue, LDA?s inability to model topic correlation,1
by changing the distributions over topics (Blei and
Lafferty, 2006; Li and McCallum, 2006; Mimno et
al., 2007; Paisley et al, 2011). Yet, none of these
change the underlying structure of the topic?s distri-
butions over words.
Overparameterization Topics are most often
parameterized as multinomial distributions over
words: increasing the topics means learning new
multinomials over large vocabularies, resulting in
models consisting of millions of parameters. This
issue was partially addressed in SAGE (Eisenstein
et al, 2011) by encouraging sparsity in the topics
which are parameterized by their difference in log-
frequencies from a fixed background distribution.
Yet the problem of overparameterization is also tied
1Two correlated topics, e.g. nutrition and exercise, are likely
to co-occur, but their word distributions might not overlap.
783
to the number of topics, and though SAGE reduces
the number of non-zero parameters, it still requires
a vocabulary-sized parameter vector for each topic.
We present the Shared Components Topic Model
(SCTM), which addresses both of these issues by
generating each topic as a normalized product of a
smaller number of underlying components. Rather
than learning each new topic from scratch, we model
a set of underlying component distributions that
constrain topic formation. Each topic can then be
viewed as a combination of these underlying com-
ponents, where in a model such as LDA, we would
say that components and topics stand in a one to one
relationship. The key advantages of the SCTM are
that it can learn and share structure between overlap-
ping topics (e.g. baseball and hockey) and that it can
represent the same number of topics in a much more
compact representation, with far fewer parameters.
Because the topics are products of components,
we present a new training algorithm for the sig-
nificantly more complex product case which re-
lies on a Contrastive Divergence (CD) objective.
Since SCTM topics, which are products of distri-
butions, could be represented directly by distribu-
tions as in LDA, our goal is not necessarily to learn
better topics, but to learn models that are substan-
tially smaller in size and generalize better to unseen
data. Experiments on two corpora show that our
model uses fewer underlying multinomials and still
achieves lower perplexity than LDA, which suggests
that these constraints could lead to better topics.
2 Shared Components Topic Models
The Shared Components Topic Model (SCTM) fol-
lows previous topic models in inducing admixture
distributions of topics that are used to generate each
document. However, here each topic multinomial
distribution over words itself results from a normal-
ized product of shared components, each a multino-
mial over words. Each topic selects a subset of com-
ponents. We begin with a review and then introduce
the SCTM.
Latent Dirichlet alocation (LDA) (Blei et al,
2003) is a probabilistic topic model which defines
a generative process whereby sets of observations
are generated from latent topic distributions. In the
SCTM, we use the same generative process of topic
assignments as LDA, but replace the K indepen-
dently generated topics (multinomials over words)
with products of C components.
Latent Dirichlet alocation generative process
For each topic k ? {1, . . . ,K}:
?k ? Dir(?) [draw distribution over words]
For each document m ? {1, . . . ,M}:
?m ? Dir(?) [draw distribution over topics]
For each word n ? {1, . . . , Nm}:
zmn ? Mult(1,?m) [draw topic]
xmn ? ?zmi [draw word]
LDA draws each topic ?k independently from a
Dirichlet. The model generates each document m
of length M , by first sampling a distribution over
topics ?m. Then, for each word n, a topic zmn is
chosen and a word type xmn is generated from that
topic?s distribution over words ?zmi .
A Product of Experts (PoE) model (Hinton,
1999) is the normalized product of the expert dis-
tributions. In the SCTM, each component (an ex-
pert) models an underlying multinomial word dis-
tribution. We let ?c be the parameters of the cth
component, where ?cv is the probability of the cth
component generating word v. If the structure of a
PoE included only components c ? C in the prod-
uct, it would have the form: p(x|?1, . . . ,?C) =Q
c?C ?cx
PV
v=1
Q
c?C ?cv
, where there are C components, and
the summation in the denominator is over the vocab-
ulary. In a PoE, each component can overrule the
others by giving low probability to some word. A
PoE can be viewed as a soft intersection of its com-
ponents, whereas a mixture is a soft union.
The Beta-Bernoulli model (Griffiths and
Ghahramani, 2006) is a distribution over binary
matrices with a fixed number of rows and columns.
It is the finite counterpart to the Indian Buffet
Process. In this work, we use the Beta-Bernoulli as
our prior for an unobserved binary matrix B with C
columns and K rows. In the SCTM, each row bk of
the matrix, a binary feature vector, defines a topic
distribution. The binary vector acts as a selector
for the structure of the PoE for that topic. The row
determines which components to include in the
product by which entries bkc are ?on? (equal to 1)
in that row. Under Beta-Bernoulli prior, for each
column, a coin with weight pic is chosen. For each
entry in the column, the coin is flipped to determine
if the entry is ?on? or ?off?. This corresponds to
784
the notion that some components are a priori more
likely to be included in topics.
The Beta-Bernoulli model generative process
For each component c ? {1, . . . , C}: [columns]
pic ? Beta(
?
C , 1) [draw probability of component c]
For each topic k ? {1, . . . ,K}: [rows]
bkc ? Bernoulli(pic) [draw whether topic includes cth
component in its PoE]
2.1 Shared Components Topic Models
The Shared Components Topic Model generates
each document just like LDA, the only difference
is the topics are not drawn independently from a
Dirichlet prior. Instead, topics are soft intersections
of underlying components, each of which is a multi-
nomial distribution over words. These components
are combined via a PoE model, and each topic is
constructed according to a length C binary vector
bk; where bkc = 1 includes and bkc = 0 excludes
component c. Stacking theK vectors forms aK?C
matrix; rows correspond to topics and columns to
components. Overlapping topics share components
in common.
Generative process SCTM?s generative process
generates topics and words, but must also generate
the binary matrix. For each of the C shared com-
ponents, we generate a distribution ?c over the V
words from a Dirichlet parametrized by ?. Next,
we generate a K ? C binary matrix using the Beta-
Bernoulli prior. These components and the binary
matrix implicitly define the complete set of K topic
distributions, each of which is a PoE.
p(x|bk,?) =
?C
c=1 ?
bkc
cx
?V
v=1
?C
c=1 ?
bkc
cv
(1)
The distribution p(?|bk,?) defines the kth topic.
Conditioned on these K topics, the remainder of the
generative process, which generates the documents,
is just like LDA.
The Shared Components Topic Model generative process
For each component c ? {1, . . . , C}:
?c ? Dir(?) [draw distribution over words]
pic ? Beta(
?
C , 1) [draw probability of component c]
For each topic k ? {1, . . . ,K}:
bkc ? Bernoulli(pic) [draw whether topic includes cth
component in its PoE]
For each document m ? {1, . . . ,M}
?m ? Dir(?) [draw distribution over topics]
For each word n ? {1, . . . , Nm}
zmn ? Mult(1,?m) [draw topic]
xmn ? p(? |bzmn ,?) given by Eq. (1) [draw word]
See Figure 1 for the graphical model.
Discussion An advantage of this formulation is the
ability to model many topics using few components.
While LDA must maintain V ?K parameters for the
topic distributions, the SCTM maintains just V ?C
parameters, plus an additional K?C binary matrix.
Since C < K  V this results in many fewer pa-
rameters for the SCTM.2 Extending the number of
topics (rows) requires storing additional binary vec-
tors, a lightweight requirement. In theory, we could
enable all 2C possible component combinations, al-
though we expect to use far less. On the other hand,
constraining the SCTM?s topics by the components
gives less flexible topics as compared to LDA. How-
ever, we find empirically that a large number of top-
ics can be effectively modeled with a smaller num-
ber of components.
Observe that we can reparameterize the SCTM as
LDA by assuming an identity square matrix; each
component corresponds to a topic in LDA, making
LDA a special case of the SCTM with an identity
matrix IC . Intuitively, SCTM learning could pro-
duce an LDA model where appropriate. Finally, we
can also think of the SCTM as learning the struc-
ture of many PoE models. In applications where ex-
perts abstain, the SCTM could learn in which setting
(row) each expert casts a vote.
3 Parameter Estimation
Parameter estimation infers values for model pa-
rameters ?, pi, and ? from data using an unsuper-
vised training procedure. Because exact inference
is intractable in the SCTM, we turn to approximate
methods. As is common in these models, we will
integrate out pi and ?, sample latent variables Z and
B, and optimize the components ?. Our algorithm
follows the outline of the Monte Carlo EM (MCEM)
algorithm (Wei and Tanner, 1990). In the Monte
Carlo E-step, we will re-sample the latent variables
Z and B based on current model parameters ? and
observed data X . In the M-step, we will find new
model parameters ?. Since these parameters corre-
spond to experts in the PoE, we rely on a contrastive
divergence (CD) objective (Hinton, 2002), popular
for PoE training, rather than maximizing the data
2The vocabulary size V could be much larger if n-grams or
relational triples are used, as opposed to unigrams.
785
log-likelihood. Normally, CD only estimates the pa-
rameters of the expert distributions. However, in our
model, the structure of the PoEs themselves change
based on the E-step. Since we generate multiple
samples in the E-step, we modify the CD objective
to compute the gradient for each E-step sample and
take the average to approximate the expectation un-
der B and Z.3
3.1 E-Step
The E-step approximates an expectation under
p(B,Z|X,?,?, ?) for latent topic assignments Z
and matrix B using Gibbs sampling. The Gibbs
sampler uses the full conditionals for both zi (7) and
bkc (12), which we derive in Appendix A. Using this
sampler, we obtain J samples of Z and B by iterat-
ing through each value of zi and bkc J times (in our
experiments, we use J=1, which appears to work as
well on this task as multiple samples). These J sam-
ples are then used in the M-step as an approximation
of the expectation of the latent variables.
3.2 M-Step
Given many samples of B and Z, the M-step opti-
mizes the component parameters ? which cannot be
collapsed out. We utilize the standard PoE training
procedure for experts: contrastive divergence (CD).
We approximate the CD gradient as the difference of
the data distribution and the one-step reconstruction
of the data according to the current parameters. As
in Generalized EM (Dempster et al, 1977), a single
gradient step in the direction of the contrastive di-
vergence objective is sufficient for each M-step. A
key difference in our model is that we must incor-
porate the expectation of the PoE model structure,
which in our case is a random variable instead of a
fixed observed structure. We achieve this by simply
3CD training within MCEM is not the only possible ap-
proach. One alternative would be to compute the CD gradient
summing over all values of B and Z, effectively training the
entire model using CD. This approach prevents the normal CD
objective derivation from being simplified into a more tractable
form. Another approach would be a pure MCMC algorithm,
which sampled ? directly. While using the natural parameters
allows the sampler to mix, it is too computationally intensive to
be practical. Finally, we could train with Generalized MCEM,
where the exact gradient of the log-likelihood (or log-posterior)
is used, but this easily gets stuck in local minima. After exper-
imenting with these and other options, we present our current
most effective estimation method.
computing the CD gradient for each PoE given each
of the J samples {Z,B}(j) from the E-Step, then
average the result.
Another difficulty arises from computing the gra-
dient directly for the multinomial?c due to the V ?1
degrees of freedom imposed by sum-to-one con-
straints. Therefore, we switch to the natural pa-
rameters, which obviates the need for considering
the sum-to-one constraint in the optimization, by
defining ?c in terms of V real valued parameters
{?c1, . . . , ?cV }:
?cv =
exp(?cv)
?V
t=1 exp(?cv)
(2)
The V parameters ?cv are then used to compute ?cv
for use in the E-step.
As explained above, the M-step does not maxi-
mize the data log-likelihood, but instead minimizes
contrastive divergence. Hinton (2002) explains that
maximizing data log-likelihood is equivalent to min-
imizing Q0||Q?? , the KL divergence between the
observed data distribution, Q0, and the model?s
equilibrium distribution,Q?? .
4 MinimizingQ0||Q??
would require the computation of an intractable ex-
pectation under the equilibrium distribution. We
avoid this by instead minimizing the contrastive di-
vergence objective,
CD(?|{Z,B}(j)) = Q0||Q?? ?Q
1
? ||Q
?
? , (3)
where Q1? is the distribution over one-step recon-
structions of the data, X given Z,B, ?, that are gen-
erated by a single step of Gibbs sampling.
Unlike standard applications of CD training, the
hidden variables (Z,B) are not contained within the
experts. Instead they define the structure of the PoE
model, where B indicates which experts to use in
each product (topic) andZ indicates which PoE gen-
erates each word. Unfortunately, CD training cannot
infer this structure since the CD derivation makes
use of a fixed structure in the one-step reconstruc-
tion. Therefore, we have taken a MCEM approach,
first sampling the PoE structure in the E-step, then
4Hinton (2002) used this notation because the data distribu-
tion,Q0, can be described as the state of a Markov chain at time
0 that was started at the data distribution. Similarly, the equilib-
rium distribution, Q?? could be obtained by running the same
Markov chain to time?.
786
M
Nm
C
K
xmn
zmn
?m
?
?c
bkc
pic
?
?
Figure 1: The graphical model for the SCTM.
fixing these samples for Z and B when computing
the one-step reconstruction of the data, X .
Contrastive Divergence Gradient We provide
the approximate derivative of the contrastive di-
vergence objective, where Z and B are treated as
fixed.5
dCD(?|{Z,B}(j))
d?
? ?
?
d log f(x|bz, ?)
d?
?
Q0
+
?
d log f(x|bz, ?)
d?
?
Q1?
where f(x|bz, ?) =
?C
c=1 ?
bzc
cx is the numerator of
p(x|bz, ?) and the derivative of its log is efficient to
compute:
d log f(x|bz, ?)
d?cv
=
{
bzc(1? ?cv) for x = v
?bzc?cv for x 6= v
To approximate the expectation under Q1? , we hold
Z,B, ? fixed and resample the data, X , using one
step of Gibbs sampling.
3.3 Summary
Our learning algorithm can be viewed
in terms of a Q function: Q(?|?(t)) ?
1
J
?J
j=1 CD(?|{Z,B}
(j))where we average over
J samples. The E-step computes Q(?|?(t)). The
M-step minimizes Q with respect to ? to obtain the
updated ?(t+1) by performing gradient descent on
the Q function as ?(t+1)cv = ?
(t)
cv ? ? ?
dQ(?|?(t))
d?cv
for
all values of c, v.
5The derivative is approximate because we drop the term:
?
dQ1?
d? ?
dQ1?||Q
?
?
dQ1?
, which is ?problematic to compute? (Hinton,
2002). This is the standard use of CD.
Algorithm 1 SCTM Training
Initialize parameters: ?c, bkc, zi.
while not converged do
{E-step:}
for j = 1 to J do
{Draw jth sample {Z,B}(j)}
for i = 1 to N do
Sample zi using Eq. (7)
for k = 1 to K do
for c = 1 to C do
Sample bkc using ratio in Eq. (12)
{M-step:}
for c = 1 to C do
for v = 1 to V do
Single gradient step over ?
?(t+1)cv = ?
(t)
cv ? ? ?
dQ(?|?(t))
d?cv
4 Related Models
The SCTM is closely related to the the Infinite
Overlapping Mixture Model (IOMM) (Heller and
Ghahramani, 2007), yet our model differs from and,
in some ways, extends theirs. The IOMM mod-
els the geometric overlap of Gaussian clusters us-
ing PoEs, and models the structure of the PoEs with
the rows of a binary matrix. The SCTM models a
finite number of columns, where the IOMM mod-
els an infinite number. The IOMM generates a row
for each data point, whereas the SCTM generates a
row for each topic. Thus, the SCTM goes beyond
the IOMM by allowing the rows to be shared among
documents and models document-specific mixtures
over the rows of the matrix.6
SAGE for topic modeling (Eisenstein et al, 2011)
can be viewed as a restricted form of the SCTM.
Consider an SCTM in which the binary matrix is re-
stricted such that the first column, b?,1, consists of
all ones and the remainder forms a diagonal matrix.
If we then set the first component, ?1, to the cor-
pus background distribution, and add a Laplace prior
on the natural parameters, ?cv, we have the SAGE
model. Note that by removing the restriction that
the matrix contain a diagonal, we could allow mul-
tiple components to combine in the SCTM fashion,
while incorporating SAGE?s sparsity benefits.
6The IOMM uses Metropolis-Hastings (MH) to sample the
parameters of the experts. This approach is computationally
feasible because their experts are Gaussian, unlike the SCTM
in which the experts are multinomials and the MH step too ex-
pensive.
787
The relation of TagLDA (Zhu et al, 2006) to
the SCTM is similar to that of SAGE and SCTM.
TagLDA has a PoE of exactly two experts: one ex-
pert for the topic, and one for the supervised word-
level tag. Examples of tags are abstract or body,
indicating which part of a research paper the word
appears in.
Unlike the SCTM and SAGE, most prior exten-
sions to LDA have enhanced the distribution over
topics for each document. One of the closest is hier-
archical LDA (hLDA) (Blei et al, 2004) and its ap-
plication to PAM (Mimno et al, 2007). Though top-
ics are still generated independently from a Dirich-
let prior, hLDA learns a tree structure underlying
the topics. Each document samples a single path
through the tree and samples words from topics
along that path. The SCTM models an orthogonal
issue to topic hierarchy: how the topics themselves
are represented as the intersection of components.
Finally, while prior work has primarily used mix-
tures for the sake of conjugacy, we take a fundamen-
tally different approach to modeling the structure by
using normalized product distributions.
5 Evaluation
We compare the SCTM with LDA in terms of over-
all model performance (held-out perplexity) as well
as parameter usage (varying numbers of components
and topics). We select LDA as our baseline since our
model differs only in how it forms topics, which fo-
cuses evaluation on the benefit of this model change.
We consider two popular data sets for compar-
ison: NIPS: A collection of 1,617 NIPS abstracts
from 1987 to 19997, with 77,952 tokens and 1,632
types. 20NEWS: 1,000 randomly selected articles
from the 20 Newsgroups dataset,8 with 70,011 to-
kens and 1,722 types. Both data sets excluded stop
words and words occurring in fewer than 10 docu-
ments. For 20NEWS, we used the standard by-date
train/test split. For NIPS, we randomly partitioned
the data by document into 75% train and 25% test.
We compare the SCTM to LDA by evaluating
the average perplexity-per-word of the held-out test
7We follow prior work (Blei et al, 2004; Li and Mc-
Callum, 2006; Li et al, 2007) in using only the abstracts:
http://www.cs.nyu.edu/?roweis/data.html
8Williamson et al (2010) created a similar subset:
http://people.csail.mit.edu/jrennie/20Newsgroups/
data, perplexity = 2? log2(data|model)/N . Exact com-
putation is intractable, so we use the left-to-right al-
gorithm (Wallach et al, 2009) as an accurate alter-
native. With the topics fixed, the SCTM is equiva-
lent to LDA and requires no adaptation of the left-
to-right algorithm.
We used a collapsed Gibbs sampler for training
LDA and the algorithm described above for training
the SCTM. Both were trained for 4000 iterations,
sampling topics every 10 iterations after a burn-in of
3000. The hyperparameter ? was optimized as an
asymmetric Dirichlet, ? as a symmetric Dirichlet,
and ? = 3.0 was fixed.9 Following the observation of
Hinton (2002) that CD training benefits from initial-
izing the experts to nearly uniform distributions, we
initialize the component distributions from a sym-
metric Dirichlet with parameter ?? = 1?106. We use
J = 1 samples per iteration and a decaying learning
rate centered at ? = 100.10 We ranged LDA from 10
to 200 topics, and the SCTM from 10 to 100 com-
ponents (C). We then selected the number of SCTM
topics (K) as K ? {C, 2C, 3C, 4C, 5C}. For each
model, we used five random restarts, selecting the
model with the highest training data likelihood.
5.1 Results
Our goal is to demonstrate that (1) modeling topics
as products of components is an expressive alterna-
tive to generating topics independently and (2) the
SCTM can both achieve lower perplexity than LDA
and use fewer model parameters in doing so.
Topics as Products of Components Figures 3b
and 3c show the perplexity for the held-out portions
of 20NEWS and NIPS for different numbers of com-
ponents C. The shaded region shows the full SCTM
perplexity range we observed for different K and
at each value of C, we label the number of topics
K (rows in the binary matrix). For each number of
components, LDA falls within the upper portion of
the shaded region. While for some (small) values of
K for the SCTM, LDA does better, the SCTM can
easily include more K (requiring few new param-
eters) to achieve better results. This supports our
hypothesis that topics can be comprised of the over-
lap between shared underlying components. More-
9On development data the model was rather insensitive to ?.
10We experimented with larger J but it had no effect.
788
Figure 2: SCTM binary matrix and topics from 3599 training documents of 20NEWS for C = 10, K = 20. Blue
squares are ?on? (equal to 1).
x
y
5
10
15
20
2 4 6 8 10
k ?k Top words for topic Top words for topic after ablating component c=1
? 1 0.306 subject organization israel return define law org organization subject israel law peace define israeli
? 2 0.031 encryption chip clipper keys des escrow security law administration president year market money senior
? 3 0.025 turkish armenian armenians war turkey turks armenia years food center year air russian war army
? 4 0.102 drive card disk scsi hard controller mac drives opinions drive hard power support cost research price
? 5 0.071 image jpeg window display code gif color mit pitt file program year center programs image division
? 6 0.018 jews israeli jewish arab peace land war arabs
? 7 0.074 org money back question years thing things point
? 8 0.106 christian bible church question christ christians life
? 9 0.011 administration president year market money senior
? 10 0.055 health medical center research information april
? 11 0.063 gun law state guns control bill rights states
? 12 0.160 world organization system israel state usa cwru reply
? 13 0.042 space nasa gov launch power wire ground air
? 14 0.038 space nasa gov launch power wire ground air
? 15 0.079 team game year play games season players hockey
? 16 0.158 car lines dod bike good uiuc sun cars
? 17 0.136 windows file government key jesus system program
? 18 0.122 article writes center page harvard virginia research
? 19 0.017 max output access digex int entry col line
? 20 0.380 lines people don university posting host nntp time # of Model Parameters (thousands)
Perp
lexi
ty
800
1000
1200
1400
l
l
l
l
l
l
l
l
l
10
100
11
120 140
201
40
60 80
10,2010,30
10,4010,50
100,200
100,300
100,400100,500
20,100
20,40
20,60
20,80
40,120
40,16040,200
40,80
60,120
60,180
60,24060,300
80,160
80,240
80,32080,400
0 100 200 300 400 500 600
l LDASCTM
(a)
# of Components
Perp
lexi
ty
800
1000
1200
1400
1600
1800
l
l
l
ll
l
l
l
10
2030
4050
100
200
300400500
100
20
40
6080
120
160200
40
80
120
180240300
60
160
240320400
80
0 20 40 60 80 100
l LDASCTM
(b)
# of Components
Perp
lexi
ty
300
400
500
600
700
l
l
l
ll
l
l
l
10
20
304050
100
2003400500
100
20
40
60
80
120160200
40
80
120
180240300
60
160240320400
80
0 20 40 60 80 100
l LDASCTM
(c)
# of Model Parameters (thousands)
Perp
lexit
y
300
350
400
450
500
550
600
l
l
l
l l l l
l
l
l
l
l
l
10
100
11
120 140 160 180
20
200
21
40
60
80
10,20
10,3010,4010,50
100,200100,300100,400100,500
20,100
20,40
20,60
20,80
40,120
40,16040,200
40,80
60,120
60,18060,24060,3 0
80,16080,240
80,32080,400
0 100 200 300 400
l LDASCTM
(d)
Figure 3: Perplexity results on held-out data for 20NEWS (b) and NIPS (c) showing the results of LDA and the SCTM
for the same number of components and varying K (SCTM). For the same number of components (multinomials), the
SCTM achieves lower perplexity by combining them into more topics. Results for 20NEWS (a) and NIPS (d) showing
non-square SCTM achieves lower perplexity than LDA with a more compact model.
over, this suggests that our products (PoEs) provide
additional and complementary expressivity over just
mixtures of topics.
Model Compactness Including an additional
topic in the SCTM only adds C binary parameters,
for an extra row in the matrix. Whereas in LDA, an
additional topic requires V (the size of the vocab-
ulary) additional parameters to represent the multi-
nomial. In both cases, the number of document-
specific parameters must increase as well. Figures
3a and 3d present held-out perplexity vs. number
of model parameters on 20NEWS and NIPS, exclud-
ing the case of square (C = K) binary matrices for
the SCTM. The regions show a confidence inter-
val (p = 0.05) around the smoothed fit to the data,
LDA labels show C, and SCTM labels show C,K.
The SCTM achieves lower perplexity with fewer
model parameters, even when the increase in non-
component parameters is taken into account. We ex-
pect that because of its smaller size the SCTM ex-
hibits lower sample complexity, allowing for better
generalization to unseen data.
5.2 Analysis
Figure 2 gives the binary matrix and topics learned
on a larger section of 20NEWS training documents.
These topics evidence that the SCTM is able to
achieve a diversity of topics by combining various
subsets of components, and we expect that the low
perplexity achieved by the SCTM can be attributed
789
k=12 ?k=0.13
problem statecontrolreinforcementproblems modelstime baseddecision markovsystems function
k=11 ?k=0.08
learningnetworks systemrecognition timenetworkdescribes handcontext viewsclassification
k=14 ?k=0.07
models imagesimage problemstructureanalysis mixtureclusteringapproach showcomputational
k=13 ?k=0.05
networksnetwork learningdistributedsystem weightvectors propertybinary pointoptimal real
k=16 ?k=0.11
training unitspaper hiddennumber outputproblem rule setorder unit showpresent methodweights task
k=15 ?k=0.12
cells neuronsvisual cortexmotion responseprocessingspatial cellpropertiespatterns spike
k=18 ?k=0.07
informationanalysiscomponent rulessignalindependentrepresentationsnoise basis
k=17 ?k=0.10
numberfunctionsweights functionlayergeneralizationerror resultsloss linear size
k=20 ?k=0.02
time networkweightsactivation delaycurrent chaoticconnecteddiscreteconnections
k=19 ?k=0.03
system networksset neuronsvisual phasefeatureprocessingfeatures outputassociative
c=1
modelinformationparameterskalman robustmatriceslikelihoodexperimentally
c=2
networknetworks datalearning optimallinear vectorindependentbinary naturalalgorithms pca
c=4
paper unitsoutput layernetworkspatterns unitpattern set rulenetwork rulesweights training
c=9
visual imageimages cellscortex scenesupport spatialfeature visioncues stimulusstatistics
k=10 ?k=0.09
neural neuronsanalog synapticneuron networksmemory timecapacity modelassociativenoise dynamics
k=9 ?k=0.02
vector featureclassificationsupport vectorskernelregressionweight inputsdimensionality
k=2 ?k=0.13
network inputinformation timerecurrent backpropagationunitsarchitectureforward layer
k=1 ?k=0.11
model learningsysteminformationparametersnetworks robustkalman rulesestimation
k=4 ?k=0.12
bayesianresults showestimationmethod basedparameterslikelihoodmethods models
k=3 ?k=0.06
objectrecognitionsystem objectsinformationvisual matchingproblem basedclassification
k=6 ?k=0.23
neural networkpaperrecognitionspeech systemsbased resultsperformanceartificial
k=5 ?k=0.04
objectrecognitionsystem objectsinformationvisual matchingproblem basedclassification
k=8 ?k=0.23
algorithmtraining errorfunction methodperformanceinputclassificationclassifier
k=7 ?k=0.08
data papernetworks networkoutput featurefeaturespatterns settrain introducedunit functions
Figure 4: Hasse diagram on NIPS for C = 10, K = 20 showing the top words for topics and unrepresented com-
ponents (in shaded box). Notice that some topics only consist of a single component. The shaded box contains the
components that didn?t appear as a topic. For the sake of clarity, we only show arrows for the subsumption rela-
tionships between the topics, and we omit the implicit arrows between the components in the shaded box and the
topics.
to the high-level of component re-use across topics.
Topics are typically interpreted by looking at the
top-N words, whereas the top-N words of a compo-
nent often do not even appear in the topics to which
it contributes. Instead, we find that the components
contribution to a topic is typically through vetoing
words. For example, the top words of component
c=1, corresponding to the first column of the binary
matrix in figure 2, are [subject organization posting apple mit
screen write window video port], yet only a few of these ap-
pear in topics k=1,2,3,4,5, which use it.
On the right of figure 2, we show what the top-
ics become when we ablate component c=1 from
the matrix by setting the column to all zeros. Topic
k=2 changes from being about information security
to general politics and is identical to k=9. Topic k=3
changes from the Turkish-Armenian War to a more
general war topic. Topic k=4 changes to a less fo-
cused version of itself. In this way, we can gain fur-
ther insight into the contribution of this component,
and the way in which components tend to increase
the specificity of a topic to which they are added.
The SCTM learns each topic as a soft intersec-
tion of its components, as represented by the binary
matrix. We can describe the overlap between topics
based on the components that they have in common.
One topic subsumes another topic when the parent
consists of a subset of the child?s components. In
this way, the binary matrix defines a Hasse diagram,
a directed acyclic graph describing all the subsump-
tion relationships between topics. Figure 4 shows
such a Hasse diagram on the NIPS data. Several top-
ics consist of only a single component, such as k=12
on reinforcement learning and k=8 on optimization.
These two topics combine with the component c=1
so that their overlap forms the topic k=4 on Bayesian
methods. These subsumption relationships are dif-
ferent from and complementary to hLDA (see ?4),
which models topic co-occurrence, not component
intersection. For example, topic k=10 on connec-
tionism and k=2 on neural networks intersect to
form k=20 which contains words that would only
appear in both of its subsuming topics, thereby ex-
plicitly modeling topic overlap.
790
The SCTM sometimes learns identical topics (two
rows with the same binary entries ?on?) such as
k=13 and k=14 in figure 2 and k=3 and k=5 in fig-
ure 4, which is likely due to the Gibbs sampler for
the binary matrix getting stuck in a local optimum.
6 Discussion
We have presented the Shared Components Topic
Model (SCTM), in which topics are products of
underlying component distributions. This model
change learns shared topic structures?as expressed
through components?as opposed to generating
each topic independently. Reducing the number of
components yields more compact models with lower
perplexity than LDA. The two main limitations of
the current SCTM are, when restricted to a square
binary matrix (C = K), the inference procedure is
unable to recover a model with perplexity as low as
a collapsed Gibbs sampler for LDA, and the compo-
nents are not consistently interpretable.
The use of components opens up interesting di-
rections of research. For example, task specific side
information can be expressed as priors or constraints
over the components, or by adding conditioning
variables tied to the components. Additionally, tasks
beyond document modeling may benefit from repre-
senting topics as products of distributions. For ex-
ample, in vision, where topics are classes of objects,
the components could be features of those objects.
For selectional preference, components could cor-
respond to semantic features that intersect to define
semantic classes (Gormley et al, 2011). We hope
new opportunities will arise as this work explores a
new research area for topic models.
Appendix A: Derivation of Full Conditionals
The model?s complete data likelihood over all
variables?observed words X , latent topic assign-
ments Z, matrix B, and component/expert distribu-
tions ?:
p(X,Z,B,?|?,?, ?) =
p(X|Z,B,?)p(Z|?)p(B|?)p(?|?) (4)
This follows from the conditional independence as-
sumptions. It is tractable to integrate out all parame-
ters except Z,B,? and hyperparameters ?,?, ?. 11
11For simplicity, we switch from indexing examples as xmn
to xi. In this presentation, xi is the ith example in the corpus,
Full conditional of zi Recall that p(Z|?) is
the Dirichlet-Multinomial distribution over topic
assignments, where ? has been integrated out.
The form of this distribution is identical to the
corresponding distribution over topics in LDA.
The derivation of the full conditional of zi ?
{1, . . . ,K}, follows from the factorization in Eq. 4:
p(zi|X,Z
?(i),B,?,?,?, ?) (5)
? p(X|Z,B,?)p(Z|?) (6)
? p(xi|bzi ,?)(n?
?(i)
mzi + ?zi) (7)
Z?(i) is the set of all topic assignments except zi.
We use the independence of each document, recall-
ing that example i belongs to document m. In prac-
tice, we cache p(x|bz,?) for all x, z (V ?K values)
and these are shared by all zi in a sampling iteration.
Above, just as in LDA, p(Z|?) is simplified by
proportionality to (n??(i)mzi + ?zi), where n?
?(i)
mk is the
count of examples for document m that are assigned
topic k excluding zi?s contribution (Heinrich, 2008).
Full conditional of bkc Recall that p(B|?) is the
prior for a Beta-Bernoulli matrix. The full condi-
tional distribution of a position in the binary vector
is (Griffiths and Ghahramani, 2006):
p(bkc = 1|B
?(kc), ?) =
n??(k)c +
?
C
K + ?C
(8)
where n??(k)c is the count of topics with component
c excluding topic k, and B?(kc) is the entire matrix
except for the entry bkc.
To find the full conditional for bkc ? {0, 1}, we
again start with the factorization from Eq. 4.
p(bkc|X,Z,B
?(kc),?,?,?, ?) (9)
? p(X|Z,B,?)p(B|?) (10)
?
[
?
i:zi=k
p(xi|bzi ,?)
]
p(bkc|B
?(kc), ?) (11)
where p(bkc|B?(kc), ?) is given by Eq. 8,
=
?
?
?
(?V
v=1 ?
n?kv
cv
)bkc
(?V
v=1
?C
j=1 ?
bkj
jv
)?||n?k||1
?
?
? p(bkc|B
?(kc), ?)
(12)
and where n?kv is the count of words assigned topic
k that are type v, and ||n?k||1 (the L1-norm of count
vector n?k) is the count of all words with topic k.
which corresponds to some m,n pair.
791
References
David Blei and John Lafferty. 2006. Correlated topic
models. In Advances in Neural Information Process-
ing Systems (NIPS), volume 18.
David Blei and Jon McAuliffe. 2007. Supervised topic
models. In Advances in Neural Information Process-
ing Systems (NIPS).
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet alocation. Journal of Machine Learning
Research, 3.
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2004. Hierarchical topic models
and the nested chinese restaurant process. In Advances
in Neural Information Processing Systems (NIPS), vol-
ume 16.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Interna-
tional Conference on Machine Learning (ICML).
Matthew R. Gormley, Mark Dredze, Benjamin Van
Durme, and Jason Eisner. 2011. Shared components
topic models with application to selectional prefer-
ence. In Learning Semantics Workshop at NIPS 2011,
December.
Thomas Griffiths and Zoubin Ghahramani. 2006. Infinite
latent feature models and the indian buffet process. In
Advances in Neural Information Processing Systems
(NIPS), volume 18.
Gregor Heinrich. 2008. Parameter estimation for text
analysis. Technical report, Fraunhofer IGD.
Katherine A. Heller and Zoubin Ghahramani. 2007. A
nonparametric bayesian approach to modeling over-
lapping clusters. In Artificial Intelligence and Statis-
tics (AISTATS), pages 187?194.
Geoffrey Hinton. 1999. Products of experts. In In-
ternational Conference on Artificial Neural Networks
(ICANN).
Geoffrey Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural Computa-
tion, 14(8):1771?1800.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In International Conference on Machine Learn-
ing (ICML), pages 577?584.
Wei Li, David Blei, and Andrew McCallum. 2007. Non-
parametric bayes pachinko allocation. In Uncertainty
in Artificial Intelligence (UAI).
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with pachinko alloca-
tion. In International Conference on Machine Learn-
ing (ICML), pages 633?640.
John Paisley, Chong Wang, and David Blei. 2011. The
discrete infinite logistic normal distribution for Mixed-
Membership modeling. In International Conference
on Artificial Intelligence and Statistics (AISTATS).
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Hanna Wallach, Ian Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In International Conference on Machine
Learning (ICML), pages 1105?1112.
Greg Wei and Martin Tanner. 1990. A monte carlo im-
plementation of the EM algorithm and the poor man?s
data augmentation algorithms. Journal of the Ameri-
can Statistical Association, 85(411):699?704.
Sinead Williamson, Chong Wang, Katherine Heller, and
David Blei. 2010. The IBP compound dirichlet
process and its application to focused topic model-
ing. In International Conference on Machine Learn-
ing (ICML).
Xiaojin Zhu, David Blei, and John Lafferty. 2006.
TagLDA: bringing document structure knowledge into
topic models. Technical Report TR-1553, University
of Wisconsin.
792
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 444?454,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Nonconvex Global Optimization for Latent-Variable Models?
Matthew R. Gormley Jason Eisner
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,jason}@cs.jhu.edu
Abstract
Many models in NLP involve latent vari-
ables, such as unknown parses, tags, or
alignments. Finding the optimal model pa-
rameters is then usually a difficult noncon-
vex optimization problem. The usual prac-
tice is to settle for local optimization meth-
ods such as EM or gradient ascent.
We explore how one might instead search
for a global optimum in parameter space,
using branch-and-bound. Our method
would eventually find the global maxi-
mum (up to a user-specified ) if run for
long enough, but at any point can return
a suboptimal solution together with an up-
per bound on the global maximum.
As an illustrative case, we study a gener-
ative model for dependency parsing. We
search for the maximum-likelihood model
parameters and corpus parse, subject to
posterior constraints. We show how to for-
mulate this as a mixed integer quadratic
programming problem with nonlinear con-
straints. We use the Reformulation Lin-
earization Technique to produce convex
relaxations during branch-and-bound. Al-
though these techniques do not yet pro-
vide a practical solution to our instance
of this NP-hard problem, they sometimes
find better solutions than Viterbi EM with
random restarts, in the same time.
1 Introduction
Rich models with latent linguistic variables are
popular in computational linguistics, but in gen-
eral it is not known how to find their optimal pa-
rameters. In this paper, we present some ?new? at-
tacks for this common optimization setting, drawn
from the mathematical programming toolbox.
We focus on the well-studied but unsolved task
of unsupervised dependency parsing (i.e., depen-
?This research was partially funded by the JHU Human
Language Technology Center of Excellence.
-180.2 
-231.0 -254.3 
-387.1 -287.3 -311.1 -467.5 
-298 -342 
!5 " -0.6 -0.6 " !5 
!5 " -2 -2 " !5 !3 " -0.6 -0.6 " !3 
!2 " -0.6 -0.6 " !2 
Branch-and-bound tree: Incumbent solution: 
$ $
M?
m=1
?mf?m = ?400
? = [?0.1,?0.6,?20,?1.3, . . . ]
f = [4, 0, 2, 21, . . .]
i=1
? 4 0
Figure 1: Each node contains a local upper bound
for its subspace, computed by a relaxation. The
node branches on a single model parameter ?m to
partition its subspace. The lower bound, -400, is
given by the best solution seen so far, the incum-
bent. The upper bound, -298, is the min of all re-
maining leaf nodes. The node with a local bound
of -467.5 can be pruned because no solution within
its subspace could be better than the incumbent.
dency grammar induction). This may be a par-
ticularly hard case, but its structure is typical.
Many parameter estimation techniques have been
attempted, including expectation-maximization
(EM) (Klein and Manning, 2004; Spitkovsky et
al., 2010a), contrastive estimation (Smith and Eis-
ner, 2006; Smith, 2006), Viterbi EM (Spitkovsky
et al, 2010b), and variational EM (Naseem et al,
2010; Cohen et al, 2009; Cohen and Smith, 2009).
These are all local search techniques, which im-
prove the parameters by hill-climbing.
The problem with local search is that it gets
stuck in local optima. This is evident for gram-
mar induction. An algorithm such as EM will find
numerous different solutions when randomly ini-
tialized to different points (Charniak, 1993; Smith,
2006). A variety of ways to find better local op-
tima have been explored, including heuristic ini-
tialization of the model parameters (Spitkovsky
et al, 2010a), random restarts (Smith, 2006),
and annealing (Smith and Eisner, 2006; Smith,
2006). Others have achieved accuracy improve-
ments by enforcing linguistically motivated pos-
terior constraints on the parameters (Gillenwater
et al, 2010; Naseem et al, 2010), such as requir-
ing most sentences to have verbs or encouraging
nouns to be children of verbs or prepositions.
We introduce a method that performs global
444
search with certificates of -optimality for both
the corpus parse and the model parameters. Our
search objective is log-likelihood. We can also im-
pose posterior constraints on the latent structure.
As we show, maximizing the joint log-
likelihood of the parses and the parameters can be
formulated as a mathematical program (MP) with
a nonconvex quadratic objective and with integer
linear and nonlinear constraints. Note that this ob-
jective is that of hard (Viterbi) EM?we do not
marginalize over the parses as in classical EM.1
To globally optimize the objective function,
we employ a branch-and-bound algorithm that
searches the continuous space of the model param-
eters by branching on individual parameters (see
Figure 1). Thus, our branch-and-bound tree serves
to recursively subdivide the global parameter hy-
percube. Each node represents a search problem
over one of the resulting boxes (i.e., orthotopes).
The crucial step is to prune nodes high in the
tree by determining that their boxes cannot contain
the global maximum. We compute an upper bound
at each node by solving a relaxed maximization
problem tailored to its box. If this upper bound is
worse than our current best solution, we can prune
the node. If not, we split the box again via another
branching decision and retry on the two halves.
At each node, our relaxation derives a linear
programming problem (LP) that can be efficiently
solved by the dual simplex method. First, we lin-
early relax the constraints that grammar rule prob-
abilities sum to 1?these constraints are nonlin-
ear in our parameters, which are log-probabilities.
Second, we linearize the quadratic objective by ap-
plying the Reformulation Linearization Technique
(RLT) (Sherali and Adams, 1990), a method of
forming tight linear relaxations of various types
of MPs: the reformulation step multiplies together
pairs of the original linear constraints to generate
new quadratic constraints, and then the lineariza-
tion step replaces quadratic terms in the new con-
straints with auxiliary variables.
Finally, if the node is not pruned, we search
for a better incumbent solution under that node
by projecting the solution of the RLT relaxation
back onto the feasible region. In the relaxation, the
model parameters might sum to slightly more than
1This objective might not be a great sacrifice: Spitkovsky
et al (2010b) present evidence that hard EM can outperform
soft EM for grammar induction in a hill-climbing setting. We
use it because it is a quadratic objective. However, maximiz-
ing it remains NP-hard (Cohen and Smith, 2010).
one and the parses can consist of fractional depen-
dency edges. We project in order to compute the
true objective and compare with other solutions.
Our results demonstrate that our method can ob-
tain higher likelihoods than Viterbi EM with ran-
dom restarts. Furthermore, we show how posterior
constraints inspired by Gillenwater et al (2010)
and Naseem et al (2010) can easily be applied
in our framework to obtain competitive accuracies
using a simple model, the Dependency Model with
Valence (Klein and Manning, 2004). We also ob-
tain an -optimal solution on a toy dataset.
We caution that the linear relaxations are very
loose on larger boxes. Since we have many dimen-
sions, the binary branch-and-bound tree may have
to grow quite deep before the boxes become small
enough to prune. This is why nonconvex quadratic
optimization by LP-based branch-and-bound usu-
ally fails with more than 80 variables (Burer and
Vandenbussche, 2009). Even our smallest (toy)
problems have hundreds of variables, so our exper-
imental results mainly just illuminate the method?s
behavior. Nonetheless, we offer the method as
a new tool which, just as for local search, might
be combined with other forms of problem-specific
guidance to produce more practical results.
2 The Constrained Optimization Task
We begin by describing how for our typical model,
the Viterbi EM objective can be formulated as a
mixed integer quadratic programming (MIQP)
problem with nonlinear constraints (Figure 2).
Other locally normalized log-linear generative
models (Berg-Kirkpatrick et al, 2010) would have
a similar formulation. In such models, the log-
likelihood objective is simply a linear function of
the feature counts. However, the objective be-
comes quadratic in unsupervised learning, be-
cause the feature counts are themselves unknown
variables to be optimized. The feature counts are
constrained to be derived from the latent variables
(e.g., parses), which are unknown discrete struc-
tures that must be encoded with integer variables.
The nonlinear constraints ensure that the model
parameters are true log-probabilities.
Concretely, (1) specifies the Viterbi EM objec-
tive: the total log-probability of the best parse
trees under the parameters ?, given by a sum of
log-probabilities ?m of the individual steps needed
to generate the tree, as encoded by the features
fm. The (nonlinear) sum-to-one constraints on the
445
Variables:
?m Log-probability for feature m
fm Corpus-wide feature count for m
esij Indicator of an arc from i to j in tree s
Indices and constants:
m Feature / model parameter index
s Sentence index
c Conditional distribution index
M Number of model parameters
C Number of conditional distributions
Mc cth Set of feature indices that sum to 1.0
S Number of sentences
Ns Number of words in the sth sentence
Objective and constraints:
max
?
m
?mfm (1)
s.t.
?
m?Mc
exp(?m) = 1, ?c (2)
A
[
f
e
]
? b (Model constraints) (3)
?m ? 0, fm, esij ? Z, ?m, s, i, j (4)
Figure 2: Viterbi EM as a mathematical program
probabilities are in (2). The linear constraints in
(3) will ensure that the arc variables for each sen-
tence es encode a valid latent dependency tree,
and that the f variables count up the features of
these trees. The final constraints (4) simply spec-
ify the range of possible values for the model pa-
rameters and their integer count variables.
Our experiments use the dependency model
with valence (DMV) (Klein and Manning, 2004).
This generative model defines a joint distribution
over the sentences and their dependency trees.
We encode the DMV using integer linear con-
straints on the arc variables e and feature counts
f . These will constitute the model constraints in
(3). The constraints must declaratively specify that
the arcs form a valid dependency tree and that the
resulting feature values are as defined by the DMV.
Tree Constraints To ensure that our arc vari-
ables, es, form a dependency tree, we employ the
same single-commodity flow constraints of Mag-
nanti and Wolsey (1994) as adapted by Martins et
al. (2009) for parsing. We also use the projectivity
constraints of Martins et al (2009).
The single-commodity flow constraints simul-
taneously enforce that each node has exactly one
parent, the special root node (position 0) has no in-
coming arcs, and the arcs form a connected graph.
For each sentence, s, the variable ?sij indicates
the amount of flow traversing the arc from i to j in
sentence s. The constraints below specify that the
root node emits Ns units of flow (5), that one unit
of flow is consumed by each each node (6), that
the flow is zero on each disabled arc (7), and that
the arcs are binary variables (8).
Single-commodity flow (Magnanti & Wolsey, 1994)
Ns?
j=1
?s0j = Ns, ?j (5)
Ns?
i=0
?sij ?
Ns?
k=1
?sjk = 1, ?j (6)
?sij ? Nsesij , ?i, j (7)
esij ? {0, 1}, ?i, j (8)
Projectivity is enforced by adding a constraint
(9) for each arc ensuring that no edges will cross
that arc if it is enabled. Xij is the set of arcs (k, l)
that cross the arc (i, j).
Projectivity (Martins et al, 2009)?
(k,l)?Xij
eskl ? Ns(1? esij), ?s, i, j (9)
DMV Feature Counts The DMV generates a
dependency tree recursively as follows. First
the head word of the sentence is generated, t ?
Discrete(?root), where ?root is a subvector of ?.
To generate its children on the left side, we flip
a coin to decide whether an adjacent child is gen-
erated, d ? Bernoulli(?dec.L.0,t). If the coin flip
d comes up continue, we sample the word of that
child as t? ? Discrete(?child.L,t). We continue gen-
erating non-adjacent children in this way, using
coin weights ?dec.L.? 1,t until the coin comes up
stop. We repeat this procedure to generate chil-
dren on the right side, using the model parameters
?dec.R.0,t, ?child.R,t, and ?dec.R.? 1,t. For each new
child, we apply this process recursively to gener-
ate its descendants.
The feature count variables for the DMV are en-
coded in our MP as various sums over the edge
variables. We begin with the root/child feature
counts. The constraint (10) defines the feature
count for model parameter ?root,t as the number
of all enabled arcs connecting the root node to a
word of type t, summing over all sentences s. The
constraint in (11) similarly defines fchild.L,t,t? to be
the number of enabled arcs connecting a parent of
446
type t to a left child of type t?. Wst is the index set
of tokens in sentences s with word type t.
DMV root/child feature counts
froot,t =
Ns?
s=1
?
j?Wst
es0j , ?t (10)
fchild.L,t,t? =
Ns?
s=1
?
j<i
?
[
i?Wst ?
j?Wst?
]
esij , ?t, t? (11)
The decision feature counts require the addi-
tion of an auxiliary count variables f (si)m ? Z in-
dicating how many times decision feature m fired
at some position in the corpus s, i. We then need
only add a constraint that the corpus wide fea-
ture count is the sum of these token-level feature
counts fm =?Ss=1
?Ns
i=1 f
(si)
m , ?m.
Below we define these auxiliary variables for
1 ? s ? S and 1 ? i ? Ns. The helper vari-
able ns,i,l counts the number of enabled arcs to the
left of token i in sentence s. Let t denote the word
type of token i in sentence s. Constraints (11) -
(16) are defined analogously for the right side fea-
ture counts.
DMV decision feature counts
ns,i,l =
i?1?
j=1
esij (12)
ns,i,l/Ns ? f (s,i)dec.L.0,t,cont ? 1 (13)
f (s,i)dec.L.0,t,stop = 1? f
(s,i)
dec.L.0,t,cont (14)
f (s,i)dec.L.? 1,t,stop = f
(s,i)
dec.L.0,t,cont (15)
f (s,i)dec.L.? 1,t,cont = ns,i,l ? f
(s,i)
dec.L.0,t,cont (16)
3 A Branch-and-Bound Algorithm
The mixed integer quadratic program with nonlin-
ear constraints, given in the previous section, max-
imizes the nonconvex Viterbi EM objective and is
NP-hard to solve (Cohen and Smith, 2010). The
standard approach to optimizing this program is
local search by the hard (Viterbi) EM algorithm.
Yet local search can only provide a lower (pes-
simistic) bound on the global maximum.
We propose a branch-and-bound algorithm,
which will iteratively tighten both pessimistic and
optimistic bounds on the optimal solution. This
algorithm may be halted at any time, to obtain the
best current solution and a bound on how much
better the global optimum could be.
A feasible solution is an assignment to all
the variables?both model parameters and corpus
parse?that satisfies all constraints. Our branch-
and-bound algorithm maintains an incumbent so-
lution: the best known feasible solution according
to the objective function. This is updated as better
feasible solutions are found.
Our algorithm implicitly defines a search tree in
which each node corresponds to a region of model
parameter space. Our search procedure begins
with only the root node, which represents the full
model parameter space. At each node we perform
three steps: bounding, projecting, and branching.
In the bounding step, we solve a relaxation of
the original problem to provide an upper bound on
the objective achievable within that node?s subre-
gion. A node is pruned when Lglobal + |Lglobal| ?
Ulocal, where Lglobal is the incumbent score, Ulocal
is the upper bound for the node, and  > 0. This
ensures that its entire subregion will not yield a
-better solution than the current incumbent.
The overall optimistic bound is given by the
worst optimistic bound of all current leaf nodes.
The projecting step, if the node is not pruned,
projects the solution of the relaxation back to the
feasible region, replacing the current incumbent if
this projection provides a better lower bound.
In the branching step, we choose a variable ?m
on which to divide. Each of the child nodes re-
ceives a lower ?minm and upper ?maxm bound for ?m.
The child subspaces partition the parent subspace.
The search tree is defined by a variable order-
ing and the splitting procedure. We do binary
branching on the variable ?m with the highest re-
gret, defined as zm ? ?mfm, where zm is the
auxiliary objective variable we will introduce in
? 4.2. Since ?m is a log-probability, we split its
current range at the midpoint in probability space,
log((exp ?minm + exp ?maxm )/2).
We perform best-first search, ordering the nodes
by the the optimistic bound of their parent. We
also use the LP-guided rule (Martin, 2000; Achter-
berg, 2007, section 6.1) to perform depth-first
plunges in search of better incumbents.
4 Relaxations
The relaxation in the bounding step computes an
optimistic bound for a subspace of the model pa-
rameters. This upper bound would ideally be not
much greater than the true maximum achievable
on that region, but looser upper bounds are gener-
ally faster to compute.
447
We present successive relaxations to the orig-
inal nonconvex mixed integer quadratic program
with nonlinear constraints from (1)?(4). First, we
show how the nonlinear sum-to-one constraints
can be relaxed into linear constraints and tight-
ened. Second, we apply a classic approach to
bound the nonconvex quadratic objective by a lin-
ear concave envelope. Finally, we present our full
relaxation based on the Reformulation Lineariza-
tion Technique (RLT) (Sherali and Adams, 1990).
We solve these LPs by the dual simplex algorithm.
4.1 Relaxing the sum-to-one constraint
In this section, we use cutting planes to create a
linear relaxation for the sum-to-one constraint (2).
When relaxing a constraint, we must ensure that
any assignment of the variables that was feasible
(i.e. respected the constraints) in the original prob-
lem must also be feasible in the relaxation. In most
cases, the relaxation is not perfectly tight and so
will have an enlarged space of feasible solutions.
We begin by weakening constraint (2) to
?
m?Mc
exp(?m) ? 1 (17)
The optimal solution under (17) still satisfies the
original equality constraint (2) because of the
maximization. We now relax (17) by approx-
imating the surface z = ?m?Mc exp(?m) bythe max of N lower-bounding linear functions on
R|Mc|. Instead of requiring z ? 1, we only require
each of these lower bounds to be ? 1, slightly
enlarging the feasible space into a convex poly-
tope. Figure 3a shows the feasible region con-
structed from N=3 linear functions on two log-
probabilities ?1, ?2.
Formally, for each c, we define the ith linear
lower bound (i = 1, . . . , N ) to be the tangent hy-
perplane at some point ??(i)c = [??(i)c,1, . . . , ??(i)c,|Mc|] ?
R|Mc|, where each coordinate is a log-probability
??(i)c,m < 0. We require each of these linear func-
tions to be ? 1:
Sum-to-one Relaxation?
m?Mc
(
?m + 1? ??(i)c,m
)
exp
(
??(i)c,m
)
? 1, ?i, ?c
(18)
4.2 ?Relaxing? the objective
Our true maximization objective?m ?mfm in (1)
is a sum of quadratic terms. If the parameters ?
(a)
0
1
2
3
4
f
sm
?15
?10
?5
?
m
?60
?40
?20
0
20
(b)
Figure 3: In (a), the area under the curve corre-
sponds to those points (?1, ?2) that satisfy (17)
(z ? 1), with equality (2) achieved along the curve
(z = 1). The shaded area shows the enlarged fea-
sible region under the linear relaxation. In (b),
the curved lower surface represents a single prod-
uct term in the objective. The piecewise-linear up-
per surface is its concave envelope (raised by 20
for illustration; in reality they touch).
were fixed, the objective would become linear in
the latent features. Although the parameters are
not fixed, the branch-and-bound algorithm does
box them into a small region, where the quadratic
objective is ?more linear.?
Since it is easy to maximize a concave function,
we will maximize the concave envelope?the con-
cave function that most tightly upper-bounds our
objective over the region. This turns out to be
piecewise linear and can be maximized with an LP
solver. Smaller regions yield tighter bounds.
Each node of the branch-and-bound tree speci-
fies a region via bounds constraints ?minm < ?m <
?maxm , ?m. In addition, we have known bounds
fminm ? fm ? fmaxm , ?m for the count variables.
McCormick (1976) described the concave enve-
lope for a single quadratic term subject to bounds
constraints (Figure 3b). In our case:
?mfm ? min[fmaxm ?m + ?minm fm ? ?minm fmaxm ,
fminm ?m + ?maxm fm ? ?maxm fminm ]
We replace our objective?m ?mfm with
?
m zm,
where we would like to constrain each auxiliary
variable zm to be = ?mfm or (equivalently) ?
?mfm, but instead settle for making it ? the con-
cave envelope?a linear programming problem:
Concave Envelope Objective
max
?
m
zm (19)
s.t. zm ? fmaxm ?m + ?minm fm ? ?minm fmaxm (20)
zm ? fminm ?m + ?maxm fm ? ?maxm fminm (21)
448
4.3 Reformulation Linearization Technique
The Reformulation Linearization Technique
(RLT)2 (Sherali and Adams, 1990) is a method
of forming tighter relaxations of various types of
MPs. The basic method reformulates the problem
by adding products of existing constraints. The
quadratic terms in the objective and in these new
constraints are redefined as auxiliary variables,
thereby linearizing the program.
In this section, we will show how the RLT can
be applied to our grammar induction problem and
contrast it with the concave envelope relaxation
presented in section 4.2.
Consider the original MP in equations (1) -
(4), with the nonlinear sum-to-one constraints in
(2) replaced by our linear constraints proposed in
(18). If we remove the integer constraints in (4),
the result is a quadratic program with purely linear
constraints. Such problems have the form
max xTQx (22)
s.t. Ax ? b (23)
?? < Li ? xi ? Ui <?, ?i (24)
where the variables are x ? Rn, A is an m ? n
matrix, and b ? Rm, and Q is an n? n indefinite3
matrix. Without loss of generality we assume Q is
symmetric. The application of the RLT here was
first considered by Sherali and Tuncbilek (1995).
For convenience of presentation, we repre-
sent both the linear inequality constraints and the
bounds constraints, under a different parameteri-
zation using the matrix G and vector g.
[
(bi ?Aix) ? 0, 1 ? i ? m
(Uk ? xk) ? 0, 1 ? k ? n
(?Lk + xk) ? 0, 1 ? k ? n
]
?
[(gi ?Gix) ? 0,
1 ? i ? m + 2n
]
The reformulation step forms all possible products
of these linear constraints and then adds them to
the original quadratic program.
(gi ?Gix)(gj ?Gjx) ? 0, ?1 ? i ? j ? m+ 2n
In the linearization step, we replace all
quadratic terms in the quadratic objective and new
quadratic constraints with auxiliary variables:
wij ? xixj , ?1 ? i ? j ? n
2The key idea underlying the RLT was originally intro-
duced in Adams and Sherali (1986) for 0-1 quadratic pro-
gramming. It has since been extended to various other set-
tings; see Sherali and Liberti (2008) for a complete survey.
3In the general case, that Q is indefinite causes this pro-
gram to be nonconvex, making this problem NP-hard to solve
(Vavasis, 1991; Pardalos, 1991).
This yields the following RLT relaxation:
RLT Relaxation
max
?
1?i?j?n
Qijwij (25)
s.t. gigj ?
n?
k=1
gjGikxk ?
n?
k=1
giGjkxk
+
n?
k=1
n?
l=1
GikGjlwkl ? 0,
?1 ? i ? j ? m+ 2n (26)
Notice above that we have omitted the original
inequality constraints (23) and bounds (24), be-
cause they are fully enforced by the new RLT con-
straints (26) from the reformulation step (Sherali
and Tuncbilek, 1995). In our experiments, we
keep the original constraints and instead explore
subsets of the RLT constraints.
If the original QP contains equality constraints
of the form Gex = ge, then we can form con-
straints by multiplying this one by each variable
xi. This gives us the following new set of con-
straints, for each equality constraint e: gexi +?n
j=1?Gejwij = 0, ?1 ? i ? n.
Theoretical Properties The new constraints in
eq. (26) will impose the concave envelope con-
straints (20)?(21) (Anstreicher, 2009).
The constraints presented above are consid-
ered to be first-level constraints corresponding to
the first-level variables wij . However, the same
technique can be applied repeatedly to produce
polynomial constraints of higher degree. These
higher level constraints/variables have been shown
to provide increasingly tighter relaxations (Sher-
ali and Adams, 1990) at the cost of a large num-
ber of variables and constraints. In the case where
x ? {0, 1}n the degree-n RLT constraints will re-
strict to the convex hull of the feasible solutions
(Sherali and Adams, 1990).
This is in direct contrast to the concave enve-
lope relaxation presented in section 4.2 which re-
laxes to the convex hull of each quadratic term in-
dependently. This demonstrates the key intuition
of the RLT relaxation: The products of constraints
are implied (and unnecessary) in the original vari-
able space. Yet when we project to a higher-
dimentional space by including the auxiliary vari-
ables, the linearized constraints cut off portions of
the feasible region given by only the concave en-
velope relaxation in eqs. (20)-(21) .
449
4.4 Adding Posterior Constraints
It is a simple extension to impose posterior con-
straints within our framework. Here we empha-
size constraints that are analogous to the universal
linguistic constraints from Naseem et al (2010).
Since we optimize the Viterbi EM objective, we
directly constrain the counts in the single corpus
parse rather than expected counts from a distribu-
tion over parses. Let E be the index set of model
parameters corresponding to edge types from Ta-
ble 1 of Naseem et al (2010), and Ns be the num-
ber of words in the sth sentence. We impose
the constraint that 75% of edges come from E :?
m?E fm ? 0.75
(?S
s=1Ns
)
.
5 Projections
A pessimistic bound, from the projecting step, will
correspond to a feasible but not necessarily opti-
mal solution to the original problem. We propose
several methods for obtaining pessimistic bounds
during the branch-and-bound search, by projecting
and improving the solutions found by the relax-
ation. A solution to the relaxation may be infea-
sible in the original problem for two reasons: the
model parameters might not sum to one, and/or the
parse may contain fractional edges.
Model Parameters For each set of model pa-
rameters Mc that should sum-to-one, we project
the model parameters onto the Mc ? 1 simplex
by one of two methods: (1) normalize the infeasi-
ble parameters or (2) find the point on the simplex
that has minimum Euclidean distance to the infea-
sible parameters using the algorithm of Chen and
Ye (2011). For both methods, we can optionally
apply add-? smoothing before projecting.
Parses Since we are interested in projecting the
fractional parse onto the space of projective span-
ning trees, we can simply employ a dynamic pro-
gramming parsing algorithm (Eisner and Satta,
1999) where the weight of each edge is given as
the fraction of the edge variable.
Only one of these projection techniques is
needed. We then either use parsing to fill in the
optimal parse trees given the projected model pa-
rameters, or use supervised parameter estimation
to fill in the optimal model parameters given the
projected parses. These correspond to the Viterbi
E step and M step, respectively. We can locally
improve the projected solution by continuing with
a few additional iterations of Viterbi EM.
Related models could use very similar projec-
tion techniques. Given a relaxed joint solution to
the parameters and the latent variables, one must
be able to project it to a nearby feasible one, by
projecting either the fractional parameters or the
fractional latent variables into the feasible space
and then solving exactly for the other.
6 Related Work
The goal of this work was to better understand and
address the non-convexity of maximum-likelihood
training with latent variables, especially parses.
Gimpel and Smith (2012) proposed a concave
model for unsupervised dependency parsing us-
ing IBM Model 1. This model did not include a
tree constraint, but instead initialized EM on the
DMV. By contrast, our approach incorporates the
tree constraints directly into our convex relaxation
and embeds the relaxation in a branch-and-bound
algorithm capable of solving the original DMV
maximum-likelihood estimation problem.
Spectral learning constitutes a wholly differ-
ent family of consistent estimators, which achieve
efficiency because they sidestep maximizing the
nonconvex likelihood function. Hsu et al (2009)
introduced a spectral learner for a large class of
HMMs. For supervised parsing, spectral learn-
ing has been used to learn latent variable PCFGs
(Cohen et al, 2012) and hidden-state dependency
grammars (Luque et al, 2012). Alas, there are
not yet any spectral learning methods that recover
latent tree structure, as in grammar induction.
Several integer linear programming (ILP) for-
mulations of dependency parsing (Riedel and
Clarke, 2006; Martins et al, 2009; Riedel et al,
2012) inspired our definition of grammar induc-
tion as a MP. Recent work uses branch-and-bound
for decoding with non-local features (Qian and
Liu, 2013). These differ from our work by treating
the model parameters as constants, thereby yield-
ing a linear objective.
For semi-supervised dependency parsing, Wang
et al (2008) used a convex objective, combin-
ing unsupervised least squares loss and a super-
vised large margin loss, This does not apply to our
unsupervised setting. Branch-and-bound has also
been applied to semi-supervised SVM training, a
nonconvex search problem (Chapelle et al, 2007),
with a relaxation derived from the dual.
450
7 Experiments
We first analyze the behavior of our method on a
toy synthetic dataset. Next, we compare various
parameter settings for branch-and-bound by esti-
mating the total solution time. Finally, we com-
pare our search method to Viterbi EM on a small
subset of the Penn Treebank.
All our experiments use the DMV for unsuper-
vised dependency parsing of part-of-speech (POS)
tag sequences. For Viterbi EM we initialize the pa-
rameters of the model uniformly, breaking parser
ties randomly in the first E-step (Spitkovsky et
al., 2010b). This initializer is state-of-the-art for
Viterbi EM. We also apply add-one smoothing
during each M-step. We use random restarts, and
select the model with the highest likelihood.
We add posterior constraints to Viterbi EM?s E-
step. First, we run a relaxed linear programming
(LP) parser, then project the (possibly fractional)
parses back to the feasible region. If the resulting
parse does not respect the posterior constraints, we
discard it. The posterior constraint in the LP parser
is tighter4 than the one used in the true optimiza-
tion problem, so the projections tends to be feasi-
ble under the true (looser) posterior constraints. In
our experiments, all but one projection respected
the constraints. We solve all LPs with CPLEX.
7.1 Synthetic Data
For our toy example, we generate sentences from a
synthetic DMV over three POS tags (Verb, Noun,
Adjective) with parameters chosen to favor short
sentences with English word order.
In Figure 4 we show that the quality of the root
relaxation increases as we approach the full set of
RLT constraints. That the number of possible RLT
constraints increases quadratically with the length
of the corpus poses a serious challenge. For just
20 sentences from this synthetic model, the RLT
generates 4,056,498 constraints.
For a single run of branch-and-bound, Figure 5
shows the global upper and lower bounds over
time.5 We consider five relaxations, each using
only a subset of the RLT constraints. Max.0k
uses only the concave envelope (20)-(21). Max.1k
uses the concave envelope and also randomly sam-
ples 1,000 other RLT constraints, and so on for
Max.10k and Max.100k. Obj.Filter includes all
480% of edges must come from E as opposed to 75%.
5The initial incumbent solution for branch-and-bound is
obtained by running Viterbi EM with 10 random restarts.
!4
!3
!2
!1
0 ! ! !
!
!
!
!
! ! ! !
0.0 0.2 0.4 0.6 0.8 1.0Proportion of RLT rows included
Up
per
 bo
und
 on
log
!li
kel
iho
od 
at r
oot
Figure 4: The bound quality at the root improves
as the proportion of RLT constraints increases, on
5 synthetic sentences. A random subset of 70%
of the 320,126 possible RLT constraints matches
the relaxation quality of the full set. This bound is
very tight: the relaxations in Figure 5 solve hun-
dreds of nodes before such a bound is achieved.
!12
!10
!8
!6
!4
!2
0
!
!
! !!!
!
! ! !!
20 40 6080
Time (sec)
Bo
un
ds 
on
 lo
g!
lik
eli
ho
od
Bound type
lower
upper
Relaxation
! RLT Obj.Filter
RLT Max.0k
RLT Max.1k
RLT Max.10k
Figure 5: The global upper and lower bounds
improve over time for branch-and-bound using
different subsets of RLT constraints on 5 syn-
thetic sentences. Each solves the problem to -
optimality for  = 0.01. A point marks every 200
nodes processed. (The time axis is log-scaled.)
constraints with a nonzero coefficient for one of
the RLT variables zm from the linearized ob-
jective. The rightmost lines correspond to RLT
Max.10k: despite providing the tightest (local)
bound at each node, it processed only 110 nodes in
the time it took RLT Max.1k to process 1164. RLT
Max.0k achieves the best balance of tight bounds
and speed per node.
7.2 Comparing branch-and-bound strategies
It is prohibitively expensive to repeatedly run our
algorithm to completion with a variety of param-
eter settings. Instead, we estimate the size of the
branch-and-bound tree and the solution time using
a high-variance estimate that is effective for com-
parisons (Lobjois and Lema??tre, 1998).
Given a fixed set of parameters for our algo-
rithm and an -optimality stopping criterion, we
451
RLT Re-
laxation
Avg. ms
per node
# Sam-
ples
Est. #
Nodes
Est. #
Hours
Obj.Filter 63 10000 3.2E+08 4.6E+09
Max.0k 6 10000 1.7E+10 7.8E+10
Max.1k 15 10000 3.5E+08 4.2E+09
Max.10k 161 10000 1.3E+09 3.4E+10
Max.100k 232259 5 1.7E+09 9.7E+13
Table 1: Branch-and-bound node count and com-
pletion time estimates. Each standard deviation
was close in magnitude to the estimate itself. We
ran for 8 hours, stopping at 10,000 samples on 8
synthetic sentences.
can view the branch-and-bound tree T as fixed and
finite in size. We wish to estimate some cost asso-
ciated with the tree C(T ) = ???nodes(T ) f(?).
Letting f(?) = 1 estimates the number of nodes;
if f(?) is the time to solve a node, then we es-
timate the total solution time using the Monte
Carlo method of Knuth (1975). Table 1 gives
these estimates, for the same five RLT relaxations.
Obj.Filter yields the smallest estimated tree size.
7.3 Real Data
In this section, we compare our global search
method to Viterbi EM with random restarts each
with or without posterior constraints. We use 200
sentences of no more than 10 tokens from the WSJ
portion of the Penn Treebank. We reduce the tree-
bank?s gold part-of-speech (POS) tags to a univer-
sal set of 12 tags (Petrov et al, 2012) plus a tag
for auxiliaries, ignoring punctuation. Each search
method is run for 8 hours. We obtain the initial
incumbent solution for branch-and-bound by run-
ning Viterbi EM for 45 minutes. The average time
to solve a node?s relaxation ranges from 3 seconds
for RLT Max.0k to 42 seconds for RLT Max.100k.
Figure 6a shows the log-likelihood of the in-
cumbent solution over time. In our global search
method, like Viterbi EM, the posterior constraints
lead to lower log-likelihoods. RLT Max.0k finds
the highest log-likelihood solution.
Figure 6b compares the unlabeled directed de-
pendency accuracy of the incumbent solution. In
both global and local search, the posterior con-
straints lead to higher accuracies. Viterbi EM
with posterior constraints demonstrates the oscil-
lation of incumbent accuracy: starting at 58.02%
accuracy, it finds several high accuracy solutions
early on (61.02%), but quickly abandons them to
increase likelihood, yielding a final accuracy of
60.65%. RLT Max.0k with posterior constraints
obtains the highest overall accuracy of 61.09% at
(a)
!3300
!3200
!3100
!3000
!2900
!
!
!
!
!
!!!
!
!!
!
!!
!
!
!!
!!
!
!!!!
!!!
! !!!
!
!
100 200 300 400
Time (min)
Lo
g!
lik
eli
ho
od
 (t
rai
n)
Algorithm
! Viterbi EM
RLT Obj.Filter
RLT Max.0k
RLT Max.1k
RLT Max.10k
RLT Max.100k
Posterior Constraints
False
True
(b)
0.35
0.40
0.45
0.50
0.55
0.60
!
!
!
!
!!
!!
!
!!
!!!
!
!
!!!
!
!!!
!!
!
! !
!!!
!
100 200 300 400
Time (min)
Ac
cu
rac
y (
tra
in)
Algorithm
! Viterbi EM
RLT Obj.Filter
RLT Max.0k
RLT Max.1k
RLT Max.10k
RLT Max.100k
Posterior Constraints
False
True
Figure 6: Likelihood (a) and accuracy (b) of in-
cumbent solution so far, on a small real dataset.
306 min and the highest final accuracy 60.73%.
8 Discussion
In principle, our branch-and-bound method can
approach -optimal solutions to Viterbi training of
locally normalized generative models, including
the NP-hard case of grammar induction with the
DMV. The method can also be used with posterior
constraints or a regularized objective.
Future work includes algorithmic improve-
ments for solving the relaxation and the develop-
ment of tighter relaxations. The Dantzig-Wolfe
decomposition (Dantzig and Wolfe, 1960) or La-
grangian Relaxation (Held and Karp, 1970) might
satisfy both of these goals by pushing the inte-
ger tree constraints into a subproblem solved by
a dynamic programming parser. Recent work on
semidefinite relaxations (Anstreicher, 2009) sug-
gests they may provide tighter bounds at the ex-
pense of greater computation time.
Perhaps even more important than tightening
the bounds at each node are search heuristics (e.g.,
surface cues) and priors (e.g., universal grammar)
that guide our global search by deciding which
node to expand next (Chomsky and Lasnik, 1993).
452
References
Tobias Achterberg. 2007. Constraint integer program-
ming. Ph.D. thesis, TU Berlin.
Warren P. Adams and Hanif D. Sherali. 1986. A
tight linearization and an algorithm for zero-one
quadratic programming problems. Management
Science, 32(10):1274?1290, October. ArticleType:
research-article / Full publication date: Oct., 1986 /
Copyright 1986 INFORMS.
Kurt Anstreicher. 2009. Semidefinite programming
versus the reformulation-linearization technique for
nonconvex quadratically constrained quadratic pro-
gramming. Journal of Global Optimization,
43(2):471?484.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
DeNero, John DeNero, and Dan Klein. 2010. Pain-
less unsupervised learning with features. In Proc. of
NAACL, June.
Samuel Burer and Dieter Vandenbussche. 2009. Glob-
ally solving box-constrained nonconvex quadratic
programs with semidefinite-based finite branch-and-
bound. Computational Optimization and Applica-
tions, 43(2):181?195.
Olivier Chapelle, Vikas Sindhwani, and S. Sathiya
Keerthi. 2007. Branch and bound for semi-
supervised support vector machines. In Proc. of
NIPS 19, pages 217?224. MIT Press.
E. Charniak. 1993. Statistical language learning. MIT
press.
Yunmei Chen and Xiaojing Ye. 2011. Projection onto
a simplex. arXiv:1101.6081, January.
Noam Chomsky and Howard Lasnik. 1993. Princi-
ples and parameters theory. In Syntax: An Interna-
tional Handbook of Contemporary Research. Berlin:
de Gruyter.
Shay Cohen and Noah A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proc. of HLT-
NAACL, pages 74?82, June.
Shay Cohen and Noah A. Smith. 2010. Viterbi training
for PCFGs: Hardness results and competitiveness of
uniform initialization. In Proc. of ACL, pages 1502?
1511, July.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2009. Lo-
gistic normal priors for unsupervised probabilistic
grammar induction. In Proceedings of NIPS.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable PCFGs. In Proc. of ACL (Volume 1:
Long Papers), pages 223?231. Association for Com-
putational Linguistics, July.
George B. Dantzig and Philip Wolfe. 1960. Decom-
position principle for linear programs. Operations
Research, 8(1):101?111, January.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of ACL, pages 457?
464, June.
Jennifer Gillenwater, Kuzman Ganchev, Joo Graa, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity
in dependency grammar induction. In Proceedings
of the ACL 2010 Conference Short Papers, pages
194?199. Association for Computational Linguis-
tics, July.
K. Gimpel and N. A. Smith. 2012. Concavity and ini-
tialization for unsupervised dependency parsing. In
Proc. of NAACL.
M. Held and R. M. Karp. 1970. The traveling-
salesman problem and minimum spanning trees.
Operations Research, 18(6):1138?1162.
D. Hsu, S. M Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden markov models.
In COLT 2009 - The 22nd Conference on Learning
Theory.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL, pages
478?485, July.
D. E. Knuth. 1975. Estimating the efficiency of
backtrack programs. Mathematics of computation,
29(129):121?136.
L. Lobjois and M. Lema??tre. 1998. Branch and bound
algorithm selection by performance prediction. In
Proc. of the National Conference on Artificial Intel-
ligence, pages 353?358.
Franco M. Luque, Ariadna Quattoni, Borja Balle, and
Xavier Carreras. 2012. Spectral learning for
non-deterministic dependency parsing. In Proc. of
EACL, pages 409?419, April.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal Trees. Center for Operations Research and
Econometrics.
Alexander Martin. 2000. Integer programs with block
structure. Technical Report SC-99-03, ZIB.
Andre? Martins, Noah A. Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proc. of ACL-IJCNLP,
pages 342?350, August.
Garth P. McCormick. 1976. Computability of global
solutions to factorable nonconvex programs: Part
I?Convex underestimating problems. Mathemati-
cal Programming, 10(1):147?175.
453
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proc. of
EMNLP, pages 1234?1244, October.
P. M. Pardalos. 1991. Global optimization algorithms
for linearly constrained indefinite quadratic prob-
lems. Computers & Mathematics with Applications,
21(6):87?97.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proc. of LREC.
Xian Qian and Yang Liu. 2013. Branch and bound al-
gorithm for dependency parsing with non-local fea-
tures. TACL, 1:37?48.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP, pages 129?
137, July.
Sebastian Riedel, David Smith, and Andrew McCal-
lum. 2012. Parse, price and cut?Delayed column
and row generation for graph based parsers. In Proc.
of EMNLP-CoNLL, pages 732?743, July.
Hanif D. Sherali and Warren P. Adams. 1990. A hi-
erarchy of relaxations between the continuous and
convex hull representations for zero-one program-
ming problems. SIAM Journal on Discrete Math-
ematics, 3(3):411?430, August.
H. Sherali and L. Liberti. 2008. Reformulation-
linearization technique for global optimization. En-
cyclopedia of Optimization, 2:3263?3268.
Hanif D. Sherali and Cihan H. Tuncbilek. 1995. A
reformulation-convexification approach for solving
nonconvex quadratic programming problems. Jour-
nal of Global Optimization, 7(1):1?31.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proc. of COLING-ACL, pages 569?576,
July.
N.A. Smith. 2006. Novel estimation methods for unsu-
pervised discovery of latent structure in natural lan-
guage text. Ph.D. thesis, Johns Hopkins University,
Baltimore, MD.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From baby steps to leapfrog: How
Less is more in unsupervised dependency parsing.
In Proc. of HLT-NAACL, pages 751?759. Associa-
tion for Computational Linguistics, June.
Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D Manning. 2010b. Viterbi train-
ing improves unsupervised dependency parsing. In
Proc. of CoNLL, pages 9?17. Association for Com-
putational Linguistics, July.
S. A. Vavasis. 1991. Nonlinear optimization: com-
plexity issues. Oxford University Press, Inc.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2008. Semi-supervised convex training for de-
pendency parsing. In Proc of ACL-HLT, pages
532?540. Association for Computational Linguis-
tics, June.
454
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775?785,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Robust Entity Clustering via Phylogenetic Inference
Nicholas Andrews and Jason Eisner and Mark Dredze
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA
{noa,eisner,mdredze}@jhu.edu
Abstract
Entity clustering must determine when two
named-entity mentions refer to the same
entity. Typical approaches use a pipeline ar-
chitecture that clusters the mentions using
fixed or learned measures of name and con-
text similarity. In this paper, we propose a
model for cross-document coreference res-
olution that achieves robustness by learn-
ing similarity from unlabeled data. The
generative process assumes that each entity
mention arises from copying and option-
ally mutating an earlier name from a sim-
ilar context. Clustering the mentions into
entities depends on recovering this copying
tree jointly with estimating models of the
mutation process and parent selection pro-
cess. We present a block Gibbs sampler for
posterior inference and an empirical evalu-
ation on several datasets.
1 Introduction
Variation poses a serious challenge for determin-
ing who or what a name refers to. For instance,
Wikipedia contains more than 100 variations of the
name Barack Obama as redirects to the U.S. Presi-
dent article, including:
President Obama Barack H. Obama, Jr.
Barak Obamba Barry Soetoro
To relate different names, one solution is to use
specifically tailored measures of name similarity
such as Jaro-Winkler similarity (Winkler, 1999; Co-
hen et al, 2003). This approach is brittle, however,
and fails to adapt to the test data. Another option is
to train a model like stochastic edit distance from
known pairs of similar names (Ristad and Yian-
ilos, 1998; Green et al, 2012), but this requires
supervised data in the test domain.
Even the best model of name similarity is not
enough by itself, since two names that are similar?
even identical?do not necessarily corefer. Docu-
ment context is needed to determine whether they
may be talking about two different people.
In this paper, we propose a method for jointly
(1) learning similarity between names and (2) clus-
tering name mentions into entities, the two major
components of cross-document coreference reso-
lution systems (Baron and Freedman, 2008; Finin
et al, 2009; Rao et al, 2010; Singh et al, 2011;
Lee et al, 2012; Green et al, 2012). Our model
is an evolutionary generative process based on the
name variation model of Andrews et al (2012),
which stipulates that names are often copied from
previously generated names, perhaps with mutation
(spelling edits). This can deduce that rather than
being names for different entities, Barak Obamba
and Barock obama more likely arose from the fre-
quent name Barack Obama as a common ancestor,
which accounts for most of their letters. This can
also relate seemingly dissimilar names via multiple
steps in the generative process:
Taylor Swift? T-Swift? T-Swizzle
Our model learns without supervision that these all
refer to the the same entity. Such creative spellings
are especially common on Twitter and other so-
cial media; we give more examples of coreferents
learned by our model in Section 8.4.
Our primary contributions are improvements on
Andrews et al (2012) for the entity clustering task.
Their inference procedure only clustered types (dis-
tinct names) rather than tokens (mentions in con-
text), and relied on expensive matrix inversions for
learning. Our novel approach features:
?4.1 A topical model of which entities from previ-
ously written text an author tends to mention
from previously written text.
?4.2 A name mutation model that is sensitive to
features of the input and output characters and
takes a reader?s comprehension into account.
?5 A scalable Markov chain Monte Carlo sam-
pler used in training and inference.
775
?7 A minimum Bayes risk decoding procedure
to pick an output clustering. The procedure is
applicable to any model capable of producing
a posterior over coreference decisions.
We evaluate our approach by comparing to sev-
eral baselines on datasets from three different gen-
res: Twitter, newswire, and blogs.
2 Overview and Related Work
Cross-document coreference resolution (CDCR)
was first introduced by Bagga and Baldwin (1998b).
Most approaches since then are based on the intu-
itions that coreferent names tend to have ?similar?
spellings and tend to appear in ?similar? contexts.
The distinguishing feature of our system is that both
notions of similarity are learned together without
supervision.
We adopt a ?phylogenetic? generative model of
coreference. The basic insight is that coreference is
created when an author thinks of an entity that was
mentioned earlier in a similar context, and men-
tions it again in a similar way. The author may
alter the name mention string when copying it, but
both names refer to the same entity. Either name
may later be copied further, leading to an evolution-
ary tree of mentions?a phylogeny. Phylogenetic
models are new to information extraction. In com-
putational historical linguistics, Bouchard-C?ot?e et
al. (2013) have also modeled the mutation of strings
along the edges of a phylogeny; but for them the
phylogeny is observed and most mentions are not,
while we observe the mentions only.
To apply our model to the CDCR task, we ob-
serve that the probability that two name mentions
are coreferent is the probability that they arose from
a common ancestor in the phylogeny. So we design
a Monte Carlo sampler to reconstruct likely phylo-
genies. A phylogeny must explain every observed
name. While our model is capable of generating
each name independently, a phylogeny will gener-
ally achieve higher probability if it explains similar
names as being similar by mutation (rather than
by coincidence). Thus, our sampled phylogenies
tend to make similar names coreferent?especially
long or unusual names that would be expensive to
generate repeatedly, and especially in contexts that
are topically similar and therefore have a higher
prior probability of coreference.
For learning, we iteratively adjust our model?s
parameters to better explain our samples. That is,
we do unsupervised training via Monte Carlo EM.
What is learned? An important component of
a CDCR system is its model of name similarity
(Winkler, 1999; Porter and Winkler, 1997), which
is often fixed up front. This role is played in our sys-
tem by the name mutation model, which we take to
be a variant of stochastic edit distance (Ristad and
Yianilos, 1996). Rather than fixing its parameters
before we begin CDCR, we learn them (without
supervision) as part of CDCR, by training from
samples of reconstructed phylogenies.
Name similarity is also an important component
of within-document coreference resolution, and ef-
forts in that area bear resemblance to our approach.
Haghighi and Klein (2010) describe an ?entity-
centered? model where a distance-dependent Chi-
nese restaurant process is used to pick previous
coreferent mentions within a document. Similarly,
Durrett and Klein (2013) learn a mention similarity
model based on labeled data. Our cross-document
setting has no observed mention ordering and no
observed entities: we must sum over all possibili-
ties, a challenging inference problem.
The second major component of CDCR is
context-based disambiguation of similar or iden-
tical names that refer to the same entity. Like
Kozareva and Ravi (2011) and Green et al (2012)
we use topics as the contexts, but learn mention
topics jointly with other model parameters.
3 Generative Model of Coreference
Let x = (x
1
, . . . , x
N
) denote an ordered sequence
of distinct named-entity mentions in documents
d = (d
1
, . . . , d
D
). We assume that each doc-
ument has a (single) known language, and that
its mentions and their types have been identified
by a named-entity recognizer. We use the object-
oriented notation x.v for attribute v of mention x.
Our model generates an ordered sequence x al-
though we do not observe its order. Thus each men-
tion x has latent position x.i (e.g., x
729
.i = 729).
The entire corpus, including these entities, is gen-
erated according to standard topic model assump-
tions; we first generate a topic distribution for a
document, then sample topics and words for the
document (Blei et al, 2003). However, any topic
may generate an entity type, e.g. PERSON, which is
then replaced by a specific name: when PERSON is
generated, the model chooses a previous mention
of any person and copies it, perhaps mutating its
name.
1
Alternatively, the model may manufacture
1
We make the closed-world assumption that the author is
776
a name for a new person, though the name itself
may not be new.
If all previous mentions were equally likely, this
would be a Chinese Restaurant Process (CRP) in
which frequently mentioned entities are more likely
to be mentioned again (?the rich get richer?). We
refine that idea by saying that the current topic, lan-
guage, and document influence the choice of which
previous mention to copy, similar to the distance-
dependent CRP (Blei and Frazier, 2011).
2
This will
help distinguish multiple John Smith entities if they
tend to appear in different contexts.
Formally, each mention x is derived from a par-
ent mention x.p where x.p.i < x.i (the parent
came first), x.e = x.p.e (same entity) and x.n is
a copy or mutation of x.p.n. In the special case
where x is a first mention of x.e, x.p is the special
symbol ?, x.e is a newly allocated entity of some
appropriate type, and the name x.n is generated
from scratch.
Our goal is to reconstruct mappings p, i, z that
specify the latent properties of the mentions x. The
mapping p : x 7? x.p forms a phylogenetic tree on
the mentions, with root?. Each entity corresponds
to a subtree that is rooted at some child of ?. The
mapping i : x 7? x.i gives an ordering consistent
with that tree in the sense that (?x)x.p.i < x.i.
Finally, the mapping z : x 7? x.z specifies, for
each mention, the topic that generated it. While i
and z are not necessary for creating coref clusters,
they are needed to produce p.
4 Detailed generative story
Given a few constants that are referenced in the
main text, we assume that the corpus d was gener-
ated as follows.
First, for each topic z = 1, . . .K and each lan-
guage `, choose a multinomial ?
z`
over the word
vocabulary, from a symmetric Dirichlet with con-
centration parameter ?. Then set m = 0 (entity
only aware of previous mentions from our corpus. This means
that two mentions cannot be derived from a common ancestor
outside our corpus. To mitigate this unrealistic assumption, we
allow any ordering x of the observed mentions, not respecting
document timestamps or forcing the mentions from a given
document to be generated as a contiguous subsequence of x.
2
Unlike the ddCRP, our generative story is careful to pro-
hibit derivational cycles: each mention is copied from a previ-
ous mention in the latent ordering. This is why our phylogeny
is a tree, and why our sampler is more complex. Also unlike
the ddCRP, we permit asymmetric ?distances?: if a certain
topic or language likes to copy mentions from another, the
compliment is not necessarily returned.
count), i = 0 (mention count), and for each docu-
ment index d = 1, . . . , D:
1. Choose the document?s length L and language
`. (The distributions used to choose these
are unimportant because these variables are
always observed.)
2. Choose its topic distribution ?
d
from an
asymmetric Dirichlet prior with parameters
m (Wallach et al, 2009).
3
3. For each token position k = 1, . . . , L:
(a) Choose a topic z
dk
? ?
d
.
(b) Choose a word conditioned on the topic
and language, w
dk
? ?
z
dk
`
.
(c) If w
dk
is a named entity type (PERSON,
PLACE, ORG, . . . ) rather than an ordinary
word, then increment i and:
i. create a new mention x with
x.e.t = w
dk
x.d = d x.` = `
x.i = i x.z = z
dk
x.k = k
ii. Choose the parent x.p from a distri-
bution conditioned on the attributes
just set (see ?4.1).
iii. If x.p = ?, increment m and set
x.e = a new entity e
m
. Else set
x.e = x.p.e.
iv. Choose x.n from a distribution con-
ditioned on x.p.n and x.` (see ?4.2).
Notice that the tokens w
dk
in document d are
exchangeable: by collapsing out ?
d
, we can re-
gard them as having been generated from a CRP.
Thus, for fixed values of the non-mention tokens
and their topics, the probability of generating the
mention sequence x is proportional to the prod-
uct of the probabilities of the choices in step 3 at
the positions dk where mentions were generated.
These choices generate a topic x.z (from the CRP
for document d), a type x.e.t (from ?
x.z
), a par-
ent mention (from the distribution over previous
mentions), and a name string (conditioned on the
parent?s name if any). ?5 uses this fact to construct
an MCMC sampler for the latent parts of x.
4.1 Sub-model for parent selection
To select a parent for a mention x of type t = x.e.t,
a simple model (as mentioned above) would be a
CRP: each previous mention of the same type is
selected with probability proportional to 1, and? is
3
Extension: This choice could depend on the language d.`.
777
selected with probability proportional to ?
t
> 0. A
larger choice of ?
t
results in smaller entity clusters,
because it prefers to create new entities of type t
rather than copying old ones.
We modify this story by re-weighting ? and
previous mentions according to their relative suit-
ability as the parent of x:
Pr?(x.p | x) =
exp (? ? f(x.p, x))
Z(x)
(1)
where x.p ranges over ? and all previous mentions
of the same type as x, that is, mentions p such that
p.i < x.i and p.e.t = x.e.t. The normalizing con-
stant Z(x)
def
=
?
p
exp (? ? f(x.p, x)) is chosen
so that the probabilities sum to 1.
This is a conditional log-linear model parameter-
ized by ?, where ?
k
? N (0, ?
2
k
). The features f
are extracted from the attributes of x and x.p. Our
most important feature tests whether x.p.z = x.z.
This binary feature has a high weight if authors
mainly choose mentions from the same topic. To
model which (other) topics tend to be selected, we
also have a binary feature for each parent topic
x.p.z and each topic pair (x.p.z, x.z).
4
4.2 Sub-model for name mutation
Let x denote a mention with parent p = x.p. As in
Andrews et al (2012), its name x.n is a stochastic
transduction of its parent?s name p.n. That is,
Pr?(x.n | p.n) (2)
is given by the probability that applying a random
sequence of edits to the characters of p.n would
yield x.n. The contextual probabilities of different
edits depend on learned parameters ?.
(2) is the total probability of all edit sequences
that derive x.n from p.n. It can be computed in
time O(|x.n| ? |p.n|) by dynamic programming.
The probability of a single edit sequence, which
corresponds to a monotonic alignment of x.n to
p.n, is a product of individual edit probabilities of
the form Pr?((
a
b
) | a?), which is conditioned on the
next input character a?. The edit (
a
b
) replaces input
a ? {, a?} with output b ? {} ? ? (where  is
4
Many other features could be added. In a multilingual
setting, one would similarly want to model whether English
authors select Arabic mentions. One could also imagine fea-
tures that reward proximity in the generative order (x.p.i ?
x.i), local linguistic relationships (when x.p.d = x.d and
x.p.k ? x.k), or social information flow (e.g., from main-
stream media to Twitter). One could also make more specific
versions of any feature by conjoining it with the entity type t.
the empty string and ? is the alphabet of language
x.`). Insertions and deletions are the cases where
respectively a =  or b = ?we do not allow both
at once. All other edits are substitutions. When
a? is the special end-of-string symbol #, the only
allowed edits are the insertion (

b
) and the substi-
tution (
#
#
). We define the edit probability using a
locally normalized log-linear model:
Pr?((
a
b
) | a?) =
exp(? ? f(a?, a, b))
?
a
?
,b
?
exp(? ? f(a?, a
?
, b
?
))
(3)
We use a small set of simple feature functions f ,
which consider conjunctions of the attributes of the
characters a? and b: character, character class (letter,
digit, etc.), and case (upper vs. lower).
More generally, the probability (2) may also be
conditioned on other variables such as on the lan-
guages p.` and x.`?this leaves room for a translit-
eration model when x.` 6= p.`?and on the entity
type x.t. The features in (3) may then depend on
these variables as well.
Notice that we use a locally normalized proba-
bility for each edit. This enables faster and sim-
pler training than the similar model of Dreyer et al
(2008), which uses a globally normalized probabil-
ity for the whole edit sequence.
When p = ?, we are generating a new name x.n.
We use the same model, taking?.n to be the empty
string (but with #
?
rather than # as the end-of-
string symbol). This yields a feature-based unigram
language model (whose character probabilities may
differ from usual insertion probabilities because
they see #
?
as the lookahead character).
Pragmatics. We can optionally make the model
more sophisticated. Authors tend to avoid names
x.n that readers would misinterpret (given the pre-
viously generated names). The edit model thinks
that Pr?(CIA | ?) is relatively high (because CIA is
a short string) and so is Pr?(CIA | Chuck?s Ice Art).
But in fact, if CIA has already been frequently used
to refer to the Central Intelligence Agency, then an
author is unlikely to use it for a different entity.
To model this pragmatic effect, we multiply
our definition of Pr?(x.n | p.n) by an extra fac-
tor Pr(x.e | x)
?
, where ? ? 0 is the effect
strength.
5
Here Pr(x.e | x) is the probability that
a reader correctly identifies the entity x.e. We
take this to be the probability that a reader who
knows our sub-models would guess some parent
5
Currently we omit the step of renormalizing this deficient
model. Our training procedure also ignores the extra factor.
778
having the correct entity (or ? if x is a first men-
tion):
?
p
?
:p
?
.e=x.e
w(p
?
, x)/
?
p
?
w(p
?
, x). Here p
?
ranges over mentions (including ?) that precede
x in the ordering i, and w(p
?
, x)?defined later in
sec. 5.3?is proportional to the posterior probabil-
ity that x.p = p
?
, given name x.n and topic x.z.
6
5 Inference by Block Gibbs Sampling
We use a block Gibbs sampler, which from an ini-
tial state (p
0
, i
0
, z
0
) repeats these steps:
1. Sample the ordering i from its conditional
distribution given all other variables.
2. Sample the topic vector z likewise.
3. Sample the phylogeny p likewise.
4. Output the current sample s
t
= (p, i, z).
It is difficult to draw exact samples at steps 1
and 2. Thus, we sample i or z from a simpler
proposal distribution, but correct the discrepancy
using the Independent Metropolis-Hastings (IMH)
strategy: with an appropriate probability, reject the
proposed new value and instead use another copy
of the current value (Tierney, 1994).
5.1 Resampling the ordering i
We resample the ordering i of the mentions x,
conditioned on the other variables. The current
phylogeny p already defines a partial order on x,
since each parent must precede its children. For
instance, phylogeny (a) below requires ? ? x and
? ? y. This partial order is compatible with 2
total orderings, ? ? x ? y and ? ? y ? x. By
contrast, phylogeny (b) requires the total ordering
? ? x ? y.
?
yx
(a)
?
x
y
(b)
We first sample an ordering i
?
(the ordering
of mentions with parent ?, i.e. all mentions) uni-
formly at random from the set of orderings compat-
ible with the current p. (We provide details about
this procedure in Appendix A.)
7
However, such or-
derings are not in fact equiprobable given the other
variables?some orderings better explain why that
phylogeny was chosen in the first place, according
6
Better, one could integrate over the reader?s guess of x.z.
7
The full version of this paper is available at
http://cs.jhu.edu/
?
noa/publications/
phylo-acl-14.pdf
to our competitive parent selection model (?4.1).
To correct for this bias using IMH, we accept the
proposed ordering i
?
with probability
a = min
(
1,
Pr(p, i?, z,x | ?,?)
Pr(p, i, z,x | ?,?)
)
(4)
where i is the current ordering. Otherwise we reject
i
?
and reuse i for the new sample.
5.2 Resampling the topics z
Each context word and each named entity is asso-
ciated with a latent topic. The topics of context
words are assumed exchangeable, and so we re-
sample them using Gibbs sampling (Griffiths and
Steyvers, 2004).
Unfortunately, this is prohibitively expensive for
the (non-exchangeable) topics of the named men-
tions x. A Gibbs sampler would have to choose
a new value for x.z with probability proportional
to the resulting joint probability of the full sample.
This probability is expensive to evaluate because
changing x.z will change the probability of many
edges in the current phylogeny p. (Equation (1)
puts x is in competition with other parents, so ev-
ery mention y that follows x must recompute how
happy it is with its current parent y.p.)
Rather than resampling one topic at a time, we re-
sample z as a block. We use a proposal distribution
for which block sampling is efficient, and use IMH
to correct the error in this proposal distribution.
Our proposal distribution is an undirected graph-
ical model whose random variables are the topics
z and whose graph structure is given by the current
phylogeny p:
Q(z) ?
?
x 6=?
?
x
(x.z)?
x.p,x
(x.p.z, x.z) (5)
Q(z) is an approximation to the posterior distri-
bution over z. As detailed below, a proposal can
be sampled from Q(z) in time O(|z|K
2
) where K
is the number of topics, because the only interac-
tions among topics are along the edges of the tree
p. The unary factor ?
x
gives a weight for each
possible value of x.z, and the binary factor ?
x.p,x
gives a weight for each possible value of the pair
(x.p.z, x.z).
The ?
x
(x.z) factors in (5) approximate the topic
model?s prior distribution over z. ?
x
(x.z) is pro-
portional to the probability that a Gibbs sampling
step for an ordinary topic model would choose this
value of x.z. This depends on whether?in the
779
current sample?x.z is currently common in x?s
document and x.t is commonly generated by x.z.
It ignores the fact that we will also be resampling
the topics of the other mentions.
The ?
x.p,x
factors in (5) approximate Pr(p |
z, i) (up to a constant factor), where p is the current
phylogeny. Specifically, ?
x.p,x
approximates the
probability of a single edge. It ought to be given
by (1), but we use only the numerator of (1), which
avoids modeling the competition among parents.
We sample from Q using standard methods, sim-
ilar to sampling from a linear-chain CRF by run-
ning the backward algorithm followed by forward
sampling. Specifically, we run the sum-product
algorithm from the leaves up to the root ?, at each
node x computing the following for each topic z:
?
x
(z)
def
= ?
x
(z) ?
?
y?children(x)
?
z
?
?
x,y
(z, z
?
) ? ?
y
(z
?
)
Then we sample from the root down to the leaves,
first sampling ?.z from ?
?
, then at each x 6= ?
sampling the topic x.z to be z with probability
proportional to ?
x.p,x
(x.p.z, z) ? ?
x
(z).
Again we use IMH to correct for the bias in Q:
we accept the resulting proposal
?
z with probability
min
(
1,
Pr(p, i,
?
z,x | ?,?)
Pr(p, i, z,x | ?,?)
?
Q(z)
Q(
?
z)
)
(6)
While Pr(p, i,
?
z,x | ?,?) might seem slow to
compute because it contains many factors (1) with
different denominators Z(x), one can share work
by visiting the mentions x in their order i. Most
summands in Z(x) were already included in Z(x
?
),
where x
?
is the latest previous mention having the
same attributes as x (e.g., same topic).
5.3 Resampling the phylogeny p
It is easy to resample the phylogeny. For each x, we
must choose a parent x.p from among the possible
parents p (having p.i < x.i and p.e.t = x.e.t).
Since the ordering i prevents cycles, the resulting
phylogeny p is indeed a tree.
Given the topics z, the ordering i, and the ob-
served names, we choose an x.p value according
to its posterior probability. This is proportional to
w(x.p, x)
def
= Pr?(x.p | x) ? Pr?(x.n | x.p.n),
independent of any other mention?s choice of par-
ent. The two factors here are given by (1) and (2)
respectively. As in the previous section, the de-
nominators Z(x) in the Pr(x.p | x) factors can be
computed efficiently with shared work.
With the pragmatic model (section 4.2), the par-
ent choices are no longer independent; then the
samples of p should be corrected by IMH as usual.
5.4 Initializing the sampler
The initial sampler state (z
0
,p
0
, i
0
) is obtained as
follows. (1) We fix topics z
0
via collapsed Gibbs
sampling (Griffiths and Steyvers, 2004). The sam-
pler is run for 1000 iterations, and the final sam-
pler state is taken to be z
0
. This process treats all
topics as exchangeable, including those associated
with named entities.(2) Given the topic assignment
z
0
, initialize p
0
to the phylogeny rooted at ? that
maximizes
?
x
logw(x.p, x). This is a maximum
rooted directed spanning tree problem that can be
solved in time O(n
2
) (Tarjan, 1977). The weight
w(x.p, x) is defined as in section 5.3?except that
since we do not yet have an ordering i, we do not
restrict the possible values of x.p to mentions p
with p.i < x.p.i. (3) Given p
0
, sample an ordering
i
0
using the procedure described in ?5.1.
6 Parameter Estimation
Evaluating the likelihood and its partial derivatives
with respect to the parameters of the model requires
marginalizing over our latent variables. As this
marginalization is intractable, we resort to Monte
Carlo EM procedure (Levine and Casella, 2001)
which iterates the following two steps:
E-step: Collect samples by MCMC simulation as
in ?5, given current model parameters ? and ?.
M-step: Improve ? and ? to increase
8
L
def
=
1
S
S
?
s=1
log Pr?,?(x,ps, is, zs) (7)
It is not necessary to locally maximize L at each
M-step, merely to improve it if it is not already
at a local maximum (Dempster et al, 1977). We
improve it by a single update: at the tth M-step, we
update our parameters to ?
t
= (?
t
,?
t
)
?
t
= ?
t?1
+ ??
t
?
?
L(x,?
t?1
) (8)
where ? is a fixed scaling term and ?
t
is an adap-
tive learning rate given by AdaGrad (Duchi et al,
2011).
We now describe how to compute the gradient
?
?
L. The gradient with respect to the parent se-
8
We actually do MAP-EM, which augments (7) by adding
the log-likelihoods of ? and ? under a Gaussian prior.
780
lection parameters ? is
?
1
S
?
?
f(p, x)?
?
p
?
Pr?(p
?
| x)f(p
?
, x)
?
?
(9)
The outer summation ranges over all edges in the
S samples. The other variables in (9) are associ-
ated with the edge being summed over. That edge
explains a mention x as a mutation of some parent
p in the context of a particular sample (p
s
, i
s
, z
s
).
The possible parents p
?
range over ? and the men-
tions that precede x according to the ordering i
s
,
while the features f and distribution Pr? depend
on the topics z
s
.
As for the mutation parameters, let c
p,x
be the
fraction of samples in which p is the parent of x.
This is the expected number of times that the string
p.n mutated into x.n. Given this weighted set of
string pairs, let c
a?,a,b
be the expected number of
times that edit (
a
b
) was chosen in context a?: this
can be computed using dynamic programming to
marginalize over the latent edit sequence that maps
p.n to x.n, for each (p, x). The gradient of L with
respect to ? is
?
a?,a,b
c
a?,a,b
(f(a?, a, b)?
?
a
?
,b
?
Pr?(a
?
, b
?
| a?)f(a?, a
?
, b
?
))
(10)
7 Consensus Clustering
From a single phylogeny p, we deterministically
obtain a clustering e by removing the root ?. Each
of the resulting connected components corresponds
to a cluster of mentions. Our model gives a distribu-
tion over phylogenies p (given observations x and
learned parameters ?)?and thus gives a posterior
distribution over clusterings e, which can be used
to answer various queries.
A traditional query is to request a single cluster-
ing e. We prefer the clustering e
?
that minimizes
Bayes risk (MBR) (Bickel and Doksum, 1977):
e
?
= argmin
e?
?
e
L(e
?
, e) Pr(e | x,?,?) (11)
This minimizes our expected loss, where L(e
?
, e)
denotes the loss associated with picking e
?
when
the true clustering is e. In practice, we again esti-
mate the expectation by sampling e values.
The Rand index (Rand, 1971)?unlike our actual
evaluation measure?is an efficient choice of loss
function L for use with (11):
R(e
?
, e)
def
=
TP + TN
TP + FP + TN + FN
=
TP + TN
(
N
2
)
where the true positives (TP), true negatives (TN),
false positives (FP), and false negatives (FN) use
the clustering e to evaluate how well e
?
classi-
fies the
(
N
2
)
mention pairs as coreferent or not.
More similar clusterings achieve larger R, with
R(e
?
, e) = 1 iff e
?
= e. In all cases, 0 ?
R(e
?
, e) = R(e, e
?
) ? 1.
The MBR decision rule for the (negated) Rand
index is easily seen to be equivalent to
e
?
= argmax
e?
E[TP] + E[TN] (12)
= argmax
e?
?
i,j: x
i
?x
j
s
ij
+
?
i,j: x
i
6?x
j
(1? s
ij
)
where ? denotes coreference according to e
?
. As
explained above, the s
ij
are coreference probabil-
ities s
ij
that can be estimated from a sample of
clusterings e.
This objective corresponds to min-max graph
cut (Ding et al, 2001), an NP-hard problem with
an approximate solution (Nie et al, 2010).
9
8 Experiments
In this section, we describe experiments on three
different datasets. Our main results are described
first: Twitter features many instances of name vari-
ation that we would like our model to be able to
learn. We also report the performance of different
ablations of our full approach, in order to see which
consistently helped across the different splits. We
report additional experiments on the ACE 2008 cor-
pus, and on a political blog corpus, to demonstrate
that our approach is applicable in different settings.
For Twitter and ACE 2008, we report the stan-
dard B
3
metric (Bagga and Baldwin, 1998a). For
the political blog dataset, the reference does not
consist of entity annotations, and so we follow the
evaluation procedure of Yogatama et al (2012).
8.1 Twitter
Data. We use a novel corpus of Twitter posts dis-
cussing the 2013 Grammy Award ceremony. This
is a challenging corpus, featuring many instances
9
In our experiments, we run the clustering algorithm five
times, initialized from samples chosen at random from the last
10% of the sampler run, and keep the clustering that achieved
highest expected Rand score.
781
of name variation. The dataset consists of five splits
(by entity), the smallest of which is 604 mentions
and the largest is 1374. We reserve the largest split
for development purposes, and report our results
on the remaining four. Appendix B provides more
detail about the dataset.
Baselines. We use the discriminative entity cluster-
ing algorithm of Green et al (2012) as our baseline;
their approach was found to outperform another
generative model which produced a flat cluster-
ing of mentions via a Dirichlet process mixture
model. Their method uses Jaro-Winkler string sim-
ilarity to match names, then clusters mentions with
matching names (for disambiguation) by compar-
ing their unigram context distributions using the
Jenson-Shannon metric. We also compare to the
EXACT-MATCH baseline, which assigns all strings
with the same name to the same entity.
Procedure. We run four test experiments in which
one split is used to pick model hyperparameters
and the remaining three are used for test. For the
discriminative baseline, we tune the string match
threshold, context threshold, and the weight of the
context model prior (all via grid search). For our
model, we tune only the fixed weight of the root
feature, which determines the precision/recall trade-
off (larger values of this feature result in more
attachments to ? and hence more entities). We
leave other hyperparameters fixed: 16 latent top-
ics, and Gaussian priors N (0, 1) on all log-linear
parameters. For PHYLO, the entity clustering is
the result of (1) training the model using EM, (2)
sampling from the posterior to obtain a distribu-
tion over clusterings, and (3) finding a consensus
clustering. We use 20 iterations of EM with 100
samples per E-step for training, and use 1000 sam-
ples after training to estimate the posterior. We
report results using three variations of our model:
PHYLO does not consider mention context (all men-
tions effectively have the same topic) and deter-
mines mention entities from a single sample of
p (the last); PHYLO+TOPIC adds context (?5.2);
PHYLO+TOPIC+MBR uses the full posterior and
consensus clustering to pick the output clustering
(?7). Our results are shown in Table 1.
10
10
Our single-threaded implementation took around 15 min-
utes per fold of the Twitter corpus on a personal laptop with
a 2.3 Ghz Intel Core i7 processor (including time required to
parse the data files). Typical acceptance rates for ordering and
topic proposals ranged from 0.03 to 0.08.
Mean Test B
3
P R F1
EXACT-MATCH 99.6 53.7 69.8
Green et al (2012) 92.1 69.8 79.3
PHYLO 85.3 91.4 88.7
PHYLO+TOPIC 92.8 90.8 91.8
PHYLO+TOPIC+MBR 92.9 90.9 91.9
Table 1: Results for the Twitter dataset. Higher B
3
scores
are better. Note that each number is averaged over four
different test splits. In three out of four experiments,
PHYLO+TOPIC+MBR achieved the highest F1 score; in one
case PHYLO+TOPIC won by a small margin.
Test B
3
P R F1
PER
EXACT-MATCH 98.0 81.2 88.8
Green et al (2012) 95.0 88.9 91.9
PHYLO+TOPIC+MBR 97.2 88.6 92.7
ORG
EXACT-MATCH 98.2 78.3 87.1
Green et al (2012) 92.1 88.5 90.3
PHYLO+TOPIC+MBR 95.5 80.9 87.6
Table 2: Results for the ACE 2008 newswire dataset.
8.2 Newswire
Data. We use the ACE 2008 dataset, which is
described in detail in Green et al (2012). It is
split into a development portion and a test portion.
The baseline system took the first mention from
each (gold) within-document coreference chain as
the canonical mention, ignoring other mentions in
the chain; we follow the same procedure in our
experiments.
11
Baselines & Procedure. We use the same base-
lines as in ?8.1. On development data, modeling
pragmatics as in ?4.2 gave large improvements for
organizations (8 points in F-measure), correcting
the tendency to assume that short names like CIA
were coincidental homonyms. Hence we allowed
? > 0 and tuned it on development data.
12
Results
are in Table 2.
8.3 Blogs
Data. The CMU political blogs dataset consists of
3000 documents about U.S. politics (Yano et al,
2009). Preprocessed as described in Yogatama et al
(2012), the data consists of 10647 entity mentions.
11
That is, each within-document coreference chain is
mapped to a single mention as a preprocessing step.
12
We used only a simplified version of the pragmatic model,
approximating w(p
?
, x) as 1 or 0 according to whether p
?
.n =
x.n. We also omitted the IMH step from section 5.3. The
other results we report do not use pragmatics at all, since we
found that it gave only a slight improvement on Twitter.
782
Unlike our other datasets, mentions are not anno-
tated with entities: the reference consists of a table
of 126 entities, where each row is the canonical
name of one entity.
Baselines. We compare to the system results
reported in Figure 2 of Yogatama et al (2012).
This includes a baseline hierarchical clustering ap-
proach, the ?EEA? name canonicalization system
of Eisenstein et al (2011), as well the model pro-
posed by Yogatama et al (2012). Like the output
of our model, the output of their hierarchical clus-
tering baseline is a mention clustering, and there-
fore must be mapped to a table of canonical entity
names to compare to the reference table.
Procedure & Results We tune our method as in
previous experiments, on the initialization data
used by Yogatama et al (2012) which consists of
a subset of 700 documents of the full dataset. The
tuned model then produced a mention clustering
on the full political blog corpus. As the mapping
from clusters to a table is not fully detailed in Yo-
gatama et al (2012), we used a simple heuristic:
the most frequent name in each cluster is taken as
the canonical name, augmented by any titles from
a predefined list appearing in any other name in
the cluster. The resulting table is then evaluated
against the reference, as described in Yogatama et
al. (2012). We achieved a response score of 0.17
and a reference score of 0.61. Though not state-of-
the-art, this result is close to the score of the ?EEA?
system of Eisenstein et al (2011), as reported in
Figure 2 of Yogatama et al (2012), which is specif-
ically designed for the task of canonicalization.
8.4 Discussion
On the Twitter dataset, we obtained a 12.6-point F1
improvement over the baseline. To understand our
model?s behavior, we looked at the sampled phy-
logenetic trees on development data. One reason
our model does well in this noisy domain is that
it is able to relate seemingly dissimilar names via
successive steps. For instance, our model learned
to relate many variations of LL Cool J:
Cool James LLCoJ El-El Cool John
LL LL COOL JAMES LLCOOLJ
In the sample we inspected, these mentions were
also assigned the same topic, further boosting the
probability of the configuration.
The ACE dataset, consisting of editorialized
newswire, naturally contains less name variation
than Twitter data. Nonetheless, we find that the
variation that does appear is often properly handled
by our model. For instance, we see several in-
stances of variation due to transliteration that were
all correctly grouped together, such as Megawati
Soekarnoputri and Megawati Sukarnoputri. The prag-
matic model was also effective in grouping com-
mon acronyms into the same entity.
We found that multiple samples tend to give dif-
ferent phylogenies (so the sampler is mobile), but
essentially the same clustering into entities (which
is why consensus clustering did not improve much
over simply using the last sample). Random restarts
of EM might create more variety by choosing dif-
ferent locally optimal parameter settings. It may
also be beneficial to explore other sampling tech-
niques (Bouchard-C?ot?e, 2014).
Our method assembles observed names into an
evolutionary tree. However, the true tree must in-
clude many names that fall outside our small ob-
served corpora, so our model would be a more
appropriate fit for a far larger corpus. Larger cor-
pora also offer stronger signals that might enable
our Monte Carlo methods to mix faster and detect
regularities more accurately.
A common error of our system is to connect
mentions that share long substrings, such as dif-
ferent PERSONs who share a last name, or differ-
ent ORGANIZATIONs that contain University of. A
more powerful name mutation than the one we use
here would recognize entire words, for example
inserting a common title or replacing a first name
with its common nickname. Modeling the internal
structure of names (Johnson, 2010; Eisenstein et
al., 2011; Yogatama et al, 2012) in the mutation
model is a promising future direction.
9 Conclusions
Our primary contribution consists of new model-
ing ideas, and associated inference techniques, for
the problem of cross-document coreference resolu-
tion. We have described how writers systematically
plunder (?) and then systematically modify (?) the
work of past writers. Inference under such models
could also play a role in tracking evolving memes
and social influence, not merely in establishing
strict coreference. Our model also provides an al-
ternative to the distance-dependent CRP.
2
Our implementation is available for re-
search use at: https://bitbucket.org/
noandrews/phyloinf.
783
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 344?355, Jeju, Korea, July.
Amit Bagga and Breck Baldwin. 1998a. Algorithms
for scoring coreference chains. In In The First In-
ternational Conference on Language Resources and
Evaluation Workshop on Linguistics Coreference,
pages 563?566.
Amit Bagga and Breck Baldwin. 1998b. Entity-
based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 1, ACL ?98, pages
79?85, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Alex Baron and Marjorie Freedman. 2008. Who
is who and what is what: Experiments in cross-
document co-reference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 274?283, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe-
matical Statistics : Basic Ideas and Selected Topics.
Holden-Day, Inc.
David M. Blei and Peter I. Frazier. 2011. Distance
dependent chinese restaurant processes. J. Mach.
Learn. Res., 12:2461?2488, November.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Alexandre Bouchard-C?ot?e, David Hall, Thomas L.
Griffiths, and Dan Klein. 2013. Automated re-
construction of ancient languages using probabilis-
tic models of sound change. Proceedings of the Na-
tional Academy of Sciences.
Alexandre Bouchard-C?ot?e. 2014. Sequential Monte
Carlo (SMC) for Bayesian phylogenetics. Bayesian
phylogenetics: methods, algorithms, and applica-
tions.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A comparison of string metrics for
matching names and records. In KDD Workshop on
data cleaning and object consolidation.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38.
C.H.Q. Ding, Xiaofeng He, Hongyuan Zha, Ming Gu,
and H.D. Simon. 2001. A min-max cut algorithm
for graph partitioning and data clustering. In Data
Mining, 2001. ICDM 2001, Proceedings IEEE Inter-
national Conference on, pages 107 ?114.
Mark Dredze, Michael J Paul, Shane Bergsma, and
Hieu Tran. 2013. Carmen: A twitter geolocation
system with applications to public health. In AAAI
Workshop on Expanding the Boundaries of Health
Informatics Using AI (HIAI).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1080?1089, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1971?1982.
Association for Computational Linguistics.
Jacob Eisenstein, Tae Yano, William W. Cohen,
Noah A. Smith, and Eric P. Xing. 2011. Structured
databases of named entities from bayesian nonpara-
metrics. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ?11, pages
2?12, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
T. Finin, Z. Syed, J. Mayfield, P. McNamee, and C. Pi-
atko. 2009. Using Wikitology for cross-document
entity coreference resolution. In AAAI Spring Sym-
posium on Learning by Reading and Learning to
Read.
Spence Green, Nicholas Andrews, Matthew R. Gorm-
ley, Mark Dredze, and Christopher D. Manning.
2012. Entity clustering across languages. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 60?69, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Stephen Guo, Ming-Wei Chang, and Emre K?c?man.
2013. To link or not to link? a study on end-to-end
tweet entity linking. In Proceedings of NAACL-HLT,
pages 1020?1030.
784
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 385?393,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Mark Johnson. 2010. Pcfgs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 1148?1157,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Zornitsa Kozareva and Sujith Ravi. 2011. Unsuper-
vised name ambiguity resolution using a generative
model. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ?11, pages
105?112, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Richard A. Levine and George Casella. 2001. Im-
plementations of the Monte Carlo EM Algorithm.
Journal of Computational and Graphical Statistics,
10(3):422?439.
Feiping Nie, Chris H. Q. Ding, Dijun Luo, and Heng
Huang. 2010. Improved minmax cut graph cluster-
ing with nonnegative relaxation. In Jos?e L. Balc?azar,
Francesco Bonchi, Aristides Gionis, and Mich`ele
Sebag, editors, ECML/PKDD (2), volume 6322 of
Lecture Notes in Computer Science, pages 451?466.
Springer.
E. H. Porter and W. E. Winkler, 1997. Approximate
String Comparison and its Effect on an Advanced
Record Linkage System, chapter 6, pages 190?199.
U.S. Bureau of the Census.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66(336):846?850.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 1050?1058, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learn-
ing string edit distance. Technical Report CS-TR-
532-96, Princeton University, Department of Com-
puter Science.
Eric Sven Ristad and Peter N. Yianilos. 1998.
Learning string edit distance. IEEE Transactions
on Pattern Recognition and Machine Intelligence,
20(5):522?532, May.
Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-scale
cross-document coreference using distributed infer-
ence and hierarchical models. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 793?803, Portland, Oregon, USA, June.
Association for Computational Linguistics.
R E Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?35.
Luke Tierney. 1994. Markov Chains for Exploring
Posterior Distributions. The Annals of Statistics,
22(4):1701?1728.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking lda: Why priors matter. In
Advances in Neural Information Processing Systems,
pages 1973?1981.
Michael Wick, Sameer Singh, and Andrew McCallum.
2012. A discriminative hierarchical model for fast
coreference at large scale. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 379?388, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
William E. Winkler. 1999. The state of record link-
age and current research problems. Technical report,
Statistical Research Division, U.S. Census Bureau.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
477?485, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dani Yogatama, Yanchuan Sim, and Noah A. Smith.
2012. A probabilistic model for canonicalizing
named entity mentions. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers - Volume 1, ACL
?12, pages 685?693, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
785
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625?630,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Stochastic Contextual Edit Distance and Probabilistic FSTs
Ryan Cotterell and Nanyun Peng and Jason Eisner
Department of Computer Science, Johns Hopkins University
{ryan.cotterell,npeng1,jason}@cs.jhu.edu
Abstract
String similarity is most often measured
by weighted or unweighted edit distance
d(x, y). Ristad and Yianilos (1998) de-
fined stochastic edit distance?a probabil-
ity distribution p(y | x) whose parame-
ters can be trained from data. We general-
ize this so that the probability of choosing
each edit operation can depend on contex-
tual features. We show how to construct
and train a probabilistic finite-state trans-
ducer that computes our stochastic con-
textual edit distance. To illustrate the im-
provement from conditioning on context,
we model typos found in social media text.
1 Introduction
Many problems in natural language processing
can be viewed as stochastically mapping one
string to another: e.g., transliteration, pronuncia-
tion modeling, phonology, morphology, spelling
correction, and text normalization. Ristad and
Yianilos (1998) describe how to train the param-
eters of a stochastic editing process that moves
through the input string x from left to right, trans-
forming it into the output string y. In this paper we
generalize this process so that the edit probabilities
are conditioned on input and output context.
We further show how to model the conditional
distribution p(y | x) as a probabilistic finite-state
transducer (PFST), which can be easily combined
with other transducers or grammars for particu-
lar applications. We contrast our probabilistic
transducers with the more general framework of
weighted finite-state transducers (WFST), explain-
ing why our restriction provides computational ad-
vantages when reasoning about unknown strings.
Constructing the finite-state transducer is tricky,
so we give the explicit construction for use by oth-
ers. We describe how to train its parameters when
the contextual edit probabilities are given by a log-
linear model. We provide a library for training
both PFSTs and WFSTs that works with OpenFST
(Allauzen et al, 2007), and we illustrate its use
with simple experiments on typos, which demon-
strate the benefit of context.
2 Stochastic Contextual Edit Distance
Our goal is to define a family of probability distri-
butions p
?
(y | x), where x ? ?
?
x
and y ? ?
?
y
are
input and output strings over finite alphabets ?
x
and ?
y
, and ? is a parameter vector.
Let x
i
denote the i
th
character of x. If i < 1 or
i > |x|, then x
i
is the distinguished symbol BOS
or EOS (?beginning/end of string?). Let x
i:j
denote
the (j ? i)-character substring x
i+1
x
i+2
? ? ?x
j
.
Consider a stochastic edit process that reads in-
put string x while writing output string y. Having
read the prefix x
0:i
and written the prefix y
0:j
, the
process must stochastically choose one of the fol-
lowing 2|?
y
|+ 1 edit operations:
? DELETE: Read x
i+1
but write nothing.
? INSERT(t) for some t ? ?
y
: Write y
j+1
= t
without reading anything.
? SUBST(t) for some t ? ?
y
: Read x
i+1
and
write y
j+1
= t. Note that the traditional
COPY operation is obtained as SUBST(x
i+1
).
In the special case where x
i+1
= EOS, the choices
are instead INSERT(t) and HALT (where the latter
may be viewed as copying the EOS symbol).
The probability of each edit operation depends
on ? and is conditioned on the left input context
C
1
= x
(i?N
1
):i
, the right input context C
2
=
x
i:(i+N
2
)
, and the left output context C
3
=
y
(j?N
3
):j
, where the constants N
1
, N
2
, N
3
? 0
specify the model?s context window sizes.
1
Note
that the probability cannot be conditioned on right
output context because those characters have not
yet been chosen. Ordinary stochastic edit dis-
tance (Ristad and Yianilos, 1998) is simply the
case (N
1
, N
2
, N
3
) = (0, 1, 0), while Bouchard-
C?ot?e et al (2007) used roughly (1, 2, 0).
Now p
?
(y | x) is the probability that this pro-
cess will write y as it reads a given x. This is the
total probability (given x) of all latent edit oper-
ation sequences that write y. In general there are
exponentially many such sequences, each imply-
ing a different alignment of y to x.
1
IfN
2
= 0, so that we do not condition on x
i+1
, we must
still condition on whether x
i+1
= EOS (a single bit). We
gloss over special handling for N
2
= 0; but it is in our code.
625
This model is reminiscent of conditional mod-
els in MT that perform stepwise generation of one
string or structure from another?e.g., string align-
ment models with contextual features (Cherry and
Lin, 2003; Liu et al, 2005; Dyer et al, 2013), or
tree transducers (Knight and Graehl, 2005).
3 Probabilistic FSTs
We will construct a probabilistic finite-state
transducer (PFST) that compactly models p
?
(y |
x) for all (x, y) pairs.
2
Then various computa-
tions with this distribution can be reduced to stan-
dard finite-state computations that efficiently em-
ploy dynamic programming over the structure of
the PFST, and the PFST can be easily combined
with other finite-state distributions and functions
(Mohri, 1997; Eisner, 2001).
A PFST is a two-tape generalization of the well-
known nondeterministic finite-state acceptor. It
is a finite directed multigraph where each arc is
labeled with an input in ?
x
? {}, an output in
?
y
?{}, and a probability in [0, 1]. ( is the empty
string.) Each state (i.e., vertex) has a halt proba-
bility in [0, 1], and there is a single initial state q
I
.
Each path from q
I
to a final state q
F
has
? an input string x, given by the concatenation
of its arcs? input labels;
? an output string y, given similarly;
? a probability, given by the product of its arcs?
probabilities and the halt probability of q
F
.
We define p(y | x) as the total probability of all
paths having input x and output y. In our applica-
tion, a PFST path corresponds to an edit sequence
that reads x and writes y. The path?s probability is
the probability of that edit sequence given x.
We must take care to ensure that for any x ? ?
?
x
,
the total probability of all paths accepting x is 1,
so that p
?
(y | x) is truly a conditional probability
distribution. This is guaranteed by the following
sufficient conditions (we omit the proof for space),
which do not seem to appear in previous literature:
? For each state q and each symbol b ? ?
x
, the
arcs from q with input label b or  must have
total probability of 1. (These are the available
choices if the next input character is x.)
2
Several authors have given recipes for finite-state trans-
ducers that perform a single contextual edit operation (Kaplan
and Kay, 1994; Mohri and Sproat, 1996; Gerdemann and van
Noord, 1999). Such ?rewrite rules? can be individually more
expressive than our simple edit operations of section 2; but it
is unclear how to train a cascade of them to model p(y | x).
? For each state q, the halt action and the arcs
from q with input label  must have total
probability of 1. (These are the available
choices if there is no next input character.)
? Every state q must be co-accessible, i.e., there
must be a path of probability > 0 from q to
some q
F
. (Otherwise, the PFST could lose
some probability mass to infinite paths. The
canonical case of this involves an loop q ? q
with input label  and probability 1.)
We take the first two conditions to be part of the
definition of a PFST. The final condition requires
our PFST to be ?tight? in the same sense as a
PCFG (Chi and Geman, 1998), although the tight-
ness conditions for a PCFG are more complex.
In section 7, we discuss the costs and benefits of
PFSTs relative to other options.
4 The Contextual Edit PFST
We now define a PFST topology that concisely
captures the contextual edit process of section 2.
We are given the alphabets ?
x
,?
y
and the context
window sizes N
1
, N
2
, N
3
? 0.
For each possible context triple C =
(C
1
, C
2
, C
3
) as defined in section 2, we construct
an edit state q
C
whose outgoing arcs correspond
to the possible edit operations in that context.
One might expect that the SUBST(t) edit oper-
ation that reads s = x
i+1
and writes t = y
j+1
would correspond to an arc with s, t as its input
and output labels. However, we give a more effi-
cient design where in the course of reaching q
C
,
the PFST has already read s and indeed the en-
tire right input context C
2
= x
i:(i+N
2
)
. So our
PFST?s input and output actions are ?out of sync?:
its read head is N
2
characters ahead of its write
head. When the edit process of section 2 has read
x
0:i
and written y
0:j
, our PFST implementation
will actually have read x
0:(i+N
2
)
and written y
0:j
.
This design eliminates the need for nondeter-
ministic guessing (of the right context x
i:(i+N
2
)
) to
determine the edit probability. The PFST?s state is
fully determined by the characters that it has read
and written so far. This makes left-to-right com-
position in section 5 efficient.
A fragment of our construction is illustrated in
Figure 1. An edit state q
C
has the following out-
going edit arcs, each of which corresponds to an
edit operation that replaces some s ? ?
x
? {}
with some t ? ?
y
? {}:
626
b     c
y     _
a     bc
z       _
a      b
x      _
a     ba
x       _
?
:
z
 
/
?
:
?
 
/
 
?
:
y
 
/
?:x / 
i
n
s
e
r
t
 
z
insert x
s
u
b
s
t
i
t
u
t
e
 
y
 
f
o
r
 
b
d
e
l
e
t
e
 
b
b      c
x      _   
r
e
a
d
 
c
c
:
?
 
/
 
1
      p(INSERT(x) | (a,bc,x) )
 
 
 
 
 
 
 
p
(
I
N
S
E
R
T
(
z
)
 
|
 
(
a
,
b
c
,
x
)
 
)
p
(
S
U
B
S
T
(
y
)
 
|
 
(
a
,
b
c
,
x
)
 
)
 
 
 
 
 
p
(
D
E
L
E
T
E
(
b
)
 
|
 
(
a
,
b
c
,
x
)
 
)
      c
Figure 1: A fragment of a PFST withN
1
= 1, N
2
= 2, N
3
=
1. Edit states are shaded. A state q
C
is drawn with left and
right input contexts C
1
, C
2
in the left and right upper quad-
rants, and left output context C
3
in the left lower quadrant.
Each arc is labeled with input:output / probability.
? A single arc with probability p(DELETE | C)
(here s = (C
2
)
1
, t = )
? For each t ? ?
y
, an arc with probability
p(INSERT(t) | C) (here s = )
? For each t ? ?
y
, an arc with probability
p(SUBST(t) | C) (here s = (C
2
)
1
)
Each edit arc is labeled with input  (because s
has already been read) and output t. The arc leads
from q
C
to q
C
?
, a state that moves s and t into
the left contexts: C
?
1
= suffix(C
1
s,N
1
), C
?
2
=
suffix(C
2
, N
2
? |s|), C
?
3
= suffix(C
3
t,N
3
).
Section 2 mentions that the end of x requires
special handling. An edit state q
C
whose C
2
=
EOS
N
2
only has outgoing INSERT(t) arcs, and has
a halt probability of p(HALT | C). The halt proba-
bility at all other states is 0.
We must also build some non-edit states of the
form q
C
where |C
2
| < N
2
. Such a state does not
have the full N
2
characters of lookahead that are
needed to determine the conditional probability of
an edit. Its outgoing arcs deterministically read
a new character into the right input context. For
each s ? ?
x
, we have an arc of probability 1 from
q
C
to q
C
?
where C
?
= (C
1
, C
2
s, C
3
), labeled with
input s and output . Following such arcs from q
C
will reach an edit state after N
2
? |C
2
| steps.
The initial state q
I
with I = (BOS
N
1
, , BOS
N
3
)
is a non-edit state. Other non-edit states are con-
structed only when they are reachable from an-
other state. In particular, a DELETE or SUBST arc
always transitions to a non-edit state, since it con-
sumes one of the lookahead characters.
5 Computational Complexity
We summarize some useful facts without proof.
For fixed alphabets ?
x
and ?
y
, our final
PFST, T , has O(|?
x
|
N
1
+N
2
|?
y
|
N
3
) states and
O(|?
x
|
N
1
+N
2
|?
y
|
N
3
+1
) arcs. Composing this T
with deterministic FSAs takes time linear in the
size of the result, using a left-to-right, on-the-fly
implementation of the composition operator ?.
Given strings x and y, we can compute p
?
(y |
x) as the total probability of all paths in x ? T ? y.
This acyclic weighted FST has O(|x| ? |y|) states
and arcs. It takes onlyO(|x| ? |y|) time to construct
it and sum up its paths by dynamic programming,
just as in other edit distance algorithms.
Given only x, taking the output language of
x ? T yields the full distribution p
?
(y | x)
as a cyclic PFSA with O(|x| ? ?
N
3
y
) states and
O(|x| ? ?
N
3
+1
y
) arcs. Finding its most probable
path (i.e., most probable aligned y) takes time
O(|arcs| log |states|), while computing every arc?s
expected number of traversals under p(y | x) takes
time O(|arcs| ? |states|).
3
p
?
(y | x) may be used as a noisy channel
model. Given a language model p(x) repre-
sented as a PFSA X , X ? T gives p(x, y) for all
x, y. In the case of an n-gram language model
with n ? N
1
+ N
2
, this composition is effi-
cient: it merely reweights the arcs of T . We
use Bayes? Theorem to reconstruct x from ob-
served y: X ? T ? y gives p(x, y) (proportional
to p(x | y)) for each x. This weighted FSA has
O(?
N
1
+N
2
x
? |y|) states and arcs.
6 Parameterization and Training
While the parameters ? could be trained via var-
ious objective functions, it is particularly effi-
cient to compute the gradient of conditional log-
likelihood,
?
k
log p
?
(y
k
| x
k
), given a sample
of pairs (x
k
, y
k
). This is a non-convex objective
function because of the latent x-to-y alignments:
we do not observe which path transduced x
k
to y
k
.
Recall from section 5 that these possible paths are
represented by the small weighted FSA x
k
?T ?y
k
.
Now, a path?s probability is defined by multiply-
ing the contextual probabilities of edit operations
e. As suggested by Berg-Kirkpatrick et al (2010),
we model these steps using a conditional log-
linear model, p
?
(e | C)
def
=
1
Z
C
exp
(
? ?
~
f(C, e)
)
.
3
Speedups: In both runtimes, a factor of |x| can be elimi-
nated from |states| by first decomposing x ?T into its O(|x|)
strongly connected components. And the |states| factor in the
second runtime is unnecessary in practice, as just the first few
iterations of conjugate gradient are enough to achieve good
approximate convergence when solving the sparse linear sys-
tem that defines the forward probabilities in the cyclic PFSA.
627
To increase log p
?
(y
k
| x
k
), we must raise the
probability of the edits e that were used to trans-
duce x
k
to y
k
, relative to competing edits from the
same contexts C. This means raising ? ? f(C, e)
and/or lowering Z
C
. Thus, log p
?
(y
k
| x
k
) de-
pends only on the probabilities of edit arcs in T
that appear in x
k
? T ? y
k
, and the competing edit
arcs from the same edit states q
C
.
The gradient?
?
log p
?
(y
k
| x
k
) takes the form
?
C,e
c(C, e)
[
~
f(C, e)?
?
e
?
p
?
(e
?
| C)
~
f(C, e
?
)
]
where c(C, e) is the expected number of times that
e was chosen in context C given (x
k
, y
k
). (That
can be found by the forward-backward algorithm
on x
k
?T ? y
k
.) So the gradient adds up the differ-
ences between observed and expected feature vec-
tors at contexts C, where contexts are weighted by
how many times they were likely encountered.
In practice, it is efficient to hold the counts
c(C, e) constant over several gradient steps, since
this amortizes the work of computing them. This
can be viewed as a generalized EM algorithm that
imputes the hidden paths (giving c) at the ?E? step
and improves their probability at the ?M? step.
Algorithm 1 provides the training pseudocode.
Algorithm 1 Training a PFST T
?
by EM.
1: while not converged do
2: reset al counts to 0 . begin the ?E step?
3: for k ? 1 to K do . loop over training data
4: M = x
k
? T
?
? y
k
. small acyclic WFST
5: ~? = FORWARD-ALGORITHM(M )
6:
~
? = BACKWARD-ALGORITHM(M )
7: for arc A ?M , from state q ? q
?
do
8: if A was derived from an arc in T
?
representing edit e, from edit state q
C
, then
9: c(C, e) += ?
q
? prob(A) ? ?
q
?
/?
q
I
10: ? ? L-BFGS(?, EVAL, max iters=5) . the ?M step?
11: function EVAL(?) . objective function & its gradient
12: F ? 0;?F ? 0
13: for context C such that (?e)c(C, e) > 0 do
14: count? 0; expected? 0; Z
C
? 0
15: for possible edits e in context C do
16: F += c(C, e) ? (? ?
~
f(C, e))
17: ?F += c(C, e) ?
~
f(C, e)
18: count += c(C, e)
19: expected += exp(? ?
~
f(C, e)) ?
~
f(C, e)
20: Z
C
+= exp(? ?
~
f(C, e))
21: F -= count ? logZ
C
;?F -= count ?expected/Z
C
22: return (F,?F )
7 PFSTs versus WFSTs
Our PFST model of p(y | x) enforces a normal-
ized probability distribution at each state. Drop-
ping this requirement gives a weighted FST
(WFST), whose path weightsw(x, y) can be glob-
ally normalized (divided by a constant Z
x
) to ob-
tain probabilities p(y | x). WFST models of con-
textual edits were studied by Dreyer et al (2008).
PFSTs and WFSTs are respectively related to
MEMMs (McCallum et al, 2000) and CRFs (Laf-
ferty et al, 2001). They gain added power from
hidden states and  transitions (although to permit
a finite-state encoding, they condition on x in a
more restricted way than MEMMs and CRFs).
WFSTs are likely to beat PFSTs as linguistic
models,
4
just as CRFs beat MEMMs (Klein and
Manning, 2002). A WFST?s advantage is that the
probability of an edit can be indirectly affected by
the weights of other edits at a distance. Also, one
could construct WFSTs where an edit?s weight di-
rectly considers local right output context C
4
.
So why are we interested in PFSTs? Because
they do not require computing a separate normal-
izing contant Z
x
for every x. This makes it com-
putationally tractable to use them in settings where
x is uncertain because it is unobserved, partially
observed (e.g., lacks syllable boundaries), or nois-
ily observed. E.g., at the end of section 5, X rep-
resented an uncertain x. So unlike WFSTs, PFSTs
are usable as the conditional distributions in noisy
channel models, channel cascades, and Bayesian
networks. In future we plan to measure their mod-
eling disadvantage and attempt to mitigate it.
PFSTs are also more efficient to train under con-
ditional likelihood. It is faster to compute the gra-
dient (and fewer steps seem to be required in prac-
tice), since we only have to raise the probabilities
of arcs in x
k
? T ? y
k
relative to competing arcs
in x
k
? T . We visit at most |x
k
| ? |y
k
| ? |?
y
| arcs.
By contrast, training a WFST must raise the prob-
ability of the paths in x
k
? T ? y
k
relative to the
infinitely many competing paths in x
k
? T . This
requires summing around cycles in x
k
?T , and re-
quires visiting all of its |x
k
| ? |?
y
|
N
3
+1
arcs.
8 Experiments
To demonstrate the utility of contextual edit trans-
ducers, we examine spelling errors in social me-
dia data. Models of spelling errors are useful in
a variety of settings including spelling correction
itself and phylogenetic models of string variation
4
WFSTs can also use a simpler topology (Dreyer et al,
2008) while retaining determinism, since edits can be scored
?in retrospect? after they have passed into the left context.
628
-8
-7
-6
-5
-4
2000 4000 6000
# Training Examples
Me
an L
og-
Lik
elih
ood
Backoff
FALSE
TRUE
Topology
T010
T020
T110
T111
2
3
4
5
6
2000 4000 6000
# Training Examples
Me
an E
xpe
cted
 Ed
it D
ista
nce
Backoff
FALSE
TRUE
Topology
T010
T020
T110
T111
Figure 2: (a) Mean log p(y | x) for held-out test examples. (b) Mean expected edit distance (similarly).
(Mays et al, 1991; Church and Gale, 1991; Ku-
kich, 1992; Andrews et al, 2014).
To eliminate experimental confounds, we use
no dictionary or language model as one would in
practice, but directly evaluate our ability to model
p(correct | misspelled). Consider (x
k
, y
k
) =
(feeel, feel). Our model defines p(y | x
k
) for all y.
Our training objective (section 6) tries to make this
large for y = y
k
. A contextual edit model learns
here that e 7?  is more likely in the context of ee.
We report on test data how much probability
mass lands on the true y
k
. We also report how
much mass lands ?near? y
k
, by measuring the ex-
pected edit distance of the predicted y to the truth.
Expected edit distance is defined as
?
y
p
?
(y |
x
k
)d(y, y
k
) where d(y, y
k
) is the Levenshtein dis-
tance between two strings. It can be computed us-
ing standard finite-state algorithms (Mohri, 2003).
8.1 Data
We use an annotated corpus (Aramaki, 2010) of
50000 misspelled words x from tweets along with
their corrections y. All examples have d(x, y) = 1
though we do not exploit this fact. We randomly
selected 6000 training pairs and 100 test pairs. We
regularized the objective by adding ??||?||
2
2
, where
for each training condition, we chose ? by coarse
grid search to maximize the conditional likelihood
of 100 additional development pairs.
8.2 Context Windows and Edit Features
We considered four different settings for the con-
text window sizes (N
1
, N
2
, N
3
): (0,1,0)=stochas-
tic edit distance, (1,1,0), (0,2,0), and (1,1,1).
Our log-linear edit model (section 6) includes
a dedicated indicator feature for each contextual
edit (C, e), allowing us to fit any conditional dis-
tribution p(e | C). In our ?backoff? setting, each
(C, e) also has 13 binary backoff features that it
shares with other (C
?
, e
?
). So we have a total of 14
feature templates, which generate over a million
features in our largest model. The shared features
let us learn that certain properties of a contextual
edit tend to raise or lower its probability (and the
regularizer encourages such generalization).
Each contextual edit (C, e) can be character-
ized as a 5-tuple (s, t, C
1
, C
?
2
, C
3
): it replaces
s ? ?
x
? {} with t ? ?
y
? {} when s falls be-
tween C
1
and C
?
2
(so C
2
= sC
?
2
) and t is preceded
by C
3
. Then each of the 14 features of (C, e) in-
dicates that a particular subset of this 5-tuple has a
particular value. The subset alays includes s, t,
or both. It never includes C
1
or C
?
2
without s, and
never includes C
3
without t.
8.3 Results
Figures 2a and 2b show the learning curves. We
see that both metrics improve with more training
data; with more context; and with backoff. With
backoff, all of the contextual edit models substan-
tially beat ordinary stochastic edit distance, and
their advantage grows with training size.
9 Conclusion
We have presented a trainable, featurizable model
of contextual edit distance. Our main contribu-
tion is an efficient encoding of such a model as
a tight PFST?that is, a WFST that is guaranteed
to directly define conditional string probabilities
without need for further normalization. We are re-
leasing OpenFST-compatible code that can train
both PFSTs and WFSTs (Cotterell and Renduch-
intala, 2014). We formally defined PFSTs, de-
scribed their speed advantage at training time, and
noted that they are crucial in settings where the in-
put string is unknown. In future, we plan to deploy
our PFSTs in such settings.
629
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Implementation and Application of Au-
tomata, pages 11?23. Springer.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2014. Robust entity clustering via phylogenetic in-
ference. In Proceedings of ACL.
Eiji Aramaki. 2010. Typo corpus. Available at http:
//luululu.com/tweet/#cr, January.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
HLT-NAACL, pages 582?590.
Alexandre Bouchard-C?ot?e, Percy Liang, Thomas L.
Griffiths, and Dan Klein. 2007. A probabilistic ap-
proach to language change. In NIPS.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings
of ACL, pages 88?95.
Zhiyi Chi and Stuart Geman. 1998. Estimation
of probabilistic context-free grammars. Computa-
tional Linguistics, 24(2):299?305.
Kenneth W. Church and William A. Gale. 1991. Prob-
ability scoring for spelling correction. Statistics and
Computing, 1(2):93?103.
Ryan Cotterell and Adithya Renduchintala. 2014.
brezel: A library for training FSTs. Technical re-
port, Johns Hopkins University.
Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008. Latent-variable modeling of string transduc-
tions with finite-state methods. In Proceedings of
EMNLP, EMNLP ?08, pages 1080?1089.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proceedings of NAACL-
HLT, pages 644?648.
Jason Eisner. 2001. Expectation semirings: Flexible
EM for learning finite-state transducers. In Proceed-
ings of the ESSLLI Workshop on Finite-State Meth-
ods in NLP.
Dale Gerdemann and Gertjan van Noord. 1999. Trans-
ducers from rewrite rules with backreferences. In
Proceedings of EACL.
Ronald M. Kaplan and Martin Kay. 1994. Regu-
lar models of phonological rule systems. Compu-
tational Linguistics, 20(3):331?378.
Dan Klein and Christopher D. Manning. 2002. Condi-
tional structure versus conditional estimation in NLP
models. In Proceedings of EMNLP, pages 9?16.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natural
language processing. In Proc. of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLing).
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys
(CSUR), 24(4):377?439.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of ACL, pages 459?466.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Informa-
tion Processing & Management, 27(5):517?522.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmentation. In
Proceedings of ICML.
Mehryar Mohri and Richard Sproat. 1996. An efficient
compiler for weighted rewrite rules. In Proceedings
of ACL, pages 231?238.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269?311.
Mehryar Mohri. 2003. Edit-distance of weighted au-
tomata: General definitions and algorithms. Inter-
national Journal of Foundations of Computer Sci-
ence, 14(06):957?982.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(5):522?
532.
630
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 9?10,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
Structured Belief Propagation for NLP
Matthew R. Gormley Jason Eisner
Department of Computer Science
Johns Hopkins University, Baltimore, MD
{mrg,jason}@cs.jhu.edu
1 Tutorial Overview
Statistical natural language processing relies on
probabilistic models of linguistic structure. More
complex models can help capture our intuitions
about language, by adding linguistically meaning-
ful interactions and latent variables. However, in-
ference and learning in the models we want often
poses a serious computational challenge.
Belief propagation (BP) and its variants pro-
vide an attractive approximate solution, especially
using recent training methods. These approaches
can handle joint models of interacting compo-
nents, are computationally efficient, and have ex-
tended the state-of-the-art on a number of com-
mon NLP tasks, including dependency parsing,
modeling of morphological paradigms, CCG pars-
ing, phrase extraction, semantic role labeling, and
information extraction (Smith and Eisner, 2008;
Dreyer and Eisner, 2009; Auli and Lopez, 2011;
Burkett and Klein, 2012; Naradowsky et al., 2012;
Stoyanov and Eisner, 2012).
This tutorial delves into BP with an emphasis on
recent advances that enable state-of-the-art perfor-
mance in a variety of tasks. Our goal is to eluci-
date how these approaches can easily be applied
to new problems. We also cover the theory under-
lying them. Our target audience is researchers in
human language technologies; we do not assume
familarity with BP.
In the first three sections, we discuss applica-
tions of BP to NLP problems, the basics of mod-
eling with factor graphs and message passing, and
the theoretical underpinnings of ?what BP is do-
ing? and how it relates to other variational infer-
ence techniques. In the second three sections, we
cover key extensions to the standard BP algorithm
to enable modeling of linguistic structure, efficient
inference, and approximation-aware training. We
survey a variety of software tools and introduce a
new software framework that incorporates many
of the modern approaches covered in this tutorial.
2 Outline
1. Applications [15 min., Eisner]
? Intro: Modeling with factor graphs
? Morphological paradigms
? Dependency and constituency parsing
? Alignment; Phrase extraction
? Relation extraction; Semantic role labeling
? Targeted sentiment
? Joint models for NLP
2. Belief Propagation Basics [40 min., Eisner]
? Messages and beliefs
? Sum-product, max-product, and determin-
istic annealing
? Relation to forward-backward and inside-
outside
? Acyclic vs. loopy graphs
? Synchronous vs. asynchronous propaga-
tion
3. Theory [25 min., Gormley]
? From arc consistency to BP
? From Gibbs sampling to particle BP to BP
? Other message-passing algorithms
? Bethe free energy
? Connection to PFCGs and FSMs
4. Incorporating Structure into Factors and Vari-
ables [30 min., Gormley]
? Embedding dynamic programs (e.g.
inside-outside) within factors
? String-valued and tree-valued variables
5. Message approximation and scheduling [20
min., Eisner]
? Pruning messages
? Variational approximations
? Residual BP and new variants
6. Approximation-aware Training [30 min., Gorm-
ley]
? Empirical risk minimization under approx-
imations (ERMA)
? BP as a computational expression graph
? Automatic differentiation (AD)
7. Software [10 min., Gormley]
9
3 Instructors
Matt Gormley is a PhD student at Johns Hopkins
University working with Mark Dredze and Jason
Eisner. His current research focuses on joint mod-
eling of multiple linguistic strata in learning set-
tings where supervised resources are scarce. He
has authored papers in a variety of areas including
topic modeling, global optimization, semantic role
labeling, and grammar induction.
Jason Eisner is an Associate Professor in Com-
puter Science and Cognitive Science at Johns
Hopkins University, where he has received two
school-wide awards for excellence in teaching.
His 80+ papers have presented many models and
algorithms spanning numerous areas of NLP. His
goal is to develop the probabilistic modeling, in-
ference, and learning techniques needed for a uni-
fied model of all kinds of linguistic structure. In
particular, he and his students introduced struc-
tured belief propagation, which integrates classi-
cal NLP models and their associated dynamic pro-
gramming algorithms, as well as loss-calibrated
training for use with belief propagation.
References
Michael Auli and Adam Lopez. 2011. A compari-
son of loopy belief propagation and dual decompo-
sition for integrated CCG supertagging and parsing.
In Proceedings of ACL.
David Burkett and Dan Klein. 2012. Fast inference in
phrase extraction models with belief propagation. In
Proceedings of NAACL.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proceedings of
EMNLP.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization
of hidden syntactic structure. In Proceedings of
EMNLP 2012.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
Veselin Stoyanov and Jason Eisner. 2012. Minimum-
risk training of approximate CRF-Based NLP sys-
tems. In Proceedings of NAACL-HLT.
10
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 18?26,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Introducing Computational Concepts in a Linguistics Olympiad
Patrick Littell
Department of Linguistics
University of British Columbia
Vancouver, BC V6T1Z4, Canada
littell@interchange.ubc.ca
Lori Levin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
lsl@cs.cmu.edu
Jason Eisner
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Dragomir R. Radev
Department of EECS
School of Information
and Department of Linguistics
University of Michigan
radev@umich.edu
Abstract
Linguistics olympiads, now offered
in more than 20 countries, provide
secondary-school students a compelling
introduction to an unfamiliar field. The
North American Computational Lin-
guistics Olympiad (NACLO) includes
computational puzzles in addition to
purely linguistic ones. This paper ex-
plores the computational subject matter
we seek to convey via NACLO, as well
as some of the challenges that arise
when adapting problems in computational
linguistics to an audience that may have
no background in computer science,
linguistics, or advanced mathematics.
We present a small library of reusable
design patterns that have proven useful
when composing puzzles appropriate for
secondary-school students.
1 What is a Linguistics Olympiad?
A linguistics olympiad (LO) (Payne and Derzhan-
ski, 2010) is a puzzle contest for secondary-school
students in which contestants compete to solve
self-contained linguistics problem sets. LOs have
their origin in the Moscow Traditional Olympiad
in Linguistics, established in 1965, and have since
spread around the world; an international contest
(http://www.ioling.org) has been held
yearly since 2003.
In an LO, every problem set is self-contained,
so no prior experience in linguistics is necessary
to compete. In fact, LO contests are fun and re-
warding for exactly this reason: by the end of the
contest, contestants are managing to read hiero-
glyphics, conjugate verbs in Swahili, and perform
other amazing feats. Furthermore, they have ac-
complished this solely through their own analyti-
cal abilities and linguistic intuition.
Based on our experience going into high
schools and presenting our material, this ?linguis-
tic? way of thinking about languages almost al-
ways comes as a novel surprise to students. They
largely think about languages as collections of
known facts that you learn in classes and from
books, not something that you can dive into and
figure out for yourself. This is a hands-on antidote
to the common public misconception that linguists
are fundamentally polyglots, rather than language
scientists, and students come out of the experience
having realized that linguistics is a very different
field (and hopefully a more compelling one) than
they had assumed it to be.
2 Computational Linguistics at the LO
Our goal, since starting the North American
Computational Linguistics Olympiad (NACLO) in
2007 (Radev et al, 2008), has been to explore how
this LO experience can be used to introduce stu-
dents to computational linguistics. Topics in com-
putational linguistics have been featured before in
LOs, occasionally in the Moscow LO and with
some regularity in the Bulgarian LO.
Our deliberations began with some trou-
bling statistics regarding enrollments in computer
science programs (Zweben, 2013). Between
2003 and 2007 enrollments in computer science
dropped dramatically. This was attributed in part
to the dip in the IT sector, but it also stemmed in
18
part from a perception problem in which teenagers
view computer science careers as mundane and
boring: ?I don?t want to be Dilbert,1 sitting in a
cubicle programming payroll software my whole
life.? This is an unrealistically narrow percep-
tion of the kinds of problems computer scientists
tackle, and NACLO began in part as a way to pub-
licize to teenagers that many interesting problems
can be approached using computational methods.
Although enrollments are not yet back to the
2003 levels, there has been a sharp increase since
2007 (Zweben, 2013). The resurgence can be at-
tributed in part to the strength of the IT sector, but
also to the realization that computer science is rel-
evant to almost every area of science and technol-
ogy (Thibodeau, 2013). NACLO aims to be part
of this trend by showing students that computer
science is used in studying fascinating problems
related to human language.
Even ?traditional? LO puzzles are inherently
computational in that they require pattern recog-
nition, abstraction, generalization, and establish-
ing and pruning a solution space. However, we
also want to teach computational linguistics more
explicitly. NACLO puzzles have featured a wide
variety of topics in computational linguistics and
computer science; they may focus on the applica-
tion itself, or on concepts, tools, and algorithms
that underlie the applications. Broadly, computa-
tional LO topics fall into three types, summarized
below.
2.1 Technological applications
NACLO has included puzzles on technologies that
most people are familiar with, including spell
checking, information retrieval, machine transla-
tion, document summarization, and dialogue sys-
tems. In a typical applications puzzle, the contes-
tants would discover how the application works,
how it handles difficult cases, or what its limita-
tions are. In ?Summer Eyes? (Radev and Hester-
berg, 2009), the contestant discovers the features
that are used for selecting sentences in a sum-
marization program, including the position of a
sentence in the article, the number of words the
sentence shares with the title, etc. In ?Spring-
ing Up Baby? (Srivastava and Bender, 2008) and
?Running on MT? (Somers, 2011), contestants ex-
plore word sense disambiguation in the context of
1An engineer in the eponymous American comic strip,
Dilbert has a famously dysfunctional workplace and unre-
warding job.
machine translation, while ?Tiger Tale? (Radev,
2011) highlights some realistic sources of knowl-
edge for machine translation such as cognates
and cross-language syntactic similarities. ?Thorny
Stems? (Breck, 2008) and ?A fox among the h?
(Littell, 2012b) introduce stemming.
2.2 Formal grammars and algorithms
Some puzzles introduce the formal tools of com-
putational linguistics and linguistic concepts that
are important in computational linguistics, of-
ten in a whimsical way. For example, ?Sk8
Parsr? (Littell, 2009) introduces shift-reduce pars-
ing by means of a hypothetical skateboarding
video game. ?Aw-TOM-uh-tuh? (Littell, 2008)
introduces a finite-state machine that determines
which strings form legal words in the Rotokas
language. ?Orwellspeak? (Eisner, 2009) asks
solvers to modify a simple context-free grammar,
and then to discover that a 4-gram model can-
not model this language without precision or re-
call errors. ?Twodee? (Eisner, 2012) invents a
two-dimensional writing system, shown below, as
a vehicle for helping students discover parsing
ambiguity?and production ambiguity?without
the full formal apparatus of grammars, nontermi-
nals, or tree notation.
?The Little Engine That Could. . . Read? (Littell
and Pustejovsky, 2012) explores quantifier mono-
tonicity, while ?Grice?s Grifter Gadgets? (Boyd-
Graber, 2013) covers Grice?s maxims as part of the
specification of a computerized game assistant.
2.3 Computational concepts
NACLO puzzles have also introduced computa-
tional concepts that go beyond computational lin-
guistics. ?Texting, Texting, One Two Three? (Lit-
tell, 2010b) and ?The Heads and Tails of Huff-
man? (DeNero, 2013) introduce data compression.
?One, Two, Tree? (Smith et al, 2012) introduces
the Catalan numbers and other recurrences via bi-
nary bracketing of ambiguous compound nouns.
19
?Nok-nok? (Fink, 2009) introduces Levenshtein
distance by describing a hypothetical typing tutor
for very bad spellers.
3 The Challenge of Writing
Computational Problems
To achieve our goals, it becomes necessary to
write computational linguistics puzzles in such a
way that they are self-contained, requiring no prior
experience in linguistics, computer science, or ad-
vanced math. This has proven very difficult, but
not impossible, and in the past seven years we have
managed to learn a lot about how to (and how not
to) write them.
Perhaps the hardest part of writing any LO puz-
zle is that authors have to remove themselves from
their knowledge and experience: to forget techni-
cal definitions of ?phrase? or ?noun? or ?string? or
?function,? and to forget the facts and insights and
history that formed our modern understanding of
these. This is doubly hard when it comes to puz-
zles involving computational methods. The ability
to write an algorithm that a computer could actu-
ally interpret is a specialized skill that we learned
through education, and it is very, very hard to back
up and imagine what it would be like to not be
able to think like this. (It is almost like trying to
remember what it was like to not be able to read?
not simply not knowing a particular alphabet or
language, but not even understanding how reading
would work.)
Here is an illustration of an interesting but
nonetheless inappropriate LO puzzle:
Here are fourteen English compound
words:
birdhouse housework
blackbird tablespoon
blackboard teacup
boardroom teaspoon
boathouse workhouse
cupboard workroom
houseboat worktable
Even if you didn?t know any English, you
could probably determine by looking at
this list which English words were used
to make up the compounds: ?black?,
?bird?, ?board?, etc...
How would you do this if you were a
computer?
This task, although potentially appropriate for a
programming competition, is inappropriate for an
LO: the intended task requires some prior knowl-
edge about what computers can and cannot do.
Note that nowhere in the puzzle itself are the prop-
erties of this imaginary computer specified. It is
assumed that the solver knows roughly the state of
modern computing machinery and what kinds of
instructions it can execute.
Imagine for a moment what a right answer to
this puzzle would look like, and then picture what
a wrong answer might look like. Your right answer
was probably an algorithm that could run on an ab-
stract computer with capabilities very much like
real computers. The wrong answer probably made
incorrect assumptions about what sorts of opera-
tions computers are capable of, or treated enor-
mously complex operations as if they were primi-
tive.2
The problem with the above puzzle is that it is
very open-ended, and in the absence of a large
body of shared knowledge between the author and
the solver, the solver cannot know what it is the
author wants or when they have solved it to the
author?s satisfaction.
In order to avoid this, it is best to set up the puz-
zle so that the ?search space? for possible answers
is relatively constrained, and the ?win? conditions
are clear. Ideally, if a contestant has solved a puz-
zle, they should know they have solved it, and thus
be able to move on confidently to the next puz-
zle.3 In this respect, LO puzzles are akin to cross-
word puzzles, problems from other Olympiads, or
online puzzle games. This feeling of accomplish-
ment is key to the kind of rewarding learning ex-
perience that have made LOs so successful.
4 Design Patterns for CL Puzzles
Over the years, we have found several reliable
strategies for turning ideas and topics from com-
putational linguistics into solvable, rewarding puz-
2Keep in mind that today?s contestants were born in the
late 1990s. They are unlikely to even remember a world with-
out ubiquitous Internet and powerful natural language search.
Their conception of ?what computers basically do? is not nec-
essarily going to be the same as those of us who encountered
computers when they were still recognizable as a kind of so-
phisticated calculator.
3This is not to say, however, that only those who solve a
puzzle in its entirety should feel accomplished or rewarded.
The best puzzles often contain layers of mysteries: it may be
that only a few will solve every mystery in the puzzle, but
most contestants come away with the satisfaction of having
discovered something.
20
zles.
Not every computational puzzle makes use of
these?some are entirely unique?but many do.
In addition, these strategies are not mutually ex-
clusive; many computational puzzles utilize sev-
eral of these at once. For example, a ?Broken Ma-
chine? puzzle may then present the solver with a
?Troublemaker? task, or an ?Assembly Required?
machine may, upon assembly, turn out to be a
?Broken? one.
4.1 Assembly Required
The solver is presented with a task to complete,
and also a partially specified algorithm for doing
so. The partial specification illustrates the de-
sired formal notation and the model of computa-
tion. But it may be missing elements, or the or-
dering or relationship between the elements is un-
clear, or some other aspect of the system remains
unfinished. The solver is asked to complete the
system so that it performs the appropriate task or
produces the appropriate outputs.
For example, NACLO 2008 included a puzzle
on stemming, ?Thorny Stems? (Breck, 2008), in
which contestants help develop an algorithm to
isolate the stems of various words. In this puzzle,
the solver is not required to invent an algorithm
ex nihilo; this would merely have rewarded those
who already understand algorithms, not introduce
algorithmic thinking to neophytes. Instead, the
overall structure of the intended algorithm (an or-
dered sequence of if-thens) is made explicit, and
the solver?s task is to fill in the details:
Rule 1: If a word ends in , then
replace with to form the
stem.
Rule 2: If a word ends in , then
replace with to form the
stem.
In another puzzle from the same contest, ?Aw-
TOM-uh-tuh? (Littell, 2008), the solver must
complete an unfinished finite-state automaton so
that it performs a language recognition task. The
solver is given a brief introduction to FSAs and a
simple sample FSA, and then given an incomplete
FSA whose labels lack edges. The solver?s task is
to place the labels on the correct edges so that the
FSA accepts certain inputs and rejects others.
Other examples of the ?Assembly Required?
pattern can be found in the puzzles ?Sk8 Parsr?
(Littell, 2009), ?The Heads and Tails of Huff-
man? (DeNero, 2013), and ?BrokEnglish!? (Lit-
tell, 2011).
4.2 Black Box
The solver is presented with the inputs to a system
and the outputs, and must work out how the system
generated the outputs. Unlike in the ?Assembly
Required? pattern, little or no information about
the algorithm is provided to the solver; the solver?s
fundamental task is to characterize this unknown
algorithm as thoroughly as possible.
For example, NACLO 2010 featured a puzzle
on Huffman text compression, ?Texting, Texting,
One Two Three? (Littell, 2010b), in which an un-
specified algorithm converts strings of letters to
strings of numbers:
Testing testing = 33222143224142341-
1222143224142341331
Does anyone copy = 33233322143131-
42343324221124232342343331
Working out the basic number-letter correspon-
dences is relatively straightforward, but the real
puzzle is working out the rationale behind these
correspondences. Some of the answers require let-
ters (like ?r? and ?x?) that do not occur anywhere
in the data, but can be deduced once the system as
a whole is fully understood.
NACLO 2009 featured a puzzle on Levenshtein
distance, ?Nok-nok!? (Fink, 2009), that also
used this pattern. In it, a spell-checker is rat-
ing how well (or poorly) a user has spelled a word.
21
User Input Correct word Output
owll owl ?almost right?
ples please ?quite close?
reqird required ?quite close?
plez please ?a bit confusing?
mispeln misspelling ?very confusing?
The solver?s task is to work out the algorithm suf-
ficiently to predict how the system would respond
to novel inputs.
Other examples of the ?Black Box? pattern can
be found in ?The Deschamps Codice? (Piperski,
2012) and ?The Little Engine that Could. . . Read?
(Littell and Pustejovsky, 2012).
Depending on the intended algorithm, the
?Black Box? pattern may or may not be appro-
priate. This pattern works best when the nature
of the transformation between input and output is
relatively straightforward and the purpose of the
transformation is relatively clear. In the Huff-
man coding puzzle, for example, the nature of
the transformation is entirely obvious (replace let-
ters with number sequences) and thus the solution
space of the puzzle is relatively constrained (figure
out which letters correspond to which number se-
quences and then try to figure out why). In the
spell-checking puzzle, the purpose of the trans-
formation is easily understood, giving the solver
a head start on figuring out which features of the
input the algorithm might be considering.
When the nature of the transformation is less
obvious?for example, the generation of numbers
of unclear significance, rating some unknown as-
pect of a text passage??Black Box? is not as ap-
propriate as the other patterns. The potential prob-
lem is that not only must the solver come up with
an algorithm on their own, they must come up with
the same algorithm the author did. Given a com-
plicated algorithm, even small implementation de-
tails may lead to very different outputs, so a solver
can even have found a basically correct solution
but nevertheless not managed to produce the in-
tended outputs.
In such cases, the ?Assembly Required? or
?Broken Machine? patterns are potentially more
appropriate.
4.3 Broken Machine
The solver is presented with a system that purports
to perform a particular task, but actually fails on
particular inputs. The solver is tasked with fig-
uring out what went wrong and, potentially, fixing
the system so that it works. In some cases, the sys-
tem simply has an error in it; in others, the system
is correct but cannot handle certain difficult cases.
NACLO has featured a wide variety of broken
machines, often with humorous outputs. ?Help my
Camera!? (Bender, 2009) features a dialogue sys-
tem that could not correctly resolve pronoun refer-
ences:
Human: ?There?s this restaurant on
Bancroft that?s supposed to be really
good that I heard about from my mother.
Can you help me find it??
Computer: ?Where did you last see your
mother??
?BrokEnglish!? (Littell, 2011) features a run-
away script that replaced certain ISO 639-1 codes
with language names:
Hebrewy, ChamorRomanianrICHebre-
wcHebrewnlandic! whEnglish you
get a FrEnglishcHebrewe momEnglisht,
cHebrewck out thICHebrewcHebrewn-
landic niCHebrewcHebrewn little pRo-
maniangram i wRomaniante.
Solvers are then tasked with determining why
this script produced such a bizarre output, and ad-
ditionally tasked with determining in what order
the replacements had to have occurred in order to
get this exact output.
?Orwellspeak? (Eisner, 2009) involves a
context-free grammar that produces sentences
that were grammatically correct but counter to the
ideals of a fictional totalitarian Party. The solver
must rewrite the grammar so that only ?correct?
thoughts can be uttered. In the second part of the
puzzle, the solver must show that Markov models
would be inherently broken.
Other examples of ?Broken Machines? are ?The
Lost Tram? (Iomdin, 2007), ?Sk8 Parsr? (Lit-
tell, 2009), ?A fox among the h? (Littell, 2012b),
?The Little Engine that Could. . . Read? (Littell and
Pustejovsky, 2012), and ?Grice?s Grifter Gadgets?
(Boyd-Graber, 2013).
4.4 Troublemaker
The solver is presented with a system and some
sample inputs and outputs, and must discover an
input that causes the system to fail, or produce out-
puts that are strange, suboptimal, or have some un-
usual property.
22
Few puzzles make use of only the ?Trouble-
maker? pattern. Many are basically ?Assembly
Required? or ?Broken Machine? puzzles that use a
?Troublemaker? task to get the contestant thinking
about the ways in which the system is limited or
imperfect. They are also often creative?the con-
testant usually invents their own inputs?and thus
can serve as a refreshing change of pace.4
NACLO 2009 featured a ?Broken Machine?
puzzle about shift-reduce parsing (?Sk8 Parsr?)
(Littell, 2009), couched in terms of a fictional
skateboarding videogame. The solver is given an
algorithm by which button presses are transformed
into skateboard trick ?combos? like those shown
below, but many well-formed ?combos? cannot
correctly be parsed due to a shift-reduce conflict.
The solver is given an example of one such class
of inputs, and then asked to discover other classes
of inputs that likewise fail.
?Troublemaker? puzzles are not always
couched in terms of bugs. ?This problem is pretty
// easy? (Radev, 2007a) asks solvers to construct
eye-catching garden path sentences. In the
Huffman text compression puzzle detailed above
(?Texting, Texting, One Two Three?) (Littell,
2010b), a ?Troublemaker? task is introduced to
get contestants thinking about the limits of com-
pression. Although the compression algorithm
is not ?broken? in any way, any compression
algorithm will ?fail? on some possible input and
return an output longer than the input, and the
solver is tasked to discover such an input.
?Troublemaker? tasks can also be found in
?Grammar Rules? (Schalley and Littell, 2013) and
?Yesbot? (Mitkov and Littell, 2013).
4If the ?Troublemaker? task asks for an input with a par-
ticular formal property (i.e., a sentence generated or not gen-
erated from a particular grammar), automated grading scripts
can determine the correctness of the answer without human
intervention. This means that contestants can get a chance
to enter ?creative? answers even in large contests (like the
NACLO Open Round) that utilize automatic grading.
4.5 Jabberwock
Not all puzzle types revolve around abstract ma-
chines. Another recurring puzzle type, the ?Jab-
berwock?, involves asking the solver to puzzle out
the syntactic or semantic properties of unknown
words. Often these words are nonsense words, but
this puzzle type can also work on natural language
data. To perform this task, solvers often have to
use the same methods that a computer would.
?We are all molistic in a way? (Radev, 2007b)
asks solvers to infer the polarity of various non-
sense adjectives based on a series of sentences.5
The teacher is danty and cloovy.
Mary is blitty but cloovy.
Strungy and struffy, Diane was a plea-
sure to watch.
Even though weasy, John is strungy.
Carla is blitty but struffy.
The solver must work out from sentences such
as these whether words like ?danty? and ?weasy?
have positive or negative associations. In doing so,
the solver has essentially constructed and solved a
semi-supervised learning problem.
In ?Gelda?s House of Gelbelgarg? (Littell,
2010a), solvers are presented with a page of fab-
ricated restaurant reviews for an entirely fictional
cuisine:
?A hidden gem in Lower Uptown! Get
the fa?rsel-fo?rsel with gorse-weebel and
you?ll have a happy stomach for a week.
And top it off with a flebba of sweet-
bolger while you?re at it!?
5The list given here includes a subset of the examples used
in the real puzzle in 2007.
23
?I found the food confusing and disori-
enting. Where is this from? I randomly
ordered the fa?rsel-fo?rsel and had to send
them back!?
Using various grammatical cues (article and pro-
noun choice, ?less? vs. ?fewer?, etc.), solvers have
to sort the items into things most likely to be dis-
crete, countable objects, things most likely to be
liquids or masses, and things most likely to be con-
tainers or measures.
This type of puzzle often violates the common
LO restriction on using nonsense words and made-
up languages, but it is not always possible to base
this sort of puzzle on a completely unfamiliar lan-
guage. Many ?Jabberwock? puzzles involve infer-
ring syntactic or semantic information about un-
known words in an otherwise known language.
The two puzzles above therefore require contes-
tants to consult their own intuitions about English.
These puzzles would have been entirely different
(and prohibitively difficult) if the language had
been completely unfamiliar.
Other Jabberwock puzzles include ?Tiger Tale?
(Radev, 2011) and ?Cat and Mouse Story? (Littell,
2012a).
4.6 Combinatorial Problems
Some puzzles effectively force the solver to design
and run an algorithm, to get an answer that would
be too difficult to compute by brute force. Such
puzzles involve computational thinking. But since
the solver only has to give the output of the algo-
rithm, there is no need to agree on a type of com-
puting device or a notation for writing algorithms
down.
Such puzzles include combinatorial tasks that
involve the counting, maximization, or existence
of linguistic objects. They require mathematical
and algorithmic skills (just as in math or program-
ming competitions), and demonstrate how these
skills apply to linguistics or NLP.
Portions of ?One, Two, Tree? (Smith et
al., 2012) and ?Twodee? (Eisner, 2012) require
solvers to count all ways to parse a sentence, or
to count all sentences of a certain type. Because
the counts are large, the solver must find the pat-
tern, which involves writing down a closed-form
formula such as 2n or a more complex dynamic
programming recurrence.
5 Conclusions
Researchers and teachers from the ACL commu-
nity are invited to contact the NACLO organizing
committee at naclo14org@umich.edu6 with
their ideas for new puzzles or new types of puz-
zles. All of the past puzzles and solutions can
be browsed at http://www.naclo.cs.cmu.
edu/practice.html. In general, puzzles in
Round 1 each year should be easier and automat-
ically gradable. Puzzles in Round 2 permit more
involved questions and answers; this is a smaller
contest in which the top Round 1 scorers (usu-
ally, the top 10 percent) can qualify for the Inter-
national Linguistic Olympiad.
Thus far, NACLO?s computational puzzles have
reached at least 6,000 students at more than 150
testing sites7 in the U.S. and Canada, as well as at
least 10,000 students in the three other English-
language countries that share LO puzzles with
NACLO.
We observe that most computational puzzles do
not need obscure languages, staying on the contes-
tant?s home turf of English and technology. This
does not mean, however, that the computational
puzzles are purely formal and lack linguistic con-
tent. Some of them in fact probe subtle facts about
English (the introspective method in linguistics),
and some of them cover areas of linguistics that
are underserved by traditional LO puzzles. Tra-
ditional LO puzzles instead ask the solver to sort
out vocabulary and basic morphophonological or
orthographic patterns in a mystery language (the
fieldwork method in linguistics). Students who en-
joy ?top-down? thinking or who are deeply inter-
ested in ?how to do things with words? may prefer
the former kind of puzzle.
Competitions are popular in many North Amer-
ican high schools, perhaps in part as a way to im-
press college admissions officers. We have ex-
ploited this to give students a taste of our inter-
disciplinary field before they choose a college ma-
jor. Some students may be specifically attracted to
NACLO by the word ?computational? or the word
?linguistics,? or may be intrigued by their juxta-
position. Many NACLO participants reveal that
they had started to study linguistics on their own
before encountering NACLO, and have welcomed
6Or nacloXXorg@umich.edu, where XX is the last
two digits of the calendar year of the upcoming February.
7NACLO tests have been given at more than 100 high
schools and more than 50 university sites; the latter are open
to students from all local high schools.
24
NACLO as an outlet for their enthusiasm and a
place where they can interact with other students
who have the same interests.
NACLO?s past puzzles remain freely available
on the web for anyone who is interested. Two
volumes of NACLO-style puzzles (most of them
from real competitions), edited by program chair
Dragomir Radev, have recently been published by
Springer (Radev, 2013a; Radev, 2013b). Adult
hobbyists and home-schooled students may dis-
cover computational linguistics through encoun-
tering these puzzles. Avid LO contestants use
them to prepare for upcoming contests. Finally,
high school and college teachers can use them
as the basis of whole-class or small-group class-
room activities that expose students to computa-
tional thinking.
Acknowledgments
We would like to thank the National Science Foun-
dation for supporting NACLO through the fol-
lowing grants: IIS0633871, BCS1137828, and
IIS0838848. We also express our gratitude to NSF
program managers Tatiana Korelsky, Terry Lan-
gendoen, and Joan Maling for their effort in ini-
tiating and maintaining NACLO. The Linguistic
Society of America and the North American Chap-
ter of the Association for Computational Linguis-
tics provide ongoing support. Other sponsors, vol-
unteers, and problem writers are too numerous to
name. They are listed on the contest booklets each
year, which can be found on the NACLO web site:
http://www.naclo.cs.cmu.edu.
References
Emily Bender. 2009. Help my camera! In
North American Computational Linguistics
Olympiad 2009. http://www.naclo.cs.cmu.edu/
assets/problems/naclo09F.pdf.
Jordan Boyd-Graber. 2013. Grice?s grifter gad-
gets. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND2.pdf.
Eric Breck. 2008. Thorny stems. In North Amer-
ican Computational Linguistics Olympiad 2008.
http://www.naclo.cs.cmu.edu/assets/problems/
NACLO08h.pdf.
John DeNero. 2013. The heads and tails of Huff-
man. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND1.pdf.
Jason Eisner. 2009. Orwellspeak. In North Amer-
ican Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09M.pdf.
Jason Eisner. 2012. Twodee. In North
American Computational Linguistics Olympiad
2013. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Eugene Fink. 2009. Nok-nok! In North Ameri-
can Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09B.pdf.
Boris Iomdin. 2007. The lost tram. In North Amer-
ican Computational Linguistics Olympiad 2007.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo07 f.pdf.
Patrick Littell and James Pustejovsky. 2012.
The little engine that could. . . read. In North
American Computational Linguistics Olympiad
2012. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Patrick Littell. 2008. Aw-TOM-uh-tuh. In North
American Computational Linguistics Olympiad
2008. http://www.naclo.cs.cmu.edu/assets/
problems/NACLO08i.pdf.
Patrick Littell. 2009. Sk8 parsr. In North Ameri-
can Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09G.pdf.
Patrick Littell. 2010a. Gelda?s house of gelbel-
garg. In North American Computational Linguis-
tics Olympiad 2010. http://www.naclo.cs.cmu.edu/
problems2010/A.pdf.
Patrick Littell. 2010b. Texting, texting, one two
three. In North American Computational Linguis-
tics Olympiad 2010. http://www.naclo.cs.cmu.edu/
problems2010/E.pdf.
Patrick Littell. 2011. BrokEnglish! In North Amer-
ican Computational Linguistics Olympiad 2011.
http://www.naclo.cs.cmu.edu/problems2011/E.pdf.
Patrick Littell. 2012a. Cat and mouse story. In North
American Computational Linguistics Olympiad
2012. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND1.pdf.
Patrick Littell. 2012b. A fox among the
h. In North American Computational Linguis-
tics Olympiad 2012. http://www.naclo.cs.cmu.edu/
problems2012/NACLO2012ROUND2.pdf.
Ruslan Mitkov and Patrick Littell. 2013. Grammar
rules. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND2.pdf.
25
Thomas E. Payne and Ivan Derzhanski. 2010. The lin-
guistics olympiads: Academic competitions in lin-
guistics for secondary school students. In Kristin
Denham and Anne Lobeck, editors, Linguistics at
school. Cambridge University Press.
Alexander Piperski. 2012. The Deschamps
codice. In North American Computational Linguis-
tics Olympiad 2012. http://www.naclo.cs.cmu.edu/
problems2012/NACLO2012ROUND2.pdf.
Dragomir Radev and Adam Hesterberg. 2009. Sum-
mer eyes. In North American Computational Lin-
guistics Olympiad 2009. http://www.naclo.cs.cmu.
edu/assets/problems/naclo09E.pdf.
Dragomir R. Radev, Lori Levin, and Thomas E.
Payne. 2008. The North American Computa-
tional Linguistics Olympiad (NACLO). In Proceed-
ings of the Third Workshop on Issues in Teaching
Computational Linguistics, pages 87?96, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics. http://www.aclweb.org/anthology/W/W08/
W08-0211.
Dragomir Radev. 2007a. This problem is pretty //
easy. In North American Computational Linguis-
tics Olympiad 2007. http://www.naclo.cs.cmu.edu/
assets/problems/naclo07 h.pdf.
Dragomir Radev. 2007b. We are all molistic in a
way. In North American Computational Linguis-
tics Olympiad 2007. http://www.naclo.cs.cmu.edu/
assets/problems/naclo07 a.pdf.
Dragomir Radev. 2011. Tiger tale. In North American
Computational Linguistics Olympiad 2011. http://
www.naclo.cs.cmu.edu/problems2011/F.pdf.
Dragomir Radev, editor. 2013a. Puzzles in Logic,
Languages, and Computation: The Green Book.
Springer: Berlin.
Dragomir Radev, editor. 2013b. Puzzles in Logic, Lan-
guages, and Computation: The Red Book. Springer:
Berlin.
Andrea Schalley and Patrick Littell. 2013. Grammar
rules! In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND1.pdf.
Noah Smith, Kevin Gimpel, and Jason Eisner.
2012. One, two, tree. In North American
Computational Linguistics Olympiad 2012.
http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Harold Somers. 2011. Running on MT. In North
American Computational Linguistics Olympiad
2011. http://www.naclo.cs.cmu.edu/problems2011/
A.pdf.
Ankit Srivastava and Emily Bender. 2008. Springing
up baby. In North American Computational Lin-
guistics Olympiad 2008. http://www.naclo.cs.cmu.
edu/assets/problems/prob08b.pdf.
Patrick Thibodeau. 2013. Computer science en-
rollments soared last year, rising 30%, March.
http://www.computerworld.com/s/article/9237459/
Computer science enrollments soared last year
rising 30 .
Stuart Zweben. 2013. Computing degree and enroll-
ment trends, March. http://cra.org/govaffairs/blog/
wp-content/uploads/2013/03/CRA Taulbee CS
Degrees and Enrollment 2011-12.pdf.
26
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 66?76,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Virtual Manipulative for Learning Log-Linear Models
Francis Ferraro and Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD, USA
{ferraro, jason}@cs.jhu.edu
Abstract
We present an open-source virtual ma-
nipulative for conditional log-linear mod-
els. This web-based interactive visual-
ization lets the user tune the probabili-
ties of various shapes?which grow and
shrink accordingly?by dragging sliders
that correspond to feature weights. The
visualization displays a regularized train-
ing objective; it supports gradient as-
cent by optionally displaying gradients
on the sliders and providing ?Step? and
?Solve? buttons. The user can sam-
ple parameters and datasets of differ-
ent sizes and compare their own pa-
rameters to the truth. Our web-
site, http://cs.jhu.edu/?jason/
tutorials/loglin/, guides the user
through a series of interactive lessons and
provides auxiliary readings, explanations,
practice problems and resources.
1 Introduction
We argue that if one is going to teach only a sin-
gle machine learning technique in a computational
linguistics course, it should be conditional log-
linear modeling. Such models are pervasive in nat-
ural language processing. They have the form
p~?(y | x) ? exp
(
~? ? ~f (x, y)
)
, (1)
where ~f extracts a feature vector from context x
and outcome y ? Y(x). The set of possible out-
comes Y(x) might depend on the context x.1
1The model is equivalent to logistic regression when y is
a binary variable, that is, when Y(x) = {0, 1}.
We then present an interactive web visualiza-
tion that guides students through playing with log-
linear models and their estimation. This open-
source tool, available at http://cs.jhu.
edu/?jason/tutorials/loglin/, is in-
tended to develop intuitions, so that basic log-
linear models can be then taken for granted in fu-
ture lectures. It can be used near the start of a
course, perhaps after introducing probability no-
tation and n-gram models.
We used the tool in our Natural Language Pro-
cessing (NLP) class and received very positive
feedback. Students were excited by it, with some
saying the tool helped develop their ?physical in-
tuition? for log-linear models. Other test users
with no technical background also enjoyed work-
ing through the introductory lessons and found
that they began to understand the model.
The app includes 18 ready-to-use lessons for in-
dividual or small-group study or classroom use.
Each lesson, e.g. Figure 1, guides the student to
fit a probability model p~?(y | x) over some collec-
tion Y of shapes, words, or other images such as
parse trees. Each lesson is peppered with ques-
tions; students can be asked to answer some of
these questions in writing.2 Ambitious instruc-
tors can add new lessons or edit existing ones by
writing configuration files (see section 5.3). This
is useful for emphasizing specific concepts or ap-
plications. Section 8 provides some history and
applications of log-linear modeling, as well as as-
signment ideas.
2There are approximately 6 questions per lesson. We
found that answering all the questions took our students about
2300 words, or just under 23 words per question, which was
probably both unreasonable and unnecessary.
66
Figure 1: The first lesson; the lower half is larger on the actual application.
2 Why Teach With Log-Linear Models?
Log-linear models are very handy in NLP. They
can be used throughout a course, when one needs
? a global classifier for an applied task, such as
detecting sentiment, topic, spam, or gender;
? a local classifier for structure annotation,
such as tags or segment boundaries;
? a local classifier to be applied repeatedly in
sequential decision-making;
? a local conditional probability within some
generative process, such as an n-gram model,
HMM, PCFG, probabilistic FSA or FST,
noisy-channel MT model, or Bayes net;
? a global structured prediction method. Here
y is a complete structured object such as a
tagging, segmentation, parse, alignment, or
translation. Then p(y | x) is a Markov ran-
dom field or a conditional random field, de-
pending on whether x is empty or not.
Log-linear models over discrete variables are
also sufficiently expressive for an NLP course.
Students may experiment freely with adding their
own creative model features that refer to salient at-
tributes or properties of the data, since the proba-
bility (1) may consider any number of informative
features of the (x, y) pair.
How about training? Estimation of the pa-
rameter weights ~? from a set of fully observed
(x, y) pairs is simply a convex optimization prob-
lem. Maximizing the regularized conditional log-
likelihood
F (~?) =
(
N?
i=1
log p~? (yi | xi)
)
? C ?R(~?) (2)
is a simple, uniform training principle that can be
used throughout the course. The scaled regular-
izer C ? R(~?) prevents overfitting on sparse fea-
tures. This is arguably more straightforward than
the traditional NLP smoothing methods for esti-
mating probabilities from sparse data (Chen and
Goodman, 1996), which require applying various
ad hoc formulas to counts, and which do not gen-
eralize well to settings where there is not a natural
sequence of backoff models. There exist fast and
usable tools that students can use to train their log-
67
linear models, including, among others, MegaM
(Daume? III, 2004), and NLTK (Bird et al, 2009).3
Formally, log-linear models are a good gate-
way to a more general understanding of undirected
graphical models and the exponential family, in-
cluding globally normalized joint or conditional
distributions over trees and sequences.
One reason that log-linear models are both ver-
satile and pedagogically useful is that they do not
just make predictions, but explicitly model proba-
bilities. These can be
? combined with other probabilities using the
usual rules of probability;
? marginalized at test time to obtain the prob-
ability that the outcome y has a particular
property (e.g., one can sum over alignments);
? marginalized at training time in the case of
incomplete data y (e.g., the training data may
not include alignments);
? used to choose among possible decisions by
computing their expected loss (risk).
The training procedure also takes a probabilis-
tic view. Equation (2) helps illustrate impor-
tant statistical principles such as maximum likeli-
hood,4 regularization (the bias-variance tradeoff),
and cross-validation, as well as optimization prin-
ciples such as gradient ascent.
Log-linear models also provide natural exten-
sions of commonly taught NLP methods. For ex-
ample, under a probabilistic context-free gram-
mar (PCFG),5 p(parse tree | sentence) is propor-
tional to a product of rule probabilities. Simply
replacing each rule probability with an arbitrary
non-negative potential?an exponentiated weight,
or sum of weights of features of that rule?gives
an instance of (1). The same parsing algorithms
still apply without modification, as does the same
inside-outside approach to computing the poste-
rior expectation of rule counts and feature counts.
Immediate variants include CRF CFGs (Finkel
3A caveat is that generic log-linear training tools will iter-
ate over the setY(x) in order to maximize (1) and to compute
the constant of proportionality in (1) and the gradient of (2).
This is impractical when Y(x) is large, as in language mod-
eling or structured prediction. See Section 8.
4Historically, this objective has been regarded as the opti-
mization dual of a maximum entropy problem (Berger et al,
1996), motivating the log-linear form of (2). We have consid-
ered adding a maximum entropy view to our manipulative.
5Likewise for Markov or hidden Markov models.
et al, 2008), in which the rule features become
position-dependent and sentence-dependent, and
log-linear PCFGs (Berg-Kirkpatrick et al, 2010),
in which the feature-rich rule potentials are locally
renormalized into rule probabilities via (1).
For all these reasons, we recommend log-linear
models as one?s ?go-to? machine learning tech-
nique when teaching. Other linear classifiers,
such as perceptrons and SVMs, similarly choose
y given x based on a linear score ~f ? ~?(x, y)?but
these scores have no probabilistic interpretation,
and the procedures for training ~? are harder to un-
derstand or to justify. Thus, they can be taught
as variants later on or in another course. Further
reading includes (Smith, 2011).
3 The Teaching Challenge
Unfortunately, there is a difficulty with introduc-
ing log-linear models early in a course. Once
grasped, they seem very simple. But they are not
so easy to grasp for a student who has not had
any experience with high-dimensional parametric
functions, feature design, or statistical estimation.
The interaction among the parameters can be be-
wildering. Log-likelihood, gradient ascent, and
overfitting may also be new ideas.
Students who lack intuitions about these mod-
els will fail to follow subsequent lectures. They
will also have trouble with homework projects?
interpreting the weights learned by their model,
and diagnosing problems with their features or
their implementation. A student cannot even de-
sign appropriate feature sets without understand-
ing how the weights of these features interact to
define a distribution. We will discuss some of the
necessary intuitions in sections 6 and 7.
We would like equations (1), (2), and the gradi-
ent formula to be more than just recipes. The stu-
dent should regard them as familiar objects with
predictable behavior. Like computer science, ped-
agogy proceeds by layering new ideas on top of
already-familiar abstractions. A solid understand-
ing of basic log-linear models is prerequisite to
? using them in NLP applications that have
their own complexities,
? using them as component distributions within
larger probability models or decision rules,
? generalizing the algorithms for working with
(1) and (2) to settings where one cannot eas-
ily enumerate Y .
68
4 (Virtual) Manipulatives
Familiar concrete concepts have often been in-
voked to help develop intuitions about abstract
mathematical concepts. Specifically within early
math education, manipulatives?tactile objects?
have been shown to be effective hands-on teaching
tools. Examples include Cuisenaire rods for ex-
ploring arithmetic concepts like sums, ratios, and
place value, or geoboards for exploring geometric
concepts like area and perimeter.6 The key idea is
to ground and link the mathematical language to a
well-known physical object that can be inspected
and manipulated. For more, see the classic and re-
cent analyses from Sowell (1989) and Carbonneau
et al (2013).
Research has shown concrete manipulatives to
be effective, but practical widespread use of them
presents certain problems, including procurement
of necessary materials, replicability, and applica-
bility to certain groups of students and to con-
cepts that have no simple physical realization.
These issues have spurred interest over the past
two decades in virtual manipulatives implemented
in software, including the creation of the National
Library of Virtual Manipulatives.7 Both Clements
and McMillen (1996) and Moyer et al (2002) pro-
vide accessible overviews of virtual manipulatives
in early math education. Virtual manipulatives
give students the ability to effect changes on a
complex system and so learn its underlying prop-
erties (Moyer et al, 2002). This last point is par-
ticularly relevant to log-linear models.
Members of the NLP and speech communities
have previously explored manipulatives and the
idea of ?learning by doing.? Eisner (2002) im-
plemented HMM posterior inference and forward-
backward training on a spreadsheet, so that editing
the data or initial parameters changed the numeri-
cal computations and the resulting graphs. VIS-
PER, an applied educational tool that wrapped
various speech technologies, was targeted toward
understanding the acoustics and overall recog-
nition pipeline (Nouza et al, 1997). Light et
al. (2005) developed web interfaces for a num-
ber of core NLP technologies and systems, such
as parsers, part-of-speech taggers, and finite-state
6Cuisenaire rods are color-coded blocks with lengths from
1 to 10. A geoboard is a board representing the plane, with
pegs at the integral points. A rubber band can be stretched
around selected pegs to define a polygon.
7nlvm.usu.edu/en/nav/vlibrary.html and
enlvm.usu.edu/ma/nav/doc/intro.jsp
transducers. Matt Post created a Model 1 stack
decoder visualization for a recent machine trans-
lation class (Lopez et al, 2013).8 Most manipula-
tives/interfaces targeted at NLP have been virtual,
but a notable exception is van Halteren (2002),
who created a (physical) board game for parsing.
In machine learning, there is a plethora of vir-
tual manipulatives demonstrating central concepts
such as decision boundaries and kernel methods.9
There are also several systems for teaching artifi-
cial intelligence: these tend to to involve control-
ling virtual robots10 or physical ones (Tokic and
Bou Ammar, 2012). Overall, manipulatives for
NLP and ML seem to be a successful pedagogi-
cal direction that we hope will continue.
Next, we present our main contribution, a vir-
tual manipulative that teaches log-linear models.
We ground the models in simple objects such as
circles and regular polygons, in order to appeal to
the students? physical intuitions. Later lessons can
move on from shapes, instead using words or im-
ages from a particular application of interest.
5 Our Log-Linear Virtual Manipulative
Figure 1 shows a screenshot of the tool,
available at http://cs.jhu.edu/?jason/
tutorials/loglin/. We encourage you to
play with it as you read.
5.1 Student Interface
Successive lessons introduce various challenges or
subleties. In each lesson, the user experiments
with modeling some given dataset D using some
given set of K features. Dataset: For each context
x, the outcomes y ? Y(x) are displayed as shapes,
images or words. Features: For each feature fi,
there is a slider to manipulate ?i.
Each shape y is sized proportionately to its
model probability p~?(y | x) (equation (1)), so it
grows or shrinks as the user changes ~?. In con-
trast, the empirical probability
p? (y | x) =
c(x, y)
c(x)
(= ratio of counts) (3)
is constant and is shown by a gray outline.
8github.com/mjpost/stack-decoder
9E.g., http://cs.cmu.edu/?ggordon/SVMs/
svm-applet.html.
10E.g., http://www-inst.eecs.berkeley.edu/
?cs188/pacman/pacman.html and http://www.
cs.rochester.edu/trac/quagents.
69
The size and color of y indicate how p~?(y | x)
compares to this empirical probability (Figure 2).
Reinforcing this, the observed count c(x, y) is
shown at the upper left of y, while the expected
count c(x) ? p~?(y | x) is shown at the upper right,
following the same color scheme (Figure 1).
We begin with globally normalized models
(only one context x). For example, the data in
Figure 1?30 solid circles, 15 striped circles, 10
solid triangles, and 5 striped triangles?are to be
modeled with the two indicator features fcircle and
fsolid. With ~? = 0 we have the uniform dis-
tribution, so the solid circle is contained in its
gray outline (p?(solid circle) > p~?(solid circle)),
the striped triangle contains its gray outline
(p?(striped triangle) < p~?(striped triangle)), and
the striped circle and gray outline are coincident
(p?(striped circle) = p~?(striped circle)).
A student can try various activities:
In the outcome matching activity, the goal is to
match the model p~? to p?. The game is to make all
of the outcomes match their corresponding gray
outlines in size (and color). The student ?wins?
once the maximum number of objects turn gray.
In the feature matching activity, the goal is to
match the expected feature vector Ep~? [
~f ] to the
observed feature vector Ep?[~f ]. In Figure 1, the
student would seek a model that correctly predicts
the total number of circles and the total number
of solid objects?even if the specific number of
solid circles is predicted wrong. (The predicted
and observed counts for a feature can easily be
found by adding up the displayed counts of indi-
vidual outcomes having that feature. For conve-
nience, they are also displayed in a tooltip on the
feature?s slider.) This game can always be won,
even if the given features are not adequately ex-
pressive to succeed at outcome matching on the
given dataset.
In the log-likelihood activity, the goal is to max-
imize the log-likelihood. The log-likelihood bar
(Figure 1) adapts to changes in ~?, just like the
shapes. The game is to make the bar as long as
possible.11 In later lessons, the student instead
tries to maximize a regularized version of the log-
likelihood bar, which is visibly shortened by a
penalty for large weights (to prevent overfitting).
Winning any of these games with more complex
models becomes difficult or at least tedious, so au-
11Once the gradient is introduced in a later lesson, knowing
when you have ?won? becomes clearer.
Quantity
of Interest
> 0 = 0 < 0
red gray blue
p?? p~?
Ep? [?]?
Ep~? [?]
15 30 60
?~?F
Figure 2: Color and area indicate differences be-
twen the empirical distribution (gray outline) and
model distribution. Red (or blue) indicates a
model probability or parameter that should be in-
creased (or decreased) to fit the data.
Figure 3: Gradient components use the same color
coding as given in Figure 2. The length of each
component indicates its potential effect on the ob-
jective. Note that the sliders use a nonlinear scale
from ?? to +?.
tomatic methods come as a relief. The student may
view hints on the sliders, showing which way each
slider should be nudged (Figure 3). These hints
correspond to components of the log-likelihood
gradient. Further automation is offered by the
?Step? button, which automatically nudges all pa-
rameters by taking a step of gradient ascent,12 and
even more by the ?Solve? button, which steps all
the way to the maximum.13
Our lessons guide the student to appreciate the
relationship among the three activities. First, fea-
ture matching is a weaker, attainable version of
outcome matching (when outcome matching is
12When `1 regularization is used, the optimal ~? often con-
tains many 0 weights, and a step is not permitted to jump
over a (possibly optimal) weight of 0. It stops at 0, though if
warranted, it can continue past 0 on the next step.
13The ?Solve? button adapts the stepsize at each step, using
a backtracking line search with the Armijo condition. This
ensures convergence.
70
possible it certainly achieves feature matching as
well). Second, feature matching is equivalent
to maximizing the (unregularized) log-likelihood.
Thus the mismatch is 0 iff the gradient of log-
likelihood is 0. In fact, the mismatch equals the
gradient even when they are not 0! Thus, drag-
ging the sliders in the direction of the gradient
hints can be viewed as a correct strategy for either
the feature matching game or the log-likelihood
game. This connection shows that the current gra-
dient of log-likelihood can easily be computed by
summing up the observed and currently predicted
counts of each feature. After understanding this
and playing with the ?Step? and ?Solve? buttons,
the student should be able to imagine writing code
to train log-linear models.
5.2 Guided Exploration
We expect students to ?learn by playing.? The user
can experiment at any time with the sliders, with
gradient ascent and its stepsize, with the type and
strength of regularization, and with the size of the
dataset. The user can also sample new data or new
parameters, and can peek at the true parameters.
These options are described further in Section 7.
We encourage experimentation by providing
tooltips that appear whenever a student hovers the
mouse pointer over a element of the GUI. Tooltips
provide guidance about whatever the student is
looking at right then. Some are static explanations
(e.g., what does this gray bar represent?). Others
dynamically update with changes to the parame-
ters (e.g., the tooltips on the feature sliders show
the observed and expected counts of that feature).
Students see the tooltips repeatedly, which can
help them absorb and reinforce concepts over an
extended period of time. Students who like to
learn by browsing and experimenting can point to
various tooltips and get a sense of how the differ-
ent concepts fit together. Some tooltips explicitly
refer to one another, linking GUI elements such as
the training objective, the regularization choices,
and the gradient.
Though the user is welcome to play, we also
provide some guidance. Each lesson displays in-
structions that explain the current dataset, jus-
tify modeling choices, introduce new functional-
ity, lead the user through a few activities, and ask
lesson-specific questions. The first lesson also
links to a handout with a more formal textbook-
style treatment. The last lesson links to further
Figure 4: Inventory of available shapes
(circle/triangle/square/pentagon) and fills
(solid/striped/hollow). Text and arbitrary im-
ages may be used instead of shapes. Color and
size are reserved to indicate how the current
model?s predictions of outcome counts or feature
counts compare to the empirical values?see
Figure 2.
reading and exercises.
5.3 Instructor Interface: Creating and
Tailoring Lessons
An instructor may optionally wish to tailor lessons
to his or her students? needs, interests, and abil-
ities. Shapes provide a nice introduction to log-
linear models, but eventually NLP students will
want to think about NLP problems, whereas vi-
sion students will want to think about vision prob-
lems. Thus, we have designed the manipulative to
handle text and arbitrary images, as well as the 12
shape-fill combinations shown in Figure 4.
Tailoring lessons to the students? needs is as
simple as editing a couple of text files. These must
specify (1) a set of features, (2) a set of contexts,14
and (3) for each context, a set of featurized events,
including counts and visual positions. This simple
format allows one to describe some rather involved
models. Some of the features may be ?hidden?
from the student, thereby allowing the student to
experience model mismatch. Note that the visual
positioning information is pedagogically impor-
tant: aligning objects by orthogonal descriptions
can make feature contrasts stand out more, e.g.,
circles vs. triangles or solid vs. striped.
The configuration files can turn off certain fea-
tures on a per-lesson basis (without program-
14The set of contexts may be omitted when there is only
one context (i.e., an unconditioned model).
71
ming). This is useful for, e.g., hiding the ?Solve?
button in early lessons, adding new tooltips, or
specializing the existing tooltips on a per-lesson
basis.
However, being a manipulative rather than a
tutoring system, our software does not monitor
the user?s progress through a lesson and provide
guidance via lesson-specific hints, warnings, ques-
tions, or feedback. (The software is open-source,
so others are free to extend it in this way.)
5.4 Back-End Implementation
Anyone can use our virtual manipulative sim-
ply by visiting its website. There is no start-up
cost. Aside from reading the data, model and in-
structions from the web server, it is fully client-
side. The Javascript back-end uses common and
well-supported open-source libraries that provide
a consistent experience across browsers.15 The
manipulative relies on certain capabilities from the
HTML5 standard. Not all browsers in current
use support these capabilities, notably Internet Ex-
plorer 9 and under. The tool works with recent
versions of Firefox, Chrome and Safari.
6 Pedagogical Aims
6.1 Modeling and Estimation
When faced with a dataset D of (x, y) pairs, one
often hopes to choose an appropriate model.
When are log-linear models appropriate? Why
does their hypothesis space include the uniform
distribution? For what feature sets does it include
every distribution?
One should also understand statistical estima-
tion. How do the features interact? When esti-
mating their weights, can raising one weight alter
or reverse the desired changes to other weights?
How can parameter estimation go wrong statis-
tically (overfitting, perhaps driving parameters to
??)? What might happen if we have a very
large feature set? Can we design regularized es-
timators that prevent overfitting (the bias-variance
tradeoff)? What is the effect of the regularization
constant on small and large datasets? On rare and
frequent contexts? On rare and frequent features?
On useful features (including features that always
or never fire) and useless ones?
15Specifically and in order, d3 (d3js.org/), jQuery
(jquery.com/), jQuery UI (jqueryui.com),
jQuery Tools (jquerytools.org/), and qTip
(craigsworks.com/projects/qtip/).
Finally, one is responsible for feature design.
Which features usefully distinguish among the
events? How do non-binary features work and
when are they appropriate? When can a feature
safely be omitted because it provides no additional
modeling power? How does the choice of features
affect generalization, particularly if the objective
is regularized? In particular, how do shared fea-
tures and backoff features allow a model to gen-
eralize to novel contexts and outcomes (or rare
ones)? How do the resulting patterns of general-
ization relate qualitatively to traditional smoothing
techniques in NLP (Chen and Goodman, 1996)?
6.2 Training Algorithm
We also aim to convey intuitions about a specific
training algorithm. We use the regularized condi-
tional log-likelihood (2) to define the goodness of
a parameter vector ~?. The best choice is then the ~?
that solves equation (4):
0 = ?~?F = Ep?
[
~f(X,Y )
]
? Ep~?
[
~f(X,Y )
]
? C?~?R(
~?)
(4)
where because our model is conditional, p~?(x, y)
denotes the hybrid distribution p?(x) ? p~?(y | x).
Many important concepts are visible in (2) and
(4). As discussed earlier, (4) includes the dif-
ference between observed and expected feature
counts,
Ep?
[
~f(X,Y )
]
? Ep~?
[
~f(X,Y )
]
. (5)
Students must internalize this concept and the
meaning of the two counts above. This prepares
them to understand the extension to structured pre-
diction, where these counts can be more diffi-
cult to compute (see Section 8). It also prepares
them to generalize to training latent-variable mod-
els (Petrov and Klein, 2008). In that setting, the
observed count can no longer be observed but is
replaced by another expectation under the model,
conditioned on the partial training data.
(4) also includes a weight decay term for regu-
larization. We allow both `1 and `2 regularization:
R(~?) = ?~??1 versus R(~?) = ?~??22. One can see
experimentally that strong `1 regularization tries
to use a few larger weights and leave the rest at
0, while strong `2 regularization tries to share the
work among many smaller weights. One can ob-
serve how for a given C, the regularization term is
72
more important for small datasets, since for larger
datasets it is dominated by the log-likelihood.
Once one can compute the gradient, one can
?follow? it along the surface, in a way that is guar-
anteed to increase the convex objective function up
to its global maximum. The ?Solve? button does
this and indeed one can watch the log-likelihood
bar continually increase. Yet one should observe
what might go wrong here as well. Gradient ascent
can oscillate if a fixed stepsize is used (by click-
ing ?Step? repeatedly). One may also notice that
?Solve? is somewhat slow to converge on some
problems, which motivates considering alternative
optimization algorithms (Malouf, 2002).
We should note that we are not concerned
with efficiency issues, e.g., tractably computing
the normalizers Z(x). Efficient normalization is
a crucial practical ingredient in using log-linear
models, but our primary concern is to impart a
near-physical intuitive understanding of the mod-
els themselves. See Section 8 or Smith (2011) for
strategies on computing the normalizer.
7 Provided Lessons
In this section we provide an overview of the 18
currently available lessons. (Of course, you can
work through the lessons yourself for further de-
tails.) ?Core? lessons that build intuition precede
the ?applied? lessons focused on NLP tasks or
problems. Instructors should feel especially free
to replace or reorder the ?applied? lessons.
Core lessons 1?5 provide a basic introduction
to log-linear modeling, using unconditioned distri-
butions over only four shapes as shown in Figure
1. We begin by matching outcomes using just ?cir-
cle? and ?solid? features. We discover in lesson 2
that it is redundant to add ?triangle? and ?striped?
features. In lesson 3 we encounter a dataset which
these features cannot fit, because the shape and
fill attributes are not statistically independent. We
remedy this in lesson 4 with a conjunctive ?striped
triangle? feature.
Because outcome matching fails in lesson 3,
lessons 3?4 introduce feature matching and log-
likelihood as suitable alternatives. Lesson 5 briefly
illustrates a non-binary feature function, ?number
of sides? (taking values 3, 4, and 5 on triangles,
squares, and pentagons). This clarifies the match-
ing of feature counts: here we are trying to predict
the total number of sides in the dataset.
Lessons 6?8 focus on optimization. They move
up to the harder setting of 9 shapes with 6 fea-
tures, so we tell students how to turn on the gra-
dient ?hints? on the sliders. We explain how these
hints relate to feature matching and log-likelihood.
We invite the students to try using the hints on
earlier lessons?and on new random datasets that
they can generate by clicking. In Lesson 7, we
introduce the ?Step? and ?Solve? buttons to help
even more with a difficult dataset. Students use all
these GUI elements to climb the convex objective
and increase the log-likelihood bar.
At this point we introduce regularization. Les-
son 6 invited students to generate small random
datasets and observe their high variance and the
tendency to overfit them. Lesson 8 gives a more
dramatic illustration of overfitting: with no ob-
served pentagons, the solver sends ?pentagon ?
?? to make p~?(pentagon) ? 0. We prevent this
by adding a regularization penalty, which reserves
some probability for pentagons. Striped pentagons
turn out to be the least likely pentagons, because
stripes were observed to be uncommon on other
shapes (so ?striped < 0). Thus we see that our
choice of features allows this ?smoothing method?
to make useful generalizations about novel out-
comes.
Lessons 9?10 consider the effect of `1 versus
`2 regularization, and the competition between the
regularizer (scaled by the constant C) and the log-
likelihood (scaled by the dataset size N ).16
Lessons 11?13 introduce conditional models,
showing how features are shared among three con-
texts. The third context is unobserved, yet our
trained model makes plausible predictions about
it. The conditional probabilities of unobserved
shapes are positive even without regularization, in
contrast to the joint probabilities in lesson 9.
We see that a frequent context x generally has
more influence on the parameters. But this need
not be true if the parameters do not help to distin-
guish among the particular outcomes Y(x).
Lessons 14?15 explore feature design in condi-
tional models. We model conditional probabilities
of the form p(fill | shape). ?Unigram? features
can favor certain fills y regardless of the shape.
?Bigram? features that look at y and x together
can favor different fills for each shape type. We
see that features that depend only on the shape x
cannot distinguish among fills y, and so have no
16Clever students may think to try setting C < 0, which
breaks convexity of the objective function.
73
effect on the conditional probabilities p(y | x).
Lesson 15 illustrates how regularization pro-
motes generalization and feature selection. Once
we have a full set of bigram features, the uni-
gram features are redundant. We never have to
put a high weight on ?solid?: we can accomplish
the same thing by putting high weights on ?solid
triangle? and ?solid circle? separately. Yet this
misses a generalization because it does not pre-
dict that ?solid? is also likely for pentagons. For-
tunately, regularization encourages us to avoid too
many high weights. So we prefer to put a single
high weight on ?solid,? and use the ?solid triangle?
and ?solid circle? features only to model smaller
shape-specific deviations from that generalization.
As a result, we will indeed extrapolate that pen-
tagons tend to be solid as well.
Lesson 16 begins the application-driven
lessons:
One lesson builds on the ?unigram? and ?bi-
gram? concepts to create a ?bigram language
model??a model of shape sequences over a vo-
cabulary of 9 shapes. A shape?s probability de-
pends not only on its attributes but also on the
attributes that it shares with the previous shape.
What is the probability of a striped square given
that the previous shape was also striped, or a
square, or a striped square?
We also apply log-linear modeling to the task
of text categorization (spam detection). We chal-
lenge the students to puzzle out how this model
is set up and how to generalize it to three-way
categorization. Our contexts in this case are
documents?actually very short phrases. Most
contexts are seen only once, with an outcome of ei-
ther ?mail? or ?spam.? Our feature set implements
logistic regression (footnote 1): each feature con-
joins y = spam with some property of the text
x, such as ?contains ?parents?,? ?has boldface,? or
?mentions money.?
Additional linguistic application lessons may be
added in the near future?e.g., modeling the rela-
tive probability of grammar rules or parse trees.
The final lesson summarizes what has been
learned, mentions connections to other ideas in
machine learning, and points the student to further
resources.
8 Graduating to Real Applications
At the time of writing, 3266 papers in the ACL
Anthology mention log-linear models, with 137
using ?log-linear,? ?maximum entropy? or ?max-
ent? in the paper title. These cover a wide range of
applications that can be considered in lectures or
homework projects.
Early papers may cover the most fundamen-
tal applications and the clearest motivation. Con-
ditional log-linear models were first popularized
in computational linguistics by a group of re-
searchers associated with the IBM speech and lan-
guage group, who called them ?maximum entropy
models,? after a principle that can be used to mo-
tivate their form (Jaynes, 1957). They applied the
method to various binary or multiclass classifica-
tion problems in NLP, such as prepositional phrase
attachment (Ratnaparkhi et al, 1994), text catego-
rization (Nigam et al, 1999), and boundary pre-
diction (Beeferman et al, 1999).
Log-linear models can be also used for struc-
tured prediction problems in NLP such as tagging,
parsing, chunking, segmentation, and language
modeling. A simple strategy is to reduce struc-
tured prediction to a sequence of multiclass pre-
dictions, which can be individually made with a
conditional log-linear model (Ratnaparkhi, 1998).
A more fully probabilistic approach?used in the
original ?maximum entropy? papers?is to use (1)
to define the conditional probabilities of the steps
in a generative process that gradually produces the
structure (Rosenfeld, 1994; Berger et al, 1996).17
This idea remains popular today and can be used
to embed rich distributions into a variety of gener-
ative models (Berg-Kirkpatrick et al, 2010). For
example, a PCFG that uses richly annotated non-
terminals involves a large number of context-free
rules. Rather than estimating their probabilities
separately, or with traditional backoff smoothing,
a better approach is to use (1) to model the proba-
bility of all rules given their left-hand sides, based
on features that consider attributes of the nonter-
minals.18
The most direct approach to structured predic-
tion is to simply predict the structured output all
at once, so that y is a large structured object with
many features. This is conceptually natural but
means that the normalizer Z(x) involves sum-
ming over a large space Y(x) (footnote 3). One
17Even predicting the single next word in a sentence can be
broken down into a sequence of binary decisions in this way.
This avoids normalizing over the large vocabulary (Mnih and
Hinton, 2008).
18E.g., case, number, gender, tense, aspect, mood, lexical
head. In the case of a terminal rule, the spelling or morphol-
ogy of the terminal symbol can be considered.
74
can restrict Y(x) before training (Johnson et al,
1999). More common is to sum efficiently by
dynamic programming or sampling, as is typical
in linear-chain conditional random fields (Lafferty
et al, 2001), whole-sentence language modeling
(Rosenfeld et al, 2001), and CRF CFGs (Finkel
et al, 2008). This topic is properly deferred until
such algorithmic techniques are introduced later in
an NLP class, for example in a unit on parsing (see
discussion in section 2). We prepare students for
it by mentioning this point in our final lesson.19
Our final lesson also leads to a web page where
we link to log-linear software and to various
pencil-and-paper problems, homework projects,
and readings that an instructor may consider as-
signing. We welcome suggested additions to this
page.
9 Conclusion
We have introduced an open-source, web-based
virtual manipulative for log-linear models. In-
cluded with the code are 18 lessons peppered with
questions, a handout that gives a formal treatment
of the necessary derivations, and auxiliary infor-
mation including further reading, practice prob-
lems, and recommended software. A version is
available at http://cs.jhu.edu/?jason/
tutorials/loglin/.
Acknowledgements We would like to thank the
anonymous reviewers for their helpful feedback
and suggestions and the entire Fall 2012 Natural
Language Processing course at Johns Hopkins.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1?3):177?210.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
DeNero, John DeNero, and Dan Klein. 2010. Pain-
less unsupervised learning with features. In Pro-
ceedings of NAACL, June.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum-entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
19This material can also be connected to other topics in
machine learning. Dynamic programming and sampling are
also used for exact or approximate computation of normal-
izers in undirected graphical models (Markov random fields
or conditional random fields), which are really just log-linear
models for structured prediction of tuples.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Kira J. Carbonneau, Scott C. Marley, and James P.
Selig. 2013. A meta-analysis of the efficacy of
teaching mathematics with concrete manipulatives.
Journal of Educational Psychology, 105(2):380 ?
400.
Stanley Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques. In Proceedings
of ACL.
Douglas H. Clements and Sue McMillen. 1996. Re-
thinking ?concrete? manipulatives. Teaching Chil-
dren Mathematics, 2(5):pp. 270?279.
Hal Daume? III. 2004. Notes on CG and
LM-BFGS optimization of logistic regression.
Paper available at http://pub.hal3.name#
daume04cg-bfgs, implementation available at
http://hal3.name/megam/, August.
Jason Eisner. 2002. An interactive spreadsheet
for teaching the forward-backward algorithm. In
Dragomir Radev and Chris Brew, editors, Proceed-
ings of the ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 10?
18, Philadelphia, July.
Jenny Rose Finkel, Alex Kleeman, and Christopher D
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. Proceedings of ACL-
08: HLT, pages 959?967.
E. T. Jaynes. 1957. Information theory and statistical
mechanics. Physics Reviews, 106:620?630.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Pro-
ceedings of ACL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Marc Light, Robert Arens, and Xin Lu. 2005. Web-
based interfaces for natural language processing
tools. In Proceedings of the Second ACL Workshop
on Effective Tools and Methodologies for Teaching
NLP and CL, pages 28?31, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Adam Lopez, Matt Post, Chris Callison-Burch,
Jonathan Weese, Juri Ganitkevitch, Narges Ah-
midi, Olivia Buzek, Leah Hanson, Beenish Jamil,
Matthias Lee, et al 2013. Learning to translate with
products of novices: Teaching MT with competi-
tive challenge problems. Transactions of the ACL
(TACL), 1.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of CoNLL, pages 49?55.
75
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In Pro-
ceedings of NIPS.
Patricia S. Moyer, Johnna J. Bolyard, and Mark A.
Spikell. 2002. What are virtual manipulatives?
Teaching Children Mathematics, 8(6):372?377.
Kamal Nigam, John Lafferty, and Andrew McCallum.
1999. Using maximum entropy for text classifica-
tion. In IJCAI-99 Workshop on Machine Learning
for Information Filtering, pages 61?67.
Jan Nouza, Miroslav Holada, and Daniel Hajek. 1997.
An educational and experimental workbench for vi-
sual processing of speech data. In Fifth European
Conference on Speech Communication and Technol-
ogy.
Slav Petrov and Dan Klein. 2008. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of EMNLP, pages 867?876, October.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of the ARPA Hu-
man Language Technology Workshop, pages 250?
255.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania, July.
Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: A vehicle for linguistic-statistical integration.
Computer Speech and Language, 15(1).
Ronald Rosenfeld. 1994. Adaptive Statistical Lan-
guage Modeling: A Maximum Entropy Approach.
Ph.D. thesis, Carnegie Mellon University.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Synthesis Lectures on Human Language Technolo-
gies. Morgan and Claypool, May.
Evelyn J Sowell. 1989. Effects of manipulative materi-
als in mathematics instruction. Journal for Research
in Mathematics Education, pages 498?505.
Michel Tokic and Haitham Bou Ammar. 2012. Teach-
ing reinforcement learning using a physical robot.
In Proceedings of the ICML Workshop on Teaching
Machine Learning.
Hans van Halteren. 2002. Teaching nlp/cl through
games: The case of parsing. In Proceedings of
the ACL Workshop on Effective Tools and Method-
ologies for Teaching NLP and CL, pages 1?9,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
76
