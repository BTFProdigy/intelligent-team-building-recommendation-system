Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 871?879,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Multilingual Spectral Clustering
Using Document Similarity Propagation
Dani Yogatama and Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology, University of Tokyo
13F Akihabara Daibiru, 1-18-13 Kanda Chiyoda-ku, Tokyo, Japan
yogatama@cl.ci.i.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jp
Abstract
We present a novel approach for multilin-
gual document clustering using only com-
parable corpora to achieve cross-lingual
semantic interoperability. The method
models document collections as weighted
graph, and supervisory information is
given as sets of must-linked constraints for
documents in different languages. Recur-
sive k-nearest neighbor similarity propa-
gation is used to exploit the prior knowl-
edge and merge two language spaces.
Spectral method is applied to find the best
cuts of the graph. Experimental results
show that using limited supervisory in-
formation, our method achieves promis-
ing clustering results. Furthermore, since
the method does not need any language
dependent information in the process, our
algorithm can be applied to languages in
various alphabetical systems.
1 Introduction
Document clustering is unsupervised classifica-
tion of text collections into distinct groups of sim-
ilar documents. It has been used in many in-
formation retrieval tasks, including data organiza-
tion (Siersdorfer and Sizov, 2004), language mod-
eling (Liu and Croft, 2004), and improving per-
formances of text categorization system (Aggar-
wal et al, 1999). Advance in internet technology
has made the task of managing multilingual docu-
ments an intriguing research area. The growth of
internet leads to the necessity of organizing docu-
ments in various languages. There exist thousands
of languages, not to mention countless minor ones.
Creating document clustering model for each lan-
guage is simply unfeasible. We need methods to
deal with text collections in diverse languages si-
multaneously.
Multilingual document clustering (MLDC) in-
volves partitioning documents, written in more
than one languages, into sets of clusters. Simi-
lar documents, even if they are written in differ-
ent languages, should be grouped together into
one cluster. The major challenge of MLDC is
achieving cross-lingual semantic interoperability.
Most monolingual techniques will not work since
documents in different languages are mapped into
different spaces. Spectral method such as Latent
Semantic Analysis has been commonly applied
for MLDC task. However, current techniques
strongly rely on the presence of common words
between different languages. This method would
only work if the languages are highly related, i.e.,
languages that share the same root. Therefore, we
need another method to improve the robustness of
MLDC model.
In this paper, we focus on the problem of bridg-
ing multilingual space for document clustering.
We are given text documents in different lan-
guages and asked to group them into clusters such
that documents that belong to the same topic are
grouped together. Traditional monolingual ap-
proach is impracticable since it is unable to pre-
dict how similar two multilingual documents are.
They have two different spaces which make con-
ventional cosine similarity irrelevant. We try to
solve this problem utilizing prior knowledge in
the form of must-linked constraints, gathered from
comparable corpora. Propagation method is used
to guide the language-space merging process. Ex-
perimental results show that the approach gives
encouraging clustering results.
This paper is organized as follows. In section 2,
we review related work. In section 3, we propose
our algorithm for multilingual document cluster-
ing. The experimental results are shown in section
4. Section 5 concludes with a summary.
871
2 Related Work
Chen and Lin (2000) proposed methods to clus-
ter multilingual documents using translation tech-
nology, relying on cross-lingual dictionary and
machine-translation system. Multilingual ontol-
ogy, such as Eurovoc, is also popular for MLDC
(Pouliquen et al, 2004). However, such resources
are scarce and expensive to build. Several other
drawbacks of using this technique include dictio-
nary limitation and word ambiguity.
More recently, parallel texts have been used to
connect document collections from different lan-
guages (Wei et al, 2008). This is done by collaps-
ing columns in a term by document matrix that are
translations of each other. Nevertheless, building
parallel texts is also expensive and requires a lot of
works, hence shifting the paradigm of multilingual
works to comparable corpora.
Comparable corpora are collections of texts in
different languages regarding similar topics pro-
duced at the same time. The key difference be-
tween comparable corpora and parallel texts is that
documents in comparable corpora are not neces-
sarily translations of each other. They are easier
to be acquired, and do not need exhaustive works
to be prepared. News agencies often give informa-
tion in many different languages and can be good
sources for comparable corpora. Terms in com-
parable corpora, being about the same topic, up
to some point explain the same concepts in differ-
ent languages. Pairing comparable corpora with
spectral method such as Latent Semantic Analysis
has become prevalent, e.g. (Gliozzo and Strappar-
ava, 2005). They rely on the presence of common
words and proper nouns among various languages
to build a language-independent space. The per-
formance of such method is highly dependent on
the languages being used. Here, we present an-
other approach to exploit knowledge in compa-
rable corpora; using propagation method to aid
spreading similarity between collections of docu-
ments in different languages.
Spectral clustering is the task of finding good
clusters by using information contained in the
eigenvectors of a matrix derived from the data.
It has been successfully applied in many applica-
tions including information retrieval (Deerwester
et al, 2003) and computer vision (Meila and Shi,
2000). An in-depth analysis of spectral algo-
rithm for clustering problems is given in (Ng et
al., 2002). Zhang and Mao (2008) used a related
technique called Modularity Eigenmap to extract
community structure features from the document
network to solve hypertext classification problem.
Semi-supervised clustering enhances clustering
task by incorporating prior knowledge to aid clus-
tering process. It allows user to guide the cluster-
ing process by giving some feedback to the model.
In traditional clustering algorithm, only unlabeled
data is used to find assignments of data points
to clusters. In semi-supervised clustering, prior
knowledge is given to improve performance of the
system. The supervision is usually given as pair
of must-linked constraints and cannot link con-
straints, first introduced in (Wagstaff and Cardie,
2000). Kamvar et al (2003) proposed spectral
learning algorithm that can take supervisory infor-
mation in the form of pairwise constraints or la-
beled data. Their algorithm is intended to be used
in monolingual context, while our algorithm is de-
signed to work in multilingual context.
3 Multilingual Spectral Clustering
There have been several works on multilingual
document clustering as mention previously in Sec-
tion 2. Our key contribution here is the propaga-
tion method to make spectral clustering algorithm
works for multilingual problems. The clustering
model exploits the supervisory information by de-
tecting k nearest neighbors of the newly-linked
documents, and propagates document similarity to
these neighbors. The model can be applied to any
multilingual text collections regardless of the lan-
guages. Overall algorithm is given in Section 3.1
and the method to merge multilingual spaces by
similarity propagation is given in Section 3.2.
3.1 Spectral Clustering Algorithm
Spectral clustering tries to find good clusters by
using top eigenvectors of normalized data affin-
ity matrix. The document set is being modeled as
undirected graph G(V,E,W ), where V , E, and
W denote the graph vertex set, edge set, and tran-
sition probability matrix, respectively. In graph
G, v ? V represents a document, and weight
w
ij
?W represents transition probability between
document v
i
to v
j
. The transition probabilities
can be interpreted as edge flows in Markov ran-
dom walk over graph vertices (documents in col-
lections).
Algorithm to perform spectral clustering is
given in Algorithm 1. Let A be affinity matrix
872
where element A
ij
is cosine similarity between
document v
i
and v
j
(Algorithm 1, line 1). It is
straightforward that documents belonging to dif-
ferent languages will have similarity zero. Rare
exception occurs when they have common words
because the languages are related one another.
As a consequence, the similarity matrix will have
many zeros. Our model amplifies prior knowledge
in the form of comparable corpora by perform-
ing document similarity propagation, presented in
Section 3.2 (Algorithm 1, line 4; Algorithm 2, ex-
plained in Section 3.2). After propagation, the
affinity matrix is post-processed (Algorithm 1, line
6, explained in Section 3.2) before being trans-
formed into transition probability matrix.
The transformation can be done using any nor-
malization for spectral methods. Define N =
D
?1
A, as in (Meila and Shi, 2001), where D is the
diagonal matrix whose elements D
ij
=
?
j
A
ij
(Algorithm 1, line 7). Alternatively, we can define
N = D
?1/2
AD
?1/2
(Ng et al, 2002), or N =
(A + d
max
I ? D)/d
max
(Fiedler, 1975), where
d
max
is the maximum rowsum of A. For our ex-
periment, we use the first normalization method,
though other methods can be applied as well.
Meila and Shi (2001) show that probability tran-
sition matrix N with t strong clusters will have t
piecewise constant eigenvectors. They also sug-
gest using these t eigenvectors in clustering pro-
cess. We use the information contains in t largest
eigenvectors of N (Algorithm 1, line 8-11) and
perform K-means clustering algorithm to find the
data clusters (Algorithm 1, line 12).
3.2 Propagating Prior Knowledge
We use information obtained from comparable
corpora to merge multilingual language spaces.
Suppose we have text collections in L different
languages. We combine this collections with com-
parable corpora, also in L languages, that act as
our supervisory information. Comparable corpora
are used to gather prior knowledge by making
must-linked constraints for documents in different
languages that belong to the same topic in the cor-
pora, propagating similarity to other documents
while doing so.
Initially, our affinity matrix A represents cosine
similarity between all pairs of documents. A
ij
is
set to zero if j is not the top k nearest neighbors
of i and likewise. Next, set A
ij
and A
ji
to 1 if
document i and document j are different in lan-
Algorithm 1 Multilingual Spectral Clustering
Input: Term by document matrix M , pairwise
constraints
Output: Document clusters
1: Create graph affinity matrix A ? R
n?n
where
each element A
ij
represents the similarity be-
tween document v
i
and v
j
.
2: for all pairwise constraints in comparable cor-
pora do
3: A
ij
? 1, A
ji
? 1.
4: Recursive Propagation (A,S, ?, k, v
i
, v
j
).
5: end for
6: Post-process matrix A so that every value in
A is greater than ? and less than 1.
7: Form a diagonal matrix D, where D
ii
=
?
j
A
ij
. Normalize N = D
?1
A.
8: Find x
1
, x
2
? ? ? , x
t
, the t largest eigenvectors
of N.
9: Form matrix X = [x
1
, x
2
, ? ? ? , x
t
] ? R
n?t
.
10: Normalize row X to be unit length.
11: Project each document into eigen-space
spanned by the above t eigenvectors (by treat-
ing each row of X as a point in R
t
, row i rep-
resents document v
i
).
12: ApplyK-means algorithm in this space to find
document clusters.
guage and belong to the same topic in our com-
parable corpora. This will incorporate the must-
linked constraint to our model. We can also give
supervisory information for pairs of document in
the same language, but this is optional. We also do
not use cannot-linked constraints since the main
goal is to merge multilingual spaces. In our exper-
iment we show that using only must-linked con-
straints with propagation is enough to achieve en-
couraging clustering results.
The supervisory information acquired from
comparable corpora only connects two nodes in
our graph. Therefore, the number of edges be-
tween documents in different languages is about
as many as the number of must-linked constraints
given. We argue that we need more edges between
pairs of documents in different languages to get
better results.
We try to build more edges by propagating sim-
ilarity to other documents that are most similar to
the newly-linked documents. Figure 1 gives an il-
lustration of edge-creation process when two mul-
tilingual documents (nodes) are connected. Sup-
873
yx1
v
i
y
x2
z
x1
v
j
z
x2
(a) Connect two nodes
y
x1
v
i
y
x2
z
x1
v
j
z
x2
(b) Effect on neighbor nodes
Figure 1: Pairing two multilingual documents af-
fect their neighbors. v
i
and v
j
are documents in
two different languages. y
x
and z
x
are neighbors
of v
i
and v
j
respectively.
pose that we have six documents in two differ-
ent languages. Initially, documents are only con-
nected with other documents that belong to the
same language. The supervisory information tells
us that two multilingual documents v
i
and v
j
should be connected (Figure 1(a)). We then build
an edge between these two documents. Further-
more, we also use this information to build edges
between v
i
and neighbors of v
j
and likewise (Fig-
ure 1(b)).
This follows from the hypothesis that bringing
together two documents should also bring other
documents that are similar to those two closer in
our clustering space. Klein et al (2002) stated
that a good clustering algorithm, besides satisfy-
ing known constraints, should also be able to sat-
isfy the implications of those constraints. Here,
we allow not only instance-level inductive impli-
cations, but utilize it to get higher-level inductive
implications. In other words, we alter similarity
space so that it can detect other clusters by chang-
ing the topology of the original space.
The process is analogous to shortening the dis-
tance between sets of documents in Euclidean
space. In vector space model, two documents that
are close to each other have high similarity, and
thus will belong to the same cluster. Pairing two
documents can be seen as setting the distance in
this space to 0, thus raising their similarity to 1.
While doing so, each document would also draw
sets of documents connected to it closer to the cen-
tre of the merge, which is equivalent to increasing
their similarities.
Suppose we have document v
i
and v
j
, and y and
z are sets of their respective k nearest neighbors,
where |y| = |z| = k. The propagation method
is a recursive algorithm with base S, the num-
ber of desired level of propagation. Recursive k-
nearest neighbor makes decision to give high sim-
ilarity between multilingual documents not only
determined by their similarity to the newly-linked
documents, but also their similarity to the k near-
est neighbors of the respective document. Several
documents are affected by a single supervisory in-
formation. This will prove useful when only lim-
ited amount of supervisory information given. It
uses document similarity matrix A, as defined in
the previous section.
1. For y
x
? y we propagate ?A
v
i
y
x
to A
v
j
y
x
.
Set A
y
x
v
j
= A
v
j
y
x
(Algorithm 2, line 5-6).
In other words, we propagate the similarity
between document v
i
and y nearest neighbors
of v
i
to document v
j
.
2. Similarly, for z
x
? z we propagate ?A
v
j
z
x
to A
v
i
z
x
. Set A
z
x
v
i
= A
v
i
z
x
(Algorithm 2,
line 10-11). In other words, we propagate the
similarity between document v
j
and z nearest
neighbors of v
j
to document v
i
.
3. Propagate higher order similarity to k nearest
neighbors of y and z, discounting the similar-
ity quadratically, until required level of prop-
agation S is reached (Algorithm 2, line 7 and
12).
The coefficient ? represents the degree of en-
forcement that the documents similar to a docu-
ment in one language, will also have high simi-
larity with other document in other language that
is paired up with its ancestor. On the other hand,
k represents the number of documents that are af-
fected by pairing up two multilingual documents.
After propagation, similarity of documents that
falls below some threshold ? is set to zero (Al-
gorithm 1, line 6). This post-processing step is
performed to nullify insignificant similarity values
propagated to a document. Additionally, if there
exists similarity of documents that is higher than
one, it is set to one.
874
Algorithm 2 Recursive Propagation
Input: Affinity matrix A, level of propagation S,
?, number of nearest neighbors k, document v
i
and v
j
Output: Propagated affinity matrix
1: if S = 0 then
2: return
3: else
4: for all y
x
? k-NN document v
i
do
5: A
v
j
y
x
? A
v
j
y
x
+ ?A
v
i
y
x
6: A
y
x
v
j
? A
v
j
y
x
7: Recursive Propagation (A,S ? 1,
?
2
, k, y
x
, v
j
)
8: end for
9: for all z
x
? k-NN document v
j
do
10: Set A
v
i
z
x
? A
v
i
z
x
+ ?A
v
j
z
x
11: Set A
z
x
v
i
? A
v
i
z
x
12: Recursive Propagation (A,S ? 1,
?
2
, k, v
i
, z
x
)
13: end for
14: end if
4 Performance Evaluation
The goals of empirical evaluation include (1) test-
ing whether the propagation method can merge
multilingual space and produce acceptable clus-
tering results; (2) comparing the performance to
spectral clustering method without propagation.
4.1 Data Description
We tested our model using Reuters Corpus Vol-
ume 2 (RCV2), a multilingual corpus contain-
ing news in thirteen different languages. For our
experiment, three different languages: English,
French, and Spanish; in six different topics: sci-
ence, sports, disasters accidents, religion, health,
and economy are used. We discarded documents
with multiple category labels.
We do not apply any language specific pre-
processing method to the raw text data. Mono-
lingual TFIDF is used for feature weighting. All
document vectors are then converted into unit vec-
tor by dividing by its length. Table 1 shows the
average length of documents in our corpus.
4.2 Evaluation Metric
For our experiment, we used Rand Index (RI)
which is a common evaluation technique for clus-
tering task where the true class of unlabeled data
English French Spanish Total
Science 290.10 165.10 213.45 222.88
Sports 182.55 156.83 189.75 176.37
Disasters 154.29 175.89 165.31 165.16
Religion 317.77 177.91 242.67 246.11
Health 251.19 233.70 227.25 237.38
Economy 266.89 192.55 306.11 255.08
Total 243.79 183.61 224.09 217.16
Table 1: Average number of words of documents
in the corpus. Each language consists of 600 doc-
uments, and each topic consists of 100 documents
(per language).
is known. Rand Index measures the percentage of
decisions that are correct, or simply the accuracy
of the model. Rand Index is defined as:
RI =
TP + TN
TP + FP + TN + FN
Rand Index penalizes false positive and false neg-
ative decisions during clustering. It takes into ac-
count decision that assign two similar documents
to one cluster (TP), two dissimilar documents to
different clusters (TN), two similar documents to
different clusters (FN), and two dissimilar docu-
ments to one cluster (FP). We do not include links
created by supervisory information when calculat-
ing true positive decisions and only consider the
number of free decisions made.
We also used F
?
-measure, the weighted har-
monic mean of precision (P) and recall (R). F
?
-
measure is defined as:
F
?
=
(?
2
+ 1)PR
?
2
P +R
P =
TP
TP + FP
R =
TP
TP + FN
Last, we used purity to evaluate the accuracy of
assignments. Purity is defined as:
Purity =
1
N
?
t
max
j
|?
t
? c
j
|
whereN is the number of documents, t is the num-
ber of clusters, j is the number of classes, ?
t
and
c
j
are sets of documents in cluster t and class j
respectively.
875
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
an
d 
In
de
x
Proportion of supervisory information
With propagation
Without propagation
LSA
(a) Rand Index for 6 topics
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
an
d 
In
de
x
Proportion of supervisory information
With propagation
Without propagation
LSA
(b) Rand Index for 4 topics
Figure 2: Rand Index on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4
topics as the proportion of supervisory information increases. k = 30, ? = 0.03, ? = 0.5, t = number of
topics, and S = 2.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
ur
ity
Proportion of supervisory information
With propagation
Without propagation
LSA
(a) Purity for 6 topics
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
ur
ity
Proportion of supervisory information
With propagation
Without propagation
LSA
(b) Purity for 4 topics
Figure 3: Purity on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4 topics
as the proportion of supervisory information increases. k = 30, ? = 0.03, ? = 0.5, t = number of topics,
and S = 2.
4.3 Experimental Results
To prove the effectiveness of our clustering algo-
rithm, we performed the following experiments on
our data set. We first tested our algorithm on four
topics, science, sports, religion, and economy. We
then tested our algorithm using all six topics to
get an understanding of the performance of our
model in larger collections with more topics. We
used subset of our data as supervisory informa-
tion and built must-linked constraints from it. The
proportion of supervisory information provided to
the system is given in x-axis (Figure 2 - Figure
4.3). 0.2 here means 20% of documents in each
language are taken to be used as prior knowledge.
Since the number of documents in each language
for our experiment is the same, we have the same
numbers of documents in subset of English col-
lection, subset of French collection, and subset of
Spanish collection. We also ensure there are same
numbers of documents for a particular topic in all
three languages. We can build must-linked con-
straints as follows. For each document in the sub-
set of English collection, we create must-linked
constraints with one randomly selected document
from the subset of French collection and one ran-
domly selected document from the subset of Span-
ish collection that belong to the same topic with it.
We then create must-linked constraint between the
respective French and Spanish documents. The
constraints given to the algorithm are chosen so
that there are several links that connect every topic
in every language. Note that the class label in-
876
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
F
2-
m
e
a
s
u
re
Proportion of supervisory information
With propagation
Without propagation
LSA
(a) F
2
-measure for 6 topics
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
F
2-
m
e
a
s
u
re
Proportion of supervisory information
With propagation
Without propagation
LSA
(b) F
2
-measure for 4 topics
Figure 4: F
2
-measure on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4
topics as the proportion of supervisory information increases. k = 30, ? = 0.03, ? = 0.5, t = number of
topics, and S = 2.
formation is only used to build must-linked con-
straints between documents, and we do not assign
the documents to a particular cluster.
Figure 2 shows the Rand Index as proportion
of supervisory information increases. Figure 3
and Figure 4.3 give purity and F
2
-measure for
the algorithm respectively. To show the impor-
tance of the propagation in multilingual space, we
give comparison with spectral clustering model
without propagation. Three lines in Figure 2 to
Figure 4.3 indicate: (1) results with propagation
(solid line); (2) results without propagation (long-
dashed line); and (3) results using Latent Se-
mantic Analysis(LSA)-based method by exploit-
ing common words between languages (short-
dashed line). For each figure, 6 plots are taken
starting from 0 in 0.2-point-increments. We con-
ducted the experiments three times for each pro-
portion of supervisory information and use the av-
erage values. As we can see from Figure 2, Fig-
ure 3, and Figure 4.3, the propagation method can
significantly improve the performance of spectral
clustering algorithm. For 1800 documents in 6
topics, we manage to achieve RI = 0.91, purity
= 0.84, and F
2
-measure = 0.76 with only 20% of
documents (360 documents) used as supervisory
information. Spectral clustering algorithm with-
out propagation can only achieve 0.69, 0.30, 0.28
for RI, purity, and F
2
-measure respectively. The
propagation method is highly effective when only
small amount of supervisory information given to
the algorithm. Obviously, the more supervisory in-
formation given, the better the performance is. As
the number of supervisory information increases,
the difference of the model performance with and
without propagation becomes smaller. This is
because there are already enough links between
multilingual documents, so we do not necessar-
ily build more links through similarity propagation
anymore. However, even when there are already
many links, our model with propagation still out-
performs the model without propagation.
We compare the performance of our algorithm
to LSA-based multilingual document clustering
model. We performed LSA to the multilingual
term by document matrix. We do not use paral-
lel texts and only rely on common words across
languages as well as must-linked constraints to
build multilingual space. The results show that ex-
ploiting common words between languages alone
is not enough to build a good multilingual se-
mantic space, justifying the usage of supervisory
information in multilingual document clustering
task. When supervisory information is introduced,
our method achieves better results than LSA-based
method. In general, the LSA-based method per-
forms better than the model without propagation.
We assess the sensitivity of our algorithm to
parameter ?, the penalty for similarity propaga-
tion. We assess the sensitivity of our algorithm
to parameter ?, the penalty for similarity prop-
agation. We tested our algorithm using various
?, starting from 0 to 1 in 0.2-point-increments,
while other parameters being held constant. Fig-
ure 5(a) shows that changing ? to some extent af-
fects the performance of the algorithm. However,
after some value of reasonable ? is found, increas-
ing ? does not have significant impact on the per-
877
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
an
d 
In
de
x
?
(a) Changing ?, k = 30, t = 6
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100
R
an
d 
In
de
x
k
(b) Changing k, ? = 0.5, t = 6
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5  10  15  20
R
an
d 
In
de
x
t
(c) Changing t, ? = 0.5, k = 30
Figure 5: Rand Index on the RCV2 task with 1800 documents and 6 topics as (a) ? increases; (b)
k increases; and (c) t increases. ? = 0.03, S = 2, and 20% of documents are used as supervisory
information.
formance of the algorithm. We also tested our al-
gorithm using various k, starting from 0 to 100
in 20-point-increments. Figure 5(b) reveals that
the performances of the model with different k are
comparable, as long as k is not too small. How-
ever, using too large k will slightly decrease the
performance of the model. Too many propaga-
tions make several dissimilar documents receive
high similarity value that cannot be nullified by
the post-processing step. Last, we experimented
using various t ranging from 2 to 20. Figure 5(c)
shows that the method performs best when t = 10,
and for reasonable value of t the method achieves
comparable performance.
5 Conclusion
We present here a multilingual spectral cluster-
ing model that is able to work irrespective of the
languages being used. The key component of
our model is the propagation algorithm to merge
multilingual spaces. We tested our algorithm
on Reuters RCV2 Corpus and compared the per-
formance with spectral clustering model without
propagation. Experimental results reveal that us-
ing limited supervisory information, the algorithm
achieves encouraging clustering results.
References
Charu C. Aggarwal, Stephen C. Gates and Philip S.
Yu. 1999. On The Merits of Building Catego-
rization Systems by Supervised Clustering. In Pro-
ceedings of Conference on Knowledge Discovery in
Databases:352-356.
Hsin-Hsi Chen and Chuan-Jie Lin. 2000. A Mul-
tilingual News Summarizer. In Proceedings of
18th International Conference on Computational
Linguistics:159-165.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harsh-
man. 1990. Indexing by Latent Semantic Analy-
sis. Journal of the American Society of Information
Science:41(6):391-407.
Miroslav Fiedler. 1975. A Property of Eigenvectors of
Nonnegative Symmetric Matrices and its Applica-
tions to Graph Theory. Czechoslovak Mathematical
Journal, 25:619-672.
878
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage Text Categorization by acquiring Multilingual
Domain Models from Comparable Corpora. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts:9-16.
Sepandar D. Kamvar, Dan Klein, and Christopher D.
Manning. 2003. Spectral Learning. In Proceed-
ings of the International Joint Conference on Artifi-
cial Intelligence (IJCAI).
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In The Nineteenth In-
ternational Conference on Machine Learning.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-
based Retrieval using Language Models. In Pro-
ceedings of the 27th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval:186-193.
Marinla Meil?a and Jianbo Shi. 2000. Learning seg-
mentation by random walks. In Advances in Neural
Information Processing Systems:873-879.
Marinla Meil?a and Jianbo Shi. 2001. A Random Walks
View of Spectral Segmentation. In AI and Statistics
(AISTATS).
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss.
2002. On Spectral Clustering: Analysis and an al-
gorithm. In Proceedings of Advances in Neural In-
formation Processing Systems (NIPS 14).
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat,
Emilia K?asper, and Irina Temnikova. 2004. Mul-
tilingual and Cross-lingual News Topic Tracking. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Stefan Siersdorfer and Sergej Sizov. 2004. Restrictive
Clustering and Metaclustering for Self-Organizing
Document. In Proceedings of the 27th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval.
Kiri Wagstaff and Claire Cardie 2000. Clustering
with Instance-level Constraints. In Proceedings
of the 17th International Conference on Machine
Learning:1103-1110.
Chih-Ping Wei, Christopher C. Yang, and Chia-Min
Lin. 2008. A Latent Semantic Indexing Based Ap-
proach to Multilingual Document Clustering. In De-
cision Support Systems, 45(3):606-620
Dell Zhang and Robert Mao. 2008. Extracting Com-
munity Structure Features for Hypertext Classifi-
cation. In Proceedings of the 3rd IEEE Interna-
tional Conference on Digital Information Manage-
ment (ICDIM).
879
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594?604,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting a Scientific Community?s Response to an Article
Dani Yogatama Michael Heilman Brendan O?Connor Chris Dyer
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of predicting mea-
surable responses to scientific articles based
primarily on their text content. Specif-
ically, we consider papers in two fields
(economics and computational linguistics)
and make predictions about downloads and
within-community citations. Our approach is
based on generalized linear models, allowing
interpretability; a novel extension that cap-
tures first-order temporal effects is also pre-
sented. We demonstrate that text features
significantly improve accuracy of predictions
over metadata features like authors, topical
categories, and publication venues.
1 Introduction
Written communication is an essential component
of the complex social phenomenon of science. As
such, natural language processing is well-positioned
to provide tools for understanding the scientific pro-
cess, by analyzing the textual artifacts (papers, pro-
ceedings, etc.) that it produces. This paper is about
modeling collections of scientific documents to un-
derstand how their textual content relates to how a
scientific community responds to them. While past
work has often focused on citation structure (Borner
et al, 2003; Qazvinian and Radev, 2008), our em-
phasis is on the text content, following Ramage et
al. (2010) and Gerrish and Blei (2010).
Instead of task-independent exploratory data anal-
ysis (e.g., topic modeling) or multi-document sum-
marization, we consider supervised models of the
collective response of a scientific community to a
published article. There are many measures of im-
pact of a scientific paper; ours come from direct
measurements of the number of downloads (from
an established website where prominent economists
post papers before formal publication) and citations
(within a fixed scientific community). We adopt a
discriminative approach based on generalized lin-
ear models that can make use of any text or meta-
data features, and show that simple lexical fea-
tures offer substantial power in modeling out-of-
sample response and in forecasting response for fu-
ture articles. Realistic forecasting evaluations re-
quire methodological care beyond the usual best
practices of train/test separation, and we elucidate
these issues.
In addition, we introduce a new regularization
technique that leverages the intuition that the rela-
tionship between observable features and response
should evolve smoothly over time. This regularizer
allows the learner to rely more strongly on more re-
cent evidence, while taking into account a long his-
tory of training data. Our time series-inspired regu-
larizer is computationally efficient in learning and is
a significant advance over earlier text-driven fore-
casting models that ignore the time variable alto-
gether (Kogan et al, 2009; Joshi et al, 2010).
We evaluate our approaches in two novel experi-
mental settings: predicting downloads of economics
articles and predicting citation of papers at ACL
conferences. Our approaches substantially outper-
594
0
15
00
4 9
log(# downloads)
# d
oc
s.
0
25
00
0 18
# citations
# d
oc
s.
Figure 1: Left: the distribution of log download counts
for papers in the NBER dataset one year after post-
ing. Right: the distribution of within-dataset citations of
ACL papers within three years of publication (outliers ex-
cluded for readability).
form text-ignorant baselines on ground-truth predic-
tions. Our time series models permit flexibility in
features and offer a novel and perhaps more inter-
pretable view of the data than summary statistics.
2 Data
We make use of two collections of scientific litera-
ture, one from the economics domain, and the other
from computational linguistics and natural language
processing. Statistics are summarized in Table 1.
2.1 NBER
Our first dataset consists of research papers in eco-
nomics from the National Bureau of Economic
Research (NBER) from 1999 to 2009 (http://
www.nber.org). Approximately 1,000 research
economists are affiliated with the NBER. New
NBER working papers are posted to the website
weekly. The papers are not yet peer-reviewed, but
given the prominence of many economists affiliated
with the NBER, many of the papers are widely read.
Text from the abstracts of the papers and related
metadata are publicly available. Full text is available
to subscribers (universities typically have access).
The NBER provided us with download statistics
for these papers. For each paper, we computed
the total number of downloads in the first year af-
ter each paper?s posting.1 The download counts are
log-normally distributed, as shown in Figure 1, and
so our regression models (?3) minimize squared er-
rors in the log space. Our download logs begin in
1For the vast majority of papers, most of the downloads oc-
cur soon after the paper?s posting. We explored different mea-
sures with different download windows (two years, for exam-
ple) with broadly similar results. We leave a more detailed anal-
ysis of the time series patterns of downloads to future work.
Dataset # Docs. Avg. #
Words
Response
NBER 8,814 155 # downloads in first
year (mean 761)
ACL 4,026 3,966 at least 1 citation in
first 3 years? (54% no)
Table 1: Descriptive statistics about the datasets.
1999. We use the 8,814 papers from 1999?2009 pe-
riod (there are 16,334 papers in the full dataset dat-
ing back to 1985). We only use text from the ab-
stracts, since we were able to obtain full texts for
just a portion of the papers, and since the OCR of
the full texts we do have is very noisy.
2.2 ACL
Our second dataset consists of research papers
from the Association for Computational Linguis-
tics (ACL) from 1980 to 2006 (Radev et al, 2009a;
Radev et al, 2009b). We have the full texts for pa-
pers (OCR output) as well as structured citation data.
There are 15,689 papers in the whole dataset. For
the citation prediction task, we include conference
papers from ACL, EACL, HLT, and NAACL.2 We
remove journal papers, since they are characteristi-
cally different from conference papers, as well as
workshop papers. We do include short papers, in-
teractive demo session papers, and student research
papers that are included in the companion volumes
for these conferences (such papers are cited less than
full papers, but many are still cited). The resulting
dataset contains 4,026 papers. The number of pa-
pers in each year varies because not all conferences
are annual.
We look at citations in the three-year window fol-
lowing publication, excluding self-citations and only
considering citations from papers within these con-
ferences. Figure 1 shows a histogram; note that
many papers (54%) are not cited at all, and the dis-
tribution of citations per paper is neither normal nor
log-normal. We organize the papers into two classes:
those with zero citations and those with non-zero ci-
tations in the three-year window.
2EMNLP is a relatively recent conference, and, in this col-
lection, complete data for its papers postdate the end of the last
training period, so we chose to exclude it from our dataset.
595
3 Model
Our forecasting approach is based on generalized
linear models for regression and classification. The
models are trained with an `2-penalty, often called
a ?ridge? model (Hoerl and Kennard, 1970).3 For
the NBER data, where (log) number of downloads is
nearly a continuous measure, we use linear regres-
sion. For the ACL data, where response is the bi-
nary cited-or-not variable we use logistic regression,
often referred to as a ?maximum entropy? model
(Berger et al, 1996) or a log-linear model. We
briefly review the class of models. Then, we de-
scribe a time series model appropriate for time series
data.
3.1 Generalized Linear Models
Consider a model that predicts a response y given a
vector input x = ?x1, . . . , xd? ? Rd. Our models
are linear functions of x and parameterized by the
vector ?. Given a corpus of M document features,
X , and responses Y , we estimate:
?? = argmin? R(?) + L(?,X, Y ) (1)
where L is a model-dependent loss function and R
is a regularization penalty to encourage models with
small weight vectors. We describe models and loss
functions first and then turn to regularization.
For the NBER data, the (log) number of down-
loads is continuous, and so we use least-squares
linear regression model. The loss function is the
sum of the squared errors for the M documents in
our training data: L(?,X, Y ) = ?Mi=1(yi ? y?i)2,
where the prediction rule for new documents is:
y? =
?d
j=0 ?jxj . Probabilistically, this equates to an
assumption that ?>x is the mean of a normal (i.e.,
Gaussian) distribution from which random variable
y is drawn.
For the ACL data, we predict y from a discrete
set C (specifically, the binary set of zero citations or
more than zero citations), and we use logistic regres-
sion. This model assumes that for the ith training
input xi, the output yi is drawn according to:
p(yi | xi) =
(
exp?>c xi
) /(?
c??C exp?>c?xi
)
3Preliminary experiments found no consistent benefit from
`1 (?lasso?) models, though we note that `1-regularization leads
to sparse, compact models that may be more interpretable.
where there is a feature vector ?c for each class
c ? C. Under this interpretation, parameter esti-
mation is maximum a posteriori inference for ?,
and R(?) is a log-prior for the weights. The loss
function is the negative log likelihood for the M
documents: L(?,X, Y ) = ??Mi=1 log p(yi | xi).
The prediction rule for a new document is: y? =
argmaxc?C
?d
j=0 ?c,jxj . Generalized linear mod-
els and penalized regression are well-studied with
an extensive literature (Mccullagh and Nelder, 1989;
Hastie et al, 2009). We leave other types of mod-
els, such as Poisson (Cameron and Trivedi, 1998)
or ordinal (McCullagh, 1980) regression models, to
future work.
3.2 Ridge Regression
With large numbers of features, regularization is
crucial to avoid overfitting. In ridge regression (Ho-
erl and Kennard, 1970), a standard method to which
we compare the time series regularization discussed
in ?3.3, the penalty R(?) is proportional to the `2-
norm of ?:
R(?) = ????2 = ?
?
j ?2j
where ? is a regularization hyperparameter that is
tuned on development data or by cross-validation.4
This penalty pushes many ?j close (but not com-
pletely) to zero. In practice, we multiply the penalty
by the number of examples M to facilitate tuning of
?.
The ridge linear regression model can be inter-
preted probabilistically as each coefficient ?j is
drawn i.i.d. from a normal distribution with mean
0 and variance 2??1.
3.3 Time Series Regularization
A simple way to capture temporal variation is to con-
join traditional features with a time variable. Here,
we divide the dataset into T time steps (years). In the
new representation, the feature space expands from
Rd to RT?d. For a document published at year t, the
elements of x are non-zero only for those features
that correspond to year-t; that is xt?,j = 0 for all
t? 6= t.
4The linear regression has a bias ?0 that is always active.
The logistic regression also has an unpenalized bias ?c,0 for
each class c. This weight is not regularized.
596
Estimating this model with the new features using
the `2-penalty would be effectively estimating sepa-
rate models for each year under the assumption that
each ?t,j is independent; even for features that dif-
fered only temporally (e.g., ?t,j and ?t+1,j).
In this work, we apply time series regularization
to GLMs, enabling models that have coefficients that
change over time but prefer gradual changes across
time steps. Boyd and Vandenberghe (2004, ?6.3) de-
scribe a general version of this sort of regularizer.
To our knowledge, such regularizers have not previ-
ously been applied to temporal modeling of text.
The time series regularization penalty becomes:
R(?) = ?
T?
t=1
d?
j=1
?2t,j+??
T?
t=2
d?
j=0
(?t,j ? ?t?1,j)2
It includes a standard `2-penalty on the coefficients,
and a penalty for differences between coefficients
for adjacent time steps to induce smooth changes.5
Similar to the previous model, in practice, we mul-
tiply the regularization constant ? by MT to facili-tate tuning of ? for datasets with different numbers
of examples M and numbers of time steps T . The
new parameter, ?, controls the smoothness of the es-
timated coefficients. Setting ? to zero imposes no
penalty for time-variation in the coefficients and re-
sults in independent ridge regressions at each time
step. Also, when the number of examples is con-
stant across time steps, setting a large ? parameter
(? ? ?) results in a single ridge regression over all
years since it imposes ?t,j = ?t+1,j for all t ? T .
The partial derivative is:
?R/??t,j = 2??t,j
+ 1{t > 1}2??(?t,j ? ?t?1,j)
+ 1{t < T}2??(?t,j ? ?t+1,j)
This time series regularization can be applied more
generally, not just to linear and logistic regression.
With either ridge regularization or this time se-
ries regularization scheme, Eq. 1 is an unconstrained
convex optimization problem for the linear models
5Our implementation of the time series regularizer does not
penalize the magnitude of the weight for the bias feature (as in
ridge regression). It does, however, penalize the difference in
the bias weight between time steps (as with other features).
?
1
?
2
?
3
?
T
Y
1
Y
2
Y
3
Y
T
...
X
1
X
2
X
3
X
T
?,?
Figure 2: Time series regression as a graphical model;
the variables Xt and Yt are the sets of feature vectors
and response variables from documents dated t.
we describe here. There exist a number of optimiza-
tion procedures for it; we use the L-BFGS quasi-
Newton algorithm (Liu and Nocedal, 1989).
Probabilistic Interpretation
We can interpret the time series regularization prob-
abilistically as follows. Let the coefficient for the
jth feature over time be ?j = ??1,j , ?2,j , ..., ?T,j?.
?j are draws from a multivariate normal distribu-
tion with a tridiagonal precision matrix ??1 = ? ?
RT?T :
? = ?
?
??????
1 + ? ?? 0 0 . . .
?? 1 + 2? ?? 0 . . .
0 ?? 1 + 2? ?? . . .
0 0 ?? 1 + 2? . . .
... ... ... ... . . .
?
??????
The form of R(?) follows from noting:
?2 log p(?j ;?, ?) = ?>j ??j + constant
The squared difference between adjacent time steps
comes from the off-diagonal entries in the preci-
sion matrix.6 Figure 2 shows a graphical represen-
tation of the time series regularization in our model.
Its Markov chain structure corresponds to the off-
diagonals.
There is a rich literature on time series analysis
(Box et al, 2008; Hamilton, 1994). The prior dis-
tribution over the sequence ??1,j , . . . , ?T,j? that our
regularizer posits is closely linked to a first-order au-
toregressive process, AR(1).
6Consistent with the previous section, we assume that pa-
rameters for different features, ?j and ?k, are independent.
597
NBER ACL
Response log(#downloads+1) 1{#citations > 0}
GLM type normal / squared-loss logistic / log-loss
Metric 1 mean absolute error accuracy
Metric 2 Kendall?s ? Kendall?s ?
Table 2: Summary of the setup for the NBER download
and ACL citation prediction experiments.
4 Features
NBER metadata features
? Authors? last names. We treat each name as a bi-
nary feature. If a paper has multiple authors, all
authors are used and they have equal weights re-
gardless of their ordering.
? NBER program(s).7 There are 19 major re-
search programs at the NBER (e.g., Monetary
Economics, Health Economics, etc.).
ACL metadata features
? Authors? last names as binary features.
? Conference venues. We use first letter of the ACL
anthology paper ID, which correlates with its con-
ference venue (e.g., P for the ACL main confer-
ence, H for the HLT conference, etc.).8
Text features
? Binary indicator features for the presence of each
unigram, bigram, and trigram. For the NBER
data, we have separate features for titles and ab-
stracts. For the ACL data, we have separate fea-
tures for titles and full texts. We pruned text fea-
tures by document frequency (details in ?5).
? Log transformed word counts. We include fea-
tures for the numbers of words in the title and the
abstract (NBER) or the full text (ACL).
7Almost all NBER papers are tagged with one or more pro-
grams (we assign untagged papers a ?null? tag). The complete
list of NBER programs can be found at http://www.nber.
org/programs
8Papers in the ACL dataset have a tag which shows which
workshop, conference, or journal they appeared in. However,
sometimes a conference is jointly held with another confer-
ence, such that meta information in the dataset is different even
though the conference is the same. For this reason, we simply
use the first letter of the paper ID.
5 Experiments
For each of the datasets in ?2, we test our models
for two tasks: forecasting about future papers (i.e.,
making predictions about papers that appeared af-
ter a training dataset) and modeling held-out papers
from the past (i.e., making predictions within the
same time period as the training dataset, on held-out
examples).
For the NBER dataset, the task is to predict the
number of downloads a paper will receive in its first
year after publication. For the ACL dataset, the task
is to predict whether a paper will be cited at all (by
another ACL paper in our dataset) within the first
three years after its publication. To our knowledge,
clean, reliable citation counts are not available for
the NBER dataset; nor are download statistics avail-
able for the ACL dataset. Table 2 summarizes the
variables of interest, model types, and evaluation
metrics for the tasks.
5.1 Extrapolation
The lag between a paper?s publication and when its
outcome (download or citation count) can be ob-
served poses a unique methodological challenge.
Consider predicting the number of downloads over
g future time steps. If t is the time of forecasting,
we can observe the texts of all articles published be-
fore t. However, any article published in the interval
[t ? g, t] is too recent for the outcome measurement
of y to be taken. We refer to the interval [t? g, t] as
the ?forecast gap?. Since recent articles are some-
times the most relevant predictions at t, we do not
want to ignore them. Consider a paper at time step
t?, t?g < t? < t. To extrapolate its number of down-
loads, we consider the observed number in [t?, t], and
then estimate the ratio r of downloads that occur in
the first t?t? time steps, against the first g time steps,
using the fully observed portion of the training data.
We then scale the observed downloads during [t?, t]
by r?1 to extrapolate. The same method is used to
extrapolate citation counts.
In preliminary experiments, we observed that ex-
trapolating responses for papers in the forecast gap
led to better performance in general. For example,
for the ridge regressions trained on all past years
with the full feature set, the error dropped from 262
to 259 when using extrapolation compared to with-
598
out extrapolation. Also, the extrapolated download
counts were quite close to the true values (which we
have but do not use because of the forecast gap): for
example, the mean absolute error of the extrapolated
responses was 99 when extrapolated based on the
median of the fully observed portion of the training
data (measured monthly).
5.2 Forecasting NBER Downloads
In our first set of experiments, we predict the number
of downloads of an NBER paper within one year of
its publication.
We compare four approaches for predicting
downloads. The first is a baseline that simply uses
the median of the log of the training and develop-
ment data as the prediction. The second and third
use GLMs with ridge regression-style regularization
(?3.2), trained on all past years (?all years?) and on
the single most recent past year (?one year?), respec-
tively. The last model (?time series?) is a GLM with
time series regularization (?3.3).
We divided papers by year. Figure 3 illustrates the
experimental setup. We held out a random 20% of
papers for each year from 1999?2007 as a test set for
the task of modeling the past. To define the feature
set and tune hyperparameters, we used the remain-
ing 80% of papers from 1999?2005 as our training
data and the remaining papers in 2006 as our devel-
opment data. After pruning,9 we have 37,251 to-
tal features, of which 2,549 are metadata features.
When tuning hyperparameters, we simulated the ex-
istence of a forecast gap by using extrapolated re-
sponses for papers in the last year of the training
data instead of their true responses. We considered
? ? 5{2,1,...,?5,?6}, and ? ? 5{3,2,...,?1,?2} and se-
lected those that led to the best performance on the
development set.
We then used the selected feature set and hyperpa-
rameters to test the forecasting and modeling capa-
bilities of each model. For forecasting, we predicted
numbers of downloads of papers in 2008 and 2009.
We used the baseline median, ridge regression, and
time series regularization models trained on papers
in 1999?2007 and 1999?2008, respectively. We
treated the last year of the training data (2007 and
9For NBER, text features appearing in less than 0.1% or
more than 99.9% of the training documents were removed. For
ACL, the thresholds were 2% and 98%.
training
modeling test (unused)
gap dev.
trr taitan tag
training gap test
trr tai tam tao
ddd
ddd
modeling test (unused)
oae
lae
oae
lae
training gap test
trr tamddd
modeling test
oae
lae
tao tar
NBER Experiments
ACL Experiments
training
modeling test (unused)
dev.
toa tro ta ddd
oae
lae
s(u)p)v.(uu)p)vProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 685?693,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Probabilistic Model for Canonicalizing Named Entity Mentions
Dani Yogatama Yanchuan Sim Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,ysim,nasmith}@cs.cmu.edu
Abstract
We present a statistical model for canonicalizing
named entity mentions into a table whose rows rep-
resent entities and whose columns are attributes (or
parts of attributes). The model is novel in that it
incorporates entity context, surface features, first-
order dependencies among attribute-parts, and a no-
tion of noise. Transductive learning from a few
seeds and a collection of mention tokens combines
Bayesian inference and conditional estimation. We
evaluate our model and its components on two
datasets collected from political blogs and sports
news, finding that it outperforms a simple agglom-
erative clustering approach and previous work.
1 Introduction
Proper handling of mentions in text of real-world
entities?identifying and resolving them?is a cen-
tral part of many NLP applications. We seek an al-
gorithm that infers a set of real-world entities from
mentions in a text, mapping each entity mention to-
ken to an entity, and discovers general categories of
words used in names (e.g., titles and last names).
Here, we use a probabilistic model to infer a struc-
tured representation of canonical forms of entity at-
tributes through transductive learning from named
entity mentions with a small number of seeds (see
Table 1). The input is a collection of mentions found
by a named entity recognizer, along with their con-
texts, and, following Eisenstein et al (2011), the
output is a table in which entities are rows (the num-
ber of which is not pre-specified) and attribute words
are organized into columns.
This paper contributes a model that builds on the
approach of Eisenstein et al (2011), but also:
? incorporates context of the mention to help with
disambiguation and to allow mentions that do not
share words to be merged liberally;
? conditions against shape features, which improve
the assignment of words to columns;
? is designed to explicitly handle some noise; and
? is learned using elements of Bayesian inference
with conditional estimation (see ?2).
We experiment with variations of our model,
comparing it to a baseline clustering method and the
model of Eisenstein et al (2011), on two datasets,
demonstrating improved performance over both at
recovering a gold standard table. In a political
blogs dataset, the mentions refer to political fig-
ures in the United States (e.g., Mrs. Obama and
Michelle Obama). As a result, the model discov-
ers parts of names??Mrs., Michelle, Obama??
while simultaneously performing coreference res-
olution for named entity mentions. In the sports
news dataset, the model is provided with named en-
tity mentions of heterogenous types, and success
here consists of identifying the correct team for ev-
ery player (e.g., Kobe Bryant and Los Angeles Lak-
ers). In this scenario, given a few seed examples,
the model begins to identify simple relations among
named entities (in addition to discovering attribute
structures), since attributes are expressed as named
entities across multiple mentions. We believe this
adaptability is important, as the salience of different
kinds of names and their usages vary considerably
across domains.
Bill Clinton Mr.
George Bush Mr. W.
Barack Obama Sen. Hussein
Hillary Clinton Mrs. Sen.
Bristol Palin Ms.
Emil Jones Jr.
Kay Hutchison Bailey
Ben Roethlisberger Steelers
Bryant Los Angeles
Derek Jeter New York
Table 1: Seeds for politics (above) and sports (below).
685
 x ?
 
1
1
f
w c
r
s
?
 
?
?
M
L
T
?
C
Figure 1: Graphical representation of our model. Top,
the generation of the table: C is the number of at-
tributes/columns, the number of rows is infinite, ? is a
vector of concentration parameters, ? is a multinomial
distribution over strings, and x is a word in a table cell.
Lower left, for choosing entities to be mentioned: ? deter-
mines the stick lengths and ? is the distribution over en-
tities to be selected for mention. Middle right, for choos-
ing attributes to use in a mention: f is the feature vector,
and ? is the weight vector drawn from a Laplace distri-
bution with mean zero and variance ?. Center, for gen-
erating mentions: M is the number of mentions in the
data, w is a word token set from an entity/row r and at-
tribute/column c. Lower right, for generating contexts: s
is a context word, drawn from a multinomial distribution
? with a Dirichlet prior ?. Variables that are known or
fixed are shaded; variables that are optimized are double
circled. Others are latent; dashed lines imply collapsing.
2 Model
We begin by assuming as input a set of mention to-
kens, each one or more words. In our experiments
these are obtained by running a named entity recog-
nizer. The output is a table in which rows are un-
derstood to correspond to entities (types, not men-
tion tokens) and columns are fields, each associated
with an attribute or a part of it. Our approach is
based on a probabilistic graphical model that gener-
ates the mentions, which are observed, and the table,
which is mostly unobserved, similar to Eisenstein et
al. (2011). Our learning procedure is a hybrid of
Bayesian inference and conditional estimation. The
generative story, depicted in Figure 1, is:
? For each column j ? {1, . . . , C}:
? Draw a multinomial distribution ?j over the
vocabulary from a Dirichlet process: ?j ?
DP(?j , G0). This is the lexicon for field j.
? Generate table entries. For each row i (of which
there are infinitely many), draw an entry xi,j
for cell i, j from ?j . A few of these entries (the
seeds) are observed; we denote those x?.
? Draw weights ?j that associate shape and po-
sitional features with columns from a 0-mean,
?-variance Laplace distribution.
? Generate the distribution over entities to be men-
tioned in general text: ? ? GEM(?) (?stick-
breaking? distribution).
? Generate context distributions. For each row r:
? Draw a multinomial over the context vocabu-
lary (distinct from mention vocabulary) from a
Dirichlet distribution, ?r ? Dir(?).
? For each mention token m:
? Draw an entity/row r ? ?.
? For each word in the mention w, given some of
its features f (assumed observed):
. Choose a column c ? 1Z exp(?
>
c f). This
uses a log-linear distribution with partition
function Z. In one variation of our model,
first-order dependencies among the columns
are enabled; these introduce a dynamic char-
acter to the graphical model that is not shown
in Figure 1.
. With probability 1 ? , set the text wm` to
be xrc. Otherwise, generate any word from a
unigram-noise distribution.
? Generate mention context. For each of the T =
10 context positions (five before and five after
the mention), draw the word s from ?r.
Our choices of prior distributions reflect our be-
liefs about the shapes of the various distributions.
We expect field lexicons ?j and the distributions
over mentioned entities ? to be ?Zipfian? and so use
tools from nonparametric statistics to model them.
We expect column-feature weights ? to be mostly
zero, so a sparsity-inducing Laplace prior is used
(Tibshirani, 1996).
Our goal is to maximize the conditional likeli-
hood of most of the evidence (mentions, contexts,
and seeds), p(w, s, x? | ?,?, ?, ?, ?, ,f) =
?
r
?
c
?
x\x?
?
d?
?
d?
?
d?
p(w, s, r, c, x, ?, ?, ? | ?,?, ?, ?, ?, ,f)
686
with respect to ? and ? . We fix ? (see ?3.3 for the
values of ? for each dataset), ? = 2 (equivalent to
add-one smoothing), ? = 2 ? 10?8,  = 10?10,
and each mention word?s f . Fixing ?, ?, and ? is
essentially just ?being Bayesian,? or fixing a hyper-
parameter based on prior beliefs. Fixing f is quite
different; it is conditioning our model on some ob-
servable features of the data, in this case word shape
features. We do this to avoid integrating over fea-
ture vector values. These choices highlight that the
design of a probabilistic model can draw from both
Bayesian and discriminative tools. Observing some
of x as seeds (x?) renders this approach transductive.
Exact inference in this model is intractable, so we
resort to an approximate inference technique based
on Markov Chain Monte Carlo simulation. The opti-
mization of ? can be described as ?contrastive? esti-
mation (Smith and Eisner, 2005), in which some as-
pects of the data are conditioned against for compu-
tational convenience. The optimization of ? can be
described as ?empirical Bayesian? estimation (Mor-
ris, 1983) in which the parameters of a prior are
fit to data. Our overall learning procedure is a
Monte Carlo Expectation Maximization algorithm
(Wei and Tanner, 1990).
3 Learning and Inference
Our learning procedure is an iterative algorithm con-
sisting of two steps. In the E-step, we perform col-
lapsed Gibbs sampling to obtain distributions over
row and column indices for every mention, given the
current value of the hyperparamaters. In the M-step,
we obtain estimates for the hyperparameters, given
the current posterior distributions.
3.1 E-step
For the mth mention, we sample row index r, then
for each word wm`, we sample column index c.
3.1.1 Sampling Rows
Similar to Eisenstein et al (2011), when we sam-
ple the row for a mention, we use Bayes? rule and
marginalize the columns. We further incorporate
context information and a notion of noise.
p(rm = r | . . .) ? p(rm = r | r?m, ?)
(
?
`
?
c p(wm` | x, rm = r, cm` = c))
(
?
t p(smt | rm = r))
We consider each quantity in turn.
Prior. The probability of drawing a row index fol-
lows a stick breaking distribution. This allows us
to have an unbounded number of rows and let the
model infer the optimal value from data. A standard
marginalization of ? gives us:
p(rm = r | r?m, ?) =
{
N?mr
N+? if N
?m
r > 0
?
N+? otherwise,
where N is the number of mentions, Nr is the num-
ber of mentions assigned to row r, and N?mr is the
number of mentions assigned to row r, excludingm.
Mention likelihood. In order to compute the likeli-
hood of observing mentions in the dataset, we have
to consider a few cases. If a cell in a table has al-
ready generated a word, it can only generate that
word. This hard constraint was a key factor in the
inference algorithm of Eisenstein et al (2011); we
speculate that softening it may reduce MCMC mix-
ing time, so introduce a notion of noise. With proba-
bility  = 10?10, the cell can generate any word. If a
cell has not generated any word, its probability still
depends on other elements of the table. With base
distribution G0,1 and marginalizing ?, we have:
p(wm` | x, rm = r, cm` = c, ?c) = (1)
?
????
????
1?  if xrc = wm`
 if xrc 6? {wm`,?}
N?m`cw
N?m`c +?c
if xrc = ? and Ncw > 0
G0(wm`)
?c
N?m`c +?c
if xrc = ? and Ncw = 0
where N?m`c is the number of cells in column c that
are not empty and N?m`cw is the number of cells in
column c that are set to the word wm`; both counts
excluding the current word under consideration.
Context likelihood. It is important to be able to
use context information to determine which row
a mention should go into. As a novel extension,
our model also uses surrounding words of a men-
tion as its ?context??similar context words can en-
courage two mentions that do not share any words
to be merged. We choose a Dirichlet-multinomial
distribution for our context distribution. For every
row in the table, we have a multinomial distribution
over context vocabulary ?r from a Dirichlet prior ?.
1We let G0 be a uniform distribution over the vocabulary.
687
Therefore, the probability of observing the tth con-
text word for mention m is p(smt | rm = r, ?)
=
{
N?mtrs +?s?1
N?mtr +
P
v ?v?V
if N?mtr > 0
?s?1P
v ?v?V
otherwise,
whereN?mtr is the number of context words of men-
tions assigned to row r, N?mtrs is the number of con-
text words of mentions assigned to row r that are
smt, both excluding the current context word, and v
ranges over the context vocabulary of size V .
3.1.2 Sampling Columns
Our column sampling procedure is novel to this
work and substantially differs from that of Eisen-
stein et al (2011). First, we note that when we sam-
ple column indices for each word in a mention, the
row index for the mention r has already been sam-
pled. Also, our model has interdependencies among
column indices of a mention.2 Standard Gibbs sam-
pling procedure breaks down these dependencies.
For faster mixing, we experiment with first-order
dependencies between columns when sampling col-
umn indices. This idea was suggested by Eisenstein
et al (2011, footnote 1) as a way to learn structure
in name conventions. We suppressed this aspect of
the model in Figure 1 for clarity.
We sample the column index c1 for the first word
in the mention, marginalizing out probabilities of
other words in the mention. After we sample the
column index for the first word, we sample the col-
umn index c2 for the second word, fixing the pre-
vious word to be in column c1, and marginalizing
out probabilities of c3, . . . , cL as before. We repeat
the above procedure until we reach the last word
in the mention. In practice, this can be done effi-
ciently using backward probabilities computed via
dynamic programming. This kind of blocked Gibbs
sampling was proposed by Jensen et al (1995) and
used in NLP by Mochihashi et al (2009). We have:
p(cm` = c | . . .) ?
p(cm` = c | fm`, ?)p(cm` = c | cm`? = c?)
(?
c+ pb(cm` = c | cm`+ = c+)
)
p(wm` | x, rm = r, cm` = c, ?c),
2As shown in Figure 1, column indices in a mention form
?v-structures? with the row index r. Since everyw` is observed,
there is an active path that goes through all these nodes.
where `? is the preceding word and c? is its sam-
pled index, `+ is the following word and c+ is its
possible index, and pb(?) are backward probabilities.
Alternatively, we can perform standard Gibbs sam-
pling and drop the dependencies between columns,
which makes the model rely more heavily on the fea-
tures. For completeness, we detail the computations.
Featurized log linear distribution. Our model can
use arbitrary features to choose a column index.
These features are incorporated as a log-linear dis-
tribution, p(cm` = c | fm`,?) =
exp(?>c fm`)P
c? exp(?
>
c?
fm`)
.
The list of features used in our experiments is:
1{w is the first word in the mention}; 1{w ends
with a period}; 1{w is the last word in the men-
tion}; 1{w is a Roman numeral}; 1{w starts with
an upper-case letter}; 1{w is an Arabic number};
1{w ? {mr,mrs,ms,miss, dr,mdm} }; 1{w con-
tains ? 1 punctuation symbol}; 1{w ? {jr, sr}};
1{w ? {is, in, of, for}}; 1{w is a person entity};
1{w is an organization entity}.
Forward and backward probabilities. Since
we introduce first-order dependencies between
columns, we have forward and backward probabili-
ties, as in HMMs. However, we always sample from
left to right, so we do not need to marginalize ran-
dom variables to the left of the current variable be-
cause their values are already sampled. Our transi-
tion probabilities are as follows:
p(cm` = c | cm`? = c?) =
N?mc?,c
P
c??
N?m
c??,c
,
whereN?mc?,c is the number of times we observe tran-
sitions from column c? to c, excluding mention m.
The forward and backward equations are simple (we
omit them for space).
Mention likelihood. Mention likelihood p(wm` |
x, rm = r, cm` = c, ?c) is identical to when we
sample the row index (Eq. 1).
3.2 M-step
In the M-step, we use gradient-based optimization
routines, L-BFGS (Liu and Nocedal, 1989) and
OWL-QN (Andrew and Gao, 2007) respectively, to
maximize with respect to ? and ?.
688
3.3 Implementation Details
We ran Gibbs sampling for 500 iterations,3 discard-
ing the first 200 for burn-in and averaging counts
over every 10th sample to reduce autocorrelation.
For each word in a mention w, we introduced 12
binary features f for our featurized log-linear distri-
bution (?3.1.2).
We then downcased all words in mentions for the
purpose of defining the table and the mention words
w. Ten context words (5 each to the left and right)
define s for each mention token.
For non-convex optimization problems like ours,
initialization is important. To guide the model to
reach a good local optimum without many restarts,
we manually initialized feature weights and put a
prior on transition probabilities to reflect phenom-
ena observed in the initial seeds. The initializer was
constructed once and not tuned across experiments.4
The M-step was performed every 50 Gibbs sampling
iterations. After inference, we filled each cell with
the word that occurred at least 80% of the time in the
averaged counts for the cell, if such a word existed.
4 Experiments
We compare several variations of our model to
Eisenstein et al (2011) (the authors provided their
implementation to us) and a clustering baseline.
4.1 Datasets
We collected named entity mentions from two cor-
pora: political blogs and sports news. The political
blogs corpus is a collection of blog posts about poli-
tics in the United States (Eisenstein and Xing, 2010),
and the sports news corpus contains news summaries
of major league sports games (National Basketball
3On our moderate-sized datasets (see ?4.1), each iteration
takes approximately three minutes on a 2.2GHz CPU.
4For the politics dataset, we set C = 6, ? =
?1.0, 1.0, 10?12, 10?15, 10?12, 10?8?, initialized ? = 1, and
used a Dirichlet prior on transition counts such that before ob-
serving any data: N0,1 = 10, N0,5 = 5, N2,0 = 10, N2,1 =
10, N2,3 = 10, N2,4 = 5, N3,0 = 10, N3,1 = 10, N5,1 = 15
(others are set to zero). For the sports dataset, we set C = 5,
? = ?1.0, 1.0, 10?15, 10?6, 10?6?, initialized ? = 1, and
used a Dirichlet prior on transition counts N0,1 = 10, N2,3 =
20, N3,4 = 10 (others are set to zero). We also manually initial-
ized the weights of some features? for both datasets. These val-
ues were obtained from preliminary experiments on a smaller
sample of the datasets, and updated on the first EM iteration.
Politics Sports
# source documents 3,000 700
# mentions 10,647 13,813
# unique mentions 528 884
size of mention vocabulary 666 1,177
size of context vocabulary 2,934 2,844
Table 2: Descriptive statistics about the datasets.
Association, National Football League, and Major
League Baseball) in 2009. Due to the large size of
the corpora, we uniformly sampled a subset of doc-
uments for each corpus and ran the Stanford NER
tagger (Finkel et al, 2005), which tagged named en-
tities mentions as person, location, and organization.
We used named entity of type person from the po-
litical blogs corpus, while we are interested in per-
son and organization entities for the sports news cor-
pus. Mentions that appear less than five times are
discarded. Table 2 summarizes statistics for both
datasets of named entity mentions.
Reference tables. We use Eisenstein et al?s man-
ually built 125-entity (282 vocabulary items) refer-
ence table for the politics dataset. Each entity in the
table is represented by the set of all tokens that app-
pear in its references, and the tokens are placed in its
correct column. For the sports data, we obtained a
roster of all NBA, NFL, and MLB players in 2009.
We built our sports reference table using the play-
ers? names, teams and locations, to get 3,642 play-
ers and 15,932 vocabulary items. The gold standard
table for the politics dataset is incomplete, whereas
it is complete for the sports dataset.
Seeds. Table 1 shows the seeds for both datasets.
4.2 Evaluation Scores
We propose both a row evaluation to determine
how well a model disambiguates entities and merges
mentions of the same entity and a column evaluation
to measure how well the model relates words used in
different mentions. Both scores are new for this task.
The first step in evaluation is to find a maximum
score bipartite matching between rows in the re-
sponse and reference table.5 Given the response and
5Treating each row as a set of words, we can optimize the
matching using the Jonker and Volgenant (1987) algorithm.
The column evaluation is identical, except that sets of words
that are matched are defined by columns. We use the Jaccard
similarity?for two sets A and B, |A?B||A?B|?for our similarity
function, Sim(i, j).
689
reference tables, xres and xref , we can compute:
Sres = 1|xres |
?
i?xres ,j?xref :Match(i,j)=1
Sim(i, j)
Sref = 1
|
xref |
?
i?xres ,j?xref :Match(i,j)=1
Sim(i, j)
where i and j denote rows, Match(i, j) is one if i and
j are matched to each other in the optimal matching
or zero otherwise. Sres is a precision-like score, and
Sref is a recall-like score.6 Column evaluation is the
same, but compares columns instead.
4.3 Baselines
Our simple baseline is an agglomerative clustering
algorithm that clusters mentions into entities using
the single-linkage criterion. Initially, each unique
mention forms its own cluster that we incremen-
tally merge together to form rows in the table. This
method requires a similarity score between two clus-
ters. For the politics dataset, we follow Eisenstein et
al. (2011) and use the string edit distance between
mention strings in each cluster to define the score.
For the sports dataset, since mentions contain per-
son and organization named entity types, our score
for clustering uses the Jaccard distance between con-
text words of the mentions. However, such cluster-
ings do not produce columns. Therefore, we first
match words in mentions of type person against
a regular expression to recognize entity attributes
from a fixed set of titles and suffixes, and the first,
middle and last names. We treat words in mentions
of type organization as a single attribute.7 As we
merge clusters together, we arrange words such that
6Eisenstein et al (2011) used precision and recall for their
similarity function. Precision prefers a more compact response
row (or column), which unfairly penalizes situations like those
of our sports dataset, where rows are heterogeneous (e.g., in-
cluding people and organizations). Consider a response ta-
ble made up of two rows: ?Kobe, Bryant? and ?Los, Ange-
les, Lakers?, and a reference table containing all NBA play-
ers and their team names, e.g., ?Kobe, Bryant, Los, Angeles,
Lakers?. Evaluating with the precision similarity function, we
will have perfect precision by matching the first row to the ref-
erence row for Kobe Bryant and the latter row to any Lakers
player. The system is not rewarded for merging the two rows
together, even if they are describing the same entity. Our eval-
uation scores, however, reward the system for accurately filling
in more cells.
7Note that the baseline system uses NER tags, list of titles
and suffixes; which are also provided to our model through the
features in ?3.1.2.
all words within a column belong to the same at-
tribute, creating columns as necessary to accomo-
date multiple similar attributes as a result of merg-
ing. We can evaluate the tables produced by each
step of the clustering to obtain the entire sequence
of response-reference scores.
As a strong baseline, we also compare our ap-
proach with the original implementation of the
model of Eisenstein et al (2011), denoted by EEA.
4.4 Results
For both the politics and sports dataset, we run the
procedure in ?3.3 three times and report the results.
Politics. The results for the politics dataset are
shown in Figure 2. Our model consistently outper-
formed both baselines. We also analyze how much
each of our four main extensions (shape features,
context information, noise, and first-order column
dependencies) to EEA contributes to overall per-
formance by ablating each in turn (also shown in
Fig. 2). The best-performing complete model has
415 rows, of which 113 were matched to the ref-
erence table. Shape features are useful: remov-
ing them was detrimental, and they work even bet-
ter without column dependencies. Indeed, the best
model did not have column dependencies. Remov-
ing context features had a strong negative effect,
though perhaps this could be overcome with a more
carefully tuned initializer.
In row evaluation, the baseline system can achieve
a high reference score by creating one entity row per
unique string, but as it merges strings, the clusters
encompass more word tokens, improving response
score at the expense of reference score. With fewer
clusters, there are fewer entities in the response ta-
ble for matching and the response score suffers. Al-
though we use the same seed table in both exper-
iments, the results from EEA are below the base-
line curve because it has the additional complexity
of inferring the number of columns from data. Our
model is simpler in this regard since it assumes that
the number of columns is known (C = 6). In col-
umn evaluation, our method without column depen-
dencies was best.
Sports. The results for the sports dataset are shown
in Figure 3. Our best-performing complete model?s
response table contains 599 rows, of which 561
were matched to the reference table. In row eval-
690
 0.2 0.2
1
 0.2
2
 0.2
3
 0.2
4
 0.2
5  0
.1 
0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
response score
ref
ere
nce
 sc
ore
 0.3 0.3
5 0.4
 0
 0.0
5 0.1 0.1
5 0.2 0.2
5 0.3
 0.1
 0.1
5
 0.2
 0.2
5
 0.3
 0.3
5
response score
ref
ere
nce
 sc
ore
bas
elin
e
EE
A
com
ple
te
-de
pen
den
cie
s
-no
ise
-co
nte
xt
-fe
atu
res
Figure 2: Row (left) and column (right) scores for the politics dataset. For all but ?baseline? (clustering), each point
denotes a unique sampling run. Note the change in scale in the left plot at y = 0.25. For the clustering baseline, points
correspond to iterations.
 0.2
5 0.3 0.3
5 0.4
 0
 0.0
2 
0.0
4 
0.0
6 
0.0
8
 0.1
response score
ref
ere
nce
 sc
ore
 0
 0.0
5 0.1 0.1
5 0.2 0.2
5  0
 0.0
5
 0.1
 0.1
5
 0.2
 0.2
5
response score
ref
ere
nce
 sc
ore
bas
elin
e
EE
A
com
ple
te
-de
pen
den
cie
s
-no
ise
-co
nte
xt
-fe
atu
res
Figure 3: Row (left) and column (right) scores for the sports dataset. Each point denotes a unique sampling run. The
reference score is low since the reference set includes all entities in the NBA, NFL, and MLB, but most of them were
not mentioned in our dataset.
uation, our model lies above the baseline response-
reference score curve, demonstrating its ability to
correctly identify and combine player mentions with
their team names. Similar to the previous dataset,
our model is also substantially better in column eval-
uation, indicating that it mapped mention words into
a coherent set of five columns.
4.5 Discussion
The two datasets illustrate that our model adapts to
somewhat different tasks, depending on its input.
Furthermore, fixing C (unlike EEA) does appear to
have benefits.
In the politics dataset, most of the mentions con-
tain information about people. Therefore, besides
canonicalizing named entities, the model also re-
solves within-document and cross-document coref-
erence, since it assigned a row index for every men-
tion. For example, our model learned that most men-
tions of John McCain, Sen. John McCain, Sen. Mc-
Cain, and Mr. McCain refer to the same entity. Ta-
ble 3 shows a few noteworthy entities from our com-
plete model?s output table.
Barack Obama Mr. Sen. Hussein
Michelle Obama Mrs.
Norm Coleman Sen.
Sarah Palin Ms.
John McCain Mr. Sen. Hussein
Table 3: A small segment of the output table for the poli-
tics dataset, showing a few noteworthy correct (blue) and
incorrect (red) examples. Black indicates seeds. Though
Ms. is technically correct, there is variation in prefer-
ences and conventions. Our data include eight instances
of Ms. Palin and none of Mrs. Palin or Mrs. Sarah
Palin.
The first entity is an easy example since it only
had to complete information provided in the seed ta-
ble. The model found the correct gender-specific ti-
tle for Barack Obama, Mr.. The rest of the examples
were fully inferred from the data. The model was es-
sentially correct for the second, third, and fourth en-
tities. The last row illustrates a partially erroneous
example, in which the model confused the middle
name of John McCain, possibly because of a com-
bination of a strong prior to reuse this row and the
691
Derek Jeter New York
Ben Roethlisberger Pittsburgh Steelers
Alex Rodriguez New York Yankees
Michael Vick Philadelphia Eagles
Kevin Garnett Los Angeles Lakers
Dave Toub The Bears
Table 4: A small segment of the output table for the sports
dataset, showing a few noteworthy correct (blue) and in-
correct (red) examples. Black indicates seed examples.
introduction of a notion of noise.
In the sports dataset, persons and organizations
are mentioned. Recall that success here consists of
identifying the correct team for every player. The
EEA model is not designed for this and performed
poorly. Our model can do better, since it makes use
of context information and features, and it can put a
person and an organization in one row even though
they do not share common words. Table 4 shows a
few noteworthy entities from our complete model?s
output.
Surprisingly, the model failed to infer that Derek
Jeter plays for New York Yankees, although men-
tions of the organization New York Yankees can be
found in our dataset. This is because the model did
not see enough evidence to put them in the same row.
However, it successfully inferred the missing infor-
mation for Ben Roethlisberger. The next two rows
show cases where our model successfully matched
players with their teams and put each word token to
its respective column. The most frequent error, by
far, is illustrated in the fifth row, where a player is
matched with a wrong team. Kevin Garnett plays for
the Boston Celtics, not the Los Angeles Lakers. It
shows that in some cases context information is not
adequate, and a possible improvement might be ob-
tained by providing more context to the model. The
sixth row is interesting because Dave Toub is indeed
affiliated with the Chicago Bears. However, when
the model saw a mention token The Bears, it did not
have any other columns to put the word token The,
and decided to put it in the fourth column although it
is not a location. If we added more columns, deter-
miners could become another attribute of the entities
that might go into one of these new columns.
5 Related Work
There has been work that attempts to fill predefined
templates using Bayesian nonparametrics (Haghighi
and Klein, 2010) and automatically learns template
structures using agglomerative clustering (Cham-
bers and Jurafsky, 2011). Charniak (2001) and El-
sner et al (2009) focused specifically on names and
discovering their structure, which is a part of the
problem we consider here. More similar to our
work, Eisenstein et al (2011) introduced a non-
parametric Bayesian approach to extract structured
databases of entities. A fundamental difference of
our approach from any of the previous work is it
maximizes conditional likelihood and thus allows
beneficial incorporation of arbitrary features.
Our model is focused on the problem of canoni-
calizing mention strings into their parts, though its r
variables (which map mentions to rows) could be in-
terpreted as (within-document and cross-document)
coreference resolution, which has been tackled us-
ing a range of probabilistic models (Li et al, 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al, 2011). We have not evaluated it
as such, believing that further work should be done
to integrate appropriate linguistic cues before such
an application.
6 Conclusions
We presented an improved probabilistic model for
canonicalizing named entities into a table. We
showed that the model adapts to different tasks de-
pending on its input and seeds, and that it improves
over state-of-the-art performance on two corpora.
Acknowledgements
The authors thank Jacob Eisenstein and Tae Yano for
helpful discussions and providing us with the implemen-
tation of their model, Tim Hawes for helpful discussions,
Naomi Saphra for assistance in developing the gold stan-
dard for the politics dataset, and three anonymous review-
ers for comments on an earlier draft of this paper. This re-
search was supported in part by the U.S. Army Research
Office, Google?s sponsorship of the Worldly Knowledge
project at CMU, and A?STAR (fellowship to Y. Sim); the
contents of this paper do not necessarily reflect the posi-
tion or the policy of the sponsors, and no official endorse-
ment should be inferred.
692
References
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
N. Chambers and D. Jurafsky. 2011. Template-based
information extraction without the templates. In Proc.
of ACL-HLT.
E. Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proc. of NAACL.
J. Eisenstein and E. P. Xing. 2010. The CMU 2008 po-
litical blog corpus. Technical report, Carnegie Mellon
University.
J. Eisenstein, T. Yano, W. W. Cohen, N. A. Smith, and
E. P. Xing. 2011. Structured databases of named
entities from Bayesian nonparametrics. In Proc. of
EMNLP Workshop on Unsupervised Learning in NLP.
M. Elsner, E. Charniak, and M. Johnson. 2009. Struc-
tured generative models for unsupervised named-
entity clustering. In Proc. of NAACL-HLT.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In Proc. of ACL.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric Bayesian model.
In Proc. of ACL.
A. Haghighi and D. Klein. 2010. An entity-level ap-
proach to information extraction. In Proc. of ACL
Short Papers.
C. S. Jensen, U. Kjaerulff, and A. Kong. 1995. Blocking
Gibbs sampling in very large probabilistic expert sys-
tem. International Journal of Human-Computer Stud-
ies, 42(6):647?666.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
X. Li, P. Morie, and D. Roth. 2004. Identification and
tracing of ambiguous names: discriminative and gen-
erative approaches. In Proc. of AAAI.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming B, 45(3):503?528.
D. Mochihashi, T. Yamada, and N. Ueda. 2009.
Bayesian unsupervised word segmentation with nested
Pitman-Yor language modeling. In Proc. of ACL-
IJCNLP.
C. Morris. 1983. Parametric empirical Bayes inference:
Theory and applications. Journal of the American Sta-
tistical Association, 78(381):47?65.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with Markov logic. In Proc. of
EMNLP.
S. Singh, A. Subramanya, F. Pereira, and A. McCallum.
2011. Large-scale cross-document coreference using
distributed inference and hierarchical models. In Proc.
of ACL-HLT.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of Royal Statistical Society B,
58(1):267?288.
G. C. G. Wei and M. A. Tanner. 1990. A Monte Carlo
implementation of the EM algorithm and the poor
man?s data augmentation algorithms. Journal of the
American Statistical Association, 85(411):699?704.
693
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 786?796,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Linguistic Structured Sparsity in Text Categorization
Dani Yogatama
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dyogatama@cs.cmu.edu
Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We introduce three linguistically moti-
vated structured regularizers based on
parse trees, topics, and hierarchical word
clusters for text categorization. These
regularizers impose linguistic bias in fea-
ture weights, enabling us to incorporate
prior knowledge into conventional bag-
of-words models. We show that our
structured regularizers consistently im-
prove classification accuracies compared
to standard regularizers that penalize fea-
tures in isolation (such as lasso, ridge,
and elastic net regularizers) on a range of
datasets for various text prediction prob-
lems: topic classification, sentiment anal-
ysis, and forecasting.
1 Introduction
What is the best way to exploit linguistic infor-
mation in statistical text processing models? For
tasks like text classification, sentiment analysis,
and text-driven forecasting, this is an open ques-
tion, as cheap ?bag-of-words? models often per-
form well. Much recent work in NLP has fo-
cused on linguistic feature engineering (Joshi et
al., 2010) or representation learning (Glorot et al,
2011; Socher et al, 2013).
In this paper, we propose a radical alternative.
We embrace the conventional bag-of-words repre-
sentation of text, instead bringing linguistic bias
to bear on regularization. Since the seminal work
of Chen and Rosenfeld (2000), the importance of
regularization in discriminative models of text?
including language modeling, structured predic-
tion, and classification?has been widely recog-
nized. The emphasis, however, has largely been
on one specific kind of inductive bias: avoiding
large weights (i.e., coefficients in a linear model).
Recently, structured (or composite) regulariza-
tion has been introduced; simply put, it reasons
about different weights jointly. The most widely
explored variant, group lasso (Yuan and Lin, 2006)
seeks to avoid large `
2
norms for groups of
weights. Group lasso has been shown useful in
a range of applications, including computational
biology (Kim and Xing, 2008), signal processing
(Lv et al, 2011), and NLP (Eisenstein et al, 2011;
Martins et al, 2011; Nelakanti et al, 2013). For
text categorization problems, Yogatama and Smith
(2014) proposed groups based on sentences, an
idea generalized here to take advantage of richer
linguistic information.
In this paper, we show how linguistic informa-
tion of various kinds?parse trees, thematic topics,
and hierarchical word clusterings?can be used to
construct group lasso variants that impose linguis-
tic bias without introducing any new features. Our
experiments demonstrate that structured regulariz-
ers can squeeze higher performance out of conven-
tional bag-of-words models on seven out of eight
of text categorization tasks tested, in six cases with
more compact models than the best-performing
unstructured-regularized model.
2 Notation
We represent each document as a feature vector
x ? R
V
, where V is the vocabulary size. x
v
is the
frequency of the vth word (i.e., this is a ?bag of
words? model).
Consider a linear model that predicts a binary
response y ? {?1,+1} given x and weight vector
w ? R
V
. We denote our training data of D doc-
uments in the corpus by {x
d
, y
d
}
D
d=1
. The goal of
the learning procedure is to estimate w by mini-
mizing the regularized training data loss:
w? = arg min
w
?(w) +
?
D
d=1
L(x
d
,w, y
d
),
where L(x,w, y) is the loss function for docu-
ment d and ?(w) is the regularizer.
In this work, we use the log loss:
L(x
d
,w, y
d
) = ? log(1 + exp(?y
d
w
>
x
d
)),
786
Other loss functions (e.g., hinge loss, squared loss)
can also be used with any of the regularizers dis-
cussed in this paper.
Our focus is on the regularizer, ?(w). For high
dimensional data such as text, regularization is
crucial to avoid overfitting.
1
The usual starting points for regularization are
the ?lasso? (Tibshirani, 1996) and the ?ridge? (Ho-
erl and Kennard, 1970), based respectively on the
`
1
and squared `
2
norms:
?
las
(w) = ?
las
?w?
1
= ?
?
j
|w
j
|
?
rid
(w) = ?
rid
?w?
2
2
= ?
?
j
w
2
j
Both methods disprefer weights of large magni-
tude; smaller (relative) magnitude means a feature
(here, a word) has a smaller effect on the predic-
tion, and zero means a feature has no effect.
2
The
hyperparameter ? in each case is typically tuned
on a development dataset. A linear combination
of ridge and lasso is known as the elastic net (Zou
and Hastie, 2005). The lasso, ridge, and elastic net
are three strong baselines in our experiments.
3 Group Lasso
Structured regularizers penalize estimates of w in
which collections of weights are penalized jointly.
For example, in the group lasso (Yuan and Lin,
2006), predefined groups of weights (subvectors
of w) are encouraged to either go to zero (as
a group) or not (as a group)?this is known as
?group sparsity.?
3
The variant of group lasso we explore here uses
an `
1,2
norm. Let g index the G predefined groups
of weights and w
g
denote the subvector of w con-
taining weights for group g:
?
glas
(w) =?
glas
?
G
g=1
?
g
?w
g
?
2
,
1
A Bayesian interpretation of regularization is as a prior
on the weight vector w; in many cases ? can be under-
stood as a log-prior representing beliefs about the model held
before exposure to data. For lasso regression, the prior is
a zero-mean Laplace distribution, whereas for ridge regres-
sion the prior is a zero-mean Gaussian distribution. For non-
overlapping group lasso, the prior is a two-level hierarchical
Bayes model (Figueiredo, 2002). The Bayesian interpretation
of overlapping group lasso is not yet well understood.
2
The lasso leads to strongly sparse solutions, in which
many elements of the estimated w are actually zero. This
is an attractive property for efficiency and (perhaps) inter-
pretability. The ridge encourages weights to go toward zero,
but usually not all the way to zero; for this reason its solutions
are known as ?weakly? sparse.
3
Other structured regularizers include the fused lasso
(Tibshirani et al, 2005) and the elitist lasso (Kowalski and
Torresani, 2009).
where ?
glas
is a hyperparameter tuned on a devel-
opment data, and ?
g
is a group specific weight.
Typically the groups are non-overlapping, which
offers computational advantages, but this need not
be the case (Jacob et al, 2009; Jenatton et al,
2011).
4 Structured Regularizers for Text
Past work applying the group lasso to NLP prob-
lems has considered four ways of defining the
groups. Eisenstein et al (2011) defined groups
of coefficients corresponding to the same inde-
pendent variable applied to different (continuous)
output variables in multi-output regression. Mar-
tins et al (2011) defined groups based on fea-
ture templates used in chunking and parsing tasks.
Nelakanti et al (2013) defined groups based on n-
gram histories for language modeling. In each of
these cases, the groups were defined based on in-
formation from feature types alone; given the fea-
tures to be used, the groups were known.
Here we build on a fourth approach that exploits
structure in the data.
4
Yogatama and Smith (2014)
introduced the sentence regularizer, which uses
patterns of word cooccurrence in the training data
to define groups. We review this method, then ap-
ply the idea to three more linguistically informed
structure in text data.
4.1 Sentence Regularizer
The sentence regularizer exploits sentence bound-
aries in each training document. The idea is to
define a group g
d,s
for every sentence s in every
training document d. The group contains coeffi-
cients for words that occur in its sentence. This
means that a word is a member of one group for
every distinct (training) sentence it occurs in, and
that the regularizer is based on word tokens, not
types as in the approach of Martins et al (2011)
and Nelakanti et al (2013). The regularizer is:
?
sen
(w) =
?
D
d=1
?
S
d
s=1
?
d,s
?w
d,s
?
2
,
where S
d
is the number of sentences in document
d. This regularizer results in tens of thousands
to millions of heavily overlapping groups, since
a standard corpus typically contains thousands to
millions of sentences and many words that appear
in more than one sentence.
4
This provides a compelling reason not to view such
methods in a Bayesian framework: if the regularizer is in-
formed by the data, then it does not truly correspond to a
prior.
787
c0,++
c1 c4,+
c2 c3
The actors
c5,++ c8
c6 c7,+
are fantastic
.
Figure 1: An example of a parse tree from the Stanford sen-
timent treebank, which annotates sentiment at the level of
every constituent (indicated here by + and ++; no mark-
ing indicates neutral sentiment). The sentence is The ac-
tors are fantastic. Our regularizer constructs nine groups for
this sentence, corresponding to c
0
, c
1
, . . . , c
8
. g
c
0
consists of
5 weights??w
the
, w
actors
, w
are
, w
fantastic
, w
.
?, exactly the
same as the group in the sentence regularizer?g
c
1
consists
of 2 words, g
c
4
of 3 words, etc. Notice that c
2
, c
3
, c
6
, c
7
,
and c
8
each consist of only 1 word. The Stanford sentiment
treebank has an annotation of sentiments at the constituent
level. As in this example, most constituents are annotated as
neutral.
If the norm of w
g
d,s
is driven to zero, then the
learner has deemed the corresponding sentence ir-
relevant to the prediction. It is important to point
out that, while the regularizer prefers to zero out
the weights for all words in irrelevant sentences, it
also prefers not to zero out weights for words in
relevant sentences. Since the groups overlap and
may work against each other, the regularizer may
not be able to drive many weights to zero on its
own. Yogatama and Smith (2014) used a linear
combination of the sentence regularizer and the
lasso (a kind of sparse group lasso; Friedman et
al., 2010) to also encourage weights of irrelevant
word types to go to zero.
5
4.2 Parse Tree Regularizer
Sentence boundaries are a rather superficial kind
of linguistic structure; syntactic parse trees pro-
vide more fine-grained information. We introduce
a new regularizer, the parse tree regularizer, in
which groups are defined for every constituent in
every parse of a training data sentence.
Figure 1 illustrates the group structures derived
from an example sentence from the Stanford sen-
timent treebank (Socher et al, 2013). This regu-
larizer captures the idea that phrases might be se-
lected as relevant or (in most cases) irrelevant to
a task, and is expected to be especially useful in
sentence-level prediction tasks.
The parse-tree regularizer (omitting the group
5
Formally, this is equivalent to including one additional
group for each word type.
coefficients and ?) for one sentence with the parse
tree shown in Figure 1 is:
?
tree
(w) =
p
|w
the
|
2
+ |w
actors
|
2
+ |w
are
|
2
+ |w
fantastic
|
2
+ |w
.
|
2
+
p
|w
are
|
2
+ |w
fantastic
|
2
+ |w
2
.
|
+
p
|w
the
|
2
+ |w
actors
|
2
+
p
|w
are
|
2
+ |w
fantastic
|
2
+ |w
the
|+ |w
actors
|+ |w
are
|+ |w
fantastic
|+ |w
.
|
The groups have a tree structure, in that assign-
ing zero values to the weights in a group corre-
sponding to a higher-level constituent implies the
same for those constituents that are dominated by
it. This resembles the tree-guided group lasso in
Kim and Xing (2008), although the leaf nodes in
their tree represent tasks in multi-task regression.
Of course, in a corpus there are many parse trees
(one per sentence, so the number of parse trees is
the number of sentences). The parse-tree regular-
izer is:
?
tree
(w) =
?
D
d=1
?
S
d
s=1
?
C
d,s
c=1
?
d,s,c
?w
d,s,c
?
2
,
where ?
d,s,c
= ?
glas
?
?
size(g
d,s,c
), d ranges
over (training) documents and c ranges over con-
stituents in the parse of sentence s in docu-
ment d. Similar to the sentence regularizer,
the parse-tree regularizer operates on word to-
kens. Note that, since each word token is it-
self a constituent, the parse tree regularizer in-
cludes terms just like the lasso naturally, penal-
izing the absolute value of each word?s weight
in isolation. For the lasso-like penalty on each
word, instead of defining the group weights to be
1 ? the number of tokens for each word type, we
tune one group weight for all word types on a de-
velopment data. As a result, besides ?
glas
, we have
an additional hyperparameter, denoted by ?
las
.
To gain an intuition for this regularizer, consider
the case where we apply the penalty only for a sin-
gle tree (sentence), which for ease of exposition is
assumed not to use the same word more than once
(i.e., ?x?
?
= 1). Because it instantiates the tree-
structured group lasso, the regularizer will require
bigger constituents to be ?included? (i.e., their
words given nonzero weight) before smaller con-
stituents can be included. The result is that some
words may not be included. Of course, in some
sentences, some words will occur more than once,
and the parse tree regularizer instantiates groups
for constituents in every sentence in the training
corpus, and these groups may work against each
other. The parse tree regularizer should therefore
788
be understood as encouraging group behavior of
syntactically grouped words, or sharing of infor-
mation by syntactic neighbors.
In sentence level prediction tasks, such as
sentence-level sentiment analysis, it is known that
most constituents (especially those that corre-
spond to shorter phrases) in a parse tree are un-
informative (neutral sentiment). This was verified
by Socher et al (2013) when annotating phrases
in a sentence for building the Stanford sentiment
treebank. Our regularizer incorporates our prior
expectation that most constituents should have no
effect on prediction.
4.3 LDA Regularizer
Another type of structure to consider is topics.
For example, if we want to predict whether a pa-
per will be cited or not (Yogatama et al, 2011),
the model can perform better if it knows before-
hand the collections of words that represent certain
themes (e.g., in ACL papers, these might include
machine translation, parsing, etc.). As a result,
the model can focus on which topics will increase
the probability of getting citations, and penalize
weights for words in the same topic together, in-
stead of treating each word separately.
We do this by inferring topics in the training
corpus by estimating the latent Dirichlet aloca-
tion (LDA) model (Blei et al, 2003)). Note that
LDA is an unsupervised method, so we can in-
fer topical structures from any collection of docu-
ments that are considered related to the target cor-
pus (e.g., training documents, text from the web,
etc.). This contrasts with typical semi-supervised
learning methods for text categorization that com-
bine unlabeled and labeled data within a genera-
tive model, such as multinomial na??ve Bayes, via
expectation-maximization (Nigam et al, 2000) or
semi-supervised frequency estimation (Su et al,
2011). Our method does not use unlabeled data
to obtain more training documents or estimate the
joint distributions of words better, but it allows the
use of unlabeled data to induce topics. We leave
comparison with other semi-supervised methods
for future work.
There are many ways to associate inferred top-
ics with group structure. In our experiments, we
choose the R most probable words given a topic
and create a group for them.
6
The LDA regular-
6
Another possibility is to group the smallest set of words
whose total probability given a topic amounts to P (e.g.,
0.99). mass of a topic. Preliminary experiments found this
izer can be written as:
?
lda
(w) =
?
K
k=1
?
k
?w
k
?
2
,
where k ranges over the K topics. Similar to our
earlier notations, w
k
corresponds to the subvec-
tor of w such that the corresponding features are
present in topic k. Note that in this case we can
also have overlapping groups, since words can ap-
pear in the top R of many topics.
k = 1 k = 2 k = 3 k = 4
soccer injury physics monday
striker knee gravity tuesday
midfielder ligament moon april
goal shoulder sun june
defender cruciate relativity sunday
Table 1: A toy example of K = 4 topics. The top R = 5
words in each topics are displayed. The LDA regularizer
will construct four groups from these topics. The first group
is ?w
soccer
, w
striker
, w
midfielder
, w
goal
, w
defender
?, the sec-
ond group is ?w
injury
, w
knee
, w
ligament
, w
shoulder
, w
cruciate
?,
etc. In this example, there are no words occurring in the top
R of more than one topic, but that need not be the case in
general.
To gain an intuition for this regularizer, consider
the toy example in Table 1. the case where we
have K = 4 topics and we select R = 5 top words
from each topic. Supposed that we want to clas-
sify whether an article is a sports article or a sci-
ence article. The regularizer might encourage the
weights for the fourth topics? words toward zero,
since they are less useful for the task. Addition-
ally, the regularizer will penalize words in each of
the other three groups collectively. Therefore, if
(for example) ligament is deemed a useful feature
for classifying an article to be about sports, then
the other words in that topic will have a smaller ef-
fective penalty for getting nonzero weights?even
weights of the opposite sign as w
ligament
. It is im-
portant to distinguish this from unstructured reg-
ularizers such as the lasso, which penalize each
word?s weight on its own without regard for re-
lated word types.
Unlike the parse tree regularizer, the LDA regu-
larizer is not tree structured. Since the lasso-like
penalty does not occur naturally in a non tree-
structured regularizer, we add an additional lasso
penalty for each word type (with hyperparameter
?
las
) to also encourage weights of irrelevant words
to go to zero. Our LDA regularizer is an instance
of sparse group lasso (Friedman et al, 2010).
not to work well.
789
4.4 Brown Cluster Regularizer
Brown clustering is a commonly used unsuper-
vised method for grouping words into a hierarchy
of clusters (Brown et al, 1992). Because it uses
local information, it tends to discover words with
similar syntactic behavior, though semantic group-
ings are often evident, especially at the more fine-
grained end of the hierarchy.
We incorporate Brown clusters into a regular-
izer in a similar way to the topical word groups
inferred using LDA in ?4.3, but here we make use
of the hierarchy. Specifically, we construct tree-
structured groups, one per cluster (i.e., one per
node in the hierarchy). The Brown cluster regu-
larizer is:
?
brown
(w) =
?
N
v=1
?
v
?w
v
?
2
,
where v ranges over the N nodes in the Brown
cluster tree. As a tree structured regularizer, this
regularizer enforces constraints that a node v?s
group is given nonzero weights only if those nodes
that dominate v (i.e., are on a path from v to the
root) have their groups selected.
Consider a similar toy example to the LDA reg-
ularizer (sports vs. science) and the hierarchical
clustering of words in Figure 2. In this case, the
Brown cluster regularizer will create 17 groups,
one for every node in the clustering tree. The regu-
larizer for this tree (omitting the group coefficients
and ?) is:
?
brown
(w) =
?
7
i=0
?w
v
i
?
2
+ |w
goal
|+ |w
striker
|
+ |w
midfielder
|+ |w
knee
|+ |w
injury
|
+ |w
gravity
|+ |w
moon
|+ |w
sun
|
The regularizer penalizes words in a cluster to-
gether, exploiting discovered syntactic related-
ness. Additionally, the regularizer can zero out
weights of words corresponding to any of the in-
ternal nodes, such as v
7
if the words monday and
sunday are deemed irrelevant to prediction.
Note that the regularizer already includes terms
like the lasso naturally. Similar to the parse
tree regularizer, for the lasso-like penalty on each
word, we tune one group weight for all word types
on a development data with a hyperparameter ?
las
.
A key difference between the Brown cluster
regularizer and the parse tree regularizer is that
there is only one tree for Brown cluster regularizer,
whereas the parse tree regularizer can have mil-
lions (one per sentence in the training data). The
v0
v1 v5
v2 v4
v3 v10
v8 v9
goal striker
midfielder
v11 v12
knee injury
v6 v7
v13 v14
moon sun
v15 v16
monday sunday
Figure 2: An illustrative example of Brown clusters for N =
9. The Brown cluster regularizer constructs 17 groups, one
per node in for this tree, v
0
, v
1
, . . . , v
16
. v
0
contains 8 words,
v
1
contains 5, etc. Note that the leaves, v
8
, v
9
, . . . , v
16
, each
contain one word.
LDA and Brown cluster regularizers offer ways to
incorporate unlabeled data, if we believe that the
unlabeled data can help us infer better topics or
clusters. Note that the processes of learning topics
or clusters, or parsing training data sentences, are
a separate stage that precedes learning our predic-
tive model.
5 Learning
There are many optimization methods for learn-
ing models with structured regularizers, particu-
lary group lasso (Jacob et al, 2009; Jenatton et al,
2011; Chen et al, 2011; Qin and Goldfarb, 2012;
Yuan et al, 2013). We choose the optimization
method of Yogatama and Smith (2014) since it
handles millions of overlapping groups effectively.
The method is based on the alternating directions
method of multipliers (ADMM; Hestenes, 1969;
Powell, 1969). We review it here in brief, for com-
pleteness, and show how it can be applied to tree-
structured regularizers (such as the parse tree and
Brown cluster regularizers in ?4) in particular.
Our learning problem is, generically:
min
w
?(w) +
?
D
d=1
L(x
d
,w, y
d
).
Separating the lasso-like penalty for each word
type from our group regularizers, we can rewrite
this problem as:
min
w,v
?
las
(w) + ?
glas
(v) +
?
D
d=1
L(x
d
,w, y
d
)
s.t. v = Mw
where v consists of copies of the elements of
w. Notice that we work directly on w instead
of the copies for the lasso-like penalty, since it
does not have overlaps and has its own hyper-
parameters ?
las
. For the remaining groups with
size greater than one, we create copies v of size
790
L =
?
G
g=1
size(g). M ? {0, 1}
L?V
is a ma-
trix whose 1s link elements of w to their copies.
7
We now have a constrained optimization prob-
lem, from which we can create an augmented La-
grangian problem; let u be the Lagrange variables:
?
las
(w) + ?
glas
(v) + L(w)
+ u
>
(v ?Mw) +
?
2
?v ?Mw?
2
2
ADMM proceeds by iteratively updating each
of w, v, and u, amounting to the following sub-
problems:
min
w
?
las
(w) + L(w)? u
>
Mw +
?
2
?v ?Mw?
2
2
(1)
min
v
?
glas
(v) + u
>
v +
?
2
?v ?Mw?
2
2
(2)
u = u + ?(v ?Mw) (3)
Yogatama and Smith (2014) show that Eq. 1
can be rewritten in a form quite similar to `
2
-
regularized loss minimization.
8
Eq. 2 is the proximal operator of
1
?
?
glas
ap-
plied to Mw ?
u
?
. As such, it depends on the
form of M. Note that when applied to the col-
lection of ?copies? of the parameters, v, ?
glas
no
longer has overlapping groups. Defined M
g
as
the rows of M corresponding to weight copies as-
signed to group g. Let z
g
, M
g
w ?
u
g
?
. De-
note ?
g
= ?
glas
?
size(g). The problem can be
solved by applying the proximal operator used in
non-overlapping group lasso to each subvector:
v
g
= prox
?
glas
,
?
g
?
(z
g
)
=
?
?
?
0 if ?z
g
?
2
?
?
g
?
?z
g
?
2
?
?
g
?
?z
g
?
2
z
g
otherwise.
For a tree structured regularizer, we can get
speedups by working from the root node towards
the leaf nodes when applying the proximal oper-
ator in the second step. If g is a node in a tree
which is driven to zero, all of its children h that
has ?
h
? ?
g
will also be driven to zero.
Eq. 3 is a simple update of the dual variable u.
Algorithm 1 summarizes our learning procedure.
9
7
For the parse tree regularizer, L is the sum, over all
training-data word tokens t, of the number of constituents t
belongs to. For the LDA regularizer, L = R ? K. For the
Brown cluster regularizer, L = V ? 1.
8
The difference lies in that the squared `
2
norm in the
penalty penalizes the difference between w and a vector that
depends on the current values of u and v. This does not affect
the algorithm or its convergence in any substantive way.
9
We use relative changes in the `
2
norm of the parameter
vector w as our convergence criterion (threshold of 10
?3
),
and set the maximum number of iterations to 100. Other cri-
teria can also be used.
Algorithm 1 ADMM for overlapping group lasso
Input: augmented Lagrangian variable ?, regularization
strengths ?
glas
and ?
las
while stopping criterion not met do
w = arg min
w
?
las
(w)+L(w)+
?
2
P
V
i=1
N
i
(w
i
??
i
)
2
for g = 1 to G do
v
g
= prox
?
glas
,
?
g
?
(z
g
)
end for
u = u + ?(v ?Mw)
end while
6 Experiments
6.1 Datasets
We use publicly available datasets to evaluate our
model described in more detail below.
Topic classification. We consider four binary
categorization tasks from the 20 Newsgroups
dataset.
10
Each task involves categorizing a
document according to two related categories:
comp.sys: ibm.pc.hardware vs. mac.hardware;
rec.sport: baseball vs. hockey; sci: med vs. space;
and alt.atheism vs. soc.religion.christian.
Sentiment analysis. One task in sentiment anal-
ysis is predicting the polarity of a piece of text, i.e.,
whether the author is favorably inclined toward a
(usually known) subject of discussion or proposi-
tion (Pang and Lee, 2008). Sentiment analysis,
even at the coarse level of polarity we consider
here, can be confused by negation, stylistic use of
irony, and other linguistic phenomena. Our sen-
timent analysis datasets consist of movie reviews
from the Stanford sentiment treebank (Socher et
al., 2013),
11
and floor speeches by U.S. Congress-
men alongside ?yea?/?nay? votes on the bill under
discussion (Thomas et al, 2006).
12
For the Stan-
ford sentiment treebank, we only predict binary
classifications (positive or negative) and exclude
neutral reviews.
Text-driven forecasting. Forecasting from text
requires identifying textual correlates of a re-
sponse variable revealed in the future, most of
which will be weak and many of which will be
spurious (Kogan et al, 2009). We consider two
such problems. The first one is predicting whether
a scientific paper will be cited or not within three
years of its publication (Yogatama et al, 2011);
10
http://qwone.com/
?
jason/20Newsgroups
11
http://nlp.stanford.edu/sentiment/
12
http://www.cs.cornell.edu/
?
ainur/data.html
791
Dataset D # Dev. # Test V
2
0
N
science 952 235 790 30,154
sports 958 239 796 20,832
relig. 870 209 717 24,528
comp. 929 239 777 20,868
S
e
n
t
.
movie 6,920 872 1,821 17,576
vote 1,175 257 860 24,508
F
o
r
e
.
science 3,207 280 539 42,702
bill 37,850 7,341 6,571 10,001
Table 2: Descriptive statistics about the datasets.
the dataset comes from the ACL Anthology and
consists of research papers from the Association
for Computational Linguistics and citation data
(Radev et al, 2009). The second task is predicting
whether a legislative bill will be recommended by
a Congressional committee (Yano et al, 2012).
13
Table 2 summarizes statistics about the datasets
used in our experiments. In total, we evaluate our
method on eight binary classification tasks.
6.2 Setup
In all our experiments, we use unigram features
plus an additional bias term which is not regu-
larized. We compare our new regularizers with
state-of-the-art methods for document classifica-
tion: lasso, ridge, and elastic net regularization, as
well as the sentence regularizer discussed in ?4.1
(Yogatama and Smith, 2014).
14
We parsed all corpora using the Berkeley parser
(Petrov and Klein, 2007).
15
For the LDA regular-
izers, we ran LDA
16
on training documents with
K = 1, 000 and R = 10. For the Brown cluster
regularizers, we ran Brown clustering
17
on train-
ing documents with 5, 000 clusters for the topic
classification and sentiment analysis datasets, and
1, 000 for the larger text forecasting datasets (since
they are bigger datasets that took more time).
13
http://www.ark.cs.cmu.edu/bills
14
Hyperparameters are tuned on a separate develop-
ment dataset, using accuracy as the evaluation crite-
rion. For lasso and ridge models, we choose ? from
{10
?2
, 10
?1
, 1, 10, 10
2
, 10
3
}. For elastic net, we perform
grid search on the same set of values as ridge and lasso
experiments for ?
rid
and ?
las
. For the sentence, Brown
cluster, and LDA regularizers, we perform grid search on
the same set of values as ridge and lasso experiments for
?, ?
glas
, ?
las
. For the parse tree regularizer, because there
are many more groups than other regularizers, we choose
?
glas
from {10
?4
, 10
?3
, 10
?2
, 10
?1
, 10}, ? and ?
las
from
the same set of values as ridge and lasso experiments. If there
is a tie on development data we choose the model with the
smallest number of nonzero weights.
15
https://code.google.com/p/berkeleyparser/
16
http://www.cs.princeton.edu/
?
blei/lda-c/
17
https://github.com/percyliang/brown-cluster
6.3 Results
Table 3 shows the results of our experiments on
the eight datasets. The results demonstrate the su-
periority of structured regularizers. One of them
achieved the best result on all but one dataset.
18
It
is also worth noting that in most cases all variants
of the structured regularizers outperformed lasso,
ridge, and elastic net. In four cases, the new regu-
larizers in this paper outperform the sentence reg-
ularizer.
We can see that the parse tree regularizer per-
formed the best for the movie review dataset. The
task is to predict sentence-level sentiment, so each
training example is a sentence. Since constituent-
level annotations are available for this dataset, we
only constructed groups for neutral constituents
(i.e., we drive neutral constituents to zero during
training). It has been shown that syntactic in-
formation is helpful for sentence-level predictions
(Socher et al, 2013), so the parse tree regularizer
is naturally suitable for this task.
The Brown cluster and LDA regularizers per-
formed best for the forecasting scientific articles
dataset. The task is to predict whether an article
will be cited or not within three years after publi-
cation. Regularizers that exploit the knowledge of
semantic relations (e.g., topical categories), such
as the Brown cluster and LDA regularizers, are
therefore suitable for this type of prediction.
Table 4 shows model sizes obtained by each
of the regularizers for each dataset. While lasso
prunes more aggressively, it almost always per-
forms worse. Our structured regularizers were
able to obtain a significantly smaller model (27%,
34%, 19% as large on average for parse tree,
Brown, and LDA regularizers respectively) com-
pared to the ridge model.
Topic and cluster features. Another way to in-
corporate LDA topics and Brown clusters into a
linear model is by adding them as additional fea-
tures. For the 20N datasets, we also ran lasso,
ridge, and elastic net with additional LDA topic
and Brown cluster features.
19
Note that these new
baselines use more features than our model. We
can also add these additional features to our model
18
This ?bill? dataset, where they offered no improvement,
is the largest by far (37,850 documents), and therefore the
one where regularizers should matter the least. Note that the
differences are small across regularizers for this dataset.
19
For LDA, we took the top 10 words in a topic as a feature.
For Brown clusters, we add a cluster as an additional feature
if its size is less than 50.
792
Task Dataset
Accuracy (%)
m.f.c. lasso ridge elastic sentence parse Brown LDA
20N
science 50.13 90.63 91.90 91.65 96.20 92.66 93.04 93.67
sports 50.13 91.08 93.34 93.71 95.10 93.09 93.71 94.97
religion 55.51 90.52 92.47 92.47 92.75 94.98 92.89 93.03
computer 50.45 85.84 86.74 87.13 90.86 89.45 86.36 88.42
Sentiment
movie 50.08 78.03 80.45 80.40 80.72 81.55 80.34 78.36
vote 58.37 73.14 72.79 72.79 73.95 73.72 66.86 73.14
Forecasting
science 50.28 64.00 66.79 66.23 67.71 66.42 69.02 69.39
bill 87.40 88.36 87.70 88.48 88.11 87.98 88.20 88.27
Table 3:
Classification
accuracies on
various datasets.
?m.f.c.? is the
most frequent
class baseline.
Boldface shows
best results.
Task Dataset
Model size (%)
m.f.c. lasso ridge elastic sentence parse Brown LDA
20N
science - 1 100 34 12 2 42 9
sports - 2 100 15 3 3 16 9
religion - 0.3 100 48 94 72 41 15
computer - 2 100 24 10 5 24 8
Sentiment
movie - 10 100 54 83 87 59 12
vote - 2 100 44 6 2 30 4
Forecasting
science - 31 100 43 99 9 50 90
bill - 7 100 7 8 37 7 7
Table 4: Model
sizes (percentages
of nonzero
features in the
resulting models)
on various
datasets.
Dataset
+ LDA features LDA
lasso ridge elastic reg.
science 90.63 91.90 91.90 93.67
sports 91.33 93.47 93.84 94.97
religion 91.35 92.47 91.35 93.03
computer 85.20 86.87 86.35 88.42
Dataset
+ Brown features Brown
lasso ridge elastic reg.
science 86.96 90.51 91.14 93.04
sports 82.66 88.94 85.43 93.71
religion 94.98 96.93 96.93 92.89
computer 55.72 96.65 67.57 86.36
Table 5: Classification accuracies on the 20N datasets for
lasso, ridge, and elastic net models with additional LDA fea-
tures (top) and Brown cluster features (bottom). The last col-
umn shows structured regularized models from Table 3.
and treat them as regular features (i.e., they do
not belong to any groups and are regularized with
standard regularizer such as the lasso penalty).
The results in Table 5 show that for these datasets,
models that incorporate this information through
structured regularizers outperformed models that
encode this information as additional features in
4 out 4 of cases (LDA) and 2 out of 4 cases
(Brown). Sparse models with Brown clusters ap-
pear to overfit badly; recall that the clusters were
learned on only the training data?clusters from
a larger dataset would likely give stronger re-
sults. Of course, better performance might also
be achieved by incorporating new features as well
as using structured regularizers.
6.4 Examples
To gain an insight into the models, we inspect
group sparsity patterns in the learned models by
looking at the parameter copies v. This lets us see
which groups are considered important (i.e., ?se-
lected? vs. ?removed?). For each of the proposed
regularizers, we inspect the model a task in which
it performed well.
For the parse tree regularizer, we inspect the
model for the 20N:religion task. We observed that
the model included most of the sentences (root
node groups), but in some cases removed phrases
from the parse trees, such as ozzy osbourne in the
sentence ozzy osbourne , ex-singer and main char-
acter of the black sabbath of good ole days past ,
is and always was a devout catholic .
For the LDA regularizer, we inspect zero and
nonzero groups (topics) in the forecasting scien-
tific articles task. In this task, we observed that
642 out of 1,000 topics are driven to zero by
our model. Table 6 shows examples of zero and
nonzero topics for the dev.-tuned hyperparameter
values. We can see that in this particular case, the
model kept meaningful topics such as parsing and
speech processing, and discarded general topics
that are not correlated with the content of the pa-
pers (e.g., acknowledgment, document metadata,
equation, etc.). Note that most weights for non-
selected groups, even in w, are near zero.
For the Brown cluster regularizer, we inspect
the model from the 20N:science task. 771 out
of 5,775 groups were driven to zero for the best
model tuned on the development set. Examples
of zero and nonzero groups are shown in Ta-
ble 7. Similar to the LDA example, the groups
that were driven to zero tend to contain generic
words that are not relevant to the predictions. We
can also see the tree structure effect in the regu-
larizer. The group {underwater, industrial} was
793
= 0
?acknowledgment?: workshop arpa program session darpa research papers spoken technology systems
?document metadata?: university references proceedings abstract work introduction new been research both
?equation?: pr w h probability wi gram context z probabilities complete
?translation?: translation target source german english length alignment hypothesis translations position
6= 0
?translation?: korean translation english rules sentences parsing input evaluation machine verb
?speech processing?: speaker identification topic recognition recognizer models acoustic test vocabulary independent
?parsing?: parser parsing probabilistic prediction parse pearl edges chart phase theory
?classification?: documents learning accuracy bayes classification wt document naive method selection
Table 6: Examples of LDA regularizer-removed and -selected groups (in v) in the forecasting scientific articles dataset. Words
with weights (in w) of magnitude greater than 10
?3
are highlighted in red (not cited) and blue (cited).
= 0
underwater industrial
spotted hit reaped rejuvenated destroyed stretched undertake shake run
seeing developing tingles diminishing launching finding investigating receiving
maintaining
adds engage explains builds
6= 0
failure reproductive ignition reproduction
cyanamid planetary nikola fertility astronomical geophysical # lunar cometary
supplying astronautical
magnetic atmospheric
std underwater hpr wordscan exclusively aneutronic industrial peoples obsessive
congenital rare simple bowel hereditary breast
Table 7: Examples of Brown
regularizer-removed and
-selected groups (in v) in the
20N:science task. # denotes
any numeral. Words with
weights (in w) of magnitude
greater than 10
?3
are
highlighted in red (space) and
blue (medical).
driven to zero, but not once it combined with other
words such as hpr, std, obsessive. Note that we
ran Brown clustering only on the training docu-
ments; running it on a larger collection of (unla-
beled) documents relevant to the prediction task
(i.e., semi-supervised learning) is worth exploring
in future work.
7 Related and Future Work
Overall, our results demonstrate that linguistic
structure in the data can be used to improve bag-
of-words models, through structured regulariza-
tion. State-of-the-art approaches to some of these
problems have used additional features and repre-
sentations (Yessenalina et al, 2010; Socher et al,
2013). For example, for the vote sentiment analy-
sis datasets, latent variable models of Yessenalina
et al (2010) achieved a superior result of 77.67%.
To do so, they sacrificed convexity and had to rely
on side information for initialization. Our exper-
imental focus has been on a controlled compari-
son between regularizers for a fixed model family
(the simplest available, linear with bag-of-words
features). However, the improvements offered by
our regularization methods can be applied in fu-
ture work to other model families with more care-
fully engineered features, metadata features (espe-
cially important in forecasting), latent variables,
etc. In particular, note that other kinds of weights
(e.g., metadata) can be penalized conventionally,
or incorporated into the structured regularization
where it makes sense to do so (e.g., n-grams, as in
Nelakanti et al, 2013).
8 Conclusion
We introduced three data-driven, linguistically
informed structured regularizers based on parse
trees, topics, and hierarchical word clusters. We
empirically showed that models regularized us-
ing our methods consistently outperformed stan-
dard regularizers that penalize features in isolation
such as lasso, ridge, and elastic net on a range
of datasets for various text prediction problems:
topic classification, sentiment analysis, and fore-
casting.
Acknowledgments
The authors thank Brendan O?Connor for help
with visualization and three anonymous review-
ers for helpful feedback on an earlier draft of this
paper. This research was supported in part by
computing resources provided by a grant from the
Pittsburgh Supercomputing Center, a Google re-
search award, and the Intelligence Advanced Re-
search Projects Activity via Department of In-
terior National Business Center contract number
D12PC00347. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. The views and conclusions
contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either ex-
pressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
794
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467?479.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for me models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Car-
bonell, and Eric P. Xing. 2011. Smoothing prox-
imal gradient method for general structured sparse
learning. In Proc. of UAI.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. of ACL.
Mario A. T. Figueiredo. 2002. Adaptive sparseness
using Jeffreys? prior. In Proc. of NIPS.
Jerome Friedman, Trevor Hastie, and Robert Tibshiran.
2010. A note on the group lasso and a sparse group
lasso. Technical report, Stanford University.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc. of
ICML.
Magnus R. Hestenes. 1969. Multiplier and gradient
methods. Journal of Optimization Theory and Ap-
plications, 4:303?320.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55?67.
Laurent Jacob, Guillaume Obozinski, and Jean-
Philippe Vert. 2009. Group lasso with overlap and
graph lasso. In Proc. of ICML.
Rodolphe Jenatton, Jean-Yves Audibert, and Fran-
cis Bach. 2011. Structured variable selection
with sparsity-inducing norms. Journal of Machine
Learning Research, 12:2777?2824.
Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and
Noah A. Smith. 2010. Movie reviews and rev-
enues: An experiment in text regression. In Proc.
of NAACL.
Seyoung Kim and Eric P. Xing. 2008. Feature selec-
tion via block-regularized regression. In Proc. of
UAI.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc.
of HLT-NAACL.
Matthieu Kowalski and Bruno Torresani. 2009. Spar-
sity and persistence: mixed norms provide simple
signal models with dependent coefficients. Signal,
Image and Video Processing, 3(3):251?0264.
Xiaolei Lv, Guoan Bi, and Chunru Wan. 2011. The
group lasso for stable recovery of block-sparse sig-
nal representations. IEEE Transactions on Signal
Processing, 59(4):1371?1382.
Andre F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Mario A. T. Figueiredo. 2011. Struc-
tured sparsity in structured prediction. In Proc. of
EMNLP.
Anil Nelakanti, Cedric Archambeau, Julien Mairal,
Francis Bach, and Guillaume Bouchard. 2013.
Structured penalties for log-linear language models.
In Proc. of EMNLP.
Kamal Nigam, Andrew McCallum, Sebastian Thrun,
and Tom Mitchell. 2000. Text classification from la-
beled and unlabeled documents using em. Machine
Learning, 39(2-3):103?134.
Bo Pang and Lilian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of HLT-NAACL.
M. J. D. Powell. 1969. A method for nonlinear con-
straints in minimization problems. In R. Fletcher,
editor, Optimization, pages 283?298. Academic
Press.
Zhiwei (Tony) Qin and Donald Goldfarb. 2012. Struc-
tured sparsity via alternating direction methods.
Journal of Machine Learning Research, 13:1435?
1468.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology net-
work corpus. In Proc. of ACL Workshop on Natural
Language Processing and Information Retrieval for
Digital Libraries.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Chris Manning, Andrew Ng, and Chris
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Proc.
of EMNLP.
Jiang Su, Jelber Sayyad-Shirabad, and Stan Matwin.
2011. Large scale text classication using semi-
supervised multinomial naive Bayes. In Proc. of
ICML.
Matt Thomas, Bo Pang, and Lilian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proc. of
EMNLP.
795
Robert Tibshirani, Michael Saunders, Saharon Ros-
set, Ji Zhu, and Keith Knight. 2005. Sparsity and
smoothness via the fused lasso. Journal of Royal
Statistical Society B, 67(1):91?108.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of Royal Statistical
Society B, 58(1):267?288.
Tae Yano, Noah A. Smith, and John D. Wilkerson.
2012. Textual predictors of bill survival in congres-
sional committees. In Proc. of NAACL.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document
sentiment classification. In Proc. of EMNLP.
Dani Yogatama and Noah A. Smith. 2014. Making the
most of bag of words: Sentence regularization with
alternating direction method of multipliers. In Proc.
of ICML.
Dani Yogatama, Michael Heilman, Brendan O?Connor,
Chris Dyer, Bryan R. Routledge, and Noah A.
Smith. 2011. Predicting a scientific community?s
response to an article. In Proc. of EMNLP.
Ming Yuan and Yi Lin. 2006. Model selection
and estimation in regression with grouped variables.
Journal of the Royal Statistical Society, Series B,
68(1):49?67.
Lei Yuan, Jun Liu, and Jieping Ye. 2013. Efficient
methods for overlapping group lasso. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 35(9):2104?2116.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
796
Dynamic Language Models for Streaming Text
Dani Yogatama? Chong Wang? Bryan R. Routledge? Noah A. Smith? Eric P. Xing?
?School of Computer Science
?Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
?{dyogatama,chongw,nasmith,epxing}@cs.cmu.edu, ?routledge@cmu.edu
Abstract
We present a probabilistic language model that
captures temporal dynamics and conditions on
arbitrary non-linguistic context features. These
context features serve as important indicators
of language changes that are otherwise difficult
to capture using text data by itself. We learn
our model in an efficient online fashion that is
scalable for large, streaming data. With five
streaming datasets from two different genres?
economics news articles and social media?we
evaluate our model on the task of sequential
language modeling. Our model consistently
outperforms competing models.
1 Introduction
Language models are a key component in many NLP
applications, such as machine translation and ex-
ploratory corpus analysis. Language models are typi-
cally assumed to be static?the word-given-context
distributions do not change over time. Examples
include n-gram models (Jelinek, 1997) and proba-
bilistic topic models like latent Dirichlet allocation
(Blei et al., 2003); we use the term ?language model?
to refer broadly to probabilistic models of text.
Recently, streaming datasets (e.g., social media)
have attracted much interest in NLP. Since such data
evolve rapidly based on events in the real world, as-
suming a static language model becomes unrealistic.
In general, more data is seen as better, but treating all
past data equally runs the risk of distracting a model
with irrelevant evidence. On the other hand, cau-
tiously using only the most recent data risks overfit-
ting to short-term trends and missing important time-
insensitive effects (Blei and Lafferty, 2006; Wang
et al., 2008). Therefore, in this paper, we take steps
toward methods for capturing long-range temporal
dynamics in language use.
Our model also exploits observable context vari-
ables to capture temporal variation that is otherwise
difficult to capture using only text. Specifically for
the applications we consider, we use stock market
data as exogenous evidence on which the language
model depends. For example, when an important
company?s price moves suddenly, the language model
should be based not on the very recent history, but
should be similar to the language model for a day
when a similar change happened, since people are
likely to say similar things (either about that com-
pany, or about conditions relevant to the change).
Non-linguistic contexts such as stock price changes
provide useful auxiliary information that might indi-
cate the similarity of language models across differ-
ent timesteps.
We also turn to a fully online learning framework
(Cesa-Bianchi and Lugosi, 2006) to deal with non-
stationarity and dynamics in the data that necessitate
adaptation of the model to data in real time. In on-
line learning, streaming examples are processed only
when they arrive. Online learning also eliminates
the need to store large amounts of data in memory.
Strictly speaking, online learning is distinct from
stochastic learning, which for language models built
on massive datasets has been explored by Hoffman
et al. (2013) and Wang et al. (2011). Those tech-
niques are still for static modeling. Language model-
ing for streaming datasets in the context of machine
translation was considered by Levenberg and Os-
borne (2009) and Levenberg et al. (2010). Goyal
et al. (2009) introduced a streaming algorithm for
large scale language modeling by approximating n-
gram frequency counts. We propose a general online
learning algorithm for language modeling that draws
inspiration from regret minimization in sequential
predictions (Cesa-Bianchi and Lugosi, 2006) and on-
181
Transactions of the Association for Computational Linguistics, 2 (2014) 181?192. Action Editor: Eric Fosler-Lussier.
Submitted 10/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
line variational algorithms (Sato, 2001; Honkela and
Valpola, 2003).
To our knowledge, our model is the first to bring
together temporal dynamics, conditioning on non-
linguistic context, and scalable online learning suit-
able for streaming data and extensible to include
topics and n-gram histories. The main idea of our
model is independent of the choice of the base lan-
guage model (e.g., unigrams, bigrams, topic models,
etc.). In this paper, we focus on unigram and bi-
gram language models in order to evaluate the basic
idea on well understood models, and to show how it
can be extended to higher-order n-grams. We leave
extensions to topic models for future work.
We propose a novel task to evaluate our proposed
language model. The task is to predict economics-
related text at a given time, taking into account the
changes in stock prices up to the corresponding day.
This can be seen an inverse of the setup considered
by Lavrenko et al. (2000), where news is assumed
to influence stock prices. We evaluate our model
on economics news in various languages (English,
German, and French), as well as Twitter data.
2 Background
In this section, we first discuss the background for
sequential predictions then describe how to formulate
online language modeling as sequential predictions.
2.1 Sequential Predictions
Let w1, w2, . . . , wT be a sequence of response vari-
ables, revealed one at a time. The goal is to design
a good learner to predict the next response, given
previous responses and additional evidence which
we denote by xt ? RM (at time t). Throughout this
paper, we use the term features for x. Specifically, at
each round t, the learner receives xt and makes a pre-
diction w?t, by choosing a parameter vector?t ? RM .
In this paper, we refer to ? as feature coefficients.
There has been an enormous amount of work on
online learning for sequential predictions, much of it
building on convex optimization. For a sequence of
loss functions `1, `2, . . . , `T (parameterized by ?),
an online learning algorithm is a strategy to minimize
the regret, with respect to the best fixed ?? in hind-
sight.1 Regret guarantees assume a Lipschitz con-
1Formally, the regret is defined as RegretT (??) =
dition on the loss function ` that can be prohibitive
for complex models. See Cesa-Bianchi and Lugosi
(2006), Rakhlin (2009), Bubeck (2011), and Shalev-
Shwartz (2012) for in-depth discussion and review.
There has also been work on online and stochastic
learning for Bayesian models (Sato, 2001; Honkela
and Valpola, 2003; Hoffman et al., 2013), based on
variational inference. The goal is to approximate pos-
terior distributions of latent variables when examples
arrive one at a time.
In this paper, we will use both kinds of techniques
to learn language models for streaming datasets.
2.2 Problem Formulation
Consider an online language modeling problem, in
the spirit of sequential predictions. The task is to
build a language model that accurately predicts the
texts generated on day t, conditioned on observ-
able features up to day t, x1:t. Every day, after
the model makes a prediction, the actual texts wt
are revealed and we suffer a loss. The loss is de-
fined as the negative log likelihood of the model
`t = ? log p(wt | ?,?1:t?1,x1:t?1,n1:t?1), where
? and ?1:T are the model parameters andn is a back-
ground distribution (details are given in ?3.2). We
can then update the model and proceed to day t+ 1.
Notice the similarity to the sequential prediction de-
scribed above. Importantly, this is a realistic setup for
building evolving language models from large-scale
streaming datasets.
3 Model
3.1 Notation
We index timesteps by t ? {1, . . . , T} and word
types by v ? {1, . . . , V }, both are always given as
subscripts. We denote vectors in boldface and use
1 : T as a shorthand for {1, 2, . . . , T}. We assume
words of the form {wt}Tt=1 for wt ? RV , which is
the vector of word frequences at timetstep t. Non-
linguistic context features are {xt}Tt=1 for xt ? RM .
The goal is to learn parameters ? and ?1:T , which
will be described in detail next.
3.2 Generative Story
The main idea of our model is illustrated by the fol-
lowing generative story for the unigram language
PT
t=1 `t(xt,?t, wt)? inf??
PT
t=1 `t(xt,??, wt).
182
model. (We will discuss the extension to higher-order
language models later.) A graphical representation
of our proposed model is given in Figure 1.
1. Draw feature coefficients ? ? N(0, ?I).2 Here
? is a vector in RM , where M is the dimension-
ality of the feature vector.
2. For each timestep t:
(a) Observe non-linguistic context features xt.
(b) Draw ?t ?
N
(?t?1
k=1 ?k
exp(?>f(xt,xk))Pt?1
j=1 ?j exp(?>f(xt,xj))
?k, ?I
)
.
Here, ?t is a vector in RV , where V is
the size of the word vocabulary, ? is
the variance parameter and ?k is a fixed
hyperparameter; we discuss them below.
(c) For each word wt,v, draw wt,v ?
Categorical
(
exp(n1:t?1,v+?t,v)P
j?V exp(n1:t?1,j+?t,j)
)
.
In the last step, ?t and n are mapped to the V -
dimensional simplex, forming a distribution over
words. n1:t?1 ? RV is a background (log) distri-
bution, inspired by a similar idea in Eisenstein et al.
(2011). In this paper, we set n1:t?1,v to be the log-
frequency of v up to time t? 1. We can interpret ?
as a time-dependent deviation from the background
log-frequencies that incorporates world-context. This
deviation comes in the form of a weighted average of
earlier deviation vectors.
The intuition behind the model is that the probabil-
ity of a word appearing at day t depends on the back-
ground log-frequencies, the deviation coefficients of
the word at previous timesteps ?1:t?1, and the sim-
ilarity of current conditions of the world (based on
observable features x) to previous timesteps through
f(xt,xk). That is, f is a function that takes d-
dimensional feature vectors at two timesteps xt and
xk and returns a similarity vector f(xt,xk) ? RM
(see ?6.1.1 for an example of f that we use in our
experiments). The similarity is parameterized by ?,
and decays over time with rate ?k. In this work, we
assume a fixed window size c (i.e., we consider c
most recent timesteps), so that ?1:t?c?1 = 0 and
?t?c:t?1 = 1. This allows up to cth order depen-
dencies.3 Setting ? this way allows us to bound the
2Feature coefficients ? can be also drawn from other distri-
butions such as ? ? Laplace(0, ?).
3In online Bayesian learning, it is known that forgetting
inaccurate estimates from earlier timesteps is important (Sato,
 
xtxsxrxq
wq wr ws wt
 t s r q
?
NrNq Ns Nt
T
Figure 1: Graphical representation of the model. The
subscript indices q, r, s are shorthands for the previ-
ous timesteps t ? 3, t ? 2, t ? 1. Only four timesteps
are shown here. There are arrows from previous
?t?4,?t?5, . . . ,?t?c to ?t, where c is the window size
as described in ?3.2. They are not shown here, for read-
ability.
number of past vectors ? that need to be kept in
memory. We set ?0 to 0.
Although the generative story described above
is for unigram language models, extensions can be
made to more complex models (e.g., mixture of un-
igrams, topic models, etc.) and to longer n-gram
contexts. In the case of topic models, the model
will be related to dynamic topic models (Blei and
Lafferty, 2006) augmented by context features, and
the learning procedure in ?4 can be used to perform
online learning of dynamic topic models. However,
our model captures longer-range dependencies than
dynamic topic models, and can condition on non-
linguistic features or metadata. In the case of higher-
order n-grams, one simple way is to draw more ?,
one for each history. For example, for a bigram
model, ? is in RV 2 , rather than RV in the unigram
model. We consider both unigram and bigram lan-
guage models in our experiments in ?6. However, the
main idea presented in this paper is largely indepen-
dent of the base model.
Related work. Mimno and McCallum (2008) and
Eisenstein et al. (2010) similarly conditioned text on
2001; Honkela and Valpola, 2003). Since we set ?1:t?c?1 = 0,
at every timestep t, ?k leads to forgetting older examples.
183
observable features (e.g., author, publication venue,
geography, and other document-level metadata), but
conducted inference in a batch setting, thus their ap-
proaches are not suitable for streaming data. It is not
immediately clear how to generalize their approach to
dynamic settings. Algorithmically, our work comes
closest to the online dynamic topic model of Iwata
et al. (2010), except that we also incorporate context
features.
4 Learning and Inference
The goal of the learning procedure is to minimize the
overall negative log likelihood,
? logL(D) =
? log
?
d?1:T p(?1:T | ?,x1:T )p(w1:T | ?1:T ,n).
However, this quantity is intractable. Instead, we
derive an upper bound for this quantity and minimize
that upper bound. Using Jensen?s inequality, the vari-
ational upper bound on the negative log likelihood
is:
? logL(D) ? ?
?
d?1:T q(?1:T | ?1:T ) (4)
log p(?1:T | ?,x1:T )p(w1:T | ?1:T ,n)q(?1:T | ?1:T )
.
Specifically, we use mean-field variational inference
where the variables in the variational distribution q
are completely independent. We use Gaussian distri-
butions as our variational distributions for ?, denoted
by ? in the bound in Eq. 4. We denote the parameters
of the Gaussian variational distribution for ?t,v (word
v at timestep t) by ?t,v (mean) and ?t,v (variance).
Figure 2 shows the functional form of the varia-
tional bound that we seek to minimize, denoted by B?.
The two main steps in the optimization of the bound
are inferring ?t and updating feature coefficients ?.
We next describe each step in detail.
4.1 Learning
The goal of the learning procedure is to minimize the
upper bound in Figure 2 with respect to ?. However,
since the data arrives in an online fashion, and speed
is very important for processing streaming datasets,
the model needs to be updated at every timestep t (in
our experiments, daily).
Notice that at timestep t, we only have access
to x1:t and w1:t, and we perform learning at every
timestep after the text for the current timestep wt
is revealed. We do not know xt+1:T and wt+1:T .
Nonetheless, we want to update our model so that
it can make a better prediction at t + 1. Therefore,
we can only minimize the bound until timestep t.
Let Ck , exp(?>f(xt,xk))Pt?1
j=t?c exp(?>f(xt,xj))
. Our learning al-
gorithm is a variational Expectation-Maximization
algorithm (Wainwright and Jordan, 2008).
E-step Recall that we use variational inference and
the variational parameters for ? are ? and ?. As
shown in Figure 2, since the log-sum-exp in the last
term of B is problematic, we introduce additional
variational parameters ? to simplify B and obtain
B? (Eqs. 2?3). The E-step deals with all the local
variables ?, ?, and ?.
Fixing other variables and taking the derivative
of the bound B? w.r.t. ?t and setting it to zero,
we obtain the closed-form update for ?t: ?t =?
v?V exp (n1:t?1,v) exp
(
?t,v + ?t,v2
).
To minimize with respect to ?t and ?t, we apply
gradient-based methods since there are no closed-
form solutions. The derivative w.r.t. ?t,v is:
?B?
??t,v
=?t,v ? Ck?k,v?
? nt,v +
nt
?t
exp (n1:t?1,v) exp
(
?t,v +
?t,v
2
)
,
where nt =?v?V nt,v.
The derivative w.r.t. ?t,v is:
?B?
??t,v
= 12?t,v
+ 12? +
nt
2?t
exp (n1:t?1,v) exp
(
?t,v +
?t,v
2
)
.
Although we require iterative methods in the E-step,
we find it to be reasonably fast in practice.4 Specifi-
cally, we use the L-BFGS quasi-Newton algorithm
(Liu and Nocedal, 1989).
We can further improve the bound by updating
the variational parameters for timestep 1 : t? 1, i.e.,
?1:t?1 and?1:t?1, as well. However, this will require
storing the texts from previous timesteps. Addition-
ally, this will complicate the M-step update described
4Approximately 16.5 seconds/day (walltime) to learn the
model on the EN:NA dataset on a 2.40GHz CPU with 24GB
memory.
184
B =?
T?
t=1
Eq[log p(?t | ?k,?,xt)]?
T?
t=1
Eq[log p(wt | ?t,nt)]?H(q) (1)
=
T?
t=1
?
??
??
1
2
?
j?V
log ?t,j? ? Eq
?
???
(
?t ?
?t?1
k=t?c Ck?k
)2
2?
?
??? Eq
?
??
v?wt
n1:t?1,v + ?t,v ? log
?
j?V
exp(n1:t?1,j + ?t,j)
?
?
?
??
??
(2)
?
T?
t=1
?
??
??
1
2
?
j?V
log ?t,v? +
(
?t ?
?t?1
k=t?c Ck?k
)2
2? +
?t +
?t?1
k=t?c C2k?k
2?
?
?
v?wt
?
??t,v ? log ?t ?
1
?t
?
j?V
exp (n1:t?1,j) exp
(
?t,j +
?t,j
2
)
?
?
?
??
??
+ const (3)
Figure 2: The variational bound that we seek to minimize, B. H(q) is the entropy of the variational distribution q. The
derivation from line 1 to line 2 is done by replacing the probability distributions p(?t | ?k,?,xt) and p(wt | ?t,nt)
by their respective functional forms. Notice that in line 3 we compute the expectations under the variational distributions
and further bound B by introducing additional variational parameters ? using Jensen?s inequality on the log-sum-exp in
the last term. We denote the new bound B?.
below. Therefore, for each s < t, we choose to fix
?s and ?s once they are learned at timestep s.
M-step In the M-step, we update the global pa-
rameter ?, fixing ?1:t. Fixing other parameters and
taking the derivative of B? w.r.t. ?, we obtain:5
?B?
?? =
(?t ?
?t?1
k=t?cCk?k)(?
?t?1
k=t?c
?Ck
?? )
?
+
?t?1
k=t?cCk?k ?Ck??
? ,
where:
?Ck
?? =Ckf(xt,xk)
?Ck
?t?1
s=t?c f(xt,xs) exp(?>f(xt,xs))?t?1
s=t?c exp(?>f(xt,xs))
.
We follow the convex optimization strategy and sim-
ply perform a stochastic gradient update: ?t+1 =
?t + ?t ?B???t (Zinkevich, 2003). While the variational
bound B? is not convex, given the local variables ?1:t
5In our implementation, we augment ? with a squared L2
regularization term (i.e., we assume that ? is drawn from a
normal distribution with mean zero and variance ?) and use the
FOBOS algorithm (Duchi and Singer, 2009). The derivative
of the regularization term is simple and is not shown here. Of
course, other regularizers (e.g., the L1-norm, which we use for
other parameters, or the L1/?-norm) can also be explored.
and ?1:t, optimizing ? at timestep t without know-
ing the future becomes a convex problem.6 Since
we do not reestimate ?1:t?1 and ?1:t?1 in the E-step,
the choice to perform online gradient descent instead
of iteratively performing batch optimization at every
timestep is theoretically justified.
Notice that our overall learning procedure is still
to minimize the variational upper bound B?. All these
choices are made to make the model suitable for
learning in real time from large streaming datasets.
Preliminary experiments showed that performing
more than one EM iteration per day does not consid-
erably improve performance, so in our experiments
we perform one EM iteration per day.
To learn the parameters of the model, we rely on
approximations and optimize an upper bound B?. We
have opted for this approach over alternatives (such
as MCMC methods) because of our interest in the
online, large-data setting. Our experiments show that
we are still able to learn reasonable parameter esti-
mates by optimizing B?. Like online variational meth-
ods for other latent-variable models such as LDA
(Sato, 2001; Hoffman et al., 2013), open questions re-
main about the tightness of such approximations and
the identifiability of model parameters. We note, how-
6As a result, our algorithm is Hannan consistent w.r.t. the
best fixed ? (for B?) in hindsight; i.e., the average regret goes to
zero as T goes to?.
185
ever, that our model does not include latent mixtures
of topics and may be generally easier to estimate.
5 Prediction
As described in ?2.2, our model is evaluated by the
loss suffered at every timestep, where the loss is
defined as the negative log likelihood of the model
on text at timestep wt. Therefore, at each timestep t,
we need to predict (the distribution of)wt. In order
to do this, for each word v ? V , we simply compute
the deviation means ?t,v as weighted combinations
of previous means, where the weights are determined
by the world-context similarity encoded in x:
Eq[?t,v | ?t,v] =
t?1?
k=t?c
exp(?>f(xt,xk))?t?1
j=t?c exp(?>f(xt,xj))
?k,v.
Recall that the word distribution that we use for
prediction is obtained by applying the operator pi
that maps ?t and n to the V -dimensional simplex,
forming a distribution over words: pi(?t,n1:t?1)v =
exp(n1:t?1,v+?t,v)P
j?V exp(n1:t?1,j+?t,j)
, where n1:t?1,v ? RV is a
background distribution (the log-frequency of word
v observed up to time t? 1).
6 Experiments
In our experiments, we consider the problem of pre-
dicting economy-related text appearing in news and
microblogs, based on observable features that reflect
current economic conditions in the world at a given
time. In the following, we describe our dataset in de-
tail, then show experimental results on text prediction.
In all experiments, we set the window size c = 7 (one
week) or c = 14 (two weeks), ? = 12|V | (V is thesize of vocabulary of the dataset under consideration),
and ? = 1.
6.1 Dataset
Our data contains metadata and text corpora. The
metadata is used as our features, whereas the text
corpora are used for learning language models and
predictions. The dataset (excluding Twitter) can
be downloaded at http://www.ark.cs.cmu.
edu/DynamicLM.
6.1.1 Metadata
We use end-of-day stock prices gathered from
finance.yahoo.com for each stock included in
the Standard & Poor?s 500 index (S&P 500). The
index includes large (by market value) companies
listed on US stock exchanges.7 We calculate daily
(continuously compounded) returns for each stock, o:
ro,t = logPo,t? logPo,t?1, where Po,t is the closing
stock price.8 We make a simplifying assumption that
text for day t is generated after Po,t is observed.9
In general, stocks trade Monday to Friday (except
for federal holidays and natural disasters). For days
when stocks do not trade, we set ro,t = 0 for all
stocks since any price change is not observed.
We transform returns into similarity values as fol-
lows: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k)
and 0 otherwise. While this limits the model by ig-
noring the magnitude of price changes, it is still rea-
sonable to capture the similarity between two days.10
There are 500 stocks in the S&P 500, so xt ? R500
and f(xt,xk) ? R500.
6.1.2 Text data
We have five streams of text data. The first four
corpora are news streams tracked through Reuters.11
Two of them are written in English, North American
Business Report (EN:NA) and Japanese Investment
News (EN:JP). The remaining two are German Eco-
nomic News Service (DE, in German) and French
Economic News Service (FR, in French). For all four
of the Reuters streams, we collected news data over
a period of thirteen months (392 days), 2012-05-26
to 2013-06-21. See Table 1 for descriptive statistics
of these datasets. Numerical terms are mapped to a
single word, and all letters are downcased.
The last text stream comes from the Deca-
hose/Gardenhose stream from Twitter. We collected
public tweets that contain ticker symbols (i.e., sym-
bols that are used to denote stocks of a particular
company in a stock market), preceded by the dollar
7For a list of companies listed in the S&P 500 as of
2012, see http://en.wikipedia.org/wiki/List_
of_S\%26P_500_companies. This set was fixed during
the time periods of all our experiments.
8We use the ?adjusted close? on Yahoo that includes interim
dividend cash flows and also adjusts for ?splits? (changes in the
number of outstanding shares).
9This is done in order to avoid having to deal with hourly
timesteps. In addition, intraday price data is only available
through commercial data provided.
10Note that daily stock returns are equally likely to be positive
or negative and display little serial correlation.
11http://www.reuters.com
186
Dataset Total # Doc. Avg. # Doc. #Days Unigrams BigramsTotal # Tokens Size Vocab. Total # Tokens Size Vocab.
EN:NA 86,683 223 392 28,265,550 10,000 11,804,201 5,000
EN:JP 70.807 182 392 16,026,380 10,000 7,047,095 5,000
FR 62,355 160 392 11,942,271 10,000 3,773,517 5,000
DE 51,515 132 392 9,027,823 10,000 3,499,965 5,000
Twitter 214,794 336 639 1,660,874 10,000 551,768 5,000
Table 1: Statistics about the datasets. Average number of documents (third column) is per day.
sign $ (e.g., $GOOG, $MSFT, $AAPL, etc.). These
tags are generally used to indicate tweets about the
stock market. We look at tweets from the period
2011-01-01 to 2012-09-30 (639 days). As a result,
we have approximately 100?800 tweets per day. We
tokenized the tweets using the CMU ARK TweetNLP
tools,12 numerical terms are mapped to a single word,
and all letters are downcased.
We perform two experiments using unigram and
bigram language models as the base models. For
each dataset, we consider the top 10,000 unigrams
after removing corpus-specific stopwords (the top
100 words with highest frequencies). For the bigram
experiments, we only use 5,000 words to limit the
number of unique bigrams so that we can simulate
experiments for the entire time horizon in a reason-
able amount of time. In standard open-vocabulary
language modeling experiments, the treatment of un-
known words deserves care. We have opted for a
controlled, closed-vocabulary experiment, since stan-
dard smoothing techniques will almost surely interact
with temporal dynamics and context in interesting
ways that are out of scope in the present work.
6.2 Baselines
Since this is a forecasting task, at each timestep, we
only have access to data from previous timesteps.
Our model assumes that all words in all documents
in a corpus come from a single multinomial distri-
bution. Therefore, we compare our approach to the
corresponding base models (standard unigram and bi-
gram language models) over the same vocabulary (for
each stream). The first one maintains counts of every
word and updates the counts at each timestep. This
corresponds to a base model that uses all of the avail-
able data up to the current timestep (?base all?). The
second one replaces counts of every word with the
12https://www.ark.cs.cmu.edu/TweetNLP
counts from the previous timestep (?base one?). Ad-
ditionally, we also compare with a base model whose
counts decay exponentially (?base exp?). That is, the
counts from previous timesteps decay by exp(??s),
where s is the distance between previous timesteps
and the current timestep and ? is the decay constant.
We set the decay constant ? = 1. We put a symmetric
Dirichlet prior on the counts (?add-one? smoothing);
this is analogous to our treatment of the background
frequencies n in our model. Note that our model,
similar to ?base all,? uses all available data up to
timestep t? 1 when making predictions for timestep
t. The window size c only determines which previ-
ous timesteps? models can be chosen for making a
prediction today. The past models themselves are es-
timated from all available data up to their respective
timesteps.
We also compare with two strong baselines: a lin-
ear interpolation of ?base one? models for the past
week (?int. week?) and a linear interpolation of ?base
all? and ?base one? (?int one all?). The interpolation
weights are learned online using the normalized expo-
nentiated gradient algorithm (Kivinen and Warmuth,
1997), which has been shown to enjoy a stronger
regret guarantee compared to standard online gra-
dient descent for learning a convex combination of
weights.
6.3 Results
We evaluate the perplexity on unseen dataset to eval-
uate the performance of our model. Specifically, we
use per-word predictive perplexity:
perplexity = exp
(
?
?T
t=1 log p(wt | ?,x1:t,n1:t?1)?T
t=1
?
j?V wt,j
)
.
Note that the denominator is the number of tokens
up to timestep T . Lower perplexity is better.
Table 2 and Table 3 show the perplexity results for
187
Dataset base all base one base exp int. week int. one all c = 7 c = 14
EN:NA 3,341 3,677 3,486 3,403 3,271 3,262 3,285
EN:JP 2,802 3,212 2,750 2,949 2,708 2,656 2,689
FR 3,603 3,910 3,678 3,625 3,416 3,404 3,438
DE 3,789 4,199 3,979 3,926 3,634 3,649 3,687
Twitter 3,880 6,168 5,133 5,859 4,047 3,801 3,819
Table 2: Perplexity results for our five data streams in the unigram experiments. The base models in ?base all,? ?base
one,? and ?base exp? are unigram language models. ?int. week? is a linear interpolation of ?base one? from the past
week. ?int. one all? is a linear interpolation of ?base one? and ?base all?. The rightmost two columns are versions of
our model. Best results are highlighted in bold.
Dataset base all base one base exp int. week int. one all c = 7
EN:NA 242 2,229 1,880 2,200 244 223
EN:JP 185 2,101 1,726 2,050 189 167
FR 159 2,084 1,707 2,068 166 139
DE 268 2,634 2,267 2,644 282 243
Twitter 756 4,245 4,253 5,859 4,046 739
Table 3: Perplexity results for our five data streams in the bigram experiments. The base models in ?base all,? ?base
one,? and ?base exp? are bigram language models. ?int. week? is a linear interpolation of ?base one? from the past
week. ?int. one all? is a linear interpolation of ?base one? and ?base all?. The rightmost column is a version of our
model with c = 7. Best results are highlighted in bold.
each of the datasets for unigram and bigram experi-
ments respectively. Our model outperformed other
competing models in all cases but one. Recall that we
only define the similarity function of world context
as: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k) and
0 otherwise. A better similarity function (e.g., one
that takes into account market size of the company
and the magnitude of increase or decrease in the stock
price) might be able to improve the performance fur-
ther. We leave this for future work. Furthermore,
the variations can be captured using models from the
past week. We discuss why increasing c from 7 to 14
did not improve performance of the model in more
detail in ?6.4.
We can also see how the models performed over
time. Figure 4 traces perplexity for four Reuters news
stream datasets.13 We can see that in some cases the
performance of the ?base all? model degraded over
time, whereas our model is more robust to temporal
13In both experiments, in order to manage the time and space
complexities of updating ?, we apply a sparsity shrinkage tech-
nique by using OWL-QN (Andrew and Gao, 2007) when maxi-
mizing it, with regularization constant set to 1. Intuitively, this
is equivalent to encouraging the deviation vector to be sparse
(Eisenstein et al., 2011).
shifts.
In the bigram experiments, we only ran our model
with c = 7, since we need to maintain ? in RV 2 ,
instead of RV in the unigram model. The goal of
this experiment is to determine whether our method
still adds benefit to more expressive language mod-
els. Note that the weights of the linear interpolation
models are also learned in an online fashion since
there are no classical training, development, and test
sets in our setting. Since the ?base one? model per-
formed poorly in this experiment, the performance of
the interpolated models also suffered. For example,
the ?int. one all? model needed time to learn that the
?base one? model has to be downweighted (we started
with all interpolated models having uniform weights),
so it was not able to outperform even the ?base all?
model.
6.4 Analysis and Discussion
It should not be surprising that conditioning on
world-context reduces perplexity (Cover and Thomas,
1991). A key attraction of our model, we believe, lies
in the ability to inspect its parameters.
Deviation coefficients. Inspecting the model al-
lows us to gain insight into temporal trends. We
188
Twitter:Google
timestep
?
0 100 200 300 400 500 600
0.0
0.5
1.0
1.5
2.0
goog
@google
google+
#goog
r
GOOG
Twitter:Microsoft
timestep
?
0 100 200 300 400 500 600
0.0
0.5
1.0
1.5
microsoft
msft
#microsoft
r
MSFT
Figure 3: Deviation coefficients ? over time for Google- and Microsoft-related words on Twitter with unigram base
model (c = 7). Significant changes (increases or decreases) in the returns of Google and Microsoft stocks are usually
followed by increases in ? of related words.
investigate the deviations learned by our model on the
Twitter dataset. Examples are shown in Figure 3. The
left plot shows ? for four words related to Google:
goog, #goog, @google, google+. For compari-
son, we also show the return of Google stock for the
corresponding timestep (scaled by 50 and centered at
0.5 for readability, smoothed using loess (Cleveland,
1979), denoted by rGOOG in the plot). We can see
that significant changes of return of Google stocks
(e.g., the rGOOG spikes between timesteps 50?100,
150?200, 490?550 in the plot) occurred alongside
an increase in ? of Google-related words. Similar
trends can also be observed for Microsoft-related
words in the right plot. The most significant loss of
return of Microsoft stocks (the downward spike near
timestep 500 in the plot) is followed by a sudden
sharp increase in ? of the words #microsoft and
microsoft.
Feature coefficients. We can also inspect the
learned feature coefficients ? to investigate which
stocks have higher associations with the text that
is generated. Our feature coefficients are designed
to reflect which changes (or lack of changes) in
stock prices influence the word distribution more,
not which stocks are talked about more often. We
find that the feature coefficients do not correlate with
obvious company characteristics like market capi-
talization (firm size). For example, on the Twitter
dataset with bigram base models, the five stocks with
the highest weights are: ConAgra Foods Inc., Intel
Corp., Bristol-Myers Squibb, Frontier Communica-
tions Corp., and Amazon.com Inc. Strongly negative
weights tended to align with streams with less activ-
time lags
frequ
ency
0
20
40
60
80
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Figure 5: Distributions of the selection probabilities of
models from the previous c = 14 timesteps, on the EN:NA
dataset with unigram base model. For simplicity, we show
E-step modes. The histogram shows that the model tends
to favor models from days closer to the current date.
ity, suggesting that these were being used to smooth
across all c days of history. A higher weight for stock
o implies an increase in probability of choosing mod-
els from previous timesteps s, when the state of the
world for the current timestep t and timestep s is the
same (as represented by our similarity function) with
respect to stock o (all other things being equal), and
a decrease in probability for a lower weight.
Selected models. Besides feature coefficients, our
model captures temporal shift by modeling similar-
ity across the most recent c days. During inference,
our model weights different word distributions from
the past. The similarity is encoded in the pairwise
features f(xt,xk) and the parameters ?. Figure 5
shows the distributions of the strongest-posterior
models from previous timesteps, based on how far
189
EN:NA
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
EN:JP
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
FR
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
DE
timestep
perpl
exity
0 50 100 150 200 250 300 350
300
500
700
base all
complete
int. one all
Figure 4: Perplexity over time for four Reuters news streams (c = 7) with bigram base models.
190
in the past they are at the time of use, aggregated
across rounds on the EN:NA dataset, for window size
c = 14. It shows that the model tends to favor models
from days closer to the current date, with the t ? 1
models selected the most, perhaps because the state
of the world today is more similar to dates closer to
today compare to more distant dates. The plot also
explains why increasing c from 7 to 14 did not im-
prove performance of the model, since most of the
variation in our datasets can be captured with models
from the past week.
Topics. Latent topic variables have often figured
heavily in approaches to dynamic language model-
ing. In preliminary experiments incorporating single-
membership topic variables (i.e., each document be-
longs to a single topic, as in a mixture of unigrams),
we saw no benefit to perplexity. Incorporating top-
ics also increases computational cost, since we must
maintain and estimate one language model per topic,
per timestep. It is straightforward to design mod-
els that incorporate topics with single- or mixed-
membership as in LDA (Blei et al., 2003), an in-
teresting future direction.
Potential applications. Dynamic language models
like ours can be potentially useful in many applica-
tions, either as a standalone language model, e.g.,
predictive text input, whose performance may de-
pend on the temporal dimension; or as a component
in applications like machine translation or speech
recognition. Additionally, the model can be seen as
a step towards enhancing text understanding with
numerical, contextual data.
7 Conclusion
We presented a dynamic language model for stream-
ing datasets that allows conditioning on observable
real-world context variables, exemplified in our ex-
periments by stock market data. We showed how to
perform learning and inference in an online fashion
for this model. Our experiments showed the predic-
tive benefit of such conditioning and online learning
by comparing to similar models that ignore temporal
dimensions and observable variables that influence
the text.
Acknowledgements
The authors thank several anonymous reviewers for help-
ful feedback on earlier drafts of this paper and Brendan
O?Connor for help with collecting Twitter data. This re-
search was supported in part by Google, by computing
resources at the Pittsburgh Supercomputing Center, by
National Science Foundation grant IIS-1111142, AFOSR
grant FA95501010247, ONR grant N000140910758, and
by the Intelligence Advanced Research Projects Activ-
ity via Department of Interior National Business Center
contract number D12PC00347. The U.S. Government is
authorized to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright annotation
thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as nec-
essarily representing the official policies or endorsements,
either expressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable training
of l1-regularized log-linear models. In Proc. of ICML.
David M. Blei and John D. Lafferty. 2006. Dynamic topic
models. In Proc. of ICML.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Se?bastien Bubeck. 2011. Introduction to online opti-
mization. Technical report, Department of Operations
Research and Financial Engineering, Princeton Univer-
sity.
Nicolo` Cesa-Bianchi and Ga?bor Lugosi. 2006. Prediction,
Learning, and Games. Cambridge University Press.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American Statistical Association, 74(368):829?836.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of
Information Theory. John Wiley & Sons.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10(7):2899?
2934.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Proc. of
ICML.
Amit Goyal, Hal Daume III, and Suresh Venkatasubrama-
nian. 2009. Streaming for large scale NLP: Language
modeling. In Proc. of HLT-NAACL.
191
Matt Hoffman, David M. Blei, Chong Wang, and John
Paisley. 2013. Stochastic variational inference. Jour-
nal of Machine Learning Research, 14:1303?1347.
Antti Honkela and Harri Valpola. 2003. On-line varia-
tional Bayesian learning. In Proc. of ICA.
Tomoharu Iwata, Takeshi Yamada, Yasushi Sakurai, and
Naonori Ueda. 2010. Online multiscale dynamic topic
models. In Proc. of KDD.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Jyrki Kivinen and Manfred K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132:1?63.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Mining
of concurrent text and time series. In Proc. of KDD
Workshop on Text Mining.
Abby Levenberg and Miles Osborne. 2009. Stream-based
randomised language models for SMT. In Proc. of
EMNLP.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for sta-
tistical machine translation. In Proc. of HLT-NAACL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
David Mimno and Andrew McCallum. 2008. Topic mod-
els conditioned on arbitrary features with Dirichlet-
multinomial regression. In Proc. of UAI.
Alexander Rakhlin. 2009. Lecture notes on online learn-
ing. Technical report, Department of Statistics, The
Wharton School, University of Pennsylvania.
Masaaki Sato. 2001. Online model selection based on the
variational bayes. Neural Computation, 13(7):1649?
1681.
Shai Shalev-Shwartz. 2012. Online learning and online
convex optimization. Foundations and Trends in Ma-
chine Learning, 4(2):107?194.
Martin J. Wainwright and Michael I. Jordan. 2008. Graph-
ical models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1(1?2):1?305.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Proc.
of UAI.
Chong Wang, John Paisley, and David M. Blei. 2011. On-
line variational inference for the hierarchical Dirichlet
process. In Proc. of AISTATS.
Martin Zinkevich. 2003. Online convex programming
and generalized infinitesimal gradient ascent. In Proc.
of ICML.
192
