Example-based Machine Translation Based on Syntactic Transfer
with Statistical Models
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Eiichiro Sumita
ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, ?Keihanna Science City?
Kyoto, 619-0288, Japan
{kenji.imamura,hideo.okuma,taro.watanabe,eiichiro.sumita}@atr.jp
Abstract
This paper presents example-based machine
translation (MT) based on syntactic trans-
fer, which selects the best translation by us-
ing models of statistical machine translation.
Example-based MT sometimes generates in-
valid translations because it selects similar ex-
amples to the input sentence based only on
source language similarity. The method pro-
posed in this paper selects the best transla-
tion by using a language model and a trans-
lation model in the same manner as statisti-
cal MT, and it can improve MT quality over
that of ?pure? example-based MT. A feature
of this method is that the statistical models
are applied after word re-ordering is achieved
by syntactic transfer. This implies that MT
quality is maintained even when we only ap-
ply a lexicon model as the translation model.
In addition, translation speed is improved by
bottom-up generation, which utilizes the tree
structure that is output from the syntactic
transfer.
1 Introduction
In response to the ongoing expansion of bilingual
corpora, many machine translation (MT) meth-
ods have been proposed that automatically ac-
quire their knowledge or models from the cor-
pora. Recently, two major approaches to such ma-
chine translation have emerged: example-based
machine translation and statistical machine trans-
lation.
Example-based MT (Nagao, 1984) regards a
bilingual corpus as a database and retrieves exam-
ples that are similar to an input sentence. Then,
a translation is generated by modifying the tar-
get part of the examples while referring to trans-
lation dictionaries. Most example-based MT sys-
tems employ phrases or sentences as the unit for
examples, so they can translate while consider-
ing case relations or idiomatic expressions. How-
ever, when some examples conflict during re-
E =
J =
A =
NULL0 show1 me2 the3 one4 in5 the6 window7
uindo1 no2 shinamono3 o4 mise5 telidasai6
7 0 4 0 1 1( )
Figure 1: Example of Word Alignment between
English and Japanese (Watanabe and Sumita,
2003)
trieval, example-based MT selects the best exam-
ple scored by the similarity between the input and
the source part of the example. This implies that
example-based MT does not check whether the
translation of the given input sentence is correct
or not.
On the other hand, statistical MT employing
IBM models (Brown et al, 1993) translates an in-
put sentence by the combination of word transfer
and word re-ordering. Therefore, when it is ap-
plied to a language pair in which the word order is
quite different (e.g., English and Japanese, Figure
1), it becomes difficult to find a globally optimal
solution due to the enormous search space (Watan-
abe and Sumita, 2003).
Statistical MT could generate high-quality
translations if it succeeded in finding a globally
optimal solution. Therefore, the models employed
by statistical MT are superior indicators of the
quality of machine translation. Using this feature,
Akiba et al (2002) achieved selection of the best
translation among those output by multiple MT
engines.
This paper presents an example-based MT
method based on syntactic transfer, which selects
the best translation by using models of statisti-
cal MT. This method is roughly structured using
two modules (Figure 2). One is an example-based
syntactic transfer module. This module constructs
Input Sentence Output Sentence
Example-based
Syntactic Transfer
Thesaurus
Preprocessing Postprocessing
Statistical
Generation
Translation
Dictionary
Transfer
Rules
Translation
Model
Language
Model
Figure 2: Structure of Proposed Method
tree structures of the target language by parsing
and mapping the input sentence while referring to
transfer rules. The other is a statistical generation
module, which selects the best word sequence of
the target language in the same manner as statis-
tical MT. Therefore, this method is sequentially
combined example-based and statistical MT.
The proposed method has the following advan-
tages.
? From the viewpoint of example-based MT, the
quality of machine translation improves by se-
lecting the best translation not only from the
similarity judgment between the input sen-
tence and the source part of the examples but
also from the scoring of translation correctness
represented by the word transfer and word con-
nection.
? From the viewpoint of statistical MT, an ap-
propriate translation can be obtained even if
we use simple models because a global search
is applied after word re-ordering by syntac-
tic transfer. In addition, the search space
becomes smaller because the example-based
transfer generates syntactically correct candi-
dates for the most appropriate translation.
The rest of this paper is organized as follows:
Section 2 describes the example-based syntactic
transfer, Section 3 describes the statistical gen-
eration, Section 4 evaluates an experimental sys-
tem that uses this method, and Section 5 compares
other hybrid methods of example-based and statis-
tical MT.
2 Example-based Syntactic Transfer
The example-based syntactic transfer used in this
paper is a revised version of the Hierarchical
Phrase Alignment-based Translator (HPAT, re-
fer to (Imamura, 2002)). This section gives an
overview with an example of Japanese-to-English
machine translation.
2.1 Transfer Rules
Transfer rules are automatically acquired from
bilingual corpora by using hierarchical phrase
alignment (HPA; (Imamura, 2001)). HPA parses
bilingual sentences and acquires corresponding
syntactic nodes of the source and target sentences.
The transfer rules are created from their node cor-
respondences. Figure 3 shows an example of the
transfer rules. Variables, such as X and Y in Fig-
ure 3, denote non-terminal symbols that corre-
spond between source and target grammar. The
set of transfer rules is regarded as synchronized
context-free grammar.
The difference between this approach and con-
ventional synchronized context-free grammar is
that source examples are added to each transfer
rule. The source example is an instance (i.e., a
headword) of the variables that appeared in the
training corpora. For example, the source exam-
ple of Rule 1 in Figure 3 is obtained from a phrase
pair of the Japanese verb phrase ?furaito (flight)
wo yoyaku-suru (reserve)? and the English verb
phrase ?make a reservation for the flight.?
2.2 Syntactic Transfer Process
When an input sentence is given, the target tree
structure is constructed in the following three
steps.
1. The input sentence is parsed by using the
source grammar of the transfer rules.
2. The nodes in the source tree are mapped to the
target nodes by using transfer rules.
3. If non-terminal symbols remain in the leaves of
the target tree, candidates of translated words
are inserted by referring to the translation dic-
tionary.
An example of the syntactic transfer process is
shown in Figure 4 for the input sentence ?basu
wa 11 ji ni de masu (The bus will leave at 11
o?clock).? There are two points worthy of notice in
this figure. First, nodes in which the word order is
inverted are generated after transfer (cf. VP node
represented by a bold frame). Word re-ordering
is achieved by syntactic transfer. Second, words
No. Source Grammar Target Grammar Source Example
1 VP ? X
PP
Y
VP
? VP ? Y
VP
X
PP
((furaito (flight), yoyaku-suru (reserve)) ..)
2 VP ? Y
VP
X
ADVP
((soko (there), yuku (go)) ..)
3 VP ? Y
BEVP
X
NP
((hashi (bridge), aru (be)) ..)
4 S ? X
NP
wa Y
VP
masu ? S ? X
NP
Y
VP
((kare (he), enso-suru (play)) ..)
5 S ? X
NP
will Y
VP
((basu (bus), tomaru (stop)) ..)
Figure 3: Example of Transfer Rules
bus
bath
go
leave
start
11
NP -> a X3
NP -> the X3
NP -> X3
VP -> Y2 X2
VP -> X5 PP -> at X4
ADVP -> X4
NP -> X6 o?clock
NP -> X6
basu
(bus)
11
deru
(leave)
NP
X3
NP
X6 ji
                 (o?clock)
PP
X4 ni
VP
X5
VP
X2 Y2
S
X1 wa Y1 masu
X1
Y1
Y2 X2
Japanese English
S -> X1 will Y1
Figure 4: Example of Syntactic Transfer Process
(Bold frames are syntactic nodes mentioned in text)
that do not correspond between the source and tar-
get sentences (e.g., the determiner ?a? or ?the?)
are automatically inserted or eliminated by the tar-
get grammar (cf. NP node represented by a bold
frame). Namely, transfer rules work in a manner
similar to the functions of distortion, fertility, and
NULL in IBM models.
2.3 Usage of Source Examples
Example-based transfer utilizes the source exam-
ples for disambiguation of mapping and parsing.
Specifically, the semantic distance (Sumita and
Iida, 1991) is calculated between the source exam-
ples and the headwords of the input sentence, and
the transfer rules that contain the nearest exam-
ple are used to construct the target tree structure.
The semantic distance between words is defined
as the distance from the leaf node to the most spe-
cific common abstraction (MSCA) in a thesaurus
(Ohno and Hamanishi, 1984).
For example, if the input phrase ?ie (home) ni
kaeru (return)? is given, Rules 1 to 3 in Figure 3
are used for the syntactic transfer, and three target
nodes are generated without any disambiguation.
However, when we compare the source examples
with the headword of the variables X (ie) and Y
(kaeru), only Rule 2 is used for the transfer be-
cause the semantic distance of the example (soko
(there), yuku (go)) is the nearest. In the current
implementation, all rules that contain examples of
the same distance are used.
Consequently, example-based transfer achieves
translation while considering case relations or id-
iomatic expressions based on the semantic dis-
tance from the source examples.
3 Statistical Generation
3.1 Translation Model and Language Model
Statistical generation searches for the most ap-
propriate sequence of target words from the tar-
get tree output from the example-based syntactic
transfer. The most appropriate sequence is deter-
mined from the product of the translation model
and the language model in the same manner as sta-
tistical MT. In other words, when F and E denote
the channel target and channel source sequence,
respectively, the output word sequence E? that sat-
isfies the following equation is searched for.
E? = argmax
E
P (E|F )
= argmax
E
P (E)P (F |E). (1)
We only utilize the lexicon model as the trans-
lation model in this paper, similar to the models
proposed by Vogel et al (2003). Namely, when f
and e denote the channel target and channel source
word, respectively, the translation probability is
computed by the following equation.
P (F |E) =
?
j
?
i
t(f
j
|e
i
). (2)
The IBM models include other models, such
as fertility, NULL, and distortion models. As we
described in Section 2.2, the quality of machine
translation is maintained using only the lexicon
model because syntactical correctness is already
preserved by example-based transfer.
For the language model, we utilize a standard
word n-gram model.
3.2 Bottom-up Generation
We can construct word graphs by serializing the
target tree structure, which allows us to select the
best word sequence from the graphs. However,
the tree structure already shares nodes transferred
from the same input sub-sequence. The cost of
calculating probabilities is equivalent if we cal-
culate the probabilities while serializing the tree
structure. We call this method bottom-up genera-
tion in this paper.
Figure 5 shows a partial example of bottom-
up generation when the target tree in Figure 4
is given. For each node, word sub-sequences
and their probabilities (language and translation)
are obtained from child nodes. Then, the new
probabilities of the word sequence combination
are calculated, and the n-best sequences are se-
lected. These n-best sequences and their prob-
abilities are reused to calculate the probabilities
of parent nodes. When the translation probabil-
ity is calculated, the source word sub-sequence is
obtained by tracing transfer mapping, and the ap-
plied translation model is restricted to the source
sub-sequence. In other words, the translation
probability is locally calculated between the cor-
responding phrases.
Set Name Item English Japanese
Training # of Sentences 152,170
# of Words 886,708 1,007,484
Test # of Sentences 510
# of Words 2,973 3,340
Table 1: Corpus Size
When the generation reaches the top node, the
language probability is re-calculated with marks
for start-of-sentence and end-of-sentence, and the
n-best list is re-sorted. As a result, the translation
?The bus will leave at 11 o?clock? is obtained from
the tree of Figure 4.
Bottom-up generation calculates the probabili-
ties of shared nodes only once, so it effectively
uses tree information.
4 Evaluation
In order to evaluate the effect when models of sta-
tistical MT are integrated into example-based MT,
we compared various methods that changed the
statistical generation module.
4.1 Experimental Setting
Bilingual Corpus The corpus used in the fol-
lowing experiments is the Basic Travel Expression
Corpus (Takezawa et al, 2002; Kikui et al, 2003).
This is a collection of Japanese sentences and their
English translations based on expressions that are
usually found in phrasebooks for foreign tourists.
We divided it into subsets for training and testing
as shown in Table 1.
Transfer Rules Transfer rules were acquired
from the training set using hierarchical phrase
alignment, and low-frequency rules that appeared
less than twice were removed. The number of
rules was 24,310.
Translation Model and Language Model We
used a lexicon model of IBM Model 4 learned by
GIZA++ (Och and Ney, 2003) and word bigram
and trigram models learned by CMU-Cambridge
Statistical Language Modeling Toolkit (Clarkson
and Rosenfeld, 1997).
Compared Methods We compared the follow-
ing four methods.
? Baseline (Example-based Transfer only)
The best translation that had the same seman-
tic distance was randomly selected from the
the bus TM: -0.07LM: -1.94
bus TM: -0.07LM: -0.0
XNP n-best
n-best n-best
will
YVP
leave at 11 o?clock TM: -2.72LM: -4.58
start at 11 o?clock TM: -3.62LM: -4.17
leaves at 11 o?clock TM: -2.72LM: -3.11
YVP
leave TM: -1.88LM: -0.0
start TM: -2.78LM: -0.0
leaves TM: -1.88LM: -0.0
XPP
at 11 o?clock TM: -0.84LM: -2.79
at 11 TM: -4.91LM: -2.26
a bus TM: -0.07LM: -2.11
S
bus will start at 11 o?clock
the bus will leave at 11 o?clock
bus will leave at 11 o?clock TM: -7.13LM: -14.30
TM: -8.03
LM: -13.84
TM: -7.13
LM: -13.54
<s> </s>
Figure 5: Example of Bottom-up Generation
(TM and LM denote log probabilities of the translation and language models, respectively)
tree that was output from the example-based
transfer module. The translation words were
selected in advance as those having the highest
frequency in the training corpus. This is the
baseline for translating a sentence when using
only the example-based transfer.
? Bottom-up
The bottom-up generation selects the best
translation from the outputs of the example-
based transfer. We used a 100-best criterion
in this experiment.
? All Search
For all combinations that can be generated
from the outputs of the example-based trans-
fer, we calculated the translation and language
probabilities and selected the best translation.
Namely, a globally optimal solution was se-
lected when the search space was restricted by
the example-based transfer.
? LM Only
In the same way as All Search, the best trans-
lation was searched for, but only the language
model was used for calculating probabilities.
The purpose of this experiment is to measure
the influence of the translation model.
Evaluation Metrics From the test set, 510 sen-
tences were evaluated by the following automatic
and subjective evaluation metrics. The number
of reference translations for automatic evaluation
was 16 per sentence.
BLEU: Automatic evaluation by BLEU score
(Papineni et al, 2002).
NIST: Automatic evaluation by NIST score
(Doddington, 2002).
mWER: The mean rate by calculating the word
error rates between the MT results and all ref-
erence translations, where the lowest rate is se-
lected.
Subjective Evaluation: Subjective evaluation
by an English native speaker into the four ranks
of A: Perfect, B: Fair, C: Acceptable, and D:
Nonsense.
Automatic Evaluation Subjective Evaluation Translation Speed
Method BLEU NIST mWER A A+B A+B+C Mean Worst
(sec./sent.) (sec.)
Baseline 0.410 9.06 0.423 51.6% 64.3% 70.4% 0.180 10.82
Bottom-up 0.491 9.99 0.366 62.2% 72.5% 80.4% 0.211 5.03
All Search 0.498 10.04 0.353 62.9% 73.1% 80.8% 1.23 171.31
LM Only 0.491 9.11 0.385 57.6% 66.9% 72.0% 1.624 220.69
Table 2: MT Quality and Translation Speed vs. Generation Methods
4.2 Results
Table 2 shows the results of the MT quality and
translation speed among each method.
First, comparing the baseline with the statisti-
cal generations (Bottom-up and All Search), the
MT quality of statistical generation improved in
all evaluation metrics. Accordingly, the models of
statistical MT are effective for improving the MT
quality of example-based MT.
Next, comparing Bottom-up with All Search,
the MT quality of bottom-up generation was
slightly low. Bottom-up generation locally applies
the translation model to a partial tree. In other
words, the probability is calculated without word
alignment linked to the outside of the tree. This re-
sult indicates that the results of bottom-up genera-
tion are not equal to the global optimal solution.
Comparing LM Only with the statistical gener-
ations, the MT quality of ranks A+B+C by subjec-
tive evaluation significantly decreased. This is be-
cause the n-gram language model used here does
not consider output length, and shorter translations
are preferred. Although the language model was
effective to some degree, it could not evaluate the
equivalence of the translation and the input sen-
tence. Therefore, we concluded that the transla-
tion model is necessary for improving MT quality.
Finally, focusing on translation speed, the worst
time for Bottom-up generation was dramatically
faster than that for All Search. Bottom-up gen-
eration effectively uses shared nodes of the target
tree, so it can improve translation speed. There-
fore, bottom-up generation is suitable for tasks
that require real-time processing, such as spoken
dialogue translation.
5 Discussion
We incorporated example-based MT in models
of statistical MT. However, some methods to ob-
tain initial solutions of statistical MT by example-
based MT have already been proposed. For
example, Marcu (2001) proposed a method in
which initial translations are constructed by com-
bining bilingual phrases from translation mem-
ory, which is followed by modifying the transla-
tions by greedy decoding (Germann et al, 2001).
Watanabe and Sumita (2003) proposed a decoding
algorithm in which translations that are similar to
the input sentence are retrieved from bilingual cor-
pora and then modified by greedy decoding.
The difference between our method and these
methods involves whether modification is applied.
Our approach simply selects the best translation
from candidates that are output from example-
based MT. Even though example-based MT can
output appropriate translations to some degree,
our method assumes that the candidates contain
a globally optimal solution. This means that
the upper bound of MT quality is limited by the
example-based transfer, so we have to improve
this stage in order to further improve MT quality.
For instance, example-based MT can be improved
by applying an optimization algorithm that uses
an automatic evaluation of MT quality (Imamura
et al, 2003).
6 Conclusions
This paper demonstrated that example-based MT
can be improved by incorporating it in models of
statistical MT. The example-based MT used in this
paper is based on syntactic transfer, so word re-
ordering is achieved in the transfer module. Us-
ing this feature, the best translation was selected
by using only a lexicon model and an n-gram lan-
guage model. In addition, bottom-up generation
achieved faster translation speed by using the tree
structure of the target sentence.
Acknowledgements
The authors would like to thank Kadokawa Pub-
lishers, who permitted us to use the hierarchy of
Ruigo-shin-jiten.
The research reported here is supported in part
by a contract with the Telecommunications Ad-
vancement Organization of Japan entitled, ?A
study of speech dialogue translation technology
based on a large corpus.?
References
Yasuhiro Akiba, Taro Watanabe, and Eiichiro
Sumita. 2002. Using language and transla-
tion models to select the best among outputs
from multiple MT systems. In Proceedings of
COLING-2002, pages 8?14.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997.
Statistical language modeling using the CMU-
Cambridge toolkit. In Proceedings of Eu-
roSpeech 97, pages 2707?2710.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram
co-occurrence statistics. In Proceedings of the
HLT Conference, San Diego, California.
Ulrich Germann, Michael Jahr, Kevin Knight,
Daniel Marcu, and Kenji Yamada. 2001. Fast
decoding and optimal decoding for machine
translation. In Proceedings of 39th Annual
Meeting of the Association for Computational
Linguistics, pages 228?235.
Kenji Imamura, Eiichiro Sumita, and Yuji Mat-
sumoto. 2003. Feedback cleaning of machine
translation rules using automatic evaluation. In
Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL 2003), pages 447?454.
Kenji Imamura. 2001. Hierarchical phrase align-
ment harmonized with parsing. In Proceed-
ings of the 6th Natural Language Processing
Pacific Rim Symposium (NLPRS 2001), pages
377?384.
Kenji Imamura. 2002. Application of transla-
tion knowledge acquired by hierarchical phrase
alignment for pattern-based MT. In Proceed-
ings of the 9th Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-2002), pages 74?84.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki
Takezawa, and Seiichi Yamamoto. 2003. Cre-
ating corpora for speech-to-speech translation.
In Proceedings of EuroSpeech 2003, pages
381?384.
Daniel Marcu. 2001. Towards a unified approach
to memory- and statistical-based machine trans-
lation. In Proceedings of 39th Annual Meeting
of the Association for Computational Linguis-
tics, pages 386?393.
Makoto Nagao. 1984. A framework of mechani-
cal translation between Japanese and English by
analogy principle. In Artificial and Human In-
telligence, pages 173?180, Amsterdam: North-
Holland.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19?51.
Susumu Ohno and Masato Hamanishi. 1984.
Ruigo-Shin-Jiten. Kadokawa, Tokyo. in
Japanese.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for au-
tomatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 311?318.
Eiichiro Sumita and Hitoshi Iida. 1991. Experi-
ments and prospects of example-based machine
translation. In Proceedings of the 29th ACL,
pages 185?192.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki
Sugaya, Hirofumi Yamamoto, and Seiichi Ya-
mamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel con-
versations in the real world. In Proceedings
of the Third International Conference on Lan-
guage Resources and Evaluation (LREC 2002),
pages 147?152.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia
Tribble, Ashish Venugopal, Bing Zhao, and
Alex Waibel. 2003. The CMU statistical ma-
chine translation system. In Proceedings of the
9th Machine Translation Summit (MT Summit
IX), pages 402?409.
Taro Watanabe and Eiichiro Sumita. 2003.
Example-based decoding for statistical machine
translation. In Proceedings of Machine Trans-
lation Summit IX, pages 410?417.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 165?168
Manchester, August 2008
Multilingual Mobile-Phone Translation Services for World Travelers
Michael Paul, Hideo Okuma, Hirofumi Yamamoto, Eiichiro Sumita,
Shigeki Matsuda, Tohru Shimizu, Satoshi Nakamura
? NICT Spoken Language Communication Group
? ATR Spoken Language Communication Research Labs
Hikaridai 2-2-2, Keihanna Science City, 619-0288 Kyoto, Japan
Michael.Paul@nict.go.jp
Abstract
This demonstration introduces two new
multilingual translation services for mo-
bile phones. The first translation service
provides state-of-the-art text-to-text trans-
lations of Japanese as well as English con-
versational spoken language in the travel
domain into 17 languages using statistical
machine translation technologies trained
automatically from a large-scale multilin-
gual corpus. The second demonstration
is a speech translation service between
Japanese and English for real environ-
ments. It is based on distributed speech
recognition with noise suppression. Flexi-
ble interfaces between internal and exter-
nal speech translation resources ease the
portability of the system to other languages
and enable real-time location-free commu-
nication world-wide.
1 Introduction
Spoken language translation technologies attempt
to bridge the language barriers between people
with different native languages who each want
to engage in conversation by using their mother-
tongue. The importance of these technologies is
increasing due to increases in the number of op-
portunities for cross-language communication in
face-to-face conversation, especially in the domain
of tourism.
We demonstrate two multilingual translation
services for mobile phones that are built on corpus-
based speech recognition and translation technolo-
gies. These services enable smooth and location-
free communication in real environments covering
the major languages of most nations (see Figure 1).
c
?NICT/ATR, 2008. Licensed under the Creative
Commons Attribution-Noncommercial-Share Alike 3.0 Un-
ported license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
Figure 1: Global Language Coverage
The first multilingual translation service de-
scribed in this paper is a text-to-text translation
service that enables users to translate Japanese
and English conversational spoken language sen-
tences in the travel domain into 17 other languages.
The system?s core components consist of a mul-
tilingual, sentence-aligned spoken language cor-
pus covering 18 of the major world languages
and state-of-the-art statistical machine translation
(SMT) engines that are trained automatically from
this corpus covering 306 (=18x17) translation di-
rections. A graphical user-interface (GUI) allows
24x7 world-wide access to the translation service
(see Section 2).
The second multilingual translation service is an
extension of the text-based translation service that
additionally provides speech recognition capabil-
ities. This is the first commercial speech transla-
tion service in the world. The system is based on
distributed speech recognition and operates as fol-
lows: (1) front-end processing (noise suppression,
feature extraction, and feature parameter compres-
sion) is carried out on the mobile phone, (2) back-
end processing (recognition, translation) is done
on a server and (3) translation results are sent back
and displayed on the mobile phone (see Section 3).
2 Multilingual Text Translation Service
(MTTS)
The multilingual text translation service for mo-
bile phones can be accessed via ?http://atr-
langue.jp/smlt? or by using the QR code in Figure 2
that also illustrates the graphical user interface of
165
Figure 2: QR Code and GUI of MTTS
the translation service. Two different modes are
distinguished: (1) the multilingual mode where the
input is translated into all 17 languages simultane-
ously and the translation results are displayed side-
by-side and (2) the bilingual mode where a single
language out of 17 languages can be selected as the
target language of the Japanese or English input
text translation. The bilingual mode also features
back-translation functionality, i.e., a reverse trans-
lation of the generated translation output into the
source language, that enables immediate feedback
on the quality of the translation output. In order to
solve font problems of mobile phones, the trans-
lated sentences are rendered on the server side and
an image is sent and displayed in the mobile phone.
2.1 Multilingual Travel Conversation Corpus
The translation engines used for the translation ser-
vice are trained on the Basic Travel Expressions
Corpus (ATR-BTEC) which is a collection of sen-
tences that travel experts consider useful for people
Table 1: Language Characteristics
Language Order Segments Morphology
Arabic (ar) SVO phrase rich
Danish (da) SVO words medium
German (de) SVO words medium
English (en) SVO words poor
Spanish (es) SVO words medium
French (fr) SVO words medium
Indonesian (id) SVO words rich
Italian (it) SVO words medium
Japanese (ja) SOV none poor
Korean (ko) SOV phrase poor
Malay (ms) SVO words rich
Dutch (nl) SVO words medium
Portuguese (pt) SVO words medium
Brazilian (pt-b) SVO words medium
Portuguese
Russian (ru) SVO words rich
Thai (th) SVO none none
Vietnamese (vi) SVO phrase none
Chinese (zh) SVO none none
going abroad and cover a large variety of topics in
travel situations like shopping or stay (Kikui et al,
2006). The multilingual corpus consists of 160K
sentences for each of the 18 languages, aligned
at the sentence-level. The characteristics of all
ATR-BTEC corpus languages are summarized in
Table 1. These languages differ largely in word or-
der (SVO, SOV), segmentation unit (phrase, word,
none), and morphology (poor, medium, rich). Con-
cerning word segmentation, the corpora were pre-
processed using simple tokenization tools for all
European languages and language-specific word-
segmentation tools for languages like Chinese,
Japanese, Korean, or Thai that do not use white-
space to separate word/phrase tokens. All data sets
were lower-cased and punctuation marks were re-
moved.
2.2 Statistical Machine Translation Engines
Phrase-based statistical machine translation ap-
proaches continue to dominate the field of machine
translation. The translation service makes use of
state-of-the-art phrase-based SMT systems within
the framework of feature-based exponential mod-
els containing the following features:
? Phrase translation probability
? Inverse phrase translation probability
? Lexical weighting probability
? Inverse lexical weighting probability
? Phrase penalty
? Language model probability
? Simple distance-based distortion model
? Word penalty
166
Table 2: Language Model Perplexity
Lang Entropy Total Eval Data
uage Entropy Words Vocab
ar 5.73 21,663 3,780 1,067
da 5.66 17,411 3,077 884
de 5.58 16,698 2,995 910
en 4.53 14,370 3,169 807
es 5.35 15,622 2,919 943
fr 4.77 16,793 3,521 929
id 6.09 18,145 2,977 908
it 5.52 16,078 2,914 956
ja 4.03 15,080 3,745 929
ko 4.21 15,011 3,567 943
ms 6.43 19,144 2,977 909
nl 5.66 17,609 3,110 909
pt-b 5.73 16,981 2,962 932
pt 5.54 16,064 2,900 946
ru 6.20 16,040 2,587 1,143
th 5.12 20,230 3,953 738
vi 4.84 19,531 4,034 792
zh 5.11 14,748 2,887 944
The basic framework within which all the MT
systems were constructed is shown in Figure 3.
SourceL a n g ua g eI n p ut
T a rg etL a n g ua g eO ut p ut
D ecod i n g  A l g ori t h margmax P ( s rc | t rg) *  P ( t rg)
T ra n s l a t i onM od el s L a n g ua g eM od el s
st a t i st i c a la n a l y si s
P a ra l l elT ex t  C orp ora M on ol i n g ua lT ex t  C orp ora
Figure 3: SMT Framework
Translation examples from the respective bilin-
gual text corpus are aligned in order to extract
phrasal equivalences and to calculate the bilingual
feature probabilities. Monolingual features like the
language model probability are trained on mono-
lingual text corpora of the target language whereby
standard word alignment and language modeling
tools were used. For decoding, the CleopATRa
decoder (Finch et al, 2007), a multi-stack phrase-
based SMT decoder is used.
2.3 Evaluation
In order to get an idea of how difficult the trans-
lation tasks are, we trained standard 5gram lan-
guage models on 160K sentence pairs and eval-
uated the entropy and total entropy, i.e., the en-
tropy multiplied by word counts, of each language
on an evaluation data set of 510 sentences each.
Table 2 shows that the total entropy of European
Table 3: Automatic Evaluation Results
BLEU (%) METEOR (%)
en-* *-en ja-* *-ja en-* *-en ja-* *-ja
ar 18.21 51.01 13.03 46.09 40.90 69.01 37.52 58.02
da 59.70 70.90 45.94 55.34 75.08 82.56 64.41 65.83
de 56.48 69.25 41.99 59.20 74.01 81.48 63.69 69.61
en ? ? 61.56 68.53 ? ? 78.19 75.39
es 65.22 73.82 51.77 63.24 78.15 85.28 68.30 72.17
fr 64.69 71.04 52.36 63.16 79.28 83.05 71.14 72.82
id 48.35 59.69 40.59 57.24 66.82 75.83 62.33 69.00
it 56.80 70.43 43.45 60.77 72.41 82.96 62.35 70.70
ja 68.53 61.56 ? ? 75.39 78.19 ? ?
ko 37.00 58.82 69.96 85.10 57.89 75.92 83.25 89.73
ms 40.99 57.63 36.13 55.84 61.08 74.75 58.73 67.33
nl 57.46 72.85 41.43 59.70 75.88 84.52 63.42 72.19
pt-b 59.99 69.41 46.50 58.07 72.77 80.70 64.68 69.14
pt 62.81 70.25 48.24 59.20 75.65 83.32 67.38 68.32
ru 44.46 61.23 36.08 55.13 66.41 73.75 60.59 64.55
th 46.49 51.35 43.75 50.85 62.47 73.12 60.25 62.91
vi 55.18 57.42 50.86 55.07 71.04 73.98 68.67 70.81
zh 53.08 59.33 51.68 69.43 69.85 74.68 65.88 77.62
languages like Danish, German, English, Span-
ish, etc. does not differ much. Moreover, lan-
guages with phrasal segments and/or rich morphol-
ogy like Arabic, Malay, Russian or Vietnamese
have a high total entropy and thus can be expected
to be more difficult to translate. This is confirmed
by the translation experiments in which the eval-
uation data sets were translated using the servers
translation engines and the translation quality was
evaluated using the standard automatic evaluation
metrics BLEU (Papineni et al, 2002) and ME-
TEOR (Banerjee and Lavie, 2005) where scores
range between 0 (worst) and 1 (best). Besides Ko-
rean (single references only), all languages were
evaluated using 16 reference translations. The
evaluation results in Table 3 show that closely
related language pairs like Japanese-Korean or
Portuguese-Brazilian can be translated very accu-
rately, whereas translations into languages with
high total entropy are of lower quality.
3 Multilingual Speech Translation
Service (MSTS)
The speech translation service1 can be accessed via
?http://www.atr-trek.co.jp/contents html? or using
the QR code in Figure 4 that also illustrates the
graphical user interface of the translation service.
After connecting to the top page, the translation
service is activated by selecting the ?Translation?
option. In order to achieve robust speech recogni-
1The speech translation service for Japanese?English on
Docomo 905i mobile phones started November 2007.
167
Speak!
Figure 4: QR Code and GUI of MSTS
tion, the service features a push-to-talk function-
ality, i.e., the user (1) presses the key to start the
service (2) speaks freely into the integrated micro-
phone of the mobile phone, and (3) presses the key
again after the speech input is finished. Fast and
accurate front-end and back-end processing algo-
rithms enable high-speed speech translation of the
input. Both, the speech recognition results as well
as the translation results are sent back to and dis-
played on the mobile phone.
3.1 Multilingual Speech Corpus
Similar to the statistical machine translation ap-
proach introduced in Section 2.2, the speech recog-
nition components are based on large-sized mul-
tilingual speech corpora. For Japanese, speech
recordings of 4000 speakers were collected result-
ing on a total of 200 hours of speech. For English,
almost the same amount of speech data were col-
lected from 500 speakers in North America (300
speakers), the UK (100 speakers), and Australia
(100 speakers).
3.2 Distributed Speech Recognition
The speech interface is based on distributed speech
recognition (DSR) that is integrated as a client-
server architecture compatible with the ETSI ES
202 050 standards. The usage of Speech Trans-
lation Markup Language (STML) enables flexible
connections between internal and external speech
translation resources like speech recognition and
translation servers via a network. Figure 5 illus-
trates the architecture of the utilized DSR system.
The front-end processing includes noise suppres-
sion, feature extraction and feature parameter com-
pression and is carried out on the mobile phone.
The data stream is then sent via internet to the ap-
plication service provider (ASP) for back-end pro-
cessing, i.e. speech recognition and statistical ma-
chine translation. The recognition and translation
results are sent back to the mobile phone for dis-
play to the user.
Back-e n dN e t w o r kF r o n t -e n d
A p p l i cat i o n  S e r v i ce  P r o v i d e r  ( A S P )  A p p l i t i o n  S e r v i e  P r o v i d e r  ( A S P )  
ETSI ES 202  050c o m p a t i b l e  B i t -s t r e a mD a t a  ( 4 . 8 k b i t s / s )
Sp e e c h  R e c o g n i t i o n  a n d  Tr a n s l a t i o n  R e s u l t s
Speechr eco g n i t i o n L a n g u a g et r a n s l a t i o n
Figure 5: MSTS Architecture
4 Conclusion
This paper introduced the first commercial speech
translation service in the world. State-of-the-
art spoken language translation technologies (dis-
tributed speech recognition with noise suppres-
sion, multilingual statistical machine translation)
are implemented into a flexible client-server archi-
tecture that covers the major languages of most
countries and enables users to communicate in real
environments all over the world using their own
mobile phones.
5 Acknowledgments
This work is partly supported by the Grant-in-Aid
for Scientific Research (C) and the Special Coordi-
nation Funds for Promoting Science and Technol-
ogy of the Ministry of Education, Culture, Sports,
Science and Technology, Japan.
References
Banerjee, S. and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization, pages 65?72, Ann Arbor, Michigan.
Finch, A., E. Denoual, H. Okuma, M. Paul, H. Ya-
mamoto, K. Yasuda, R. Zhang, and E. Sumita.
2007. The NICT/ATR Speech Translation System
for IWSLT 2007. In Proc. of the IWSLT, pages 103?
110, Trento, Italy.
Kikui, G., S. Yamamoto, T. Takezawa, and E. Sumita.
2006. Comparative study on corpora for speech
translation. IEEE Transactions on Audio, Speech
and Language Processing, 14(5):1674?1682.
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th ACL, pages
311?318, Philadelphia, USA.
168
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 1?9,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Imposing Constraints from the Source Tree on ITG Constraints for SMT
Hirofumi Yamamoto?,??,??? Hideo Okuma?,??
?National Institute of Information and Communications Technology
/ 2-2-2 Hikaridai Seika-cho Soraku-gun Kyoto Japan
??ATR Spoken Language Communication Research Labs.
???Kinki University School of Sience and Engineering Department of Information
{hirofumi.yamamoto,hideo.okuma,eichiro.sumita}@nict.go.jp
Eiichiro Sumita?,??
Abstract
In current statistical machine translation
(SMT), erroneous word reordering is one of
the most serious problems. To resolve this
problem, many word-reordering constraint
techniques have been proposed. The inver-
sion transduction grammar (ITG) is one of
these constraints. In ITG constraints, target-
side word order is obtained by rotating nodes
of the source-side binary tree. In these node
rotations, the source binary tree instance is
not considered. Therefore, stronger con-
straints for word reordering can be obtained
by imposing further constraints derived from
the source tree on the ITG constraints. For
example, for the source word sequence { a
b c d }, ITG constraints allow a total of
twenty-two target word orderings. How-
ever, when the source binary tree instance ((a
b) (c d)) is given, our proposed ?imposing
source tree on ITG? (IST-ITG) constraints
allow only eight word orderings. The re-
duction in the number of word-order permu-
tations by our proposed stronger constraints
efficiently suppresses erroneous word order-
ings. In our experiments with IST-ITG using
the NIST MT08 English-to-Chinese transla-
tion track?s data, the proposed method re-
sulted in a 1.8-points improvement in char-
acter BLEU-4 (35.2 to 37.0) and a 6.2%
lower CER (74.1 to 67.9%) compared with
our baseline condition.
1 Introduction
Statistical methods are widely used for machine
translation. One of the popular statistical machine
translation paradigms is the phrase-based model
(PBSMT) (Marcu et al, 2002; Koehn et al, 2003;
Och et al, 2004). In PBSMT, errors in word re-
ordering, especially in global reordering, are one of
the most serious problems. Approaches used to re-
solve this problem are categorized into two types.
The first type is linguistically syntax-based. In this
approach, source (Quirk et al, 2005; Liu et al, 2006;
Huang et al, 2006), target (Yamada et al, 2000; Gal-
ley et al, 2006; Marcu et al, 2006), or both side
(Melamed 2004; Ding et al, 2005) tree structures
are used for model training. The second type is for-
mal constraints on word permutations. IBM con-
straints (Berger et al, 1996), lexical word reordering
model (Tillmann, 2004), and inversion transduction
grammar (ITG) constraints (Wu, 1995; Wu, 1997)
belong to this type of approach. Our approach is an
extension of ITG constraints and is a hybrid of the
first and second type of approach.
We propose ?imposing source tree on ITG? (IST-
ITG) constraints for directly introducing source sen-
tence structure into our set of constraints. In IST-
ITG, ITG constraints under the given source sen-
tence tree structure are used as stronger constraints
than the original ITG. For example, IST-ITG allows
only eight word orderings for a four-word sentence,
even though twenty-two word orderings are possible
with respect of in the original ITG constraints.
In Section 2, we present the proposed IST-ITG
for word-based translation. In Section 3, the pro-
posed method is extended to phrase-based transla-
tion. In Section 4, we present a real-time decoding
algorithm for IST-ITG constraints. In Section 5, we
give details of the experiments and present the re-
sults. Finally, in Section 6, we offer a summary and
some concluding remarks.
1
2 Imposing the Source Tree on ITG
Constraints
First, we introduce three previous studies on word
reordering constraints: IBM constraints; lexical re-
ordering model; and ITG constraints. Here, we con-
sider one-to-one word-aligned source and target lan-
guage sentence pairs as the simplest cases.
2.1 IBM constraints
In this constraint, a distortion penalty is given in ac-
cordance with the gap between the previously and
the currently translated words, which is represented
as the following equation.
pD = exp(?
?
i
di) (1)
where di for each i is defined as:
di = abs(position(ei?1) + 1 ? position(ei)) (2)
where ei represents the translated word from the ith
source word fi, position(w) represents the position
of the word w. Sometimes, a limit is set for di
for similar language pairs such as French and En-
glish. However, for dissimilar language pairs, such
as Japanese and English or Chinese and English,
limiting di is not beneficial.
2.2 Lexical Reordering Model
In the lexical reordering model, reordering proba-
bilities are assigned to each word pair {fi, ei}. Re-
ordering positions are categorized into three types,
monotone, swap, and discontinuous. The probabil-
ity is assigned to left and right sides as ps(t|fi, ei),
where, s is left (l) or right (r), t is monotone (m),
swap (s), or discontinuous (d). Therefore, a total of
six probabilities are assigned to each word pair. For
the source word sub-sequence fi?1, fi, probabilities
of target sub-sequences are calculated as follows:
? p(ei?1, ei) = pr(m|fi?1, ei?1)pl(m|fi, ei)
? p(ei, ei?1) = pr(s|fi?1, ei?1)pl(s|fi, ei)
? p(otherwise) = pr(d|fi?1, ei?1)pl(d|fi, ei)
2.3 ITG Constraints
In one-to-one word-alignment, the source word fi is
translated into the target word ei. The source sen-
tence [f1, f2, ..., fN ] is translated into a reordered
sequence of word [e1, e2, ..., eN ]. The number of re-
orderings is N !. When ITG is introduced, this com-
bination N ! can be reduced in accordance with the
following constraints.
? All possible binary tree structures are generated
from the source word sequence.
? The target sentence is obtained by rotating any
node of the binary trees.
When N = 4, the ITG constraints can re-
duce the number of combinations from 4! =
24 to 22 by rejecting combinations [e3, e1, e4, e2]
and [e2, e4, e1, e3]. For a 4-word sentence,
the search space is reduced to 92%(22/24), but
for 10-word sentence, the search space is only
6%(206,098/3,628,800) of the original full space.
2.4 Imposing Source Tree Constraints
In ITG constraints, the source-side binary tree
instance is not considered. Therefore, if the
source sentence binary tree is utilized, stronger
constraints than the original ITG can be created.
By parsing the source sentence, a parse tree is
obtained. After parsing, a bracketed sentence is
obtained by removing the node labels, and this
bracketed sentence can be converted to a binary
tree. For example, the parse tree, (S1 (S (NP (DT
This)) (VP (AUX is) (NP (DT a) (NN pen))))),
is obtained from the source sentence ?This is a
pen?. By removing the node labels, a bracketed
sentence ((This) ((is) ((a) (pen)))) is obtained. Such
a bracketed sentence (equivalent to a binary tree)
can be used to produce constraints. If IST-ITG is
applied, the number of word orderings in N = 4 is
reduced to 8, down from 22 with ITG. For example,
for the source-side bracketed tree ((f1 f2)(f3 f4)),
eight target sequences [e1, e2, e3, e4], [e2, e1, e3, e4],
[e1, e2, e4, e3], [e2, e1, e4, e3], [e3, e4, e1, e2],
[e3, e4, e2, e1], [e4, e3, e1, e2], and [e4, e3, e2, e1]
are accepted. For the source-side bracketed tree
(((f1 f2)f3)f4), eight sequences [e1, e2, e3, e4],
[e2, e1, e3, e4], [e3, e1, e2, e4], [e3, e1, e2, e4],
[e4, e1, e2, e3], [e4, e2, e1, e3], [e4, e3, e1, e2], and
2
[e4, e3, e2, e1] are accepted. Generally, the number
of word orderings is reduced to 2N?1. Table 1
shows the number of word orderings in a target
word sequence for each N with ITG, IST-ITG, and
no constraints.
Table 1: Number of word orderings in each type of
constraint
N IST-ITG ITG No Constraint
1 1 1 1
2 2 2 2
3 4 6 6
4 8 22 24
5 16 90 120
6 32 394 720
7 64 1806 5040
8 128 8558 40320
9 256 41586 362880
10 512 206098 3628800
15 16384 745387038 1307674368000
2.5 Extension to Non-binary Tree
In the above subsection, a source binary tree was as-
sumed in order to perform IST-ITG. However, pars-
ing results sometimes are not binary trees. In this
case, some tree nodes have more than two branches.
For a non-binary node, any reordering of branches is
allowed. In a non-binary tree (f1(f2 f3 f4)), twelve
target-side sequences [e1, e2, e3, e4], [e1, e2, e4, e3],
[e1, e3, e2, e4], [e1, e3, e4, e2], [e1, e4, e2, e3],
[e1, e4, e3, e2], [e2, e3, e4, e1], [e2, e4, e3, e1],
[e3, e2, e4, e1], [e3, e4, e2, e1], [e4, e2, e3, e1], and
[e4, e3, e2, e1] are allowed. For nodes that have more
than three branches, the original ITG constraints
are locally applied. Therefore, for a non-binary tree
(f1(f2 f3 f4 f5)), 22 ? 2 = 44 word orderings are
allowed in the target-side and represented by the
following formula.
n
?
i=1
(SBi) (3)
where Sk represents the number of combinations
from the original ITG constraints for N = k and Bi
represents the number of branches at the ith node.
3 IST-ITG in Phrase-based SMT
In the above section, we described each constraint
in the case of a one-to-one word-alignment. In this
section, we consider phrase-based models. When
a phrase-based model is used, each constraint must
be extended. For IBM constraints, equation (2) is
rewritten using phrase Pen instead of word en as
follows:
di = abs(last position(Pei?1) + 1
?first position(Pei)) (4)
where last position(Pen) represents the posi-
tion of the last word in nth phrase, and
first position(Pen) represents the position of the
first word in nth phrase. The lexical reordering
model and ITG constraints can be extended by
changing the model (or constraint) unit from ?word?
to ?phrase?. However, in IST-ITG, ?word? must be
used for the constraint unit since the parse (brack-
eted tree) unit is in ?words?. To absorb different
units between translation models and IST-ITG con-
straints, we investigated a new limitation for word
ordering as follows.
? Word ordering that destroys a phrase is not al-
lowed.
When this limitation is applied, the translated word
ordering is obtained from the bracketed source sen-
tence tree by reordering the nodes in the tree, the
same as for one-to-one word-alignment. According
to this limitation, the following nodes cannot be re-
ordered. If a sub-tree with root node X includes part
of a phrase ph, node X cannot be reordered. Con-
sider the source bracketed source tree ( ( ea eb ec )
( ( ed ee ) ( ef eg ) ) ), in which eb ec, and ed form
a phrase eph as in Figure 1. Node 1 cannot be re-
ordered since part of the phrase eb ec is included in
node 1?s sub-tree. For the same reason, node 2 and 4
cannot be reordered. Node 3 can be reordered since
the sub-tree does not include the phrase (target se-
quence [fafphfefgff ] is obtained by rotating node
3). Node 5 also can be reordered since it includes
the whole phrase (target sequence [fgfffefphfa] is
obtained by rotating node 5). If node 2 is reordered,
phrase ph is split into two parts, and translated in
two parts in the target sentence. It is inconsistent
3
with the condition that phrase-to-phrase alignment
is one-to-one. As a result, only the target sequences
[fafphfefffg], [fafphfefgff ], [fgfffefphfa], and
[fffgfefphfa] are allowed. Here, fph represents an
equivalent phrase in the translation for eph.
e e ee
1 2 3
5
e
e e
4
e
a ph e f g
b c d
Figure 1: Example sentence tree with a phrase
4 Decoding with IST-ITG Constraints
In this section, we describe a one-pass decoding
algorithm that uses IST-ITG constraints in the de-
coder. The translation target sentence is sequen-
tially generated from left (sentence head) to right
(sentence tail). To introduce the IST-ITG constraints
into a decoder, the target candidate must be checked
whether it satisfies the IST-ITG constraints or not
whenever a new phrase is selected to extend a target
candidate.
To explain this checking algorithm, we catego-
rized source sub-trees into four types UNTRANS-
LATED, TRANSLATED, TRANSLATING, and
NG (no good) as follows:
? If a sub-tree consists of only leaf word nodes,
and all leaf words are not yet translated, this
sub-tree is defined as UNTRANSLATED.
? If a sub-tree consists of only UNTRANS-
LATED sub-trees, this sub-tree is also UN-
TRANSLATED.
? If a sub-tree consists of only leaf word nodes,
and all leaf words are already translated, this
sub-tree is defined as TRANSLATED.
? If a sub-tree consists of only TRANSLATED
sub-trees, this sub-tree is also TRANSLATED.
? If a sub-tree consists of only leaf word nodes
with both translated and untranslated words,
this sub-tree is defined as TRANSLATING.
? If a sub-tree consists of both TRANSLATED
and UNTRANSLATED sub-trees, this sub-
tree is TRANSLATING.
? If a sub-tree includes only one TRANSLAT-
ING sub-tree and any number (including zero)
of TRANSLATED and UNTRANSLATED
sub-trees, this sub-tree is TRANSLATING.
? If a sub-tree includes more than one TRANS-
LATING sub-tree, this sub-tree is NG.
? If a sub-tree includes NG sub-tree, this sub-tree
is also NG.
If a translation candidate includes TRANSLAT-
ING sub-tree t, t must become TRANSLATED
before anything else can happen. Given sub-tree
((ab)c), a is translated, b and c are not yet trans-
lated. In this case, b must be translated before c.
If c is translated before b, the target word order be-
comes ACB. This word order does not satisfy the
IST-ITG constraints. For the same reason, a can-
didate that includes an NG sub-tree does not satisfy
the IST-ITG constraints. The checking algorithm for
IST-ITG constraints is as follows.
1. For old translation candidates, the smallest
TRANSLATING sub-tree t and its untrans-
lated part u are calculated.
2. When a new target phrase fph is generated, the
source phrase eph and untranslated part u cal-
culated in above step are compared. If eph does
not include and is not included in u, the new
candidate is rejected. For example, in Figure
1, only source word ea is already translated.
The smallest TRANSLATING sub-tree is 1
and its untranslated part u is [ebec]. In this case,
phrases containing [eb], [ec], or [ebec] are ac-
cepted since these are included in u. Phrases
[ebeced] or [ebecedee] are also accepted since
these include u.
3. If a new candidate includes NG sub-trees, this
candidate is rejected.
4
5 Experiments
5.1 Evaluation Measures
We evaluated the proposed method using four eval-
uation measures, BLEU (Papineni et al, 2002),
NIST (Doddington 2002), WER(word error rate),
and PER(position independent word error rate). Be-
fore discussing the evaluation, the characteristics of
each one are analyzed.
? BLEU: This evaluation measure takes
into account middle range word order,
but does not take into account global
word order. When the translation result is
[w1, w2, ..., wj?1, X,wj+1, ..., wn] for refer-
ence translation [w1, w2, ..., wn], both WER
and BLEU scores will be high. For a transla-
tion result [wj+1, ..., wn, X,w1, w2, ..., wj?1],
the BLEU score will be the same as the
previous result since BLEU only takes into
account 4grams. However, the WER score will
be zero since global word positions are taken
into account. Therefore, the effectiveness of
the proposed method using BLEU is less than
that of using WER.
? NIST: This evaluation measure only takes into
account n-grams like BLEU. However, impor-
tance of higher order n-grams are less than
BLEU. Therefore, the effectiveness of the pro-
posed method using NIST will be less than that
of using BLEU.
? WER: This evaluation measure takes into ac-
count not only local but also global word or-
der, and is the most suitable for evaluating our
method.
? PER: With this evaluation measure, we are
almost incapable of considering word order.
Therefore, our proposed method would seem to
offer no improvement in this evaluation mea-
sure.
5.2 English and Japanese Patent Corpus
Experiments
First, we conducted experiments on English and
Japanese patent translations. Details of the experi-
mental corpus are shown in Table 2. This corpus is
created by automatic sentence alignment (Uchiyama
2003). The first nine hundred sentence pairs with the
best alignment scores were used as the evaluation
data (single reference) and the next thousand sen-
tence pairs were used as the development data. This
corpus is a subset of the training corpus that will be
used in the NTCIR-7 Workshop patent translation
track.
Table 2: E-J patent corpus
# of sent. Total words # of entries
E/J Train 1.8M 60M/64M 188K/118K
E/J Dev 916 30K/32K 4,072/3,646
E/J Eval 899 29K/32K 3,967/3,682
5.2.1 English-to-Japanese Translation
The translation direction of the first experiment
was English-to-Japanese (E-J). For phrase-based
translation model training, we used the GIZA++
toolkit (Och et al, 2003). For language model train-
ing, the SRI language model tool kit (Stolcke 2002)
was used. The language model type was word 5-
gram smoothed by Kneser-Ney discounting (Kneser
1995). For tuning of decoder parameters, we con-
ducted minimum error training (Och 2003) with re-
spect to the BLEU score using 916 development
sentence pairs. For extraction of source sentence
tree structure, we used the Charniak parser (Char-
niak 2000). We used Chasen for segmentation of the
Japanese. The numbers of entries in the language
models were 0.1 M, 2.1 M, 4.3 M, 6.2 M, and 6.9 M
for 1, 2, 3, 4, and 5grams respectively. The number
of entries in the phrase-table was 76 M. For decod-
ing, we used an in-house decoder that is a close rel-
ative to the Moses decoder. The performance of this
decoder was configured to be the same as Moses.
Another conditions are the same as the default con-
ditions of Moses decoder.
In the previous work (Zens et al, 2003, 2004),
an IBM constraints and an ITG constraints are com-
pared. In these experiments, a lexical reordering
model, the proposed IST-ITC, and combinations of
these are added as comparison targets. The combi-
nation of constraints in these experiments is as fol-
lows.
5
1. Monotone: Monotone translation (no reorder-
ing).
2. No constraints: There were no constraints for
word reordering. Any word order was allowed
without penalty.
3. IBM: IBM constraints without distortion limit.
4. ITG: ITG constraints.
5. IBM+ITG: Both IBM and ITG constraints were
used at the same time.
6. IBM+LR: Both IBM constraints and lexical re-
ordering model.
7. IST: Only the proposed IST-ITC constraints.
8. IBM+IST: Both IBM and IST-ITC constraints.
9. IBM+LR+IST: IBM constraints, Lexical re-
ordering model, and IST-ITG constraints were
used at the same time.
Table 3 shows the following experimental results.
In comparing the original ITG constraints (ITG)
with the proposed IST-ITG (IST) method, the im-
provement in BLEU was 2.67 points, and in WER
was 5.39%. WER had the largest improvement, next
was BLEU. This particular improvement order was
the same as in the previous subsection. The large im-
provement of WER helped us confirm the effective-
ness of the proposed method for global word order-
ing. When IBM constraints were used at the same
time (IBM+ITG and IBM+IST), the BLEU score
improved by 1.57 points and WER improved by
4.63%. When the lexical reordering model was used
at the same time (IBM+LR and IBM+LR+IST),
BLEU improved by 1.03 points and WER improved
by 5.12%. The lexical reordering model fixed phrase
position for the monotone and swap categories, but
did not fix phrase position for the discontinuous cat-
egory. IST-ITG fixed phrase position for the dis-
continuous category, even though it did not assign
a probability. Combinations of the lexical reorder-
ing model and IST-ITG resulted in a better WER
than with both IBM+LR and IBM+IST since both
position and probability could be assigned for the
discontinuous category.
Table 3: Evaluation results in E-J patent translation
BLEU NIST WER PER
Monotone 24.91 6.95 79.97 42.02
No constraint 26.83 7.19 81.10 39.52
IBM 28.35 7.29 78.35 39.25
ITG 27.59 7.26 80.29 39.15
IBM+ITG 28.50 7.30 78.01 39.29
IBM+LR 31.17 7.50 76.30 38.61
IST 30.26 7.41 74.90 38.93
IBM+IST 30.07 7.41 73.38 39.05
IBM+LR+IST 32.20 7.61 71.18 38.15
5.2.2 Japanese-to-English Translation
Next, we conducted J-E translation experiments
using the same corpus. The numbers of entries in
the language models were 0.2 M, 3.1 M, 4.1 M, 5.7
M, and 5.9 M for 1, 2, 3, 4, and 5grams The im-
provement. The number of entries in the phrase-
table was 76 M. For parsing of Japanese, we used
the dependency structure analyzer CaboCha. From
the dependency structure, Japanese bracketed trees
were generated. The combination of constraints in
these experiments was the same as those of the E-J
translation experiments.
Table 4 shows the translation results of sentence
evaluation with the top five alignment scores. In
comparing the original ITG constraints (ITG) with
the proposed IST-ITG (IST), BLEU was improved
by 1.21 points, and by in 3.81% in WER. The
largest improvement was in WER, and BLEU had
the next largest. This particular improvement order
of these evaluation measures was the same as that
of the E-J translation experiments. When IBM con-
straints were used at the same time (IBM+ITG and
IBM+IST), there was no improvement in BLEU, but
WER improved by 3.89%. When the lexical reorder-
ing model was used at the same time (IBM+LR and
IBM+LR+IST), there was also no improvement in
BLEU, but WER improved by 4.47%. One pos-
sible reason for the small (or no) improvement in
BLEU is the lower parsing accuracy of Japanese
compared with that of the English. However, better
the WER figure indicates that using IST-ITC con-
straints leads to better word order. In the Appendix,
differences in the translation results for the first five
6
evaluation sentences between IBM+LR (Baseline:)
and IBM+LR+IST (Proposed:) are shown.
Table 4: Evaluation results in J-E patent translation
BLEU NIST WER PER
Monotone 26.29 7.25 76.42 40.85
No constraint 26.20 7.18 81.41 40.76
IBM 27.87 7.34 78.16 39.94
ITG 27.01 7.24 80.43 40.50
IBM+ITG 28.16 7.35 78.04 40.07
IBM+LR 29.93 7.54 77.27 39.12
IST 28.32 7.31 76.62 40.67
IBM+IST 28.14 7.32 74.13 40.40
IBM+LR+IST 29.77 7.50 72.80 39.73
5.3 NIST MT08 English-to-Chinese
Translation Experiments
Next, we conducted English-to-Chinese (E-C) news-
paper translation experiments for different language
pairs. The training and evaluation corpora were used
in the NIST MT08 evaluation campaign English-to-
Chinese translation track. For the translation model
training, we used 6.2M bilingual sentences. For the
language model training, we used 20.1M sentences.
A development set with 1,664 sentences was used
as evaluation data in the Chinese-to-English transla-
tion track in the NIST MT07 evaluation campaign.
A single reference was used in the development
set. The evaluation set with 1,859 sentences is the
same as MT08?s evaluation data, with 4 references.
Model training and decoding conditions were the
same as those in the E-J experiments. In both base-
line and proposed condition, IBM constraints and
lexical reordering model were used at the same time.
Therefore, the baseline conditions correspond to the
IBM+LR condition in the J-E experiments, the pro-
posed conditions correspond to the IBM+LR+IST in
the J-E experiments.
The evaluation unit was both the Chinese char-
acter and word as defined by the PKU corpus. As
in the E-J experiments, the improvements in WER
and CER (character error rate) were large. The im-
provements in WER, CER, word BLEU, and charac-
ter BLEU were 5.3% (from 75.0% to 69.7%), 6.2%
(from 74.1% to 67.9%), 2.2-points (from 21.0 to
23.2), and 1.8-points (from 35.2 to 37.0) respec-
tively. We again demonstrated that the proposed
method is effective (especially in WER) for multi-
ple language pairs.
6 Conclusion
We proposed new word reordering constraints for
PBSMT using source tree structure. The proposed
IST-ITG constraints are extensions of the ITG con-
straints. In ITG constraints, the instance of the
source-side tree is not taken into account. On the
other hand, in IST-ITG constraints, the tree that
is obtained by source sentence parsing is imposed
on the decoding process. Therefore, IST-ITG con-
straints are stronger than those of the original ITG.
For example, for four-word source sentences, IST-
ITG constraints allow eight word orderings in a tar-
get sentence compared with twenty-two orderings
under the original ITG constraints. IST-ITG con-
straints can be applied to a common decoder to de-
termine a target sentence from one-pass without re-
scoring. In our E-J patent translation experiments,
the proposed method resulted in a 2.7-point im-
provement in BLEU and a 5.7% improvement in
WER compared with those of the original ITG con-
straints. In this paper we have argued the WER is
the most appropriate measure to gauge the effective-
ness of our approach since it gives importance to the
global word order. Our approach gave rise to con-
siderable gains in term of WER in all of our experi-
ments, indicating that a respectable improvement in
global word order was achieved. The improvement
could clearly be seen from visual inspection of the
output, a few examples of which are presented in
the following Appendix.
A Samples from the Translation of
Japanese Patent into English
A.1 Sentence 1
Source:
 
	

	
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 69?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Reordering Model Using Syntactic Information of a Source Tree for
Statistical Machine Translation
Kei Hashimoto?1, Hirohumi Yamamoto?2?3, Hideo Okuma?2?4,
Eiichiro Sumita?2?4, and Keiichi Tokuda?1?2
?1Nagoya Institute of Technology Department of Computer Science and Engineering
/ Gokiso-cho Syouwa-ku Nagoya-city Aichi Japan
?2National Institute of Information and Communications Technology
?3Kinki University School of Science and Engineering Department of Informaiton
?4ATR Spoken Language Communication Research Labs.
Abstract
This paper presents a reordering model us-
ing syntactic information of a source tree for
phrase-based statistical machine translation.
The proposed model is an extension of IST-
ITG (imposing source tree on inversion trans-
duction grammar) constraints. In the pro-
posed method, the target-side word order is
obtained by rotating nodes of the source-side
parse-tree. We modeled the node rotation,
monotone or swap, using word alignments
based on a training parallel corpus and source-
side parse-trees. The model efficiently sup-
presses erroneous target word orderings, espe-
cially global orderings. Furthermore, the pro-
posed method conducts a probabilistic evalu-
ation of target word reorderings. In English-
to-Japanese and English-to-Chinese transla-
tion experiments, the proposed method re-
sulted in a 0.49-point improvement (29.31 to
29.80) and a 0.33-point improvement (18.60
to 18.93) in word BLEU-4 compared with
IST-ITG constraints, respectively. This indi-
cates the validity of the proposed reordering
model.
1 Introduction
Statistical machine translation has been wiedely ap-
plied in many state-of-the-art translation systems. A
popular statistical machine translation paradigms is
the phrase-based model (Koehn et al, 2003; Och
and Ney, 2004). In phrase-based statistical ma-
chine translation, errors in word reordering, espe-
cially global reordering, are one of the most se-
rious problems. To resolve this problem, many
word-reordering constraint techniques have been
proposed. These techniques are categorized into
two types. The first type is linguistically syntax-
based. In this approach, tree structures for the source
(Quirk et al, 2005; Huang et al, 2006), target (Ya-
mada and Knight, 2000; Marcu et al, 2006), or both
(Melamed, 2004) are used for model training. The
second type is formal constraints on word permuta-
tions. IBM constraints (Berger et al, 1996), the lex-
ical word reordering model (Tillmann, 2004), and
inversion transduction grammar (ITG) constraints
(Wu, 1995; Wu, 1997) belong to this type of ap-
proach. For ITG constraints, the target-side word
order is obtained by rotating nodes of the source-
side binary tree. In these node rotations, the source
binary tree instance is not considered. Imposing
a source tree on ITG (IST-ITG) constraints (Ya-
mamoto et al, 2008) is an extension of ITG con-
straints and a hybrid of the first and second type of
approach. IST-ITG constraints directly introduce a
source sentence tree structure. Therefore, IST-ITG
can obtain stronger constraints for word reordering
than the original ITG constraints. For example, IST-
ITG constraints allows only eight word orderings for
a four-word sentence, even though twenty-two word
orderings are possible with respect to the original
ITG constraints. Although IST-ITG constraints ef-
ficiently suppress erroneous target word orderings,
the method cannot assign the probability to the tar-
get word orderings.
This paper presents a reordering model using syn-
tactic information of a source tree for phrase-based
statistical machine translation. The proposed re-
ordering model is an extension of IST-ITG con-
69
straints. In the proposed method, the target-side
word order is obtained by rotating nodes of a source-
side parse-tree in a similar fashion to IST-ITG con-
straints. We modeled the rotating positions, mono-
tone or swap, from word alignments of a training
parallel corpus and source-side parse-trees. The pro-
posed method conducts a probabilistic evaluation of
target word orderings using syntactic information of
the source tree.
The rest of this paper is organized as follows.
Section 2 describes the previous approach to re-
solving erroneous word reordering. In Section 3,
the reordering model using syntactic information of
a source tree is presented. Section 4 shows ex-
perimental results. Finally, Section 5 presnts the
summary and some concluding remarks and future
works.
2 Previous Works
First, we introduce two previous studies on related
word reordering constraints, ITG and IST-ITG con-
straints.
2.1 ITG Constraints
In one-to-one word-alignment, the source word fi
is translated into the target word ei. The source
sentence [f1, f2, ? ? ? , fN ] is translated into the tar-
get sentence which is the reordered target word se-
quence [e1, e2, ? ? ? , eN ]. The number of reorderings
is N !. When ITG constraints are introduced, this
combination N ! can be reduced in accordance with
the following constraints.
? All possible binary tree structures are generated
from the source word sequence.
? The target sentence is obtained by rotating any
node of the binary trees.
When N = 4, the ITG constraints can reduce
the number of combinations from 4! = 24 to
22 by rejecting the combinations [e3, e1, e4, e2]
and [e2, e4, e1, e3]. For a four-word sentence, the
search space is reduced to 92% (22/24), but for
a 10-word sentence, the search space is only 6%
(206,098/3,628,800) of the original full space.
2.2 IST-ITG Constraints
In ITG constraints, the source-side binary tree in-
stance is not considered. Therefore, if a source sen-
tence tree structure is utilized, stronger constraints
than the original ITG constraints can be created.
IST-ITG constraints directly introduce a source sen-
tence tree structure. The target sentence is obtained
with the following constraints.
? A source sentence tree structure is generated
from the source sentence.
? The target sentence is obtained by rotating any
node of the source sentence tree structure.
By parsing the source sentence, the parse-tree is
obtained. After parsing the source sentence, a
bracketed sentence is obtained by removing the
node syntactic labels; this bracketed sentence can
then be converted into a tree structure. For example,
the parse-tree ?(S1 (S (NP (DT This)) (VP (AUX
is) (NP (DT a) (NN pen)))))? is obtained from the
source sentence ?This is a pen,? which consists of
four words. By removing the node syntactic labels,
the bracketed sentence ?((This) ((is) ((a) (pen))))?
is obtained. Such a bracketed sentence can be used
to produce constraints. If IST-ITG constraints is
applied, the number of word orderings in N = 4
is reduced to 8, down from 22 with ITG cn-
straints. For example, for the source-side bracketed
tree ?((f1f2) (f3f4)),? the eight target sequences
[e1, e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3],
[e2, e1, e4, e3], [e3, e4, e1, e2], [e3, e4, e2, e1],
[e4, e3, e1, e2], and [e4, e3, e2, e1] are accepted. For
the source-side bracketed tree ?(((f1f2) f3) f4),?
the eight sequences [e1, e2, e3, e4], [e2, e1, e3, e4],
[e3, e1, e2, e4], [e3, e2, e1, e4], [e4, e1, e2, e3],
[e4, e2, e1, e3], [e4, e3, e1, e2], and [e4, e3, e2, e1] are
accepted. When the source sentence tree structure
is a binary tree, the number of word orderings is
reduced to 2N?1. The parsing results sometimes do
not produce binary trees. In this case, some subtrees
have more than two child nodes. For a non-binary
subtree, any reordering of child nodes is allowed. If
a subtree has three child nodes, six reorderings of
the nodes are accepted.
In phrase-based statistical machine translation, a
source ?phrase? is translated into a target ?phrase?.
However, with IST-ITG constraints, ?word? must be
70
used for the constraint unit since the parse unit is a
?word?. To absorb different units between transla-
tion models and IST-ITG constraints, a new limita-
tion for word reordering is applied.
? Word ordering that destroys a phrase is not al-
lowed.
When this limitation is applied, the translated word
ordering is obtained from the bracketed source sen-
tence tree by reordering the nodes in the tree, which
is the same as for one-to-one word-alignment.
3 Reordering Model Using Syntactic
Information of the Source Tree
In this section, we present a new reordering model
using syntactic information of a source-side parse-
tree.
3.1 Abstract of Proposed Method
The IST-ITG constraints method efficiently sup-
presses erroneous target word orderings. However,
IST-ITG constraints cannot evaluate the accuracy of
the target word orderings; i.e., IST-ITG constraints
assign an equal probability to all target word order-
ings. This paper proposes a reordering model us-
ing syntactic information of the source tree as an
extension of IST-ITG constraints. The proposed re-
ordering model conducts a probabilistic evaluation
of target word orderings using syntactic information
of the source-side parse-tree.
In the proposed method, the target-side word or-
der is obtained by rotating nodes of the source-
side parse-tree in a similar fashion to IST-ITG con-
straints. Reordering probabilities are assigned to
each subtree of source-side parse-tree S by reorder-
ing the positions into two types: monotone and
swap. If the subtree has more than two child nodes,
the number of child node order is more than two.
However, we assume the child node order other than
monotone to be swap. The source-side parse-tree
S consists of subtrees {s1, s2, ? ? ? , sK}, where K
is the number of subtrees included in the source-
side parse-tree. The subtree sk is which is repre-
sented by the parent node?s syntactic label and the
order, from sentence head to sentence tail, of the
child node?s syntactic labels. For example, Fig-
ure 1 shows a source-side parse-tree for a four-word
Source-side parse-tree
Source sentence
S
NP VP
NPAUX
DT NN
Figure 1: Example of a source-side parse-tree fo a four-
word source sentence consisting of three subtrees.
source sentence consisting of three subtrees. In Fig-
ure 1, the subtrees s1, s2, and s3 are represented by
S+NP+VP, VP+AUX+NP, and NP+DT+NN, re-
spectively. Each subtree has a probability P (t | sk),
where t is monotone (m) or swap (s). The proba-
bility of the target word reordering is calculated as
follows.
Pr =
K?
k=1
P (t | sk) (1)
Each target candidate is assigned the different re-
ordering probability by Equation (1). Since the pro-
posed reordering model uses the syntactic labels,
which is not considered in IST-ITG constraints, the
different parse-tree assigns the different reordering
probability. The proposed model is effective for
global word reordering, because reordering proba-
bilities are also assigned to higher-level subtrees of
the source-side parse-tree.
3.2 Training of the Proposed Model
We modeled monotone or swap node rotating auto-
matically from word alignments of a training paral-
lel corpus and source-side parse-trees. The training
algorithm for the proposed reordering model is as
follows.
1. The training process begins with a word-
aligned corpus. We obtained the word align-
ments using Koehn et al?s method (2003),
71
32
2,34
2,3,41
Figure 2: Example of a source-side parse-tree with word
alignments using the training algorithm of the proposed
model.
which is based on Och and Ney?s work (2004).
This involves running GIZA++ (Och and Ney,
2003) on the corpus in both directions, and ap-
plying refinement rules (the variant they desig-
nate is ?final-and?) to obtain a single many-to-
many word alignment for each sentence.
2. Source-side parse-trees are created using a
source language phrase structure parser, which
annotates each node with a syntactic label. A
source-side parse-tree consists of several sub-
trees with syntactic labels. For example, the
parse-tree ?(S1 (S (NP (DT This)) (VP (AUX
is) (NP (DT a) (NN pen)))))? is obtained from
the source sentence ?This is a pen? which con-
sists of four words.
3. Word alignments and source-side parse-trees
are combined. Leaf nodes are assigned target
word positions obtained from word alignments.
Via the bottom-up process, target word posi-
tions are assigned to all nodes. For example,
in Figure 2, the left-side (sentence head) child
node of subtree s2 is assigned the target word
position ?4,? and the right-side (sentence tail)
child node is assigned the target word positions
?2? and ?3,? which are assigned to the child
nodes of subtree s3.
4. The monotone and swap reordering positions
are checked and counted for each subtree. By
Subtree type Monotone probability
S+PP+,+NP+VP+. 0.764
PP+IN+NP 0.816
NP+DT+NN+NN 0.664
VP+AUX+VP 0.864
VP+VBN+PP 0.837
NP+NP+PP 0.805
NP+DT+JJ+NN 0.653
NP+DT+JJ+VBP+NN 0.412
NP+DT+NN+CC+VB 0.357
Table 1: Example of proposed reordering models.
comparing the target word positions, which are
assigned in the above step, the reordering posi-
tion is determined. If the target word position
of the left-side child node is smaller than one of
the right-side child node, the reordering posi-
tion determined as monotone. For example, in
Figure 2, the subtrees s1, s2 and s3 are mono-
tone, swap, and monotone, respectively.
5. The reordering probability of the subtree can
be directly estimated by counting the reorder-
ing positions in the training data.
P (t | s) = ct(s)?
t ct(s)
(2)
where ct(s) is the count of reordering positon t
included all training samples for the subtree s.
The parsing results sometimes do not produce bi-
nary trees. For a non-binary subtree, any reorder-
ing of child nodes is allowed. However, the pro-
posed reordering model assumes that reordering po-
sitions are only two, monotone and swap. That
is, the reordering position which the order of child
nodes do not change is monotone, and the other po-
sitions are swap. Therefore, the probability of swap
P (s | sk) is derived from the probability of mono-
tone P (m | sk) as follows.
P (s | sk) = 1.0 ? P (m | sk) (3)
Table 1 shows the example of proposed reordering
models.
If a subtree is represented by a binary-tree, there
are L3 possible subtrees, where L is the number of
72
Figure 3: Example of a target word order which is not
derived from rotating the nodes of source-side parse trees.
syntactic labels. However, in the possible subtrees,
there are subtrees observed only a few times in train-
ing sentences, especially when the subtree consists
of more than three child nodes. Although a large
number of subtree models can capture variations in
the training samples, too many models lead to the
over-fitting problem. Therefore, subtrees where the
number of training samples is less than a heuristic
threshold and unseen subtrees are clustered to deal
with the data sparseness problem for robust model
estimations.
After creating word alignments of a training par-
allel corpus, there are target word orders which are
not derived from rotating nodes of source-side parse-
trees. Figure 3 shows a sample which is not derived
from rotating nodes. Some are due to linguistic rea-
sons, structual differences such as negation (French
?ne...pas? and English ?not?), adverb, modal and so
on. Others are due to non-linguistic reasons, er-
rors of automatic word alignments, syntactic anal-
ysis, or human translation (Fox, 2002). The pro-
posed method discards such problematic cases. In
Figure 3, the subtree s1 is then removed from train-
ing samples, and the subtrees s2 and s3 are used as
training samples.
3.3 Decoding Using the Proposed Reordering
Model
In this section, we describe a one-pass phrase-based
decoding algorithm that uses the proposed reorder-
ing model in the decoder. The translation target sen-
tence is sequentially generated from left (sentence
Figure 4: Example of a target candidate including a
phrase.
head) to right (sentence tail), and all reordering is
conducted on the source side. To introduce the pro-
posed reordering model into the decoder, the target
candidate must be checked for whether the reorder-
ing position of a subtree is either monotone or swap
whenever a new phrase is selected to extend a target
candidate. The checking algorithm is as follows.
1. For old translation candidates, the subtree s,
which includes both translated and untranslated
words, and its untranslated part u are calcu-
lated.
2. When a new target phrase e? is generated, the
source phrase f? and the untranslated part u cal-
culated in the above step are compared. If the
source phrase f? does not include the untrans-
lated part u and is not included u, the new can-
didate is rejected.
3. In the accepted candidate, the reordering po-
sitions for all subtrees included the source side
parse-tree are checked by comparing the source
phrase f? with the source phrase sequence used
before.
Subtrees checked reordering positions are assigned a
probability?monotone or swap?by the proposed re-
ordering model, and the target word order is evalu-
ated by Equation (1).
Phrase-based statistical machine translation uses
a ?phrase? as the translation unit. However, the pro-
posed reordering model needs a ?word? order. Be-
cause ?word? alignments form the source phrase to
target phrase are not clear, we cannot determine the
73
Figure 5: Example of a non-binary subtree including a
phrase.
reordering position of subtree included in a phrase.
Therefore, in the decoding process using the pro-
posed reordering model, we define that higher prob-
ability, monotone or swap, are assigned to subtrees
included in a source phrase. For example, in Fig-
ure 4, the source sentence [[f1, f2], f3, f4] is trans-
lated into the target sentence [[e1, e2], e4, e3], where
[f1, f2] and [e1, e2] are used as phrases. Then, the
source phrase [f1, f2] includes the subtree s2. If the
monotone probabilities of subtrees s1, s2, and s3 are
0.8, 0.4 and 0.7, the proposed reordering probabil-
ity is 0.8 ? 0.6 ? 0.3 = 0.144. If a source phrase
is [f1, f2, f3, f4] and a source-side parse-tree has the
same tree structure used in Figure 4, the subtrees s1,
s2, and s3 are assigned higher reordering probabili-
ties. If the source phrase [f1, f2, f3, f4] used in Fig-
ure 4, the subtrees s1, s2, and s3 are assigned higher
reordering probabilities.
Non-binary subtrees are often observed in the
source-side parse-tree. When a source phrase f? is
included in a non-binary subtree and does not in-
clude a non-binary subtree, we cannot determine the
reordering position. For example, the reordering po-
sition of subtree s2 in Figure 5, which includes the
phrase [f3, f4], can not be determined. In this case,
we define that such subtrees are also to be assigned
a higher probability.
4 Experiments
To evaluate the proposed model, we conducted two
experiments: English-to-Japanese and English-to-
Chinese translation.
English Japanese
Train Sentences 1.0M
Words 24.6M 24.6M
Dev Sentences 2.0K
Words 50.1K 58.7K
Test Sentences 2.0K
Words 49.5K 58.0K
Table 2: Statistics of training, development and test cor-
pus for E-J translation.
4.1 English-to-Japanese Paper Abstract
Translation Experiments
The first experiment was the English-to-Japanese
(E-J) translation. Table 2 shows the training, de-
velopment and test corpus statistics. JST Japanese-
English paper abstract corpus consists of 1.0M
parallel sentences were used for model training.
This corpus was constructed from 2.0M Japanese-
English paper abstract corpus belongs to JST by
NICT using the method of Uchiyama and Isahara
(2007). For phrase-based translation model training,
we used the GIZA++ toolkit (Och and Ney, 2003),
and 1.0M bilingual sentences. For language model
training, we used the SRI language model toolkit
(Stolcke, 2002), and 1.0M sentences for the trans-
lation model training. The language model type was
word 5-gram smoothed by Kneser-Ney discounting
(Kneser and Ney, 1995). To tune the decoder pa-
rameters, we conducted minimum error rate training
(Och, 2003) with respect to the word BLEU score
(Papineni et al, 2002) using 2.0K development sen-
tence pairs. The test set with 2.0K sentences is used.
In the evaluation and development sets, a single ref-
erence was used. For the creation of English sen-
tence parse trees and segmentation of the English,
we used the Charniak parser (Charniak, 2000). We
used Chasen for segmentation of the Japanese sen-
tences. For decoding, we used an in-house decoder
that is a close relative of the Moses decoder. The
performance of this decoder was configured to be
the same as Moses. Other conditions were the same
as the default conditions of the Moses decoder.
In this experiment, the following three methods
were compared.
? Baseline : The IBM constraints and the lexi-
cal reordering model were used for target word
74
Baseline IST-ITG Proposed
BLEU 27.87 29.31 29.80
Table 3: BLEU score results for E-J translation. (1-
reference)
reordering.
? IST-ITG : The IST-ITG constraints, the IBM
constraints, and the lexical reordering model
were used for target word reordering.
? Proposed : The proposed reordering model,
the IBM constraints, and the lexical reordering
model were used for target word reordering.
During minimum error training, each method used
each reordering model and reordering constraint.
The proposed reordering model are trained from
1.0M bilingual sentences for the translation model
training. The amount of available training samples
represented by subtrees was 9.8M. In the available
training samples, there were 54K subtree types. The
heuristic threshold was 10, and subtrees with train-
ing samples of less than 10 were clustered. The pro-
posed reordering model consisted of 5,960 subtrees
types and one clustered model ?other?. The models
not including ?other? covered 99.29% of all training
samples.
The BLEU scores are presented in Table 3.
In comparing ?Baseline? method with ?IST-ITG?
method, the improvement in BLEU was a 1.44-
point. Furthermore, in comparing ?IST-ITG?
method with ?Proposed? method, the improvement
in BLEU was a 0.49-point. Both the IST-ITG con-
straints and the proposed reordering model fixed the
phrase position for the global reorderings. How-
ever, the proposed method can conduct a probabilis-
tic evaluation of target word reorderings which the
IST-ITG constraints cannot. Therefore, ?Proposed?
method resulted in a better BLEU.
4.2 NIST MT08 English-to-Chinese
Translation Experiments
Next, we conducted English-to-Chinese (E-C) news-
paper translation experiments for different lan-
guage pairs. The NIST MT08 evaluation campaign
English-to-Chinese translation track was used for
the training and evaluation corpora. Table 4 shows
English Chinese
Train Sentences 4.6M
Words 79.6M 73.4M
Dev Sentences 1.6K
Words 46.4K 39.0K
Test Sentences 1.9K
Words 45.7K 47.0K (Ave.)
Table 4: Statistics of training, development and test cor-
pus for E-C translation.
Baseline IST-ITG Proposed
BLEU 17.54 18.60 18.93
Table 5: BLEU score results for E-C translation. (4-
reference)
the training, development and test corpus statistics.
For the translation model training, we used 4.6M
bilingual sentences. For the language model train-
ing, we used 4.6M sentences which are used for
the translation model training. The language model
type was word 3-gram smoothed by Kneser-Ney
discounting. A development set with 1.6K sen-
tences was used as evaluation data in the Chinese-to-
English translation track for the NIST MT07 eval-
uation campaign. A single reference was used in
the development set. The evaluation set with 1.9K
sentences is the same as the MT08 evaluation data,
with 4 references. In this experiment, the compared
methods were the same as in the E-J experiment.
The proposed reordering model are trained from
4.6M bilingual sentences for the translation model
training. The amount of available training samples
represented by subtrees was 39.6M. In the available
training samples, there were 193K subtree types.
As in the E-J experiments, the heuristic threshold
was 10. The proposed reordering model consisted
of 18,955 subtree types and one clustered model
?other.? The models not including ?other? covered
99.45% of all training samples.
The BLEU scores are presented in Table 5.
In comparing ?Baseline? method with ?IST-ITG?
method, the improvement in BLEU was a 1.06-
point. In comparing ?IST-ITG? method with ?Pro-
posed? method, the improvement in BLEU was a
0.33-point. As in the E-J experiments, ?Proposed?
method performed the highest BLEU. We demon-
75
strated that the proposed method is effective for mul-
tiple language pairs. However, the improvement
of BLEU score in E-C translation is smaller than
the improvement in E-J translation, because English
and Chinese are similar sentence structures, such as
SVO-languages (Japanese is SOV-language). When
the sentence structures are different, the proposed re-
ordering model is effective.
5 Conclusion
This paper proposed a new word reordering model
using syntactic information of a source tree for
phrase-based statistical machine translation. The
proposed model is an extension of the IST-ITG con-
straints. In both IST-ITG constraints and the pro-
posed method, the target-side word order is obtained
by rotating nodes of the source-side tree structure.
Both the IST-ITG constraints and the proposed re-
ordering model fix the phrase position for the global
reorderings. However, the proposed method can
conduct a probabilistic evaluation of target word re-
orderings which the IST-ITG constraints cannot. In
E-J and E-C translation experiments, the proposed
method resulted in a 0.49-point improvement (29.31
to 29.80) and a 0.33-point improvement (18.60 to
18.93) in word BLEU-4 compared with IST-ITG
constraints, respectively. This indicates the validity
of the proposed reordering model.
Future work will focus on a reduction of com-
putational cost of decoding including the proposed
reordering model, and a simultaneous training of
translation and reordering models. Moreover, we
will deal with difference between source and target
in multi level like in Gally et al (2004).
The improvement could clearly be seen from vi-
sual inspection of the output, a few examples of
which are presented in the following Appendix.
A Samples from the English-to-Japanese
Translation
A.1 Sentence 1
Source: Aggravation was obvious from the latter
half of March to the end of April, and he contracted
the disease in February to the beginning of May.
Baseline: ?????????????????
????????????????
Reference: ?????????????????
????????????
Proposed: ?????????????????
????????????????
A.2 Sentence 2
Source: The value of TF, on the other hand, was
higher in the reverse order, indicating that high ox-
idation rate causes severe defects on the surface of
Ni crystallites.
Baseline: ?????????????????
??????????????????????
??????????????
Reference: ?????????????????
??????????????????????
?????????
Proposed: ?????????????????
??????????????????????
?????????????
A.3 Sentence 3
Source: After diagnosing the pleural effusion and
ascites, vein catheter was left in place under the echo
guide, and after removing the pleural effusion and
ascites, OK-432 was administered locally.
Baseline: ??????????????????
??????????????????????
?????????????????
Reference: ?????????????????
??????????????????????
???????????????????
Proposed: ?????????????????
??????????????????????
???????????????????????
A.4 Sentence 4
Source: From result of the consideration, it was
pointed that radiation from the loop elements was
weak.
Baseline: ?????????????????
?????????????
Reference: ?????????????????
???????????
Proposed: ?????????????????
???????????
76
References
Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra,
Vincent J. Della Pietra, Andrew S. Kehler, and Robert
L. Mercer 1996. Language translation apparatus
and method of using context-based translation models.
United States patent, patent number 5510981.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL 2000, pages 132?
139.
Chasen
http://chasen-legacy.sourceforge.jp/
Heidi J. Fox, 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP, pages
304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical Syntax-Directed Translation with Extended
Domain of Locality. In Proceedings of AMTA.
Japanese-English paper abstract corpus
http://www.jst.go.jp
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language model In Proceed-
ings of ICASSP 1995, pages 181?184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 127?133.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Machine
Translation with Syntactified Target Language Phrases
In Proceedings of EMNLP2006, pages 44?52.
Dan Melamed. 2004. Statistical machine translation by
parsing In Proceedings of ACL, pages 653?660.
Moses
http://www.statmt.org/moses/
Franz josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1), pages 19?51.
Franz josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Franz josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4), pages 417?
449.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL, pages 271?279.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Andreas Stolcke. 2002. SRILM - An Ex-
tensible Language Model Toolkit In Pro-
ceedings of ICSLP2002, pages 901?904.
http://www.speech.sri.com/projects/srilm/
Christopher Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proceed-
ings of HLT-NAACL, pages 101?104.
Masao Uchiyama and Hitoshi Isahara. 2007. 2007. A
japanese-english patent parallel corpus. In MT sum-
mit XI, pages 475?482.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of IJCAI, pages 1328?1334.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguiatics, 23(3), pages 377?403.
Kenji Yamada and Kevin Knight. 2000. A syntax-based
statistical translation model In Proceedings of ACL,
pages 523?530.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing Constraints from the Source
Tree on ITG Constraints for SMT. In Proceedings of
ACL : HLT Second Workshop on Syntax and Structure
in Statistical Translation (SSST-2), pages 1?9.
77
