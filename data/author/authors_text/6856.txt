Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683?691,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Natural Language Generation as Planning Under Uncertainty for Spoken
Dialogue Systems
Verena Rieser
School of Informatics
University of Edinburgh
vrieser@inf.ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
olemon@inf.ed.ac.uk
Abstract
We present and evaluate a new model for
Natural Language Generation (NLG) in
Spoken Dialogue Systems, based on statis-
tical planning, given noisy feedback from
the current generation context (e.g. a user
and a surface realiser). We study its use in
a standard NLG problem: how to present
information (in this case a set of search re-
sults) to users, given the complex trade-
offs between utterance length, amount of
information conveyed, and cognitive load.
We set these trade-offs by analysing exist-
ing MATCH data. We then train a NLG pol-
icy using Reinforcement Learning (RL),
which adapts its behaviour to noisy feed-
back from the current generation context.
This policy is compared to several base-
lines derived from previous work in this
area. The learned policy significantly out-
performs all the prior approaches.
1 Introduction
Natural language allows us to achieve the same
communicative goal (?what to say?) using many
different expressions (?how to say it?). In a Spo-
ken Dialogue System (SDS), an abstract commu-
nicative goal (CG) can be generated in many dif-
ferent ways. For example, the CG to present
database results to the user can be realized as a
summary (Polifroni and Walker, 2008; Demberg
andMoore, 2006), or by comparing items (Walker
et al, 2004), or by picking one item and recom-
mending it to the user (Young et al, 2007).
Previous work has shown that it is useful to
adapt the generated output to certain features of
the dialogue context, for example user prefer-
ences, e.g. (Walker et al, 2004; Demberg and
Moore, 2006), user knowledge, e.g. (Janarthanam
and Lemon, 2008), or predicted TTS quality, e.g.
(Nakatsu and White, 2006).
In extending this previous work we treat NLG
as a statistical sequential planning problem, anal-
ogously to current statistical approaches to Dia-
logue Management (DM), e.g. (Singh et al, 2002;
Henderson et al, 2008; Rieser and Lemon, 2008a)
and ?conversation as action under uncertainty?
(Paek and Horvitz, 2000). In NLG we have
similar trade-offs and unpredictability as in DM,
and in some systems the content planning and DM
tasks are overlapping. Clearly, very long system
utterances with many actions in them are to be
avoided, because users may become confused or
impatient, but each individual NLG action will
convey some (potentially) useful information to
the user. There is therefore an optimization prob-
lem to be solved. Moreover, the user judgements
or next (most likely) action after each NLG action
are unpredictable, and the behaviour of the surface
realizer may also be variable (see Section 6.2).
NLG could therefore fruitfully be approached
as a sequential statistical planning task, where
there are trade-offs and decisions to make, such as
whether to choose another NLG action (and which
one to choose) or to instead stop generating. Re-
inforcement Learning (RL) allows us to optimize
such trade-offs in the presence of uncertainty, i.e.
the chances of achieving a better state, while en-
gaging in the risk of choosing another action.
In this paper we present and evaluate a new
model for NLG in Spoken Dialogue Systems as
planning under uncertainty. In Section 2 we argue
for applying RL to NLG problems and explain the
overall framework. In Section 3 we discuss chal-
lenges for NLG for Information Presentation. In
Section 4 we present results from our analysis of
the MATCH corpus (Walker et al, 2004). In Sec-
tion 5 we present a detailed example of our pro-
posed NLG method. In Section 6 we report on
experimental results using this framework for ex-
ploring Information Presentation policies. In Sec-
tion 7 we conclude and discuss future directions.
683
2 NLG as planning under uncertainty
We adopt the general framework of NLG as plan-
ning under uncertainty (see (Lemon, 2008) for the
initial version of this approach). Some aspects of
NLG have been treated as planning, e.g. (Koller
and Stone, 2007; Koller and Petrick, 2008), but
never before as statistical planning.
NLG actions take place in a stochastic environ-
ment, for example consisting of a user, a realizer,
and a TTS system, where the individual NLG ac-
tions have uncertain effects on the environment.
For example, presenting differing numbers of at-
tributes to the user, and making the user more or
less likely to choose an item, as shown by (Rieser
and Lemon, 2008b) for multimodal interaction.
Most SDS employ fixed template-based gener-
ation. Our goal, however, is to employ a stochas-
tic realizer for SDS, see for example (Stent et al,
2004). This will introduce additional noise, which
higher level NLG decisions will need to react
to. In our framework, the NLG component must
achieve a high-level Communicative Goal from
the Dialogue Manager (e.g. to present a number
of items) through planning a sequence of lower-
level generation steps or actions, for example first
to summarize all the items and then to recommend
the highest ranking one. Each such action has un-
predictable effects due to the stochastic realizer.
For example the realizer might employ 6 attributes
when recommending item i4, but it might use only
2 (e.g. price and cuisine for restaurants), depend-
ing on its own processing constraints (see e.g. the
realizer used to collect the MATCH project data).
Likewise, the user may be likely to choose an item
after hearing a summary, or they may wish to hear
more. Generating appropriate language in context
(e.g. attributes presented so far) thus has the fol-
lowing important features in general:
? NLG is goal driven behaviour
? NLG must plan a sequence of actions
? each action changes the environment state or
context
? the effect of each action is uncertain.
These facts make it clear that the problem of
planning how to generate an utterance falls nat-
urally into the class of statistical planning prob-
lems, rather than rule-based approaches such as
(Moore et al, 2004; Walker et al, 2004), or super-
vised learning as explored in previous work, such
as classifier learning and re-ranking, e.g. (Stent et
al., 2004; Oh and Rudnicky, 2002). Supervised
approaches involve the ranking of a set of com-
pleted plans/utterances and as such cannot adapt
online to the context or the user. Reinforcement
Learning (RL) provides a principled, data-driven
optimisation framework for our type of planning
problem (Sutton and Barto, 1998).
3 The Information Presentation Problem
We will tackle the well-studied problem of Infor-
mation Presentation in NLG to show the benefits
of this approach. The task here is to find the best
way to present a set of search results to a user
(e.g. some restaurants meeting a certain set of con-
straints). This is a task common to much prior
work in NLG, e.g. (Walker et al, 2004; Demberg
and Moore, 2006; Polifroni and Walker, 2008).
In this problem, there there are many decisions
available for exploration. For instance, which pre-
sentation strategy to apply (NLG strategy selec-
tion), how many attributes of each item to present
(attribute selection), how to rank the items and at-
tributes according to different models of user pref-
erences (attribute ordering), how many (specific)
items to tell them about (conciseness), how many
sentences to use when doing so (syntactic plan-
ning), and which words to use (lexical choice) etc.
All these parameters (and potentially many more)
can be varied, and ideally, jointly optimised based
on user judgements.
We had two corpora available to study some of
the regions of this decision space. We utilise the
MATCH corpus (Walker et al, 2004) to extract an
evaluation function (also known as ?reward func-
tion?) for RL. Furthermore, we utilise the SPaRKy
corpus (Stent et al, 2004) to build a high quality
stochastic realizer. Both corpora contain data from
?overhearer? experiments targeted to Information
Presentation in dialogues in the restaurant domain.
While we are ultimately interested in how hearers
engaged in dialogues judge different Information
Presentations, results from overhearers are still di-
rectly relevant to the task.
4 MATCH corpus analysis
The MATCH project made two data sets available,
see (Stent et al, 2002) and (Whittaker et al, 2003),
which we combine to define an evaluation function
for different Information Presentation strategies.
684
strategy example av.#attr av.#sentence
SUMMARY ?The 4 restaurants differ in food quality, and cost.?
(#attr = 2,#sentence = 1)
2.07?.63 1.56?.5
COMPARE ?Among the selected restaurants, the following offer
exceptional overall value. Aureole?s price is 71 dol-
lars. It has superb food quality, superb service and
superb decor. Daniel?s price is 82 dollars. It has su-
perb food quality, superb service and superb decor.?
(#attr = 4,#sentence = 5)
3.2?1.5 5.5?3.11
RECOMMEND ?Le Madeleine has the best overall value among the
selected restaurants. Le Madeleine?s price is 40 dol-
lars and It has very good food quality. It?s in Mid-
town West. ? (#attr = 3,#sentence = 3)
2.4?.7 3.5?.53
Table 1: NLG strategies present in the MATCH corpus with average no. attributes and sentences as found
in the data.
The first data set, see (Stent et al, 2002), com-
prises 1024 ratings by 16 subjects (where we only
use the speech-based half, n = 512) on the follow-
ing presentation strategies: RECOMMEND, COM-
PARE, SUMMARY. These strategies are realized
using templates as in Table 2, and varying num-
bers of attributes. In this study the users rate the
individual presentation strategies as significantly
different (F (2) = 1361, p < .001). We find that
SUMMARY is rated significantly worse (p = .05
with Bonferroni correction) than RECOMMEND
and COMPARE, which are rated as equally good.
This suggests that one should never generate
a SUMMARY. However, SUMMARY has different
qualities from COMPARE and RECOMMEND, as
it gives users a general overview of the domain,
and probably helps the user to feel more confi-
dent when choosing an item, especially when they
are unfamiliar with the domain, as shown by (Po-
lifroni and Walker, 2008).
In order to further describe the strategies, we ex-
tracted different surface features as present in the
data (e.g. number of attributes realised, number of
sentences, number of words, number of database
items talked about, etc.) and performed a step-
wise linear regression to find the features which
were important to the overhearers (following the
PARADISE framework (Walker et al, 2000)). We
discovered a trade-off between the length of the ut-
terance (#sentence) and the number of attributes
realised (#attr), i.e. its informativeness, where
overhearers like to hear as many attributes as pos-
sible in the most concise way, as indicated by
the regression model shown in Equation 1 (R2 =
.34). 1
score = .775?#attr + (?.301)?#sentence;
(1)
The second MATCH data set, see (Whittaker et
al., 2003), comprises 1224 ratings by 17 subjects
on the NLG strategies RECOMMEND and COM-
PARE. The strategies realise varying numbers of
attributes according to different ?conciseness? val-
ues: concise (1 or 2 attributes), average (3
or 4), and verbose (4,5, or 6). Overhearers
rate all conciseness levels as significantly different
(F (2) = 198.3, p < .001), with verbose rated
highest and concise rated lowest, supporting
our findings in the first data set. However, the rela-
tion between number of attributes and user ratings
is not strictly linear: ratings drop for #attr = 6.
This suggests that there is an upper limit on how
many attributes users like to hear. We expect this
to be especially true for real users engaged in ac-
tual dialogue interaction, see (Winterboer et al,
2007). We therefore include ?cognitive load? as a
variable when training the policy (see Section 6).
In addition to the trade-off between length and
informativeness for single NLG strategies, we are
interested whether this trade-off will also hold for
generating sequences of NLG actions. (Whittaker
et al, 2002), for example, generate a combined
strategy where first a SUMMARY is used to de-
scribe the retrieved subset and then they RECOM-
MEND one specific item/restaurant. For example
?The 4 restaurants are all French, but differ in
1For comparison: (Walker et al, 2000) report on R2 be-
tween .4 and .5 on a slightly lager data set.
685
Figure 1: Possible NLG policies (X=stop generation)
food quality, and cost. Le Madeleine has the best
overall value among the selected restaurants. Le
Madeleine?s price is 40 dollars and It has very
good food quality. It?s in Midtown West.?
We therefore extend the set of possible strate-
gies present in the data for exploration: we allow
ordered combinations of the strategies, assuming
that only COMPARE or RECOMMEND can follow a
SUMMARY, and that only RECOMMEND can fol-
low COMPARE, resulting in 7 possible actions:
1. RECOMMEND
2. COMPARE
3. SUMMARY
4. COMPARE+RECOMMEND
5. SUMMARY+RECOMMEND
6. SUMMARY+COMPARE
7. SUMMARY+COMPARE+RECOMMEND
We then analytically solved the regression
model in Equation 1 for the 7 possible strategies
using average values from the MATCH data. This is
solved by a system of linear inequalities. Accord-
ing to this model, the best ranking strategy is to
do all the presentation strategies in one sequence,
i.e. SUMMARY+COMPARE+RECOMMEND. How-
ever, this analytic solution assumes a ?one-shot?
generation strategy where there is no intermediate
feedback from the environment: users are simply
static overhearers (they cannot ?barge-in? for ex-
ample), there is no variation in the behaviour of the
surface realizer, i.e. one would use fixed templates
as in MATCH, and the user has unlimited cogni-
tive capabilities. These assumptions are not real-
istic, and must be relaxed. In the next Section we
describe a worked through example of the overall
framework.
5 Method: the RL-NLG model
For the reasons discussed above, we treat the
NLG module as a statistical planner, operat-
ing in a stochastic environment, and optimise
it using Reinforcement Learning. The in-
put to the module is a Communicative Goal
supplied by the Dialogue Manager. The CG
consists of a Dialogue Act to be generated,
for example present items(i1, i2, i5, i8),
and a System Goal (SysGoal) which is
the desired user reaction, e.g. to make the
user choose one of the presented items
(user choose one of(i1, i2, i5, i8)). The
RL-NLG module must plan a sequence of lower-
level NLG actions that achieve the goal (at lowest
cost) in the current context. The context consists
of a user (who may remain silent, supply more
constraints, choose an item, or quit), and variation
from the sentence realizer described above.
Now let us walk-through one simple ut-
terance plan as carried out by this model,
as shown in Table 2. Here, we start
with the CG present items(i1, i2, i5, i8)&
user choose one of(i1, i2, i5, i8) from the
system?s DM. This initialises the NLG state (init).
The policy chooses the action SUMMARY and this
transitions us to state s1, where we observe that
4 attributes and 1 sentence have been generated,
and the user is predicted to remain silent. In this
state, the current NLG policy is to RECOMMEND
the top ranked item (i5, for this user), which takes
us to state s2, where 8 attributes have been gener-
ated in a total of 4 sentences, and the user chooses
an item. The policy holds that in states like s2 the
686
init s1 s2
summarise recommend
end
stop
atts=4user=silent atts=8user=chooseENVIRONMENT:
ACTIONS:
GOAL
Reward
Figure 2: Example RL-NLG action sequence for Table 4
State Action State change/effect
init SysGoal: present items(i1, i2, i5, i8)& user choose one of(i1, i2, i5, i8) initialise state
s1 RL-NLG: SUMMARY(i1, i2, i5, i8) att=4, sent=1, user=silent
s2 RL-NLG: RECOMMEND(i5) att=8, sent=4, user=choose(i5)
end RL-NLG: stop calculate Reward
Table 2: Example utterance planning sequence for Figure 2
best thing to do is ?stop? and pass the turn to the
user. This takes us to the state end, where the total
reward of this action sequence is computed (see
Section 6.3), and used to update the NLG policy
in each of the visited state-action pairs via back-
propagation.
6 Experiments
We now report on a proof-of-concept study where
we train our policy in a simulated learning envi-
ronment based on the results from the MATCH cor-
pus analysis in Section 4. Simulation-based RL
allows to explore unseen actions which are not in
the data, and thus less initial data is needed (Rieser
and Lemon, 2008b). Note, that we cannot directly
learn from the MATCH data, as therefore we would
need data from an interactive dialogue. We are
currently collecting such data in a Wizard-of-Oz
experiment.
6.1 User simulation
User simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al, 2007). A user simulation for
NLG is very similar, in that it is a predictive model
of the most likely next user act. However, this user
act does not actually change the overall dialogue
state (e.g. by filling slots) but it only changes the
generator state. In other words, the NLG user sim-
ulation tells us what the user is most likely to do
next, if we were to stop generating now. It also
tells us the probability whether the user chooses
to ?barge-in? after a system NLG action (by either
choosing an item or providing more information).
The user simulation for this study is a simple
bi-gram model, which relates the number of at-
tributes presented to the next likely user actions,
see Table 3. The user can either follow the goal
provided by the DM (SysGoal), for example
choosing an item. The user can also do some-
thing else (userElse), e.g. providing another
constraint, or the user can quit (userQuit).
For simplification, we discretise the number of
attributes into concise-average-verbose,
reflecting the conciseness values from the MATCH
data, as described in Section 4. In addition, we
assume that the user?s cognitive abilities are lim-
ited (?cognitive load?), based on the results from
the second MATCH data set in Section 4. Once the
number of attributes is more than the ?magic num-
ber 7? (reflecting psychological results on short-
term memory) (Baddeley, 2001)) the user is more
likely to become confused and quit.
The probabilities in Table 3 are currently man-
ually set heuristics. We are currently conducting a
Wizard-of-Oz study in order to learn these proba-
687
bilities (and other user parameters) from real data.
SysGoal userElse userQuit
concise 20.0 60.0 20.0
average 60.0 20.0 20.0
verbose 20.0 20.0 60.0
Table 3: NLG bi-gram user simulation
6.2 Realizer model
The sequential NLG model assumes a realizer,
which updates the context after each generation
step (i.e. after each single action). We estimate
the realiser?s parameters from the mean values we
found in the MATCH data (see Table 1). For this
study we first (randomly) vary the number of at-
tributes, whereas the number of sentences is fixed
(see Table 4). In current work we replace the re-
alizer model with an implemented generator that
replicates the variation found in the SPaRKy real-
izer (Stent et al, 2004).
#attr #sentence
SUMMARY 1 or 2 2
COMPARE 3 or 4 6
RECOMMEND 2 or 3 3
Table 4: Realizer parameters
6.3 Reward function
The reward function defines the final goal of the
utterance generation sequence. In this experiment
the reward is a function of the various data-driven
trade-offs as identified in the data analysis in Sec-
tion 4: utterance length and number of provided
attributes, as weighted by the regression model
in Equation 1, as well as the next predicted user
action. Since we currently only have overhearer
data, we manually estimate the reward for the
next most likely user act, to supplement the data-
driven model. If in the end state the next most
likely user act is userQuit, the learner gets a
penalty of ?100, userElse receives 0 reward,
and SysGoal gains +100 reward. Again, these
hand coded scores need to be refined by a more
targeted data collection, but the other components
of the reward function are data-driven.
Note that RL learns to ?make compromises?
with respect to the different trade-offs. For ex-
ample, the user is less likely to choose an item
if there are more than 7 attributes, but the real-
izer can generate 9 attributes. However, in some
contexts it might be desirable to generate all 9 at-
tributes, e.g. if the generated utterance is short.
Threshold-based approaches, in contrast, cannot
(easily) reason with respect to the current content.
6.4 State and Action Space
We now formulate the problem as a Markov De-
cision Process (MDP), relating states to actions.
Each state-action pair is associated with a transi-
tion probability, which is the probability of mov-
ing from state s at time t to state s? at time t+1 af-
ter having performed action awhen in state s. This
transition probability is computed by the environ-
ment model (i.e. user and realizer), and explic-
itly captures noise/uncertainty in the environment.
This is a major difference to other non-statistical
planning approaches. Each transition is also as-
sociated with a reinforcement signal (or reward)
rt+1 describing how good the result of action a
was when performed in state s.
The state space comprises 9 binary features rep-
resenting the number of attributes, 2 binary fea-
tures representing the predicted user?s next ac-
tion to follow the system goal or quit, as well as
a discrete feature reflecting the number of sen-
tences generated so far, as shown in Figure 3.
This results in 211 ? 6 = 12, 288 distinct genera-
tion states. We trained the policy using the well
known SARSA algorithm, using linear function ap-
proximation (Sutton and Barto, 1998). The policy
was trained for 3600 simulated NLG sequences.
In future work we plan to learn lower level deci-
sions, such as lexical adaptation based on the vo-
cabulary used by the user.
6.5 Baselines
We derive the baseline policies from Informa-
tion Presentation strategies as deployed by cur-
rent dialogue systems. In total we utilise 7 differ-
ent baselines (B1-B7), which correspond to single
branches in our policy space (see Figure 1):
B1: RECOMMEND only, e.g. (Young et al, 2007)
B2: COMPARE only, e.g. (Henderson et al, 2008)
B3: SUMMARY only, e.g. (Polifroni and Walker,
2008)
B4: SUMMARY followed by RECOMMEND, e.g.
(Whittaker et al, 2002)
B5: Randomly choosing between COMPARE and
RECOMMEND, e.g. (Walker et al, 2007)
688
??
?
?
?
?
?
?
?
?
action:
?
?
?
?
?
SUMMARY
COMPARE
RECOMMEND
end
?
?
?
?
?
state:
?
?
?
?
?
?
?
?
?
?
attributes |1 |-|9 |:
{
0,1
}
sentence:
{
1-11
}
userGoal:
{
0,1
}
userQuit:
{
0,1
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: State-Action space for RL-NLG
B6: Randomly choosing between all 7 outputs
B7: Always generating whole sequence, i.e.
SUMMARY+COMPARE+RECOMMEND, as
suggested by the analytic solution (see
Section 4).
6.6 Results
We analyse the test runs (n=200) using an ANOVA
with a PostHoc T-Test (with Bonferroni correc-
tion). RL significantly (p < .001) outperforms all
baselines in terms of final reward, see Table 5. RL
is the only policy which significantly improves the
next most likely user action by adapting to features
in the current context. In contrast to conventional
approaches, RL learns to ?control? its environment
according to the estimated transition probabilities
and the associated rewards.
The learnt policy can be described as follows:
It either starts with SUMMARY or COMPARE af-
ter the init state, i.e. it learnt to never start with a
RECOMMEND. It stops generating after COMPARE
if the userGoal is (probably) reached (e.g. the
user is most likely to choose an item in the next
turn, which depends on the number of attributes
generated), otherwise it goes on and generates a
RECOMMEND. If it starts with SUMMARY, it al-
ways generates a COMPARE afterwards. Again, it
stops if the userGoal is (probably) reached, oth-
erwise it generates the full sequence (which corre-
sponds to the analytic solution B7).
The analytic solution B7 performs second best,
and significantly outperforms all the other base-
lines (p < .01). Still, it is significantly worse
(p < .001) than the learnt policy as this ?one-shot-
strategy? cannot robustly and dynamically adopt to
noise or changes in the environment.
In general, generating sequences of NLG ac-
tions rates higher than generating single actions
only: B4 and B6 rate directly after RL and B7,
while B1, B2, B3, B5 are all equally bad given
our data-driven definition of reward and environ-
ment. Furthermore, the simulated environment
allows us to replicate the results in the MATCH
corpus (see Section 4) when only comparing sin-
gle strategies: SUMMARY performs significantly
worse, while RECOMMEND and COMPARE per-
form equally well.
policy reward (?std)
B1 99.1 (?129.6)
B2 90.9 (?142.2)
B3 65.5 (?137.3)
B4 176.0 (?154.1)
B5 95.9 (?144.9)
B6 168.8 (?165.3)
B7 229.3 (?157.1)
RL 310.8 (?136.1)
Table 5: Evaluation Results (p < .001 )
7 Conclusion
We presented and evaluated a new model for Nat-
ural Language Generation (NLG) in Spoken Dia-
logue Systems, based on statistical planning. After
motivating and presenting the model, we studied
its use in Information Presentation.
We derived a data-driven model predicting
users? judgements to different information presen-
tation actions (reward function), via a regression
analysis on MATCH data. We used this regression
model to set weights in a reward function for Re-
inforcement Learning, and so optimize a context-
adaptive presentation policy. The learnt policy was
compared to several baselines derived from previ-
ous work in this area, where the learnt policy sig-
nificantly outperforms all the baselines.
There are many possible extensions to this
model, e.g. using the same techniques to jointly
optimise choosing the number of attributes, aggre-
gation, word choice, referring expressions, and so
on, in a hierarchical manner.
689
We are currently collecting data in targeted
Wizard-of-Oz experiments, to derive a fully data-
driven training environment and test the learnt
policy with real users, following (Rieser and
Lemon, 2008b). The trained NLG strategy
will also be integrated in an end-to-end statis-
tical system within the CLASSiC project (www.
classic-project.org).
Acknowledgments
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no. 216594 (CLASSiC project
project: www.classic-project.org) and
from the EPSRC project no. EP/E019501/1.
References
A. Baddeley. 2001. Working memory and language:
an overview. Journal of Communication Disorder,
36(3):189?208.
Vera Demberg and Johanna D. Moore. 2006. Infor-
mation presentation in spoken dialogue systems. In
Proceedings of EACL.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement / supervised
learning of dialogue policies from fixed datasets.
Computational Linguistics (to appear).
Srinivasan Janarthanam and Oliver Lemon. 2008. User
simulations for online adaptation and knowledge-
alignment in Troubleshooting dialogue systems. In
Proc. of SEMdial.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of ACL.
Oliver Lemon. 2008. Adaptive Natural Language
Generation in Dialogue using Reinforcement Learn-
ing. In Proceedings of SEMdial.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, com-
parative descriptions in spoken dialogue. In Proc.
FLAIRS.
Crystal Nakatsu and Michael White. 2006. Learning
to say it well: Reranking realizations by predicted
synthesis quality. In Proceedings of ACL.
Alice Oh and Alexander Rudnicky. 2002. Stochastic
natural language generation for spoken dialog sys-
tems. Computer, Speech & Language, 16(3/4):387?
407.
Tim Paek and Eric Horvitz. 2000. Conversation as
action under uncertainty. In Proc. of the 16th Con-
ference on Uncertainty in Artificial Intelligence.
Joseph Polifroni and Marilyn Walker. 2008. Inten-
sional Summaries as Cooperative Responses in Di-
alogue Automation and Evaluation. In Proceedings
of ACL.
Verena Rieser and Oliver Lemon. 2008a. Does this
list contain what you were searching for? Learn-
ing adaptive dialogue strategies for Interactive Ques-
tion Answering. J. Natural Language Engineering,
15(1):55?72.
Verena Rieser and Oliver Lemon. 2008b. Learn-
ing Effective Multimodal Dialogue Strategies from
Wizard-of-Oz data: Bootstrapping and Evaluation.
In Proceedings of ACL.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with Reinforce-
ment Learning: Experiments with the NJFun sys-
tem. JAIR, 16:105?133.
Amanda Stent, Marilyn Walker, Steve Whittaker, and
Preetam Maloor. 2002. User-tailored generation for
spoken dialogue: an experiment. In In Proc. of IC-
SLP.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards developing general mod-
els of usability with PARADISE. Natural Language
Engineering, 6(3).
Marilyn Walker, S. Whittaker, A. Stent, P. Maloor,
J. Moore, M. Johnston, and G. Vasireddy. 2004.
User tailored generation in the match multimodal di-
alogue system. Cognitive Science, 28:811?840.
MarilynWalker, Amanda Stent, Franc?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413?456.
Steve Whittaker, Marilyn Walker, and Johanna Moore.
2002. Fish or Fowl: A Wizard of Oz evaluation
of dialogue strategies in the restaurant domain. In
Proc. of the International Conference on Language
Resources and Evaluation (LREC).
Stephen Whittaker, Marilyn Walker, and Preetam Mal-
oor. 2003. Should i tell all? an experiment on
conciseness in spoken dialogue. In Proc. European
Conference on Speech Processing (EUROSPEECH).
690
Andi Winterboer, Jiang Hu, Johanna D. Moore, and
Clifford Nass. 2007. The influence of user tailoring
and cognitive load on user performance in spoken
dialogue systems. In Proc. of the 10th International
Conference of Spoken Language Processing (Inter-
speech/ICSLP).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
691
Proceedings of ACL-08: HLT, pages 638?646,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz
data: Bootstrapping and Evaluation
Verena Rieser
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
vrieser@inf.ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
olemon@inf.ed.ac.uk
Abstract
We address two problems in the field of au-
tomatic optimization of dialogue strategies:
learning effective dialogue strategies when no
initial data or system exists, and evaluating the
result with real users. We use Reinforcement
Learning (RL) to learn multimodal dialogue
strategies by interaction with a simulated envi-
ronment which is ?bootstrapped? from small
amounts of Wizard-of-Oz (WOZ) data. This
use of WOZ data allows development of op-
timal strategies for domains where no work-
ing prototype is available. We compare the
RL-based strategy against a supervised strat-
egy which mimics the wizards? policies. This
comparison allows us to measure relative im-
provement over the training data. Our results
show that RL significantly outperforms Super-
vised Learning when interacting in simulation
as well as for interactions with real users. The
RL-based policy gains on average 50-times
more reward when tested in simulation, and
almost 18-times more reward when interacting
with real users. Users also subjectively rate
the RL-based policy on average 10% higher.
1 Introduction
Designing a spoken dialogue system is a time-
consuming and challenging task. A developer may
spend a lot of time and effort anticipating the po-
tential needs of a specific application environment
and then deciding on the most appropriate system
action (e.g. confirm, present items,. . . ). One of the
key advantages of statistical optimisation methods,
such as Reinforcement Learning (RL), for dialogue
strategy design is that the problem can be formu-
lated as a principled mathematical model which can
be automatically trained on real data (Lemon and
Pietquin, 2007; Frampton and Lemon, to appear). In
cases where a system is designed from scratch, how-
ever, there is often no suitable in-domain data. Col-
lecting dialogue data without a working prototype
is problematic, leaving the developer with a classic
chicken-and-egg problem.
We propose to learn dialogue strategies by
simulation-based RL (Sutton and Barto, 1998),
where the simulated environment is learned from
small amounts of Wizard-of-Oz (WOZ) data. Us-
ing WOZ data rather than data from real Human-
Computer Interaction (HCI) allows us to learn op-
timal strategies for domains where no working di-
alogue system already exists. To date, automatic
strategy learning has been applied to dialogue sys-
tems which have already been deployed using hand-
crafted strategies. In such work, strategy learning
was performed based on already present extensive
online operation experience, e.g. (Singh et al, 2002;
Henderson et al, 2005). In contrast to this preced-
ing work, our approach enables strategy learning in
domains where no prior system is available. Opti-
mised learned strategies are then available from the
first moment of online-operation, and tedious hand-
crafting of dialogue strategies is omitted. This inde-
pendence from large amounts of in-domain dialogue
data allows researchers to apply RL to new appli-
cation areas beyond the scope of existing dialogue
systems. We call this method ?bootstrapping?.
In a WOZ experiment, a hidden human operator,
the so called ?wizard?, simulates (partly or com-
638
pletely) the behaviour of the application, while sub-
jects are left in the belief that they are interacting
with a real system (Fraser and Gilbert, 1991). That
is, WOZ experiments only simulate HCI. We there-
fore need to show that a strategy bootstrapped from
WOZ data indeed transfers to real HCI. Further-
more, we also need to introduce methods to learn
useful user simulations (for training RL) from such
limited data.
The use of WOZ data has earlier been proposed
in the context of RL. (Williams and Young, 2004)
utilise WOZ data to discover the state and action
space for MDP design. (Prommer et al, 2006)
use WOZ data to build a simulated user and noise
model for simulation-based RL. While both stud-
ies show promising first results, their simulated en-
vironment still contains many hand-crafted aspects,
which makes it hard to evaluate whether the suc-
cess of the learned strategy indeed originates from
the WOZ data. (Schatzmann et al, 2007) propose to
?bootstrap? with a simulated user which is entirely
hand-crafted. In the following we propose an en-
tirely data-driven approach, where all components
of the simulated learning environment are learned
from WOZ data. We also show that the resulting
policy performs well for real users.
2 Wizard-of-Oz data collection
Our domains of interest are information-seeking di-
alogues, for example a multimodal in-car interface
to a large database of music (MP3) files. The corpus
we use for learning was collected in a multimodal
study of German task-oriented dialogues for an in-
car music player application by (Rieser et al, 2005).
This study provides insights into natural methods
of information presentation as performed by human
wizards. 6 people played the role of an intelligent
interface (the ?wizards?). The wizards were able
to speak freely and display search results on the
screen by clicking on pre-computed templates. Wiz-
ards? outputs were not restricted, in order to explore
the different ways they intuitively chose to present
search results. Wizard?s utterances were immedi-
ately transcribed and played back to the user with
Text-To-Speech. 21 subjects (11 female, 10 male)
were given a set of predefined tasks to perform, as
well as a primary driving task, using a driving simu-
lator. The users were able to speak, as well as make
selections on the screen. We also introduced artifi-
cial noise in the setup, in order to closer resemble
the conditions of real HCI. Please see (Rieser et al,
2005) for further detail.
The corpus gathered with this setup comprises 21
sessions and over 1600 turns. Example 1 shows a
typical multimodal presentation sub-dialogue from
the corpus (translated from German). Note that the
wizard displays quite a long list of possible candi-
dates on an (average sized) computer screen, while
the user is driving. This example illustrates that even
for humans it is difficult to find an ?optimal? solu-
tion to the problem we are trying to solve.
(1) User: Please search for music by Madonna .
Wizard: I found seventeen hundred and eleven
items. The items are displayed on the screen.
[displays list]
User: Please select ?Secret?.
For each session information was logged, e.g. the
transcriptions of the spoken utterances, the wizard?s
database query and the number of results, the screen
option chosen by the wizard, and a rich set of con-
textual dialogue features was also annotated, see
(Rieser et al, 2005).
Of the 793 wizard turns 22.3% were annotated
as presentation strategies, resulting in 177 instances
for learning, where the six wizards contributed about
equal proportions.
Information about user preferences was obtained,
using a questionnaire containing similar questions to
the PARADISE study (Walker et al, 2000). In gen-
eral, users report that they get distracted from driv-
ing if too much information is presented. On the
other hand, users prefer shorter dialogues (most of
the user ratings are negatively correlated with dia-
logue length). These results indicate that we need
to find a strategy given the competing trade-offs be-
tween the number of results (large lists are difficult
for users to process), the length of the dialogue (long
dialogues are tiring, but collecting more information
can result in more precise results), and the noise in
the speech recognition environment (in high noise
conditions accurate information is difficult to ob-
tain). In the following we utilise the ratings from the
user questionnaires to optimise a presentation strat-
egy using simulation-based RL.
639
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
acquisition action:
?
?
?
?
askASlot
implConfAskASlot
explConf
presentInfo
?
?
?
?state:
?
?
?
?
?
?
filledSlot 1 |2 |3 |4 |:
{
0,1
}
confirmedSlot 1 |2 |3 |4 |:
{
0,1
}
DB:
{
1--438
}
?
?
?
?
?
?
presentation action:
[
presentInfoVerbal
presentInfoMM
]
state:
?
?
?
?
?
?
DB low:
{
0,1
}
DB med:
{
0,1
}
DB high
{
0,1
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: State-Action space for hierarchical Reinforcement Learning
3 Simulated Learning Environment
Simulation-based RL (also know as ?model-free?
RL) learns by interaction with a simulated environ-
ment. We obtain the simulated components from the
WOZ corpus using data-driven methods. The em-
ployed database contains 438 items and is similar in
retrieval ambiguity and structure to the one used in
the WOZ experiment. The dialogue system used for
learning comprises some obvious constraints reflect-
ing the system logic (e.g. that only filled slots can be
confirmed), implemented as Information State Up-
date (ISU) rules. All other actions are left for opti-
misation.
3.1 MDP and problem representation
The structure of an information seeking dialogue
system consists of an information acquisition phase,
and an information presentation phase. For informa-
tion acquisition the task of the dialogue manager is
to gather ?enough? search constraints from the user,
and then, ?at the right time?, to start the information
presentation phase, where the presentation task is to
present ?the right amount? of information in the right
way? either on the screen or listing the items ver-
bally. What ?the right amount? actually means de-
pends on the application, the dialogue context, and
the preferences of users. For optimising dialogue
strategies information acquisition and presentation
are two closely interrelated problems and need to
be optimised simultaneously: when to present in-
formation depends on the available options for how
to present them, and vice versa. We therefore for-
mulate the problem as a Markov Decision Process
(MDP), relating states to actions in a hierarchical
manner (see Figure 1): 4 actions are available for
the information acquisition phase; once the action
presentInfo is chosen, the information presen-
tation phase is entered, where 2 different actions
for output realisation are available. The state-space
comprises 8 binary features representing the task for
a 4 slot problem: filledSlot indicates whether a
slots is filled, confirmedSlot indicates whether
a slot is confirmed. We also add features that hu-
man wizards pay attention to, using the feature se-
lection techniques of (Rieser and Lemon, 2006b).
Our results indicate that wizards only pay attention
to the number of retrieved items (DB). We there-
fore add the feature DB to the state space, which
takes integer values between 1 and 438, resulting in
28 ? 438 = 112, 128 distinct dialogue states. In to-
tal there are 4112,128 theoretically possible policies
for information acquisition. 1 For the presentation
phase the DB feature is discretised, as we will further
discuss in Section 3.6. For the information presenta-
tion phase there are 22
3
= 256 theoretically possible
policies.
3.2 Supervised Baseline
We create a baseline by applying Supervised Learn-
ing (SL). This baseline mimics the average wizard
behaviour and allows us to measure the relative im-
provements over the training data (cf. (Henderson et
al., 2005)). For these experiments we use the WEKA
toolkit (Witten and Frank, 2005). We learn with the
decision tree J4.8 classifier, WEKA?s implementation
of the C4.5 system (Quinlan, 1993), and rule induc-
1In practise, the policy space is smaller, as some of combi-
nations are not possible, e.g. a slot cannot be confirmed before
being filled. Furthermore, some incoherent action choices are
excluded by the basic system logic.
640
baseline JRip J48
timing 52.0(? 2.2) 50.2(? 9.7) 53.5(?11.7)
modality 51.0(? 7.0) 93.5(?11.5)* 94.6(? 10.0)*
Table 1: Predicted accuracy for presentation timing and
modality (with standard deviation ?), * denotes statisti-
cally significant improvement at p < .05
tion JRIP, the WEKA implementation of RIPPER (Co-
hen, 1995). In particular, we learn models which
predict the following wizard actions:
? Presentation timing: when the ?average? wizard
starts the presentation phase
? Presentation modality: in which modality the
list is presented.
As input features we use annotated dialogue con-
text features, see (Rieser and Lemon, 2006b). Both
models are trained using 10-fold cross validation.
Table 1 presents the results for comparing the ac-
curacy of the learned classifiers against the major-
ity baseline. For presentation timing, none of the
classifiers produces significantly improved results.
Hence, we conclude that there is no distinctive pat-
tern the wizards follow for when to present informa-
tion. For strategy implementation we therefore use a
frequency-based approach following the distribution
in the WOZ data: in 0.48 of cases the baseline policy
decides to present the retrieved items; for the rest of
the time the system follows a hand-coded strategy.
For learning presentation modality, both classifiers
significantly outperform the baseline. The learned
models can be rewritten as in Algorithm 1. Note that
this rather simple algorithm is meant to represent the
average strategy as present in the initial data (which
then allows us to measure the relative improvements
of the RL-based strategy).
Algorithm 1 SupervisedStrategy
1: if DB ? 3 then
2: return presentInfoVerbal
3: else
4: return presentInfoMM
5: end if
3.3 Noise simulation
One of the fundamental characteristics of HCI is an
error prone communication channel. Therefore, the
simulation of channel noise is an important aspect of
the learning environment. Previous work uses data-
intensive simulations of ASR errors, e.g. (Pietquin
and Dutoit, 2006). We use a simple model simulat-
ing the effects of non- and misunderstanding on the
interaction, rather than the noise itself. This method
is especially suited to learning from small data sets.
From our data we estimate a 30% chance of user
utterances to be misunderstood, and 4% to be com-
plete non-understandings. We simulate the effects
noise has on the user behaviour, as well as for the
task accuracy. For the user side, the noise model de-
fines the likelihood of the user accepting or rejecting
the system?s hypothesis (for example when the sys-
tem utters a confirmation), i.e. in 30% of the cases
the user rejects, in 70% the user agrees. These prob-
abilities are combined with the probabilities for user
actions from the user simulation, as described in the
next section. For non-understandings we have the
user simulation generating Out-of-Vocabulary utter-
ances with a chance of 4%. Furthermore, the noise
model determines the likelihood of task accuracy as
calculated in the reward function for learning. A
filled slot which is not confirmed by the user has a
30% chance of having been mis-recognised.
3.4 User simulation
A user simulation is a predictive model of real user
behaviour used for automatic dialogue strategy de-
velopment and testing. For our domain, the user
can either add information (add), repeat or para-
phrase information which was already provided at
an earlier stage (repeat), give a simple yes-no an-
swer (y/n), or change to a different topic by pro-
viding a different slot value than the one asked for
(change). These actions are annotated manually
(? = .7). We build two different types of user
simulations, one is used for strategy training, and
one for testing. Both are simple bi-gram models
which predict the next user action based on the pre-
vious system action (P (auser|asystem)). We face
the problem of learning such models when train-
ing data is sparse. For training, we therefore use
a cluster-based user simulation method, see (Rieser
641
and Lemon, 2006a). For testing, we apply smooth-
ing to the bi-gram model. The simulations are evalu-
ated using the SUPER metric proposed earlier (Rieser
and Lemon, 2006a), which measures variance and
consistency of the simulated behaviour with respect
to the observed behaviour in the original data set.
This technique is used because for training we need
more variance to facilitate the exploration of large
state-action spaces, whereas for testing we need sim-
ulations which are more realistic. Both user simula-
tions significantly outperform random and majority
class baselines. See (Rieser, 2008) for further de-
tails.
3.5 Reward modelling
The reward function defines the goal of the over-
all dialogue. For example, if it is most important
for the dialogue to be efficient, the reward penalises
dialogue length, while rewarding task success. In
most previous work the reward function is manu-
ally set, which makes it ?the most hand-crafted as-
pect? of RL (Paek, 2006). In contrast, we learn the
reward model from data, using a modified version
of the PARADISE framework (Walker et al, 2000),
following pioneering work by (Walker et al, 1998).
In PARADISE multiple linear regression is used to
build a predictive model of subjective user ratings
(from questionnaires) from objective dialogue per-
formance measures (such as dialogue length). We
use PARADISE to predict Task Ease (a variable ob-
tained by taking the average of two questions in the
questionnaire) 2 from various input variables, via
stepwise regression. The chosen model comprises
dialogue length in turns, task completion (as manu-
ally annotated in the WOZ data), and the multimodal
user score from the user questionnaire, as shown in
Equation 2.
TaskEase = ? 20.2 ? dialogueLength+
11.8 ? taskCompletion+ 8.7 ?multimodalScore; (2)
This equation is used to calculate the overall re-
ward for the information acquisition phase. Dur-
ing learning, Task Completion is calculated online
according to the noise model, penalising all slots
which are filled but not confirmed.
2?The task was easy to solve.?, ?I had no problems finding
the information I wanted.?
For the information presentation phase, we com-
pute a local reward. We relate the multimodal score
(a variable obtained by taking the average of 4 ques-
tions) 3 to the number of items presented (DB) for
each modality, using curve fitting. In contrast to
linear regression, curve fitting does not assume a
linear inductive bias, but it selects the most likely
model (given the data points) by function interpo-
lation. The resulting models are shown in Figure
3.5. The reward for multimodal presentation is a
quadratic function that assigns a maximal score to
a strategy displaying 14.8 items (curve inflection
point). The reward for verbal presentation is a linear
function assigning negative scores to all presented
items ? 4. The reward functions for information
presentation intersect at no. items=3. A comprehen-
sive evaluation of this reward function can be found
in (Rieser and Lemon, 2008a).
-80
-70
-60
-50
-40
-30
-20
-10
 0
 10
 0  10  20  30  40  50  60  70
us
er
 sc
or
e
no. items
reward function for information presentation
intersection point
turning point:14.8
multimodal presentation: MM(x)
verbal presentation: Speech(x)
Figure 2: Evaluation functions relating number of items
presented in different modalities to multimodal score
3.6 State space discretisation
We use linear function approximation in order to
learn with large state-action spaces. Linear func-
tion approximation learns linear estimates for ex-
pected reward values of actions in states represented
as feature vectors. This is inconsistent with the idea
3?I liked the combination of information being displayed on
the screen and presented verbally.?, ?Switching between modes
did not distract me.?, ?The displayed lists and tables contained
on average the right amount of information.?, ?The information
presented verbally was easy to remember.?
642
of non-linear reward functions (as introduced in the
previous section). We therefore quantise the state
space for information presentation. We partition
the database feature into 3 bins, taking the first in-
tersection point between verbal and multimodal re-
ward and the turning point of the multimodal func-
tion as discretisation boundaries. Previous work
on learning with large databases commonly quan-
tises the database feature in order to learn with large
state spaces using manual heuristics, e.g. (Levin et
al., 2000; Heeman, 2007). Our quantisation tech-
nique is more principled as it reflects user prefer-
ences for multi-modal output. Furthermore, in pre-
vious work database items were not only quantised
in the state-space, but also in the reward function,
resulting in a direct mapping between quantised re-
trieved items and discrete reward values, whereas
our reward function still operates on the continuous
values. In addition, the decision when to present a
list (information acquisition phase) is still based on
continuous DB values. In future work we plan to en-
gineer new state features in order to learn with non-
linear rewards while the state space is still continu-
ous. A continuous representation of the state space
allows learning of more fine-grained local trade-offs
between the parameters, as demonstrated by (Rieser
and Lemon, 2008b).
3.7 Testing the Learned Policies in Simulation
We now train and test the multimodal presentation
strategies by interacting with the simulated learn-
ing environment. For the following RL experiments
we used the REALL-DUDE toolkit of (Lemon et al,
2006b). The SHARSHA algorithm is employed for
training, which adds hierarchical structure to the
well known SARSA algorithm (Shapiro and Langley,
2002). The policy is trained with the cluster-based
user simulation over 180k system cycles, which re-
sults in about 20k simulated dialogues. In total, the
learned strategy has 371 distinct state-action pairs
(see (Rieser, 2008) for details).
We test the RL-based and supervised baseline
policies by running 500 test dialogues with a
smoothed user simulation (so that we are not train-
ing and testing on the same simulation). We then
compare quantitative dialogue measures performing
a paired t-test. In particular, we compare mean val-
ues of the final rewards, number of filled and con-
firmed slots, dialog length, and items presented mul-
timodally (MM items) and items presented ver-
bally (verbal items). RL performs signifi-
cantly better (p < .001) than the baseline strategy.
The only non-significant difference is the number
of items presented verbally, where both RL and SL
strategy settled on a threshold of less than 4 items.
The mean performance measures for simulation-
based testing are shown in Table 2 and Figure 3.
The major strength of the learned policy is that
it learns to keep the dialogues reasonably short (on
average 5.9 system turns for RL versus 8.4 turns
for SL) by presenting lists as soon as the number
of retrieved items is within tolerance range for the
respective modality (as reflected in the reward func-
tion). The SL strategy in contrast has not learned the
right timing nor an upper bound for displaying items
on the screen. The results show that simulation-
based RL with an environment bootstrapped from
WOZ data allows learning of robust strategies which
significantly outperform the strategies contained in
the initial data set.
One major advantage of RL is that it allows us
to provide additional information about user pref-
erences in the reward function, whereas SL simply
mimics the data. In addition, RL is based on de-
layed rewards, i.e. the optimisation of a final goal.
For dialogue systems we often have measures indi-
cating how successful and/or satisfying the overall
performance of a strategy was, but it is hard to tell
how things should have been exactly done in a spe-
cific situation. This is what makes RL specifically
attractive for dialogue strategy learning. In the next
section we test the learned strategy with real users.
4 User Tests
4.1 Experimental design
For the user tests the RL policy is ported to a work-
ing ISU-based dialogue system via table look-up,
which indicates the action with the highest expected
reward for each state (cf. (Singh et al, 2002)). The
supervised baseline is implemented using standard
threshold-based update rules. The experimental con-
ditions are similar to the WOZ study, i.e. we ask the
users to solve similar tasks, and use similar ques-
tionnaires. Furthermore, we decided to use typed
user input rather than ASR. The use of text input
643
Measure SL baseline RL Strategy
SIM REAL SIM REAL
av. turns 8.42(?3.04) 5.86(?3.2) 5.9(?2.4)*** 5.07(?2.9)***
av. speech items 1.04(?.2) 1.29(?.4) 1.1(?.3) 1.2(?.4)
av. MM items 61.37(?82.5) 52.2(?68.5) 11.2(?2.4)*** 8.73(?4.4)***
av. reward -1741.3(?566.2) -628.2(?178.6) 44.06(?51.5)*** 37.62(?60.7)***
Table 2: Comparison of results obtained in simulation (SIM) and with real users (REAL) for SL and RL-based strate-
gies; *** denotes significant difference between SL and RL at p < .001
Figure 3: Graph comparison of objective measures: SLs
= SL policy in simulation; SLr = SL policy with real
users; RLs = RL policy in simulation; RLr = RL policy
with real users.
allows us to target the experiments to the dialogue
management decisions, and block ASR quality from
interfering with the experimental results (Hajdinjak
and Mihelic, 2006). 17 subjects (8 female, 9 male)
are given a set of 6?2 predefined tasks, which they
solve by interaction with the RL-based and the SL-
based system in controlled order. As a secondary
task users are asked to count certain objects in a driv-
ing simulation. In total, 204 dialogues with 1,115
turns are gathered in this setup.
4.2 Results
In general, the users rate the RL-based significantly
higher (p < .001) than the SL-based policy. The re-
sults from a paired t-test on the user questionnaire
data show significantly improved Task Ease, better
presentation timing, more agreeable verbal and mul-
timodal presentation, and that more users would use
the RL-based system in the future (Future Use). All
the observed differences have a medium effects size
(r ? |.3|).
We also observe that female participants clearly
favour the RL-based strategy, whereas the ratings by
male participants are more indifferent. Similar gen-
der effects are also reported by other studies on mul-
timodal output presentation, e.g. (Foster and Ober-
lander, 2006).
Furthermore, we compare objective dialogue per-
formance measures. The dialogues of the RL strat-
egy are significantly shorter (p < .005), while fewer
items are displayed (p < .001), and the help func-
tion is used significantly less (p < .003). The mean
performance measures for testing with real users are
shown in Table 2 and Figure 3. However, there is
no significant difference for the performance of the
secondary driving task.
5 Comparison of Results
We finally test whether the results obtained in sim-
ulation transfer to tests with real users, following
(Lemon et al, 2006a). We evaluate the quality of
the simulated learning environment by directly com-
paring the dialogue performance measures between
simulated and real interaction. This comparison en-
ables us to make claims regarding whether a policy
which is ?bootstrapped? from WOZ data is transfer-
able to real HCI. We first evaluate whether objective
dialogue measures are transferable, using a paired
t-test. For the RL policy there is no statistical dif-
ference in overall performance (reward), dialogue
length (turns), and the number of presented items
(verbal and multimodal items) between simulated
644
Measure WOZ SL RL
av. Task Ease .53?.14 .63?.26 .79?.21***
av. Future Use .56?.16 .55?.21 .67?.20***
Table 3: Improved user ratings over the WOZ study
where *** denotes p < .001
and real interaction (see Table 2, Figure 3). This in-
dicates that the learned strategy transfers well to real
settings. For the SL policy the dialogue length for
real users is significantly shorter than in simulation.
From an error analysis we conclude that real users
intelligently adapt to poor policies, e.g. by changing
topic, whereas the simulated users do not react in
this way.
Furthermore, we want to know whether the sub-
jective user ratings for the RL strategy improved
over the WOZ study. We therefore compare the user
ratings from the WOZ questionnaire to the user rat-
ings of the final user tests using a independent t-test
and a Wilcoxon Signed Ranks Test. Users rate the
RL-policy on average 10% higher. We are especially
interested in the ratings for Task Ease (as this was
the ultimate measure optimised with PARADISE) and
Future Use, as we believe this measure to be an im-
portant indicator of acceptance of the technology.
The results show that only the RL strategy leads to
significantly improved user ratings (increasing av-
erage Task Ease by 49% and Future Use by 19%),
whereas the ratings for the SL policy are not signifi-
cantly better than those for the WOZ data, see Table
3. 4 This indicates that the observed difference is in-
deed due to the improved strategy (and not to other
factors like the different user population or the em-
bedded dialogue system).
6 Conclusion
We addressed two problems in the field of automatic
optimization of dialogue strategies: learning effec-
tive dialogue strategies when no initial data or sys-
tem exists, and evaluating the result with real users.
We learned optimal strategies by interaction with a
simulated environment which is bootstrapped from
4The ratings are normalised as some of the questions were
on different scales.
a small amount of Wizard-of-Oz data, and we evalu-
ated the result with real users. The use of WOZ data
allows us to develop optimal strategies for domains
where no working prototype is available. The de-
veloped simulations are entirely data driven and the
reward function reflects real user preferences. We
compare the Reinforcement Learning-based strategy
against a supervised strategy which mimics the (hu-
man) wizards? policies from the original data. This
comparison allows us to measure relative improve-
ment over the training data. Our results show that
RL significantly outperforms SL in simulation as
well as in interactions with real users. The RL-based
policy gains on average 50-times more reward when
tested in simulation, and almost 18-times more re-
ward when interacting with real users. The human
users also subjectively rate the RL-based policy on
average 10% higher, and 49% higher for Task Ease.
We also show that results obtained in simulation are
comparable to results for real users. We conclude
that a strategy trained from WOZ data via boot-
strapping is transferable to real Human-Computer-
Interaction.
In future work will apply similar techniques to
statistical planning for Natural Language Generation
in spoken dialogue (Lemon, 2008; Janarthanam and
Lemon, 2008), (see the EC FP7 CLASSiC project:
www.classic-project.org).
Acknowledgements
The research leading to these results has re-
ceived funding from the European Community?s
7th Framework Programme (FP7/2007-2013) un-
der grant agreement no. 216594 (CLASSiC project
www.classic-project.org), the EC FP6
project ?TALK: Talk and Look, Tools for Am-
bient Linguistic Knowledge (IST 507802, www.
talk-project.org), from the EPSRC, project
no. EP/E019501/1, and from the IRTG Saarland
University.
645
References
W. W. Cohen. 1995. Fast effective rule induction. In
Proc. of the 12th ICML-95.
M. E. Foster and J. Oberlander. 2006. Data-driven gen-
eration of emphatic facial displays. In Proc. of EACL.
M. Frampton and O. Lemon. (to appear). Recent re-
search advances in Reinforcement Learning in Spoken
Dialogue Systems. Knowledge Engineering Review.
N. M. Fraser and G. N. Gilbert. 1991. Simulating speech
systems. Computer Speech and Language, 5:81?99.
M. Hajdinjak and F. Mihelic. 2006. The PARADISE
evaluation framework: Issues and findings. Computa-
tional Linguistics, 32(2):263?272.
P. Heeman. 2007. Combining reinforcement learn-
ing with information-state update rules. In Proc. of
NAACL.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hy-
brid Reinforcement/Supervised Learning for Dialogue
Policies from COMMUNICATOR data. In Proc. of IJ-
CAI workshop on Knowledge and Reasoning in Prac-
tical Dialogue Systems, pages 68?75.
S. Janarthanam and O. Lemon. 2008. User simula-
tions for online adaptation and knowledge-alignment
in Troubleshooting dialogue systems. In Proc. of the
12th SEMDIAL Workshop (LONdial).
O. Lemon and O. Pietquin. 2007. Machine learning for
spoken dialogue systems. In Proc. of Interspeech.
O. Lemon, K. Georgila, and J. Henderson. 2006a.
Evaluating Effectiveness and Portability of Reinforce-
ment Learned Dialogue Strategies with real users: the
TALK TownInfo Evaluation. In Proc. of IEEE/ACL
workshop on Spoken Language Technology (SLT).
O. Lemon, X. Liu, D. Shapiro, and C. Tollander. 2006b.
Hierarchical reinforcement learning of dialogue poli-
cies in a development environment for dialogue sys-
tems: REALL-DUDE. In Proc. of the 10th SEMDIAL
Workshop (BRANdial).
O. Lemon. 2008. Adaptive Natural Language Gener-
ation in Dialogue using Reinforcement Learning. In
Proc. of the 12th SEMDIAL Workshop (LONdial).
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing, 8(1).
T. Paek. 2006. Reinforcement learning for spoken dia-
logue systems: Comparing strengths and weaknesses
for practical deployment. In Proc. Dialog-on-Dialog
Workshop, Interspeech.
O. Pietquin and T. Dutoit. 2006. A probabilistic
framework for dialog simulation and optimal strategy
learnin. IEEE Transactions on Audio, Speech and
Language Processing, 14(2):589?599.
T. Prommer, H. Holzapfel, and A. Waibel. 2006. Rapid
simulation-driven reinforcement learning of multi-
modal dialog strategies in human-robot interaction. In
Proc. of Interspeech/ICSLP.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann.
V. Rieser and O. Lemon. 2006a. Cluster-based user sim-
ulations for learning dialogue strategies. In Proc. of
Interspeech/ICSLP.
V. Rieser and O. Lemon. 2006b. Using machine learning
to explore human multimodal clarification strategies.
In Proc. of ACL.
V. Rieser and O. Lemon. 2008a. Automatic learning
and evaluation of user-centered objective functions for
dialogue system optimisation. In LREC.
V. Rieser and O. Lemon. 2008b. Does this list con-
tain what you were searching for? Learning adaptive
dialogue strategies for interactive question answering.
Journal of Natural Language Engineering (special is-
sue on Interactive Question answering, to appear).
V. Rieser, I. Kruijff-Korbayova?, and O. Lemon. 2005. A
corpus collection and annotation framework for learn-
ing multimodal clarification strategies. In Proc. of the
6th SIGdial Workshop.
V. Rieser. 2008. Bootstrapping Reinforcement Learning-
based Dialogue Strategies from Wizard-of-Oz data (to
appear). Ph.D. thesis, Saarland University.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based user simulation for
bootstrapping a POMDP dialogue system. In Proc. of
HLT/NAACL.
D. Shapiro and P. Langley. 2002. Separating skills from
preference: Using learning to program by reward. In
Proc. of the 19th ICML.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with reinforcement
learning: Experiments with the NJFun system. JAIR,
16.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
MIT Press.
M. Walker, J. Fromer, and S. Narayanan. 1998. Learn-
ing optimal dialogue strategies: A case study of a
spoken dialogue agent for email. In Proceedings of
ACL/COLING.
M. Walker, C. Kamm, and D. Litman. 2000. Towards de-
veloping general models of usability with PARADISE.
Journal of Natural Language Engineering, 6(3).
J. Williams and S. Young. 2004. Using Wizard-of-Oz
simulations to bootstrap reinforcement-learning-based
dialog management systems. In Proc. of the 4th SIG-
dial Workshop.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques (2nd Edi-
tion). Morgan Kaufmann.
646
Proceedings of the 43rd Annual Meeting of the ACL, pages 239?246,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Implications for Generating Clarification Requests in Task-oriented
Dialogues
Verena Rieser
Department of Computational Linguistics
Saarland University
Saarbru?cken, D-66041
vrieser@coli.uni-sb.de
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
J.Moore@ed.ac.uk
Abstract
Clarification requests (CRs) in conversa-
tion ensure and maintain mutual under-
standing and thus play a crucial role in
robust dialogue interaction. In this pa-
per, we describe a corpus study of CRs
in task-oriented dialogue and compare our
findings to those reported in two prior
studies. We find that CR behavior in
task-oriented dialogue differs significantly
from that in everyday conversation in a
number of ways. Moreover, the dialogue
type, the modality and the channel qual-
ity all influence the decision of when to
clarify and at which level of the ground-
ing process. Finally we identify form-
function correlations which can inform the
generation of CRs.
1 Introduction
Clarification requests in conversation ensure and
maintain mutual understanding and thus play a sig-
nificant role in robust and efficient dialogue interac-
tion. From a theoretical perspective, the model of
grounding explains how mutual understanding is es-
tablished. According to Clark (1996), speakers and
listeners ground mutual understanding on four lev-
els of coordination in an action ladder, as shown in
Table 1.
Several current research dialogue systems can de-
tect errors on different levels of grounding (Paek
and Horvitz, 2000; Larsson, 2002; Purver, 2004;
Level Speaker S Listener L
Convers. S is proposing activity
?
L is considering pro-
posal ?
Intention S is signalling that p L is recognizing that p
Signal S is presenting signal ? L is identifying signal
?
Channel S is executing behavior
?
L is attending to behav-
ior ?
Table 1: Four levels of grounding
Schlangen, 2004). However, only the work of
Purver (2004) addresses the question of how the
source of the error affects the form the CR takes.
In this paper, we investigate the use of form-
function mappings derived from human-human di-
alogues to inform the generation of CRs. We iden-
tify the factors that determine which function a CR
should take and identify function-form correlations
that can be used to guide the automatic generation
of CRs.
In Section 2, we discuss the classification
schemes used in two recent corpus studies of CRs
in human-human dialogue, and assess their applica-
bility to the problem of generating CRs. Section 3
describes the results we obtained by applying the
classification scheme of Rodriguez and Schlangen
(2004) to the Communicator Corpus (Bennett and
Rudnicky, 2002). Section 4 draws general conclu-
sions for generating CRs by comparing our results
to those of (Purver et al, 2003) and (Rodriguez and
Schlangen, 2004). Section 5 describes the correla-
tions between function and form features that are
present in the corpus and their implications for gen-
erating CRs.
239
Attr. Value Category Example
form non Non-Reprise ?What did you say??
wot Conventional ?Sorry??
frg Reprise Fragment ?Edinburgh??
lit Literal Reprise ?You want a flight to Edinburgh??
slu Reprise Sluice ?Where??
sub Wh-substituted Reprise ?You want a flight where??
gap Gap ?You want a flight to...??
fil Gap Filler ?...Edinburgh??
other Other x
readings cla Clausal ?Are you asking/asserting that X??
con Constituent ?What do you mean by X??
lex Lexical ?Did you utter X??
corr Correction ?Did you intend to utter X instead??
other Other x
Table 2: CR classification scheme by PGH
2 CR Classification Schemes
We now discuss two recently proposed classifica-
tion schemes for CRs, and assess their usefulness for
generating CRs in a spoken dialogue system (SDS).
2.1 Purver, Ginzburg and Healey (PGH)
Purver, Ginzburg and Healey (2003) investigated
CRs in the British National Corpus (BNC) (Burnard,
2000). In their annotation scheme, a CR can take
seven distinct surface forms and four readings, as
shown in Table 2. The examples for the form feature
are possible CRs following the statement ?I want a
flight to Edinburgh?. The focus of this classification
scheme is to map semantic readings to syntactic sur-
face forms. The form feature is defined by its rela-
tion to the problematic utterance, i.e., whether a CR
reprises the antecedent utterance and to what extent.
CRs may take the three different readings as defined
by Ginzburg and Cooper (2001), as well as a fourth
reading which indicates a correction.
Although PGH report good coverage of the
scheme on their subcorpus of the BNC (99%), we
found their classification scheme to to be too coarse-
grained to prescribe the form that a CR should take.
As shown in example 1, Reprise Fragments (RFs),
which make up one third of the BNC, are ambigu-
ous in their readings and may also take several sur-
face forms.
(1) I would like to book a flight on Monday.
(a) Monday?
frg, con/cla
(b) Which Monday?
frg, con
(c) Monday the first?
frg, con
(d) The first of May?
frg, con
(e) Monday the first or Monday the eighth?
frg, (exclusive) con
RFs endorse literal repetitions of part of the prob-
lematic utterance (1.a); repetitions with an addi-
tional question word (1.b); repetition with further
specification (1.c); reformulations (1.d); and alter-
native questions (1.e)1.
In addition to being too general to describe such
differences, the classification scheme also fails to
describe similarities. As noted by (Rodriguez and
Schlangen, 2004), PGH provide no feature to de-
scribe the extent to which an RF repeats the prob-
lematic utterance.
Finally, some phenomena cannot be described at
all by the four readings. For example, the readings
do not account for non-understanding on the prag-
matic level. Furthermore the readings may have sev-
eral problem sources: the clausal reading may be
appropriate where the CR initiator failed to recog-
nise the word acoustically as well as when he failed
to resolve the reference. Since we are interested in
generating CRs that indicate the source of the error,
we need a classification scheme that represents such
information.
2.2 Rodriguez and Schlangen (R&S)
Rodriguez and Schlangen (2004) devised a multi-
dimensional classification scheme where form and
1Alternative questions would be interpreted as asking a polar
question with an exclusive reading.
240
function are meta-features taking sub-features as at-
tributes. The function feature breaks down into
the sub-features source, severity, extent, reply and
satisfaction. The sources that might have caused
the problem map to the levels as defined by Clark
(1996). These sources can also be of different
severity. The severity can be interpreted as de-
scribing the set of possible referents: asking for
repetition indicates that no interpretation is avail-
able (cont-rep); asking for confirmation means
that the CR initiator has some kind of hypothesis
(cont-conf). The extent of a problem describes
whether the CR points out a problematic element in
the problem utterance. The reply represents the an-
swer the addressee gives to the CR. The satisfaction
of the CR-initiator is indicated by whether he renews
the request for clarification or not.
The meta-feature form describes how the CR is
lingustically realised. It describes the sentence?s
mood, whether it is grammatically complete, the re-
lation to the antecedent, and the boundary tone. Ac-
cording to R&S?s classification scheme our illustra-
tive example would be annotated as follows2:
(2) I would like to book a flight on Monday.
(a) Monday?
mood: decl
completeness: partial
rel-antecedent: repet
source: acous/np-ref
severity: cont-repet
extent: yes
(b) Which Monday?
mood: wh-question
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-repet
extent: yes
(c) Monday the first?
mood: decl
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-conf
extent: yes
(d) The first of May?
mood: decl
completeness: partial
2The source features answer and satisfaction are ignored as
they depend on how the dialogue continues. The interpretation
of the source is dependent on the reply to the CR. Therefore all
possible interpretations are listed.
rel-antecedent: reformul
source: np-ref
severity: cont-conf
extent: yes
(d) Monday the first or Monday the eighth?
mood: alt-q
completeness: partial
rel-antecedent: addition
source: np-ref
severity: cont-repet
extent: yes
In R&S?s classification scheme, ambiguities
about CRs having different sources cannot be re-
solved entirely as example (2.a) shows. However,
in contrast to PGH, the overall approach is a differ-
ent one: instead of explaining causes of CRs within
a theoretic-semantic model (as the three different
readings of Ginzburg and Cooper (2001) do), they
infer the interpretation of the CR from the context.
Ambiguities get resolved by the reply of the ad-
dressee and the satisfaction of the CR initiator in-
dicates the ?mutually agreed interpretation? .
R&S?s multi-dimensional CR description allows
the fine-grained distinctions needed to generate nat-
ural CRs to be made. For example, PGH?s general
category of RFs can be made more specific via the
values for the feature relation to antecedent. In ad-
dition, the form feature is not restricted to syntax; it
includes features such as intonation and coherence,
which are useful for generating the surface form of
CRs. Furthermore, the multi-dimensional function
feature allows us to describe information relevant to
generating CRs that is typically available in dialogue
systems, such as the level of confidence in the hy-
pothesis and the problem source.
3 CRs in the Communicator Corpus
3.1 Material and Method
Material: We annotated the human-human travel
reservation dialogues available as part of the
Carnegie Mellon Communicator Corpus (Bennett
and Rudnicky, 2002) because we were interested
in studying naturally occurring CRs in task-oriented
dialogue. In these dialogues, an experienced travel
agent is making reservations for trips that people in
the Carnegie Mellon Speech Group were taking in
the upcoming months. The corpus comprises 31 di-
alogues of transcribed telephone speech, with 2098
dialogue turns and 19395 words.
241
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
form:
?
?
?
?
?
?
?
distance-src:
{
1 | 2 | 3 | 4 | 5 | more
}
mood:
{
none | decl | polar-q | wh-q | alt-q | imp | other
}
form:
{
none | particle | partial | complete
}
relation-antecedent:
{
none | add | repet | repet-add | reformul | indep
}
boundary-tone:
{
none | rising | falling | no-appl
}
?
?
?
?
?
?
?
function:
?
?
?
?
?
?
?
?
?
source:
{
none | acous | lex | parsing | np-ref | deitic-ref | act-ref |
int+eval | relevance | belief | ambiguity | scr-several
}
extent:
{
none | fragment | whole
}
severity:
{
none | cont-conf | cont-rep | cont-disamb | no-react
}
answer:
{
none | ans-repet | ans-y/n | ans-reformul | ans-elab |
ans-w-defin | no-react
}
satisfaction:
{
none | happy-yes | happy-no | happy-ambig
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: CR classification scheme
Annotation Scheme: Our annotation scheme,
shown in Figure 1, is an extention of the R&S
scheme described in the previous section. R&S?s
scheme was devised for and tested on the Bielefeld
Corpus of German task-oriented dialogues about
joint problem solving.3 To annotate the Commu-
nicator Corpus we extended the scheme in the fol-
lowing ways. First, we found the need to distin-
guish CRs that consist only of newly added infor-
mation, as in example 3, from those that add in-
formation while also repeating part of the utterance
to be clarified, as in 4. We augmented the scheme
to allow two distinct values for the form feature
relation-antecedent, add for cases like 3
and repet-add for cases like 4.
(3) Cust: What is the last flight I could come back on?
Agent: On the 29th of March?
(4) Cust: I?ll be returning on Thursday the fifth.
Agent: The fifth of February?
To the function feature source we added the val-
ues belief to cover CRs like 5 and ambiguity
refinement to cover CRs like 6.
(5) Agent: You need a visa.
Cust: I do need one?
Agent: Yes you do.
(6) Agent: Okay I have two options . . . with Hertz . . . if not
they do have a lower rate with Budget and that is
fifty one dollars.
Cust: Per day?
Agent: Per day um mm.
Finally, following Gabsdil (2003) we introduced
an additional value for severity, cont-disamb, to
3http://sfb360.uni-bielefeld.de
cover CRs that request disambiguation when more
than one interpretation is available.
Method: We first identified turns containing CRs,
and then annotated them with form and function fea-
tures. It is not always possible to identify CRs from
the utterance alone. Frequently, context (e.g., the
reaction of the addressee) or intonation is required
to distinguish a CR from other feedback strategies,
such as positive feedback. See (Rieser, 2004) for a
detailed discussion. The annotation was only per-
formed once. The coding scheme is a slight varia-
tion of R&S, which has been shown relaiable with
Kappa of 0.7 for identifying source.
3.2 Forms and Functions of CRs in the
Communicator Corpus
The human-human dialogues in the Communica-
tor Corpus contain 98 CRs in 2098 dialogue turns
(4.6%).
Forms: The frequencies for the values of the
individual form features are shown in Table 3.
The most frequent type of CRs were partial
declarative questions, which combine the mood
value declarative and the completeness value
partial.4 These account for 53.1% of the CRs
in the corpus. Moreover, four of the five most
frequent surface forms of CRs in the Communi-
cator Corpus differ only in the value for the fea-
ture relation-antecedent. They are partial
declaratives with rising boundary tone, that either re-
formulate (7.1%) the problematic utterance, repeat
4Declarative questions cover ?all cases of non-interrogative
word-order, i.e., both declarative sentences and fragments? (Ro-
driguez and Schlangen, 2004).
242
Feature Value Freq. (%)
Mood declarative 65
polar 21
wh-question 7
other 7
Completeness partial 58
complete 38
other 4
Relation antecedent rep-add 27
independent 21
reformulation 19
repetition 18
addition 10
other 5
Boundary tone rising 74
falling 22
other 4
Table 3: Distribution of values for the form features
the problematic constituent (11.2%), add only new
information (7.1%), or repeat the problematic con-
stituent and add new information (10.2%). The fifth
most frequent type is conventional CRs (10.2%).5
Functions: The distributions of the function fea-
tures are given in Figure 4. The most frequent source
of problems was np-reference. Next most frequent
were acoustic problems, possibly due to the poor
channel quality. Third were CRs that enquire about
intention. As indicated by the feature extent, al-
most 80% of CRs point out a specific element of
the problematic utterance. The features severity and
answer illustrate that most of the time CRs request
confirmation of an hypothesis (73.5%) with a yes-
no-answer (64.3%). The majority of the provided
answers were satisfying, which means that the ad-
dressee tends to interpret the CR correctly and an-
swers collaboratively. Only 6.1% of CRs failed to
elicit a response.
4 CRs in Task-oriented Dialogue
4.1 Comparison
In order to determine whether there are differences
as regards CRs between task-oriented dialogues and
everyday conversations, we compared our results to
those of PGH?s study on the BNC and those of R&S
5Conventional forms are ?Excuse me??, ?Pardon??, etc.
Feature Value Freq. (%)
Source np-reference 40
acoustic 31
intention 8
belief 6
ambiguity 4
contact 4
others 3
relevance 2
several 2
Extent yes 80
no 20
Severity confirmation 73
repetition 20
other 7
Answer y/n answer 64
other 15
elaboration 13
no reaction 6
Table 4: Distribution of values for the function fea-
tures
on the Bielefeld Corpus. The BNC contains a 10
million word sub-corpus of English dialogue tran-
scriptions about topics of general interest. PGH
analysed a portion consisting of ca. 10,600 turns,
ca. 150,000 words. R&S annotated 22 dialogues
from the Bielefeld Corpus, consisting of ca. 3962
turns, ca. 36,000 words.
The major differences in the feature distributions
are listed in Table 5. We found that there are no
significant differences between the feature distri-
butions for the Communicator and Bielefeld cor-
pora, but that the differences between Communica-
tor and BNC, and Bielefeld and BNC are significant
at the levels indicated in Table 5 using Pearson?s
?2. The differences between dialogues of differ-
ent types suggest that there is a different grounding
strategy. In task-oriented dialogues we see a trade-
off between avoiding misunderstanding and keeping
the conversation as efficient as possible. The hy-
pothesis that grounding in task-oriented dialogues is
more cautious is supported by the following facts (as
shown by the figures in Table 5):
? CRs are more frequent in task-oriented dia-
logues.
? The overwhelming majority of CRs directly
follow the problematic utterance.
243
Corpus
Feature Communicator Bielefeld BNC
CRs 98 230 418
frequency 4.6% 5.8%*** 3.9%
distance-src=1 92.8%* 94.8%*** 84.4%
no-react 6.1%* 8.7%** 17.0%
cont-conf 73.5%*** 61.7%*** 46.6%
partial 58.2%** 76.5%*** 42.4%
independent 21.4%*** 9.6%*** 44.2%
cont-rep 19.8%*** 14.8%*** 39.5%
y/n-answer 64.3% 44.8% n/a
Table 5: Comparison of CR forms in everyday vs. task-
oriented corpora (* denotes p < .05, ** is p < .01, *** is
p < .005.)
? CRs in everyday conversation fail to elicit a re-
sponse nearly three times as often.6
? Even though dialogue participants seem to
have strong hypotheses, they frequently con-
firm them.
Although grounding is more cautious in task-
oriented dialogues, the dialogue participants try to
keep the dialogue as efficient as possible:
? Most CRs are partial in form.
? Most of the CRs point out one specific element
(with only a minority being independent as
shown in Table 5). Therefore, in task-oriented
dialogues, CRs locate the understanding prob-
lem directly and give partial credit for what was
understood.
? In task-oriented dialogues, the CR-initiator
asks to confirm an hypothesis about what he
understood rather than asking the other dia-
logue participant to repeat her utterance.
? The addressee prefers to give a short y/n answer
in most cases.
Comparing error sources in the two task-oriented
corpora, we found a number of differences as shown
in Table 6. In particular:
6Another factor that might account for these differences is
that the BNC contains multi-party conversations, and questions
in multi-party conversations may be less likely to receive re-
sponses. Furthermore, due to the poor recording quality of the
BNC, many utterances are marked as ?not interpretable?, which
could also lower the response rate.
Corpus
Feature Communicator Bielefeld Significance
contact 4.1% 0 inst n/a
acoustic 30.6% 11.7% ***
lexical 1 inst 1 inst n/a
parsing 1 inst 0 inst n/a
np-ref 39.8% 24.4% **
deict-ref 1 inst 27.4% ***
ambiguity 4.1% not eval. n/a
belief 6.1% not eval. n/a
relevance 2.1% not eval. n/a
intention 8.2% 22.2% **
several 2.0% 14.3% ***
Table 6: Comparison of CR problem sources in task-oriented
corpora
? Dialogue type: Belief and ambiguity refine-
ment do not seem to be a source of problems
in joint problem solving dialogues, as R&S did
not include them in their annotation scheme.
For CRs in information seeking these features
need to be added to explain quite frequent phe-
nomena. As shown in Table 6, 10.2% of CRs
were in one of these two classes.
? Modality: Deictic reference resolution causes
many more understanding difficulties in dia-
logues where people have a shared point of
view than in telephone communication (Biele-
feld: most frequent problem source; Communi-
cator: one instance detected). Furthermore, in
the Bielefeld Corpus, people tend to formulate
more fragmentary sentences. In environments
where people have a shared point of view, com-
plete sentences can be avoided by using non-
verbal communication channels. Finally, we
see that establishing contact is more of a prob-
lem when speech is the only modality available.
? Channel quality: Acoustic problems are much
more likely in the Communicator Corpus.
These results indicate that the decision process for
grounding needs to consider the modality, the do-
main, and the communication channel. Similar ex-
tensions to the grounding model are suggested by
(Traum, 1999).
244
4.2 Consequences for Generation
The similarities and differences detected can be
used to give recommendations for generating CRs.
In terms of when to initiate a CR, we can state
that clarification should not be postponed, and im-
mediate, local management of uncertainty is criti-
cal. This view is also supported by observations of
how non-native speakers handle non-understanding
(Paek, 2003).
Furthermore, for task-oriented dialogues the sys-
tem should present an hypothesis to be confirmed,
rather than ask for repetition. Our data suggests that,
when they are confronted with uncertainty, humans
tend to build up hypotheses from the dialogue his-
tory and from their world knowledge. For example,
when the customer specified a date without a month,
the travel agent would propose the most reasonable
hypothesis instead of asking a wh-question. It is in-
teresting to note that Skantze (2003) found that users
are more satisfied if the system ?hides? its recog-
nition problem by asking a task-related question to
help to confirm the hypothesis, rather than explicitly
indicating non-understanding.
5 Correlations between Function and
Form: How to say it?
Once the dialogue system has decided on the func-
tion features, it must find a corresponding surface
form to be generated. Many forms are indeed re-
lated to the function as shown in Table 7, where we
present a significance analysis using Pearson?s ?2
(with Yates correction).
Source: We found that the relation to the an-
tecedent seems to distinguish fairly reliably be-
tween CRs clarifying reference and those clarify-
ing acoustic understanding. In the Communicator
Corpus, for acoustic problems the CR-initiator tends
to repeat the problematic part literally, while refer-
ence problems trigger a reformulation or a repeti-
tion with addition. For both problem sources, par-
tial declarative questions are preferred. These find-
ings are also supported by R&S. For the first level
of non-understanding, the inability to establish con-
tact, complete polar questions with no relation to the
antecedent are formulated, e.g., ?Are you there??.
Severity: The severity indicates how much was
understood, i.e., whether the CR initiator asks to
confirm an hypothesis or to repeat the antecedent
utterance. The severity of an error strongly cor-
relates with the sentence mood. Declarative and
polar questions, which take up material from the
problematic utterance, ask to confirm an hypothe-
sis. Wh-questions, which are independent, refor-
mulations or repetitions with additions (e.g., wh-
substituted reprises) of the problematic utterance
usually prompt for repetition, as do imperatives. Al-
ternative questions prompt the addressee to disam-
biguate the hypothesis.
Answer: By definition, certain types of question
prompt for certain answers. Therefore, the feature
answer is closely linked to the sentence mood of
the CR. As polar questions and declarative ques-
tions generally enquire about a proposition, i.e., an
hypothesis or belief, they tend to receive yes/no
answers, but repetitions are also possible. Wh-
questions, alternative questions and imperatives tend
to get answers providing additional information (i.e.,
reformulations and elaborations).
Extent: The function feature extent is logically in-
dependent from the form feature completeness, al-
though they are strongly correlated. Extent is a bi-
nary feature indicating whether the CR points out
a specific element or concerns the whole utterance.
Most fragmentary declarative questions and frag-
mentary polar questions point out a specific element,
especially when they are not independent but stand
in some relation to the antecedent utterance. In-
dependent complete imperatives address the whole
previous utterance.
The correlations found in the Communicator Cor-
pus are fairly consistent with those found in the
Bielefeld Corpus, and thus we believe that the guide-
lines for generating CRs in task-oriented dialogues
may be language independent, at least for German
and English.
6 Summary and Future Work
In this paper we presented the results of a corpus
study of naturally occurring CRs in task-oriented di-
alogue. Comparing our results to two other stud-
ies, one of a task-oriented corpus and one of a cor-
245
Function
Form source severity extent answer
mood ?
2(24) = 112.20
p < 0.001
?2(5) = 30.34
p < 0.001
?2(5) = 24.25
df = p < 0.005
?2(5) = 25.19
p < 0.001
bound-tone indep. indep. indep. indep.
rel-antec ?
2(24) = 108.23
p < 0.001
?2(4) = 11.69
p < 0.005
?2(4) = 42.58
p < 0.001
indep.
complete ?
2(7) = 27.39
p < 0.005
indep. ?
2(1) = 27.39
p < 0.001
indep.
Table 7: Significance analysis for form/function correlations.
pus of everyday conversation, we found no signif-
icant differences in frequency of CRs and distribu-
tion of forms in the two task-oriented corpora, but
many significant differences between CRs in task-
oriented dialogue and everyday conversation. Our
findings suggest that in task-oriented dialogues, hu-
mans use a cautious, but efficient strategy for clar-
ification, preferring to present an hypothesis rather
than ask the user to repeat or rephrase the problem-
atic utterance. We also identified correlations be-
tween function and form features that can serve as
a basis for generating more natural sounding CRs,
which indicate a specific problem with understand-
ing. In current work, we are studying data collected
in a wizard-of-oz study in a multi-modal setting, in
order to study clarification behavior in multi-modal
dialogue.
Acknowledgements
The authors would like thank Kepa Rodriguez, Oliver Lemon,
and David Reitter for help and discussion.
References
Christina L. Bennett and Alexander I. Rudnicky. 2002.
The Carnegie Mellon Communicator Corpus. In Pro-
ceedings of the International Conference of Spoken
Language Processing (ICSLP02).
Lou Burnard. 2000. The British National Corpus Users
Reference Guide. Technical report, Oxford Universiry
Computing Services.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press.
Malte Gabsdil. 2003. Clarification in Spoken Dialogue
Systems. Proceedings of the 2003 AAAI Spring Sym-
posium. Workshop on Natural Language Generation in
Spoken and Written Dialogue.
Jonathan Ginzburg and Robin Cooper. 2001. Resolving
Ellipsis in Clarification. In Proceedings of the 39th
meeting of the Association for Computational Linguis-
tics.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, Goteborg University.
Tim Paek and Eric Horvitz. 2000. Conversation as Ac-
tion Under Uncertainty. In Proceedings of the Six-
teenth Conference on Uncertainty in Artificial Intelli-
gence.
Tim Paek. 2003. Toward a Taxonomy of Communica-
tion Errors. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
Matthew Purver, Jonathan Ginzburg, and Patrick Healey.
2003. On the Means for Clarification in Dialogue. In
R. Smith and J. van Kuppevelt, editors, Current and
New Directions in Discourse and Dialogue.
Matthew Purver. 2004. CLARIE: The Clarification En-
gine. In Proceedings of the Eighth Workshop on For-
mal Semantics and Dialogue.
Verena Rieser. 2004. Fragmentary Clarifications on Sev-
eral Levels for Robust Dialogue Systems. Master?s
thesis, School of Informatics, University of Edinburgh.
Kepa J. Rodriguez and David Schlangen. 2004. Form,
Intonation and Function of Clarification Requests in
German Task-orientaded Spoken Dialogues. In Pro-
ceedings of the Eighth Workshop on Formal Semantics
and Dialogue.
David Schlangen. 2004. Causes and Strategies for Re-
question Clarification in Dialogue. Proceedings of the
5th SIGdial Workshop on Discourse and Dialogue.
Gabriel Skantze. 2003. Exploring Human Error Han-
dling Strategies: Implications for Spoken Dialogue
Systems. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
David R. Traum. 1999. Computational Models of
Grounding in Collaborative Systems. In Proceedings
of the AAAI Fall Symposium on Psychological Models
of Communication.
246
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 659?666,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Machine Learning to Explore Human Multimodal Clarification
Strategies
Verena Rieser
Department of Computational Linguistics
Saarland University
Saarbru?cken, D-66041
vrieser@coli.uni-sb.de
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
olemon@inf.ed.ac.uk
Abstract
We investigate the use of machine learn-
ing in combination with feature engineer-
ing techniques to explore human multi-
modal clarification strategies and the use
of those strategies for dialogue systems.
We learn from data collected in a Wizard-
of-Oz study where different wizards could
decide whether to ask a clarification re-
quest in a multimodal manner or else use
speech alone. We show that there is a
uniform strategy across wizards which is
based on multiple features in the context.
These are generic runtime features which
can be implemented in dialogue systems.
Our prediction models achieve a weighted
f-score of 85.3% (which is a 25.5% im-
provement over a one-rule baseline). To
assess the effects of models, feature dis-
cretisation, and selection, we also conduct
a regression analysis. We then interpret
and discuss the use of the learnt strategy
for dialogue systems. Throughout the in-
vestigation we discuss the issues arising
from using small initial Wizard-of-Oz data
sets, and we show that feature engineer-
ing is an essential step when learning from
such limited data.
1 Introduction
Good clarification strategies in dialogue systems
help to ensure and maintain mutual understand-
ing and thus play a crucial role in robust conversa-
tional interaction. In dialogue application domains
with high interpretation uncertainty, for example
caused by acoustic uncertainties from a speech
recogniser, multimodal generation and input leads
to more robust interaction (Oviatt, 2002) and re-
duced cognitive load (Oviatt et al, 2004). In this
paper we investigate the use of machine learning
(ML) to explore human multimodal clarification
strategies and the use of those strategies to decide,
based on the current dialogue context, when a di-
alogue system?s clarification request (CR) should
be generated in a multimodal manner.
In previous work (Rieser and Moore, 2005)
we showed that for spoken CRs in human-
human communication people follow a context-
dependent clarification strategy which systemati-
cally varies across domains (and even across Ger-
manic languages). In this paper we investigate
whether there exists a context-dependent ?intu-
itive? human strategy for multimodal CRs as well.
To test this hypothesis we gathered data in a
Wizard-of-Oz (WOZ) study, where different wiz-
ards could decide when to show a screen output.
From this data we build prediction models, using
supervised learning techniques together with fea-
ture engineering methods, that may explain the un-
derlying process which generated the data. If we
can build a model which predicts the data quite re-
liably, we can show that there is a uniform strategy
that the majority of our wizards followed in certain
contexts.
Figure 1: Methodology and structure
The overall method and corresponding structure
of the paper is as shown in figure 1. We proceed
659
as follows. In section 2 we present the WOZ cor-
pus from which we extract a potential context us-
ing ?Information State Update? (ISU)-based fea-
tures (Lemon et al, 2005), listed in section 3. We
also address the question how to define a suit-
able ?local? context definition for the wizard ac-
tions. We apply the feature engineering methods
described in section 4 to address the questions of
unique thresholds and feature subsets across wiz-
ards. These techniques also help to reduce the
context representation and thus the feature space
used for learning. In section 5 we test different
classifiers upon this reduced context and separate
out the independent contribution of learning al-
gorithms and feature engineering techniques. In
section 6 we discuss and interpret the learnt strat-
egy. Finally we argue for the use of reinforcement
learning to optimise the multimodal clarification
strategy.
2 The WOZ Corpus
The corpus we are using for learning was col-
lected in a multimodal WOZ study of German
task-oriented dialogues for an in-car music player
application, (Kruijff-Korbayova? et al, 2005) . Us-
ing data from a WOZ study, rather than from real
system interactions, allows us to investigate how
humans clarify. In this study six people played the
role of an intelligent interface to an MP3 player
and were given access to a database of informa-
tion. 24 subjects were given a set of predefined
tasks to perform using an MP3 player with a mul-
timodal interface. In one part of the session the
users also performed a primary driving task, us-
ing a driving simulator. The wizards were able
to speak freely and display the search results or
the playlist on the screen by clicking on vari-
ous pre-computed templates. The users were also
able to speak, as well as make selections on the
screen. The user?s utterances were immediately
transcribed by a typist. The transcribed user?s
speech was then corrupted by deleting a varying
number of words, simulating understanding prob-
lems at the acoustic level. This (sometimes) cor-
rupted transcription was then presented to the hu-
man wizard. Note that this environment introduces
uncertainty on several levels, for example multiple
matches in the database, lexical ambiguities, and
errors on the acoustic level, as described in (Rieser
et al, 2005). Whenever the wizard produced a
CR, the experiment leader invoked a questionnaire
window on a GUI, where the wizard classified
their CR according to the primary source of the
understanding problem, mapping to the categories
defined by (Traum and Dillenbourg, 1996).
2.1 The Data
The corpus gathered with this setup comprises
70 dialogues, 1772 turns and 17076 words. Ex-
ample 1 shows a typical multimodal clarification
sub-dialogue, 1 concerning an uncertain reference
(note that ?Venus? is an album name, song title,
and an artist name), where the wizard selects a
screen output while asking a CR.
(1) User: Please play ?Venus?.
Wizard: Does this list contain the song?
[shows list with 20 DB matches]
User: Yes. It?s number 4. [clicks on item 4]
For each session we gathered logging informa-
tion which consists of e.g., the transcriptions of
the spoken utterances, the wizard?s database query
and the number of results, the screen option cho-
sen by the wizard, classification of CRs, etc. We
transformed the log-files into an XML structure,
consisting of sessions per user, dialogues per task,
and turns.2
2.2 Data analysis:
Of the 774 wizard turns 19.6% were annotated
as CRs, resulting in 152 instances for learning,
where our six wizards contributed about equal
proportions. A ?2 test on multimodal strategy
(i.e. showing a screen output or not with a CR)
showed significant differences between wizards
(?2(1) = 34.21, p < .000). On the other hand, a
Kruskal-Wallis test comparing user preference for
the multimodal output showed no significant dif-
ference across wizards (H(5)=10.94, p > .05). 3
Mean performance ratings for the wizards? multi-
modal behaviour ranged from 1.67 to 3.5 on a five-
point Likert scale. Observing significantly differ-
ent strategies which are not significantly different
in terms of user satisfaction, we conjecture that the
wizards converged on strategies which were ap-
propriate in certain contexts. To strengthen this
1Translated from German.
2Where a new ?turn? begins at the start of each new user
utterance after a wizard utterance, taking the user utterance as
a most basic unit of dialogue progression as defined in (Paek
and Chickering, 2005).
3The Kruskal-Wallis test is the non-parametric equivalent
to a one-way ANOVA. Since the users indicated their satis-
faction on a 5-point likert scale, an ANOVA which assumes
normality would be invalid.
660
hypothesis we split the data by wizard and and per-
formed a Kruskal-Wallis test on multimodal be-
haviour per session. Only the two wizards with the
lowest performance score showed no significant
variation across session, whereas the wizards with
the highest scores showed the most varying be-
haviour. These results again indicate a context de-
pendent strategy. In the following we test this hy-
pothesis (that good multimodal clarification strate-
gies are context-dependent) by building a predic-
tion model of the strategy an average wizard took
dependent on certain context features.
3 Context/Information-State Features
A state or context in our system is a dialogue in-
formation state as defined in (Lemon et al, 2005).
We divide the types of information represented
in the dialogue information state into local fea-
tures (comprising low level and dialogue features),
dialogue history features, and user model fea-
tures. We also defined features reflecting the ap-
plication environment (e.g. driving). All fea-
tures are automatically extracted from the XML
log-files (and are available at runtime in ISU-
based dialogue systems). From these features we
want to learn whether to generate a screen out-
put (graphic-yes), or whether to clarify using
speech only (graphic-no). The case that the
wizard only used screen output for clarification did
not occur.
3.1 Local Features
First, we extracted features present in the ?lo-
cal? context of a CR, such as the number
of matches returned from the data base query
(DBmatches), how many words were deleted
by the corruption algorithm4 (deletion), what
problem source the wizard indicated in the pop-
up questionnaire (source), the previous user
speech act (userSpeechAct), and the delay be-
tween the last wizard utterance and the user?s reply
(delay). 5
One decision to take for extracting these local
features was how to define the ?local? context of
a CR. As shown in table 1, we experimented with
a number of different context definitions. Context
1 defined the local context to be the current turn
only, i.e. the turn containing the CR. Context 2
4Note that this feature is only an approximation of the
ASR confidence score that we would expect in an automated
dialogue system. See (Rieser et al, 2005) for full details.
5We introduced the delay feature to handle clarifications
concerning contact.
id Context (turns) acc/ wf-
score ma-
jority(%)
acc/ wf-score
Na??ve Bayes
(%)
1 only current turn 83.0/54.9 81.0/68.3
2 current and next 71.3/50.4 72.01/68.2
3 current and previous 60.50/59.8 76.0*/75.3
4 previous, current, next 67.8/48.9 76.9*/ 74.8
Table 1: Comparison of context definitions for lo-
cal features (* denotes p < .05)
also considered the current turn and the turn fol-
lowing (and is thus not a ?runtime? context). Con-
text 3 considered the current turn and the previous
turn. Context 4 is the maximal definition of a lo-
cal context, namely the previous, current, and next
turn (also not available at runtime). 6
To find the context type which provides the rich-
est information to a classifier, we compared the ac-
curacy achieved in a 10-fold cross validation by
a Na??ve Bayes classifier (as a standard) on these
data sets against the majority class baseline, us-
ing a paired t-test, we found that that for context
3 and context 4, Na??ve Bayes shows a significant
improvement (with p < .05 using Bonferroni cor-
rection). In table 1 we also show the weighted
f-scores since they show that the high accuracy
achieved using the first two contexts is due to over-
prediction. We chose to use context 3, since these
features will be available during system runtime
and the learnt strategy could be implemented in an
actual system.
3.2 Dialogue History Features
The history features account for events in the
whole dialogue so far, i.e. all information gath-
ered before asking the CR, such as the number of
CRs asked (CRhist), how often the screen output
was already used (screenHist), the corruption
rate so far (delHist), the dialogue duration so
far (duration), and whether the user reacted to
the screen output, either by verbally referencing
(refHist) , e.g. using expressions such as ?It?s
item number 4?, or by clicking (clickHist) as
in example 1.
3.3 User Model Features
Under ?user model features? we consider features
reflecting the wizards? responsiveness to the be-
6Note that dependent on the context definition a CR
might get annotated differently, since placing the question
and showing the graphic might be asynchronous events.
661
haviour and situation of the user. Each session
comprised four dialogues with one wizard. The
user model features average the user?s behaviour
in these dialogues so far, such as how responsive
the user is towards the screen output, i.e. how of-
ten this user clicks (clickUser) and how fre-
quently s/he uses verbal references (refUser);
how often the wizard had already shown a screen
output (screenUser) and how many CRs were
already asked (CRuser); how much the user?s
speech was corrupted on average (delUser), i.e.
an approximation of how well this user is recog-
nised; and whether this user is currently driving or
not (driving). This information was available
to the wizard.
LOCAL FEATURES
DBmatches: 20
deletion: 0
source: reference resolution
userSpeechAct: command
delay: 0
HISTORY FEATURES
[CRhist, screenHist, delHist,
refHist, clickHist]=0
duration= 10s
USER MODEL FEATURES
[clickUser,refUser,screenUser,
CRuser]=0
driving= true
Figure 2: Features in the context after the first turn
in example 1.
3.4 Discussion
Note that all these features are generic over
information-seeking dialogues where database re-
sults can be displayed on a screen; except for
driving which only applies to hands-and-eyes-
busy situations. Figure 2 shows a context for ex-
ample 1, assuming that it was the first utterance by
this user.
This potential feature space comprises 18 fea-
tures, many of them taking numeric attributes as
values. Considering our limited data set of 152
training instances we run the risk of severe data
sparsity. Furthermore we want to explore which
features of this potential feature space influenced
the wizards? multimodal strategy. In the next
two sections we describe feature engineering tech-
niques, namely discretising methods for dimen-
sionality reduction and feature selection methods,
which help to reduce the feature space to a sub-
set which is most predictive of multimodal clarifi-
cation. For our experiments we use implementa-
tions of discretisation and feature selection meth-
ods provided by the WEKA toolkit (Witten and
Frank, 2005).
4 Feature Engineering
4.1 Discretising Numeric Features
Global discretisation methods divide all contin-
uous features into a smaller number of distinct
ranges before learning starts. This has two advan-
tages concerning the quality of our data for ML.
First, discretisation methods take feature distribu-
tions into account and help to avoid sparse data.
Second, most of our features are highly positively
skewed. Some ML methods (such as the standard
extension of the Na??ve Bayes classifier to handle
numeric features) assume that numeric attributes
have a normal distribution. We use Proportional
k-Interval (PKI) discretisation as a unsupervised
method, and an entropy-based algorithm (Fayyad
and Irani, 1993) based on the Minimal Description
Length (MDL) principle as a supervised discreti-
sation method.
4.2 Feature Selection
Feature selection refers to the problem of select-
ing an optimum subset of features that are most
predictive of a given outcome. The objective of se-
lection is two-fold: improving the prediction per-
formance of ML models and providing a better un-
derstanding of the underlying concepts that gener-
ated the data. We chose to apply forward selec-
tion for all our experiments given our large fea-
ture set, which might include redundant features.
We use the following feature filtering methods:
correlation-based subset evaluation (CFS) (Hall,
2000) and a decision tree algorithm (rule-based
ML) for selecting features before doing the actual
learning. We also used a wrapper method called
Selective Na??ve Bayes, which has been shown to
perform reliably well in practice (Langley and
Sage, 1994). We also apply a correlation-based
ranking technique since subset selection models
inner-feature relations at the expense of saying
less about individual feature performance itself.
4.3 Results for PKI and MDL Discretisation
Feature selection and discretisation influence one-
another, i.e. feature selection performs differently
on PKI or MDL discretised data. MDL discreti-
sation reduces our range of feature values dra-
matically. It fails to discretise 10 of 14 nu-
meric features and bars those features from play-
ing a role in the final decision structure because
the same discretised value will be given to all
instances. However, MDL discretisation cannot
replace proper feature selection methods since
662
Table 2: Feature selection on PKI-discretised data (left) and on MDL-discretised data (right)
it doesn?t explicitly account for redundancy be-
tween features, nor for non-numerical features.
For the other 4 features which were discretised
there is a binary split around one (fairly low)
threshold: screenHist (.5), refUser (.375),
screenUser (1.0), CRUser (1.25).
Table 2 shows two figures illustrating the dif-
ferent subsets of features chosen by the feature
selection algorithms on discretised data. From
these four subsets we extracted a fifth, using all
the features which were chosen by at least two
of the feature selection methods, i.e. the features
in the overlapping circle regions shown in figure
2. For both data sets the highest ranking fea-
tures are also the ones contained in the overlapping
regions, which are screenUser, refUser
and screenHist. For implementation dialogue
management needs to keep track of whether the
user already saw a screen output in a previous in-
teraction (screenUser), or in the same dialogue
(screenHist), and whether this user (verbally)
reacted to the screen output (refUser).
5 Performance of Different Learners and
Feature Engineering
In this section we evaluate the performance of fea-
ture engineering methods in combination with dif-
ferent ML algorithms (where we treat feature op-
timisation as an integral part of the training pro-
cess). All experiments are carried out using 10-
fold cross-validation. We take an approach similar
to (Daelemans et al, 2003) where parameters of
the classifier are optimised with respect to feature
selection. We use a wide range of different multi-
variate classifiers which reflect our hypothesis that
a decision is based on various features in the con-
text, and compare them against two simple base-
line strategies, reflecting deterministic contextual
behaviour.
5.1 Baselines
The simplest baseline we can consider is to always
predict the majority class in the data, in our case
graphic-no. This yields a 45.6% wf-score.
This baseline reflects a deterministic wizard strat-
egy never showing a screen output.
A more interesting baseline is obtained by us-
ing a 1-rule classifier. It chooses the feature
which produces the minimum error (which is
refUser for the PKI discretised data set, and
screenHist for the MDL set). We use the im-
plementation of a one-rule classifier provided in
the WEKA toolkit. This yields a 59.8% wf-score.
This baseline reflects a deterministic wizard strat-
egy which is based on a single feature only.
5.2 Machine Learners
For learning we experiment with five different
types of supervised classifiers.We chose Na??ve
Bayes as a joint (generative) probabilistic model,
using the WEKA implementation of (John and Lan-
gley, 1995)?s classifier; Bayesian Networks as a
graphical generative model, again using the WEKA
implementation; and we chose maxEnt as a dis-
criminative (conditional) model, using the Max-
imum Entropy toolkit (Le, 2003). As a rule in-
duction algorithm we used JRIP, the WEKA imple-
mentation of (Cohen, 1995)?s Repeated Incremen-
tal Pruning to Produce Error Reduction (RIPPER).
And for decision trees we used the J4.8 classi-
fier (WEKA?s implementation of the C4.5 system
(Quinlan, 1993)).
5.3 Comparison of Results
We experimented using these different classifiers
on raw data, on MDL and PKI discretised data,
and on discretised data using the different fea-
ture selection algorithms. To compare the clas-
sification outcomes we report on two measures:
accuracy and wf-score, which is the weighted
663
Feature transformation/
(acc./ wf-score (%))
1-rule
baseline
Rule
Induction
Decision
Tree
maxEnt Na??ve
Bayes
Bayesian
Network
Average
raw data 60.5/59.8 76.3/78.3 79.4/78.6 70.0/75.3 76.0/75.3 79.5/72.0 73.62/73.21
PKI + all features 60.5/ 64.6 67.1/66.4 77.4/76.3 70.7/76.7 77.5/81.6 77.3/82.3 71.75/74.65
PKI+ CFS subset 60.5/64.4 68.7/70.7 79.2/76.9 76.7/79.4 78.2/80.6 77.4/80.7 73.45/75.45
PKI+ rule-based ML 60.5/66.5 72.8/76.1 76.0/73.9 75.3/80.2 80.1/78.3 80.8/79.8 74.25/75.80
PKI+ selective Bayes 60.5/64.4 68.2/65.2 78.4/77.9 79.3/78.1 84.6/85.3 84.5/84.6 75.92/75.92
PKI+ subset overlap 60.5/64.4 70.9/70.7 75.9/76.9 76.7/78.2 84.0/80.6 83.7/80.7 75.28/75.25
MDL + all features 60.5/69.9 79.0/78.8 78.0/78.1 71.3/76.8 74.9/73.3 74.7/73.9 73.07/75.13
MDL + CFS subset 60.5/69.9 80.1/78.2 80.6/78.2 76.0/80.2 75.7/75.8 75.7/75.8 74.77/76.35
MDL + rule-based ML 60.5/75.5 80.4/81.6 78.7/80.2 79.3/78.8 82.7/82.9 82.7/82.9 77.38/80.32
MDL + select. Bayes 60.5/75.5 80.4/81.6 78.7/80.8 79.3/80.1 82.7/82.9 82.7/82.9 77.38/80.63
MDL + overlap 60.5/75.5 80.4/81.6 78.7/80.8 79.3/80.1 82.7/82.9 82.7/82.9 77.38/80.63
average 60.5/68.24 74.9/75.38 78.26/78.06 75.27/78.54 79.91/79.96 80.16/79.86
Table 3: Average accuracy and wf-scores for models in feature engineering experiments .
sum (by class frequency in the data; 39.5%
graphic-yes, 60.5% graphic-no) of the f-
scores of the individual classes. In table 3 we
see fairly stable high performance for Bayesian
models with MDL feature selection. However, the
best performing model is Na??ve Bayes using wrap-
per methods (selective Bayes) for feature selection
and PKI discretisation. This model achieves a wf-
score of 85.3%, which is a 25.5% improvement
over the 1-rule baseline.
We separately explore the models and feature
engineering techniques and their impact on the
prediction accuracy for each trial/cross-validation.
In the following we separate out the independent
contribution of models and features. To assess
the effects of models, feature discretisation and
selection on performance accuracy, we conduct
a hierarchical regression analysis. The models
alone explain 18.1% of the variation in accuracy
(R2 = .181) whereas discretisation methods only
contribute 0.4% and feature selection 1% (R2 =
.195). All parameters, except for discretisation
methods have a significant impact on modelling
accuracy (P < .001), indicating that feature selec-
tion is an essential step for predicting wizard be-
haviour. The coefficients of the regression model
lead us to the following hypotheses which we ex-
plore by comparing the group means for models,
discretisation, and features selection methods. Ap-
plying a Kruskal-Wallis test with Mann-Whitney
tests as a post-hoc procedure (using Bonferroni
correction for multiple comparisons), we obtained
the following results: 7
? All ML algorithms are significantly better
than the majority and one-rule baselines. All
7We cannot report full details here. Supplementary
material is available at www.coli.uni-saarland.de/
?vrieser/acl06-supplementary.html
except maxEnt are significantly better than
the Rule Induction algorithm. There is no
significant difference in the performance of
Decision Tree, maxEnt, Na??ve Bayes, and
Bayesian Network classifiers. Multivariate
models being significantly better than the
two baseline models indicates that we have
a strategy that is based on context features.
? For discretisation methods we found that the
classifiers were performing significantly bet-
ter on MDL discretised data than on PKI or
continuous data. MDL being significantly
better than continuous data indicates that all
wizards behaved as though using thresholds
to make their decisions, and MDL being bet-
ter than PKI supports the hypothesis that de-
cisions were context dependent.
? All feature selection methods (except for
CFS) lead to better performance than using
all of the features. Selective Bayes and rule-
based ML selection performed significantly
better than CFS. Selective Bayes, rule-based
ML, and subset-overlap showed no signifi-
cant differences. These results show that wiz-
ards behaved as though specific features were
important (but they suggest that inner-feature
relations used by CFS are less important).
Discussion of results: These experimental re-
sults show two things. First, the results indi-
cate that we can learn a good prediction model
from our data. We conclude that our six wiz-
ards did not behave arbitrarily, but selected their
strategy according to certain contextual features.
By separating out the individual contributions of
models and feature engineering techniques, we
have shown that wizard behaviour is based on
multiple features. In sum, Decision Tree, max-
664
Ent, Na??ve Bayes, and Bayesian Network clas-
sifiers on MDL discretised data using Selective
Bayes and Rule-based ML selection achieved
the best results. The best performing feature
subset was screenUser,screenHist, and
userSpeechAct. The best performing model
uses the richest feature space including the feature
driving.
Second, the regression analysis shows that us-
ing these feature engineering techniques in combi-
nation with improved ML algorithms is an essen-
tial step for learning good prediction models from
the small data sets which are typically available
from multimodal WOZ studies.
6 Interpretation of the learnt Strategy
For interpreting the learnt strategies we discuss
Rule Induction and Decision Trees since they are
the easiest to interpret (and to implement in stan-
dard rule-based dialogue systems). For both we
explain the results obtained by MDL and selective
Bayes, since this combination leads to the best per-
formance.
Rule induction: Figure 3 shows a reformula-
tion of the rules from which the learned classifier
is constructed. The feature screenUser plays
a central role. These rules (in combination with
the low thresholds) say that if you have already
shown a screen output to this particular user in
any previous turn (i.e. screenUser > 1), then
do so again if the previous user speech act was
a command (i.e. userSpeechAct=command)
or if you have already shown a screen out-
put in a previous turn in this dialogue (i.e.
screenHist>0.5). Otherwise don?t show
screen output when asking a clarification.
Decision tree: Figure 4 shows the decision tree
learnt by the classifier J4.8. The five rules
contained in this tree also heavily rely on the
user model as well as the previous screen his-
tory. The rules constructed by the first two nodes
(screenUser, screenHist) may lead to a
repetitive strategy since the right branch will result
in the same action (graphic-yes) in all future
actions. The only variation is introduced by the
speech act, collapsing the tree to the same rule set
as in figure 3. Note that this rule-set is based on
domain independent features.
Discussion: Examining the classifications made
by our best performing Bayesian models we found
that the learnt conditional probability distribu-
tions produce similar feature-value mappings to
the rules described above. The strategy learnt
by the classifiers heavily depends on features ob-
tained in previous interactions, i.e. user model fea-
tures. Furthermore these strategies can lead to
repetitive action, i.e. if a screen output was once
shown to this user, and the user has previously
used or referred to the screen, the screen will be
used over and over again.
For learning a strategy which varies in context
but adapts in more subtle ways (e.g. to the user
model), we would need to explore many more
strategies through interactions with users to find
an optimal one. One way to reduce costs for build-
ing such an optimised strategy is to apply Rein-
forcement Learning (RL) with simulated users. In
future work we will begin with the strategy learnt
by supervised learning (which reflects sub-optimal
average wizard behaviour) and optimise it for dif-
ferent user models and reward structures.
Figure 4: Five-rule tree from J4.8 (?inf? = ?)
7 Summary and Future Work
We showed that humans use a context-dependent
strategy for asking multimodal clarification re-
quests by learning such a strategy from WOZ data.
Only the two wizards with the lowest performance
scores showed no significant variation across ses-
sions, leading us to hypothesise that the better wiz-
ards converged on a context-dependent strategy.
We were able to discover a runtime context based
on which all wizards behaved uniformly, using
feature discretisation methods and feature selec-
tion methods on dialogue context features. Based
on these features we were able to predict how
an ?average? wizard would behave in that context
with an accuracy of 84.6% (wf-score of 85.3%,
which is a 25.5% improvement over a one rule-
based baseline). We explained the learned strate-
gies and showed that they can be implemented in
665
IF screenUser>1 AND (userSpeechAct=command OR screenHist>0.5) THEN graphic=yes
ELSE graphic=no
Figure 3: Reformulation of the rules learnt by JRIP
rule-based dialogue systems based on domain in-
dependent features. We also showed that feature
engineering is essential for achieving significant
performance gains when using large feature spaces
with the small data sets which are typical of di-
alogue WOZ studies. By interpreting the learnt
strategies we found them to be sub-optimal. In
current research, RL is applied to optimise strate-
gies and has been shown to lead to dialogue strate-
gies which are better than those present in the orig-
inal data (Henderson et al, 2005). The next step
towards a RL-based system is to add task-level and
reward-level annotations to calculate reward func-
tions, as discussed in (Rieser et al, 2005). We
furthermore aim to learn more refined clarifica-
tion strategies indicating the problem source and
its severity.
Acknowledgements
The authors would like thank the ACL reviewers,
Alissa Melinger, and Joel Tetreault for help and dis-
cussion. This work is supported by the TALK project,
www.talk-project.org, and the International Post-
Graduate College for Language Technology and Cognitive
Systems, Saarbru?cken.
References
William W. Cohen. 1995. Fast effective rule induction.
In Proceedings of the 12th ICML-95.
Walter Daelemans, Ve?ronique Hoste, Fien De Meul-
der, and Bart Naudts. 2003. Combined optimization
of feature selection and algorithm parameter interac-
tion in machine learning of language. In Proceed-
ings of the 14th ECML-03.
Usama Fayyad and Keki Irani. 1993. Multi-
interval discretization of continuousvalued attributes
for classification learning. In Proc. IJCAI-93.
Mark Hall. 2000. Correlation-based feature selection
for discrete and numeric class machine learning. In
Proc. 17th Int Conf. on Machine Learning.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2005. Hybrid Reinforcement/Supervised
Learning for Dialogue Policies from COMMUNI-
CATOR data. In IJCAI workshop on Knowledge and
Reasoning in Practical Dialogue Systems,.
George John and Pat Langley. 1995. Estimating con-
tinuous distributions in bayesian classifiers. In Pro-
ceedings of the 11th UAI-95. Morgan Kaufmann.
Ivana Kruijff-Korbayova?, Nate Blaylock, Ciprian Ger-
stenberger, Verena Rieser, Tilman Becker, Michael
Kaisser, Peter Poller, and Jan Schehl. 2005. An ex-
periment setup for collecting data for adaptive out-
put planning in a multimodal dialogue system. In
10th European Workshop on NLG.
Pat Langley and Stephanie Sage. 1994. Induction of
selective bayesian classifiers. In Proceedings of the
10th UAI-94.
Zhang Le. 2003. Maximum entropy modeling toolkit
for Python and C++.
Oliver Lemon, Kallirroi Georgila, James Henderson,
Malte Gabsdil, Ivan Meza-Ruiz, and Steve Young.
2005. Deliverable d4.1: Integration of learning and
adaptivity with the ISU approach.
Sharon Oviatt, Rachel Coulston, and Rebecca
Lunsford. 2004. When do we interact mul-
timodally? Cognitive load and multimodal
communication patterns. In Proceedings of the 6th
ICMI-04.
Sharon Oviatt. 2002. Breaking the robustness bar-
rier: Recent progress on the design of robust mul-
timodal systems. In Advances in Computers. Aca-
demic Press.
Tim Paek and David Maxwell Chickering. 2005.
The markov assumption in spoken dialogue manage-
ment. In Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Verena Rieser and Johanna Moore. 2005. Implica-
tions for Generating Clarification Requests in Task-
oriented Dialogues. In Proceedings of the 43rd ACL.
Verena Rieser, Ivana Kruijff-Korbayova?, and Oliver
Lemon. 2005. A corpus collection and annota-
tion framework for learning multimodal clarification
strategies. In Proceedings of the 6th SIGdial Work-
shop on Discourse and Dialogue.
David Traum and Pierre Dillenbourg. 1996. Mis-
communication in multi-modal collaboration. In
Proceedings of the Workshop on Detecting, Repair-
ing, and Preventing Human-Machine Miscommuni-
cation. AAAI-96.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques
(Second Edition). Morgan Kaufmann.
666
An Experiment Setup for Collecting Data for Adaptive Output Planning
in a Multimodal Dialogue System
Ivana Kruijff-Korbayova?, Nate Blaylock,
Ciprian Gerstenberger, Verena Rieser
Saarland University, Saarbru?cken, Germany
korbay@coli.uni-sb.de
Tilman Becker, Michael Kai?er,
Peter Poller, Jan Schehl
DFKI, Saarbru?cken, Germany
tilman.becker@dfki.de
Abstract
We describe a Wizard-of-Oz experiment setup for
the collection of multimodal interaction data for a
Music Player application. This setup was devel-
oped and used to collect experimental data as part
of a project aimed at building a flexible multimodal
dialogue system which provides an interface to an
MP3 player, combining speech and screen input
and output. Besides the usual goal of WOZ data
collection to get realistic examples of the behav-
ior and expectations of the users, an equally im-
portant goal for us was to observe natural behavior
of multiple wizards in order to guide our system
development. The wizards? responses were there-
fore not constrained by a script. One of the chal-
lenges we had to address was to allow the wizards
to produce varied screen output a in real time. Our
setup includes a preliminary screen output planning
module, which prepares several versions of possi-
ble screen output. The wizards were free to speak,
and/or to select a screen output.
1 Introduction
In the larger context of the TALK project1 we are develop-
ing a multimodal dialogue system for a Music Player appli-
cation for in-car and in-home use, which should support nat-
ural, flexible interaction and collaborative behavior. The sys-
tem functionalities include playback control, manipulation of
playlists, and searching a large MP3 database. We believe
that in order to achieve this goal, the system needs to provide
advanced adaptive multimodal output.
We are conducting Wizard-of-Oz experiments
[Bernsen et al, 1998] in order to guide the development
of our system. On the one hand, the experiments should
give us data on how the potential users interact with such
an application. But we also need data on the multimodal
interaction strategies that the system should employ to
achieve the desired naturalness, flexibility and collaboration.
We therefore need a setup where the wizard has freedom of
1TALK (Talk and Look: Tools for Ambient Linguistic Knowl-
edge; www.talk-project.org) is funded by the EU as project
No. IST-507802 within the 6th Framework program.
choice w.r.t. their response and its realization through single
or multiple modalities. This makes it different from previous
multimodal experiments, e.g., in the SmartKom project
[Tu?rk, 2001], where the wizard(s) followed a strict script.
But what we need is also different in several aspects from
taking recordings of straight human-human interactions: the
wizard does not hear the user?s input directly, but only gets a
transcription, parts of which are sometimes randomly deleted
(in order to approximate imperfect speech recognition);
the user does not hear the wizard?s spoken output directly
either, as the latter is transcribed and re-synthesized (to
produce system-like sounding output). The interactions
should thus more realistically approximate an interaction
with a system, and thereby contain similar phenomena (cf.
[Duran et al, 2001]).
The wizard should be able to present different screen out-
puts in different context, depending on the search results and
other aspects. However, the wizard cannot design screens on
the fly, because that would take too long. Therefore, we de-
veloped a setup which includes modules that support the wiz-
ard by providing automatically calculated screen output op-
tions the wizard can select from if s/he want to present some
screen output.
Outline In this paper we describe our experiment setup and
the first experiences with it. In Section 2 we overview the
research goals that our setup was designed to address. The
actual setup is presented in detail in Section 3. In Section 4
we describe the collected data, and we summarize the lessons
we learnt on the basis of interviewing the experiment partici-
pants. We briefly discuss possible improvements of the setup
and our future plans with the data in Section 5.
2 Goals of the Multimodal Experiment
Our aim was to gather interactions where the wizard can com-
bine spoken and visual feedback, namely, displaying (com-
plete or partial) results of a database search, and the user can
speak or select on the screen.
Multimodal Presentation Strategies The main aim was to
identify strategies for the screen output, and for the multi-
modal output presentation. In particular, we want to learn
Figure 1: Multimodal Wizard-of-Oz data collection setup for
an in-car music player application, using the Lane Change
driving simulator. Top right: User, Top left: Wizard, Bottom:
transcribers.
when and what content is presented (i) verbally, (ii) graphi-
cally or (iii) by some combination of both modes. We expect
that when both modalities are used, they do not convey the
same content or use the same level of granularity. These are
important questions for multimodal fission and for turn plan-
ning in each modality.
We also plan to investigate how the presentation strategies
influence the responses of the user, in particular w.r.t. what
further criteria the user specifies, and how she conveys them.
Multimodal Clarification Strategies The experiments
should also serve to identify potential strategies for multi-
modal clarification behavior and investigate individual strat-
egy performance. The wizards? behavior will give us an ini-
tial model how to react when faced with several sources of
interpretation uncertainty. In particular we are interested in
what medium the wizard chooses for the clarification request,
what kind of grounding level he addresses, and what ?sever-
ity? he indicates. 2 In order to invoke clarification behavior
we introduced uncertainties on several levels, for example,
multiple matches in the database, lexical ambiguities (e.g., ti-
tles that can be interpreted denoting a song or an album), and
errors on the acoustic level. To simulate non-understanding
on the acoustic level we corrupted some of the user utterances
by randomly deleting parts of them.
3 Experiment Setup
We describe here some of the details of the experiment. The
experimental setup is shown schematically in Figure 1. There
are five people involved in each session of the experiment: an
experiment leader, two transcribers, a user and a wizard.
The wizards play the role of an MP3 player application
and are given access to a database of information (but not
actual music) of more than 150,000 music albums (almost 1
2Severity describes the number of hypotheses indicated by the
wizard: having no interpretation, an uncertain interpretation, or sev-
eral ambiguous interpretations.
Figure 2: Screenshot from the FreeDB-based database appli-
cation, as seen by the wizard. First-level of choice what to
display.
million songs), extracted from the FreeDB database.3 Fig-
ure 2 shows an example screen shot of the music database
as it is presented to the wizard. Subjects are given a set of
predefined tasks and are told to accomplish them by using
an MP3 player with a multimodal interface. Tasks include
playing songs/albums and building playlists, where the sub-
ject is given varying amounts of information to help them
find/decide on which song to play or add to the playlist. In
a part of the session the users also get a primary driving task,
using a Lane Change driving simulator [Mattes, 2003]. This
enabled us to test the viability of combining primary and sec-
ondary task in our experiment setup. We also aimed to gain
initial insight regarding the difference in interaction flow un-
der such conditions, particularly with regard to multimodal-
ity.
The wizards can speak freely and display the search result
or the playlist on the screen. The users can also speak as well
as make selections on the screen.
The user?s utterances are immediately transcribed by a typ-
ist and also recorded. The transcription is then presented to
the wizard.4 We did this for two reasons: (1) To deprive
the wizards of information encoded in the intonation of utter-
ances, because our system will not have access to it either. (2)
To be able to corrupt the user input in a controlled way, sim-
ulating understanding problems at the acoustic level. Unlike
[Stuttle et al, 2004], who simulate automatic speech recogni-
tion errors using phone-confusion models, we used a tool that
?deletes? parts of the transcribed utterances, replacing them
by three dots. Word deletion was triggered by the experiment
leader. The word deletion rate varied: 20% of the utterances
got weakly and 20% strongly corrupted. In 60% of the cases
the wizard saw the transcribed speech uncorrupted.
The wizard?s utterances are also transcribed (and recorded)
3Freely available at http://www.freedb.org
4We were not able to use a real speech recognition system, be-
cause we do not have one trained for this domain. This is one of the
purposes the collected data will be used for.
Figure 3: Screenshot from the display presentation tool offer-
ing options for screen output to the wizard for second-level
of choice what to display an how.
and presented to the user via a speech synthesizer. There are
two reasons for doing this: One is to maintain the illusion for
the subjects that they are actually interacting with a system,
since it is known that there are differences between human-
human and human-computer dialogue [Duran et al, 2001],
and we want to elicit behavior in the latter condition; the
other has to do with the fact that synthesized speech is imper-
fect and sometimes difficult to understand, and we wanted to
reproduce this condition.
The transcription is also supported by a typing and spelling
correction module to minimize speech synthesis errors and
thus help maintain the illusion of a working system.
Since it would be impossible for the wizard to construct
layouts for screen output on the fly, he gets support for his
task from the WOZ system: When the wizard performs a
database query, a graphical interface presents him a first level
of output alternatives, as shown in Figure 2. The choices are
found (i) albums, (ii) songs, or (iii) artists. For a second level
of choice, the system automatically computes four possible
screens, as shown in Figure 3. The wizard can chose one of
the offered options to display to the user, or decide to clear
the user?s screen. Otherwise, the user?s screen remains un-
changed. It is therefore up to the wizard to decide whether
to use speech only, display only, or to combine speech and
display.
The types of screen output are (i) a simple text-message
conveying how many results were found, (ii) output of a list
of just the names (of albums, songs or artists) with the cor-
responding number of matches (for songs) or length (for al-
bums), (iii) a table of the complete search results, and (iv) a
table of the complete search results, but only displaying a sub-
set of columns. For each screen output type, the system uses
heuristics based on the search to decide, e.g., which columns
should be displayed. These four screens are presented to the
wizard in different quadrants on a monitor (cf. Figure 3),
allowing for selection with a simple mouse click. The heuris-
tics for the decision what to display implement preliminary
strategies we designed for our system. We are aware that due
to the use of these heuristics, the wizard?s output realization
may not be always ideal. We have collected feedback from
both the wizards and the users in order to evaluate whether
the output options were satisfactory (cf. Section 4 for more
details).
Technical Setup To keep our experimental system modu-
lar and flexible we implemented it on the basis of the Open
Agent Architecture (OAA) [Martin et al, 1999], which is a
framework for integrating a community of software agents in
a distributed environment. Each system module is encapsu-
lated by an OAA wrapper to form an OAA agent, which is
able to communicate with the OAA community. The exper-
imental system consists of 12 agents, all of them written in
Java. We made use of an OAA monitor agent which comes
with the current OAA distribution to trace all communication
events within the system for logging purposes.
The setup ran distributed over six PCs running different
versions of Windows and Linux.5
4 Collected Data and Experience
The SAMMIE-26 corpus collected in this experiment contains
data from 24 different subjects, who each participated in one
session with one of our six wizards. Each subject worked on
four tasks, first two without driving and then two with driving.
The duration was restricted to twice 15 minutes. Tasks were
of two types: searching for a title either in the database or in
an existing playlist, building a playlist satisfying a number of
constraints. Each of the two sets for each subject contained
one task of each type. The tasks again differed in how specific
information was provided. We aimed to keep the difficulty
level constant across users. The interactions were carried out
in German.7
The data for each session consists of a video and audio
recording and a logfile. Besides the transcriptions of the spo-
ken utterances, a number of other features have been anno-
tated automatically in the log files of the experiment, e.g.,
the wizard?s database query and the number of found results,
the type and form of the presentation screen chosen by the
wizard, etc. The gathered logging information for a single
experiment session consists of the communication events in
chronological order, each marked by a timestamp. Based on
this information, we can recapitulate the number of turns and
the specific times that were necessary to accomplish a user
task. We expect to use this data to analyze correlations be-
5We would like to thank our colleagues from CLT Sprachtech-
nologie http://www.clt-st.de/ for helping us to set up the
laboratory.
6SAMMIE stands for Saarbru?cken Multimodal MP3 Player In-
teraction Experiment. We have so far conducted two series of data-
collection experiments: SAMMIE-1 involved only spoken interaction
(cf. [Kruijff-Korbayova? et al, 2005] for more details), SAMMIE-2 is
the multimodal experiment described in this paper.
7However, most of the titles and artist names in the music
database are in English.
tween queries, numbers of results, and spoken and graphical
presentation strategies.
Whenever the wizard made a clarification request, the
experiment leader invoked a questionnaire window on the
screen, where the wizard had to classify his clarification re-
quest according to the primary source of the understanding
problem. At the end of each task, users were asked to what
extent they believed they accomplished their tasks and how
satisfied they were with the results. Similar to methods used
by [Skantze, 2003] and [Williams and Young, 2004], we plan
to include subjective measures of task completion and cor-
rectness of results in our evaluation matrix, as task descrip-
tions can be interpreted differently by different users.
Each subject was interviewed immediately after the ses-
sion. The wizards were interviewed once the whole experi-
ment was over. The interviews were carried out verbally, fol-
lowing a prepared list of questions. We present below some
of the points gathered through these interviews.
Wizard Interviews All 6 wizards rated the overall under-
standing as good, i.e., that communication completed suc-
cessfully. However, they reported difficulties due to delays in
utterance transmission in both directions, which caused un-
necessary repetitions due to unintended turn overlap.
There were differences in how different wizards rated and
used the different screen output options: The table containing
most of the information about the queried song(s) or album(s)
was rated best and shown most often by some wizards, while
others thought it contained too much information and would
not be clear at first glance for the users and hence they used
it less or never. The screen option containing the least infor-
mation in tabular form, namely only a list of songs/albums
with their length, received complementary judgments: some
of the wizards found it useless because it contained too little
information, and they thus did not use it, and others found it
very useful because it would not confuse the user by present-
ing too much information, and they thus used it frequently.
Finally, the screen containing a text message conveying only
the number of matches, if any, has been hardly used by the
wizards. The differences in the wizards? opinions about what
the users would find useful or not clearly indicate the need
for evaluation of the usefulness of the different screen output
options in particular contexts from the users? view point.
When showing screen output, the most common pattern
used by the wizards was to tell the user what was shown (e.g.,
I?ll show you the songs by Prince), and to display the screen.
Some wizards adapted to the user?s requests: if asked to show
something (e.g., Show me the songs by Prince), they would
show it without verbal comments; but if asked a question
(e.g., What songs by Prince are there? or What did you find?),
they would show the screen output and answer in speech.
Concerning the adaptation of multimodal presentation
strategies w.r.t. whether the user was driving or not, four
of the six wizards reported that they consciously used speech
instead of screen output if possible when the user was driving.
The remaining two wizards did not adapt their strategy.
On the whole, interviewing the wizards brought valuable
information on presentation strategies and the use of modal-
ities, but we expect to gain even more insight after the an-
notation and evaluation of the collected data. Besides ob-
servations about the interaction with the users, the wizards
also gave us various suggestions concerning the software used
in the experiment, e.g., the database interface (e.g., the pos-
sibility to decide between strict search and search for par-
tial matches, and fuzzy search looking for items with similar
spelling when no hits are found), the screen options presenter
(e.g., ordering of columns w.r.t. their order in the database in-
terface, the possibility to highlight some of the listed items),
and the speech synthesis system.
Subject Interviews In order to use the wizards? behavior as
a model for interaction design, we need to evaluate the wiz-
ards? strategies. We used user satisfaction, task experience,
and multi-modal feedback behavior as evaluation metrics.
The 24 experimental subjects were all native speakers of
German with good English skills. They were all students
(equally spread across subject areas), half of them male and
half female, and most of them were between 20 to 30 years
old.
In order to calculate user satisfaction, users were inter-
viewed to evaluate the system?s performance with a user sat-
isfaction survey. The survey probed different aspects of the
users? perception of their interaction with the system. We
asked the users to evaluate a set of five core metrics on a
5-point Likert scale. We followed [Walker et al, 2002] def-
inition of the overall user satisfaction as the sum of text-to-
speech synthesis performance, task ease, user expertise, over-
all difficulty and future use. The mean for user satisfaction
across all dialogues was 15.0 (with a standard derivation of
2.9). 8 A one-way ANOVA for user satisfaction between wiz-
ards (df=5, F=1.52 p=0.05) shows no significant difference
across wizards, meaning that the system performance was
judged to be about equally good for all wizards.
To measure task experience we elicited data on perceived
task success and satisfaction on a 5-point Likert scale after
each task was completed. For all the subjects the final per-
ceived task success was 4.4 and task satisfaction 3.9 across
the 4 tasks each subject had to complete. For task success
as well as for task satisfaction no significant variance across
wizards was detected.
Furthermore the subjects were asked about the employed
multi-modal presentation and clarification strategies.
The clarification strategies employed by the wizards
seemed to be successful: From the subjects? point of view,
mutual understanding was very good and the few misunder-
standings could be easily resolved. Nevertheless, in the case
of disambiguation requests and when grounding an utterance,
subjects ask for more display feedback. It is interesting to
note that subjects judged understanding difficulties on higher
levels of interpretation (especially reference resolution prob-
lems and problems with interpreting the intention) to be more
costly than problems on lower levels of understanding (like
the acoustic understanding). For the clarification strategy this
8[Walker et al, 2002] reported an average user satisfaction of
16.2 for 9 Communicator systems.
implies that the system should engage in clarification at the
lowest level a error was detected.9
Multi-modal presentation strategies were perceived to be
helpful in general, having a mean of 3.1 on a 5-point Lik-
ert scale. However, the subjects reported that too much in-
formation was being displayed especially for the tasks with
driving. 85.7% of the subjects reported that the screen out-
put was sometimes distracting them. 76.2% of the sub-
jects would prefer to more verbal feedback, especially while
driving. On a 3-point Likert scale subjects evaluated the
amount of the information presented verbally to be about
right (mean of 1.8), whereas they found the information pre-
sented on the screen to be too much (mean of 2.3). Stud-
ies by [Bernsen and Dybkjaer, 2001] on the appropriateness
of using verbal vs. graphical feedback for in-car dialogues
indicate that the need for text output is very limited. Some
subjects in that study, as well subjects in our study report that
they would prefer to not have to use the display at all while
driving. On the other hand subjects in our study perceived the
screen output to be very helpful in less stressful driving situa-
tions and when not driving (e.g. for memory assistance, clari-
fications etc.). Especially when they want to verify whether a
complex task was finally completed (e.g. building a playlist),
they ask for a displayed proof. For modality selection in in-
car dialogues the driver?s mental workload on primary and
secondary task has to be carefully evaluated with respect to a
situation model.
With respect to multi-modality subjects also asked for
more personalized data presentation. We therefore need to
develop intelligent ways to reduce the amount of data being
displayed. This could build on prior work on the generation
of ?tailored? responses in spoken dialogue according to a user
model [Moore et al, 2004].
The results for multi-modal feedback behavior showed no
significant variations across wizards except for the general
helpfulness of multi-modal strategies. An ANOVA Planned
Comparison of the wizard with the lowest mean against the
other wizards showed that his behavior was significantly
worse. It is interesting to note, that this wizard was using
the display less than the others. We might consider not to in-
clude the 4 sessions with this wizard in our output generation
model.
We also tried to analyze in more detail how the wizards?
presentation strategies influenced the results. The option
which was chosen most of the time was to present a table
with the search results (78.6%); to present a list was only cho-
sen in 17.5% of the cases and text only 0.04%. The wizards?
choices varied significantly only for presenting the table op-
tion. The wizard who was rated lowest for multimodality was
using the table option less, indicating that this option should
be used more often. This is also supported by the fact that the
show table option is the only presentation strategy which is
positively correlated to how the user evaluated multimodality
(Spearman?s r = 0.436*). We also could find a 2-tailed corre-
9Note that engaging at the lowest level just helps to save dialogue
?costs?. Other studies have shown that user satisfaction is higher
for strategies that would ?hide? the understanding error by asking
questions on higher levels [Skantze, 2003], [Raux et al, 2005]
lation between user satisfaction and multimodality judgment
(Spearman?s r = 0.658**). This indicates the importance of
good multimodal presentation strategies for user satisfaction.
Finally, the subjects were asked for own comments. They
liked to be able to provide vague information, e.g., ask for ?an
oldie?, and were expecting collaborative suggestions. They
also appreciated collaborative proposals based on inferences
made from previous conversations.
In sum, as the measures for user satisfaction, task experi-
ence, and multi-modal feedback strategies, the subjects? judg-
ments show a positive trend. The dialogue strategies em-
ployed by most of the wizards seem to be a good starting
point for building a baseline system. Furthermore, the results
indicate that intelligent multi-modal generation needs to be
adaptive to user and situation models.
5 Conclusions and Future Steps
We have presented an experiment setup that enables us to
gather multimodal interaction data aimed at studying not only
the behavior of the users of the simulated system, but also
that of the wizards. In order to simulate a dialogue system in-
teraction, the wizards were only shown transcriptions of the
user utterances, sometimes corrupted, to simulate automatic
speech recognition problems. The wizard?s utterances were
also transcribed and presented to the user through a speech
synthesizer. In order to make it possible for the wizards to
produce contextually varied screen output in real time, we
have included a screen output planning module which auto-
matically calculated several screen output versions every time
the wizard ran a database query. The wizards were free to
speak and/or display screen output. The users were free to
speak or select on the screen. In a part of each session, the
user was occupied by a primary driving task.
The main challenge for an experiment setup as described
here is the considerable delay between user input and wizard
response. This is due partly to the transcription and spelling
correction step and partly due to the time it takes the wizard to
decide on and enter a query to the database, then select a pre-
sentation and in parallel speak to the user. We have yet to ana-
lyze the exact distribution of time needed for these tasks. Sev-
eral ways can be chosen to speed up the process. Transcrip-
tion can be eliminated either by using speech recognition and
dealing with its errors, or instead applying signal processing
software, e.g., to filter out prosodic information from the user
utterance and/or to transform the wizard?s utterance into syn-
thetically sounding speech (e.g., using a vocoder). Database
search can be sped up in a number of ways too, ranging from
allowing selection directly from the transcribed text to auto-
matically preparing default searches by analyzing the user?s
utterance. Note, however, that the latter will most likely prej-
udice the wizard to stick to the proposed search.
We plan to annotate the corpus, most importantly w.r.t.
wizard presentation strategies and context features relevant
for the choice between them. We also plan to compare the
presentation strategies to the strategies in speech-only mode,
for which we collected data in an earlier experiment (cf.
[Kruijff-Korbayova? et al, 2005]).
For clarification strategies previous studies already showed
that the decision process needs to be highly dynamic by tak-
ing into account various features such as interpretation uncer-
tainties and local utility [Paek and Horvitz, 2000]. We plan
to use the wizard data to learn an initial multi-modal clarifi-
cation policy and later on apply reinforcement learning meth-
ods to the problem in order to account for long-term dialogue
goals, such as task success and user satisfaction.
The screen output options used in the experiment will also
be employed in the baseline system we are currently imple-
menting. The challenges involved there are to decide (i) when
to produce screen output, (ii) what (and how) to display and
(iii) what the corresponding speech output should be. We will
analyze the corpus in order to determine what the suitable
strategies are.
References
[Bernsen and Dybkjaer, 2001] Niels Ole Bernsen and Laila
Dybkjaer. Exploring natural interaction in the car. In
CLASS Workshop on Natural Interactivity and Intelligent
Interactive Information Representation, 2001.
[Bernsen et al, 1998] N. O. Bernsen, H. Dybkj?r, and
L. Dybkj?r. Designing Interactive Speech Systems ?
From First Ideas to User Testing. Springer, 1998.
[Duran et al, 2001] Christine Duran, John Aberdeen, Laurie
Damianos, and Lynette Hirschman. Comparing several as-
pects of human-computer and human-human dialogues. In
Proceedings of the 2nd SIGDIAL Workshop on Discourse
and Dialogue, Aalborg, 1-2 September 2001, pages 48?57,
2001.
[Kruijff-Korbayova? et al, 2005] Ivana Kruijff-Korbayova?,
Tilman Becker, Nate Blaylock, Ciprian Gerstenberger,
Michael Kai?er, Peter Poler, Jan Schehl, and Verena
Rieser. Presentation strategies for flexible multimodal
interaction with a music player. In Proceedings of
DIALOR?05 (The 9th workshop on the semantics and
pragmatics of dialogue (SEMDIAL), 2005.
[Martin et al, 1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. The open agent architecture: A framework for
building distributed software systems. Applied Artificial
Intelligence: An International Journal, 13(1?2):91?128,
Jan?Mar 1999.
[Mattes, 2003] Stefan Mattes. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of IGfA,
2003.
[Moore et al, 2004] Johanna D. Moore, Mary Ellen Foster,
Oliver Lemon, and Michael White. Generating tailored,
comparative descriptions in spoken dialogue. In Proceed-
ings of the Seventeenth International Florida Artificial In-
telligence Research Sociey Conference, AAAI Press, 2004.
[Paek and Horvitz, 2000] Tim Paek and Eric Horvitz. Con-
versation as action under uncertainty. In Proceedings of
the Sixteenth Conference on Uncertainty in Artificial In-
telligence, 2000.
[Raux et al, 2005] Antoine Raux, Brian Langner, Dan Bo-
hus, Allan W. Black, and Maxine Eskenazi. Let?s go pub-
lic! taking a spoken dialog system to the real world. 2005.
[Skantze, 2003] Gabriel Skantze. Exploring human error
handling strategies: Implications for spoken dialogue sys-
tems. In Proceedings of the ISCA Tutorial and Research
Workshop on Error Handling in Spoken Dialogue Systems,
2003.
[Stuttle et al, 2004] Matthew Stuttle, Jason Williams, and
Steve Young. A framework for dialogue data collection
with a simulated asr channel. In Proceedings of the IC-
SLP, 2004.
[Tu?rk, 2001] Ulrich Tu?rk. The technical processing in
smartkom data collection: a case study. In Proceedings
of Eurospeech2001, Aalborg, Denmark, 2001.
[Walker et al, 2002] Marylin Walker, R. Passonneau, J. Ab-
erdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom,
J. Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky,
G. Sandersa, S. Seneff, D. Stallard, and S. Whittaker.
Cross-site evaluation in darpa communicator: The june
2000 data collection. 2002.
[Williams and Young, 2004] Jason D. Williams and Steve
Young. Characterizing task-oriented dialog using a sim-
ulated asr channel. In Proceedings of the ICSLP, 2004.
The SAMMIE Multimodal Dialogue Corpus Meets the Nite XML Toolkit
Ivana Kruijff-Korbayova?, Verena Rieser,
Ciprian Gerstenberger
Saarland University, Saarbru?cken, Germany
vrieser@coli.uni-sb.de
Jan Schehl, Tilman Becker
DFKI, Saarbru?cken, Germany
jan.schehl@dfki.de
Abstract
We demonstrate work in progress1 us-
ing the Nite XML Toolkit on a cor-
pus of multimodal dialogues with an
MP3 player collected in a Wizard-of-Oz
(WOZ) experiments and annotated with
a rich feature set at several layers. We
designed an NXT data model, converted
experiment log file data and manual tran-
scriptions into NXT, and are building an-
notation tools using NXT libraries.
1 Introduction
In the TALK project2 we are developing a mul-
timodal dialogue system for an MP3 application
for in-car and in-home use. The system should
support natural, flexible interaction and collabo-
rative behavior. To achieve this, it needs to pro-
vide advanced adaptive multimodal output.
To determine the interaction strategies and
range of linguistic behavior naturally occurring
in this scenario, we conducted two WOZ exper-
iments: SAMMIE-1 involved only spoken inter-
action, SAMMIE-2 was multimodal, with speech
and screen input and output.3
We have been annotating the corpus on sev-
eral layers, representing linguistic, multimodal
and context information. The annotated corpus
will be used (i) to investigate various aspects of
1Our demonstration results from the efforts of a larger
team including also N. Blaylock, B. Fromkorth, M. Gra?c,
M. Kai?er, A. Moos, P. Poller and M. Wirth.
2TALK (Talk and Look: Tools for Ambient Linguis-
tic Knowledge; http://www.talk-project.org), funded by the
EU 6th Framework Program, project No. IST-507802.
3SAMMIE stands for Saarbru?cken Multimodal MP3
Player Interaction Experiment.
multimodal presentation and interaction strate-
gies both within and across the annotation lay-
ers; (ii) to design an initial policy for reinforce-
ment learning of multimodal clarifications.4 We
use the Nite XML Toolkit (NXT) (Carletta et al,
2003) to represent and browse the data and to de-
velop annotation tools.
Below we briefly describe our experiment
setup, the collected data and the annotation lay-
ers; we comment on methods and tools for data
representation and annotation, and then present
our NXT data model.
2 Experiment Setup
24 subjects in SAMMIE-1 and 35 in SAMMIE-2
performed several tasks with an MP3 player ap-
plication simulated by a wizard. For SAMMIE-
1 we had two, for SAMMIE-2 six wizards. The
tasks involved searching for titles and building
playlists satisfying various constraints. Each ses-
sion was 30 minutes long. Both users and wiz-
ards could speak freely. The interactions were
in German (although most of the titles and artist
names in the database were English).
SAMMIE-2 had a more complex setup. The
tasks the subjects had to fulfill were divided in
two classes: with vs. without operating a driv-
ing simulator. When presenting the search re-
sults, the wizards were free to produce mono-
or multimodal output as they saw fit; they could
speak freely and/or select one of four automati-
cally generated screen outputs, which contained
tables and lists of found songs/albums. The
users also had free choice between unconstrained
4See (Kruijff-Korbayova? et al, 2006) for more details
about the annotation goals and further usage of the corpus.
69
natural language and/or selecting items on the
screen. Both wizard and user utterances were im-
mediately transcribed. The wizard?s utterances
were presented to the user via a speech synthe-
sizer. To simulate acoustic understanding prob-
lems, the wizard sometimes received only part
of the transcribed user?s utterance, to elicit CRs.
(See (Kruijff-Korbayova? et al, 2005) for details.)
3 Collected Data
The SAMMIE-2 data for each session consists of
a video and audio recording and a log file.5 The
gathered logging information per session con-
sists of Open Agent Architecture (Martin et al,
1999) (OAA) messages in chronological order,
each marked by a timestamp. The log files con-
tain various information, e.g., the transcriptions
of the spoken utterances, the wizard?s database
query and the number of results, the screen op-
tion chosen by the wizard, classification of clari-
fication requests (CRs), etc.
4 Annotation Methods and Tools
The rich set of features we are interested in nat-
urally gives rise to a multi-layered view of the
corpus, where each layer is to be annotated inde-
pendently, but subsequent investigations involve
exploration and automatic processing of the inte-
grated data across layers.
There are two crucial technical requirements
that must be satisfied to make this possible: (i)
stand-off annotation at each layer and (ii) align-
ment of base data across layers. Without the for-
mer, we could not keep the layers separate, with-
out the latter we would not be able to align the
separate layers. An additional equally important
requirement is that elements at different layers
of annotation should be allowed to have overlap-
ping spans; this is crucial because, e.g., prosodic
units and syntactic phrases need not coincide.
Among the existing toolkits that support
multi-layer annotation, it was decided to use
NXT (Carletta et al, 2003)6 in the TALK
project. The NXT-based SAMMIE-2 corpus we
5For 19 sessions the full set of data files exists.
6http://www.ltg.ed.ac.uk/NITE/
are demonstrating has been created in several
steps: (1) The speech data was manually tran-
scribed using the Transcriber tool.7 (2) We auto-
matically extracted features at various annotation
layers by parsing the OAA messages in the log
files. (3) We automatically converted the tran-
scriptions and the information from the log files
into our NXT-based data representation format;
features annotated in the transcriptions and fea-
tures automatically extracted from the log files
were assigned to elements at the appropriate lay-
ers of representation in this step.
Manual annotation: We use tools specifi-
cally designed to support the particular annota-
tion tasks. We describe them below.
As already mentioned, we used Transcriber for
the manual transcriptions. We also performed
certain relatively simple annotations directly on
the transcriptions and coded them in-line by us-
ing special notation. This includes the identifica-
tion of self-speech, the identification of expres-
sions referring to domain objects (e.g., songs,
artists and albums) and the identification of utter-
ances that convey the results of database queries.
For other manual annotation tasks (the annota-
tion of CRs, task segmentation and completion,
referring expressions and the relations between
them) we have been building specialized tools
based on the NXT library of routines for build-
ing displays and interfaces based on Java Swing
(Carletta et al, 2003). Although NXT comes
with a number of example applications, these are
tightly coupled with the architecture of the cor-
pora they were built for. We therefore developed
a core basic tool for our own corpus; we mod-
ify this tool to suite each annotation task. To fa-
cilitate tool development, NXT provides GUI el-
ements linked directly to corpora elements and
support for handling complex multi-layer cor-
pora. This proved very helpful.
Figure 4 shows a screenshot of our CR anno-
tation tool. It allows one to select an utterance
in the left-hand side of the display by clicking
on it, and then choose the attribute values from
the pop-down lists on the right-hand side. Cre-
7http://trans.sourceforge.net/
70
ating relations between elements and creating el-
ements on top of other elements (e.g., words or
utterances) are extensions we are currently im-
plementing (and will complete by the time of the
workshop). First experiences using the tool to
identify CRs are promising.8 When demonstrat-
ing the system we will report the reliability of
other manual annotation tasks.
Automatic annotation using indexing: NXT
also provides a facility for automatic annotation
based on NiteQL query matches (Carletta et al,
2003). Some of our features, e.g., the dialogue
history ones, can be easily derived via queries.
5 The SAMMIE NXT Data Model
NXT uses a stand-off XML data format that con-
sist of several XML files that point to each other.
The NXT data model is a multi-rooted tree with
arbitrary graph structure. Each node has one set
of children, and can have multiple parents.
Our corpus consists of the following layers.
Two base layers: words and graphical output
events; both are time-aligned. On top of these,
structural layers correspond to one session per
subject, divided into task sections, which con-
sist of turns, and these consist of individual ut-
terances, containing words. Graphical output
events will be linked to turns at a featural layer.
Further structural layers are defined for CRs
and dialogue acts (units are utterances), domain
objects and discourse entities (units are expres-
sions consisting of words). We keep independent
layers of annotation separate, even when they can
in principle be merged into a single hierarchy.
Figure 2 shows a screenshot made with Ami-
gram (Lauer et al, 2005), a generic tool for
browsing and searching NXT data. On the left-
hand side one can see the dependencies between
the layers. The elements at the respective layers
are displayed on the right-hand side.
Below we indicate the features per layer:
? Words: Time-stamped words and other
sounds; we mark self-speech, pronuncia-
tion, deletion status, lemma and POS.
8Inter-annotator agreement of 0.788 (? corrected for
prevalence).
? Graphical output: The type and amount of
information displayed, the option selected
by the wizard, and the user?s choices.
? Utterances: Error rates due to word dele-
tion, and various features describing the
syntactic structure, e.g., mood, polarity,
diathesis, complexity and taxis, the pres-
ence of marked syntactic constructions such
as ellipsis, fronting, extraposition, cleft, etc.
? Turns: Time delay, dialogue duration so
far, and other dialogue history features, i.e.
values which accumulate over time.
? Domain objects and discourse entities:
Properties of referring expressions reflect-
ing the type and information status of dis-
course entities, and coreference/bridging
links between them.
? Dialogue acts: DAs based on an agent-
based approach to dialogue as collaborative
problem-solving (Blaylock et al, 2003),
e.g., determining joint objectives, find-
ing and instantiating recipes to accomplish
them, executing recipes and monitoring for
success. We also annotate propositional
content and the database queries.
? CRs: Additional features including the
source and degree of uncertainty, and char-
acteristics of the CRs strategy.
? Tasks: A set of features for estimating user
satisfaction online for reinforcement learn-
ing (Rieser et al, 2005).
? Session: Subject and wizard information,
user questionnaire aswers, and accumulat-
ing attribute values from other layers.
6 Summary
We described a multi-layered corpus of multi-
modal dialogues represented and annotated us-
ing NXT-based tools. Our data model relates lin-
guistic and graphical realization to a rich set of
context features and represents structural, hierar-
chical interactions between different annotation
layers. We combined different annotation meth-
ods to construct the corpus. Manual annotation
and annotation evaluation is on-going. The cor-
pus will be used (i) investigate multimodal pre-
sentation and interaction strategies with respect
71
Figure 1: NXT-based tool for annotating CRs
Figure 2: SAMMIE-2 corpus displayed in Amigram
to dialogue context and (ii) to design an initial
policy for reinforcement learning of multimodal
clarification strategies.
References
[Blaylock et al2003] N. Blaylock, J. Allen, and G. Fergu-
son. 2003. Managing communicative intentions with
collaborative problem solving. In Current and New
Directions in Discourse and Dialogue, pages 63?84.
Kluwer, Dordrecht.
[Carletta et al2003] J. Carletta, S. Evert, U. Heid, J. Kil-
gour, J. Robertson, and H. Voormann. 2003. The NITE
XML Toolkit: flexible annotation for multi-modal lan-
guage data. Behavior Research Methods, Instruments,
and Computers, special issue on Measuring Behavior.
Submitted.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG.
[Kruijff-Korbayova? et al2006] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, V. Rieser, and J. Schehl. 2006. The SAMMIE
corpus of multimodal dialogues with an mp3 player. In
Proc. of LREC (to appear).
[Lauer et al2005] C. Lauer, J. Frey, B. Lang, T. Becker,
T. Kleinbauer, and J. Alexandersson. 2005. Amigram
- a general-purpose tool for multimodal corpus annota-
tion. In Proc. of MLMI.
[Martin et al1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. 1999. The open agent architecture: A frame-
work for building distributed software systems. Applied
Artificial Intelligence: An International Journal, 13(1?
2):91?128, Jan?Mar.
[Rieser et al2005] V. Rieser, I. Kruijff-Korbayova?, and
O. Lemon. 2005. A corpus collection and annotation
framework for learning multimodal clarification strate-
gies. In Proc. of SIGdial.
72
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 82?93, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Optimising Incremental Dialogue Decisions Using Information Density for
Interactive Systems
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
School of Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh, Scotland
n.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.uk
Abstract
Incremental processing allows system design-
ers to address several discourse phenomena
that have previously been somewhat neglected
in interactive systems, such as backchannels
or barge-ins, but that can enhance the re-
sponsiveness and naturalness of systems. Un-
fortunately, prior work has focused largely
on deterministic incremental decision mak-
ing, rendering system behaviour less flexible
and adaptive than is desirable. We present a
novel approach to incremental decision mak-
ing that is based on Hierarchical Reinforce-
ment Learning to achieve an interactive op-
timisation of Information Presentation (IP)
strategies, allowing the system to generate
and comprehend backchannels and barge-ins,
by employing the recent psycholinguistic hy-
pothesis of information density (ID) (Jaeger,
2010). Results in terms of average rewards
and a human rating study show that our learnt
strategy outperforms several baselines that are
not sensitive to ID by more than 23%.
1 Introduction
Recent work on incremental systems has shown
that adapting a system?s turn-taking behaviour to be
more human-like can improve the user?s experience
significantly, based on incremental models of auto-
matic speech recognition (ASR) (Baumann et al
2011), dialogue management (Buss et al2010), and
speech generation (Skantze and Hjalmarsson, 2010).
All of these approaches are based on the same gen-
eral abstract architecture of incremental processing
(Schlangen and Skantze, 2011). While this archi-
tecture offers inherently incremental mechanisms to
update and revise input hypotheses, it is affected
by a number of drawbacks, shared by determinis-
tic models of decision making in general: they rely
on hand-crafted rules which can be time-consuming
and expensive to produce, they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, they are unable to gener-
alise and adapt flexibly to unseen situations, and
they do not use automatic optimisation. Statisti-
cal approaches to incremental processing that ad-
dress some of these problems have been suggested
by Raux and Eskenazi (2009), who use a cost matrix
and decision theoretic principles to optimise turn-
taking in a dialogue system under the constraint that
users prefer no gaps and no overlap at turn bound-
aries. Also, DeVault et al2009) use maximum en-
tropy classification to support responsive overlap in
an incremental system by predicting the completions
of user utterances. Selfridge et al2011) use logis-
tic regression models to predict the stability and ac-
curacy of incremental speech recognition results to
enhance performance without causing delay. For re-
lated work on (deterministic) incremental language
generation, please see (Kilger and Finkler, 1995;
Purver and Otsuka, 2003).
Recent years have seen a number of data-driven
approaches to interactive systems that automatically
adapt their decisions to the dialogue context us-
ing Reinforcement Learning (Levin et al2000;
Walker, 2000; Young, 2000; Singh et al2002;
Pietquin and Dutoit, 2006; Henderson et al2008;
Cuaya?huitl et al2010; Thomson, 2009; Young et
al., 2010; Lemon, 2011; Janarthanam and Lemon,
2010; Rieser et al2010; Cuaya?huitl and Dethlefs,
82
2011; Dethlefs and Cuaya?huitl, 2011). While these
approaches have been shown to enhance the perfor-
mance and adaptivity of interactive systems, unfor-
tunately none of them has yet been combined with
incremental processing.
In this paper, we present a novel approach to in-
cremental decision making for output planning that
is based on Hierarchical Reinforcement Learning
(HRL). In particular, we address the problem of op-
timising IP strategies while allowing the system to
generate and comprehend backchannels and barge-
ins based on a partially data-driven reward func-
tion. Generating backchannels can be beneficial for
grounding in interaction. Similarly, barge-ins can
lead to more efficient interactions, e.g. when a sys-
tem can clarify a bad recognition result immediately
before acting based on a misrecognition.
A central concept to our approach is Information
Density (ID) (Jaeger, 2010), a psycholinguistic hy-
pothesis that human utterance production is sensitive
to a uniform distribution of information across the
utterance. This hypothesis has also been adopted for
low level output planning recently, see e.g. Rajku-
mar and White (2011). Our results in terms of av-
erage rewards and a human rating study show that a
learning agent that is sensitive to ID can learn when
it is most beneficial to generate feedback to a user,
and outperforms several other agents that are not
sensitive to ID.
2 Incremental Information Presentation
2.1 Information Presentation Strategies
Our example domain of application is the Infor-
mation Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al2010). This previous work
incrementally constructs IP strategies according to
the predicted user reaction, whereas our approach
focuses on whether and when to generate backchan-
nels and barge-ins and how to react to user barge-
ins in the context of dynamically changing input hy-
potheses. We therefore implement a simplified ver-
sion of Rieser et al model. Their system distin-
guished two steps: the selection of an IP strategy
and the selection of attributes to present to the user.
We assume here that the choice of attributes is deter-
mined by matching the types specified in the user in-
put, so that our system only needs to choose a strat-
egy for presenting its results. Attributes include cui-
sine, food quality, location, price range and service
quality of a restaurant. The system then performs a
database lookup and chooses among three main IP
strategies summary, comparison, recommendation
and several ordered combinations of these. Please
see Rieser et al2010) for details. Table 1 shows
examples of the main types of IP strategies that we
generate.
2.2 Backchannels and Barge-ins
An important advantage of incremental processing
can be the increased reactiveness of systems. In this
paper, we focus on the phenomena of backchannels
and barge-ins that can act as feedback in an interac-
tion for both user and system. Figure 1 shows some
examples. Backchannels can often be interpreted as
signals of grounding. Coming from the user, the sys-
tem may infer that the user is following the presenta-
tion of information or is confirming a piece of infor-
mation without trying to take the turn. Similarly, we
can allow a system to generate backchannels to the
user to confirm that it understands the user?s prefer-
ences, i.e. receives high confidence scores from the
ASR module. An important decision for a dialogue
system is then when to generate a backchannel?
Barge-ins typically occur in different situations.
The user may barge-in on the system to correct an
ASR error (such as ?Italian? instead of ?Indian? in
Figure 1) or the system may want to barge-in on the
user to confirm a low-confidence ASR hypothesis so
as to be able to start an immediate database look up
for results. In the former case, the user barging-in
on the system, we assume that the system has two
choices: yielding the turn to the user, or trying to
keep the turn. In the latter case, the system barging-
in on the user, the system would have to decide if and
when it would be beneficial to barge-in on a user ut-
terance. In the following sections, we will develop
a model of dialogue optimisation that can address
these question based on Hierarchical RL that opti-
mises system behaviour based on trade-offs defined
in terms of ID.
83
Type Example
Comparison The restaurant Roma is in the medium price range, but does not serve excellent food.
The restaurants Firenze and Verona both have great food but are more expensive. The
restaurant Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expen-
sive, but has great food and service.
Summary I found 24 Italian restaurants in the city centre that match your query. 11 of them are
in the medium price range, 5 are cheap and 8 are expensive.
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
Backchannel 1 (the system backchannels)
USR I want Italian food [500 ms] in the city centre. . .
SYS uh-huh
SYS OK. I found 24 Italian restaurants in the city centre. The
restaurant Roma is in the medium price range, but does not
have great food. The restaurants Firenze and Verona . . .
Backchannel 2 (the user backchannels)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 central Italian restaurants . . .
USR OK.
SYS The restaurant Verona has great food but is also a bit
expensive. The Roma is cheaper, but not as central as Verona . . .
Barge-ins 1 (the user barges-in on system)
USR I want Italian food in the centre of town . . .
SYS I found 35 Indian . . .
USR Not Indian, I want Italian.
SYS OK, Italian . . .
SYS I have 24 Italian restaurants . . .
Barge-ins 2 (the system barges-in on user)
USR I need an Italian restaurant that is located . . .
SYS I?m sorry. Did you say
Indian or Italian?
USR I said Italian. And in the centre of town please.
SYS OK, let me see. I have 24 Italian restaurants . . .
Figure 1: Example phenomena generated with the learnt
policy. The agent has learnt to produce backchannels
and barge-ins at the appropriate moment and alternative
strategies to deal with user barge-ins.
3 Information Theory
Information Theory as introduced by Shannon
(1948) is based on two main concepts: a communi-
cation channel through which information is trans-
ferred in bits and the information gain, i.e. the in-
formation load that each bit carries. For natural lan-
guage, the assumption is that people aim to com-
municate according to the channel?s capacity, which
corresponds to the hearer?s capacity in terms of cog-
nitive load. If they go beyond that, the cognitive load
of the listener gets too high. If they stay (far) below,
too little information is transferred per bit (i.e., the
utterance is inefficient or uninformative). The in-
formation gain of each word, which is indicative of
how close we are to the channel?s capacity, can be
computed using entropy measures.
3.1 Information Density
Psycholinguistic research has presented evidence for
users distributing information across utterances uni-
formly, so that each word is carrying roughly the
same amount of information. This has been ob-
served for phonetic phenomena based on words
(Bell et al2003) and syllables (Aylett and Turk,
2004), and for syntactic phenomena (Levy and
Jaeger, 2007; Jaeger, 2010). Relating ID to likeli-
hood, we can say that the less frequent a word is, the
more information it is likely to carry (Jaeger, 2010).
For example the word ?the? often has a high corpus
frequency but a low ID.
The ID is defined as the log-probability of an
event (i.e. a word) (Shannon, 1948; Levy and Jaeger,
2007), so that for an utterance u consisting of the
word sequence w1 . . . wi?1, we can compute the ID
at each point during the utterance as:
log 1P (u) =
n
?
i=1
log 1P (wi|w1 . . . wi?1)
(1)
While typically the context of a word is given by
all preceding words of the utterance, we follow Gen-
zel and Charniak (2002) in restricting our computa-
tion to tri-grams for computability reasons. Given a
84
language model of the domain, we can therefore op-
timise ID in system-generated discourse, where we
treat ID as ?an optimal solution to the problem of
rapid yet error-free communication in a noisy envi-
ronment? (Levy and Jaeger (2007), p.2). We will
now transfer the notion of ID to IP and investigate
the distribution of information over user restaurant
queries.
3.2 Information Density in User Utterances
We aim to use ID for incremental IP in two ways:
(1) to estimate the best moment for generating
backchannels or barge-ins to the user, and (2) to de-
cide whether to yield or keep the current system turn
in case of a user barge-in. While we do not have spe-
cific data on human barge-in behaviour, we know
from the work of (Jaeger, 2010), e.g., that ID influ-
ences human language production. We therefore hy-
pothesise a relationship between ID and incremen-
tal phenomena. A human-human data collection is
planned for the near future.
To compute the ID of user and system utterances
at each time step, we estimated an n?gram lan-
guage model (using Kneser-Ney smoothing) based
on a transcribed corpus of human subjects interact-
ing with a system for restaurant recommendations of
Rieser et al2011).1 The corpus contained user ut-
terances as exemplified in Figure 1 and allowed us to
compute the ID at any point during a user utterance.2
In this way, we can estimate points of low density
which may be eligible for a barge-in or a backchan-
nel. Figure 2 shows some example utterances drawn
from the corpus and their ID including the first sen-
tence from Figure 1. These examples were typical
for what could generally be observed from the cor-
pus. We see that while information is transmitted
with varying amounts of density, the main bits of in-
formation are transmitted at a scale between 2 and
7.
Due to a lack of human data for the system utter-
ances, we use the same corpus data to compute the
ID of system utterances.3 The learning agent can use
1Available at http://www.macs.hw.ac.uk/
ilabarchive/classicproject/data/login.php.
2Note that our model does not currently handle out-of-
domain words. In future work, we will learn when to seek clar-
ification.
3We plan a data collection of such utterances for the future,
1 2 3 4 5 6 7 8 9 10 11 12
0
2
4
6
8
10
12
14
16
18
20
In
fo
rm
at
io
n 
De
ns
ity
Time 
 
 
I want Italian food in the city centre.
Yes, I need a moderately priced restaurant in the New Chesterton area.
I need the address of a Thai restaurant.
Figure 2: Information Density for example utterances,
where peaks indicate places of high density.
this information to consider the trade-off of yielding
a current turn to the user or trying to keep it, e.g., in
case of a user barge-in given the ID of its own turn
and of the user?s incoming turn. Such decisions will
be made incrementally in our domain given dynam-
ically changing hypotheses of user input.
4 Incremental Utterance Optimisation
To optimise incremental decision making for an in-
teractive system given the optimisation measure of
ID, we formalise the dialogue module as a Hierar-
chical Reinforcement Learning agent and learn an
optimal action policy by mapping states to actions
and optimising a long-term reward signal. The di-
alogue states can be seen as representing the sys-
tem?s knowledge about the task, the user and the
environment. The dialogue actions correspond to
the system?s capabilities, such as present the re-
sults or barge-in on the user. They also handle in-
cremental updates in the system. In addition, we
need a transition function that specifies the way
that actions change the environment (as expressed
in the state representation) and a reward function
which specifies a numeric value for each action
taken. In this way, decision making can be seen
as a finite sequence of states, actions and rewards
{s0, a0, r1, s1, a1, ..., rt?1, st}, where the goal is to
induce an optimal strategy automatically using Rein-
forcement Learning (RL) (Sutton and Barto, 1998).
We used Hierarchical RL, rather than flat RL, be-
cause the latter is affected by the curse of dimen-
sionality, the fact that the state space grows expo-
nentially according to the state variables taken into
account. This affects the scalability of flat RL agents
but for now make the assumption that using the corpus data is
informative since they are from the same domain.
85
and limits their application to small-scale problems.
Since timing is crucial for incremental approaches,
where processing needs to be fast, we choose a hi-
erarchical setting for better scalability. We denote
the hierarchy of RL agents as M ij where the in-
dexes i and j only identify an agent in a unique
way, they do not specify the execution sequence of
subtasks, which is subject to optimisation. Each
agent of the hierarchy is defined as a Semi-Markov
Decision Process (SMDP) consisting of a 4-tuple
< Sij , Aij , T ij , Rij >. Here, Sij denotes the set of
states, Aij denotes the set of actions, and T ij is a
probabilistic state transition function that determines
the next state s? from the current state s and the per-
formed action a. Rij(s?, ? |s, a) is a reward function
that specifies the reward that an agent receives for
taking an action a in state s lasting ? time steps
(Dietterich, 1999). Since actions in SMDPs may
take a variable number of time steps to complete,
the variable ? represents this number of time steps.
The organisation of the learning process into dis-
crete time steps allows us to define incremental hy-
pothesis updates as state updates and transitions in
an SMDP. Whenever conditions in the learning en-
vironment change, such as the recogniser?s best hy-
pothesis of the user input, we represent them as tran-
sitions from one state to another. At each time step,
the agent checks for changes in its state represen-
tation and takes the currently best action according
to the new state. The best action in an incremental
framework can also include generating a backchan-
nel to the user to indicate the status of grounding
or barging-in to confirm an uncertain piece of infor-
mation. Once information has been presented to the
user, it is committed or realised. Realised informa-
tion is represented in the agent?s state, so that it can
monitor its own output.
Actions in a Hierarchical Reinforcement learner
can be either primitive or composite. The former
are single-step actions that yield single rewards, and
the latter are multi-step actions that correspond to
SMDPs and yield cumulative rewards. Decision
making occurs at any time step of an SMDP: after
each single-step action, we check for any updates
of the environment that require a system reaction or
change of strategy. If no system action is required
(e.g. because the user is speaking), the system can
decide to do nothing. The goal of each SMDP is to
find an optimal policy pi? that maximises the reward
for each visited state, according to
pi?ij(s) = argmaxa?A Q
?i
j(s, a), (2)
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing pi?. We use HSMQ-Learning to induce dia-
logue policies, see (Cuaya?huitl, 2009), p. 92.
5 Experimental Setting
5.1 Hierarchy of Learning Agents
The HRL agent in Figure 3 shows how the tasks of
(1) dealing with incrementally changing input hy-
potheses, (2) choosing a suitable IP strategy and (3)
presenting information, are connected. Note that
we focus on a detailed description of models M10...3
here, which deal with barge-ins and backchannels
and are the core of this paper. Please see Dethlefs et
al. (2012) for details of an RL model that deals with
the remaining decisions.
Briefly, model M00 deals with dynamic input hy-
potheses. It chooses when to listen to an incoming
user utterance (M13 ) and when and how to present
information (M10...2) by calling and passing control
to a child subtask. The variable ?incrementalStatus?
characterises situations in which a particular (incre-
mental) action is triggered, such as a floor holder ?let
me see?, a correction or self-correction. The variable
?presStrategy? indicates whether a strategy for IP has
been chosen or not, and the variable ?userReaction?
shows the user?s reaction to an IP episode. The
?userSilence? variable indicates whether the user is
speaking or not. The detailed state and action space
of the agents is given in Figure 4. We distinguish ac-
tions for Information Presentation (IP), actions for
attribute presentation and ordering (Slot-ordering),
and incremental actions (Incremental).
Models M10...2 correspond to different ways of
presenting information to the user. They perform
attribute selection and ordering and then call the
child agents M20...4 for attribute realisation. When-
ever a user barges in over the system, these agents
will decide to either yield the turn to the user or to
try and keep the turn based on information density.
The variables representing the status of the cuisine,
86
Root
Summary Comparison Recommendation
Observe
 User
Present
Cuisine
Present
 Food
Present
Location
Present
  Price
Present
Service
Figure 3: Hierarchy of learning agent for incremental In-
formation Presentation and Slot Ordering.
food, location, price and service of restaurants indi-
cate whether the slot is of interest to the user (we as-
sume that 0 means that the user does not care about
this slot), and what input confidence score is cur-
rently associated with the value of the slot. For ex-
ample, if our current best hypothesis is that the user
is interested in Indian restaurants, the variable ?sta-
tusCuisine? will have a value between 1-3 indicating
the strength of this hypothesis. Once slots have been
presented to the user, they are realised and can only
be changed through a correction or self-correction.
Model M13 is called whenever the user is speak-
ing. The system?s main choice here is to remain
silent and listen to the user or barge-in to request
the desired cuisine, location, or price range of a
restaurant. This can be beneficial in certain situa-
tions, such as when the system is able to increase its
confidence for a slot from ?low? to ?high? through
barging-in with a direct clarification request, e.g.
?Did you say Indian?? (and thereby saving sev-
eral turns that may be based on a wrong hypoth-
esis). This can also be harmful in certain situa-
tions, though, assuming that users have a general
preference for not being barged-in on. The learning
agent will need to learn to distinguish these situa-
tions. This agent is also responsible for generating
backchannels and will over time learn the best mo-
ments to do this.
Models M20...4 choose surface forms for presenta-
tion to the user from hand-crafted templates. They
are not the focus of this paper, however, and there-
fore not presented in detail. The state-action space
size of this agent is roughly 1.5 million.4 The agent
4Note that a flat RL agent, in contrast, would need 8? 1025
million state-actions to represent this problem.
States M00
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
observeUser {0=unfilled,1=filled}
presStrategy {0=unfilled,1=filled}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions M00
IP: compare M11 , recommend M12 , summarise M10 , sum-
mariseCompare, summariseRecommend, summariseCompar-
eRecommend,
Incremental: correct, selfCorrect, holdFloor, observeUser
Goal State M00 0, 1, 1, 0, ?
States M10...2
IDSystem={0=low,1=medium, 2=high}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusQuality {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
turnType {0=holding, 1=resuming, 2=keeping, 3=yielding}
userBargeIn {0=false, 1=true}
Actions M10...2
Slot-ordering: presentCuisine M20 , presentQuality M21 ,
presentLocation M22 , presentPrice M23 , presentService M24 ,
Incremental: yieldTurn, keepTurn
Goal State M10...2 ?, ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, ?, ?
States M13
bargeInOnUser={0=undecided,1=yes, 2=no}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
Actions M13
Incremental: doNotBargeIn, bargeInCuisine, bargeInLocation,
bargeInPrice, backchannel
Goal State M13 >0, ?, 0 ? 4, 0 ? 4, 0 ? 4
States M20...4
IDSystem={0=low,1=medium, 2=high}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
surfaceForm {0=unrealised,1=realised}
Actions M20...4
Surface Realisation: [alternative surface realisations]
e.g. ?$number$ restaurants serve $cuisine$ food?, ?$number$
places are located in $area$, etc.
Goal State M20...4 ?, ?, 1
Figure 4: The state and action space of the HRL agent.
The goal state is reached when all items (that the user
specified in the search query) have been presented. Ques-
tion marks mean that a variable does not affect the goal
state, which can be reached regardless of the variable?s
value.
reaches its goal state (defined w.r.t. the state vari-
87
ables in Fig. 4) when an IP strategy has been chosen
and all information has been presented.
5.2 The Simulated Environment
For a policy to converge, a learning agent typically
needs several thousand interactions in which it is ex-
posed to a multitude of different circumstances. For
our domain, we designed a simulated environment
with three main components addressing IP, incre-
mental input hypotheses and ID. Using this simula-
tion, we trained the agent for 10 thousand episodes,
where one episode corresponds to one recommenda-
tion dialogue.
5.2.1 Information Presentation
To learn a good IP strategy, we use a user simula-
tion5 by Rieser et al2010) which was estimated
from human data and uses bi-grams of the form
P (au,t|IPs,t), where au,t is the predicted user reac-
tion at time t to the system?s IP strategy IPs,t in state
s at time t. We distinguish the user reactions of se-
lect a restaurant, addMoreInfo to the current query to
constrain the search, and other. The last category is
usually considered an undesirable user reaction that
the system should learn to avoid. The simulation
uses linear smoothing to account for unseen situa-
tions. In this way, we can predict the most likely
user reaction to each system action. Even though
previous work has shown that n-gram-based simu-
lations can lead to dialogue inconsistencies, we as-
sume that for the present study this does not present
a problem, since we focus on generating single utter-
ances and on obtaining user judgements for single,
independent utterances.
5.2.2 Input Hypothesis Updates
While the IP strategies can be used for incremen-
tal and non-incremental dialogue, the second part of
the simulation deals explicitly with the dynamic en-
vironment updates that the system will need to be
sensitive to in an incremental setting. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores for each value are
5The simulation data are available from www.
classic-project.org.
shown in Table 2. A score of 0 means that the user
does not care about the attribute, 1 means that the
system?s confidence in the attribute?s value is low, 2
that the confidence is medium, and 3 means that the
confidence is high. A value of 4 means that the at-
tribute has already been realised, i.e. communicated
to the user. At the beginning of a learning episode,
we assign each attribute a possible value and con-
fidence score with equal probability. For food and
service quality, we assume that the user is never in-
terested in bad food or service. Subsequently, con-
fidence scores can change at each time step. In fu-
ture work these transition probabilities will be esti-
mated from a data collection, though the following
assumptions are realistic based on our experience.
We assume that a confidence score of 0 changes to
any other value with a likelihood of 0.05. A confi-
dence score of 1 changes with a probability of 0.3,
a confidence score of 2 with a probability of 0.1
and a confidence score of 3 with a probability of
0.03. Once slots have been realised, their value is
set to 4. They cannot be changed then without an ex-
plicit correction. We also assume that realised slots
change with a probability of 0.1. If they change,
we assume that half of the time, the user is the ori-
gin of the change (because they changed their mind)
and half of the time the system is the origin of the
change (because of an ASR or interpretation error).
Each time a confidence score is changed, it has a
probability of 0.5 for also changing its value. The
resulting input to the system are data structures of
the form present(cuisine=Indian), confidence=low.
The probability of observing this data structure in
our simulation is 0.1 (for Indian) ? 0.2 (for low
confidence) = 0.02. Its probability of changing
to present(cuisine=italian), confidence=high is 0.1
(for changing from low to medium) ? 0.05 (for
changing from Indian to Italian) = 0.005.
5.2.3 Information Density Updates
We simulate ID of user utterances based on proba-
bilistic context-free grammars (PCFG) that were au-
tomatically induced from the corpus data in Section
3.2 using the ABL algorithm (van Zaanen, 2000).
This algorithm takes a set of strings as input and
computes a context-free grammar as output by align-
ing strings based on Minimum Edit Distance. We
use the n?gram language models trained earlier to
88
Attribute Values Confidence
Cuisine Chinese, French, German, In-, 0, 1, 2, 3, 4
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Quality bad, adequate, good, very good 0, 1, 2, 3, 4
Location 7 distinct areas of the city 0, 1, 2, 3, 4
Price cheap, good-price-for-value,
expensive, very expensive 0, 1, 2, 3, 4
Service bad, adequate, good, very good 0, 1, 2, 3, 4
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
add probabilities to grammar rules. We use these
PCFGs to simulate user utterances to which the sys-
tem has to react. They can be meaningful utter-
ances such as ?Show me restaurants nearby? or less
meaningful fragments such as ?um let me see, do
you. . . hm?. The former type is more frequent in
the data, but both types can be simulated along with
their ID (clearly, the first type is more dense than the
second).
In addition to simulating user utterances, we
hand-crafted context-free grammars of system ut-
terances and augmented them with probabilities es-
timated using the same user corpus data as above
(where again, we make the assumption that this is
to some extent feasible given the shared domain).
We use the simulated system utterances to compute
varying degrees of ID for the system.
Both measures, the ID of user and system utter-
ances, can inform the system during learning to bal-
ance the trade-off between them for generating and
receiving backchannels and barge-ins.
5.3 A Reward Function for Incremental
Dialogue Based on Information Density
To train the HRL agent, we use a partially data-
driven reward function. For incremental IP, we use
rewards that are based on human intuition. The
agent receives
R =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+100 if the user selects an item,
0 if the user adds further con-
straints to the search,
-100 if the user does something else
or a self-correction,
-0.5 for the system holding a turn,
-1 otherwise.
The agent is encouraged to choose those sequences
of actions that lead to the user selecting a restaurant
as quickly as possible. If the agent is not sure what to
say (because planning has not finished), it can gen-
erate a floor holding marker, but should in any case
avoid a self-correction due to having started speak-
ing too early.
The remaining rewards are based on ID scores
computed incrementally during an interaction. The
agent receives the following rewards, where info-
Density(Usr) and infoDensity(Sys) refer to the ID of
the current user and system utterance, respectively,
as defined in Equation 1.
R =
?
?
?
?
?
?
?
-infoDensity(Usr) for keeping a turn,
barging-in or
a backchannel,
-infoDensity(Sys) for yielding a turn.
These two measures encourage the agent to consider
the trade-offs between its own ID and the one trans-
mitted by an incoming user utterance. Barging-in
on a user utterance at a low ID point then yields a
small negative reward, whereas barging-in on a user
utterance at a high ID point yields a high negative
reward. Both rewards are negative because barging-
in on the user always contains some risk. Similarly,
keeping a turn over a non-dense user utterance re-
ceives a smaller negative reward than keeping it over
a dense user utterance. A reward of ?2 is assigned
for barging-in over a user utterance fragment with a
falling ID to reflect results from a qualitative study
of our corpus data: humans tend to barge-in between
information peaks, so that a barge-in to clarify a low-
confidence slot appears immediately before the ID is
rising again for a new slot. The exact best moment
for barge-ins and backchannels to occur will be sub-
ject to optimisation.
89
6 Experimental Results
The agent learns to barge-in or generate backchan-
nels to users at points where the ID is low but rising.
In particular, the agent learns to barge-in right before
information density peaks in an incoming user utter-
ance to clarify or request slots that are still open from
the previous information density peak. If a user has
specified their desired cuisine type but the system
has received a low ASR confidence score for it, it
may barge-in to clarify the slot. This case was illus-
trated in the last example in Figure 1, where the sys-
tem clarified the previous (cuisine) slot (which is as-
sociated with a high ID) just before the user specifies
the location slot (which again would have a high ID).
The main benefit the system can gain through clar-
ification barge-ins is to avoid self-corrections when
having acted based on a low ASR confidence, lead-
ing to more efficient interactions.
The system learns to generate backchannels after
information peaks to confirm newly acquired slots
that have a high confidence. An example is shown
in the first dialogue fragment in Figure 1.
In addition, the system learns to yield its current
turn to a user that is barging-in if its own ID is low,
falling or rising, or if the ID of the incoming user
utterance is high. If the system?s own ID is high, but
the user?s is not, it will try to keep the turn.6 This is
exemplified in the third dialogue fragment in Figure
1.
We compare our learnt policy against two base-
lines. Baseline 1 was designed to always generate
barge-ins after an information peak in a user utter-
ance, i.e. when ID has just switched from high to
falling. We chose this baseline to confirm that users
indeed prefer barge-ins before information peaks
rather than at any point of low ID. Baseline 1 yields
a turn to a user barge-in if its own ID is low and tries
to keep it otherwise. Baseline 2 generates barge-ins
and backchannels randomly and at any point during
a user utterance. The decision of yielding or keeping
a turn in case of a user barge-in is also random. Both
baselines also use HRL to optimise their IP strategy.
We do not compare different IP strategies, which has
been done in detail by Rieser et al2010). All re-
6Incidentally, this also helps to prevent the system yielding
its turn to a user backchannel; cf. Example 2 in Fig. 1.
101 102 103 104
?120
?100
?80
?60
?40
?20
0
20
40
60
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Learnt
Baseline1
Baseline2
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the HRL agent and its baselines.
sults are summarised in Table 3.
6.1 Average Rewards over Time
Figure 5 shows the performance of all systems in
terms of average rewards in simulation. The learnt
policy outperforms both baselines. While the learnt
policy and Baseline 1 appear to achieve similar per-
formance, an absolute comparison of the last 1000
episodes of each behaviour shows that the improve-
ment of the HRL agent over Baseline 1 corresponds
to 23.42%. The difference between the learnt policy
and its baselines is significant at p < 0.0001 accord-
ing to a paired t-test and has a high effect size of
r = 0.85.
The main reason for these different performances
is the moment each system will barge-in. Since
Baseline 1 barges-in on users after an information
peak, when ID may still be high, it continuously re-
ceives a negative reward reflecting the user prefer-
ence for late barge-ins. As a result of this contin-
uous negative reward, the agent will then learn to
avoid barge-ins altogether, which may in turn lead
to less efficient interactions because low confidence
ASR scores are clarified only late in the interaction.
The main problem of the random barge-ins of
Baseline 2 is that users may often have to restart
a turn because the system barged-in too early or
in the middle of an information peak. In addition,
Baseline 2 needs to occasionally self-correct its own
utterances because it started to present information
too early, when input hypotheses were not yet stable
enough to act upon them.
90
Policy Average Reward User Rating (%)
Learnt 55.54??,? 43%??
Baseline 1 45.0?? 26%
Baseline 2 1.47 31%
Table 3: Comparison of policies in terms of average re-
wards and user ratings. ? indicates a significant improve-
ment over Baseline 1 and ?? over Baseline 2.
6.2 Human Rating Study
To confirm our simulation-based results, we con-
ducted a user rating study on the CrowdFlower
crowd sourcing platform.7 Participants were
shown user utterances along with three options of
barging-in over them. For example: | I want
[OPTION 1] Italian food [OPTION 2] in the
city [OPTION 3] centre|, where OPTION 1 cor-
responds to the learnt policy, OPTION 2 to Baseline
2 and OPTION 3 to Baseline 1.
Users were asked to choose one option which they
considered the best moment for a barge-in. Partici-
pants in the study rated altogether 144 utterances.
They preferred the learnt system 63 times (43%),
Baseline 1 37 times (26%) and Baseline 2 44 times
(31%). This is statistically significant at p < 0.02
according to a Chi-Square test (?2 = 7.542, df =
2). In a separate test, directly comparing the learnt
policy and Baseline 1, learnt was chosen signifi-
cantly more often than Baseline 1; i.e. 79% of the
time (for 127 utterances, using a 1-tailed Sign test,
p < 0.0001). Finally, learnt was directly compared
to Baseline 2 and shown to be significantly more of-
ten chosen; i.e. 59% of the time (138 utterances, 1-
tailed Sign test, p < 0.025). These results provide
evidence that an optimisation of the timing of gener-
ating barge-ins and backchannels in incremental di-
alogue can be sensitive to fine-grained cues in evolv-
ing ID and therefore achieve a high level of adaptiv-
ity. Such sensitivity is difficult to hand-craft as can
be concluded w.r.t. the performance of Baseline 1,
which received similar rewards to learnt in simula-
tion, but is surprisingly beaten by the random Base-
line 2 here. This indicates a strong human dislike
for late barge-ins. The bad performance of Base-
line 2 in terms of average rewards was due to the
random barge-ins leading to less efficient dialogues.
7www.crowdflower.com
Regarding user ratings however, Baseline 2 was pre-
ferred over Baseline 1. This is most likely due to the
timing of barge-ins: since Baseline 2 has a chance
of barging-in at earlier occasions than Baseline 1,
it may have received better ratings. The evaluation
shows that humans care about timing of a barge-in
regarding the density of information that is currently
conveyed and dislike late barge-ins. ID is then useful
in determining when to barge-in. We can therefore
further conclude that ID can be a feasible optimisa-
tion criterion for incremental decision making.
7 Conclusion and Future Work
We have presented a novel approach to incremen-
tal dialogue decision making based on Hierarchical
RL combined with the notion of information den-
sity. We presented a learning agent in the domain of
IP for restaurant recommendations that was able to
generate backchannels and barge-ins for higher re-
sponsiveness in interaction. Results in terms of av-
erage rewards and a human rating study have shown
that a learning agent that is optimised based on a
partially data-driven reward function that addresses
information density can learn to decide when and if
it is beneficial to barge-in or backchannel on user
utterances and to deal with backchannels and barge-
ins from the user. Future work can take several di-
rections. Given that ID is a measure influencing
human language production, we could replace our
template-based surface realiser by an agent that op-
timises the information density of its output. Cur-
rently we learn the agent?s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situa-
tions could be achieved if the agent was able to learn
from ongoing interactions. Finally, we can confirm
the human results obtained from an overhearer-style
evaluation in a real interactive setting and explicitly
extend our language model to discourse phenomena
such as pauses or hesitations to take them into ac-
count in measuring ID.
Acknowledgements
The research leading to this work has received funding
from EC?s FP7 programmes: (FP7/2011-14) under grant
agreement no. 287615 (PARLANCE); (FP7/2007-13) un-
der grant agreement no. 216594 (CLASSiC); (FP7/2011-
14) under grant agreement no. 270019 (SPACEBOOK);
91
and (FP7/2011-16) under grant agreement no. 269427
(STAC). Many thanks to Michael White for discussion
of the original idea of using information density as an op-
timisation metric.
References
Matthew Aylett and Alice Turk. 2004. The smooth signal
redundancy hypothesis: A functional explanation for
the relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31?56.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Alan Bell, Dan Jurafsky, Eric Fossler-Lussier, Cynthia
Girand, Michelle Gregory, and Daniel Gildea. 2003.
Effects of disfluencies, predictability, and utterance
position on word form variation in english conver-
sation. Journal of the Acoustic Society of America,
113(2):1001?1024.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proceedings of 11th An-
nual SIGdial Meeting on Discourse and Dialogue.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011.
Spatially-aware Dialogue Control Using Hierarchi-
cal Reinforcement Learning. ACM Transactions on
Speech and Language Processing (Special Issue on
Machine Learning for Robust and Adaptive Spoken
Dialogue System), 7(3).
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395?429.
Heriberto Cuaya?huitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD Thesis,
University of Edinburgh, School of Informatics.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the International
Conference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Pro-
ceedings of the 10th Annual SigDial Meeting on Dis-
course and Dialogue, Queen Mary University, UK.
Thomas G. Dietterich. 1999. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227?303.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
Rate Constancy in Text. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 199?206.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid Reinforcement/Supervised Learning of
Dialogue Policies from Fixed Data Sets. Computa-
tional Linguistics, 34(4):487?511.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology, 61:23?62.
Srini Janarthanam and Oliver Lemon. 2010. Learning
to Adapt to Unknown Users: Referring Expression
Generation in Spoken Dialogue Systems. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 69?78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Oliver Lemon. 2011. Learning What to Say and How to
Say It: Joint Optimization of Spoken Dialogue Man-
agement and Natural Language Generation.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A Stochastic Model of Computer-Human Inter-
action for Learning Dialogue Strategies. IEEE Trans-
actions on Speech and Audio Processing, 8:11?23.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
Advances in Neural Information Processing Systems,
19.
Olivier Pietquin and Dutoit. 2006. A Probabilis-
tic Framework for Dialogue Simulation and Optimal
Strategy Learning. IEEE Transactions on Speech and
Audio Processing, 14(2):589?599.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Rajakrishnan Rajkumar and Michael White. 2011. Lin-
guistically Motivated Complementizer Choice in Sur-
face Realization. In Proceedings of the EMNLP-11
Workshop on Using Corpora in NLG, Edinburgh, Scot-
land.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proceedings of the 10th Conference of the
92
North American Chapter of the Association for Com-
putational Linguistics?Human Language Technolo-
gies (NAACL-HLT), Boulder, Colorado.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Uppsala, Sweden.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
David Schlangen and Gabriel Skantze. 2011. A General,
Abstract Model of Incremental Dialogue Processing.
Dialogue and Discourse, 2(1).
Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-
son Williams. 2011. Stability and Accuracy in In-
cremental Speech Recognition. In Proceedings of the
12th Annual SigDial Meeting on Discourse and Dia-
logue, Portland, Oregon.
Claude Shannon. 1948. A Mathematical Theory of
Communications. Bell Systems Technical Journal,
27(4):623?656.
Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105?133.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proceedings of the 11th Annual SigDial Meeting on
Discourse and Dialogue, Tokyo, Japan.
Richard Sutton and Andrew Barto. 1998. Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA.
Blaise Thomson. 2009. Statistical Methods for Spo-
ken Dialogue Management. Ph.D. thesis, University
of Cambridge.
Menno van Zaanen. 2000. Bootstrapping Syntax and
Recursion using Alignment-Based Learning. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning, ICML ?00, pages 1063?1070.
Marilyn Walker. 2000. An Application of Reinforcement
Learning to Dialogue Strategy Selection in a Spoken
Dialogue System for Email. Journal of Artificial In-
telligence Research (JAIR), 12:387?416.
Steve Young, Milica Gasic, Simon Keizer, Francois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The Hidden Information State Model: A
Practical Framework for POMDP-based Spoken Dia-
logue Management. Computer Speech and Language,
24(2):150?174.
Steve Young. 2000. Probabilistic Methods in Spoken
Dialogue Systems. Philosophical Transactions of the
Royal Society (Series A), 358(1769):1389?1402.
93
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702?711,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Cluster-based Prediction of User Ratings for Stylistic Surface Realisation
Nina Dethlefs, Heriberto Cuaya?huitl, Helen Hastie, Verena Rieser and Oliver Lemon
Heriot-Watt University, Mathematical and Computer Sciences, Edinburgh
n.s.dethlefs@hw.ac.uk
Abstract
Surface realisations typically depend on
their target style and audience. A challenge
in estimating a stylistic realiser from data is
that humans vary significantly in their sub-
jective perceptions of linguistic forms and
styles, leading to almost no correlation be-
tween ratings of the same utterance. We ad-
dress this problem in two steps. First, we
estimate a mapping function between the
linguistic features of a corpus of utterances
and their human style ratings. Users are
partitioned into clusters based on the sim-
ilarity of their ratings, so that ratings for
new utterances can be estimated, even for
new, unknown users. In a second step, the
estimated model is used to re-rank the out-
puts of a number of surface realisers to pro-
duce stylistically adaptive output. Results
confirm that the generated styles are recog-
nisable to human judges and that predictive
models based on clusters of users lead to
better rating predictions than models based
on an average population of users.
1 Introduction
Stylistic surface realisation aims not only to find
the best realisation candidate for a semantic input
based on some underlying trained model, but also
aims to adapt its output to properties of the user,
such as their age, social group, or location, among
others. One of the first systems to address stylis-
tic variation in generation was Hovy (1988)?s
PAULINE, which generated texts that reflect dif-
ferent speaker attitudes towards events based on
multiple, adjustable features. Stylistic variation
in such contexts can often be modelled systemat-
ically as a multidimensional variation space with
several continuous dimensions, so that varying
stylistic scores indicate the strength of each di-
mension in a realisation candidate. Here, we fo-
cus on the dimensions of colloquialism, politeness
and naturalness. Assuming a target score on one
or more dimensions, candidate outputs of a data-
driven realiser can then be ranked according to
their predicted affinity with the target scores.
In this paper, we aim for an approach to stylis-
tic surface realisation which is on the one hand
based on natural human data so as to reflect stylis-
tic variation that is as natural as possible. On the
other hand, we aim to minimise the amount of
annotation and human engineering that informs
the design of the system. To this end, we esti-
mate a mapping function between automatically
identifiable shallow linguistic features character-
istic of an utterance and its human-assigned style
ratings. In addition, we aim to address the high
degree of variability that is often encountered in
subjective rating studies, such as assessments of
recommender systems (O?Mahony et al., 2006;
Amatriain et al., 2009), sentiment analysis (Pang
and Lee, 2005), or surface realisations, where user
ratings have been shown to differ significantly
(p<0.001) for the same utterance (Walker et al.,
2007). Such high variability can affect the per-
formance of systems which are trained from an
average population of user ratings. However, we
are not aware of any work that has addressed this
problem principally by estimating ratings for both
known users, for whom ratings exists, and un-
known users, for whom no prior ratings exist. To
achieve this, we propose to partition users into
clusters of individuals who assign similar ratings
to linguistically similar utterances, so that their
ratings can be estimated more accurately than
702
based on an average population of users. This is
similar to Janarthanam and Lemon (2014), who
show that clustering users and adapting to their
level of domain expertise can significantly im-
prove task success and user ratings. Our resulting
model is evaluated with realisers not originally
built to deal with stylistic variation, and produces
natural variation recognisable by humans.
2 Architecture and Domain
We aim to with generating restaurant recommen-
dations as part of an interactive system. To do
this, we assume that a generator input is provided
by a preceding module, e.g. the interaction man-
ager, and that the task of the surface realiser is
to find a suitable stylistically appropriate realisa-
tion. An example input is inform(food=Italian,
name=Roma), which could be expressed as The
restaurant Roma serves Italian food. A further
aspect is that users are initially unknown to the
system, but that it should adapt to them over time
by discovering their stylistic preferences. Fu-
ture work involves integrating the surface realiser
into the PARLANCE1 (Hastie et al., 2013) spo-
ken dialogue system with a method for triggering
the different styles. Here, we leave the question
of when different styles are appropriate as future
work and focus on being able to generate them.
The architecture of our model is shown in Fig-
ure 1. Training of the regression model from sty-
listically-rated human corpora is shown in the top-
left box (grey). Utterance ratings from human
judges are used to extract shallow linguistic fea-
tures as well as to estimate user clusters. Both
types of information inform the resulting stylis-
tic regression model. For surface realisation (top-
right box, blue), a semantic input from a preced-
ing model is given as input to a surface realiser.
Any realiser is suitable that returns a ranked list of
output candidates. The resulting list is re-ranked
according to stylistic scores estimated by the re-
gressor, so that the utterance which most closely
reflects the target score is ranked highest. The re-
ranking process is shown in the lower box (red).
3 Related Work
3.1 Stylistic Variation in Surface Realisation
Our approach is most closely related to work by
Paiva and Evans (2005) and Mairesse and Walker
1http://parlance-project.eu
User Clusters
Regressor Surface Realisation
Ranking + 
Evaluation
Figure 1: Architecture of stylistic realisation model.
Top left: user clusters are estimated from corpus ut-
terances described by linguistic features and ratings.
Top right: surface realisation ranks a list of output can-
didates based on a semantic input. These are ranked
stylistically given a trained regressor.
(2011), discussed in turn here. Paiva and Evans
(2005) present an approach that uses multivari-
ate linear regression to map individual linguistic
features to distinguishable styles of text. The ap-
proach works in three steps. First, a factor anal-
ysis is used to determine the relevant stylistic di-
mensions from a corpus of human text using shal-
low linguistic features. Second, a hand-crafted
generator is used to produce a large set of ut-
terances, keeping traces of each generator deci-
sion, and obtaining style scores for each output
based on the estimated factor model. The result
is a dataset of <generator decision, style score>
pairs which can be used in a correlation analy-
sis to identify the predictors of particular output
styles. During generation, the correlation equa-
tions inform the generator at each choice point so
as to best express the desired style. Unfortunately,
no human evaluation of the model is presented so
that it remains unclear to what extent the gener-
ated styles are perceivable by humans.
Closely related is work by Mairesse and Walker
(2011) who present the PERSONAGE system,
which aims to generate language reflecting par-
ticular personalities. Instead of choosing genera-
tor decisions by considering their predicted style
scores, however, Mairesse and Walker (2011) di-
rectly predict generator decisions based on tar-
get personality scores. To obtain the generator,
the authors first generate a corpus of utterances
which differ randomly in their linguistic choices.
All utterances are rated by humans indicating the
703
extent to which they reflect different personality
traits. The best predictive model is then chosen in
a comparison of several classifiers and regressors.
Mairesse and Walker (2011) are the first to evalu-
ate their generator with humans and show that the
generated personalities are indeed recognisable.
Approaches on replicating personalities in re-
alisations include Gill and Oberlander (2002) and
Isard et al. (2006). Porayska-Pomsta and Mellish
(2004) and Gupta et al. (2007) are approaches to
politeness in generation, based on the notion of
face and politeness theory, respectively.
3.2 User Preferences in Surface Realisation
Taking users? individual content preferences into
account for training generation systems can
positively affect their performance (Jordan and
Walker, 2005; Dale and Viethen, 2009). We are
interested in individual user perceptions concern-
ing the surface realisation of system output and
the way they relate to different stylistic dimen-
sions. Walker et al. (2007) were the first to show
that individual preferences exist for the perceived
quality of realisations and that these can be mod-
elled in trainable generation. They train two ver-
sions of a rank-and-boost generator, a first version
of which is trained on the average population of
user ratings, whereas a second one is trained on
the ratings of individual users. The authors show
statistically that ratings from different users are
drawn from different distributions (p<0.001) and
that significantly better performance is achieved
when training and testing on data of individual
users. In fact, training a model on one user?s rat-
ings and testing it on another?s performs as badly
as a random baseline. However, no previous work
has modelled the individual preferences of unseen
users?for whom no training data exists.
4 Estimation of Style Prediction Models
4.1 Corpora and Style Dimensions
Our domain of interest is the automatic generation
of restaurant recommendations that differ with re-
spect to their colloquialism and politeness and are
as natural as possible. All three stylistic dimen-
sion were identified from a qualitative analysis of
human domain data. To estimate the strength of
each of them in a single utterance, we collect user
ratings for three data sets that were collected un-
der different conditions and are freely available.
Corpus Colloquial Natural Polite
LIST 3.38 ? 1.5 4.06 ? 1.2 4.35 ? 0.8
MAI 3.95 ? 1.2 4.32 ? 1.0 4.27 ? 0.8
CLASSIC 4.29 ? 1.1 4.20 ? 1.2 3.64 ? 1.3
Table 1: Average ratings with standard deviations.
Ratings between datasets (except one) differ signifi-
cantly at p<0.01, using the Wilcoxon signed-rank test.
? LIST is a corpus of restaurant recommenda-
tions from the website The List.2 It consists
of professionally written reviews. An exam-
ple is ?Located in the heart of Barnwell, Bel-
uga is an excellent restaurant with a smart
menu of modern Italian cuisine.?
? MAI is a dataset collected by Mairesse et
al. (2010),3 using Amazon Mechanical Turk.
Turkers typed in recommendations for vari-
ous specified semantics; e.g. ?I recommend
the restaurant Beluga near the cathedral.?
? CLASSIC is a dataset of transcribed spoken
user utterances from the CLASSiC project.4
The utterances consist of user queries for
restaurants, such as ?I need an Italian
restaurant with a moderate price range.?
Our joint dataset consists of 1, 361 human ut-
terances, 450 from the LIST, 334 from MAI,
and 577 from CLASSIC. We asked users on the
CrowdFlower crowdsourcing platform5 to read
utterances and rate their colloquialism, politeness
and naturalness on a 1-5 scale (the higher the bet-
ter). The following questions were asked.
? Colloquialism: The utterance is colloquial,
i.e. could have been spoken.
? Politeness: The utterance is polite / friendly.
? Naturalness: The utterance is natural, i.e.
could have been produced by a human.
The question on naturalness can be seen as a gen-
eral quality check for our training set. We do
not aim to generate unnatural utterances. 167
users took part in our rating study leading to a
rated dataset of altogether 3, 849 utterances. All
users were from the USA. The average ratings per
dataset and stylistic dimension are summarised
in Table 1. From this, we can see that LIST ut-
terances were perceived as the least natural and
2http://www.list.co.uk/
3http://people.csail.mit.edu/francois/
research/bagel/
4http://www.classic-project.org/
5http://crowdflower.com/
704
colloquial, but as the most polite. CLASSIC ut-
terances were perceived as the most colloquial,
but the least polite, and MAI utterances were rated
as the most natural. Differences between ratings
for each dimension and dataset are significant at
p<0.01, using the Wilcoxon signed-rank test, ex-
cept the naturalness for MAI and CLASSIC.
Since we are mainly interested in the lexical
and syntactic features of utterances here, the fact
that CLASSIC utterances are spoken, whereas the
other two corpora are written, should not affect
the quality of the resulting model. Similarly, some
stylistic categories may seem closely related, such
as colloquialism and naturalness, or orthogonal
to each other, such as politeness and colloqui-
alism. However, while ratings for colloquialism
and naturalness are very close for the CLASSIC
dataset, they vary significantly for the two other
datasets (p<0.01). Also, the ratings for colloqui-
alim and politeness show a weak positive corre-
lation of 0.23, i.e. are not perceived as orthogo-
nal by users. These results suggest that all in all
our three stylistic categories are perceived as suf-
ficiently different from each other and suitable for
training to predict a spectrum of different styles.
Another interesting aspect is that individual
user ratings vary significantly, leading to a high
degree of variability for identical utterances. This
will be the focus of the following sections.
4.2 Feature Estimation
Table 2 shows the feature set we will use in our
regression experiments. We started from a larger
subset including 45 lexical and syntactic features
as well as unigrams and bigrams, all of which
could be identified from the corpus without man-
ual annotation. The only analysis tool we used
was the Stanford Parser,6 which identified certain
types of words (pronouns, wh-words) or the depth
of syntactic embedding. A step-wise regression
analysis was then carried out to identify those
features that contributed significantly (at p<0.01)
to the overall regression equation obtained per
stylistic dimension. Of all lexical features (uni-
grams and bigrams), the word with was the only
contributor. A related feature was the average tf-
idf score of the content words in an utterance.
6http://nlp.stanford.edu/software/
lex-parser.shtml
Feature Type
Length of utterance num
Presence of personal pronouns bool
Presence of WH words bool
with cue word bool
Presence of negation bool
Average length of content words num
Ave tf-idf score of content words num
Depth of syntactic embedding num
Table 2: Features used for regression, which were
identified as significant contributors (p<0.01) from a
larger feature set in a step-wise regression analysis.
4.3 Regression Experiments
Based on the features identified in Section 4.2, we
train a separate regressor for each stylistic dimen-
sion. The task of the regressor is to predict, based
on the extracted linguistic features of an utterance,
a score in the range of 1-5 for colloquialism, po-
liteness and naturalness. We compare: (1) a mul-
tivariate multiple regressor (MMR), (2) an M5P
decision tree regressor, (3) a support vector ma-
chine (SVM) with linear kernel, and (4) a ZeroR
classifier, which serves as a majority baseline. We
used the R statistics toolkit7 for the MMR and the
Weka toolkit8 for the remaining models.
Average User Ratings The regressors were first
trained to predict the average user ratings of an ut-
terance and evaluated in a 10-fold cross validation
experiment. Table 3 shows the results. Here, r
denotes the Pearson correlation coefficient, which
indicates the correlation between the predicted
and the actual user scores; R2 is the coefficient of
determination, which provides a measure of how
well the learnt model fits the data; and RMSE
refers to the Root Mean Squared Error, the error
between the predicted and actual user ratings.
We can observe that MMR achieves the best
performance for predicting colloquialism and nat-
uralness, whereas M5P best predicts politeness.
Unfortunately, all regressors achieve at best a
moderate correlation with human ratings. Based
on these results, we ran a correlation analysis for
all utterances for which more than 20 original
user ratings were available. The purpose was to
find out to what extent human raters agree with
each other. The results showed that user agree-
ment in fact ranges from a high positive corre-
7http://www.r-project.org/
8http://www.cs.waikato.ac.nz/ml/weka/
705
Model r R2 RMSE
Colloquial
MMR 0.50 0.25 0.85
SVM 0.47 0.22 0.86
M5P 0.48 0.23 0.85
ZeroR -0.08 0.006 0.97
Natural
MMR 0.30 0.09 0.78
SVM 0.24 0.06 0.81
M5P 0.27 0.07 0.78
ZeroR -0.09 0.008 0.81
Polite
MMR 0.33 0.11 0.71
SVM 0.31 0.09 0.73
M5P 0.42 0.18 0.69
ZeroR -0.09 0.008 0.76
Table 3: Comparison of regression models per dimen-
sion using average user ratings. The best model is
indicated in bold-face for the correlation coefficient.
Model r R2 RMSE
Colloquial
MMR 0.61 0.37 1.05
SVM 0.36 0.13 1.3
M5P 0.56 0.31 1.07
ZeroR -0.06 0.004 1.3
Natural
MMR 0.55 0.30 0.96
SVM 0.36 0.13 1.13
M5P 0.49 0.24 0.99
ZeroR -0.08 0.06 1.13
Polite
MMR 0.69 0.48 0.76
SVM 0.54 0.30 0.92
M5P 0.71 0.50 0.73
ZeroR -0.04 0.002 1.04
Table 4: Comparison of regression models per dimen-
sion using individual user ratings. The best model is
indicated in bold-face for the correlation coefficient.
lation of 0.79 to a moderate negative correlation
of ?0.55. The average is 0.04 (SD=0.95), i.e.
indicating no correlation between user ratings,
even for the same utterance. This observation is
partially in line with related work that has found
high diversity in subjective user ratings. Yeh and
Mellish (1997) report only 70% agreement of hu-
man judges on the best choice of referring ex-
pression. Amatriain et al. (2009) report incon-
sistencies in user ratings in recommender systems
with an RMSE range of 0.55 to 0.81 and argue
that this constitutes a lower bound for system per-
formance. This inconsistency is exacerbated by
raters recruited via crowdsourcing platforms as
in our study (Koller et al., 2010; Rieser et al.,
2011). However, while crowdsourced data have
been shown to contain substantially more noise
than data collected in a lab environment, they do
tend to reflect the general tendency of their more
controlled counterparts (Gosling et al., 2004).
Individual User Ratings Given that individual
preferences exist for surface realisation (Walker
et al., 2007), we included the user?s ID as a re-
gression feature and re-ran the experiments. The
hypothesis was that if users differ in their pref-
erences for realisation candidates, they may also
differ in terms of their perceptions of linguistic
styles. The results shown in Table 4 support this:
the obtained correlations are significantly higher
(p<0.001, using the Fisher r-to-z transformation)
than those without the user?s ID (though we are
still not able to model the full variation observed
in ratings). Importantly, this shows that user rat-
ings are intrinsically coherent (not random) and
that variation exists mainly for inter-user agree-
ment. This model performs satisfactorily for a
known population of users. However, it does not
allow the prediction of ratings of unknown users,
who we mostly encounter in generation.
5 Clustering User Rating Behaviour
5.1 Spectral Clustering
The goal of this section is to find a number of k
clusters which partition our data set of user rat-
ings in a way that users in one cluster rate ut-
terances with particular linguistic properties most
similarly to each other, while rating them most
dissimilarly to users in other clusters. We as-
sume a set of n data points x
1
. . . x
n
, which
in our case correspond to an individual user or
group of users, characterised in terms of word
bigrams, POS tag bigrams, and assigned rat-
ings of the utterance they rated. An example
is Beluga NNP serves VBZ Italian JJ food NN;
[col=5.0, nat=5.0, pol=4.0]. Features were cho-
sen as a subset of relevant features from the larger
set used for regression above.
Using spectral clustering (von Luxburg, 2007),
clusters can be identified from a set of eigenvec-
tors of an affinity matrix S derived from pair-wise
similarities between data points s
ij
= s(x
i
, x
j
)
using a symmetric and non-negative similarity
function. To do that, we use a cumulative simi-
larity based on the Kullback-Leibler divergence,
D(P,Q) =
?
i
p
i
log
2
(
p
i
q
i
) +
?
j
q
j
log
2
(
q
j
p
j
)
2
,
where P is a distribution of words, POS tags or
ratings in data point x
i
; and Q a similar distribu-
tion in data point x
j
. The lower the cumulative di-
706
0.
3
0.
4
0.
5
0.
6
Number of Clusters
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
1 3 5 7 9 20 40 60 80 100 167
Individual
Clusters
Average
Figure 2: Average correlation coefficient for different
numbers of clusters. For comparison, results from av-
erage and individual user ratings are also shown.
vergence between two data sets, the more similar
they are. To find clusters of similar users from the
affinity matrix S, we use the algorithm described
in Ng et al. (2001). It derives clusters by choosing
the k largest eigenvectors u
1
, u
2
, . . . , u
k
from the
Laplacian matrix L = D1/2?SD1/2 (where D is
a diagonal matrix), arranging them into columns
in a matrix U = [u
1
u
2
. . . u
k
] and then normalis-
ing them for length. The result is a new matrix T ,
obtained through t
ij
= u
ij
/(
?
k
u
2
ik
)
1/2
. The set
of clusters C
1
, . . . C
k
can then be obtained from T
using the K-means algorithm, where each row in
T serves as an individual data point. Finally, each
original data point x
i
(row i of T ) is assigned to a
cluster C
j
. In comparison to other clustering algo-
rithms, experiments by Ng et al. (2001) show that
spectral clustering is robust for convex and non-
convex data sets. The authors also demonstrate
why using K-means only is often not sufficient.
The main clusters obtained describe surface
realisation preferences by particular groups of
users. An example is the realisation of the loca-
tion of a restaurant as a prepositional phrase or as
a relative clause as in restaurant in the city centre
vs. restaurant located in the city centre; or the re-
alisation of the food type as an adjective, an Ital-
ian restaurant, vs. a clause, this restaurant serves
Italian food. Clusters can then be characterised as
different combinations of such preferences.
5.2 Results: Predicting Stylistic Ratings
Figure 2 shows the average correlation coefficient
r across dimensions in relation to the number
of clusters, in comparison to the results obtained
with average and individual user ratings. We can
see that the baseline without user information is
outperformed with as few as three clusters. From
30 clusters on, a medium correlation is obtained
until another performance jump occurs around 90
clusters. Evidently, the best performance would
be achieved by obtaining one cluster per user, i.e.
167 clusters, but nothing would be gained in this
way, and we can see that useful generalisations
can be made from much fewer clusters. Based on
the clusters found, we will now predict the ratings
of known and unknown users.
Known Users For known users, first of all, Fig-
ure 3 shows the correlations between the predicted
and actual ratings for colloquialism, politeness
and naturalness based on 90 user clusters. Cor-
relation coefficients were obtained using an MMR
regressor. We can see that a medium correlation is
achieved for naturalness and (nearly) strong cor-
relations are achieved for politeness and colloqui-
alism. This confirms that clustering users can help
to better predict their ratings than based on shal-
low linguistic features alone, but that more gener-
alisation is achieved than based on individual user
ratings that include the user?s ID as a regression
feature. The performance gain in comparison to
predicting average ratings is significant (p<0.01)
from as few as three clusters onwards.
Unknown Users We initially sort unknown
users into the majority cluster and then aim to
make more accurate cluster allocations as more
information becomes available. For example, af-
ter a user has assigned their first rating, we can
take it into account to re-estimate their cluster
more accurately. Clusters are re-estimated with
each new rating, based on our trained regression
model. While estimating a user cluster based on
linguistic features alone yields an average corre-
lation of 0.38, an estimation based on linguistic
features and a single rating alone already yields an
average correlation of 0.45. From around 30 rat-
ings, the average correlation coefficients achieved
are as good as for known users. More importantly,
though, estimations based on a single rating alone
significantly outperform ratings based on the av-
707
(a)
1 2 3 4 5
2
3
4
5
Correlation: Colloquialism
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
(b)
1 2 3 4 5
1
2
3
4
5
Correlation: Naturalness
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
(c)
1 2 3 4 5
1
2
3
4
5
Correlation: Politeness
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
Figure 3: Correlations per dimension between actual and predicted user ratings based on 90 user clusters: (a)
Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001).
erage population of users (p<0.001). Fig. 4 shows
this process. It shows the correlation between pre-
dicted and actual user ratings for unknown users
over time. This is useful in interactive scenarios,
where system behaviour is refined as more infor-
mation becomes available (Cuaya?huitl and Deth-
lefs, 2011; Gas?ic? et al., 2011), or for incremental
systems (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012b; Dethlefs et al., 2012a).
0.
3
0.
4
0.
5
0.
6
Number of Ratings
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
1 2 3 4 5 6 7 8 9 10 15 20 30
90 Clusters
Ratings
Average
Figure 4: Average correlation coefficient for unknown
users with an increasing number of ratings. Results
from 90 clusters and average ratings are also shown.
6 Evaluation: Stylistically-Aware
Surface Realisation
To evaluate the applicability of our regression
model for stylistically-adaptive surface realisa-
tion, this section describes work that compares
four different surface realisers, which were not
originally developed to produce stylistic variation.
To do that, we first obtain the cluster for each in-
put sentence s: c? = argmin
c?C
?
x
D(P
x
s
|Q
x
c
),
where x refers to n-grams, POS tags or ratings
(see Section 5.1); P refers to a discrete probability
distribution of sentence s; and Q refers to a dis-
crete probability distribution of cluster c. The best
cluster is used to compute the style score of sen-
tence s using: score(s) =
?
n
i
?
i
f
i
(s), c
?
? F ,
where ?
i
are the weights estimated by the regres-
sor, and f
i
are the features of sentence s; see Table
2. The idea is that if well-phrased utterances can
be generated, whose stylistic variation is recog-
nisable to human judges, then our regressor can
be used in combination with any statistical sur-
face realiser. Note however that the stylistic vari-
ation observed depends on the stylistic spectrum
that each realiser covers. Here, our goal is mainly
to show that whatever stylistic variation exists in
a realiser can be recognised by our model.
6.1 Overview of Surface Realisers
In a human rating study, we compare four surface
realisers (ordered alphabetically), all of which
are able to return a ranked list of candidate re-
alisations for a semantic input. Please refer to
the references given for details of each system.
The BAGEL and SPaRKy realisers were compared
based on published ranked output lists.9
? BAGEL is a surface realiser based on dy-
namic Bayes Nets originally trained using
Active Learning by Mairesse et al. (2010).
It was shown to generate well-phrased utter-
ances from unseen semantic inputs.
? CRF (global) treats surface realisation as a
9Available from http://people.csail.mit.
edu/francois/research/bagel and http://
users.soe.ucsc.edu/
?
maw/downloads.html.
708
System Utterance
BAGEL Beluga is a moderately priced
restaurant in the city centre area.
Col = 4.0, Pol = 4.0, Nat = 4.0
CRF (global) Set in the city centre, Beluga is a
moderately priced location for the
celebration of the Italian spirit.
Col = 2.0, Pol = 5.0, Nat = 2.0
pCRU Beluga is located in the city centre
and serves cheap Italian food.
Col = 4.0, Pol = 3.0, Nat = 5.0
SPaRKy Beluga has the best overall quality
among the selected restaurants
since this Italian restaurant has
good decor, with good service.
Col = 3.0, Pol = 4.0, Nat = 5.0
Table 5: Example utterances for the BAGEL, CRF
(global), pCRU and SPaRKy realisers shown to users.
Sample ratings from individual users are also shown.
sequence labelling task: given a set of (ob-
served) linguistic features, it aims to find the
best (hidden) sequence of phrases realising a
semantic input (Dethlefs et al., 2013).
? pCRU is based on probabilistic context-
free grammars and generation is done using
Viterbi search, sampling (used here), or ran-
dom search. It is based on Belz (2008).
? SPaRKy is based on a rank-and-boost ap-
proach. It learns a mapping between the lin-
guistic features of a target utterance and its
predicted user ratings and ranks candidates
accordingly (Walker et al., 2007).
6.2 Results: Recognising Stylistic Variation
242 users from the USA took part in a rating study
on the CrowdFlower platform and rated altogether
1, 702 utterances, from among the highest-ranked
surface realisations above. For each utterance
they read, they rated the colloquialism, natura-
less and politeness based on the same questions
as in Section 4.1, used to obtain the training data.
Based on this, we compare the perceived strength
of each stylistic dimension in an utterance to the
one predicted by the regressor. Example utter-
ances and ratings are shown in Table 5. Results
are shown in Table 6 and confirm our observa-
tions: ratings for known users can be estimated
with a medium (or high) correlation based on
clusters of users who assign similar ratings to ut-
terances with similar linguistic features. We can
also see that such estimations do not depend on a
particular data set or realiser.
System Colloquial Polite Natural
BAGEL 0.78 0.66 0.69
CRF global 0.58 0.63 0.63
pCRU 0.67 0.42 0.77
SPaRKy 0.87 0.56 0.81
Table 6: Correlation coefficients between subjective
user ratings and ratings predicted by the regressor for
known users across data-driven surface realisers.
A novel aspect of our technique in compari-
son to previous work on stylistic realisation is
that it does not depend on the time- and resource-
intensive design of a hand-coded generator, as in
Paiva and Evans (2005) and Mairesse and Walker
(2011). Instead, it can be applied in conjunc-
tion with any system designer?s favourite realiser
and preserves the realiser?s original features by
re-ranking only its top n (e.g. 10) output candi-
dates. Our method is therefore able to strike a
balance between highly-ranked and well-phrased
utterances and stylistic adaptation. A current lim-
itation of our model is that some ratings can still
not be predicted with a high correlation with hu-
man judgements. However, even the medium cor-
relations achieved have been shown to be signif-
icantly better than estimations based on the aver-
age population of users (Section 5.2).
7 Conclusion and Future Work
We have presented a model of stylistic realisation
that is able to adapt its output along several stylis-
tic dimensions. Results show that the variation is
recognisable by humans and that user ratings can
be predicted for known as well as unknown users.
A model which clusters individual users based
on their ratings of linguistically similar utterances
achieves significantly higher performance than a
model trained on the average population of rat-
ings. These results may also play a role in other
domains in which users display variability in their
subjective ratings, e.g. recommender systems,
sentiment analysis, or emotion generation. Future
work may explore the use of additional cluster-
ing features as a more scalable alternative to re-
ranking. It also needs to determine how user feed-
back can be obtained during an interaction, where
asking users for ratings may be disruptive. Possi-
bilities include to infer user ratings from their next
dialogue move, or from multimodal information
such as hesitations or eye-tracking.
709
Acknowledgements This research was funded
by the EC FP7 programme FP7/2011-14 under
grant agreements no. 270019 (SPACEBOOK)
and no. 287615 (PARLANCE).
References
Xavier Amatriain, Josep M. Pujol, and Nuria Oliver.
2009. I like It... I Like It Not: Evaluating User Rat-
ings Noise in Recommender Systems. In In the 17th
International Conference on User Modelling, Adap-
tation, and Personalisation (UMAP), pages 247?
258, Trento, Italy. Springer-Verlag.
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language En-
gineering, 14(4):431?455.
Penelope Brown and Stephen Levinson. 1987. Some
Universals in Language Usage. Cambridge Univer-
sity Press, Cambridge, UK.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011. Op-
timizing Situated Dialogue Management in Un-
known Environments. In INTERSPEECH, pages
1009?1012.
Robert Dale and Jette Viethen. 2009. Referring
Expression Generation Through Attribute-Based
Heuristics. In Proceedings of the 12th Euro-
pean Workshop on Natural Language Generation
(ENLG), Athens, Greece.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the Interna-
tional Conference on Natural Language Generation
(INLG), Chicago, Illinois, USA.
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl,
and Oliver Lemon. 2013. Conditional Random
Fields for Responsive Surface Realisation Using
Global Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Sofia, Bulgaria.
Michael Fleischman and Eduard Hovy. 2002. Emo-
tional Variation in Speech-Based Natural Language
Generation. In Proceedings of the 2nd International
Natural Language Generation Conference.
Milica Gas?ic?, Filip Jurc???c?ek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-Line Policy Optimi-
sation of Spoken Dialogue Systems via Interaction
with Human Subjects. In Proceedings of the IEEE
Automatic Speech Recognition and Understanding
(ASRU) Workshop.
Alastair Gill and Jon Oberlander. 2002. Taking Care
of the Linguistic Features of Extraversion. In Pro-
ceedings of the 24th Annual Conference of the Cog-
nitive Science Society, pages 363?368, Fairfax, VA.
Samuel Gosling, Simine Vazire, Sanjay Srivastava,
and Oliver John. 2004. Should We Trust Web-
Based Studies? A Comparative Analysis of Six Pre-
conceptions About Internet Questionnaires. Ameri-
can Psychologist, 59(2):93?104.
Swati Gupta, Marilyn Walker, and Daniela Romano.
2007. How Rude Are You? Evaluating Politeness
and Affect in Interaction. In Proceedings of the
2nd International Conference on Affective Comput-
ing and Intelligent Interaction.
Helen Hastie, Marie-Aude Aufaure, Panos Alex-
opoulos, Heriberto Cuayhuitl, Nina Dethlefs,
James Henderson Milica Gasic, Oliver Lemon,
Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis,
Yves Vanrompay, Boris Villazon-Terrazas, and
Steve Young. 2013. Demonstration of the PAR-
LANCE System: A Data-Driven, Incremental, Spo-
ken Dialogue System for Interactive Search. In Pro-
ceedings of the 14th Annual Meeting of the Special
Interest Group on Discourse and Dialogue (SIG-
dial).
Eduard Hovy. 1988. Generating Natural Language
under Pragmatic Constraints. Lawrence Erlbaum
Associates, Hillsdale, NJ.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and Alignment in Generated
Dialogues. In Proceedings of the 4th International
Natural Language Generation Conference (INLG),
Sydney, Australia.
Srini Janarthanam and Oliver Lemon. 2014. Adaptive
generation in dialogue systems using dynamic user
modeling. Computational Linguistics. (in press).
Pamela Jordan and Marilyn Walker. 2005. Learning
Content Selection Rules for Generating Object De-
scriptions in Dialogue. Journal of Artificial Intelli-
gence Research, 24:157?194.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, and Johanna Moore.
2010. The First Challenge on Generating Instruc-
tions in Virtual Environments. In M. Theune and
E. Krahmer, editors, Empirical Methods in Natu-
ral Language Generation, pages 337?361. Springer
Verlag, Berlin/Heidelberg.
Franc?ois Mairesse and Marilyn Walker. 2011. Con-
trolling User Perceptions of Linguistic Style: Train-
able Generation of Personality Traits. Computa-
tional Linguistics, 37(3):455?488, September.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Si-
mon Keizer, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Phrase-based statistical language
generation using graphical models and active learn-
ing. In Proceedings of the Annual Meeting of the
710
Association for Computational Linguistics (ACL),
pages 1552?1561.
Andrew Ng, Michael Jordan, and Yair Weiss. 2001.
On Spectral Clustering: Analysis and an Algorithm.
In Advances in Neural Information Processing Sys-
tems, pages 849?856. MIT Press.
Michael O?Mahony, Neil Hurley, and Gue?nole? Sil-
vestre. 2006. Detecting Noise in Recommender
System Databases. In Proceedings of the Inter-
national Conference on Intelligent User Interfaces
(IUI)s. ACM Press.
Daniel Paiva and Roger Evans. 2005. Empirically-
Based Control of Natural Language Generation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
Ann Arbor, Michigan, USA.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploit-
ing Class Relationships for Sentiment Categoriza-
tion with Respect to Rating Scales. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL).
Kaska Porayska-Pomsta and Chris Mellish. 2004.
Modelling Politness in Natural Language Gener-
ation. In Proceedings of the 3rd International
Natural Language Generation Conference (INLG),
Brighton, UK.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual Sig-
Dial Meeting on Discourse and Dialogue, Tokyo,
Japan.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17(4).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413?456.
Ching-long Yeh and Chris Mellish. 1997. An Empir-
ical Study on the Generation of Anaphora in Chi-
nese. Computational Linguistics, 23:169?190.
711
Learning and Evaluation of Dialogue
Strategies for New Applications:
Empirical Methods for Optimization
from Small Data Sets
Verena Rieser?
School of GeoSciences/University of
Edinburgh
Oliver Lemon??
School of Mathematical and Computer
Sciences/Heriot-Watt University
We present a new data-driven methodology for simulation-based dialogue strategy learning,
which allows us to address several problems in the field of automatic optimization of dialogue
strategies: learning effective dialogue strategies when no initial data or system exists, and
determining a data-driven reward function. In addition, we evaluate the result with real users,
and explore how results transfer between simulated and real interactions. We use Reinforce-
ment Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated
environment which is ?bootstrapped? from small amounts of Wizard-of-Oz (WOZ) data. This
use of WOZ data allows data-driven development of optimal strategies for domains where no
working prototype is available. Using simulation-based RL allows us to find optimal policies
which are not (necessarily) present in the original data. Our results show that simulation-based
RL significantly outperforms the average (human wizard) strategy as learned from the data by
using Supervised Learning. The bootstrapped RL-based policy gains on average 50 times more
reward when tested in simulation, and almost 18 times more reward when interacting with real
users. Users also subjectively rate the RL-based policy on average 10% higher. We also show that
results from simulated interaction do transfer to interaction with real users, and we explicitly
evaluate the stability of the data-driven reward function.
1. Introduction
Statistical learning approaches, such as Reinforcement Learning (RL), for Spoken Dia-
logue Systems offer several potential advantages over the standard rule-based hand-
coding approach to dialogue systems development: a data-driven development cycle,
? Centre for Environmental Change and Sustainability, School of GeoSciences, Drummond Street,
Edinburgh EH89XP, UK. E-mail: verena.rieser@ed.ac.uk.
?? The Interaction Lab, School of Mathematical and Computer Sciences (MACS), Heriot-Watt University,
Edinburgh EH14 4AS, UK. E-mail: o.lemon@hw.ac.uk.
Submission received: 23 January 2009; revised submission received: 13 August 2010; accepted for publication:
13 September 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
provably optimal action policies, a precise mathematical model for action selection, pos-
sibilities for generalization to unseen states, and automatic optimization of competing
trade-offs in the objective function. See Young (2000), Lemon and Pietquin (2007), and
Frampton and Lemon (2009) for an introduction to dialogue strategy learning.
One of the major limitations of this approach is that it relies on a large quantity of
data being available. In cases when a fixed data set is used for learning (e.g., Walker
2000; Singh et al 2002; Henderson, Lemon, and Georgila 2008), the optimal policy can
only be discovered when it is present in the data set. (Note, by a policy being ?present
in a data set? we mean that the set of state-action mappings which define the policy
is contained in that data set. When a policy is not present in a data set, either some
states covered by the policy are not seen at all in that data, or the actions chosen by the
policy in some states are different to those seen in the data.) To overcome this problem,
simulated learning environments are being used to explore optimal policies which were
previously unseen in the data (e.g., Eckert, Levin, and Pieraccini 1997; Ai, Tetreault,
and Litman 2007; Young et al 2009). However, several aspects of the components of
this simulated environment are usually hand-crafted, and thus limit the scope of policy
learning. In particular, the optimization (or reward) function is often manually set (Paek
2006). In order to build simulation components from real data, annotated in-domain
dialogue corpora have to be available which explore a range of dialogue management
decisions. Collecting dialogue data without a working prototype is problematic, leaving
the developer with a classic ?chicken-or-egg? problem.
We therefore propose to learn dialogue strategies using simulation-based RL, where
the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data.
In a WOZ experiment, a hidden human operator, the so-called ?wizard,? simulates
(partly or completely) the behavior of the application, while subjects are left in the belief
that they are interacting with a real system (Fraser and Gilbert 1991).
In contrast to preceding work, our approach enables strategy learning in domains
where no prior system is available. Optimized learned strategies are then available
from the first moment of on-line operation, and handcrafting of dialogue strategies
is avoided. This independence from large amounts of in-domain dialogue data allows
researchers to apply RL to new application areas beyond the scope of existing dialogue
systems. We call this method ?bootstrapping.?
In addition, our work is the first using a data-driven simulated environment. Previ-
ous approaches to simulation-based dialogue strategy learning usually handcraft some
of their components.
Of course, some human effort is needed in developing the WOZ environment and
annotating the collected data, although automatic dialogue annotation could be ap-
plied (Georgila et al 2009). The alternative?collecting data using hand-coded dialogue
strategies?would still require annotation of the user actions, and has the disadvantage
of constraining the system policies explored in the collected data. Therefore, WOZ data
allows exploration of a range of possible strategies, as intuitively generated by the
wizards, in contrast to using an initial system which can only explore a pre-defined
range of options.
However, WOZ experiments usually only produce a limited amount of data, and
the optimal policy is not likely to be present in the original small data set. Our method
shows how to use these data to build a simulated environment in which optimal
policies can be discovered. We show this advantage by comparing RL-based strategy
against a supervised strategy which captures average human wizard performance on
the dialogue task. This comparison allows us to measure relative improvement over the
training data.
154
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
The use of WOZ data has earlier been proposed in the context of RL. Williams and
Young (2004) use WOZ data to discover the state and action space for the design of
a Markov Decision Process (MDP). Prommer, Holzapfel, and Waibel (2006) use WOZ
data to build a simulated user and noise model for simulation-based RL. Although
both studies show promising first results, their simulated environments still contain
many hand-crafted aspects, which makes it hard to evaluate whether the success of the
learned strategy indeed originates from the WOZ data. Schatzmann et al (2007) propose
to ?bootstrap? with a simulated user which is entirely hand-crafted. In the following we
propose what is currently the most strongly data-driven approach to these problems. We
also show that the resulting policy performs well for real users. In particular we propose
a five-step procedure (see Figure 1):
1. We start by collecting data in a WOZ experiment, as described in Section 2.
2. From these data we train and test different components of our simulated
environment using Supervised Learning techniques (Section 3). In
Figure 1
Data-driven methodology for simulation-based dialogue strategy learning for new applications.
155
Computational Linguistics Volume 37, Number 1
particular, we extract a supervised policy, reflecting human (wizard)
performance on this task (see Section 3.3). We build a noise simulation
(Section 3.4), and two different user simulations (Section 3.5), as well as a
data-driven reward function (Section 3.6).
3. We then train and evaluate dialogue policies by interacting with the
simulated environment (Section 4).
4. Once the learned policies are ?good enough? in simulation, we test them
with real users (Section 5).
5. In addition, we introduce a final phase where we meta-evaluate the whole
framework (Section 6). This final step is necessary because WOZ
experiments only simulate human?computer interaction (HCI). We
therefore need to show that a strategy bootstrapped from WOZ data
indeed transfers to real HCI. We first show that the results between
simulated and real interaction are compatible (Section 6.1). We also
meta-evaluate the reward function, showing that it is a stable, accurate
estimate for real user?s preferences (Section 6.2).
Note that RL is fundamentally different to Supervised Learning (SL): RL is a statisti-
cal planning approach which allows us to find an optimal policy (sequences of actions)
with respect to an overall goal (Sutton and Barto 1998); SL, in contrast, is concerned with
deducing a function from training data for predicting/classifying events. This article
is not concerned with showing differences between SL and RL on a small amount of
data, but we use SL methods to capture the average human wizard strategy in the
original data, and show that simulation-based RL is able to find new policies that were
previously unseen.
We apply this framework to optimize multimodal information-seeking dialogue
strategies for an in-car digital music player. Dialogue Management and multimodal
output generation are two closely interrelated problems for information seeking dia-
logues: the decision of when to present information depends on how many pieces of
information to present and the available options for how to present them, and vice versa.
We therefore formulate the problem as a hierarchy of joint learning decisions which are
optimized together. We see this as a first step towards an integrated statistical model of
Dialogue Management and more advanced output planning/Natural Language Gener-
ation (Lemon 2008; Rieser and Lemon 2009b; Lemon 2011; Rieser, Lemon, and Liu 2010;
Janarthanam and Lemon 2010).
In the following, Section 2 describes the Wizard-of-Oz data collection (i.e., how
to collect appropriate data when no initial data or system exists), Section 3 explains
the construction of the simulated learning environment (including how to determine a
data-driven reward function), Section 4 presents training and evaluation of the learned
policies in simulation (i.e., how to learn effective dialogue strategies), Section 5 presents
the results of the tests with real users, and Section 6 presents a meta-evaluation of the
framework, including transfer results.
2. Wizard-of-Oz Data Collection
The corpus used for learning was collected in a multimodal study of German task-
oriented dialogues for an in-car music player application. The corpus was created
156
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
in the larger context of the TALK project1 and is also known as the SAMMIE corpus
(Kruijff-Korbayova? et al 2006). In contrast to conventional WOZ trials we were not only
interested in the users? behavior, but also in the behavior of our human wizards. This
study provides insights into natural strategies of information presentation as performed
by human wizards.
2.1 Experimental Setup
Six people played the role of an intelligent interface (the ?wizards?). The wizards
were able to speak freely and display search results on the screen by clicking on pre-
computed templates. Wizards? outputs were not restricted, in order to explore the
different ways they intuitively chose to present search results. Wizard?s utterances were
immediately transcribed and played back to the user with Text-To-Speech. Twenty-one
subjects (11 women, 10 men) were given a set of predefined tasks to perform, as well as
a primary driving task, using a driving simulator (Mattes 2003). The users were able to
speak, as well as make selections on the screen.
The experiment proceeded as follows. First the wizards were trained to use the
database interface and they were also given general instructions about how to interact
with the user. Training took 45 minutes, including five example tasks.
After the user arrived s/he was introduced to the driving simulator and had to
perform a short test drive. The users solved two sets of tasks with two tasks in each.
After each task the user filled out a task-specific questionnaire, in which they indicated
perceived task success and satisfaction on a five-point Likert scale. Finally, the user was
interviewed by the experiment leader following a questionnaire containing questions
similar to the PARADISE study (Walker, Kamm, and Litman 2000), including questions
on task ease, timing, multimodal and verbal presentation, as well as future use of such
systems. All subjects reported that they were convinced that they were interacting with
a real system.
To approximate speech recognition errors we used a tool that randomly deletes
parts of the transcribed utterances. Due to the fact that humans are able to make sense
of even heavily corrupted input, this method not only covers non-understandings, but
wizards also built up their own hypotheses about what the user really said, which can
lead to misunderstandings. The word deletion rate varied: 20% of the utterances were
weakly corrupted (= deletion rate of 20%), and 20% were strongly corrupted (= deletion
rate of 50%). In 60% of the cases the wizard saw the transcribed speech uncorrupted.
Example (1) illustrates the kind of corrupted utterances the wizard had to deal with.
(1) uncorrupted: ?Zu dieser Liste bitte Track ?Tonight? hinzufu?gen.?
[ ?Add track ?Tonight? to this list.?]
weakly corrupted: ?Zu dieser Liste bitte Track ?Tonight? . . . . ?
[?. . . track ?Tonight? to this list.?]
strongly corrupted: ?Zu . . . Track ?Tonight? . . . .
[?. . . track ?Tonight? to . . . ?]
There are some shortcomings of this technique, which we discuss in Rieser and
Lemon (2009a). However, the data are useful for our purposes because our main interest
1 TALK (Talk and Look: Tools for Ambient Linguistic Knowledge; www.talk-project.org) was funded by
the EU as project no. IST-507802 within the 6th Framework program.
157
Computational Linguistics Volume 37, Number 1
here is in multimodal presentation strategies (in the presence of some input noise).
Other studies have specifically targeted the Dialogue Management question of how to
handle ASR input noise (e.g., Stuttle, Williams, and Young 2004; Skantze 2005).
2.2 Data Collected
The corpus gathered with this set-up comprises 21 sessions and over 1,600 turns. Some
example dialogues can be found in Appendix B. Example (2) shows a typical multi-
modal presentation sub-dialogue from the corpus (translated from German). Note that
the wizard displays quite a long list of possible candidates on an (average sized) com-
puter screen, while the user is driving. This example illustrates that even for humans it
is difficult to find an ?optimal? solution to the problem we are trying to solve.
(2) User: ?Please search for music by Bjo?rk.?
Wizard: ?I found 43 items. The items are displayed on the screen.?
[displays list]
User: ?Please select Human Behaviour.?
Information was logged for each session, for example, the transcriptions of the
spoken utterances, the wizard?s database query and the number of results, and the
screen option chosen by the wizard. A rich set of contextual dialogue features was
also annotated, as listed in Section 3.1. Also see Rieser, Kruijff-Korbayova?, and Lemon
(2005).
Of the 793 wizard turns 22.3% were annotated as presentation strategies, re-
sulting in 177 instances for learning, where the six wizards contributed about equal
proportions.
A ?2 test on presentation strategy (comparing whether wizards chose to present
in multimodal or verbal modality) showed significant differences between wizards
(?2(1) = 34.21, p < .001). On the other hand, a Kruskal-Wallis test comparing user
preferences for the multimodal output showed no significant difference across wizards
(H(5)=10.94, p > .05).2 Mean performance ratings for the wizards? multimodal behavior
ranged from 1.67 to 3.5 on a five-point Likert scale. We also performed an analysis of
whether wizards improved their performance over time (learning effects). The results
show that the wizard?s average user satisfaction scores in general slightly decreased
with the number of sessions that they performed, however.
Observing significantly different strategies that are not significantly different in
terms of user satisfaction, we conjecture that the wizards converged on strategies which
were appropriate in certain contexts. To strengthen this hypothesis we split the data by
wizard and performed a Kruskal-Wallis test on multimodal behavior per session. Only
the two wizards with the lowest performance score showed no significant variation
across session, whereas the wizards with the highest scores showed the most varying
behavior. These results again indicate a context-dependent strategy.
In Section 3.1 we test this hypothesis (that good multimodal clarification strate-
gies are context-dependent) by using feature selection techniques in order to find the
2 The Kruskal-Wallis test is the non-parametric equivalent to a one-way ANOVA. Because the users
indicated their satisfaction on a five-point likert scale, an ANOVA which assumes normality would be
invalid.
158
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
features which are most predictive for the wizards? behavior. The dialogues show that
common ?mistakes? were that the wizards either displayed too much information on
the screen, see Example (1) in Appendix B, or the wizards fail to present results early
enough, see Example (2) in Appendix B. In general, users report that they get distracted
from driving if too much information is presented. On the other hand, users prefer
shorter dialogues (most of the user ratings are negatively correlated with dialogue
length).
These results indicate that we need to find a strategy given the competing trade-
offs between the number of results (large lists are difficult for users to process), the
length of the dialogue (long dialogues are tiring, but collecting more information can
result in more precise results), and the noise in the speech recognition environment
(in high noise conditions accurate information is difficult to obtain). In the following
we utilize the ratings from the user questionnaires to optimize a presentation strategy
using simulation-based RL.
3. Simulated Learning Environment
Simulation-based RL learns by interaction with a simulated environment (Sutton and
Barto 1998). We obtain the simulated components from the WOZ corpus using data-
driven methods. Although this requires quite a large effort, the exercise is important as
a case study for exploring the proposed methodology.
The employed database contains 438 items and is similar in retrieval ambiguity and
structure to the one used in the WOZ experiment. The dialogue system used for learning
implements a multimodal information presentation strategy which is untrained, but
comprises some obvious constraints reflecting the system logic (e.g., that only filled
slots can be confirmed), implemented as Information State Update (ISU) rules (see also
Heeman 2007; Henderson, Lemon, and Georgila 2008).
Other behavior which is hand-coded in the system is to greet the user in the
beginning of a dialogue and to provide help if the user requests help. The help function
provides the user with some examples of what to say next (see system prompt s6 in the
Example Dialogue in Table 1 in Appendix D). All other actions are left for optimization.
3.1 Feature Space
A state or context in our system is a dialogue ?information state? as defined in (Lemon
et al, 2005). We divide the types of information represented in the dialogue information
state into local features (constituting low-level and dialogue features), dialogue history
features, and user model features. We also defined features reflecting the application
environment (e.g., driving). The information state features are shown in Tables 1, 2,
and 3, and further described below. All features are automatically extracted from the
WOZ log-files (as described in Section 2.2), and are available at runtime in ISU-based
dialogue systems.
Local features. First, we extracted features present in the ?local? context of a wizard
action, as shown in Table 1, such as the number of matches returned from the database
query (DB), whether any words were deleted by the corruption algorithm (see Sec-
tion 2.1), and the previous user speech act (user-act) of the antecedent utterance. The
159
Computational Linguistics Volume 37, Number 1
Table 1
Contextual/information-state features: Local features.
Local features
DB: database matches (integer)
deletion: words deleted (yes/no)
user-act: add, repeat, y/n, change, others
user actions are annotated manually by two annotators (? = .632). Please see Table 1 in
Appendix A for detailed results on inter-annotator agreement.
Also, note that the deletion feature (and later delHist, and delUser) counts the
number of words deleted by the corruption tool (see Section 2.1) and serves as an
approximation to automatic speech recognition (ASR) confidence scores as observed by
the system. Equally, the human wizard will be able to infer when words in a sentence
were deleted and hence has a certain confidence that the input is complete.
Dialogue history features. The history features account for events in the whole dialogue
so far, that is, all information gathered before entering the presentation phase, as
shown in Table 2. We include features such as the number of questions that the wizard
asked so far (questHist), how often the screen output was already used (screenHist),
the average corruption rate so far (delHist), the dialogue length measured in turns
(dialogueLength), the dialogue duration in seconds (dialogueDuration), and whether
the user reacted to the screen output, either by verbally referencing (refHist), for
example, using expressions such as It?s item number 4, or by clicking (clickHist).
User model features. Under ?user model features? we consider features reflecting the wiz-
ards? responsiveness to the behavior and situation of the user. Each session comprises
four dialogues with one wizard. The user model features average the user?s behavior in
these dialogues so far, as shown in Table 3, such as how responsive the user is towards
the screen output, namely, how often this user clicks (clickUser) and how frequently
s/he used verbal references so far (refUser); how often the wizard had already shown a
screen output (screenUser) and how many questions were already asked (questUser);
how much the user?s speech was corrupted on average so far (delUser), that is, an
approximation of how well this user is recognized; and whether this user is currently
driving or not (driving). This information was available to the wizards.
Table 2
Contextual/information-state features: History features.
Dialogue History Features
questHist: number of questions (integer)
screenHist: number screen outputs (integer)
delHist: average corruption rate; no.wordsDeletedInDialogueSoFarno.utterancesInDialogueSoFar (real)
dialogueLength: length in turns (integer)
dialogueDuration: time in sec (real)
refHist: number of verbal user references to screen output (integer)
clickHist: number of click events (integer)
160
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 3
Contextual/information-state features: User model features.
User model features
clickUser: average number of clicks (real)
refUser: average number of verbal references (real)
delUser: average corruption rate for that user; no.wordsDeletedForUserSoFarno.utterancesForUserSoFar (real)
screenUser: average number of screens shown to that user (real)
questUser: average number of questions asked to user (real)
driving: user driving (yes/no)
Note that all these features are generic over information-seeking dialogues where
database results can be displayed on a screen; except for driving which only applies
to hands-and-eyes-busy situations. This potential feature space comprises 16 features,
many of them taking numerical attributes as values. Including them all in the state space
for learning would make the RL problem unnecessarily complex. In the next section we
describe automatic feature selection techniques, which help to reduce the feature space
to a subset which is most predictive of when and how to present search results.
3.1.1 Feature Selection. We use feature selection techniques to identify the context features
which are most predictable for the wizards choosing a specific action. We choose to
apply forward selection for all our experiments in order to not include redundant
features, given our large feature set. We use the following feature filtering methods:
correlation-based subset evaluation (CFS; Hall 2000) and a decision tree algorithm (rule-
based SL). We also apply a correlation-based ?2 ranking technique. Filtering techniques
account for inter-feature relations, selecting subsets of predictive features at the expense
of saying less about individual feature performance itself. Ranking techniques evaluate
each feature individually. For our experiments we use implementations of selection
techniques provided by the WEKA toolkit (Witten and Frank 2005).
First, we investigated the wizards? information acquisition strategy, namely, which
features are related to the wizards? decision when to present a list (presentInfo)?that
is, the task is to predict presentInfo vs. all other possible dialogue acts. None of the
feature selection techniques were able to identify any predictive feature for this task.
Next, we investigated the wizards? information presentation strategy, that is, which
features are related to the wizards? decision to present a list verbally (presentInfo-
verbal) or multi-modally (presentInfo-multimodal). All the feature selection tech-
niques consistently choose the feature DB (number of retrieved items from the database).
This result is maybe not very surprising, but it supports the claim that using feature
selection on WOZ data delivers valid results. Relevant features for other domains may
be less obvious. For example, Levin and Passonneau (2006) suggest the use of WOZ
data in order to discover the state space for error recovery strategies. For this task many
other contextual features may come into play, as shown by Gabsdil and Lemon (2004)
and Lemon and Konstas (2009) for automatic ASR re-ranking.
We use this information to construct the state space for RL, as described in the
following section, as well as using these feature selection methods to construct the
wizard strategy as described in Section 3.3.
161
Computational Linguistics Volume 37, Number 1
Figure 2
State-action space for hierarchical Reinforcement Learning.
3.2 MDP and Problem Representation
The structure of an information-seeking dialogue system consists of an information
acquisition phase, and an information presentation phase. For information acquisition
the task of the dialogue manager is to gather ?enough? search constraints from the user,
and then, ?at the right time,? to start the information presentation phase, where the pre-
sentation task is to present ?the right amount? of information in the right way?either
on the screen or listing the items verbally. What ?the right amount? actually means
depends on the application, the dialogue context, and the preferences of users. For
optimizing dialogue strategies information acquisition and presentation are two closely
interrelated problems and need to be optimized jointly: When to present information
depends on the available options for how to present them, and vice versa.
We therefore formulate the problem as an MDP, relating states to actions in a hierar-
chical manner (see Figure 2): Four actions are available for the information acquisition
phase; once the action presentInfo is chosen, the information presentation phase is
entered, where two different actions for output realization are available.
The state space is constructed semi-automatically. We manually enumerate the task-
related features needed to learn about the dialogue task. For example, we manually
specified the number of slots, and information about the ?grounded-ness? of the slots,
needed to learn confirmation strategies.3 We also added the features which were auto-
matically discovered by the feature selection techniques defined in Section 3.1.1.
The state-space comprises eight binary features representing the task for a four-
slot problem: filledSlot indicates whether a slot is filled, confirmedSlot indicates
whether a slot is confirmed. We also add the number of retrieved items (DB). We found
that human wizards especially pay attention to this feature, using the feature selection
techniques of Rieser and Lemon (2006b). The feature DB takes integer values between
1 and 438, resulting in 28 ? 438 = 112, 128 distinct dialogue states for the state space.
In total there are 4112,128 theoretically possible policies for information acquisition.4 For
3 Note that we simplified the notion of a slot being grounded as a binary feature, following Henderson,
Lemon, and Georgila (2008). More recent work uses more fine-grained notions of confidence in
user-provided information (e.g., Roque and Traum 2008), or the notion of ?belief states? in Partially
Observable Markov Decision Processes (e.g., Williams and Young 2007). This does lead to new policies in
information acquisition, but is not the focus of this article.
4 In practice, the policy space is smaller, as some combinations are not possible (e.g., a slot cannot be
confirmed before being filled). Furthermore, some incoherent action choices are excluded by the basic
system logic.
162
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
the presentation phase the DB feature is discretized, as we will further discuss in Section
3.7. For the information presentation phase there are 22
3
= 256 theoretically possible
policies.
3.3 Wizard Behavior
Our hypothesis is that simulation-based RL allows us to find optimal policies which are
superior to those present in the original data. Therefore we create a policy which mimics
the average wizard behavior, and this allows us to measure the relative improvements
over the training data (cf. Henderson, Lemon, and Georgila 2008). We create this base-
line by applying SL. For these experiments we use the WEKA toolkit (Witten and Frank
2005). We learn with the decision tree J4.8 classifier, WEKA?s implementation of the C4.5
system (Quinlan 1993), and rule induction JRIP, the WEKA implementation of RIPPER
(Cohen 1995). In particular, we learn models which predict the following wizard actions:
 Presentation timing: when the ?average? wizard starts the presentation
phase on a turn level (binary decision).
 Presentation modality: in which modality the list is presented (multimodal
vs. verbal).
We use annotated dialogue context features as input, as described in Section 3.1,
with feature selection techniques as described in Section 3.1.1. Both models are trained
using 10-fold cross validation, comparing the predicted labels against the true labels in
a hold-out test set. Table 4 presents the results for comparing the accuracy of the learned
classifiers against the majority baseline.
A data analysis shows that all of the wizards are more likely to show a graphic
on the screen when the number of database hits is ? 4. However, none of the wizards
strictly follows that strategy.
For presentation timing, none of the classifiers produces significantly improved
results. Hence, we conclude that there is no distinctive pattern observable by the SL
algorithms for when to present information. For strategy implementation we therefore
use a frequency-based approach following the distribution in the WOZ data: In 48% of
cases the baseline policy decides to present the retrieved items; for the rest of the time
the system follows a hand-coded strategy.
For learning presentation modality, both classifiers significantly outperform the ma-
jority baseline. The learned models both learn the same rule set, which can be rewritten
as in Listing 1. Note that this rather simple algorithm is meant to represent the average
strategy as learned by SL from the initial data (which then allows us to measure the
relative improvements of the RL-based strategy).
Table 4
Predicted accuracy for presentation timing and modality (with standard deviation ?).
majority baseline JRip J48
timing 52.0(? 2.2) 50.2(? 9.7) 53.5(? 11.7)
modality 51.0(? 7.0) 93.5(?11.5)* 94.6(? 10.0)*
* Statistically significant improvement at p < .05.
163
Computational Linguistics Volume 37, Number 1
Listing 1
Supervised Strategy
i f ( db<4){
return present InfoVerba l ; }
else {
return presentInfoMM ;}
3.4 Noise Simulation
One of the fundamental characteristics of HCI is an error-prone communication chan-
nel. Therefore, the simulation of channel noise is an important aspect of the learn-
ing environment. Previous work uses data-intensive simulations of ASR errors (e.g.,
Pietquin and Dutoit 2006; Schatzmann, Thomson, and Young 2007a). Because we only
have limited data available, we use a simple model simulating the effects of non- and
misunderstanding on the interaction, rather than the noise itself. This method is espe-
cially suited to learning from small data sets. From our data we estimate a 30% chance
of user utterances to be misunderstood, and 4% to be complete non-understandings,
which is a realistic estimation for deployed dialogue systems (cf. Litman and Pan 1999;
Carpenter et al 2001; Hirschberg, Litman, and Swerts 2001; Georgila, Henderson, and
Lemon 2005).
We simulate the effects that noise has on the user behavior, as well as for the task
accuracy.5 For the user side, the noise model defines the likelihood of the user accepting
or rejecting the system?s hypothesis (e.g., when the system utters a confirmation), that
is, in 30% of the cases the user rejects, in 70% the user agrees. These probabilities are
combined with the probabilities for user actions from the user simulation, as described
in the next section. For non-understandings we have the user simulation generating
Out-of-Vocabulary utterances with a chance of 4%. Furthermore, the noise model deter-
mines the likelihood of task accuracy as calculated in the reward function for learning.
A filled slot which is not confirmed by the user has a 30% chance of having been mis-
recognized, see Task Completion as defined in Section 3.6.
3.5 User Simulation
A user simulation is a predictive model of real user behavior used for automatic di-
alogue strategy development and testing. See Schatzmann et al (2006) for a compre-
hensive survey. Simulations on the intention/dialogue act level are most popular for
RL-based strategy learning, as they outperform the lower level approaches in terms of
robustness, portability, and scalability. For our domain, the user can either add new
information (add), repeat or paraphrase information which was already provided at an
earlier stage (repeat), give a simple yes/no answer (y/n), or change to a different topic
by providing a different slot value than the one asked for (change). Examples from the
corpus are given in Table 5 and in the dialogues listed in Appendix B. These actions are
annotated manually by two annotators (? = .632, see Appendix A).
5 Simulating the effects of noise, rather than the noise itself, is sufficient to learn presentation strategies in
the presence of noise (e.g., whether a slot has to be confirmed before a result can be presented). Note that
other work has focused on learning dialogue strategies under different noise conditions (e.g., Bohus et al
2006; Williams and Young 2007).
164
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 5
User action types and frequencies as annotated in the data.
# action type freq % example (original) translation
1 add 54 30.5 a?h, Ella Fitzgerald. er, Ella Fitzgerald.
3 repeat 57 32.2 ja, Smile ja. yes, Smile yes.
2 y/n 14 7.9 ja, in Ordnung. yes, that?s OK.
4 change 17 9.6 dann machen wir was anderes
und zwar ha?tte ich gern eine
Playlist mit drei Liedern.
Let?s try something else then. I
would like a playlist with three
songs.
others 35 19.8 ? no answer, comment, aside
In this work, we are challenged to learn user simulations from a small data set. We
first construct a simple bigram model in order to explore the quality of the data. Bigram
(or more general n-gram) models for user simulations were first introduced by Eckert,
Levin, and Pieraccini (1997, 1998). An n-gram?based user simulation predicts the user
action ?au,t at time t that is most probable given the dialogue history of system and user
actions, see Equation (1) where as,t denotes the system action at time t.
?au,t = argmaxau,tP(au,t|as,t, as,t?1, au,t?1, ..., au,t?n+1, as,t?n+1) (1)
The bigram model obtained from our WOZ data and the observed frequencies are
shown in Figure 3. When examining the distributions of user replies per system turn for
the bigram model, we can see that 25% of the state?action pairs have zero frequencies.
However, user simulations should allow the learner to also find strategies which are
not in the data. Especially when learning from small data sets, user simulations for
Figure 3
User action frequencies following a system act (bigram model): 25% zero frequencies for
state-action pairs due to data sparsity.
165
Computational Linguistics Volume 37, Number 1
automatic strategy training should cover the whole variety of possible user action for
each state in order to produce robust strategies. Ai, Tetreault, and Litman (2007), for
example, show that random models outperform more accurate ones if the latter fail to
provide enough coverage. On the other hand, user simulations used for testing should
be more accurate with respect to the data in order to test under realistic conditions (e.g.,
Mo?ller et al 2006).
We therefore apply two learning methods to deal with data sparsity (for n-gram
models): First, we develop a user simulation which is based on a new clustering tech-
nique; second, we apply smoothing (which is the standard technique applied to account
for zero frequencies in n-gram models).
3.5.1 Cluster-Based User Simulation. We introduce a cluster-based technique for building
user simulations from small amounts of data (see also Rieser and Lemon 2006a). A sim-
ilar approach has later been suggested by Schatzmann, Thomson, and Young (2007b),
called the ?summary-space mapping technique,? where similar states are summarized,
and a distribution of possible user behavior is assigned to a set of states, which we call
?clusters.? This method allows one to generate the full range of possible user behavior
in every state.
Cluster-based user simulations generate explorative user behavior which is similar
but not identical to user behavior observed in the original data. In contrast to the bigram
model, where the likelihood of the next user act is conditioned on the previous system
action, the likelihood for the cluster-based model is conditioned on a cluster of similar
system states (see Equation (2)).
?au,t ? argmaxau,tP(au,t|clusters,t?1) (2)
The underlying idea is that, with sparse training data, we want user simulations to be
?similar to real users in similar situation.? This user simulation should generate any
kind of observed user behavior in a context (as opposed to the zero frequencies for
sparse data), while still generating behavior which is pragmatically plausible in this sit-
uation. That is, we want our user simulation to generate behavior which is complete and
consistent with respect to the observed actions in the data. We also want our model to
generate actions which show some variability with respect to the observed behavior, that
is, a controlled degree of randomness. This variance will help us to explore situations
which are not observed in the data, which is especially valuable when building a model
from sparse training data (cf. Ai, Tetreault, and Litman 2007).
Clustering is applied in order to identify more general situations than the previ-
ously annotated system speech acts by grouping them according to their similarity. For
building such clusters we apply the Expectation-Maximization (EM) algorithm. The EM
algorithm is an incremental approach to clustering (Dempster, Laird, and Rubin 1977),
which fits parameters of Gaussian density distributions to the data. In order to define
similarity between system actions, we need to describe their (semantic) properties.
We therefore annotate the system acts using a fine-grained scheme by Rodriguez and
Schlangen (2004) and Rieser and Moore (2005), which allows classification of dialogue
acts in terms of different forms and functions.
We use a slightly modified version of the scheme, where we only use a subset of the
suggested annotation tags, while adding another level describing the output modality,
as summarized in Figure 4. In particular, the annotation scheme describes wizard
actions in terms of their communication level, which describes the linguistic target
after Clark (1996). We distinguish between utterances which aim to elicit acoustic
166
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Figure 4
Annotation scheme of discourse functions for wizard?s actions.
information (e.g., Sorry, can you please repeat? and utterances which aim to elicit fur-
ther information to uniquely identify the user?s reference (e.g., By which artist?). As
well as utterances trying to establish contact (e.g., Can you hear me?), and utterances
about the user?s intention (e.g., What do you want me to do?). The problem severity
describes which type of feedback the system requests from the user, namely, asking for
confirmation, for repetition, or for elaboration. The modality of the dialogue act
can either be verbal or multimodal.
Table 6 shows a mapping between system speech acts as described in Figure 2 and
the annotated discourse functions. We use these features for clustering the speech acts
according to their similarity in discourse function and form.
The EM algorithm generates three state clusters: The system acts askAQuestion and
implConfirm are summarized into cluster 1; explConf and reject are in cluster 2; and
presentListVerbal and presentListMM are in cluster 3. For every cluster we assign the
observed frequencies of user actions (i.e., all the user actions which occur with one of
the states belonging to that cluster), as shown in Figure 5.
3.5.2 Smoothed Bigram User Simulation. For our second user simulation model we apply
smoothing to a bigram model. We implement a simple smoothing technique called
?add-one smoothing? (Jurafsky and Martin 2000). This technique discounts some non-
zero counts in order to obtain probability mass that will be assigned to the zero counts.
We apply this technique to the original frequency-based bigram model. The resulting
model is shown in Figure 6.
In general, the smoothed model is closer to the original data than the cluster-based
one (thus being more realistic at the expense of allowing less exploratory behavior). In
the next section we introduce an evaluation metric which allows us to assess the level of
exploratory versus realistic user behavior as exhibited by the different user simulations.
Table 6
System speech acts and corresponding discourse functions.
speech act level severity modality
reject acoustic repetition verbal
explicitConfirm acoustic confirmation verbal
askAQuestion goal elaboration verbal
implicitConfirm goal confirmation+elaboration verbal
presentVerbal goal confirmation verbal
presentMM goal confirmation multimodal
167
Computational Linguistics Volume 37, Number 1
Figure 5
User action frequencies from the cluster-based user simulation.
Figure 6
User action frequencies from the smoothed bigram user simulation.
3.5.3 Evaluation of User Simulations. Several metrics have been proposed to evaluate
user simulations (e.g., Scheffler and Young 2001; Schatzmann, Georgila, and Young
2005; Ai and Litman 2006; Georgila, Henderson, and Lemon 2006; Williams 2007).
A good measure of dialogue similarity is based on the Kullback?Leibler (KL) diver-
168
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 7
Kullback?Leibler divergence scores for the different user simulations.
User simulations Baselines
smoothed cluster random majority
0.087 0.095 0.43 0.48
gence6 (as also used by Cuaya?huitl et al 2005; Jung et al 2009), which is defined as
follows:
DKL(P||Q) =
M
?
i=1
P(i) ? log P(i)
Q(i)
(3)
This metric measures the divergence between distributions P and Q in a context
with M responses. Ideally, the KL divergence between two similar distributions is close
to zero.
KL allows us to compare the raw probabilities as observed in the original data with
the probabilities generated by our user simulation models. We then compare the KL
results of the cluster-based and the smoothed user simulation against a random model
and a majority baseline (see Table 7). The random model is constructed by assigning
equal frequency to all four actions, whereas the majority baseline always predicts the
most frequent action in one context. The comparison against the random baseline tests
the hypothesis that our user simulations are more consistent with the observed data
than random behavior. The majority baseline represents the hypothesis that our user
simulation explores a significantly wider range of behavior than the most frequent user
action.
The user simulation models have a small divergence from the original data suggest-
ing that they are good simulations for training and testing policies. The smoothed and
the cluster-based model gain on average five times lower KL scores than the baselines.
We therefore conclude that both simulations show consistent (i.e., better than random)
as well as varying (i.e., better than the majority decision) behavior.
As mentioned previously, we want user simulations for policy training to allow
more exploration, whereas for testing we want user simulations which are more realis-
tic. We therefore choose to test with the smoothed model because its low KL score shows
that it is closest to the data, and we use the cluster-based simulation for training.
Note that the KL divergence only measures consistency with respect to specific
dialogue contexts. However, user simulations also need to be coherent with respect
to the dialogue history and the current user goal. We therefore model the user?s goal
(i.e., the song s/he is looking for) similar to ?agenda-based user models?(Schatzmann
et al 2007; Schatzmann, Thomson, and Young 2007b). The user goal corresponds to
a database entry, which is randomly chosen in the beginning of each dialogue. Every
time the user simulation generates a speech act, the corresponding value is chosen from
the goal record, dependent on the slot value the system was asking for.
6 Also known as information divergence, information gain, or relative entropy.
169
Computational Linguistics Volume 37, Number 1
3.6 Data-Driven Reward Modeling
The reward function defines the goal of the overall dialogue. For example, if it is most
important for the dialogue to be efficient, the function penalizes dialogue length, while
rewarding task success. In most previous work the reward function is manually set,
which makes it ?the most hand-crafted aspect? of RL (Paek 2006). For example, Williams
and Young (2007) use +10 points for task completion and ?1 point per turn, but there
is no empirical basis for this specific ratio. In contrast, we learn the reward model
from data, using a modified version of the PARADISE framework (Walker, Kamm, and
Litman 2000), following pioneering work by Walker, Fromer, and Narayanan (1998). In
PARADISE multiple linear regression is used to build a predictive model of subjective
user ratings (from questionnaires) from objective dialogue performance measures (such
as dialogue length). The subjective measure that we wish to optimize for our application
is Task Ease, a variable obtained by taking the average of two questions in the question-
naire.7 We use PARADISE to predict Task Ease from various input variables, via stepwise
regression. The chosen model comprises dialogue length in turns, task completion (as
manually annotated in the WOZ data), and the multimodal user score from the user
questionnaire, as shown in Equation (4) (R2 = .144,R2adjusted = .123).
TaskEase = ? 20.2 ? dialogueLength +
11.8 ? taskCompletion + 8.7 ? multimodalScore; (4)
This equation is used to calculate the overall reward for the information acquisition
phase. Task completion is calculated on-line during learning, penalizing all slots which
are filled but not confirmed. Slots that are filled but not confirmed have a 30% chance
of being incorrect according to the noise model (see Section 3.4). For the information
presentation phase, we compute a local reward. We relate the multimodal score (a
variable obtained by taking the average of four questions)8 to the number of items
presented (DB) for each modality, using curve fitting. In contrast to linear regression,
curve fitting does not assume a linear inductive bias, but it selects the most likely model
(given the data points) by function interpolation. The resulting models are shown in
Figure 7. The reward for multimodal presentation is a quadratic function that assigns a
maximal score to a strategy displaying 14.8 items (curve inflection point). The reward
for verbal presentation is a linear function assigning negative scores to all presented
items ? 4. The reward functions for information presentation intersect at no. items = 3.
A comprehensive evaluation of this reward function can be found in Section 6.2.
3.7 State Space Discretization
We use linear function approximation in order to learn with large state-action spaces.
Linear function approximation learns linear estimates for expected reward values of
actions in states represented as feature vectors. This is inconsistent with the idea of non-
linear reward functions (as introduced in the previous section). We therefore quantize
the state space for information presentation. We partition the database feature into three
7 ?The task was easy to solve?, ?I had no problems finding the information I wanted.?
8 ?I liked the combination of information being displayed on the screen and presented verbally?,
?Switching between modes did not distract me?, ?The displayed lists and tables contained on average the
right amount of information?, ?The information presented verbally was easy to remember.?
170
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Figure 7
Evaluation functions relating number of items presented in different modalities to multimodal
score.
bins, taking the first intersection point between verbal and multimodal reward and the
turning point of the multimodal function as discretization boundaries. Previous work
on learning with large databases commonly quantizes the database feature in order
to learn with large state spaces using manual heuristics (e.g., Levin, Pieraccini, and
Eckert 2000; Heeman 2007). Our quantization technique is more principled as it reflects
user preferences for multi-modal output. Furthermore, in previous work database items
were not only quantized in the state-space, but also in the reward function, resulting in a
direct mapping between quantized retrieved items and discrete reward values, whereas
our reward function still operates on the continuous values. In addition, the decision of
when to present a list (information acquisition phase) is still based on continuous DB
values. In future work we plan to engineer new state features in order to learn with
non-linear rewards while the state space is still continuous. A continuous representation
of the state space allows learning of more fine-grained local trade-offs between the
parameters, as demonstrated by Rieser and Lemon (2008a).
4. Training and Testing the Learned Policies in Simulation
We now train and test the multimodal presentation strategies by interacting with the
simulated learning environment. For the following RL experiments we used the REALL-
DUDE toolkit of Lemon et al (2006). The SHARSHA algorithm is employed for train-
ing, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and
Langley 2002). The policy is trained with the cluster-based user simulation over 180k
system cycles, which results in about 20k simulated dialogues. In total, the learned strat-
egy has 371 distinct state-action pairs as presented in the look-up table in Appendix E.
We test the RL-based and the SL wizard baseline policy, as listed in Listing 1,
which allows us to measure relative improvement over the training data. We run 500
171
Computational Linguistics Volume 37, Number 1
test dialogues with the smoothed user simulation, as described in Section 3.5.2, so that
we are not training and testing on the same simulation. We then compare quantitative
dialogue measures by performing a paired t-test. In particular, we compare mean
values of the final rewards, number of filled and confirmed slots, dialogue length,
and items presented multimodally (MM items) and items presented verbally (verbal
items). RL performs significantly better (p < .001) than the baseline strategy. The only
non-significant difference is the number of items presented verbally, where both the RL
and the average wizard strategy settled on a threshold of fewer than four items. The
mean performance measures for simulation-based testing are shown in Table 8.
The major strength of the RL policy is that it learns to keep the dialogues reasonably
short (on average 5.9 system turns for RL versus 8.4 turns for SL wizard) by presenting
lists as soon as the number of retrieved items is within tolerance range for the respec-
tive modality (as reflected in the reward function). The SL strategy in contrast has
not learned the right timing nor an upper bound for displaying items on the screen
(note that the distribution for MM items is highly positively skewed with a maximum of
283 items being displayed). See example dialogues in Appendix C.
The results show that simulation-based RL with an environment bootstrapped
from WOZ data allows learning of robust strategies which significantly outperform the
strategies learned by SL from the original data set. This confirms our hypothesis that
simulation-based RL allows us to find optimal policies which are not easily discoverable
(by Supervised Learning) in the original data.
Furthermore, RL allows us to provide additional information about user prefer-
ences in the reward function, whereas SL simply mimics the data. In addition, RL
is based on delayed rewards, namely, the optimization of a final goal. For dialogue
systems we often have measures indicating how successful and/or satisfying the overall
performance of a strategy was, but it is hard to tell how exactly things should have been
done in a specific situation. This is what makes RL specifically attractive for dialogue
strategy learning. In the next section we test the learned strategy with real users.
5. Tests with Real Users
5.1 Experimental Design
For the user tests the RL policy is ported to a working ISU-based dialogue system via
table look-up (see table in Appendix E) , which indicates the action with the highest
expected reward for each state (cf. Singh et al 2002). The supervised average wizard
baseline is implemented using standard threshold-based update rules. The experimen-
Table 8
Comparison of results for SL wizard and RL-based strategies in simulation.
Measure SL wizard baseline RL Strategy
avg. turns 8.42(?3.04) 5.9(?2.4)***
avg. speech items 1.04(?.2) 1.1(?.3)
avg. MM items 61.37(?82.5) 11.2(?2.4)***
avg. reward ?1,741.3(?566.2) 44.06(?51.5)***
*** Significant difference between SL and RL at p < .001 (with standard deviation ?).
172
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 9
Comparison of mean user ratings for SL wizard baseline and RL policies (with standard
deviation ?).
Measure Wizard Strategy RL Strategy
Task Ease 4.78 (?1.84) 5.51 (?1.44)***
Timing 4.42 (?1.84) 5.36 (?1.46)***
MM Presentation 4.57 (?1.87) 5.32 (?1.62)***
Verbal Presentation 4.94 (?1.52) 5.55 (?1.38)***
Future Use 3.86 (?1.44) 4.68 (?1.39)***
*** Statistical significance at p < .001.
tal conditions are similar to the WOZ study, that is, we ask the users to solve similar
tasks, and use similar questionnaires.9 Furthermore, we decided to use typed user
input rather than ASR. The use of text input allows us to target the experiments to the
dialogue management decisions on presentation strategies, and prevents ASR quality
from interfering with the experimental results, especially because subjective user scores
are highly sensitive to ASR noise (Hajdinjak and Mihelic 2006). Both RL and SL wizard
policies are trained to handle noisy conditions, so that they usually confirm user input,
which makes dialogues longer but more reliable. The lack of noise in this experiment
means that confirmation happens more than is strictly required (although there are
still text input spelling mistakes), but the information presentation decisions are not
affected.
Seventeen subjects (8 women, 9 men) are given a set of 12 (6?2) predefined, ordered
tasks, which they solve by interaction with the RL-based and the SL-based average
wizard system in a cyclic order. As a secondary task users are asked to count certain
objects in a driving simulation. In total, 204 dialogues with 1,115 turns are gathered in
this set-up. See also Rieser (2008).
5.2 Results
In general, the users rate the RL-based policy significantly higher (p < .001) than the
SL-based average wizard policy. The results from a Wilcoxon Signed Ranks Test on
the user questionnaire data (see Table 9) show significantly improved Task Ease, better
presentation timing, more agreeable verbal and multimodal presentation, and that more
users would use the RL-based system in the future (Future Use). All the observed
differences have a medium effects size (r ? |.3|).
We also observe that female participants clearly favor the RL-based strategy,
whereas the ratings by male participants are more indifferent. Similar gender effects
are also reported by other studies on multimodal output presentation (e.g., Foster and
Oberlander 2006; Jokinen and Hurtig 2006).
Furthermore, we compare objective dialogue performance measures. The dialogues
of the RL strategy are significantly shorter (p < .005), while fewer items are dis-
played (p < .001), and the help function is used significantly less (p < .003). The mean
9 The WOZ study was performed in German, whereas the user tests are performed in English. Therefore, a
different database had to be used and task sets and user questionnaires had to be translated.
173
Computational Linguistics Volume 37, Number 1
performance measures for testing with real users are shown in Table 10. Also see
example dialogues in Appendix D. However, there is no significant difference for the
performance of the secondary driving task.
6. Meta Evaluation
We introduce a final phase where we meta-evaluate the whole framework. This final
step is necessary because WOZ experiments only simulate HCI. We therefore need to
show that a strategy bootstrapped from WOZ data indeed transfers to real HCI. We first
show that the results for simulated and real interaction are compatible (Section 6.1). We
also meta-evaluate the reward function, showing that it is a stable, accurate estimate for
real users? preferences (Section 6.2).
6.1 Transfer Between Simulated and Real Environments
We first test whether the results obtained in simulation transfer to tests with real users,
following Lemon, Georgila, and Henderson (2006). We evaluate the quality of the simu-
lated learning environment by directly comparing the dialogue performance measures
between simulated and real interaction. This comparison enables us to make claims
regarding whether a policy which is ?bootstrapped? from WOZ data is transferable to
real HCI. We first evaluate whether objective dialogue measures are transferable, using
a paired t-test, comparing overall mean performance.
For the RL policy there is no statistical difference in overall performance (reward),
dialogue length (turns), and the number of presented items (verbal and multimodal
items) between simulated and real interaction (see Figure 8). This fact (that the per-
formances are not different) indicates that the learned strategy transfers well to real
settings. For the SL wizard policy the dialogue length for real users is significantly
(t(101) = 5.5, p < .001, r = .48) shorter than in simulation. We conclude from an er-
ror analysis that this length difference is mainly due to the fact that real users tend
to provide the most ?informative? slot value (i.e., the most specific value from the
experimental task description) right at the beginning of the task (and therefore more
efficiently contribute to solve the task), whereas simulated users use a default ordering
of slot values and most of the time they provide the slot value that the system was asking
for (provide info). This difference becomes more prominent for the SL wizard policy
than for the RL-based policy, as the SL wizard policy in general asks more questions
before presenting the information. In future work the user simulation therefore should
learn optimal slot ordering.
Table 10
Comparison of results for SL average wizard and RL-based strategies with real users.
Measure Wizard Strategy RL Strategy
avg. turns 5.86(?3.2) 5.07(?2.9)***
avg. speech items 1.29(?.4) 1.2(?.4)
avg. MM items 52.2(?68.5) 8.73(?4.4)***
avg. reward ?628.2(?178.6) 37.62(?60.7)***
*** Significant difference between SL and RL at p < .001 (with standard deviation ?).
174
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Figure 8
Graph comparison of objective measures. SLs = SL policy in simulation; SLr = SL policy with
real users; RLs = RL policy in simulation; RLr = RL policy with real users.
6.2 Evaluation of the Learned Reward Function
We propose a new method for meta-evaluation of the reward (or ?objective?) function.
One major advantage of RL-based dialogue strategy development is that the dialogue
strategy can be automatically trained and evaluated using the same objective function
(Walker 2005). Despite its central aspect for RL, quality assurance for objective functions
has received little attention so far. In fact, as noted in Section 3.6, the reward function is
one of the most hand-coded aspects of RL (Paek 2006).
Here, we bring together two strands of research for evaluating the reward function:
One strand uses Reinforcement Learning to automatically optimize dialogue strategies
(e.g., Singh et al 2002; Henderson, Lemon, and Georgila 2008; Rieser and Lemon 2008a,
2008b); the other focuses on automatic evaluation of dialogue strategies (e.g., the PAR-
ADISE framework [Walker et al 1997]), and meta-evaluation of dialogue metrics (e.g.,
Engelbrecht and Mo?ler 2007; Paek 2007). Clearly, automatic optimization and evaluation
of dialogue policies, as well as quality control of the objective function, are closely inter-
related problems: How can we make sure that we optimize a system according to real
users? preferences?
In Section 3.6 we constructed a data-driven objective function using the PARADISE
framework, and used it for automatic dialogue strategy optimization, following work
by Walker, Former, and Narayanan (1998). However, it is not clear how reliable such
a predictive model is, that is, whether it indeed estimates real user preferences. The
175
Computational Linguistics Volume 37, Number 1
models obtained with PARADISE often fit the data poorly (Engelbrecht and Mo?ller 2007).
It is also not clear how general they are across different systems and user groups
(Walker, Kamm, and Litman 2000; Paek 2007). Furthermore, it is not clear how they per-
form when being used for automatic strategy optimization within the RL framework.
In the following we evaluate different aspects of the reward function. In Sec-
tion 6.2.1 we test the model stability in a test?retest comparison across different user
populations and data sets. In Section 6.2.2 we measure its prediction accuracy.
6.2.1 Reward Model Stability. We first test the reward model?s stability by re-constructing
it from the data gathered in the real user tests (see Section 5) and comparing it to the
original model constructed from the WOZ data. By replicating the regression model
on different data sets we test whether the automatic estimate of Task Ease general-
izes beyond the conditions and assumptions of a particular experimental design. The
resulting models are shown in Equations (5)?(7), where TaskEaseWOZ is the regression
model obtained from the WOZ data,10 TaskEaseSL is obtained from the user test data
running the supervised average wizard policy, and TaskEaseRL is obtained from the
user test data running the RL-based policy. They all reflect the same trends: Longer
dialogues (measured in turns) result in a lower Task Ease, whereas a good performance
in the multimodal information presentation phase (multimodal score) will positively
influence Task Ease. For the user tests almost all the tasks were completed; therefore
task completion was only chosen to be a predictive factor for the WOZ model.
TaskEaseWOZ = 1.58 + .12? taskCompl + .09 ? mmScore ? .20? dialogueLength (5)
TaskEaseSL = 3.50 + .54? mmScore ? .34 ? dialogueLength (6)
TaskEaseRL = 3.80 + .49 ? mmScore ? .36? dialogueLength (7)
To evaluate the obtained regression models we use two measures: how well they
fit the data and how close the functions are to each other (model replicability). Both
are measured using goodness-of-fit R2. For the WOZ model the data fit was rather
low (R2WOZ = .123),
11 whereas for the models obtained from the user tests the fit has
improved (R2RL = .48, and R
2
SL = .55).
Next, we compare how well the models from different data sets fit each other.
Although the models obtained from the user test data show almost perfect overlap
(R2 = .98), the (reduced) WOZ model differs (R2 = .22) in the sense that it assigns less
weight to dialogue length and the multimodal presentation score, and more weight
is assigned to task completion. Task completion did not play a role for the user tests,
as mentioned earlier. This shows that multimodal presentation and dialogue length
become even more important once the tasks are being completed. Overall, then, the
data-driven reward model is relatively stable across the different data sets (WOZ, real
users with the SL policy, and real users using the RL policy).
6.2.2 Reward Model Performance: Prediction Accuracy. We now investigate how well these
reward models generalize by testing their prediction accuracy. Previous research evalu-
10 In contrast to the model in Equation (4) we now include the constant in the regression.
11 For R2 we use the adjusted values.
176
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 11
Prediction accuracy for models within (1?3) and across (4?5) data sets.
ID train test RMSE % error
1 WOZ WOZ 0.82 16.42
2 SL SL 1.27 18.14
3 RL RL 1.06 15.14
4 RL SL 1.23 17.57
5 SL RL 1.03 14.71
ated two aspects: how well a given objective function/reward model is able to predict
unseen scores from the original system (Engelbrecht and Mo?ller 2007), and how well it
is able to predict unseen scores of a new/different system (Walker, Kamm, and Litman
2000). We evaluate these two aspects as well, the only difference is that we use the
Root Mean Standard Error (RMSE) instead of R2 for measuring the model?s prediction
accuracy. The RMSE is a frequently used measure of the differences between values
predicted by a model or an estimator and the values actually observed. It is defined over
[0,?], where 0 indicates perfect overlap. The maximum RMSE possible (= worst case)
in our set-up is 7 for SL/RL and 5 for WOZ. In order to present results from different
scales we also report the percentage of the RMSE of the maximum error (% error). RMSE
is (we argue) more robust for small data sets.12
R2 = 1?
?n
i=1(yi ? ?yi)2
?n
i=1(y ? y)
(8)
RMSE =
?
?
?
?
1
n
n
?
i=1
(yi ? ?yi)2 (9)
First, we measure the predictive power of our models within the same data set using
10-fold cross validation, and then across the different systems by testing models trained
on one system to predict perceived Task Ease scores for another system, following a
method introduced by Walker, Kamm, and Litman (2000).
The results for comparing the RMSE for training and testing within data sets (ID
1-3) and across data sets (ID 4?5) are shown in Table 11. RMSE measures the average of
the square of the ?error.? As such, lower RMSE values are better. The contrary is true
for R2, where ?1? indicates perfect overlap between two functions.
The results show that predictions according to PARADISE can lead to accurate test
results despite the low data fit. Whereas for the regression model obtained from the
WOZ data the fit was 10 times lower than for SL/RL, the prediction performance is
comparably good (see Table 11, ID 1?3). The models also generalize well across systems
(see Table 11, ID 4?5).
Table 12 visualizes the results (ID 1?3): Mean values for predicted and for true
ratings are plotted per turn (see Engelbrecht and Mo?ller 2007). The top two graphs in the
12 In particular, we argue that, by correcting for variance, R2 can lead to artificially good results when using
small tests sets (which typically vary more) and is sensitive to outliers (see Equation (8)). RMSE instead
measures the (root) mean difference between actual and predicted values (see Equation (9)).
177
Computational Linguistics Volume 37, Number 1
Table 12
Average Task Ease ratings for dialogues of different length (in turns); the solid lines are the true
ratings and the dashed line the predicted values; from top: RL, SL wizard, WOZ data.
178
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
table show that the predicted mean values are fairly accurate for the SL and RL objective
functions. The graph at the bottom indicates that the predictions are less accurate for the
WOZ data, especially for low numbers of turns. This seems to contradict the previous
results in Table 11, which show low error rates for the WOZ data. However, this is due
to the fact that most of the observations in the WOZ data set are in the region where the
predictions are accurate (i.e., most of the dialogues in the WOZ data are over 14 turns
long, where the curves converge).
We conclude that, according to our measures, an objective function obtained from
WOZ data is a valid first estimate of real users? preferences. Despite a low fit to the initial
data, the objective function obtained from WOZ data makes accurate and useful predic-
tions for automatic dialogue evaluation/reward. The models obtained from the tests
with a real system follow the same trends, but can be seen as more reliable estimates
of the objective function in this domain. In future work we will explore incrementally
training a system according to improved representations of real user preferences, for
example, gathered on-line from a deployed spoken dialogue system.
7. Conclusion
We have presented a new data-driven methodology for simulation-based dialogue
strategy learning. It allows us to address several problems in the field of automatic
optimization of dialogue strategies: learning effective dialogue strategies when no ini-
tial data or system exists, and determining a data-driven reward function. We learned
optimal strategies by interaction with a simulated environment which is bootstrapped
from a small amount of Wizard-of-Oz data, and we evaluated the result with real users.
The use of WOZ data allows us to develop optimal strategies for domains where no
working prototype is available. In contrast to previous work, the developed simulations
are largely data-driven and the reward function reflects real user preferences.
We compare the Reinforcement Learning?based strategy against a supervised strat-
egy which mimics the (human) wizards? policies from the original data. This compari-
son allows us to measure relative improvement over the training data. Our results show
that RL significantly outperforms the average wizard strategy in simulation as well as in
interactions with real users. The RL-based policy gains on average 50 times more reward
when tested in simulation, and almost 18 times more reward when interacting with
real users. The human users also subjectively rate the RL-based policy on average 10%
higher, and 49% higher for Task Ease. We also show that results obtained in simulation
are comparable to results for real users and we also evaluate the reward function. We
conclude that a strategy trained from WOZ data via our bootstrapping method works
well for real HCI.
Regarding scaling up such methods to larger databases, we would still quantize
the number of database hits into High, Medium, and Low (more hits would be added
to the ?High? bin). In fact, wizards seemed to treat everything over 60 database hits
equivalently (see Section 3.7). For learning, the state space grows linearly with the
number of possible database hits. Techniques such as linear function approximation are
useful for handling the resulting larger state spaces (Henderson, Lemon, and Georgila
2008).
In ongoing work we apply similar techniques to statistical planning for Natural
Language Generation in spoken dialogue (Lemon 2011, 2008; Janarthanam and Lemon
2008; Rieser and Lemon 2009b; Rieser, Lemon, and Liu 2010; Janarthanam and Lemon
2010), see the EC FP7 CLASSiC project: www.classic-project.org.
179
Computational Linguistics Volume 37, Number 1
Appendix A: Measuring Inter-Annotator Agreement on User Act Using the
? Statistics
Cohen?s Kappa (often simply called kappa, ?) is a measure of agreement between the
two individuals. We use this measure to assess inter-annotator agreement on user act,
following Carletta et al (1997). Detailed results are in the table.
Table 1
Inter-annotator agreement on user act with ? = .632.
annotator1 annotator2
user reply add change others repeat y/n total
add 58 5 2 1 0 66
change 4 11 0 0 0 15
others 9 5 21 1 0 36
repeat 9 0 7 9 0 25
y/n 2 0 2 1 30 35
total 82 21 32 12 30 177
180
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Appendix B: Wizard-of-Oz Example Dialogues
Table 1
Example dialogue from the SAMMIE WOZ corpus (mixed wizard strategy).
s1 User: Bitte suche das Lied ,Layla?.
Transl.: Search for the song ?Layla? please.
SA* user: add -> song-title(Layla)
Wizard: Wir haben das Lied gefunden. Es existiert 142 mal in verschiedenen
Ausfu?hrungen
Transl.: The song is retrieved. There are 142 different versions.
mmAction: [ Table with 142 entries is shown on the screen. ]
SA
wizard:
presentMM -> song-title (db:142)
s2 User: Bitte suche das Lied on dem Komponisten ,Eric Clapton?.
Transl.: Search for the song by the composer ?Eric Clapton? please.
mm
Action:
[ User does not react to what is displayed. ]
SA user: change -> artist(?Eric Clapton?)
Wizard: Es gibt das Lied von ,Eric Clapton? oder von ?Derek & the Dominos?.
Transl.: The song exists by ?Eric Clapton? or by ?Derek & the Dominos?.
SA
wizard:
presentVerbal -> song-title, artist (db:2)
s3 User: Bitte spiele das Lied von ,Eric Clapton Unplugged? ab.
Transl.: Play the song from ?Eric Clapton Unplugged? please.
SA user: repeat ->album(?Eric Clapton Unplugged?)
Wizard: Ich spiele das Lied jetzt ab.
Transl.: OK I am going to play the song now.
SA
wizard:
feedback/others
mmAction: [ music ]
user
ratings
taskEase: 2, timing:2, mmPresent.: 1 , verbalPresent: 3, futureUse:3
* SA = Speech Act
Comments:
In this example the wizard uses a mixed strategy presenting information multimodally in turn 1
and verbally in turn 2. The first time 142 items are presented multimodally by verbally reporting
the number of retrieved items while displaying them on the screen. The database used in the WOZ
experiments contained over a million songs and is searched with partial string match. Therefore
the retrieved items for ?Layla? can be as high as 142. The user does not directly react to what is
shown on the screen (either by referring to it or clicking), but continues providing information
until the wizard (verbally) presents two alternatives, where the user indirectly chooses one
by providing an album name which also disambiguates the artist. The user rankings from the
questionnaire are about average.
181
Computational Linguistics Volume 37, Number 1
Table 2
Example dialogue from the SAMMIE WOZ corpus (wizard not starting the presentation phase
early enough).
s1 User: Also, ich suche das Lied ,Smile?.
Transl.: Well, I am searching for the song ?Smile?.
SA user: add -> song-title(Smile)
Wizard: Welches Lied suchst du?
Transl.: What song are you searching for?
comment: [ Song name was deleted ]
SA
wizard:
reject(song-title)
s2 User: Smile.
Transl.: Smile.
SA user: repeat -> song-title(Smile)
Wizard: Du suchst das Lied ,Smile??
Transl.: Are you searching for the song ?Smile??
SA
wizard:
explConf(song-title)
s3 User: Ja, Smile ja.
Transl.: Yes, Smile yes.
SA user: y-answer -> song-title(Smile)
Wizard: Ich suche es... Ich habe vier Lieder mit dem einfachen Titel ,Smile?.
Transl.: I am searching... I found four songs with only ?Smile? as the title.
SA
wizard:
presentVerbal(song-title)
s4 User: Ja denn, das Lied bitte anzeigen.
Transl.: Very well then, why don?t you show it?
SA user: others
Wizard: OK.
Transl.: OK.
SA
wizard:
presentMM(song-title)
user
ratings
taskEase: 2, timing: 2, mmPresent.: 5, verbalPresent: 3, futureUse:3
Comments:
In this example the user is asking to see a short list on the screen (after resolving some under-
standing errors induced by the deletion tool), which is indicating that (a) the wizard did not enter
the presentation phase early enough, and (b) that for the user four items seems to be a reasonable
number to display. The user scores reflect this fact: The presentation mode got a maximal score
(which will influence learning towards showing short lists), whereas the task ease got a lower
score (task ease is used as an overall reward, i.e., this ?tells? the learner to enter the presentation
mode earlier).
182
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Appendix C: Example Dialogues from Simulated Interaction
Table 1
Example dialogue with the SL policy operating in the simulated environment.
s1 SL policy: greet (db:438)
sim.User: add->genre(Rock)
state: filled [slot1], confirmed [], db:142
s2 SL policy: implConfAskASlot -> genre, artist)
sim.User: repeat->genre(Rock)
state: filled [slot1], confirmed[slot1], db:142
s3 SL policy: present[mm] ->genre
sim.User: change->artist(Jimi Hendrix)
state: filled [slot1,slot2], confirmed [slot1], db:4
s4 SL policy: implConfAskASlot->artist, album
sim.User: add->album(Experience Hendrix: The Best of Jimi Hendrix)
state: filled [slot1,slot2,slot3], confirmed [slot1,slot2], db:4
s5 SL policy: implConfAskASlot->album, song-title
sim.User: add->song-title(Voodoo Child)
state: filled [slot1,slot2,slot3,slot4], confirmed
[slot1,slot2,slot3], db:1
s6 SL policy: explicitConfirm->song-title
sim.User: repeat->song-title(Voodoo Child)
state: filled [slot1,slot2,slot3,slot4], confirmed
[slot1,slot2,slot3,slot4], db:1
s7 SL policy: present[verbal] ->genre, artist, album, song-title
sim.User: yes-answer->yes(yes)
state filled [slot1,slot2,slot3,slot4], confirmed
[slot1,slot2,slot3,slot4], db:1
reward: lengthPenalty : ?141.4, complVal : 118, presentVal : 8.7? (4.33? 2151.23)
finalReward:-18701.43
Comments:
This example shows a dialogue where the SL policy is operating in the simulated environment.
Presentation timing and amount of presented information are not ideal (and therefore leading to
quite a long dialogue): In turn 2 the system presents a list of 238 items, whereas when only having
four alternatives (from turn 3 onwards) it fails to show a list.
183
Computational Linguistics Volume 37, Number 1
Table 2
Example dialogue with the RL policy operating in the simulated environment (mixed strategy).
s1 RL policy: greet (db: 438)
sim.User: add->artist(Nirvana)
state: filled [slot2], confirmed [], db:26
s2 RL policy: implConfAskASlot -> artist, album)
sim.User: add->album(MTV Unplugged)
state: filled [slot2,slot3], confirmed [slot2], db:14
s3 RL policy: present[mm] ->artist, album
sim.User: click->song-title(On a Plain),genre(Alternative)*
state: filled [slot1,slot2,slot3,slot4], confirmed
[slot2,slot3], db:1
s4 RL policy: present[verbal] ->artist, album, song-title
sim.User: yes-answer->yes(yes)
state: filled [slot1,slot2,slot3,slot4], confirmed
[slot1,slot2,slot3,slot4], db:1
reward: lengthPenalty : ?80.8, complVal : 118, presentVal : 8.7? (4.33 + 7.23)
finalReward:140.3
Comments:
This example shows the RL policy interacting in the simulated environment. The system follows
a mixed strategy: first presenting a list of 14 items on the screen and then presenting a unique
item verbally.
* the user acts add after a multimodal presentation are equivalent to clicking
Table 3
Example dialogue with the RL policy operating in the simulated environment: presenting
alternatives.
s1 RL policy: greet (db: 438)
sim.User: add->artist(Bob Dylan)
state: filled [slot2], confirmed [], db:56
s2 RL policy: implConf(artist)AskASlot(genre)
sim.User: change->song-title(Hurricane)
state: filled [slot2,slot4], confirmed [slot2], db:2
s3 RL policy: present[verbal] ->album1,album2
sim.User: add->album(Desire)
state: filled [slot2, slot3, slot4], confirmed [slot2,slot4], db:1
s4 RL policy: present[verbal] ->artist, album, song-title
sim.User: yes-answer->yes(yes)
state: filled [slot2, slot3, slot4], confirmed
[slot2, slot3, slot4], db:1
reward: lengthPenalty : ?80.8, complVal : 118, presentVal : 8.7? (4.327 + 3.395)
finalReward: 104.38
Comments:
This example shows the RL policy interacting in the simulated environment. The system presents
two alternatives and a single item using only verbal output.
184
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Appendix D: Example Dialogues from User Testing
Table 1
Example dialogue with the SL policy operating with real users.
s1 SL system: Hi, this is iTalk your talkative music player. How may I help you today?
SA system: greet (db: 438)
User: i would like to listen to a blues song
SA user: add->genre(Blues)
state: filled [slot1], confirmed [], db:175
s2 SL system: There are 175 items matching your query. A list of possible candidates are
shown on the screen.
SA system: presentMM-> genre (db:175.0)
User: ?
SA user: nothing (time out)
state: filled [slot1], confirmed [], db:175
s3 SL system: A Blues song. By which artist?
SA system: implConf(genre), askASlot(artist)
User: eric clapton
SA user: add->artist(?Eric Clapton) (db:25.0)
state: filled [slot1, slot2], confirmed [slot1], db:25
s4 SL system: Searching for music by Eric Clapton. Which album?
SA system: implConf(?Eric Clapton?), askASlot(album)
User: mtv unplugged
SA user: add->album(?MTV unplugged?)
state: filled [slot1, slot2, slot3], confirmed [slot1, slot2], db:13
s5 SL system: A song from the album MTV unplugged. What is the title of the song?
SA system: implConf(album), askASlot(song-title)
User: what do you have
SA user: request-help
state: filled [slot1, slot2, slot3], confirmed [slot1, slot2], db:13
s6 SL system: You have several options. For example: Layla , Nobody Knows You When
You Are Down and Out , Lonely Stranger , Old Love , and San Francisco Bay
Blues. Do you want to hear more?
SA system: give-help
User: no layla please
SA user: add->song-title(Layla)
state: filled [slot1, slot2, slot3, slot4], confirmed
[slot1, slot2, slot3], db:1
s7 SL system: You will now hear Layla by Eric Clapton from the album MTV unplugged.
Are you happy with this option?
SA system: presentVerbal->genre,artist,album,song-title
User: yes
SA user: yes-answer
state: filled [slot1, slot2, slot3, slot4], confirmed
[slot1, slot2, slot3, slot4], db:1
reward: lengthPenalty : ?141.4, complVal : 118, presentVal : 8.7? (4.33? 7.63)
finalReward: -6577.03
user ratings taskEase:2, timing:2, mmPresent:1, verbalPresent:2, futureUse:2
Comments:
This example shows the SL policy applied with real users. In turn 2 the user ignores the 175 items displayed,
then the system does not display the 13 retrieved items. The user asks for help.
185
Computational Linguistics Volume 37, Number 1
Table 2
Example dialogue with the RL policy operating with real users: multimodal and verbal
presentation.
s1 RL system: Hi. this is iTalk your talkative music player. How may I help you today?
SA system greet (db:438)
User: play me some blues
SA user: add->genre(Blues) (db:175)
state: filled [slot1], confirmed [], db:175
s2 RL system: Blues for type of music. By which artist?
SA system: implConf(genre),askASlot(artist)
User: from the album desire
SA user: change->album(Desire)(db:10)
state: filled [slot1, slot3], confirmed [slot1], db:10
s3 RL system: There are 10 items matching your query. A list of possible candidates are
shown on the screen.
SA system: presentMM->genre,album
User: [clicks]
SA user: click->genre,song-title
state: filled [slot1, slot2, slot3, slot4], confirmed [slot1, slot3], db:1
s4 RL system: You selected an item on the screen. Please wait a few moments. [. . . ] You will
now hear Mozambique by Bob Dylan from the album Desire. Are you happy
with this option
SA presentVerbal->genre,album,artist,song-title
User: yes
SA yes-answer
state: filled [slot1, slot2, slot3, slot4], confirmed
[slot1, slot2, slot3, slot4], db:1
reward: lengthPenalty : ?60.6, complVal : 118, presentVal : 8.7? (4.33 + 6.57)
finalReward: 152.16
user ratings taskEase:6, timing:7, mmPresent.:7 verbalPresent: 6 futureUse:6
Comments:
This example shows the RL policy interacting with real users. The system uses a mixed strategy, showing
multimodal output as soon as the number of retrieved items is below 15 (and therefore the reward will be
positive), and presenting one item verbally.
186
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 3
Example dialogue with the RL policy operating with real users: Verbal presentation of options.
s1 RL system: Hi. this is iTalk your talkative music player. How may I help you today?
SA system: greet (db:438)
User: i want to hear polly
SA user: add->song-title(Polly)
state: filled [slot4], confirmed [], db:2
s2 RL system: There are 2 results matching your query all by the artist Nirvana all from the
genre Alternative all having the song title Polly, but a different album. You
have the following options: Nevermind and MTV Unplugged. Which album
would you like?
SA system: presentVerbal->song-title,artist,genre
comment: systems fills unique slots.
User: mtv unplugged
SA user: add->album(?MTV unplugged?) (db:1)
state: filled [slot3, slot4], confirmed [song4], db:1
s3 RL system: There is only one result matching your query. Please wait a few moments.
[. . . ] You will now hear Polly by Nirvana from the album MTV unplugged.
Are you happy with this option ?
SA system: presentVerbal->song-title,artist,genre,album
User: yes
SA user: yes-answer
state: filled [slot1, slot2, slot3, slot4], confirmed
[slot1, slot2, slot3, slot4], db:1
reward: lengthPenalty : ?60.6, complVal : 118, presentVal : 8.7? (4.33 + 3.4)
finalReward: 124.6
user ratings taskEase:7, timing:7, mmPresent.:7 verbalPresent: 7 futureUse:6
Comments:
This example shows the RL policy interacting with real users. Two alternatives are presented verbally.
187
Computational Linguistics Volume 37, Number 1
Appendix E: Learned State Action Mappings
The table in this section presents the learned state action mappings, and is to be read
as follows. The first two columns constitute the state space. The first column shows the
slots that have been filled and/or confirmed. The slots are:
slot 1: genre
slot 2: artist
slot 3: album
slot 4: song title
The second column represents possible numbers of database hits. Note that the possible
number of items returned from the database is constrained by the structure of the task
(i.e., how combinations of different slots values constrain the search space).
The third column is the optimal action for that state. The ?x?s in the second column
denote the numbers of database hits that share the same optimal action (given the set
of filled and confirmed slots). Horizontal lines are drawn between sets of states with
different filled slots.
188
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
L
ea
rn
ed
S
ta
te
A
ct
io
n
M
ap
p
in
gs
st
at
e
sp
ac
e
sy
st
em
ac
ti
on
sl
ot
va
lu
es
n
o.
D
B
h
it
s
1
2
3
4
5
6
7
8
9
10
11
12
13
14
16
18
20
25
26
34
40
54
56
92
14
2
17
5
43
8
fi
ll
ed
[]
,c
on
fi
rm
ed
[]
x
as
kA
Sl
ot
fi
ll
ed
[s
lo
t1
],
co
n
fi
rm
ed
[]
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
],
co
n
fi
rm
ed
[s
lo
t1
]
x
x
x
as
kA
Sl
ot
fi
ll
ed
[s
lo
t2
],
co
n
fi
rm
ed
[]
x
x
x
x
x
x
im
p
lC
on
f
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t2
],
co
n
fi
rm
ed
[s
lo
t2
]
x
x
x
x
x
x
as
kA
Sl
ot
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t3
],
co
n
fi
rm
ed
[]
x
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t3
]
x
x
x
x
as
kA
Sl
ot
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t4
],
co
n
fi
rm
ed
[]
x
ex
p
lC
on
f
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
as
kA
Sl
ot
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
],
co
n
fi
rm
ed
[]
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
]
x
x
x
x
x
x
as
kA
Sl
ot
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
],
co
n
fi
rm
ed
[s
lo
t1
]
x
x
x
x
x
x
im
p
lC
on
f
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
],
co
n
fi
rm
ed
[s
lo
t2
]
x
x
x
x
x
x
im
p
lC
on
f
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t3
],
co
n
fi
rm
ed
[]
x
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t3
]
x
x
x
x
as
kA
Sl
ot
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
]
x
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t3
]
x
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
x
im
p
lC
on
f
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t4
]
x
as
kA
Sl
ot
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
]
x
im
p
lC
on
f
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
im
p
lC
on
f
x
x
x
p
re
se
n
tV
er
ba
l
189
Computational Linguistics Volume 37, Number 1
L
ea
rn
ed
S
ta
te
A
ct
io
n
M
ap
p
in
gs
st
at
e
sp
ac
e
sy
st
em
ac
ti
on
sl
ot
va
lu
es
n
o.
D
B
h
it
s
1
2
3
4
5
6
7
8
9
10
11
12
13
14
16
18
20
25
26
34
40
54
56
92
14
2
17
5
43
8
fi
ll
ed
[s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[]
x
x
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
]
x
x
as
kA
Sl
ot
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t2
]
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t3
]
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
x
x
ex
p
lC
on
f
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t4
]
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,
sl
ot
4]
,
co
n
fi
rm
ed
[s
lo
t2
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[]
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
]
x
x
x
as
kA
Sl
ot
x
x
x
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
]
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t3
]
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
]
x
x
x
x
x
x
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
]
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t2
]
x
x
x
x
x
x
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t3
]
x
x
x
x
x
x
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
x
im
p
lC
on
f
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t4
]
x
p
re
se
n
tM
M
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
190
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
L
ea
rn
ed
S
ta
te
A
ct
io
n
M
ap
p
in
gs
st
at
e
sp
ac
e
sy
st
em
ac
ti
on
sl
ot
va
lu
es
n
o.
D
B
h
it
s
1
2
3
4
5
6
7
8
9
10
11
12
13
14
16
18
20
25
26
34
40
54
56
92
14
2
17
5
43
8
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t1
,
sl
ot
2,
sl
ot
3,
sl
ot
4]
,
co
n
fi
rm
ed
[s
lo
t2
,
sl
ot
3,
sl
ot
4]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
ex
p
lC
on
f
191
Computational Linguistics Volume 37, Number 1
Acknowledgments
We would like to thank Professor Manfred
Pinkal for discussing this work. We would
also like to thank the anonymous reviewers
for their useful comments. The research
leading to these results has received funding
from the European Community?s Seventh
Framework Programme (FP7, 2007?2013)
under grant agreement number 216594
(?Computational Learning in Adaptive
Systems for Spoken Conversation,? CLASSiC
project: www.classic-project.org), the EC
FP6 project ?TALK: Talk and Look, Tools for
Ambient Linguistic Knowledge? (IST 507802,
www.talk-project.org), from EPSRC grant
numbers EP/E019501/1 and EP/G069840/1,
and from the International Research Training
Group in Language Technology and
Cognitive Systems, Saarland University.
References
Ai, Hua and Diane Litman. 2006. Comparing
real-real, simulated-simulated, and
simulated-real spoken dialogue corpora.
In Proceedings of the AAAI Workshop on
Statistical and Empirical Approaches for
Spoken Dialogue Systems, Boston, MA.
Ai, Hua, Joel Tetreault, and Diane Litman.
2007. Comparing user simulation models
for dialog strategy learning. In Proceedings
of the North American Meeting of the
Association of Computational Linguistics
(NAACL), pages 1?4, Rochester, NY.
Bohus, Dan, Brian Langner, Antoine Raux,
Alan W. Black, Maxine Eskenazi, and Alex
Rudnicky. 2006. Online supervised
learning of non-understanding recovery
policies. In Proceedings of the IEEE/ACL
workshop on Spoken Language Technology
(SLT), Aruba, pages 170?173.
Carletta, Jean, Amy Isard, Stephen Isard,
Jacqueline C. Kowtko, Gwyneth
Doherty-Sneddon, and Anne H. Anderson.
1997. The reliability of a dialogue structure
coding scheme. Computational Linguistics,
23(1):13?31.
Carpenter, Paul, Chun Jin, Daniel Wilson,
Rong Zhang, Dan Bohus, and Alexander I.
Rudnicky. 2001. Is this conversation on
track? In Proceedings of the European
Conference on Speech Communication and
Technology (Eurospeech), page 2121,
Aalborg.
Clark, Herbert. 1996. Using Language.
Cambridge University Press.
Cohen, William W. 1995. Fast effective rule
induction. In Proceedings of the 12th
International Conference on Machine
Learning (ICML), pages 115?123, Tahoe
City, CA.
Cuaya?huitl, Heriberto, Steve Renals, Oliver
Lemon, and Hiroshi Shimodaira. 2005.
Human?Computer dialogue simulation
using Hidden Markov Models. In
Proceedings of the IEEE workshop on
Automatic Speech Recognition and
Understanding (ASRU), pages 290?295,
San Juan.
Dempster, A., N. Laird, and D. Rubin. 1977.
Maximum likelihood from incomplete
data via the EM algorithm. Journal of
Royal Statistical Society B, 39:1?38.
Eckert, W., E. Levin, and R. Pieraccini. 1997.
User modeling for spoken dialogue
system evaluation. In Proceedings of the
IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU),
pages 80?87, Santa Barbara, CA.
Eckert, W., E. Levin, and R. Pieraccini.
1998. Automatic evaluation of spoken
dialogue systems. In Proceedings of Formal
semantics and pragmatics of dialogue
(TWLT13), pages 99?110, Twente, NL.
Engelbrecht, Klaus-Peter and Sebastian
Mo?ller. 2007. Pragmatic usage of linear
regression models for the predictions of
user judgements. In Proceedings of the
8th SIGdial workshop on Discourse and
Dialogue, pages 291?294, Antwerp.
Foster, Mary Ellen and Jon Oberlander.
2006. Data-driven generation of
emphatic facial displays. In Proceedings
of the 11th Conference of the European
Chapter of the Association for
Computational Linguistics (EACL),
pages 353?360, Trento.
Frampton, Matthew and Oliver Lemon.
2009. Recent research advances in
Reinforcement Learning in Spoken
Dialogue Systems. Knowledge Engineering
Review, 24(4):375?408.
Fraser, Norman M. and G. Nigel Gilbert.
1991. Simulating speech systems. Computer
Speech and Language, 5:81?99.
Gabsdil, Malte and Oliver Lemon. 2004.
Combining acoustic and pragmatic
features to predict recognition
performance in spoken dialogue systems.
In Proceedings of the Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 343?350, Barcelona.
Georgila, K., J. Henderson, and O. Lemon.
2006. User simulation for spoken dialogue
systems: Learning and evaluation. In
Proceedings of the International Conference
of Spoken Language Processing (Interspeech/
ICSLP), Pittsburgh, PA, pages 1065?1068.
192
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Georgila, Kallirroi, James Henderson, and
Oliver Lemon. 2005. Learning user
simulations for information state update
dialogue systems. In Proceedings of the
International Conference of Spoken Language
Processing (Interspeech/ICSLP),
pages 893?896, Lisbon, Portugal.
Georgila, Kallirroi, Oliver Lemon, James
Henderson, and Johanna Moore. 2009.
Automatic annotation of context and
speech acts for dialogue corpora.
Natural Language Engineering,
15(3):315?353.
Hajdinjak, Melita and France Mihelic. 2006.
The PARADISE evaluation framework:
Issues and findings. Computational
Linguistics, 32(2):263?272.
Hall, Mark. 2000. Correlation-based feature
selection for discrete and numeric class
machine learning. In Proceedings of the
17th International Conference on Machine
Learning (ICML), pages 359?366,
San Francisco, CA.
Heeman, Peter. 2007. Combining
Reinforcement Learning with
information-state update rules. In
Proceedings of the North American Chapter of
the Association for Computational Linguistics
Annual Meeting (NAACL), pages 268?275,
Rochester, NY.
Henderson, James, Oliver Lemon, and
Kallirroi Georgila. 2008. Hybrid
Reinforcement/Supervised Learning of
dialogue policies from fixed datasets.
Computational Linguistics, 34(4):487?513.
Hirschberg, Julia, Diane Litman, and Marc
Swerts. 2001. Identifying user corrections
automatically in spoken dialogue systems.
In Proceedings of the North American Meeting
of the Association of Computational
Linguistics (NAACL), pages 1?8,
Pittsburgh, PA.
Janarthanam, Srini and Oliver Lemon. 2008.
User simulations for online adaptation and
knowledge-alignment in troubleshooting
dialogue systems. In Proceedings of the 12th
SEMdial Workshop on the Semantics and
Pragmatics of Dialogues, pages 149?156,
London, UK.
Janarthanam, Srini and Oliver Lemon. 2010.
Learning to adapt to unknown users:
Referring expression generation in Spoken
Dialogue Systems. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 69?78, Uppsala.
Jokinen, Kristiina and Topi Hurtig. 2006.
User expectations and real experience
on a multimodal interactive system.
In Proceedings of the International Conference
of Spoken Language Processing (Interspeech/
ICSLP), pages 1049?1055.
Jung, Sangkeun, Cheongjae Lee, Kyungduk
Kim, Minwoo Jeong, and Gary Geunbae
Lee. 2009. Data-driven user simulation for
automated evaluation of spoken dialog
systems. Computer Speech & Language,
23:479?509.
Jurafsky, Daniel and James H. Martin. 2000.
Speech and Language Processing. An
Introduction to Natural Language Processing,
Computational Linguistics, and Speech
Recognition. Prentice-Hall: New Jersey, US.
Kruijff-Korbayova?, Ivana, Tilmann Becker,
Nate Blaylock, Ciprian Gerstenberger,
Michael Kaisser, Peter Poller, Verena
Rieser, and Jan Schehl. 2006. The SAMMIE
corpus of multimodal dialogues with an
MP3 player. In Proceedings of the 5th
International Conference on Language
Resources and Evaluation (LREC),
pages 2018?2023, Genoa, Italy.
Lemon, Oliver. 2008. Adaptive natural
language generation in dialogue using
Reinforcement Learning. In Proceedings of
the 12th SEMdial Workshop on the Semantics
and Pragmatics of Dialogues, pages 149?156,
London, UK.
Lemon, Oliver. (2011). Learning what
to say and how to say it: joint optimization
of spoken dialogue management and
Natural Language Generation. Computer
Speech and Language, 2(2):210?221.
Lemon, Oliver, Kallirroi Georgila, and James
Henderson. 2006. Evaluating effectiveness
and portability of Reinforcement Learned
dialogue strategies with real users: The
TALK TownInfo evaluation. In Proceedings
of the IEEE/ACL Workshop on Spoken
Language Technology (SLT), Aruba,
pages 170?173.
Lemon, Oliver, Kallirroi Georgila, James
Henderson, Malte Gabsdil, Ivan
Meza-Ruiz, and Steve Young. 2005.
Deliverable D4.1: Integration of learning
and adaptivity with the ISU approach.
Technical report, TALK Project.
Lemon, Oliver and Ioannis Konstas. 2009.
User simulations for context-sensitive
speech recognition in spoken dialogue
systems. In European Conference of the
Association for Computational Linguistics,
pages 505?513, Athens.
Lemon, Oliver, Xingkun Liu, Daniel Shapiro,
and Carl Tollander. 2006. Hierarchical
Reinforcement Learning of dialogue
policies in a development environment for
dialogue systems: REALL-DUDE. In
193
Computational Linguistics Volume 37, Number 1
Proceedings of the 10th SEMdial Workshop on
the Semantics and Pragmatics of Dialogues,
pages 185?186, Potsdam, Germany.
Lemon, Oliver and Olivier Pietquin. 2007.
Machine Learning for spoken dialogue
systems. In Proceedings of the International
Conference of Spoken Language Processing
(Interspeech/ICSLP), pages 2685?2688,
Antwerp, Belgium.
Levin, E., R. Pieraccini, and W. Eckert. 2000.
A stochastic model of human?machine
interaction for learning dialog strategies.
IEEE Transactions on Speech and Audio
Processing, 8(1), pages 11?23.
Levin, Esther and Rebecca Passonneau. 2006.
A WOz variant with contrastive
conditions. In Proceedings Dialog-on-Dialog
Workshop, Interspeech, Pittsburgh, PA.
Litman, D. and S. Pan. 1999. Empirically
evaluating an adaptable spoken
dialogue system. In Proceedings of the
7th International Conference on User
Modeling, pages 55?64, Banff.
Mattes, Stefan. 2003. The Lane-Change-task
as a tool for driver distraction evaluation.
In H. Strasser, K. Kluth, H. Rausch, and
H. Bubb, editors, Quality of Work and
Products in Enterprises of the Future.
Ergonomica Verlag, Stuttgart, Germany,
pages 57?60.
Mo?ller, Sebastian, Roman Englert, Klaus
Engelbrecht, Verena Hafner, Anthony
Jameson, Antti Oulasvirta, Alexander
Raake, and Norbert Reithinger. 2006.
MeMo: Towards automatic usability
evaluation of spoken dialogue services
by user error simulations. In Proceedings of
the International Conference of Spoken
Language Processing (Interspeech/ICSLP),
Pittsburgh, PA, pages 1786?1789.
Paek, Tim. 2006. Reinforcement Learning for
spoken dialogue systems: Comparing
strengths and weaknesses for practical
deployment. In Proceedings Dialog-on-
Dialog Workshop, Interspeech, Pittsburgh, PA.
Paek, Tim. 2007. Toward evaluation that
leads to best practices: Reconciling
dialogue evaluation in research and
industry. In Proceedings of the NAACL-HLT
Workshop on Bridging the Gap: Academic and
Industrial Research in Dialog Technologies,
Rochester, NY, pages 40?47.
Pietquin, O. and T. Dutoit. 2006. A
probabilistic framework for dialog
simulation and optimal strategy learning.
IEEE Transactions on Audio, Speech and
Language Processing, 14(2):589?599.
Prommer, Thomas, Hartwig Holzapfel, and
Alex Waibel. 2006. Rapid simulation-
driven Reinforcement Learning of
multimodal dialog strategies in human-
robot interaction. In Proceedings of the
International Conference of Spoken Language
Processing (Interspeech/ICSLP), Pittsburgh,
PA, pages 1918?1924.
Quinlan, Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
San Francisco.
Rieser, Verena. 2008. Bootstrapping
Reinforcement Learning-based Dialogue
Strategies from Wizard-of-Oz data. Ph.D.
thesis, Saarbrueken Dissertations in
Computational Linguistics and Language
Technology, Vol 28.
Rieser, Verena, Ivana Kruijff-Korbayova?, and
Oliver Lemon. 2005. A corpus collection
and annotation framework for learning
multimodal clarification strategies. In
Proceedings of the 6th SIGdial Workshop on
Discourse and Dialogue, pages 97?106,
Lisbon.
Rieser, Verena and Oliver Lemon. 2006a.
Cluster-based user simulations for
learning dialogue strategies. In Proceedings
of the 9th International Conference of Spoken
Language Processing (Interspeech/ICSLP),
Pittsburgh, PA.
Rieser, Verena and Oliver Lemon. 2006b.
Using Machine Learning to explore human
multimodal clarification strategies. In
Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL),
pages 659?666, Sydney.
Rieser, Verena and Oliver Lemon. 2008a.
Does this list contain what you were
searching for? Learning adaptive dialogue
strategies for interactive question
answering. Natural Language Engineering,
15(1):55?72.
Rieser, Verena and Oliver Lemon. 2008b.
Learning effective multimodal dialogue
strategies from Wizard-of-Oz data:
Bootstrapping and evaluation. In
Proceedings of the 21st International
Conference on Computational Linguistics and
46th Annual Meeting of the Association for
Computational Linguistics (ACL/HLT),
pages 638?646, Columbus, OH.
Rieser, Verena and Oliver Lemon. 2009a.
Learning human multimodal dialogue
strategies. Natural Language Engineering,
16(1):3?23.
Rieser, Verena and Oliver Lemon. 2009b.
Natural Language Generation as planning
under uncertainty for Spoken Dialogue
Systems. In Proceedings of the Conference of
194
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
the European Chapter of the ACL (EACL),
pages 683?691, Athens.
Rieser, Verena, Oliver Lemon, and Xingkun
Liu. 2010. Optimising information
presentation for Spoken Dialogue
Systems. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 1009?1018,
Uppsala.
Rieser, Verena and Johanna Moore. 2005.
Implications for generating clarification
requests in task-oriented dialogues. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 239?246,
Ann Arbor, MI.
Rodriguez, Kepa J. and David Schlangen.
2004. Form, intonation and function
of clarification requests in German
task-orientated spoken dialogues. In
Proceedings of the 8th SEMdial Workshop on
the Semantics and Pragmatics of Dialogues,
pages 101?108, Barcelona.
Roque, Antonio and David Traum. 2008.
Degrees of grounding based on
evidence of understanding. In
Proceedings of SIGdial, pages 54?63,
Columbus, OH.
Schatzmann, J., K. Weilhammer, M. Stuttle,
and S. Young. 2006. A survey of statistical
user simulation techniques for
reinforcement-learning of dialogue
management strategies. Knowledge
Engineering Review, 21(2):97?126.
Schatzmann, Jost, Kallirroi Georgila, and
Steve Young. 2005. Quantitative evaluation
of user simulation techniques for Spoken
Dialogue Systems. In Proceedings of the 6th
SIGdial Workshop on Discourse and Dialogue,
pages 45?54, Lisbon.
Schatzmann, Jost, Blaise Thomson, Karl
Weilhammer, Hui Ye, and Steve Young.
2007. Agenda-based user simulation for
bootstrapping a POMDP dialogue system.
In Proceedings of the North American Meeting
of the Association of Computational
Linguistics (NAACL), pages 149?152,
Rochester, NY.
Schatzmann, Jost, Blaise Thomson, and Steve
Young. 2007a. Error simulation for training
statistical dialogue systems. In Proceedings
of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU),
pages 526?531, Kyoto.
Schatzmann, Jost, Blaise Thomson, and Steve
Young. 2007b. Statistical user simulation
with a hidden agenda. In Proceedings of the
8th SIGdial Workshop on Discourse and
Dialogue, pages 273?282, Antwerp.
Scheffler, Konrad and Steve Young. 2001.
Corpus-based dialogue simulation for
automatic strategy learning and
evaluation. In Proceedings NAACL
Workshop on Adaptation in Dialogue Systems,
Pittsburgh, PA, pages 64?70.
Shapiro, Dan and P. Langley. 2002.
Separating skills from preference: Using
learning to program by reward. In
Proceedings of the 19th International
Conference on Machine Learning (ICML),
pages 570?577, Sydney.
Singh, S., D. Litman, M. Kearns, and
M. Walker. 2002. Optimizing dialogue
management with Reinforcement
Learning: Experiments with the NJFun
system. JAIR, 16:105?133.
Skantze, Gabriel. 2005. Exploring human
error recovery strategies: Implications for
spoken dialogue systems. Speech
Communication, 43(3):325?341.
Stuttle, Matthew N., Jason D. Williams, and
Steve Young. 2004. A framework for
dialogue data collection with a simulated
ASR channel. In Proceedings of the
International Conference of Spoken Language
Processing (Interspeech/ICSLP), Jeju.
Sutton, R. and A. Barto. 1998. Reinforcement
Learning. MIT Press, Cambridge, MA.
Walker, M., C. Kamm, and D. Litman.
2000. Towards developing general
models of usability with PARADISE.
Natural Language Engineering, 6(3),
pages 363?377.
Walker, Marilyn. 2000. An application for
Reinforcement Learning to dialogue
strategy selection in a spoken dialogue
system for email. Artificial Intelligence
Research, 12:387?416.
Walker, Marilyn. 2005. Can we talk? Methods
for evaluation and training of spoken
dialogue system. Language Resources and
Evaluation, 39(1):65?75.
Walker, Marilyn, Jeanne Fromer, and
Shrikanth Narayanan. 1998. Learning
optimal dialogue strategies: A case study
of a spoken dialogue agent for email.
In Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and 17th International
Conference on Computational Linguistics
(ACL/COLING), pages 1345?1351,
Montreal.
Walker, Marylin, Diane Litman, Candace
Kamm, and Alicia Abella. 1997.
PARADISE: A general framework for
evaluating spoken dialogue agents. In
Proceedings of the 35th Annual General
Meeting of the Association for
195
Computational Linguistics Volume 37, Number 1
Computational Linguistics (ACL),
pages 271?280, Madrid.
Williams, J. and S. Young. 2007. Partially
observable Markov decision processes for
Spoken Dialog Systems. Computer Speech
and Language, 21(2):231?422.
Williams, Jason. 2007. A method for
evaluating and comparing user
simulations: The Cramer?von Mises
divergence. In Proceedings of the IEEE
Workshop on Automatic Speech Recognition
and Understanding (ASRU), pages 508?513,
Kyoto, Japan.
Williams, Jason and Steve Young. 2004. Using
Wizard-of-Oz simulations to bootstrap
Reinforcement-Learning-based dialog
management systems. In Proceedings of the
4th SIGDIAL Workshop on Discourse and
Dialogue, pages 135?139, Sapporo, Japan.
Witten, Ian H. and Eibe Frank. 2005. Data
Mining: Practical Machine Learning Tools and
Techniques (2nd Edition). Morgan Kaufmann,
San Francisco.
Young, Steve. 2000. Probabilistic methods in
spoken dialogue systems. Philosophical
Trans Royal Society (Series A),
358(1769):1389?1402.
Young, Steve, M. Gasic, S. Keizer, F. Mairesse,
J. Schatzmann, B. Thomson, and K. Yu.
2009. The Hidden Information State
Model: A practical framework for
POMDP-based spoken dialogue
management. Computer Speech and
Language, 24(2):150?174.
196
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009?1018,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimising Information Presentation for Spoken Dialogue Systems
Verena Rieser
University of Edinburgh
Edinburgh, United Kingdom
verena.rieser@ed.ac.uk
Oliver Lemon
Heriot-Watt University
Edinburgh, United Kingdom
o.lemon@hw.ac.uk
Xingkun Liu
Heriot-Watt University
Edinburgh, United Kingdom
x.liu@hw.ac.uk
Abstract
We present a novel approach to Informa-
tion Presentation (IP) in Spoken Dialogue
Systems (SDS) using a data-driven statis-
tical optimisation framework for content
planning and attribute selection. First we
collect data in a Wizard-of-Oz (WoZ) ex-
periment and use it to build a supervised
model of human behaviour. This forms
a baseline for measuring the performance
of optimised policies, developed from this
data using Reinforcement Learning (RL)
methods. We show that the optimised poli-
cies significantly outperform the baselines
in a variety of generation scenarios: while
the supervised model is able to attain up to
87.6% of the possible reward on this task,
the RL policies are significantly better in 5
out of 6 scenarios, gaining up to 91.5% of
the total possible reward. The RL policies
perform especially well in more complex
scenarios. We are also the first to show
that adding predictive ?lower level? fea-
tures (e.g. from the NLG realiser) is im-
portant for optimising IP strategies accord-
ing to user preferences. This provides new
insights into the nature of the IP problem
for SDS.
1 Introduction
Work on evaluating SDS suggests that the Infor-
mation Presentation (IP) phase is the primary con-
tributor to dialogue duration (Walker et al, 2001),
and as such, is a central aspect of SDS design.
During this phase the system returns a set of items
(?hits?) from a database, which match the user?s
current search constraints. An inherent problem
in this task is the trade-off between presenting
?enough? information to the user (for example
helping them to feel confident that they have a
good overview of the search results) versus keep-
ing the utterances short and understandable.
In the following we show that IP for SDS can
be treated as a data-driven joint optimisation prob-
lem, and that this outperforms a supervised model
of human ?wizard? behaviour on a particular IP
task (presenting sets of search results to a user).
A similar approach has been applied to the
problem of Referring Expression Generation in di-
alogue (Janarthanam and Lemon, 2010).
1.1 Previous work on Information
Presentation in SDS
Broadly speaking, IP for SDS can be divided into
two main steps: 1) IP strategy selection and 2)
Content or Attribute Selection. Prior work has
presented a variety of IP strategies for structur-
ing information (see examples in Table 1). For ex-
ample, the SUMMARY strategy is used to guide the
user?s ?focus of attention?. It draws the user?s at-
tention to relevant attributes by grouping the cur-
rent results from the database into clusters, e.g.
(Polifroni and Walker, 2008; Demberg and Moore,
2006). Other studies investigate a COMPARE strat-
egy, e.g. (Walker et al, 2007; Nakatsu, 2008),
while most work in SDS uses a RECOMMEND strat-
egy, e.g. (Young et al, 2007). In a previous proof-
of-concept study (Rieser and Lemon, 2009) we
show that each of these strategies has its own
strengths and drawbacks, dependent on the partic-
ular context in which information needs to be pre-
sented to a user. Here, we will also explore pos-
sible combinations of the strategies, for example
SUMMARY followed by RECOMMEND, e.g. (Whittaker
et al, 2002), see Figure 1.
Prior work on Content or Attribute Selection
has used a ?Summarize and Refine? approach (Po-
lifroni and Walker, 2008; Polifroni and Walker,
2006; Chung, 2004). This method employs utility-
based attribute selection with respect to how each
attribute (e.g. price or food type in restaurant
1009
search) of a set of items helps to narrow down
the user?s goal to a single item. Related work ex-
plores a user modelling approach, where attributes
are ranked according to user preferences (Dem-
berg and Moore, 2006; Winterboer et al, 2007).
Our data collection (see Section 3) and training en-
vironment incorporate these approaches.
The work in this paper is the first to ap-
ply a data-driven method to this whole decision
space (i.e. combinations of Information Presenta-
tion strategies as well as attribute selection), and to
show the utility of both lower-level features (e.g.
from the NLG realiser) and higher-level features
(e.g. from Dialogue Management) for this prob-
lem. Previous work has only focused on individual
aspects of the problem (e.g. how many attributes
to generate, or when to use a SUMMARY), using a
pipeline model for SDS with DM features as input,
and where NLG has no knowledge of lower level
features (e.g. behaviour of the realiser). In Section
4.3 we show that lower level features significantly
influence users? ratings of IP strategies. In the fol-
lowing we use a Reinforcement Learning (RL) as a
statistical planning framework (Sutton and Barto,
1998) to explore the contextual features for mak-
ing these decisions, and propose a new joint opti-
misation method for IP strategies combining con-
tent structuring and attribute selection.
2 NLG as planning under uncertainty
We follow the overall framework of NLG as plan-
ning under uncertainty (Lemon, 2008; Rieser and
Lemon, 2009; Lemon, 2010), where each NLG ac-
tion is a sequential decision point, based on the
current dialogue context and the expected long-
term utility or ?reward? of the action. Other re-
cent approaches describe this task as planning, e.g.
(Koller and Petrick, 2008), or as contextual de-
cision making according to a cost function (van
Deemter, 2009), but not as a statistical planning
problem, where uncertainty in the stochastic envi-
ronment is explicitly modelled. Below, we apply
this framework to Information Presentation strate-
gies in SDS using Reinforcement Learning, where
the example task is to present a set of search results
(e.g. restaurants) to users. In particular, we con-
sider 7 possible policies for structuring the content
(see Figure 1): Recommending one single item,
comparing two items, summarising all of them,
or ordered combinations of those actions, e.g. first
summarise all the retrieved items and then recom-
mend one of them. The IP module has to decide
which action to take next, how many attributes to
mention, and when to stop generating.
Figure 1: Possible Information Presentation struc-
tures (X=stop generation)
3 Wizard-of-Oz data collection
In an initial Wizard-of-Oz (WoZ) study, we asked
humans (our ?wizards?) to produce good IP ac-
tions in different dialogue contexts, when interact-
ing in spoken dialogues with other humans (the
?users?), who believed that they were talking to an
automated SDS. The wizards were experienced re-
searchers in SDS and were familiar with the search
domain (restaurants in Edinburgh). They were in-
structed to select IP structures and attributes for
NLG so as to most efficiently allow users to find a
restaurant matching their search constraints. They
also received prior training on this task.
The task for the wizards was to decide which
IP structure to use next (see Section 3.2 for a
list of IP strategies to choose from), which at-
tributes to mention (e.g. cuisine, price range, lo-
cation, food quality, and/or service quality), and
whether to stop generating, given varying num-
bers of database matches, varying prompt reali-
sations, and varying user behaviour. Wizard ut-
terances were synthesised using a state-of-the-art
text-to-speech engine. The user speech input was
delivered to the wizard using Voice Over IP. Figure
2 shows the web-based interface for the wizard.
3.1 Experimental Setup and Data collection
We collected 213 dialogues with 18 subjects and 2
wizards (Liu et al, 2009). Each user performed a
total of 12 tasks, where no task set was seen twice
by any one wizard. The majority of users were
from a range of backgrounds in a higher educa-
tion institute, in the age range 20-30, native speak-
ers of English, and none had prior experience of
1010
Figure 2: Wizard interface. [A:] The wizard selects attribute values as specified by the user?s query. [B:] The retrieved
database items are presented in an ordered list. We use a User Modelling approach for ranking the restaurants, see e.g. (Polifroni
and Walker, 2008). [C:] The wizard then chooses which strategy and which attributes to generate next, by clicking radio buttons.
The attribute/s specified in the last user query are pre-selected by default. The strategies can only be combined in the orders as
specified in Figure 1. [D:] An utterance is automatically generated by the NLG realiser every time the wizard selects a strategy,
and is displayed in an intermediate text panel. [E:] The wizard can decide to add the generated utterance to the final output
panel or to start over again. The text in the final panel is sent to the user via TTS, once the wizard decides to stop generating.
Strategy Example utterance
SUMMARY no
UM
I found 26 restaurants, which have Indian cuisine. 11 of the restaurants are in the expensive price
range. Furthermore, 10 of the restaurants are in the cheap price range and 5 of the restaurants
are in the moderate price range.
SUMMARY UM 26 restaurants meet your query. There are 10 restaurants which serve Indian food and are in the
cheap price range. There are also 16 others which are more expensive.
COMPARE by
Item
The restaurant called Kebab Mahal is an Indian restaurant. It is in the cheap price range. And
the restaurant called Saffrani, which is also an Indian restaurant, is in the moderate price range.
COMPARE by
Attribute
The restaurant called Kebab Mahal and the restaurant called Saffrani are both Indian restaurants.
However, Kebab Mahal is in the cheap price range while Saffrani is moderately priced.
RECOMMEND The restaurant called Kebab Mahal has the best overall quality amongst the matching restau-
rants. It is an Indian restaurant, and it is in the cheap price range.
Table 1: Example realisations, generated when the user provided cuisine=Indian, and where the
wizard has also selected the additional attribute price for presentation to the user.
Spoken Dialogue Systems. After each task the
user answered a questionnaire on a 6 point Lik-
ert scale, regarding the perceived generation qual-
ity in that task. The wizards? IP strategies were
highly ranked by the users on average (4.7), and
users were able to select a restaurant in 98.6% of
the cases. No significant difference between the
wizards was observed.
The data contains 2236 utterances in total: 1465
wizard utterances and 771 user utterances. We au-
tomatically extracted 81 features (e.g #sentences,
#DBhits, #turns, #ellipsis)1 from the XML logfiles
after each dialogue. Please see (Rieser et al, 2009)
1The full corpus and list of features is available at
https://www.classic-project.org/corpora/
for more details.
3.2 NLG Realiser
In the Wizard-of-Oz environment we implemented
a NLG realiser for the chosen IP structures and
attribute choices, in order to realise the wizards?
choices in real time. This generator is based on
data from the stochastic sentence planner SPaRKy
(Stent et al, 2004). We replicated the variation ob-
served in SPaRKy by analysing high-ranking ex-
ample outputs (given the highest possible score
by the SPaRKy judges) and implemented the vari-
ance using dynamic sentence generation. The real-
isations vary in sentence aggregation, aggregation
operators (e.g. ?and?, period, or ellipsis), contrasts
1011
(e.g. ?however?, ?on the other hand?) and referring
expressions (e.g. ?it?, ?this restaurant?) used. The
length of an utterance also depends on the num-
ber of attributes chosen, i.e. the more attributes the
longer the utterance. All of these variations were
logged.
In particular, we realised the following IP strate-
gies (see examples in Table 1):
? SUMMARY of all matching restaurants with
or without a User Model (UM), following
(Polifroni and Walker, 2008). The approach
using a UM assumes that the user has cer-
tain preferences (e.g. cheap) and only tells
him about the relevant items, whereas the
approach with no UM lists all the matching
items.
? COMPARE the top 2 restaurants by Item (i.e.
listing all the attributes for the first item and
then for the other) or by Attribute (i.e. di-
rectly comparing the different attribute val-
ues).
? RECOMMEND the top-ranking restaurant (ac-
cording to UM).
Note that there was no discernible pattern in
the data about the wizards? decisions between
the UM/no UM and the byItem/byAttribute ver-
sions of the strategies. In this study we will
therefore concentrate on the higher level decisions
(SUMMARY vs. COMPARE vs. RECOMMEND) and model
these different realisations as noise in the realiser.
3.3 Supervised Baseline strategy
We analysed the WoZ data to explore the best-
rated strategies (the top scoring 50%, n = 205)
that were employed by humans for this task. Here
we used a variety of Supervised Learning meth-
ods to create a model of the highly rated wizard
behaviour. Please see (Rieser et al, 2009) for fur-
ther details. The best performing method was Rule
Induction (JRip). 2 The model achieved an accu-
racy of 43.19% which is significantly (p < .001)
better than the majority baseline of always choos-
ing SUMMARY (34.65%). 3 The resulting rule set is
shown in Figure 3.
2The WEKA implementation of (Cohen, 1995)?s RIPPER.
3Note that the low accuracy is due to data sparsity and
diverse behaviour of the wizards. However, in (Rieser et al,
2009) we show that this model is significantly different from
the policy learned using the worse scoring 50%.
IF (dbHits <= 9)& (prevNLG = summary):
THEN nlgStrategy=compare;
IF (dbHits = 1):
THEN nlgStrategy= Recommend;
IF(prevNLG=summaryRecommend)&(dbHits>=10):
THEN nlgStrategy= Recommend;
ELSE nlgStrategy=summary;
Figure 3: Rules learned by JRip for the wizard
model (?dbHits?= number of database matches,
?prevNLG?= previous NLG action)
The features selected by this model were only
?high-level? features, i.e. the input (previous ac-
tion, number of database hits) that an IP module
receives as input from a Dialogue Manager (DM).
We further analysed the importance of different
features using feature ranking and selection meth-
ods (Rieser et al, 2009), finding that the human
wizards in this specific setup did not pay signifi-
cant attention to any lower level features, e.g. from
surface realisation, although the generated output
was displayed to them (see Figure 2).
Nevertheless, note that the supervised model
achieves up to 87.6% of the possible reward on
this task, as we show in Section 5.2, and so can
be considered a serious baseline against which to
measure performance. Below, we will show that
Reinforcement Learning (RL) produces a signifi-
cant improvement over the strategies present in the
original data, especially in cases where RL has ac-
cess to ?lower level? features of the context.
4 The Simulation / Learning
Environment
Here we ?bootstrap? a simulated training environ-
ment from the WoZ data, following (Rieser and
Lemon, 2008).
4.1 User Simulations
User Simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al, 2007). A user simulation for
NLG is very similar, in that it is a predictive model
of the most likely next user act. 4 However, this
NLG predicted user act does not actually change
the overall dialogue state (e.g. by filling slots) but
it only changes the generator state. In other words,
4Similar to the internal user models applied in recent
work on POMDP (Partially Observable Markov Decision
Process) dialogue managers (Young et al, 2007; Henderson
and Lemon, 2008; Gasic et al, 2008) for estimation of user
act probabilities.
1012
the NLG user simulation tells us what the user is
most likely to do next, if we were to stop generat-
ing now.
We are most interested in the following user re-
actions:
1. select: the user chooses one of the pre-
sented items, e.g. ?Yes, I?ll take that one.?.
This reply type indicates that the Informa-
tion Presentation was sufficient for the user
to make a choice.
2. addInfo: The user provides more at-
tributes, e.g. ?I want something cheap.?. This
reply type indicates that the user has more
specific requests, which s/he wants to specify
after being presented with the current infor-
mation.
3. requestMoreInfo: The user asks for
more information, e.g. ?Can you recommend
me one??, ?What is the price range of the
last item??. This reply type indicates that the
system failed to present the information the
user was looking for.
4. askRepeat: The user asks the system to
repeat the same message again, e.g. ?Can you
repeat??. This reply type indicates that the
utterance was either too long or confusing for
the user to remember, or the TTS quality was
not good enough, or both.
5. silence: The user does not say anything.
In this case it is up to the system to take ini-
tiative.
6. hangup: The user closes the interaction.
We build user simulations using n-gram mod-
els of system (s) and user (u) acts, as first
introduced by (Eckert et al, 1997). In or-
der to account for data sparsity, we apply dif-
ferent discounting (?smoothing?) techniques in-
cluding back-off, using the CMU Statistical Lan-
guage Modelling toolkit (Clarkson and Rosen-
feld, 1997). We construct a bi-gram model5
for the users? reactions to the system?s IP struc-
ture decisions (P (au,t|IPs,t)), and a tri-gram
(i.e. IP structure + attribute choice) model for
predicting user reactions to the system?s com-
bined IP structure and attribute selection deci-
sions: P (au,t|IPs,t, attributess,t).
5Where au,t is the predicted next user action at time t,
IPs,t was the system?s Information Presentation action at t,
and attributess,t is the attributes selected by the system at t.
We evaluate the performance of these models
by measuring dialogue similarity to the original
data, based on the Kullback-Leibler (KL) diver-
gence, as also used by, e.g. (Cuaya?huitl et al,
2005; Jung et al, 2009; Janarthanam and Lemon,
2009). We compare the raw probabilities as ob-
served in the data with the probabilities generated
by our n-gram models using different discounting
techniques for each context, see table 2. All the
models have a small divergence from the origi-
nal data (especially the bi-gram model), suggest-
ing that they are reasonable simulations for train-
ing and testing NLG policies.
The absolute discounting method for the bi-
gram model is most dissimilar to the data, as is the
WittenBell method for the tri-gram model, i.e. the
models using these discounting methods have the
highest KL score. The best performing methods
(i.e. most similar to the original data), are linear
discounting for the bi-gram model and GoodTur-
ing for the tri-gram. We use the most similar user
models for system training, and the most dissimi-
lar user models for testing NLG policies, in order
to test whether the learned policies are robust and
adaptive to unseen dialogue contexts.
discounting method bi-gram US tri-gram US
WittenBell 0.086 0.512
GoodTuring 0.086 0.163
absolute 0.091 0.246
linear 0.011 0.276
Table 2: Kullback-Leibler divergence for the dif-
ferent User Simulations (US)
4.2 Database matches and ?Focus of
attention?
An important task of Information Presentation is
to support the user in choosing between all the
available items (and ultimately in selecting the
most suitable one) by structuring the current infor-
mation returned from the database, as explained in
Section 1.1. We therefore model the user?s ?fo-
cus of attention? as a feature in our learning ex-
periments. This feature reflects how the differ-
ent IP strategies structure information with dif-
ferent numbers of attributes. We implement this
shift of the user?s focus analogously to discover-
ing the user?s goal in Dialogue Management: ev-
ery time the predicted next user act is to add in-
1013
formation (addInfo), we infer that the user is
therefore only interested in a subset of the previ-
ously presented results and so the system will fo-
cus on this new subset of database items in the rest
of the generated utterance. For example, the user?s
focus after the SUMMARY (with UM) in Table 1 is
DBhits = 10, since the user is only interested in
cheap, Indian places.
4.3 Data-driven Reward function
The reward/evaluation function is constructed
from the WoZ data, using a stepwise linear regres-
sion, following the PARADISE framework (Walker
et al, 2000). This model selects the features
which significantly influenced the users? ratings
for the NLG strategy in the WoZ questionnaire.
We also assign a value to the user?s reactions
(valueUserReaction), similar to optimising task
success for DM (Young et al, 2007). This reflects
the fact that good IP strategies should help the
user to select an item (valueUserReaction =
+100) or provide more constraints addInfo
(valueUserReaction = ?0), but the user should
not do anything else (valueUserReaction =
?100). The regression in equation 1 (R2 =
.26) indicates that users? ratings are influenced by
higher level and lower level features: Users like to
be focused on a small set of database hits (where
#DBhits ranges over [1-100]), which will enable
them to choose an item (valueUserReaction),
while keeping the IP utterances short (where
#sentence is in the range [2-18]):
Reward = (?1.2)?#DBhits (1)
+(.121)? valueUserReaction
?(1.43)?#sentence
Note that the worst possible reward for an NLG
move is therefore (?1.20?100)? (.121?100)?
(18 ? 1.43) = ?157.84. This is achieved by pre-
senting 100 items to the user in 18 sentences6, in
such a way that the user ends the conversation un-
successfully. The top possible reward is achieved
in the rare cases where the system can immedi-
ately present 1 item to the user using just 2 sen-
tences, and the user then selects that item, i.e. Re-
ward =?(1.20?1)+(.121?100)?(2?1.43) =
8.06
6Note that the maximum possible number of sentences
generated by the realizer is 18 for the full IP sequence SUM-
MARY+COMPARE+RECOMMEND using all the attributes.
5 Reinforcement Learning experiments
We now formulate the problem as a Markov De-
cision Process (MDP), where states are NLG di-
alogue contexts and actions are NLG decisions.
Each state-action pair is associated with a transi-
tion probability, which is the probability of mov-
ing from state s at time t to state s? at time t+1 af-
ter having performed action a when in state s. This
transition probability is computed by the environ-
ment model (i.e. the user simulation and realiser),
and explicitly captures the uncertainty in the gen-
eration environment. This is a major difference
to other non-statistical planning approaches. Each
transition is also associated with a reinforcement
signal (or ?reward?) rt+1 describing how good the
result of action a was when performed in state s.
The aim of the MDP is to maximise long-term ex-
pected reward of its decisions, resulting in a policy
which maps each possible state to an appropriate
action in that state.
We treat IP as a hierarchical joint optimisation
problem, where first one of the IP structures (1-
3) is chosen and then the number of attributes is
decided, as shown in Figure 4. At each genera-
tion step, the MDP can choose 1-5 attributes (e.g.
cuisine, price range, location, food quality, and/or
service quality). Generation stops as soon as the
user is predicted to select an item, i.e. the IP task
is successful. (Note that the same constraint is op-
erational for the WoZ baseline.)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ACTION:
?
?IP:
?
?
?
SUMMARY
COMPARE
RECOMMEND
?
?
?
{
attr: 1-5
}
?
?
STATE:
?
?
?
?
?
?
?
?
?
?
?
attributes:
{
1-15
}
sentence:
{
2-18
}
dbHitsFocus:
{
1-100
}
userSelect:
{
0,1
}
userAddInfo:
{
0,1
}
userElse:
{
0,1
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 4: State-Action space for the RL-NLG
problem
States are represented as sets of NLG dia-
logue context features. The state space comprises
?lower-level? features about the realiser behaviour
(two discrete features representing the number of
attributes and sentences generated so far) and three
binary features representing the user?s predicted
next action, as well as ?high-level? features pro-
1014
vided by the DM (e.g. current database hits in the
user?s focus (dbHitsFocus)). We trained the
policy using the SHARSHA algorithm (Shapiro and
Langley, 2002) with linear function approximation
(Sutton and Barto, 1998), and the simulation envi-
ronment described in Section 4. The policy was
trained for 60,000 iterations.
5.1 Experimental Set-up
We compare the learned strategies against the WoZ
baseline as described in Section 3.3. For attribute
selection we choose a majority baseline (randomly
choosing between 3 or 4 attributes) since the at-
tribute selection models learned by Supervised
Learning on the WoZ data didn?t show significant
improvements.
For training, we used the user simulation model
most similar to the data, see Section 4.1. For
testing, we test with the different user simulation
model (the one which is most dissimilar to the
data).
We first investigate how well IP structure (with-
out attribute choice) can be learned in increas-
ingly complex generation scenarios. A genera-
tion scenario is a combination of a particular kind
of NLG realiser (template vs. stochastic) along
with different levels of variation introduced by cer-
tain features of the dialogue context. In general,
the stochastic realiser introduces more variation
in lower level features than the template-based re-
aliser. The Focus model introduces more varia-
tion with respect to #DBhits and #attributes as de-
scribed in Section 4.2. We therefore investigate
the following cases:
1.1. IP structure choice, Template realiser:
Predicted next user action varies according to
the bi-gram model (P (au,t|IPs,t)); Number
of sentences and attributes per IP strategy is
set by defaults, reflecting a template-based
realiser.
1.2. IP structure choice, Stochastic realiser:
IP structure where number of attributes per
NLG turn is given at the beginning of each
episode (e.g. set by the DM); Sentence gen-
eration according to the SPaRKy stochastic
realiser model as described in Section 3.2.
We then investigate different scenarios for
jointly optimising IP structure (IPS) and attribute
selection (Attr) decisions.
2.1. IPS+Attr choice, Template realiser:
Predicted next user action varies according
to tri-gram (P (au,t|IPs,t, attributess,t))
model; Number of sentences per IP structure
set to default.
2.2. IPS+Attr choice, Template realiser+Focus model:
Tri-gram user simulation with Template re-
aliser and Focus of attention model with
respect to #DBhits and #attributes as
described in section 4.2.
2.3. IPS+Attr choice, Stochastic realiser: Tri-
gram user simulation with sentence/attribute
relationship according to Stochastic realiser
as described in Section 3.2.
2.4. IPS+Attr choice, Stochastic realiser+Focus:
i.e. the full model = Predicted next user ac-
tion varies according to tri-gram model+
Focus of attention model + Sentence/attribute
relationship according to stochastic realiser.
5.2 Results
We compare the average final reward (see Equa-
tion 1) gained by the baseline against the trained
RL policies in the different scenarios for each
1000 test runs, using a paired samples t-test. The
results are shown in Table 3. In 5 out of 6 scenar-
ios the RL policy significantly (p < .001) outper-
forms the supervised baseline. We also report on
the percentage of the top possible reward gained
by the individual policies, and the raw percentage
improvement of the RL policy. Note that the best
possible (100%) reward can only be gained in rare
cases (see Section 4.3).
The learned RL policies show that lower level
features are important in gaining significant im-
provements over the baseline. The more complex
the scenario, the harder it is to gain higher rewards
for the policies in general (as more variation is in-
troduced), but the relative improvement in rewards
also increases with complexity: the baseline does
not adapt well to the variations in lower level fea-
tures whereas RL learns to adapt to the more chal-
lenging scenarios. 7
An overview of the range of different IP strate-
gies learned for each setup can be found in Table 4.
Note that these strategies are context-dependent:
the learner chooses how to proceed dependent on
7Note, that the baseline does reasonably well in scenarios
with variation introduced by only higher level features (e.g.
scenario 2.2).
1015
Scenario
Wizard Baseline
average Reward
RL average Reward
RL % - Baseline %
= % improvement
1.1 -15.82(?15.53) -9.90***(?15.38) 89.2% - 85.6%= 3.6%
1.2 -19.83(?17.59) -12.83***(?16.88) 87.4% - 83.2%= 4.2%
2.1 -12.53(?16.31) -6.03***(?11.89) 91.5% - 87.6%= 3.9%
2.2 -14.15(?16.60) -14.18(?18.04) 86.6% - 86.6%= 0.0%
2.3 -17.43(?15.87) -9.66***(?14.44) 89.3% - 84.6%= 4.7%
2.4 -19.59(?17.75) -12.78***(?15.83) 87.4% - 83.3%= 4.1%
Table 3: Test results for 1000 dialogues, where *** denotes that the RL policy is significantly (p < .001)
better than the Baseline policy.
the features in the state space at each generation
step.
Scenario strategies learned
1.1
RECOMMEND
COMPARE
COMPARE+RECOMMEND
SUMMARY
SUMMARY+COMPARE
SUMMARY+RECOMMEND
SUMMARY+COMPARE+RECOMMEND.
1.2
RECOMMEND
COMPARE
COMPARE+RECOMMEND
SUMMARY
SUMMARY+COMPARE
SUMMARY+RECOMMEND
SUMMARY+COMPARE+RECOMMEND.
2.1
RECOMMEND(5)
SUMMARY(2)
SUMMARY(2)+COMPARE(4)
SUMMARY(2)+COMPARE(1)
SUMMARY(2)+COMPARE(4)+RECOMMEND(5)
SUMMARY(2)+COMPARE(1)+RECOMMEND(5)
2.2
RECOMMEND(5)
SUMMARY(4)
SUMMARY(4)+RECOMMEND(5)
2.3
RECOMMEND(2)
SUMMARY(1)
SUMMARY(1)+COMPARE(4)
SUMMARY(1)+COMPARE(1)
SUMMARY(1)+COMPARE(4)+RECOMMEND(2)
2.4
RECOMMEND(2)
SUMMARY(2)
SUMMARY(2)+COMPARE(4)
SUMMARY(2)+RECOMMEND(2)
SUMMARY(2)+COMPARE(4)+RECOMMEND(2)
SUMMARY(2)+COMPARE(1)+RECOMMEND(2)
Table 4: RL strategies learned for the different sce-
narios, where (n) denotes the number of attributes
generated.
For example, the RL policy for scenario 1.1
learned to start with a SUMMARY if the initial num-
ber of items returned from the database is high
(>30). It will then stop generating if the user is
predicted to select an item. Otherwise, it contin-
ues with a RECOMMEND. If the number of database
items is low, it will start with a COMPARE and then
continue with a RECOMMEND, unless the user selects
an item. Also see Table 4. Note that the WoZ strat-
egy behaves as described in Figure 3.
In addition, the RL policy for scenario 1.2
learns to adapt to a more complex scenario:
the number of attributes requested by the DM
and produced by the stochastic sentence re-
aliser. It learns to generate the whole sequence
(SUMMARY+COMPARE+RECOMMEND) if #attributes is
low (<3), because the overall generated utterance
(final #sentences) is still relatively short. Other-
wise the policy is similar to the one for scenario
1.1.
The RL policies for jointly optimising IP strat-
egy and attribute selection learn to select the num-
ber of attributes according to the generation sce-
narios 2.1-2.4. For example, the RL policy learned
for scenario 2.1 generates a RECOMMEND with 5 at-
tributes if the database hits are low (<13). Oth-
erwise, it will start with a SUMMARY using 2 at-
tributes. If the user is predicted to narrow down
his focus after the SUMMARY, the policy continues
with a COMPARE using 1 attribute only, otherwise it
helps the user by presenting 4 attributes. It then
continues with RECOMMEND(5), and stops as soon
as the user is predicted to select one item.
The learned policy for scenario 2.1 generates
5.85 attributes per NLG turn on average (i.e. the
cumulative number of attributes generated in the
whole NLG sequence, where the same attribute
may be repeated within the sequence). This strat-
egy primarily adapts to the variations from the user
simulation (tri-gram model). For scenario 2.2 the
average number of attributes is higher (7.15) since
the number of attributes helps to narrow down the
user?s focus via the DBhits/attribute relationship
specified in section 4.2. For scenario 2.3 fewer
attributes are generated on average (3.14), since
here the number of attributes influences the sen-
tence realiser, i.e. fewer attributes results in fewer
sentences, but does not impact the user?s focus.
In scenario 2.4 all the conditions mentioned above
influence the learned policy. The average number
of attributes selected is still low (3.19).
In comparison, the average (cumulative) num-
1016
ber of attributes for the WoZ baseline is 7.10. The
WoZ baseline generates all the possible IP struc-
tures (with 3 or 4 attributes) but is restricted to use
only ?high-level? features (see Figure 3). By beat-
ing this baseline we show the importance of the
?lower-level? features. Nevertheless, this wizard
policy achieves up to 87.6% of the possible reward
on this task, and so can be considered a serious
baseline against which to measure performance.
The only case (scenario 2.2) where RL does not
improve significantly over the baseline is where
lower level features do not play an important role
for learning good strategies: scenario 2.2 is only
sensitive to higher level features (DBhits).
6 Conclusion
We have presented a new data-driven method for
Information Presentation (IP) in Spoken Dialogue
Systems using a statistical optimisation frame-
work for content structure planning and attribute
selection. This work is the first to apply a data-
driven optimisation method to the IP decision
space, and to show the utility of both lower-level
and higher-level features for this problem.
We collected data in a Wizard-of-Oz (WoZ)
experiment and showed that human ?wizards?
mostly pay attention to ?high-level? features from
Dialogue Management. The WoZ data was used
to build statistical models of user reactions to
IP strategies, and a data-driven reward function
for Reinforcement Learning (RL). We show that
lower level features significantly influence users?
ratings of IP strategies. We compared a model of
human behaviour (the ?human wizard baseline?)
against policies optimised using Reinforcement
Learning, in a variety of scenarios. Our optimised
policies significantly outperform the IP structuring
and attribute selection present in the WoZ data, es-
pecially when performing in complex generation
scenarios which require adaptation to, e.g. number
of database results, utterance length, etc. While
the human wizards were able to attain up to 87.6%
of the possible reward on this task, the RL poli-
cies are significantly better in 5 out of 6 scenarios,
gaining up to 91.5% of the total possible reward.
We have also shown that adding predictive
?lower level? features, e.g. from the NLG realiser
and a user reaction model, is important for learn-
ing optimal IP strategies according to user pref-
erences. Future work could include the predicted
TTS quality (Boidin et al, 2009) as a feature.
We are now working on testing the learned poli-
cies with real users, outside of laboratory condi-
tions, using a restaurant-guide SDS, deployed as a
VOIP service. Previous work in SDS has shown
that results for Dialogue Management obtained
with simulated users are able to transfer to eval-
uations with real users (Lemon et al, 2006).
This methodology provides new insights into
the nature of the IP problem, which has previously
been treated as a module following dialogue man-
agement with no access to lower-level context fea-
tures. The data-driven planning method applied
here promises a significant upgrade in the perfor-
mance of generation modules, and thereby of Spo-
ken Dialogue Systems in general.
Acknowledgments
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
Cedric Boidin, Verena Rieser, Lonneke van der Plas,
Oliver Lemon, and Jonathan Chevelu. 2009. Pre-
dicting how it sounds: Re-ranking alternative in-
puts to TTS using latent variables (forthcoming). In
Proc. of Interspeech/ICSLP, Special Session on Ma-
chine Learning for Adaptivity in Spoken Dialogue
Systems.
Grace Chung. 2004. Developing a flexible spoken dia-
log system using simulation. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
P.R. Clarkson and R. Rosenfeld. 1997. Statisti-
cal Language Modeling Using the CMU-Cambridge
Toolkit. In Proc. of ESCA Eurospeech.
William W. Cohen. 1995. Fast effective rule induction.
In Proceedings of the 12th International Conference
on Machine Learning (ICML).
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
Vera Demberg and Johanna D. Moore. 2006. Infor-
mation presentation in spoken dialogue systems. In
Proceedings of EACL.
1017
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and S. Young. 2008. Training and
Evaluation of the HIS POMDP Dialogue System in
Noise. In Proc. of SIGdial Workshop on Discourse
and Dialogue.
James Henderson and Oliver Lemon. 2008. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proc. of ACL.
Srinivasan Janarthanam and Oliver Lemon. 2009. A
Two-tier User Simulation Model for Reinforcement
Learning of Adaptive Referring Expression Genera-
tion Policies. In Proc. of SIGdial.
Srini Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: Referring expression
generation in spoken dialogue systems. In Proceed-
ings of ACL.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Computer, Speech & Lan-
guage, 23:479?509.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating Effectiveness and Portabil-
ity of Reinforcement Learned Dialogue Strategies
with real users: the TALK TownInfo Evaluation. In
IEEE/ACL Spoken Language Technology.
Oliver Lemon. 2008. Adaptive Natural Language
Generation in Dialogue using Reinforcement Learn-
ing. In Proceedings of SEMdial.
Oliver Lemon. 2010. Learning what to say and how to
say it: joint optimization of spoken dialogue man-
agement and Natural Language Generation. Com-
puter, Speech & Language, to appear.
Xingkun Liu, Verena Rieser, and Oliver Lemon. 2009.
A wizard-of-oz interface to study information pre-
sentation strategies for spoken dialogue systems. In
Proc. of the 1st International Workshop on Spoken
Dialogue Systems.
Crystal Nakatsu. 2008. Learning contrastive connec-
tives in sentence realization ranking. In Proc. of
SIGdial Workshop on Discourse and Dialogue.
Joseph Polifroni and Marilyn Walker. 2006. Learning
database content for spoken dialogue system design.
In Proc. of the IEEE/ACL workshop on Spoken Lan-
guage Technology (SLT).
Joseph Polifroni and Marilyn Walker. 2008. Inten-
sional Summaries as Cooperative Responses in Di-
alogue Automation and Evaluation. In Proceedings
of ACL.
Verena Rieser and Oliver Lemon. 2008. Learn-
ing Effective Multimodal Dialogue Strategies from
Wizard-of-Oz data: Bootstrapping and Evaluation.
In Proc. of ACL.
Verena Rieser and Oliver Lemon. 2009. Natural Lan-
guage Generation as Planning Under Uncertainty for
Spoken Dialogue Systems. In Proc. of EACL.
Verena Rieser, Xingkun Liu, and Oliver Lemon. 2009.
Optimal Wizard NLG Behaviours in Context. Tech-
nical report, Deliverable 4.2, CLASSiC Project.
Dan Shapiro and P. Langley. 2002. Separating skills
from preference: Using learning to program by re-
ward. In Proc. of the 19th International Conference
on Machine Learning (ICML).
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
Kees van Deemter. 2009. What game theory can do
for NLG: the case of vague language. In 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG).
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards developing general mod-
els of usability with PARADISE. Natural Language
Engineering, 6(3).
M. Walker, R. Passonneau, and J. Boland. 2001. Quan-
titative and qualitative evaluation of DARPA Com-
municator spoken dialogue systems. In Proc. of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413?456.
Steve Whittaker, Marilyn Walker, and Johanna Moore.
2002. Fish or Fowl: A Wizard of Oz evaluation
of dialogue strategies in the restaurant domain. In
Proc. of the International Conference on Language
Resources and Evaluation (LREC).
Andi Winterboer, Jiang Hu, Johanna D. Moore, and
Clifford Nass. 2007. The influence of user tailoring
and cognitive load on user performance in spoken
dialogue systems. In Proc. of the 10th International
Conference of Spoken Language Processing (Inter-
speech/ICSLP).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
1018
Generation under Uncertainty
Oliver Lemon
Heriot-Watt University
Edinburgh, United Kingdom
o.lemon@hw.ac.uk
Srini Janarthanam
Edinburgh University
Edinburgh, United Kingdom
s.janarthanam@ed.ac.uk
Verena Rieser
Edinburgh University
Edinburgh, United Kingdom
vrieser@inf.ed.ac.uk
Abstract
We invite the research community to con-
sider challenges for NLG which arise from
uncertainty. NLG systems should be able
to adapt to their audience and the genera-
tion environment in general, but often the
important features for adaptation are not
known precisely. We explore generation
challenges which could employ simulated
environments to study NLGwhich is adap-
tive under uncertainty, and suggest possi-
ble metrics for such tasks. It would be par-
ticularly interesting to explore how differ-
ent planning approaches to NLG perform
in challenges involving uncertainty in the
generation environment.
1 Introduction
We would like to highlight the design of NLG sys-
tems for environments where there may be incom-
plete or faulty information, where actions may not
always have the same results, and where there may
be tradeoffs between the different possible out-
comes of actions and plans.
There are various sources of uncertainty in sys-
tems which employ NLG techniques, for example:
? the current state of the user / audience (e.g.
their knowledge, preferred vocabulary, goals,
preferences....),
? the likely user reaction to the generated out-
put,
? the behaviour of related components (e.g. a
surface realiser, or TTS module),
? noise in the environment (for spoken output),
? ambiguity of the generated output.
The problem here is to generate output that
takes these types of uncertainty into account ap-
propriately. For example, you may need to choose
a referring expression for a user, even though you
are not sure whether they are an expert or novice in
the domain. In addition, the next time you speak
to that user, you need to adapt to new informa-
tion you have gained about them (Janarthanam and
Lemon, 2010). The issue of uncertainty for refer-
ring expression generation has been discussed be-
fore by (Reiter, 1991; Horacek, 2005).
Another example is in planning an Information
Presentation for a user, when you cannot know
with certainty how they will respond to it (Rieser
and Lemon, 2009; Rieser et al, 2010). In the worst
case, you may even be uncertain about the user?s
goals or information needs (as in ?POMDP? ap-
proaches to dialogue management (Young et al,
2009; Henderson and Lemon, 2008a)), but you
still need to generate output for them in an appro-
priate way.
In particular, in interactive applications of NLG:
? each NLG action changes the environment
state or context,
? the effect of each NLG action is uncertain.
Several recent approaches describe NLG tasks
as different kinds of planning, e.g. (Koller and Pet-
rick, 2008; Rieser et al, 2010; Janarthanam and
Lemon, 2010), or as contextual decision making
according to a cost function (van Deemter, 2009).
It would be very interesting to explore how differ-
ent approaches perform in NLG problems where
different types of uncertainty are present in the
generation environment.
In the following we discuss possible genera-
tion challenges arising from such considerations,
which we hope will lead to work on an agreed
shared challenge in this research community. In
section 2 we briefly review recent work showing
that simulated environments can be used to evalu-
ate generation under uncertainty, and in section 3
we discuss some possible metrics for such tasks.
Section 4 concludes by considering how a useful
generation challenge could be constructed using
similar methods.
2 Generation in Uncertain Simulated
Environments
Finding the best (or ?optimal?) way to generate
under uncertainty requires exploring the possible
outcomes of actions in stochastic environments.
Therefore, related research on Dialogue Strategy
learning has used data-driven simulated environ-
ments as a cheap and efficient way to explore un-
certainty (Lemon and Pietquin, 2007). However,
building good simulated environments is a chal-
lenge in its own right, as we illustrate in the fol-
lowing using the examples of Information Presen-
tation and Referring Expression Generation. We
also point out the additional challenges these sim-
ulations have to face when being used for NLG.
2.1 User Simulations for Information
Presentation
User Simulations can provide a model of proba-
ble, but uncertain, user reactions to NLG actions,
and we propose that they are a useful potential
direction for exploring and evaluate different ap-
proaches to handling uncertainty in generation.
User Simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al, 2007). A user simulation for
Information Presentation is very similar, in that it
is a predictive model of the most likely next user
act. 1 However, this NLG predicted user act does
not actually change the overall dialogue state (e.g.
by filling slots) but it only changes the generator
state. In other words, this NLG user simulation
tells us what the user is most likely to do next, if
we were to stop generating now.
In addition to the challenges of building user
simulations for learning Dialogue policies, e.g.
modelling, evaluation, and available data sets
(Lemon and Pietquin, 2007), a crucial decision for
NLG is the level of detail needed to train sensible
1Similar to the internal user models applied in recent
work on POMDP (Partially Observable Markov Decision
Process) dialogue managers (Young et al, 2007; Henderson
and Lemon, 2008b; Gasic et al, 2008) for estimation of user
act probabilities.
policies. While high-level dialogue act descrip-
tions may be sufficient for dialogue policies, NLG
decisions may require a much finer level of detail.
The finer the required detail of user reactions, the
more data is needed to build data-driven simula-
tions.
For content selection in Information Presen-
tation tasks (choosing presentation strategy and
number of attributes), for example, the level of de-
scription can still be fairly abstract. We were most
interested in probability distributions over the fol-
lowing possible user reactions:
1. select: the user chooses one of the pre-
sented items, e.g. ?Yes, I?ll take that one.?.
This reply type indicates that the informa-
tion presentation was sufficient for the user
to make a choice.
2. addInfo: The user provides more at-
tributes, e.g. ?I want something cheap.?. This
reply type indicates that the user has more
specific requests, which s/he wants to specify
after being presented with the current infor-
mation.
3. requestMoreInfo: The user asks for
more information, e.g. ?Can you recommend
me one??, ?What is the price range of the
last item??. This reply type indicates that the
system failed to present the information the
user was looking for.
4. askRepeat: The user asks the system to
repeat the samemessage again, e.g. ?Can you
repeat??. This reply type indicates that the
utterance was either too long or confusing for
the user to remember, or the TTS quality was
not good enough, or both.
5. silence: The user does not say anything.
In this case it is up to the system to take ini-
tiative.
6. hangup: The user closes the interaction.
We have built user simulations using n-gram
models of system (s) and user (u) acts, as first
introduced by (Eckert et al, 1997). In order to
account for data sparsity, we apply different dis-
counting (?smoothing?) techniques including au-
tomatic back-off, using the CMU Statistical Lan-
guage Modelling toolkit (Clarkson and Rosenfeld,
1997). For example we have constructed a bi-
gram model2 for the users? reactions to the sys-
tem?s IP structure decisions (P (au,t|IPs,t)), and
a tri-gram (i.e. IP structure + attribute choice)
model for predicting user reactions to the system?s
combined IP structure and attribute selection deci-
sions: P (au,t|IPs,t, attributess,t).
We have evaluated the performance of these
models by measuring dialogue similarity to the
original data, based on the Kullback-Leibler (KL)
divergence, as also used by e.g. (Cuaya?huitl et al,
2005; Jung et al, 2009; Janarthanam and Lemon,
2009). We compared the raw probabilities as ob-
served in the data with the probabilities generated
by our n-gram models using different discounting
techniques for each context. All the models have a
small divergence from the original data (especially
the bi-gram model), suggesting that they are rea-
sonable simulations for training and testing NLG
policies (Rieser et al, 2010).
2.2 Other Simulated Components
In some systems, NLG decisions may also depend
on related components, such as the database, sub-
sequent generation steps, or the Text-to-Speech
module for spoken generation. Building simula-
tions for these components to capture their inher-
ent uncertainty, again, is an interesting challenge.
For example, one might want to adapt the gen-
erated output according to the predicted TTS qual-
ity. Therefore, one needs a model of the expected/
predicted TTS quality for a TTS engine (Boidin et
al., 2009).
Furthermore, NLG decisions might be inputs
to a stochastic sentence realiser, such as SPaRKy
(Stent et al, 2004). However, one might not have
a fully trained stochastic sentence realiser for this
domain (yet). In (Rieser et al, 2010) we therefore
modelled the variance as observed in the top rank-
ing SPaRKy examples.
2.3 Generating Referring Expressions under
uncertainty
In this section, we present an example user simu-
lation (US) model, that simulates the dialogue be-
haviour of users who react to referring expressions
depending on their domain knowledge. These ex-
ternal simulation models are different from inter-
nal user models used by dialogue systems. In
2Where au,t is the predicted next user action at time t,
IPs,t was the system?s Information Presentation action at t,
and attributess,t is the set of attributes selected by the sys-
tem at t.
particular, such models must be sensitive to a
system?s choices of referring expressions. The
simulation has a statistical distribution of in-built
knowledge profiles that determines the dialogue
behaviour of the user being simulated. Uncer-
tainty arises because if the user does not know a
referring expression, then he is more likely to re-
quest clarification. If the user is able to interpret
the referring expressions and identify the refer-
ences then he is more likely to follow the system?s
instruction. This behaviour is simulated by the ac-
tion selection models described below.
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). We used two kinds of action selection mod-
els: a corpus-driven statistical model and a hand-
coded rule-based model.
2.4 Corpus-driven action selection model
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). The US produces a clarification request cr
based on the class of the referent C(Ri), type of
the referring expression Ti, and the current domain
knowledge of the user for the referring expression
DKu,t(Ri, Ti). Domain entities whose jargon ex-
pressions raised clarification requests in the cor-
pus were listed and those that had more than the
mean number of clarification requests were clas-
sified as difficult and others as easy enti-
ties (for example, ?power adaptor? is easy - all
users understood this expression, ?broadband fil-
ter? is difficult). Clarification requests are
produced using the following model.
P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) ? RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment actionEAu,t
is generated using the following model.
P (EAu,t|As,t) if Au,t! = cr(Ri, Ti)
Finally, the user action is an instruction re-
sponse which is determined by the system ac-
tion As,t. Instruction responses can be ei-
ther provide info, acknowledgement or other
based on the system?s instruction.
P (Au,t = ir|EAu,t, As,t)
All the above models were trained on our cor-
pus data using maximum likelihood estimation
and smoothed using a variant of Witten-Bell dis-
counting. According to the data, clarification re-
quests are much more likely when jargon expres-
sions are used to refer to the referents that be-
long to the difficult class and which the user
doesn?t know about. When the system uses ex-
pressions that the user knows, the user gener-
ally responds to the instruction given by the sys-
tem. These user simulation models have been
evaluated and found to produce behaviour that is
very similar to the original corpus data, using the
Kullback-Leibler divergence metric (Janarthanam
and Lemon, 2010).
3 Metrics
Here we discuss some possible evaluation met-
rics that will allow different approaches to NLG
under uncertainty to be compared. We envisage
that other metrics should be explored, in particular
those measuring adaptivity of various types.
3.1 Adaptive Information Presentation
Given a suitable corpus, a data-driven evaluation
function can be constructed, using a stepwise lin-
ear regression, following the PARADISE frame-
work (Walker et al, 2000).
For example, in (Rieser et al, 2010) we
build a model which selects the features which
significantly influenced the users? ratings for
NLG strategies in a Wizard-of-Oz study. We
also assign a value to the user?s reactions
(valueUserReaction), similar to optimising task
success for DM (Young et al, 2007). This re-
flects the fact that good Information Presentation
strategies should help the user to select an item
(valueUserReaction = +100) or provide more
constraints addInfo (valueUserReaction =
?0), but the user should not do anything else
(valueUserReaction = ?100). The regression
in equation 1 (R2 = .26) indicates that users? rat-
ings are influenced by higher level and lower level
features: Users like to be focused on a small set
of database hits (where #DBhits ranges over [1-
100]), which will enable them to choose an item
(valueUserReaction), while keeping the IP ut-
terances short (where #sentence was in the range
[2-18]):
Reward = (?1.2)?#DBhits (1)
+(.121)? valueUserReaction
?(1.43)?#sentence
3.2 Measuring Adaptivity of Referring
Expressions
We have also designed a metric for the goal of
adapting referring expressions to each user?s do-
main knowledge. We present the Adaptation Ac-
curacy score AA that calculates how accurately
the agent chose the expressions for each referent
r, with respect to the user?s knowledge. Appro-
priateness of an expression is based on the user?s
knowledge of the expression. So, when the user
knows the jargon expression for r, the appropri-
ate expression to use is jargon, and if s/he doesn?t
know the jargon, an descriptive expression is ap-
propriate. Although the user?s domain knowledge
is dynamically changing due to learning, we base
appropriateness on the initial state, because our
objective is to adapt to the initial state of the user
DKu,initial. However, in reality, designers might
want their system to account for user?s changing
knowledge as well. We calculate accuracy per ref-
erent RAr as the ratio of number of appropriate
expressions to the total number of instances of the
referent in the dialogue. We then calculate the
overall mean accuracy over all referents as shown
below.
RAr =
#(appropriate expressions(r))
#(instances(r))
AdaptationAccuracyAA = 1#(r)?rRAr
4 Conclusion
We have invited the research community to con-
sider challenges for NLG which arise from uncer-
tainty. We argue that NLG systems, like dialogue
managers, should be able to adapt to their audi-
ence and the generation environment. However,
often the important features for adaptation are not
precisely known. We then summarised 2 potential
directions for such challenges ? example genera-
tion tasks which employ simulated uncertain en-
vironments to study adaptive NLG, and discussed
some possible metrics for such tasks. We hope
that this will lead to discussions on a shared chal-
lenge allowing comparison of different approaches
to NLG with respect to how well they handle un-
certainty.
Acknowledgments
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
Cedric Boidin, Verena Rieser, Lonneke van der Plas,
Oliver Lemon, and Jonathan Chevelu. 2009. Pre-
dicting how it sounds: Re-ranking alternative in-
puts to TTS using latent variables (forthcoming). In
Proc. of Interspeech/ICSLP, Special Session on Ma-
chine Learning for Adaptivity in Spoken Dialogue
Systems.
P.R. Clarkson and R. Rosenfeld. 1997. Statisti-
cal Language Modeling Using the CMU-Cambridge
Toolkit. In Proc. of ESCA Eurospeech.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and S. Young. 2008. Training and
Evaluation of the HIS POMDP Dialogue System in
Noise. In Proc. of SIGdial Workshop on Discourse
and Dialogue.
James Henderson and Oliver Lemon. 2008a. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proceedings of
ACL.
James Henderson and Oliver Lemon. 2008b. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proc. of ACL.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In ENLG.
Srinivasan Janarthanam and Oliver Lemon. 2009. A
Two-tier User Simulation Model for Reinforcement
Learning of Adaptive Referring Expression Genera-
tion Policies. In Proc. of SIGdial.
Srini Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: Referring expression
generation in spoken dialogue systems. In Proceed-
ings of ACL. (to appear).
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Computer, Speech & Lan-
guage, 23:479?509.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Oliver Lemon and Olivier Pietquin. 2007. Machine
learning for spoken dialogue systems. In Inter-
speech.
E. Reiter. 1991. Generating Descriptions that Exploit a
User?s Domain Knowledge. In R. Dale, C. Mellish,
and M. Zock, editors, Current Research in Natural
Language Generation, pages 257?285. Academic
Press.
Verena Rieser and Oliver Lemon. 2009. Natural lan-
guage generation as planning under uncertainty for
spoken dialogue systems. In EACL.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proceedings of ACL. (to appear).
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
Kees van Deemter. 2009. What game theory can do
for NLG: the case of vague language. In 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG).
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2009. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language. To appear.
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 49?58,
Utica, May 2012. c?2012 Association for Computational Linguistics
Optimising Incremental Generation for Spoken Dialogue Systems:
Reducing the Need for Fillers
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
Heriot Watt University
EH14 4AS, Edinburgh
n.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.uk
Abstract
Recent studies have shown that incremental
systems are perceived as more reactive, nat-
ural, and easier to use than non-incremental
systems. However, previous work on incre-
mental NLG has not employed recent ad-
vances in statistical optimisation using ma-
chine learning. This paper combines the two
approaches, showing how the update, revoke
and purge operations typically used in in-
cremental approaches can be implemented as
state transitions in a Markov Decision Process.
We design a model of incremental NLG that
generates output based on micro-turn inter-
pretations of the user?s utterances and is able
to optimise its decisions using statistical ma-
chine learning. We present a proof-of-concept
study in the domain of Information Presen-
tation (IP), where a learning agent faces the
trade-off of whether to present information as
soon as it is available (for high reactiveness)
or else to wait until input ASR hypotheses are
more reliable. Results show that the agent
learns to avoid long waiting times, fillers and
self-corrections, by re-ordering content based
on its confidence.
1 Introduction
Traditionally, the smallest unit of speech processing
for interactive systems has been a full utterance with
strict, rigid turn-taking. Components of these inter-
active systems, including NLG systems, have so far
treated the utterance as the smallest processing unit
that triggers a module into action. More recently,
work on incremental systems has shown that pro-
cessing smaller ?chunks? of user input can improve
the user experience (Skantze and Schlangen, 2009;
Buss et al, 2010; Skantze and Hjalmarsson, 2010;
Baumann et al, 2011). Incrementality in NLG sys-
tems enables the system designer to model several
dialogue phenomena that play a vital role in hu-
man discourse (Levelt, 1989) but have so far been
absent from NLG systems. These include more
natural turn-taking through rapid system responses,
grounding through the generation of backchannels
and feedback, and barge-ins (from both user and sys-
tem). In addition, corrections and self-corrections
through constant monitoring of user and system ut-
terances play an important role, enabling the system
to recover smoothly from a recognition error or a
change in the user?s preferences. Some examples of
the phenomena we are targeting are given in Fig. 1.
Skantze and Hjalmarsson (2010) present a model
of incremental speech generation in which input pro-
cessing and output planning are parallel processes
and the system can self-monitor its own genera-
tion process. In an evaluation with human users
they showed that their incremental system started
to speak significantly faster than a non-incremental
system (roughly 600 ms) and was perceived as sig-
nificantly more polite and efficient. Users also in-
dicated that they knew better when to start speaking
themselves. Alternative approaches to incremental
NLG include Kilger and Finkler (1995) who present
an early approach based on Tree-Adjoining Gram-
mar, and Purver and Otsuka (2003) who define an
incremental generator based on Dynamic Syntax.
Both of these generators can monitor their own out-
put and initiate corrections if necessary.
Over recent years, adaptive and data-driven ap-
49
Self-correction (the system made a mistake)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 Indian restaurants . . .
USR No, I want Italian.
SYS oh sorry . . .
SYS I have 24 Italian restaurants in the city centre . . .
Correction (the user changed their mind)
USR I want Indian food in the centre of town . . .
SYS There are 35 Indian . . .
USR Oh sorry, I meant Italian.
SYS OK, Italian . . .
SYS I have 24 Italian restaurants . . .
Re-ordering (from high to low confidence)
USR I want Italian food . . .
SYS [waits]
USR in the city centre.
SYS I have 120 places in the city centre . . .
USR I love Italian . . .
SYS 24 of them Italian . . .
Holding the floor
USR I want cheap Italian food . . .
SYS ok let me see
SYS I have 3 cheap Italian places . . .
Figure 1: Example phenomena generated with the trained
policy. The agent has learnt to produce backchannels
when the user pauses, monitor and (self-)correct its out-
put, and present information according to its confidence.
proaches to NLG have also been developed and
shown to outperform the previous (handcrafted,
rule-based) methods for specific problems (Rieser et
al., 2010; Janarthanam and Lemon, 2010; Dethlefs
and Cuaya?huitl, 2011). This work has established
that NLG can fruitfully be treated as a data-driven
statistical planning process, where the objective is
to maximise expected utility of the generated utter-
ances (van Deemter, 2009), by adapting them to the
context and user. Statistical approaches to sentence
planning and surface realisation have also been ex-
plored (Stent et al, 2004; Belz, 2008; Mairesse et
al., 2010; Angeli et al, 2010). The advantages of
data-driven methods are that NLG is more robust in
the face of noise, can adapt to various contexts and,
trained on real data, can produce more natural and
desirable variation in system utterances.
This paper describes an initial investigation into a
novel NLG architecture that combines incremental
processing with statistical optimisation. In order to
move away from conventional strict-turn taking, we
have to be able to model the complex interactions
observed in human-human conversation. Doing this
in a deterministic fashion through hand-written rules
would be time consuming and potentially inaccu-
rate, with no guarantee of optimality. In this paper,
we demonstrate that it is possible to learn incremen-
tal generation behaviour in a reward-driven fashion.
2 Previous Work: Incremental Processing
Architectures
The smallest unit of processing in incremental sys-
tems is called incremental unit (IU). Its instantia-
tion depends on the particular processing module. In
speech recognition, IUs can correspond to phoneme
sequences that are mapped onto words (Baumann
and Schlangen, 2011). In dialogue management, IUs
can correspond to dialogue acts (Buss et al, 2010).
In speech synthesis, IUs can correspond to speech
unit sequences which are mapped to segments and
speech plans (Skantze and Hjalmarsson, 2010). IUs
are typically linked to other IUs by two types of rela-
tions: same-level links connect IUs sequentially and
express relationships at the same level; grounded-in
links express hierarchical relations between IUs.
2.1 Buffer-Based Incremental Processing
A general abstract model of incremental process-
ing based on buffers and a processor was devel-
oped by Schlangen and Skantze (2009) and is illus-
trated in Figure 2. It assumes that the left buffer
of a module, such as the NLG module, receives
IUs from one or more other processing modules,
such as the dialogue manager. These input IUs are
then passed on to the processor, where they are
mapped to corresponding (higher-level) IUs. For
an NLG module, this could be a mapping from the
dialogue act present(cuisine=Indian) to the realisa-
tion ?they serve Indian food?. The resulting IUs are
passed on to the right buffer which co-incides with
the left buffer of another module (for example the
speech synthesis module in our example). Same-
level links are indicated as dashed arrows in Figure
2 and grounded-in links as stacked boxes of IUs.
The figure also shows that the mapping between
IUs can be a one-to-many mapping (IU1 and IU2
are mapped to IU3) or a one-to-one mapping (IU3 is
50
IU1 IU2
IU1 IU2
IU3
IU3 IU3
IU4
IU4
Left buffer Processor Right buffer
Left buffer Processor Right buffer
Figure 2: The buffer-based model showing two connected
modules (from Skantze and Hjalmarsson (2010).
IU1
IU2 IU3 IU4 IU5
IU6 IU7 IU8 IU9 . . .
Figure 3: The ISU-model for incremental processing
(adapted from Buss and Schlangen (2011)).
mapped to IU4). The model distinguishes four op-
erations that handle information processing: update,
revise, purge and commit. Whenever new IUs en-
ter the module?s left buffer, the module?s knowledge
base is updated to reflect the new information. Such
information typically corresponds to the current best
hypothesis of a preceding processing module. As
a property of incremental systems, however, such
hypotheses can be revised by the respective preced-
ing module and, as a result, the knowledge bases of
all subsequent modules need to be purged and up-
dated to the newest hypothesis. Once a hypothesis
is certain to not be revised anymore, it is commit-
ted. For concrete implementations of this model, see
Skantze and Schlangen (2009), Skantze and Hjal-
marsson (2010), Baumann and Schlangen (2011).
An implementation of an incremental dialogue
manager is based on the Information State Update
(ISU) model (Buss et al, 2010; Buss and Schlangen,
2011). The model is related in spirit to the buffer-
based architecture, but all of its input processing and
output planning is realised by ISU rules. This is true
for the incremental ?house-keeping? actions update,
revise, etc. and all types of dialogue acts. The in-
cremental ISU model is shown in Figure 3. Note
that this hierarchical architecture transfers well to
the ?classical? division of NLG levels into utterance
(IU1), content selection (IU2 - IU5) and surface re-
alisations (IU6 - IU9, etc.).
2.2 Beat-Driven Incremental Processing
In contrast to the buffer-based architectures, alterna-
tive incremental systems do not reuse previous par-
tial hypotheses of the user?s input (or the system?s
best output) but recompute them at each process-
ing step. We follow Baumann et al (2011) in call-
ing them ?beat-driven? systems. Raux and Eskenazi
(2009) use a cost matrix and decision theoretic prin-
ciples to optimise turn-taking in a dialogue system
under the constraint that users prefer no gaps and no
overlap at turn boundaries. DeVault et al (2009) use
maximum entropy classification to support respon-
sive overlap in an incremental system by predicting
the completions of user utterances.
2.3 Decision-making in Incremental Systems
Some of the main advantages of the buffer- and ISU-
based approaches include their inherently incremen-
tal mechanisms for updating and revising system hy-
potheses. They are able to process input of varying
size and type and, at the same time, produce arbi-
trarily complex output which is monitored and can
be modified at any time. On the other hand, current
models are based on deterministic decision making
and thus share some of the same drawbacks that non-
incremental systems have faced: (1) they rely on
hand-written rules which are time-consuming and
expensive to produce, (2) they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, and (3) they are unable to
generalise and adapt flexibly to unseen situations.
For NLG in particular, we have seen that incre-
mentality can enhance the responsiveness of sys-
tems and facilitate turn-taking. However, this ad-
vantage was mainly gained by the system produc-
ing semantically empty fillers such as um, let me
see, well, etc. (Skantze and Hjalmarsson, 2010). It
is an open research question whether such markers
of planning or turn-holding can help NLG systems,
but for now it seems that they could be reduced to
a minimum by optimising the timing and order of
Information Presentation. In the following, we de-
velop a model for incremental NLG that is based on
reinforcement learning (RL). It learns the best mo-
ment to present information to the user, when faced
with the options of presenting information as soon
as it becomes available or else waiting until the in-
51
Type Example
Comparison The restaurant Roma is in the medium price range, but does not have great food. The Firenze
and Verona both have great food but are more expensive. The Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expensive,
but has great food and service.
Summary I have 43 Italian restaurants in the city centre that match your query. 10 of them are in the
medium price range, 5 are cheap and 8 are expensive.
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
put hypotheses of the system are more stable. This
also addresses the general trade-off that exists in in-
cremental systems between the processing speed of
a system and the output quality.
3 Information Presentation Strategies
Our domain of application will be the Informa-
tion Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al (2010), (see also Walker et
al. (2004) for an alternative approach). Rieser et
al. incrementally construct IP strategies according
to the predicted user reaction, whereas our approach
focuses on timing and re-ordering of information
according to dynamically changing input hypothe-
ses. We therefore implement a simplified version
of Rieser et al?s model. Their system distinguished
two steps: the selection of an IP strategy and the
selection of attributes to present to the user. We as-
sume here that the choice of attributes is determined
by matching the types specified in the user input,
so that our system only needs to choose a strategy
for presenting its results (in the future, though, we
will include attribute selection into the decision pro-
cess). Attributes include cuisine, food quality, lo-
cation, price range and service quality of a restau-
rant. The system then performs a database lookup
and chooses among three main IP strategies sum-
mary, comparison, recommendation and several or-
dered combinations of these. Please see Rieser et al
(2010) for details. Table 1 shows examples of the
main types of presentation strategies we address.
4 Optimising Incremental NLG
To optimise the NLG process within an incremen-
tal model of dialogue processing, we define an RL
agent with incremental states and actions for the IP
task. An RL agent is formalised as a Markov De-
cision Process, or MDP, which is characterised as a
four-tuple < S,A, T,R >, where S is a set of states
representing the status of the NLG system and all in-
formation available to it, A is a set of NLG actions
that combine strategies for IP with handling incre-
mental updates in the system, T is a probabilistic
transition function that determines the next state s?
from the current state s and the action a according
to a conditional probability distribution P (s?|s, a),
and R is a reward function that specifies the reward
(a numeric value) that an agent receives for taking
action a in state s. Using such an MDP, the NLG
process can be seen as a finite sequence of states,
actions and rewards {s0, a0, r1, s1, a1, ..., rt?1, st},
where t is the time step. Note that a learning episode
falls naturally into a number of time steps at each of
which the agent observes the current state of the en-
vironment st, takes an action at and makes a tran-
sition to state st+1. This organisation into discrete
time steps, and the notion of a state space that is ac-
cessible to the learning agent at any time allows us to
implement the state update, revoke and purge opera-
tions typically assumed by incremental approaches
as state updates and transitions in an MDP. Any
change in the environment, such as a new best hy-
pothesis of the recogniser, can thus be represented
as a transition from one state to another. At each
time step, the agent then takes the currently best ac-
tion according to the new state. The best action in
an incremental framework can include correcting a
previous output, holding the floor as a marker of
planning, or to wait until presenting information.1
1We treat these actions as part of NLG content selection
here, but are aware that in alternative approaches, they could
52
States
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
presStrategy {0=unfilled,1=filled}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusFood {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions
IP: compare, recommend, summarise, summariseCompare,
summariseRecommend, summariseCompareRecommend,
Slot-ordering: presentCuisine, presentFood, presentLocation,
presentPrice, presentService,
Incremental: backchannel, correct, selfCorrect, holdFloor,
waitMore
Goal State 0, 1, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 1, 0 ? 1
Figure 4: The state and action space of the learning agent.
The goal state is reached when all items (that the user may
be interested in) have been presented.
Once information has been presented to the user,
it is committed or realised. We again represent re-
alised IUs in the agent?s state representation, so that
it can monitor its own output. The goal of an MDP
is to find an optimal policy pi? according to which
the agent receives the maximal possible reward for
each visited state. We use the Q-Learning algorithm
(Watkins, 1989) to learn an optimal policy according
to pi?(s) = argmaxa?A Q?(s, a), where Q? speci-
fies the expected reward for executing action a in
state s and then following policy pi?.
5 Experimental Setting
5.1 The State and Action Space
The agent?s state space needs to contain all infor-
mation relevant for choosing an optimal IP strat-
egy and an optimal sequence of incremental ac-
tions. Figure 4 shows the state and action space
of our learning agent. The states contain infor-
mation on the incremental and presentation sta-
tus of the system. The variable ?incrementalSta-
tus? characterises situations in which a particular
(incremental) action is triggered. For example, a
holdFloor is generated when the user has finished
speaking, but the system has not yet finished its
database lookup. A correction is needed when
also be the responsibility of a dialogue manager.
the system has to modify already presented infor-
mation (because the user changed their preferences)
and a selfCorrection is needed when previously
presented information is modified because the sys-
tem made a mistake (in recognition or interpreta-
tion). The variable ?presStrategy? indicates whether
a strategy for IP has been chosen. It is ?filled? when
this is the case, and ?unfilled? otherwise. The vari-
ables representing the status of the cuisine, food, lo-
cation, price and service indicate whether the slot
is of interest to the user (0 means that the user does
not care about it), and what input confidence score is
currently associated with its value. Once slots have
been presented, they are realised and can only be
changed through a correction or self-correction.
The variable ?userReaction? shows the user?s re-
action to an IP episode. The user can select a restau-
rant, provide more information to further constrain
the search or do something else. The ?userSilence?
variable indicates whether the user is speaking or
not. This can be relevant for holding the floor or
generating backchannels. The action set comprises
IP actions, actions which enable us to learn the or-
dering of slots, and actions which allow us to cap-
ture incremental phenomena. The complete state-
action space size of this agent is roughly 3.2 mil-
lion. The agent reaches its goal state (defined w.r.t.
the state variables in Figure 4) when an IP strategy
has been chosen and all relevant attributes have been
presented.
5.2 The Simulated Environment
Since a learning agent typically needs several thou-
sand interactions to learn a reasonable policy, we
train it in a simulated environment with two compo-
nents. The first one deals with different IP strategies
generally (not just for the incremental case), and the
second one focuses on incrementally updated user
input hypothesis during the interaction.
To learn a good IP strategy, we use a user simula-
tion by Rieser et al (2010),2 which was estimated
from human data and uses bi-grams of the form
P (au,t|IPs,t), where au,t is the predicted user re-
action at time t to the system?s IP strategy IPs,t in
state s at time t. We distinguish the user reactions of
2The simulation data are available from http://www.
classic-project.org/.
53
select a restaurant, addMoreInfo to the current query
to constrain the search, and other. The last category
is considered an undesired user reaction that the sys-
tem should learn to avoid. The simulation uses lin-
ear smoothing to account for unseen situations. In
this way, we can then predict the most likely user
reaction to each system action.
While the IP strategies can be used for incremen-
tal and non-incremental NLG, the second part of the
simulation deals explicitly with the dynamic envi-
ronment updates during an interaction. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores are shown in Table 2
and denote the same as described in Section 5.1.
At the beginning of a learning episode, we as-
sign each attribute a possible value and confidence
score with equal probability. For food and service
quality, we assume that the user is never interested
in bad food or service. Subsequently, confidence
scores can change at each time step. (In future work
these transition probabilities will be estimated from
a data collection, though the following assumptions
are realistic, based on our experience.) We assume
that a confidence score of 0 changes to any other
value with a likelihood of 0.05. A confidence score
of 1 changes with a probability of 0.3, a confidence
score of 2 with a probability of 0.1 and a confidence
score of 3 with a probability of 0.03. Once slots
have been realised, their value is set to 4. They
cannot be changed then without an explicit correc-
tion. We also assume that realised slots change with
a probability of 0.1. If they change, we assume
that half of the time, the user is the origin of the
change (because they changed their mind) and half
of the time the system is the origin of the change
(because of an ASR or interpretation error). Each
time a confidence score is changed, it has a proba-
bility of 0.5 to also change its value. The resulting
input to the NLG system are data structures of the
form present(cuisine=Indian), confidence=low.
5.3 The Reward Function
The main trade-off to optimise for IP in an incre-
mental setting is the timing and order of presenta-
tion. The agent has to decide whether to present
Attribute Values Confidence
Cuisine Chinese, French, German, In-, 0, 1, 2, 3, 4
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Food bad, adequate, good, very good 0, 1, 2, 3, 4
Location 7 distinct areas of the city 0, 1, 2, 3, 4
Price cheap, expensive, good-price-
for-value, very expensive 0, 1, 2, 3, 4
Service bad, adequate, good, very good 0, 1, 2, 3, 4
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
information as soon as it becomes available or else
wait until confidence for input hypotheses is more
stable. Alternatively, it can reorder information to
account for different confidence scores. We assign
the following rewards3: +100 if the user selects
an item, 0 if the user adds more search constraints,
?100 if the user does something else or the sys-
tem needs to self-correct,?0.5 for holding the floor,
and ?1 otherwise. In addition, the agent receives
an increasing negative reward for the waiting time,
waiting time2 (to the power of two), in terms of the
number of time steps passed since the last item was
presented. This reward is theoretically ??. The
agent is thus penalised stronger the longer it delays
IP. The rewards for user reactions are assigned at the
end of each episode, all other rewards are assigned
after each time step. One episode stretches from the
moment that a user specifies their initial preferences
to the moment in which they choose a restaurant.
The agent was trained for 10 thousand episodes.
6 Experimental Results
After training, the RL agent has learnt the following
incremental IP strategy. It will present information
slots as soon as they become available if they have
a medium or high confidence score. The agent will
then order attributes so that those slots with the high-
est confidence scores are presented first and slots
with lower confidence are presented later (by which
time they may have achieved higher confidence). If
no information is known with medium or high con-
3Handcrafted rewards are sufficient for this proof-of-
concept study, and can be learned from data for future models
(Rieser and Lemon, 2011).
54
101 102 103 104
?100
?80
?60
?40
?20
0
20
40
60
80
100
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
RL
Base1
Base2
Base3
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the RL agent and its baselines.
fidence, the agent will hold the floor or wait. In this
way, it can prevent self-corrections and minimise
waiting time?both of which yield negative rewards.
It can thus start speaking very early (avoiding long
pauses or semantically empty utterances) and still
has a low likelihood of having to self-correct.
For a comparison of the learnt policy with possi-
ble hand-crafted policies (because current incremen-
tal NLG systems are rule-based), we designed three
baselines. Baseline 1 always presents information
as soon as it is available, i.e. never waits. Base-
line 2 always waits until all information is known
with high confidence (i.e. until all confidence scores
are 3). Baseline 3 was chosen to be more ambi-
tious. It always presents information as soon as
possible, using a decreasing order of confidence to
minimise self-corrections (i.e. very similar to the
learnt policy). It chooses randomly among slots with
equal confidence. All baseline policies have an op-
timised IP strategy (recommend/summary etc.) and
differ only in their incremental processing strategies.
Baseline 1 is most similar to the current approach
used in spoken dialogue systems, where the Dia-
logue Manager triggers an NLG component as soon
as a task-relevant user utterance is processed.
Here we do not compare the different strategies
for IP generally because this has been done by
Rieser et al (2010), even if not for incremental
NLG. Figure 5 shows the performance of all be-
haviours in terms of average rewards. Baseline 1
obtains on average roughly 100 reward points less
than the RL agent. This corresponds to the (neg-
ative) reward of one self-correction (?100). Since
information is always presented as soon as it is avail-
able, this baseline needs to produce on average one
Baseline 1 (present information as soon as
it becomes available)
USR Please show me Italian places
(cuisine = Indian;conf score = 0.2)
SYS I have found 35 Indian . . .
USR No, ITALIAN
(cuisine = Italian; conf score = 0.8)
SYS Oh, you meant Italian. I have 104 Italian places in Edin-
burgh. 24 in the city centre, 21 in the New Town, 14 in . . .
USR Show me some in the city centre
(location = centre; conf score = 0.6)
SYS OK. I found 24 Italian restaurants in the city centre . . .
Baseline 2 (always wait until confidence is high)
USR Do you have Italian restaurants in the centre of town?
cuisine = Italian; conf score = 0.4
location = centre; conf score = 0.2
SYS waits
USR Italian in the centre.
cuisine = Italian, conf score = 0.7
location = centre, conf score = 0.5
SYS I have 104 Italian restaurants.
USR waits
SYS waits
USR city centre please
location = centre, conf score = 0.7
SYS I have 24 Italian restaurants in the city centre . . .
Baseline 3 (present information in decreasing
order of confidence)
USR I want Italian food . . .
cuisine = Indian, conf score = 0.2
location = centre, conf score = 0.3
SYS hmm (holding turn) . . .
USR in the centre of town
location = centre, conf score = 0.9
SYS In the centre, let me see, Indian . . .
USR Italian, please.
cuisine = Italian, conf score = 0.7
SYS Oh I see. I have 24 Italian places in the centre . . .
Figure 6: Example dialogues generated with the baseline
policies for a user who wants Italian food in the city cen-
tre. Confidence scores for cuisine and location variables
for the restaurants are shown as updated.
self-correction per episode. Baseline 2 needs to wait
until all information is known with high confidence
and obtains on average 125 to 130 rewards less than
the RL agent. This corresponds to approximately
11 time steps of waiting (for input to reach higher
confidence) before presentation since 11 is (approxi-
mately) the square root of 130. Baseline 3 is roughly
a reward of ?10 worse than the RL agent?s be-
55
haviour, which is due to a combination of more self-
corrections, even if they just occur occasionally, and
a higher number of turn holding markers. The latter
is due to the baseline starting to present as soon as
possible, so that whenever all confidence scores are
too low to start presenting, a turn holding marker
is generated. The learning agent learns to outper-
form all baselines significantly, by presenting infor-
mation slots in decreasing order of confidence, com-
bined with waiting and holding the floor at appro-
priate moments. Anticipating the rewards for wait-
ing vs. holding the floor at particular moments is the
main reason that the learnt policy outperforms Base-
line 3. Subtle moments of timing as in this case are
difficult to hand-craft and more appropriately bal-
anced using optimisation. An absolute comparison
of the last 1000 episodes of each behaviour shows
that the improvement of the RL agent corresponds
to 126.8% over Baseline 1, to 137.7% over Baseline
2 and to 16.76% over Baseline 3. All differences are
significant at p < 0.001 according to a paired t-test
and have a high effect size r > 0.9. The high per-
centage improvement of the learnt policy over Base-
lines 1 and 2 is mainly due to the high numeric val-
ues chosen for the rewards as can be observed from
their qualitative behaviour. Thus, if the negative nu-
meric values of, e.g., a self-correction were reduced,
the percentage reward would reduce, but the pol-
icy would not change qualitatively. Figure 1 shows
some examples of the learnt policy including several
incremental phenomena. In contrast, Figure 6 shows
examples generated with the baselines.
7 Conclusion and Future Directions
We have presented a novel framework combining in-
cremental and statistical approaches to NLG for in-
teractive systems. In a proof-of-concept study in the
domain of Information Presentation, we optimised
the timing and order of IP. The learning agent op-
timises the trade-off of whether to present informa-
tion as soon as it becomes available (for high respon-
siveness) or else to wait until input hypotheses were
more stable (to avoid self-corrections). Results in a
simulated environment showed that the agent learns
to avoid self-corrections and long waiting times, of-
ten by presenting information in order of decreas-
ing confidence. It outperforms three hand-crafted
baselines due to its enhanced adaptivity. In pre-
vious work, incremental responsiveness has mainly
been implemented by producing semantically empty
fillers such as um, let me see, well, etc. (Skantze and
Hjalmarsson, 2010). Our work avoids the need for
these fillers by content reordering.
Since this paper has focused on a proof-of-
concept study, our goal has not been to demonstrate
the superiority of automatic optimisation over hand-
crafted behaviour. Previous studies have shown
the advantages of optimisation (Janarthanam and
Lemon, 2010; Rieser et al, 2010; Dethlefs et al,
2011). Rather, our main goal has been to demon-
strate that incremental NLG can be phrased as an op-
timisation problem and that reasonable action poli-
cies can be learnt so that an application within an
incremental framework is feasible. This observation
allows us to take incremental systems, which so far
have been restricted to deterministic decision mak-
ing, one step further in terms of their adaptability
and flexibility. To demonstrate the effectiveness of
a synergy between RL and incremental NLG on a
large scale, we would like to train a fully incremental
NLG system from human data using a data-driven
reward function. Further, an evaluation with human
users will be required to verify the advantages of dif-
ferent policies for Information Presentation.
Regarding the scalability of our optimisation
framework, RL systems are known to suffer from the
curse of dimensionality, the problem that their state
space grows exponentially according to the number
of variables taken into account. While the appli-
cation of flat RL is therefore limited to small-scale
problems, we can use RL with a divide-and-conquer
approach, hierarchical RL, which has been shown to
scale to large-scale NLG applications (Dethlefs and
Cuaya?huitl, 2011), to address complex NLG tasks.
Future work can take several directions. Cur-
rently, we learn the agent?s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situ-
ations could be achieved if the agent was able to
learn from ongoing interactions using online learn-
ing. In addition, current NLG systems tend to as-
sume that the user?s goals and situational circum-
stances are known with certainty. This is often an
unrealistic assumption that future work could ad-
dress using POMDPs (Williams and Young, 2007).
56
Acknowledgements
The research leading to this work has received fund-
ing from EC?s FP7 programmes: (FP7/2011-14)
under grant agreement no. 287615 (PARLANCE);
(FP7/2007-13) under grant agreement no. 216594
(CLASSiC); (FP7/2011-14) under grant agreement
no. 270019 (SPACEBOOK); (FP7/2011-16) under
grant agreement no. 269427 (STAC).
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proc. of EMNLP, pages 502?512.
Timo Baumann and David Schlangen. 2011. Predict-
ing the Micro-Timing of User Input for an Incremental
Spoken Dialogue System that Completes a User?s On-
going Turn. In Proc. of 12th Annual SIGdial Meeting
on Discourse and Dialogue, Portland, OR.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language Engi-
neering, 14(4):431?455.
Okko Buss and David Schlangen. 2011. DIUM?An In-
cremental Dialogue Manager That Can Produce Self-
Corrections. In Proc. of the Workshop on the Seman-
tics and Pragmatics of Dialogue (SemDIAL / Los An-
gelogue), Los Angeles, CA.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proc. of 11th Annual SIG-
dial Meeting on Discourse and Dialogue.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proc. of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
Nina Dethlefs, Heriberto Cuaya?huitl, and Jette Viethen.
2011. Optimising Natural Language Generation Deci-
sion Making for Situated Dialogue. In Proceedings of
the 12th Annual Meeting on Discourse and Dialogue
(SIGdial), Portland, Oregon, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Proc.
of the 10th Annual SigDial Meeting on Discourse and
Dialogue, Queen Mary University, UK.
Srini Janarthanam and Oliver Lemon. 2010. Learning to
Adapt to Unknown Users: Referring Expression Gen-
eration in Spoken Dialogue Systems. In Proc. of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 69?78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Willem Levelt. 1989. Speaking: From Intenion to Artic-
ulation. MIT Press.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation us-
ing graphical models and active learning. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1552?1561.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proc. of the 10th Conference of the North
American Chapter of the Association for Compu-
tational Linguistics?Human Language Technologies
(NAACL-HLT), Boulder, Colorado.
Verena Rieser and Oliver Lemon. 2011. Reinforcement
Learning for Adaptive Dialogue Systems: A Data-
driven Methodology for Dialogue Management and
Natural Language Generation. Book Series: The-
ory and Applications of Natural Language Processing,
Springer, Berlin/Heidelberg.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Dia-
logue Systems. In Proc. of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
Uppsala, Sweden.
David Schlangen and Gabriel Skantze. 2009. A General,
Abstract Model of Incremental Dialogue Processing.
In Proc. of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proc. of the 11th Annual SigDial Meeting on Dis-
course and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proc. of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Athens, Greece.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialogue systems. In
57
Proc. of the Annual Meeting of the Association for
Computational Linguistics.
Kees van Deemter. 2009. What game theory can do for
NLG: the case of vague language. In 12th European
Workshop on Natural Language Generation (ENLG).
Marilyn Walker, Steve Whittaker, Amanda Stent, Pre-
taam Maloor, Johanna Moore, and G Vasireddy.
2004. Generation and Evaluation of User Tailored Re-
sponses in Multimodal Dialogue. Cognitive Science,
28(5):811?840.
Chris Watkins. 1989. Learning from Delayed Rewards.
PhD Thesis, King?s College, Cambridge, UK.
Jason Williams and Steve Young. 2007. Partially
Observable Markov Decision Processes for Spoken
Dialog Systems. Computer Speech and Language,
21(2):393?422.
58
Proceedings of the SIGDIAL 2013 Conference, pages 154?156,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Parlance system: a data-driven,
incremental, spoken dialogue system for interactive search
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Gasic, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, Steve Young
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
The Parlance system for interactive
search processes dialogue at a micro-
turn level, displaying dialogue phe-
nomena that play a vital role in hu-
man spoken conversation. These di-
alogue phenomena include more nat-
ural turn-taking through rapid sys-
tem responses, generation of backchan-
nels, and user barge-ins. The Par-
lance demonstration system differen-
tiates from other incremental systems
in that it is data-driven with an infras-
tructure that scales well.
1 Introduction
The Parlance system provides interactive
search through a Spoken Dialogue System
(SDS). This SDS aims to be incremental to al-
low for more natural spoken interaction. Tra-
ditionally, the smallest unit of speech process-
ing for interactive systems has been a full ut-
terance with strict, rigid turn-taking. The
Parlance architecture, however, is an incre-
mental framework that allows for processing
of smaller ?chunks? of user input, which en-
ables one to model dialogue phenomena such
as barge-ins and backchannels. This work is
carried out under the FP7 EC project Par-
lance 1, the goal of which is to develop inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
San Francisco. An example dialogue is given
in Table 1.
?Authors are in alphabetical order
1http://www.parlance-project.eu
SYS Thank you for calling the Parlance Restaurant
system. You may ask for information by cuisine
type, price range or area. How may I help you?
USR I want to find an Afghan restaurant.........which is
in the cheap price range.
SYS .......................................................[uhuhh]........
The Helmand Palace is a cheerful setting for au-
thentic Afghan cuisine.
USR What is the address and phone number?
SYS The address 2424 Van Ness Ave ....
Table 1: Example dialogue excerpt for restaurant in-
formation in San Francisco
2 Background
Previous work includes systems that can deal
with ?micro-turns? (i.e. sub-utterance process-
ing units), resulting in dialogues that are more
fluid and responsive. This has been backed up
by a large body of psycholinguistic literature
that indicates that human-human interaction
is in fact incremental (Levelt, 1989).
It has been shown that incremental dia-
logue behaviour can improve the user experi-
ence (Skantze and Schlangen, 2009; Baumann
et al, 2011; Selfridge et al, 2011) and en-
able the system designer to model several di-
alogue phenomena that play a vital role in
human discourse (Levelt, 1989) but have so
far been absent from systems. These dialogue
phenomena that will be demonstrated by the
Parlance system include more natural turn-
taking through rapid system responses, gener-
ation of backchannels and user barge-ins. The
system differentiates from other incremental
systems in that it is entirely data-driven with
an infrastructure that potentially scales well.
3 System Architecture
Figure 1 gives an overview of the Par-
lance system architecture, which maintains
154
LOCAL SEARCH ENGINE
AUTOMATIC SPEECH RECOGNITION
NLG
AUDIO I/O
TTS
BACKCHANNEL GENERATOR
IM
MIM
HUB
KNOWLEDGE BASE
WavePackets
1-Best Words
Segmentlabel
N-Best Phrase List
WavePackets
Micro-Turn Dialogue Act
System Dialogue Act
String Packets
StringPackets
VoIP Interface (PJSIP)
N-best Dialogue Act Units
 API call ( + metadata)
Search Response
Partial Dialogue Act (in case of interruption)
PartialString(in case of interruption)SPOKEN LANGUAGE UNDERSTANDING Decode from t0 to t1
Figure 1: Overview of the Parlance system
architecture
the modularity of a traditional SDS while at
the same time allowing for complex interaction
at the micro-turn level between components.
Each component described below makes use
of the PINC (Parlance INCremental) dialogue
act schema. In this scheme, a complete dia-
logue act is made up of a set of primitive di-
alogue acts which are defined as acttype-item
pairs. The PINC dialogue act scheme supports
incrementality by allowing SLU to incremen-
tally output primitive dialogue acts whenever
a complete acttype-item pair is recognised with
sufficient confidence. The complete dialogue
act is then the set of these primitive acts out-
put during the utterance.
3.1 Recognition and Understanding
The Automatic Speech Recogniser (ASR) and
Spoken Language Understanding (SLU) com-
ponents operate in two passes. The audio in-
put is segmented by a Voice Activity Detec-
tor and then coded into feature vectors. For
the first pass of the ASR2, a fast bigram de-
coder performs continuous traceback generat-
ing word by word output. During this pass,
while the user is speaking, an SLU module
called the ?segment decoder? is called incre-
2http://mi.eng.cam.ac.uk/research/dialogue/
ATK_Manual.pdf
mentally as words or phrases are recognised.
This module incrementally outputs the set of
primitive dialogue acts that can be detected
based on each utterance prefix. Here, the ASR
only provides the single best hypothesis, and
SLU only outputs a single set of primitive dia-
logue acts, without an associated probability.
On request from the Micro-turn Interaction
Manager (MIM), a second pass can be per-
formed to restore the current utterance using a
trigram language model, and return a full dis-
tribution over the complete phrase as a con-
fusion network. This is then passed to the
SLU module which outputs the set of alter-
native complete interpretations, each with its
associated probability, thus reflecting the un-
certainty in the ASR-SLU understanding pro-
cess.
3.2 Interaction Management
Figure 1 illustrates the role of the Micro-turn
Interaction Manager (MIM) component in the
overall Parlance architecture. In order to
allow for natural interaction, the MIM is re-
sponsible for taking actions such as listening to
the user, taking the floor, and generating back-
channels at the micro-turn level. Given various
features from different components, the MIM
selects a micro-turn action and sends it to the
IM and back-channel generator component to
generate a system response.
Micro-turn Interaction Manager A
baseline hand-crafted MIM was developed
using predefined rules. It receives turn-taking
information from the TTS, the audio-output
component, the ASR and a timer, and updates
turn-taking features. Based on the current
features and predefined rules, it generates
control signals and sends them to the TTS,
ASR, timer and HUB. In terms of micro-turn
taking, for example, if the user interrupts
the system utterance, the system will stop
speaking and listen to the user. The system
also outputs a short back-channel and stays in
user turn state if the user utterance provides
limited information.
Interaction Manager Once the MIM has
decided when the system should take the floor,
it is the task of the IM to decide what to say.
The IM is based on the partially observable
155
Markov decision process (POMDP) frame-
work, where the system?s decisions can be op-
timised via reinforcement learning. The model
adopted for Parlance is the Bayesian Update
of Dialogue State (BUDS) manager (Thom-
son and Young, 2010). This POMDP-based
IM factors the dialogue state into condition-
ally dependent elements. Dependencies be-
tween these elements can be derived directly
from the dialogue ontology. These elements
are arranged into a dynamic Bayesian network
which allows for their marginal probabilities
to be updated during the dialogue, compris-
ing the belief state. The belief state is then
mapped into a smaller-scale summary space
and the decisions are optimised using the nat-
ural actor critic algorithm.
HUB The HUB manages the high level flow
of information. It receives turn change infor-
mation from the MIM and sends commands
to the SLU/IM/NLG to ?take the floor? in the
conversation and generate a response.
3.3 Generation and TTS
We aim to automatically generate language,
trained from data, that is (1) grammatically
well formed, (2) natural, (3) cohesive and (4)
rapidly produced at runtime. Whilst the first
two requirements are important in any dia-
logue system, the latter two are key require-
ments for systems with incremental processing,
in order to be more responsive. This includes
generating back-channels, dynamic content re-
ordering (Dethlefs et al, 2012), and surface
generation that models coherent discourse phe-
nomena, such as pronominalisation and co-
reference (Dethlefs et al, 2013). Incremen-
tal surfacce generation requires rich context
awareness in order to keep track of all that has
been generated so far. We therefore treat sur-
face realisation as a sequence labelling task and
use Conditional Random Fields (CRFs), which
take semantically annotated phrase structure
trees as input, in order to represent long dis-
tance linguistic dependencies. This approach
has been compared with a number of compet-
itive state-of-the art surface realisers (Deth-
lefs et al, 2013), and can be trained from
minimally labelled data to reduce development
time and facilitate its application to new do-
mains.
The TTS component uses a trainable HMM-
based speech synthesizer. As it is a paramet-
ric model, HMM-TTS has more flexibility than
traditional unit-selection approaches and is es-
pecially useful for producing expressive speech.
3.4 Local Search and Knowledge Base
The domain ontology is populated by the local
search component and contains restaurants in
5 regional areas of San Francisco. Restaurant
search results are returned based on their lon-
gitude and latitude for 3 price ranges and 52
cuisine types.
4 Future Work
We intend to perform a task-based evaluation
using crowd-sourced users. Future versions
will use a dynamic Knowledge Base and User
Model for adapting to evolving domains and
personalised interaction respectively.
Acknowledgements
The research leading to this work was funded by the EC
FP7 programme FP7/2011-14 under grant agreement
no. 287615 (PARLANCE).
References
T. Baumann, O. Buss, and D. Schlangen. 2011. Eval-
uation and Optimisation of Incremental Processors.
Dialogue and Discourse, 2(1).
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of INLG, Chicago, USA.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation Using Global Features. In Pro-
ceedings of ACL, Sofia, Bulgaria.
W. Levelt. 1989. Speaking: From Intenion to Articu-
lation. MIT Press.
E. Selfridge, I. Arizmendi, P. Heeman, and J. Williams.
2011. Stability and Accuracy in Incremental Speech
Recognition. In Proceedings of SIGDIAL, Portland,
Oregon.
G. Skantze and D. Schlangen. 2009. Incremental Dia-
logue Processing in a Micro-Domain. In Proceedings
of EACL, Athens, Greece.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
156
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 174?179,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Evaluating Distant Supervision for Subjectivity and Sentiment Analysis
on Arabic Twitter Feeds
Eshrag Refaee and Verena Rieser
Interaction Lab, School of Mathematical and Computer Sciences,
Heriot-Watt University,
EH14 4AS Edinburgh, United Kingdom.
eaar1@hw.ac.uk, v.t.rieser@hw.ac.uk
Abstract
Supervised machine learning methods for
automatic subjectivity and sentiment anal-
ysis (SSA) are problematic when applied
to social media, such as Twitter, since they
do not generalise well to unseen topics. A
possible remedy of this problem is to ap-
ply distant supervision (DS) approaches,
which learn from large amounts of auto-
matically annotated data. This research
empirically evaluates the performance of
DS approaches for SSA on Arabic Twitter
feeds. Results for emoticon- and lexicon-
based DS show a significant performance
gain over a fully supervised baseline, es-
pecially for detecting subjectivity, where
we achieve 95.19% accuracy, which is a
48.47% absolute improvement over previ-
ous fully supervised results.
1 Introduction
Subjectivity and sentiment analysis (SSA) aims to
determine the attitude of an author with respect
to some topic, e.g. objective or subjective, or
the overall contextual polarity of an utterance, e.g.
positive or negative. Previous work on automatic
SSA has used manually annotated gold standard
data sets to analyse which feature sets and mod-
els perform best for this task, e.g. (Wilson et al.,
2009; Wiebe et al., 1999). Most of this work is in
English, but there have been first attempts to apply
similar techniques to Arabic, e.g. (Abdul-Mageed
et al., 2011; Mourad and Darwish, 2013). While
these models work well when tested using cross-
validation on limited static data sets, our previ-
ous results reveal that these models do not gen-
eralise to new data sets, e.g. collected at a later
point in time, due to their limited coverage (Refaee
and Rieser, 2014). While there is a growing inter-
est within the NLP community in building Arabic
corpora by harvesting the web, e.g. (Al-Sabbagh
and Girju, 2012; Abdul-Mageed and Diab, 2012;
Zaidan and Callison-Burch, 2013), these resources
have not been publicly released yet and only small
amounts of these data-sets are (manually) anno-
tated. We therefore turn to an approach known
as distant supervision (DS), as first proposed by
(Read, 2005), which uses readily available fea-
tures, such as emoticons, as noisy labels in or-
der to efficiently annotate large amounts of data
for learning domain-independent models. This ap-
proach has been shown to be successful for En-
glish SSA, e.g. (Go et al., 2009), and SSA for
under-resourced languages, such as Chinese (Yuan
and Purver, 2012).
The contributions of this paper are as follows:
we first collect two large corpora using emoticons
and lexicon-based features as noisy labels, which
we plan to release as part of this submission. Sec-
ond, this work is the first to apply and empirically
evaluate DS approaches on Arabic Twitter feeds.
We find that DS significantly outperforms fully su-
pervised SSA on our held-out test set. However,
compared to a majority baseline, predicting nega-
tive sentiment proves to be difficult using DS ap-
proaches. Third, we conduct an error analysis to
critically evaluate the results and give recommen-
dations for future directions.
2 Arabic Twitter SSA Corpora
We start by collecting three corpora at different
times over one year to account for the cyclic ef-
fects of topic change in social media (Eisenstein,
2013). Table 1 shows the distributions of labels in
our data-sets:
1. A gold standard data-set which we use for
training and evaluation (spring 2013);
2. A data-set for DS using emoticon-based
queries (autumn 2013);
3. Another data-set for DS using a lexicon-
based approach (winter 2014).
174
Data set Neutral Polar Positive Negative Total
Gold standard training 1,157 937 470 467 3,031
Emoticon-based training 55,076 62,466 32,842 33,629 184,013
Lexicon-based training 55,076 55,538 18,442 5,013 134,069
Manually labelled test 422 579 278 301 1,580
Table 1: Sentiment label distribution of the gold standard manually annotated and distant supervision
data sets.
Gold Standard Data-set: We harvest two gold
standard data sets at different time steps, which
we label manually. We first harvest a data set of
3,031 multi-dialectal Arabic tweets randomly re-
trieved over the period from February to March
2013. We use this set as a training set for our
fully supervised approach. We also manually label
1,580 tweets collected in autumn 2013, which we
use as an independent held-out test set. Two na-
tive speakers were recruited to manually annotate
the collected data for subjectivity and sentiment,
where we define sentiment as a positive or nega-
tive emotion, opinion or attitude, following (Wil-
son et al., 2009). Our gold standard annotations
reached a weighted ? = 0.76, which indicates re-
liable annotations (Carletta, 1996). We also auto-
matically annotate the corpus with a rich set of lin-
guistically motivated features using freely avail-
able processing tools for Arabic, such as MADA
(Nizar Habash and Roth, 2009), see Table 2. For
more details on gold standard corpus annotation
please see (Refaee and Rieser, 2014).
1
Type Feature-sets
Morphological diacritic, aspect, gender, mood, per-
son, part of speech (POS), state, voice,
has morphological analysis.
Syntactic n-grams of words and POS, lem-
mas, including bag of words (BOW),
bag of lemmas.
Semantic has positive lexicon,
has negative lexicon,
has neutral lexicon, has negator,
has positive emoticon,
has negative emoticon.
Table 2: Annotated Feature-sets
Emoticon-Based Queries: In order to investi-
gate DS approaches to SSA, we also collect a
much larger data set of Arabic tweets, where
we use emoticons as noisy labels, following e.g.
(Read, 2005; Go et al., 2009; Pak and Paroubek,
2010; Yuan and Purver, 2012; Suttles and Ide,
2013). We query Twitter API for tweets with vari-
ations of positive and negative emoticons to ob-
tain pairs of micro-blog texts (statuses) and using
Emoticon Sentiment label
:) , :-) , :)), (: , (-: ,
((:
positive
:( , :-( , :(( , :(( , ): ,
)): )-:
negative
Table 3: Emoticons used to automatically label the
training data-set.
emoticons as author-provided emotion labels. In
following (Purver and Battersby, 2012; Zhang et
al., 2011; Suttles and Ide, 2013), we also utilise
some sentiment-bearing hash tags to query emo-
tional tweets, e.g. hQ
	
? happiness and
	
?
	
Q k
sadness. Note that emoticons and hash-tags are
merely used to collect and build the training set
and were replaced by the standard (positive/ neg-
ative) labels. In order to collect neutral instances,
we query a set of official news accounts, following
an approach by (Pak and Paroubek, 2010). Exam-
ples of the accounts queried are: BBC-Arabic, Al-
Jazeera Arabic, SkyNews Arabia, Reuters Arabic,
France24-Arabic, and DW Arabic. We then au-
tomatically extract the same set of linguistically
motivated features as for the gold standard corpus.
Lexicon-Based Annotation: We also inves-
tigate an alternative approach to DS, which
combines rule-driven lexicon-based SSA, e.g.
(Taboada et al., 2011), with machine learning ap-
proaches, following (Zhang et al., 2011). We
build a new training dataset by combining three
lexica. We first exploit two existing subjectiv-
ity lexica: a manually annotated Arabic subjectiv-
ity lexicon (Abdul-Mageed and Diab, 2012) and
a publicly available English subjectivity lexicon,
MPQA (Wilson et al., 2009), which we automati-
cally translate using Google Translate, following a
1
This GS data-set has been shared via
a special LREC repository available at
http://www.resourcebook.eu/shareyourlr/index.php
175
similar technique to (Mourad and Darwish, 2013).
The translated lexicon is manually corrected by re-
moving translations with neutral or no clear senti-
ment indicator.
2
This results in 2,627 translated in-
stances after correction. We then construct a third
dialectal lexicon of 484 words that we extracted
from an independent Twitter development set and
manually annotated for sentiment. All lexicons
were merged into a combined lexicon of 4,422 an-
notated sentiment words (duplicates removed). In
order to obtain automatic labels for positive and
negative instances, we follow a simplified version
of the rule-based aggregation approach of Taboada
et al. (2011). First, all lexicons and tweets are lem-
matised. For each tweet, matched sentiment words
are marked with either (+1) or (-1) to incorporate
the semantic orientation of individual constituents.
This achieves a coverage level of 76.62% (which
is computed as a percentage of tweets with at least
one lexicon word) using the combined lexicon.
The identified sentiment words are replaced by
place-holders to avoid bias. To account for nega-
tion, we reverse the polarity (switch negation) fol-
lowing (Taboada et al., 2011). The sentiment ori-
entation of the entire tweet is then computed by
summing up the sentiment scores of all sentiment
words in a given tweet into a single score that au-
tomatically determines the label as being: positive
or negative. Instances where the score equals zero
are excluded from the training set as they represent
mixed-sentiment instances with an even number of
sentiment words. We validate this lexicon-based
labelling approach against a separate development
set by comparing the automatically computed la-
bels against manually annotated ones, reaching an
accuracy of 69.06%.
3 Classification Experiments Using
Distant Supervision
We experiment with a number of machine learn-
ing methods and we report the results of the best
performing scheme, namely Support Vector Ma-
chines (SVMs), where we use the implementation
provided by WEKA (Witten and Frank, 2005). We
report the results on two metrics: F-score and ac-
curacy. We use paired t-tests to establish signifi-
cant differences (p < .05). We experiment with
different feature sets and report on the best results
(Bag-of-Words (BOW) + morphological + seman-
2
For instance, the day of judgement is assigned with a neg-
ative label while its Arabic translation is neutral considering
the context-independent polarity.
tic). We compare our results against a majority
baseline and against a fully supervised approach.
It is important to mention the most prominent pre-
vious work on SSA of Arabic tweets like (Abdul-
Mageed et al., 2012) who trained SVM classifiers
on a nearly 3K manually labelled data-set to curry
out two-stage binary classification attaining accu-
racy up to 65.87% for the sentiment classification
task. In a later work, (Mourad and Darwish, 2013)
employ SVM and Naive Bayes classifiers trained
on a set of 2,300 manually labelled Arabic tweets.
With 10-fold cross-validation settings, the author
reported an accuracy score of 72.5% for the senti-
ment classification task (positive vs. negative).
We evaluate the approaches on a separate held-
out test-set that is collected at a later point in time,
as described in Section 2.
3.1 Emoticon-Based Distant Supervision
We first evaluate the potential of exploiting train-
ing data that is automatically labelled using emoti-
cons. The results are summarised in Table 4.
Polar vs. neutral: The results show a signifi-
cant improvement over the majority baseline, as
well as over the classifier trained on the gold stan-
dard data set: We achieve 95.19% accuracy on
the held-out set, which is a 48.47% absolute im-
provement over our previous fully supervised re-
sults. We attribute this improvement to two fac-
tors. First, the emoticon-based data set is about 60
times bigger than the gold standard data set (see
Table 1) and thus the emoticon-based model better
generalises to unseen events. Note that this perfor-
mance is comparable with (Suttles and Ide, 2013)
who achieved up to 98% accuracy using emoticon-
based DS on English tweets using 5.9 million
tweets. Second, neutral instances were sampled
from news accounts, which are mainly written in
modern standard Arabic (MSA), whereas we as-
sume that tweets including emoticons (which we
use for acquiring polar instances) are mainly writ-
ten in dialectal Arabic (DA). In future work, we
plan to investigate this hypothesis further by au-
tomatically detecting MSA/DA for a given tweet,
e.g. (Zaidan and Callison-Burch, 2013). Abdul-
Mageed et al. (2012) show that having such a fea-
ture can result in no significant impact on the over-
all performance of both subjectivity and sentiment
analysis tasks.
Positive vs. negative: For sentiment classifica-
tion, the performance of the emoticon-based ap-
proach degrades notably to 51%, which is still
176
Data-set majority
baseline
fully super-
vised
emoticon DS lexicon-
presence
lexicon-aggr.
F Acc. F Acc. F Acc. F Acc. F Acc.
polar vs. neutral 0.69 53.0 0.43 46.62 0.95 95.19 0.95 95.09 0.91 91.09
positive vs. negative 0.67 50.89 0.41 49.65 0.51 51.25 0.53 57.06 0.52 52.98
Table 4: 2-level and single-level SSA classification using distant supervision (DS).
significantly better that the fully supervised base-
line, but nevertheless worse than a simple majority
baseline. These results are much lower than previ-
ous results on emoticon-based sentiment analysis
on English tweets by (Go et al., 2009; Bifet and
Frank, 2010) which both achieved around 83% ac-
curacy. The confusion matrix shows that mostly
negative instances are misclassified as positive,
with a very low recall on negative instances, see
Table 5. Next, we investigate possible reasons in a
detailed error analysis.
Data set Precision Recall
emoticon DS
positive 0.479 0.81
negative 0.556 0.212
lexicon-presence DS
positive 0.521 0.866
negative 0.733 0.317
lexicon-aggregation DS
positive 0.496 0.650
negative 0.583 0.426
Table 5: Recall and precision for Sentiment Anal-
ysis
3.1.1 Error Analysis for Emoticon-Based DS
In particular, we investigate the use of sarcasm and
the direction emoticons face in right-to-left alpha-
bets.
Use of sarcasm and irony: Using an emoticon
as a label is naturally noisy, since we cannot know
for sure the intended meaning the author wishes
to express. This is especially problematic when
emoticons are used in a sarcastic way, i.e. their
intended meaning is the opposite of the expressed
emotion. An example from our data set is:
(1) ):
?


?
?@ A

K


?J


?
g
.
great job Ahli :( ? refer-
ring to a famous football
team.
Research in psychology shows that up to 31% of
the time, emoticons are used sarcastically (Wolf,
2000). In order to investigate this hypothesis
we manually labelled a random sample of 303
misclassified instances for neutral, positive, nega-
tive, as well as sarcastic, mixed and unclear sen-
timents, see Table 6. Interestingly, the sarcas-
tic instances represent only 4.29%, while tweets
with mixed (positive and negative) sentiments rep-
resent 5.94% of the manually annotated sub-set.
In 34.32% of the instances, the manual labels
have matched the automatic emoticon-based la-
bels. Surprisingly, automatic emoticon-based la-
bel contrasts the manual labels in 36.63% of the
instances. Instances labelled as neutral represent
4.95%. The rest of the instances were assigned
?unclear sentiment orientation?.
Emoticon
Label
Predicted
label
Manual label # in-
stances
Positive Negative Mixed 8
Negative Positive Mixed 10
Positive Negative Negative 59
Negative Positive Negative 42
Positive Negative Neutral 29
Negative Positive Neutral 7
Positive Negative Positive 62
Negative Positive Positive 52
Positive Negative Sarcastic 8
Negative Positive Sarcastic 5
Positive Negative Unclear senti-
ment indicator
19
Negative Positive Unclear senti-
ment indicator
2
Table 6: Results of labelling sarcasm, mixed emo-
tions and unclear sentiment for misclassified in-
stances.
Facing of emoticons: We therefore investigate
another possible error source following (Mourad
and Darwish, 2013), who claim that the right-to-
left alphabetic writing of Arabic might result in
emoticons being mistakenly interchanged while
typing. On some Arabic keyboards, typing ? )?
will produce the opposite ? (? parentheses. The
following example (2) illustrates a case of a mis-
classified instance, where we assume that the fac-
ing of emoticons might have been interchanged or
mistyped.
(2) (: ??@ ?


	
?A

? ?C

	
g no hope anymore :)
3.2 Lexicon-Based Distant Supervision
To avoid the issue of ambiguity in the direction
of facing, we experiment with a lexicon-based ap-
proach to DS: instead of using emoticons, we now
177
utilise the adjectives in our sentiment lexicon as
noisy labels. We experiment with two different
settings for the lexicon-based DS approach. First,
we experiment with a lexicon-presence approach
that automatically labels a tweet as a positive in-
stance if it only includes positive lexicon(s) and
the same for the negative class. Data instances
having mixed positive and negative lexicons or no
sentiment lexicons are excluded from the training
set. The second approach is based on assigning
a numerical value to sentiment words and aggre-
gating the value into a single score, see Section 2.
The results are summarised in Table 4.
Polar vs. neutral: We can observe that the mod-
els trained with the lexicon-presence approach sig-
nificantly outperform the majority baseline, the
fully supervised learning, as well as the lexicon-
aggregation approach. The lexicon-presence and
the emoticon-based DS approaches reach almost
identical performance on our test set.
Positive vs. negative: Again, we observe that
it is difficult to discriminate negative instances
for both lexicon-based approaches. The lexicon-
presence approach significantly outperforms the
majority baseline, the fully supervised learn-
ing, and the lexicon-aggregation approach. But
this time it also significantly outperforms the
emoticon-based approach, which allows us to con-
clude that lexicon-based labelling introduces less
noise for sentiment analysis. However, our re-
sults are significantly worse than the lexicon-based
approach of Taboada et al. (2011), with up to
80% accuracy, and the learning-based approach
of Zhanh et al. (2011), with up to 85% accu-
racy on English tweets. The lexicon-presence ap-
proach achieves the highest precision for negative
tweets, see table 5, but still has a low recall. The
lexicon-aggregation approach has the highest re-
call for negative tweets, but its precision is almost
identical to the emoticon-based approach.
3.2.1 Error Analysis for Lexicon-Based DS
We conduct an error analysis in order to fur-
ther investigate the difference in performance
between the lexicon-presence and the lexicon-
aggregation approach. We hypothesise that the
lexicon-aggregation might perform better on in-
stances with mixed emotions, i.e. tweets with
positive and negative indicators, but a clear over-
all sentiment. We therefore manually add 36 in-
stances to the test set which contain mixed emo-
tions (but a unique sentiment label). However, the
results on the new test set confirm the superiority
of the lexicon-presence approach. In general, both
lexicon-based approaches perform worse for sen-
timent classification. Taboada et al. (2011) high-
light the issue of ?positive bias? associated with
lexicon-based approaches of sentiment analysis,
as people tend to prefer using positive expressions
and understate negative ones.
4 Conclusion and Future Work
We address the task of subjectivity and sentiment
analysis (SSA) for Arabic Twitter feeds. We em-
pirically investigate the performance of distant su-
pervision (DS) approaches on a manually labelled
independent test set, in comparison to a fully su-
pervised baseline, trained on a manually labelled
gold standard data set. Our experiments reveal:
(1) DS approaches to SSA for Arabic Twitter
feeds show significantly higher performance in ac-
curacy and F-score than a fully supervised ap-
proach. Despite providing noisy labels, they allow
larger amounts of data to be rapidly annotated, and
thus, can account for the topic shifts observed in
social media.
(2) DS approaches which use a subjectivity lex-
icon for labelling outperform approaches using
emoticon-based labels for sentiment analysis, but
achieve a very similar performance for subjectiv-
ity detection. We hypothesise that this can be at-
tributed to unclear facings of the emoticons.
(3) We also find that both our DS approaches
achieve good results of up to 95% accuracy for
subjectivity analysis, which is comparable to pre-
vious work on English tweets. However, we detect
a decrease in performance for sentiment analysis,
where negative instances repeatedly get misclas-
sified as positive. We assume that this can be at-
tributed to the more indirect ways adopted by peo-
ple to express their emotions verbally via social
media (Taboada et al., 2011). Other possible rea-
sons for this, which we will explore in future work,
include culturally specific differences (Hong et al.,
2011), as well as pragmatic/ context-dependent as-
pects of opinion (Sayeed, 2013).
178
References
Muhammad Abdul-Mageed and Mona Diab. 2012.
AWATIF: A multi-genre corpus for modern standard
Arabic subjectivity and sentiment analysis. In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and senti-
ment analysis of modern standard Arabic. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 587?591, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Muhammad Abdul-Mageed, Sandra Kuebler, and
Mona Diab. 2012. SAMAR: A system for subjec-
tivity and sentiment analysis of Arabic social me-
dia. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 19?28. Association for Computa-
tional Linguistics.
Rania Al-Sabbagh and Roxana Girju. 2012. YADAC:
Yet another dialectal Arabic corpus. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey. European Language Resources Associ-
ation (ELRA).
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Discov-
ery Science, pages 1?15. Springer.
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In Proceedings of NAACL-
HLT, pages 359?369.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Lichan Hong, Gregorio Convertino, and Ed H Chi.
2011. Language matters in twitter: A large scale
study. In ICWSM.
Ahmed Mourad and Kareem Darwish. 2013. Sub-
jectivity and sentiment analysis of modern stan-
dard Arabic and Arabic microblogs. WASSA 2013,
page 55.
Owen Rambow Nizar Habash and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, pos
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools, Cairo, Egypt, April.
The MEDAR Consortium.
A. Pak and P. Paroubek. 2010. Twitter as a corpus for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
fication. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 482?491, Avi-
gnon, France, April. Association for Computational
Linguistics.
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop, pages 43?48. Association
for Computational Linguistics.
Eshrag Refaee and Verena Rieser. 2014. An Arabic
twitter corpus for subjectivity and sentiment anal-
ysis. In Proceedings of the Ninth International
Conference on Language Resources and Evalua-
tion (LREC?14), Reykjavik, Iceland, may. European
Language Resources Association (ELRA).
Asad Sayeed. 2013. An opinion about opinions about
opinions: subjectivity and the aggregate reader. In
Proceedings of NAACL-HLT, pages 691?696.
Jared Suttles and Nancy Ide. 2013. Distant supervi-
sion for emotion classification with discrete binary
values. In Computational Linguistics and Intelligent
Text Processing, pages 121?136. Springer.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Com-
putational Linguistics, ACL ?99, pages 246?253,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399?433.
Ian H Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.
Alecia Wolf. 2000. Emotional expression online: Gen-
der differences in emoticon use. CyberPsychology
& Behavior, 3(5):827?833.
Zheng Yuan and Matthew Purver. 2012. Predicting
emotion labels for chinese microblog texts. In Pro-
ceedings of the 1st International Workshop on Senti-
ment Discovery from Affective Data (SDAD), pages
40?47, Bristol, UK, September.
Omar F. Zaidan and Chris Callison-Burch. 2013. Ara-
bic dialect identification. Computational Linguis-
tics.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-
ichun Hsu, and Bing Liu. 2011. Combining lex-
iconbased and learning-based methods for twitter
sentiment analysis. HP Laboratories, Technical Re-
port HPL-2011, 89.
179
Proceedings of the SIGDIAL 2014 Conference, pages 260?262,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
The Parlance Mobile Application for Interactive Search in
English and Mandarin
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos,
Hugues Bouchard, Catherine Breslin, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Ga?i?, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Tim Potter, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay,
Boris Villazon-Terrazas, Majid Yazdani, Steve Young and Yanchao Yu
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
We demonstrate a mobile application in
English and Mandarin to test and eval-
uate components of the Parlance di-
alogue system for interactive search un-
der real-world conditions.
1 Introduction
With the advent of evaluations ?in the wild?,
emphasis is being put on converting re-
search prototypes into mobile applications that
can be used for evaluation and data col-
lection by real users downloading the ap-
plication from the market place. This is
the motivation behind the work demonstrated
here where we present a modular framework
whereby research components from the Par-
lance project (Hastie et al., 2013) can be
plugged in, tested and evaluated in a mobile
environment.
The goal of Parlance is to perform inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
Cambridge, UK for Mandarin and San Fran-
cisco, USA for English. The scenario is that
Mandarin speaking tourists would be able to
download the application and use it to learn
about restaurants in English speaking towns
and cities.
2 System Architecture
Here, we adopt a client-server approach as il-
lustrated in Figure 1 for Mandarin and Figure
2 for English. The front end of the demon-
stration system is an Android application that
calls the Google Automatic Speech Recogni-
tion (ASR) API and sends the recognized user
utterance to a server running the Interaction
?Authors are in alphabetical order
Manager (IM), Spoken Language Understand-
ing (SLU) and Natural Language Generation
(NLG) components.
Figure 1: Overview of the Parlance Man-
darin mobile application system architecture
Figure 2: Overview of the Parlance En-
glish mobile application system architecture
extended to use the Yahoo API to populate
the application with additional restaurant in-
formation
When the user clicks the Start button, a di-
alogue session starts. The phone application
first connects to the Parlance server (via
the Java Socket Server) to get the initial sys-
tem greeting which it speaks via the Google
260
Text-To-Speech (TTS) API. After the system
utterance finishes the recognizer starts to lis-
ten for user input to send to the SLU compo-
nent. The SLU converts text into a semantic
interpretation consisting of a set of triples of
communicative function, attribute, and (op-
tionally) value1. Probabilities can be associ-
ated with candidate interpretations to reflect
uncertainty in either the ASR or SLU. The
SLU then passes the semantic interpretation
to the IM within the same server.
Chinese sentences are composed of strings of
characters without any space to mark words as
other languages do, for example:
In order to correctly parse and understand
Chinese sentences, Chinese word segmenta-
tions must be performed. To do this segmen-
tation, we use the Stanford Chinese word seg-
mentor2, which relies on a linear-chain condi-
tional random field (CRF) model and treats
word segmentation as a binary decision task.
The Java Socket Server then sends the seg-
mented Chinese sentence to the SLU on the
server.
The IM then selects a dialogue act, accesses
the database and in the case of English passes
back the list of restaurant identification num-
bers (ids) associated with the relevant restau-
rants. For the English demonstration system,
these restaurants are displayed on the smart
phone as seen in Figures 4 and 5. Finally,
the NLG component decides how best to re-
alise the restaurant descriptions and sends the
string back to the phone application for the
TTS to realise. The example output is illus-
trated in Figure 3 for Mandarin and Figure 4
for English.
As discussed above, the Parlance mobile
application can be used as a test-bed for com-
paring alternative techniques for various com-
ponents. Here we discuss two such compo-
nents: IM and NLG.
1This has been implemented for English; Mandarin
uses the rule-based Phoenix parser.
2http://nlp.stanford.edu/projects/chinese-
nlp.shtml
Figure 3: Screenshot and translation of the
Mandarin system
Figure 4: Screenshot of dialogue and the list
of recommended restaurants shown on a map
and in a list for English
2.1 Interaction Management
The Parlance Interaction Manager is based
on the partially observable Markov decision
process (POMDP) framework, where the sys-
tem?s decisions can be optimised via reinforce-
ment learning. The model adopted for Par-
lance is the Bayesian Update of Dialogue
State (BUDS) manager (Thomson and Young,
2010). This POMDP-based IM factors the di-
alogue state into conditionally dependent ele-
ments. Dependencies between these elements
can be derived directly from the dialogue on-
tology. These elements are arranged into a dy-
namic Bayesian network which allows for their
marginal probabilities to be updated during
the dialogue, comprising the belief state. The
belief state is then mapped into a smaller-scale
summary space and the decisions are optimised
using the natural actor critic algorithm. In the
Parlance application, hand-crafted policies
261
Figure 5: Screenshot of the recommended
restaurant for the English application
can be compared to learned ones.
2.2 Natural Language Generation
As mentioned above, the server returns the
string to be synthesised by the Google TTS
API. This mobile framework allows for testing
of alternative approaches to NLG. In particu-
lar, we are interested in comparing a surface re-
aliser that uses CRFs against a template-based
baseline. The CRFs take semantically anno-
tated phrase structure trees as input, which it
uses to keep track of rich linguistic contexts.
Our approach has been compared with a num-
ber of competitive state-of-the art surface real-
izers (Dethlefs et al., 2013), and can be trained
from example sentences with annotations of se-
mantic slots.
2.3 Local Search and Knowledge Base
For the English system, the domain database is
populated by the search Yahoo API (Bouchard
and Mika, 2013) with restaurants in San Fran-
sisco. These restaurant search results are
returned based on their longitude and lati-
tude within San Francisco for 5 main areas, 3
price categories and 52 cuisine types contain-
ing around 1,600 individual restaurants.
The Chinese database has been partially
translated from an English database for restau-
rants in Cambridge, UK and search is based
on 3 price categories, 5 areas and 35 cuisine
types having a total of 157 restaurants. Due
to the language-agnostic nature of the Par-
lance system, only the name and address
fields needed to be translated.
3 Future Work
Investigating application side audio compres-
sion and audio streaming over a mobile in-
ternet connection would enable further assess-
ment of the ASR and TTS components used
in the original Parlance system (Hastie et
al., 2013). This would allow for entire research
systems to be plugged directly into the mobile
interface without the use of third party ASR
and TTS.
Future work also involves developing a feed-
back mechanism for evaluation purposes that
does not put undue effort on the user and put
them off using the application. In addition,
this framework can be extended to leverage
hyperlocal and social information of the user
when displaying items of interest.
Acknowledgements
The research leading to this work was funded
by the EC FP7 programme FP7/2011-14
under grant agreement no. 287615 (PAR-
LANCE).
References
H. Bouchard and P. Mika. 2013. Interactive hy-
perlocal search API. Technical report, Yahoo
Iberia, August.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and
O. Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation Using Global
Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (ACL), Sofia, Bulgaria.
H. Hastie, M.A. Aufaure, P. Alexopoulos,
H. Cuay?huitl, N. Dethlefs, M. Gasic,
J. Henderson, O. Lemon, X. Liu, P. Mika,
N. Ben Mustapha, V. Rieser, B. Thomson,
P. Tsiakoulis, Y. Vanrompay, B. Villazon-
Terrazas, and S. Young. 2013. Demonstration
of the PARLANCE system: a data-driven
incremental, spoken dialogue system for in-
teractive search. In Proceedings of the 14th
Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), Metz,
France, August.
B. Thomson and S. Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588.
262
