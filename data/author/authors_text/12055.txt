Annotation and Disambiguation of Semantic Types in Biomedical 
Text: a Cascaded Approach to Named Entity Recognition
Dietrich Rebholz-Schuhmann, Harald Kirsch, 
Sylvain Gaudan, Miguel Arregui
European Bioinformatics Institute (EBI), 
Wellcome Trust Genome Campus, Hinxton, Cambridge, UK
{rebholz,kirsch,gaudan,arregui}@ebi.ac.uk
Goran Nenadic
School of Informatics
University of Manchester
Manchester, UK
g.nenadic@manchester.ac.uk
Abstract
Publishers  of  biomedical  journals  in-
creasingly  use  XML  as  the  underlying 
document format.  We present a modular 
text-processing pipeline that inserts XML 
markup  into  such  documents  in  every 
processing step, leading to multi-dimen-
sional markup.  The markup introduced 
is  used  to  identify  and  disambiguate 
named entities of several semantic types 
(protein/gene,  Gene  Ontology  terms, 
drugs  and species)  and to  communicate 
data from one module to the next.  Each 
module  independently  adds,  changes  or 
removes markup, which allows for mod-
ularization  and  a  flexible  setup  of  the 
processing  pipeline.  We  also  describe 
how the cascaded approach is embedded 
in  a  large-scale  XML-based  application 
(EBIMed) used for on-line access to bio-
medical  literature.   We discuss  the  les-
sons  learnt  so  far,  as  well  as  the  open 
problems  that  need  to  be  resolved.   In 
particular,  we  argue  that  the  pragmatic 
and tailored solutions allow for reduction 
in  the need for overlapping annotations 
? although not completely without cost.
1 Introduction
Publishers  of  biomedical  journals  have  widely 
adopted  XML  as  the  underlying  format  from 
which other formats,  such as PDF and HTML, 
are generated.  For example, documents in XML 
format are available from the National Library of 
Medicine1 (Medline abstracts and Pubmed2 Cent-
ral documents), and from BioMed Central3 (full 
text journal articles). Other publishers are head-
ing  into  the  same  direction.   Such  documents 
contain logical markup to organize meta-inform-
1 National Library of Medicine, http://www.nlm.nih.gov/
2 PubMed, http://www.pubmed.org 
3 BioMed Central Ltd, http://www.biomedcentral.com/
ation such as title, author(s), sections, headings, 
citations,  references,  etc.   Inside  the  text  of  a 
document,  XML is  used  for  physical  markup, 
e.g. text in italic or boldface, subscript and super-
script  insertions,  etc.   Manually  generated  se-
mantic markup is available only on the document 
level (e.g. MeSH terms).
One of the most distinguished feature of sci-
entific biomedical literature is that it contains a 
large amount of terms and entities, the majority 
of  which are explained in public electronic data-
bases. Terms (such as names of genes, proteins, 
gene products, organisms, drugs, chemical com-
pounds, etc.) are a key factor for accessing and 
integrating  the  information  stored  in  literature 
(Krauthammer  and  Nenadic,  2004).  Identifica-
tion  and  markup  of  names  and  terms  in  text 
serves several purposes:  
(1) The users profit from highlighted semantic 
types, e.g. protein/gene, drug, species, and from 
links to the defining database for immediate ac-
cess and exploration.
(2) Identified terms facilitate and improve stat-
istical and NLP based text analysis (Hirschman 
et al, 2005; Kirsch et al, 2005).  
In this paper we describe a cascaded approach 
to named-entity recognition (NER) and markup 
in biomedicine that is embedded into EBIMed4, 
an on-line service to access the literature (Reb-
holz-Schuhmann  et  al.,  forthcoming).  EBIMed 
facilitates  both  purposes  mentioned  above.   It 
keeps the annotations provided by publishers and 
inserts  XML  annotations  while  processing  the 
text.  Named entities from different resources are 
identified  in  the  text.  The  individual  modules 
provide annotation of protein names with unique 
identifiers, disambiguation of protein names that 
are  ambiguous  acronyms,  annotation  of  drugs, 
Gene Ontology5 terms and species.  The identi-
fication of protein named entities can be further 
used in an alternative pipeline to identify events 
4 EBIMed, www.ebi.ac.uk/Rebholz-srv/ebimed
5 GO, Gene Ontology, http://geneontology.org, (GO con-
sortium, 2005).
11
such as  protein-protein interactions  and associ-
ations between terms and mutations (Blaschke et 
al., 1999; Rzhetsky et al, 2004; Rebholz-Schuh-
mann  et  al.,  2004;  Nenadic  and  Ananiadou, 
2006).
The rest of the paper is organised as follows. 
In  Section  2  we  briefly  discuss  problems with 
biomedical NER.  The cascaded approach and an 
online text mining system are described in sec-
tions 3 and 4 respectively.  We discuss the les-
sons learnt from the on-line application and re-
mainig open problems in Section 5, while con-
clusions are presented in Section 6.
2 Biomedical Named Entity Recognition 
Terms and named-entities (NEs) are the means 
of scientific communication as they are used to 
identify  the  main  concepts  in  a  domain.  The 
identification  of  terminology in  the  biomedical 
literature is one of the most challenging research 
topics  both  in  the  NLP  and  biomedical  com-
munities (Hirschman et al,  2005; Kirsch et al, 
2005).
Identification  of  named  entities  (NEs)  in  a 
document can be viewed as a three-step proced-
ure (Krauthammer and Nenadic, 2004).  In the 
first step, single or multiple adjacent words that 
indicate the presence of domain concepts are re-
cognised (term recognition). In the second step, 
called  term categorisation, the recognised terms 
are classified into broader domain classes (e.g. as 
genes, proteins, species). The final step is  map-
ping of terms into referential databases. The first 
two steps are commonly referred to as named en-
tity recognition (NER).
One of the main challenges in NER is a huge 
number of new terms and entities that appear in 
the  biomedical  domain.  Further,  terminological 
variation, recognition of boundaries of multiword 
terms, identification of nested terms and ambigu-
ity of terms are the difficult issues when mapping 
terms from the literature to biomedical database 
entries  (Hirschman  et  al.,  2005;  Krauthammer 
and Nenadic, 2004). 
On one hand, NER in the biomedical domain 
(in particular  the recognition part)  profits  from 
large,  freely available terminological  resources, 
which  are  either  provided  as  ontologies  (e.g. 
Gene Ontology, ChEBI6, UMLS7) or result from 
biomedical  databases containing named entities 
(e.g.  UniProt/Swiss-Prot8).  On  the  other  hand, 
combining sets of terms from different termino-
6 ChEBI, Chemical Entities of Biological Interest, 
http://www.ebi.ac.uk/chebi/m
7  UMLS, Unified Medical Language System 
http://www.nlm.nih.gov/research/umls/, (Browne et al, 
2003).
logical resources leads to naming conflicts such 
as homonymous use of names and terminological 
ambiguities. The most obvious problem is when 
the same span of text is assigned to different se-
mantic types (e.g. ?rat? denotes a species and a 
protein). In this case, there are three types of am-
biguities:  
(Amb1) A name is used for different entries in 
the  same database,  e.g.  the  same protein name 
serves  for  a  given  protein  in  different  species 
(Chen et al, 2005).
(Amb2) A name is used for entries in multiple 
databases and thus represents different types, e.g. 
?rat? is a protein and a species.
(Amb3) A name is not only used as a biomed-
ical term but also as part of common English (in 
contrast  to  the  biomedical  terminology),  e.g. 
?who? and  ?how?,  which  are  used  as  protein 
names.
In some cases (i.e. Amb2), broader classifica-
tion can help to disambiguate between different 
entries  (e.g.  differentiate  between  ?CAT? as  a 
protein, animal or medical device). However, it 
is ineffective in situations where names can be 
mapped to several different entries in the same 
data  source.  In  such  situations,  disambiguation 
on the resource level is needed (see, for example, 
(Liu et al, 2002) for disambiguation of terms as-
sociated with several entries in the UMLS Meta-
thesaurus).
In many solutions, the three steps in biomedic-
al NER (namely, recognition, categorisation and 
mapping  to  databases)  are  merged  within  one 
module. For example, using an existing termino-
logical database for recognition of NEs, effect-
ively  leads  to  complete  term  identification  (in 
cases where there are no ambiguities). Some re-
searchers, however, have stressed the advantages 
of tackling each step as a separate task, pointing 
at different sources and methods needed to ac-
complish each of the subtasks (Torii et al, 2003; 
Lee et al, 2003). Also, in the case of modularisa-
tion,  it  is  easier  to  integrate  different  solutions 
for each specific problem. However, it has been 
suggested  that  whether  a  clear  separation  into 
single steps would improve term identification is 
an  open  issue  (Krauthammer  and  Nenadic, 
2004). In this paper we discuss a cascaded, mod-
ular approach to biomedical NER.
3 Biomedical  NER based on XML an-
notation:  Modules in a pipeline
In this Section we present a modular approach to 
identification, disambiguation and annotation of 
8 UniProt, http://www.ebi.uniprot.org/, (Bairoch et al, 
2005); Swiss-Prot, http://ca.expasy.org/sprot/
12
several  biomedical  semantic  types  in  the  text. 
Full identification of NEs and resolving ambigu-
ities in particular, may require a full parse tree of 
a  sentence  in  addition  to  the  analysis  of  local 
context  information.  On  the  other  hand,  full 
parse trees may be only derivable after NEs are 
resolved.  Methods to efficiently overcome these 
problems are not yet available today and in order 
to come up with an applicable solution,  it  was 
necessary to choose a more pragmatic approach.
We  first  discuss  the  basic  principles  and 
design of the processing pipeline, which is based 
on  a  pragmatic  cascade  of  modules,  and  then 
present each of the modules separately.
3.1 Modular  design  of  a  text  processing 
pipeline
Our methodology is based on the idea of separat-
ing the process into clearly defined functions ap-
plied one after  another  to  text,  in  a processing 
pipeline characterized  by  the  following  state-
ments:
(P1)  The complete  text  processing task con-
sists of separate and independent modules.
(P2)  The  task  is  performed  by  running  all 
modules exactly once in a fixed sequence.
(P3) Each module operates continuously on an 
input  stream and  performs  its  function  on 
stretches or  ?windows? of text  that  are usually 
much smaller than the whole input.  As soon as a 
window is  processed,  the module  produces  the 
resulting output.
(P4) After the startup phase, all modules run 
in parallel.  Incoming requests for annotation are 
accepted by a master process that ensures that all 
required modules are approached in the right or-
der.
(P5) Communication of information between 
the modules is strictly downstream and all meta-
information is contained in the data stream itself 
in the form of XML markup.
An instance of a processing pipeline (which is 
actually  embedded in  EBIMed)  is  presented in 
Figure 1. The modules M-1 to M-8 are run in this 
order,  and  no  communication  between them is 
needed apart  from streaming the  text  from the 
output  of  one  module  to  the  input  of  another. 
The text contains the meta-data as XML markup. 
The modules are described below.
Figure 1. A processing pipeline embedded in 
EBIMed
Although  this  is  the  standard  pipeline  for 
EBIMed, it is possible to re-arrange the modules 
to  favour  identification  of  specific  semantic 
types. More precisely, in our modular approach, 
after identification of a term in the text, disam-
biguation only decides whether the term is of that 
type or not. If it is not, the specific annotation is 
removed and left to the downstream modules to 
tag the term differently.  While this  requires  n 
identification steps, adding identification of new 
types is independent of modules already present. 
However, the prioritization of semantic types is 
enforced  by  the  order  of  the  associated  term 
identification modules.  
3.2 Input documents and pre-processing
Input  documents  are  XML-formatted  Medline 
abstracts as provided from the National Library 
of  Medicine  (NLM).   The  XML  structure  of 
Medline abstracts includes meta information at-
tached  to  the  original  document,  such  as  the 
journal, author list, affiliations, publication dates 
as well as annotations inserted by the NLM such 
as  creation  date  of  the  Medline  entry,  list  of 
chemicals associated with the document, as well 
as related MeSH headings. 
The  text  processing  modules  are  only  con-
cerned with the document  parts  that  consist  of 
natural  language  text.   In  Medline  abstracts, 
these stretches of text are marked up as  Article-
Title and AbstractText.  Inside these elements we 
add  another  XML element,  called  text,  to  flag 
natural language text independent of the original 
input  document format  (module  M-1 in  Figure 
1).  Thereby the subsequent text processing mod-
ules become independent of the document struc-
ture: other document types, e.g. BioMed Central 
13
full  text  papers,  can  easily  be  fed  into  the 
pipeline providing a simple adaptation of the in-
put pre-processor. 
As  a  final  pre-processing  step  (M-2),  sen-
tences  are  identified  and  marked  using  the 
<SENT> tag.
3.3 Finding protein names in text
For identification of protein names (M-3 in Fig-
ure  1),  we  use  an  existing  protein  repository. 
UniProt/Swiss-Prot  contains  roughly  190,000 
protein/gene  names  (PGNs)  in  database  entries 
that also annotate proteins with protein function, 
species  and  tissue  type.   PGNs  from 
UniProt/Swiss-Prot are matched with regular ex-
pressions which account for morphological vari-
ability.  These terms are tagged using the <z:uni-
prot> tag (see Figure 2).  The list  of  identifiers 
(ids attribute) contains the accession numbers of 
the mentioned protein in the UniProt/Swiss-Prot 
database.  All  synonyms  from a  database  entry 
are kept,  and in the case of homonymy, where 
one name refers to several  database entries,  all 
accession numbers are stored.  The pair consist-
ing of the database name and the accession num-
ber(s) forms a unique identifier (UID) that rep-
resents  the  semantics  of  the  term  and  can  be 
trivially  rewritten  into  a  URL  pointing  to  the 
database entry. Each entity also contains the at-
tribute  fb which  provides  the  frequency of  the 
term in the British National Corpus (BNC).
3.4 Resolving (some) protein name ambigu-
ities
The approach to finding names that we presented 
can create three types of ambiguities mentioned 
above in Section 2.
In the current implementation,  Amb1 (ambi-
guity  within a  given resource)  is  not  resolved. 
Rather, the links to  all entries in the same data-
base are maintained.  Amb2 and Amb3 are par-
tially  resolved  for  protein/gene  names  as  ex-
plained below (steps M-4 and M-5).  Note that 
Amb2 is  resolved  on  ?first-come  first-serve? 
basis, meaning that an annotation introduced by 
one module is not overwritten by a subsequent 
module.
Many protein names are indeed or at least look 
like abbreviations. It has been proved that ambi-
guities of abbreviations and acronyms found in 
Medline abstracts can be automatically resolved 
with high accuracy  (Yu et  al.,  2002;  Schwartz 
and Hearst, 2003; Gaudan et al, 2005). 
<SENT sid=?2? pm=?.?> Aberrant 
Wnt signaling, which results from 
mutations of either <z:uniprot fb=?0? ids=?P26233,P35222,P35223, 
P35224,Q02248, Q9WU82?>beta-
catenin</z:uniprot> or adenomat-
ous polyposis coli (<z:uniprot fb=?28? ids=?P25054?>APC </z:uni-prot>), renders <z:uniprot fb=?0? ids= ?P26233,P35222, P35223, 
P35224,Q02248, Q9WU82?> beta-
catenin</z:uniprot> resistant to 
degradation, and has been associ-
ated with multiple types of human 
cancers 
</SENT>
Figure 2. XML annotation of UniProt/Swiss-Prot 
proteins  .
In our approach (Gaudan et al, 2005) all ac-
ronyms from Medline have been gathered togeth-
er  
with their expanded forms, called senses.  In ad-
dition all morphological and syntactical variants 
of a known expanded form have been extracted 
from Medline.   Expanded forms were  categor-
ised  into  classes  of  semantically  equivalent 
forms.   Feature  representations  of  Medline  ab-
stracts containing the acronym and the expanded 
form were used to train support vector machines 
(SVMs).   Disambiguation of acronyms to their 
senses in Medline abstracts based on the SVMs 
was achieved at an accuracy of above 98%.  This 
was independent from the presence of the expan-
ded form in the Medline abstract.  This disam-
biguation solution lead to the solution integrated 
into the processing pipeline.  
A potential protein has to be evaluated against 
three possible outcomes: either a name is an ac-
ronym and can be resolved as (a) a protein or (b) 
not a protein, or (c) a name cannot be resolved. 
To  distinguish  cases  (a)  and  (b)  the  document 
content  is  processed  to  identify  the  expanded 
form of the acronym and to check whether the 
expanded form refers to a protein name.  In case 
of (c), the frequency of the name in the  British 
National  Corpus  (BNC)  is  compared  with  a 
threshold.   If  the  frequency is  higher  than  the 
threshold, the name is assumed not to be a pro-
tein name.  The threshold was chosen not to ex-
clude important protein names that have already 
entered common English (such as insulin).
The disambiguation module (M-4) runs on the 
results of the previous module that performs pro-
tein-name  matching  and  indiscriminately  as-
sumes each match to  be  a  protein name.  The 
14
module  M-4 marks  up all  known acronym ex-
pansions in the text and combines the two pieces 
of  information:  a  marked  up  protein  name  is 
looked up in the list of abbreviations.  If the ab-
breviation has an expansion that is marked up in 
the vicinity and denotes a protein name, the ab-
breviation is verified as a protein name (case (a) 
above)  by  adding  an  attribute  with  a  suitable 
value to the protein tag.  The annotation also in-
cludes  the  normalised  form  of  the  acronym, 
which serves as an identifier for further database 
lookups. Similarly, if the expansion is clearly not 
a protein name, the same attribute is used with 
the according value.
Finally, the module M-5 removes the protein 
name markup if the name is either (b) clearly not 
a  protein,  or  in case (c)  has a BNC frequency 
beyond the threshold.
3.5 Finding other names in text
Further modules (M-6, M-7 and M-8 in Fig. 1) 
perform  matching  and  markup  for  drugs  from 
MedlinePlus9,  species  from Entrez  Taxonomy10 
and terms from the Gene Ontology (GO).  As for 
proteins,  the  semantic  type  is  signified  by  the 
element name and a unique ID referencing the 
source database is added as an attribute.  Disam-
biguation for these names and terms is, however, 
not yet available.
Finding GO ontology terms in text can be dif-
ficult, as these names are typically ?descriptions? 
rather than real terms (e.g. GO:0016886,  ligase 
activity,  forming  phosporic  ester  bonds),  and 
therefore  are  not  likely  to  appear  in  text  fre-
quently  (McCray  et  al.,  2002;  Verspoor  et  al., 
2003; Nenadic et al, 2004).
Figure 3 shows an example of a sentence an-
notated for semantic types and POS information 
using the pipeline from the Figure 1.  Note that 
POS tags are inside the type tags although type 
annotation has been performed prior to the POS 
tagging.
3.6 Other modules in the pipeline
The modular text processing pipeline of EBIMed 
is currently being extended to include other mod-
ules. The part-of-speech tagger (POS-tagger) is a 
separate module and combines tokenization and 
POS annotation.  It leaves previously annotated 
entities  as  single  tokens,  even  for  multi-word 
terms,  and  assigns  a  noun  POS  tag  to  every 
named entity.
9 MedlinePlus, National Library of Medicine, http://www.n-
lm.nih.gov/medlineplus/
10 Entrez Taxonomy, National Center for Biotechnology In-
formation, http://www.ncbi.nlm.nih.gov/entrez/
Shallow parsing is introduced as another layer 
in the multidimensional annotation of biomedical 
documents.  After the NER modules, the shallow 
parsing modules extract events of protein-protein 
interactions.  Shallow parsing basically annotates 
noun  phrases  (NP)  and  verb  groups.   Noun 
phrases  that  contain  a  protein  name  receive  a 
modified NP tag (Protein-NP) to simplify finding 
of  protein-protein interaction phrases.   Patterns 
of Protein-NPs in conjunction with selected verb 
groups are annotated as final result.
<abs id='1' db='unknown'>
<text><SENT sid="0" pm="."><tagged>
<tok><sur> </sur><lem cat="bos" 
mor=""></lem></tok><z:uniprot fb="0" ids="P50144,P50145"><tok><sur>Cholecystokinin</sur><lem 
cat="n" mor=":e">cholecystokinin</lem></tok> </z:uniprot>
<tok><sur>and</sur><lem cat="cnj" 
mor=":K">and</lem></tok><z:uniprot fb="4" ids="O02686,P01350"><tok><sur>gastrin</sur><lem cat="n" 
mor=":e">gastrin</lem></tok></z:uniprot>
<tok><sur>differed</sur><lem cat="v" 
mor=":V:P">differ</lem></tok>
<tok><sur>in</sur><lem cat="prep" 
mor="">in</lem></tok>
<tok><sur>stimulatin</sur><lem cat="n" 
mor=":e:m">stimulatin</lem></tok><z:uniprot fb="4" ids="O02686,P01350"><tok><sur>gastrin</sur><lem cat="n" 
mor=":e">gastrin</lem></tok></z:uniprot><z:go ids="GO:0046903" 
onto="biological_process"><tok><sur>secretion</sur><lem cat="n" 
mor=":e">secretion</lem></tok></z:go>
<tok><sur>in</sur><lem cat="prep" 
mor="">in</lem></tok><z:species ids="9986"><tok><sur>rabbit</sur><lem cat="n" 
mor=":e">rabbit</lem></tok></z:species>
<tok><sur>gastric</sur><lem cat="adj" 
mor=":b">gastric</lem></tok>
<tok><sur>glands</sur><lem cat="n" 
mor=":m">gland</lem></tok>
<tok><sur>.</sur><lem cat="eos" 
mor=""></lem></tok>
</tagged></SENT>
</text>
</abs>
Figure 3.  XML annotation of a sentence con-
taining different semantic types and POS tags. 
15
4 EBIMed
This cascaded approach to NER has been incor-
porated into EBIMed, a system for mining bio-
medical literature.
EBIMed is a service that combines document 
retrieval  with  co-occurrence-based  summariza-
tion  of  Medline  abstracts.  Upon  a  keyword 
query, EBIMed retrieves abstracts from EMBL-
EBI?s installation of Medline and filters for bio-
medical terminology.  The final result is organ-
ised in a view displaying pairs of concepts.  Each 
pair co-occurs in at least one sentence in the re-
trieved  abstracts.  The  findings  (e.g. 
UniProt/Swiss-Prot  proteins,  GO  annotations, 
drugs and species) are listed in conjunction with 
the  UniProt/Swiss-Prot  protein  that  appears  in 
the same biological context.  All terms, retrieved 
abstracts and extracted sentences are automatic-
ally linked to contextual information, e.g. entries 
in biomedical databases. 
The annotation modules are also available via 
HTTP  request  that  allows  for  specification  of 
which modules to run (cf. Whatizit11).  Note that 
with  suitable  pre-processing  to  insert  the 
<text> tags, even well formed HTML can be 
processed. 
5 Lessons Learnt so far
Our  text  mining  solution  EBIMed successfully 
applies multi-dimensional markup in a pipeline 
of text processing modules to facilitate online re-
trieval  and mining of  the  biomedical  literature. 
The final goal is semantic annotation of biomed-
ical  terms  with  UID,  and  ? in  the  next  step  ? 
shallow parsing based text  processing for  rela-
tionship  identification.   The  following  lessons 
have been learnt during design, implementation 
and use of our system.  
The end-users expect to see the original docu-
ment at all times and therefore we have to rely on 
proper  formatting  of  the  original  and  the  pro-
cessed text.  Consequently, when adding semant-
ic information, all  other meta-information must 
be  preserved  to  allow  for  proper  rendering  as 
similar  as  possible  to  the  original  document. 
Therefore,  our  approach  does  not  remove  any 
pre-existing annotations supplied by the publish-
er, i.e. the original document could be recovered 
by removing all introduced markup.
All modules only process sections of the docu-
ment containing the natural language text, which 
improves modularisation.  The document struc-
ture is irrelevant to single modules and facilitates 
reading  and  writing  to  the  input  and  output 
11 http://www.ebi.ac.uk/Rebholz-srv/whatizit/pipe
stream, respectively, without taking notice of the 
beginning and/or the end of a single document. 
All  information exchanged between modules is 
contained in the data stream. This facilitates run-
ning all the modules in a given pipeline in paral-
lel, after an initial start-up. Even more, the mod-
ules can be distributed on separate machines with 
no implementation overheads for the communic-
ation over the network.  Adding more modules 
with their own processors does not significantly 
impair overall runtime behaviour for large data-
sets and leads to fast text processing throughput 
combined with a reasonable ? albeit not yet per-
fect ? quality, which allows for new and prac-
tically  useful  text  mining  solutions  such  as 
EBIMed.
Modularisation  of  the  text  processing  tasks 
leads to improved scalability and maintainability 
inherent to all modular software solutions.  In the 
case of the presented solution, the modular ap-
proach allows for a selection of the setup and or-
dering of the modules, leading to a flexible soft-
ware design, which can be adapted to different 
types of documents and which allows for an (in-
cremental)  replacement  of  methods  to  improve 
the quality of the output.  This can also facilitate 
improved  interoperability  of  XML-based  NLP 
tools.
Semantic  annotation  of  named  entities  and 
terms  blends  effectively  with  logical  markup, 
simply because there is no overlap between doc-
ument  structure  and  named  entities  and  terms. 
On the other hand, some physical markup (such 
as <i> in the BMC corpus) is in some documents 
used to highlight names or terms of a semantic 
type, e.g. gene names.  With consistent semantic 
markup, this kind of physical tags could be aban-
doned to be replaced by external style informa-
tion.  However, some semantic annotations still 
must be combined with physical markup as in the 
term B-sup that initially was annotated by a pub-
lisher  as  <b>B</b>-sup and  that  now  (after 
NER)  would  be  marked  as 
<z:uniprot><b>B</b>-sup</z:uniprot>.
Matching  of  names  of  a  semantic  type,  e.g. 
protein/gene,  is  done on a ?longest  of  the left-
most? basis and prioritization of semantic types 
is enforced by the order of the term identification 
modules.   Both  choices  lead  to  the  result  that 
overlapping annotations are preempted and that 
annotations  automatically  endorse  a  link  to  a 
unique identifier, unless there are ambiguity on 
the  level  of  biomedical  resource..  This  type of 
ambiguity is not resolved in our text processing 
solution.  Instead, for a given biomedical term, 
links to all  entries referring to this  term in the 
same database are kept.  
16
One approach to the disambiguation of Amb2 
(multiple resources)  and  Amb3 (common Eng-
lish words) ambiguities would be to integrate all 
terms into  one massive dictionary,  identify  the 
strings in the text and then disambiguate between 
n semantic types.  This would require the disam-
biguation module be trained to distinguish all se-
mantic types. If a new type is added, the disam-
biguation  module  would  need  to  be  retrained, 
which limits the possibilities for expansion and 
tailoring of text mining solutions.
Open Problems: We consider two categories of 
open  problems:  NLP-based  and  XML-based 
problems.
Bio  NLP-based  problems include  challenges 
in recognition and disambiguation of biomedical 
names in text. One of the main issues in our ap-
proach  is  annotation  of  compound  and  nested 
terms.  The  presented  methodology can  lead  to 
the following annotations:
1. the head noun belongs to the same semantic 
type, but is not part of the protein name (as 
represented in the terminological resource): 
<z:uniprot>Wnt-2</z:uniprot> protein
2. the head noun belongs to a different semantic 
type not covered by any of the available ter-
minological resources:
<z:uniprot>WNT8B</z:uniprot> mRNA
3. a compound term consists of terms from dif-
ferent semantic types, but its semantic type is 
not known:
<z:uniprot  fb=?0?  ids=???>beta-
catenin</z:uniprot>  <z:go  ids=??? 
onto= ???>binding </z:go> domain
Therefore,  an important  open problem is the 
annotation of nested terms where an entity name 
is part of a larger term that may or may not be in 
one of the  dictionaries.  Once the  inner term is 
marked up with inline annotation, simple string 
pattern matching (utilised in our approach) can-
not be used easily to find the outer, because the 
XML structure is in the way.  A more effective 
solution could be  a combination of inline  with 
stand-off annotation.
Further, in a more complex case such as in
htr-wnt-<uniprot>A protein</uniprot>
neither wnt nor htr refer to a single protein but 
to a protein family, and whereas A protein is 
a known protein, this is not the case for wnt-A. 
The most obvious annotation <uniprot>htr-
wnt-A protein</uniprot> cannot be re-
solved  by  the  terminology  from  the 
UniProt/Swiss-Prot  database,  as  it  simply  does 
not exist in the database.
More work is also needed on disambiguation 
of  terms  that  correspond  to  common  English 
words.  
Annotation (i.e. XML)-based problems mainly 
relate to an open question whether different tag 
names should be used for various semantic types, 
or semantic types should be represented via at-
tributes  of  a  generalised  named entity  or  term 
tag.  In EBIMed, specific tags are used to denote 
specific semantic types.  A similar challenge is 
how to treat and make use of entities such as in-
line references, citations and formulas (typically 
annotated in journals), which are commonly ig-
nored by NLP modules.
The most important issue, however, is how to 
represent still unresolved ambiguities, so that an-
notations might be modified at a later stage, e.g. 
when POS information or even the full parse tree 
is available. This also includes the issues on kind 
of information that should be made available for 
later  processing.  For  example,  as  (compound) 
term identification is done before POS tagging, 
an  open  question  is  whether  POS  information 
should be assigned to individual components of a 
compound term (in addition to the term itself), 
since this information could be used to complete 
NER or adjust the results in a later stage.  
6 Conclusions 
In  this  paper,  we  have described  a  pipeline  of 
XML-based modules for  identification and dis-
ambiguation  of  several  semantic  types  of  bio-
medical  named entities.  The pipeline  processes 
and semantically enriches documents by adding, 
changing  or  removing  annotations.   More  pre-
cisely, the documents are augmented with UIDs 
referring to referential databases.  In the course 
of the processing, the number of annotated NEs 
increases and the quality of  the annotation im-
proves.  Thus, one of the main issues is to repres-
ent still  unresolved ambiguities consistently, so 
that  the  following  modules  can  perform  both 
identification  and  disambiguation  of  new  se-
mantic types. As subsequent modules try to add 
new  semantic  annotations,  prioritization  of  se-
mantic types is enforced by the order of the term 
identification modules.  
We have shown that such approach can be em-
ployed in a real-world, online information min-
ing  system EBIMed.   The  end-users  expect  to 
view the original layout of the documents at all 
times, and thus the solution needs to provide an 
efficient multidimensional markup that preserves 
and combines existing markup (from publishers) 
with semantic NLP-derived tags.  Since, in the 
biomedical  domain,  it  is  essential  to  provide 
17
links from term and named-entity occurrences to 
referential databases, EBIMed provides identific-
ation and disambiguation of such entities and in-
tegrates text with other knowledge sources.
The existing solution to annotate only longest 
non-overlapped entities is useful for real world 
use scenarios, but we also need ways to improve 
annotations  by  representing  nested  and  over-
lapped terms.
Acknowledgements
The development of EBIMed is supported by the 
Network of Excellence ?Semantic Interoperabil-
ity  and  Data  Mining  in  Biomedicine?  (NoE 
507505).   Medline  abstracts  are provided from 
the National  Library of Medicine (NLM, Beth-
esda,  MD,  USA)  and  PubMed  is  the  premier 
Web portal to access the data.
Sylvain Gaudan is supported by an ?E-STAR? 
fellowship funded by the EC?s FP6 Marie Curie 
Host fellowship for Early Stage Research Train-
ing  under  contract  number  MESTCT-  2004-
504640. Goran Nenadic acknowledges supported 
from the UK BBSRC grant ?Mining Term Asso-
ciations  from Literature  to  Support  Knowledge 
Discovery in Biology? (BB/C007360/1).
EBI thanks IBM for the grant of an IBM eS-
erver BladeCenter for use in its research work.
References 
A. Bairoch, R. Apweiler, C.H. Wu, W.C. Barker, B. 
Boeckmann, S. Ferro, E. Gasteiger, H. Huang, R. 
Lopez, M. Magrane, M.J. Martin, D.A. Natale, C. 
O?Donovan, N. Redaschi and L.S. Yeh.  2005. The 
Universal  Protein  Resource  (UniProt).   Nucleic  
Acids Research, 33(Database issue):D154-9.
C.  Blaschke,  M.A.  Andrade,  C.  Ouzounis  and  A. 
Valencia. 1999. Automatic extraction of biological 
information from scientific text: Protein-protein in-
teractions. Proc. ISMB, 7:60?7.
A.C. Browne, G. Divita, A.R Aronson and A.T. Mc-
Cray. 2003. UMLS language and vocabulary tools. 
AMIA Annual Symposium Proc., p. 798.
L. Chen, H. Liu and C. Friedman. 2005. Gene name 
ambiguity of eukaryotic nomenclature.  Bioinform-
atics, 21(2):248-56
S.  Gaudan,  H.  Kirsch  and  D.  Rebholz-Schuhmann. 
2005.  Resolving abbreviations to their  senses in 
Medline.  Bioinformatics, 21(18):3658-64
GO Consortium.  2006.   The Gene Ontology (GO) 
project  in  2006.  Nucleic  Acids  Research, 
34(suppl_1):D322-D326.
L. Hirschman, A. Yeh, C. Blaschke and A. Valencia. 
2005.  Overview  of  BioCreAtIvE:  critical  assess-
ment of information extraction for biology.  BMC 
Bioinformatics, 6 Suppl 1:S1.
H.  Kirsch,  S.  Gaudan  and  D.  Rebholz-Schuhmann. 
2005.  Distributed modules for text annotation and 
IE applied to the biomedical domain.  Internation-
al  Journal  Medical  Informatics.   (doi:10.1016/ 
j.ijmedinf.2005.06.011)
M. Krauthammer and G. Nenadic. 2004. Term identi-
fication in the biomedical literature.  Journal Bio-
medical Informatics, 37(6):512-26.
K.  Lee,  Y.  Hwang,  and  H.  Rim.  2003.  Two-Phase 
Biomedical  NE  Recognition  based  on  SVMs. 
Proc. of NLP in Biomedicine, ACL 2003. p. 33-40.
H. Liu, S.B. Johnson, and C. Friedman, 2002. Auto-
matic resolution of ambiguous terms based on ma-
chine  learning  and  conceptual  relations  in  the 
UMLS.  J  Am Med  Inform Assoc,  2002.  9(6):  p. 
621-36.
A. McCray, A. Browne and O. Bodenreider O. 2002. 
The  lexical  properties  of  Gene  ontology  (GO). 
Proceedings of AMIA 2002. 2002:504-8.
G. Nenadic, I. Spasic, and S. Ananiadou. 2005. Min-
ing  Biomedical  Abstracts:  What?s  in  a  Term?, 
LNAI Vol. 3248, pp. 797-806, Springer-Verlag
G.  Nenadic  and  S.  Ananiadou.  2006.  Mining  Se-
mantically Related Terms from Biomedical Literat-
ure. ACM Transactions on ALIP, 01/2006 (Special 
Issue Text Mining and Management in  Biomedi-
cine)
xD. Rebholz-Schuhmann, H. Kirsch, M. Arregui,  S. 
Gaudan, M. Rynbeek and P. Stoehr. (forthcoming) 
Identification of proteins and their biological con-
text  from  Medline:   EBI?s  text  mining  service 
EBIMed. 
D.  Rebholz-Schuhmann,  S.  Marcel,  S.  Albert,  R. 
Tolle, G. Casari and H. Kirsch.  2004. Automatic 
extraction of  mutations  from Medline and cross-
validation with OMIM.  Nucleid Acids Research, 
32(1):135?142.
A. Rzhetsky, I. Iossifov, T. Koike, M. Krauthammer, 
P. Kra, et al  2004. GeneWays: A system for ex-
tracting,  analyzing,  visualizing,  and  integrating 
molecular  pathway  data.  Journal  Biomedical  In-
formatics, 37:43?53.
A.S. Schwartz and M.A. Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in 
biomedical text. Proceedings of Pac Symp Biocom-
put. 2003. p. 451-62.
M. Torii, S. Kamboj and K. Vijay-Shanker. 2003. An 
Investigation  of  Various  Information  Sources  for 
Classifying  Biological  Names.  Proceedings  of 
NLP in Biomedicine, ACL 2003. p. 113-120
CM Verspoor, C. Joslyn and G. Papcun. 2003. The 
Gene  ontology  as  a  source  of  lexical  semantic 
knowledge for  a  biological  natural  language pro-
cessing  application.  Proc.  of  Workshop  on  Text  
Analysis and Search for Bioinformatics, SIGIR 03
H. Yu, G. Hripcsak and C. Friedman. 2002. Mapping 
abbreviations to full forms in biomedical articles. J 
Am Med Inform Assoc, 2002. 9(3): p. 262-72. 
18
Enhancing automatic term recognition through recognition of variation  
Goran Nenadi?*  
Department of Computation 
UMIST 
Manchester, UK, M60 1QD 
G.Nenadic@umist.ac.uk 
Sophia Ananiadou* 
Computer Science 
University of Salford 
Salford, UK, M5 4WT  
S.Ananiadou@salford.ac.uk 
John McNaught* 
Department of Computation 
UMIST 
Manchester, UK, M60 1QD 
J.McNaught@umist.ac.uk 
 
                                                     
* Co-affiliation: National Centre for Text Mining, Manchester, UK 
Abstract 
Terminological variation is an integral part of the 
linguistic ability to realise a concept in many ways, 
but it is typically considered an obstacle to 
automatic term recognition (ATR) and term 
management. We present a method that integrates 
term variation in a hybrid ATR approach, in which 
term candidates are recognised by a set of 
linguistic filters and termhood assignment is based 
on joint frequency of occurrence of all term 
variants. We evaluate the effectiveness of 
incorporating specific types of term variation by 
comparing it to the performance of a baseline 
method that treats term variants as separate terms. 
We show that ATR precision is enhanced by 
considering joint termhoods of all term variants, 
while recall benefits by the introduction of new 
candidates through consideration of different 
variation types. On a biomedical test corpus we 
show that precision can be increased by 20?70% 
for the top ranked terms, while recall improves 
generally by 2?25%. 
1 Introduction 
Terminological processing has long been 
recognised as one of the crucial aspects of 
systematic knowledge acquisition and of many 
NLP applications (IR, IE, corpus querying, etc.). 
However, term variation has been under-discussed 
and is rarely accounted for in such applications.  
When naming a new concept, scientists and 
specialists usually follow some predefined term 
formation patterns, a process which does not 
exclude the usage of term variations or alternative 
names for concepts. Term variations are very 
frequent: approximately one third of term 
occurrences are variants (Jacquemin, 2001). They 
occur not only in text, but also in controlled, 
manually curated terminological resources (e.g. 
UMLS (NLM, 2004)).  
The task of an automatic term recognition (ATR) 
system is not only to suggest the most likely 
candidate terms from text, but also to correlate 
them with synonymous term variants. In this paper, 
we briefly present an analysis of term variation 
phenomena, whose results are subsequently 
incorporated into a corpus-based ATR method in 
order to enhance its performance.  
The paper is organised as follows. In Section 2, 
we analyse the main types of term variation, and 
briefly examine how existing ATR systems treat 
them. Our approach to incorporating variants into 
ATR is presented in Section 3. In Section 4, we 
evaluate our approach by comparing it to a 
baseline method (the method without variation re-
cognition), and we conclude the paper in Section 5. 
2 Background 
Terms are linguistic units that are assigned to 
concepts and used by domain specialists to 
describe and refer to specific concepts in a domain. 
In this sense, terms are preferred designators of 
concepts. In text, however, concepts are frequently 
denoted by different surface realisations of 
preferred terms, which we denote as their term 
variants. Consequently, a concept can be 
linguistically represented using any of the surface 
forms that are variants of the corresponding 
preferred term. We consider the following types of 
term variation: 
 
(i) orthographic: e.g. usage of hyphens and slashes 
(amino acid and amino-acid), lower and upper 
cases (NF-KB and NF-kb), spelling variations 
(tumour and tumor), different Latin/Greek 
transcriptions (oestrogen and estrogen), etc. 
(ii) morphological: the simplest variations are 
related to inflectional phenomena (e.g. singular, 
plural). Derivational transformations can lead to 
variants in some cases (cellular gene and cell 
gene), but not always (activated factor vs. 
activating factor); 
(iii) lexical: genuine lexical synonyms, which may 
be interchangeably used (carcinoma and 
cancer, haemorrhage and blood loss); 
(iv) structural: e.g. possessive usage of nouns 
using prepositions (clones of human and human 
clones), prepositional variants (cell in blood, 
cell from blood), term coordinations (adrenal 
glands and gonads); 
(v) acronyms and abbreviations: very frequent 
term variation phenomena in technical 
sublanguages, especially in biomedicine; 
sometimes they may be even preferred terms 
(DNA for deoxyribonucleic acid).  
 
Note that variation types (i) ? (iii) affect 
individual constituents, while (iv) and (v) involve 
variation in structure of the preferred term. In any 
case, they do not ?change? the meaning as they 
refer to the same concept. Daille et al (1996) and 
Jacquemin (1999, 2001) further identified types of 
variation that modified the meaning of terms.  
Although many authors mention the problems 
related to term variation, few have dealt with 
linking the corresponding term variants. Also, the 
recognition of variants is typically performed as a 
separate operation, and not as part of ATR.  
The simplest technique to handle some types of 
term variation (e.g. morphological) is based on 
stemming: if two term forms share a stemmed 
representation, they are considered as mutual 
variants (Jacquemin and Tzoukermann, 1999; 
Ananiadou et al, 2000). However, stemming may 
result in ambiguous denotations related to ?over-
stemming? (i.e. resulting in the conflation of terms 
which are not real variants) and ?under-stemming? 
(i.e. resulting in the failure to link real term 
variants).  
Other approaches to the recognition of term 
variants use preferred terms and known synonyms 
from existing term dictionaries and approximate 
string matching techniques to link or generate 
different term variants (Krauthammer et al, 2001; 
Tsuruoka and Tsujii, 2003).  
Jacquemin (2001) presents a rule-based system, 
FASTR, which supports several hundred meta-
rules dealing with morphological, syntactic (i.e. 
structural) and semantic term variation. Term 
variation recognition is based on the 
transformation of basic term structures into variant 
structures. However, the variants recognised by 
FASTR are more conceptual variants than 
terminological ones, as non-terminological units 
(such as verb phrases, extended insertions, etc.) are 
also linked to terms in order to improve indexing 
and retrieval.   
 
3 Incorporating term variation into ATR 
Our approach to ATR combines the C-value 
method (Frantzi et al, 2000) with the recognition 
of term variation, which is incorporated as an 
integral part of the term extraction process.  
C-value is a hybrid approach combining term 
formation patterns with corpus-based statistical 
measures. Term formation patterns act as linguistic 
filters to a POS tagged corpus: filtered sequences 
are considered as potential realisations of domain 
concepts (term candidates). They are subsequently 
assigned termhoods (i.e. likelihood to represent 
terms) according to a statistical measure. The 
measure amalgamates four corpus-based 
characteristics of a term candidate, namely its 
frequency of occurrence, its frequency of 
occurrence as a form nested within other candidate 
terms, the number of candidate terms inside which 
it is nested, and the number of words it contains.   
The original C-value method treats term variants 
that correspond to the same concept as separate 
term candidates. Consequently, by providing 
separate frequencies of occurrence for individual 
variants instead of a single frequency of 
occurrence calculated for a term candidate unifying 
all variants, the corpus-based measures and 
termhoods are distributed across different variants. 
Therefore, we aim at enhancing the statistical 
evaluation of termhoods through conflation of 
different surface representations of a given term, 
and through joint frequencies of occurrence of all 
equivalent surface forms that correspond to a 
single concept.  
In order to conflate equivalent surface 
expressions, we carry out linguistic normalisation 
of individual term candidates (see examples in 
Table 1). Firstly, each term candidate is mapped to 
a specific canonical representative (CR) by 
semantically isomorphic transformations. Then, we 
establish an equivalence relation, where two term 
candidates are related iff they share the same CR. 
The partitions of this relation are denoted as 
synterms: a synterm contains surface term 
representations sharing the same CR.  
 
synterm canonical representative 
human cancers 
cancer in humans 
human?s cancer 
human carcinoma } human cancer 
Table 1: Term normalisation examples 
 
Our aim is to form synterms prior to the syntactic 
estimation of termhoods for term candidates. 
Therefore, after the extraction of individual term 
candidates, we subsequently normalise them in 
order to generate synterms, where the 
normalisation is performed according to the 
typology of variations described in Section 2. More 
precisely, we consider separately the normalisation 
of variations that affect term candidate constituents 
and variations that involve structural changes. The 
general architecture of our ATR approach is 
presented in Figure 1. 
 
 
P O S  tagger 
 
In flec tiona l n orm alisa tio n  
S tructura l  n orm alisation  
O rthographic  no rm alisa tion  
E x trac ted  syn term s 
Inp u t d ocu m ents 
T erm h ood  es tim ation  
E xtrac tio n  of term  cand id ates  
A cron ym  acq uis ition  
 
Figure 1: The architecture of the ATR process 
 
3.1 Normalising term constituent variation 
In the case of variations that do not affect the 
structure of terms, the formation of CRs is based 
on a POS tagger (for inflectional variation) and 
simple heuristics (for orthographic normalisation). 
For example, different transcriptions of 
neoclassical combining forms are treated by 
replacements of specific character combinations 
(ae ? e, ph ? f) in such forms (and only in such 
forms). Inflectional normalisation is based on POS 
tagging: a canonical term candidate form is a 
singular form containing no possessives (Down?s 
syndrome ? down syndrome). 
In order to address lexical variants, one can use 
dictionaries of synonyms where the preferred terms 
are used for normalisation purposes ({hepatic 
microsomes, liver microsomes} ? liver 
microsomes). In experiments reported here, we did 
not attempt to normalise lexical variation. 
 
3.2 Normalising term structure variation 
Variations affecting term structure are less frequent 
but more complex. Here we consider two types of 
term variation: prepositional term candidates and 
coordinated term candidates (for a detailed analysis 
of these variations see (Nenadic et al, 2004)). 
Prepositional term candidates are normalised by 
transformation into corresponding expressions 
without prepositions. Using prepositions of, in, for 
and by as anchors, we generate semantically 
isomorphic CRs by inversion. For example, the 
candidate nuclear factor of activated T cell is 
transformed into activated T cell nuclear factor. 
Here is a simplified example of a rule describing 
the transformation of a term candidate that 
contains the preposition of: 
 
if  structure of  term candidate is   
   (A|N)1* N1 Prep(of) (A|N)2* N2  
then   CR = (A|N)2* N2 (A|N)1* N1 
 
In order to address the problems of determining 
the boundaries of term constituents in text (to the 
right and left of prepositions), for each 
prepositional term candidate we generate all 
possible nested candidates? and their corresponding 
CRs. For example, for the candidate regulation of 
gene expression, we generate both gene regulation 
and gene expression regulation. Since this 
approach also generates a number of false 
candidates, additional heuristics are used to 
enhance precision, such as removing adverbials 
and determiners, using a stop list of 
terminologically irrelevant prepositional 
expressions (e.g. number of ..., list of ..., case of ..., 
in presence of ...), etc.  
A similar approach is used for the recognition of 
coordinated term candidates: coordinating 
conjunctions (and, or, but not, as well as, etc.) are 
used as anchors, and when a coordinating structure 
is recognised in text, the corresponding CRs of the 
candidate terms involved are generated.  
We differentiate between head coordination 
(where term heads are coordinated, e.g. adrenal 
glands and gonads) and argument coordination 
(where term arguments/modifiers are coordinated, 
e.g. SMRT and Trip-1 mRNAs). 
The recognition and extraction of coordinated 
terms is highly ambiguous even for human 
specialists, since coordinated terms and term 
conjunctions share the same structures (see Table 
2). Also, similar patterns cover both argument and 
head coordinations, which makes it difficult to 
extract coordinated constituents (i.e. terms). Not 
only is the recognition of term coordinations and 
their subtypes ambiguous, but also internal 
boundaries of coordinated terms are blurred. In a 
separate study, we have shown that 
morphosyntactic features are insufficient both for 
the successful recognition of coordinations and for 
the extraction of coordinated terms: in many cases, 
the correct interpretation and decoding of term 
coordinations is only possible with sufficient 
background knowledge (Nenadic et al, 2004). 
                                                     
? Each constituent extracted from a nested pre-
positional term candidate has to follow a pattern used 
for the extraction of individual candidate terms. 
 
example adrenal  glands and gonads 
head 
coordination [adrenal [glands and gonads]] 
term  
conjunction  [adrenal glands] and [gonads] 
Table 2: Ambiguities within coordinated structures 
In order to address the problems of structural 
ambiguities and boundaries of coordinated terms, 
we also generate all possible nested coordination 
expressions and corresponding term candidates. 
For example, from a candidate coordination viral 
gene expression and replication we generate two 
pairs of coordinated term candidates: 
 
viral gene expression  and  viral gene replication 
viral gene expression  and  viral replication 
 
Patterns for the extraction of term candidates 
from coordinations have been acquired semi-
manually for a subset of term coordinations. For 
each pattern, we define a procedure for the 
extraction of coordinated term candidates and 
generation of the corresponding CRs (see Table 3 
for examples). The generated candidates from 
coordinated structures are subsequently treated as 
individual term candidates. 
 
3.3 Normalising acronym variation  
We treat acronym extraction as part of the ATR 
process (see Figure 1). In (Nenadic et al, 2002) we 
suggested a simple procedure for acquiring 
acronyms and their expanded forms (EFs), which 
was mainly based on using orthographic and 
syntactic features of contexts where acronyms 
were introduced. The model is based on three types 
of patterns: acronym patterns (defining common 
internal acronym structures and forms), definition 
patterns (based on syntactic patterns which 
describe typical contexts where acronyms are 
introduced in text) and matching patterns (the set 
of matching rules between acronyms and their 
corresponding EFs).  
Acronyms also exhibit variation (e.g. RAR alpha, 
RAR-alpha, RARA, RARa, RA receptor alpha etc. 
are all acronyms for retinoic acid receptor alpha). 
Therefore, in addition to extracting acronyms, we 
further gather all acronym variants and their EFs, 
and we map them into a single CR. Since in this 
paper acronyms are taken as term variants, we  
?replace? acronym occurrences by the CR of their 
EFs. In order to bypass the problem of acronym 
ambiguity, we replace/normalise only acronyms 
that are introduced in a given document. 
 
(N|A)1 & (N|A)2 (N+)3 
candidate1 = (N|A)2 (N+)3 
candidate2 = (N|A)1 nested(N+3 ) 
 
e.g.   B and T cell antigen    
          T cell antigen    
          B cell antigen, B antigen 
N1 & N2 A3 N+4 
candidate1 = N2 A3 N+4 
candidate2 = N1 A3 N+4 
 
e.g.  function or surface antigenic profile 
     surface antigenic profile   
     function antigenic profile 
N+1 N2 & (N|A)3 
candidate1 = N+1 N2  
candidate2 = nested(N+1) (N|A)3  
 
e.g.  breast cancer therapy and prevention 
     breast cancer therapy  
     breast caner prevention, breast  prevention 
N+1 (A+)2  A3 &  A4 
candidate1 = N+1 (A+)2 A3  
candidate2 = N+1 (A+)2 A4  
 
e.g.  RNA polymerases II and III 
      RNA polymerasis II 
      RNA polymerasis III 
Table 3: Examples of patterns used for the 
extraction of term candidates from coordinations  
(nested denotes the generation of all possible 
linearly nested substrings)  
3.4 Calculating termhoods with variants 
Term variants sharing the same CR are grouped 
together into synterms, and the calculation of C-
values (i.e. termhoods) is performed for the whole 
synterm rather than for individual term candidates. 
The main reason for doing this is to avoid the 
distribution of frequencies of occurrence of term 
candidates across different variants, as these 
frequencies have a significant impact on estimating 
termhoods. Instead of providing separate 
frequencies of occurrence and obtaining termhoods 
for individual term candidates, we provide a single 
frequency of occurrence and joint termhood 
calculated for a synterm, which unifies all variants. 
Similarly to the estimation of C-values for 
individual term candidates (Frantzi et al, 2000), 
the formula for calculating the termhoods for 
synterms is as follows: 
 
 
??
??
?
?
??
=
?
?
nestednot  is CR),(||log
nested is CR,))(||
1)((||log  )value(-C
2
2
CRfCR
bfTCRfCRc CRTbCR
 
where c denotes a synterm whose elements share a 
canonical representative (denoted as CR in the 
formula), f(CR) corresponds to the cumulative 
frequency with which all term candidates from the 
synterm c occur in a given corpus, |CR| denotes 
the average length of the term candidates (the 
number of constituents), and TCR is a set of all 
synterms whose CRs contain the given CR as a 
nested substring. 
This approach ensures that all term variants are 
naturally dealt with jointly, thus supporting the fact 
that they denote the same concept. As a 
consequence, we expect that precision would be 
enhanced by considering joint frequencies of 
occurrence and termhoods for all variants of 
candidate terms, while recall would benefit by the 
introduction of new candidates through 
consideration of different variation types. 
 
4 Evaluation and discussion 
In order to assess the effectiveness of incorporating 
specific types of term variation into ATR, we 
compared the performance of the baseline C-value 
method (without considering variations) with the 
approach including recognition and conflation of 
term variants. Here we are not interested in an 
absolute measure of the ATR performance, but 
rather in the comparison of results obtained 
through handling different variation types.  
We conducted two sets of experiments: in the 
first experiment, we analysed the incorporation of 
term candidates resulting from considering term 
variations individually, while, in the second, we 
experimented with the integration of combined 
variations in the ATR process. 
The evaluation was carried out using the GENIA 
corpus (GENIA, 2004), which contains 2,000 
abstracts in the biomedical domain with 76,592 
manually marked occurrences of terms. These 
occurrences (which include different term variants) 
correspond to 29,781 different, unique terms. Each 
occurrence of a term in the corpus (except 
occurrences of acronyms) is linked to the 
corresponding ?normalised? term (typically a 
singular form), while coordinated terms are 
identified, marked and normalised within term 
coordinations. A third of occurrences of GENIA 
terms are affected by inflectional variations, and 
almost half of GENIA terms have inflectional 
variants appearing in the corpus. On the other 
hand, only 0.5% of terms contain a preposition, 
while 2% of all term occurrences are coordinated, 
involving 9% of distinct GENIA terms (for a 
detailed analysis of GENIA terms see (Nenadic et 
al., 2004)). 
We used the list of GENIA terms as a gold 
standard for the evaluation. Since our ATR method 
produces a ranked list of suggested synterms, we 
considered precision at fixed rank cut-offs 
(intervals): precision was calculated as the ratio 
between the number of correctly recognised terms 
and the total number of entities recognised in a 
given interval (where an interval included all terms 
from the top ranked synterms).? The baseline 
method (original C-value) was treated in the same 
way, as term candidates suggested by the original 
C-value could be seen as singleton synterms. In 
order to estimate the influence on recall, we also 
used all variants from suggested synterms.  
The incorporation of individual variations 
affecting term constituents into ATR had 
considerable positive effects, especially on the 
most frequently occurring terms (see Figures 2a 
and 2b): for some intervals, inflectional variants, 
for example, improved precision by almost 50%. 
Similarly, the integration of acronyms improved 
precision, in particular for frequent terms (up to 
70%), as acronyms are typically introduced for 
such terms. As one would expect, the combined 
constituent-level variations further improved 
interval precisions compared both to the baseline 
method and individual variations (see Figure 2c). 
However, the incorporation of structural variants 
(in particular for prepositional terms) negatively 
influenced precision compared to the baseline 
method, as many false candidates were introduced.  
In order to assess the quality of extracted 
prepositional term candidates, we evaluated a set 
of the 117 most frequently occurring candidates 
with prepositions: 80% of suggested expressions 
were deemed relevant by domain experts, although 
they were not included in the gold GENIA 
standard (such as expression of genes or binding of 
NF kappa B). Still, the recognition of prepositional 
term candidates is difficult as they are infrequent 
and there are no clear morphosyntactic cues that 
can differentiate between terminologically relevant 
and irrelevant prepositional phrases. 
The incorporation of coordinated term candidates 
had only marginal influence on precision, mainly 
because they were not frequent in the GENIA 
corpus. Furthermore, simple term conjunctions 
                                                     
? It was an open question whether to count the 
recognition of each term form (e.g. singular and plural 
forms, an acronym and its EF, prepositional and non-
prepositional forms) separately (i.e. as two positive 
?hits?) or as one positive ?hit? (see also (Church, 
1995)). Since the evaluation of the baseline method 
(original C-value) typically counts such hits separately, 
we decided to follow this approach, and consequently 
count all positive hits from synterms.  
 
were far more frequent than term coordinations, 
which made their extraction highly ambiguous. 
Still, using only the patterns from Table 3, we have 
correctly extracted 35.76% of all GENIA 
coordinated terms, with more than a half of all 
suggested candidates being found among those that 
appeared exclusively in coordinations. However, 
these patterns also generated a number of false 
coordination expressions, and consequently a 
number of false term candidates. 
The integration of term variants was also useful 
for re-ranking of true positive term candidates: the 
combined rank was typically higher than the 
separate ranks of term variants. Furthermore, some 
terms, not suggested by the baseline method at all, 
were ranked highly when variants were conflated 
(for example, the term T-lymphocyte was 
recognised only as a coordinated term candidate, 
while replication of HIV-1 was extracted only by 
considering prepositional term candidates). In 
order to estimate the overall influence on recall of 
ATR, we used all terms from the respective 
synterms (see Table 4 for the detailed results). In 
general, the incorporation of inflectional variants 
increased recall by ?, while acronyms improved 
recall by almost ? when only the most frequent 
terms were considered. It is interesting that 
acronym acquisition can further improve recall by 
extracting variants that have more complex internal 
structures (such as EFs containing prepositions 
(REA = repressor of estrogen activity) and/or 
coordinations (SMRT = silencing mediator of 
retinoic and thyroid receptor)). Prepositional and 
coordination candidate terms had some influence 
on recall, in particular as they increased the 
likelihood of some candidates to be suggested as 
terms. Low recall of term coordinations may be 
increased by adding more patterns (which would 
probably negatively affect precision).  
Summarising, experiments performed on the 
GENIA corpus have shown that the incorporation 
of term variations into the ATR process resulted in 
significantly better precision and recall. In general, 
acronyms and inflectional unification are the most 
important variation types (at least in the domain of 
biomedicine). Individually, they increased 
precision by 20?70% for the top ranked synterm 
intervals, while recall is generally improved, in 
some cases up to 25%. Other term variations had 
only marginal influence on the performance, 
mainly because they were infrequent in the test 
corpus (compared to the total number of term 
occurrences, and not only with regard to specific 
individual candidates, but also in general). For 
these variations, larger-scale corpora may show 
their stronger influence.  
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
prepositions inflectional acronyms
Figure 2a: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of individual term variants 
(terms with frequency > 5) 
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
prepositions inflectional acronyms
Figure 2b: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of individual term variants 
(terms with frequency > 0) 
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
inflectional infl & acro all
Figure 2c: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of combined term variants 
(terms with frequency > 0) 
 
term sets prep. coord. infl. acro. 
freq. ? 5 +5.30% +12.42% +17.52% +60.49%
freq. > 0 +2.36% +2.53% +25.25% +8.52% 
Table 4: Improvement in recall when variations 
are considered as an integral part of ATR 
 
5 Conclusion 
In this paper we discussed possibilities for the 
extraction and conflation of different types of 
variation of term candidates. We demonstrated that 
the incorporation of treatment of term variation 
enhanced the performance of an ATR system, and 
that tackling term variation phenomena was an 
essential step for ATR. In our case, precision was 
boosted by considering joint frequencies of 
occurrence and termhoods for all candidate terms 
from candidate synterms, while recall benefited 
from the introduction of new candidates through 
consideration of different variation types. Although 
we experimented with a biomedical corpus, our 
techniques are general and can be applied to other 
domains.  
Variations affecting single term candidate 
constituents are the most frequent phenomena, and 
also straightforward for implementation as part of 
an ATR process. The conflation of such term 
candidate variants can be further tuned for a 
specific domain by using lists of combining forms 
and affixes. The incorporation of acronyms had a 
significant high positive effect, in particular on 
more frequent terms (since acronyms are 
introduced for terms that are used more 
frequently). 
However, more complex structural phenomena 
had a moderate positive influence on recall, but, in 
general, the negative effect on precision. The main 
reason for such performances is structural and 
terminological ambiguity of these expressions, in 
addition to their low frequency of occurrence 
(compared to the total number of term 
occurrences). For handling such complex variants, 
a knowledge-intensive and domain-specific 
approach is needed, as coordinated term candidates 
or candidates with prepositions need to be 
additionally semantically analysed in order to 
suggest more reliable term candidates, and to 
introduce fewer false candidates.  
Apart  from being useful for boosting precision 
and recall, the integration of term variation into 
ATR is particularly important for smaller corpora 
(where linking related occurrences is vital for 
successful terminology management) as well as for 
many text-mining tasks (such as IR, IE, term or 
document clustering and classification, etc.). 
Finally, as future work, we plan to investigate 
more knowledge intensive, domain-specific 
treatment of prepositional and coordinated terms, 
as well as pronominal term references. 
6 Acknowledgements 
This research has been partially supported by the 
JISC-funded National Centre for Text Mining 
(NaCTeM), Manchester, UK. 
References  
S. Ananiadou, S. Albert and D. Schuhmann. 2000. 
Evaluation of Automatic Term Recognition of 
Nuclear Receptors from Medline. Genome 
Informatics Series, vol. 11. 
K.W. Church. 1995. One Term or Two? Proc. of 
SIGIR-95, pp. 310-318. 
B. Daille, B. Habert and C. Jacquemin. 1996. 
Empirical Observation of Term Variation and 
Principles for Their Description. Terminology 
3(2), pp. 197?258. 
K. Frantzi, S. Ananiadou and H. Mima. 2000. 
Automatic Recognition of Multi-Word Terms: 
the C/NC value method. International Journal of 
Digital Libraries, vol. 3:2, pp. 115?130. 
C. Jacquemin. 1999. Syntagmatic and paradigmatic 
representations of term variation. Proc. of 37th 
Annual Meeting of ACL, pp. 341?348. 
C. Jacquemin. 2001. Spotting and Discovering 
Terms through NLP. MIT Press, Cambridge MA. 
C. Jacquemin and E. Tzoukermann. 1999. NLP for 
Term Variant Extraction: A Synergy of 
Morphology, Lexicon and Syntax, in T. 
Strzalkowski (ed.), Natural Language 
Information Retrieval, Kluwer, pp. 25-74 
M. Krauthammer, A. Rzhetsky, P. Morozov, and 
C. Friedman. 2001. Using BLAST for 
identifying gene and protein names in journal 
articles. Gene, 259(1?2): pp. 245?52. 
GENIA. 2004. GENIA resources. Available at 
http://www.tsujii.is.u-tokyo.ac.jp/~Genia/ 
G. Nenadic, I. Spasic and S. Ananiadou. 2002. 
Automatic Acronym Acquisition and Term 
Variation Management within Domain-Specific 
Texts. Proc. of LREC 2002, pp. 2155?2162. 
G. Nenadic, I. Spasic and S. Ananiadou. 2004. 
Mining Biomedical Abstracts: What?s in a 
Term? Proc. of IJC-NLP, pp. 247-254.  
NLM. 2004. National Library of Medicine, Unified 
Medical Language System. 
Y. Tsuruoka and J. Tsujii. 2003. Probabilistic 
Term Variant Generator for Biomedical Terms. 
Proc. of 26th Annual ACM SIGIR Conference. 
243
244
245
246
247
248
249
250
Using Domain-Specific Verbs for Term Classification 
 
Irena Spasi? 
Computer Science 
University of Salford, UK 
I.Spasic@salford.ac.uk
Goran Nenadi? 
Department of Computing 
UMIST, UK 
G.Nenadic@umist.ac.uk
Sophia Ananiadou 
Computer Science  
University of Salford, UK 
S.Ananiadou@salford.ac.uk
 
Abstract 
In this paper we present an approach to 
term classification based on verb com-
plementation patterns. The complementa-
tion patterns have been automatically 
learnt by combining information found in 
a corpus and an ontology, both belonging 
to the biomedical domain. The learning 
process is unsupervised and has been im-
plemented as an iterative reasoning pro-
cedure based on a partial order relation 
induced by the domain-specific ontology. 
First, term recognition was performed by 
both looking up the dictionary of terms 
listed in the ontology and applying the 
C/NC-value method. Subsequently, do-
main-specific verbs were automatically 
identified in the corpus. Finally, the 
classes of terms typically selected as ar-
guments for the considered verbs were in-
duced from the corpus and the ontology. 
This information was used to classify 
newly recognised terms. The precision of 
the classification method reached 64%. 
1 Introduction 
Basic notions used when describing a specific 
problem domain are concepts, classes and attrib-
utes (or features). The identification of concepts, 
linguistically represented by domain-specific terms 
(Maynard and Ananiadou, 2000), is a basic step in 
the automated acquisition of knowledge from tex-
tual documents. Textual documents describing new 
knowledge in an intensively expanding domain are 
swamped by new terms representing newly identi-
fied or created concepts. Dynamic domains, such 
as biomedicine, cannot be represented by static 
models, since new discoveries give rise to the ap-
pearance of new terms. This makes the automatic 
term recognition (ATR) tools essential assets for 
efficient knowledge acquisition.  
However, ATR itself is not sufficient when it 
comes to organizing newly acquired knowledge. 
Concepts are natively assorted into groups and a 
well-formed model of a domain, represented 
through terms and their relations, needs to reflect 
this property consistently. Dynamic domain mod-
els should be able to adapt to the advent of new 
terms representing newly discovered or identified 
concepts. In other words, newly extracted terms 
need to be incorporated into an existing model by 
associating them with one another and with already 
established terms preferably in an automated man-
ner. This goal may be achieved by relying on term 
clustering (the process of linking semantically 
similar terms together) and term classification (the 
process of assigning terms to classes from a pre-
defined classification scheme). In particular, classi-
fication results can be used for efficient and consis-
tent term management through populating and 
updating existing ontologies in expanding domains 
such as biomedicine. In this paper, we compare 
some of the term classification approaches and in-
troduce another approach to this problem.  
The paper is organised as follows. In Section 2 
we provide a brief overview of the existing term 
classification approaches and suggest the main idea 
of our approach to this problem. Section 3 de-
scribes the learning phase of our classification 
method. Further, Section 4 provides details on the 
classification algorithm. Finally, in Section 5 we 
describe the evaluation strategy and provide the 
results, after which we conclude the paper. 
2 Term Classification Approaches 
Similarly to general classification algorithms, the 
existing term classification approaches typically 
rely on learning techniques. These techniques are 
most often statistically based (e.g. hidden Markov 
models, naive Bayesian learning, etc.). Other tech-
niques include decision trees, inductive rule learn-
ing, support-vector machines (SVMs), etc. We, on 
the other hand, suggest the use of a genetic algo-
rithm as a learning engine for the classification 
task. Let us now discuss some approaches to the 
automatic classification of biomedical terms. 
Nobata et al (2000) implemented a statistical 
method for term classification. In their approach, 
each class was represented by a list of (single) 
words. The first step was to estimate the condi-
tional probability P(c | w) of each word w being 
assigned to a specific class c, based on the assump-
tion that each word occurrence is independent of 
its context and position in the text. Further, yet an-
other strong restriction was made by assuming that 
there was one-to-one correspondence between 
terms and their classes. In addition, this approach 
is not applicable to ?unknown? terms, i.e. terms 
containing words for which no classification prob-
abilities had been determined. A special class, re-
ferring to ?other?, was introduced to cover such 
words. Bearing in mind the increasing number of 
new terms, such an approach is bound to produce 
skewed results, where many of the terms would 
simply be classified as ?other?. 
While Nobata et al (2000) statistically proc-
essed the information found inside the terms, Col-
lier et al (2001) applied statistical techniques to 
the information found outside the terms. A hidden 
Markov model based on n-grams (assuming that a 
term?s class may be induced from the previous n-1 
lexical items and their classes) was used as a theo-
retical basis for their classification method. The 
method relied on the orthographic features includ-
ing numerals, capital and Greek letters, special 
characters (such as `-`, `/`, `+`, etc.), parenthesis, 
etc. In the biomedical domain, such features often 
provide hints regarding the class of a specific term. 
Each unclassified term was assigned a class of the 
most similar (with respect to the orthographic fea-
tures) term from the training set. This approach 
encountered the minority class prediction problem. 
Namely, the best classification results in terms of 
recall and precision were achieved for the most 
frequent class of terms in their training corpus, 
while the worst results were those achieved for the 
least frequent class. 
Hatzivassiloglou et al (2001) proposed a 
method for unsupervised learning of weights for 
context elements (including words as context con-
stituents and the corresponding positional and 
morphological information) of known terms and 
using these weights for term classification. Three 
well-known learning techniques were used: naive 
Bayesian learning, decision trees, and inductive 
rule learning. Simplified classification experiments 
in which a classification algorithm was choosing 
between two or three options respectively were 
conducted. The precision of binary classification 
was around 76% for all three learning algorithms, 
and the precision dropped to approximately 67% 
when choosing between three options. If the pro-
posed techniques were to be applied for general 
classification where the number of options is arbi-
trary, the precision is expected to decrease even 
further. 
Nenadic et al (2003b) conducted a series of 
large-scale experiments with different types of fea-
tures for a multi-class SVM. These features in-
cluded document identifiers, single words, their 
lemmas and stems, and automatically recognised 
terms. The results indicated that the performance 
was approximately the same (around 60% in the 
best case) when using single words, lemmas or 
stems. On the other side, terms proved to be better 
(more than 90% precision) than single words at 
lower recall points (less than 10%), which means 
that terms as features can improve the precision for 
minority classes. The best results were achieved 
with document identifiers, but such features cannot 
be used on the fly in new documents.  
Spasic et al (2002) used a genetic algorithm 
(GA) based on a specific crossover operator to ex-
plore the relationships between verbs and the terms 
complementing them. The GA performed reason-
ing about term classes allowed to be combined 
with specific verbs by using an existing ontology 
as a seed for learning. In this paper, we use the re-
sults of the proposed methodology as a platform 
for term classification. In the following section we 
briefly overview the method for the acquisition of 
verb complementation patterns. 
3 Verb Complementation Patterns 
By looking at the context of an isolated verb occur-
rence it is difficult to predict all term classes that 
can be combined with the given verb. On the other 
hand, the whole ?population? of terms comple-
menting a specific verb is likely to provide a cer-
tain conclusion about that verb with respect to its 
complementation patterns. This was a primary mo-
tivation for Spasic et al (2002) to use a GA as it 
operates on a population of individuals as opposed 
to a single individual. This fact also makes the ap-
proach robust, since it does not rely on every spe-
cific instance of verb-term combination to be 
correctly recognised.  
As not all verbs are equally important for the 
term classification task, we are primarily interested 
in domain-specific verb complementation patterns. 
In our approach, a complementation pattern of a 
domain-specific verb is defined as a disjunction of 
terms and/or their classes that are used in combina-
tion with the given verb. The automatic acquisition 
of these patterns is performed in the following 
steps: term recognition, domain-specific verb ex-
traction, and the learning of complementation pat-
terns. Let us describe each of these steps in more 
detail. 
 
3.1   Term Recognition 
 
First, a corpus is terminologically processed: both 
terms present in the ontology and the terms recog-
nised automatically are tagged. Terms already 
classified in the ontology are used to learn the 
classes allowed by the domain-specific verbs, 
while the new terms are yet to be classified based 
on the learnt classes. New terms are recognized by 
the C/NC-value method (Frantzi et al, 2000), 
which extracts multi-word terms. This method rec-
ognises terms by combining linguistic knowledge 
and statistical analysis. Linguistic knowledge is 
used to propose term candidates through general 
term formation patterns. Each term candidate t is 
then quantified by its termhood C-value(t) calcu-
lated as a combination of its numerical characteris-
tics: length |t| as the number of words, absolute 
frequency f(t) and two types of frequency relative 
to the set S(t) of candidate terms containing a 
nested candidate term t (frequency of occurrence 
nested inside other candidate terms and the number 
of different term candidates containing a nested 
candidate term): 
 
??
??
?
????
?=?
=? ?
?
)( if  ,))(|)(|
1)((||ln
)( if  ),(||ln
)(
)(
tSsftStft
tStft
tvalueC
tSs
 
 
Obviously, the higher the frequency of a candi-
date term the greater its termhood. The same holds 
for its length. On the other side, the more fre-
quently the candidate term is nested in other term 
candidates, the more its termhood is reduced. 
However, this reduction decreases with the in-
crease in the number of different host candidate 
terms as it is hypothesised that the candidate term 
is more independent if the set of its host terms is 
more versatile. 
Term distribution in top-ranked candidate terms 
is further improved by taking into account their 
context. The relevant context words, including 
nouns, verbs and adjectives, are extracted and as-
signed weights based on how frequently they co-
occur with top-ranked term candidates. Subse-
quently, context factors are assigned to candidate 
terms according to their co-occurrence with top-
ranked context words. Finally, new termhood esti-
mations (NC-values) are calculated as a linear 
combination of the C-values and context factors.  
Nenadic et al (2003a) modified the C/NC-value 
to recognise acronyms as a special type of single-
word terms, and, thus, enhanced the recall of the 
method. On the other hand, the modified version 
incorporates the unification of term variants into 
the linguistic part of the method, which also im-
proved the precision, since the statistical analysis is 
more reliable when performed over classes of 
equivalent term variants instead of separate terms. 
 
3.2   Domain-Specific Verb Recognition 
 
Verbs are extracted from the corpus and ranked 
based on the frequency of occurrence and the fre-
quency of their co-occurrence with terms. A stop 
list of general verbs frequently mentioned in scien-
tific papers independently of the domain (e.g. ob-
serve, explain, etc.) was used to filter out such 
verbs. The top ranked verbs are selected and 
considered to be domain-specific. Moreover, these 
verbs are also corpus-specific (e.g. activate, 
bind, etc.). Table 3 provides a list of such verbs, 
which were used in the experiments. 
 
3.3   Complementation Pattern Learning 
 
In order to learn a verb complementation pattern 
for each of the selected verbs separately, terms are 
collected from the corpus by using these verbs as 
anchors. A GA has been implemented as an itera-
tive reasoning procedure based on a partial order 
relation induced by the domain-specific ontology.1 
In each iteration pairs of verb complementation 
patterns represented as sets of terms and term 
classes are merged. This operation involves the 
substitution of less general terms/classes by their 
more general counterparts, if there is a path in the 
ontology connecting them. Otherwise, the disjunc-
tion of the terms is formed and passed to the next 
iteration. Figure 1 depicts the process of learning a 
verb complementation pattern. 
Since the partial order relation induced by the 
ontology is transitive, the order in which terms are 
processed is of no importance. The final verb com-
plementation patterns are minimal in the sense that 
the number of terms in a verb complementation 
pattern and the depth of each individual term in the 
ontology are minimised. 
 
 
Figure 1. Learning the complementation pattern 
                    for the verb bind 
4 Term Classification Method 
The verb complementation patterns have been ob-
tained by running the GA on a set of terms some of 
which were present in an ontology, which is used 
                                                           
1 The partial order relation is based on the hierarchy of 
terms/classes: term/class t1 is in relation with t2, if there is a 
path in the ontology from t2 to t1. In that case, we say that t2  is 
more general than t1. 
during the learning process. The newly recognised 
terms (i.e. the ones not found in the ontology) will 
remain included in the final verb complementation 
patterns as non-classified terms, since at this point 
it is not known which classes could replace them. 
All elements of the final verb complementation 
patterns can be thus divided into two groups based 
on the criterion of their (non)existence in the on-
tology. The elements already present in the ontol-
ogy are candidate classes for the newly recognised 
terms. Let us now describe the classification 
method in more detail. 
Let V = {v1, v2, ... , vn} be a set of automatically 
identified domain-specific verbs. During the phase 
of learning verb complementation patterns, each of 
these verbs is associated with a set of classes and 
terms it co-occurs with. Let Ci = {ci,1, ci,2, ... , ci,mi} 
denote a set of classes assigned automatically to 
the verb vi (1 ? i ? n) by a learning algorithm based 
on the information found in the corpus and the 
training ontology. As indicated earlier, we define 
such set to be a verb complementation pattern for 
the given verb.  
 
4.1   Statistical Analysis 
 
As we planned to use verb complementation pat-
terns for term classification, we modified the origi-
nal learning algorithm (Spasic et al, 2002) by 
attaching the frequency information to terms and 
their classes. When substituting a less general class 
by its more general counterpart, 2  the frequency 
information is updated by summing the two 
respective frequencies of occurrence. In the final 
verb complementation pattern, each class ci,j has 
the frequency feature fi,j, which aggregates the fre-
quency of co-occurrence with vi (1 ? i ? n; 1 ? j ?  
mi) for the given class and its subclasses. The fre-
quency information is used to estimate the class 
probabilities given a verb, P(ci,j | vi): 
 
?
=
=
lm
l
li
ji
ji
f
fp
1
,
,
,  
                                                           
2 The ontology used for learning allowed multiple inheritance 
only at the leaf level, that way incurring no ambiguities when 
substituting subclass by its superclass. The multiple inheri-
tance at the leaf level was resolved by mapping each term to 
all its classes, which were then processed by a GA. 
Unclassified terms remain present in the final 
verb complementation patterns, and, like classes, 
they are also assigned the information on the fre-
quency of co-occurrence with the given verb. 
When classifying a specific term, this information 
is used to select the verb based on whose pattern 
the term will be classified. Precisely, the verb the 
given term most frequently co-occurs with is cho-
sen, as it is believed to be the most indicative one 
for the classification purpose. 
 
4.2   Term Similarity Measure 
 
A complementation pattern associated with the 
chosen verb typically contain several classes. In 
order to link the newly recognised terms to specific 
candidate classes, we used a hybrid term similarity 
measure, called the CLS similarity measure. It 
combines contextual, lexical and syntactic proper-
ties of terms in order to estimate their similarity 
(Nenadic et al, 2002).  
Lexical properties used in the CLS measure re-
fer to constituents shared by the compared terms. 
The rationale behind the lexical term similarity 
involves the following hypotheses: (1) Terms shar-
ing a head are likely to be hyponyms of the same 
term (e.g. progesterone receptor and oes-
trogen receptor). (2) A term derived by modi-
fying another term is likely to be its hyponym (e.g. 
nuclear receptor and orphan nuclear re-
ceptor). Counting the number of common con-
stituents is a simple and straightforward approach 
to measuring term similarity, but it falls short when 
it comes to single-word terms and those introduced 
in an ad-hoc manner. Thus, properties other than 
lexical need to be included.  
We use syntactic properties in the form of spe-
cific lexico-syntactical patterns indicating parallel 
usage of terms (e.g. both Term and Term). All 
terms used within a parallel structure have identi-
cal syntactic features and are used in combination 
with the same verb, preposition, etc., and, hence, 
can be regarded as similar with high precision. 
However, patterns used as syntactic properties of 
terms have relatively low frequency of occurrence 
compared to the total number of terms, and in or-
der to have a good recall, a large-size corpus is 
needed. In order to remedy for small-size corpora, 
other contextual features are exploited.  
Context patterns (CPs) in which terms appear 
are used as additional features for term compari-
son. CPs consist of the syntactic categories and 
other grammatical and lexical information (e.g. 
PREP NP V:stimulate). They are ranked ac-
cording to a measure called CP-value  (analogue to 
C-value for ATR). The ones whose CP-value is 
above a chosen threshold are deemed significant 
and are used to compare terms. Each term is asso-
ciated with a set of its CPs, and contextual similar-
ity between terms is then measured by comparing 
the corresponding sets. Automatically collected 
CPs are indeed domain-specific, but the method for 
their extraction is domain independent. 
 
4.3   Term-Class Similarity 
 
The CLS similarity measure applies to pairs of 
terms. However, in case of multiple choices pro-
vided by the verb complementation patterns, we 
need to compare terms to classes. In order to do so, 
we use the similarity between the given term and 
the terms belonging to the classes. The selection of 
terms to be compared is another issue. One possi-
bility is to use the full or random set of terms (be-
longing to the given class) that occur in the corpus. 
Alternatively, some ontologies provide a set of 
prototypical instances for each class, which can be 
used for comparison of terms and classes.3 More 
formally, if c is a class, e1, e2,..., ek are terms repre-
senting the class, and t is a term, then the similarity 
between the term t and the class c is calculated in 
the following way: 
?
=
?
=
k
j
j
i
ki
etCLS
etCLSctEx
1
2
},...,1{
),(
),(max),(  
 
This example-based similarity measure maxi-
mises the value of the CLS measure between the 
term and the instances representing the class. In 
addition, the values of the CLS measure are 
mapped into the interval (0,1) by performing vec-
tor normalisation in order to make them compara-
ble to the class probability estimations.  
 
4.4   Term Classification 
 
Finally, given the term t and the verb vi it most  
frequently co-occurs with, a score is calculated for 
                                                           
3 For example, in the UMLS ontology each class is assigned a 
number of its prototypical examples represented by terms. 
each class ci,j from the set Ci according to the fol-
lowing formula: 
 
),()1(),( ,,, jijiji ctExapactC ??+?=    (1) 
 
where a (0 ? a ? 1) is a parameter, which balances 
the impact of the class probabilities and the simi-
larity measure.4 A class with the highest C(t, ci,j) 
score is used to classify the term t. Alternatively, 
multiple classes may be suggested by setting a 
threshold for C(t, ci,j). 
At this point, let us reiterate that the final verb 
complementation patterns are minimal in the sense 
that the number of terms in a verb complementa-
tion pattern and the depth of each individual term 
in the ontology are minimised. The latter condition 
may cause the classification to be crude, that is ? 
new terms will be assigned to classes close to the 
root of the ontology. For more fine-grained classi-
fication results, the classes placed close to the root 
of the ontology should be either removed from the 
initial verb complementation patterns, thus being 
unable to override the classes found lower in the 
hierarchy or in other way prevented from substitut-
ing less general terms. The depth up to which the 
terms are to be blocked may be empirically deter-
mined. 
5 Experiments and Evaluation 
5.1   Resources 
 
The resources used for the experiments include an 
ontology and a corpus, both belonging to the do-
main of biomedicine. We used an ontology, which 
is a part of the UMLS (Unified Medical Language 
System) knowledge sources (UMLS, 2002). 
UMLS integrates biomedical information from a 
variety of sources and is regularly updated. 
Knowledge sources maintained under the UMLS 
project include: METATHESAURUS linking term 
variants referring to the same concepts; 
SPECIALIST LEXICON providing syntactic informa-
tion for terms, their component words, and general 
                                                           
4 Note that when a = 0, the classification method resembles 
the nearest neighbour classification method, where the exam-
ples are used as a training set. On the other hand, when a = 1, 
the method is similar to naive Bayesian learning. However, in 
both cases the method represents a modification of the men-
tioned approaches, as the classes used in formula (1) are not 
all classes, but the ones learned by the GA. 
English words; and SEMANTIC NETWORK contain-
ing information about the classes to which all 
METATHESAURUS concepts have been assigned. 
The knowledge sources used in our term classi-
fication experiments include METATHESAURUS 
and SEMANTIC NETWORK. As the number of terms 
in  METATHESAURUS was too large (2.10 million 
terms) and the classification scheme too broad 
(135 classes) for the preliminary experiments, we 
made a decision to focus only on terms belonging 
to a subtree of the global hierarchy of the 
SEMANTIC NETWORK. The root of this subtree re-
fers to substances, and it contains 28 classes. 
The corpus used in conjunction with the above 
ontology consisted of 2082 abstracts on nuclear 
receptors retrieved from the MEDLINE database 
(MEDLINE, 2003). The majority of terms found in 
the corpus were related to nuclear receptors and 
other types of biological substances, as well as the 
domain-specific verbs extracted automatically 
from the corpus in the way described in Section 3.  
 
5.2   Evaluation Framework 
 
When retrieving terms found in the context of do-
main-specific verbs (see Section 3 for details) both 
terms found in the ontology and terms recognised 
on the fly by the C/NC-value method should be 
extracted. However, for the purpose of evaluation, 
only terms classified in the ontology were used. In 
that case, it was possible to automatically verify 
whether such terms were correctly classified by 
comparing the classes suggested by the classifica-
tion method to the original classification informa-
tion found in the ontology. 
During the phase of retrieving the verb-term 
combinations, some of the terms were singled out 
for testing. Namely, for each verb, 10% of the re-
trieved terms were randomly selected for testing, 
and the union of all such terms formed a testing set 
(138 terms) for the classification task. The remain-
ing terms constituted a training set (1618 terms) 
and were used for the learning of complementation 
patterns.  
 
5.3   Results 
 
Based on the training set, domain-specific verbs 
were associated with the complementation patterns 
given (see Table 1 for examples). Then, each term 
from the training set was associated with the verb 
it most frequently co-occurred with. The comple-
mentation pattern learnt for that verb was used to 
classify the term in question.  
 
Verb Complementation pattern 
activate
bind
Immunologic Factor 
Receptor   
Enzyme 
Hormone 
Organic Chemical 
Hazardous or Poisonous Substance 
Pharmacologic Substance 
Table 1. Learnt verb complementation patterns 
 
Since the UMLS ontology contains a number of 
prototypical examples for each class, we have used 
these class representatives to compare unclassified 
terms to their potential classes as indicated in Sec-
tion 4. Table 2 shows the results for some of the 
terms from the testing set and compares them to 
the correct classifications obtained from the ontol-
ogy. 
  
Term Suggested 
class 
Correct classes 
4 hydroxy-
tamoxifen
Organic   
   chemical Organic chemical 
benzoic
acid
Organic  
   chemical 
Organic chemical 
Pharmacologic      
   substance 
testoster-
one
Pharmacologic 
   substance 
Steroid  
Pharmacologic 
   substance 
Hormone 
Table 2. Examples of the classification results 
 
Note that in UMLS one term can be assigned to 
multiple classes. We regarded a testing term to be 
correctly classified if the automatically suggested 
class was among these classes. Table 3 provides 
information on the performance of the classifica-
tion method for each of the considered verbs sepa-
rately and for the combined approach in which the 
verb most frequently co-occurring with a given 
term was used for its classification. The combined 
approach provided considerably higher recall 
(around 50%) and a slight improvement in preci-
sion (around 64%) compared to average values 
obtained with the same method for each of the 
verbs separately. The classification precision did 
not tend to very considerably, and was not affected 
by the recall values. The recall could be improved 
by taking into account more domain-specific verbs, 
while the improvement of precision depends on 
proper tuning of: (1) the module for learning the 
verb complementation patterns, and (2) the similar-
ity measure used for the classification. Another 
possibility is to generalize the classification 
method by relying on domain-specific lexico-
syntactic patterns instead of verbs. Such patterns 
would have higher discriminative power than verbs 
alone. Moreover, they could be acquired automati-
cally. For instance, the CP-value method can be 
used for their extraction from a corpus (Nenadic et 
al., 2003a). 
 
Verb Recall Precision F-measure 
activate 19.28 66.59 29.90 
bind 29.30 66.53 40.68 
compete   3.58 63.16   6.78 
conserve   2.41 61.82   4.64 
inhibit 16.62 62.81 26.28 
interact 13.16 64.31 21.85 
mediate 11.68 62.75 19.69 
modulate 10.44 64.13 17.96 
repress   6.18 62.91 11.25 
stimulate   9.39 63.25 16.35 
Average: 12.20 63.83 20.48 
Combined: 49.88 64.18 56.13 
Table 3. The performance of the classification  
                  method 
 
The values for precision and recall provided in 
Table 3 refer to the classification method itself. If 
it were to be used for the automatic ontology up-
date, then the success rate of such update would 
also depend on the performance of the term recog-
nition method, as the classification module would 
operate on its output. We used the C/NC-value 
method for ATR; still any other method may be 
used for this purpose. We have chosen the C/NC-
value method because it is constantly improving 
and is currently performing around 72% recall and 
98% precision (Nenadic et al, 2002). 
6 Conclusion 
Efficient update of the existing knowledge re-
positories in many rapidly expanding domains is a 
burning issue. Due to an enormous number of 
terms and the complex structure of the terminol-
ogy, manual update approaches are prone to be 
both inefficient and inconsistent. Thus, it has be-
come absolutely essential to implement efficient 
and reliable term recognition and term classifica-
tion methods as means of maintaining the knowl-
edge repositories. In this paper, we have suggested 
a domain independent classification method as a 
way of incorporating automatically recognised 
terms into an existing ontology. For the prelimi-
nary experiments, we used the UMLS ontology in 
the domain of biomedicine, but the method can be 
easily adapted to use other ontologies in any other 
domain.  
The classification method makes use of the 
contextual information. Not all word types found 
in the context are of equal importance in the 
process of reasoning about the terms: the most in-
formative are verbs, noun phrases (especially 
terms) and adjectives. The presented term 
classification approach revolves around domain-
specific verbs. These verbs are used to collect 
unclassified terms and to suggest their potential 
classes based on the automatically learnt verb 
complementation patterns.  
Note that not every term appearing in a corpus 
is guaranteed to be classified by the proposed clas-
sification method due to the fact that a term need 
not occur as a complement of a domain-specific 
verb. Still, for a large number of terms the classifi-
cation method is expected to obtain the classifica-
tion information, as it is highly probable (though 
not certain) for a term to occur in a context of a 
domain-specific verb. The main goal of the method 
is to provide aid for the automatic ontology update 
by populating newly recognised terms into an ex-
isting ontology, rather than classifying arbitrary 
term occurrences in the corpus.  
The presented classification method can be eas-
ily modified to use lexical classes other than verbs 
as a criterion for classification. Even more, it can 
be further generalised to use a combination of lexi-
cal classes, which can be specified as a set of 
lexico-syntactic patterns. Further experiments with 
the generalisation of the classification method by 
basing it on a set of domain-specific lexico-
syntactic patterns instead of domain-specific verbs 
are expected to demonstrate better performance in 
terms of recall and precision. These facts suggest 
that our classification approach, in combination 
with the C/NC-value method, could be reliably 
used as a (semi)automatic ontology maintenance 
procedure.  
References 
Nigel Collier, Chikashi Nobata and Junichi Tsujii. 2001. 
Automatic Acquisition and Classification of Termi-
nology Using a Tagged Corpus in the Molecular Bi-
ology Domain. Journal of Terminology, John 
Benjamins. 
Katerina Frantzi, Sophia Ananiadou and Hideki Mima. 
2000. Automatic Recognition of Multi-Word Terms: 
the C-value/NC-value Method. International Journal 
on Digital Libraries 3(2):115-130. 
Vasileios Hatzivassiloglou, Pablo Duboue and Andrey 
Rzhetsky. 2001. Disambiguating Proteins, Genes, 
and RNA in Text: A Machine Learning Approach. 
Bioinformatics, 1(1):1-10. 
Diana Maynard and Sophia Ananiadou. 2000. Identify-
ing Terms by their Family and Friends. Proceedings 
of COLING 2000, Saarbrucken, Germany, 530-536.  
MEDLINE. 2003. National Library of Medicine. Avail-
able at: http://www.ncbi.nlm.nih.gov/PubMed/ 
Goran Nenadic, Irena Spasic and Sophia Ananiadou. 
2002. Automatic Acronym Acquisition and Term 
Variation Management within Domain-Specific 
Texts. Proceedings of LREC-3, Las Palmas, Spain, 
2155-2162.  
Goran Nenadic, Irena Spasic and Sophia Ananiadou. 
2003a. Automatic Discovery of Term Similarities Us-
ing Pattern Mining. To appear in Terminology. 
Goran Nenadic, Simon Rice, Irena Spasic, Sophia 
Ananiadou and Benjamin Stapley. 2003b. Selecting 
Features for Text-Based Classification: from Docu-
ments to Terms. Proceedings of ACL Workshop on 
Natural Language Processing in Biomedicine, 
Sapporo, Japan. 
Chikashi Nobata, Nigel Collier and Junichi Tsujii. 2000. 
Automatic Term Identification and Classification in 
Biology Texts. Proceedings of the Natural Language 
Pacific Rim Symposium (NLPRS?2000), 369-375. 
Irena Spasic, Goran Nenadic and Sophia Ananiadou. 
2002. Tuning Context Features with Genetic Algo-
rithms. Proceedings of 3rd International Conference 
on Language,  Resources and Evaluation, Las Pal-
mas, Spain, 2048-2054. 
UMLS. 2002. UMLS Knowledge Sources. National 
Library of Medicine, 13th edition.  
 
Selecting Text Features for Gene Name Classification:  
from Documents to Terms 
 
Goran Nenadi?1,2, Simon Rice2, Irena Spasi?3, Sophia Ananiadou3, Benjamin Stapley2 
 
1Dept. of Computation 
UMIST 
Manchester, M60 1QD 
 
2Dept. of BioMolecular Sciences 
UMIST 
Manchester, M60 1QD 
 
3Computer Science 
University of Salford 
Salford, M5 4WT 
Abstract 
In this paper we discuss the performance 
of a text-based classification approach by 
comparing different types of features. We 
consider the automatic classification of 
gene names from the molecular biology 
literature, by using a support-vector ma-
chine method. Classification features 
range from words, lemmas and stems, to 
automatically extracted terms. Also, sim-
ple co-occurrences of genes within docu-
ments are considered. The preliminary 
experiments performed on a set of 3,000 
S. cerevisiae gene names and 53,000 
Medline abstracts have shown that using 
domain-specific terms can improve the 
performance compared to the standard 
bag-of-words approach, in particular for 
genes classified with higher confidence, 
and for under-represented classes.  
1 Introduction 
Dynamic development and new discoveries in the 
domain of biomedicine have resulted in the huge 
volume of the domain literature, which is con-
stantly expanding both in the size and thematic 
coverage (Blaschke et al, 2002). The literature, 
which is still the most relevant and the most useful 
knowledge source, is swamped by newly coined 
terms and relationships representing and linking 
newly identified or created compounds, genes, 
drugs, reactions, etc., which makes the existing 
terminological resources rarely up-to-date. There-
fore, domain knowledge sources need to frequently 
adapt to the advent of such terms by assorting them 
into appropriate classes, in order to allow biolo-
gists to rapidly acquire, analyse and visualise enti-
ties or group of entities (Stapley et al, 2002).  
Naming conventions solely cannot be used as 
reliable classification criteria, since they typically 
do not systematically reflect any particular func-
tional property or relatedness between biological 
entities. On the other hand, it has proved surpris-
ingly difficult to automatically predict classes for 
some types of biological entities based solely on 
experimental data (e.g. the prediction of protein 
cellular locations from sequences (Eisenhaber and 
Bork, 1998) or the amino acid composition of pro-
teins (Nishikawa and Ooi, 1982)).  
In order to overcome this problem, several lit-
erature-based classification methods have been 
developed (Collier et al 2001; Hatzivassiloglou et 
al., 2001). Classification methods typically rely on 
supervised machine learning techniques that ex-
amine the wider context in which terms are used. 
For example, Raychaudhuri et al (2002) used 
document-based word counts and naive Bayesian 
classification, maximum entropy modelling and 
nearest-neighbour classification to assign the GO 
ontology codes to a set of genes. Recently, sup-
port-vector machines (SVMs, (Vapnik, 1995)) 
have been widely used as fast, effective and reli-
able means for text-based classification, both for 
document classification (Joachims, 1998) and clas-
sification of specific named entities (Stapley et al, 
2002; Kazama et al, 2002). 
Regardless of the learning approach and target 
entities (documents or terms), different types of 
text features have been employed for the classifica-
tion task. For example, a bag-of-words approach 
was used by Stapley et al (2002) to classify pro-
teins, while Collier et al (2001) used orthographic 
features to classify different biological entities. On 
the other hand, Hatzivassiloglou et al (2001) ex-
perimented with morphological, distributional and 
shallow-syntactic information to discriminate be-
tween proteins, genes and RNAs.  
In this paper we analyse the impact of different 
types of features on the performance of an SVM-
based classifier. More precisely, we discuss the 
multi-class SVM performance with respect to the 
type of features used, ranging from document iden-
tifiers, through words, lemmas and stems, to auto-
matically extracted terms.   
The paper is organised as follows. After pre-
senting the related work on feature selection in 
Section 2, the methods used for engineering fea-
tures in our approach are explained in Section 3. 
Section 4 discusses the experiments and results. 
2 Related work 
An SVM is a binary classification method that 
combines statistical learning and optimisation tech-
niques with kernel mapping (Vapnik, 1995). The 
main idea of the method is to automatically learn a 
separation hyperplane from a set of training 
examples, which splits classified entities into two 
subsets according to a certain classification prop-
erty. The optimisation part is used to maximise the 
distance (called the margin) of each of the two 
subsets from the hyperplane.     
The SVM approach has been used for different 
classification tasks quite successfully, in particular 
for document classification, where the method out-
performed many alternative approaches (Joachims, 
1998). Similarly, SVMs have been used for term 
classification. For example, a bag-of-simple-words 
approach with idf-like weights was used to learn a 
multi-class SVM classifier for protein cellular lo-
cation classification (Stapley et al, 2002). Proteins 
were represented by feature vectors consisting of 
simple words co-occurring with them in a set of 
relevant Medline abstracts. The precision of the 
method was better than that of a classification 
method based on experimental data, and similar to 
a rule-based classifier.  
Unlike many other classification methods that 
have difficulties coping with huge dimensions, one 
of the main advantages of the SVM approach is 
that its performance does not depend on the dimen-
sionality of the space where the hyperplane separa-
tion takes place. This fact has been exploited in the 
way that many authors have suggested that ?there 
are few irrelevant features? and that ?SVMs elimi-
nate the need for feature selection? (Joachims, 
1998). It has been shown that even the removal of 
stop-words is not necessary (Leopold and Kinder-
mann, 2002). 
Few approaches have been undertaken only re-
cently to tune the original SVM approach by se-
lecting different features, or by using different 
feature weights and kernels, mostly for the docu-
ment classification task. For example, Leopold and 
Kindermann (2002) have discussed the impact of 
different feature weights on the performance of 
SVMs in the case of document classification in 
English and German. They have reported that an 
entropy-like weight was generally performing bet-
ter than idf, in particular for larger documents. 
Also, they suggested that, if using single words as 
features, the lemmatisation was not necessary, as it 
had no significant impact on the performance.   
Lodhi et al (2002) have experimented with dif-
ferent kernels for document classification. They 
have shown that a string kernel (which generates 
all sub-sequences of a certain number of charac-
ters) could be an effective alternative to linear ker-
nel SVMs, in particular in the sense of efficiency.  
In the case of term classification, Kazama et al 
(2002) used a more exhaustive feature set contain-
ing lexical information, POS tags, affixes and their 
combinations in order to recognise and classify 
terms into a set of general biological classes used 
within the GENIA project (GENIA, 2003). They 
investigated the influence of these features on the 
performance. For example, they claimed that suffix 
information was helpful, while POS and prefix 
features did not have clear or stable influence.  
While each of these studies used some kind of 
orthographical and/or lexical indicators to generate 
relevant features, we wanted to investigate the us-
age of semantic indicators (such as domain-
specific terms) as classification features, and to 
compare their performance with the classic lexi-
cally-based features. 
3 Feature selection and engineering 
The main aim while selecting classification fea-
tures is to find (and use) textual attributes that can 
improve the classification accuracy and accelerate 
the learning phase. In our experiments we exam-
ined the impact of different types of features on the 
performance of an SVM-based gene name classifi-
cation task. The main objective was to investigate 
whether additional linguistic pre-processing of 
documents could improve the SVM results, and, in 
particular, whether semantic processing (such as 
terminological analysis) was beneficial for the 
classification task. In other words, we wanted to 
see which textual units should be generated as in-
put feature vectors, and what level of pre-
processing was appropriate in order to produce 
more accurate predictions. 
We have experimented with two types of tex-
tual features: in the first case, we have used a clas-
sic bag-of-single-words approach, with different 
levels of lexical pre-processing (i.e. single words, 
lemmas, and stems). In the second case, features 
related to semantic pre-processing of documents 
have been generated: a set of automatically ex-
tracted multi-word terms (other than gene names to 
be classified) has been used as a feature set. Addi-
tionally, we have experimented with features 
reflecting simple gene-gene co-occurrences within 
the same documents.  
3.1 Single words as features 
The first set of experiments included a classic bag-
of-single-words approach. All abstracts (from a 
larger collection, see Section 4) that contained at 
least one occurrence of a given gene or its aliases 
have been selected as documents relevant for that 
gene. These documents have been treated as a sin-
gle virtual document pertinent to the given gene. 
All words co-occurring with a given gene in any of 
the abstracts were used as its features.  
A word has been defined as an alphanumeric 
sequence between two standard separators, with all 
numeric expressions that were not part of other 
words filtered out. In addition, a standard list of 
around 300 stop-words has been used to exclude 
some frequent non-content words. 
An idf-like measure has been used for feature 
weights: the weight of a word w for gene g is given 
by 
(1)                        |)|1(
)(1
log
gw
Rj
j
RN
wf
g
+
+ ?
?
 
where Rg  is a set of relevant documents for the 
gene g,  fj(w) is the frequency of w in document j, 
and Nw is the global frequency of w. Gene vectors, 
containing weights for all co-occurring words, 
have been used as input for the SVM. 
It is widely accepted that rare words do not 
have any significant influence on accuracy (cf. 
(Leopold and Kindermann, 2002)), neither do 
words appearing only in few documents. In our 
experiments (demonstrated in Section 4), we com-
pared the performance between the ?all-words ap-
proach? and an approach featuring words appearing 
in at least two documents. In the latter case, the 
dimension of the problem (expressed as the num-
ber of features) was significantly reduced (with 
factor 3), and consequently the training time was 
shortened (see Section 4). 
Since many authors claimed that the biomedical 
literature contained considerably more linguistic 
variations than text in general (cf. Yakushiji et al, 
2001), we applied two standard transformations in 
order to reduce the level of lexical variability. In 
the first case, we used the EngCG POS tagger 
(Voutilainen and Heikkila, 1993) to generate lem-
mas, so that lemmatised words were used as fea-
tures, while, in the second case, we generated 
stems by the Porter?s algorithm (Porter, 1980). 
Analogously to words, the same idf-based measure 
was used for weights, and experiments were also 
performed with all features and with the features 
appearing in no less than two documents. 
3.2 Terms as features 
Many literature-mining techniques rely heavily on 
the identification of main concepts, linguistically 
represented by domain specific terms (Nenadic et 
al., 2002b). Terms represent the most important 
concepts in a domain and have been used to char-
acterise documents semantically (Maynard and 
Ananiadou, 2002). Since terms are semantic indi-
cators used in scientific discourse, we hypothesised 
that they might be useful classification features.  
The high neology rate for terms makes existing 
glossaries incomplete for active and time-limited 
research, and thus automatic term extraction tools 
are needed for efficient terminological processing. 
In order to automatically generate term as features, 
we have used an enhanced version of the C-value 
method (Frantzi et al, 2000), which assigns term-
hoods to automatically extracted multi-word term 
candidates. The method combines linguistic forma-
tion patterns and statistical analysis. The linguistic 
part includes part-of-speech tagging, syntactic pat-
tern matching and the use of a stop list to eliminate 
frequent non-terms, while statistical termhoods 
amalgamate four numerical characteristic of a can-
didate term, namely: the frequency of occurrence, 
the frequency of occurrence as a nested element, 
the number of candidate terms containing it as a 
nested element, and term?s length.   
Due to the extensive term variability in the do-
main, the same concept may be designated by 
more than one term. Therefore, term variants con-
flation rules have been added to the linguistic part 
of the C-value method, in order to enhance the re-
sults of the statistical part. When term variants are 
processed separately by the statistical module, their 
termhoods are distributed across different variants 
providing separate frequencies for individual vari-
ants instead of a single frequency calculated for a 
term candidate unifying all of its variants. Hence, 
in order to make the most of the statistical part of 
the C-value method, all variants of the candidate 
terms are matched to their normalised forms by 
applying rule-based transformations and treated 
jointly as a term candidate  (Nenadic et al, 2002a). 
In addition, acronyms are acquired prior to the se-
lection of the term candidates and also mapped to 
their expanded forms, which are normalised in the 
same manner as other term candidates.  
Once a corpus has been terminologically proc-
essed, each target gene is assigned a set of terms 
appearing in the corresponding set of documents 
relevant to the given gene. Thus, in this case, gene 
vectors used in the SVM classifier contain co-
occurring terms, rather than single words. As term 
weights, we have used a formula analogous to (1). 
Also, similarly to single-word features, we have 
experimented with terms appearing in at least two 
documents. 
3.3 Combining word and term features 
The C-value method extracts only multi-word 
terms, which may be enriched during the normali-
sation process with some single-word terms, sourc-
ing from e.g. acronyms or orthographic variations. 
In order to assess impact of both single and multi-
word terms as features, we experimented with 
combining single-word based features with multi-
word terms by using a simple kernel modification 
that concatenates the corresponding feature vec-
tors. Thus, gene vectors used in this case contain 
both words and terms that genes co-occur with. 
3.4 Document identifiers as features 
Term co-occurrences have been traditionally used 
as an indication of their similarity (Ushioda, 1986), 
with documents considered as bags of words in the 
majority of approaches. For example, Stapley et al 
(2000) used document co-occurrence statistics of 
gene names in Medline abstracts to predict their 
connections. The co-occurrence statistics were rep-
resented by the reciprocal Dice coefficient. Similar 
approach has been undertaken by Jenssen et al 
(2001): they identified co-occurrences of gene 
names within abstracts, and assigned weights to 
their ?relationship? based on frequency of co-
occurrence.  
In our experiments, abstract identifiers (Pub-
Med identifiers, PMIDs) have been used as fea-
tures for classification, where the dimensionality of 
the feature space was equal to the number of 
documents in the document set. As feature 
weights, binary values (i.e. a gene is present/absent 
in a document) were used.  
We would like to point out that ? contrary to 
other features ? this approach is not a general 
learning approach, as document identifiers are not 
classification attributes that can be learnt and used 
against other corpora. Instead, this approach can be 
only used to classify new terms that appear in a 
closed corpus used for training. 
4 Experiments and discussions 
An experimental environment was set up by using 
the following resources:  
a) corpus: a set of documents has been ob-
tained by collecting Medline abstracts (NLM, 
2003) related to the baker?s yeast (S. cerevisiae), 
resulting in 52,845 abstracts; this set, containing 
almost 5 million word occurrences, was used as 
both training and testing corpus. 
b) classification entities: a set of 5007 S. cere-
visiae gene names has been retrieved from the 
SGD (Saccharomyces Genome Database) gene 
registry1, which also provided synonyms and ali-
ases of genes; 2975 gene names appearing in the 
corpus have been used for the classification task. 
c) classification scheme: each gene name has 
been classified according to a classification scheme 
based on eleven categories (see Table 1) of the up-
                                                           
1 http://genome-www.stanford.edu/Saccharomyces/registry.html  
per part of the GO ontology (Ashburner et al, 
2000)2. 
d) training and testing sets: positive examples 
for each class were split evenly between the train-
ing and testing sets, and, also, the number of nega-
tive examples in the training set was set equal to 
the number of positive examples within each class. 
The only exception was the metabolism class, 
which had far more positive than negatives exam-
ples. Therefore, in this case, we have evenly split 
negative examples between the training and testing 
sets. Table 1 presents the distribution of positive 
and negative examples for each class. 
d) SVM engine: for training the multi-class 
SVM, we used SVM Light package v3.50 
(Joachims, 1998) with a linear kernel function with 
the regulation parameter calculated as avg(<x,x>)-1.  
  
 
examples Category 
(GO code) training testing 1 testing 2 
autophagy 
(GO:0006914) 12/12 11/2940 11/11 
cell organisation 
(GO:0016043) 379/379 378/1839 378/378 
cell cycle 
(GO:0007049) 226/226 225/2298 225/225 
intracellular 
protein transport 
(GO:0006886) 
135/135 134/2571 134/134 
ion homeostasis 
(GO:0006873) 37/37 37/2864 37/37 
meiosis 
(GO:0007126) 45/45 44/2841 44/44 
metabolism 
(GO:0008152) 1118/370 1117/370 370/370 
signal  
transduction 
(GO:0007165) 
68/68 68/2771 68/68 
sporulation (sc) 
(GO:0007151) 27/27 27/2894 27/27 
response to 
stress 
(GO:0006950) 
91/91 91/2702 91/91 
transport 
(GO:0006810) 284/284 284/2123 284/284 
  
Table 1. Classification categories and the number 
of examples in the training and the testing sets 
                                                           
2 The January 2003 release of the GO ontology was used. A 
similar classification scheme was used in (Raychaudhuri et al, 
2002). 
Features have been generated according to the 
methods explained in Section 3 (Table 2 shows the 
number of features generated). As indicated earlier, 
the experiments have been performed by using ei-
ther all features or by selecting only those that ap-
peared in at least two documents. As a rule, there 
were no significant differences in the classification 
performance between the two.  
 
feature no. of all features 
no. of features 
appearing in >1 docs 
words 160k 60k 
lemmas 150k 54k 
stems 140k 50k 
terms 127k 62k 
  
Table 2. The number of features generated 
 
To evaluate the classification performance we 
have firstly generated precision/recall plots for 
each class. In the majority of classes, terms have 
demonstrated the best performance (cf. Figures 1 
and 2). However, the results have shown a wide 
disparity in performance across the classes, de-
pending on the size of the training set. The classes 
with fairly large number of training entities (e.g. 
metabolism) have been predicted quite accurately 
(regardless of the features used), while, on the 
other hand, under-represented classes (e.g. sporu-
lation) performed quite modestly (cf. Figure 1).   
 
 
Figure 1. Precision/recall plots for some classes  
using words and terms  
 
Comparison between performances on different 
classes is difficult if the classes contain fairly dif-
ferent ratios of positive/negative examples in the 
testing sets, as it was the case in our experiments 
(see Table 1, column testing 1). Therefore, we re-
evaluated the results by selecting ? for each class ? 
the same number of positive and negative exam-
ples (see Table 1, column testing 2), so that we 
could compare relative performance across classes. 
The results shown in Figure 2 actually indicate 
which classes are ?easier? to learn (only the per-
formance of single-words and terms are presented). 
To assess the global performance of classifica-
tion methods, we employed micro-averaging of the 
precision/recall data presented in Figure 2. In mi-
cro-averaging (Yang, 1997), the precision and re-
call are averaged over the number of entities that 
are classified (giving, thus, an equal weight to the 
performance on each gene). In other words, micro-
average shows the performance of the classifica-
tion system on a gene selected randomly from the 
testing set. 
The comparison of micro-averaging results for 
words, lemmas and stems has shown that there was 
no significant difference among them. This out-
come matches the results previously reported for 
the document classification task (Leopold and 
Kindermann, 2002), which means that there is no 
need to pre-process documents. 
Figure 3 shows the comparison of micro-
averaging plots for terms and lemmas. Terms per-
form generally much better at lower recall points, 
while there is just marginal difference between the 
two at the higher recall points. Very high precision 
points at lower recall mean that terms may be use-
ful classification features for precise predictions 
for genes classified with the highest confidence. 
 
 
 
Figure 2. Precision/recall plots for the 11 classes using words and terms  
(horizontal lines indicate the performance of a random classifier) 
 
Figure 3. Micro-averaging plot for 11 classes using 
lemmas and terms 
 
The results obtained by combining terms and 
words have not shown any improvements over us-
ing only terms as classification features. We be-
lieve that adding more features has introduced 
additional noise that derogated the overall per-
formance of terms. 
Finally, Figure 4 presents the comparison of 
classification results using terms and abstract iden-
tifiers. Although PMIDs outperformed terms, we 
reiterate that ? while other features allow learning 
more general properties that can be applied on 
other corpora ? PMIDs can be only used to classify 
new terms that appear in a closed training/testing 
corpus. 
 
 
 
Figure 4. Micro-averaging plot for 11 classes using 
PMIDs and terms 
 
 
5 Conclusion 
Due to an enormous number of terms and the 
complex and inconsistent structure of the biomedi-
cal terminology, manual update of knowledge re-
positories are prone to be both inefficient and 
inconsistent (Nenadic et al, 2002b; Stapley et al, 
2002). Therefore, automatic text-based classifica-
tion of biological entities (such as gene and protein 
names) is essential for efficient knowledge man-
agement and systematic approach that can cope 
with huge volume of the biomedical literature. Fur-
thermore, classified terms irrefutably have a posi-
tive impact on improving the results of IE/IR, 
knowledge acquisition, document classification 
and terminology management (Blaschke et al, 
2002).  
In this paper we have examined the procedures 
for engineering text-based features at various lev-
els of linguistic pre-processing, and considered 
their impacts on the performance of an SVM-based 
gene name classifier. The experiments have shown 
that simple linguistic pre-processing (such as lem-
matisation and stemming) does not have significant 
influence on the performance, i.e. there is no need 
to pre-process documents. Also, reducing the fea-
ture space by selecting only features that appear in 
more documents does not result in decrease of the 
performance, but can significantly reduce the time 
needed for training. PMID-based classification has 
shown very good performance, but a PMID-based 
classifier can be applied only on the training set of 
documents. 
The experiments have also shown that using 
semantic indicators (represented by dynamically 
extracted domain-specific terms) can improve the 
performance compared to the standard bag-of-
words approach, in particular at lower recall 
points, and for rare classes. This means that terms 
can be used as reliable features for classifying 
genes with higher confidence, and for under-
represented classes. However, terminological 
analysis requires considerable pre-processing time.  
Our further research will focus on generating 
the biological interpretation and justification of the 
classification results by using terms (that have 
been used as key distinguishing features for classi-
fication) as semantic indicators of the correspond-
ing classes.  
 
References 
 
M. Ashburner, et al. 2000. Gene Ontology: Tool for the 
Unification of Biology. Nature, 25:25-29.  
C. Blaschke, L. Hirschman and A. Valencia. 2002. In-
formation Extraction in Molecular Biology. Briefings 
in Bioinformatics, 3(2):154-165. 
N. Collier, C. Nobata and J. Tsujii. 2001. Automatic 
Acquisition and Classification of Terminology Using 
a Tagged Corpus in the Molecular Biology Domain. 
Journal of Terminology, John Benjamins. 
F. Eisenhaber and P. Bork. 1998. Wanted: Subcellular 
Localization of Proteins Based on Sequences. Trends 
Cell Biology, 8(4):169-170. 
K. Frantzi, S. Ananiadou and H. Mima. 2000. Automatic 
Recognition of Multi-Word Terms: the C-value/NC-
value Method. International Journal on Digital Li-
braries 3(2):115-130. 
GENIA project. 2003. GENIA resources. Available at:  
http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/ 
V. Hatzivassiloglou, P. Duboue and A. Rzhetsky. 2001. 
Disambiguating Proteins, Genes, and RNA in Text: A 
Machine Learning Approach. Bioinformatics, 1(1):1-
10. 
T. Jenssen, A. Laegreid, J. Komorowski and E. Hovig. 
2001. A literature Network of Human Genes for 
High-throughput Analysis of Gene Expressions. Na-
ture Genetics, 28: 21-28. 
T. Joachims. 1998. Text Categorization with Support 
Vector Machines: Learning Many Relevant Features. 
Proceedings of 10th European Conference on Ma-
chine Learning, Springer-Verlag, Heidelberg, 137-
142. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. Tun-
ing Support Vector Machines for Biomedical Named 
Entity Recognition. Proceedings of the Workshop 
NLP in Biomedicine, ACL 2002. 
E. Leopold and J. Kindermann. 2002. Text Categoriza-
tion with Support Vector Machines. How to Repre-
sent Texts in Input Space? Machine Learning, 
46:423-444. 
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini 
and C. Watkins. 2002. Text Classification using 
String Kernels. Journal of Machine Learning Re-
search, 2:419-444. 
D. Maynard and S. Ananiadou. 2000. Identifying Terms 
by their Family and Friends. Proceedings of 
COLING 2000, Saarbrucken, Germany, 530-536.  
 
 
K. Nishikawa and T. Ooi. 1982. Correlation of the 
Amino Acid Composition of a Protein to its Struc-
tural and Biological Characters. Journal of Bio-
chemistry (Tokyo), 91(5):1281-1824. 
G. Nenadic, I. Spasic and S. Ananiadou. 2002a. Auto-
matic Acronym Acquisition and Term Variation 
Management within Domain-Specific Texts. Proceed-
ings of LREC-3, Las Palmas, Spain, 2155-2162.  
G. Nenadic, H. Mima, I. Spasic, S. Ananiadou and J. 
Tsujii. 2002b. Terminology-based Literature Mining 
and Knowledge Acquisition in Biomedicine. Interna-
tional Journal of Medical Informatics, 67(1-3):33-48. 
NLM, National Library of Medicine. 2003. Medline. 
Available at http://www.ncbi.nlm.nih.gov/PubMed/ 
M. Porter. 1980: An Algorithm for Suffix Stripping. Pro-
gram, 14(1):130-137. 
S. Raychaudhuri, J. Chang, P. Sutphin and R. Altman. 
2002. Associating Genes with Gene Ontology Codes 
Using a Maximum Entropy Analysis of Biomedical 
Literature. Genome Research, 12:203-214. 
B. Stapley and G. Benoit. 2000. Bibliometrics: Informa-
tion Retrieval and Visualization from Co-occurrence 
of Gene Names in Medline Abstracts. Proceedings of 
the Pacific Symposium on Bio-computing, PSB 2000 
B. Stapley, L. Kelley and M. Sternberg. 2002. Predict-
ing the Sub-Cellular Location of Proteins from Text 
Using Support Vector Machines. Proceedings of the 
Pacific Symposium on Bio-computing, PSB 2002. 
A. Ushioda. 1996. Hierarchical Clustering of Words. 
Proceedings of COLING 96. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer Verlag, Heidelberg. 
A. Voutilainen and J. Heikkila. 1993. An English Con-
straint Grammar (ENGCG) a Surface-Syntactic 
Parser of English. In Fries, U et al (Eds.): Creating 
and Using English Language Corpora, Rodopi, Am-
sterdam/Atlanta, 189-199. 
A. Yakushiji, Y. Tateisi, Y. Miyao and J. Tsujii. 2001. 
Event Extraction From Biomedical Papers Using a 
Full Parser. Proceedings PSB 2001, Hawaii, USA, 
408-419. 
Y. Yang. 1997. An Evaluation of Statistical Approaches 
to Text Categorization. Information Retrieval, 
1(1/2):69-90. 
Morpho-syntactic Clues for Terminological Processing in Serbian 
Goran Nenadi? 
Department of Computing 
UMIST, UK 
G.Nenadic@umist.ac.uk
Irena Spasi? 
Computer Science 
University of Salford, UK 
I.Spasic@salford.ac.uk
Sophia Ananiadou 
Computer Science  
University of Salford, UK 
S.Ananiadou@salford.ac.uk
 
 
Abstract 
In this paper we discuss morpho-syntactic 
clues that can be used to facilitate termi-
nological processing in Serbian. A 
method (called SRCE) for automatic ex-
traction of multiword terms is presented. 
The approach incorporates a set of ge-
neric morpho-syntactic filters for recogni-
tion of term candidates, a method for 
conflation of morphological variants and 
a module for foreign word recognition. 
Morpho-syntactic filters describe general 
term formation patterns, and are imple-
mented as generic regular expressions. 
The inner structure together with the 
agreements within term candidates are 
used as clues to discover the boundaries 
of nested terms. The results of the termi-
nological processing of a textbook corpus 
in the domains of mathematics and com-
puter science are presented.  
1 Introduction 
An overwhelming amount of textual information 
presented in newswire, scientific literature, legal 
texts, etc., makes it difficult for a human to effi-
ciently localise the information of interest. In 
particular, it is doubtful that anybody could proc-
ess such huge amount of information without an 
automated help, especially when the information 
content spans across domains. The amount of e-
documents and their fuzzy structure require 
effective tools that can help users to 
systematically gather and make use of the 
information encoded in text documents. For these 
reasons, different text and/or literature mining 
techniques have been developed recently (e.g. 
(Hearst et al, 2000; Grobelnik et al, 2000)) in 
order to facilitate efficient discovery of knowl-
cient discovery of knowledge contained in large 
scientific or legal text collections. The main goal 
is to retrieve the knowledge ?buried? in a text 
and to present it to users in a digested form.  
The discovery (and transfer) of knowledge re-
lies heavily on the identification of relevant con-
cepts, which are linguistically represented by 
domain specific terms. Terms represent the most 
important notions in a domain and characterise 
documents semantically, and thus should be used 
as a basis for sophisticated knowledge acquisi-
tion. Still, few text-mining systems incorporate 
deep and dynamic terminology processing, al-
though there is an increasing amount of new 
terms that represent newly created concepts in 
rapidly developing fields. Existing term diction-
aries and standardised terminologies offer only a 
partial solution, as they are almost never up-to-
date. Although naming conventions do exist for 
some types of concepts (e.g. gene and protein 
names in biomedicine), these are only guidelines 
and as such do not impose restrictions to domain 
experts, who frequently introduce ad-hoc terms. 
Thus, the lack of clear naming conventions 
makes the automatic term recognition (ATR) task 
difficult even for languages that are not morpho-
logically and derivationally rich.  
ATR tools have been developed for English 
(Frantzi et al, 2000), French (Jacquemin, 2001), 
Japanese (Nakagawa and Mori, 2000), etc. Some 
methods rely purely on linguistic information, 
namely morpho-syntactic features of term candi-
dates (Ananiadou, 1994). Hybrid approaches 
combining linguistic patterns and statistical 
measures (e.g. (Frantzi et al, 2000)) and ma-
chine-learning techniques (e.g. (Hatzivassiloglou 
et al, 2001)) have been also used.  
However, few studies have been done for 
morphologically rich Slavic languages. For ex-
ample, Vintar (2000) presented two methods for 
extraction of terminological collocations in order 
to assist the translation process in Slovene. The 
statistical approach was based on the mutual ex-
pectation and LocalMax measures, and involved 
collocation extraction from raw text. The ex-
tracted collocations were filtered with a stop-
word list, and only collocations containing sin-
gle-word terms (devised previously by bilingual 
alignment) were accepted as relevant. In another 
approach, she used regular expression patterns to 
extract term collocations from a morpho-
syntactically tagged corpus. However, these pat-
terns are too general, and consequently not all 
extracted phrases were terminologically relevant.  
In this paper we discuss automatic terminology 
recognition in Serbian, in particular, the extrac-
tion of multiword terms, which are very frequent1 
in certain domains (e.g. natural sciences, mathe-
matics, etc.). Since Serbian is a highly inflective 
and morphologically and derivationally rich lan-
guage, morpho-syntactic clues are indispensable 
in the ATR process. Our hybrid approach (called 
SRCE ? Serbian C-value) combines morpho-
syntactic features of term candidates and statisti-
cal analysis of their occurrences in text. In addi-
tion, since terms appear in texts in many different 
forms due to their morphological and derivational 
variations, the necessity of taking these variations 
into account becomes particularly apparent. 
Therefore, the SRCE method incorporates generic 
morpho-syntactic patterns, a term normalisation 
approach and a foreign word detection method.  
The paper is organised as follows: in Section 2 
we present an overview of the core term extrac-
tion method, called the C-value method. In Sec-
tion 3 we discuss morpho-syntactic clues, the 
normalisation approach and the foreign word 
recognition that are used for singling out terms in 
Serbian. The experiments and evaluation are de-
scribed in Section 4. 
 
2 Automatic Term Recognition: the core 
C-value method 
Our approach to ATR is based on the C-value 
method (Frantzi et al, 2000), which extracts 
multi-word terms. It is a general term recognition 
approach in the sense that it is not limited to spe-
cific classes of concepts. The approach is hybrid: 
the method combines linguistic knowledge (term 
                                                           
1 In English, more than 85% of domain-specific terms are 
multi-words (Nakagawa and Mori, 2000).  
formation patterns) and statistical analysis. Lin-
guistic knowledge is used to single out term can-
didates, while their statistical features are used to 
measure the likelihood of term candidates being 
?real? terms. The method uses a POS tagged text 
as input, and outputs a list of extracted terms 
ranked according to their termhoods. Termhood 
is a numeric estimation of the degree to which a 
given linguistic unit (a multiword compound) is 
related to a domain-specific concept. However, 
the values are not normalised in the sense that a 
multiword, having a termhood value 10, is 10 
times more likely to be a term than a term candi-
date with a termhood value 1.  
In general, the C-value method enhances the 
commonly used baseline method that extracts 
most frequent term candidates (assuming that 
termhoods directly correspond to frequencies of 
occurrence) by making it sensitive to a particular 
type of terms ? nested terms2.   
The method is implemented as a two-step pro-
cedure. In the first step, term candidates are ex-
tracted using a set of morpho-syntactic filters, 
which describe general term formation patterns in 
a given language. As a rule, terms form a proper 
subset of noun phrases (NPs). For example, a set 
of general filters for English may include the fol-
lowing patterns:3 
 
Noun+ Noun  
(Adj | Noun)+ Noun  
(Adj | Noun)+| ((Adj | Noun)* Prep?) (Adj | Noun)* Noun  
 
Although these patterns are regular expressions, 
the filters are implemented as unification-like 
LR(1) rules (Mima et al, 1995) in order to facili-
tate processing of grammatical agreements (if 
any) within term candidates.  
For each term candidate extracted by a filter, a 
set of nested term candidates is generated (see 
Table 1 for an example in English). The proce-
dure for the generation of nested term candidates 
is implemented via transformation rules for each 
morpho-syntactic filter that is used to extract 
                                                           
2 For example, nuclear receptor is a nested term in hormone 
nuclear receptor. Similarly, baza podataka (Engl. database) 
is a nested term in a?uriranje baze podataka (Engl. update of 
database).  
3 Noun, Adj and Prep denote POS tags that correspond to 
nouns, adjectives and prepositions respectively. These filters 
were used for ATR from newswire corpora and in biomedi-
cine (Frantzi et al, 2000; Nenadi? et al, 2002).  
term candidates. The main indicator that a nested 
term candidate might be a real term is that it also 
appears on its own in the corpus. 
 
Term Term candidate: 
      steroid hormone receptor factor + 
Nested term candidates: 
steroid hormone receptor 
hormone receptor factor 
steroid hormone 
hormone receptor  
receptor factor 
 
+ 
- 
+ 
+ 
- 
Table 1: Nested term candidates 
 
In the second step, the term candidates are as-
signed termhoods (referred to as C-values) ac-
cording to a statistical measure. The measure 
amalgamates four numerical corpus-based char-
acteristic of a candidate term, namely the fre-
quency of occurrence, the frequency of occurring 
as nested within other candidate terms, the num-
ber of candidate terms inside which the given 
candidate term is nested, and the number of 
words contained in the candidate term. Formally,  
where a denotes a term candidate, f(a) corre-
sponds to its frequency, |a| denotes the number of 
words in a, and Ta is a set of terms that contain 
term a as a nested term. Term candidates are 
ranked according to their C-values, and terms 
whose C-values are higher than a chosen thresh-
old are presented as terms. 
Evaluation of the C-value method for English 
has shown that using additional statistical infor-
mation (frequency of ?nestedness?) improves the 
precision with slight loss on recall (Frantzi et al, 
2000). Also, systematic term normalisation may 
further improve precision and recall of the 
method (Nenadi? et al, 2002). 
3 Morpho-syntactic clues for extraction 
of terms in Serbian 
In order to adjust the core C-value method for 
Serbian, we have defined an appropriate set of 
morpho-syntactic filters and rules for inflectional 
normalisation of term candidates, and, addition-
ally, a module for foreign word recognition.  
3.1 Term formation patterns 
As a rule, the vast majority of multiword terms in 
Serbian match the following general formation 
pattern:4 
 
(1)           (Adj | ProAdj | Num | Noun )+ Noun 
 
which has been used for recognition of NPs in 
Serbian (Nenadi? and Vitas, 1998a). Of course, 
not all NPs that follow this pattern are terms.5 
Moreover, when applied to an initially POS 
tagged text6, this pattern may be too general even 
for description of NPs, as not all word sequences 
in a text that match this pattern are valid NPs. For 
example, in a sequence koji se naziva relacioni 
model (Engl. which is called the relational 
model), a word naziva can be initially tagged ei-
ther as a noun naziv (Engl. name) or a verb na-
zivati (Engl. call), although, in this sentence, only 
the latter is correct. Thus, without further POS 
disambiguation, the string naziva relacioni model 
follows the pattern (1), although it is not a valid 
NP. This means that classical regular expressions 
are not sufficient for the representation of such 
constraints, and that we need more expressive 
means to model constraints related to the NP 
structure and agreements of multiword constitu-
ents on case, number and gender. We used the 
notion of generic patterns as an extension of 
regular expressions (Nenadi? and Vitas, 1998b). 
For example, a generic pattern 
 
(2)      Adj.x1y1z1  Noun.x1y1z1   Adj.x2y2g   Noun.x2y2g 
 
models obligatory agreements that each NP from 
a specific class has to fulfil: both first and second 
pairs of adjectives and nouns must have the same 
values for certain morphological features (i.e. 
values for gender, number and case denoted by xi, 
                                                           
4 ProAdj and Num denote possessive adjectives and numbers 
respectively. 
5 For example, ovaj na?in (Engl. this way), veliki deo (Engl. 
large part), etc. This is a reason why we need additional 
processing to recognise semantically relevant NPs. 
6 Initially (or lexically) tagged POS text is a text in which 
every word occurrence is associated with all of its possible 
lexical and grammatical interpretations. The initial POS 
tagging is intrinsically ambiguous as each word is analysed 
separately, without considering neighbouring words (Ne-
nadi? and Vitas, 1998a). Thus, as a result of initial tagging, a 
lot of lexical ambiguities arise resulting in highly ambiguous 
word sequences. See Section 4 for further discussion. 
? ? ? 
? ? ? 
? 
? 
? 
= ? ? ?
otherwise                                   
        )), ( | | 
1 )( ( | | log 
nested, not   is                ), ( | | log 
) ( 2 
2 
a a Tb b fT a f a
a af a
a value C 
yi and zi respectively), while these values may be 
different for each respective pair. The last adjec-
tive and noun are ?frozen? in the genitive case 
(g), while the case (z1) in the first pair is ?free?. 
By defining generic patterns one can model the 
agreements within various lexical structures in a 
highly inflective language such as Serbian (Ne-
nadi? and Vitas, 1998b). As a result, these 
agreements can be used to detect the boundaries 
of the structures in questions. 
A set of generic patterns has been used to 
model the most frequent term formation patterns 
in Serbian. The set is mainly based on patterns 
used to model NPs in Serbian. Table 2 presents 
some of them. First four patterns describe NPs 
containing a nested NP whose lexical properties 
(such as case and/or number) are invariant in all 
inflected forms of the host NP. As a rule, the fro-
zen part is in genitive. Depending on NP con-
stituents, some agreements are obligatory within 
frozen part (see, for example, the third pattern ? 
agreements between an adjective and the corre-
sponding noun), or not (see the fourth pattern ? 
no necessary agreement between the last two 
nouns in gender, number). The fifth pattern (Ta-
ble 2) corresponds to NPs that do not have in-
variant parts. 
 
Generic patterns Examples 
1 N1   N gen baza podataka nejednakost trougla 
2 A1   N1   N gen manipulativni aspekt modela grani?na vrednost niza 
3 N1   A gen  N gen operacija prirodnog spajanja niz realnih brojeva 
4 N1   N 2;gen N gen integritet baze podataka kriterijum konvergencije niza 
5 A1+  N1 pro?ireni relacioni model  kompletan metri?ki  prostor 
Table 2: Frequent term formation patterns7 
 
While these patterns are used to single out 
term candidates from an initially tagged text, 
agreements within NPs are used to generate pos-
sible nested structures. While the rules for nested 
structures are more ?blurred? in English (since 
                                                           
7 In order to improve readability of filters, the generic pat-
terns in this table are encoded using the following syntax: A 
and N stand for Adj and Noun respectively, while X1 stands 
for X.x1y1z1 , Xgen stands for X.xyg and X2;gen stands for 
X.x2y2g (for X ? {A, N}). Also, invariant parts are underlined 
in the given examples. 
 
nouns are usually used as modifiers), ?nested-
ness? in Serbian has to preserve the necessary 
structure and inner agreements, which are spe-
cific for the NP class in question. Therefore, gen-
eration of nested term candidates depends on the 
type of host term candidates (consider examples 
in Table 3). Nested structures that are not them-
selves NPs are not considered as term candidates. 
 
     Nested term candidates  NP Term 
 
2 
 
manipulativni aspekt modela 
   manipulativni aspekt  
   aspekt modela 
+ 
+ 
+ 
+ 
- 
- 
 
3 
 
operacija prirodnog spajanja  
   operacija prirodnog 
   prirodnog spajanja 
+ 
- 
+ 
+ 
- 
+ 
 
4 
 
integritet baze podataka 
 integritet baze  
 baze podataka 
+ 
+ 
+ 
+ 
- 
+ 
 
5 
 
kompletan metri?ki prostor 
  kompletan metri?ki  
  metri?ki prostor 
+ 
- 
+ 
+ 
- 
+ 
Table 3: Nested term candidates (in Serbian) 
3.2 Conflating morphological variants 
If we aim at systematic recognition of terms, then 
handling term variation has to be treated as an 
essential part of terminology retrieval. Term 
variation ranges from simple orthographic (e.g. 
oestrogen ? estrogen, vitamin ? vitamine) and 
morphological variants (e.g. clone ? clones) to 
more complex semantic variation (e.g. eye sur-
gery ? ophthalmologic surgery).  
Several methods for term variation manage-
ment have been developed. For example, the 
BLAST system (Krauthammer et al, 2000) used 
approximate text string matching techniques and 
dictionaries to recognise spelling variations in 
gene and protein names. FASTR (Jacquemin, 
2001) handles morphological and syntactic varia-
tions by means of meta-rules used to describe 
term normalisation, while semantic variants are 
handled via WordNet.  
The necessity of taking term variants into ac-
count as part of ATR process becomes particu-
larly apparent in highly inflective languages. In 
Serbian, for example, the simplest morphological 
variations generally give rise to 14 possible vari-
ants of a single term (seven cases and two num-
bers (singular and plural) ? see Table 4). If the 
core C-value method were to be applied without 
conflating morphological variants, then term-
hoods would be distributed across different mor-
phological variants providing separate 
frequencies for individual variants instead of a 
single frequency calculated for a term candidate 
unifying all of its variants. In addition, the ?nest-
ing? factor of the C-value method would cause 
skewed results, since the case property of nested 
terms does not have normal distribution. Namely, 
as indicated previously (see Table 2), the major-
ity of nested terms in Serbian are in genitive case, 
which means that the termhood for a term candi-
date in genitive case would differ significantly 
from its counterparts in other cases. Moreover, 
this deviation cannot be remedied later by sum-
ming up individual termhoods, since C-value is 
not an additive measure. Hence, in order for the 
C-value method to be applied correctly in a 
highly inflective language, term candidates must 
be (at least inflectionally) normalised prior to the 
calculation of termhoods.  
 
Canonical form: 
operacija prirodnog spajanja (nom. sing. = ns) 
 
Morphological variants: 
operacija prirodnog spajanja (ns;gp) 
operacije prirodnog spajanja (gs;np;ap;vp) 
operaciji  prirodnog spajanja (ds;ls) 
operaciju prirodnog spajanja (as) 
operacijo prirodnog spajanja (vs) 
operacijom prirodnog spajanja (is) 
operacijama prirodnog spajanja (dp;ip;lp) 
Normalised form: 
    operacija (ns) prirodno (nsm) spajanje (ns)  
 
Table 4: Variants and normalisation of term 
candidates ? an example for term operacija prirod-
nog spajanja (Engl. natural join operation) 
 
Our approach to morphological normalisation 
of term variants is based on the normalisation of 
individual term constituents. Namely, each word 
that is a part of a term candidate is mapped onto 
its lemma, and term candidates are treated as se-
quences of lemmas. At the end of the ATR proc-
ess, terms are converted into their canonical form 
(singular, nominative case), which is not neces-
sarily identical to the normalised form (the se-
quence of the corresponding singular words in 
singular, nominative case). The normalisation 
process is illustrated in Table 4. 
At this point, the usage of generic patterns in 
order to check the agreements in case, number 
and gender during the phase of filtering of term 
candidates might seem unnecessary, since all 
these features are subsequently normalised. How-
ever, in order to enhance the precision of the 
SRCE method, it is important for term candidates 
to be correctly recognised prior to the statistical 
analysis. This means that the necessary agree-
ments between NP constituents have to be 
checked. Once the term candidates are identified, 
they are normalised in order to make the most of 
the statistical part of the method.   
3.3 Foreign word detection 
Despite the efforts to rely mostly on Serbian vo-
cabulary when building a terminology, many of 
the terms used in specific scientific domains bor-
row some of their building blocks from lan-
guages other than Serbian at various levels. For 
example, at morphological level, foreign suffixes, 
mostly originating from Latin and Greek, are of-
ten ?preferred? to their Serbian counterparts in, 
for example, the biomedical domain, even when 
they are used to modify a root that is in fact Ser-
bian (e.g. amino-kiselina (Engl. amino acid)). 
Similarly, at lexical level, words of foreign origin 
are used to form multi-word terms (e.g. redun-
dantan atribut (Engl. redundant attribute)). This 
is particularly obvious in fairly recently expanded 
disciplines such as computer science, where, for 
many of the original terms used in English, it has 
not been simple to adapt new terms in Serbian. 
Consequently, many of the terms have been sim-
ply transcribed into Serbian or, even worse, they 
are still used in their original form. Not only do 
foreign words appear as ?valid? parts of terms, 
but they have also proved to be good indicators 
of terms. It is, thus, necessary to develop proce-
dures for their detection.  
In our approach, the recognition of foreign 
words has been integrated into the ATR process 
for Serbian. The following morphological fea-
tures are used to indicate occurrences of potential 
foreign words (Spasi?, 1996): 
 
? characters (e.g. x, y, q) that do not belong to 
Serbian graphemic system, 
? successive vowel occurrences, 
? exception to the palatalisation rule,   
? exception to the assimilation rules, 
? occurrence of atypical consonant bi/tri-grams  
? occurrence of bi-grams or tri-grams typical 
for other languages (especially Latin and 
English), and 
? foreign affixes. 
 
The words satisfying some of the above crite-
ria are not necessarily foreign words. The preci-
sion of these rules varies from one to another. For 
example, the first rule is the strongest indicator of 
the presence of foreign words, since the alpha-
betical system used is not Serbian. Other rules 
may be tuned to a certain extent in order to in-
crease their precision.  
Let us, for instance, consider the second rule. 
The successive usage of vowels is fairly frequent 
in Serbian, but the majority of such cases follow 
certain restrictions8 under which they can occur. 
Moreover, these restrictions can be described by 
regular expressions. Any other occurrence of 
successive vowels can be used to indicate a po-
tential foreign word. 
Foreign word detection has been incorporated 
into the ATR process in two ways: during the 
selection of term candidates and for the calcula-
tion of termhoods. First, it is used before the ini-
tial POS tagging process in order to locate 
foreign words, which are tagged accordingly. 
Otherwise, foreign words would be typically 
considered as unknown. As explained earlier, it is 
very likely for foreign words in Serbian scientific 
and technical texts to be related to domain-
specific concepts, and their mishandling would 
significantly decrease the recall of the ATR 
method. This information is used by the linguistic 
part of the SRCE-method, where we introduced a 
special category corresponding to foreign words.  
In the second step, that is - once the term can-
didates have been selected - the information 
about foreign origin is used to increase the term-
hood of term candidates containing such words. 
This time, foreign word recognition is used to 
improve the precision of the ATR method. 
                                                           
8 For example, verbs in the paste tense, masculine gender 
always end with a pair of vowels (e.g. ispitivao (Engl. exam-
ined)). Further, some adjectives in masculine gender (e.g. 
beo (Engl. white)), as well as some nouns in masculine gen-
der (e.g. smisao (Engl. sense)) also end with a pair of vow-
els. The usage of prefixes is another example where vowels 
may occur successively (e.g. za+ustaviti (Engl. to stop)). 
4 Experiments and discussion 
The preliminary ATR experiments were con-
ducted using the SRCE system on a corpus con-
taining samples from university textbooks in 
mathematics9 and computer science10 (altogether 
120k words). 
Texts were pre-processed, i.e. initially tagged, 
by a system of electronic dictionaries (e-
dictionaries) containing simple nominal words 
for Serbian (Vitas, 1993). E-dictionaries contain 
exhaustive description of morpho-syntactic 
characteristics and are used for lexical 
recognition and initial lemmatisation of words 
that occur in a text.  This process is realised by e-
dictionary look-up, which results in an initially 
tagged text: each textual word is associated with 
its lemma(s) and corresponding morpho-syntactic 
categories (tags) retrieved from the  
e-dictionary. In general, e-dictionaries cannot 
resolve lexical ambiguities that result from the 
fact that there is no one-to-one correspondence 
between word forms and their morpho-syntactic 
features. There are different methods to resolve 
ambiguities (e.g. cache-dictionaries or local 
grammars), but in our experiments no disam-
biguation techniques were applied.  
In order to extract a list of term candidates, the 
set of morpho-syntactic filters described in 3.1 
was applied to the initially tagged corpus. We 
performed two sets of experiments. 
In the first experiment, we did not use any 
stoplist to discard unwanted constituents of term 
candidates. For each term candidate, we gener-
ated a canonical form (nominative, singular), a 
morphologically normalised form (list of normal-
ised words comprising the term candidate) and a 
list of nested term candidates (see Table 3 for 
examples).  In the next step, C-values for term 
candidates were calculated using statistics based 
on occurrences of normalised forms, and all term 
candidates with C-values above an empirically 
chosen threshold were selected as terms.  
Table 5 gives some examples of the recognised 
terms. In order to calculate the precision, we ex-
                                                           
9 N. La?eti?, Matematika II/1, Nau?na knjiga, Beograd, 
1994 
10 G. Pavlovi?-La?eti?, Osnove relacionih baza podataka, 
Vesta - Matemati?ki fakultet, Beograd, 1996. We would like 
to thank the authors of both textbooks for giving us permis-
sion to use their texts for experiments. 
amined separately interval precisions in sub-
corpora in mathematical analysis and computer 
science (see Table 6). Intervals are sets of recog-
nised terms that are placed at certain positions 
within the list. For example, interval 1-50 con-
tains top 50 terms, while the interval over 150 
contains all terms whose positions in the list are 
above 150. Terms have been inspected by the 
first two authors, who are Serbian native speakers 
and are specialists in both computer science and 
mathematics. 
 
Term  C-value 
metri?ki prostor   
topolo?ki prostor 
otvoren skup      
normiran prostor  
Ko?ijev niz 
zatvoren skup  
vektorski prostor  
prirodan broj 
nejednakost trougla    
neprekidnost preslikavanja 
Hausdorfov topolo?ki prostor     
633.55 
175.13 
93.20 
88.00 
68.11 
59.20 
53.13 
44.41 
33.98 
28.02 
19.43 
Table 5: Top ranked terms in the domain of  
mathematical analysis 
 
Interval Mathematical analysis 
Computer 
science 
1 ? 50 98% 90% 
50 ? 100 88% 70% 
100 ? 150 52% 58% 
> 150 69% 68% 
Table 6: Precision of the ATR method  
(without the usage of a stoplist) 
 
In the first 50 terms for the domain of mathe-
matical analysis, there was only one false term 
candidate (specijalna klasa neprekidnih pres-
likavanja), which contained an ?unwanted? adjec-
tive specijalna (Engl. special). The reason for the 
significant drop in the precision in the second and 
third intervals is mainly the same: apart from few 
true negatives11, the majority of false term candi-
dates contained common ?unwanted? constitu-
ents, which are sampled in Table 7. The results 
for the computer science sub-corpus were slightly 
worse since the mathematical language seems to 
be more consistent and restricted. 
                                                           
11 Such as: toplo?ka ta?ka gledi?ta, kompletnost prostora igra, 
kod preslikavnja. 
In the second experiment, we used a stoplist 
containing the words detected as frequent 
?wrong? constituents in the previous experi-
ments. The results are summarised in Table 8. 
 
prozvoljan 
tra?en 
specijalan 
va?an 
odgovaraju?i 
definisan 
op?ti 
dokazan 
globalan 
jedinstven 
poznat 
veliki 
pojam 
specifi?nost 
svojstvo 
slu?aj 
posledica 
gledi?te 
Table 7: A sample of normalised stop-words 
 
Interval Mathematical analysis 
Computer 
science 
1 ? 50 100% 94% 
50 ? 100 92% 92% 
100 ? 150 80% 74% 
> 150 74% 70% 
Table 8: Precision of the ATR method 
 (with the usage of a stoplist) 
 
The majority of remaining errors originate 
from the ambiguous POS tagging (more than 
50%, problematic words being naziv(a), igra, 
kod, etc.). Since no further processing of text has 
been performed, another source of problems is 
the detection of boundaries of frozen parts in 
prepositional phrases (e.g. na osnovu (Engl. 
based on), u slu?aju (Engl. in the case of)), 
which may be resolved by using a set of corre-
sponding local grammars (Nenadi? and Vitas, 
1998b). In addition, for the computer science 
domain, some of the false terms were related to a 
specific application area (the text intensively 
used examples from a university information sys-
tem, so candidates such as zvanje nastavnika 
(Engl. lecturer position), godina studija (Engl. 
year of study), etc. were wrongly suggested as 
computer science terms). 
5 Conclusion 
In this paper we have presented an approach to 
automatic extraction of terminology in a morpho-
logically rich language, such as Serbian. Terms 
extracted automatically may be used as semantic 
indicators for a range of classic IR/IE tasks. 
The approach is hybrid: it combines morpho-
syntactic filters for extraction of term candidates, 
and statistical analysis that ranks term candidates 
according to their termhood.  
Extraction of term candidates is based on the 
recognition of proper NPs. In order to enhance 
both the precision and recall of the ATR method, 
it is inevitable to incorporate significant linguistic 
knowledge. Since describing NPs by means of 
regular expressions is not sufficient for modelling 
agreements between NP constituents, we have 
used generic morpho-syntactic patterns. Further, 
since not all NPs are terms that semantically 
characterise documents, we have used a statisti-
cal measure in order to estimate semantic signifi-
cance of term candidates. Also, once the term 
candidates are correctly identified, they are nor-
malised in order to make the most of the statisti-
cal part of the method. Term candidates 
suggested as terms by the statistical part of the 
SRCE method are finally mapped into the canoni-
cal form of the original term. 
The preliminary experiments show that the 
precision is in line with the results for English, 
and that for the top ranked terms the precision is 
well above 90%. The analysis of errors shows 
that the majority of them appear due to lexical 
ambiguity of the input text. Certainly, if the cor-
pora were lexically disambiguated, we would 
have better precision.  
In order to improve the recall, additional mor-
pho-syntactic filters need to be identified. In par-
ticular, we plan to study terms that contain 
prepositions, as this is a common formation pat-
tern in many domains. Further, the broader han-
dling of term variants (e.g. dialectic variants, 
acronyms, derivational variants) may also im-
prove both precision and recall. Currently we 
deal only with inflectional variants by mapping 
them to a canonical form. Term variants unifica-
tion and normalisation also provide a broader 
basis for further IR and IE tasks, as queries can 
be expanded by referring to a class of synony-
mous terms as opposed to a single term.  
 
References  
Ananiadou S. 1994.  Methodology for Automatic Term 
Recognition. In Proceedings of COLING-94, 
Kyoto, Japan 
Frantzi K.T., Ananiadou S. and Mima H. 2000. Auto-
matic Recognition of Multi-word Terms: the C-
value/NC-value Method. Int. J. on Digital Libraries, 
3/2, pp. 115-130. 
Grobelnik M., Mladeni? D. and Mili?-Frayling N. 
2000. Text Mining as Integration of Several Re-
lated Research Areas, KDD 2000 Workshop on 
Text Mining, Boston, USA 
Hatzivassiloglou V., Duboue P. and Rzetsky A. 2001. 
Disambiguating Proteins, Genes, and RNA in Text: 
A Machine Learning Approach. Bioinformatics, 
17/1, pp. S97-S106 
Hearst M. 2000. Text Mining Tools: Instruments for 
Scientific Discovery, in IMA Text Mining Work-
shop, Institute for Mathematics and its Applica-
tions, Minneapolis, USA, 2000 
Jacquemin C. 2001. Spotting and discovering terms 
through NLP. MIT Press, Cambridge MA, 378 p. 
Krauthammer M., Rzhetsky A., Morozov P. and 
Friedman C. 2000. Using BLAST for identifying 
gene and protein names in journal articles. Gene, 
259, pp. 245-252. 
Mima H., Ando K. and Aoe J. 1995: Incremental 
Generation of LR(1) Parse Tables. In Proceedings 
of NLPRS?95, Pacific-Rim Symp., Seoul, Korea 
Nakagawa H. and Mori T. 2000. Nested Collocation 
and Compound Noun for Term Recognition. Proc. 
of COMPUTERM 98, pp. 64?70 
Nenadi? G. and Vitas D. 1998a. Formal Model of 
Noun Phrases in Serbo-Croatian. BULAG 23, 
Universite Franche-Compte, Besan?on, France. 
Nenadi? G. and Vitas D. 1998b. Using Local Gram-
mars for Agreement Modelling in Highly Inflective 
Languages. In Proceedings of TSD 98. Masaryk 
University, Brno, pp. 91-96. 
Nenadi? G., Mima H., Spasi? I., Ananiadou S. and 
Tsujii J. 2002. Terminology-driven Literature Min-
ing and Knowledge Acquisition in Biomedicine. In-
ternational Journal of Medical Informatics, 1-16. 
Spasi? I. 1996. Automatic Foreign Words Recognition 
in a Serbian Scientific or Technical Text. In Pro-
ceedings of Standardisation of Terminology, Bel-
grade, Yugoslavia, 1996 
Vintar ?. 2000. Extracting Terms and Terminological 
Collocations from the ELAN Slovene-English Par-
allel Corpus. In Proceedings of the 5th EAMT 
Workshop, Ljubljana, Slovenia, 2000 
Vitas D. 1993. Mathematical Model of Serbo-
Croatian Morphology (Nominal Inflection). PhD 
thesis. Faculty of Mathematics, Belgrade. 
 
Proceedings of the Workshop on BioNLP: Shared Task, pages 115?118,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Detection using Rules,  
Conditional Random Fields and Parse Tree Distances 
 
Farzaneh Sarafraz*, James Eales*, Reza Mohammadi?, Jonathan Dickerson?,  
David Robertson?, Goran Nenadic* 
*School of Computer Science, University of Manchester 
?Faulty of Life Sciences, University of Manchester 
?Dept. of Mathematics and Computer Science, Sharif University of Technology 
sarafraf@cs.man.ac.uk, g.nenadic@manchester.ac.uk 
 
Abstract 
This paper reports on a system developed for 
the BioNLP'09 shared task on detection and 
characterisation of biomedical events. Event 
triggers and types were recognised using a 
conditional random field classifier and a set of 
rules, while event participants were identified 
using a rule-based system that relied on rela-
tive distances between candidate entities and 
the trigger in the associated parse tree. The re-
sults on previously unseen test data were en-
couraging: for non-regulatory events, the F-
score was almost 50% (with precision above 
60%), with the overall F-score of around 30% 
(49% precision). The performance on more 
complex regulatory events was poor  
(F-measure of 7%). Among the 24 teams 
submitting the test results, our results were 
ranked 12th for the overall F-score and 8th for 
the F-score of non-regulation events. 
1 Introduction 
The aim of the BioNLP'09 shared task 1 was to 
characterise molecular events being reported in a 
Medline abstract by identifying the textual trigger, 
event type and participating entities (Kim et al 
2009). Nine event types were considered: gene 
expression, transcription, protein catabolism, lo-
calisation, phosporylation, binding, regulation, 
positive regulation, and negative regulation. De-
pending on the event type, the task included the 
identification of either one (for the first five event 
types mentioned above) or more (e.g. for binding) 
participating proteins. Information requested for 
regulatory events was more complex: in addition to 
one theme (a protein or another event), these 
events could also have a cause (a protein or an-
other event) that needed to be identified. 
 
The organisers have distributed a training 
dataset of 800 abstracts, with gene and gene prod-
uct mentions pre-annotated in text. In addition, a 
development set (150 abstracts) was provided to 
assess the quality of the extractions during the 
training and development phases. 
2 Methods 
The system developed for the challenge consists of 
three main modules: (1) event trigger and type de-
tection, (2) event participant detection, and  
(3) post-processing of the results. 
2.1 Event Trigger and Type Detection 
Our view of the event trigger and type detection 
subtask was that each token in a sentence needed 
to be tagged either as a trigger for one of the nine 
event types, or as a non-trigger/event token. We 
therefore decided to identify event types and trig-
gers in a single step by training a conditional ran-
dom field (CRF) classifier that assigned one of ten 
(nine types plus non-trigger) tags to each token. 
CRFs have been shown to be particularly suitable 
for tagging sequential data such as natural lan-
guage text, because they take into account features 
and tags of neighbouring tokens when evaluating 
the probability of a tag for a given token.  
Tokens and their part-of-speech (POS) tags 
were recognised using the Genia Tagger (Tsuruoka 
et al 2005). Each stemmed token was represented 
using a feature vector consisting of the following 
features:  
? A binary feature indicating whether the to-
ken is a protein; 
? A binary feature indicating whether the to-
ken is a known protein-protein interaction 
word (we used a pre-complied dictionary of 
115
such words collected from previous studies 
(Fu et al 2008; Yang et al 2008); 
? The token's POS tag; 
? The log-frequencies of the token being a 
trigger for each event type in the training 
data (nine features); 
? The number of proteins in the given sen-
tence. 
Other features (e.g. separating the known inter-
action words according to the nine event types) 
were explored during the development phase, but 
were not included in the final feature list since they 
increased the sparseness of the data and did not 
improve the overall results. The CRF parameters 
were adjusted for maximum performance, includ-
ing the choice of training algorithms, the number 
of training steps, the size of the window within 
which the tokens can affect any certain token, and 
the number of training abstracts used in each train-
ing step. It was interesting to notice that there were 
no significant improvements in the performance 
after training on 100, 400 or 800 abstracts from the 
training set (data not shown).  
2.2 Locating Event Themes 
After detecting potential triggers and associated 
event types, the next task was to locate possible 
participants (i.e. ?themes? and ?causes?) for each 
event. It was obvious that participants did not have 
to be the nearest to the trigger on the surface level, 
so our approach was based on distances within the 
parse trees associated with the sentences contain-
ing candidate events. Parse tree distances have 
been studied previously in clustering and automatic 
translation tasks (Emms 2008), so we hypothesised 
that we could use them to identify the most likely 
participants. The training data was analysed for the 
proximities between the triggers and the (correct) 
event participants in the parse tree of the sentence.1 
Figure 1 gives a detailed density function of these 
distances (ignoring non-protein nodes). The analy-
sis showed that a theme was usually amongst the 
nearest proteins to the trigger in terms of parse tree 
distances: for example, in 60% of all single theme 
events (e.g. localisation, phosphorylation) the cor-
rect protein participant was the trigger?s nearest or 
second nearest protein in the parse tree. A further 
                                                           
1
 The parse trees were produced by the GDep parser (Sagae 
and Tsujii, 2007) and supplied by the challenge organisers. 
analysis demonstrated that it was more likely for a 
theme to appear in the sub-tree of the correspond-
ing trigger, with 70% of all single theme events 
having a theme which appeared in the sub-tree of 
the trigger. Furthermore, specific analyses of the 
parse trees associated to the binding events (which 
can have more than one theme) suggested a linear 
relationship between the parse tree distance and 
binding event participant number (participant1 is 
the nearest, participant2 is the second nearest, etc.). 
 
 
Figure 1: Probability density function of the distance 
between the trigger and the theme in the parse tree  
(ignoring the tokens that are not proteins) 
 
We used this distributional analysis (derived 
from the training data) to design a rule-based 
method for the identification of participating 
themes. The rules were manually derived for each 
of the nine event classes, by defining:  
? a threshold for the maximum distance  to the 
trigger in the sub-tree for the given event 
type; 
? a threshold for the difference between the 
maximum distance in the whole tree and the 
given sub-tree for the given event type; 
? the number of nearest proteins to be re-
ported for each trigger. 
 
All entities that satisfied a distance-based rule 
for a given trigger were selected as the correspond-
ing theme(s). For example, if the event type is 
binding, then up to the second closest protein in the 
sub-tree, and the first closest protein in the rest of 
the tree are reported as themes.  
116
Figure 2 provides an example of the method ap-
plied to a sentence with multiple events. Regulates 
and secretion are correctly identified as triggers for 
a regulation and a localization event in the first 
phase. Using the rules for localization, the themes 
for two localization events are correctly recognised 
as proteins T2 and T3, whereas T1 was ignored 
since it did not appear in the trigger's sub-tree.  
Engineering and applying rules for non-
regulatory events was relatively straightforward. 
However, regulatory events can have different 
kinds of participants (a protein or an event). In the 
case of an event, we were trying to locate the near-
est trigger for the event (being regulated) in the 
parse tree. For example, in Figure 2, the nearest 
option to the regulation trigger (secretion) was the 
trigger of the two localization events, and both 
events should be (correctly) reported as the themes 
of two regulation events. Therefore, we require a 
number of recursions in the application of the rules 
to represent higher-order regulatory dependences. 
For the purposes of this challenge, only regulations 
up to the second ?order? were detected, allowing 
other events to act as themes and causes as well as 
proteins. Attempts to find more complicated regu-
latory events using this method resulted in a de-
creased precision and/or F-score.  
 
2.3 Post-processing Event Profiles 
The performance of the first two phases was 
studied on the development dataset: we noted a 
number of false-positive and false-negative results 
that were mostly due to a set of recurring triggers. 
We therefore decided to perform a post-processing 
step to improve the identification of event triggers 
and associated types. In the first step (improving 
the event trigger and type detection), the output of 
the CRF was overridden in cases where the triggers 
appeared in a list of negatively discriminated trig-
ger words which was collected after the manual 
analysis of the false positive results on the training 
and development data. Similarly, in cases where 
the CRF missed a highly indicative trigger (from a 
manually collected set) for a given event type, the 
trigger was added as part of post-processing. In the 
latter case, the sentence was then processed for the 
event theme detection (as described in 2.2).  
In the second step of the pre-processing phase, 
we forced highly indicative regulation triggers (if 
not previously identified) to be associated with an 
 
Figure 2: The parse tree of sentence Monocyte tethering 
by P-selectin regulates monocyte chemotactic protein-1 
and tumor necrosis factor-alpha secretion. The triggers 
are shown in boxes, and the entities are numbered.  
 
event by assigning proteins appearing in the sen-
tence to them, even when no protein in the sen-
tence satisfied the theme or cause criteria described 
in Section 2.2. This was aimed at improving the 
extremely low recall for regulatory events. 
Finally, since triggers could consist of more than 
one consecutive token, a set of simple rules were 
applied to remove typical false-negative constitu-
ents identified by the CRF as part of triggers (e.g. 
sometimes linking words appeared within triggers). 
3 Results and discussion 
The task 1 assessment was based on the output of 
the system when applied to the test dataset of 260 
previously unseen abstracts. An event was counted 
as a true positive if its type, trigger and all partici-
pants had been correctly identified. The overall F-
score for our system was 30.35% with 48.61% pre-
cision (approximate span matching, see Table 1). 
The best performing event types were phosphory-
lation (the best F-score and the best recall) and 
gene expression (the best precision with a reasona-
bly good F-measure). While the results for non-
regulatory events were encouraging, they were low 
for regulatory events. Among the 24 teams submit-
ting the test results, our results were ranked 12th for 
the overall F-score and 8th for the F-score of non-
regulation events. 
A preliminary analysis of the results was per-
formed on the development data (as the test data is 
not available), which had around 5% higher overall 
F-score than the test data (9% for non-regulation 
events, see Table 2 for details). 
117
Event Class #Gold R P F-score 
Localisation 174 44.83 53.06 48.60 
Binding 347 12.68 40.37 19.30 
Gene expression 722 52.63 69.34 59.84 
Transcription 137 15.33 67.74 25.00 
Protein catabolism 14 42.86 50.00 46.15 
Phosphorylation 135 78.52 53.81 63.86 
Non-reg total 1529 41.53 60.82 49.36 
Regulation 291 3.09 19.15  5.33 
Positive regulation 983 1.12 8.87 1.99 
Neg. regulation 379 12.4 20.52 15.46 
Regulatory total 1653 4.05 16.75 6.53 
All total 3182 22.06 48.61 30.35 
 
Table 1: Evaluation of the test data (260 abstracts), 
(approximate span matching; #Gold = the number of  
examples in the gold standard) 
 
 
In order to assess the effects of different steps 
in our approach, we evaluated the performance of 
the event trigger and event participant detection 
steps separately. The results presented in Table 3 
indicated that the performance of the CRF module 
was not much better than the overall performance 
of the system (an F-score of 43% vs. 35%), sug-
gesting that the CRF part was mostly responsible 
for the errors, by both missing triggers and falsely 
reporting them. This was particularly the case with 
non-regulatory events (even for binding). Con-
versely, when considering only those events whose 
triggers were correctly identified, their participants 
were also correctly recognised in most cases. 
Overall, the analysis suggested that the parse tree 
distance method performed reasonable well, de-
spite a reduction in recall of approximately 12%.  
There are a number of possibilities for im-
provements. We believe applying the CRF model 
in two stages would be a better approach to detect 
 
Event Class #Gold R P F-score 
Localisation 40 77.50 47.69 59.05 
Binding 180 33.33 54.55 41.38 
Gene expression 282 76.60 58.54 66.36 
Transcription 68 58.82 18.60 28.27 
Protein catabolism 19 84.21 88.89 86.49 
Phosphorylation 40 97.50 81.25 88.64 
Non-reg total 629 63.91 48.73 55.30 
Regulation 138 13.04 62.07 21.56 
Positive regulation 462 13.85 54.24 22.07 
Neg. regulation 153 29.41 45.92 35.86 
All total 1382 38.28 49.44 43.15 
 
Table 3: Trigger-only evaluation of the development data  
Event Class #Gold R P F-score 
Localisation 53 67.92 46.75 55.38 
Binding 312 21.47 63.81 32.13 
Gene expression 356 64.61 76.33 69.98 
Transcription 82 53.66 89.80 67.18 
Protein catabolism 21 90.48 67.86 77.55 
Phosphorylation 47 91.49 53.09 67.19 
Non-reg total 871 50.4 68.44 58.05 
Regulation 172 5.23 33.33 9.05 
Positive regulation 632 3.48 21.36 5.99 
Neg. regulation 201 9.45 15.08 11.62 
Regulatory total 1005 4.98 19.53 7.93 
All total 1876 26.07 54.46 35.26 
 
Table 2: Evaluation of the development data (150 abstracts) 
(approximate span matching; #Gold as in Table 1) 
 
events: first identify triggers and then link them to 
event classes. In addition, the rules employed for 
determining themes need to be more specific to 
reflect both event type and grammatical structure.  
In the case of regulatory events, however, signifi-
cantly better results were noticed in the trigger de-
tection part when compared to the overall scores, 
indicating that it was difficult to identify regulatory 
participants, as any of those participants could be 
either a protein or another event.  
Overall, the results achieved by our system 
suggest that combining parse tree results, rules and 
CRFs is a promising approach for the identification 
of non-regulatory events in the literature, while 
more work would be needed for regulatory events. 
References  
Emms M. 2008. Tree-distance and some other Variants 
of evalb. Proc. of LREC 2008, pp 1373-1379. 
Fu W. et al 2008. Human Immunodeficiency Virus type 
1, Human protein interaction database at NCBI, Nu-
cleic Acid Research 2008, D417-D422 
Kim JD et al 2009. Overview of BioNLP'09 Shared 
Task on Event Extraction, Proc. of BioNLP NAACL 
2009 Workshop (to appear) 
Sagae K, Tsujii J. 2007. Dependency Parsing and Do-
main Adaptation with LR Models and Parser Ensem-
bles. Proc. of the CoNLL 2007 Shared Task, 1044-50 
Tsuruoka Y. et al 2005. Developing a Robust Part of-
Speech Tagger for Biomedical Text. Advances in In-
formatics, 382?392. 
Yang H. et al 2008. Identification of Transcription Fac-
tor Contexts in Literature using Machine Learning 
Approaches. BMC Bioinformatics, Vol. 9(3):S11. 
118
A Methodology for Terminology-based 
Knowledge Acquisition and Integration 
 
Hideki Mima1
?
, Sophia Ananiadou2, Goran Nenadic2 and Junichi Tsujii1 
 
1Dept. of Information Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan 
{mima, tsujii}@is.s.u-tokyo.ac.jp 
2Computer Science, University of Salford 
Newton Building, Salford M5 4WT, UK 
{S.Ananiadou, G.Nenadic}@salford.ac.uk 
 
                                                
? Current affiliation: Dept. of Engineering, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113- 8656, Japan 
Abstract  
In this paper we propose an integrated knowledge 
management system in which terminology-based 
knowledge acquisition, knowledge integration, 
and XML-based knowledge retrieval are 
combined using tag information and ontology 
management tools. The main objective of the 
system is to facilitate knowledge acquisition 
through query answering against XML-based 
documents in the domain of molecular biology. 
Our system integrates automatic term recognition, 
term variation management, context-based 
automatic term clustering, ontology-based 
inference, and intelligent tag information retrieval. 
Tag-based retrieval is implemented through 
interval operations, which prove to be a powerful 
means for textual mining and knowledge 
acquisition. The aim is to provide efficient access 
to heterogeneous biological textual data and 
databases, enabling users to integrate a wide 
range of textual and non-textual resources 
effortlessly. 
Introduction 
With the recent increasing importance of 
electronic communication and data sharing over 
the Internet, there exist an increasingly growing 
number of publicly accessible knowledge sources, 
both in the form of documents and factual 
databases. These knowledge sources (KSs) are 
intrinsically heterogeneous and dynamic. They 
are heterogeneous since they are autonomously 
developed and maintained by independent 
organizations for different purposes. They are 
dynamic since constantly new information is 
being revised, added and removed. Such an 
heterogeneous and dynamic nature of KSs 
imposes challenges on systems that help users to 
locate and integrate knowledge relevant to their 
needs. 
   Knowledge, encoded in textual documents, is 
organised around sets of specialised (technical) 
terms (e.g. names of proteins, genes, acids). 
Therefore, knowledge acquisition relies heavily 
on the recognition of terms. However, the main 
problems that make term recognition difficult are 
the lack of clear naming conventions and 
terminology variation (cf. Jacquemin and 
Tzoukermann (1999)), especially in the domain 
of molecular biology. Therefore, we need a 
scheme to integrate terminology management as 
a key prerequisite for knowledge acquisition and 
integration. 
   However, automatic term extraction is not the 
ultimate goal itself, since the large number of 
new terms calls for a systematic way to access 
and retrieve the knowledge represented through 
them. Therefore, the extracted terms need to be 
placed in an appropriate framework by 
discovering relations between them, and by 
establishing the links between the terms and 
different factual databases. 
   In order to solve the problem, several 
approaches have been proposed. MeSH Term in 
MEDLINE (2002) and Gene Ontology (2002) 
provide a top-down controlled ontology 
framework, which aims to describe and constrain 
the terminology in the domain of molecular 
biology. On the other hand, automatic term 
acquisition approaches have been developed in 
order to address a dynamic and corpus-driven 
knowledge acquisition methodology (Mima et al, 
1999; 2001a).  
   Different approaches to linking relevant 
resources have also been suggested. The 
Semantic Web framework (Berners-Lee (1998)) 
aims to link relevant Web resources in bottom-up 
manner using the Resource Description 
Framework (RDF) (Bricklet and Guha, 2000) and 
an ontology. However, although the Semantic 
Web framework is powerful to express content of 
resources to be semantically retrieved, some 
manual description is expected using the 
RDF/ontology. Since no solution to the 
well-known difficulties in manual ontology 
development, such as the ontology 
conflictions/mismatches (Visser et al, 1997) is 
provided, an automated ontology management is 
required for the efficient and consistent 
knowledge acquisition and integration. TAMBIS 
(Baker et al, 1998) tried to provide a filter from 
biological information services by building a 
homogenising layer on top of the different 
sources using the classical mediator/wrapper 
architecture. It intended to provide source 
transparency using a mapping from terms placed 
in a conceptual knowledge base of molecular 
biology onto terms in external sources.  
   In this paper we introduce TIMS, an integrated 
knowledge management system in the domain of 
molecular biology, where terminology-based 
knowledge acquisition (KA), knowledge 
integration (KI), and XML-based knowledge 
retrieval are combined using tag information and 
ontology management tools. The management of 
knowledge resources, similarly to the Semantic 
Web, is based on XML, RDF, and 
ontology-based inference. However, our aim is to 
facilitate the KA and KI tasks not only by using 
manually defined resource descriptions, but also 
by exploit ing NLP techniques such as automatic 
term recognition (ATR) and automatic term 
clustering (ATC), which are used for automatic 
and systematic ontology population.  
    
   The paper is organised as follows: in section 1 
we present the overall TIMS architecture and 
briefly describe the components incorporated in 
the system, while section 2 gives the details of the 
proposed method for KA and KI. In the last 
section we present results, evaluation and 
discussion. 
1 TIMS ? system architecture 
XML-based Tag Information Management 
System (TIMS) is a core machinery for managing 
XML tag information obtained from sub 
functional components. Its main aim is to 
facilitate an efficient mechanism for KA and KI 
through a query answering system for 
XML-based documents in the domain of 
molecular biology, by using a tag information 
database.  
   Figure 1 shows the system architecture of 
TIMS. It integrates the following modules via  
XML-based data exchange: JTAG ? an 
annotation tool, ATRACT ? an automatic term 
recognition and clustering workbench, and the 
LiLFeS abstract machine, which we briefly 
describe in this section. ATRACT and LiLFeS 
play a central role in the knowledge acquisition 
process, which includes term recognition, 
ontology population, and ontology-based 
inference. In addition to these modules, TIMS 
implements an XML-data manager and a TIQL 
query processor (see Section 2).  
1.1 JTAG 
JTAG is an XML-based manual annotation and 
resource description aid tool. Its purpose is to 
support manual annotation (e.g. semantic 
tagging), adjusting term recognition results, 
developing RDF logic, etc. In addition, ontology 
information described in XML can also be 
developed and modified using the tool. All the 
annotations can be managed via a GUI.  
1.2 ATRACT 
In the domain of molecular biology, there is an 
increasing amount of new terms that represent 
newly created concepts. Since existing term 
Figure 1: System architecture of TIMS 
 
XML Data 
Retrieval 
TIMS
Tag Information Database  
XML Data 
Management 
ATRACT 
Automatic Term 
Recognition 
and Term 
Clustering 
XML data
XML data
XML data
Document/
Database
Retriever
L iLFeS 
Syntactic and 
Semantic Parser / 
RDF and Ontology 
Manager 
JTAG 
Manual Resource 
Description 
Aid Interface XML data
dictionaries cannot cover the needs of specialists, 
automatic term extraction tools are important for 
consistent term discovery. ATRACT (Mima et al, 
2001a) is a terminology management workbench 
that integrates ATR and ATC. Its main aim is to 
help biologists to gather and manage terminology 
in the domain. The module retrieves and 
classifies terms on the fly and sends the results as 
XML tag information to TIMS.  
   The ATR method is based on the C/NC-value 
method (Frantzi et al, 2000). The original 
method has been augmented with acronym 
acquisition and term variation management 
(Nenadic et al 2002), in order to link different 
terms that denote the same concept. Term 
variation management is based on term 
normalisation as an integral part of the ATR 
process. All orthographic, morphological and 
syntactic term variations and acronym variants (if 
any) are conflated prior to the statistical analysis, 
so that term candidates comprise all variants that 
appear in a corpus. 
   Besides term recognition, term clustering is an 
indispensable component in a knowledge 
management process (see figure 2). Since 
terminological opacity and polysemy are very 
common in molecular biology, term clustering is 
essential for the semantic integration of terms, 
the construction of domain ontology and for 
choosing the appropriate semantic information.  
   The ATC method is based on Ushioda?s AMI 
(Average Mutual Information)-hierarchical 
clustering method (Ushioda, 1996). Our 
implementation uses parallel symmetric 
processing for high speed clustering and is built 
on the C/NC-value results. As input, we use 
co-occurrences of automatically recognised 
terms and their contexts, and the output is a 
dendrogram of hierarchical term clusters (like a 
thesaurus). The calculated term cluster 
information is stored in LiLFeS (see below) and 
combined with a predefined ontology according 
to the term classes automatically assigned. 
1.3 LiLFeS 
LiLFeS (Miyao et al, 2000) is a Prolog-like 
programming language and language processor 
used for defining definite clause programs with 
typed feature structures. Since typed feature 
structures can be used like first order terms in 
Prolog, the LiLFeS language can describe 
various kinds of applications based on feature 
structures. Examples include HPSG parsers, 
HPSG-based grammars and compilers from 
HPSG to CFG. Furthermore, other NLP modules 
can be easily developed because feature structure 
processing can be directly written in the LiLFeS 
language. Within TIMS, LiLFeS is used to: 1) 
infer similarity between terms using hierarchical 
matching, and 2) parse sentences using 
HPSG-based parsers and convert the results into 
an XML-based formalism. 
 
2 Knowledge Integration and Management  
 
Knowledge integration and management in 
TIMS is organised by integrating XML-data 
management (section 2.1) and tag- and 
ontology-based information extraction (section 
2.2). Figure 3 illustrates a model of the 
knowledge management based on the knowledge 
integration and question-answering process 
within TIMS. In this scenario, a user formulates a 
query, which is processed by a query manager. 
The tag data manager retrieves the relevant data 
from the collection of documents via a tag 
database and ontology-based inference (such as 
POS Tagger 
Acronym Recognition 
C-value ATR 
Orthographic Variants 
Morphological Variants 
Syntactic Variants 
NC-value Analyzer 
Term Clustering  
(Semantic Analyzer) 
XML Documents Including 
Term Tags and Term 
Variation/Class Information 
Input Documents 
Figure 2. Term Ontology Development 
Recognition of Term 
Variations (synonyms) 
Recognition of Term 
Classes (Similar Terms) 
hierarchical matching of term classes).  
2.1 XML-tag data management 
Communication within TIMS is based on 
XML-data exchange.  TIMS initially parses the 
XML documents (which contain relevant 
terminology information generated automatically 
by ATRACT) and ?de-tags? them. Then, like in 
the TIPSTER architecture (Grishman, 1995), 
every tag information is stored separately from 
the original documents and managed by an 
external database software. This facility allows, 
as shown in figure 4, different types of tags (POS, 
syntactic, semantic, etc.) for the same document 
to be supported. 
2.2 Tag- and ontology-based IE 
The key feature of KA and KI within TIMS is a 
facility to logically retrieve data that is 
represented by different tags. This feature is 
implemented via interval operations. The main 
assumption is that the XML tags specify certain 
intervals within documents. Interval operations 
are XML specific text/data retrieval operations, 
which operate on such textual intervals. Each 
interval operation takes two sets of intervals as 
input and returns a set of intervals according to 
the specified logical operations. Currently, we 
define four types of logical operations: 
? Intersection ??? returns intersected intervals 
of all the intervals given. 
? Union ??? returns merged intervals of all the 
intersected intervals. 
? Subtraction ?y? returns differences in 
intervals of all the intersected intervals. 
? Concatenation ?+? returns concatenated 
intervals of all the continuous intervals. 
 
For example, the interval operation 1 
<VP>?(<V>?<term>) describes all verb 
(<V>)-term (<term>) pairs within a verb phrase 
(<VP>). Similarly, suppose X denotes a set of 
intervals of manually annotated tags for a 
document and Y denotes a set of intervals of 
automatically annotated tags for the same 
document. The interval operation ((X?Y) 
?{X?Y}) results in the differences between 
human and machine annotations (see figure 5). 
Interval operations are powerful means for 
textual mining from different sources using tag 
information.  In addition, LiLFeS enables tag 
(interval) retrieval to process not only regular 
                                                
1 ??? denotes a merged set of all the elements. 
 
Figure 3: Question-answering process in TIMS 
Database 
A
  
A
  
A
  
A
  
A
  
A
  
XML / HTML 
Knowledge 
Sources 
Tag Data 
Language 
Analyzer 
 
 
Tag Data 
Manager 
TIQL 
Processor
NLP Components 
TIMS  
Query to TIQL
Translator 
Query 
ATRACT 
Ontology 
Data 
A
  
A
  
A
  
A
  
A
  
A
  
 
LiLFeS 
Knowledge Acquisition 
Knowledge Integration 
 ? 
 
26 15 VERB 
? 
 
110 100 ADJ 
? 150 140 DNA 
? ? ? ?.. 
? 209 203 RNA 
? 10 5 NOUN
. . . end start Tag 
? 
 
35 15 VP 
? 
 
100 35 PP 
? 
 
150 100 VP 
? ? ? ?.. 
? 209 203 NP 
? 
 
10 5 NP 
. . . end start Tag 
Figure 4: Tag data management 
Part-of-speech tags 
? 
 
80 40 PROTEIN 
? 
 
180 160 DNA 
? 
 
220 200 DNA 
? ? ? ?.. 
? 
 
260 240 RNA 
? 
 
18 5 DNA 
. . . end start Tag 
Semantic tags 
Syntactic tags 
 
X = {                                                           }
Y = {                                                            }
 
X?Y  = {                                                              } 
?{X?Y}={                                                        
} 
 
(X?Y) ?{X?Y}={                                             
                                                                                               }
Figure 5. (X?Y) ?{X ?Y} 
pattern/string matching using tag information, 
but also the ontological hierarchy matching to 
subordinate classes using either predefined or 
automatically derived term ontology. Thus, 
semantically-based tag information retrieval can 
be achieved. For example, the interval operation2 
<VP>?<nucleic_acid*> will retrieve all 
subordinate terms/classes of nucleic acid, which 
are contained within a VP. 
   The interval operations can be performed over 
the specified documents and/or tag sets (e.g. 
syntactic, semantic tags, etc.) simultaneously or 
in batch mode, by selecting the documents/tag 
sets from a list. This accelerates the process of 
KA, as users are able to retrieve information from 
multiple KSs simultaneously. 
2.3 TIQL - Tag Information Query Language  
In order to integrate and expand the above 
components, we have developed a tag 
information query language (TIQL). Using this 
language, a user can specify the interval 
operations to be performed on selected 
documents (including the ontology inference to 
expand queries). The basic expression in TIQL 
has the following form: 
 
SELECT [n-tuple variables]  
FROM [XML document(s)] 
WHERE [interval operation] 
      FROM [XML document(s)] 
WHERE [interval operation] 
                     ?? 
where, [n-tuple variables] specifies the 
table output format, [XML document(s)] 
denotes the document(s) to be processed, and 
[interval operation] denotes an interval 
operation to be performed over the corresponding 
document with variables of each interval to be 
bound. 
For example, the following expression: 
 
SELECT   x1, x2  
 FROM   ?paper-1.xml? 
    WHERE  
<VP>?{x1:<EVENT*>?x2:<nucleic_acid*>} 
  FROM   ?paper-2.xml? 
 WHERE  
<VP>?{x1:<EVENT*>?x2:<nucleic_acid*>} 
 
                                                
2 ?*? denotes hierarchical matching. 
extracts all the hierarchically subordinate classes 
matched to (<EVENT>, <nucleic_acid>) pair 
within a VP from the specified XML-documents,  
and then automatically builds a table to display 
the results (see figure 6).  
   Since formulating an appropriate TIQL 
expression using interval operations might be 
cumbersome, in particular for novice users, 
TIMS was augmented with a capability of 
?recycling? predefined queries and macros. 
3 Evaluation and discussion 
We have conducted preliminary experiments 
using the proposed framework. In this paper we 
briefly present the quality of automatic term 
recognition and similarity measure calculation 
via automatically clustered terms. After that, we 
discuss the practical performance of tag 
manipulation in TIMS compared to string-based 
XML tag manipulation to show the advantage of 
the tag information management scheme.  
   The term recognition evaluation was performed 
on the NACSIS AI-domain corpus (Koyama et 
al., 1998), which includes 1800 abstracts and on a 
set of MEDLINE abstracts. Table 1 shows a 
sample of extracted terms and term variants. The 
ATR precisions of the top 100 intervals range 
from 93% to 98% (see figure 7; for detailed 
evaluation, see Mima et al (2001b) and Nenadic 
et al (2002)).  
 
 Title 
Background 
                                      
                  
........<DNA>androgen 
receptor gene</DNA>   
............... 
                       
         
                      
paper-2.xml 
Title 
Background 
                                      
                  
........<RNA>HB-EGF 
mRNA</RNA>   
............... 
                       
         
                            
paper-1.xml 
nucleic_acid 
 nucleic acid EVENT 
EVENT 
androgen receptor 
gene acid HB-EGF mRNA 
... 
activate 
bind 
... 
Figure 6. Ontology-based Tagged 
Information Retrieval 
 
   terms (and term variants) term-hood 
retinoic acid receptor                                              
     retinoic acid receptor 
     retinoic acid receptors 
     RAR, RARs 
6.33 
nuclear receptor  
     nuclear receptor 
     nuclear receptors 
     NR, NRs 
6.00 
all-trans retionic acid 
     all trans retionic acid 
     all-trans-retinoic acids 
     ATRA, at-RA, atRA 
4.75 
9-cis-retinoic acid 
     9-cis retinoic acid 
     9cRA, 9-c-RA 
4.25 
 
Table 1: Sample of recognised terms  
85
90
95
100
2.65-3.99 4.00-5.99 6.00-Top
C-value
pr
ec
is
io
n
 
Figure 7: ATR interval precision 
 
   For term clustering and tag manipulation 
performance we used the GENIA resources 
(GENIA corpus, 2002), which include 1,000 
MEDLINE abstracts (MEDLINE, 2002), with 
overall 40,000 (16,000 distinct) semantic tags 
annotated for terms in the domain of nuclear 
receptors. We used the similarity measure 
calculation as the central computing mechanism 
for inferring the relevance between the XML tags 
and tags specified in the TIQL/interval operation,  
determining the most relevant tags in the 
XML-based KS(s). As a gold standard, we used 
similarities between the terms that were 
calculated according to the hierarchy of the 
clustered terms according to the GENIA 
ontology. In this experiment, we have adopted a 
semantic similarity calculation method for 
measuring the similarity between terms described 
in (Oi et al, 1997). The three major sets of 
classes (namely, nucleic_acid, amino_acid, 
SOURCE) of manually classified terms from 
GENIA ontology (GENIA corpus, 2002) were 
used to calculate the average similarities (AS) of 
the elements. ASs of the elements within the 
same classes were greater than the ASs between 
elements from different classes, which proves 
that the terms were clustered reliably according 
to their semantic features. 
   In order to examine the tag manipulation 
performance of TIMS, we measured the 
processing times consumed for executing an 
interval operation in TIMS compared to the time 
needed by using string-based regular expression 
matching (REM). We focused on measuring the 
interval operation ??? with intervals (tags) 
<title> and <term> (i.e. extracting all terms 
within titles).   In the evaluation process, we used 
5 different samples to examine IE performances 
according to their size (namely the number of 
tags and file size in Kb).  
 
 Sample1 Sample2 Sample3 Sample4 Sample5 
TIMS 
(millisec.) 16 28 40 44 62 
REM 
(millisec.) 24 38 58 80 104 
# of tags 1146 2383 3730 4799 5876 
Size  
(K bytes) 92 191 298 382 470 
 
Table 2: TIMS - practical performance 
 
Table 2 and Figure 8 show the results: the 
processing times of TIMS were about 1.4-1.8 
times faster (depending on number of tags and 
corpus length) than those of REM. Therefore, we 
assume that the TIMS tag information 
management scheme can be considered as an 
efficient mechanism to facilitate knowledge 
acquisition and information extraction process. 
0
20
40
60
80
100
120
0 2000 4000 6000
# of tags
tim
e 
(m
ill
i s
ec
.)
TIMS
REM
Figure 8. IE performance (TIMS vs. REM) 
Conclusion 
In this paper, we presented a methodology for 
KA and KI over large KSs. We described TIMS, 
an XML-based integrated KA aid system, in 
which we have integrated automatic term 
recognition, term clustering, tagged data 
management and ontology-based knowledge  
retrieval. TIMS allows users to search and 
combine information from various sources. An 
important source of information in the system is 
derived from terminological knowledge, which is 
provided automatically in the XML format. 
Tag-based retrieval is implemented through 
interval operations, which ? in combination with 
hierarchical matching ? prove to be powerful 
means for textual mining and knowledge 
acquisition. 
   The system has been tested in the domain of 
molecular biology. The preliminary experiments 
show that the TIMS tag information management 
scheme is an efficient methodology to facilitate 
KA and IE in specialised fields. 
   Important areas of future research will involve 
expanding the scalability of the system to real 
WWW knowledge acquisition tasks and 
experiments with fine-grained term 
classification. 
References  
Baker P. G., Brass A., Bechhofer S., Goble C., Paton 
N. and Stevens R. (1998) TAMBIS: Transparent 
Access to Multiple Bioinformatics Information 
Sources. An Overview in Proc. of the Sixth 
International Conference on Intelligent Systems for 
Molecular Biology, ISMB98, Montreal. 
Berners-Lee, T. (1998) The Semantic Web as a 
longuage of logic, available at: http://www.w3.org/ 
DesignIssues/Logic.html 
Brickle, D. and Guha R. (2000) Resource Description 
Framework (RDF) Schema Specification 1.0, W3C 
Candidate Recommendation, available at 
http://www.w3.org/TR/rdf-schema 
Frantzi K. T., Ananiadou S. and Mima H. (2000) 
Automatic Recognition of Multi-Word Terms: the 
C-value/NC-value method, in International Journal 
on Digital Libraries, Vol. 3, No. 2, 115?130. 
Gene Ontology Consortium (2002) GO ontology. 
available at  http:// www.geneontology.org/ 
GENIA corpus (2002) GENIA project home page. 
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/ 
Grishman R (1995) TIPSTER Phase II Architecture 
Design Document. New York University, available 
at http://www.tipster.org/arch.htm 
Jacquemin C. and Tzoukermann E. (1999) NLP for 
Term Variant Extraction: A Synergy of Morphology, 
Lexicon and Syntax. In T. Strzalkowski (editor), 
Natural Language Information Retrieval, Kluwer, 
Boston, pp. 25-74. 
Koyama T., Yoshioka M. and Kageura K. (1998) The 
Construction of a Lexically Motivated Corpus - The 
Problem with Defining Lexical Unit. In Proceedings 
of LREC 1998, Granada, Spain, pp. 1015?1019. 
MEDLINE (2002) National Library of Medicine, 
http://www.ncbi.nlm.nih.gov/PubMed/ 
Mima H., Ananiadou S. and Nenadic G. (2001a) 
ATRACT Workbench: An Automatic Term 
Recognition and Clustering of Te rms, in Text, 
Speech and Dialogue - TSD2001, Lecture Notes in 
AI 2166, Springer Verlag 
Mima H. and Ananiadou S. (2001b) An Application 
and Evaluation of the C/NC-value Approach for the 
Automatic term Recognition of Multi-Word units in 
Japanese, in International Journal on Terminology, 
Vol. 6(2), pp 175-194. 
Mima H., Ananiadou S. and Tsujii J. (1999) A 
Web-based integrated knowledge mining aid 
system using term-oriented natural language 
processing, in Proceedings of The 5th Natural 
Language Processing Pacific Rim Symposium, 
NLPRS'99, pp. 13?18. 
Miyao Y., Makino T., Torisawa K. and Tsujii J. 
(2000) The LiLFeS abstract machine and its 
evaluation with the LinGO grammar. Journal of 
Natural Language Engineering, Cambridge 
University Press, Vol. 6(1), pp.47-62. 
Nenadic G., Spasic I. and Ananiadou S. (2002) 
Automatic Acronym Acquisition and Term 
Variation Management within Domain Specific 
Texts, in Proc. of LREC 2002, Las Palmas, Spain, 
pp. 2155-2162. 
Oi K., Sumita E. and Iida H. (1997) Document 
Retrieval Method Using Semantic Similarity and 
Word Sense Disambiguation (in Japanese), in 
Journal of Natural Language Processing, Vol.4, 
No.3, pp.51-70. 
Visser P.R.S., Jones D.M., Bench-Capon T.J.M. and 
Shave M.J.R. (1997) An Analysis of Ontology 
Mismatches; Heterogeneity versus Interoperability. 
In AAAI 1997 Spring Symposium on Ontological 
Engineering, Stanford University, California, USA. 
Ushioda A. (1996) Hierarchical Clustering of Words. 
In Proc. of COLING ?96, Copenhagen 
Automatic Discovery of Term Similarities Using Pattern Mining
Goran NENADI?, Irena SPASI? and Sophia ANANIADOU
Computer Science, University of Salford
Salford, M5 4WT, UK
{G.Nenadic, I.Spasic, S.Ananiadou}@salford.ac.uk
Abstract
Term recognition and clustering are key topics in automatic knowledge acquisition and text mining. In
this paper we present a novel approach to the automatic discovery of term similarities, which serves as
a basis for both classification and clustering of domain-specific concepts represented by terms. The
method is based on automatic extraction of significant patterns in which terms tend to appear. The
approach is domain independent: it needs no manual description of domain-specific features and it is
based on knowledge-poor processing of specific term features. However, automatically collected
patterns are domain specific and identify significant contexts in which terms are used. Beside features
that represent contextual patterns, we use lexical and functional similarities between terms to define a
combined similarity measure. The approach has been tested and evaluated in the domain of molecular
biology, and preliminary results are presented.
Introduction
In a knowledge intensive discipline such as
molecular biology, the vast and constantly
increasing amount of information demands
innovative techniques to gather and systematically
structure knowledge, usually available only from
text/document resources. In order to discover new
knowledge, one has to identify main concepts,
which are linguistically represented by domain
specific terms (Maynard and Ananiadou (2000)).
There is an increased amount of new terms that
represent newly created concepts. Since existing
term dictionaries usually do not meet the needs of
specialists, automatic term extraction tools are
indispensable for efficient term discovery and
dynamic update of term dictionaries.
   However, automatic term recognition (ATR) is
not the ultimate aim: terms recognised should be
related to existing knowledge and/or to each other.
This entails the fact that terms should be classified
or clustered so that semantically similar terms are
grouped together. Classification and/or clustering
of terms are indispensable for improving
information extraction, knowledge acquisition, and
document categorisation. Classification can also be
used for efficient term management and populating
and updating existing ontologies in a consistent
manner. Both classification and clustering methods
are built on top of a specific similarity measure.
The notion of term similarity has been defined and
considered in different ways: terms can have
functional and/or structural similarities, though
they can be correlated by different relationships
(Grefenstette (1994), Maynard and Ananiadou
(2000)). In this paper we suggest a novel, domain-
independent method for the automatic discovery of
term similarities, which can serve as a basis for
both classification and clustering of terms. The
method is mainly based on the automatic discovery
of significant term features through pattern mining.
Automatically collected patterns are domain
dependent and they identify significant contexts in
which terms tend to appear. In addition, the
measure combines lexical and syntactical
similarities between terms.
   The paper is organised as follows. In Section 1
we overview term management approaches.
Section 2 introduces the term similarity measure
and Section 3 presents results and experiments.
1   Terminology Management
Since vast amount of knowledge still remains
unexplored, several systems have been proposed to
help scientists to acquire relevant knowledge from
scientific literature. For example, GENIES
(Friedman et al (2001)) uses a semantic grammar
and substantial syntactic knowledge in order to
extract comprehensive information about signal-
transduction pathways. Some of the systems are
terminology-based, since technical terms
semantically characterise documents and therefore
represent starting place for knowledge acquisition
tasks. For example, Mima et al (2002) introduce
TIMS, a terminology-based knowledge acquisition
system, which integrates automatic term
recognition, term variation management, context-
based automatic term clustering, ontology-based
inference, and intelligent tag information retrieval.
The system?s aim is to provide efficient access and
integration of heterogeneous biological textual data
and databases.
   There are numerous approaches to ATR. Some
methods (Bourigault (1992), Ananiadou (1994))
rely purely on linguistic information, namely
morpho-syntactic features of term candidates.
Recently, hybrid approaches combining linguistic
and statistical knowledge are becoming
increasingly used (Frantzi et al (2000), Nakagawa
et al (1998)).
   There is a range of clustering and classification
approaches that are based on statistical measures of
word co-occurrences (e.g. Ushioda (1996)), or
syntactic information derived from corpora (e.g.
Grefenstette  (1994)). However, few of them deal
with term clustering: Maynard and Ananiadou
(2000) present a method that uses manually
defined semantic frames for specific classes,
Hatzivassiloglou et al (2001) use machine learning
techniques to disambiguate names of proteins,
genes and RNAs, while Friedman et al (2001)
describe extraction of specific molecular pathways
from journal articles.
   In our previous work, an integrated knowledge
mining system in the domain of molecular biology,
ATRACT, has been developed (Mima et al
(2001)). ATRACT (Automatic Term Recognition
and Clustering for Terms) is a part of the ongoing
BioPath1 project, and its main aim is to facilitate an
efficient expert-computer interaction during term-
based knowledge acquisition. Term management is
based on integration of automatic term recognition
and automatic term clustering (ATC). ATR is
based on the C/NC-value method (Frantzi et al
                                                          
1 BioPath is a Eureka funded project, coordinated by
LION BioScience (http://www.lionbioscience.com) and
funded by the German Ministry of Research.
(2000)), a hybrid approach combining linguistic
knowledge (term formation patterns) and statistical
knowledge (term length, frequency of occurrence,
etc). The extension of the method handles
orthographic, morphological and syntactic term
variants and acronym recognition as an integral
part of the ATR process (Nenadi? et al (2002a)),
providing that all term occurrences of a term are
considered. The ATC method is based on the
Ushioda?s AMI (Average Mutual Information)
hierarchical clustering method (Ushioda (1996)).
Co-occurrence based term similarities are used as
input, and a dendrogram of terms is generated.2
2   Term Similarity Measures
In this section we introduce a novel hybrid method
to measure term similarity. Our method
incorporates three types of similarity measures,
namely contextual, lexical and syntactical
similarity. We use a linear combination of the three
similarities in order to estimate similarity between
terms. In the following subsections we describe
each of the three similarity measures.
2.1   Contextual Similarity
Determining the similarity of terms based on their
contexts is a standard approach based on the
hypothesis that similar terms tend to appear in
similar contexts. Contextual similarity, however,
may be determined in a number of ways depending
on the way in which the context is defined. For
example, some approaches consider only terms
that appear in a close proximity to each other
(Maynard and Ananiadou (2000)), while in other
approaches, grammatical roles such as object or
subject are taken into account (Grefenstette
(1994)).
   Our approach to contextual similarity is based on
automatic pattern mining. The aim is to
automatically identify and learn the most important
context patterns in which terms appear. Context
pattern (CP) is a generalised regular expression
that corresponds to either left or right context of a
term. 3 The following example shows a sample left
context pattern of the term high affinity:
                                                          
2 For the evaluation of the ATR and ATC methods
incorporated in ATRACT, see Mima et al (2001).
3 Left and right contexts are treated separately.
V:bind TERM:rxr_heterodimers PREP:with
   Let us now describe the process of constructing
CPs and determining their importance. First, we
collect concordances for all automatically
recognised terms. Context constituents, which we
consider important for discriminating terms (e.g.
noun and verb phrases, prepositions, and terms
themselves) are identified by a tagger and by
appropriate local grammars, which define syntactic
phrases (e.g. NPs, VPs). The grammatical and
lexical information attached to the context
constituents is used to construct CPs.  In the
simplest case, contexts are mapped into the
syntactic categories of their constituents. However,
the lemmatised form for each of the syntactic
categories can be used as well. For example, when
encountered in a context, the preposition with can
be either mapped to its POS tag, i.e. PREP, or
instead, the lemma can be added, in which case we
have an instantiated chunk: PREP:with.  Further,
some of the syntactic categories can be removed
from the context patterns, as not all syntactic
categories are equally significant in providing
useful contextual information (Maynard and
Ananiadou (2000)). Such CPs will be regarded as
normalised CPs. In our approach, one can define
which categories to instantiate and which to
remove. In the examples provided later in the
paper (Section 3) we decided to remove the
following categories: adjectives (that are not part
of a term), adverbs, determiners and so-called
linking words (e.g. however, moreover, etc.).
Also, we instantiated terms and either verbs or
prepositions, as these categories are significant for
discriminating terms.
   Once we have normalised CPs, we calculate the
values of a measure called CP-value in order to
estimate the importance of the CPs. CP-value is
defined similarly to the C/NC-value for terms
(Frantzi et al (2000)). It assesses a CP (p)
according to its total frequency (f(p)), its length
(|p|, as the number of constituents) and the
frequency of its occurrence within other CPs (|Tp|,
where Tp is a set of all CPs that contain p):
The CPs whose CP-value is above a chosen
threshold are deemed important. Note that these
patterns are domain-specific and that they are
automatically extracted from a domain specific
corpus. Tables 1 and 2 show samples of significant
left context patterns extracted from a MEDLINE
corpus (MEDLINE (2002)).
CPs CP-value
PREP   NP 272.65
PREP   NP   PREP 186.47
.   .   . .   .   .
PREP  NP   V:stimulate 9.32
V:indicate   NP 5.00
PREP   NP   PREP   V:involve NP 4.64
PREP   TERM:transcriptional_activity 4.47
V:require   NP   PREP 4.38
PREP TERM:nuclear_receptor PREP 4.00
Table 1: Sample of left CPs
(only terms and most frequent verbs are instantiated)
CPs CP-value
PREP:of   NP 121.49
V  NP 71.42
PREP:of   NP   V 62.83
NP   PREP:of   NP 59.72
PREP:in   NP 59.55
NP   PREP:of 43.37
PREP:of   NP  V   NP 37.64
PREP:of  TERM:transcriptional_activity 36.60
Table 2: Sample of left CPs
(only terms and prepositions are instantiated)
   At this point, each term is associated with a set of
the most characteristic patterns in which it occurs.
We treat CPs as term features, and we use a feature
contrast model (Santini and Jain (1999)) to
calculate similarity between terms as a function of
both common and distinctive features. Let us now
formally define the contextual similarity measure.
Let C1 and C2 be two sets of CPs associated with
terms t1 and t2 respectively. Then, the contextual
similarity (CS) between t1 and t2 corresponds to the
ratio between the number of common and
distinctive contexts:
2.2   Lexical Similarity
We also examine the lexical similarity between
words that constitute terms. For example, if terms
share the same head, they are assumed to have the













	


??
?
=

?
; otherwise
nestedis not
)(||
1)(log
);(log
)(
2
2
Tpbp
bfTpfp
ppfp
pCP
|\||\|||2
||2),(
212121
21
21 CCCCCC
CCttCS
++?
?
=
same concept as an (in)direct hypernym (e.g.
progesterone receptor and oestrogen
receptor). Further, if one of such terms has
additional modifiers, this may indicate concept
specialisation (e.g. nuclear receptor and
orphan nuclear receptor). Bearing that in
mind, we base the definition of lexical similarity
on having a common head and/or modifier(s).
Formally, if t1 and t2 are terms, H1 and H2 their
heads, and M1 and M2 the sets of the stems of their
modifiers, then their lexical similarity (LS) is
calculated according to the following formula:
where a and b are weights such that a > b, since we
give higher priority to shared heads over shared
modifiers.
   Note that the lexical similarity between two
different terms can have a positive value only if at
least one of them is a multiword term. Also, when
calculating lexical similarity between terms that
are represented by corresponding acronyms, we
use normalised expanded forms. 4
2.3   Syntactical Similarity
By analysing the distribution of similar terms in
corpora, we observed that some general (i.e.
domain independent) lexico-syntactic patterns
indicate functional similarity between terms. For
instance, the following example:
... steroid receptors such as
estrogen receptor, glucocorticoid
receptor,and progesterone receptor.
suggests that all the terms involved are highly
correlated, since they appear in an enumeration
(represented by the such-as pattern) which
indicates their similarity (based on the is_a
relationship). Some of these patterns have been
previously used to discover hyponym relations
between words (Hearst (1992)). We generalised
                                                          
4 For our approach to acronym acquisition and term
normalisation, see Nenadic et al (2002).
the approach by taking into account patterns in
which the terms are used concurrently within the
same context. We hypothesise that the parallel
usage of terms within the same context, as a
specific type of co-occurrence, shows their
functional similarity. Namely, all the terms within
a parallel structure have the same syntactic
function within the sentence (e.g. object or subject)
and are used in combination with the same verb or
preposition. This fact is used as an indicator of
their semantic similarity.
   In our approach, several types of lexico-
syntactical patterns are considered: enumeration
expressions, coordination, apposition, and
anaphora. However, currently we do not
discriminate between different similarity
relationships among terms (which are represented
by different patterns), but instead, we consider
terms appearing in the same syntactical roles as
highly semantically correlated.
   A sample of enumeration patterns is shown in
Table 3. 5 Manually defined patterns are applied as
syntactic filters in order to retrieve sets of similar
terms. These patterns provide relatively good recall
and precision. We also used coordination patterns
(Klavans et al (1997)) as another type of parallel
syntactic structure. Two types of argument
coordination and two types of head coordination
patterns were considered (see Table 4). However,
not all the sequences that match the coordination
patterns are coordinated structures (see Table 5).
Therefore, these patterns provide relatively good
recall, but not high precision if one wants to
retrieve terms involved in such expression.6
However, both term coordination and (nominal)
conjunction of terms indicate their similarity.
   Based on co-occurrence of terms in these parallel
lexico-syntactical patterns, we define the
syntactical similarity (SS) measure for a pair of
terms as 1 if the two terms appear together in any
of the patterns, and 0 otherwise.
                                                          
5 Non-terminal syntactic categories are given in angle
brackets. Non-terminal <&> denotes a conjunctive word
sequence, i.e. the following regular expression: (as
well as)| (and[/or])|(or[/and]). Special
characters (, ), [, ], |, and * have the usual
interpretation in regular expression notation.
6 In the experiments that we have performed, the
precision of expanding terms from coordinated
structures was 70%.
( +?
+
= ||1),( 21*21 HHabattLS
)|\||\|||2
||2
212121
21* MMMMMM
MMb
++?
?
+
<TERM>([(](such as)|like|(e.g.[,])) <TERM> (,<TERM>)* [[,] <&> <TERM>] [)]
<TERM> (,<TERM>)* [,] <&> other <TERM>
<TERM> [,] (including|especially) <TERM> (,<TERM>)* [[,] <&> <TERM>]
both <TERM> and <TERM>
either <TERM> or <TERM>
neither <TERM> nor <TERM>
Table 3: Sample of enumeration lexico-syntactic patterns
(<N>|<Adj>) (,(<N>|<Adj>))* [,] <&> (<N>|<Adj>) <TERM>
(<N>|<Adj>)/(<N>|<Adj>) <TERM>
(<N>|<Adj>) <TERM> (,<TERM>)* [,] <&> <TERM>
(<N>|<Adj>) <TERM>/<TERM>
Table 4: Sample of coordination patterns
head coordination [adrenal [glands and gonads]]
term conjunction [adrenal glands] and [gonads]
Table 5: Ambiguities of coordinated structures
2.4   Hybrid CLS Similarity
None of the similarities introduced so far is
sufficient on its own to reliably estimate similarity
between two arbitrary terms. For example, if a
term appears infrequently or within very specific
CPs, the number of its significant CPs will
influence its contextual similarity to other terms.
Further, there are concepts that have idiosyncratic
names (e.g. a protein named Bride of
sevenless), which thus cannot be classified
relying exclusively on lexical similarity. Our
experiments also show that syntactical similarity
provides high precision, but low recall when used
on its own, as not all terms appear in a parallel
lexico-syntactical expression.
   Therefore, we introduce a hybrid term similarity
measure, called the CLS similarity, as a linear
combination of the three similarity measures:
CLS(t1, t2) = ? CS(t1, t2) + ? LS(t1, t2) + ? SS(t1, t2)
The choice of the weights ?, ?, and ? in the
previous formula is not a trivial problem. In our
preliminary experiments (Section 3) we used
manually chosen values. However, the parameters
have also been fine-tuned automatically by
supervised learning method based on a genetic
algorithm approach (Spasi? et al (2002)). A
domain specific ontology has been used to evaluate
the generated similarity measures and to set the
direction of their convergence. The differences
between results based on the various parameters
are presented in the following section.
3   Results, Evaluation and Discussion
The CLS measure was tested on a corpus of 2008
abstracts retrieved from MEDLINE database
(MEDLINE (2002)) with manually chosen values
0.3, 0.3 and 0.4 for ?, ?, and ? respectively.
Random samples of results have been evaluated by
a domain expert, and the combined measure
proved to be a good indicator of semantic
similarity. Table 6 shows the similarity of term
retinoic acid receptor to a number of
terms. The examples point out the importance of
combining different types of term similarities. For
instance, the low value of contextual similarity7 for
retinoid X receptor is balanced out by the
other two similarity values, thus correctly
indicating it as a term similar to term retinoic
acid receptor. Similarly, the high value of the
contextual similarity for signal transduction
pathway is neutralised by the other two similarity
                                                          
7 The low value is caused by relatively low frequency of
the term?s occurrences in the corpus.
values, hence preventing it as being labelled as
similar to retinoic acid receptor.
Term CS SS LS CLS
nuclear receptor 0.58 1.00 0.50 0.72
retinoid X receptor 0.32 1.00 0.33 0.60
retinoic acid 0.31 0.00 1.00 0.39
receptor complex 0.52 0.00 0.50 0.31
progesteron receptor 0.35 0.00 0.50 0.25
signal transduction
pathway
0.75 0.00 0.00 0.22
Table 6: Similarity values between retinoic
acid receptor and other terms
   The combined measure also proved to be
consistent in the sense that similar terms share the
same "friends" (Maynard and Ananiadou (2000)).
For example, the similarity values of two similar
terms glucocorticoid receptor and
estrogen receptor (the value of their
similarity is 0.68) with respect to other terms are
mainly approximate (Table 7).
Term glucocotricoid
receptor
estrogen
receptor
steroid receptor 0.66 0.64
progesterone receptor 0.55 0.59
human estrogen
t
0.28 0.37
retinoid x receptor 0.27 0.36
nuclear receptor 0.30 0.33
receptor complex 0.31 0.33
retinoic acid receptor 0.27 0.28
retinoid nuclear
t
0.26 0.26
Table 7: Similarity values for glucocorticoid
receptor and estrogen receptor
  The supervised learning of parameters resulted in
the values 0.13, 0.81 and 0.06 for ?, ?, and ?
respectively (see Spasi? et al (2002)). The
measure with these values showed a higher degree
of stability relative to the ontology-based similarity
measure. Note that the lexical similarity appears to
be the most important and the syntactical similarity
to be insignificant. The ontology used as a seed for
learning term similarities contained well-
structured, standardised and preferred terms which
resulted in promoting the lexical similarity as the
most significant. On the other hand, the SS
similarity is corpus-dependent: the size of the
corpus and the frequency with which the
concurrent lexico-syntactic patterns are realised in
it, affect the syntactical similarity. In the training
corpus such patterns occurred infrequently relative
to the number of terms, which indicates that a
bigger corpus is needed in the training phase. In
order to increase the number of concurrent
patterns, we also aim at including additional
patterns that describe appositions and
implementing procedures for resolution of co-
referring terms. We also plan to experiment with
parametrising the values of syntactical similarity
depending on the number and type of patterns in
which two terms appear simultaneously.
   The main purpose of discovering term
similarities is to produce a similarity matrix to
identify term clusters. In Nenadi? et al (2002b) we
present some preliminary results on term clustering
using the CLS hybrid term similarity measure. Two
different methods (namely the nearest neighbour
and the Ward?s method) have been used, and both
achieved around 70% precision in clustering
semantically similar terms.
Conclusions and Further Research
In this paper we have presented a novel method for
the automatic discovery of term similarities. The
method is based on the combination of contextual,
lexical and syntactical similarities between terms.
Lexical similarity exposes the resemblance
between the words that constitute terms, while
syntactical similarity is based on mutual co-
occurrence in parallel lexico-syntactic patterns.
Contextual similarity is based on the automatic
discovery of significant contexts through
contextual pattern mining.   Although the approach
is domain independent and knowledge-poor,
automatically collected patterns are domain
specific and they identify significant contexts in
which terms tend to appear. However, in order to
learn domain-appropriate term similarity
parameters, we need to customise the method by
incorporating domain-specific knowledge. For
example, we have used an ontology to represent
such knowledge.
   The preliminary results in the domain of
molecular biology have shown that the measure
proves to be a good indicator of semantic similarity
between terms. Furthermore, the similarity
measure is consistent at assigning weights: similar
terms tend to share the same ?friends?, i.e. there is
a significant degree of overlapping between terms
that are similar. These results are encouraging, as
terms are grouped reliably according to their
contextual, syntactical and lexical similarities.
   Besides term clustering (presented in Nenadi? et
al. (2002b)), the similarity measure can be used for
several term-oriented knowledge management
tasks.  Our future work will focus on the term
classification and the consistent population and
update of ontologies. However, specific term
relationship identification that will direct placing
terms in a hierarchy is needed. Further, term
similarities can be used for term sense
disambiguation as well, which is essential for
resolving terminological confusion occurring in
many domains.
Acknowledgement
We would like to thank Dr. Sylvie Albert and Dr.
Dietrich Schuhmann from LION Bioscience for the
evaluation of the results.
References
Ananiadou S. (1994): A Methodology for Automatic
Term Recognition. Proceedings of COLING-94,
Kyoto, Japan.
Bourigault D. (1992): Surface Grammatical Analysis for
the Extraction of Terminological Noun Phrases.
Proceedings of 14th International Conference on
Computational Linguistics, Nantes, France, pp. 977-
981.
Frantzi K.T., Ananiadou S. and Mima H. (2000):
Automatic Recognition of Multi-Word Terms: the C-
value/NC-value method. International Journal on
Digital Libraries, 3/2, pp. 115-130.
Friedman C., Kra P., Yu H., Krauthammer M. and
Rzhetsky A. (2001): GENIES: A Natural Language
Processing System for the Extraction of Molecular
Pathways from Journal Articles. Bioinformatics, 17/1,
pp. S74-S82.
Grefenstette G. (1994): Exploration in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Massachusetts, p. 302.
Hatzivassiloglou V., Duboue P. and Rzetsky A. (2001):
Disambiguating Proteins, Genes, and RNA in Text: A
Machine Learning Approach. Bioinformatics, 17/1,
pp. S97-S106
Hearst M.A. (1992): Automatic acquisition of hyponyms
from large text corpora. Proceedings of the 14th
International Conference on Computational
Linguistics, Nantes, France.
Klavans J. L., Tzoukermann E. and Jacquemin C.
(1997): A Natural Language Approach to Multi-Word
Term Conflation. Proceedings of Workshop DELOS,
Zurich, pp. 33-40.
Maynard D. and Ananiadou S. (2000): Identifying
Terms by Their Family and Friends.  Proceedings of
COLING 2000, Luxembourg, pp.530-536.
MEDLINE (2002): National Library of Medicine.
http://www.ncbi.nlm.nih.gov/PubMed/
Mima H., Ananiadou S. and Nenadi? G. (2001):
ATRACT Workbench: An Automatic Term
Recognition and Clustering of Terms. Text, Speech
and Dialogue - TSD 2001, LNAI 2166, Springer-
Verlag, Berlin, pp. 126-133.
Mima H., Ananiadou S., Nenadi? G. and Tsujii J.
(2002): A Methodology for Terminology-based
Knowledge Acquisition and Integration. Proceedings
of COLING 2002, Taiwan
Nakagawa H. and Mori, T. (2000): Nested Collocation
and Compound Noun for Term Recognition.
Proceedings of the First Workshop on Computational
Terminology COMPUTERM 98, pp. 64-70.
Nenadi? G., Spasi? I. and Ananiadou S. (2002a):
Automatic Acronym Acquisition and Term Variation
Management within Domain-specific Texts.
Proceedings of LREC 2002, Las Palmas, Spain, pp.
2155-2162.
Nenadi? G., Spasi? I. and Ananiadou S. (2002b): Term
Clustering using a Corpus-Based Similarity Measure.
Text, Speech and Dialogue - TSD 2002, LNAI series,
Springer-Verlag, Berlin
Santini S. and Jain R. (1999): Similarity Measures.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 21/9, pp. 871-88
Spasi? I., Nenadi? G. and Ananiadou S. (2002):
Supervised Learning of Term Similarities. IDEAL
2002, LNAI series, Springer-Verlag, Berlin
Ushioda A. (1996): Hierarchical Clustering of Words.
Proceedings of COLING ?96, Copenhagen, pp. 1159-
1162.
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 53?57, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ManTIME: Temporal expression identification and normalization in the
TempEval-3 challenge
Michele Filannino, Gavin Brown, Goran Nenadic
The University of Manchester
School of Computer Science
Manchester, M13 9PL, UK
{m.filannino, g.brown, g.nenadic}@cs.man.ac.uk
Abstract
This paper describes a temporal expression
identification and normalization system, Man-
TIME, developed for the TempEval-3 chal-
lenge. The identification phase combines
the use of conditional random fields along
with a post-processing identification pipeline,
whereas the normalization phase is carried out
using NorMA, an open-source rule-based tem-
poral normalizer. We investigate the perfor-
mance variation with respect to different fea-
ture types. Specifically, we show that the use
of WordNet-based features in the identifica-
tion task negatively affects the overall perfor-
mance, and that there is no statistically sig-
nificant difference in using gazetteers, shal-
low parsing and propositional noun phrases
labels on top of the morphological features.
On the test data, the best run achieved 0.95
(P), 0.85 (R) and 0.90 (F1) in the identifica-
tion phase. Normalization accuracies are 0.84
(type attribute) and 0.77 (value attribute). Sur-
prisingly, the use of the silver data (alone or in
addition to the gold annotated ones) does not
improve the performance.
1 Introduction
Temporal information extraction (Verhagen et al,
2007; Verhagen et al, 2010) is pivotal for many Nat-
ural Language Processing (NLP) applications such
as question answering, text summarization and ma-
chine translation. Recently the topic aroused in-
creasing interest also in the medical domain (Sun et
al., 2013; Kovac?evic? et al, 2013).
Following the work of Ahn et al (2005), the
temporal expression extraction task is now conven-
tionally divided into two main steps: identification
and normalization. In the former step, the effort
is concentrated on how to detect the right bound-
ary of temporal expressions in the text. In the nor-
malization step, the aim is to interpret and repre-
sent the temporal meaning of the expressions using
TimeML (Pustejovsky et al, 2003) format. In the
TempEval-3 challenge (UzZaman et al, 2012) the
normalization task is focused only on two temporal
attributes: type and value.
2 System architecture
ManTIME mainly consists of two components, one
for the identification and one for the normalization.
2.1 Identification
We tackled the problem of identification as a se-
quencing labeling task leading to the choice of Lin-
ear Conditional Random Fields (CRF) (Lafferty et
al., 2001). We trained the system using both human-
annotated data (TimeBank and AQUAINT corpora)
and silver data (TE3Silver corpus) provided by the
organizers of the challenge in order to investigate the
importance of the silver data.
Because the silver data are far more numerous
(660K tokens vs. 95K), our main goal was to rein-
force the human-annotated data, under the assump-
tion that they are more informative with respect to
the training phase. Similarly to the approach pro-
posed by Adafre and de Rijke (2005), we developed
a post-processing pipeline on top of the CRF se-
quence labeler to boost the results. Below we de-
scribe each component in detail.
53
2.1.1 Conditional Random Fields
The success of applying CRFs mainly depends on
three factors: the labeling scheme (BI, BIO, BIOE
or BIOEU), the topology of the factor graph and
the quality of the features used. We used the BIO
format in all the experiments performed during this
research. The factor graph has been generated us-
ing the following topology: (w0), (w?1), (w?2),
(w+1), (w+2), (w?2?w?1), (w?1?w0), (w0?w+1),
(w?1?w0?w+1), (w0?w+1?w+2), (w+1?w+2),
(w?2 ?w?1 ?w0), (w?1 ?w+1) and (w?2 ?w+2).
The system tokenizes each document in the cor-
pus and extracts 94 features. These belong to the
following four disjoint categories:
? Morphological: This set includes a compre-
hensive list of features typical of Named En-
tity Recognition (NER) tasks, such as the word
as it is, lemma, stem, pattern (e.g. ?Jan-2003?:
?Xxx-dddd?), collapsed pattern (e.g. ?Jan-
2003?: ?Xx-d?), first 3 characters, last 3 charac-
ters, upper first character, presence of ?s? as last
character, word without letters, word without
letters or numbers, and verb tense. For lemma
and POS tags we use TreeTagger (Schmid,
1994). Boolean values are included, indicating
if the word is lower-case, alphabetic, digit, al-
phanumeric, titled, capitalized, acronym (cap-
italized with dots), number, decimal number,
number with dots or stop-word. Additionally,
there are features specifically crafted to han-
dle temporal expressions in the form of regu-
lar expression matching: cardinal and ordinal
numbers, times, dates, temporal periods (e.g.
morning, noon, nightfall), day of the week, sea-
sons, past references (e.g. ago, recent, before),
present references (e.g. current, now), future
references (e.g. tomorrow, later, ahead), tem-
poral signals (e.g. since, during), fuzzy quan-
tifiers (e.g. about, few, some), modifiers, tem-
poral adverbs (e.g. daily, earlier), adjectives,
conjunctions and prepositions.
? Syntactic: Chunks and propositional noun
phrases belong to this category. Both are
extracted using the shallow parsing software
MBSP1.
1http://www.clips.ua.ac.be/software/mbsp-for-python
? Gazetteers: These features are expressed us-
ing the BIO format because they can include
expressions longer than one word. The inte-
grated gazetteers are: male and female names,
U.S. cities, nationalities, world festival names
and ISO countries.
? WordNet: For each word we use the number of
senses associated to the word, the first and the
second sense name, the first 4 lemmas, the first
4 entailments for verbs, the first 4 antonyms,
the first 4 hypernyms and the first 4 hyponyms.
Each of them is defined as a separate feature.
The features mentioned above have been com-
bined in 4 different models:
? Model 1: Morphological only
? Model 2: Morphological + syntactic
? Model 3: Morphological + gazetteers
? Model 4: Morphological + gazetteers + Word-
Net
All the experiments have been carried out using
CRF++ 0.572 with parameters C = 1, ? = 0.0001
and L2-regularization function.
2.1.2 Model selection
The model selection was performed over the
entire training corpus. Silver data and human-
annotated data were merged, shuffled at sentence-
level (seed = 490) and split into two sets: 80% as
cross-validation set and 20% as real-world test set.
The cross-validation set was shuffled 5 times, and
for each of these, the 10-fold cross validation tech-
nique was applied.
The analysis is statistically significant (p =
0.0054 with ANOVA test) and provides two impor-
tant outcomes: (i) the set of WordNet features nega-
tively affects the overall classification performance,
as suggested by Rigo et al (2011). We believe this is
due to the sparseness of the labels: many tokens did
not have any associated WordNet sense. (ii) There
is no statistically significant difference among the
first three models, despite the presence of apparently
important information such as chunks, propositional
2https://code.google.com/p/crfpp/
54
Figure 1: Differences among models using 5x10-fold
cross-validation
noun phrases and gazetteers. The Figure 1 shows the
box plots for each model.
In virtue of this analysis, we opted for the smallest
feature set (Model 1) to prevent overfitting.
In order to get a reliable estimation of the perfor-
mance of the selected model on the real world data,
we trained it on the entire cross-validation set and
tested it against the real-word test set. The results
for all the models are shown in the following table:
System Pre. Rec. F?=1
Model 1 83.20 85.22 84.50
Model 2 83.57 85.12 84.33
Model 3 83.51 85.12 84.31
Model 4 83.15 84.44 83.79
Precision, Recall and F?=1 score are computed
using strict matching.
The models used for the challenge have been
trained using the entire training set.
2.1.3 Post-processing identification pipeline
Although CRFs already provide reasonable per-
formance, equally balanced in terms of precision
and recall, we focused on boosting the baseline per-
formance through a post-processing pipeline. For
this purpose, we introduced 3 different modules.
Probabilistic correction module averages the
probabilities from the trained CRFs model with the
ones extracted from human-annotated data only. For
each token, we extracted: (i) the conditional proba-
bility for each label to be assigned (B, I or O), and
(ii) the prior probability of the labels in the human-
annotated data only. The two probabilities are aver-
aged for every label of each token. The list of tokens
extracted in the human-annotated data was restricted
to those that appeared within the span of temporal
expressions at least twice. The application of this
module in some cases has the effect of changing the
most likely label leading to an improvement of re-
call, although its major advantage is making CRFs
predictions less strict.
BIO fixer fixes wrong label sequences. For the
BIO labeling scheme, the sequence O-I is necessar-
ily wrong. We identified B-I as the appropriate sub-
stitution. This is the case in which the first token
has been incorrectly annotated (e.g. ?Three/O days/I
ago/I ./O? is converted into ?Three/B days/I ago/I
./O?). We also merged close expressions such as B-
B or I-B, because different temporal expressions are
generally divided at least by a symbol or a punctu-
ation character (e.g. ?Wednesday/B morning/B? is
converted into ?Wednesday/B morning/I?).
Threshold-based label switcher uses the prob-
abilities extracted from the human-annotated data.
When the most likely label (in the human-annotated
data) has a prior probability greater than a certain
threshold, the module changes the CRFs predicted
label to the most likely one. This leads to force
the probabilities learned from the human-annotated
data.
Through repeated empirical experiments on a
small sub-set of the training data, we found an
optimal threshold value (0.87) and an optimal se-
quence of pipeline components (Probabilistic cor-
rection module, BIO fixer, Threshold-based label
switcher, BIO fixer).
We analyzed the effectiveness of the post-
processing identification pipeline using a 10-fold
cross-validation over the 4 models. The difference
between CRFs and CRFs + post-processing pipeline
is statistically significant (p = 3.51 ? 10?23 with
paired T-test) and the expected average increment is
2.27% with respect to the strict F?=1 scores.
2.2 Normalization
The normalization component is an updated version
of NorMA (Filannino, 2012), an open-source rule-
based system.
55
# Training data
Identification Normalization
Overall
Strict matching Lenient matching Accuracy
run (post-processing) Pre. Rec. F?=1 Pre. Rec. F??=1 Type Value score
1 Human&Silver (no) 78.57 63.77 70.40 97.32 78.99 87.20 88.99 77.06 67.20
2 Human&Silver (yes) 79.82 65.94 72.22 97.37 80.43 88.10 87.38 75.68 66.67
3 Human (no) 76.07 64.49 69.80 94.87 80.43 87.06 87.39 77.48 67.45
4 Human (yes) 78.86 70.29 74.33 95.12 84.78 89.66 86.31 76.92 68.97
5 Silver (no) 77.68 63.04 69.60 97.32 78.99 87.20 88.99 77.06 67.20
6 Silver (yes) 81.98 65.94 73.09 98.20 78.99 87.55 90.83 77.98 68.27
Table 1: Performance on the TempEval-3 test set.
3 Results and Discussion
We submitted six runs as combinations of different
training sets and the use of the post-processing iden-
tification pipeline. The results are shown in Table 1
where the overall score is computed as multiplica-
tion between lenient F?=1 score and the value accu-
racy.
In all the runs, recall is lower than precision. This
is an indication of a moderate lexical difference be-
tween training data and test data. The relatively low
type accuracy testifies the normalizer?s inability to
recognize new lexical patterns. Among the correctly
typed temporal expressions, there is still about 10%
of them for which an incorrect value is provided.
The normalization task is proved to be challenging.
The training of the system by using human-
annotated data only, in addition to the post-
processing pipeline, provided the best results, al-
though not the highest normalization accuracy. Sur-
prisingly, the silver data do not improve the per-
formance, both when used alone or in addition
to human-annotated data (regardless of the post-
processing pipeline usage).
The post-processing pipeline produces the high-
est precision when applied to the silver data only.
In this case, the pipeline acts as a reinforcement of
the human-annotated data. As expected, the post-
processing pipeline boosts the performance of both
precision and recall. We registered the best improve-
ment with the human-annotated data.
Due to the small number of temporal expressions
in the test set (138), further analysis is required to
draw more general conclusions.
4 Conclusions
We described the overall architecture of ManTIME,
a temporal expression extraction pipeline, in the
context of TempEval-3 challenge.
This research shows, in the limits of its general-
ity, the primary and exhaustive importance of mor-
phological features to the detriment of syntactic fea-
tures, as well as gazetteer and WordNet-related ones.
In particular, while syntactic and gazetteer-related
features do not affect the performance, WordNet-
related features affect it negatively.
The research also proves the use of a post-
processing identification pipeline to be promising
for both precision and recall enhancement.
Finally, we found out that the silver data do not
improve the performance, although we consider the
test set too small for this result to be generalizable.
To aid replicability of this work, the system
code, machine learning pre-trained models, statis-
tical validation details and an online DEMO are
available at: http://www.cs.man.ac.uk/
?filannim/projects/tempeval-3/
Acknowledgments
We would like to thank the organizers of the
TempEval-3 challenge. The first author would like
also to acknowledge Marilena Di Bari, Joseph Mel-
lor and Daniel Jamieson for their support and the UK
Engineering and Physical Science Research Coun-
cil for its support in the form of a doctoral training
grant.
56
References
Sisay Fissaha Adafre and Maarten de Rijke. 2005. Fea-
ture engineering and post-processing for temporal ex-
pression recognition using conditional random fields.
In Proceedings of the ACL Workshop on Feature En-
gineering for Machine Learning in Natural Language
Processing, FeatureEng ?05, pages 9?16, Stroudsburg,
PA, USA. Association for Computational Linguistics.
David Ahn, Sisay Fissaha Adafre, and Maarten de Ri-
jke. 2005. Towards task-based temporal extraction
and recognition. In Graham Katz, James Pustejovsky,
and Frank Schilder, editors, Annotating, Extracting
and Reasoning about Time and Events, number 05151
in Dagstuhl Seminar Proceedings, Dagstuhl, Germany.
Internationales Begegnungs- und Forschungszentrum
fu?r Informatik (IBFI), Schloss Dagstuhl, Germany.
Michele Filannino. 2012. Temporal expression
normalisation in natural language texts. CoRR,
abs/1206.2010.
Aleksandar Kovac?evic?, Azad Dehghan, Michele Filan-
nino, John A Keane, and Goran Nenadic. 2013. Com-
bining rules and machine learning for extraction of
temporal expressions and events from clinical narra-
tives. Journal of American Medical Informatics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. Timeml: Robust specification of event
and temporal expressions in text. In in Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5.
Stefan Rigo and Alberto Lavelli. 2011. Multisex - a
multi-language timex sequential extractor. In Tempo-
ral Representation and Reasoning (TIME), 2011 Eigh-
teenth International Symposium on, pages 163?170.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.
Evaluating temporal relations in clinical text: 2012
i2b2 challenge. Journal of the American Medical In-
formatics Association.
Naushad UzZaman, Hector Llorens, James F. Allen,
Leon Derczynski, Marc Verhagen, and James Puste-
jovsky. 2012. Tempeval-3: Evaluating events,
time expressions, and temporal relations. CoRR,
abs/1206.5333.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 75?
80, Prague.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, SemEval
?10, pages 57?62, Stroudsburg, PA, USA. Association
for Computational Linguistics.
57
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 72?80,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
 
 
An Exploration of Mining Gene Expression Mentions and their  
Anatomical Locations from Biomedical Text 
 
 
Martin Gerner 
Faculty of Life Sciences 
University of Manchester 
Manchester, UK 
martin.gerner@postgrad. 
manchester.ac.uk 
Goran Nenadic 
School of Computer Science 
University of Manchester 
Manchester, UK 
g.nenadic@ 
manchester.ac.uk 
 
Casey M. Bergman 
Faculty of Life Sciences 
University of Manchester 
Manchester, UK 
casey.bergman@ 
manchester.ac.uk 
  
 
Abstract 
Here we explore mining data on gene expres-
sion from the biomedical literature and 
present Gene Expression Text Miner 
(GETM), a tool for extraction of information 
about the expression of genes and their ana-
tomical locations from text. Provided with 
recognized gene mentions, GETM identifies 
mentions of anatomical locations and cell 
lines, and extracts text passages where au-
thors discuss the expression of a particular 
gene in specific anatomical locations or cell 
lines. This enables the automatic construction 
of expression profiles for both genes and ana-
tomical locations. Evaluated against a ma-
nually extended version of the BioNLP '09 
corpus, GETM achieved precision and recall 
levels of 58.8% and 23.8%, respectively. Ap-
plication of GETM to MEDLINE and 
PubMed Central yielded over 700,000 gene 
expression mentions. This data set may be 
queried through a web interface, and should 
prove useful not only for researchers who are 
interested in the developmental regulation of 
specific genes of interest, but also for data-
base curators aiming to create structured re-
positories of gene expression information. 
The compiled tool, its source code, the ma-
nually annotated evaluation corpus and a 
search query interface to the data set ex-
tracted from MEDLINE and  PubMed Cen-
tral is available at http://getm-
project.sourceforge.net/. 
1 Introduction 
With almost 2000 articles being published daily 
in 2009, the amount of available research litera-
ture in the biomedical domain is increasing ra-
pidly. Currently, MEDLINE contains reference 
records for almost 20 million articles (with about 
10 million abstracts), and PubMed Central 
(PMC) contains almost two million full-text ar-
ticles. These resources store an enormous wealth 
of information, but are proving increasingly dif-
ficult to navigate and interpret. This is true both 
for researchers seeking information on a particu-
lar subject and for database curators aiming to 
collect and annotate information in a structured 
manner. 
Text-mining tools aim to alleviate this prob-
lem by extracting structured information from 
unstructured text. Considerable attention has 
been given to some areas in text-mining, such as 
recognizing named entities (e.g. species, genes 
and drugs) (Rebholz-Schuhmann et al, 2007; 
Hakenberg et al, 2008; Gerner et al, 2010) and 
extracting molecular relationships, e.g. protein-
protein interactions (Donaldson et al, 2003; 
Plake et al, 2006; Chowdhary et al, 2009). 
Many other areas of text mining in the biomedi-
cal domain are less mature, including the extrac-
tion of information about the expression of genes 
(Kim et al, 2009). The literature contains a large 
amount of information about where and when 
genes are expressed, as knowledge about the ex-
pression of a gene is critical for understanding its 
function and has therefore often been reported as 
part of gene studies. Gene expression profiles 
from genome-wide studies are available in spe-
cialized databases such as the NCBI Gene Ex-
pression Omnibus (Barrett et al, 2009) and 
FlyAtlas (Chintapalli et al, 2007), but results on 
gene expression from smaller studies remain 
locked in the primary literature. 
Previously, a number of data-mining projects 
have combined text-mining methods with struc-
tured genome-wide gene expression data in order 
72
  
to allow further interpretation of the gene expres-
sion data (Natarajan et al, 2006; Fundel, 2007). 
However, only recently has interest in text-
mining tools aimed at extracting gene expression 
profiles from primary literature started to grow. 
The 2009 BioNLP shared task (Kim et al, 2009) 
aimed at extracting biological "events", where 
one of the event types was gene expression. For 
this event type, participants were asked to deter-
mine locations in text documents where authors 
discussed the expression of a gene or protein and 
extract a trigger keyword (e.g. "expression") and 
its associated gene participant (the gene whose 
expression is discussed). The group that achieved 
the highest accuracy on the "simple event" task 
(where gene expression extraction was included) 
achieved recall and precision levels of 64.2% and 
77.5%, respectively (Bj?rne et al, 2009). A key 
limitation of the 2009 shared task was that all 
genes had been annotated prior to the beginning 
of the task, making it difficult to anticipate the 
accuracy of tools that do not rely on pre-
annotated entities. 
Biologists are interested not only in finding 
statements of gene expression events, but also in 
knowing where and when a gene is expressed. 
However, to the best of our knowledge, no effort 
has previously been made to extract and map the 
expression of genes to specific tissues and cell 
types (and vice versa) from the literature. Thus, 
we have taken preliminary steps to construct a 
software tool, named Gene Expression Text 
Miner (GETM), capable of extracting informa-
tion about what genes are expressed and where 
they are expressed. An additional goal of this 
work is to apply this tool to the whole of MED-
LINE and PMC, and make both the tool and the 
extracted data available to researchers.  
We anticipate that the data extracted by 
GETM will provide researchers an overview 
about where a specific gene is expressed, or what 
genes are expressed in a specific anatomical lo-
cation. Moreover, GETM will aid in the curation 
of gene expression databases by providing text 
passages and identifiers to database curators for 
verification.  
2 Methods 
An overview of the workflow of GETM is given 
in Figure 1. Articles are initially scanned for 
mentions of gene entities, anatomical entities and 
keywords indicating the discussion of gene ex-
pression (called triggers following BioNLP ter-
minology, e.g. "expression" and "expressed in"). 
After the detection of the entities and triggers, 
abbreviations are detected and entities are 
grouped in the cases of enumerations. Finally, 
sentences are split and each sentence is 
processed in order to associate triggers with gene 
and anatomical entities. Each step is described 
below in more detail. 
2.1 Named entity recognition and abbrevia-
tion detection 
In order to extract information on the expression 
of genes and their anatomical locations, a key 
requirement is the accurate recognition and nor-
malization (mapping the recognized terms to da-
tabase identifiers) of both the genes and anatom-
ical locations in question. In order to locate and 
identify gene names, we utilized GNAT (Haken-
berg et al, 2008), an inter-species gene name 
recognition software package. Among the gene 
name recognition tools capable of gene normali-
zation, GNAT is currently showing the best ac-
curacy (compared to the BioCreative corpora 
(Hirschman et al, 2005; Morgan et al, 2008)). 
The species identification component of GNAT, 
used to help disambiguate gene mentions across 
species, was performed by LINNAEUS (Gerner 
et al, 2010). 
In order to perform named entity recognition 
(NER) of anatomical locations, we investigated 
the use of various anatomical ontologies. A key 
challenge with these ontologies is that the terms 
NER, trigger 
detection
Gen
(GNAT)
Anatomy
(Dictionaries)
Articles
Triggers
(Dictionaries)
Detect 
enumerations and 
abbreviations
Sentence splitting
Determine gene 
and anatomy 
targets of triggers
Results
(web access)
 
Figure 1. Schematic overview of the processing workflow of GETM. 
 
73
  
vary significantly from one species to another. 
For example, fruit flies have wings while humans 
do not, and humans have fingers, while fruit flies 
do not. Efforts have been made in creating uni-
fied species-independent anatomical ontologies, 
such as Uberon (Haendel et al, 2009; Mungall et 
al., 2010). However, in preliminary experiments 
we found that the coverage of Uberon was not 
extensive enough for this particular application 
(data not shown), motivating us to instead use a 
combination of various species-specific anatomi-
cal ontologies hosted at the OBO Foundry 
(Smith et al, 2007). These ontologies (n = 13) 
were chosen in order to cover terms from the 
main model organisms that are used in research 
(e.g. human, fruit fly, mouse, Caenorhabditis 
elegans) and a few larger groups of organisms 
such as e.g. amphibians and fungi. It is worth 
noting that the more general terms, such as e.g. 
"brain", are likely to match anatomical locations 
in other species as well. In total, the selected on-
tologies contain terms for 38,459 different ana-
tomical locations. 
 We also utilized an ontology of cell lines 
(Romano et al, 2009), containing terms for a 
total of 8,408 cell lines (ranging across 60 spe-
cies), as cell lines can be viewed as biological 
proxies for the anatomical locations that gave 
rise to them. For example, the HeLa cell line was 
derived from human cervical cells, and the THP1 
cell line was derived from human monocytes 
(Romano et al, 2009). 
The anatomical and cell line NER, utilizing 
the OBO Foundry and cell line ontologies, was 
performed using dictionary-matching methods 
similar to those employed by LINNAEUS 
(Gerner et al, 2010). 
 After performing gene and anatomical NER 
on the document, abbreviations were detected 
(using the algorithm by Schwartz and Hearst 
(2003)) in order to allow the detection and mar-
kup of abbreviated entity names in the cases 
where the abbreviations do not exist in any of the 
ontologies that are used. 
2.2 Trigger detection 
The trigger keywords indicating that an author is 
discussing the expression of one or several 
genes, such as e.g. "expression" and "expressed 
in" were detected using a manually created list of 
regular expressions. The regular expressions 
were designed to match variations of a set of 
terms, listed below, that were identified when 
inspecting documents not used when building the 
gold-standard corpus (see Section 3.1). 
The terms used to construct the trigger regular 
expressions were orthographical, morphological 
and derivational variations of "expression", "pro-
duction" and "transcription". Descriptions of the 
level of expression were also considered for the 
different terms, such as "over-expression," "un-
der-expression," "positively expressed," "nega-
tively expressed," etc. 
Each gene expression mention that has been 
extracted by GETM contains information about 
the trigger term used by the author, allowing re-
searchers to look only at e.g. the "negative" men-
tions (where genes are e.g. "under-expressed" or 
"negatively expressed") or the "positive" men-
tions (where genes are e.g. "over-expressed"). 
2.3 Association of entities to the trigger 
To help associate triggers with the correct gene 
and anatomical entities, articles were first split 
into sentences, allowing each sentence to be 
processed in turn. In order to reduce the number 
of false positives and preserve a high level of 
precision, any sentences that did not contain a 
trigger, at least one gene mention and at least one 
anatomical mention were ignored. For the sen-
tences that did contain a combination of all three 
requirements (trigger, gene and anatomical men-
tion), the following pattern- and distance-based 
rules were employed in order to associate each 
trigger with the correct gene and anatomical 
mention: 
1. If there is only one gene mention and only 
one anatomical mention in the sentence, the 
trigger is associated with those mentions. 
2. If there is one gene mention (G) and one 
anatomical mention (A) in the sentence 
such that they match one of the patterns 
"<G> is expressed in <A>", "expression of 
<G> in <A>", "<A> transcribes <G>" or 
"<A> produces <G>", the gene mention 
<G> and anatomical mention <A> are asso-
ciated with the trigger (variations of the 
triggers, such as "over-expressed" and 
"negative expression" are considered as 
well). Additional gene or anatomical men-
tions that fall outside the pattern are ig-
nored. 
3. If neither of the above rules applies, the 
trigger is associated with the gene and ana-
tomical mentions that are closest to the trig-
ger. 
For the purposes of these rules, an enumera-
tion of several genes or anatomical locations was 
74
  
handled as if it was only a single mention. For 
example, Rule 1 might trigger even if there are 
several genes mentioned in the same sentence, as 
long as they are mentioned together as part of an 
enumeration. 
In order to detect these enumerations, a rule-
based algorithm for connecting enumerated gene 
and anatomical entity mentions (as in e.g. "... 
RelB and DC-CK1 gene expression ...") was also 
implemented. Being able to detect enumerations 
allowed the rules described above to recognize 
that a particular gene expression mention do not 
refer to only e.g. "RelB" or "DC-CK1", but both 
of them at the same time. 
Each trigger was processed independently, al-
lowing the potential extraction of multiple gene 
expression statements from a single sentence. 
Initially, experiments were performed using 
stricter rules where only variations of Rule 2, 
requiring gene and anatomical mentions to con-
form to certain patterns, were used. However, 
recall was in these cases found to be extremely 
low (below 5%, data not shown). The current 
rules are more permissive, allowing higher recall. 
 The fact that the method requires a combina-
tion of a trigger, a gene and an anatomical loca-
tion makes it susceptible to false negatives: if 
any one of them cannot be found by the NER or 
trigger detection methods, the whole combina-
tion is missed. 
3 Evaluation 
3.1 Extending the BioNLP shared task 
gold-standard corpus 
In order to make a meaningful evaluation of the 
accuracy of text-mining applications, a gold-
standard corpus, consisting of manually anno-
tated mentions for a set of documents, is re-
quired. Previously, no such corpus existed that 
was suitable for this problem (providing annota-
tions linked to mentions of both gene and ana-
tomical locations). However, the BioNLP corpus 
(Ohta et al, 2009) which is based on the GENIA 
corpus (Kim et al, 2008), does contain annota-
tions about gene expression. Annotations in the 
corpus contain trigger terms that are linked to 
genes (or gene products) where the authors dis-
cuss gene expression. However, anatomical loca-
tions have not been annotated in this corpus.  
In order to allow evaluation of the accuracy of 
our software, we extended the annotations of 
gene expression events in part of the BioNLP 
corpus. Each gene expression entry in the corpus 
was linked to the anatomical location or cell line 
that the author mentioned. In cases where gene 
expression was only discussed generally without 
referring to expression in a particular location, no 
association to an anatomical location could be 
made (these entries were ignored during evalua-
tion). Note that named entities were only linked 
to their locations in the text, not to unique data-
base identifiers (such as Entrez Gene or OBO 
Foundry identifiers). Because of this, subsequent 
evaluation in this extended corpus is limited to 
the accuracy of recognition (locating the entities 
in the text), but not normalization (linking the 
entities to database identifiers). 
In total, annotations for 150 abstracts (consti-
tuting the development set of the BioNLP cor-
pus) were extended to also include anatomical 
locations. These abstracts contained 377 anno-
tated gene expression events, of which 267 
(71%) could be linked to anatomical locations. 
These results demonstrate that the majority of 
gene expression mentions include reference to an 
anatomical location. For a few cases where the 
author described the expression of a gene in sev-
eral cell types, a single gene expression event 
gave rise to several distinct "entries" in the ex-
tended corpus, creating a total of 279 final gene 
expression entries that are linked to anatomical 
locations. 
4 Results 
In order to evaluate the accuracy of GETM, it 
was first run on the 150 abstracts in the gold-
standard corpus, after which the extracted results 
were compared against the annotations of the 
corpus. GETM was also applied to the whole of 
MEDLINE and PMC, in order to extract a sear-
chable and structured data set of gene expression 
mentions in published biomedical articles. 
4.1 Accuracy 
The gene expression mentions extracted by 
GETM from the corpus were compared against 
the manually created annotations in order to es-
timate the accuracy of the software. After in-
specting the false positives and false negatives, 
we noted that a number of the false positives ac-
tually were correctly identified by our system 
and had been marked as false positives only be-
cause of incomplete annotations in the corpus. 
Because of this, all false positives were manually 
examined in order to determine the "correct" 
number of false positives. For one of the cor-
rected expression mentions, two anatomical loca-
tions were enumerated, with GETM only locat-
75
  
ing one of them. This introduced both a new true 
positive (for the one that was recognized) and a 
new false negative (for the one that was not). The 
number of true positives, false positives, false 
negatives, precision and recall (before and after 
correction) are shown in Table 1.  
 
 Original Corrected 
TP 53 67  
FP 61 (p = 46.5%) 47 (p = 58.8%) 
FN 214 (r = 19.8%) 215 (r = 23.8%) 
Table 1. The number of true positives (TP), false 
positives (FP), false negatives (FN) and levels of 
precision (p) and recall (r) for GETM when 
compared against the gold-standard corpus. 
4.2 Analysis of false negatives 
In order to determine the causes of the relatively 
high number of false negatives, the gene entities, 
anatomical entities and triggers identified by 
GNAT and GETM were compared to the ex-
tended corpus, allowing us to determine the 
number of corpus entities that could not be found 
by the GNAT and GETM NER tools. An analy-
sis was also performed in order to determine the 
number of corpus entries that were spread across 
several sentences, as any expression mentions 
spread over several sentences are missed by 
GETM.  
The analysis results can be seen in Table 2, 
showing that virtually all false negatives are 
caused either by incomplete NER or multi-
sentence entries. Only considering the NER, 68% 
of the gold-standard corpus annotated entries 
contain either a trigger (example FN: "detected"), 
gene (example FN: CD4) or anatomical location 
(example FN: "lymphoblastoid cells") that could 
not be located automatically. GETM was further 
limited by entities being spread across several 
sentences (n=66, 23.6%). In total, 74.3% of all 
entries could not be extracted correctly due to 
either incomplete NER, incomplete trigger detec-
tion or the entities being spread across multiple 
sentences. This limited recall to 25.7%, even if 
the rule-based method was working perfectly. 
4.3 Analysis of false positives 
Manual inspection of the false positives (after 
adjusting the false positives caused by incom-
plete annotations) allowed the identification of 
one clear cause: if the NER methods fail to rec-
ognize the entity associated with a manually an-
notated expression entry, but there are other enti-
ties (that have been recognized) in the sentence, 
those entities might be incorrectly associated 
with the trigger instead. For example, in the sen-
tence "In conclusion, these data show that IL-10 
induces c-fos expression in human B-cells by 
activation of tyrosine and serine/threonine kinas-
es." (Bonig et al, 1996) (the correct entities and 
trigger are italicized), a correctly extracted entry 
would link c-fos to B-cells through the trigger 
expression. However, the gene NER component 
failed to recognize c-fos but did recognize IL-10, 
causing GETM to incorrectly associate IL-10 
with B-cells. Either increasing the accuracy of 
the NER methods or performing deeper gram-
matical parsing could potentially reduce the 
number of false positives of this type. We note  
that the number of cases for this category (n = 
15; 34%) only make up a minority of the total 
number of false positives, and the remainder 
have no easily identifiable common cause. 
4.4 Application to MEDLINE and PMC 
documents 
GETM was applied to the whole set of 
10,240,192 MEDLINE entries from the 2010 
baseline files that contain an abstract (many 
MEDLINE entries do not contain an abstract). 
From these abstracts, 578,319 statements could 
be extracted containing information about the 
expression of a gene and the location of this ex-
pression. In addition, GETM was also applied to 
the set of 186,616 full-text articles that make up 
the open-access portion of PMC (downloaded 
February 5th, 2010). The full-text articles al-
lowed the extraction of 145,796 statements (an 
18-fold increase in entries per article compared 
Problem type Number of occurrences 
Trigger not found 58 (20.7%) 
Gene not found 139 (49.6%) 
Anatomical location not found 74 (26.4%) 
Any of the entities or trigger not found 190 (67.9%) 
Total number of entities not contained in a single sentence 66 (23.6%) 
Total number of entities either not found or not in the same sentence 208 (74.3%) 
Table 2. Breakdown of the causes for false negatives in GETM, relative to the total number of 
entries in the gold-standard corpus. 
 
76
  
to the MEDLINE abstracts). In total, 716,541 
statements were extracted, not counting the ab-
stracts in MEDLINE that also appear in PMC. 
Overall, the combined extracted information 
ranges across 25,525 different genes (the most 
common being tumor necrosis factor (TNF su-
perfamily, member 2) in human) and 3,655 dif-
ferent anatomical locations (the most common 
being T cells). The most common combination 
concerns the expression of human interleukin 2 
in T cells. The 10 most commonly mentioned 
combinations of genes and anatomical locations 
are shown in Table 3. Overall, these results sug-
gest that studies on gene expression in the field 
of mammalian immunology are the dominant 
signal in MEDLINE and PMC. The genes that 
were recognized and normalized range across 15 
species, out of the 23 supported by GNAT (Ha-
kenberg et al, 2008). The most common species 
is human, as expected (Gerner et al, 2010), fol-
lowed by mouse, rat, chicken and cow. 
The majority of statements were associated to 
anatomical locations from the OBO Foundry on-
tologies (n=649,819; 89.7%), while the remaind-
er were associated to cell lines (n=74,294; 
10.3%). This result demonstrates the importance 
of taking cell lines into account when attempting 
to identify anatomical entities.  
Finally, a total of 73,721 (11.7%) of the state-
ments extracted from MEDLINE contained ei-
ther genes or anatomical locations that had been 
enumerated by the author, underscoring the im-
portance of considering enumerations when de-
signing text-mining algorithms. 
4.5 Availability 
GETM is available under an open source license, 
and researchers may freely download GETM, its 
source code and the extended gold-standard cor-
pus from http://getm-project.sourceforge.net/. 
Also available on the web site is a search query 
interface where researchers may search for ex-
tracted gene expression entries relating to a par-
ticular gene, anatomical location or a combina-
tion of the two and view these in the context of 
the surrounding text.  
5 Discussion 
5.1 Overview of design philosophy 
When constructing text-mining applications, a 
balance between precision (reflecting the relative 
number of false positives) and recall (reflecting 
the relative number of false negatives) is often 
used to optimize system performance. Accor-
dingly, a measure which often is used to evaluate 
the accuracy of software is the F-score (the har-
monic mean of the precision and recall). In this 
work, we have decided that rather than trying to 
maximize the F-score, we have put more focus 
on precision in order to ensure that the data ex-
tracted by GETM are of as high quality as possi-
ble. This typically leads to lower recall, causing 
the software to detect a relatively smaller number 
of relevant passages. Nonetheless, we believe 
that for this particular application, a smaller 
amount of data with higher quality would be 
more useful to curators and biologists than a 
larger amount of data that is less reliable. 
5.2 Comparison with previous work 
It is difficult to compare the precision and recall 
levels of GETM (at 58.8% and 23.8%, respec-
tively) against other tools, as GETM is the first 
tool aiming to perform this particular task. The 
closest comparison that can be made is against 
the software evaluated in the BioNLP shared task 
(Kim et al, 2009). However, software developed 
for the BioNLP shared task did not attempt to 
extract the anatomical location of gene expres-
sion mentions, nor did they need to identify the 
component entities involved. The tool with the 
highest accuracy for the simple event task (where 
gene expression extraction was included) showed 
Gene Anatomical location Number of mentions 
Interleukin 2 T cells 3511 
Interferon, gamma T cells 2088 
CD4 T cells 1623 
TNF Macrophages 1596 
TNF Monocytes 1539 
Interleukin 4 T cells 1323 
Integrin, alpha M Neutrophils 1063 
Inteleukin 10 T cells 971 
ICAM 1 Endothelial cells 964 
Interleukin 2 Lymphocytes 876 
Table 3. The ten most commonly mentioned combinations of genes and anatomical locations 
 
77
  
precision and recall levels of 77.5% and 64.2%, 
respectively (Bj?rne et al, 2009). It is not clear 
how tools evaluated in the 2009 BioNLP shared 
task would perform if they identified entities 
themselves rather than using pre-annotated enti-
ties. 
5.3 Limits on accuracy 
When investigating the cause of the low level of 
recall, the main reason that emerged for the high 
number of false negatives was the high number 
of annotated entries that could not be automati-
cally extracted due to at least one of the gene, 
anatomical or trigger mentions not being recog-
nized. This fact underscores the importance of 
accurate NER for applications that rely on the 
extracted entity mentions, especially those that 
attempt to extract information from multiple enti-
ty types, like GETM. The results also demon-
strate that NER, particularly in the case of gene 
name normalization, continues to pose a chal-
lenging problem. It is possible that using a com-
bination of GNAT and other gene NER tools 
would improve the overall gene NER accuracy. 
We further explored the effects of "perfect" 
gene NER on the accuracy of GETM by using 
the manual gene mention annotations supplied in 
the BioNLP corpus. Using the pre-annotated 
gene names increased the number of gene ex-
pression mentions recognized and the number of 
true positives, significantly improving recall 
(from 23.8% to 37.8%; data not shown). Howev-
er, a number of additional false positives were 
also introduced, causing precision to decrease 
very slightly from 58.8% to 58.5% (data not 
shown). This demonstrates the complexity of 
gene expression mentions in text, indicating that 
a combination of accurate trigger detection, ac-
curate NER (for both genes and anatomical loca-
tions) and deeper NLP methods are needed in 
order to accurately capture gene expression pro-
files in text. 
A secondary cause of false negatives was a 
relatively high number of annotated corpus en-
tries that spanned several sentences. The high 
proportion (23%) of multi-sentence entries in our 
extended corpus differs from previously reported 
results. For the event annotations in the BioNLP 
corpus, previous analyses showed that only 5% 
of all entries spanned several sentences (Bj?rne 
et al, 2009). This suggests that the mentions of 
anatomical locations are located outside of the 
"trigger sentence" more often than gene mentions 
or other entities in the BioNLP corpus. 
6 Conclusions 
In this paper, we have explored integrated min-
ing of gene expression mentions and their ana-
tomical locations from the literature and pre-
sented a new tool, GETM, which can be used to 
extract information about the expression of genes 
and where they are expressed from biomedical 
text. We have also extended part of a previously 
existing gold-standard corpus in order to allow 
evaluation of GETM. When evaluated against 
the gold-standard corpus, GETM performed with 
precision and recall levels of 58.8% and 23.8%, 
respectively.  
The relatively low level of recall was primari-
ly caused by incomplete recognition of individu-
al entities, indicating that ? in order to increase 
the recall of GETM ? future work would primari-
ly need to focus on increasing the accuracy of the 
NER methods. With more accurate NER, while 
increasing recall, the higher number of recog-
nized entities is also expected to increase the 
number of false positives, causing a need for 
deeper NLP methods in order to preserve and 
increase the level of precision. 
While having a low level of recall, GETM was 
nonetheless able to extract 716,541 statements 
from MEDLINE and PMC, constituting a large 
and potentially useful data set for researchers 
wishing to get an overview of gene expression 
for a particular gene or anatomical location. The 
high number of mentions extracted from MED-
LINE can give an indication of the amount of 
data available in MEDLINE: if the recall on the 
BioNLP corpus is representative for MEDLINE 
as a whole, a tool with perfect accuracy might be 
able to extract almost 2.5 million entries. 
The level of precision (p = 58.8%) will most 
likely not be high enough for researchers to rely 
on the extracted data for high-throughput bioin-
formatical experiments without some kind of 
verification. However, we believe that it none-
theless will be of high enough quality that re-
searchers and curators will not feel inconve-
nienced by false positives, as currently the only 
alternatives are multi-word free text searches 
through PubMed or Google. Additionally, we  
provide an interface with the text context sur-
rounding gene expression statements, making it 
easier for researchers to quickly locate relevant 
results. 
In the future, we will aim to evaluate the nor-
malization of entities detected by GETM in order 
to quantify the level to which the identifiers as-
signed to the entities are correct. In addition, 
78
  
both the gene and anatomical NER components 
could be improved in order to both reduce the 
number of false negatives and cover gene and 
anatomical terms for a wider range of species, 
beyond the common model organisms. We also 
believe that extending this work by utilizing dee-
per NLP methods (e.g. dependency parsers) 
could further improve the accuracy of GETM 
and related approaches to mining the abundance 
of data on gene expression in the biomedical lite-
rature. 
Acknowledgements 
We thank J?rg Hakenberg (Arizona State Uni-
versity) for providing access to GNAT. We also 
thank members of the Bergman and Nenadic 
groups for helpful comments and suggestions 
throughout the project, and three anonymous re-
viewers of this article for valuable comments that 
helped improve the manuscript. This work was 
funded by the University of Manchester and a 
BBSRC CASE studentship (to M.G.). 
References 
Barrett, T., Troup, D. B., Wilhite, S. E., Ledoux, P., 
Rudnev, D., Evangelista, C., Kim, I. F., Soboleva, 
A., Tomashevsky, M., Marshall, K. A., Phillippy, 
K. H., Sherman, P. M., Muertter, R. N. and Edgar, 
R. (2009). "NCBI GEO: archive for high-
throughput functional genomic data." Nucleic Ac-
ids Res 37(Database issue): D885-90. 
Bj?rne, J., Heimonen, J., Ginter, F., Airola, A., 
Pahikkala, T. and Salakoski, T. (2009). "Extracting 
complex biological events with rich graph-based 
feature sets." In Proceedings of the Workshop on 
BioNLP: Shared Task  Boulder, Colorado: 10-18. 
Bonig, H., Korholz, D., Pafferath, B., Mauz-Korholz, 
C. and Burdach, S. (1996). "Interleukin 10 induced 
c-fos expression in human B cells by activation of 
divergent protein kinases." Immunol Invest 25(1-
2): 115-28. 
Chintapalli, V. R., Wang, J. and Dow, J. A. T. (2007). 
"Using FlyAtlas to identify better Drosophila mod-
els of human disease." Nature Genetics 39: 715-
720. 
Chowdhary, R., Zhang, J. and Liu, J. S. (2009). 
"Bayesian inference of protein-protein interactions 
from biological literature." Bioinformatics 25(12): 
1536-42. 
Donaldson, I., Martin, J., de Bruijn, B., Wolting, C., 
Lay, V., Tuekam, B., Zhang, S., Baskin, B., Bader, 
G. D., Michalickova, K., Pawson, T. and Hogue, C. 
W. (2003). "PreBIND and Textomy--mining the 
biomedical literature for protein-protein interac-
tions using a support vector machine." BMC Bioin-
formatics 4: 11. 
Fundel, K. (2007). Text Mining and Gene Expression 
Analysis Towards Combined Interpretation of 
High Throughput Data. Dissertation. Faculty of 
Mathematics, Computer Science and Statistics. 
M?nchen, Ludwig-Maximilians Universit?t. 
Gerner, M., Nenadic, G. and Bergman, C. M. (2010). 
"LINNAEUS: a species name identification system 
for biomedical literature." BMC Bioinformatics 11: 
85. 
Haendel, M. A., Gkoutos, G. V., Lewis, S. E. and 
Mungall, C. J. (2009). "Uberon: towards a compre-
hensive multi-species anatomy ontology." In Inter-
national Conference on Biomedical Ontology Buf-
falo, NY. 
Hakenberg, J., Plake, C., Leaman, R., Schroeder, M. 
and Gonzales, G. (2008). "Inter-species normaliza-
tion of gene mentions with GNAT." Bioinformatics 
24(16): i126-i132. 
Hirschman, L., Yeh, A., Blaschke, C. and Valencia, 
A. (2005). "Overview of BioCreAtIvE: critical as-
sessment of information extraction for biology." 
BMC Bioinformatics 6 Suppl 1: S1. 
Kim, J. D., Ohta, T., Pyysalo, S., Kano, Y. and Tsujii, 
J. i. (2009). "Overview of BioNLP?09 Shared Task 
on Event Extraction." In Proceedings of the Work-
shop on BioNLP: Shared Task, Boulder, Colorado, 
Association for Computational Linguistics: 1-9. 
Kim, J. D., Ohta, T. and Tsujii, J. (2008). "Corpus 
annotation for mining biomedical events from lite-
rature." BMC Bioinformatics 9: 10. 
Morgan, A., Lu, Z., Wang, X., Cohen, A., Fluck, J., 
Ruch, P., Divoli, A., Fundel, K., Leaman, R., Ha-
kenberg, J., Sun, C., Liu, H., Torres, R., Krau-
thammer, M., Lau, W., Liu, H., Hsu, C., Schuemie, 
M., Cohen, K. and Hirschman, L. (2008). "Over-
view of BioCreative II gene normalization." Ge-
nome Biology 9(Suppl 2): S3. 
Mungall, C. J., Gkoutos, G. V., Smith, C. L., Haendel, 
M. A., Lewis, S. E. and Ashburner, M. (2010). "In-
tegrating phenotype ontologies across multiple 
species." Genome Biol 11(1): R2. 
Natarajan, J., Berrar, D., Dubitzky, W., Hack, C., 
Zhang, Y., DeSesa, C., Van Brocklyn, J. R. and 
Bremer, E. G. (2006). "Text mining of full-text 
journal articles combined with gene expression 
analysis reveals a relationship between sphingo-
sine-1-phosphate and invasiveness of a glioblasto-
ma cell line." BMC Bioinformatics 7: 373. 
Ohta, T., Kim, J.-D., Pyysalo, S., Wang, Y. and Tsu-
jii, J. i. (2009). "Incorporating GENETAG-style 
annotation to GENIA corpus." In Workshop on Bi-
oNLP, Boulder, Colorado: 106-107. 
79
  
Plake, C., Schiemann, T., Pankalla, M., Hakenberg, J. 
and Leser, U. (2006). "AliBaba: PubMed as a 
graph." Bioinformatics 22(19): 2444-5. 
Rebholz-Schuhmann, D., Arregui, M., Gaudan, M., 
Kirsch, H. and Jimeno, A. (2007). "Text processing 
through Web services: Calling Whatizit." Bioin-
formatics 23(2): e237-e244. 
Romano, P., Manniello, A., Aresu, O., Armento, M., 
Cesaro, M. and Parodi, B. (2009). "Cell Line Data 
Base: structure and recent improvements towards 
molecular authentication of human cell lines." 
Nucl. Acids Res. 37(suppl_1): D925-932. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Schwartz, A. S. and Hearst, M. A. (2003). "A simple 
algorithm for identifying abbreviation definitions 
in biomedical text." Pac Symp Biocomput: 451-62. 
Smith, B., Ashburner, M., Rosse, C., Bard, J., Bug, 
W., Ceusters, W., Goldberg, L. J., Eilbeck, K., 
Ireland, A., Mungall, C. J., Leontis, N., Rocca-
Serra, P., Ruttenberg, A., Sansone, S. A., 
Scheuermann, R. H., Shah, N., Whetzel, P. L. and 
Lewis, S. (2007). "The OBO Foundry: coordinated 
evolution of ontologies to support biomedical data 
integration." Nat Biotechnol 25(11): 1251-5. 
80
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 78?85,
Uppsala, July 2010.
Using SVMs with the Command Relation Features to  
Identify Negated Events in Biomedical Literature  
Farzaneh Sarafraz 
School of Computer Science  
University of Manchester 
Manchester, United Kingdom 
sarafraf@cs.man.ac.uk 
Goran Nenadic 
School of Computer Science  
University of Manchester 
Manchester, United Kingdom 
g.nenadic@manchester.ac.uk 
  
 
 
Abstract 
In this paper we explore the identification of 
negated molecular events (e.g. protein binding, 
gene expressions, regulation, etc.) in biomedi-
cal research abstracts. We construe the prob-
lem as a classification task and apply a ma-
chine learning (ML) approach that uses lexi-
cal, syntactic, and semantic features associated 
with sentences that represent events. Lexical 
features include negation cues, whereas syn-
tactic features are engineered from constitu-
ency parse trees and the command relation be-
tween constituents. Semantic features include 
event type and participants. We also consider a 
rule-based approach that uses only the com-
mand relation. On a test dataset, the ML ap-
proach showed significantly better results 
(51% F-measure) compared to the command-
based rules (35-42% F-measure). Training a 
separate classifier for each event class proved 
to be useful, as the micro-averaged F-score 
improved to 63% (with 88% precision), dem-
onstrating the potential of task-specific ML 
approaches to negation detection. 
1 Introduction 
With almost 2000 new papers published every 
day, biomedical knowledge is mainly communi-
cated through a growing body of research papers. 
As the amount of textual information increases, 
the need for sophisticated information extraction 
(IE) methods are becoming more than evident. IE 
methods rely on a range of language processing 
methods such as named entity recognition and 
parsing to extract the required information in a 
more structured form which can be used for 
knowledge exploration and hypothesis genera-
tion (Donaldson et al 2003; Natarajan et al 
2006). 
Given the large number of publications, the 
identification of conflicting or contradicting facts 
is critical for systematic mining of biomedical 
literature and knowledge consolidation. Detec-
tion of negations is of particular importance for 
IE methods, as it often can hugely affect the 
quality of the extracted information. For exam-
ple, when mining molecular events, a key piece 
of information is whether the text states that the 
two proteins are or are not interacting, or that a 
given gene is or is not expressed. In recent years, 
several challenges and shared tasks have in-
cluded the extraction of negations, typically as 
part of other tasks (e.g. the BioNLP?09 Shared 
Task 3 (Kim et al 2009)). 
Several systems and methods have aimed to 
handle negation detection in order to improve the 
quality of extracted information (Hakenberg et 
al. 2009; Morante and Daelemans 2009). Prior 
research on this topic has primarily focused on 
finding negated concepts by negation cues and 
scopes. These concepts are usually represented 
by a set of predefined terms, and negation detec-
tion typically aims to determine whether a term 
falls within the scope of a negation cue. 
In this paper we address the task of identifi-
cation of negated events. We present a machine 
learning (ML) method that combines a set of fea-
tures mainly engineered from a sentence parse 
tree with lexical cues. More specifically, parse-
based features use the notion of the command 
relation that models the scope affected by an 
element (Langacker, 1969). We use molecular 
events as a case study and experiment on the 
BioNLP?09 data, which comprises a gold-
standard corpus of research abstracts manually 
annotated for events and negations (Kim et al 
2009). The evaluation shows that, by using the 
proposed approach, negated events can be identi-
fied with precision of 88% and recall of 49% 
(63% F-measure). We compare these results with 
two rule-based approaches that achieved the 
maximum F-measure of 42%. 
78
The rest of this paper is organised as follows. 
Section 2 summarises and reviews previous re-
search on negation extraction. Section 3 defines 
the problem and introduces the data used for the 
case study. Section 4 focuses on the ML-based 
methodology for extracting negated events. The 
final sections contain the results and discussions. 
2 Related Work 
There have been numerous contemplations of the 
concept of negation (Lawler, 2010), but no gen-
eral agreement so far exists on its definition, 
form, and function. We adopt here a definition of 
negation as given by Cambridge Encyclopedia of 
Language Sciences: ?Negation is a comparison 
between a ?real? situation lacking some element 
and an ?imaginal? situation that does not lack it?. 
The imaginal situation is affirmative compared 
with the negative real situation. The element 
whose polarity differs between the two situations 
is the negation target. 
Negations in natural language can be ex-
pressed by syntactically negative expressions, i.e. 
with the use of negating words such as no, not, 
never, etc. The word or phrase that makes the 
sentence wholly or partially negative is the nega-
tion cue and the part of the sentence that is af-
fected by the negation cue and has become nega-
tive is the negation scope.  
We briefly review two classes of approaches 
to detect negations: those aiming at negated con-
cepts and those targeting negated events.  
2.1 Detecting Negated Concepts and 
Phrases 
There have been a number of approaches sug-
gested for detection of negated targets and 
scopes. Most of them rely on task-specific, hand-
crafted rules of various complexities. They differ 
in the size and composition of the list of negation 
cues, and the way to utilise such a list. Some 
methods use parse trees, whilst others use results 
of shallow parsing. 
Rule-based methods range from simple co-
occurrence based approaches to patterns that rely 
on shallow parsing. The ?bag-of-words? ap-
proach, looking for proximate co-occurrences of 
negation cues and terms in the same sentence, is 
probably the simplest method for finding nega-
tions, and is used by many as a baseline method.  
Many approaches have targeted the clinical 
and biomedical domains. NegEx (Chapman et al 
2001), for example, uses two generic regular ex-
pressions that are triggered by negation phrases 
such as: 
 
<negation cue> * <target term> 
<target term> * <negation cue>  
 
where the asterisk (*) represents a string of up to 
five tokens. Target terms represent domain con-
cepts that are terms from the Unified Medical 
Language System (UMLS1). The cue set com-
prises 272 clinically-specific negation cues, in-
cluding those such as denial of or absence of. 
Although simple, the proposed approach showed 
good results on clinical data (78% sensitivity 
(recall), 84% precision, and 94% specificity). 
In addition to concepts that are explicitly ne-
gated by negation phrases, Patrick et al (2006) 
further consider so-called pre-coordinated nega-
tive terms (e.g. headache) that have been col-
lected from SNOMED CT2 medical terminology. 
Similarly, NegFinder uses hand-crafted rules to 
detect negated UMLS terms, including simple 
conjunctive and disjunctive statements (Mutalik 
et al 2001). They used a list of 60 negation cues. 
Tolentino et al (2006), however, show that using 
rules on a small set of only five negation cues 
(no, neither/nor, ruled out, denies, without) can 
still be reasonably successful in detecting nega-
tions in medical reports (F-score 91%). 
Huang and Lowe (2007) introduced a negation 
grammar that used regular expressions and de-
pendency parse trees to identify negation cues 
and their scope in the sentence. They applied the 
rules to a set of radiology reports and reported a 
precision of 99% and a recall of 92%. 
Not many efforts have been reported on using 
machine learning to detect patterns in sentences 
that contain negative expressions. Still, Morante 
and Daelemans (2009), for example, used vari-
ous classifiers (Memory-based Learners, Support 
Vector Machines, and Conditional Random 
Fields) to detect negation cues and their scope. 
An extensive list of features included the token?s 
stem and part-of-speech, as well as those of the 
neighbouring tokens. Separate classifiers were 
used for detecting negation cues and negation 
scopes. The method was applied to clinical text, 
biomedical abstracts, and biomedical papers with 
F-scores of 80%, 77%, and 68% respectively. 
2.2 Detecting Negated Events 
Several approaches have recently been suggested 
for the extraction of negated events, particularly 
                                                 
1
   http://www.nlm.nih.gov/research/umls/ 
2
   http://www.snomed.org 
79
in the biomedical domain. Events are typically 
represented via participants (biomedical entities 
that take part in an event) and event triggers (to-
kens that indicate presence of the event). Van 
Landeghem et al (2008) used a rule-based ap-
proach based on token distances in sentence and 
lexical information in event triggers to detect 
negated molecular events. Kilicoglu and Bergler 
(2009), Hakenberg et al (2009), and Sanchez 
(2007) used a number of heuristic rules concern-
ing the type of the negation cue and the type of 
the dependency relation to detect negated mo-
lecular events described in text. For example, a 
rule can state that if the negation cue is ?lack? or 
?absence?, then the trigger has to be in the 
prepositional phrase of the cue; or that if the cue 
is ?unable? or ?fail?, then the trigger has to be in 
the clausal complement of the cue (Kilicoglu and 
Bergler 2009). As expected, such approaches 
suffer from lower recall. 
MacKinlay et al (2009), on the other hand, 
use ML, assigning a vector of complex deep 
parse features (including syntactic predicates to 
capture negation scopes, conjunctions and se-
mantically negated verbs) to every event trigger. 
The system achieved an F-score of 36% on the 
same dataset as used in this paper. 
We note that the methods mentioned above 
mainly focus on finding negated triggers in order 
to detect negated events. In this paper we explore 
not only negation of triggers but also phrases in 
which participants are negated (consider, for ex-
ample, ?SLP-76? in the sentence ?In contrast, 
Grb2 can be coimmunoprecipitated with Sos1 
and Sos2 but not with SLP-76.?) 
3 Molecular Events  
As a case study, we look at identification of ne-
gated molecular events. In general, molecular 
events include various types of reactions that 
affect genes and protein molecules. Each event is 
of a particular type (e.g. binding, phosphoryla-
tion, regulation, etc.). Depending on the type, 
each event may have one or more participating 
proteins (sometimes referred to as themes). 
Regulatory events are particularly complex, as 
they can have a cause (a protein or another 
event) in addition to a theme, which can be either 
a protein or another event. Table 1 shows exam-
ples of five events, where participants are bio-
medical entities (events 1-3) or other events 
(events 4 and 5). Note that a sentence can ex-
press more than one molecular event. 
Identification of molecular events in the litera-
ture is a challenging IE task (Kim et al 2009; 
Sarafraz et al 2009). For the task of identifying 
negated events, we assume that events have al-
ready been identified in text. Each event is repre-
sented by its type, a textual trigger, and one or 
more participants or causes (see Table 1). Since 
the participants of different event types can vary 
in both their number and type, we consider three 
classes of events to support our analysis (see 
Section 5):  
? Class I comprises events with exactly one 
entity theme (e.g. transcription, protein ca-
tabolism, localization, gene expression, 
phosphorylation). 
? Class II events include binding events only, 
which have one or more entity participants. 
? Class III contains regulation events, which 
have exactly one theme and possibly one 
cause. However, the theme and the cause can 
be entities or events of any type.  
 
The corpus used in this study is provided by 
the BioNLP?09 challenge (Kim et al 2009). It 
contains two sets of biomedical abstracts: a 
?training? set (containing 800 abstracts used for 
training and analysis purposes) and a ?develop-
ment? set (containing 150 abstracts used for test-
ing purposes only). Both document sets are 
manually annotated with information about en-
tity mentions (e.g. genes and proteins). Sentences 
that report molecular events are further annotated 
with the corresponding event type, textual trigger 
and participants. In total, nine event types are 
?The effect of this synergism was perceptible at the level of induction of the IL-2 gene.? 
Event Trigger Type Participant (theme) Cause 
Event 1 ?induction? Gene expression IL-2  
     
?Overexpression of full-length ALG-4 induced transcription of FasL and, consequently, apoptosis.? 
Event Trigger Type Participant (theme) Cause 
Event 2 ?transcription? Transcription FasL  
Event 3 ?Overexpression? Gene expression ALG-4  
Event 4 ?Overexpression? Positive regulation Event 3  
Event 5 ?induced? Positive regulation Event 2 Event 4 
Table 1: Examples of how molecular events described in text are characterised. 
 
80
considered (gene expression, transcription, pro-
tein catabolism, localization, phosphorylation, 
binding, regulation, positive regulation, and 
negative regulation). In addition, every event has 
been tagged as either affirmative (reporting a 
specific interaction) or negative (reporting that a 
specific interaction has not been observed).   
Table 2 provides an overview of the two 
BioNLP?09 datasets. We note that only around 
6% of events are negated. 
 
Training  
data 
Development 
data Event  
class total negated total  negated 
Class I 2,858 131 559 26
Class II 887 44 249 15
Class III 4,870 440 987 66
Total 9,685 615 1,795 107
 
Table 2: Overview of the total number of events and 
negated event annotations in the two datasets. 
4 Methodology  
We consider two approaches to extract negated 
events. We first discuss a rule-based approach 
that uses constituency parse trees and the com-
mand relation to identify negated events. Then, 
we introduce a ML method that combines lexi-
cal, syntactic and semantic features to identify 
negated events. Note that in all cases, input sen-
tences have been pre-annotated for entity men-
tions, event triggers, types, and participants. 
4.1 Negation Detection Using the Command 
Relation Rules 
The question of which parts of a syntactic struc-
ture affect the other parts has been extensively 
investigated. Langacker (1969) introduced the 
concept of command to determine the scope 
within a sentence affected by an element. More 
precisely, if a and b are nodes in the constituency 
parse tree of a sentence, then a X-commands b 
iff the lowest ancestor of a with label X is also 
an ancestor of b. Note that the command relation 
is not symmetrical. Langacker observed that 
when a S-commands b, then a affects the scope 
containing b. For simplicity, we say ?command? 
when we mean S-command.  
To determine whether token a commands to-
ken b, given the parse tree of a sentence, we use 
a simple algorithm introduced by McCawley 
(1993): trace up the branches of the constituency 
parse tree from a until you hit a node that is la-
belled X. If b is reachable by tracing down the 
branches of the tree from that node, then a X-
commands b; otherwise, it does not. 
We hypothesise that if a negation cue com-
mands an event trigger or participant, then the 
associated event is negated. 
4.2 Negation Detection Using Machine 
Learning on Parse Tree Features 
Given a sentence that describes an event, we fur-
ther construe the negation detection problem as a 
classification task: the aim is to classify the event 
as affirmative or negative. We explore both a 
single SVM (support vector machine) classifier 
for all events and three separate SVMs for each 
of the event classes. The following features have 
been engineered from an event-representing sen-
tence: 
 
1. Event type (one of the nine types as defined 
in BioNLP?09); 
2. Whether the sentence contains a negation 
cue from the cue list; 
3. The negation cue itself (if present); 
4. The part-of-speech (POS) tag of the negation 
cue; 
5. The POS tag of the event trigger; 
6. The POS tag of the participants of the event. 
If the participant is another event, the POS 
tag of the trigger of that event is used; 
7. The parse node type of the lowest common 
ancestor of the trigger and the cue (i.e. the 
type of the smallest phrase that contains both 
the trigger and the cue, e.g. S, VP, PP, etc.); 
8. Whether or not the negation cue commands 
any of the participants; nested events (for 
Class III) are treated as above (i.e. as being 
represented by their triggers); 
9. Whether or not the negation cue commands 
the trigger; 
10. The parse-tree distance between the event 
trigger and the negation cue. 
 
We use a default value (null) where none of 
the other values apply (e.g. when there is no cue 
in feature 3, 4, 7). These features have been used 
to train four SVMs on the training dataset: one 
modelled all events together, and the others 
modelled the three event classes separately. 
5 Results 
All the results refer to the methods applied on the 
development dataset (see Table 2). If the nega-
tion detection task is regarded as an information 
extraction task of finding positive instances (i.e. 
81
negated events), then precision, recall, and F-
score would be appropriate measures. If we con-
sider the classification aspect of the task, speci-
ficity is more appropriate if true negative hits are 
considered as valuable as true positive ones. We 
therefore use the following metrics to evaluate 
the two methods: 
 
Precision= TPTP+FP   
 
Recall=Sensitivity= TPTP+FN  
 
F1= 2? Precision? Recall
Precision+Recall  
 
Specificity= TNTN+FP  
where TP denotes the number of true positives 
(the number of correctly identified negated 
events), FN is the number of false negatives (the 
number of negated events that have been re-
ported as affirmative), with TN and FP defined 
accordingly. 
Two sets of negation cues were used in order 
to compare their influence. A smaller set was 
derived from related work, whereas additional 
cues were semi-automatically extracted by ex-
ploring the training data. The small negation cue 
set contains 14 words3, whereas the larger nega-
tion cue set contains 32 words4. As expected, the 
larger set resulted in increased recall, but de-
creased precision. However, the effects on the F-
score were typically not significant. The results 
are only shown using the larger cue set. 
The texts were processed using the GENIA 
tagger (Tsuruoka and Tsujii 2005).We used con-
stituency parse trees automatically produced by 
two different constituency parsers reported in 
(McClosky et al 2006) and (Bikel 2004). No 
major differences were observed in the results 
using the two parsers. The data shown in the re-
sults are produced by the former.  
5.1 Baseline Results 
Our baseline method relies on an implementation 
of the NegEx algorithm as explained in Section 
2.1. Event triggers were used as negation targets 
for the algorithm. An event is then considered to 
be negated if the trigger is negated; otherwise it 
                                                 
3
 Negation cues in this set include: no, not, none, 
negative, without, absence, fail, fails, failed, fail-
ure, cannot, lack, lacking, lacked. 
4
 Negation cues in this set include the smaller set and 
18 task-specific words: inactive, neither, nor, in-
hibit, unable, blocks, blocking, preventing, pre-
vents, absent, never, unaffected, unchanged, im-
paired, little, independent, except, and exception. 
is affirmative. The results (see Table 3) are sub-
stantially lower than those reported for NegEx on 
clinical data (specificity of 94% and sensitivity 
of 78%). For comparison, the table also provides 
an even simpler baseline approach that tags as 
negated any event whose associated sentence 
contains any negation cue word. 
 
Approach P R F1 Spec. 
any negation cue present 20% 78% 32% 81% 
NegEx 36% 37% 36% 93% 
 
Table 3: Baseline results. 
(NegEx and a ?bag-of-words? approach) 
5.2 Rules Based on the Command Relation 
Table 4 shows the results of applying the S-
command relation rule for negation detection. 
We experimented with three possible ap-
proaches: an event is considered negated if  
- the negation cue commands any event 
participant in the parse tree;  
- the negation cue commands the event 
trigger in the tree; 
- the negation cue commands both. 
 
Approach P R F1 Spec. 
negation cue commands 
any participant 23% 76% 35% 84% 
negation cue  
commands trigger 23% 68% 34% 85% 
negation cue  
commands both 23% 68% 35% 86% 
 
Table 4: Performance when only the S-command  
relation is used. 
 
Compared with the baseline methods, the rules 
based on the command relation did not improve 
the performance. While precision was low 
(23%), recall was high (around 70%), indicating 
that in the majority of cases there is an S-
command relation in particular with the partici-
pants (the highest recall). We also note a signifi-
cant drop in specificity, as many affirmative 
events have triggers/participants S-commanded 
by a negation cue (not ?linked? to a given event). 
5.3 Machine Learning Results 
All SVM classifiers have been trained on the 
training dataset using a Python implementation 
of SVM Light using the linear kernel and the 
default parameters (Joachims 1999). Table 5 
shows the results of the single SVM classifier 
that has been trained for all three event classes 
together (applied on the development data). 
82
Compared to previous methods, there was sig-
nificant improvement in precision, while recall 
was relatively low. Still, the overall F-measure 
was significantly better compared with the rule-
based methods (51% vs. 35%). 
 
Feature set P R F1 Spec. 
Features 1-7 43% 8% 14% 99.2% 
Features 1-8 73% 19% 30% 99.3% 
Features 1-9 71% 38% 49% 99.2% 
Features 1-10 76% 38% 51% 99.2% 
 
Table 5: The results of the single SVM classifier. Fea-
tures 1-7 are lexical and POS tag-based features. Fea-
ture 8 models whether the cue S-commands any of the 
participants. Feature 9 is related to the cue S-
commanding the trigger. Feature 10 is the parse-tree 
distance between the cue and trigger.  
 
We first experimented with the effect of differ-
ent types of feature on the quality of the negation 
prediction. Table 5 shows the results of the first 
classifier with an incremental addition of lexical 
features, parse tree-related features, and finally a 
combination of those with the command relation 
between the negation cue and event trigger and 
participants. It is worth noting that both precision 
and recall improved as more features are added. 
We also separately trained classifiers on the 
three classes of events (see Table 6). This further 
increased the performance: compared with the 
results of the single classifier, the F1 micro-
average improved from 51% to 63%, with simi-
lar gains for both precision and recall. 
 
Event class P R F1 Spec. 
Class I 
(559 events) 94% 65% 77% 99.8% 
Class II 
(249 events) 100% 33% 50% 100% 
Class III 
(987 events) 81% 44% 57% 99.2% 
Micro Average 
(1,795 events) 88% 49% 63% 99.4% 
Macro Average 
(3 classes) 92% 47% 62% 99.7% 
 
Table 6: The results of the separate classifiers on dif-
ferent classes using common features. 
6 Discussion 
As expected, approaches that focus only on event 
triggers and their surface distances from negation 
cues proved inadequate for biomedical scientific 
articles. Low recall was mainly caused by many 
event triggers being too far from the negation cue 
to be detected as within the scope. 
Furthermore, compared to clinical notes, for 
example, sentences that describe molecular 
events are significantly more complex. For ex-
ample, the event-describing sentences in the 
training data have on average 2.6 event triggers. 
The number of events per sentence is even 
higher, as the same trigger can indicate multiple 
events, sometimes with opposite polarities. Con-
sider for example the sentence 
 
?We also demonstrate that the IKK complex, 
but not p90 (rsk), is responsible for the in vivo 
phosphorylation of I-kappa-B-alpha mediated 
by the co-activation of PKC and calcineurin.? 
 
Here, the trigger (phosphorylation) is linked with 
one affirmative and one negative regulatory 
event by two different molecules, hence trigger-
ing two events of opposite polarities. 
These findings, together with previous work, 
suggested that for any method to effectively de-
tect negations, it should be able to link the nega-
tion cue to the specific token, event trigger or 
entity name in question. Therefore, more com-
plex models are needed to capture the specific 
structure of the sentence as well as the composi-
tion of the interaction and the arrangement of its 
trigger and participants. 
By combining several feature types (lexical, 
syntactic and semantic), the machine learning 
approach proved to provide significantly better 
results. In the incremental feature addition explo-
ration process, adding the cue-commands-
participant feature had the greatest effect on the 
F-score, suggesting the significance of treating 
event participants. We note, however, that many 
of the previous attempts focus on event triggers 
only, although participants do play an important 
role in the detection of negations in biomedical 
events and thus should be used as negation tar-
gets instead of or in addition to triggers. It is in-
teresting that adding parse-tree distance between 
the trigger and negation cue improves precision 
by 5%. 
Differences in event classes (in the number 
and type of participants) proved to be important. 
Significant improvement in performance was 
observed when individual classifiers were trained 
for the three event classes, suggesting that events 
with different numbers or types of participants 
are expressed differently in text, at least when 
negations are considered. Class I events are the 
simplest (one participant only), so it was ex-
pected that negated events in this class would be 
83
the easiest to detect (F-score of 77%). Class II 
negated events (which can have multiple partici-
pants), demonstrated the lowest recall (33%). A 
likely reason is that the feature set used is not 
suitable for multi-participant events: for exam-
ple, feature 8 focuses on the negation cue com-
manding any of the participants, and not all of 
them. It is surprising that negated regulation 
events (Class III) were not the most difficult to 
identify, given their complexity. 
We applied the negation detection on the 
type, trigger and participants of pre-identified 
events in order to explore the complexity of ne-
gations, unaffected by automatic named entity 
recognition, event trigger detection, participant 
identification, etc. As these steps are typically 
performed before further characterisation of 
events, this assumption is not superficial and 
such information can be used as input to the ne-
gation detection module. MacKinlay et al (2009) 
also used gold annotations as input for negation 
detection, and reported precision, recall, and F-
score of 68%, 24%, and 36% respectively on the 
same dataset (compared to 88%, 49% and 63% 
in our case). The best performing negation detec-
tion approach in the BioNLP?09 shared task re-
ported recall of up to 15%, but with overall event 
detection sensitivity of 33% (Kilicoglu and Ber-
gler 2009) on a ?test? dataset (different from that 
used in this study). This makes it difficult to di-
rectly compare their results to our work, but we 
can still provide some rough estimates: had all 
events been correctly identified, their negation 
detection approach could have reached 45% re-
call (compared to 49% in our case). With preci-
sion of around 50%, their projected F-score, 
again assuming perfect event identification, 
could have been in the region of 50% (compared 
to 63% in our case). 
The experiments with rules that were based 
on the command relations have proven to be ge-
neric, providing very high recall (~70%) but with 
poor precision. Although only the results with S-
command relations have been reported here (see 
Table 4), we examined other types of command 
relation, namely NP-, PP-, SBAR-, and VP-
command. The only variation able to improve 
prediction accuracy was whether the cue VP-
commands any of the participants, with an F-
score of 42%, which is higher than the results 
achieved by the S-command (F-score of 35%). 
The S-command relation was used in the SVM 
modules as VP-command did not make the re-
sults significantly better. 
One of the issues we faced was the manage-
ment of multi-token and sub-token entities and 
triggers (e.g. alpha B1 and alpha B2 in ?alpha 
B1/alpha B2 ratio?, which will be typically to-
kenised as ?alpha?, ?B1/alpha?, and ?B2?). In 
our approach, we considered all the entities that 
are either multi-token or sub-token. However, if 
we assign participants that are both multi-token 
and sub-token simultaneously to events and ex-
tract similar features for the classifier from them 
as from simple entities, the F-score is reduced by 
about 2%. It would be probably better to assign a 
new category to those participants and add a new 
value for them specifically in every feature. 
7 Conclusions 
Given the number of published articles, detection 
of negations is of particular importance for bio-
medical IE. Here we explored the identification 
of negated molecular events, given their triggers 
(to characterise event type) and participants. We 
considered two approaches: 5  a rule-based ap-
proach using constituency parse trees and the 
command relation to identify negation cues and 
scopes, and a machine learning method that 
combines a set of lexical, syntactic and semantic 
features engineered from the associated sentence. 
When compared with a regular-expression-based 
baseline method (NegEx-like), the proposed ML 
method achieved significantly better results: 63% 
F-score with 88% precision. The best results 
were obtained when separate classifiers were 
trained for each of the three event classes, as dif-
ferences between them (in the number and type 
of participants) proved to be important. 
The results presented here were obtained by 
using the ?gold? event annotations as the input. It 
would be interesting to explore the impact of 
typically noisy automatic event extraction on 
negation identification. Furthermore, an immedi-
ate future step would be to explore class-specific 
features (e.g. type of theme and cause for Class 
III events, and whether the cue S-commands all 
participants for Class II events). In addition, in 
the current approach we used constituency parse 
trees. Our previous attempts to identify molecu-
lar events (Sarafraz et al 2009) as well as those 
discussed in Section 2 use dependency parse 
trees. A topic open for future research will be to 
combine information from both dependency and 
constituency parse trees as features for detecting 
negated events. 
                                                 
5
 Available at http://bit.ly/bzBaUX 
84
Acknowledgments 
We are grateful to the organisers of BioNLP?09 
for providing the annotated data. 
References 
Daniel Bikel. 2004. A Distributional Analysis of a 
Lexicalized Statistical Parsing. Proc. Conference 
on Empirical Methods in Natural Language. 
Wendy Chapman. 2001. A Simple Algorithm for 
Identifying Negated Findings and Diseases in Dis-
charge Summaries. Journal of Biomedical Infor-
matics, 34(5):301-310. 
Ian Donaldson, Martin, J., de Bruijn, B., Wolting, C., 
Lay, V., Tuekam, B., Zhang, S., Baskin, B., Bader, 
G. D., Michalickova, K., Pawson, T. and Hogue, C. 
W. 2003. PreBIND and Textomy--mining the bio-
medical literature for protein-protein interactions 
using a support vector machine. BMC Bioinf. 4: 11. 
J?rg Hakenberg, Ill?s Solt, Domonkos Tikk, Luis 
Tari, Astrid Rheinl?nder, Quang L. Ngyuen, 
Graciela Gonzalez and Ulf Leser. 2009. Molecular 
event extraction from link grammar parse trees. 
BioNLP?09: Proceedings of the Workshop on 
BioNLP. 86-94. 
Yang Huang and Henry J. Lowe. 2007. A Novel Hy-
brid Approach to Automated Negation Detection in 
Clinical Radiology Reports. Journal of the Ameri-
can Medical Informatics Association, 14(3):304-
311. 
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.) MIT-Press, MA. 
Halil Kilicoglu, and Sabine Bergler. 2009. Syntactic 
dependency based heuristics for biological event 
extraction. BioNLP?09: Proceedings of the Work-
shop on BioNLP. 119-127. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yo-
shinobu Kano, Jun?ichi Tsujii. 2009. Overview of 
BioNLP?09 shared task on event extraction. 
BioNLP?09: Proceedings of the Workshop on 
BioNLP. 1-9. 
Sofie Van Landeghem, Yvan Saeys, Bernard De 
Baets and Yves Van de Peer. 2008. Extracting Pro-
tein-Protein Interactions from Text using Rich Fea-
ture Vectors and Feature Selection. Proceedings of 
the Third International Symposium on Semantic 
Mining in Biomedicine. 77-84. 
Ronald Langacker. 1969. On Pronominalization and 
the Chain of Command. In D. Reibel and S. Schane 
(eds.), Modern Studies in English, Prentice-Hall, 
Englewood Cliffs, NJ. 160?186. 
John Lawler. 2010. Negation and Negative Polarity. 
The Cambridge Encyclopedia of the Language Sci-
ences. Patrick Colm Hogan (ed.) Cambridge Uni-
versity Press. Cambridge, UK. 
Andrew MacKinlay, David Martinez and Timothy 
Baldwin. 2009. Biomedical Event Annotation with 
CRFs and Precision Grammars. BioNLP?09: Pro-
ceedings of the Workshop on BioNLP. 77-85. 
James McCawley. 1993. Everything that Linguists 
have Always Wanted to Know about Logic But 
Were Ashamed to Ask. 2nd edition. The University 
of Chicago Press. Chicago, IL. 
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective Self-Training for Parsing. 
Proceedings of HLT/NAACL 2006. 152-159. 
Roser Morante and Walter Daelemans. 2009. A 
Metalearning Approach to Processing the Scope of 
Negation. CoNLL ?09: Proceedings of the 13th 
Conference on Computational Natural Language 
Learning. 21-29. 
Pradeep Mutalik, Aniruddha Deshpande, and Prakash 
M. Nadkarni. 2001. Use of General-purpose Nega-
tion Detection to Augment Concept Indexing of 
Medical Documents: A Quantitative Study using 
the UMLS. Journal of the American Medical In-
formatics Association : JAMIA. 8(6):598-609. 
Jeyakumar Natarajan, Berrar, D., Dubitzky, W., Hack, 
C., Zhang, Y., DeSesa, C., Van Brocklyn, J. R. and 
Bremer, E.G. 2006. Text mining of full-text journal 
articles combined with gene expression analysis 
reveals a relationship between sphingosine-1-
phosphate and invasiveness of a glioblastoma cell 
line. BMC Bioinformatics. 7: 373. 
Jon Patrick, Yefeng Wang, and Peter Budd. 2006. 
Automatic Mapping Clinical Notes to Medical 
Terminologies. Proc. Of the 2006 Australian Lan-
guage Technology Workshop. 75-82. 
Olivia Sanchez. 2007. Text mining applied to biologi-
cal texts: beyond the extraction of protein-protein 
interactions. PhD Thesis.  
Farzaneh Sarafraz, James Eales, Reza Mohammadi, 
Jonathan Dickerson, David Robertson and Goran 
Nenadic. 2009. Biomedical Event Detection using 
Rules, Conditional Random Fields and Parse Tree 
Distances. BioNLP?09: Proceedings of the Work-
shop on BioNLP. 
Herman Tolentino, Michael Matters, Wikke Walop, 
Barbara Law, Wesley Tong, Fang Liu, Paul Fon-
telo, Katrin Kohl, and Daniel Payne. 2006. Concept 
Negation in Free Text Components of Vaccine 
Safety Reports. AMIA Annual Symposium proc. 
Yoshimasa Tsuruoka, and Jun?ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy 
for Tagging Sequence Data. Proceedings of 
HLT/EMNLP 2005, 467-474. 
85
