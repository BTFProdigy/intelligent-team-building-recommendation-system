Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228?232,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Unsupervised joke generation from big data
Sas?a Petrovic?
School of Informatics
University of Edinburgh
sasa.petrovic@ed.ac.uk
David Matthews
School of Informatics
University of Edinburgh
dave.matthews@ed.ac.uk
Abstract
Humor generation is a very hard problem.
It is difficult to say exactly what makes a
joke funny, and solving this problem al-
gorithmically is assumed to require deep
semantic understanding, as well as cul-
tural and other contextual cues. We depart
from previous work that tries to model this
knowledge using ad-hoc manually created
databases and labeled training examples.
Instead we present a model that uses large
amounts of unannotated data to generate I
like my X like I like my Y, Z jokes, where
X, Y, and Z are variables to be filled in.
This is, to the best of our knowledge, the
first fully unsupervised humor generation
system. Our model significantly outper-
forms a competitive baseline and gener-
ates funny jokes 16% of the time, com-
pared to 33% for human-generated jokes.
1 Introduction
Generating jokes is typically considered to be a
very hard natural language problem, as it implies
a deep semantic and often cultural understanding
of text. We deal with generating a particular type
of joke ? I like my X like I like my Y, Z ? where X
and Y are nouns and Z is typically an attribute that
describes X and Y. An example of such a joke is
I like my men like I like my tea, hot and British ?
these jokes are very popular online.
While this particular type of joke is not interest-
ing from a purely generational point of view (the
syntactic structure is fixed), the content selection
problem is very challenging. Indeed, most of the
X, Y, and Z triples, when used in the context of
this joke, will not be considered funny. Thus, the
main challenge in this work is to ?fill in? the slots
in the joke template in a way that the whole phrase
is considered funny.
Unlike the previous work in humor generation,
we do not rely on labeled training data or hand-
coded rules, but instead on large quantities of
unannotated data. We present a machine learning
model that expresses our assumptions about what
makes these types of jokes funny and show that by
using this fairly simple model and large quantities
of data, we are able to generate jokes that are con-
sidered funny by human raters in 16% of cases.
The main contribution of this paper is, to the
best of our knowledge, the first fully unsupervised
joke generation system. We rely only on large
quantities of unlabeled data, suggesting that gener-
ating jokes does not always require deep semantic
understanding, as usually thought.
2 Related Work
Related work on computational humor can be di-
vided into two classes: humor recognition and hu-
mor generation. Humor recognition includes dou-
ble entendre identification in the form of That?s
what she said jokes (Kiddon and Brun, 2011),
sarcastic sentence identification (Davidov et al,
2010), and one-liner joke recognition (Mihalcea
and Strapparava, 2005). All this previous work
uses labeled training data. Kiddon and Brun
(2011) use a supervised classifier (SVM) trained
on 4,000 labeled examples, while Davidov et al
(2010) and Mihalcea and Strapparava (2005) both
use a small amount of training data followed by a
bootstrapping step to gather more.
Examples of work on humor generation include
dirty joke telling robots (Sjo?bergh and Araki,
2008), a generative model of two-liner jokes (Lab-
utov and Lipson, 2012), and a model of punning
riddles (Binsted and Ritchie, 1994). Again, all this
work uses supervision in some form: Sjo?bergh and
Araki (2008) use only human jokes collected from
various sources, Labutov and Lipson (2012) use a
supervised approach to learn feasible circuits that
connect two concepts in a semantic network, and
228
ZYX
?1(Z)
?(Y, Z)?(X, Z)
?(X, Y)
?2(Z)
Figure 1: Our model presented as a factor graph.
Binsted and Ritchie (1994) have a set of six hard-
coded rules for generating puns.
3 Generating jokes
We generate jokes of the form I like my X like I like
my Y, Z, and we assume that X and Y are nouns,
and that Z is an adjective.
3.1 Model
Our model encodes four main assumptions about
I like my jokes: i) a joke is funnier the more often
the attribute is used to describe both nouns, ii) a
joke is funnier the less common the attribute is, iii)
a joke is funnier the more ambiguous the attribute
is, and iv) a joke is funnier the more dissimilar
the two nouns are. A graphical representation of
our model in the form of a factor graph is shown
in Figure 1. Variables, denoted by circles, and fac-
tors, denoted by squares, define potential functions
involving the variables they are connected to.
Assumption i) is the most straightforward, and
is expressed through ?(X,Z) and ?(Y, Z) factors.
Mathematically, this assumption is expressed as:
?(x, z) = p(x, z) = f(x, z)?
x,z f(x, z)
, (1)
where f(x, z)1 is a function that measures the co-
occurrence between x and z. In this work we sim-
ply use frequency of co-occurrence of x and z in
some large corpus, but other functions, e.g., TF-
IDF weighted frequency, could also be used. The
same formula is used for ?(Y,Z), only with dif-
ferent variables. Because this factor measures the
1We use uppercase to denote random variables, and low-
ercase to denote random variables taking on a specific value.
similarity between nouns and attributes, we will
also refer to it as noun-attribute similarity.
Assumption ii) says that jokes are funnier if the
attribute used is less common. For example, there
are a few attributes that are very common and can
be used to describe almost anything (e.g., new,
free, good), but using them would probably lead
to bad jokes. We posit that the less common the
attribute Z is, the more likely it is to lead to sur-
prisal, which is known to contribute to the funni-
ness of jokes. We express this assumption in the
factor ?1(Z):
?1(z) = 1/f(z) (2)
where f(z) is the number of times attribute z ap-
pears in some external corpus. We will refer to this
factor as attribute surprisal.
Assumption iii) says that more ambiguous at-
tributes lead to funnier jokes. This is based on the
observation that the humor often stems from the
fact that the attribute is used in one sense when
describing noun x, and in a different sense when
describing noun y. This assumption is expressed
in ?2(Z) as:
?2(z) = 1/senses(z) (3)
where senses(z) is the number of different senses
that attribute z has. Note that this does not exactly
capture the fact that z should be used in different
senses for the different nouns, but it is a reason-
able first approximation. We refer to this factor as
attribute ambiguity.
Finally, assumption iv) says that dissimilar
nouns lead to funnier jokes. For example, if the
two nouns are girls and boys, we could easily find
many attributes that both nouns share. However,
since the two nouns are very similar, the effect of
surprisal would diminish as the observer would ex-
pect us to find an attribute that can describe both
nouns well. We therefore use ?(X,Y ) to encour-
age dissimilarity between the two nouns:
?(x, y) = 1/sim(x, y), (4)
where sim is a similarity function that measures
how similar nouns x and y are. We call this fac-
tor noun dissimilarity. There are many similar-
ity functions proposed in the literature, see e.g.,
Weeds et al (2004); we use the cosine between
the distributional representation of the nouns:
sim(x, y) =
?
z p(z|x)p(z|y)??
z p(z|x)2 ?
?
z p(z|y)2
(5)
229
Equation 5 computes the similarity between the
nouns by representing them in the space of all at-
tributes used to describe them, and then taking the
cosine of the angle between the noun vectors in
this representation.
To obtain the joint probability for an (x, y, z)
triple we simply multiply all the factors and nor-
malize over all the triples.
4 Data
For estimating f(x, y) and f(z), we use Google
n-gram data (Michel et al, 2010), in particular the
Google 2-grams. We tag each word in the 2-grams
with the part-of-speech (POS) tag that corresponds
to the most common POS tag associated with that
word in Wordnet (Fellbaum, 1998). Once we have
the POS-tagged Google 2-gram data, we extract
all (noun, adjective) pairs and use their counts to
estimate both f(x, z) and f(y, z). We discard 2-
grams whose count in the Google data is less than
1000. After filtering we are left with 2 million
(noun, adjective) pairs. We estimate f(z) by sum-
ming the counts of all Google 2-grams that con-
tain that particular z. We obtain senses(z) from
Wordnet, which contains the number of senses for
all common words.
It is important to emphasize here that, while we
do use Wordnet in our work, our approach does not
crucially rely on it, and we use it to obtain only
very shallow information. In particular, we use
Wordnet to obtain i) POS tags for Google 2-grams,
and ii) number of senses for adjectives. POS tag-
ging could be easily done using any one of the
readily available POS taggers, but we chose this
approach for its simplicity and speed. The number
of different word senses for adjectives is harder to
obtain without Wordnet, but this is only one of the
four factors in our model, and we do not depend
crucially on it.
5 Experiments
We evaluate our model in two stages. Firstly, using
automatic evaluation with a set of jokes collected
from Twitter, and secondly, by comparing our ap-
proach to human-generated jokes.
5.1 Inference
As the focus of this paper is on the model, not the
inference methods, we use exact inference. While
this is too expensive for estimating the true proba-
bility of any (x, y, z) triple, it is feasible if we fix
one of the nouns, i.e., if we deal with P (Y, Z|X =
x). Note that this is only a limitation of our infer-
ence procedure, not the model, and future work
will look at other ways (e.g., Gibbs sampling) to
perform inference. However, generating Y and
Z given X , such that the joke is funny, is still a
formidable challenge that a lot of humans are not
able to perform successfully (cf. performance of
human-generated jokes in Table 2).
5.2 Automatic evaluation
In the automatic evaluation we measure the effect
of the different factors in the model, as laid out in
Section 3.1. We use two metrics for this evalua-
tion. The first is similar to log-likelihood, i.e., the
log of the probability that our model assigns to a
triple. However, because we do not compute it on
all the data, just on the data that contains the Xs
from our development set, it is not exactly equal
to the log-likelihood. It is a local approximation
to log-likelihood, and we therefore dub it LOcal
Log-likelihood, or LOL-likelihood for short. Our
second metric computes the rank of the human-
generated jokes in the distribution of all possible
jokes sorted decreasingly by their LOL-likelihood.
This Rank OF Likelihood (ROFL) is computed
relative to the number of all possible jokes, and
like LOL-likelihood is averaged over all the jokes
in our development data. One advantage of ROFL
is that it is designed with the way we generate
jokes in mind (cf. Section 5.3), and thus more di-
rectly measures the quality of generated jokes than
LOL-likelihood. For measuring LOL-likelihood
and ROFL we use a set of 48 jokes randomly sam-
pled from Twitter that fit the I like my X like I like
my Y, Z pattern.
Table 1 shows the effect of the different fac-
tors on the two metrics. We use a model with
only noun-attribute similarity (factors ?(X,Z)
and ?(Y, Z)) as the baseline. We see that the sin-
gle biggest improvement comes from the attribute
surprisal factor, i.e., from using rarer attributes.
The best combination of the factors, according to
automatic metrics, is using all factors except for
the noun similarity (Model 1), while using all the
factors is the second best combination (Model 2).
5.3 Human evaluation
The main evaluation of our model is in terms of
human ratings, put simply: do humans find the
jokes generated by our model funny? We compare
four models: the two best models from Section 5.2
230
Model LOL-likelihood ROFL
Baseline -225.3 0.1909
Baseline + ?(X,Y ) -227.1 0.2431
Baseline + ?1(Z) -204.9 0.1467
Baseline + ?2(Z) -224.6 0.1625
Baseline + ?1(Z) + ?2(Z) (Model 1) -198.6 0.1002
All factors (Model 2) -203.7 0.1267
Table 1: Effect of different factors.
(one that uses all the factors (Model 2), and one
that uses all factors except for the noun dissimilar-
ity (Model 1)), a baseline model that uses only the
noun-attribute similarity, and jokes generated by
humans, collected from Twitter. We sample a fur-
ther 32 jokes from Twitter, making sure that there
was no overlap with the development set.
To generate a joke for a particular x we keep the
top n most probable jokes according to the model,
renormalize their probabilities so they sum to one,
and sample from this reduced distribution. This al-
lows our model to focus on the jokes that it consid-
ers ?funny?. In our experiments, we use n = 30,
which ensures that we can still generate a variety
of jokes for any given x.
In our experiments we showed five native En-
glish speakers the jokes from all the systems in a
random, per rater, order. The raters were asked
to score each joke on a 3-point Likert scale: 1
(funny), 2 (somewhat funny), and 3 (not funny).
Naturally, the raters did not know which approach
each joke was coming from. Our model was used
to sample Y and Z variables, given the same Xs
used in the jokes collected from Twitter.
Results are shown in Table 2. The second col-
umn shows the inter-rater agreement (Randolph,
2005), and we can see that it is generally good, but
that it is lower on the set of human jokes. We in-
spected the human-generated jokes with high dis-
agreement and found that the disagreement may
be partly explained by raters missing cultural ref-
erences in the jokes (e.g., a sonic screwdriver is
Doctor Who?s tool of choice, which might be lost
on those who are not familiar with the show).
We do not explicitly model cultural references,
and are thus less likely to generate such jokes,
leading to higher agreement. The third column
shows the mean joke score (lower is better), and
we can see that human-generated jokes were rated
the funniest, jokes from the baseline model the
least funny, and that the model which uses all the
Model ? Mean % funny jokes
Human jokes 0.31 2.09 33.1
Baseline 0.58 2.78 3.7
Model 1 0.52 2.71 6.3
Model 2 0.58 2.56 16.3
Table 2: Comparison of different models on the
task of generating Y and Z given X.
factors (Model 2) outperforms the model that was
best according to the automatic evaluation (Model
1). Finally, the last column shows the percentage
of jokes the raters scored as funny (i.e., the num-
ber of funny scores divided by the total number of
scores). This is a metric that we are ultimately
interested in ? telling a joke that is somewhat
funny is not useful, and we should only reward
generating a joke that is found genuinely funny
by humans. The last column shows that human-
generated jokes are considered funnier than the
machine-generated ones, but also that our model
with all the factors does much better than the other
two models. Model 2 is significantly better than
the baseline at p = 0.05 using a sign test, and
human jokes are significantly better than all three
models at p = 0.05 (because we were testing mul-
tiple hypotheses, we employed Holm-Bonferroni
correction (Holm, 1979)). In the end, our best
model generated jokes that were found funny by
humans in 16% of cases, compared to 33% ob-
tained by human-generated jokes.
Finally, we note that the funny jokes generated
by our system are not simply repeats of the human
jokes, but entirely new ones that we were not able
to find anywhere online. Examples of the funny
jokes generated by Model 2 are shown in Table 3.
6 Conclusion
We have presented a fully unsupervised humor
generation system for generating jokes of the type
231
I like my relationships like I like my source, open
I like my coffee like I like my war, cold
I like my boys like I like my sectors, bad
Table 3: Example jokes generated by Model 2.
I like my X like I like my Y, Z, where X, Y, and Z are
slots to be filled in. To the best of our knowledge,
this is the first humor generation system that does
not require any labeled data or hard-coded rules.
We express our assumptions about what makes a
joke funny as a machine learning model and show
that by estimating its parameters on large quanti-
ties of unlabeled data we can generate jokes that
are found funny by humans. While our experi-
ments show that human-generated jokes are fun-
nier more of the time, our model significantly im-
proves upon a non-trivial baseline, and we believe
that the fact that humans found jokes generated by
our model funny 16% of the time is encouraging.
Acknowledgements
The authors would like to thank the raters for their
help and patience in labeling the (often not so
funny) jokes. We would also like to thank Micha
Elsner for this helpful comments. Finally, we
thank the inhabitants of offices 3.48 and 3.38 for
putting up with our sniggering every Friday after-
noon.
References
Kim Binsted and Graeme Ritchie. 1994. An imple-
mented model of punning riddles. In Proceedings
of the twelfth national conference on Artificial intel-
ligence (vol. 1), AAAI ?94, pages 633?638, Menlo
Park, CA, USA. American Association for Artificial
Intelligence.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116.
Christiane Fellbaum. 1998. Wordnet: an electronic
lexical database. MIT Press.
Sture Holm. 1979. A simple sequentially rejective
multiple test procedure. Scandinavian journal of
statistics, pages 65?70.
Chloe? Kiddon and Yuriy Brun. 2011. That?s what she
said: double entendre identification. In Proceedings
of the 49th Annual Meeting of the ACL: Human Lan-
guage Technologies: short papers - Volume 2, pages
89?94.
Igor Labutov and Hod Lipson. 2012. Humor as cir-
cuits in semantic networks. In Proceedings of the
50th Annual Meeting of the ACL (Volume 2: Short
Papers), pages 150?155, July.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hol-
berg, Dan Clancy, Peter Norvig, Jon Orwant, Steven
Pinker, Martin A. Nowak, and Erez Lieberman
Aiden. 2010. Quantitative analysis of culture using
millions of digitized books. Science.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: investigations in automatic humor
recognition. In Proceedings of the conference on
Human Language Technology and EMNLP, pages
531?538.
Justus J. Randolph. 2005. Free-marginal multi-
rater kappa (multirater free): An alternative to fleiss
fixed- marginal multirater kappa. In Joensuu Uni-
versity Learning and Instruction Symposium.
Jonas Sjo?bergh and Kenji Araki. 2008. A com-
plete and modestly funny system for generating and
performing japanese stand-up comedy. In Coling
2008: Companion volume: Posters, pages 111?114,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
232
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 177?186,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Multiple-stream Language Models for Statistical Machine Translation
Abby Levenberg
Dept. of Computer Science
University of Oxford
ablev@cs.ox.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
David Matthews
School of Informatics
University of Edinburgh
dave.matthews@ed.ac.uk
Abstract
We consider using online language models for
translating multiple streams which naturally
arise on the Web. After establishing that us-
ing just one stream can degrade translations
on different domains, we present a series of
simple approaches which tackle the problem
of maintaining translation performance on all
streams in small space. By exploiting the dif-
fering throughputs of each stream and how
the decoder translates prior test points from
each stream, we show how translation perfor-
mance can equal specialised, per-stream lan-
guage models, but do this in a single language
model using far less space. Our results hold
even when adding three billion tokens of addi-
tional text as a background language model.
1 Introduction
There is more natural language data available today
than there has ever been and the scale of its produc-
tion is increasing quickly. While this phenomenon
provides the Statistic Machine Translation (SMT)
community with a potentially extremely useful re-
source to learn from, it also brings with it nontrivial
computational challenges of scalability.
Text streams arise naturally on the Web where
millions of new documents are published each day in
many different languages. Examples in the stream-
ing domain include the thousands of multilingual
websites that continuously publish newswire stories,
the official proceedings of governments and other
bureaucratic organisations, as well as the millions
of ?bloggers? and host of users on social network
services such as Facebook and Twitter.
Recent work has shown good results using an in-
coming text stream as training data for either a static
or online language model (LM) in an SMT setting
(Goyal et al, 2009; Levenberg and Osborne, 2009).
A drawback of prior work is the oversimplified sce-
nario that all training and test data is drawn from the
same distribution using a single, in-domain stream.
In a real world scenario multiple incoming streams
are readily available and test sets from dissimilar do-
mains will be translated continuously. As we show,
using stream data from one domain to translate an-
other results in poor average performance for both
streams. However, combining streams naively to-
gether hurts performance further still.
In this paper we consider this problem of multiple
stream translation. Since monolingual data is very
abundant, we focus on the subtask of updating an on-
line LM using multiple incoming streams. The chal-
lenges in multiple stream translation include dealing
with domain differences, variable throughput rates
(the size of each stream per epoch), and the need
to maintain constant space. Importantly, we impose
the key requirement that our model match transla-
tion performance reached using the single stream ap-
proach on all test domains.
We accomplish this using the n-gram history of
prior translations plus subsampling to maintain a
constant bound on memory required for language
modelling throughout all stream adaptation. In par-
ticular, when considering two test streams, we are
able to improve performance on both streams from
an average (per stream) BLEU score of 39.71 and
37.09 using a single stream approach (Tables 2 and
3) to an average BLEU score of 41.28 and 42.73 us-
ing multiple streams within a single LM using equal
memory (Tables 6 and 7). We also show additive im-
177
provements using this approach when using a large
background LM consisting of over one billion n-
grams. To our knowledge our approach is the first
in the literature to deal with adapting an online LM
to multiple streams in small space.
2 Previous Work
2.1 Randomised LMs
Randomised techniques for LMs from Talbot and
Osborne (2007) and Talbot and Brants (2008) are
currently industry state-of-the-art for fitting very
large datasets into much smaller amounts of mem-
ory than lossless representations for the data. Instead
of representing the n-grams exactly, the randomised
representation exchanges a small, one-sided error of
false positives for massive space savings.
2.2 Stream-based LMs
An unbounded text stream is an input source of natu-
ral language documents that is received sequentially
and so has an implicit timeline attached. In Leven-
berg and Osborne (2009) a text stream was used to
initially train and subsequently adapt an online, ran-
domised LM (ORLM) with good results. However,
a weakness of Levenberg and Osborne (2009) is that
the experiments were all conducted over a single in-
put stream. It is an oversimplification to assume that
all test material for a SMT system will be from a sin-
gle domain. No work was done on the multi-stream
case where we have more than one incoming stream
from arbitrary domains.
2.3 Domain Adaptation for SMT
Within MT there has been a variety of approaches
dealing with domain adaptation (for example (Wu et
al., 2008; Koehn and Schroeder, 2007)). Our work
is related to domain adaptation but differs in that we
are not skewing the distribution of an out-of-domain
LM to accommodate some test data for which we
have little or no training data for. Rather, we have
varying amounts of training data from all the do-
mains via the incoming streams and the LM must
account for each domain appropriately. However,
known domain adaptation techniques are potentially
applicable to multi-stream translation as well.
3 Multiple Streams and their Properties
Any source that provides a continuous sequence
of natural language documents over time can be
thought of as an unbounded stream which is time-
stamped and access to it is given in strict chronolog-
ical order. The ubiquity of technology and the In-
ternet means there are many such text streams avail-
able already and their number is increasing quickly.
For SMT, multiple text streams provide a potentially
abundant source of new training data that may be
useful for combating model sparsity.
Of primary concern is building models whose
space complexity is independent of the size of the
incoming stream. Allowing unbounded memory to
handle unbounded streams is unsatisfactory. When
dealing with more than one stream we must also
consider how the properties of single streams inter-
act in a multiple stream setting.
Every text stream is associated with a particular
domain. For example, we may draw a stream from
a newswire source, a daily web crawl of new blogs,
or the output of a company or organisation. Obvi-
ously the distribution over the text contained in these
streams will be very different from each other. As
is well-known from the work on domain adaptation
throughout the SMT literature, using a model from
one domain to translate a test document from an-
other domain would likely produce poor results.
Each stream source will also have a different
rate of production, or throughput, which may vary
greatly between sources. Blog data may be received
in abundance but the newswire data may have a sig-
nificantly lower throughput. This means that the text
stream with higher throughput may dominate and
overwhelm the more nuanced translation options of
the stream with less data in the LM during decod-
ing. This is bad if we want to translate well for all
domains in small space using a single model.
4 Multi-Stream Retraining
In a stream-based translation setting we can expect
to translate test points from various domains on any
number of incoming streams. Our goal is a single
unified LM that obtains equal performance in less
space than when using a separate LM per stream.
The underlying LMs could be exact, but here we use
randomised versions based on the ORLM.
178
?stream a1LM2m3
stream a1LM2mi
stream a1LM2mn
pum3
pumi
pumK
Naive Combination Approach
tLwmLroch tLwmLroch
Figure 1: In the naive approach all K streams are simply
combined into a single LM for each new epoch encoun-
tered.
Given an incoming number K of unbounded
streams over a potentially infinite timeline T , with
t ? T an epoch or windowed subset of the timeline,
the full set of n-grams in all K streams over all T
is denoted with S. By St we denote n-grams from
all K streams and Skt, k ? [1,K], as the n-grams
in the kth stream over epoch t. Since the streams
are unbounded, we do not have access to all the n-
grams in S at once. Instead we select n-grams from
each stream Skt ? S. We define the collection of
n-grams encoded in the LM at time t over all K
streams as Ct. Initially, at time t = 0 the LM is
composed of the n-grams in the stream so C0 = S0.
Since it is unsatisfactory to allow unbounded
memory usage for the model and more bits are
needed as we see more novel n-grams from the
streams, we enforce a memory constraint and use
an adaptation scheme to delete n-grams from the
LM Ct?1 before adding any new n-grams from the
streams to get the current n-gram set Ct. Below
we describe various approaches of updating the LM
with data from the streams.
4.1 Naive Combinations
Approach The first obvious approach for an online
LM using multiple input streams is to simply store
all the streams in one LM. That is, n-grams from
all the streams are only inserted into the LM once
and their stream specific counts are combined into a
single value in the composite LM.
Modelling the Stream In the naive case we retrain
the LM Ct in full at epoch t using all the new data
from the streams. We have simply
Ct =
K
?
k=1
Skt (1)
stream 1 LM 1
stream 1 LM 2
stream 1 LM 3
input stream 1
stream 2 LM 1
stream 2 LM 2
stream 2 LM 3
input stream 2
?
stream K LM 1
stream K LM 2
stream K LM 3
input stream K
Multiple LM Approach
new epoch new epoch
Figure 2: Each stream 1 . . . K gets its own stream-based
LM using the multiple LM approach.
where each of the K streams is combined into a sin-
gle model and the n-grams counts are merged lin-
early. Here we carry no n-grams over from the LM
Ct?1 from the previous epoch. The space needed is
the number of unique n-grams present in the com-
bined streams for each epoch.
Resulting LM To query the resulting LM Ct dur-
ing decoding with a test n-gram wni = (wi, . . . , wn)
we use a simple smoothing algorithm called Stupid
Backoff (Brants et al, 2007). This returns the
probability of an n-gram as
P (wi|wi?1i?n+1) :=
?
?
?
Ct(wii?n+1)
Ct(wi?1i?n+1)
if Ct(wii?n+1) > 0
?P (wi|wi?1i?n+2) otherwise
(2)
where Ct(.) denotes the frequency count returned by
the LM for an n-gram and ? is a backoff parameter.
The recursion ends once the unigram is reached in
which case the probability is P (wi) := wi/N where
N is the size of the current training corpus.
Each stream provides a distribution over the n-
grams contained in it and, for SMT, if a separate
LM was constructed for each domain it would most
likely cause the decoder to derive different 1-best
hypotheses than using a LM built from all the stream
data. Using the naive approach blurs the distribution
distinctions between streams and negates any stream
specific differences when the decoder produces a 1-
best hypothesis. It has been shown that doing lin-
ear combinations of this type produces poor perfor-
mance in theory (Mansour et al, 2008).
179
4.2 Weighted Interpolation
Approach An improved approach to using multi-
ple streams is to build a separate LM for each stream
and using a weighted combination of each during
decoding. Each stream is stored in isolation and we
interpolate the information contained within each
during decoding using a weighting on each stream.
Modelling the Stream Here we model the streams
by simply storing each stream at time t in its own
LM so Ckt = Skt for each stream Sk. Then the LM
after epoch t is
Ct = {C1t, . . . , CKt}.
We use more space here than all other approaches
since we must store each n-gram/count occurring in
each stream separately as well as the overhead in-
curred for each separate LM in memory.
Resulting LM During decoding, the probability of
a test n-gram wni is a weighted combination of all
the individual stream LMs. We can write
P (wni ) :=
K
?
k=1
fkPCkt(wni ) (3)
where we query each of the individual LMs Ckt to
get a score from each LM using Equation 2 and
combine them together using a weighting fk spe-
cific to each LM. Here we impose the restriction on
the weights that
?K
k=1 fk = 1. (We discuss specific
weight selections in the next section.)
By maintaining multiple stream specific LMs we
maintain the particular distribution of the individual
streams. This keeps the more nuanced translations
from the lower throughput streams available during
decoding without translations being dominated by a
stream with higher throughput. However using mul-
tiple distinct LMs is wasteful of memory.
4.3 Combining Models via History
Approach We want to combine the streams into
a single LM using less memory than when storing
each stream separately but still achieve at least as
good a translation for each test point. Naively com-
bining the streams removes stream specific transla-
tions but using the history of n-grams selected by the
decoder during the previous test point in the stream
was done in Levenberg and Osborne (2009) for the
single stream case with good results. This is appli-
cable to the multi-stream case as well.
Modelling the Stream For multiple streams and
epoch t > 0 we model the stream combination as
Ct = fT (Ct?1) ?
K
?
k=1
(Skt). (4)
where for each epoch a selected subset of the previ-
ous n-grams in the LM Ct?1 is merged with all the
newly arrived stream data to create the new LM set
Ct. The parameter fT denotes a function that filters
over the previous set of n-grams in the model. It
represents the specific adaptation scheme employed
and stays constant throughout the timeline T . In this
work we consider any n-grams queried by the de-
coder in the last test point as potentially useful to
the next point. Since all of the n-grams St in the
stream at time t are used the space required is of the
same order of complexity as the naive approach.
Resulting LM Since all the n-grams from the
streams are now encoded in a single LM Ct we can
query it using Equation 2 during decoding. The goal
of retraining using decoding history is to keep use-
ful n-grams in the current model so a better model
is obtained and performance for the next transla-
tion point is improved. Note that making use of the
history for hypothesis combination is theoretically
well-founded and is the same approach used here for
history based combination. (Mansour et al, 2008)
4.4 Subsampling
Approach The problem of multiple streams with
highly varying throughput rates can be seen as a type
of class imbalance problem in the machine learning
literature. Given a binary prediction problem with
two classes, for instance, the imbalance problem oc-
curs when the bulk of the examples in the training
data are instances of one class and only a much
smaller proportion of examples are available from
the other class. A frequently used approach to bal-
ancing the distribution for the statistical model is to
use random under sampling and select only a sub-
set of the dominant class examples during training
(Japkowicz and Stephen, 2002).
This approach is applicable to the multiple stream
translation problem with imbalanced throughput
rates between streams. Instead of storing the n-
grams from each stream separately, we can apply a
180
?stream a1LM2m3
stream a1LM2mi
stream a1LM2mn
pum3
LM 2 + (subset of LM 1)
LM 3 + (subset of LM 2)
Naive ComebnatAvaetoprr eAch
new epoch new epoch
SMT Decoder
Figure 3: Using decoding history all the streams are com-
bined into a unified LM.
subsampling selection scheme directly to the incom-
ing streams to balance each stream?s contribution in
the final LM. Note that subsampling is also related
to weighting interpolation. Since all returned LM
scores are based on frequency counts of the n-grams
and their prefixes, taking a weighting on a full prob-
ability of an n-gram is akin to having fewer counts
of the n-grams in the LM to begin with.
Modelling the Stream To this end we use the
weighted function parameter fk from Equation 3 to
serve as the sampling probability rate for accepting
an n-gram from a given stream k. The sampling
rate serves to limit the amount of stream data from
a stream that ends up in the model. For K > 1 we
have
Ct = fT (Ct?1) ?
K
?
k=1
fk(Skt) (5)
where fk is the probability a particular n-gram from
stream Sk at epoch t will be included in Ct. The
adaptation function fT remains the same as in Equa-
tion 4. The space used in this approach is now de-
pendent on the rate fk used for each stream.
Resulting LM Again, since we obtain a single LM
from all the streams, we use Equation 2 to get the
probability of an n-gram during decoding.
The subsampling method is applicable to all of the
approaches discussed in this section. However, since
we are essentially limiting the amount of data that
we store in the final LM we can expect to take a per-
formance hit based on the rate of acceptance given
by the parameters fk. By using subsampling with
the history combination approach we obtain good
performance for all streams in small space.
Stream 1-grams 3-grams 5-grams
EP 19K 520K 760K
GW (xie) 120K 3M 5M
RCV1 630K 21M 42M
Table 1: Sample statistics of unique n-gram counts from
the streams from epoch 2 of our timeline. The throughput
rate varies a lot between streams.
5 Experiments
Here we report on our SMT experiments with multi-
ple streams for translation using the approaches out-
lined in the previous section.
5.1 Experimental Setup
The SMT setup we employ is standard and all re-
sources used are publicly available. We translate
from Spanish into English using phrase-based de-
coding with Moses (Koehn and Hoang, 2007) as our
decoder. Our parallel data came from Europarl.
We use three streams (all are timestamped):
RCV1 (Rose et al, 2002), Europarl (EP) (Koehn,
2003), and Gigaword (GW) (Graff et al, 2007). GW
is taken from six distinct newswire sources but in
our initial experiments we limit the incoming stream
from Gigaword to one of the sources (xie). GW and
RCV1 are both newswire domain streams with high
rates of incoming data whereas EP is a more nu-
anced, smaller throughput domain of spoken tran-
scripts taken from sessions of the European Parlia-
ment. The RCV1 corpus only spans one calender
year from October, 1996 through September, 1997
so we selected only data in this time frame from
the other two streams so our timeline consists of the
same full calendar year for all streams.
For this work we use the ORLM. The crux of the
ORLM is an online perfect hash function that pro-
vides the ability to insert and delete from the data
structure. Consequently the ORLM has the abil-
ity to adapt to an unbounded input stream whilst
maintaining both constant memory usage and error
rate. All the ORLMs were 5-gram models built with
training data from the streams discussed above and
used Stupid Backoff smoothing for n-gram scoring
(Brants et al, 2007). All results are reported using
the BLEU metric (Papineni et al, 2001).
For testing we held-out three random test points
181
LM Type Test 1 Test 2 Test 3
RCV1 (Static) 39.30 38.28 33.06
RCV1 (Online) 39.30 40.64 39.19
EP (Online) 30.22 30.31 26.66
RCV1+EP (Online) 39.00 40.15 39.46
RCV1+EP+GW (Online) 41.29 41.73 40.41
Table 2: Results for the RCV1 test points. RCV1 and GW
streams are in-domain and EP is out-of-domain. Transla-
tion results are improved using more stream data since
most n-grams are in-domain to the test points.
from both the RCV1 and EP stream?s timeline for
a total of six test points. This divided the streams
into three epochs, and we updated the online LM
using the data encountered in the epoch prior to each
translation point. The n-grams and their counts from
the streams are combined in the LM using one of the
approaches from the previous section.
Using the notation from Section 4 we have the
RCV1, EP, and GW streams described above and
K = 3 as the number of incoming streams from two
distinct domains (newswire and spoken dialogue).
Our timeline T is one year?s worth of data split into
three epochs, t ? {1, 2, 3}, with test points at the
end of each epoch t. Since we have no test points
from the GW stream it acts as a background stream
for these experiments. 1
5.2 Baselines and Naive Combinations
In this section we report on our translation exper-
iments using a single stream and the naive linear
combination approach with multiple incoming data
streams from Section 4.1.
Using the RCV1 corpus as our input stream we
tested single stream translation first. Here we are
replicating the experiments from Levenberg and Os-
borne (2009) so both training and test data comes
from a single in-domain stream. Results are in Table
2 where each row represents a different LM type.
RCV1 (Static) is the traditional baseline with no
adaptation where we use the training data for the first
epoch of the stream. RCV1 (Online) is the online
LM adapted with data from the in-domain stream.
Confirming the previous work we get improvements
1A background stream is one that only serves as training
data for all other test domains.
LM Type Test 1 Test 2 Test 3
EP (Static) 42.09 44.15 36.42
EP (Online) 42.09 45.94 37.22
RCV1 (Online) 36.46 42.10 32.73
EP+RCV1 (Online) 40.82 44.07 35.01
EP+RCV1+GW (Online) 40.91 44.05 35.56
Table 3: EP results using in and out-of-domain streams.
The last two rows show that naive combination gets poor
results compared to single stream approaches.
when using an online LM that incorporates recent
data against a static baseline.
We then ran the same experiments using a stream
generated from the EP corpus. EP consists of the
proceedings of the European Parliament and is a sig-
nificantly different domain than the RCV1 newswire
stream. We updated the online LM using n-grams
from the latest stream epoch before translating each
in-domain EP test set. Results are in Table 3 and fol-
low the same naming convention as Table 2 (except
now in-domain is EP and out-of-domain is RCV1).
Using a single stream we also cross tested and
translated each test point using the online LM
adapted on the out-of-domain stream. As expected,
translation performance decreases (sometimes dras-
tically) in this case since the data of the out-of-
domain stream are not suited to the domain of the
current test point being translated.
We then tested the naive approach and combined
both streams into a single LM by taking the union of
the n-grams and adding their counts together. This
is the RCV1+EP (Online) row in Tables 2 and 3 and
clearly, though it contains more data compared to
each single stream LM, the naively combined LM
does not help the RCV1 test points much and de-
grades the performance of the EP translation results.
This translation hit occurs as the throughput of each
stream is significantly different. The EP stream con-
tains far less data per epoch than the RCV1 counter-
part (see Table 1) hence using a naive combination
means that the more abundant newswire data from
the RCV1 stream overrides the probabilities of the
more domain specific EP n-grams during decoding.
When we added a third newswire stream from a
portion of GW, shown in the last row of Tables 2
and 3, improvements are obtained for the RCV1 test
182
Weighting Test 1 Test 2 Test 3
.33R + .33E + .33G 38.97 39.78 35.66
.50R + .25E + .25G 39.59 40.40 37.22
.25R + .50E + .25G 36.57 38.03 34.23
.70R + 0.0E + .30G 40.54 41.46 39.23
Table 4: Weighted LM interpolation results for the RCV1
test points where E = Europarl, R = RCV1, and G =
Gigaword (xie).
points due to the addition of in-domain data but the
EP test performance still suffers.
This highlights why naive combination is unsat-
isfactory. While using more in-domain data aids
in the translation of the newswire tests, for the EP
test sets, naively combining the n-grams from all
streams means the hypotheses the decoder selects
are weighted heavily in favor of the out-of-domain
data. As the out-of-domain stream?s throughput is
significantly larger it swamps the model.
5.3 Interpolating Weighted Streams
Straightforward linear stream combination into a
single LM results in degradation of translations for
test points whose in-domain training data is drawn
from a stream with lower throughput than the other
data streams. We could maintain a separate MT sys-
tem for each streaming domain but intuitively some
combination of the streams may benefit average per-
formance since using all the data available should
benefit test points from streams with low through-
put. To test this we used an alternative approach de-
scribed in Section 4.2 and used a weighted combi-
nation of the single stream LMs during decoding.
We tested this approach using our three streams:
RCV1, EP and GW (xie). We used a separate
ORLM for each stream and then, during testing, the
result returned for an n-gram queried by the decoder
was a value obtained from some weighted interpola-
tion of each individual LM?s score for that n-gram.
To get the interpolation weights for each streaming
LM we minimised the perplexity of all the mod-
els on held-out development data from the streams.
2 Then we used the corresponding stream specific
2Due to the lossy nature of the encoding of the ORLM
means that the perplexity measures were approximations.
Nonetheless the weighting from this approach had the best per-
formance.
Weighting Test 1 Test 2 Test 3
.33E + .33R + .33G 40.75 45.65 35.77
.50E + .25R + .25G 41.46 46.37 36.94
.25E + .50R + .25G 40.57 44.90 35.77
.70E + .20R + .10G 42.47 46.83 38.08
Table 5: EP results in BLEU for the interpolated LMs.
weights to decode the test points from that domain.
Results are shown in Tables 4 and 5 using the
weighting scheme described above plus a selec-
tion of random parameter settings for comparison.
Using the notation from Section 4.2, a caption of
?.5R+ .25E+ .25G?, for example, denotes a weight-
ing of fRCV 1 = 0.5 for the scores returned from the
RCV1 stream LM while fEP and fGW = 0.25 for
the EP and GW stream LMs.
The weighted interpolation results suggest that
while naive combination of the streams may be mis-
guided, average translation performance can be im-
proved upon when using more than a single in-
domain stream. Comparing the best results in Tables
2 and 3 to the single stream baselines in Tables 4 and
5 we achieve comparable, if not improved, transla-
tion performance for both domains. This is espe-
cially true for test domains such as EP which have
low training data throughput from the stream. Here
adding some information from the out-of-domain
stream that contains a lot more data aids signifi-
cantly in the translation of in-domain test points.
However, the optimal weighting differs between
each test domain. For instance, the weighting that
gives the best results for the EP tests results in much
poorer translation performance for the RCV1 test
points requiring us to track which stream we are
decoding and then select the appropriate weighting.
This adds unnecessary complexity to the SMT sys-
tem. And, since we store each stream separately,
memory usage is not optimal using this scheme.
5.4 History and Subsampling
For space efficiency we want to represent multi-
ple streams non-redundantly instead of storing each
stream/domain in its own LM. Here we report on
experiments using both the history combination and
subsampling approaches from Sections 4.3 and 4.4.
Results are in Tables 6 and 7 for the RCV1 and
183
LM Type Test 1 Test 2 Test 3
Multi-fk 41.19 41.73 39.23
Multi-fT 41.29 42.23 40.51
Multi-fk + fT 41.19 42.52 40.12
Table 6: RCV1 test results using history and subsampling
approaches.
LM Type Test 1 Test 2 Test 3
Multi-fk 40.91 43.50 36.11
Multi-fT 40.91 47.84 39.29
Multi-fk + fT 40.91 48.05 39.23
Table 7: Europarl test results with history and subsam-
pling approaches.
EP test sets respectively with the column headers
denoting the test point. The row Multi-fk shows
results using only the random subsampling param-
eter fk and the rows Multi-fT show results with just
the time-based adaptation parameter without sub-
sampling. The final row Multi-fk + fT uses both
the f parameters with random subsampling as well
as taking decoding history into account.
Multi-fk uses the random subsampling parame-
ter fk to filter out higher order n-grams from the
streams. All n-grams that are sampled from the
streams are then combined into the joint LM. The
counts of n-grams sampled from more than one
stream are added together in the composite LM. The
parameter fk is set dependent on a stream?s through-
put rate, we only subsample from the streams with
high throughput, and the rate was chosen based on
the weighted interpolation results described previ-
ously. In Tables 6 and 7 the subsampling rate fk =
0.3 for the combined newswire streams RCV1 and
GW and we kept all of the EP data. We experi-
mented with various other values for the fk sampling
rates and found translation results only minorly im-
pacted. Note that the subsampling is truly random
so two adaptation runs with equal subsampling rates
may produce different final translations. Nonethe-
less, in our experiments we saw expected perfor-
mance, observing slight variation in performance for
all test points that correlated to the percentage of in-
domain data residing in the model.
The next row, Multi-fT , uses recency criteria to
keep potentially useful n-grams but uses no subsam-
pling and accepts all n-grams from all streams into
the LM. Here we get better results than naive combi-
nation or plain subsampling at the expense of more
memory for the same error rate for the ORLM.
The final row, Multi-fk + fT uses both the sub-
sampling function fk and fT so maintains a history
of the n-grams queried by the decoder for the prior
test points. This approach achieves significantly bet-
ter results than naive adaptation and compares to us-
ing all the data in the stream. Combining translation
history as well as doing random subsampling over
the stream means we match the performance of but
use far less memory than when using multiple online
LMs whilst maintaining the same error rate.
5.5 Experiments Summary
We have shown that using data from multiple
streams benefits SMT performance. Our best ap-
proach, using history based combination along with
subsampling, combines all incoming streams into a
single, succinct LM and obtains translation perfor-
mance equal to single stream, domain specific LMs
on all test domains. Crucially we do this in bounded
space, require less memory than storing each stream
separately, and do not incur translation degradations
on any single domain.
A note on memory usage. The multiple LM ap-
proach uses the most memory since this requires
all overlapping n-grams in the streams to be stored
separately. The naive and history combination ap-
proaches use less memory since they store all n-
grams from all the streams in a unified LM. For the
sampling the exact amount of memory is of course
dependent on the sampling rate used. For the results
in Tables 6 and 7 we used significantly less memory
(300MB) but still achieved comparable performance
to approaches that used more memory by storing the
full streams (600MB).
6 Scaling Up
The experiments described in the preceding section
used combinations of relatively small (compared to
current industry standards) input streams. The ques-
tion remains if using such approaches aids in the per-
formance of translation if used in conjunction with
large static LMs trained on large corpora. In this
section we describe scaling up the previous stream-
184
Order Count
1-grams 3.7M
2-grams 46.6M
3-grams 195.5M
4-grams 366.8M
5-grams 454.2M
Total 1067M
Table 8: Singleton-pruned n-gram counts (in millions)
for the GW3 background LM.
LM Type Test 1 Test 2 Test 3
GW (static) 41.69 42.40 35.48
+ RCV1 (online) 42.44 43.83 40.55
+ EP (online) 42.80 43.94 38.82
Table 9: Test results for the RCV1 stream using the large
background LM. Using stream data benefits translation.
based translation experiments using a large back-
ground LM trained on a billion n-grams.
We used the same setup described in Section 5.1.
However, instead of using only a subset of the GW
corpus as one of our incoming streams, we trained
a static LM using the full GW3 corpus of over three
billion tokens and used it as a background LM. As
the n-gram statistics for this background LM show
in Table 8, it contains far more data than each of the
stream specific LMs (Table 1). We tested whether
using streams atop this large background LM had a
positive effect on translation for a given domain.
Baseline results for all test points using only the
GW background LM are shown in the top row in
Tables 9 and 10. We then interpolated the ORLMs
with this LM. For each stream test point we interpo-
lated with the big GW LM an online LM built with
the most recent epoch?s data. Here we used sepa-
rate models per stream so the RCV1 test points used
the GW LM along with a RCV1 specific ORLM. We
used the same mechanism to obtain the interpolation
weights as described in Section 5.3 and minimised
the perplexity of the static LM along with the stream
specific ORLM. Interestingly, the tuned weights re-
turned gave approximately a 50-50 weighting be-
tween LMs and we found that simply using a 50-50
weighting for all test points resulted had no negative
effect on BLEU. In the third row of the Tables 9 and
10 we show the results of interpolating the big back-
LM Type Test 1 Test 2 Test 3
GW (static) 40.78 44.26 34.36
+ EP (online) 43.94 47.82 38.71
+ RCV1 (online) 43.07 47.72 39.15
Table 10: EP test results using the background GW LM.
ground LM with ORLMs built using the approach
described in Section 4.4. In this case all streams
were combined into a single LM using a subsam-
pling rate for higher order n-grams. As before our
sampling rate for the newswire streams was 30%
chosen by the perplexity reduction weights.
The results show that even with a large amount
of static data adding small amounts of stream spe-
cific data relevant to a given test point has an im-
pact on translation quality. Compared to only us-
ing the large background model we obtain signifi-
cantly better results when using a streaming ORLM
to compliment it for all test domains. However the
large amount of data available to the decoder in
the background LM positively impacts translation
performance compared to single-stream approaches
(Tables 2 and 3). Further, when we combine the
streams into a single LM using the subsampling ap-
proach we get, on average, comparable scores for all
test points. Thus we see that the patterns for multi-
ple stream adaptation seen in previous sections hold
in spite of big amounts of static data.
7 Conclusions and Future Work
We have shown how multiple streams can be effi-
ciently incorporated into a translation system. Per-
formance need not degrade on any of the streams.
As well, these results can be additive. Even when
using large amounts of additional background data,
adding stream specific data continues to improve
translation. Further, we achieve all results in
bounded space. Future work includes investigating
more sophisticated adaptation for multiple streams.
We also plan to explore alternative ways of sampling
the stream when incorporating data.
Acknowledgements
Special thanks to Adam Lopez and Conrad Hughes
and Phil Blunosm for helpful discussion and advice.
This work was sponsored in part by the GALE pro-
185
gram, DARPA Contract No. HR0011-06-C-0022
and by ESPRC Grant No. EP/I010858/1bb.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Amit Goyal, Hal Daume? III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In North American Chapter of the
Association for Computational Linguistics (NAACL),
Boulder, CO.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium (LDC-2007T07).
Nathalie Japkowicz and Shaju Stephen. 2002. The class
imbalance problem: A systematic study. Intell. Data
Anal., 6:429?449, October.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn. 2003. Europarl: A multilingual corpus
for evaluation of machine translation. Available at:
http://www.statmt.org/europarl/.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Domain adaptation with multiple
sources. In NIPS, pages 1041?1048.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volume 1 - from yester-
days news to tomorrows language resources. In In
Proceedings of the Third International Conference on
Language Resources and Evaluation, pages 29?31.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505?513, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468?476.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 993?
1000. Coling 2008 Organizing Committee, August.
186
