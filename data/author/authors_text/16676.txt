Proceedings of the 7th Workshop on Statistical Machine Translation, pages 253?260,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Probes in a Taxonomy of Factored Phrase-Based Models ?
Ondr?ej Bojar, Bushra Jawaid, Amir Kamran
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,jawaid,kamran}@ufal.mff.cuni.cz
Abstract
We introduce a taxonomy of factored phrase-
based translation scenarios and conduct a
range of experiments in this taxonomy. We
point out several common pitfalls when de-
signing factored setups. The paper also de-
scribes our WMT12 submissions CU-BOJAR
and CU-POOR-COMB.
1 Introduction
Koehn and Hoang (2007) introduced ?factors? to
phrase-based MT to explicitly capture arbitrary fea-
tures in the phrase-based model. In essence, input
and output tokens are no longer atomic units but
rather vectors of atomic values encoding e.g. the lex-
ical and morphological information separately. Fac-
tored translation has been successfully applied to
many language pairs and with diverse types of infor-
mation encoded in the additional factors, i.a. (Bojar,
2007; Avramidis and Koehn, 2008; Stymne, 2008;
Badr et al, 2008; Ramanathan et al, 2009; Koehn et
al., 2010; Yeniterzi and Oflazer, 2010). On the other
hand, it happens quite frequently, that the factored
setup causes a loss compared to the phrase-based
baseline. The underlying reason is the complexity of
the search space which gets boosted when the model
explicitly includes detailed information, see e.g. Bo-
jar and Kos (2010) or Toutanova et al (2008).
? This work was supported by the project EuroMatrixPlus
(FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of
the Czech Republic) and the Czech Science Foundation grants
P406/11/1499 and P406/10/P259. We are grateful for review-
ers? comments but we have to obey the 6 page limit. Thanks
also to Ales? Tamchyna for supplementary material on MERT.
Number of Number of
Translation Independent Structure
Steps Searches of Searches Nickname
One One ? Direct
Several
One ? Single-Step
Several
Serial Two-Step
Complex Complex
Figure 1: A taxonomy of factored phrase-based models.
In this paper, we first provide a taxonomy of
(phrase-based) translation setups and then we exam-
ine a range of sample configurations in this taxon-
omy. We don?t state universal rules, because the ap-
plicability of each of the setups depends very much
on the particular language pair, text domain and
amount of data available, but we hope to draw at-
tention to relevant design decisions.
The paper also serves as the description of our
WMT12 submissions CU-BOJAR and CU-POOR-
COMB between English and Czech.
2 A Taxonomy of Factored P-B Models
Figure 1 suggests a taxonomy of various Moses se-
tups. Following the definitions of Koehn and Hoang
(2007), a search consists of several translation and
generation steps: translation steps map source fac-
tors to target factors and generation steps produce
target factors from other target factors.
The taxonomy is vaguely linked to the types of
problems that can be expected with a given config-
uration. Direct translation is likely to suffer from
out-of-vocabulary issues (due to insufficient gener-
alization) on either side. Single-step scenarios have
253
a very high risk of combinatorial explosion of trans-
lation options (think cartesian product of all target
side factors) and/or of spurious ambiguity (several
derivations leading to the same output). Such added
ambiguity can lead to n-best lists with way fewer
unique items than the given n, which in turn ren-
ders MERT unstable, see also Bojar and Tamchyna
(2011). Serially connected setups (two as our Two-
Step or more) can lose relevant candidates between
the searches, unless some ambiguous representation
like lattices is passed between the steps.
An independent axis on which Moses setups can
be organized consists of the number and function of
factors on the source and the target side.
We use a very succint notation for the setups ex-
cept the ?complex? one: tX-Y denotes a translation
step between the factors X in the source language
and Y in the target language. Generation steps are
denoted with gY-Z, where both Y and Z are target-
side factors. Individual mapping steps are combined
with a plus, while individual source or target factors
are combined with an ?a?.
As a simple example, tF-F denotes the direct
translation from source form (F ) to the target form.
A linguistically motivated scenario with one search
can be written as tL-L+tT-T+gLaT-F : translate (1)
the lemma (L) to lemma, (2) the morphological tag
(T) to tag independently and (3) finally generate the
target form from the lemma and the tag.
We use two more operators: ?:? delimits al-
ternative decoding paths (Birch et al, 2007) used
within one search and ?=? delimits two independent
searches. A plausible setup is e.g. tF-LaT=tLaT-
F:tL-F motivated as follows: the source word form
is translated to the lemma and tag in the target lan-
guage. Then a second search (whose translation ta-
bles can be trained on larger monolingual data) con-
sists of two alternative decoding paths: either the
pair of L and T is translated into the target form, or
as a fallback, the tag is disregarded and the target
form is guessed only from the lemma (and the con-
text as scored by the language model). The example
also illustrated the priorities of the operators.
3 Common Settings
Throughout the experiments, we use the Moses
toolkit (Koehn et al, 2007) and GIZA++ (Och
Dataset Sents (cs/en) Toks (cs/en) Source
Small 197k parallel 4.2M/4.8M CzEng 1.0 news
Large 14.8M parallel 205M/236M CzEng 1.0 all
Mono 18M/50M 317M/1.265G WMT12 mono
Table 1: Summary of training data.
Decoding Path Language Models BLEU
tF-FaLaT form + lemma + tag 13.05?0.44
tF-FaT form + tag 13.01?0.44
tF-FaLaT form + tag 12.99?0.44
tF-F (baseline) form 12.42?0.44
tF-FaT form 12.19?0.44
tF-FaLaT form 12.08?0.45
Table 2: Direct en?cs translation (a single search with
one translation step only).
and Ney, 2000). The texts were processed us-
ing the Treex platform (Popel and Z?abokrtsky?,
2010)1, which included lemmatization and tagging
by Morce (Spoustova? et al, 2007). After the tag-
ging, we tokenized further so words like ?23-year?
or ?Aktualne.cz? became three tokens.
Our training data is summarized in Table 1.2
In most experiments reported here, we use the
Small dataset only. The language model (LM) for
these experiments is a 5-gram one based on the
target-side of Small only.
Our WMT12 submissions are based on the Large
and Mono data. The language model for the large
experiments uses 6-grams of forms and optionally
8-grams of morphological tags. As in previous
years, the language models are interpolated (to-
wards the best cross entropy on WMT08 dataset)
from domain-specific LMs, e.g. czeng-news, czeng-
techdoc, wmtmono-2011, wmtmono-2012.
Except where stated otherwise, we tune on the of-
ficial WMT10 test set and report BLEU (Papineni et
al., 2002) scores on the WMT11 test set.
4 Direct Setups
Table 2 lists our experiments with direct translation,
various factors and language models in our notation.
1http://ufal.mff.cuni.cz/treex/
2We did not include the parallel en-cs data made available
by the WMT12 organizers. This probably explains our loss
compared to UEDIN but allows a direct comparison with CU
TECTOMT, a deep syntactic MT based on the same data.
254
Decoding Paths LMs Avg. BLEU Eff. Nbl. Size
tL-L+tT-T+gLaT-F:tF-FaLaT F + L + T 13.31?0.06 12.24?1.33
tL-L+tT-T+gLaT-F F + L + T 13.30?0.05 40.33?3.82
tL-L+tT-T+gLaT-F F + T 13.17?0.01 39.91?2.58
tL-L+tT-T+gLaT-F:tF-FaLaT, 200-best-list F + L + T 13.15?0.24 20.47?5.63
tF-FaLaT F + L + T 13.13?0.06 34.28?3.08
tL-L+tT-T+gLaT-F:tF-FaLaT L + T 13.09?0.06 16.65?1.07
tF-FaT F + T 13.08?0.05 39.67?2.21
tL-L+tT-T+gLaT-F:tF-FaT F + T 13.01?0.43 14.87?5.04
tF-F (baseline) F 12.38?0.03 43.13?0.48
tL-L+tT-T+gLaT-F:tF-F F 12.30?0.03 17.83?3.27
Table 3: Results of three MERT runs of several single-step configurations.
Explicit modelling of target-side morphology im-
proves translation quality, compare tF-FaLaT with
the baseline tF-F. However, two results document
that if some detailed information is distinguished in
the output, it introduces target ambiguity and leads
to a loss in BLEU, unless the detailed information is
actually used in the language model: (1) tF-FaLaT
with LM on forms is worse than the baseline tF-F
but tF-FaLaT with all the three language models is
better, (2) tF-FaLaT with two LMs (forms and tags)
is negligibly worse than tF-FaT with the same lan-
guage models.
5 Single-Step Experiments
Single-step scenarios consist of more than one trans-
lation steps within a single search. We do not distin-
guish whether all the translation steps belong to the
same decoding path or to alternative decoding paths.
Table 3 lists several single-step configurations
(and three direct translations for a compari-
son). The single-step configurations always include
the linguistically-motivated tL-L+tT-T+gLaT-F with
varying language models and optionally with an al-
ternative decoding path to serve as the fallback.
Aware of the low stability of MERT (Clark et al,
2011), we run MERT three times and report the av-
erage BLEU score including the standard deviation.
The last column in Table 3 lists the average num-
ber of distinct candidates per sentence in the n-
best lists during MERT, dubbed ?effective n-best list
size?. Unless stated otherwise, we used 100-best
lists. We see that due to spurious ambiguity, e.g.
various segmentations of the input into phrases, the
effective size does not reach even a half of the limit.
We make three observations here:
(1) In this small data setting with a very morpho-
logically rich language, the complex setup tL-L+tT-
T+gLaT-F does not even need the alternative decod-
ing path tF-F. Ramanathan et al (2009) report gains
in English-to-Hindi translation and also probably do
not use alternative decoding paths.
(2) Reducing the range of language models used
leads to worse scores, which is in line with the ob-
servation made with direct setups. We are surprised
by the relative importance of the lemma-based LM.
(3) Alternative decoding paths significantly re-
duce effective n-best list size to just 12?18 unique
candidates per sentence. However, we don?t see
an obvious relation to the stability of MERT: the
standard deviations of BLEU average are very
similar except for two outliers: 13.15?0.24 and
13.01?0.43. One of the outliers, 13.15, is actually
a repeated run of the 13.31 with n-best-list size set
to 200. Here we see a slight increase in the effec-
tive size (20 instead of 12) but also a slight loss
in BLEU. We repeated the 13.31 experiment also
with n ? {300, 400, 500, 600}, three MERT runs for
each n. All the runs reached BLEU of about 13.30
except for one (n = 600) where the score dropped
to 11.50. The low result was obtained when MERT
ended at 25 iterations, the standard limit. On the
other hand, several successful runs also exhausted
the limit.
Figure 2 plots the BLEU scores in the 25 itera-
tions of the underperforming run with n = 600. The
MERT implementation in the Moses toolkit reports
at each iteration what we call ?predicted BLEU?,
i.e. the BLEU of translations selected by the current
255
 
0
 
0.0
2
 
0.0
4
 
0.0
6
 
0.0
8
 
0.1
 
0.1
2
 
0.1
4  0
 
5
 
10
 
15
 
20
 
25
 
0.1
285
 
0.1
29
 
0.1
295
 
0.1
3
 
0.1
305
 
0.1
31
 
0.1
315
BLEU
Iter
atio
ny2: 
Pre
dict
ed
y: P
red
icte
d
y: R
eal
Figure 2: Predicted and real devset BLEU scores.
weight settings from the (accumulated) n-best list.
We plot this predicted BLEU twice: once on the y2
axis alone and for the second time on the primary
y axis together with the real BLEU, i.e. the BLEU
of the dev set when Moses is actually run with the
weight settings. The real BLEU drops several times,
indicating that the prediction was misleading. Sim-
ilar drops were observed in all runs. With bad luck
as here, the iteration limit is reached when the opti-
mization is still recovering from such a drop.
To avoid such a pitfall, one should check the real
BLEU and continue or simply rerun the optimization
if the iteration limit was reached.
6 Two-Step Experiments
The linguistically motivated setups used in the pre-
vious sections are prohibitively expensive for large
data, see also Bojar et al (2009). A number of
researchers have thus tried diving the complexity
of search into two independent phases: (1) transla-
tion and reordering, and (2) conjugation and declina-
tion. The most promising results were obtained with
the second step predicting individual morphological
features using a specialized tool (Toutanova et al,
2008; Fraser et al, 2012). Here, we simply use one
more Moses search as Bojar and Kos (2010).
In the first step, source English gets translated to
a simplified Czech and in the second step, the sim-
plified Czech gets fully inflected.
6.1 Factors in Two-Step Setups
Two-step setups can use factors in the source, middle
or the target language. We experiment with factors
only in the middle language (affecting both the first
and the second search) and use only the form in both
source and target sides.
In the middle language, we experiment with one
or two factors. For presentation purposes, we always
speak about two factors: ?LOF? (?lemma or form?,
i.e. a representation of the lexical information) and
?MOT? (?modified tag?, i.e. representing the mor-
phological properties). In the single-factor experi-
ments the LOF and MOT are simply concatenated
into a token in the shape LOF+MOT.
Figure 3 illustrates the range of LOFs and MOTs
we experimented with. LOF0 and MOT0 are identi-
cal to the standard Czech lemma and morphological
tag as used e.g. in the Prague Dependency Treebank
(Hajic? et al, 2006).
LOF1 and MOT1 together make what Bojar and
Kos (2010) call ?pluslemma?. MOT1 is less com-
plex than the full tag by disregarding morphological
attributes not generally overt in the English source
side. For most words, LOF1 is simply the lemma,
but for frequent words, the full form is used. This
includes punctuation, pronouns and the verbs ?by?t?
(to be) and ?m??t? (to have).
MOT2 uses a more coarse grained part of speech
(POS) than MOT1. Depending on the POS, dif-
ferent attributes are included: gender and number
for nouns, pronouns, adjectives and verbs; case for
nouns, pronouns, adjectives and prepositions; nega-
tion for nouns and adjectives; tense and voice for
verbs and finally grade for adjectives. The remain-
ing grammatical categories are encoded using POS,
number, grade and negation.
6.2 Decoding Paths in Two-Step Setups
Each of the searches in the two-step setup can be
as complex as the various single-step configurations.
We test just one decoding path for the one or two
factors in the middle language.
All experiments with one middle factor (i.e. ?+?)
follow this config: tF-LOF+MOT = tLOF+MOT-F,
i.e. two direct translations where the first one pro-
duces the concatenated LOF and MOT tokens and
the second one consumes them. The first step uses a
5-gram LOF+MOT language model and the second
step uses a 5-gram LM based on forms.
This setup has the capacity to improve transla-
tion quality by producing forms of words never seen
aligned with a given source form. For example the
English word green would be needed in the parallel
256
Word Form LOF0 LOF1 MOT0 MOT1 MOT2 Gloss
lide? c?love?k c?love?k NNMP1-----A---1 NPA- NMP1-A people
by by?t by Vc------------- c--- V----- would
neoc?eka?vali oc?eka?vat oc?eka?vat VpMP---XR-NA--- pPN- VMP-RA expect
Figure 3: Examples of LOFs and MOTs used in our experiments.
Middle Factors 1 2
+ |
LOF0 +/|MOT0 11.11?0.48 12.42?0.48
LOF1 +/|MOT1 12.10?0.48 11.85?0.42
LOF1 +/|MOT2 11.87?0.51 12.47?0.51
Table 4: Two-step experiments.
data with all the morphological variants of the Czech
word zeleny?. Adding the middle step with appro-
priately reduced morphological information so that
only features overt in the source are represented in
the middle tokens (e.g. negation and number but not
the case) allows the model to find the necessary form
anywhere in the target-side data only:
green? zeleny?+NSA-?
{ zelene?ho (genitive)
zelene?mu (dative)
. . .
The experiments with two middle factors (i.e. ?|?)
use this path: tF-LOFaMOT = tLOFaMOT-F:LOF-
F. The first step is identical, except that now we use
two separate LMs, one for LOFs and one for MOTs.
The second step has two alternative decoding paths:
(1) as before, producing the form from both the LOF
and the MOT, and (2) ignoring the morphological
features from the source altogether and using just
target-side context to choose an appropriate form of
the word. This setup is capable of sacrificing ade-
quacy for a more fluent output.
6.3 Experiments with Two-Step Setups
Table 4 reports the BLEU scores when changing the
number of factors (?+? vs. ?|?) in the middle lan-
guage and the type of the LOF and MOT.
We see an interesting difference between MOT1
and MOT0 or 2. The more fine-grained MOT0 or 2
work better in the two-factor ?|? setup that allows
to disregard the MOT, while MOT1 works better in
the direct translation ?+?.
Overall, we see no improvement over the tF-F
baseline (BLEU of 12.42) and this is mainly due to
to the fact that we used Small data in both steps.
7 A Complex Moses Setup
Obviously, many setups fall under the ?complex?
category of our taxonomy, including also some sys-
tem combination approaches. We tried to combine
three Moses systems: (1) CU-BOJAR as described
below, (2) same setup like CU-BOJAR but optimized
towards 1-TER (Snover et al, 2006), and (3) a large-
data two-step setup.3 The system combination is
performed using a fourth Moses search that gets a
lattice (Dyer et al, 2008) of individual systems? out-
puts, performs an identity translation and scores the
candidates by language models and other features.
The lattice is created from the individual system out-
puts in the ROVER style (Matusov et al, 2008) uti-
lizing the source-to-hypothesis word alignments as
produced by the individual systems. We use our sim-
ple implementation for constructing the confusion
networks and converting them to the lattices. The
?combination Moses? was tuned on the WMT11 test
set towards BLEU. The resulting system is called
CU-POOR-COMB, because we felt it underperformed
the individual systems not only in BLEU but also in
an informal subjective evaluation.
Surprisingly, CU-POOR-COMB won the WMT12
automatic evaluation in TER. In the retrospect, this
is caused by TER overemphasizing word-level pre-
cision. CU-POOR-COMB skipped words not con-
firmed by several systems and its hypotheses are
shorter (18.1 toks/sent) than those by CU-BOJAR
(20.1 toks/sents) or the reference (21.9 toks/sent).
A quick manual inspection of 32 sentences suggests
that about one third or quarter of CU-POOR-COMB
suffer from some information loss whereas the rest
are acceptable or even better paraphrases. Prelim-
3The large two-step setup is identical to the one by (Bojar
and Kos, 2010), except that we use only the current Large and
Mono datasets as described in Section 3.
257
Our Scoring matrix.statmt.org
Test Set newstest-2011 newstest-2012
Metric BLEU TER*100 BLEU TER*100 BLEU TER
?cs
CU-POOR-COMB ?used?for? ?tuning? 14.17?0.53 64.07?0.53 14.0 0.741
CU-BOJAR (tFaT-FaT, lex. r.) 18.10?0.55 62.84?0.71 16.07?0.55 65.52?0.59 15.9 0.759
As ? but towards 1-TER 16.10?0.54 61.64?0.59 14.13?0.54 64.28?0.55 ? ?
Large Two-Step 17.34?0.57 63.47?0.66 15.37?0.54 65.85?0.57 ? ?
Unused (tFaT-FaT, dist. reord.) 18.07?0.56 62.74?0.70 15.92?0.57 65.50?0.60 ? ?
Unused (tF-FaT, dist. reord.) 17.85?0.58 63.13?0.68 15.73?0.55 65.85?0.58 ? ?
Unused (tF-F, lex. reord.) 17.73?0.58 63.04?0.68 15.61?0.57 65.76?0.58 ? ?
Unused (tFaT-F, dist. reord.) 17.62?0.56 62.97?0.70 15.33?0.58 65.70?0.59 ? ?
Unused (tF-F, dist. reord.) 17.51?0.57 63.32?0.69 15.48?0.56 65.79?0.58 ? ?
?en
CU-BOJAR (tF-F:tL-F, dist. reord.) 24.65?0.60 58.54?0.66 23.09?0.59 61.24?0.68 21.5 0.726
Unused (tF-F, dist. reord.) 24.62?0.59 58.66?0.66 22.90?0.56 61.63?0.67 ? ?
Table 5: Summary of large data runs and systems submitted to WMT12 manual evaluation. The upper part lists the
two submissions in en?cs translation and two more systems used in CU-POOR-COMB. The lower part of the table
shows the scores for CU-BOJAR when translating to English. All systems reported here use the Large and Mono data.
inary results of WMT 12 manual ranking indicate
that overall, our system combination performs poor.
8 Overview of Systems Submitted
Table 5 summarizes the scores for our two sys-
tem submissions. We report the scores in our to-
kenization on the official test sets of WMT11 and
WMT12 and also the scores as measured by http:
//matrix.statmt.org. Note that for the lat-
ter, we use the detokenized outputs processed by the
recommended normalization script.4
8.1 Details of CU-BOJAR for en?cs
We deliberately used only direct setups for the large
data and due to time constraints, we ran just a few
configurations, see Table 5.
We knew from previous years that including En-
glish (source) POS tag improves overall target sen-
tence structure: English words are often ambiguous
between noun and verb, so without the POS infor-
mation, verbs got often translated as nouns, render-
ing the sentence incomprehensible. Tagging and in-
cluding the source tag helps, as confirmed by the
tFaT-F setup being somewhat better than tF-F.
We also knew that target-side tag LM is helpful
(esp. if we can afford up to 8-grams in the LM).
This was confirmed by tF-FaT being better than tF-
F. Ultimately, we use tags on both sides: tFaT-FaT
4http://www.statmt.org/wmt11/
normalize-punctuation.perl
and get the best scores. This confirms that our par-
allel data is sufficiently large so that even the added
sparsity due to tags does not cause any trouble.
A little gain comes from a lexicalized reorder-
ing model (or-bi-fe) based on word forms, see CU-
BOJAR reaching 18.10 BLEU on WMT11 test set.
8.2 Details of CU-BOJAR for cs?en
For the translation into English, we tested just two
setups: tF-F and tF-F:tL-T. The latter setup falls
back to the Czech lemma, if the exact form is not
available. The gain is only small, because our paral-
lel data is already quite large.
9 Conclusion
We introduced a simple taxonomy of factored
phrase-based setups and conducted several probes
for English?Czech translation. We gained small
improvements in both small and large data settings.
We also warned about some common pitfalls: (1)
all target-side factors should be accompanied with a
language model to compensate for the added sparse-
ness, (2) alternative decoding paths significantly re-
duce the effective n-best list size, and (3) the infa-
mous instability of MERT can be caused by bad luck
at exhausted iteration limit.
On a general note, we learnt that a breadth-first
search for best configurations should be automated
as much as possible so that more human effort can
be invested into analysis.
258
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
pages 763?770, Columbus, Ohio, June. Association
for Computational Linguistics.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for english-to-arabic statistical machine
translation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics on Human Language Technologies: Short Pa-
pers, HLT-Short ?08, pages 153?156, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG Supertags in Factored Statistical Ma-
chine Translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 9?16,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 330?336, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin
Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k Z?abokrtsky?.
2009. English-Czech MT in 2008. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Compu-
tational Linguistics.
Ondr?ej Bojar. 2007. English-to-Czech Factored Machine
Translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 232?239,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL (Short Papers), pages 176?181. The
Association for Computer Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, pages 1012?1020, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proc. of EACL 2012. Associa-
tion for Computational Linguistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, Zdene?k Z?abokrtsky?, and Magda S?evc???kova?
Raz??mova?. 2006. Prague Dependency Treebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proc. of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu
Hoang. 2010. More linguistic annotation for statis-
tical machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 115?120, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose B. Marino,
Matthias Paulik, Salim Roukos, Holger Schwenk, and
Hermann Ney. 2008. System Combination for Ma-
chine Translation of Spoken and Written Language.
IEEE Transactions on Audio, Speech and Language
Processing, 16(7):1222?1237, September.
Franz Josef Och and Hermann Ney. 2000. A Comparison
of Alignment Models for Statistical Machine Transla-
tion. In Proceedings of the 17th conference on Com-
putational linguistics, pages 1086?1090. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
Modular NLP Framework. In Hrafn Loftsson, Eirikur
Ro?gnvaldsson, and Sigrun Helgadottir, editors, Lec-
ture Notes in Artificial Intelligence, Proceedings of the
7th International Conference on Advances in Natu-
ral Language Processing (IceTAL 2010), volume 6233
of Lecture Notes in Computer Science, pages 293?
259
304, Berlin / Heidelberg. Iceland Centre for Language
Technology (ICLT), Springer.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in english-hindi smt. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ?09, pages
800?808, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings AMTA, pages 223?231, August.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Sara Stymne. 2008. German Compounds in Factored
Statistical Machine Translation. In Bengt Nordstrm
and Aarne Ranta, editors, Advances in Natural Lan-
guage Processing, volume 5221 of Lecture Notes in
Computer Science, pages 464?475. Springer Berlin /
Heidelberg.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08: HLT,
pages 514?522, Columbus, Ohio, June. Association
for Computational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from english to turkish. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 454?464,
Uppsala, Sweden, July. Association for Computational
Linguistics.
260
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 374?381,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Selecting Data for English-to-Czech Machine Translation ?
Ales? Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos? Stanojevic?, Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{tamchyna,galuscakova,kamran,bojar}@ufal.mff.cuni.cz,
milosh.stanojevic@gmail.com
Abstract
We provide a few insights on data selection for
machine translation. We evaluate the quality
of the new CzEng 1.0, a parallel data source
used in WMT12. We describe a simple tech-
nique for reducing out-of-vocabulary rate af-
ter phrase extraction. We discuss the bene-
fits of tuning towards multiple reference trans-
lations for English-Czech language pair. We
introduce a novel approach to data selection
by full-text indexing and search: we select
sentences similar to the test set from a large
monolingual corpus and explore several op-
tions of incorporating them in a machine trans-
lation system. We show that this method can
improve translation quality. Finally, we de-
scribe our submitted system CU-TAMCH-BOJ.
1 Introduction
Selecting suitable data is important in all stages of
creating an SMT system. For training, the data size
plays an essential role, but the data should also be as
clean as possible. The new CzEng 1.0 was prepared
with the emphasis on data quality and we evaluate
it against the previous version to show whether the
effect for MT is positive.
Out-of-vocabulary rate is another problem related
to data selection. We present a simple technique to
reduce it by including words that became spurious
OOVs during phrase extraction.
? This work was supported by the project EuroMatrixPlus
(FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of
the Czech Republic) and the Czech Science Foundation grants
P406/11/1499 and P406/10/P259.
Another topic we explore is to use multiple refer-
ences for tuning to make the procedure more robust
as suggested by Dyer et al (2011). We evaluate this
approach for translating from English into Czech.
The main focus of our paper however lies in pre-
senting a method for data selection using full-text
search. We index a large monolingual corpus and
then extract sentences from it that are similar to the
input sentences. We use these sentences in several
ways: to create a new language model, a new phrase
table and a tuning set. The method can be seen as
a kind of domain adaptation. We show that it con-
tributes positively to translation quality and we pro-
vide a thorough evaluation.
2 Data and Tools
2.1 Comparison of CzEng 1.0 and 0.9
As this year?s WMT is the first to include the new
version of CzEng (Bojar et al, 2012b), we carried
out a few experiments to compare its suitability for
MT with its predecessor, CzEng 0.9. Apart from
size (which has almost doubled), there are impor-
tant differences between the two versions. In CzEng
0.9, the largest portion by far came from movie sub-
titles (a data source of varying quality), followed by
EU legislation and technical manuals. On the other
hand, CzEng 1.0 has over 4 million sentence pairs
from fiction and nearly the same amount of data
from EU legislation. Roughly 3 million sentence
pairs come from movie subtitles. This proportion
of domains suggests a higher quality of data. More-
over, sentences in CzEng 1.0 were automatically fil-
tered using a maximum entropy classifier that uti-
374
Vocab. [k]
Corpus and Domain Sents BLEU En Cs
CzEng 0.9
all 1M
14.77?0.12 187 360
CzEng 1.0 15.23?0.18 221 396
CzEng 0.9
news 100k
14.34?0.05 53 125
CzEng 1.0 14.01?0.13 47 113
Table 1: Comparison of CzEng 0.9 and 1.0.
lized a variety of features.
We trained contrastive phrase-based Moses SMT
systems?the first one on 1 million randomly se-
lected sentence pairs from CzEng 0.9, the other on
the same amount of data from CzEng 1.0. Another
contrastive pair of MT systems was based on small
in-domain data only: 100k sentences from the news
sections of CzEng 0.9 and 1.0. For each experiment,
the random selection was done 5 times. In both
experiments, identical data were used for the LM
(News Crawl corpus from 2011), tuning (WMT10
test set) and evaluation (WMT11 test set).
Table 1 shows the results. The ? sign in this case
denotes the standard deviation over the 5 experi-
ments (each with a different random sample of train-
ing data). The results indicate that overall, CzEng
1.0 is a more suitable source of parallel data?most
likely thanks to the more favorable distribution of
domains. However in the small in-domain setting,
using CzEng 0.9 data resulted in significantly higher
BLEU scores.
The vocabulary size of the news section seems to
have dropped since 0.9. We attribute this to the filter-
ing: sentences with obscure words are hard to align
so they are likely to be filtered out (the word align-
ment score as output by Giza++ received a large
weight in the classifier training). These unusual
words then do not appear in the vocabulary.
2.2 Lucene
Apache Lucene1 is a high performance open-source
search engine library written in Java. We use Lucene
to take advantage of the information retrieval (IR)
technique for domain adaptation. Each sentence of
a large corpus is indexed as a separate document; a
document is the unit of indexing and searching in
Lucene. The sentences (documents) can then be re-
1http://lucene.apache.org
trieved based on Lucene similarity formula2, given
a ?query corpus?. Lucene uses Boolean model for
initial filtering of documents. Vector Space Model
with a refined version of Tf-idf statistic is then used
to score the remaining candidates.
In the normal IR scenario, the query is usually
small. However, for domain adaptation a query can
be a whole corpus. Lucene does not allow such
big queries. This problem is resolved by taking
the query corpus sentence by sentence and search-
ing many times. The final score of a sentence in the
index is calculated as the average of the scores from
the sentence-level queries. Methods that make use
of this functionality are discussed in Section 5.
3 Reducing OOV by Relaxing Alignments
Out-of-vocabulary (OOV) rate has been shown to
increase during phrase extraction (Bojar and Kos,
2010). This is due to unfortunate alignment of some
words?no consistent phrase pair that includes them
can be extracted. This issue can be partially over-
come by adding translations of these ?lost? words
(according to Giza++ word alignment) to the ex-
tracted phrase table. This is not our original tech-
nique, it was suggested by Mermer and Saraclar
(2011), though it is not included in the published ab-
stract.
The extraction of phrases in the (hierarchical) de-
coder Jane (Stein et al, 2011) offers a range of sim-
ilar heuristics. Tinsley et al (2009) also observes
gains when extending the set of phrases consistent
with the word alignment by phrases consistent with
aligned parses.
We evaluated this technique on two sets of train-
ing data?the news section of CzEng 1.0 and the
whole CzEng 1.0. The OOV rate of the phrase table
was reduced nearly to the corpus OOV rate in both
cases, however the improvement was negligible?
only a handful of the newly added words occurred
in the test set. Table 2 shows the results. Trans-
lation performance using the improved phrase table
was identical to the baseline.
2http://tiny.cc/ca2ccw
375
Test Set OOV % New
CzEng Sections Baseline Reduced Phrases
news (197k sents) 3.69 3.66 12034
all (14.8M sents) 1.09 1.09 154204
Table 2: Source-side phrase table OOV.
Sections 1 reference 3 references
news 11.37?0.47 11.62?0.50
all 16.07?0.55 15.90?0.57
Table 3: BLEU scores on WMT12 test set when tuning
on WMT11 test set towards one or more references.
4 Tuning to Multiple Reference
Translations
Tuning towards multiple reference translations has
been shown to help translation quality, see Dyer et
al. (2011) and the cited works. Thanks to the other
references, more possible translations of each word
are considered correct, as well as various orderings
of words.
We tried two approaches: tuning to one true refer-
ence and one pseudo-reference, and tuning to multi-
ple human-translated references.
For the first method, which resembles computer-
generated references via paraphrasing as used in
(Dyer et al, 2011), we created the pseudo-reference
by translating the development set using TectoMT,
a deep syntactic MT with rich linguistic processing
implemented in the Treex platform3. We hoped that
the very different output of this decoder would be
beneficial for tuning, however we achieved no im-
provement at all.
For the second experiment we used 3 translations
of WMT11 test set. One is the true reference dis-
tributed for the shared task and two were translated
manually from the German version of the data into
Czech. We achieved a small improvement in final
BLEU score when training on a small data set. On
the complete constrained training data for WMT12,
there was no improvement?in fact, the BLEU score
as evaluated on the WMT12 test set was negligibly
lower. Table 3 summarizes our results. The ? sign
denotes the confidence bounds estimated via boot-
strap resampling (Koehn, 2004).
3http://ufal.ms.mff.cuni.cz/treex/
Used Selected Sel. Sents Avg
Models per Trans. Total BLEU?std
None ? 0 12.39?0.06
LM ? 16k ? rand. sel. 12.18?0.06
LM 3 16k 12.73?0.04
LM 100 502k 14.21?0.11
LM 1000 3.8M 15.12?0.08
LM All Sents 18.3M 15.55?0.11
Table 4: Results of experiments with Lucene, language
model adapted.
5 Experiments with Domain Adaptation
Domain adaptation is widely recognized as a tech-
nique which can significantly improve translation
quality (Wu et al, 2008; Bertoldi and Federico,
2009; Daume? and Jagarlamudi, 2011). In our ex-
periments we tried to select sentences close to the
source side of the test set and use them to improve
the final translation.
The parallel data used in this section are only
small: the news section of CzEng 1.0 (197k sentence
pairs, 4.2M Czech words, 4.8M English words). We
tuned the models on WMT09 test set and evaluated
on WMT11 test set. The techniques examined here
rely on a large monolingual corpus to select data
from. We used all the monolingual data provided by
the organizers of WMT11 (18.3M sentences, 316M
words).
5.1 Tailoring the Language Model
Our first attempt was to tailor the language model
to the test set. Our approach is similar to Zhao et
al. (2004). In Moore and Lewis (2010), the authors
compare several approaches to selecting data for LM
and Axelrod et al (2011) extend their ideas and ap-
ply them to MT.
Naturally, we only used the source side of the test
set. First we translated the test set using a baseline
translation system. Lucene indexer was then used
to select sentences similar to the translated ones in
the large target-side monolingual corpus. Finally, a
new language model was created from the selected
sentences.
The weight of the new LM has to reflect the im-
portance of the language model during both MERT
tuning as well as final application on (a different)
test set. If the new LM were based only on the final
376
test set, MERT would underestimate its value and
vice versa. Therefore, we actually translated both
our development (WMT09) as well as final test set
(WMT11) using the baseline model and created a
LM relevant to their union.
The results of performed experiments with do-
main adaptation are in Table 4. To compensate for
low stability of MERT, we ran the optimization five
times and report the average BLEU achieved. The
? value indicates the standard deviation of the five
runs.
The first row provides the scores for the baseline
experiment with no tailored language model. We
have run the experiment for three values of selected
sentences per one sentence of the test corpus: 3,
100 and 1000 closest-matching sentences were ex-
tracted. With more and more data in the LM, the
scores increase. The second line in Table 4 confirms
the usefulness of the sentence selection. Picking the
same amount of 16k sentences randomly performs
worse. As the last row indicates, taking all available
data leads to the best score.
Note that when selecting the sentences, we used
lemmas instead of word forms to reduce data sparse-
ness. So Lucene was actually indexing the lemma-
tized version of the monolingual data and the base-
line translation translated English lemmas to Czech
lemmas when creating the ?query corpus?. The final
models were created from the original sentences, not
their lemmatized versions.
5.2 Tailoring the Translation Model
Reverse self-training is a trick that allows to improve
the translation model using (target-side) monolin-
gual data and can lead to a performance improve-
ment (Bojar and Tamchyna, 2011; Lambert et al,
2011).
In our scenario, we translated the selected sen-
tences (in the opposite direction, i.e. from the target
into the source language). Then we created a new
translation model (in the original direction) based on
the alignment of selected sentences and their reverse
translation. This new model is finally combined with
the baseline model and weighted by MERT. The
whole scenario is shown in Figure 1.
The results of our experiments are in Table 5. We
ran the experiment with translation model adaptation
for 100 most similar sentences selected by Lucene.
Each experiment was again performed five times.
Due to the low stability of tuning, we also tried in-
creasing the size of n-best lists used by MERT.
Experiments with tailored translation model are
significantly better than the baseline but the im-
provement against the experiment with only the lan-
guage model adapted (with the corresponding 100
sentences selected) is very small.
5.3 Discussion of Domain Adaptation
Experiments
According to the results, using Lucene improves
translation performance already in the case when
only three sentences are selected for each translated
sentence. Our results are further supported by the
contrastive setup that used a language model cre-
ated from a random selection of the same number of
sentences?the translation quality even slightly de-
graded.
On the other hand, adding more sentences to lan-
guage model further improves results and the best
result is achieved when the language model is cre-
ated using the whole monolingual corpus. This
could have two reasons:
Too good domain match. The domain of the
whole monolingual corpus is too close to the test
corpus. Adding the whole monolingual corpus is
thus the best option. For more diverse monolingual
data, some domain-aware subsampling like our ap-
proach is likely to actually help.
Our style of retrieval. Our queries to Lucene
represent sentences as simple bags of words. Lucene
prefers less frequent words and the structure of the
sentence is therefore often ignored. For example it
prefers to retrieve sentences with the same proper
name rather than sentences with similar phrases or
longer expressions. This may not be the best option
for language modelling.
Our method can thus be useful mainly in the case
when the data available are too large to be processed
as a whole. It can also highly reduce the compu-
tation power and time necessary to achieve good
translation quality: the result achieved using the lan-
guage model created via Lucene for 1000 selected
sentences is not significantly worse than the result
achieved using the whole monolingual corpus but
the required data are 5 times smaller.
377
Tes
t Se
t [EN
]
Tra
nsl
ate
d T
S [C
S]
Sen
ten
ces
 Sim
ilar
 to 
Tra
nsl
ate
d T
S [C
S]
Re
ver
se 
Tra
nsl
ate
d S
ent
enc
es 
Sim
ilar
 to 
Tra
nsl
ate
d T
S [E
N]
Luc
ene
Ba
sel
ine
 Tr
ans
lati
on 
[EN
->C
S]
Do
ma
in A
dap
ted
 LM
Re
ver
se 
Tra
nsl
atio
n T
M
Re
ver
se 
Tra
nsl
atio
n [C
S->
EN]
Ori
gin
al L
M
Ori
gin
al T
M
Tes
t Se
t [EN
]
Tra
nsl
ate
d T
est
 Se
t [C
S]
Fin
al T
ran
sla
tion
 [EN
->C
S]
Figure 1: Scenario of reverse self-training.
Used N-Best Sel. Sents Sel. Sents Avg
Models per Trans. Sent. Total BLEU?std
None 100 ? 0 12.39?0.06
None 200 ? 0 12.4?0.03
LM + TM 100 100 502k 14.32?0.13
LM + TM 200 100 502k 14.36?0.07
Table 5: Results of experiments with Lucene, translation model applied.
5.4 Tuning Towards Selected Data
Domain adaptation can also be done by selecting a
suitable development corpus (Zheng et al, 2010; Li
et al, 2004). The final model parameters depend on
the domain of the development corpus. By choos-
ing a development corpus that is close to our test
set we might tune in the right direction. We imple-
mented this adaptation by querying the source side
of our large parallel corpus using the source side of
the test corpus. After that, the development corpus
is constructed from the selected sentences and their
corresponding reference translations.
This experiment uses a fixed model based on the
news section of CzEng 1.0. We only use different
tuning sets and run the MERT optimization. All the
resulting systems are tested on the WMT11 test set:
Baseline system is tuned on 2489 sentence pairs
selected randomly from whole CzEng 1.0 parallel
corpus. Lucene system uses 2489 sentence pairs se-
lected from CzEng 1.0 using Lucene. The selection
is done by choosing the most similar sentences to the
source side of the final test set. WMT10 system is
System avg BLEU?std
Baseline 11.41?0.25
Lucene 12.31?0.01
WMT10 12.37?0.02
Perfect selection 12.64?0.02
Bad selection 6.37?0.64
Table 6: Results of tuning with different corpora
tuned on 2489 sentence pairs of WMT10 test set. To
identify an upper bound, we also include a Perfect
selection system which is tuned on the final WMT11
test set. Naturally, this is not a fair competitor.
In order to make the results more reliable, it is
necessary to repeat the experiment several times
(Clark et al, 2011). Lucene and the WMT10 system
were tuned 3 times while baseline system was tuned
9 times because of randomness in selection of tun-
ing corpora (3 different tuning corpora each tuned 3
times). The results are shown in Table 6.
Even though the variance of the baseline system
is high (because we randomly selected corpora 3
378
times), the difference in scores between baseline
and Lucene system is high enough to conclude that
tuning on Lucene-selected corpus helps translation
quality. Still it does not give better BLEU score
than system tuned on WMT10 corpus. One possi-
ble reason is that the whole CzEng 1.0 is of some-
what lower quality than the news section. Given that
our final test set (WMT11) is also from the news
domain, tuning towards WMT10 corpus probably
leads to a better domain adaptation that tuning to-
wards all the domains in CzEng.
The tuning set must not overlap with the training
set. To illustrate the problem, we did a small exper-
iment with the same settings as above and randomly
selected 2489 sentences from training corpora. We
again ran the random selection 3 times and tuned 3
times with each of the extracted tuning sets, see the
?Bad selection? in Table 6.
In all the experiments with badly selected sen-
tences, the distortion and language model get an
extremely low weight compared to the weights of
translation model. This is because they are not use-
ful in translation of tuning data which was already
seen during training. Instead of reordering two short
phrases A and B, system already knows the transla-
tion of the phrase A B so no distortion is needed. On
unseen sentences, such weights lead to poor results.
This amplifies a drawback of our approach:
source texts have to be known prior to system tuning
or even before phrase extraction.
There are methods available that could tackle this
problem. Wuebker et al (2010) store phrase pair
counts per sentence when extracting phrases and
thus they can reestimate the probabilities when a
sentence has to be excluded from the phrase tables.
For large parallel corpora, suffix arrays (Callison-
Burch et al, 2005) have been used. Suffix arrays
allow for a quick retrieval of relevant sentence pairs,
the phrase extraction is postponed and performed on
the fly for each input sentence. It is trivial to fil-
ter out sentences belonging to the tuning set during
this delayed extraction. With dynamic suffix arrays
(Levenberg et al, 2010), one could even simply re-
move the tuning sentences from the suffix array.
6 Submitted Systems
This paper covers the submissions CU-TAMCH-BOJ.
We translated from English into Czech. Our setup
was very similar to CU-BOJAR (Bojar et al, 2012a),
but our primary submission is tuned on multiple ref-
erence translations as described in Section 4.
Apart from the additional references, this is a con-
strained setup. CzEng 1.0 were the only parallel data
used in training. We used a factored model to trans-
late the combination of English surface form and
part-of-speech tag into Czech form+POS. We used
separate 6-gram language models trained on CzEng
1.0 (interpolated by domain) and all News Crawl
corpora (18.3M setences, interpolated by years).
Additionaly, we created an 8-gram language model
on target POS tags. For reordering, we employed a
lexicalized model trained on CzEng 1.0.
Table 7 summarizes the official result of the pri-
mary submission and a contrastive baseline (tuned to
just one reference translation). There is a slight de-
crease in BLEU, but the translation error rate (TER)
is slightly better when more references were used.
The differences are however very small, suggesting
that tuning to more references did not have any sig-
nificant effect.
System BLEU TER
multiple references 14.5 0.765
contrastive baseline 14.6 0.774
Table 7: Scores of the submitted systems.
7 Conclusion
We showed that CzEng 1.0 is of better overall qual-
ity than its predecessor. We described a technique
for reducing phrase-table OOV rate, but achieved no
improvement for WMT12. Similarly, tuning to mul-
tiple references did not prove very beneficial.
We introduced a couple of techniques that exploit
full-text search in large corpora. We showed that
adding selected sentences as an additional LM im-
proves translations. Adding a new phrase table ac-
quired via reverse self-training resulted only in small
gains. Tuning to selected sentences resulted in a
better system than tuning to a random set. How-
ever the Lucene-selected corpus fails to outperform
good-quality in-domain tuning data.
379
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 355?362, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 182?189, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran. 2012a.
Probes in a Taxonomy of Factored Phrase-Based Mod-
els. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics. Submit-
ted.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The Joy of Parallelism with CzEng
1.0. In Proceedings of LREC2012, Istanbul, Turkey,
May. ELRA, European Language Resources Associa-
tion. In print.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the ACL, pages 255?262.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
2011. Better Hypothesis Testing for Statistical Ma-
chine Translation: Controlling for Optimizer Instabil-
ity. In Proceedings of the Association for Computa-
tional Lingustics. Association for Computational Lin-
guistics.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies: short papers - Vol-
ume 2, HLT ?11, pages 407?412, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English Translation System. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 337?343, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on trans-
lation model adaptation using monolingual data. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 284?293, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 394?402.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2004. Adaptive development data selection for
log-linear model in statistical machine translation. In
In Proceedings of COLING 2004.
Coskun Mermer and Murat Saraclar. 2011. Un-
supervised Turkish Morphological Segmentation for
Statistical Machine Translation. Abstract at Ma-
chine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Daniel Stein, David Vilar, Stephan Peitz, Markus Freitag,
Matthias Huck, and Hermann Ney. 2011. A Guide to
Jane, an Open Source Hierarchical Translation Toolkit.
Prague Bulletin of Mathematical Linguistics, 95:5?18,
March.
John Tinsley, Mary Hearne, and Andy Way. 2009. Ex-
ploiting parallel treebanks to improve phrase-based
statistical machine translation. In Alexander F. Gel-
bukh, editor, CICLing, volume 5449 of Lecture Notes
in Computer Science, pages 318?331. Springer.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 993?1000, Stroudsburg, PA, USA. Association
for Computational Linguistics.
380
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
475?484.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and Hao
Yu. 2010. Domain adaptation for statistical machine
translation in development corpus selection. In Uni-
versal Communication Symposium (IUCS), 2010 4th
International, pages 2 ?7, oct.
381
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 37?42,
Dublin, Ireland, August 23-29 2014.
English to Urdu Statistical Machine Translation: Establishing a
Baseline
Bushra Jawaid, Amir Kamran and Ond?ej Bojar
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?m. 25, Praha 1, CZ-118 00, Czech Republic
jawaid,kamran,bojar@ufal.mff.cuni.cz
Abstract
The aim of this paper is to categorize and present the existence of resources for English-
to-Urdu machine translation (MT) and to establish an empirical baseline for this task.
By doing so, we hope to set up a common ground for MT research with Urdu to allow
for a congruent progress in this field. We build baseline phrase-based MT (PBMT) and
hierarchical MT systems and report the results on 3 official independent test sets. On all
test sets, hierarchial MT significantly outperformed PBMT. The highest single-reference
BLEU score is achieved by the hierarchical system and reaches 21.58% but this figure
depends on the randomly selected test set. Our manual evaluation of 175 sentences
suggests that in 45% of sentences, the hierarchical MT is ranked better than the PBMT
output compared to 21% of sentences where PBMT wins, the rest being equal.
1 Introduction
Statistical Machine Translation (SMT) has always been a challenging task for language pairs with
significant word ordering differences and rich inflectional morphology. The language pair such
as English and Urdu, despite of descending from the same family of Indo-European languages,
differs heavily in syntactic strucure and morphological characteristics. English is relatively
fixed word order language and follows subject-verb-object (SVO) structure whereas Urdu uses
restricted free word order language and most commonly follows the SOV pattern. Urdu word
order is restricted for only few parts of speeches such as adjectives always precede nouns and
postpositions follow nouns. Unlike English, Urdu is a pro-drop language. The morphology of
Urdu is similar to other Indo-European languages, e.g. by having inflectional morphological
system.
To the best of our knowledge, the research on English-to-Urdu machine translation has been
very much fragmented, preventing the authors to build upon the works of others. Our underlying
motivation for this paper is to establish a common ground and provide a concise summary of
available data resources and set up reproducible baseline results of several available test sets.
With this basis, future Urdu MT research should be able to stepwise improve the state of the
art, in contrast with the scattered experiments done so far (Khan et al., 2013; Ali et al., 2013;
Ali and Malik, 2010).
In Section 2, the experimental setup and data processing tools are described. Existing corpora
are introduced in Section 3, automatic results are reported in Section 4 and manual evaluation
is discussed in Section 5.
2 Experimental Setup
This section briefly introduces the selection of SMT models that are used to build the baseline
English-Urdu SMT system and also explains the processing of parallel data before passing it to
the MT system.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and
proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.
0/
37
2.1 Two Models of SMT
The state-of-the-art MT toolkit Moses1 (Koehn et al., 2007), offers two mainstream models of
SMT: phrase-based (PBMT) and syntax-based (SBMT) that includes the hierarchical model.
The PBMT model operates only on mapping of source phrases (short sequences of words) to
target phrases. For dealing with word order differences, two rather weak models are available:
lexicalized and distance-based. The lexicalized reordering models (Tillmann, 2004) are consid-
ered more advanced as they condition reordering on the actual phrases, whereas the latter model
makes the reordering cost (paid when picking source phrases out of sequence) dependent only
on the length of the jump. The distance-based model is suited well for local reordering but it is
fairly weak in capturing any long distance reorderings.
The syntax-based model (SBMT) builds upon Synchronous Context-Free Grammar (SCFG)
that synchronously generates source and target sentences. The grammar rules can either consist
of linguistically motivated non-terminals such as NP, VP etc. or the generic non-terminal ?X?
in which case the model is called ?hierarchical phrase-based? (Chiang, 2005; Chiang, 2007). In
either case, the model is capable of capturing long-distance reordering much better than the
lexicalized reordering of PBMT.
2.2 Data Processing and MT Training
For the training of our en-ur translation systems, the standard training pipeline of Moses is used
along with the GIZA++ (Och and Ney, 2000) alignment toolkit and a 5-gram SRILM language
model (Stolcke, 2002). The source texts were processed using the Treex platform (Popel and
?abokrtsk?, 2010)2, which included tokenization and lemmatization.
The target side of the corpus is tokenized using a simple tokenization script3 by Dan Zeman and
it is lemmatized using the Urdu Shallow Parser4 developed by Language Technologies Research
Center of IIIT Hyderabad.
The alignments are learnt from the lemmatized version of the corpus. In all other cases,
word forms (i.e. no morphological decomposition) in their true case (i.e. names capitalized but
sentence starts lowercased) are used. The lexicalized reordering model uses the feature set called
?msd-bidirectional-fe?.
3 Dataset
Parallel and monolingual data resources are very scarce for low-resource language pairs such as
English-Urdu. This section highlights the existing parallel and monolingual data resources that
can be utilized for training SMT models. The number of official test sets are also exhibited.
3.1 Parallel Corpus
Our parallel corpus consists of around 79K sentences collected from five different sources. The
collection comes from several domains such as News, Religion, Technology, Language and Culture
etc. 95% of the data is used for training, whereas the rest is evenly split into dev and test sets.
? Emille: EMILLE (Enabling Minority Language Engineering) (Baker et al., 2002) is a col-
lection of monolingual (written and spoken), parallel and annotated corpora of fourteen
South Asian languages which is distributed by the European Language Resources Associa-
tion (ELRA). The Urdu-English part are documents produced by the British Departments
of Health, Social Services, Education and Skills, and Transport, Local Government and the
Regions of British government translated into Urdu.
In this work, the manually sentence aligned version of English-Urdu Emille corpus Jawaid
and Zeman (2011) is used.
1http://statmt.org/moses/
2http://ufal.mff.cuni.cz/treex/
3The tokenization script can be downloaded from: http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5
4http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php
38
? IPC: The Indic Parallel Corpus (Post et al., 2012)5 is a collection of Wikipedia documents
of six Indian sub-continent languages translated into English through crowdsourcing in the
Amazon Mechanical Turk (MTurk) platform.
The English-Urdu part generally contains four (in some cases three) English translations
for each Urdu sentence. In a separate MTurk task, the Turkers voted which of the English
translations is the best one. The official training, dev and devtest sets is first merged and
afterwards the voting list is used to retrieve only the winning English sentence ignoring
sentences with no votes altogether. The official testset is left unaltered to report our final
results on this data.
? Quran: The publicly available parallel English and Urdu translation of Quranic data6 is
used, which is collected by Jawaid and Zeman (2011) in their work. The data consists of
6K aligned parallel sentences.
? Penn Treebank: Penn Treebank (Marcus et al., 1993) is an annotated corpus of around
4.5 million words originating from Wall Street Journal (WSJ), Brown corpus, Switchboard
and ATIS. The entire treebank in English is released by the Linguistic Data Consortium
(LDC). A subset of the WSJ section whose Urdu translations are provided by Center for
Language Engineering (CLE)7 is used. Out of 2,499 WSJ stories in the Treebank, only 317
are available in Urdu.
? Afrl: Afrl, the largest of the parallel resources we were able to get, is not publicly available.
The corpus originally consists of 87K sentences coming from mix of several domains mainly
news articles. The sentence alignments are manually checked of almost two thirds of the
corpus, around 4K misaligned and 30K duplicate sentences are discarded.
The statistics shown in Table 1 are reported after removing duplicated sentences from each
source. Almost all parallel corpora contained at least tens or hundreds of duplicate sentences.
Afrl on the other hand contained larger chunks of Emille and also smaller subset of Penn Tree-
bank. Around 3K sentences from Afrl that were seen in Emille are discarded but the Penn
Treebank subset of Afrl is left intact because it provides different Urdu translations.
Each parallel corpus is randomly split into train, dev and test sets according to its relative
size.
Corpus Sentences Tokens % of Data Train Dev Test
EN UR
AFRL 50,313 960,683 1,022,563 63.6% 47,769 1,272 1,272
EMILLE 8,629 152,273 199,320 10.9% 8,193 218 218
IPC 7,478 118,644 132,968 9.46% 7,098 190 190
QURAN 6,364 251,387 269,947 8.05% 6,040 162 162
PENN 6,204 158,727 179,457 7.86% 5,888 158 158
TOTAL 78,988 - - 100% 74,988 2,000 2,000
Table 1: Statistics of English-Urdu parallel corpora.
3.2 Monolingual Corpus
Jawaid et al. (2014) release8 a plain and annotated Urdu monolingual corpus of around 95.4
million tokens distributed in around 5.4 million sentences. The monolingual corpus is a mix
5http://joshua-decoder.org/data/indian-parallel-corpora/
6http://ufal.mff.cuni.cz/legacy/umc/005-en-ur/
7http://www.cle.org.pk/software/ling_resources/UrduNepaliEnglishParallelCorpus.htm
8http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5
39
of domains such as News, Religion, Blogs, Literature, Science, Education etc. Only plain text
monolingual data is used to build our language model.
3.3 Official Testsets
In addition to the testset that is created from the parallel corpora resources, results are reported
on three official testsets.
NIST 2008 Open Machine Translation (OpenMT) Evaluation9 has distributed test data from
2 domains: Newswire and Web. The Web data is collected from user forums, discussion groups
and blogs, whereas Newswire data is a mix of newswire stories and data from web. The test data
contain 4 English translations for each Urdu sentence, the first English translation is picked in
all cases. Because the majority of test sets are created in order to faciliatate Urdu-to-English
MT, most of them contain multiple English references against each Urdu source.
Another testset is released with the IPC. Only those sentences are used whose ids are present
in the voting list. The domain of the IPC test set is discussed in Section 3.1.
CLE10 has published small test set from News domain specifically for MT evaluation. The
test data contains 3 Urdu references against each source. All reference translations are used for
the evaluation.
Table 2 shows the number of sentences in each test set that are used for the final evaluation.
We also report the coverage of each test set (calculated on vocabulary size) i.e. how many source
words in a test set were seen in the training data. The notions used in Table 2 to introduce
coverage are explained in Section 4.
NIST 2008 IPC CLE
NewsWire Web Test
Sentences 400 600 544 400
Coverage ALL 84% 91% 90% 87%Except-Afrl 80% 87% 88% 84%
Table 2: Statistics of official English-Urdu test sets.
4 Results
The BLEU metric (Papineni et al., 2002) has been used to evaluate the performance of the
systems. Models are trained on two different datasets: all parallel corpora (referred as ?ALL?)
and parallel data excluding Afrl corpus (referred as ?Except-Afrl?). The latter model is trained
due to the fact that Afrl corpus is publicly not available. The community working on English-
Urdu machine translation can thus have one common baseline that could be used to evaluate
their improved systems in the future. Including Afrl allows us to see the gains in performance
thanks to the additional data.
Table 3 shows the baseline results of phrase-based and hierarchical systems when trained on
both datasets. The results are reported on two test sets: the test set of 2,000 sentences (called
Large in Table 3) as shown in Table 1 and its subset of 728 sentences which excludes 1,272 test
sentences from Afrl (called Small in Table 3).
PBMT performs better when integrated with lexicalized reordering model but Hierarchical
MT outperforms both PBMT setups on both smaller and larger test sets. The absolute BLEU
scores drop by up to 6 points when Afrl is removed from the training data, however they return
back to ?20 when Afrl is also removed from the test set. This highlights the importance of data
overall and the match in domain in particular, as supported by the differences in vocabulary
coverage (see the column ?Coverage? in Table 3).
Table 4 shows the results of the best performing setups (i.e. phrase-based with lexicalized
reordering model and hierarchical model) trained on both training datasets and evaluated on the
9http://catalog.ldc.upenn.edu/LDC2010T21
10http://www.cle.org.pk/software/ling_resources/testingcorpusmt.htm
40
Parallel Corpora Test Set Phrase-based Phrase-based-LexReo Hierarchical Coverage
ALL Large 18.30?0.74 19.19?0.72 21.35?0.84 92%
Except-Afrl Large 12.85?0.74 13.78?0.73 15.11?0.82 78%
Except-Afrl Small 18.41?1.25 19.67?1.27 21.21?1.55 91%
Table 3: Results of Phrase-based, Phrase-based with Lexical Reordering and Hierarchical MT
systems.
official test sets. The BLEU score for CLE test set is reported using all 3 reference translations
at once as well as the average of single-reference BLEUs, taking each reference translation
separately. IPC and NIST2008 results are evaluated on a single reference.
The hierarchical MT performs significantly better than the phrase-based MT on all test sets.
The lowest scores were achieved on the NIST2008 test set but it is difficult to pinpoint any
specific reason (other than some domain difference) because the coverage is comparable to other
test sets (see Table 2). Across all the test sets, Afrl corpus brings about 2 points BLEU absolute.
CLE IPC NIST2008
3 refs 1 ref (avg.) 1 ref 1 ref
ALL Phrase-based-LexReo 18.19?1.19 11.12?1.02 15.82?1.36 15.13?0.95Hierarchical 19.29?1.31 11.81?1.09 18.70?1.64 16.69?1.06
Except-Afrl Phrase-based-LexReo 16.53?1.13 9.92?0.96 13.82?1.20 11.65?0.87Hierarchical 18.48?1.28 11.30?1.03 16.91?1.54 13.01?0.84
Table 4: Results of Phrase-based and Hierarchical systems on official test sets.
5 Manual Evaluation
To manually analyze the output of best performing models sample of 175 sentences is randomly
selected from the large test set translated using both PBMT with lexical reordering and hier-
archical models trained on ?ALL? data sets. QuickJudge11 is used to rank the outputs. The
annotator is shown the source, reference and output from both machine translation systems, the
identity of the MT systems is not known. There are four permitted outcomes of the ranking:
both systems marked as equally good; both systems are equally bad or the output of one of the
systems is better than the other one. Here is the summary of annotation by a single annotator:
? Out of 175 sentences, 41 sentences received equally bad translations from both systems.
? 17 items are marked as equally good.
? In 79 cases, the hierarchical MT is ranked better than the phrase-based MT.
? In the remaining 38 cases, the phrase-based MT is ranked better than the hierarchical MT.
The results from the manual ranking show that the hierarchical systems wins twice more often
than PBMT. The two systems tie in about one third of input sentences, of which about 70 %
are cases where the translations are bad.
6 Conclusion
In this work, a collection of sizeable English-Urdu corpora is summarized for statistical machine
translation. These resources are used to build baseline phrase-based and hierarchical MT systems
for translation into Urdu and the results are reported on 3 independent official test sets. This
11http://ufal.mff.cuni.cz/project/euromatrix/quickjudge/
41
can hopefully serve as a baseline for a wider community of researchers. The output of both
translation models is manually analyzed and it confirms that the hierarchical model is preferred
over phrase-based MT for English-to-Urdu translation.
Acknowledgments
This work has been using language resources developed and/or stored and/or distributed
by the LINDAT-Clarin project of the Ministry of Education of the Czech Republic (project
LM2010013). This work was also supported by the grant FP7-ICT-2011-7-288487 (MosesCore)
of the European Union.
References
Aasim Ali and Muhmmad Kamran Malik. 2010. Development of parallel corpus and english to urdu
statistical machine translation. Int. J. of Engineering & Technology IJET-IJENS, 10:31?33.
Aasim Ali, Arshad Hussain, and Muhammad Kamran Malik. 2013. Model for english-urdu statistical
machine translation. World Applied Sciences, 24:1362?1367.
Paul Baker, Andrew Hardie, Tony McEnery, Hamish Cunningham, and Robert J. Gaizauskas. 2002.
Emille, a 67-million word corpus of indic languages: Data collection, mark-up and harmonisation. In
LREC. European Language Resources Association.
David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of
ACL, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201?228, June.
Bushra Jawaid and Daniel Zeman. 2011. Word-order issues in english-to-urdu statistical machine trans-
lation. Number 95, pages 87?106, Praha, Czechia.
Bushra Jawaid, Amir Kamran, and Ond?ej Bojar. 2014. A Tagged Corpus and a Tagger for Urdu (to
appear). Reykjav?k, Iceland. European Language Resources Association. In print.
Nadeem Khan, Waqas Anwar, Usama Ijaz Bajwa, and Nadir Durrani. 2013. English to urdu hierarchical
phrase-based statistical machine translation. In The 4th Workshop on South and Southeast Asian NLP
(WSSANLP), IJCNLP, pages 72?76, Nagoya, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In
Proc. of ACL Companion Volume, Demo and Poster Sessions, pages 177?180, Prague, Czech Republic.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Comput. Linguist., 19(2):313?330, June.
Franz Josef Och and Hermann Ney. 2000. A Comparison of Alignment Models for Statistical Machine
Translation. In Proc. of COLING, pages 1086?1090. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. of ACL, pages 311?318.
Martin Popel and Zden?k ?abokrtsk?. 2010. TectoMT: Modular NLP Framework. In Lecture Notes in
Artificial Intelligence, Proceedings of the 7th International Conference on Advances in Natural Language
Processing (IceTAL 2010), volume 6233 of Lecture Notes in Computer Science, pages 293?304. Springer.
Matt Post, Chris Callison-Burch, and Miles Osborne. 2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proc. of WMT, ACL, pages 401?409, Montr?al, Canada.
Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In In Proceedings of the 7th
International Conference on Spoken Language Processing (ICSLP) 2002, pages 901?904.
Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proc. of
HLT-NAACL Short Papers, pages 101?104.
42
