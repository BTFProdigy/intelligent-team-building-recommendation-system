Topic Tracking Based on Linguistic Features
Fumiyo Fukumoto and Yusuke Yamaji
Interdisciplinary Graduate School of Medicine and Engineering,
Univ. of Yamanashi, 4-3-11, Takeda, Kofu, 400-8511, Japan
fukumoto@yamanashi.ac.jp, g03mk031@ccn.yamanashi.ac.jp
Abstract. This paper explores two linguistically motivated restrictions on the
set of words used for topic tracking on newspaper articles: named entities and
headline words. We assume that named entities is one of the linguistic features
for topic tracking, since both topic and event are related to a specific place and
time in a story. The basic idea to use headline words for the tracking task is that
headline is a compact representation of the original story, which helps people to
quickly understand the most important information contained in a story. Head-
line words are automatically generated using headline generation technique. The
method was tested on the Mainichi Shimbun Newspaper in Japanese, and the re-
sults of topic tracking show that the system works well even for a small number
of positive training data.
1 Introduction
With the exponential growth of information on the Internet, it is becoming increasingly
difficult to find and organize relevant materials. Tracking task, i.e. starts from a few
sample stories and finds all subsequent stories that discuss the target topic, is a new
line of research to attack the problem. One of the major problems in the tracking task
is how to make a clear distinction between a topic and an event in the story. Here, an
event refers to the subject of a story itself, i.e. a writer wants to express, in other words,
notions of who, what, where, when, why and how in the story. On the other hand,
a topic is some unique thing that occurs at a specific place and time associated with
some specific actions [1]. It becomes background among stories. Therefore, an event
drifts, but a topic does not. For example, in the stories of ?Kobe Japan quake? from the
TDT1 corpus, the event includes early reports of damage, location and nature of quake,
rescue efforts, consequences of the quake, and on-site reports, while the topic is Kobe
Japan quake.
A wide range of statistical and machine learning techniques have been applied to
topic tracking, including k-Nearest Neighbor classification, Decision Tree induction [3],
relevance feedback method of IR [12,13], hierarchical and non-hierarchical clustering
algorithms [20], and a variety of Language Modeling [15,5,10,17]. The main task of
these techniques is to tune the parameters or the threshold for binary decisions to pro-
duce optimal results. In the TDT context, however, parameter tuning is a tricky issue
for tracking. Because only the small number of labeled positive stories is available for
training. Moreover, the well-known past experience from IR that notions of who, what,
where, when, why, and how may not make a great contribution to the topic tracking task
[1] causes this fact, i.e. a topic and an event are different from each other.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 10?21, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
Topic Tracking Based on Linguistic Features 11
This paper explores two linguistically motivated restrictions on the set of words used
for topic tracking on newspaper articles: named entities and headline words. A topic is
related to a specific place and time, and an event refers to notions of who(person),
where(place), when(time) including what, why and how in a story. Therefore, we can
assume that named entities is one of the linguistic features for topic tracking. Another
linguistic feature is a set of headline words. The basic idea to use headline words for topic
tracking is that headline is a compact representation of the original story, which helps
people to quickly understand the most important information contained in a story, and
therefore, it may include words to understand what the story is about, what is characteris-
tic of this story with respect to other stories, and hopefully include words related to both
topic and event in the story. A set of headline words is automatically generated. To do this,
we use a technique proposed by Banko [2]. It produces coherent summaries by building
statistical models for content selection and surface realization. Another purpose of this
work is to create Japanese corpus for topic tracking task. We used Mainichi Shimbun
Japanese Newspaper corpus from Oct. to Dec. of 1998 which corresponds to the TDT3
corpus. We annotated these articles against the 60 topics which are defined by the TDT3.
The rest of the paper is organized as follows. The next section provides an overview
of existing topic tracking techniques. We then describe a brief explanation of a headline
generation technique proposed by Banko et al [2]. Next, we present our method for
topic tracking, and finally, we report some experiments using the Japanese newspaper
articles with a discussion of evaluation.
2 Related Work
The approach that relies mainly on corpus statistics is widely studied in the topic track-
ing task, and an increasing number of machine learning techniques have been applied
to the task. CMU proposed two methods: a k-Nearest Neighbor (kNN) classifier and a
Decision-Tree Induction (dtree) classifier [1,20,3]. Dragon Systems proposed two track-
ing systems; one is based on standard language modeling technique, i.e. unigram statis-
tics to measure story similarity [18] and another is based on a Beta-Binomial model
[10]. UMass viewed the tracking problem as an instance of on-line document classifica-
tion, i.e. it classifies documents into categories or classes [4,8,19,9,14]. They proposed a
method including query expansion with multi-word features and weight-learning steps
for building linear text classifiers for the tracking task [13]. These approaches, described
above, seem to be robust and have shown satisfactory performance in stories from dif-
ferent corpora, i.e. TDT1 and TDT2. However, Carbonell claims that something more is
needed if the system is intended for recognizing topic drift [3]. Yang et al addressed the
issue of difference between early and later stories related to the target event in the TDT
tracking task. They adapted several machine learning techniques, including k-Nearest
Neighbor(kNN) algorithm and Rocchio approach [21]. Their method combines the out-
put of a diverse set of classifiers and tuning parameters for the combined system on a
retrospective corpus. The idea comes from the well-known practice in information re-
trieval and speech recognition of combining the output of a large number of systems
to yield a better result than the individual system?s output. They reported that the new
variants of kNN reduced up to 71% in weighted error rates on the TDT3-dryrun corpus.
12 F. Fukumoto and Y. Yamaji
GE R&D proposed a method for topic tracking by using summarization technique,
i.e. using content compression rather than on corpus statistics to detect relevance and as-
sess topicality of the source material [16]. Their system operates by first creating a topic
tracking query out of the available training stories. Subsequently, it accepts incoming
stories, summarizes them topically, scores the summaries(passages) for content, then as-
sesses content relevance to the tracking query. They reported stories whose compressed
content summaries clear the empirically established threshold are classified as being
?on topic?. Unlike most previous work on summarization which focused on extractive
summarization: selecting text spans - either complete sentences or paragraphs - from the
original story, this approach solves a problem for extractive summarization, i.e. in many
cases, the most important information in the story is scattered across multiple sentences.
However, their approach uses frequency-based term weighting. Therefore, it is not clear
if the method can identify the most important information contained in a story.
These methods, described above, show that it is crucial to develop a method for
extracting words related to both topic and event in a story. Like other approaches, our
method is based on corpus statistics. However, our method uses two linguistically mo-
tivated restrictions on the set of words: named entities and headline words. We assume
that named entities is one of the linguistic features for topic tracking, since both topic
and event are related to a specific place and time in a story. Another linguistic feature
is a set of headline words. The basic idea to use headline words is that headline is a
compact representation of the original story, and therefore, it may include words to un-
derstand what the story is about, and hopefully include words related to both topic and
event in the story.
3 Generating Headline
Banko et al proposed an approach to summarization capable of generating summaries
shorter than a sentence. It produces by building statistical models for content selection
and surface realization. We used their method to extract headline words. Content selec-
tion requires that the system learns a model of the relationship between the appearance of
words in a story and the appearance of corresponding words in the headline. The proba-
bility of a candidate headline, H , consisting of words (w1,w2,? ? ?,wn), can be computed:
P (w1, ? ? ? , wn) =
n
?
i=1
P (wi ? H | wi ? D) ? P (len(H) = n)
?
n
?
i=2
P (wi | w1, ? ? ? , wi?1) (1)
In formula (1), the first term denotes the words selected for the headline, and can be
computed:
P (wi ? H | wi ? D) =
P (wi ? D | wi ? H) ? P (wi ? H)
P (wi ? D)
(2)
where H and D represent the bags of words that the headline and the story contain.
Formula (2) shows the conditional probability of a word occurring in the headline given
Topic Tracking Based on Linguistic Features 13
that the word appeared in the story. It has been estimated from a suitable story/headline
corpus. The second term in formula (1) shows the length of the resulting headline, and can
also be learned from the source story. The third term shows the most likely sequencing
of the words in the content set. Banko et al assumed that the likelihood of a word in the
story is independent of other words in the headline. Surface realization is to estimate the
probability of any particular surface ordering as a headline candidate. It can be computed
by modeling the probability of word sequences. Banko et al used a bigram language
model. When they estimate probabilities for sequences that have not been seen in the
training data, they used back-off weights [6].
Headline generation can be obtained as a weighted combination of the content and
structure model log probabilities which is shown in formula (3).
arg maxH(? ?
n
?
i=1
log(P (wi ? H | wi ? D)) + ? ? log(P (len(H) = n)) +
? ?
n
?
i=2
log(P (wi | wi?1))) (3)
To generate a headline, it is necessary to find a sequence of words that maximizes the
probability, under the content selection and surface realization models, that it was gen-
erated from the story to be summarized. In formula (3), cross-validation is used to learn
weights, ?, ? and ? for a particular story genre.
4 Extracting Linguistic Features and Tracking
We explore two linguistically motivated restrictions on the set of words used for tracking:
named entities and headline words.
4.1 Extracting Named Entities and Generating Headline Words
For identifying named entities, we use CaboCha [7] for Japanese Mainichi Shimbun
corpus, and extracted Person Name, Organization, Place, and Proper Name.
Headline generation can be obtained as a weighted combination of the content and
structure model log probabilities shown in formula (3). The system was trained on the 3
months Mainichi Shimbun articles((27,133 articles from Jan. to Mar. 1999) for Japanese
corpus. We estimate ?, ? and ? in formula (3) using 5 cross-validation1. Fig. 1 illustrates
sample output using Mainichi Shimbun corpus. Numbers to the right are log probabilities
of the word sequence.
4.2 Tracking by Hierarchical Classification
In the TDT tracking task, the number of labeled positive training stories is small (at most
16 stories) compared to the negative training stories. Therefore, the choice of good neg-
ative stories from a large number of training data is an important issue to detect subject
shifts for a binary classifier such as a machine learning technique, Support Vector Ma-
chines(SVMs) [22]. We apply hierarchical classification technique to the training data.
1 In the experiment, we set ?, ?, ? to 1.0, 1.0, 0.8, respectively.
14 F. Fukumoto and Y. Yamaji
  Headline     	  (Pakistan)           (Kashimir issue)
