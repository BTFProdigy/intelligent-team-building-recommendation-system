Parsing the Penn Chinese Treebank
with Semantic Knowledge
Deyi Xiong1,2, Shuanglong Li1,3,
Qun Liu1, Shouxun Lin1, and Yueliang Qian1
1 Institute of Computing Technology, Chinese Academy of Sciences,
P.O. Box 2704, Beijing 100080, China
{dyxiong, liuqun, sxlin}@ict.ac.cn
2 Graduate School of Chinese Academy of Sciences
3 University of Science and Technology Beijing
Abstract. We build a class-based selection preference sub-model to in-
corporate external semantic knowledge from two Chinese electronic se-
mantic dictionaries. This sub-model is combined with modifier-head gen-
eration sub-model. After being optimized on the held out data by the
EM algorithm, our improved parser achieves 79.4% (F1 measure), as well
as a 4.4% relative decrease in error rate on the Penn Chinese Treebank
(CTB). Further analysis of performance improvement indicates that se-
mantic knowledge is helpful for nominal compounds, coordination, and
NV tagging disambiguation, as well as alleviating the sparseness of in-
formation available in treebank.
1 Introduction
In the recent development of full parsing technology, semantic knowledge is sel-
dom used, though it is known to be useful for resolving syntactic ambiguities.
The reasons for this may be twofold. The first one is that it can be very difficult
to add additional features which are not available in treebanks to generative
models like Collins (see [1]), which are very popular for full parsing. For smaller
tasks, like prepositional phrase attachment disambiguation, semantic knowledge
can be incorporated flexibly using different learning algorithms (see [2,3,4,5]).
For full parsing with generative models, however, incorporating semantic knowl-
edge may involve great changes of model structures. The second reason is that
semantic knowledge from external dictionaries seems to be noisy, ambiguous and
not available in explicit forms, compared with the information from treebanks.
Given these two reasons, it seems to be difficult to combine the two different
information sources?treebank and semantic knowledge?into one integrated sta-
tistical parsing model.
One feasible way to solve this problem is to keep the original parsing model
unchanged and build an additional sub-model to incorporate semantic knowledge
from external dictionaries. The modularity afforded by this approach makes
it easier to expand or update semantic knowledge sources with the treebank
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 70?81, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Parsing the Penn Chinese Treebank with Semantic Knowledge 71
unchanged or vice versa. Further, the combination of the semantic sub-model
and the original parsing model can be optimized automatically.
In this paper, we build a class-based selection preference sub-model, which
is embedded in our lexicalized parsing model, to incorporate external seman-
tic knowledge. We use two Chinese electronic dictionaries and their combi-
nation as our semantic information sources. Several experiments are carried
out on the Penn Chinese Treebank to test our hypotheses. The results indi-
cate that a significant improvement in performance is achieved when seman-
tic knowledge is incorporated into parsing model. Further improvement analy-
sis is made. We confirm that semantic knowledge is indeed useful for nominal
compounds and coordination ambiguity resolution. And surprisingly, semantic
knowledge is also helpful to correct Chinese NV mistagging errors mentioned
by Levy and Manning (see [12]). Yet another great benefit to incorporating
semantic knowledge is to alleviate the sparseness of information available in
treebank.
2 The Baseline Parser
Our baseline parsing model is similar to the history-based, generative and lexical-
ized Model 1 of Collins (see [1]). In this model, the right hand side of lexicalized
rules is decomposed into smaller linguistic objects as follows:
P (h) ? #Ln(ln)...L1(l1)H(h)R1(r1)...Rm(rm)# .
The uppercase letters are delexicalized nonterminals, while the lowercase letters
are lexical items, e.g. head word and head tag (part-of-speech tag of the head
word), corresponding to delexicalized nonterminals. H(h) is the head constituent
of the rule from which the head lexical item h is derived according to some head
percolation rules.1 The special termination symbol # indicates that there is no
more symbols to the left/right. Accordingly, the rule probability is factored into
three distributions. The first distribution is the probability of generating the
syntactic label of the head constituent of a parent node with label P , head word
Hhw and head tag Hht:
PrH(H |P, Hht, Hhw) .
Then each left/right modifier of head constituent is generated in two steps: first
its syntactic label Mi and corresponding head tag Miht are chosen given context
features from the parent (P ), head constituent (H, Hht, Hhw), previously gen-
erated modifier (Mi?1, Mi?1ht) and other context information like the direction
(dir) and distance2 (dis) to the head constituent:
1 Here we use the modified head percolation table for Chinese from Xia (see [6]).
2 Our distance definitions are different for termination symbol and non-termination
symbol, which are similar to Klein and Manning (see [7]).
72 D. Xiong et al
PrM (Mi, Miht|HCM ) .
where the history context HCM is defined as the joint event of
P, H, Hht, Hhw, Mi?1, Mi?1ht, dir, dis .
Then the new modifier?s head word Mihw is also generated with the probability:
PrMw(Mihw|HCMw ) .
where the history context HCMw is defined as the joint event of
P, H, Hht, Hhw, Mi?1, Mi?1ht, dir, dis, Mi, Miht .
All the three distributions are smoothed through Witten-Bell interpolation
just like Collins (see [1]). For the distribution PrM , we build back-off struc-
tures with six levels, which are different from Collins? since we find our back-off
structures work better than the three-level back-off structures of Collins. For
the distribution PrMw , the parsing model backs off to the history context with
head word Hhw removed, then to the modifier head tag Miht, just like Collins.
Gildea (see [9]) and Bikel (see [10]) both observed that the effect of bilexical de-
pendencies is greatly impaired due to the sparseness of bilexical statistics. Bikel
even found that the parser only received an estimate that made use of bilexi-
cal statistics a mere 1.49% of the time. However, according to the wisdom of
the parsing community, lexical bigrams, the word pairs (Mihw, Hhw) are very
informative with semantic constraints. Along this line, in this paper, we build
an additional class-based selectional preference sub-model, which is described
in section 3, to make good use of this semantic information through selectional
restrictions between head and modifier words.
Our parser takes segmented but untagged sentences as input. The probability
of unknown words, Pr(uword|tag), is estimated based on the first character of
the word and if the first characters are unseen, the probability is estimated by
absolute discounting.
We do some linguistically motivated re-annotations for the baseline parser.
The first one is marking non-recursive noun phrases from other common noun
phrases without introducing any extra unary levels (see [1,8]). We find this basic
NP re-annotation very helpful for the performance. We think it is because of the
annotation style of the Upenn Chinese Treebank (CTB). According to Xue et al
(see [11]), noun-noun compounds formed by an uninterrupted sequence of words
POS-tagged as NNs are always left flat because of difficulties in determining
which modifies which. The second re-annotation is marking basic VPs, which we
think is beneficial for reducing multilevel VP adjunction ambiguities (see [12]).
To speed up parsing, we use the beam thresholding techniques in Xiong et
al. (see [13]). In all cases, the thresholding for completed edges is set at ct = 9
and incomplete edges at it = 7. The performance of the baseline parser is 78.5%
in terms of F1 measure of labeled parse constituents on the same CTB training
and test sets with Bikel et al (see [14])
Parsing the Penn Chinese Treebank with Semantic Knowledge 73
3 Incorporating Semantic Knowledge
In this section, we describe how to incorporate semantic knowledge from external
semantic dictionaries into parsing model to improve the performance. Firstly, we
extract semantic categories through two Chinese electronic semantic dictionaries
and some heuristic rules. Then we build a selection preference sub-model based
on extracted semantic categories. In section 3.3, we present our experiments
and results in detail. And finally, we compare parses from baseline parser with
those from the new parser incorporated with semantic knowledge. We empirically
confirm that semantic knowledge is helpful for nominal compound, coordination
and POS tagging ambiguity resolution. Additionally, we also find that semantic
knowledge can greatly alleviate problems caused by data sparseness.
3.1 Extracting Semantic Categories
Semantic knowledge is not presented in treebanks and therefore has to be ex-
tracted from external knowledge sources. We have two Chinese electronic se-
mantic dictionaries, both are good knowledge sources for us to extract semantic
categories. One is the HowNet dictionary3, which covers 67,440 words defined
by 2112 different sememes. The other is the ?TongYiCi CiLin? expanded version
(henceforth CiLin)4, which represents 77,343 words in a dendrogram.
HowNet (HN): Each sememe defined by the HowNet is regarded as a semantic
category. And through the hypernym-hyponym relation between different cat-
egories, we can extract semantic categories at various granularity levels. Since
words may have different senses, and therefore different definitions in HowNet,
we just use the first definition of words in HowNet. At the first level HN1, we ex-
tract the first definitions and use them as semantic categories of words. Through
the hypernym ladders, we can get HN2, HN3, by replacing categories at lower
level with their hypernyms at higher level. Table 1 shows information about
words and extracted categories at different levels.
CiLin (CL): CL is a branching diagram, where each node represents a semantic
category. There are three levels in total, and from the top down, 12 categories in
the first level (CL1), 97 categories in the second level (CL2), 1400 categories in
the third level (CL3). We extract semantic categories at level CL1, CL2 and CL3.
HowNet+CiLin: Since the two dictionaries have different ontologies and rep-
resentations of semantic categories, we establish a strategy to combine them:
HowNet is used as a primary dictionary, and CiLin as a secondary dictionary.
If a word is not found in HowNet but found in Cilin, we will look up other
words from its synset defined by CiLin in HowNet. If HowNet query succeeds,
the corresponding semantic category in HowNet will be assigned to this word.
3 http://www.keenage.com/.
4 The dictionary is recorded and expanded by Information Retrieval Laboratory,
Harbin Institute of Technology.
74 D. Xiong et al
Table 1. Sizes and coverage of words and semantic categories from different semantic
knowledge sources
Data HN1 HN2 HN3 CL1 CL2 CL3
words in train 9522 6040 6469
words in test 1824 1538 1581
words in both 1412 1293 1310
classes in train - 1054 381 118 12 92 1033
classes in test - 520 251 93 12 79 569
classes in both - 504 248 93 12 79 552
According to our experimental results, we choose HN2 as the primary semantic
category set and combine it with CL1, CL2 and CL3.
Heuristic Rules (HR): Numbers and time expressions are recognized using
simple heuristic rules. For a better recognition, one can define accurate regular
expressions. However, we just collect suffixes and feature characters to match
strings. For example, Chinese numbers are strings whose characters all come
from a predefined set. These two classes are merged into HowNet and labelled
by semantic categories from HowNet.
In our experiments, we combine HN2, CL1/2/3, and HR as our external
sources. In these combinations {HN2+CL1/2/3+HR}, all semantic classes come
from the primary semantic category set HN2, therefore we get the same class
coverage that we obtain from the single source HN2 but a bigger word coverage.
The number of covered words of these combinations in {train, test, both} is
{7911, 1672, 1372} respectively.
3.2 Building Class-Based Selection Preference Sub-model
There are several ways to incorporate semantic knowledge into parsing model.
Bikel (see [15]) suggested a way to capture semantic preferences by employing
bilexical-class statistics, in other words, dependencies among head-modifier word
classes. Bikel did not carry it out and therefore greater details are not available.
However, the key point, we think, is to use classes extracted from semantic
dictionary, instead of words, to model semantic dependencies between head and
modifier. Accordingly, we build a similar bilexical-class sub-model as follows:
Prclass(CMihw|CHhw, Hht, Miht, dir) .
where CMihw and CHhw represent semantic categories of words Mihw and Hhw,
respectively. This model is combined with sub-model PrMw to form a mixture
model Pmix:
Prmix = ?PrMw + (1 ? ?)Prclass . (1)
? is hand-optimized, and an improvement of about 0.5% in terms of F1 measure is
gained. However, even a very slight change in the value of ?, e.g. 0.001, will have
a great effect on the performance. Besides, it seems that the connection between
Parsing the Penn Chinese Treebank with Semantic Knowledge 75
entropy, i.e. the total negative logarithm of the inside probability of trees, and F1
measure, is lost, while this relation is observed in many experiments. Therefore,
automatic optimization algorithms, like EM, can not work in this mixture model.
The reason, we guess, is that biclass dependencies among head-modifier word
classes seem too coarse-grained to capture semantic preferences between head
and modifier. In most cases, a head word has a strong semantic constraints on
the concept ? of mw, one of its modifier words, but that doesn?t mean other
words in the same class with the head word has the same semantic preferences
on the concept ?. For example, the verb eat impose a selection restriction on
its object modifier5: it has to be solid food. On the other hand, the verb drink
specifies its object modifier to be liquid beverage. At the level HN2, verb eat
and drink have the same semantic category metabolize. However, they impose
different selection preferences on their PATIENT roles.
To sum up, bilexical dependencies are too fine-grained when being used to
capture semantic preferences and therefore lead to serious data sparseness. Bi-
class dependencies, which result in an unstable performance improvement, on the
other hand, seem to be too coarse-grained for semantic preferences. We build a
class-based selection preference model:
Prsel(CMihw|Hhw, P ) .
This model is similar to Resnik (see [2]). We use the parent node label P to
represent the grammatical relation between head and modifier. Besides, in this
model, only modifier word is replaced with its semantic category. The depen-
dencies between head word and modifier word class seem to be just right for
capturing these semantic preferences.
The final mixture model is the combination of the class-based selection pref-
erence sub-model Prsel and modifier-head generation sub-model PrMw :
Prmix = ?PrMw + (1 ? ?)Prsel . (2)
Since the connection between entropy and F1 measure is observed again, EM
algorithm is used to optimize ?. Just like Levy (see [12]), we set aside articles 1-
25 in CTB as held out data for EM algorithm and use articles 26-270 as training
data during ? optimization.
3.3 Experimental Results
We have designed several experiments to check the power of our class-based se-
lection preference model with different semantic data sources. In all experiments,
we first use the EM algorithm to optimize the parameter ?. As mentioned above,
during parameter optimization, articles 1-25 are used as held out data and ar-
ticles 26-270 are used as training data. Then we test our mixture model with
optimized parameter ? using the training data of articles 1-270 and test data of
articles 271-300 of length at most 40 words.
5 According to Thematic Role theory, this modifier has a PATIENT role.
76 D. Xiong et al
Table 2. Results for incorporating different semantic knowledge sources. The baseline
parser is described in Sect. 2. in detail.
Baseline HN1 HN2 HN3 CL1 CL2 CL3
F1(%) 78.5 78.6 79.1 78.9 77.5 78.7 78.8
Table 3. Results for combinations of different semantic knowledge sources
Baseline HN2+CL1+HR HN2+CL2+HR HN2+CL3+HR
F1(%) 78.5 79.2 79.4 79.3
Firstly, we carry out experiments on HowNet and CiLin, separately. Exper-
imental results are presented in Table 2. As can be seen, CiLin has a greater
coverage of words than that of HowNet, however, it works worse than HowNet.
And at the level CL1, coarse-grained classes even yield degraded results. It?s dif-
ficult to explain this, but the main reason may be that HowNet has a fine-grained
and substantial ontology while CiLin is designed only as a synset container.
Since HowNet has a better semantic representation and CiLin better cov-
erage, we want to combine them. The combination is described in Sect. 3.1,
where HN2 is used as the primary semantic category set. Words found by CiLin
and heuristic rules are labelled by semantic categories from HN2. Results are
shown in Table 3. Although external sources HN2+CL1/2/3+HR have the iden-
tical word coverage and yield exactly the same number of classes, the different
word-class distributions in them lead to the different results.
Due to the combination of HN2, CL2 and HR, we see that our new parser
with external semantic knowledge outperforms the baseline parser by 0.9% in
F1 measure. Given we are already at the 78% level of accuracy, an improve-
ment of 0.9% is well worth obtaining and confirms the importance of semantic
dependencies on parsing. Further, we do the significance test using Bikel?s sig-
nificance tester6 which is modified to output p-value for F1. The significance
level for F-score is at most (43376+1)/(1048576+1) = 0.041. A second 1048576
iteration produces the similar result. Therefore the improvement is statistically
significant.
3.4 Performance Improvement Analysis
We manually analyze parsing errors of the baseline parser (BP ) as well as per-
formance improvement of the new parser (IP ) with semantic knowledge from
the combination of HN2, CL2 and HR. Improvement analysis can provide an
additional valuable perspective: how semantic knowledge helps to resolve some
ambiguities. We compare BP and IP on the test data parse by parse. There are
299 sentences of length at most 40 words among the total 348 test sentences. The
two parsers BP and IP found different parses for 102 sentences, among which
6 See http://www.cis.upenn.edu/ dbikel/software.html
Parsing the Penn Chinese Treebank with Semantic Knowledge 77
Table 4. Frequency of parsing improvement types. AR represents ambiguity resolution.
Type Count Percent(%)
Nominal Compound AR 19 38
Coordination AR 9 18
NV AR in NV+noun 6 12
Other AR 16 32
IP yields better parse trees for 47 sentences according to the gold standard trees.
We have concentrated on these 47 sentences and compared parse trees found by
IP with those found by BP . Frequencies of major types of parsing improvement
is presented in Table 4. Levy and Manning (see [12])(henceforth L&M) observed
the top three parsing error types: NP-NP modification, Coordination and NV
mistagging, which are also common in our baseline parser. As can be seen, our
improved parser can address these types of ambiguities to some extent through
semantic knowledge.
Nominal Compounds (NCs) Disambiguation: Nominal compounds are no-
torious ?every way ambiguous? constructions.7 The different semantic interpre-
tations have different dependency structures. According to L&M, this ambiguity
will be addressed by the dependency model when word frequencies are large
enough to be reliable. However, even for the treebank central to a certain topic,
many very plausible dependencies occur only once.8 A good technique for re-
solving this conflict is to generalize the dependencies from word pairs to word-
class pairs. Such generalized dependencies, as noted in section 3.2, can capture
semantic preferences, as well as alleviate the data sparseness associated with
standard bilexical statistics. In our class-based selection preference model, if the
frequency of pair [CMhw, Hhw]9 is large enough, the parser can interpret nominal
compounds correctly, that is, it can tell which modify which.
NCs are always parsed as flatter structures by our baseline parser, just like
the tree a. in Figure 1. This is partly because of the annotation style of CTB,
where there is no NP-internal structure. For these NCs without internal analysis,
we re-annotated them as basic NPs with label NPB, as mentioned in section 2.
This re-annotation really helps. Another reason is that the baseline parser, or
the modifier word generating sub-model PMw , can not capture hierarchical se-
mantic dependencies of internal structures of NCs due to the sparseness of bilex-
ical dependencies. In our new parser, however, the selection preference model is
able to build semantically preferable structures through word-class dependency
statistics. For NCs like (n1, n2, n3), where ni is a noun, dependency structures
7 ?Every way ambiguous? constructions are those for which the number of analy-
ses is the number of binary trees over the terminal elements. Prepositional phrase
attachment, coordination, and nominal compounds are all ?every way ambiguous?
constructions.
8 Just as Klein et al (see [8]) said, one million words of training data just isn?t enough.
9 Henceforth, [s1, s2] denotes a dependency structure, where s1 is a modifier word or
its semantic class (C), and s2 is the head word.
78 D. Xiong et al
a. NPB




NR
??
NN
??
NN
??
b. NP




NP

NPB
NR
??
NPB
NN
??
NPB
NN
??
Fig. 1. Nominal Compounds: The North Korean government?s special envoy. a. is the
incorrect flat parse, b. is the right one in corpus
{[Cn1 , n2], [Cn1 , n3], [Cn2 , n3]} will be checked in terms of semantic acceptability
and semantically preferable structures will be built finally. For more complicated
NCs, similar analysis follows.
In our example (see Fig. 1.), the counts of word dependencies [??/North
Korea, ??/government] and [??/North Korea,??/special envoy] in the
training data both are 0. Therefore, it is impossible for the baseline parser to
have a preference between these two dependency structures. On the other hand,
the counts of word-class dependencies [???,??/government], where ???
is the semantic category of?? in HN2, is much larger than the counts of [??
?,??/special envoy] and [??,??/special envoy], where?? is the semantic
category of ?? in the training data. Therefore, the dependency structure of
[??/North Korea, ??/government] will be built.
Coordination Disambiguation: Coordination is another kind of ?every way
ambiguous? construction. For coordination structures, the head word is meaning-
less. But that doesn?t matter, since semantic dependency between the spurious
head and modifier will be used to measure the meaning similarity of coordinated
structures. Therefore, our selection preference model still works in coordination
constructions. We have also found VP coordination ambiguity, which is similar
to that observed by L&M. The latter VP in coordinated VPs is often parsed as
an IP due to pro-drop by the baseline parser. That is, the coordinated structure
VP is parsed as: V P 0 ? V P 1IP 2. This parse will be penalized by the selection
preference model because the hypothesis that the head word of IP 2 has a sim-
ilar meaning to the head word of V P 1 under the grammatical relation V P 0 is
infrequent.
NV-ambiguous Tagging Disambiguation: The lack of overt morphological
marking for transforming verbal words to nominal words in Chinese results in
ambiguity between these two categories. L&M argued that the way to resolve
this ambiguity is to look at more external context, like some function words,
e.g. adverbial or prenominal modifiers, co-occurring with NV-ambiguous words.
However, in some cases, NV-ambiguous words can be tagged correctly without
external context. Chen et al (see [16]) studied the pattern of NV+noun, which
will be analyzed as a predicate-object structure if NV is a verb and a modifier-
noun structure if NV is a noun. They found that in most cases, this pattern can
Parsing the Penn Chinese Treebank with Semantic Knowledge 79
a. VP

VPB
VV
??
NPB
NN
??
b. NPB

NN
??
NN
??
Fig. 2. NV-ambiguity: a. implement plans (incorrect parse) versus b. implementation
plans (corpus)
Table 5. Previous Results on CTB parsing for sentences of length at most 40 words
LP LR F1
Bikel and Chiang 2000 77.2 76.2 76.7
Levy and Manning 2003 78.4 79.2 78.8
Present work 80.1 78.7 79.4
Bikel Thesis 2004 81.2 78.0 79.6
Chiang and Bikel 2002 81.1 78.8 79.9
be parsed correctly without any external context. Furthermore, they argued that
semantic preferences are helpful for the resolution of ambiguity between these
two different structures. In our selection preference model, semantic preferences
interweave with grammatical relations. These semantic dependencies impose con-
straints on the structure of the pattern NV+noun and therefore on the POS
tag of NV. Figure 2 shows our new parser can correct NV mistagging errors
occurring in the pattern of NV+noun.
Smoothing:Besides the three ambiguity resolution noted above, semantic knowl-
edge indeed helps alleviate the fundamental sparseness of the lexical dependency
information available in the CTB. For many word pairs [mod,head], whose count
information is not available in the training data, the dependency statistics of head
and modifier can still work through the semantic category of mod. During our man-
ual analysis of performance improvement, many other structural ambiguities are
addressed due to the smoothing function of semantic knowledge.
4 Related Work on CTB Parsing
Previous work on CTB parsing and their results are shown in table 5. Bikel and
Chiang (see [14]) used two different models on CTB, one based on the modi-
fied BBN model which is very similar to our baseline model, the other on Tree
Insertion Grammar (TIG). While our baseline model used the same unknown
word threshold with Bikel and Chiang but smaller beam width, our result out-
performs theirs due to other features like distance, basic NP re-annotation used
by our baseline model. Levy and Manning (see [12]) used a factored model with
rich re-annotations guided by error analysis. In the baseline model, we also used
several re-annotations but find most re-annotations they suggested do not fit
80 D. Xiong et al
our model. The three parsing error types expounded above are also found by
L&M. However, we used more efficient measures to keep our improved model
from these errors.
The work of Bikel thesis (see [10]) emulated Collins? model and created a
language package to Chinese parsing. He used subcat frames and an additional
POS tagger for unseen words. Chiang and Bikel (see [17]) used the EM algorithm
on the same TIG-parser to improve the head percolation table for Chinese pars-
ing. Both these two parsers used fine-tuned features recovered from the treebank
that our model does not use. This leads to better results and indicates that there
is still room of improvement for our model.
5 Conclusions
We have shown that how semantic knowledge may be incorporated into a gener-
ative model for full parsing, which reaches 79.4% in CTB. Experimental results
are quite consistent with our intuition. After the manual analysis of performance
improvement, the working mechanism of semantic knowledge in the selection
preference model is quite clear:
1. Using semantic categories extracted from external dictionaries, the class-
based selection preference model first generalizes standard bilexical depen-
dencies, some of which are not available in training data, to word-class de-
pendencies. These dependencies are neither too fine-grained nor too coarse-
grained compared with bilexical and biclass dependencies, and really help to
alleviate fundamental information sparseness in treebank.
2. Based on the generalized word-class pairs, semantic dependencies are cap-
tured and used to address different kinds of ambiguities, like nominal com-
pounds, coordination construction, even NV-ambiguous words tagging.
Our experiments show that generative models have room for improvement
by employing semantic knowledge. And that may be also true for discrimina-
tive models, since these models can easily incorporate richer features in a well-
founded fashion. This is the subject of our future work.
Acknowledgements
This work was supported in part byNational HighTechnologyResearch andDevel-
opment Program under grant #2001AA114010 and #2003AA111010. We would
like to acknowledge anonymous reviewers who provided helpful suggestions.
References
1. Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Pars-
ing. PhD thesis, University of Pennsylvania.
2. Philip Stuart Resnik. 1993. Selection and Information: A Class-Based Approach to
Lexical Relationships. PhD thesis, University of Pennsylvania, Philadelphia, PA,
USA.
Parsing the Penn Chinese Treebank with Semantic Knowledge 81
3. Sanda Harabagiu. 1996. An Application of WordNet to Prepositional Attachement.
In Proceedings of ACL-96, June 1996, Santa Cruz CA, pages 360-363.
4. Yuval Krymolowski and Dan Roth. 1998. Incorporating Knowledge in Natural Lan-
guage Learning: A Case Study. In COLING-ACL?98 Workshop on Usage of Word-
Net in Natural Language Processing Systems,Montreal, Canada.
5. Mark McLauchlan. 2004. Thesauruses for Prepositional Phrase Attachment. In
Proceedings of CoNLL-2004,Boston, MA, USA, 2004, pp. 73-80.
6. Fei Xia. 1999. Automatic Grammar Generation from Two Different Perspectives.
PhD thesis, University of Pennsylvania.
7. Dan Klein and Christopher D. Manning. 2002. Fast Exact Natural Language Pars-
ing with a Factored Model. In Advances in Neural Information Processing Systems
15 (NIPS-2002).
8. Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In
Proceedings of ACL-03.
9. Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of
EMNLP-01, Pittsburgh, Pennsylvania.
10. Daniel M. Bikel. 2004a. On the Parameter Space of Generative Lexicalized Statis-
tical Parsing Models. PhD thesis, University of Pennsylvania.
11. Nianwen Xue and Fei Xia. 2000. The Bracketing Guidelines for Chinese Treebank
Project. Technical Report IRCS 00-08, University of Pennsylvania.
12. Roger Levy and Christopher Manning. 2003. Is it harder to parse Chinese, or the
Chinese Treebank? In Proceedings of ACL-03.
13. Deyi Xiong, Qun Liu and Shouxun Lin. 2005. Lexicalized Beam Thresholding Pars-
ing with Prior and Boundary Estimates. In Proceedings of the 6th Conference on
Intelligent Text Processing and Computational Linguistics (CICLing), Mexico City,
Mexico, 2005.
14. Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied
to the chinese treebank. In Proceedings of the Second Chinese Language Processing
Workshop, pages 1-6.
15. Daniel M. Bikel. 2004b. Intricacies of Collins? Parsing Model. to appear in Com-
putational Linguistics.
16. Kejian Chen and Weimei Hong. 1996. Resolving Ambiguities of Predicate-object
and Modifier-noun Structures for Chinese V-N Patterns. in Chinese. In Communi-
cation of COLIPS, Vol.6, #2, pages 73-79.
17. David Chiang and Daniel M. Bikel. 2002. Recovering Latent Information in Tree-
banks. In proceedings of COLING,2002.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521?528,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Maximum Entropy Based Phrase Reordering
Model for Statistical Machine Translation
Deyi Xiong
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
Graduate School of Chinese Academy of Sciences
dyxiong@ict.ac.cn
Qun Liu and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
{liuqun, sxlin}@ict.ac.cn
Abstract
We propose a novel reordering model for
phrase-based statistical machine transla-
tion (SMT) that uses a maximum entropy
(MaxEnt) model to predicate reorderings
of neighbor blocks (phrase pairs). The
model provides content-dependent, hier-
archical phrasal reordering with general-
ization based on features automatically
learned from a real-world bitext. We
present an algorithm to extract all reorder-
ing events of neighbor blocks from bilin-
gual data. In our experiments on Chinese-
to-English translation, this MaxEnt-based
reordering model obtains significant im-
provements in BLEU score on the NIST
MT-05 and IWSLT-04 tasks.
1 Introduction
Phrase reordering is of great importance for
phrase-based SMT systems and becoming an ac-
tive area of research recently. Compared with
word-based SMT systems, phrase-based systems
can easily address reorderings of words within
phrases. However, at the phrase level, reordering
is still a computationally expensive problem just
like reordering at the word level (Knight, 1999).
Many systems use very simple models to re-
order phrases 1. One is distortion model (Och
and Ney, 2004; Koehn et al, 2003) which penal-
izes translations according to their jump distance
instead of their content. For example, if N words
are skipped, a penalty of N will be paid regard-
less of which words are reordered. This model
takes the risk of penalizing long distance jumps
1In this paper, we focus our discussions on phrases that
are not necessarily aligned to syntactic constituent boundary.
which are common between two languages with
very different orders. Another simple model is flat
reordering model (Wu, 1996; Zens et al, 2004;
Kumar et al, 2005) which is not content depen-
dent either. Flat model assigns constant probabili-
ties for monotone order and non-monotone order.
The two probabilities can be set to prefer mono-
tone or non-monotone orientations depending on
the language pairs.
In view of content-independency of the dis-
tortion and flat reordering models, several re-
searchers (Och et al, 2004; Tillmann, 2004; Ku-
mar et al, 2005; Koehn et al, 2005) proposed a
more powerful model called lexicalized reorder-
ing model that is phrase dependent. Lexicalized
reordering model learns local orientations (mono-
tone or non-monotone) with probabilities for each
bilingual phrase from training data. During de-
coding, the model attempts to finding a Viterbi lo-
cal orientation sequence. Performance gains have
been reported for systems with lexicalized reorder-
ing model. However, since reorderings are re-
lated to concrete phrases, researchers have to de-
sign their systems carefully in order not to cause
other problems, e.g. the data sparseness problem.
Another smart reordering model was proposed
by Chiang (2005). In his approach, phrases are re-
organized into hierarchical ones by reducing sub-
phrases to variables. This template-based scheme
not only captures the reorderings of phrases, but
also integrates some phrasal generalizations into
the global model.
In this paper, we propose a novel solution for
phrasal reordering. Here, under the ITG constraint
(Wu, 1997; Zens et al, 2004), we need to con-
sider just two kinds of reorderings, straight and
inverted between two consecutive blocks. There-
fore reordering can be modelled as a problem of
521
classification with only two labels, straight and
inverted. In this paper, we build a maximum en-
tropy based classification model as the reordering
model. Different from lexicalized reordering, we
do not use the whole block as reordering evidence,
but only features extracted from blocks. This is
more flexible. It makes our model reorder any
blocks, observed in training or not. The whole
maximum entropy based reordering model is em-
bedded inside a log-linear phrase-based model of
translation. Following the Bracketing Transduc-
tion Grammar (BTG) (Wu, 1996), we built a
CKY-style decoder for our system, which makes
it possible to reorder phrases hierarchically.
To create a maximum entropy based reordering
model, the first step is learning reordering exam-
ples from training data, similar to the lexicalized
reordering model. But in our way, any evidences
of reorderings will be extracted, not limited to re-
orderings of bilingual phrases of length less than a
predefined number of words. Secondly, features
will be extracted from reordering examples ac-
cording to feature templates. Finally, a maximum
entropy classifier will be trained on the features.
In this paper we describe our system and the
MaxEnt-based reordering model with the associ-
ated algorithm. We also present experiments that
indicate that the MaxEnt-based reordering model
improves translation significantly compared with
other reordering approaches and a state-of-the-art
distortion-based system (Koehn, 2004).
2 System Overview
2.1 Model
Under the BTG scheme, translation is more
like monolingual parsing through derivations.
Throughout the translation procedure, three rules
are used to derive the translation
A [ ]? (A1, A2) (1)
A ? ?? (A1, A2) (2)
A ? (x, y) (3)
During decoding, the source sentence is seg-
mented into a sequence of phrases as in a standard
phrase-based model. Then the lexical rule (3) 2 is
2Currently, we restrict phrases x and y not to be null.
Therefore neither deletion nor insertion is carried out during
decoding. However, these operations are to be considered in
our future version of model.
used to translate source phrase y into target phrase
x and generate a block A. Later, the straight rule
(1) merges two consecutive blocks into a single
larger block in the straight order; while the in-
verted rule (2) merges them in the inverted order.
These two merging rules will be used continuously
until the whole source sentence is covered. When
the translation is finished, a tree indicating the hi-
erarchical segmentation of the source sentence is
also produced.
In the following, we will define the model in
a straight way, not in the dynamic programming
recursion way used by (Wu, 1996; Zens et al,
2004). We focus on defining the probabilities of
different rules by separating different features (in-
cluding the language model) out from the rule
probabilities and organizing them in a log-linear
form. This straight way makes it clear how rules
are used and what they depend on.
For the two merging rules straight and inverted,
applying them on two consecutive blocks A1 and
A2 is assigned a probability Prm(A)
Prm(A) = ??? ? 4?LMpLM (A1,A2) (4)
where the ? is the reordering score of block A1
and A2, ?? is its weight, and 4pLM (A1,A2) is the
increment of the language model score of the two
blocks according to their final order, ?LM is its
weight.
For the lexical rule, applying it is assigned a
probability Prl(A)
Prl(A) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3
?plex(y|x)?4 ? exp(1)?5 ? exp(|x|)?6
?p?LMLM (x) (5)
where p(?) are the phrase translation probabilities
in both directions, plex(?) are the lexical transla-
tion probabilities in both directions, and exp(1)
and exp(|x|) are the phrase penalty and word
penalty, respectively. These features are very com-
mon in state-of-the-art systems (Koehn et al,
2005; Chiang, 2005) and ?s are weights of fea-
tures.
For the reordering model ?, we define it on the
two consecutive blocks A1 and A2 and their order
o ? {straight, inverted}
? = f(o,A1, A2) (6)
Under this framework, different reordering mod-
els can be designed. In fact, we defined four re-
ordering models in our experiments. The first one
522
is NONE, meaning no explicit reordering features
at all. We set ? to 1 for all different pairs of
blocks and their orders. So the phrasal reorder-
ing is totally dependent on the language model.
This model is obviously different from the mono-
tone search, which does not use the inverted rule at
all. The second one is a distortion style reordering
model, which is formulated as
? =
{
exp(0), o = straight
exp(|A1|) + (|A2|), o = inverted
where |Ai| denotes the number of words on the
source side of blocks. When ?? < 0, this de-
sign will penalize those non-monotone transla-
tions. The third one is a flat reordering model,
which assigns probabilities for the straight and in-
verted order. It is formulated as
? =
{
pm, o = straight
1? pm, o = inverted
In our experiments on Chinese-English tasks, the
probability for the straight order is set at pm =
0.95. This is because word order in Chinese and
English is usually similar. The last one is the maxi-
mum entropy based reordering model proposed by
us, which will be described in the next section.
We define a derivation D as a sequence of appli-
cations of rules (1) ? (3), and let c(D) and e(D)
be the Chinese and English yields of D. The prob-
ability of a derivation D is
Pr(D) =
?
i
Pr(i) (7)
where Pr(i) is the probability of the ith applica-
tion of rules. Given an input sentence c, the final
translation e? is derived from the best derivation
D?
D? = argmax
c(D)=c
Pr(D)
e? = e(D?) (8)
2.2 Decoder
We developed a CKY style decoder that employs a
beam search algorithm, similar to the one by Chi-
ang (2005). The decoder finds the best derivation
that generates the input sentence and its transla-
tion. From the best derivation, the best English e?
is produced.
Given a source sentence c, firstly we initiate the
chart with phrases from phrase translation table
by applying the lexical rule. Then for each cell
that spans from i to j on the source side, all pos-
sible derivations spanning from i to j are gener-
ated. Our algorithm guarantees that any sub-cells
within (i, j) have been expanded before cell (i, j)
is expanded. Therefore the way to generate deriva-
tions in cell (i, j) is to merge derivations from
any two neighbor sub-cells. This combination is
done by applying the straight and inverted rules.
Each application of these two rules will generate
a new derivation covering cell (i, j). The score of
the new generated derivation is derived from the
scores of its two sub-derivations, reordering model
score and the increment of the language model
score according to the Equation (4). When the
whole input sentence is covered, the decoding is
over.
Pruning of the search space is very important for
the decoder. We use three pruning ways. The first
one is recombination. When two derivations in
the same cell have the same w leftmost/rightmost
words on the English yields, where w depends on
the order of the language model, they will be re-
combined by discarding the derivation with lower
score. The second one is the threshold pruning
which discards derivations that have a score worse
than ? times the best score in the same cell. The
last one is the histogram pruning which only keeps
the top n best derivations for each cell. In all our
experiments, we set n = 40, ? = 0.5 to get a
tradeoff between speed and performance in the de-
velopment set.
Another feature of our decoder is the k-best list
generation. The k-best list is very important for
the minimum error rate training (Och, 2003a)
which is used for tuning the weights ? for our
model. We use a very lazy algorithm for the k-best
list generation, which runs two phases similarly to
the one by Huang et al (2005). In the first phase,
the decoder runs as usual except that it keeps some
information of weaker derivations which are to be
discarded during recombination. This will gener-
ate not only the first-best of final derivation but
also a shared forest. In the second phase, the
lazy algorithm runs recursively on the shared for-
est. It finds the second-best of the final deriva-
tion, which makes its children to find their second-
best, and children?s children?s second-best, until
the leaf node?s second-best. Then it finds the third-
best, forth-best, and so on. In all our experiments,
we set k = 200.
523
The decoder is implemented in C++. Using the
pruning settings described above, without the k-
best list generation, it takes about 6 seconds to
translate a sentence of average length 28.3 words
on a 2GHz Linux system with 4G RAM memory.
3 Maximum Entropy Based Reordering
Model
In this section, we discuss how to create a max-
imum entropy based reordering model. As de-
scribed above, we defined the reordering model ?
on the three factors: order o, block A1 and block
A2. The central problem is, given two neighbor
blocks A1 and A2, how to predicate their order
o ? {straight, inverted}. This is a typical prob-
lem of two-class classification. To be consistent
with the whole model, the conditional probabil-
ity p(o|A1, A2) is calculated. A simple way to
compute this probability is to take counts from the
training data and then to use the maximum likeli-
hood estimate (MLE)
p(o|A1, A2) = Count(o,A
1, A2)
Count(A1, A2) (9)
The similar way is used by lexicalized reordering
model. However, in our model this way can?t work
because blocks become larger and larger due to us-
ing the merging rules, and finally unseen in the
training data. This means we can not use blocks
as direct reordering evidences.
A good way to this problem is to use features of
blocks as reordering evidences. Good features can
not only capture reorderings, avoid sparseness, but
also integrate generalizations. It is very straight
to use maximum entropy model to integrate fea-
tures to predicate reorderings of blocks. Under the
MaxEnt model, we have
? = p?(o|A1, A2) = exp(
?
i ?ihi(o,A1, A2))?
o exp(
?
i ?ihi(o,A1, A2))(10)
where the functions hi ? {0, 1} are model features
and the ?i are weights of the model features which
can be trained by different algorithms (Malouf,
2002).
3.1 Reordering Example Extraction
Algorithm
The input for the algorithm is a bilingual corpus
with high-precision word alignments. We obtain
the word alignments using the way of Koehn et al
(2005). After running GIZA++ (Och and Ney,
target
source
b1
b2
b3
b4
c1
c2
Figure 1: The bold dots are corners. The ar-
rows from the corners are their links. Corner c1 is
shared by block b1 and b2, which in turn are linked
by the STRAIGHT links, bottomleft and topright
of c1. Similarly, block b3 and b4 are linked by the
INVERTED links, topleft and bottomright of c2.
2000) in both directions, we apply the ?grow-
diag-final? refinement rule on the intersection
alignments for each sentence pair.
Before we introduce this algorithm, we intro-
duce some formal definitions. The first one is
block which is a pair of source and target contigu-
ous sequences of words
b = (si2i1 , t
j2
j1)
b must be consistent with the word alignment M
?(i, j) ? M, i1 ? i ? i2 ? j1 ? j ? j2
This definition is similar to that of bilingual phrase
except that there is no length limitation over block.
A reordering example is a triple of (o, b1, b2)
where b1 and b2 are two neighbor blocks and o
is the order between them. We define each vertex
of block as corner. Each corner has four links in
four directions: topright, topleft, bottomright, bot-
tomleft, and each link links a set of blocks which
have the corner as their vertex. The topright and
bottomleft link blocks with the straight order, so
we call them STRAIGHT links. Similarly, we call
the topleft and bottomright INVERTED links since
they link blocks with the inverted order. For con-
venience, we use b ?? L to denote that block b
is linked by the link L. Note that the STRAIGHT
links can not coexist with the INVERTED links.
These definitions are illustrated in Figure 1.
The reordering example extraction algorithm is
shown in Figure 2. The basic idea behind this al-
gorithm is to register all neighbor blocks to the
associated links of corners which are shared by
them. To do this, we keep an array to record link
524
1: Input: sentence pair (s, t) and their alignment M
2: < := ?
3: for each span (i1, i2) ? s do
4: find block b = (si2i1 , t
j2
j1) that is consistent with M
5: Extend block b on the target boundary with one possi-
ble non-aligned word to get blocks E(b)
6: for each block b? ? b?E(b) do
7: Register b? to the links of four corners of it
8: end for
9: end for
10: for each corner C in the matrix M do
11: if STRAIGHT links exist then
12: < := <?{(straight, b1, b2)},
b1 ?? C.bottomleft, b2 ?? C.topright
13: else if INVERTED links exist then
14: < := <?{(inverted, b1, b2)},
b1 ?? C.topleft, b2 ?? C.bottomright
15: end if
16: end for
17: Output: reordering examples <
Figure 2: Reordering Example Extraction Algo-
rithm.
information of corners when extracting blocks.
Line 4 and 5 are similar to the phrase extraction
algorithm by Och (2003b). Different from Och,
we just extend one word which is aligned to null
on the boundary of target side. If we put some
length limitation over the extracted blocks and out-
put them, we get bilingual phrases used in standard
phrase-based SMT systems and also in our sys-
tem. Line 7 updates all links associated with the
current block. You can attach the current block
to each of these links. However this will increase
reordering examples greatly, especially those with
the straight order. In our Experiments, we just at-
tach the smallest blocks to the STRAIGHT links,
and the largest blocks to the INVERTED links.
This will keep the number of reordering examples
acceptable but without performance degradation.
Line 12 and 14 extract reordering examples.
3.2 Features
With the extracted reordering examples, we can
obtain features for our MaxEnt-based reordering
model. We design two kinds of features, lexi-
cal features and collocation features. For a block
b = (s, t), we use s1 to denote the first word of the
source s, t1 to denote the first word of the target t.
Lexical features are defined on the single word
s1 or t1. Collocation features are defined on the
combination s1 or t1 between two blocks b1 and
b2. Three kinds of combinations are used. The first
one is source collocation, b1.s1&b2.s1. The sec-
ond is target collocation, b1.t1&b2.t1. The last one
hi(o, b1, b2) =
{ 1, b1.t1 = E1, o = O
0, otherwise
hj(o, b1, b2) =
{ 1, b1.t1 = E1, b2.t1 = E2, o = O
0, otherwise
Figure 3: MaxEnt-based reordering feature tem-
plates. The first one is a lexical feature, and the
second one is a target collocation feature, where
Ei are English words, O ? {straight, inverted}.
is block collocation, b1.s1&b1.t1 and b2.s1&b2.t1.
The templates for the lexical feature and the collo-
cation feature are shown in Figure 3.
Why do we use the first words as features?
These words are nicely at the boundary of blocks.
One of assumptions of phrase-based SMT is that
phrase cohere across two languages (Fox, 2002),
which means phrases in one language tend to be
moved together during translation. This indicates
that boundary words of blocks may keep informa-
tion for their movements/reorderings. To test this
hypothesis, we calculate the information gain ra-
tio (IGR) for boundary words as well as the whole
blocks against the order on the reordering exam-
ples extracted by the algorithm described above.
The IGR is the measure used in the decision tree
learning to select features (Quinlan, 1993). It
represents how precisely the feature predicate the
class. For feature f and class c, the IGR(f, c)
IGR(f, c) = En(c)? En(c|f)En(f) (11)
where En(?) is the entropy and En(?|?)
is the conditional entropy. To our sur-
prise, the IGR for the four boundary words
(IGR(?b1.s1, b2.s1, b1.t1, b2.t1?, order) =
0.2637) is very close to that for the two blocks
together (IGR(?b1, b2?, order) = 0.2655).
Although our reordering examples do not cover
all reordering events in the training data, this
result shows that boundary words do provide
some clues for predicating reorderings.
4 Experiments
We carried out experiments to compare against
various reordering models and systems to demon-
strate the competitiveness of MaxEnt-based re-
ordering:
1. Monotone search: the inverted rule is not
used.
525
2. Reordering variants: the NONE, distortion
and flat reordering models described in Sec-
tion 2.1.
3. Pharaoh: A state-of-the-art distortion-based
decoder (Koehn, 2004).
4.1 Corpus
Our experiments were made on two Chinese-to-
English translation tasks: NIST MT-05 (news do-
main) and IWSLT-04 (travel dialogue domain).
NIST MT-05. In this task, the bilingual train-
ing data comes from the FBIS corpus with 7.06M
Chinese words and 9.15M English words. The tri-
gram language model training data consists of En-
glish texts mostly derived from the English side
of the UN corpus (catalog number LDC2004E12),
which totally contains 81M English words. For the
efficiency of minimum error rate training, we built
our development set using sentences of length at
most 50 characters from the NIST MT-02 evalua-
tion test data.
IWSLT-04. For this task, our experiments were
carried out on the small data track. Both the
bilingual training data and the trigram language
model training data are restricted to the supplied
corpus, which contains 20k sentences, 179k Chi-
nese words and 157k English words. We used the
CSTAR 2003 test set consisting of 506 sentence
pairs as development set.
4.2 Training
We obtained high-precision word alignments us-
ing the way described in Section 3.1. Then we
ran our reordering example extraction algorithm to
output blocks of length at most 7 words on the Chi-
nese side together with their internal alignments.
We also limited the length ratio between the target
and source language (max(|s|, |t|)/min(|s|, |t|))
to 3. After extracting phrases, we calculated the
phrase translation probabilities and lexical transla-
tion probabilities in both directions for each bilin-
gual phrase.
For the minimum-error-rate training, we re-
implemented Venugopal?s trainer 3 (Venugopal
et al, 2005) in C++. For all experiments, we ran
this trainer with the decoder iteratively to tune the
weights ?s to maximize the BLEU score on the
development set.
3See http://www.cs.cmu.edu/ ashishv/mer.html. This is a
Matlab implementation.
Pharaoh
We shared the same phrase translation tables
between Pharaoh and our system since the two
systems use the same features of phrases. In fact,
we extracted more phrases than Pharaoh?s trainer
with its default settings. And we also used our re-
implemented trainer to tune lambdas of Pharaoh
to maximize its BLEU score. During decoding,
we pruned the phrase table with b = 100 (default
20), pruned the chart with n = 100, ? = 10?5
(default setting), and limited distortions to 4
(default 0).
MaxEnt-based Reordering Model
We firstly ran our reordering example extraction
algorithm on the bilingual training data without
any length limitations to obtain reordering ex-
amples and then extracted features from these
examples. In the task of NIST MT-05, we
obtained about 2.7M reordering examples with
the straight order, and 367K with the inverted
order, from which 112K lexical features and
1.7M collocation features after deleting those
with one occurrence were extracted. In the task
of IWSLT-04, we obtained 79.5k reordering
examples with the straight order, 9.3k with the
inverted order, from which 16.9K lexical features
and 89.6K collocation features after deleting those
with one occurrence were extracted. Finally, we
ran the MaxEnt toolkit by Zhang 4 to tune the
feature weights. We set iteration number to 100
and Gaussian prior to 1 for avoiding overfitting.
4.3 Results
We dropped unknown words (Koehn et al, 2005)
of translations for both tasks before evaluating
their BLEU scores. To be consistent with the
official evaluation criterions of both tasks, case-
sensitive BLEU-4 scores were computed For the
NIST MT-05 task and case-insensitive BLEU-4
scores were computed for the IWSLT-04 task 5.
Experimental results on both tasks are shown in
Table 1. Italic numbers refer to results for which
the difference to the best result (indicated in bold)
is not statistically significant. For all scores, we
also show the 95% confidence intervals computed
using Zhang?s significant tester (Zhang et al,
2004) which was modified to conform to NIST?s
4See http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
5Note that the evaluation criterion of IWSLT-04 is not to-
tally matched since we didn?t remove punctuation marks.
526
definition of the BLEU brevity penalty.
We observe that if phrasal reordering is totally
dependent on the language model (NONE) we
get the worst performance, even worse than the
monotone search. This indicates that our language
models were not strong to discriminate between
straight orders and inverted orders. The flat and
distortion reordering models (Row 3 and 4) show
similar performance with Pharaoh. Although they
are not dependent on phrases, they really reorder
phrases with penalties to wrong orders supported
by the language model and therefore outperform
the monotone search. In row 6, only lexical fea-
tures are used for the MaxEnt-based reordering
model; while row 7 uses lexical features and col-
location features. On both tasks, we observe that
various reordering approaches show similar and
stable performance ranks in different domains and
the MaxEnt-based reordering models achieve the
best performance among them. Using all features
for the MaxEnt model (lex + col) is marginally
better than using only lex features (lex).
4.4 Scaling to Large Bitexts
In the experiments described above, collocation
features do not make great contributions to the per-
formance improvement but make the total num-
ber of features increase greatly. This is a prob-
lem for MaxEnt parameter estimation if it is scaled
to large bitexts. Therefore, for the integration of
MaxEnt-based phrase reordering model in the sys-
tem trained on large bitexts, we remove colloca-
tion features and only use lexical features from
the last words of blocks (similar to those from the
first words of blocks with similar performance).
This time the bilingual training data contain 2.4M
sentence pairs (68.1M Chinese words and 73.8M
English words) and two trigram language models
are used. One is trained on the English side of
the bilingual training data. The other is trained on
the Xinhua portion of the Gigaword corpus with
181.1M words. We also use some rules to trans-
late numbers, time expressions and Chinese per-
son names. The new Bleu score on NIST MT-05
is 0.291 which is very promising.
5 Discussion and Future Work
In this paper we presented a MaxEnt-based phrase
reordering model for SMT. We used lexical fea-
tures and collocation features from boundary
words of blocks to predicate reorderings of neigh-
Systems NIST MT-05 IWSLT-04
monotone 20.1 ? 0.8 37.8 ? 3.2
NONE 19.6 ? 0.8 36.3 ? 2.9
Distortion 20.9 ? 0.8 38.8 ? 3.0
Flat 20.5 ? 0.8 38.7 ? 2.8
Pharaoh 20.8 ? 0.8 38.9 ? 3.3
MaxEnt (lex) 22.0 ? 0.8 42.4 ? 3.3
MaxEnt (lex + col) 22.2 ? 0.8 42.8 ? 3.3
Table 1: BLEU-4 scores (%) with the 95% confi-
dence intervals. Italic numbers refer to results for
which the difference to the best result (indicated in
bold) is not statistically significant.
bor blocks. Experiments on standard Chinese-
English translation tasks from two different do-
mains showed that our method achieves a signif-
icant improvement over the distortion/flat reorder-
ing models.
Traditional distortion/flat-based SMT transla-
tion systems are good for learning phrase transla-
tion pairs, but learn nothing for phrasal reorder-
ings from real-world data. This is our original
motivation for designing a new reordering model,
which can learn reorderings from training data just
like learning phrasal translations. Lexicalized re-
ordering model learns reorderings from training
data, but it binds reorderings to individual concrete
phrases, which restricts the model to reorderings
of phrases seen in training data. On the contrary,
the MaxEnt-based reordering model is not limited
by this constraint since it is based on features of
phrase, not phrase itself. It can be easily general-
ized to reorder unseen phrases provided that some
features are fired on these phrases.
Another advantage of the MaxEnt-based re-
ordering model is that it can take more fea-
tures into reordering, even though they are non-
independent. Tillmann et. al (2005) also use a
MaxEnt model to integrate various features. The
difference is that they use the MaxEnt model to
predict not only orders but also blocks. To do that,
it is necessary for the MaxEnt model to incorpo-
rate real-valued features such as the block trans-
lation probability and the language model proba-
bility. Due to the expensive computation, a local
model is built. However, our MaxEnt model is just
a module of the whole log-linear model of transla-
tion which uses its score as a real-valued feature.
The modularity afforded by this design does not
incur any computation problems, and make it eas-
527
ier to update one sub-model with other modules
unchanged.
Beyond the MaxEnt-based reordering model,
another feature deserving attention in our system
is the CKY style decoder which observes the ITG.
This is different from the work of Zens et. al.
(2004). In their approach, translation is generated
linearly, word by word and phrase by phrase in a
traditional way with respect to the incorporation
of the language model. It can be said that their de-
coder did not violate the ITG constraints but not
that it observed the ITG. The ITG not only de-
creases reorderings greatly but also makes reorder-
ing hierarchical. Hierarchical reordering is more
meaningful for languages which are organized hi-
erarchically. From this point, our decoder is simi-
lar to the work by Chiang (2005).
The future work is to investigate other valuable
features, e.g. binary features that explain blocks
from the syntactical view. We think that there is
still room for improvement if more contributing
features are used.
Acknowledgements
This work was supported in part by National High
Technology Research and Development Program
under grant #2005AA114140 and National Nat-
ural Science Foundation of China under grant
#60573188. Special thanks to Yajuan Lu? for
discussions of the manuscript of this paper and
three anonymous reviewers who provided valuable
comments.
References
Ashish Venugopal, Stephan Vogel. 2005. Considerations in
Maximum Mutual Information and Minimum Classifica-
tion Error training for Statistical Machine Translation. In
the Proceedings of EAMT-05, Budapest, Hungary May 30-
31.
Christoph Tillmann. 2004. A block orientation model for
statistical machine translation. In HLT-NAACL, Boston,
MA, USA.
Christoph Tillmann and Tong Zhang. 2005. A Localized
Prediction Model for statistical machine translation. In
Proceedings of ACL 2005, pages 557?564.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of ACL
2005, pages 263?270.
Dekai Wu. 1996. A Polynomial-Time Algorithm for Statis-
tical Machine Translation. In Proceedings of ACL 1996.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23:377?404.
Franz Josef Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL 2000, pages
440?447.
Franz Josef Och. 2003a. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL 2003,
pages 160?167.
Franz Josef Och. 2003b. Statistical Machine Translation:
From Single-Word Models to Alignment Templates The-
sis.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30:417?449.
Franz Josef Och, Ignacio Thayer, Daniel Marcu, Kevin
Knight, Dragos Stefan Munteanu, Quamrul Tipu, Michel
Galley, and Mark Hopkins. 2004. Arabic and Chinese MT
at USC/ISI. Presentation given at NIST Machine Transla-
tion Evaluation Workshop.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP 2002.
J. R. Quinlan. 1993. C4.5: progarms for machine learning.
Morgan Kaufmann Publishers.
Kevin Knight. 1999. Decoding complexity in wordreplace-
ment translation models. Computational Linguistics,
Squibs & Discussion, 25(4).
Liang Huang and David Chiang. 2005. Better k-best parsing.
In Proceedings of the Ninth International Workshop on
Parsing Technology, Vancouver, October, pages 53?64.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings of
HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of the Sixth Conference of the Association for
Machine Translation in the Americas, pages 115?124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,
Chris Callison-Burch, Miles Osborne and David Talbot.
2005. Edinburgh System Description for the 2005 IWSLT
Speech Translation Evaluation. In International Work-
shop on Spoken Language Translation.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Machine
Translation. In Proceedings of CoLing 2004, Geneva,
Switzerland, pp. 205-211.
Robert Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of the
Sixth Conference on Natural Language Learning (CoNLL-
2002).
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation. In
Proceedings of HLT-EMNLP.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do
we need to have a better system? In Proceedings of LREC
2004, pages 2051? 2054.
528
	
			


Proceedings of the Second Workshop on Statistical Machine Translation, pages 40?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Dependency Treelet String Correspondence
Model for Statistical Machine Translation
Deyi Xiong, Qun Liu and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
{dyxiong, liuqun, sxlin}@ict.ac.cn
Abstract
This paper describes a novel model using
dependency structures on the source side
for syntax-based statistical machine transla-
tion: Dependency Treelet String Correspon-
dence Model (DTSC). The DTSC model
maps source dependency structures to tar-
get strings. In this model translation pairs of
source treelets and target strings with their
word alignments are learned automatically
from the parsed and aligned corpus. The
DTSC model allows source treelets and tar-
get strings with variables so that the model
can generalize to handle dependency struc-
tures with the same head word but with dif-
ferent modifiers and arguments. Addition-
ally, target strings can be also discontinuous
by using gaps which are corresponding to
the uncovered nodes which are not included
in the source treelets. A chart-style decod-
ing algorithm with two basic operations?
substituting and attaching?is designed for
the DTSC model. We argue that the DTSC
model proposed here is capable of lexical-
ization, generalization, and handling discon-
tinuous phrases which are very desirable for
machine translation. We finally evaluate our
current implementation of a simplified ver-
sion of DTSC for statistical machine trans-
lation.
1 Introduction
Over the last several years, various statistical syntax-
based models were proposed to extend traditional
word/phrase based models in statistical machine
translation (SMT) (Lin, 2004; Chiang, 2005; Ding
et al, 2005; Quirk et al, 2005; Marcu et al, 2006;
Liu et al, 2006). It is believed that these models
can improve the quality of SMT significantly. Com-
pared with phrase-based models, syntax-based mod-
els lead to better reordering and higher flexibility
by introducing hierarchical structures and variables
which make syntax-based models capable of hierar-
chical reordering and generalization. Due to these
advantages, syntax-based approaches are becoming
an active area of research in machine translation.
In this paper, we propose a novel model based on
dependency structures: Dependency Treelet String
Correspondence Model (DTSC). The DTSC model
maps source dependency structures to target strings.
It just needs a source language parser. In contrast to
the work by Lin (2004) and by Quirk et al (2005),
the DTSC model does not need to generate target
language dependency structures using source struc-
tures and word alignments. On the source side, we
extract treelets which are any connected subgraphs
and consistent with word alignments. While on the
target side, we allow the aligned target sequences
to be generalized and discontinuous by introducing
variables and gaps. The variables on the target side
are aligned to the corresponding variables of treelets,
while gaps between words or variables are corre-
sponding to the uncovered nodes which are not in-
cluded by treelets. To complete the translation pro-
cess, we design two basic operations for the decod-
ing: substituting and attaching. Substituting is used
to replace variable nodes which have been already
translated, while attaching is used to attach uncov-
40
ered nodes to treelets.
In the remainder of the paper, we first define de-
pendency treelet string correspondence in section
2 and describe an algorithm for extracting DTSCs
from the parsed and word-aligned corpus in section
3. Then we build our model based on DTSC in sec-
tion 4. The decoding algorithm and related pruning
strategies are introduced in section 5. We also spec-
ify the strategy to integrate phrases into our model
in section 6. In section 7 we evaluate our current
implementation of a simplified version of DTSC for
statistical machine translation. And finally, we dis-
cuss related work and conclude.
2 Dependency Treelet String
Correspondence
A dependency treelet string correspondence pi is a
triple < D,S,A > which describes a translation
pair < D,S > and their alignment A, where D is
the dependency treelet on the source side and S is
the translation string on the target side. < D,S >
must be consistent with the word alignment M of
the corresponding sentence pair
?(i, j) ? M, i ? D ? j ? S
A treelet is defined to be any connected subgraph,
which is similar to the definition in (Quirk et al,
2005). Treelet is more representatively flexible than
subtree which is widely used in models based on
phrase structures (Marcu et al, 2006; Liu et al,
2006). The most important distinction between the
treelet in (Quirk et al, 2005) and ours is that we al-
low variables at positions of subnodes. In our defini-
tion, the root node must be lexicalized but the subn-
odes can be replaced with a wild card. The target
counterpart of a wildcard node in S is also replaced
with a wild card. The wildcards introduced in this
way generalize DTSC to match dependency struc-
tures with the same head word but with different
modifiers or arguments.
Another unique feature of our DTSC is that we al-
low target strings with gaps between words or wild-
cards. Since source treelets may not cover all subn-
odes, the uncovered subnodes will generate a gap as
its counterpart on the target side. A sequence of con-
tinuous gaps will be merged to be one gap and gaps
at the beginning and the end of S will be removed
automatically.
??
eeeeeee
?
?
?
? ??eeeeeee
s s
s s
s s
s s??
??
? ?eeeeeee
?
?
?
?
]]]]]]]]]]
the conference cooperation of the ?
??
eeeeeeebbbbbbbbbbbbb
s s
s s
s s
s s?1
w w
w w
? YYYYYYY
SSS
SSS
S ?
?2 ]]]]]]]]]]?1 keep a G with the ?2
Figure 1: DTSC examples. Note that ? represents
variable and G represents gap.
Gap can be considered as a special kind of vari-
able whose counterpart on the source side is not
present. This makes the model more flexible to
match more partial dependency structures on the
source side. If only variables can be used, the model
has to match subtrees rather than treelets on the
source side. Furthermore, the positions of variables
on the target side are fixed so that some reorderings
related with them can be recorded in DTSC. The po-
sitions of gaps on the target side, however, are not
fixed until decoding. The presence of one gap and
its position can not be finalized until attaching op-
eration is performed. The introduction of gaps and
the related attaching operation in decoding is the
most important distinction between our model and
the previous syntax-based models.
Figure 1 shows several different DTSCs automat-
ically extracted from our training corpus. The top
left DTSC is totally lexicalized, while the top right
DTSC has one variable and the bottom has two vari-
ables and one gap. In the bottom DTSC, note that
the node ? which is aligned to the gap G of the
target string is an uncovered node and therefore not
included in the treelet actually. Here we just want
to show there is an uncovered node aligned with the
gap G.
Each node at the source treelet has three attributes
1. The head word
2. The category, i.e. the part of speech of the head
word
3. The node order which specifies the local order
of the current node relative to its parent node.
41
??/VV
eeeeeeebbbbbbbbbbbbbb ]]]]]]]]]]]]]]]]]]]]]
?
?
?
?
?
??/VV
?
?
?
DD
DD
D ?/P YYYYYYY
XXXXX
XXXXX
XXXX
??/NN
eeeeeee
z z
z z
z
????/NR
\\\\\\\\\\
\\\\ ??/NN
j j j
j??
go on providingfinancial aid to Palestine
1 2 3 4 5 6 7
Figure 2: An example dependency tree and its align-
ments
Note that the node order is defined at the context of
the extracted treelets but not the context of the orig-
inal tree. For example, the attributes for the node?
in the bottom DTSC of Figure 1 are {?, P, -1}. For
two treelets, if and only if their structures are iden-
tical and each corresponding nodes share the same
attributes, we say they are matched.
3 Extracting DTSCs
To extract DTSCs from the training corpus, firstly
the corpus must be parsed on the source side and
aligned at the word level. The source structures pro-
duced by the parser are unlabelled, ordered depen-
dency trees with each word annotated with a part-of-
speech. Figure 2 shows an example of dependency
tree really used in our extractor.
When the source language dependency trees and
word alignments between source and target lan-
guages are obtained, the DTSC extraction algorithm
runs in two phases along the dependency trees and
alignments. In the first step, the extractor annotates
each node with specific attributes defined in section
3.1. These attributes are used in the second step
which extracts all possible DTSCs rooted at each
node recursively.
3.1 Node annotation
For each source dependency node n, we define three
attributes: word span, node span and crossed.
Word span is defined to be the target word sequence
aligned with the head word of n, while node span is
defined to be the closure of the union of node spans
of all subnodes of n and its word span. These two at-
tributes are similar to those introduced by Lin (Lin,
2004). The third attribute crossed is an indicator that
has binary values. If the node span of n overlaps
the word span of its parent node or the node span
of its siblings, the crossed indicator of n is 1 and
n is therefore a crossed node, otherwise the crossed
indicator is 0 and n is a non-crossed node. Only
non-crossed nodes can generate DTSCs because the
target word sequence aligned with the whole subtree
rooted at it does not overlap any other sequences and
therefore can be extracted independently.
For the dependency tree and its alignments shown
in Figure 2, only the node ?? is a crossed node
since its node span ([4,5]) overlaps the word span
([5,5]) of its parent node??.
3.2 DTSCs extraction
The DTSC extraction algorithm (shown in Figure 3)
runs recursively. For each non-crossed node, the al-
gorithm generates all possible DTSCs rooted at it by
combining DTSCs from some subsets of its direct
subnodes. If one subnode n selected in the com-
bination is a crossed node, all other nodes whose
word/node spans overlap the node span of n must be
also selected in this combination. This kind of com-
bination is defined to be consistent with the word
alignment because the DTSC generated by this com-
bination is consistent with the word alignment. All
DTSCs generated in this way will be returned to the
last call and outputted. For each crossed node, the
algorithm generates pseudo DTSCs1 using DTSCs
from all of its subnodes. These pseudo DTSCs will
be returned to the last call but not outputted.
During the combination of DTSCs from subnodes
into larger DTSCs, there are two major tasks. One
task is to generate the treelet using treelets from
subnodes and the current node. This is a basic tree
generation operation. It is worth mentioning that
some non-crossed nodes are to be replaced with a
wild card so the algorithm can learn generalized
DTSCs described in section 2. Currently, we re-
place any non-crossed node alone or together with
their sibling non-crossed nodes. The second task
is to combine target strings. The word sequences
aligned with uncovered nodes will be replaced with
a gap. The word sequences aligned with wildcard
nodes will be replaced with a wild card.
If a non-crossed node n has m direct subnodes,
all 2m combinations will be considered. This will
generate a very large number of DTSCs, which is
1Some words in the target string are aligned with nodes
which are not included in the source treelet.
42
DTSCExtractor(Dnode n)
< := ? (DTSC container of n)
for each subnode k of n do
R := DTSCExtractor(k)
L := L?R
end for
if n.crossed! = 1 and there are no subnodes whose span
overlaps the word span of n then
Create a DTSC pi =< D,S,A > where the dependency
treelet D only contains the node n (not including any chil-
dren of it)
output pi
for each combination c of n?s subnodes do
if c is consistent with the word alignment then
Generate all DTSCs R by combining DTSCs (L)
from the selected subnodes with the current node n
< := <?R
end if
end for
output <
return <
else if n.crossed == 1 then
Create pseudo DTSCs P by combining all DTSCs from
n?s all subnodes.
< := <?P
return <
end if
Figure 3: DTSC Extraction Algorithm.
undesirable for training and decoding. Therefore we
filter DTSCs according to the following restrictions
1. If the number of direct subnodes of node n is
larger than 6, we only consider combining one
single subnode with n each time because in this
case reorderings of subnodes are always mono-
tone.
2. On the source side, the number of direct subn-
odes of each node is limited to be no greater
than ary-limit; the height of treelet D is limited
to be no greater than depth-limit.
3. On the target side, the length of S (including
gaps and variables) is limited to be no greater
than len-limit; the number of gaps in S is lim-
ited to be no greater than gap-limit.
4. During DTSC combination, the DTSCs from
each subnode are sorted by size (in descending
order). Only the top comb-limit DTSCs will be
selected to generate larger DTSCs.
As an example, for the dependency tree and its
alignments in Figure 2, all DTSCs extracted by the
Treelet String
(??/VV/0) go on
(????/NR/0) Palestine
(?/P/0) to
(?/P/0 (????/NR/1)) to Palestine
(?/P/0 (?/1)) to ?
(??/NN/0 (??/NN/-1)) financial aid
(??/VV/0) providing
(??/VV/0 (?/1)) providing ?
(??/VV/0 (?/-1)) providing G ?
(??/VV/0 (??/VV/-1)) go on providing
(??/VV/0 (?/-1)) ? providing
(??/VV/0 (?1/-1) (?2/1)) providing ?2 ?1
(??/VV/0 (?1/-1 ) (?2/1)) ?1 providing ?2
Table 1: Examples of DTSCs extracted from Figure
2. Alignments are not shown here because they are
self-evident.
algorithm with parameters { ary-limit = 2, depth-
limit = 2, len-limit = 3, gap-limit = 1, comb-limit
= 20 } are shown in the table 1.
4 The Model
Given an input dependency tree, the decoder gen-
erates translations for each dependency node in
bottom-up order. For each node, our algorithm will
search all matched DTSCs automatically learned
from the training corpus by the way mentioned in
section 3. When the root node is traversed, the trans-
lating is finished. This complicated procedure in-
volves a large number of sequences of applications
of DTSC rules. Each sequence of applications of
DTSC rules can derive a translation.
We define a derivation ? as a sequence of appli-
cations of DTSC rules, and let c(?) and e(?) be the
source dependency tree and the target yield of ?, re-
spectively. The score of ? is defined to be the prod-
uct of the score of the DTSC rules used in the trans-
lation, and timed by other feature functions:
?(?) =
?
i
?(i) ? plm(e)?lm ? exp(??apA(?)) (1)
where ?(i) is the score of the ith application of
DTSC rules, plm(e) is the language model score,
and exp(??apA(?)) is the attachment penalty,
where A(?) calculates the total number of attach-
ments occurring in the derivation ?. The attach-
ment penalty gives some control over the selection
of DTSC rules which makes the model prefer rules
43
with more nodes covered and therefore less attach-
ing operations involved.
For the score of DTSC rule pi, we define it as fol-
lows:
?(pi) =
?
j
fj(pi)?j (2)
where the fj are feature functions defined on DTSC
rules. Currently, we used features proved to be ef-
fective in phrase-based SMT, which are:
1. The translation probability p(D|S).
2. The inverse translation probability p(S|D).
3. The lexical translation probability plex(D|S)
which is computed over the words that occur
on the source and target sides of a DTSC rule
by the IBM model 1.
4. The inverse lexical translation probability
plex(S|D) which is computed over the words
that occur on the source and target sides of a
DTSC rule by the IBM model 1.
5. The word penalty wp.
6. The DTSC penalty dp which allows the model
to favor longer or shorter derivations.
It is worth mentioning how to integrate the N-
gram language mode into our DTSC model. During
decoding, we have to encounter many partial transla-
tions with gaps and variables. For these translations,
firstly we only calculate the language model scores
for word sequences in the translations. Later we up-
date the scores when gaps are removed or specified
by attachments or variables are substituted. Each up-
dating involves merging two neighbor substrings sl
(left) and sr (right) into one bigger string s. Let the
sequence of n ? 1 (n is the order of N-gram lan-
guage model used) rightmost words of sl be srl and
the sequence of n?1 leftmost words of sr be slr. we
have:
LM(s) = LM(sl) + LM(sr) + LM(srl slr)
?LM(srl )? LM(slr) (3)
where LM is the logarithm of the language model
probability. We only need to compute the increment
of the language model score:
4LM = LM(srl slr)? LM(srl )? LM(slr) (4)
for each node n of the input tree T , in bottom-up order do
Get al matched DTSCs rooted at n
for each matched DTSC pi do
for each wildcard node n? in pi do
Substitute the corresponding wildcard on the target
side with translations from the stack of n?
end for
for each uncovered node n@ by pi do
Attach the translations from the stack of n@ to the
target side at the attaching point
end for
end for
end for
Figure 4: Chart-style Decoding Algorithm for the
DTSC Model.
Melamed (2004) also used a similar way to integrate
the language model.
5 Decoding
Our decoding algorithm is similar to the bottom-up
chart parsing. The distinction is that the input is a
tree rather than a string and therefore the chart is in-
dexed by nodes of the tree rather than spans of the
string. Also, several other tree-based decoding al-
gorithms introduced by Eisner (2003), Quirk et al
(2005) and Liu et al (2006) can be classified as the
chart-style parsing algorithm too.
Our decoding algorithm is shown in Figure 4.
Given an input dependency tree, firstly we generate
the bottom-up order by postorder transversal. This
order guarantees that any subnodes of node n have
been translated before node n is done. For each
node n in the bottom-up order, all matched DTSCs
rooted at n are found, and a stack is also built for it to
store the candidate translations. A DTSC pi is said to
match the input dependency subtree T rooted at n if
and only if there is a treelet rooted at n that matches
2 the treelet of pi on the source side.
For each matched DTSC pi, two operations will
be performed on it. The first one is substituting
which replaces a wildcard node with the correspond-
ing translated node. The second one is attaching
which attaches an uncovered node to pi. The two op-
erations are shown in Figure 5. For each wildcard
node n?, translations from the stack of it will be se-
lected to replace the corresponding wildcard on the
2The words, categories and orders of each corresponding
nodes are matched. Please refer to the definition of matched
in section 2.
44
(a) A
eeeeeee YYYYYYYB
eeeeeee
? + D
C ? De
?e Ae Be Ce
Substitute ?
(b) A
eeeeeee YYYYYYYB
eeeeeee
D + E
C ? Ee
De Ae Be Ce
Attach ?
(c) A
eeeeeee YYYYYYYB
eeeeeee YYYYYYY
D
C E
De Ae Be Ee Ce
Figure 5: Substituting and attaching operations for
decoding. Xe is the translation of X . Node that ? is
a wildcard node to be substituted and node ? is an
uncovered node to be attached.
target side and the scores of new translations will be
calculated according to our model. For each uncov-
ered node n@, firstly we determine where transla-
tions from the stack of n@ should be attached on the
target side. There are several different mechanisms
for choosing attaching points. Currently, we imple-
ment a heuristic way: on the source side, we find the
node n@p which is the nearest neighbor of n@ from
its parent and sibling nodes, then the attaching point
is the left/right of the counterpart of n@p on the target
side according to their relative order. As an example,
see the uncovered node ? in Figure 5. The nearest
node to it is node B. Since node ? is at the right
of node B, the attaching point is the right of Be.
One can search all possible points using an ordering
model. And this ordering model can also use infor-
mation from gaps on the target side. We believe this
ordering model can improve the performance and let
it be one of directions for our future research.
Note that the gaps on the target side are not neces-
sarily attaching points in our current attaching mech-
anism. If they are not attaching point, they will be
removed automatically.
The search space of the decoding algorithm is
very large, therefore some pruning techniques have
to be used. To speed up the decoder, the following
pruning strategies are adopted.
1. Stack pruning. We use three pruning ways.
The first one is recombination which converts
the search to dynamic programming. When
two translations in the same stack have the
same w leftmost/rightmost words, where w de-
pends on the order of the language model, they
will be recombined by discarding the transla-
tion with lower score. The second one is the
threshold pruning which discards translations
that have a score worse than stack-threshold
times the best score in the same stack. The
last one is the histogram pruning which only
keeps the top stack-limit best translations for
each stack.
2. Node pruning. For each node, we only keep
the top node-limit matched DTSCs rooted at
that node, as ranked by the size of source
treelets.
3. Operation pruning. For each operation, sub-
stituting and attaching, the decoding will gen-
erate a large number of partial translations3
for the current node. We only keep the top
operation-limit partial translations each time
according to their scores.
6 Integrating Phrases
Although syntax-based models are good at dealing
with hierarchical reordering, but at the local level,
translating idioms and similar complicated expres-
sions can be a problem. However, phrase-based
models are good at dealing with these translations.
Therefore, integrating phrases into the syntax-based
models can improve the performance (Marcu et al,
2006; Liu et al, 2006). Since our DTSC model is
based on dependency structures and lexicalized nat-
urally, DTSCs are more similar to phrases than other
translation units based on phrase structures. This
means that phrases will be easier to be integrated
into our model.
The way to integrate phrases is quite straightfor-
ward: if there is a treelet rooted at the current node,
3There are wildcard nodes or uncovered nodes to be han-
dled.
45
of which the word sequence is continuous and iden-
tical to the source of some phrase, then a phrase-
style DTSC will be generated which uses the target
string of the phrase as its own target. The procedure
is finished during decoding. In our experiments, in-
tegrating phrases improves the performance greatly.
7 Current Implementation
To test our idea, we implemented the dependency
treelet string correspondence model in a Chinese-
English machine translation system. The current im-
plementation in this system is actually a simplified
version of the DTSC model introduced above. In
this version, we used a simple heuristic way for the
operation of attaching rather than a sophisticated sta-
tistical model which can learn ordering information
from the training corpus. Since dependency struc-
tures are more?flattened? compared with phrasal
structures, there are many subnodes which will not
be covered even by generalized matched DTSCs.
This means the attaching operation is very common
during decoding. Therefore better attaching model
which calculates the best point for attaching , we be-
lieve, will improve the performance greatly and is a
major goal for our future research.
To obtain the dependency structures of the source
side, one can parse the source sentences with a de-
pendency parser or parse them with a phrasal struc-
ture parser and then convert the phrasal structures
into dependency structures. In our experiments we
used a Chinese parser implemented by Xiong et
al. (2005) which generates phrasal structures. The
parser was trained on articles 1-270 of Penn Chinese
Treebank version 1.0 and achieved 79.4% (F1 mea-
sure). We then converted the phrasal structure trees
into dependency trees using the way introduced by
Xia (1999).
To obtain the word alignments, we use the way
of Koehn et al (2005). After running GIZA++
(Och and Ney, 2000) in both directions, we apply
the ?grow-diag-final? refinement rule on the in-
tersection alignments for each sentence pair.
The training corpus consists of 31, 149 sentence
pairs with 823K Chinese words and 927K English
words. For the language model, we used SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
trigram model with modified Kneser-Ney smooth-
Systems BLEU-4
PB 20.88 ? 0.87
DTSC 20.20 ? 0.81
DTSC + phrases 21.46 ? 0.83
Table 2: BLEU-4 scores for our system and a
phrase-based system.
ing on the 31, 149 English sentences. We selected
580 short sentences of length at most 50 characters
from the 2002 NIST MT Evaluation test set as our
development corpus and used it to tune ?s by max-
imizing the BLEU score (Och, 2003), and used the
2005 NIST MT Evaluation test set as our test corpus.
From the training corpus, we learned 2, 729,
964 distinct DTSCs with the configuration { ary-
limit = 4, depth-limit = 4, len-limit = 15, gap-limit
= 2, comb-limit = 20 }. Among them, 160,694
DTSCs are used for the test set. To run our de-
coder on the development and test set, we set stack-
thrshold = 0.0001, stack-limit = 100, node-limit =
100, operation-limit = 20.
We also ran a phrase-based system (PB) with a
distortion reordering model (Xiong et al, 2006) on
the same corpus. The results are shown in table 2.
For all BLEU scores, we also show the 95% confi-
dence intervals computed using Zhang?s significant
tester (Zhang et al, 2004) which was modified to
conform to NIST?s definition of the BLEU brevity
penalty. The BLEU score of our current system with
the DTSC model is lower than that of the phrase-
based system. However, with phrases integrated, the
performance is improved greatly, and the new BLEU
score is higher than that of the phrase-based SMT.
This difference is significant according to Zhang?s
tester. This result can be improved further using a
better parser (Quirk et al, 2006) or using a statisti-
cal attaching model.
8 Related Work
The DTSC model is different from previous work
based on dependency grammars by Eisner (2003),
Lin (2004), Quirk et al (2005), Ding et al (2005)
since they all deduce dependency structures on the
target side. Among them, the most similar work is
(Quirk et al, 2005). But there are still several major
differences beyond the one mentioned above. Our
46
treelets allow variables at any non-crossed nodes and
target strings allow gaps, which are not available in
(Quirk et al, 2005). Our language model is calcu-
lated during decoding while Quirk?s language model
is computed after decoding because of the complex-
ity of their decoding.
The DTSC model is also quite distinct from pre-
vious tree-string models by Marcu et al (2006)
and Liu et al (2006). Firstly, their models are
based on phrase structure grammars. Secondly, sub-
trees instead of treelets are extracted in their mod-
els. Thirdly, it seems to be more difficult to integrate
phrases into their models. And finally, our model al-
low gaps on the target side, which is an advantage
shared by (Melamed, 2004) and (Simard, 2005).
9 Conclusions and Future Work
We presented a novel syntax-based model using
dependency trees on the source side?dependency
treelet string correspondence model?for statistical
machine translation. We described an algorithm to
learn DTSCs automatically from the training corpus
and a chart-style algorithm for decoding.
Currently, we implemented a simple version of
the DTSC model. We believe that our performance
can be improved greatly using a more sophisticated
mechanism for determining attaching points. There-
fore the most important future work should be to de-
sign a better attaching model. Furthermore, we plan
to use larger corpora for training and n-best depen-
dency trees for decoding, which both are helpful for
the improvement of translation quality.
Acknowledgements
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
References
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL.
Yuan Ding and Martha Palmer. 2005. Machine Translation Us-
ing Probabilistic Synchronous Dependency Insertion Gram-
mars. In Proceedings of ACL.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of ACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris
Callison-Burch, Miles Osborne and David Talbot. 2005.
Edinburgh System Description for the 2005 IWSLT Speech
Translation Evaluation. In International Workshop on Spo-
ken Language Translation.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation with
Syntactified Target Language Phraases. In Proceedings of
EMNLP.
I. Dan Melamed. 2004. Algorithms for Syntax-Aware Statisti-
cal Machine Translation. In Proceedings of the Conference
on Theoretical and Methodological Issues in Machine Trans-
lation (TMI), Baltimore, MD.
Dekang Lin. 2004. A path-based transfer model for machine
translation. In Proceedings of COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Translation. In
Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. Depen-
dency Treelet Translation: Syntactically Informed Phrasal
SMT. In Proceedings of ACL.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical machine
translation. In Proceedings of EMNLP, Sydney, Australia.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada.
2005. Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of International Conference on
Spoken Language Processing, volume 2, pages 901-904.
Fei Xia. 1999. Automatic Grammar Generation from Two Dif-
ferent Perspectives. PhD thesis, University of Pennsylvania.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of COLING-ACL, Sydney,
Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Island,
Korea.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC,
pages 2051? 2054.
47
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009?1016
Manchester, August 2008
Linguistically Annotated BTG for Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sg
Abstract
Bracketing Transduction Grammar (BTG)
is a natural choice for effective integration
of desired linguistic knowledge into sta-
tistical machine translation (SMT). In this
paper, we propose a Linguistically Anno-
tated BTG (LABTG) for SMT. It conveys
linguistic knowledge of source-side syn-
tax structures to BTG hierarchical struc-
tures through linguistic annotation. From
the linguistically annotated data, we learn
annotated BTG rules and train linguisti-
cally motivated phrase translation model
and reordering model. We also present an
annotation algorithm that captures syntac-
tic information for BTG nodes. The ex-
periments show that the LABTG approach
significantly outperforms a baseline BTG-
based system and a state-of-the-art phrase-
based system on the NISTMT-05 Chinese-
to-English translation task. Moreover, we
empirically demonstrate that the proposed
method achieves better translation selec-
tion and phrase reordering.
1 Introduction
Formal grammar used in statistical machine trans-
lation (SMT), such as Bracketing Transduction
Grammar (BTG) proposed by (Wu, 1997) and the
synchronous CFG presented by (Chiang, 2005),
provides a natural platform for integrating lin-
guistic knowledge into SMT because hierarchical
structures produced by the formal grammar resem-
ble linguistic structures.1 Chiang (2005) attempts
to integrate linguistic information into his formally
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1We inherit the definitions of formal and linguistic from
(Chiang, 2005) which makes a distinction between formally
syntax-based SMT and linguistically syntax-based SMT.
syntax-based system by adding a constituent fea-
ture. Unfortunately, the linguistic feature does not
show significant improvement on the test set. In
this paper, we further this effort by integrating lin-
guistic knowledge into BTG.
We want to augment BTG?s formal structures
with linguistic structures since they are both hier-
archical. In particular, our goal is to learn a more
linguistically meaningful BTG from real-world bi-
texts by projecting linguistic structures onto BTG
formal structures. In doing so, we hope to (1)
maintain the strength of phrase-based approach
since phrases are still used on BTG leaf nodes; (2)
obtain a tight integration of linguistic knowledge in
the translation model; (3) and finally avoid induc-
ing a complicated linguistic synchronous grammar
with expensive computation. The challenge, of
course, is that BTG hierarchical structures are not
always aligned with the linguistic structures in the
syntactic parse trees of source or target language.
Along this line, we propose a novel approach:
Linguistically Annotated BTG (LABTG) for SMT.
The LABTG annotates BTG rules with linguistic
elements that are learned from syntactic parse trees
on the source side through an annotation algo-
rithm, which is capable of labelling both syntactic
and non-syntactic phrases. The linguistic elements
extracted from parse trees capture both internal
lexical content and external context of phrases.
With these linguistic annotations, we expect the
LABTG to address two traditional issues of stan-
dard phrase-based SMT (Koehn et al, 2003) in a
more effective manner. They are (1) phrase trans-
lation: translating phrases according to their con-
texts; (2) phrase reordering: incorporating richer
linguistic features for better reordering.
The proposed LABTG displays two unique
characteristics when compared with BTG-based
SMT (Wu, 1996; Xiong et al, 2006). The first
is that two linguistically-informed sub-models are
introduced for better phrase translation and re-
ordering: annotated phrase translation model and
1009
annotated reordering model. The second is that
our proposed annotation algorithm and scheme are
capable of conveying linguistic knowledge from
source-side syntax structures to BTG structures.
We describe the LABTG model and the annota-
tion algorithm in Section 4. To better explain the
LABTG model, we establish a unified framework
of BTG-based SMT in Section 3. We conduct
a series of experiments to study the effect of the
LABTG in Section 5.
2 Related Work
There have been various efforts to integrate lin-
guistic knowledge into SMT systems, either from
the target side (Marcu et al, 2006; Hassan et al,
2007; Zollmann and Venugopal, 2006), the source
side (Quirk et al, 2005; Liu et al, 2006; Huang
et al, 2006) or both sides (Eisner, 2003; Ding et
al., 2005; Koehn and Hoang, 2007), just to name a
few. LABTG can be considered as, but not limited
to, a new attempt that enriches translation model
with source-side linguistic annotations.
(Huang and Knight, 2006) and (Hassan et al,
2007) introduce relabeling and supertagging on the
target side, respectively. The former re-annotates
syntactified phrases to learn grammatical distinc-
tions while the latter supertags standard plain
phrases, both applied on the target side. The differ-
ence between their work and LABTG is significant
because we annotate standard plain phrases using
linguistic elements on the source side. Compared
with the target side annotation which improves flu-
ency and grammaticality of translation output, lin-
guistic annotation on the source side helps to im-
prove translation adequacy.
Recently, some researchers have extended and
created several variations of BTG/ITG. Zhang et
al. (2005) propose lexicalized ITG for better word
alignment. Xiong et al (2006) demonstrate that
their MEBTG, a BTG variation with MaxEnt-
based reordering model, can improve phrase re-
ordering significantly. Similarly, Setiawan et al
(2007) use an enhanced BTG variation with func-
tion words for reordering. LABTG differs from
these BTG variations in that the latter does not use
any external linguistic knowledge.
Zhang et al (2007) describe a phrase reorder-
ing model based on BTG-style rules which inte-
grates source-side syntactic knowledge. Our an-
notated reordering model of LABTG differs from
their work in two key aspects. Firstly, we al-
low any phrase reorderings while they only reorder
syntactic phrases. In their model, only syntactic
phrases can use linguistic knowledge from parse
trees for reordering while non-syntactic phrases
are combined monotonously with a constant re-
ordering score since no syntactic knowledge can
be used at all. Our proposed LABTG successfully
overcomes this limitation by supporting linguis-
tic annotation on both syntactic and non-syntactic
phrases. Moreover, we show that excluding non-
syntactic phrase from reordering does hurt the
performance. Secondly, we use richer linguistic
knowledge in reordering, including head words
and syntactic labels of context nodes, when com-
pared with their model. Our experiments show that
these additional information can improve reorder-
ing.
3 BTG Based SMT
We establish a unified framework for BTG-based
SMT in this section. There are two kinds of rules
in BTG, lexical rules (denoted as rl) and merging
rules (denoted as rm):
r
l
: A ? x/y
and
r
m
: A ? [A
l
, A
r
]|?A
l
, A
r
?
Lexical rules translate source phrase x into target
phrase y and generate a leaf node A in BTG tree.
Merging rules combine left and right neighboring
phrases A
l
and A
r
into a larger phrase A in an or-
der o ? {straight, inverted}.
We define a BTG derivation D as a sequence
of independent applications of lexical and merging
rules (D = ?rl
1..n
l
, r
m
1..n
m
?). Given a source sen-
tence, the decoding task of BTG-based SMT is to
find a best derivation, which yields the best trans-
lation.
Similar to (Xiong et al, 2006), we can as-
sign a probability to each rule using a log-linear
model with different features and corresponding ?
weights, then multiply them to obtain P (D). For
convenience of notation and keeping in line with
the common understanding of standard phrase-
based model, here we re-organize these features
into translation model (P
T
), reordering model
(P
R
) and target language model (P
L
) as follows
P (D) = P
T
(r
l
1..n
l
) ? P
R
(r
m
1..n
m
)
?
R
?P
L
(e)
?
L
? exp(|e|)
?
w (1)
where exp(|e|) is the word penalty.
1010
The translation model is defined as:
P
T
(r
l
1..n
l
) =
n
l
?
i=1
P (r
l
i
)
P (r
l
) = p(x|y)
?
1
? p(y|x)
?
2
? p
lex
(x|y)
?
3
?p
lex
(y|x)
?
4
? exp(1)
?
5 (2)
where p(?) represent the phrase translation proba-
bilities in both directions, p
lex
(?) denote the lexi-
cal translation probabilities in both directions, and
exp(1) is the phrase penalty.
Similarly, the reordering model is defined on the
merging rules as follows
P
R
(r
m
1..n
m
) =
n
m
?
i=1
P (r
m
i
) (3)
In the original BTGmodel (Wu, 1996), P (rm) was
actually a prior probability which can be set based
on the order preference of the language pairs. In
MEBTG (Xiong et al, 2006), however, the prob-
ability is calculated more sophisticatedly using a
MaxEnt-based classification model with boundary
words as its features.
4 Linguistically Annotated BTG Based
SMT
We extend the original BTG into the linguistically
annotated BTG by adding linguistic annotations
from source-side parse trees to both BTG lexical
rules and merging rules. Before we elaborate how
the LABTG extends the baseline, we introduce an-
notated BTG rules.
In the LABTG, both lexical rules and merging
rules are annotated with linguistic elements as fol-
lows
ar
l
: A
a
? x#a/y
and
ar
m
: A
a
? [A
a
l
l
, A
a
r
r
]|?A
a
l
l
, A
a
r
r
?
The annotation a comprises three linguistic ele-
ments from source-side syntactic parse tree: (1)
head word hw, (2) the part-of-speech (POS) tag
ht of head word and (3) syntactic label sl. In an-
notated lexical rules, the three elements are com-
bined together and then attached to x as an anno-
tation unit. In annotated merging rules, each node
involved in merging is annotated with these three
elements individually.
There are various ways to learn the annotated
rules from training data. The straight-forward way
is to first generate the best BTG tree for each sen-
tence pair using the way of (Wu, 1997), then an-
notate each BTG node with linguistic elements
by projecting source-side syntax tree to BTG tree,
and finally extract rules from these annotated BTG
trees. This way restricts learning space to only the
best BTG trees2, and leads to the loss of many use-
ful annotated rules.
Therefore, we use an alternative way to extract
the annotated rules as illustrated below. Firstly, we
run GIZA++ (Och and Ney, 2000) on the train-
ing corpus in both directions and then apply the
ogrow-diag-finalp refinement rule (Koehn et al,
2003) to obtain many-to-many word alignments.
Secondly, we extract bilingual phrases from the
word-aligned corpus, then annotate their source
sides with linguistic elements to obtain the an-
notated lexical rules.3 Finally, we learn reorder-
ing examples (Xiong et al, 2006), annotate their
two neighboring sub-phrases and whole phrases,
and then generalize them in the annotated merging
rules. Although this alternative way may also miss
reorderings due to word alignment errors, it is still
more flexible and robust than the straight-forward
one, and can learn more annotated BTG rules with-
out constructing BTG trees explicitly.
4.1 LABTG Annotation Algorithm
During the process of rule learning and decod-
ing, we need to annotate bilingual phrases or BTG
nodes generated by the decoder given a source
sentence together with its parse tree. Since both
phrases and BTG nodes can be projected to a span
on the source sentence, we run our annotation al-
gorithm on source-side spans and then assign an-
notation results to the corresponding phrases or
nodes. If the span is exactly covered by a single
subtree in the source-side parse tree, it is called
syntactic span, otherwise non-syntactic span.
One of the challenges in this annotation algorithm
is that BTG nodes (or phrases) are not always cov-
ering syntactic span, in other words, are not always
aligned to constituent nodes in the source-side tree.
To solve this problem, we use heuristic rules to
generate pseudo head word and composite label
which consists of syntactic labels of three relevant
constituents for the non-syntactic span.
The annotation algorithm is shown in Fig. 1.
For a syntactic span, the annotation is trivial. An-
notation elements directly come from the subtree
that exactly covers the span. For a non-syntactic
2Producing BTG forest for each sentence pair is very time-
consuming.
3This makes the number of extracted annotated lexical
rules proportional to that of bilingual phrases.
1011
1: Annotator (span s = ?i, j?, source-side parse tree t)
2: if s is a syntactic span then
3: Find the subtree c in t which exactly covers s
4: s.a := {c.hw, c.ht, c.sl}
5: else
6: Find the smallest subtree c? subsuming s in t
7: if c?.hw ? s then
8: s.a.hw := c?.hw and s.a.ht := c?.ht
9: else
10: Find the word w ? s which is nearest to c?.hw
11: s.a.hw := w and s.a.ht := w.t /*w.t is the POS
tag of w*/
12: end if
13: Find the left context node ln of s in c?
14: Find the right context node rn of s in c?
15: s.a.sl := ln.sl-c?.sl-rn.sl
16: end if
Figure 1: The LABTG Annotation Algorithm.
span, the process is much complicated. Firstly,
we need to locate the smallest subtree c? subsum-
ing the span (line 6). Secondly, we try to identify
the head word/tag of the span (line 7-12) by us-
ing c??s head word hw directly if it is within the
span. Otherwise, the word within the span which
is nearest to hw will be assigned as the head word
of the span. Finally, we determine the composite
label of the span (line 13-15), which is formulated
as L-C-R. L/R refers to the syntactic label of the
left/right context node of s which is a sub-node of
c
?
. There are different ways to define the context
node of a span in the source-side parse tree. It can
be the closest neighboring node or the boundary
node which is the highest leftmost/rightmost sub-
node of c? not overlapping the span. If there is no
such context node (the span s is exactly aligned to
the left/right boundary of c?), L/R will be set to
NULL. C is the label of c?. L, R and C together
define the external syntactic context of s.
Fig. 2 shows a syntactic parse tree for a Chinese
sentence, with head word annotated for each inter-
nal node.4 Some sample annotations are given in
Table 1. We also show different composite labels
for non-syntactic spans with different definitions
of their context nodes. sl
1
is obtained when the
boundary node is defined as the context node while
sl
2
is obtained when the closest neighboring node
is defined as the context node.
4.2 LABTG Model
To better model annotated rules, the LABTG con-
tributes two significant modifications to formula
(1). First is the annotated phrase translation model
4In this paper, we use phrase labels from the Penn Chinese
Treebank (Xue et al, 2005).
IP(??)
?
?
?
?
?
H
H
H
H
H
NP(??)
?
?
H
H
NP(??)
NR
??1
Tibet
NP(??)
?H
NN
??2
financial
NN
??3
work
VP(??)
?
?
?
?
?
H
H
H
H
H
VV
??4
gain
AS
?5
NP(??)
?
?
H
H
ADJP(??)
JJ
??6
remarkable
NP(??)
NN
?7?
achievement
Figure 2: A syntactic parse tree with head word
annotated for each internal node. The superscripts
of leaf nodes denote their surface positions from
left to right.
span hw ht sl
1
(boundary node) sl
2
(neighboring node)
?1, 2? ?? NN NULL-NP-NN NULL-NP-NN
?2, 3? ?? NN NP NP
?2, 4? ?? VV NP-IP-NP NP-IP-AS
?3, 4? ?? VV NP-IP-NP NN-IP-AS
Table 1: Annotation samples according to the tree
shown in Fig. 2. hw/ht represents head word/tag,
respectively. sl means the syntactic label.
with source side linguistically enhanced to replace
the standard phrase translation model, and second
is the additional MaxEnt-based reordering model
that uses linguistic annotations as features. The
LABTG model is formulated as follows
P (D) = P
T
a
(ar
l
1..n
l
) ? P
R
b
(r
m
1..n
m
)
?
R
b
?P
R
a
(ar
m
1..n
m
)
?
R
a
? P
L
(e)
?
L
? exp(|e|)
?
w (4)
Here P
T
a
is the annotated phrase translation
model, P
R
b
is the reordering model from MEBTG
using boundary words as features and P
R
a
is the
annotated reordering model using linguistic anno-
tations of nodes as features.
Annotated Phrase Translation Model The
annotated phrase translation model P
T
a
is sim-
ilar to formula (2) except that phrase transla-
tion probabilities on both directions are p(x#a|y)
and p(y|x#a) respectively, instead of p(x|y) and
p(y|x). By introducing annotations into the trans-
lation model, we integrate linguistic knowledge
into the statistical selection of target equivalents.
Annotated Reordering Model The annotated
reordering model P
R
a
is a MaxEnt-based classi-
fication model which uses linguistic elements of
each annotated node as its features. The model can
be formulated as
P
R
a
(ar
m
) = p
?
(o|A
a
, A
a
l
l
, A
a
r
r
)
1012
=exp(
?
i
?
i
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
))
?
o
exp(
?
i
?
i
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
))
where the functions h
i
? {0, 1} are reordering fea-
tures and ?
i
are weights of these features.
Each merging rule involves 3 nodes
(Aa, Aal
l
, A
a
r
r
) and each node has 3 linguistic
elements (hw, ht, sl). Therefore, the model has 9
features in total. Taking the left node Aal
l
as an
example, the model could use its head word w as
feature as follows
h
i
(o,A
a
, A
a
l
l
, A
a
r
r
) =
{
1, A
a
l
l
.hw = w, o = straight
0, otherwise
4.3 Training
To train the annotated translation model, firstly we
extract all annotated lexical rules from source-side
parsed, word-aligned training data. Then we es-
timate the annotated phrase translation probabili-
ties p(x#a|y) and p(y|x#a) using relative counts
from all collected annotated lexical rules. For ex-
ample, p(y|x#a) can be calculated as follows
p(y|x#a) =
count(x#a, y)
?
y
count(x#a, y)
One might think that linguistic annotations would
cause serious data sparseness problem and the
probabilities should be smoothed. However, ac-
cording to our statistics (described in the next sec-
tion), the differences in annotations for the same
source phrase x are not so diverse. So we take
a direct backoff strategy to map unseen annotated
lexical rules to their un-annotated versions on the
fly during decoding, which is detailed in the next
subsection.
To train the annotated reordering model, we
generate all annotated reordering examples, then
obtain features using linguistic elements of these
examples, and finally estimate feature weights
based on the maximum entropy principle.
4.4 Decoding
A CKY-style decoder with beam search is devel-
oped, similar to (Xiong et al, 2006). Each in-
put source sentence is firstly parsed to obtain its
syntactic tree. Then the CKY-style decoder tries
to generate the best annotated BTG tree using the
trained annotated lexical and merging rules. We
store all annotated lexical rules and their proba-
bilities in a standard phrase table ?, where source
phrases are augmented with annotations. During
the application of annotated lexical rules, we la-
bel each source phrase x with linguistic annota-
tion a through the annotation algorithm given the
source-side parse tree, and retrieve x#a from ?.
In the case of unseen combination x#a, we map
it to x and lookup x in the phrase table so that we
can use the un-annotated lexical rule A ? x/y.
We set p(y|x) = max
a
?
p(y|x#a
?
) and p(x|y) =
max
a
?
p(x#a
?
|y) where (x, a?, y) ? ?. When two
neighboring nodes are merged in a specific order,
the two reordering models, P
R
b
and P
R
a
, will eval-
uate this merging independently with individual
scores. The former uses boundary words as fea-
tures while the latter uses the linguistic elements
as features, annotated on the BTG nodes through
the annotation algorithm according to the source-
side parse tree.
5 Experiments and Analysis
In this section we conducted a number of ex-
periments to demonstrate the competitiveness of
the proposed LABTG based SMT when compared
with two baseline systems: Moses (Koehn et al,
2007), a state-of-the-art phrase-based system and
MEBTG (Xiong et al, 2006), a BTG based sys-
tem. We also investigated the impact of differ-
ent annotation schemes on the LABTG model and
studied the effect of annotated phrase translation
model and annotated reordering model on transla-
tion selection and phrase reordering respectively.
All experiments were carried out on the Chinese-
to-English translation task of the NISTMT-05 with
case-sensitive BLEU scores reported.
The systems were trained on the FBIS cor-
pus. We removed 15,250 sentences, for which
the Chinese parser (Xiong et al, 2005) failed to
produce syntactic parse trees. The parser was
trained on the Penn Chinese Treebank with a F1
score of 79.4%. From the remaining FBIS corpus
(224, 165 sentence pairs), we obtained 4.55M stan-
dard bilingual phrases (including 2.75M source
phrases) for the baseline systems and 4.65M an-
notated lexical rules (including 3.13M annotated
source phrases augmented with linguistic anno-
tations) for the LABTG system using the algo-
rithm mentioned above. These statistics reveal
that there are 1.14 (3.13M/2.75M) annotations per
source phrase, which means our annotation algo-
rithm does not increase the number of extracted
rules exponentially.
We extracted 2.8M reordering examples, from
1013
System BLEU
Moses 0.2386
MEBTG 0.2498
LABTG 0.2667
Table 2: LABTG vs. Moses and MEBTG.
which we generated 114.8K reordering features for
the reordering model P
R
b
(shared by both MEBTG
and LABTG systems) using the right boundary
words of phrases and 85K features for the anno-
tated reordering model P
R
a
(only included in the
LABTG system) using linguistic annotations. We
ran the MaxEnt toolkit (Zhang, 2004) to tune re-
ordering feature weights with iteration number be-
ing set to 100 and Gaussian prior to 1 to avoid over-
fitting.
We built our four-gram language model using
Xinhua section of the English Gigaword corpus
(181.1M words) with the SRILM toolkit (Stolcke,
2002). For the efficiency of minimum-error-rate
training (Och, 2003), we built our development set
(580 sentences) using sentences not exceeding 50
characters from the NIST MT-02 evaluation test
data.
5.1 LABTG vs. phrase-based SMT and
BTG-based SMT
We compared the LABTG system with two base-
line systems. The results are given in Table 2.
The LABTG outperforms Moses and MEBTG by
2.81 and 1.69 absolute BLEU points, respectively.
These significant improvements indicate that BTG
formal structures can be successfully extended
with linguistic knowledge extracted from syntac-
tic structures without losing the strength of phrase-
based method.
5.2 The Effect of Different Annotation
Schemes
A great amount of linguistic knowledge is con-
veyed through the syntactic label sl. To obtain
this label, we tag syntactic BTG node with single
label C from its corresponding constituent in the
source-side parse tree while annotate non-syntactic
BTG node with composite label formulated as L-
C-R. We conducted experiments to study the effect
of different annotation schemes on the LABTG
model by comparing three different annotation
schemes for non-syntactic BTG node: (1) using
single label C from its corresponding smallest sub-
tree c? (C), (2) constructing composite label using
Annotation scheme BLEU
C 0.2626
N-C-N 0.2591
B-C-B 0.2667
Annotating syntactic nodes with com-
posite label
0.2464
Table 3: Comparison of different annotation
schemes.
neighboring node as context node (N-C-N), and (3)
constructing composite label using boundary node
as context node (B-C-B). The results are shown in
Table 3.
On the one hand, linguistic annotation provides
additional information for LABTG, transferring
knowledge from source-side linguistic structures
to BTG formal structures. On the other hand, how-
ever, it is also a constraint on LABTG, guiding the
annotated translation model and reordering model
to the selection of target alernatives and reorder-
ing patterns, respectively. A tight constraint al-
ways means that annotations are too specific, al-
though they incorporate rich knowledge. Too spe-
cific annotations are more sensitive to parse errors,
and easier to make the model lose correct transla-
tions or use wrong reordering patterns. That is the
reason why the annotation scheme ?N-C-N? and
?Annotating syntactic nodes with composite label?
5 both hurt the performance. Conversely, a loose
constraint means that annotations are too generic
and have less knowledge incorporated. The an-
notation scheme ?C? is such a scheme with loose
constraint and less knowledge.
Therefore, an ideal annotation scheme should
not be too specific or too generic. The annota-
tion scheme ?B-C-B? achieves a reasonable bal-
ance between knowledge incorporation and con-
straint, which obtains the best performance. There-
fore we choose boundary node as context node for
label annotation of non-syntactic BTG nodes in ex-
periments described later.
5.3 The Effect of Annotated Translation
Model
To investigate the effect of the annotated transla-
tion model on translation selection, we compared
the standard phrase translation model P
T
used
in MEBTG with the annotated phrase translation
5In this annotation scheme, we produce composite label
L-C-R for both syntactic and non-syntactic BTG nodes. For
syntactic node, sibling node is used as context node while for
non-syntactic node, boundary node is used as context node.
1014
Translation model BLEU
P
T
0.2498
P
T
a
0.2581
P
T
a
(-NULL) 0.2548
Table 4: The effect of annotated translation model.
model P
T
a
. The experiment results are shown in
Table 4. The significant improvement in the BLEU
score indicates that the annotated translation model
helps to select better translation options.
Our study on translation output shows that anno-
tating phrases with source-side linguistic elements
can provide at least two kinds of information for
translation model to improve the adequacy: cate-
gory and context. The category knowledge of a
phrase can be used to select its appropriate trans-
lation related to its category. For example, Chi-
nese phrase ??? can be translated into ?value? if
it is a verb or ?at/on? if it is a proposition. How-
ever, the baseline BTG-based system always se-
lects the proposition translation even if it is a verb
because the language model probability for propo-
sition translation is higher than that of verb trans-
lation. This wrong translation of content words is
similar to the incorrect omission reported in (Och
et al, 2003), which both hurt translation adequacy.
The annotated translation model can avoid wrong
translation by filtering out phrase candidates with
unmatched categories.
The context information (provided by context
node) is also quite useful for translation selection.
Even the ?NULL? context, which we used in label
annotation to indicate that a phrase is located at the
boundary of a constituent, provides some informa-
tion, such as, transitive or intransitive attribute of
a verb phrase. The last row of Tabel 4 shows that
if we remove ?NULL? in label annotation, the per-
formance is degraded. (Huang and Knight, 2006)
also reported similar result by using sisterhood an-
notation on the target side.
5.4 The Effect of Annotated Reordering
Model
To investigate the effect of the annotated reorder-
ing model, we integrate P
R
a
with various settings
in MEBTG while keeping its original phrase trans-
lation model P
T
and reordering model P
R
b
un-
changed. We augment P
R
a
?s feature pool incre-
mentally: firstly using only single labels 6(SL)
6For non-syntactic node, we only use the single label C,
without constructing composite label L-C-R.
Reordering Configuration BLEU
P
R
b
0.2498
P
R
b
+ P
R
a
(SL) 0.2588
P
R
b
+ P
R
a
(+BNL) 0.2627
P
R
b
+ P
R
a
(+BNL+HWT) 0.2652
P
R
b
+ P
R
a
(SL+BNL+HWT): only al-
lowed syntactic phrase reordering
0.2512
Table 5: The effect of annotated reordering model.
as features (132 features in total), then construct-
ing composite labels for non-syntactic phrases
(+BNL) (6.7K features), and finally introducing
head words into the feature pool (+BNL+HWT)
(85K features). This series of experiments demon-
strate the impact and degree of contribution made
by each feature for reordering. We also conducted
experiments to investigate the effect of restrict-
ing reordering to syntactic phrases using the best
reordering feature set (SL+BNL+HWT) for P
R
a
.
The experimental results are presented in Table 2,
from which we have the following observations:
(1) Source-side syntactic labels (SL) capture re-
ordering patterns between source structures and
their target counterparts. Even when the base-
line feature set SL with only 132 features is used
for P
R
a
, the BLEU score improves from 0.2498
to 0.2588. This is because most of the frequent
reordering patterns between Chinese and English
have been captured using syntactic labels. For ex-
ample, the pre-verbal modifier PP in Chinese is
translated into post-verbal counterpart in English.
This reordering can be described by a rule with an
inverted order: V P ? ?PP, V P ?, and captured
by our syntactic reordering features.
(2) Context information, provided by labels of
context nodes (BNL) and head word/tag pairs
(HWT), also improves phrase reordering. Produc-
ing composite labels for non-syntactic BTG nodes
(+BNL) and integrating head word/tag pairs into
P
R
a
as reordering features (+BNL+HWT) are both
effective, indicating that context information com-
plements syntactic label for capturing reordering
patterns.
(3) Restricting phrase reordering to syntactic
phrases is harmful. The BLEU score plummets
from 0.2652 to 0.2512.
6 Conclusions
In this paper, we have presented a Linguistically
Annotated BTG based approach to effectively in-
tegrate linguistic knowledge into SMT by merging
1015
source-side linguistic structures with BTG hierar-
chical structures. The LABTG brings BTG-based
SMT towards linguistically syntax-based SMT and
narrows the linguistic gap between them. Our
experimental results show that the LABTG sig-
nificantly outperforms the state-of-the-art phrase-
based SMT and the baseline BTG-based SMT. The
proposed method also offers better translation se-
lection and phrase reordering by introducing the
annotated phrase translation model and the anno-
tated reordering model with linguistic annotations.
We conclude that (1) source-side syntactic in-
formation can improve translation adequacy; (2)
linguistic annotations of BTG nodes well capture
reordering patterns between source structures and
their target counterparts; (3) integration of linguis-
tic knowledge into SMT should be carefully con-
ducted so that the incorporated knowledge could
not have negative constraints on the model7.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of ACL
2005.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency Inser-
tion Grammars. In Proceedings of ACL 2005.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of ACL 2003.
Hany Hassan, Khalil Sima?an and Andy Way. 2007. Su-
pertagged Phrase-based Statistical Machine Translation.
In Proceedings of ACL 2007.
Bryant Huang, Kevi Knight. 2006. Relabeling Syntax Trees
to Improve Syntax-Based Machine Translation Quality. In
Proceedings of NAACL-HLT 2006.
Liang Huang, Kevi Knight and Aravind Joshi. 2006. Statisti-
cal Syntax-directed Translation with Extended Domain of
Locality. In Proceedings of AMTA 2006.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings of
HLT/NAACL.
Philipp Koehn, Hieu Hoang. 2007. Factored Translation
Models. In Proceedings of EMNLP 2007.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris
Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst.
2007. Moses: Open Source Toolkit for Statistical Machine
Translation. ACL 2007, demonstration session, Prague,
Czech Republic, June 2007.
7For example, the annotation scheme ?N-C-N? incorpo-
rates rich syntactic knowledge, but also tightens the constraint
on the model, which therefore loses robustness.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Translation.
In Proceedings of ACL-COLING 2006.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phraases. In Proceed-
ings of EMNLP.
Franz Josef Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL 2000.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of ACL
2003.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,
Dragomir Radev. 2003. Final Report of Johns Hopkins
2003 SummerWorkshop on Syntax for Statistical Machine
Translation.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. Depen-
dency Treelet Translation: Syntactically Informed Phrasal
SMT. In Proceedings of ACL 2005.
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. Or-
dering Phrases with Function Words. In Proceedings of
ACL 2007.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of International Con-
ference on Spoken Language Processing, volume 2, pages
901-904.
Dekai Wu. 1996. A Polynomial-Time Algorithm for Statisti-
cal Machine Translation. In Proceedings of ACL 1996.
Dekai Wu. 1997. Stochastic Inversion Transduction Gram-
mars and Bilingual Parsing of Parallel Corpora. Computa-
tional Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Is-
land, Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum En-
tropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of ACL-COLING 2006.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: Phrase Structure An-
notation of a Large Corpus. Natural Language Engineer-
ing, 11(2):207-238.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007.
Phrase Reordering Model Integrating Syntactic Knowl-
edge for SMT. In Proceedings of EMNLP-CoNLL 2007.
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexicalized
Inversion Transduction Grammar for Alignment. In Pro-
ceedings of ACL 2005.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
Augmented Machine Translation via Chart Parsing. In
NAACL 2006 - Workshop on statistical machine transla-
tion, New York. June 4-9.
1016
Refinements in BTG-based Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sg
Haitao Mi, Qun Liu and Shouxun Lin
Key Lab of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing China, 100080
{htmi, liuqun, sxlin}@ict.ac.cn
Abstract
Bracketing Transduction Grammar (BTG)
has been well studied and used in statistical
machine translation (SMT) with promising
results. However, there are two major issues
for BTG-based SMT. First, there is no effec-
tive mechanism available for predicting or-
ders between neighboring blocks in the orig-
inal BTG. Second, the computational cost is
high. In this paper, we introduce two re-
finements for BTG-based SMT to achieve
better reordering and higher-speed decod-
ing, which include (1) reordering heuristics
to prevent incorrect swapping and reduce
search space, and (2) special phrases with
tags to indicate sentence beginning and end-
ing. The two refinements are integrated into
a well-established BTG-based Chinese-to-
English SMT system that is trained on large-
scale parallel data. Experimental results on
the NIST MT-05 task show that the proposed
refinements contribute significant improve-
ment of 2% in BLEU score over the baseline
system.
1 Introduction
Bracket transduction grammar was proposed by Wu
(1995) and firstly employed in statistical machine
translation in (Wu, 1996). Because of its good trade-
off between efficiency and expressiveness, BTG re-
striction is widely used for reordering in SMT (Zens
et al, 2004). However, BTG restriction does not
provide a mechanism to predict final orders between
two neighboring blocks.
To solve this problem, Xiong et al (2006)
proposed an enhanced BTG with a maximum en-
tropy (MaxEnt) based reordering model (MEBTG).
MEBTG uses boundary words of bilingual phrases
as features to predict their orders. Xiong et
al. (2006) reported significant performance im-
provement on Chinese-English translation tasks in
two different domains when compared with both
Pharaoh (Koehn, 2004) and the original BTG us-
ing flat reordering. However, error analysis of the
translation output of Xiong et al (2006) reveals
that boundary words predict wrong swapping, espe-
cially for long phrases although the MaxEnt-based
reordering model shows better performance than
baseline reordering models.
Another big problem with BTG-based SMT is the
high computational cost. Huang et al (2005) re-
ported that the time complexity of BTG decoding
with m-gram language model is O(n3+4(m?1)). If a
4-gram language model is used (common in many
current SMT systems), the time complexity is as
high as O(n15). Therefore with this time complexity
translating long sentences is time-consuming even
with highly stringent pruning strategy.
To speed up BTG decoding, Huang et al (2005)
adapted the hook trick which changes the time
complexity from O(n3+4(m?1)) to O(n3+3(m?1)).
However, the implementation of the hook trick with
pruning is quite complicated. Another method to in-
crease decoding speed is cube pruning proposed by
Chiang (2007) which reduces search space signifi-
cantly.
In this paper, we propose two refinements to ad-
dress the two issues, including (1) reordering heuris-
505
tics to prevent incorrect swapping and reduce search
space using swapping window and punctuation re-
striction, and (2) phrases with special tags to indicate
beginning and ending of sentence. Experimental re-
sults show that both refinements improve the BLEU
score significantly on large-scale data.
The above refinements can be easily implemented
and integrated into a baseline BTG-based SMT sys-
tem. However, they are not specially designed for
BTG-based SMT and can also be easily integrated
into other systems with different underlying trans-
lation strategies, such as the state-of-the-art phrase-
based system (Koehn et al, 2007), syntax-based sys-
tems (Chiang et al, 2005; Marcu et al, 2006; Liu et
al., 2006).
The rest of the paper is organized as follows. In
section 2, we review briefly the core elements of
the baseline system. In section 3 we describe our
proposed refinements in detail. Section 4 presents
the evaluation results on Chinese-to-English trans-
lation based on these refinements as well as results
obtained in the NIST MT-06 evaluation exercise. Fi-
nally, we conclude our work in section 5.
2 The Baseline System
In this paper, we use Xiong et al (2006)?s sys-
tem Bruin as our baseline system. Their system has
three essential elements which are (1) a stochastic
BTG, whose rules are weighted using different fea-
tures in log-linear form, (2) a MaxEnt-based reorder-
ing model with features automatically learned from
bilingual training data, (3) a CKY-style decoder us-
ing beam search similar to that of Wu (1996). We
describe the first two components briefly below.
2.1 Model
The translation process is modeled using BTG rules
which are listed as follows
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
The lexical rule (3) is used to translate source phrase
x into target phrase y and generate a block A. The
two rules (1) and (2) are used to merge two consec-
utive blocks into a single larger block in a straight or
inverted order.
To construct a stochastic BTG, we calculate rule
probabilities using the log-linear model (Och and
Ney, 2002). For the two merging rules (1) and (2),
the assigned probability Prm(A) is defined as fol-
lows
Prm(A) = ??? ? 4?LMpLM (A1,A2) (4)
where ?, the reordering score of block A1 and
A2, is calculated using the MaxEnt-based reordering
model (Xiong et al, 2006) described in the next sec-
tion, ?? is the weight of ?, and 4pLM (A1,A2) is the
increment of language model score of the two blocks
according to their final order, ?LM is its weight.
For the lexical rule (3), it is applied with a proba-
bility Prl(A)
Prl(A) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3
?plex(y|x)?4 ? exp(1)?5 ? exp(|y|)?6
?p?LMLM (y) (5)
where p(?) are the phrase translation probabilities
in both directions, plex(?) are the lexical translation
probabilities in both directions, exp(1) and exp(|y|)
are the phrase penalty and word penalty, respec-
tively and ?s are weights of features. These features
are commonly used in the state-of-the-art systems
(Koehn et al, 2005; Chiang et al, 2005).
2.2 MaxEnt-based Reordering Model
The MaxEnt-based reordering model is defined on
two consecutive blocks A1 and A2 together with
their order o ? {straight, inverted} according to
the maximum entropy framework.
? = p?(o|A1, A2) = exp(
?
i ?ihi(o,A1, A2))?
o exp(
?
i ?ihi(o,A1, A2))(6)
where the functions hi ? {0, 1} are model features
and ?i are weights of the model features trained au-
tomatically (Malouf, 2002).
There are three steps to train a MaxEnt-based re-
ordering model. First, we need to extract reordering
examples from unannotated bilingual data, then gen-
erate features from these examples and finally esti-
mate feature weights.
506
For extracting reordering examples, there are two
points worth mentioning:
1. In the extraction of useful reordering examples,
there is no length limitation over blocks com-
pared with extracting bilingual phrases.
2. When enumerating all combinations of neigh-
boring blocks, a good way to keep the number
of reordering examples acceptable is to extract
smallest blocks with the straight order while
largest blocks with the inverted order .
3 Refinements
In this section we describe two refinements men-
tioned above in detail. First, we present fine-
grained reordering heuristics using swapping win-
dow and punctuation restriction. Secondly, we inte-
grate special bilingual phrases with sentence begin-
ning/ending tags.
3.1 Reordering Heuristics
We conduct error analysis of the translation out-
put of the baseline system and observe that Bruin
sometimes incorrectly swaps two large neighboring
blocks on the target side. This happens frequently
when inverted order successfully challenges straight
order by the incorrect but strong support from the
language model and the MaxEnt-based reordering
model. The reason is that only boundary words
are used as evidences by both language model and
MaxEnt-based reordering model when the decoder
selects which merging rule (straight or inverted) to
be used 1. However, statistics show that bound-
ary words are not reliable for predicting the right
order between two larger neighboring blocks. Al-
Onaizan and Papineni (2006) also proved that lan-
guage model is insufficient to address long-distance
word reordering. If a wrong inverted order is se-
lected for two large consecutive blocks, incorrect
long-distance swapping happens.
Yet another finding is that many incorrect swap-
pings are related to punctuation marks. First, the
source sequence within a pair of balanced punctua-
tion marks (quotes and parentheses) should be kept
1In (Xiong et al, 2006), the language model uses the left-
most/rightmost words on the target side as evidences while the
MaxEnt-based reordering model uses the boundary words on
both sides.
Chinese: ?? : ??????????
?????????????????
????
Bruin: urgent action , he said : ?This is a very
serious situation , we can only hope that there
will be a possibility .?
Bruin+RH: he said : ?This is a very serious sit-
uation , we can only hope that there will be the
possibility to expedite action .?
Ref: He said: ?This is a very serious situa-
tion. We can only hope that it is possible to
speed up the operation.?
Figure 1: An example of incorrect long-distance
swap. The underlined Chinese words are incorrectly
swapped to the beginning of the sentence by the
original Bruin. RH means reordering heuristics.
within the punctuation after translation. However,
it is not always true when reordering is involved.
Sometime the punctuation marks are distorted with
the enclosed words sequences being moved out.
Secondly, it is found that a series of words is fre-
quently reordered from one side of a structural mark,
such as commas, semi-colons and colons, to the
other side of the mark for long sentences contain-
ing such marks. Generally speaking, on Chinese-
to-English translation, source words are translated
monotonously relative to their adjacent punctuation
marks, which means their order relative to punctua-
tion marks will not be changed. In summary, punctu-
ation marks place a strong constraint on word order
around them.
For example, in Figure 1, Chinese words ???
??? are reordered to sentence beginning. That is
an incorrect long-distance swapping, which makes
the reordered words moved out from the balanced
punctuation marks ??? and ???, and incorrectly
precede their previous mark ???.
These incorrect swappings definitely jeopardize
the quality of translation. Here we propose two
straightforward but effective heuristics to control
and adjust the reordering, namely swapping window
and punctuation restriction.
Swapping Window (SW): It constrains block
swapping in the following way
ACTIVATE A ? ?A1, A2? IF |A1s|+ |A2s| < sws
507
where |Ais| denotes the number of words on the
source side Ais of block Ai, sws is a pre-defined
swapping window size. Any inverted reordering be-
yond the pre-defined swapping window size is pro-
hibited.
Punctuation Restriction (PR): If two neighbor-
ing blocks include any of the punctuation marks p ?
{? ? ? ? ? ? ? ? ? ? ? ?}, the two
blocks will be merged with straight order.
Punctuation marks were already used in pars-
ing (Christine Doran, 2000) and statistical machine
translation (Och et al, 2003). In (Och et al,
2003), three kinds of features are defined, all re-
lated to punctuation marks like quotes, parentheses
and commas. Unfortunately, no statistically signifi-
cant improvement on the BLEU score was reported
in (Och et al, 2003). In this paper, we consider
this problem from a different perspective. We em-
phasize that words around punctuation marks are
reordered ungrammatically and therefore we posi-
tively use punctuation marks as a hard decision to
restrict such reordering around punctuations. This
is straightforward but yet results in significant im-
provement on translation quality.
The two heuristics described above can be used
together. If the following conditions are satisfied,
we can activate the inverted rule:
|A1s|+ |A2s| < sws && P
?
(A1s
?
A2s) = ?
where P is the set of punctuation marks mentioned
above.
The two heuristics can also speed up decoding be-
cause decoding will be monotone within those spans
which are not in accordance with both heuristics.
For a sentence with n words, the total number of
spans is O(n2). If we set sws = m (m < n),
then the number of spans with monotone search is
O((n?m)2). With punctuation restriction, the non-
monotone search space will reduce further.
3.2 Phrases with Sentence Beginning/Ending
Tags
We observe that in a sentence some phrases are more
likely to be located at the beginning, while other
phrases are more likely to be at the end. This kind of
location information with regard to the phrase posi-
tion could be used for reordering. A straightforward
way to use this information is to mark the begin-
ning and ending of word-aligned sentences with ?s?
and ?/s? respectively. This idea is borrowed from
language modeling (Stolcke, 2002). The corre-
sponding tags at the source and target sentences are
aligned to each other, i.e, the beginning tag of source
sentences is aligned to the beginning tag of target
sentences, similarly for the ending tag. Figure 2
shows a word-aligned sentence pair annotated with
the sentence beginning and ending tag.
During training, the sentence beginning and end-
ing tags (?s? and ?/s?) are treated as words. There-
fore the phrase extraction and MaxEnt-based re-
ordering training algorithm need not to be modified.
Phrases with the sentence beginning/ending tag will
be extracted and MaxEnt-based reordering features
with such tags will also be generated. For example,
from the word-aligned sentence pair in Figure 2, we
can extract tagged phrases like
?s??? ||| ?s? Tibet ?s
?? ?/s? ||| achievements ?/s?
and generate MaxEnt-based reordering features with
tags like
hi(o, b1, b2) =
{ 1, b2.t1 = ?/s?, o = s
0, otherwise
where b1, b2 are blocks, t1 denotes the last source
word, o = s means the order between two blocks
is straight. To avoid wrong alignments, we remove
tagged phrases where only the beginning/ending tag
is extracted on either side of the phrases, such as
?s? ||| ?s? Those?
?/s? ||| ?/s?
During decoding, we first annotate source sen-
tences with the beiginning/ending tags, then trans-
late them as what Bruin does. Note that phrases
with sentence beginning/ending tags will be used in
the same way as ordinary phrases without such tags
during decoding. With the additional support of lan-
guage model and MaxEnt-based reordering model,
we observe that phrases with such tags are always
moved to the beginning or ending of sentences cor-
rectly.
508
?s? ?? ?? ?? ?? ?? ?? ?/s?
?s? Tibet ?s financial work has gained remarkable achievements ?/s?
Figure 2: A word-aligned sentence pair annotated with the sentence beginning and ending tag.
4 Evaluation
In this section, we report the performance of the en-
hanced Bruin on the NIST MT-05 and NIST MT-06
Chinese-to-English translation tasks. We describe
the corpus, model training, and experiments related
to the refinements described above.
4.1 Corpus
The bilingual training data is derived from the fol-
lowing various sources: the FBIS (LDC2003E14),
Hong Kong Parallel Text (Hong Kong News and
Hong Kong Hansards, LDC2004T08), Xinhua News
(LDC2002E18), Chinese News Translation Text
Part1 (LDC2005T06), Translations from the Chi-
nese Treebank (LDC2003E07), Chinese English
News Magazine (LDC2005E47). It contains 2.4M
sentence pairs in total (68.1M Chinese words and
73.8M English words).
For the efficiency of minimum-error-rate training,
we built our development set using sentences not ex-
ceeding 50 characters from the NIST MT-02 evalu-
ation test data (580 sentences).
4.2 Training
We use exactly the same way and configuration de-
scribed in (He et al, 2006) to preprocess the training
data, align words and extract phrases.
We built two four-gram language models using
Xinhua section of the English Gigaword corpus
(181.1M words) and the English side of the bilin-
gual training data described above respectively. We
applied modified Kneser-Ney smoothing as imple-
mented in the SRILM toolkit (Stolcke, 2002).
The MaxEnt-based reordering model is trained
using the way of (Xiong et al, 2006). The difference
is that we only use lexical features generated by tail
words of blocks, instead of head words, removing
features generated by the combination of two bound-
ary words.
Bleu(%) Secs/sent
Bruin 29.96 54.3
sws RH1 RH12 RH1 RH12
5 29.65 29.95 42.6 41.2
10 30.55 31.27 46.2 41.8
15 30.26 31.40 48.0 42.2
20 30.19 31.42 49.1 43.2
Table 1: Effect of reordering heuristics. RH1 de-
notes swapping window while RH12 denotes swap-
ping window with the addition of punctuation re-
striction.
4.3 Translation Results
Table 1 compares the BLEU scores 2 and the speed
in seconds/sentence of the baseline system Bruin
and the enhanced system with reordering heuristics
applied. The second row gives the BLEU score and
the average decoding time of Bruin. The rows be-
low row 3 show the BLEU scores and speed of the
enhanced Bruin with different combinations of re-
ordering heuristics. We can clearly see that the re-
ordering heuristics proposed by us have a two-fold
effect on the performance: improving the BLEU
score and decreasing the average decoding time.
The example in Figure 1 shows how reordering
heuristics prevent incorrect long-distance swapping
which is not in accordance with the punctuation re-
striction.
Table 1 also shows that a 15-word swapping win-
dow is an inflexion point with the best tradeoff be-
tween the decoding time and the BLEU score. We
speculate that in our corpus most reorderings hap-
pen within a 15-word window. We use the FBIS
corpus to testify this hypothesis. In this corpus, we
extract all reordering examples using the algorithm
of Xiong et al (2006). Figure 3 shows the reorder-
ing length distribution curve in this corpus. Accord-
2In this paper, all BLEU scores are case-sensitive and evalu-
ated on the NIST MT-05 Chinese-to-English translation task if
there is no special note.
509
0 10 20 30 40 50 60 70 80
0
5
10
15
20
25
Pe
rce
nt 
(%
)
Reordering Length
Figure 3: Reordering length distribution. The hor-
izontal axis (reordering length) indicates the num-
ber of words on the source side of two neighboring
blocks which are to be swapped. The vertical axis
represents what proportion of reorderings with a cer-
tain length is likely to be in all reordering examples
with an inverted order.
Bleu(%)
Without Special Phrases 31.40
With Special Phrases 32.01
Table 2: Effect of integrating special phrases with
the sentence beginning/ending tag.
ing to our statistics, reorderings within a window
not exceeding 15 words have a very high proportion,
97.29%. Therefore we set sws = 15 for later exper-
iments.
Table 2 shows the effect of integrating special
phrases with sentence beginning/ending tags into
Bruin. As special phrases accounts for only 1.95%
of the total phrases used, an improvement of 0.6%
in BLEU score is well worthwhile. Further, the im-
provement is statistically significant at the 99% con-
fidence level according to Zhang?s significant tester
(Zhang et al, 2004). Figure 4 shows several exam-
ples translated with special phrases integrated. We
can see that phrases with sentence beginning/ending
tags are correctly selected and located at the right
place.
Table 3 shows the performance of two systems on
the NIST MT-05 Chinese test data, which are (1)
System Refine MT-05 MT-06
Bruin - 29.96 -
EBruin RH 31.40 30.22
EBruin RH+SP 32.01 -
Table 3: Results of different systems. The refine-
ments RH, SP represent reordering heuristics and
special phrases with the sentence beginning/ending
tag, respectively.
Bruin, trained on the large data described above; and
(2) enhanced Bruin (EBruin) with different refine-
ments trained on the same data set. This table also
shows the evaluation result of the enhanced Bruin
with reordering heuristics, obtained in the NIST MT-
06 evaluation exercise. 3
5 Conclusions
We have described in detail two refinements for
BTG-based SMT which include reordering heuris-
tics and special phrases with tags. The refinements
were integrated into a well-established BTG-based
system Bruin introduced by Xiong et al (2006). Re-
ordering heuristics proposed here achieve a twofold
improvement: better reordering and higher-speed
decoding. To our best knowledge, we are the first
to integrate special phrases with the sentence be-
ginning/ending tag into SMT. Experimental results
show that the above refinements improve the base-
line system significantly.
For further improvements, we will investigate
possible extensions to the BTG grammars, e.g.
learning useful nonterminals using unsupervised
learning algorithm.
Acknowledgements
We would like to thank the anonymous review-
ers for useful comments on the earlier version of
this paper. The first author was partially sup-
ported by the National Science Foundations of
China (No. 60573188) and the High Technology
Research and Development Program of China (No.
2006AA010108) while he studied in the Institute of
Computing Technology, Chinese Academy of Sci-
ences.
3Full results are available at http://www.nist.gov/
speech/tests/mt/doc/mt06eval official results.html.
510
With Special Phrases Without Special Phrases
?s? Japan had already pledged to provide 30 mil-
lion US dollars of aid due to the tsunami victims of
the country . ?/s?
originally has pledged to provide 30 million US
dollars of aid from Japan tsunami victimized coun-
tries .
?s? the results of the survey is based on the re-
sults of the chiefs of the Ukrainian National 50.96%
cast by chiefs . ?/s?
is based on the survey findings Ukraine 50.96% cast
by the chiefs of the chiefs of the country .
?s? and at the same time , the focus of the world have
been transferred to other areas . ?/s?
and at the same time , the global focus has shifted
he.
Figure 4: Examples translated with special phrases integrated. The bold underlined words are special phrases
with the sentence beginning/ending tag.
References
Yaser Al-Onaizan, Kishore Papineni. 2006. Distortion
Models for Statistical Machine Translation. In Pro-
ceedings of ACL-COLING 2006.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, Michael Subotin. 2005. The
Hiero Machine Translation System: Extensions, Eval-
uation, and Analysis. In Proceedings of HLT/EMNLP,
pages 779?786, Vancouver, October 2005.
David Chiang. 2007. Hierarchical Phrase-based Transla-
tion. In computational linguistics, 33(2).
Christine Doran. 2000. Punctuation in a Lexicalized
Grammar. In Proceedings of Workshop TAG+5, Paris.
Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, Qun
Liu. 2006. ICT System Description for the 2006
TC-STAR Run #2 SLT Evaluation. In Proceedings of
TC-STAR Workshop on Speech-to-Speech Translation,
Barcelona, Spain.
Liang Huang, Hao Zhang and Daniel Gildea. 2005. Ma-
chine Translation as Lexicalized Parsing with Hooks.
In Proceedings of the 9th International Workshop
on Parsing Technologies (IWPT-05), Vancouver, BC,
Canada, October 2005.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL
2007, demonstration session, Prague, Czech Republic,
June 2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of ACL-COLING 2006.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of CoNLL-2002.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phraases. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, Dragomir Radev. 2003. Final
Report of Johns Hopkins 2003 Summer Workshop on
Syntax for Statistical Machine Translation.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of IJCAL 1995, pages 1328-1334, Montreal, August.
511
Dekai Wu. 1996. A Polynomial-Time Algorithm for Sta-
tistical Machine Translation. In Proceedings of ACL
1996.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In Proceedings of ACL-
COLING 2006, pages 521?528.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Ma-
chine Translation. In Proceedings of CoLing 2004,
Geneva, Switzerland, pp. 205-211.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC 2004, pages 2051? 2054.
512
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 149?152,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Linguistically Annotated Reordering Model
for BTG-based Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{dyxiong, mzhang, aaiti, hli}@i2r.a-star.edu.sg
Abstract
In this paper, we propose a linguistically anno-
tated reordering model for BTG-based statis-
tical machine translation. The model incorpo-
rates linguistic knowledge to predict orders for
both syntactic and non-syntactic phrases. The
linguistic knowledge is automatically learned
from source-side parse trees through an an-
notation algorithm. We empirically demon-
strate that the proposed model leads to a sig-
nificant improvement of 1.55% in the BLEU
score over the baseline reordering model on
the NIST MT-05 Chinese-to-English transla-
tion task.
1 Introduction
In recent years, Bracketing Transduction Grammar
(BTG) proposed by (Wu, 1997) has been widely
used in statistical machine translation (SMT). How-
ever, the original BTG does not provide an effec-
tive mechanism to predict the most appropriate or-
ders between two neighboring phrases. To address
this problem, Xiong et al (2006) enhance the BTG
with a maximum entropy (MaxEnt) based reorder-
ing model which uses boundary words of bilingual
phrases as features. Although this model outper-
forms previous unlexicalized models, it does not uti-
lize any linguistically syntactic features, which have
proven useful for phrase reordering (Wang et al,
2007). Zhang et al (2007) integrates source-side
syntactic knowledge into a phrase reordering model
based on BTG-style rules. However, one limita-
tion of this method is that it only reorders syntac-
tic phrases because linguistic knowledge from parse
trees is only carried by syntactic phrases as far as re-
ordering is concerned, while non-syntactic phrases
are combined monotonously with a flat reordering
score.
In this paper, we propose a linguistically anno-
tated reordering model for BTG-based SMT, which
is a significant extension to the work mentioned
above. The new model annotates each BTG node
with linguistic knowledge by projecting source-side
parse trees onto the corresponding binary trees gen-
erated by BTG so that syntactic features can be used
for phrase reordering. Different from (Zhang et al,
2007), our annotation algorithm is able to label both
syntactic and non-syntactic phrases. This enables
our model to reorder any phrases, not limited to syn-
tactic phrases. In addition, other linguistic informa-
tion such as head words, is also used to improve re-
ordering.
The rest of the paper is organized as follows. Sec-
tion 2 briefly describes our baseline system while
Section 3 introduces the linguistically annotated re-
ordering model. Section 4 reports the experiments
on a Chinese-to-English translation task. We con-
clude in Section 5.
2 Baseline SMT System
The baseline system is a phrase-based system which
uses the BTG lexical rules (A ? x/y) to translate
source phrase x into target phrase y and the BTG
merging rules (A ? [A,A]|?A,A?) to combine two
neighboring phrases with a straight or inverted or-
der. The BTG lexical rules are weighted with several
features, such as phrase translation, word penalty
and language models, in a log-linear form. For the
merging rules, a MaxEnt-based reordering model
using boundary words of neighboring phrases as fea-
tures is used to predict the merging order, similar to
(Xiong et al, 2006). We call this reordering model
149
boundary words based reordering model (BWR). In
this paper, we propose to incorporate a linguistically
annotated reordering model into the log-linear trans-
lation model, so as to strengthen the BWR?s phrase
reordering ability. We train all the model scaling fac-
tors on the development set to maximize the BLEU
score. A CKY-style decoder is developed to gener-
ate the best BTG binary tree for each input sentence,
which yields the best translation.
3 Linguistically Annotated Reordering
Model
The linguistically annotated reordering
model (LAR) is a MaxEnt-based classifica-
tion model which predicts the phrase order
o ? {inverted, straight} during the application
of merging rules to combine their left and right
neighboring phrases Al and Ar into a larger phrase
A. 1 The model can be formulated as
LAR = exp(
?
i ?ihi(o,Al, Ar, A))?
o? exp(
?
i ?ihi(o?, Al, Ar, A))
(1)
where the functions hi ? {0, 1} are reordering fea-
tures and ?i are weights of these features. We define
the features as linguistic elements which are anno-
tated for each BTG node through an annotation al-
gorithm, which comprise (1) head word hw, (2) the
part-of-speech (POS) tag ht of head word and (3)
syntactic label sl.
Each merging rule involves 3 nodes (A,Al, Ar)
and each node has 3 linguistic elements (hw, ht, sl).
Therefore, the model has 9 features in total. Taking
the left node Al as an example, the model could use
its head word w as feature as follows
hi(o,A,Al, Ar) =
{ 1, Al.hw = w, o = straight
0, otherwise
3.1 Annotation Algorithm
There are two steps to annotate a phrase or a BTG
node using source-side parse tree information: (1)
determining the span on the source side which is
exactly covered by the node or the phrase, then
(2) annotating the span according to the source-side
parse tree. If the span is exactly covered by a sin-
gle subtree in the source-side parse tree, it is called
1Each phrase is also a node in the BTG tree generated by the
decoder.
1: Annotator (span s = ?i, j?, source-side parse tree t)
2: if s is a syntactic span then
3: Find the subtree c in t which exactly covers s
4: s.{ } := {c.hw, c.ht, c.sl}
5: else
6: Find the smallest subtree c? subsuming s in t
7: if c?.hw ? s then
8: s.hw := c?.hw and s.ht := c?.ht
9: else
10: Find the word w ? s which is nearest to c?.hw
11: s.hw := w and s.ht := w.t /*w.t is the POS
tag of w*/
12: end if
13: Find the left boundary node ln of s in c?
14: Find the right boundary node rn of s in c?
15: s.sl := ln.sl-c?.sl-rn.sl
16: end if
Figure 1: The Annotation Algorithm.
syntactic span, otherwise it is non-syntactic span.
One of the challenges in this annotation algorithm
is that phrases (BTG nodes) are not always cover-
ing syntactic span, in other words, they are not al-
ways aligned to all constituent nodes in the source-
side tree. To solve this problem, we use heuristic
rules to generate pseudo head word and composite
label which consists of syntactic labels of three rel-
evant constituents for the non-syntactic span. In this
way, our annotation algorithm is capable of labelling
both syntactic and non-syntactic phrases and there-
fore providing linguistic information for any phrase
reordering.
The annotation algorithm is shown in Fig. 1. For
a syntactic span, the annotation is trivial. Annotation
elements directly come from the subtree that covers
the span exactly. For a non-syntactic span, the pro-
cess is much complicated. Firstly, we need to locate
the smallest subtree c? subsuming the span (line 6).
Secondly, we try to identify the head word/tag of the
span (line 7-12) by using its head word directly if it
is within the span. Otherwise, the word within the
span which is nearest to hw will be assigned as the
head word of the span. Finally, we determine the
composite label of the span (line 13-15), which is
formulated as L-C-R. L/R means the syntactic label
of the left/right boundary node of s which is the
highest leftmost/rightmost sub-node of c? not over-
lapping the span. If there is no such boundary node
150
IP(??)
??
??
?
HH
HH
H
NP(??)
?? HH
NP(??)
NR
??1
Tibet
NP(??)
? H
NN
??2
financial
NN
??3
work
VP(??)
??
??
?
HH
HH
H
VV
??4
gain
AS
?5
NP(??)
?? HH
ADJP(??)
JJ
??6
remarkable
NP(??)
NN
??7
achievement
Figure 2: A syntactic parse tree with head word annotated
for each internal node. The superscripts of leaf nodes
denote their surface positions from left to right.
span hw ht sl
?1, 2? ?? NN NULL-NP-NN
?2, 3? ?? NN NP
?2, 4? ?? VV NP-IP-NP
?3, 4? ?? VV NP-IP-NP
Table 1: Annotation samples according to the tree shown
in Fig. 2. hw/ht represents the head word/tag, respec-
tively. sl means the syntactic label.
(the span s is exactly aligned to the left/right bound-
ary of c?), L/R will be set to NULL. C is the label of
c?. L, R and C together define the external syntactic
context of s.
Fig. 2 shows a syntactic parse tree for a Chinese
sentence, with head word annotated for each internal
node. Some sample annotations are given in Table 1.
3.2 Training and Decoding
Training an LAR model takes three steps. Firstly, we
extract annotated reordering examples from source-
side parsed, word-aligned bilingual data using the
annotation algorithm and the reordering example
extraction algorithm of (Xiong et al, 2006). We
then generate features using linguistic elements of
these examples and finally estimate feature weights.
This training process flexibly learns rich syntactic
reordering information without explicitly construct-
ing BTG tree or forest for each sentence pair.
During decoding, each input source sentence is
firstly parsed to obtain its syntactic tree. Then the
CKY-style decoder tries to generate the best BTG
tree using the lexical and merging rules. When two
neighboring nodes are merged in a specific order, the
two embedded reordering models, BWR and LAR,
evaluate this merging independently with individual
scores. The former uses boundary words as features
while the latter uses the linguistic elements as fea-
tures, annotated on the BTG nodes through the anno-
tation algorithm according to the source-side parse
tree.
4 Experiments
All experiments in this section were carried out on
the Chinese-to-English translation task of the NIST
MT-05. The baseline system and the new system
with the LAR model were trained on the FBIS cor-
pus. We removed 15,250 sentences, for which the
Chinese parser (Xiong et al, 2005) failed to pro-
duce syntactic parse trees. The parser was trained
on the Penn Chinese Treebank with a F1 score of
79.4%. The remaining FBIS corpus (224,165 sen-
tence pairs) was used to obtain standard bilingual
phrases for the systems.
We extracted 2.8M reordering examples from
these sentences. From these examples, we gener-
ated 114.8K reordering features for the BWR model
using the right boundary words of phrases and 85K
features for the LAR model using linguistic annota-
tions. We ran the MaxEnt toolkit (Zhang, 2004) to
tune reordering feature weights with iteration num-
ber being set to 100 and Gaussian prior to 1 to avoid
overfitting.
We built our four-gram language model using
Xinhua section of the English Gigaword corpus
(181.1M words) with the SRILM toolkit (Stol-
cke, 2002). For the efficiency of minimum-error-
rate training (Och, 2003), we built our development
set (580 sentences) using sentences not exceeding
50 characters from the NIST MT-02 evaluation test
data.
4.1 Results
We compared various reordering configurations in
the baseline system and new system. The base-
line system only has BWR as the reordering model,
while the new system employs two reordering mod-
els: BWR and LAR. For the linguistically anno-
tated reordering model LAR, we augment its feature
pool incrementally: firstly using only single labels
151
2(SL) as features (132 features in total), then con-
structing composite labels for non-syntactic phrases
(+BNL) (6.7K features), and finally introducing
head words and their POS tags into the feature pool
(+BNL+HWT) (85K features). This series of exper-
iments demonstrate the impact and degree of con-
tribution made by each feature for reordering. We
also conducted experiments to investigate the ef-
fect of restricting reordering to syntactic phrases in
the new system using the best reordering feature
set (SL+BNL+HWT) for LAR. The experimental
results (case-sensitive BLEU scores together with
confidence intervals) are presented in Table 2, from
which we have the following observations:
(1) The LAR model improves the performance
statistically significantly. Even we only use the base-
line feature set SL with only 132 features for the
LAR, the BLEU score improves from 0.2497 to
0.2588. This is because most of the frequent reorder-
ing patterns between Chinese and English have been
captured using syntactic labels. For example, the
pre-verbal modifier PP in Chinese is translated into
post-verbal counterpart in English. This reordering
can be described by a rule with an inverted order:
V P ? ?PP, V P ?, and captured by our syntactic
reordering features.
(2) Context information, provided by labels of
boundary nodes (BNL) and head word/tag pairs
(HWT), also improves phrase reordering. Produc-
ing composite labels for non-syntactic BTG nodes
(+BNL) and integrating head word/tag pairs into
the LAR as reordering features (+BNL+HWT) are
both effective, indicating that context information
complements syntactic label for capturing reorder-
ing patterns.
(3) Restricting phrase reordering to syntactic
phrases is harmful. The BLEU score plummets from
0.2652 to 0.2512.
5 Conclusion
In this paper, we have presented a linguistically an-
notated reordering model to effectively integrate lin-
guistic knowledge into phrase reordering by merg-
ing source-side parse trees with BTG binary trees.
Our experimental results show that, on the NIST
2For non-syntactic node, we only use the single label C,
without constructing composite label L-C-R.
Reordering Configuration BLEU (%)
BWR 24.97 ? 0.90
BWR + LAR (SL) 25.88 ? 0.95
BWR + LAR (+BNL) 26.27 ? 0.98
BWR + LAR (+BNL+HWT) 26.52 ? 0.96
Only allowed SPs reordering 25.12 ? 0.87
Table 2: The effect of the linguistically annotated reorder-
ing model. BWR denotes the boundary word based re-
ordering model while LAR denotes the linguistically an-
notated reordering model. (SL) is the baseline feature set,
(+BNL) and (+BNL+HWT) are extended feature sets for
the LAR. SP means syntactic phrase.
MT-05 task of Chinese-to-English translation, the
proposed reordering model leads to BLEU improve-
ment of 1.55%. We believe that our linguistically
annotated reordering model can be further improved
by using better annotation which transfers more
knowledge (morphological, syntactic or semantic)
to the model.
References
Franz Josef Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings of ACL 2003.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of International Conference on
Spoken Language Processing, volume 2, pages 901-904.
Chao Wang, Michael Collins and Philipp Koehn. 2007. Chi-
nese Syntactic Reordering for Statistical Machine Transla-
tion. In Proceedings of EMNLP-CoNLL 2007.
Dekai Wu. 1997. Stochastic Inversion Transduction Grammars
and Bilingual Parsing of Parallel Corpora. Computational
Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Island,
Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of ACL-COLING 2006.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007.
Phrase Reordering Model Integrating Syntactic Knowledge
for SMT. In Proceedings of EMNLP-CoNLL 2007.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
152
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 315?323,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Syntax-Driven Bracketing Model for Phrase-Based Translation
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 South Connexis, Singapore 138632
{dyxiong, mzhang, aaiti, hli}@i2r.a-star.edu.sg
Abstract
Syntactic analysis influences the way in
which the source sentence is translated.
Previous efforts add syntactic constraints
to phrase-based translation by directly
rewarding/punishing a hypothesis when-
ever it matches/violates source-side con-
stituents. We present a new model that
automatically learns syntactic constraints,
including but not limited to constituent
matching/violation, from training corpus.
The model brackets a source phrase as
to whether it satisfies the learnt syntac-
tic constraints. The bracketed phrases are
then translated as a whole unit by the de-
coder. Experimental results and analy-
sis show that the new model outperforms
other previous methods and achieves a
substantial improvement over the baseline
which is not syntactically informed.
1 Introduction
The phrase-based approach is widely adopted in
statistical machine translation (SMT). It segments
a source sentence into a sequence of phrases, then
translates and reorder these phrases in the target.
In such a process, original phrase-based decod-
ing (Koehn et al, 2003) does not take advan-
tage of any linguistic analysis, which, however,
is broadly used in rule-based approaches. Since
it is not linguistically motivated, original phrase-
based decoding might produce ungrammatical or
even wrong translations. Consider the following
Chinese fragment with its parse tree:
Src: [? [[7? 11?]NP [?? [? [?? ?]NP
]PP ]VP ]IP ]VP
Ref: established July 11 as Sailing Festival day
Output: [to/? [?[set up/?? [for/? naviga-
tion/??]] on July 11/7?11?? knots/?]]
The output is generated from a phrase-based sys-
tem which does not involve any syntactic analy-
sis. Here we use ?[]? (straight orientation) and
???? (inverted orientation) to denote the common
structure of the source fragment and its transla-
tion found by the decoder. We can observe that
the decoder inadequately breaks up the second NP
phrase and translates the two words ???? and
??? separately. However, the parse tree of the
source fragment constrains the phrase ??? ??
to be translated as a unit.
Without considering syntactic constraints from
the parse tree, the decoder makes wrong decisions
not only on phrase movement but also on the lex-
ical selection for the multi-meaning word ???1.
To avert such errors, the decoder can fully respect
linguistic structures by only allowing syntactic
constituent translations and reorderings. This, un-
fortunately, significantly jeopardizes performance
(Koehn et al, 2003; Xiong et al, 2008) because by
integrating syntactic constraint into decoding as a
hard constraint, it simply prohibits any other use-
ful non-syntactic translations which violate con-
stituent boundaries.
To better leverage syntactic constraint yet still
allow non-syntactic translations, Chiang (2005)
introduces a count for each hypothesis and ac-
cumulates it whenever the hypothesis exactly
matches syntactic boundaries on the source side.
On the contrary, Marton and Resnik (2008) and
Cherry (2008) accumulate a count whenever hy-
potheses violate constituent boundaries. These
constituent matching/violation counts are used as
a feature in the decoder?s log-linear model and
their weights are tuned via minimal error rate
training (MERT) (Och, 2003). In this way, syn-
tactic constraint is integrated into decoding as a
soft constraint to enable the decoder to reward hy-
potheses that respect syntactic analyses or to pe-
1This word can be translated into ?section?, ?festival?,
and ?knot? in different contexts.
315
nalize hypotheses that violate syntactic structures.
Although experiments show that this con-
stituent matching/violation counting feature
achieves significant improvements on various
language-pairs, one issue is that matching syn-
tactic analysis can not always guarantee a good
translation, and violating syntactic structure does
not always induce a bad translation. Marton and
Resnik (2008) find that some constituency types
favor matching the source parse while others
encourage violations. Therefore it is necessary to
integrate more syntactic constraints into phrase
translation, not just the constraint of constituent
matching/violation.
The other issue is that during decoding we are
more concerned with the question of phrase co-
hesion, i.e. whether the current phrase can be
translated as a unit or not within particular syntac-
tic contexts (Fox, 2002)2, than that of constituent
matching/violation. Phrase cohesion is one of
the main reasons that we introduce syntactic con-
straints (Cherry, 2008). If a source phrase remains
contiguous after translation, we refer this type of
phrase bracketable, otherwise unbracketable. It
is more desirable to translate a bracketable phrase
than an unbracketable one.
In this paper, we propose a syntax-driven brack-
eting (SDB) model to predict whether a phrase
(a sequence of contiguous words) is bracketable
or not using rich syntactic constraints. We parse
the source language sentences in the word-aligned
training corpus. According to the word align-
ments, we define bracketable and unbracketable
instances. For each of these instances, we auto-
matically extract relevant syntactic features from
the source parse tree as bracketing evidences.
Then we tune the weights of these features us-
ing a maximum entropy (ME) trainer. In this way,
we build two bracketing models: 1) a unary SDB
model (UniSDB) which predicts whether an inde-
pendent phrase is bracketable or not; and 2) a bi-
nary SDB model(BiSDB) which predicts whether
two neighboring phrases are bracketable. Similar
to previous methods, our SDB model is integrated
into the decoder?s log-linear model as a feature so
that we can inherit the idea of soft constraints.
In contrast to the constituent matching/violation
counting (CMVC) (Chiang, 2005; Marton and
Resnik, 2008; Cherry, 2008), our SDB model has
2Here we expand the definition of phrase to include both
syntactic and non-syntactic phrases.
the following advantages
? The SDB model automatically learns syntac-
tic constraints from training data while the
CMVC uses manually defined syntactic con-
straints: constituency matching/violation. In
our SDB model, each learned syntactic fea-
ture from bracketing instances can be consid-
ered as a syntactic constraint. Therefore we
can use thousands of syntactic constraints to
guide phrase translation.
? The SDB model maintains and protects the
strength of the phrase-based approach in a
better way than the CMVC does. It is able to
reward non-syntactic translations by assign-
ing an adequate probability to them if these
translations are appropriate to particular syn-
tactic contexts on the source side, rather than
always punish them.
We test our SDB model against the baseline
which doest not use any syntactic constraints on
Chinese-to-English translation. To compare with
the CMVC, we also conduct experiments using
(Marton and Resnik, 2008)?s XP+. The XP+ ac-
cumulates a count for each hypothesis whenever
it violates the boundaries of a constituent with a
label from {NP, VP, CP, IP, PP, ADVP, QP, LCP,
DNP}. The XP+ is the best feature among all fea-
tures that Marton and Resnik use for Chinese-to-
English translation. Our experimental results dis-
play that our SDB model achieves a substantial
improvement over the baseline and significantly
outperforms XP+ according to the BLEU metric
(Papineni et al, 2002). In addition, our analysis
shows further evidences of the performance gain
from a different perspective than that of BLEU.
The paper proceeds as follows. In section 2 we
describe how to learn bracketing instances from
a training corpus. In section 3 we elaborate the
syntax-driven bracketing model, including feature
generation and the integration of the SDB model
into phrase-based SMT. In section 4 and 5, we
present our experiments and analysis. And we fi-
nally conclude in section 6.
2 The Acquisition of Bracketing
Instances
In this section, we formally define the bracket-
ing instance, comprising two types namely binary
bracketing instance and unary bracketing instance.
316
We present an algorithm to automatically ex-
tract these bracketing instances from word-aligned
bilingual corpus where the source language sen-
tences are parsed.
Let c and e be the source sentence and the
target sentence, W be the word alignment be-
tween them, T be the parse tree of c. We
define a binary bracketing instance as a tu-
ple ?b, ?(ci..j), ?(cj+1..k), ?(ci..k)? where b ?
{bracketable, unbracketable}, ci..j and cj+1..k
are two neighboring source phrases and ?(T, s)
(?(s) for short) is a subtree function which returns
the minimal subtree covering the source sequence
s from the source parse tree T . Note that ?(ci..k)
includes both ?(ci..j) and ?(cj+1..k). For the two
neighboring source phrases, the following condi-
tions are satisfied:
?eu..v, ep..q ? e s.t.
?(m,n) ? W, i ? m ? j ? u ? n ? v (1)
?(m,n) ? W, j + 1 ? m ? k ? p ? n ? q (2)
The above (1) means that there exists a target
phrase eu..v aligned to ci..j and (2) denotes a tar-
get phrase ep..q aligned to cj+1..k. If eu..v and
ep..q are neighboring to each other or all words be-
tween the two phrases are aligned to null, we set
b = bracketable, otherwise b = unbracketable.
From a binary bracketing instance, we derive a
unary bracketing instance ?b, ?(ci..k)?, ignoring
the subtrees ?(ci..j) and ?(cj+1..k).
Let n be the number of words of c. If we ex-
tract all potential bracketing instances, there will
be o(n2) unary instances and o(n3) binary in-
stances. To keep the number of bracketing in-
stances tractable, we only record 4 representa-
tive bracketing instances for each index j: 1) the
bracketable instance with the minimal ?(ci..k), 2)
the bracketable instance with the maximal ?(ci..k),
3) the unbracketable instance with the minimal
?(ci..k), and 4) the unbracketable instance with the
maximal ?(ci..k).
Figure 1 shows the algorithm to extract brack-
eting instances. Line 3-11 find all potential brack-
eting instances for each (i, j, k) ? c but only keep
4 bracketing instances for each index j: two min-
imal and two maximal instances. This algorithm
learns binary bracketing instances, from which we
can derive unary bracketing instances.
1: Input: sentence pair (c, e), the parse tree T of c and the
word alignment W between c and e
2: < := ?
3: for each (i, j, k) ? c do
4: if There exist a target phrase eu..v aligned to ci..j and
ep..q aligned to cj+1..k then
5: Get ?(ci..j), ?(cj+1..k), and ?(ci..k)
6: Determine b according to the relationship between
eu..v and ep..q
7: if ?(ci..k) is currently maximal or minimal then
8: Update bracketing instances for index j
9: end if
10: end if
11: end for
12: for each j ? c do
13: < := < ? {bracketing instances from j}
14: end for
15: Output: bracketing instances <
Figure 1: Bracketing Instances Extraction Algo-
rithm.
3 The Syntax-Driven Bracketing Model
3.1 The Model
Our interest is to automatically detect phrase
bracketing using rich contextual information. We
consider this task as a binary-class classification
problem: whether the current source phrase s is
bracketable (b) within particular syntactic contexts
(?(s)). If two neighboring sub-phrases s1 and s2
are given, we can use more inner syntactic con-
texts to complete this binary classification task.
We construct the syntax-driven bracketing
model within the maximum entropy framework. A
unary SDB model is defined as:
PUniSDB(b|?(s), T ) =
exp(?i ?ihi(b, ?(s), T )?
b exp(
?
i ?ihi(b, ?(s), T )
(3)
where hi ? {0, 1} is a binary feature function
which we will describe in the next subsection, and
?i is the weight of hi. Similarly, a binary SDB
model is defined as:
PBiSDB(b|?(s1), ?(s2), ?(s), T ) =
exp(?i ?ihi(b, ?(s1), ?(s2), ?(s), T )?
b exp(
?
i ?ihi(b, ?(s1), ?(s2), ?(s), T )
(4)
The most important advantage of ME-based
SDB model is its capacity of incorporating more
fine-grained contextual features besides the binary
feature that detects constituent boundary violation
or matching. By employing these features, we
can investigate the value of various syntactic con-
straints in phrase translation.
317
jingfang
police
yi fengsuo
block
le baozha
bomb
xianchang
scene
NN NN
NP
VP
ASVVADNN
ADVP
VP
NP
IP
s
s1 s2
Figure 2: Illustration of syntax-driven features
used in SDB. Here we only show the features for
the source phrase s. The triangle, rounded rect-
angle and rectangle denote the rule feature, path
feature and constituent boundary matching feature
respectively.
3.2 Syntax-Driven Features
Let s be the source phrase in question, s1 and s2
be the two neighboring sub-phrases. ?(.) is the
root node of ?(.). The SDB model exploits various
syntactic features as follows.
? Rule Features (RF)
We use the CFG rules of ?(s), ?(s1) and
?(s2) as features. These features capture
syntactic ?horizontal context? which demon-
strates the expansion trend of the source
phrase s, s1 and s2 on the parse tree.
In figure 2, the CFG rule ?ADVP?AD?,
?VP?VV AS NP?, and ?VP?ADVP
VP? are used as features for s1, s2 and s
respectively.
? Path Features (PF)
The tree path ?(s1)..?(s) connecting ?(s1)
and ?(s), ?(s2)..?(s) connecting ?(s2)
and ?(s), and ?(s)..? connecting ?(s) and
the root node ? of the whole parse tree are
used as features. These features provide
syntactic ?vertical context? which shows the
generation history of the source phrases on
the parse tree.
(a) (b) (c)
Figure 3: Three scenarios of the relationship be-
tween phrase boundaries and constituent bound-
aries. The gray circles are constituent boundaries
while the black circles are phrase boundaries.
In figure 2, the path features are ?ADVP
VP?, ?VP VP? and ?VP IP? for s1, s2 and s
respectively.
? Constituent Boundary Matching Features
(CBMF)
These features are to capture the relationship
between a source phrase s and ?(s) or
?(s)?s subtrees. There are three different
scenarios3: 1) exact match, where s exactly
matches the boundaries of ?(s) (figure 3(a)),
2) inside match, where s exactly spans a
sequence of ?(s)?s subtrees (figure 3(b)), and
3) crossing, where s crosses the boundaries
of one or two subtrees of ?(s) (figure 3(c)).
In the case of 1) or 2), we set the value of
this feature to ?(s)-M or ?(s)-I respectively.
When s crosses the boundaries of the sub-
constituent ?l on s?s left, we set the value to
?(?l)-LC; If s crosses the boundaries of the
sub-constituent ?r on s?s right, we set the
value to ?(?r)-RC; If both, we set the value
to ?(?l)-LC-?(?r)-RC.
Let?s revisit the Figure 2. The source
phrase s1 exactly matches the constituent
ADVP, therefore CBMF is ?ADVP-M?. The
source phrase s2 exactly spans two sub-trees
VV and AS of VP, therefore CBMF is
?VP-I?. Finally, the source phrase s cross
boundaries of the lower VP on the right,
therefore CBMF is ?VP-RC?.
3.3 The Integration of the SDB Model into
Phrase-Based SMT
We integrate the SDB model into phrase-based
SMT to help decoder perform syntax-driven
phrase translation. In particular, we add a
3The three scenarios that we define here are similar to
those in (Lu? et al, 2002).
318
new feature into the log-linear translation model:
PSDB(b|T, ?(.)). This feature is computed by the
SDB model described in equation (3) or equation
(4), which estimates a probability that a source
span is to be translated as a unit within partic-
ular syntactic contexts. If a source span can be
translated as a unit, the feature will give a higher
probability even though this span violates bound-
aries of a constituent. Otherwise, a lower proba-
bility is given. Through this additional feature, we
want the decoder to prefer hypotheses that trans-
late source spans which can be translated as a unit,
and avoids translating those which are discontinu-
ous after translation. The weight of this new fea-
ture is tuned via MERT, which measures the extent
to which this feature should be trusted.
In this paper, we implement the SDB model in a
state-of-the-art phrase-based system which adapts
a binary bracketing transduction grammar (BTG)
(Wu, 1997) to phrase translation and reordering,
described in (Xiong et al, 2006). Whenever a
BTG merging rule (s ? [s1 s2] or s ? ?s1 s2?)
is used, the SDB model gives a probability to the
span s covered by the rule, which estimates the
extent to which the span is bracketable. For the
unary SDB model, we only consider the features
from ?(s). For the binary SDB model, we use all
features from ?(s1), ?(s2) and ?(s) since the bi-
nary SDB model is naturally suitable to the binary
BTG rules.
The SDB model, however, is not only limited
to phrase-based SMT using BTG rules. Since it
is applied on a source span each time, any other
hierarchical phrase-based or syntax-based system
that translates source spans recursively or linearly,
can adopt the SDB model.
4 Experiments
We carried out the MT experiments on Chinese-
to-English translation, using (Xiong et al, 2006)?s
system as our baseline system. We modified the
baseline decoder to incorporate our SDB mod-
els as descried in section 3.3. In order to com-
pare with Marton and Resnik?s approach, we also
adapted the baseline decoder to their XP+ feature.
4.1 Experimental Setup
In order to obtain syntactic trees for SDB models
and XP+, we parsed source sentences using a lex-
icalized PCFG parser (Xiong et al, 2005). The
parser was trained on the Penn Chinese Treebank
with an F1 score of 79.4%.
All translation models were trained on the FBIS
corpus. We removed 15,250 sentences, for which
the Chinese parser failed to produce syntactic
parse trees. To obtain word-level alignments, we
ran GIZA++ (Och and Ney, 2000) on the remain-
ing corpus in both directions, and applied the
?grow-diag-final? refinement rule (Koehn et al,
2005) to produce the final many-to-many word
alignments. We built our four-gram language
model using Xinhua section of the English Gi-
gaword corpus (181.1M words) with the SRILM
toolkit (Stolcke, 2002).
For the efficiency of MERT, we built our de-
velopment set (580 sentences) using sentences not
exceeding 50 characters from the NIST MT-02 set.
We evaluated all models on the NIST MT-05 set
using case-sensitive BLEU-4. Statistical signif-
icance in BLEU score differences was tested by
paired bootstrap re-sampling (Koehn, 2004).
4.2 SDB Training
We extracted 6.55M bracketing instances from our
training corpus using the algorithm shown in fig-
ure 1, which contains 4.67M bracketable instances
and 1.89M unbracketable instances. From ex-
tracted bracketing instances we generated syntax-
driven features, which include 73,480 rule fea-
tures, 153,614 path features and 336 constituent
boundary matching features. To tune weights of
features, we ran the MaxEnt toolkit (Zhang, 2004)
with iteration number being set to 100 and Gaus-
sian prior to 1 to avoid overfitting.
4.3 Results
We ran the MERT module with our decoders to
tune the feature weights. The values are shown
in Table 1. The PSDB receives the largest feature
weight, 0.29 for UniSDB and 0.38 for BiSDB, in-
dicating that the SDB models exert a nontrivial im-
pact on decoder.
In Table 2, we present our results. Like (Mar-
ton and Resnik, 2008), we find that the XP+ fea-
ture obtains a significant improvement of 1.08
BLEU over the baseline. However, using all
syntax-driven features described in section 3.2,
our SDB models achieve larger improvements
of up to 1.67 BLEU. The binary SDB (BiSDB)
model statistically significantly outperforms Mar-
ton and Resnik?s XP+ by an absolute improvement
of 0.59 (relatively 2%). It is also marginally better
than the unary SDB model.
319
Features
System P (c|e) P (e|c) Pw(c|e) Pw(e|c) Plm(e) Pr(e) Word Phr. XP+ PSDB
Baseline 0.041 0.030 0.006 0.065 0.20 0.35 0.19 -0.12 ? ?
XP+ 0.002 0.049 0.046 0.044 0.17 0.29 0.16 0.12 -0.12 ?
UniSDB 0.023 0.051 0.055 0.012 0.21 0.20 0.12 0.04 ? 0.29
BiSDB 0.016 0.032 0.027 0.013 0.13 0.23 0.08 0.09 ? 0.38
Table 1: Feature weights obtained by MERT on the development set. The first 4 features are the phrase
translation probabilities in both directions and the lexical translation probabilities in both directions. Plm
= language model; Pr = MaxEnt-based reordering model; Word = word bonus; Phr = phrase bonus.
BLEU-n n-gram Precision
System 4 1 2 3 4 5 6 7 8
Baseline 0.2612 0.71 0.36 0.18 0.10 0.054 0.030 0.016 0.009
XP+ 0.2720** 0.72 0.37 0.19 0.11 0.060 0.035 0.021 0.012
UniSDB 0.2762**+ 0.72 0.37 0.20 0.11 0.062 0.035 0.020 0.011
BiSDB 0.2779**++ 0.72 0.37 0.20 0.11 0.065 0.038 0.022 0.014
Table 2: Results on the test set. **: significantly better than baseline (p < 0.01). + or ++: significantly
better than Marton and Resnik?s XP+ (p < 0.05 or p < 0.01, respectively).
5 Analysis
In this section, we present analysis to perceive the
influence mechanism of the SDB model on phrase
translation by studying the effects of syntax-driven
features and differences of 1-best translation out-
puts.
5.1 Effects of Syntax-Driven Features
We conducted further experiments using individ-
ual syntax-driven features and their combinations.
Table 3 shows the results, from which we have the
following key observations.
? The constituent boundary matching feature
(CBMF) is a very important feature, which
by itself achieves significant improvement
over the baseline (up to 1.13 BLEU). Both
our CBMF and Marton and Resnik?s XP+
feature focus on the relationship between a
source phrase and a constituent. Their signifi-
cant contribution to the improvement implies
that this relationship is an important syntactic
constraint for phrase translation.
? Adding more features, such as path feature
and rule feature, achieves further improve-
ments. This demonstrates the advantage of
using more syntactic constraints in the SDB
model, compared with Marton and Resnik?s
XP+.
BLEU-4
Features UniSDB BiSDB
PF + RF 0.2555 0.2644*@@
PF 0.2596 0.2671**@@
CBMF 0.2678** 0.2725**@
RF + CBMF 0.2737** 0.2780**++@@
PF + CBMF 0.2755**+ 0.2782**++@?
RF + PF + CBMF 0.2762**+ 0.2779**++
Table 3: Results of different feature sets. * or **:
significantly better than baseline (p < 0.05 or p <
0.01, respectively). + or ++: significantly better
than XP+ (p < 0.05 or p < 0.01, respectively).
@?: almost significantly better than its UniSDB
counterpart (p < 0.075). @ or @@: significantly
better than its UniSDB counterpart (p < 0.05 or
p < 0.01, respectively).
? In most cases, the binary SDB is constantly
significantly better than the unary SDB, sug-
gesting that inner contexts are useful in pre-
dicting phrase bracketing.
5.2 Beyond BLEU
We want to further study the happenings after we
integrate the constraint feature (our SDB model
and Marton and Resnik?s XP+) into the log-linear
translation model. In particular, we want to inves-
tigate: to what extent syntactic constraints change
translation outputs? And in what direction the
changes take place? Since BLEU is not sufficient
320
System CCM Rate (%)
Baseline 43.5
XP+ 74.5
BiSDB 72.4
Table 4: Consistent constituent matching rates re-
ported on 1-best translation outputs.
to provide such insights, we introduce a new sta-
tistical metric which measures the proportion of
syntactic constituents 4 whose boundaries are con-
sistently matched by decoder during translation.
This proportion, which we call consistent con-
stituent matching (CCM) rate , reflects the ex-
tent to which the translation output respects the
source parse tree.
In order to calculate this rate, we output transla-
tion results as well as phrase alignments found by
decoders. Then for each multi-branch constituent
cji spanning from i to j on the source side, we
check the following conditions.
? If its boundaries i and j are aligned to phrase
segmentation boundaries found by decoder.
? If all target phrases inside cji ?s target span 5
are aligned to the source phrases within cji
and not to the phrases outside cji .
If both conditions are satisfied, the constituent cji
is consistently matched by decoder.
Table 4 shows the consistent constituent match-
ing rates. Without using any source-side syntac-
tic information, the baseline obtains a low CCM
rate of 43.53%, indicating that the baseline de-
coder violates the source parse tree more than it
respects the source structure. The translation out-
put described in section 1 is actually generated by
the baseline decoder, where the second NP phrase
boundaries are violated.
By integrating syntactic constraints into decod-
ing, we can see that both Marton and Resnik?s
XP+ and our SDB model achieve a significantly
higher constituent matching rate, suggesting that
they are more likely to respect the source struc-
ture. The examples in Table 5 show that the de-
coder is able to generate better translations if it is
4We only consider multi-branch constituents.
5Given a phrase alignment P = {cgf ? eqp}, if the seg-
mentation within cji defined by P is cji = cj1i1 ...c
jk
ik , and
cjrir ? evrur ? P, 1 ? r ? k, we define the target span of c
j
i
as a pair where the first element is min(eu1 ...euk ) and the
second element is max(ev1 ...evk ), similar to (Fox, 2002).
CCM Rates (%)
System <6 6-10 11-15 16-20 >20
XP+ 75.2 70.9 71.0 76.2 82.2
BiSDB 69.3 74.7 74.2 80.0 85.6
Table 6: Consistent constituent matching rates for
structures with different spans.
faithful to the source parse tree by using syntactic
constraints.
We further conducted a deep comparison of
translation outputs of BiSDB vs. XP+ with re-
gard to constituent matching and violation. We
found two significant differences that may explain
why our BiSDB outperforms XP+. First, although
the overall CCM rate of XP+ is higher than that
of BiSDB, BiSDB obtains higher CCM rates for
long-span structures than XP+ does, which are
shown in Table 6. Generally speaking, viola-
tions of long-span constituents have a more neg-
ative impact on performance than short-span vio-
lations if these violations are toxic. This explains
why BiSDB achieves relatively higher precision
improvements for higher n-grams over XP+, as
shown in Table 3.
Second, compared with XP+ that only punishes
constituent boundary violations, our SDB model
is able to encourage violations if these violations
are done on bracketable phrases. We observed in
many cases that by violating constituent bound-
aries BiSDB produces better translations than XP+
does, which on the contrary matches these bound-
aries. Still consider the example shown in section
1. The following translations are found by XP+
and BiSDB respectively.
XP+: [to/? ?[set up/?? [for the/? [naviga-
tion/?? section/?]]] on July 11/7?11??]
BiSDB: [to/? ?[[set up/?? a/?] [marine/??
festival/?]] on July 11/7?11??]
XP+ here matches all constituent boundaries while
BiSDB violates the PP constituent to translate the
non-syntactic phrase ??? ??. Table 7 shows
more examples. From these examples, we clearly
see that appropriate violations are helpful and even
necessary for generating better translations. By
allowing appropriate violations to translate non-
syntactic phrases according to particular syntac-
tic contexts, our SDB model better inherits the
strength of phrase-based approach than XP+.
321
Src: [[? [???????]NP ]PP [?? [??]NP [????]NP ]VP ]VP
Ref: show their loving hearts to people in the Indian Ocean disaster areas
Baseline: ?love/?? [for the/? ?[people/?? [to/?? [own/?? a report/??]]]? ?in/?? the Indian Ocean/?
???]?
XP+: ?[contribute/?? [its/?? [part/?? love/??]]] [for/? ?the people/?? ?in/?? the Indian Ocean/?
????]?
BiSDB: ?[[[contribute/?? its/??] part/??] love/??] [for/? ?the people/?? ?in/?? the Indian Ocean?
????]?
Src: [???? [?]ADVP [?? [[???]QP ??]NP [???]PP]VP]IP [?]PU [????...]IP
Ref: The Pentagon has dispatched 20 airplanes to South Asia, including...
Baseline: [[The Pentagon/???? has sent/???] [?[to/? [[South Asia/?? ,/?] including/????]] [20/?
? plane/???]?]]
XP+: [The Pentagon/???? [has/? [sent/?? [[20/?? planes/???] [to/? South Asia/??]]]]] [,/?
[including/????...]]
BiSDB: [The Pentagon/???? [has sent/??? [[20/?? planes/???] [to/? South Asia/??]]] [,/? [in-
cluding/????...]]
Table 5: Translation examples showing that both XP+ and BiSDB produce better translations than the
baseline, which inappropriately violates constituent boundaries (within underlined phrases).
Src: [[? [[[????????]NP [??]ADJP [??]NP]NP ?]LCP]PP ??]VP
Ref: said after a brief discussion with Powell at the US State Department
XP+: [?after/? ??[a brief/?? meeting/??] [with/? Powell/??]? [in/? the US State Department/???
??]? said/??]
BiSDB: ?said after/??? ?[a brief/?? meeting/??] ? with Powell/??? [at/? the State Department of the
United States/?????]???
Src: [? [[?? [??????]NP]VP]IP]PP [??? [??????]NP]VP
Ref: took a key step towards building future democratic politics
XP+: ?[a/? [key/??? step/???]] ?forward/?? [to/? [a/?? [future/?? political democracy/???
?]]]??
BiSDB: ?[made a/??? [key/??? step/???]] [towards establishing a/??? ?democratic politics/???
? in the future/???]?
Table 7: Translation examples showing that BiSDB produces better translations than XP+ via appropriate
violations of constituent boundaries (within double-underlined phrases).
6 Conclusion
In this paper, we presented a syntax-driven brack-
eting model that automatically learns bracketing
knowledge from training corpus. With this knowl-
edge, the model is able to predict whether source
phrases can be translated together, regardless of
matching or crossing syntactic constituents. We
integrate this model into phrase-based SMT to
increase its capacity of linguistically motivated
translation without undermining its strengths. Ex-
periments show that our model achieves substan-
tial improvements over baseline and significantly
outperforms (Marton and Resnik, 2008)?s XP+.
Compared with previous constituency feature,
our SDB model is capable of incorporating more
syntactic constraints, and rewarding necessary vi-
olations of the source parse tree. Marton and
Resnik (2008) find that their constituent con-
straints are sensitive to language pairs. In the fu-
ture work, we will use other language pairs to test
our models so that we could know whether our
method is language-independent.
References
Colin Cherry. 2008. Cohesive Phrase-based Decoding
for Statistical Machine Translation. In Proceedings
of ACL.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL, pages 263?270.
David Chiang, Yuval Marton and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and
Structural Translation Features. In Proceedings of
EMNLP.
Heidi J. Fox 2002. Phrasal Cohesion and Statistical
Machine Translation. In Proceedings of EMNLP,
pages 304?311.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL.
322
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Eval-
uation. In International Workshop on Spoken Lan-
guage Translation.
Yajuan Lu?, Sheng Li, Tiezhun Zhao and Muyun Yang.
2002. Learning Chinese Bracketing Knowledge
Based on a Bilingual Language Model. In Proceed-
ings of COLING.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrase-Based Transla-
tion. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of
ACL 2000.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatically
Evaluation of Machine Translation. In Proceedings
of ACL.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with Semantic Knowledge. In Proceed-
ings of IJCNLP, Jeju Island, Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Max-
imum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proceedings of
ACL-COLING 2006.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Linguistically Annotated BTG for Statistical
Machine Translation. In Proceedings of COLING
2008.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
323
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 255?264,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Max-Margin Synchronous Grammar Induction for Machine Translation
Xinyan Xiao and Deyi Xiong?
School of Computer Science and Technology
Soochow University
Suzhou 215006, China
xyxiao.cn@gmail.com, dyxiong@suda.edu.cn
Abstract
Traditional synchronous grammar induction
estimates parameters by maximizing likeli-
hood, which only has a loose relation to trans-
lation quality. Alternatively, we propose a
max-margin estimation approach to discrim-
inatively inducing synchronous grammars for
machine translation, which directly optimizes
translation quality measured by BLEU. In
the max-margin estimation of parameters, we
only need to calculate Viterbi translations.
This further facilitates the incorporation of
various non-local features that are defined on
the target side. We test the effectiveness of our
max-margin estimation framework on a com-
petitive hierarchical phrase-based system. Ex-
periments show that our max-margin method
significantly outperforms the traditional two-
step pipeline for synchronous rule extraction
by 1.3 BLEU points and is also better than pre-
vious max-likelihood estimation method.
1 Introduction
Synchronous grammar induction, which refers to
the process of learning translation rules from bilin-
gual corpus, still remains an open problem in sta-
tistical machine translation (SMT). Although state-
of-the-art SMT systems model the translation pro-
cess based on synchronous grammars (including
bilingual phrases), most of them still learn trans-
lation rules via a pipeline with word-based heuris-
tics (Koehn et al, 2003). This pipeline first builds
word alignments using heuristic combination strate-
gies, then heuristically extracts rules that are consis-
tent with word alignments. Such heuristic pipeline
?Corresponding author
is not elegant theoretically. It brings an undesirable
gap that separates modeling and learning in an SMT
system.
Therefore, researchers have proposed alternative
approaches to learning synchronous grammars di-
rectly from sentence pairs without word alignments,
via generative models (Marcu and Wong, 2002;
Cherry and Lin, 2007; Zhang et al, 2008; DeNero
et al, 2008; Blunsom et al, 2009; Cohn and Blun-
som, 2009; Neubig et al, 2011; Levenberg et al,
2012) or discriminative models (Xiao et al, 2012).
Theoretically, these approaches describe how sen-
tence pairs are generated by applying sequences of
synchronous rules in an elegant way. However, they
learn synchronous grammars by maximizing likeli-
hood,1 which only has a loose relation to transla-
tion quality (He and Deng, 2012). Moreover, gen-
erative models are normally hard to be extended to
incorporate useful features, and the discriminative
synchronous grammar induction model proposed by
Xiao et al (2012) only incorporates local features
defined on parse trees of the source language. Non-
local features, which encode information from parse
trees of the target language, have never been ex-
ploited before due to the computational complexity
of normalization in max-likelihood estimation.
Consequently, we would like to learn syn-
chronous grammars in a discriminative way that can
directly maximize the end-to-end translation quality
measured by BLEU (Papineni et al, 2002), and is
also able to incorporate non-local features from tar-
get parse trees.
We thus propose a max-margin estimation method
1More precisely, the discriminative model by Xiao et al
(2012) maximizes conditional likelihood.
255
to discriminatively induce synchronous grammar di-
rectly from sentence pairs without word alignments.
We try to maximize the margin between a reference
translation and a candidate translation with transla-
tion errors that are measured by BLEU. The more
serious the translation errors, the larger the margin.
In this way, our max-margin method is able to learn
synchronous grammars according to their translation
performance. We further incorporate various non-
local features defined on target parse trees. We ef-
ficiently calculate the non-local feature values of a
translation over its exponential derivation space us-
ing the inside-outside algorithm. Because our max-
margin estimation optimizes feature weights only by
the feature values of Viterbi and reference transla-
tions, we are able to efficiently perform optimization
even with non-local features.
We apply the proposed max-margin estimation
method to learn synchronous grammars for a hi-
erarchical phrase-based translation system (Chiang,
2007) which typically produces state-of-the-art per-
formance. With non-local features defined on tar-
get parse trees, our max-margin method significantly
outperforms the baseline that uses synchronous
rules learned from the traditional pipeline by 1.3
BLEU points on large-scale Chinese-English bilin-
gual training data.
The remainder of this paper is organized as fol-
lows. Section 2 presents the discriminative syn-
chronous grammar induction model with the non-
local features. In Section 3, we elaborate our max-
margin estimation method which is able to directly
optimize BLEU, and discuss how we induce gram-
mar rules. Local and non-local features are de-
scribed in Section 4. Finally, in Section 5, we verify
the effectiveness of our method through experiments
by comparing it against both the traditional pipeline
and max-likelihood estimation method.
2 Discriminative Model with Non-local
Features
Let S denotes the set of all strings in a source lan-
guage. Given a source sentence s ? S , T (s) denotes
all candidate translations in the target language that
can be generated by a synchronous grammar G. A
translation t ? T (s) is generated by a sequence of
translation steps (r1, ..., rn), where we apply a syn-
?? ? ?? ?? ??10 2 3 4 5
 [1,3]
 [1,5]
 [0,5]
[1,6]
[0,6]
[4,6]
10 2 3 4 5 6
bushi yu shalong juxing huitan
r1: ? yu shalong? with Sharon ?
r2: ? X juxing huitan? held a talk X ?
r3: ? bushi X ? Bush X ?
Figure 1: A derivation of a sentence pair represented by
a synchronous tree. The above and below part are the
parses in the source language side and the target language
side respectively. Left subscript of a node X denotes the
source span, while right subscript denotes the target span.
A dashed line denotes an alignment from a source span
to a target span. The annotation for a dashed line cor-
responds to the rewriting rule used in the corresponding
step of the derivation.
chronous rule r ? G in one step. We refer to such
a sequence of translation steps as a derivation (See
Figure 1) and denote it as d ? D(s), where D(s)
represents the derivation space of a source sentence.
Given an input source sentence s, we output a pair
?t,d? in SMT. Thus, we study the triple ?s, t,d? in
SMT.
In our discriminative model, we calculate the
value of a triple ?s, t,d? according to the following
scoring function:
f(s, t,d) = ?T?(s, t,d) (1)
where ? ? ? is a feature weight vector, and ? is the
feature function.
There are exponential outputs in SMT. Therefore
it is necessary to factorize the feature function in or-
der to perform efficient calculation over the SMT
output space using dynamic programming. We de-
compose the feature function of a triple ?s, t,d? into
256
? ??
 [1,5]
?? ??
[1,6]
Figure 2: Example features for the derivation in Figure 1.
Shaded nodes denote information encoded in the feature.
a sum of values of each synchronous rule in the
derivation d.
?(s, t,d) =
?
r?d
?(r, s)
? ?? ?
local
+
?
r?d
?(r, s, t)
? ?? ?
non-local
(2)
Our feature functions include both local and non-
local features. A feature is a local feature if and
only if it can be factored among the translation steps
in a derivation. In other words, the value of a lo-
cal feature for ?s, t,d? can be calculated as a sum of
local scores in each translation step, and the calcula-
tion of each local score only requires to look at the
rule used in corresponding step and the input sen-
tence. Otherwise, the feature is a non-local feature.
Our discriminative model allows to incorporate non-
local features that are defined on target translations.
For example, a rule feature in Figure 2(a), which
indicates the application of a specific rule in a
derivation, is a local feature. A source span bound-
ary feature in Figure 2(b) that is defined on the
source parse tree is also a local feature. However,
a target span boundary feature in Figure 2(c), which
assesses the target parse structure, is a non-local fea-
ture. According to Figure 1, the span is parsed in
step r2, but it also depends on the translation bound-
ary word ?held? generated in previous step r1. We
will describe the details of both local and non-local
features that we use in Section 4.
Non-local features enable us to model the target
parse structure in a derivation. However, it is com-
putationally expensive to calculate the expected val-
ues of non-local features over D(s), as non-local
features require to record states of target boundary
s, S, S s is a sentence in a source language;
S means source training sentences;
S denotes all the possible sentences;
t, T, T symbols for the target language that
similar to s, S, S;
d, D derivation and derivation space;
D(s) space of derivations for
a source sentence;
D(s, t) space of derivations for
a source sentence with its translation;
H(s) hypergraph that represents D(s);
H(s, t) hypergraph that represents D(s, t);
Table 1: Notations in this paper. We give an abstract of
related notations for clarity.
words and result in an extremely large number of
states during dynamic programming. Fortunately,
when integrating out derivations over the derivation
space D(s, t) of a source sentence and its transla-
tion, we can efficiently calculate the non-local fea-
tures. Because all derivations in D(s, t) share the
same translation, there is no need to maintain states
for target boundary words. We will discuss this com-
putational problem in details in Section 3.3. In the
proposed max-margin estimation described in next
section, we only need to integrate out derivation
for a Viterbi translation and a reference translation
when updating feature weights. Therefore, the de-
fined non-local features allow us to not only explore
useful knowledge on the target parse trees, but also
compute them efficiently over D(s, t) during max-
margin estimation.
3 Max-Margin Estimation
In this section, we describe how we use a parallel
training corpus {S,T} = {(s(i), t(i))}Ni=1 to esti-
mate feature weights ?, which contain parameters of
the induced synchronous grammars and the defined
non-local features.
We choose the parameters that maximize the
translation quality measured by BLEU using the
max-margin estimation (Taskar et al, 2004). Mar-
gin refers to the difference of the model score be-
tween a reference translation t(i) and a candidate
translation t. We hope that the worse the transla-
tion quality of t, the larger the margin between t
and t(i). In this way, we penalize larger translation
257
errors more severely than smaller ones. This intu-
ition is expressed by the following equation.
min 1
2
???2 (3)
s.t. f(s(i), t(i))? f(s(i), t) ? cost(t(i), t)
?t ? T (s(i))
Here, f(s, t) is the feature function of a translation,
and cost function cost(t(i), t) measures the trans-
lation errors of a candidate translation t comparing
with a reference translation t(i). We define the cost
function via the widely-used translation evaluation
metric BLEU. We use the smoothed sentence level
BLEU-4 (Lin and Och, 2004) here:
cost(t(i), t) = 1? BLEU-4(t(i), t) (4)
In Section 3.1, we will discuss how we use the
scoring function f(s, t,d) to calculate f(s, t). Then
in Section 3.2, we recast the equation (3) as an un-
constrained empirical loss minimization problem,
and describe the learning algorithm for optimizing
? and inducing G. Finally, we give the details of
inference for the learning algorithm in Section 3.3.
3.1 Integrate Out Derivation by Averaging
Although we only model the triple ?s, t,d? in the
equation (1), it?s necessary to calculate the scoring
function f(s, t) of a translation by integrating out
the variable of derivation as derivation is not ob-
served in the training data.
We use an averaging computation over all possi-
ble derivations of a translation D(s, t). We call this
an average derivation based estimation:
f(s, t) = 1
|D(s, t)|
?
d?D(s,t)
f(s, t,d) (5)
The ?average derivation? can be considered as the
geometric central point in the space D(s, t).
Another possible way to deal with the latent
derivation is max-derivation, which uses the max-
operator over D(s, t). The max derivation method
sets f(s, t) as maxd?D(s,t) f(s, t,d). It is often
adopted in traditional SMT systems. Nevertheless,
we instead use average-derivation for two reasons.2
2Imagine that H(s, t) in the Algorithm 1 is replaced by a
maximum derivation inH(s, t).
First, as a translation has an exponential number of
derivations, finding the max derivation of a refer-
ence translation for learning is nontrivial (Chiang et
al., 2009). Second, the max derivation estimation
will result in a low rule coverage, as rules in a max
derivation only covers a small fraction of rules in
the D(s, t). Because rule coverage is important in
synchronous grammar induction, we would like to
explore the entire derivation space using the average
operator.
3.2 Learning Algorithm
We reformulate the equation (3) as an unconstrained
empirical loss minimization problem as follows:
min ?
2
???2 + 1
N
N
?
n=1
L(s(i), t(i), ?) (6)
Where ? denotes the regularization strength for
L2-norm. The loss function of a sentence pair
L(s(i), t(i), ?) is a convex hinge loss function de-
noted by:
max{0,?f(s(i), t(i)) (7)
+ max
t?T (s(i))
(
f(s(i), t) + cost(t(i), t)
)
}
According to the second max-operator in the
hinge loss function, the optimization towards BLEU
is expressed by cost-augmented inference. Cost-
augmented inference finds a translation that has a
maximum model score augmented with cost.
t? = max
t?T (s(i))
(
f(s(i), t) + cost(t(i), t)
)
(8)
We applied the Pegasos algorithm for the op-
timization of equation (6) (Shalev-Shwartz et al,
2007). This is an online algorithm, which alternates
between stochastic gradient descent steps and pro-
jection steps. When the loss function is non-zero, it
updates weights according to the sub-gradient of the
hinge loss function. Using the average scoring func-
tion in the equation (5), the sub-gradient of hinge
loss function for a sentence pair is the difference of
average feature values between a Viterbi translation
258
Algorithm 1 UPDATE(s, t, ?,G) ? One step in online algorithm. s, t are short for s(i), t(i) here
1: H(s, t)? BIPARSE(s, t, ?) ? Build hypergraph of reference translation
2: G?G +H(s, t) ? Discover rules fromH(s, t)
3: t?, d?? argmax?t?,d???D(s) f(s, t?,d?) + cost(t, t?) ? Find Viterbi translation
4: H(s, t?)? BIPARSE(s, t?, ?) ? Build hypergraph of Viterbi translation
5: if f(s, t) < f(s, t?) + cost(t, t?) then
6: ? ? (1? ??)? + ? ? ?L?? (H(s, t),H(s, t?)) ? Update ? by gradient
?L
?? and learning rate ?
7: ? ? min {1, 1/
?
?
??? } ? ? ? Projection by scaling
8: return G, ?
and a reference translation.
?L
??
= 1
|D(s(i), t(i))|
?
d?D(s(i),t(i))
?(s(i), t(i),d)
? 1
|D(s(i), t?)|
?
d?D(s(i),t?)
?(s(i), t?,d) (9)
Algorithm 1 shows the procedure of one step in
the online optimization algorithm. The procedure
discovers rules and updates weights in an online
fashion. In the procedure, we first biparse the sen-
tence pair to construct a synchronous hypergraph of
a reference translation (line 1). In the biparsing al-
gorithm, synchronous rules for constructing hyper-
edges are not required to be in G, but can be any
rules that follow the form defined in Chiang (2007).
Thus, the biparsing algorithm can discover new rules
that are not in G. Then we collect the translation
rules discovered in the hypergraph of the reference
translation (line 2), which are rules indicated by hy-
peredges in the hypergraph. We then calculate the
Viterbi translation according to the scoring function
and cost function (see Section 3.3) (line 3), and build
the synchronous hypergraph for the Viterbi transla-
tion (line 4). Finally, we update weights according to
the Pegasos algorithm (line 5). The sub-gradient is
calculated based on the hypergraph of Viterbi trans-
lation and reference translation.
In practice, in order to process the data in a paral-
lel manner, we use a larger step size of 1000 for the
learning algorithm. In each step of our online opti-
mization algorithm, we first biparse 1000 reference
sentence pairs in parallel. Then, we collect grammar
rules from the generated reference hypergraphs. Af-
ter that, we compute the gradients of 1000 sentence
pairs in parallel, by calculating feature weights over
reference hypergraphs and Viterbi hypergraphs. Fi-
nally, we update the feature weights using the sum
of these gradients.
3.3 Inference
There are two parts that need to be calculated in
the learning algorithm: finding a cost-augmented
Viterbi translation according to the scoring func-
tion and cost function (Equation 8), and constructing
synchronous hypergraphs for the Viterbi and refer-
ence translation so as to discover rules and calculate
average feature values in Equation (9). Following
the traditional decoding procedure, we resort to the
cube-pruning based algorithm for approximation.
To find the Viterbi translation, we run the tra-
ditional translation decoding algorithm (Chiang,
2007) to get the best derivation. Then we use
the translation yielded by the best derivation as the
Viterbi translation. In order to obtain the BLEU
score in the cost function, we need to calculate the
ngram precision. It is calculated in a way similar to
the calculation of the ngram language model. The
computation of BLEU-4 requires to record 3 bound-
ary words in both the left and right side during dy-
namic programming. Therefore, even when we use
a language model whose order is less than 4, we still
expands the states to record 3 boundary words so as
to calculate the cost measured by BLEU.
We build synchronous hypergraphs using the
cube-pruning based biparsing algorithm (Xiao et al,
2012). Algorithm 2 shows the procedure. Using
a chart, the biparsing algorithm constructs k-best
alignments for every source word (lines 1-5) and k-
best hyperedges for every source span (lines 6-13)
from the bottom up. Thus, a synchronous hyper-
graph is generated during the construction of the
chart. More specifically, for a source span, it first
creates cubes L for all source parses ? that are in-
259
Algorithm 2 BIPARSE(s, t, ?) ? (Xiao et al, 2012)
 Create k-best alignments for each source word
1: for i? 1, .., |s| do
2: for j ? 1, .., |t| do
3: Lj ? {?, tj} ? si aligns to tj or not
4: L? ?L1, ..., L|t|?
5: chart[s, i]? KBEST(L,?,?)
 Create k-best hyperedges for each source span
6: H? ?
7: for h? 1, .., |s| do ? h is the size of span
8: for all i, j s.t. j ? i = h do
9: L? ?
10: for ? inferable from chart do
11: L? L + ?chart[?1], ..., chart[?|?|]?
12: chart[X, i, j]? KBEST(L,?,?)
13: H?H + chart[X, i, j] ? save hyperedges
14: returnH
ferable from the chart (lines 9-11). Here ?i is a par-
tial source parse that covers either a single source
word or a span of source words. Then it uses the
cube pruning algorithm to keep the top k derivations
among all partial derivations that share the same
source span [i, j] (line 12). Notably, this biparsing
algorithm does not require specific translation rules
as input. Instead, it is able to discover new syn-
chronous grammar rules when constructing a syn-
chronous hypergraph: extracting each hyperedge in
the hypergraph as a synchronous rule.
Based on the biparsing algorithm, we are able to
construct the reference hypergraph H(s(i), t(i)) and
Viterbi hypergraph H(s(i), t?). By the reference hy-
pergraph, we collect new synchronous translation
rules and record them in the grammar G. We also
calculate the average feature values of hypergraphs
using the inside-outside algorithm (Li et al, 2009),
so as to compute the gradients.
4 Features
One advantage of the discriminative method is that
it enables us to incorporate arbitrary features. As
shown in Section 2, our model incorporates both lo-
cal and non-local features.
4.1 Local Features
Rule features We associate each rule with an indi-
cator feature. Each indicator feature counts the num-
ber of times that a rule appears in a derivation. In
this way, we are able to learn a weight for every rule
according to the entire structure of sentence.
Word association features Lexicalized features
are widely used in traditional SMT systems. Here
we adopt two lexical weights called noisy-or fea-
tures (Zens and Ney, 2004). The noisy-or feature
is estimated by word translation probabilities output
by GIZA++. We set the initial weight of these two
lexical scores with equivalent positive values. The
lexical weights enable our system to score and rank
the hyperedges at the beginning. Although word
alignment features are used, we do not constrain the
derivation space of a sentence pair by prefixed word
alignment, and do not require any heuristic align-
ment combination strategy.
Length feature We integrate the length of target
translation that is used in traditional SMT system as
our feature.
Source span boundary features We use this kind
of feature to assess the source parse tree in a deriva-
tion. Previous work (Xiong et al, 2010) has shown
the importance of phrase boundary features for
translation. Actually, this kind of feature is a good
cue for deciding the boundary where a rule is to be
learnt. Following Taskar et al (2004), for a bispan
[i, j, k, l] in a derivation, we define the feature tem-
plates that indicates the boundaries of a span by its
beginning and end words: {B : si+1;E : sj ;BE :
si+1, sj}.
Source span orientation features Orientation
features are only used for those spans that are swap-
ping. In Figure 1, the translation of source span [1, 3]
is swapping with that of span [4, 5] by r2, thus ori-
entation feature for span [1, 3] is activated. We also
define three feature templates for a swapping span
similar to the boundary features: {B : si+1;E :
sj ;BE : si+1, sj}. In practice, we add a prefix to
the orientation features so as to distinguish these fea-
tures from the boundary features.
4.2 Non-local Features
Target span boundary features We also want to
assess the target tree structure in a derivation. We
define these features in a way similar to source span
boundary features. For a bispan [i, j, k, l] in a deriva-
tion, we define the feature templates that indicates
260
System Grammar Size MT03 MT04 MT05 Avg.
Moses 302.5M 34.26 36.56 32.69 34.50
Baseline 77.8M 33.83 35.81 33.23 34.29
Max-margin 59.4M 34.62 37.14 34.00 35.25+Sparse feature 35.48 37.31 34.07 35.62
Table 2: Experiment results. Baseline is an in-house implementation of hierarchical phrase based system. Moses
denotes the implementation of hierarchical phrased-model in Moses (Koehn et al, 2007). +Sparsefeature means
that those sparse features used in the grammar induction are also used during decoding. The improvement of max-
margin over Baseline is statistically significant (p < 0.01).
target span boundary as: {B : tk+1;E : tl;BE :
tk+1, tl}.
Target span orientation features Similar target
orientation features are used for a swapping span
[i, j, k, l] with feature templates {B : tk+1;E :
tl;BE : tk+1, tl}.
Relative position features Following Blunsom
and Cohn (Blunsom and Cohn, 2006), we integrate
features indicating the closeness to the alignment
matrix diagonal. For an aligned word pair with
source position i and target position j, the value of
this feature is | i|s| ?
j
|t| |. As this feature depends
on the length of the target sentence, it is a non-local
feature.
Language model We also incorporate an ngram
language model which is an important component
in SMT. For efficiency, we use a 3-gram language
model trained on the target side of our training data
during the induction of synchronous grammars.
5 Experiment
In this section, we present our experiments on the
NIST Chinese-to-English translation tasks. We first
compare our max-margin based method with the tra-
ditional pipeline on a large bitext which contains
1.1 million sentences. We then present a detailed
comparison on a smaller dataset, in order to analyze
the effectiveness of max-margin estimation compar-
ing with the max likelihood estimation (Xiao et al,
2012), and also the effectiveness of the non-local
features that are defined on the target side.
5.1 Setup
The baseline system is the hierarchical phrase based
system (Chiang, 2007). We used a bilingual corpus
that contains 1.1M sentences (44.6 million words)
of up to length 40 from the LDC data.3 Our 5-gram
language model was trained by SRILM toolkit (Stol-
cke, 2002). The monolingual training data includes
the Xinhua section of the English Gigaword corpus
and the English side of the entire LDC data (432 mil-
lion words).
We used the NIST 2002 (MT02) as our develop-
ment set, and the NIST 2003-2005 (MT03-05) as the
test set. Case-insensitive NIST BLEU-4 (Papineni
et al, 2002) is used to measure translation perfor-
mance, and also the cost function in the max-margin
estimation. Statistical significance in BLEU differ-
ences was tested by paired bootstrap re-sampling
(Koehn, 2004). We used minimum error rate train-
ing (MERT) (Och, 2003) to optimize feature weights
for the traditional log-linear model.
We used the same decoder as the baseline system
in all estimation methods. Without special explana-
tion, we used the same features as those in the tra-
ditional pipeline: forward and backward translation
probabilities, forward and backward lexical weights,
count of extracted rules, count of glue rules, length
of translation, and language model. For the lexical
weights we used the noisy-or in all configurations
including the baseline system. For the discrimina-
tive grammar induction, rule translation probabili-
ties were calculated using the expectations of rules
in the synchronous hypergraphs of sentence pairs.
As our max-margin synchronous grammar induc-
tion is trained on the entire bitext, it is necessary to
load all the rules into the memory during training.
To control the size of rule table, we used Viterbi-
3Including LDC2002E18, LDC2003E07, LDC2003E14,
LDC2004T07, LDC2005T06 and Hansards portion of
LDC2004T08.
261
System Feature Function MT03 MT04 MT05 Avg.
Baseline ? 31.76 33.08 31.06 31.96
Max-likelihood local 32.84 34.54 31.61 33.00
Max-margin local 32.97 34.92 31.99 33.29local,non-local 33.27 34.83 32.32 33.47
Table 3: Comparison of Max-margin and Max-likelihood estimation on a smaller corpus. For max-margin method, we
present two results according to the usages of non-local features. The max-margin with non-local features significantly
outperforms the Baseline (p < 0.01) and also the max-likelihood estimation (p < 0.05).
pruning (Huang, 2008) when collecting rules as
shown in line 2 of optimization procedure in Section
3.2. Furthermore, we aggressively discarded those
large rules (The number of source symbols or the
number of target symbols are more than two) that
occur only in one sentence. Whenever the learning
algorithm processes 50K sentences, we performed
this discarding operation for large rules.
5.2 Result on Large Dataset
Table 2 shows the translation results. Our method
induces 59.4 million synchronous rules, which are
76.3% of the grammar size of baseline. Note that
Moses allows the boundary words of a phrase to be
unaligned, while our baseline constraints the initial
phrase to be tightly consistent with word alignment.
Therefore, Moses extract a much larger rule table
than that of our baseline.
With fewer translation rules, our method obtains
an average improvement of +0.96 BLEU points on
the three test sets over the Baseline. As the differ-
ence between the baseline and our max-margin syn-
chronous grammar induction model only lies in the
grammar, this result clearly denotes that our learnt
grammar does outperform the grammar extracted by
the traditional two-step pipeline.
We also incorporate the sparse features during de-
coding in a way similar to Xiao et al (2012) and
Dyer et al (2011). In order to optimize these sparse
features with the dense features by MERT, we group
features of the same type into one coarse ?summary
feature?, and get three such features including: rule,
phrase-boundary and phrase orientation features. In
this way, we rescale the weights of the three ?sum-
mary features? with the 8 dense features by MERT.
We achieve a further improvement of +0.37 BLEU
points. Therefore, our training algorithm is able to
learn the useful information encoded by the sparse
features for translation.
5.3 Comparison of Estimation Objective and
Non-Local Feature
Wewant to investigate whether the max-margin esti-
mation is able to outperform the max-likelihood es-
timation method (Xiao et al, 2012). Therefore we
carried out experiments to compare them directly.
As the max-margin method is able to use non-local
features, we compare two settings of features for the
max-margin method. One uses only local features,
the other uses both local and non-local features. Be-
cause the training procedure need to run on the entire
corpus, which is time consuming, we therefore use
a smaller corpus containing 50K sentences from the
entire bitext for comparison.
Table 3 shows the results. When using only local
features, the max-margin method consistently out-
performs the max-likelihood method in all three test
sets. This clearly shows the advantage of learning
grammars by optimizing BLEU over likelihood.
When incorporating the non-local features into
the max-margin method, we achieve further im-
provement against the max-margin method with-
out non-local features. With non-local features,
our max-margin estimation method outperforms the
baseline by 1.5 BLEU points, and is better than
the max-likelihood estimation by 0.5 BLEU points.
Based on these results, we believe that non-local fea-
tures, which encode information from target parse
structures, are helpful for grammar induction. This
further confirms the advance of the max-margin es-
timation, as it provides us a convenient way to use
non-local features.
262
6 Related Work
As the synchronous grammar is the key compo-
nent in SMT systems, researchers have proposed
various methods to improve the quality of gram-
mars. In addition to the generative and discrimina-
tive models introduced in Section 1, researchers also
have made efforts on word alignment and grammar
weight rescoring.
The first line is to modify word alignment by ex-
ploring information of syntactic structures (May and
Knight, 2007; DeNero and Klein, 2010; Pauls et
al., 2010; Burkett et al, 2010; Riesa et al, 2011).
Such syntactic information is combined with word
alignment via a discriminative framework. These
methods prefer word alignments that are consistent
with syntactic structure alignments. However, la-
beled word alignment data are required in order to
learn the discriminative model.
Yet another line is to rescore the weights of trans-
lation rules. This line of work tries to improve the
relative frequency estimation used in the traditional
pipeline. They rescore the weights or probabilities
of extracted rules. The rescoring is done by using
the similar latent log-linear model as ours (Blun-
som et al, 2008; Ka?a?ria?inen, 2009; He and Deng,
2012), or incorporating various features using la-
beled word aligned bilingual data (Huang and Xi-
ang, 2010). However, in rescoring, translation rules
are still extracted by the heuristic two-step pipeline.
Therefore these previous work still suffers from the
inelegance problem of the traditional pipeline.
Our work also relates to the discriminative train-
ing (Och, 2003; Watanabe et al, 2007; Chiang et al,
2009; Xiao et al, 2011; Gimpel and Smith, 2012)
that has been widely used in SMT systems. Notably,
these discriminative training methods are not used to
learn grammar. Instead, they assume that grammar
are extracted by the traditional two-step pipeline.
7 Conclusion
In this paper we have presented a max-margin esti-
mation for discriminative synchronous grammar in-
duction. By associating the margin with the transla-
tion quality, we directly learn translation rules that
optimize the translation performance measured by
BLEU. Max-margin estimation also provides us a
convenient way to incorporate non-local features.
Experiment results validate the effectiveness of opti-
mizing parameters by BLEU, and the importance of
incorporating non-local features defined on the tar-
get language. These results confirm the advantage of
our max-margin estimation framework as it can both
optimize BLEU and incorporate non-local features.
Feature engineering is very important for discrim-
inative models. Researchers have proposed various
types of features for machine translation, which are
often estimated from word alignments. We would
like to investigate whether further improvement can
be achieved by incorporating such features, espe-
cially the context model (Shen et al, 2009) in the
future. Because our proposed model is quite general,
we are also interested in applying this method to
induce linguistically motivated synchronous gram-
mars for syntax-based SMT.
Acknowledgments
The first author was partially supported by 863
State Key Project (No. 2011AA01A207) and
National Key Technology R&D Program (No.
2012BAH39B03). We are grateful to the anony-
mous reviewers for their insightful comments. We
also thank Yi Lin for her invaluable feedback.
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Prob. ACL 2006, July.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL 2008.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. ACL 2009.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Proc. NAACL 2010.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proc. SSST 2007, NAACL-HLT Workshop on Syntax
and Structure in Statistical Translation, April.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
263
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. EMNLP 2009.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proc. ACL 2010.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP 2008.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The cmu-ark german-english
translation system. In Proc. WMT 2011.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. NAACL 2012.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation models.
In Proc. ACL 2012.
Fei Huang and Bing Xiang. 2010. Feature-rich discrimi-
native phrase rescoring for smt. In Proc. Coling 2010.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL 2008.
Matti Ka?a?ria?inen. 2009. Sinuhe ? statistical machine
translation using a globally trained conditional expo-
nential family translation model. In Proc. EMNLP
2009.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL 2007 (demonstration session).
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A bayesian model for learning scfgs with discontigu-
ous rules. In Proc. EMNLP 2012. Association for
Computational Linguistics, July.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL 2009.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Pro. Coling 2004.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP 2002.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Proc.
EMNLP 2007.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL 2011.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proc. NAACL
2010.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In Proc.
EMNLP 2011.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proc. ICML 2007.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. EMNLP 2004.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proc. EMNLP-CoNLL
2007.
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Fast generation of translation forest for large-
scale smt discriminative training. In Proc. EMNLP
2011.
Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and
Shouxun Lin. 2012. Unsupervised discriminative in-
duction of synchronous grammar for machine transla-
tion. In Proc. Coling 2012.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In Proc. NAACL2010.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Prob.
NAACL 2004.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proc. ACL 2008.
264
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1563?1573,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Lexical Chain Based Cohesion Models for
Document-Level Statistical Machine Translation
Deyi Xiong1, Yang Ding2, Min Zhang1? and Chew Lim Tan2
1School of Computer Science and Technology, Soochow University, Suzhou, China 215006
{dyxiong, minzhang}@suda.edu.cn
2School of Computing, National University of Singapore, Singapore 117417
{a0082379, tancl}@comp.nus.edu.sg
Abstract
Lexical chains provide a representation of the
lexical cohesion structure of a text. In this pa-
per, we propose two lexical chain based co-
hesion models to incorporate lexical cohesion
into document-level statistical machine trans-
lation: 1) a count cohesion model that rewards
a hypothesis whenever a chain word occurs in
the hypothesis, 2) and a probability cohesion
model that further takes chain word transla-
tion probabilities into account. We compute
lexical chains for each source document to be
translated and generate target lexical chains
based on the computed source chains via max-
imum entropy classifiers. We then use the
generated target chains to provide constraints
for word selection in document-level machine
translation through the two proposed lexical
chain based cohesion models. We verify the
effectiveness of the two models using a hier-
archical phrase-based translation system. Ex-
periments on large-scale training data show
that they can substantially improve translation
quality in terms of BLEU and that the prob-
ability cohesion model outperforms previous
models based on lexical cohesion devices.
1 Introduction
Given a source document, traditionally most statisti-
cal machine translation (SMT) systems translate the
document sentence by sentence. In such a transla-
tion scheme, sentences are translated independent
of any other sentences. However, a text is normally
written cohesively, in which sentences are connected
?Corresponding author
to each other via syntactic and lexical devices. This
linguistic phenomenon is called as textual cohesion
(Halliday and Hasan, 1976).
Cohesion is a surface-level property of well-
formed texts. It deals with five categories of rela-
tionships between text units, namely co-reference,
ellipsis, substitution, conjunction and lexical cohe-
sion that is realized via semantically related words.
The former four cohesion relations can be grouped
as grammatical cohesion. Generally speaking,
grammatical cohesion is less common and harder
to identify than lexical cohesion (Barzilay and El-
hadad, 1997).
As most SMT systems translate a text in a
sentence-by-sentence fashion, they tend to build less
lexical cohesion than human translators (Wong and
Kit, 2012). We therefore study lexical cohesion for
document-level translation. We use lexical chains
(Morris and Hirst, 1991) to capture lexical cohe-
sion in a text. Lexical chains are connected graphs
that represent the lexical cohesion structure of a text.
They have been successfully used for information
retrieval (Stairmand, 1996), document summariza-
tion (Barzilay and Elhadad, 1997) and so on. In this
paper, we investigate how lexical chains can be used
to incorporate lexical cohesion into document-level
translation.
Our basic assumption is that the lexical chains of
a target document are direct correspondences of the
lexical chains of its counterpart source document.
This assumption is reasonable as the target docu-
ment translation should be faithful to the source doc-
ument in terms of both text meaning and structure.
Based on this assumption, we propose a framework
1563
to incorporate lexical cohesion into target document
translation via lexical chains, which works as fol-
lows.
? Compute lexical chains for each source docu-
ment that is to be translated;
? Project the computed source lexical chains onto
the corresponding target document by translat-
ing source chain words into target chain words
using maximum entropy classifiers;
? Incorporate lexical cohesion into the target doc-
ument translation via cohesion models built on
the projected target lexical chains .
We build two lexical chain based cohesion mod-
els. The first model is a count model that rewards a
hypothesis whenever a word in the projected target
lexical chains occur in the hypothesis. As a source
chain word may be translated into many different
target words, we further extend the count model to
a second cohesion model: a probability model that
takes chain word translation probabilities into ac-
count.
We test the two lexical chain based cohesion mod-
els on a hierarchical phrase-based SMT system that
is trained with large-scale Chinese-English bilin-
gual data. Experiment results show that our lexi-
cal chain based cohesion models can achieve sub-
stantial improvements over the baseline. Further-
more, the probability cohesion model is better than
the count model and it also outperforms previous
cohesion models based on lexical cohesion devices
(Xiong et al, 2013).
To the best of our knowledge, this is the first at-
tempt to explore lexical chains for statistical ma-
chine translation. The remainder of this paper is or-
ganized as follows. Section 2 discusses related work
and highlights the differences between our method
and previous work. Section 3 briefly introduces
lexical chains and algorithms that compute lexical
chains. Section 4 elaborates the proposed lexical
chain based framework, including details on source
lexical chain computation, target lexical chain gen-
eration and the two lexical chain based cohesion
models. Section 5 presents our large-scale experi-
ments and results. Finally, we conclude with future
directions in Section 6.
2 Related Work
Recent years have witnessed growing research in-
terests in document-level statistical machine trans-
lation. Such research efforts can be roughly di-
vided into two groups: 1) general document-level
machine translation that does not explore or ex-
plores very little linguistic discourse information;
2) linguistically-motivated document-level machine
translation that incorporates discourse information
such as cohesion and coherence into SMT. Recent
studies (Guillou, 2013; Beigman Klebanov and
Flor, 2013) show that this discourse information is
very important for document-level machine transla-
tion.
General Document-Level Machine Translation
Tiedemann (2010) propose cache-based language
and translation models for document-level machine
translation. These models are built on recently trans-
lated sentences. Following this cache-based ap-
proach, Gong et al (2011) further introduce two
additional caches. They use a static cache to store
bilingual phrases extracted from documents in train-
ing data that are similar to the document being trans-
lated. They also adopt a topic cache with target
language topic words. Xiao et al (2011) study
the translation consistency issue in document-level
machine translation. They use a hard constraint to
consistently translate ambiguous source words into
the most frequent translation options. Ture et al
(2012) soften this consistency constraint by integrat-
ing three counting features into decoder.
Using Lexical Cohesion Devices in Document-
Level SMT Lexical cohesion devices are seman-
tically related words, including word repetition,
synonyms/near-synonyms, hyponyms and so on.
They are also the cohesion-building elements in lex-
ical chains.
Wong and Kit (2012) use lexical cohesion device
based metrics to improve machine translation evalu-
ation at the document level. These metrics measure
the proportion of content words that are used as lex-
ical cohesion devices in machine-generated transla-
tions. Hardmeier et al (2012) propose a document-
wide phrase-based decoder and integrate a semantic
language model into the decoder. They argue that
their semantic language model can capture lexical
cohesion by exploring n-grams that cross sentence
1564
boundaries.
Most recently Xiong et al (2013) integrate
three categories of lexical cohesion devices into
document-level machine translation. They define
three cohesion models based on lexical cohesion de-
vices: a direct reward model, a conditional probabil-
ity model and a mutual information trigger model.
The latter two models measure the strength of lexical
cohesion relation between two lexical items. They
are incorporated into SMT to calculate how appro-
priately lexical cohesion devices are used in doc-
ument translation. As lexical chains capture lexi-
cal cohesion relations among sequences of related
words rather than those only between two words, ex-
periments in Section 5 show that our lexical chain
based probability cohesion model is better than the
lexical cohesion device based trigger model, which
is the best among the three cohesion models pro-
posed by Xiong et al (2013).
Modeling Coherence in Document-Level SMT
In discourse analysis, cohesion is often studied to-
gether with coherence which is another dimension
of the linguistic structure of a text (Barzilay and
Elhadad, 1997). Cohesion is related to the sur-
face structure of a text while coherence is concerned
with the underlying meaning connectedness in a text
(Vasconcellos, 1989). Compared with cohesion, co-
herence is not easy to be detected. Even so, various
models have been proposed to explore coherence for
document summarization and generation (Barzilay
and Lapata, 2008; Louis and Nenkova, 2012). Fol-
lowing this line, Xiong and Zhang (2013) integrate
a topic-based coherence model into document-level
machine translation, where coherence is defined as a
continuous sentence topic transition.
Our lexical chain based cohesion models are also
related to previous work on using word and phrase
sense disambiguation for lexical choice in SMT
(Carpuat and Wu, 2007b; Carpuat and Wu, 2007a;
Chan et al, 2007). The difference is that we use
document-wide lexical chains to build our cohesion
models rather than sentence-level context features.
In our framework, lexical choice is performed to
make the selected words consistent with the lexical
cohesion structure of a document.
Carpuat (2009) explores the principle of one sense
per discourse (Gale et al, 1992) in the context of
SMT and imposes the constraint of one translation
per discourse on document translation. We also
use the one sense per discourse principle to perform
word sense disambiguation on the source side in our
lexical chaining algorithm (See Section 4.1).
3 Background: Lexical Chain and Chain
Computation
Lexical chains are sequences of semantically related
words (Morris and Hirst, 1991). They represent the
lexical cohesion structure of a text. Figure 2 displays
six lexical chains computed from the Chinese news
article shown in Figure 1. Words in these lexical
chains have lexical cohesion relations such as rep-
etition, synonym, which may range over the entire
text. For example, in the lexical chain LC1 of Fig-
ure 2, the same word ?dWgu_? (Germany) repeats
9 times. In the lexical chain LC3, the two words
?z`ngcSi? (president) and ?zhdx[? (chairman) are
synonym words. Generally, a text can have many
different lexical chains, each of which represents a
thread of cohesion through the text.
Several lexical chaining algorithms have been
proposed to compute lexical chains from texts. Nor-
mally they need an ontology to obtain semantic re-
lations between words. Word sense disambiguation
(WSD) is also used to determine the sense of each
word in a text. Generally a lexical chain compu-
tation algorithm completes the following three sub-
tasks:
? Building a representation of a text with a set
of candidate words and assigning semantic re-
lations between the candidate words according
to the ontology;
? Choosing the right sense for each candidate
word via WSD;
? Building chains over the semantically related
and disambiguated candidate words.
These three sub-tasks can be done separately or si-
multaneously.
Morris and Hirst (Morris and Hirst, 1991) de-
fine the first lexical chain computation algorithm
that adopts a greedy strategy to immediately disam-
biguate a word at its first occurrence. This algo-
rithm runs in linear time but suffers from inaccu-
rate disambiguation. Barzilay and Elhadad (Barzi-
lay and Elhadad, 1997) significantly improve WSD
1565
dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma c[zh[
dWgu_ diUnx]n g^ngsZ xuRnbe , qiSn jiRnsh]hu] zhdx[ qZsh[Yr su] de xZlYXr jiRng dRnrYn gRi
g^ngsZ de l[nsh[ z`ngcSi , wWiqZ lie gY yuY , zh[dUo su`ma de j]rYn rWnxuTn jiVrYn wWizh\"
( fTxZnshY b^ Sng diUn ) dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma jZntiRn c[qe tR de zh[we , tR
shu^ , y_uyc tR xiTnrSn bc zUi shaudUo dWgu_ diUnx]n g^ngsZ jiRnsh]hu] de ch^ngfYn x]nrYn ,
c[zh[ sh] tR wWiyZ de xuTnzW"
t_uzZrWn huRny[ng zhYxiUng xuRnbe , dWgu_ diUnx]n g^ngsZ de gdpiUo yZnc\ zUi fTlSnkYfc
gdpiUo jiRoy] sh]chTng shUng zhTng bTifYnzhZsh[yZ y\shUng"
su`ma zUi dWgu_ diUnx]n g^ngsZ b^ Sng z`ngbe zhUokRi jiRnsh]hu] tYbiW hu]y] zh^ng fRbiTo
yZ xiUng shVngm[ng , tR shu^ : 7 w` y\ yUoqic jiRnsh]hu] jiXchc w` de zh[we"8
y_uyc liTng gY yuY hau jiRng jdx[ng dUxuTn , dUn liSnhWzhYngfd zUi m[n diUo zh^ng shVngwUng
luahau , dWgu_ z`ngl\ shZ ruadW s]hb xZwUng zUi gdjiU xiUcua zh] xZn dZ sh[ , dWgu_ diUnx]n
g^ngsZ shebTiwUn m[ng xiTo gdd^ng de zZjZn b]ng wYi xiRoshZ , Wr zhZch[ tR"
dWgu_ diUnx]n gdjiU haulSi hu[ wXn , y\ sh[yZdiTnyZbR ^uyuSn zua sh^u , shUngzhTng
bTifYnzhZbRdiTnwds]"
dWgu_ cSizhYngbe huRny[ng su`ma c[zh[ de juWd]ng"
Figure 1: An example of a Chinese news article (written in pinyin).
LC1: {dWgu_, dWgu_, b^, dWgu_, dWgu_, dWgu_,
dWgu_, b^, dWgu_, dWgu_, dWgu_}
LC2:{jiRnsh]hu], fTxZnshY, jiRnsh]hu], z`ngbe,
jiRnsh]hu], jiRnsh]hu]}
LC3: {z`ngcSi, zhdx[, z`ngcSi, z`ngl\}
LC4: {c[zh[, c[qe, c[zh[, c[zh[}
LC5: {zhTng, xiUcua, shUngzhTng}
LC6: {xuRnbe, xuRnbe, fRbiTo}
Figure 2: Six lexical chains from the example in Figure
1.
accuracy by processing all possible combinations of
word senses in a text to disambiguate words. Un-
fortunately, their algorithm runs slowly in quadratic
time. Galley and Mckeown (2003) present an algo-
rithm that are better than the former two algorithms
both in terms of running efficiency and WSD accu-
racy. They separate the WSD sub-task from the task
of lexical chain building and impose a ?one sense
per discourse? constraint in the WSD step.
4 Translating Documents Using Lexical
Chains
In this section, we describe how we incorporate lex-
ical cohesion into document-level machine transla-
tion using lexical chains. We divide the lexical chain
based document-level machine translation process
into three steps: (1) computing lexical chains for
source documents with a source language ontology,
(2) generating target lexical chains from the com-
puted source lexical chains, and finally (3) incorpo-
rating lexical cohesion encoded in the generated tar-
get lexical chains into document-level translation via
lexical chain based cohesion models. The remainder
of this section will elaborate these three steps.
4.1 Source Lexical Chains Computation
We follow the chain computation algorithm intro-
duced by Galley and McKeown (2003) to build lex-
ical chains on source (Chinese) documents. In the
algorithm, the chaining process includes three steps:
choosing candidate words to build a disambiguation
graph (Galley and McKeown, 2003) for each doc-
ument, disambiguating the candidate words and fi-
nally building lexical chains over the disambiguated
candidate words.
The disambiguation graph can be considered as
a representation of all possible interpretations of its
corresponding text. In the graph, nodes are candi-
date words with different senses and edges between
word senses are weighted according to their seman-
tic relations, such as synonym, hypernym and so on.
We use an extended version of a Chinese thesaurus
Tongyici Cilin (Cilin for short) to define word senses
and semantic relations between senses. The ex-
1566
level 1
level 2
level 3
level 4
level 5
Figure 3: The architecture of the extended Cilin. For sim-
plicity, we only draw a binary tree to represent the hier-
archical structure of Cilin. This doesn?t mean that each
semantic class at level i has only two sub-classes at level
i+ 1. Actually, they have multiple sub-classes.
tended Cilin contains 77,343 Chinese words, which
are organized in a hierarchical structure containing
5 levels as shown in Figure 3. In the 5th level, each
node represents an atomic concept which consists of
a set of synonyms. These atomic concepts are just
like synsets in WordNet. We use them to represent
senses of words in the disambiguation graph.
We select nouns, verbs, abbreviations and idioms
as candidate words for the disambiguation graph.
These words are identified by a Chinese part-to-
speech tagger LTP (Che et al, 2010) in a preprocess-
ing step. In order to build the disambiguation graph,
we first build an array indexed by the atomic con-
cepts of Cilin, then insert a copy of each candidate
word into its all concept (sense) entries in the array.
After that, we create all semantic links among senses
of different candidate words in the disambiguation
graph following Galley and McKeown (2003).
In the second step, we use the principle of one
sense per discourse to perform WSD for each can-
didate word in the disambiguation graph. We sum
the weights of all semantic links under the different
senses of the candidate word in question. The sense
with the highest sum of weights is considered as the
most probable sense for this word. We then assign
this sense to all occurrences of the word in the doc-
ument by adopting the constraint of one sense per
discourse.
Once all candidate words are disambiguated, we
can build lexical chains over these words by remov-
ing all semantic links that connect those unselected
word senses. The six lexical chains shown in Fig-
ure 2 are computed from the Chinese document in
Figure 1 exactly following the algorithm of Galley
and McKeown (2003). The only difference is that
we use Cilin rather than WordNet as the ontology.
4.2 Target Lexical Chains Generation
Since a faithful target document translation should
follow the same cohesion structure as that in its cor-
responding source document, we generate target lex-
ical chains from the computed source lexical chains.
Given a source lexical chain LCs = {s
j
i} where the
ith chain word sji is from the jth sentence of the
source document Ds, we generate a target lexical
chain LCt = {t
j
i} using maximum entropy (Max-
Ent) classifiers. Particularly, we translate a word sji
in the source lexical chain into a target word tji in
the target lexical chain using a corresponding Max-
Ent classifier as follows1.
P (tji |C(s
j
i )) =
exp(
?
k ?kfk(t
j
i , C(s
j
i )))
?
t exp(
?
k ?kfk(t, C(s
j
i )))
(1)
where fk are binary features, ?k are weights of these
features, and C(sji ) is the surrounding context of
chain word sji .
We train one MaxEnt classifier per unique source
chain word. For each classifier, we define two
groups of binary features: 1) the preceding and
succeeding two words of sji in the jth sentence
({w?2, w?1, s
j
i , w+1, w+2}); 2) the preceding and
succeeding one word of sji in the lexical chain LCs
({spi?1, s
j
i , s
q
i+1}). All features are in the following
binary form.
f(tji , C(s
j
i )) =
{
1, if tji = ? and C(s
j
i ).? = ?
0, else
(2)
where the symbol ? is a placeholder for a possible
target word, the symbol? indicates a contextual ele-
ment for the chain word sji (e.g., the preceding word
in the jth sentence or the succeeding word in the
lexical chain LCs), and the symbol ? represents the
value of ?.
Given a source document Ds and its N lexical
chains {LCks }
N
k=1 computed from the document as
1We collect training instances from word-aligned bilingual
data to train the MaxEnt classifier.
1567
described in Section 4.1, we can generate the N
target lexical chains {LCkt }
N
k=1 using our MaxEnt
classifiers. Each target word tji in the target lexi-
cal chain LCkt is the translation of its corresponding
source word sji in the source lexical chain LC
k
s with
the highest probability P (tji |C(s
j
i )) according to Eq.
(1).
As we know, the MaxEnt classifier can gen-
erate multiple translations for each source word.
In order to incorporate these multiple chain word
translations, we can generate a super target lexi-
cal chain LCt from a source lexical chain LCs,
where  is a pre-defined threshold used to se-
lect multiple translations. For example, given a
source lexical chain LCs = {a, b, c}, we can
have the corresponding super target lexical chain
LCt = {{a1t , a
2
t ...}, {b
1
t , b
2
t ...}, {c
1
t , c
2
t ...}}, where
xit is the translation of x with a translation probabil-
ity P (xit|C(x)) ?  according to Eq. (1). Integrat-
ing multiple translations for each source chain word,
we can reduce the error propagation of the MaxEnt
classifier to some extent. Our experiments also con-
firm that the super target lexical chains with multi-
ple translation options for each chain word are better
than the target lexical chains with only one transla-
tion per chain word. Therefore we build our cohe-
sion models based on the super target lexical chains,
which will be described in the next section.
4.3 Lexical Chain Based Cohesion Models
Once we generate the super target lexical chains
{LCkt }
N
k=1 for the target document Dt, we can use
them to provide constraints for the target document
translation. Our key interest is to make the target
document translation TDt as cohesive as possible.
We therefore propose lexical chain based cohesion
models to measure the cohesion of the target docu-
ment translation. The basic idea is to reward a trans-
lation hypothesis if a word from the super target lexi-
cal chains occurs in the hypothesis. According to the
difference in the reward strategy, we have two cohe-
sion models: a count cohesion model and a proba-
bility cohesion model.
Count Cohesion Model Mc(TDt , {LC
k
t }
N
k=1):
This model rewards a translation hypothesis of the
jth sentence in the document whenever a lexical
chain word tji occurs in the hypothesis. The model
maintains a counter and accumulates the counter
when necessary. It is factorized into the sentence
cohesion metric Mc(Tj , {LCkt }
N
k=1), where Tj is
the translation of the jth sentence in the target docu-
ment. Mc(Tj , {LCkt }
N
k=1) is formulated as follows.
Mc(Tj , {LC
k
t }
N
k=1) =
?
w?Tj
?
tji?C
e?(w,t
j
i ) (3)
where C represents {LCkt }
N
k=1, and the ? function
is defined as follows.
?(w, tji ) =
{
1, if tji = w
0, otherwise
(4)
Probability Cohesion Model
Mp(TDt , {LC
k
t }
N
k=1): This model rewards a
translation hypothesis according to the translation
probability of a chain word that occurs in the
hypothesis. The translation probability is computed
by Eq. (1). The model is also factorized into the
sentence cohesion metric Mp(Tj , {LCkt }
N
k=1)
which is formulated as follows.
Mp(Tj ,{LC
k
t }
M
k=1) =
?
w?Tj
?
tji?C
e?(w,t
j
i ) ? P (tji |C(s
j
i )) (5)
where P (tji |C(s
j
i ) is the translation probability com-
puted according to Eq. (1).
4.4 Decoding
The proposed lexical chain based cohesion models
are integrated into the log-linear translation frame-
work of SMT as a cohesion feature. Before translat-
ing a source document, we compute lexical chains
for the source document as described in Section 4.1.
We then generate the super target lexical chains. In
order to efficiently calculate our lexical chain based
cohesion models, we reorganize words in the super
target lexical chains into vectors. We associate each
source sentence Sj a vector to store target lexical
chain words that are to occur in the corresponding
target sentence Tj .
Although we still translate a source document
sentence by sentence, we capture the global cohe-
sion structure of the document via lexical chains and
use the lexical chain based cohesion models to con-
strain word selection in document translation. Fig-
ure 4 shows the architecture of an SMT system with
the lexical chain based cohesion model.
1568
Figure 4: Architecture of an SMT system with the lexical
chain based cohesion model.
5 Experiments
In this section, we conducted a series of experiments
to validate the effectiveness of the proposed lexical
chain based cohesion models for Chinese-to-English
document-level machine translation. We used a hier-
archical phrased-based SMT system (Chiang, 2007)
trained on large-scale data. In particular, we aim at:
? Measuring the impact of the threshold  on the
probability cohesion model and selecting the
best threshold on a development test set.
? Investigating the effect of the two lexical-chain
based cohesion models.
? Comparing our lexical chain based cohesion
models against the previous lexical cohesion
device based models (Xiong et al, 2013).
5.1 Setup
We collected our bilingual training data from
LDC, which includes the corpus LDC2002E18,
LDC2003E07, LDC2003E14, LDC2004E12,
LDC2004T07, LDC2004T08 (Only Hong Kong
News), LDC2005T06 and LDC2005T10. The
collected bilingual training data contains 3.8M
sentence pairs with 96.9M Chinese words and
109.5M English words. We trained a 4-gram
language model on the Xinhua portion of the
English Gigaword corpus (306 million words) via
the SRILM toolkit (Stolcke, 2002) with Kneser-Ney
smoothing.
Training MT05 MT06 MT08
#Doc 103,236 100 79 109
#Sent 2.80M 1,082 1,664 1,357
#Chain 3.52M 1700 2172 1693
#AvgC 35.72 17 27.49 15.53
#AvgW 14.81 5.89 6.89 5.63
Table 1: Statistics of the training, development and test
sets, which show the number of documents (#Doc) and
sentences (#Sent), the number of lexical chains extracted
from the source documents (#Chain), the average number
of lexical chains per document (#AvgC) and the average
number of words per lexical chain (#AvgW).
In order to build the lexical chain based cohesion
models, we selected corpora with document bound-
aries explicitly provided from the bilingual training
data together with the whole Hong Kong parallel
text corpus as the cohesion model training data2. We
show the statistics of these selected corpora in Table
1. They contain 103,236 documents and 2.80M sen-
tences. Averagely, each document consists of 28.4
sentences. From the source documents of the se-
lected corpora, we extract 3.52M lexical chains. On
average, there are 35.72 lexical chains per document
and 14.81 words per lexical chain.
We used the off-the-shelf MaxEnt toolkit3 to train
one MaxEnt classifier per unique source lexical
chain word (61,121 different source chain words in
total). We performed 100 iterations of the L-BFGS
algorithm implemented in the training toolkit for
each chain word with both Gaussian prior and event
cutoff set to 1 to avoid overfitting. After event cutoff,
we have an average of 17.75 different classes (target
translations) per source chain word.
We used the NIST MT05 as the tuning set for the
minimum error rate training (MERT) [Och, 2003],
the NIST MT06 as the development test set and the
MT08 as the final test set. The numbers of doc-
uments/sentences in the NIST MT05, MT06 and
MT08 are 100/1082, 79/1664 and 109/1357 respec-
tively. They contain 17, 27.49 and 15.53 lexical
chains per document respectively.
We used the case-insensitive BLEU-4 (Papineni
2The training data includes LDC2003E14, LDC2004T07,
LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong
Hansards/Laws/News).
3Available at: http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
1569
 MT06
0.05 30.53
0.1 31.64
0.2 31.45
0.3 30.73
0.4 31.01
Table 2: BLEU scores of the probability cohesion
model Mp(TDt , {LC
k
t }
N
k=1) with different values for
the threshold .
et al, 2002) as our evaluation metric. As MERT is
normally instable, we ran the tuning process three
times for all our experiments and presented the av-
erage BLEU scores on the three MERT runs as sug-
gested by Clark et al(2011).
5.2 Setting the Threshold 
As the two lexical chain based cohesion models are
built on the super target lexical chains that are asso-
ciated with a parameter , we need to tune the thresh-
old parameter  on the development test set NIST
MT06. We conducted a group of experiments using
the probability cohesion model defined in Eq. (5)
to find the best threshold. Experiment results are
shown in Table 2.
If we set the threshold too small (e.g., 0.05), the
super target lexical chains may contain too many
noisy words that are not the translations of source
lexical chain words, which may jeopardise the qual-
ity of the super target lexical chains. The cohesion
model built on these noisy super target lexical chains
may select incorrect words rather than the proper
lexical chain words. On the other hand, if we set the
threshold too large (e.g., 0.3 or 0.4), we may take
the risk of not selecting the appropriate chain word
translations into the super target lexical chains. It
seems that the best threshold is 0.1 as we obtained
the highest BLEU score 31.64 on the NIST MT06
with this threshold. Therefore we set the threshold 
to 0.1 in all experiments thereafter.
5.3 Effect of the Count and Probability
Cohesion Model
After we found the best threshold, we carried out ex-
periments to test the effect of the two lexical chain
based cohesion models: the count and probability
cohesion model. We compared them against the
System MT06 MT08 Avg
Baseline 30.43 23.32 26.88
LexChainCount(top 1) 30.46 23.52 26.99
LexChainCount 30.79 23.34 27.07
LexChainProb 31.64 24.54 28.09
Table 3: Effects of the lexical chain based count and
probability cohesion models. LexChainCount: the count
model defined in Eq. (3). LexChainProb: the probability
model defined in Eq. (5).
baseline system that does not integrate any lexical
chain information. We also compared the count co-
hesion model (LexChainCount(top1)) built on the
target lexical chains where each target chain word is
the best translation of its corresponding source lex-
ical chain word according to Eq. (1). Experiment
results are shown in Table 3.
From Table 3, we can observe that
? Our lexical chain based cohesion models are
able to substantially improve the translation
quality in terms of BLEU score. We achieve
an average improvement of up to 1.21 BLEU
points over the baseline on the two test sets
MT06 and MT08.
? The count cohesion model built on the super
target lexical chains is better than that based
on the target lexical chains only with top one
translations (27.07 vs. 26.99). This shows
the advantage of the super target lexical chains
{LCkt }
N
k=1 over the standard target lexical chi-
ans {LCkt }
N
k=1.
? Finally, the probability cohesion model is much
better than the count cohesion model (28.09
vs. 27.07). This suggests that we should take
into account chain word translation probabili-
ties when we reward hypotheses where target
lexical chain words occur.
5.4 Lexical Chains vs. Lexical Cohesion
Devices
As we have mentioned in Section 2, lexical cohe-
sion devices can be also used to build lexical cohe-
sion models to capture lexical cohesion relations in a
text. We therefore want to compare our lexical chain
based cohesion models with the lexical cohesion de-
vice based cohesion models.
1570
System MT06 MT08 Avg
Baseline 30.43 23.32 26.88
LexDeviceTrigger 31.35 24.11 27.73
LexChainProb 31.64 24.54 28.09
Table 4: The lexical chain based probability cohesion
model (LexChainProb) vs. the lexical cohesion device
based trigger model (LexDeviceTrigger).
We re-implemented the mutual information trig-
ger model that is the best lexical cohesion model
based on lexical cohesion devices among the three
models proposed by Xiong et al (2013). The mu-
tual information trigger model measures the associ-
ation strength of two lexical cohesion items x and y
in a lexical cohesion relation xRy. In the model, it
is required that x occurs in a sentence preceding the
sentence where y occurs and that the two items have
a lexical cohesion relation such as word repetition,
synonym. The model treats x as the trigger and y as
the triggered item. The mutual information between
the trigger x and the triggered item y estimates how
possible y will occur given x is mentioned in a text.
The comparison results are reported in Table 4.
Our lexical chain based probability cohesion model
outperforms the lexical cohesion device based trig-
ger model by 0.36 BLEU points. The reason for this
superiority of our cohesion model over the trigger
model may be that the former model captures lex-
ical cohesion relations among sequences of words
through lexical chains while the latter model cap-
tures lexical cohesion relations only between two re-
lated words.
6 Conclusions
We have presented two lexical chain based cohesion
models that incorporate the lexical cohesion struc-
ture of a text into document-level machine transla-
tion. We project the lexical chains of a source docu-
ment to the corresponding target document by trans-
lating each word in each source lexical chain into
their counterparts via MaxEnt classifiers. The pro-
jected target lexical chains provide a representation
of the lexical cohesion structure of the target doc-
ument that is to be generated. We build two co-
hesion models based on the projected target lexi-
cal chains: a count model that rewards a hypothesis
according to the time of occurrence of target lexi-
cal chain words in the hypothesis and a probability
model that further takes translation probabilities into
account when rewarding hypotheses. These two co-
hesion models are used to constrain word selection
for document translation so that the generated doc-
ument is consistent with the projected lexical cohe-
sion structure.
We have integrated the two proposed cohesion
models into a hierarchical phrase-based SMT sys-
tem. Experiment results on large-scale data validate
that
? The lexical chain based cohesion models are
able to substantially improve translation qual-
ity in terms of BLEU.
? The probability cohesion model is better than
the count cohesion model.
? The lexical chain based probability cohesion
model is better than the previous mutual infor-
mation trigger model that adopts lexical cohe-
sion devices to capture lexical cohesion rela-
tions between two related words.
As we mentioned in Section 2, cohesion is closely
connected to coherence. It provides a surface indi-
cator for coherence identification (Barzilay and El-
hadad, 1997). In the future, we would like to use
lexical chains to identify coherence and incorporate
both cohesion and coherence into document-level
machine translation.
References
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In In Proceedings
of the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Beata Beigman Klebanov and Michael Flor. 2013. As-
sociative texture is lost in translation. In Proceedings
of the Workshop on Discourse in Machine Translation,
pages 27?32, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense disam-
biguation for statistical machine translation. In Pro-
ceedings of the 11th Conference on Theoretical and
1571
Methodological Issues in Machine Translation, pages
43?52.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 61?72, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado, June.
Association for Computational Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33?40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
a chinese language technology platform. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Demonstrations, COLING ?10,
pages 13?16, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176?181, Port-
land, Oregon, USA, June.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, Harriman, NY, February.
Michel Galley and Kathleen McKeown. 2003. Improv-
ing word sense disambiguation in lexical chaining. In
Proceedings of the 18th international joint conference
on Artificial intelligence, IJCAI?03, pages 1486?1488,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 909?919, Edinburgh, Scotland, UK., July.
Liane Guillou. 2013. Analysing lexical consistency in
translation. In Proceedings of the Workshop on Dis-
course in Machine Translation, pages 10?18, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion in
English. London: Longman.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiedemann.
2012. Document-wide decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1179?1190, Jeju Island,
Korea, July.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157?1168, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator of
the structure of text. Comput. Linguist., 17(1):21?48,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
M.A. Stairmand. 1996. A computational analysis of
lexical cohesion with applications in information re-
trieval. UMIST.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado, USA, September.
Jo?rg Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Process-
ing, pages 8?15, Uppsala, Sweden, July.
Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012.
Encouraging consistent translation choices. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 417?
426, Montre?al, Canada, June.
Muriel Vasconcellos. 1989. Cohesion and coherence in
the presentation of machine translation products. In
James E.Alatis, editor, Geogetown University Round
Table on Languages and Linguistics 1989, pages 89?
105, Washington, D.C. Georgetown University Press.
Billy T. M. Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lexi-
cal cohesion to document level. In Proceedings of the
1572
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1060?1068, Jeju Island,
Korea, July.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In Proceedings of the 2011 MT sum-
mit XIII, pages 131?138, Xiamen, China, September.
Deyi Xiong and Min Zhang. 2013. A topic-based co-
herence model for statistical machine translation. In
Proceedings of the Twenty-Seventh AAAI Conference
on Artificial Intelligence (AAAI-13), Bellevue, Wash-
ington, USA, July.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third International Joint Conference on
Artificial Intelligence (IJCAI-13), Beijing, China, Au-
gust.
1573
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546?556,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Modeling Term Translation for Document-informed Machine Translation
Fandong Meng
1, 2
Deyi Xiong
3
Wenbin Jiang
1, 2
Qun Liu
4, 1
1
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
2
University of Chinese Academy of Sciences
{mengfandong,jiangwenbin,liuqun}@ict.ac.cn
3
School of Computer Science and Technology, Soochow University
dyxiong@suda.edu.cn
4
Centre for Next Generation Localisation, Dublin City University
Abstract
Term translation is of great importance for
statistical machine translation (SMT), es-
pecially document-informed SMT. In this
paper, we investigate three issues of term
translation in the context of document-
informed SMT and propose three cor-
responding models: (a) a term trans-
lation disambiguation model which se-
lects desirable translations for terms in the
source language with domain information,
(b) a term translation consistency model
that encourages consistent translations for
terms with a high strength of translation
consistency throughout a document, and
(c) a term bracketing model that rewards
translation hypotheses where bracketable
source terms are translated as a whole
unit. We integrate the three models into
hierarchical phrase-based SMT and eval-
uate their effectiveness on NIST Chinese-
English translation tasks with large-scale
training data. Experiment results show
that all three models can achieve sig-
nificant improvements over the baseline.
Additionally, we can obtain a further
improvement when combining the three
models.
1 Introduction
A term is a linguistic expression that is used as
the designation of a defined concept in a language
(ISO 1087). As terms convey concepts of a text,
term translation becomes crucial when the text is
translated from its original language to another
language. The translations of terms are often af-
fected by the domain in which terms are used and
the context that surrounds terms (Vasconcellos et
al., 2001). In this paper, we study domain-specific
and context-sensitive term translation for SMT.
In order to achieve this goal, we focus on three
issues of term translation: 1) translation ambigu-
ity, 2) translation consistency and 3) bracketing.
First, term translation ambiguity is related to trans-
lations of the same term in different domains. A
source language term may have different transla-
tions when it occurs in different domains. Second,
translation consistency is about consistent trans-
lations for terms that occur in the same document.
Usually, it is undesirable to translate the same term
in different ways as it occurs in different parts of
a document. Finally, bracketing concerns whether
a multi-word term is bracketable during transla-
tion. Normally, a multi-word term is translated as
a whole unit into a contiguous target string.
We study these three issues in the context
of document-informed SMT. We use document-
informed information to disambiguate term trans-
lations in different documents and maintain con-
sistent translations for terms that occur in the same
document. We propose three different models for
term translation that attempt to address the three
issues mentioned above. In particular,
? Term Translation Disambiguation Model: In
this model, we condition the translations of
terms in different documents on correspond-
ing per-document topic distributions. In do-
ing so, we enable the decoder to favor trans-
lation hypotheses with domain-specific term
translations.
? Term Translation Consistency Model: This
model encourages the same terms with a high
strength of translation consistency that occur
in different parts of a document to be trans-
lated in a consistent fashion. We calculate
the translation consistency strength of a term
based on the topic distribution of the docu-
ments where the term occurs in this model.
? Term Bracketing Model: We use the brack-
eting model to reward translation hypothe-
546
ses where bracketable multi-word terms are
translated as a whole unit.
We integrate the three models into hierarchical
phrase-based SMT (Chiang, 2007). Large-scale
experiment results show that they are all able to
achieve significant improvements of up to 0.89
BLEU points over the baseline. When simulta-
neously integrating the three models into SMT,
we can gain a further improvement, which outper-
forms the baseline by up to 1.16 BLEU points.
In the remainder of this paper, we begin with
a brief overview of related work in Section 2,
and bilingual term extraction in Section 3. We
then elaborate the proposed three models for term
translation in Section 4. Next, we conduct experi-
ments to validate the effectiveness of the proposed
models in Section 5. Finally, we conclude and pro-
vide directions for future work in Section 6.
2 Related Work
In this section, we briefly introduce related work
and highlight the differences between our work
and previous studies.
As we approach term translation disambigua-
tion and consistency via topic modeling, our mod-
els are related to previous work that explores the
topic model (Blei et al., 2003) for machine trans-
lation (Zhao and Xing, 2006; Su et al., 2012;
Xiao et al., 2012; Eidelman et al., 2012). Zhao
and Xing (2006) employ three models that enable
word alignment process to leverage topical con-
tents of document-pairs with topic model. Su et al.
(2012) establish the relationship between out-of-
domain bilingual corpus and in-domain monolin-
gual corpora via topic mapping and phrase-topic
distribution probability estimation for translation
model adaptation. Xiao et al. (2012) propose a
topic similarity model for rule selection. Eidel-
man et al. (2012) use topic models to adapt lexical
weighting probabilities dynamically during trans-
lation. In these studies, the topic model is not used
to address the issues of term translation mentioned
in Section 1.
Our work is also related to document-level
SMT in that we use document-informed informa-
tion for term translation. Tiedemann (2010) pro-
pose cache-based language and translation mod-
els, which are built on recently translated sen-
tences. Gong et al. (2011) extend this by further
introducing two additional caches. They employ
a static cache to store bilingual phrases extracted
from documents in training data that are similar to
the document being translated and a topic cache
with target language topic words. Recently we
have also witnessed efforts that model lexical co-
hesion (Hardmeier et al., 2012; Wong and Kit,
2012; Xiong et al., 2013a; Xiong et al., 2013b)
as well as coherence (Xiong and Zhang, 2013)
for document-level SMT. Hasler et al. (2014a)
use topic models to learn document-level transla-
tion probabilities. Hasler et al. (2014b) use topic-
adapted model to improve lexical selection. The
significant difference between our work and these
studies is that term translation has not been inves-
tigated in these document-level SMT models.
Itagaki and Aikawa (2008) employ bilingual
term bank as a dictionary for machine-aided trans-
lation. Ren et al. (2009) propose a binary feature
to indicate whether a bilingual phrase contains a
term pair. Pinis and Skadins (2012) investigate that
bilingual terms are important for domain adapta-
tion of machine translation. These studies do not
focus on the three issues of term translation as
discussed in Section 1. Furthermore, domain and
document-informed information is not used to as-
sist term translation.
Itagaki et al. (2007) propose a statistical method
to calculate translation consistency for terms with
explicit domain information. Partially inspired
by their study, we introduce a term translation
consistency metric with document-informed infor-
mation. Furthermore, we integrate the proposed
term translation consistency model into an actual
SMT system, which has not been done by Itagaki
et al. (2007). Ture et al. (2012) use IR-inspired
tf-idf scores to encourage consistent translation
choice. Guillou (2013) investigates what kind of
words should be translated consistently. Term
translation consistency has not been investigated
in these studies.
Our term bracketing model is also related
to Xiong et al. (2009)?s syntax-driven bracket-
ing model for phrase-based translation, which pre-
dicts whether a phrase is bracketable or not using
rich syntactic constraints. The difference is that
we construct the model with automatically created
bilingual term bank and do not depend on any syn-
tactic knowledge.
3 Bilingual Term Extraction
Bilingual term extraction is to extract terms from
two languages with the purpose of creating or ex-
547
tending a bilingual term bank, which in turn can
be used to improve other tasks such as information
retrieval and machine translation. In this paper, we
want to automatically build a bilingual term bank
so that we can model term translation to improve
translation quality of SMT. Our interest is to ex-
tract multi-word terms.
Currently, there are mainly two strategies to
conduct bilingual term extraction from parallel
corpora. One of them is to extract term candi-
dates separately for each language according to
monolingual term metrics, such as C-value/NC-
value (Frantzi et al., 1998; Vu et al., 2008), or
other common cooccurrence measures such as
Log-Likelihood Ratio, Dice coefficient and Point-
wise Mutual Information (Daille, 1996; Piao et
al., 2006). The extracted monolingual terms are
then paired together (Hjelm, 2007; Fan et al.,
2009; Ren et al., 2009). The other strategy is to
align words and word sequences that are transla-
tion equivalents in parallel corpora and then clas-
sify them into terms and non-terms (Merkel and
Foo, 2007; Lefever et al., 2009; Bouamor et al.,
2012). In this paper, we adopt the first strategy.
In particular, for each sentence pair, we collect all
source phrases which are terms and find aligned
target phrases for them via word alignments. If
the target side is also a term, we store the source
and target term as a term pair.
We conduct monolingual term extraction using
the C-value/NC-value metric and Log-Likelihood
Ratio (LLR) measure respectively. We then com-
bine terms extracted according to the two metrics
mentioned above. For the C-value/NC-value met-
ric based term extraction, we implement it in the
same way as described in Frantzi et al. (1998).
This extraction method recognizes linguistic pat-
terns (mainly noun phrases) listed as follows.
((Adj|Noun)
+
|((Adj|Noun)
?
(NounPrep)
?
)(Adj|Noun)
?
)Noun
It captures the linguistic structures of terms. For
the LLR metric based term extraction, we imple-
ment it according to Daille (1996), who estimate
the propensity of two words to appear together as a
multi-word expression. We then adopt LLR-based
hierarchical reducing algorithm proposed by Ren
et al. (2009) to extract terms with arbitrary lengths.
Since the C-value/NC-value metric based extrac-
tion method can obtain terms in strict linguistic
patterns while the LLR measure based method ex-
tracts more flexible terms, these two methods are
complementary to each other. Therefore, we use
these two methods to extract monolingual multi-
word terms and then combine the extracted terms.
4 Models
This section presents the three models of term
translation. They are the term translation dis-
ambiguation model, term translation consistency
model and term bracketing model respectively.
4.1 Term Translation Disambiguation Model
The most straightforward way to disambiguate
term translations in different domains is to cal-
culate the conditional translation probability of
a term given domain information. We use the
topic distribution of a document obtained by a
topic model to represent the domain information
of the document. Since Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) is the most widely-
used topic model, we exploit it for inferring topic
distributions of documents. Xiao et al. (2012)
proposed a topic similarity model for rule selec-
tion. Different from their work, we take an eas-
ier strategy that estimates topic-conditioned term
translation probabilities rather than rule-topic dis-
tributions. This makes our model easily scalable
on large training data.
With the bilingual term bank created from the
training data, we calculate the source-to-target
term translation probability for each term pair con-
ditioned on the topic distribution of the source
document where the source term occurs. We main-
tain a K-dimension (K is the number of topics)
vector for each term pair. The k-th component
p(t
e
|t
f
, z = k) measures the conditional transla-
tion probability from source term t
f
to target term
t
e
given the topic k.
We calculate p(t
e
|t
f
, z = k) via maximum
likelihood estimation with counts from training
data. When the source part of a bilingual term
pair occurs in a document D with topic distribu-
tion p(z|D) estimated via LDA tool, we collect
an instance (t
f
, t
e
, p(z|D), c), where c is the frac-
tion count of the instance as described in Chiang
(2007). After collection, we get a set of instances
I = {(t
f
, t
e
, p(z|D), c)}with different document-
topic distributions for each bilingual term pair. Us-
ing these instances, we calculate the probability
548
p(t
e
|t
f
, z = k) as follows:
p(t
e
|t
f
, z = k)
=
?
i?I,i.t
f
=t
f
,i.t
e
=t
e
i.c ? p(z = k|D)
?
i?I,i.t
f
=t
f
i.c ? p(z = k|D)
(1)
We associate each extracted term pair in our
bilingual term bank with its corresponding topic-
conditioned translation probabilities estimated in
the Eq. (1). When translating sentences of docu-
ment D
?
, we first get the topic distribution of D
?
using LDA tool. Given a sentence which contains
T terms {t
f
i
}
T
1
in D
?
, our term translation disam-
biguation model TermDis can be denoted as
TermDis =
T
?
i=1
P
d
(t
e
i
|t
f
i
, D
?
) (2)
where the conditional source-to-target term trans-
lation probability P
d
(t
e
i
|t
f
i
, D
?
) given the docu-
ment D
?
is formulated as follows:
P
d
(t
e
i
|t
f
i
, D
?
)
=
K
?
k=1
p(t
e
i
|t
f
i
, z = k) ? p(z = k|D
?
) (3)
Whenever a source term t
f
i
is translated into t
e
i
,
we check whether the pair of t
f
i
and its translation
t
e
i
can be found in our bilingual term bank. If it
can be found, we calculate the conditional transla-
tion probability from t
f
i
to t
e
i
given the document
D
?
according to Eq. (3).
The term translation disambiguation model is
integrated into the log-linear model of SMT as a
feature. Its weight is tuned via minimum error rate
training (MERT) (Och, 2003). Through the fea-
ture, we can enable the decoder to favor translation
hypotheses that contain target term translations ap-
propriate for the domain represented by the topic
distribution of the corresponding document.
4.2 Term Translation Consistency Model
The term translation disambiguation model helps
the decoder select appropriate translations for
terms that are in accord with their domains. Yet
another translation issue related to the domain-
specific term translation is to what extent a term
should be translated consistently given the domain
where it occurs. Term translation consistency in-
dicates the translation stability that a source term
is translated into the same target term (Itagaki et
al., 2007). When translating a source term, if the
translation consistency strength of the source term
is high, we should take the corresponding target
term as the translation for it. Otherwise, we may
need to create a new translation for it according to
its context. In particular, we want to enable the
decoder to choose between: 1) translating a given
source term into the extracted corresponding tar-
get term or 2) translating it in another way accord-
ing to the strength of its translation consistency.
In doing so, we can encourage consistent transla-
tions for terms with a high translation consistency
strength throughout a document.
Our term translation consistency model can ex-
actly measure the strength of term translation con-
sistency in a document. Since the essential com-
ponent of our term translation consistency model
is the translation consistency strength of the source
term estimated under the topic distribution, we de-
scribe how to calculate it before introducing the
whole model.
With the bilingual term bank created from
training data, we first group each source term
and all its corresponding target terms into a 2-
tuple G?t
f
, Set(t
e
)?, where t
f
is the source term
and Set(t
e
) is the set of t
f
?s corresponding tar-
get terms. We maintain a K-dimension (K is
the number of topics) vector for each 2-tuple
G?t
f
, Set(t
e
)?. The k-th component measures the
translation consistency strength cons(t
f
, k) of the
source term t
f
given the topic k.
We calculate cons(t
f
, k) for each
G?t
f
, Set(t
e
)? with counts from training data as
follows:
cons(t
f
, k) =
M
?
m=1
N
m
?
n=1
(
q
mn
? p(k|m)
Q
k
)
2
(4)
Q
k
=
M
?
m=1
N
m
?
n=1
q
mn
? p(k|m) (5)
where M is the number of documents in which
the source term t
f
occurs, N
m
is the number of
unique corresponding term translations of t
f
in the
mth document, q
mn
is the frequency of the nth
translation of t
f
in the mth document, p(k|m) is
the conditional probability of the mth document
over topic k, and Q
k
is the normalization factor.
All translations of t
f
are from Set(t
e
). We adapt
Itagaki et al. (2007)?s translation consistency met-
ric for terms to our topic-based translation consis-
tency measure in the Eq. (4). This equation cal-
culates the translation consistency strength of the
source term t
f
given the topic k according to the
distribution of t
f
?s translations in each document
549
where they occur. According to Eq. (4), the trans-
lation consistency strength is a score between 0
and 1. If a source term only occurs in a document
and all its translations are the same, the translation
consistency strength of this term is 1.
We reorganize our bilingual term bank into a
list of 2-tuples G?t
f
, Set(t
e
)?s, each of which is
associated with a K-dimension vector storing the
topic-conditioned translation consistency strength
calculated in the Eq. (4). When translating sen-
tences of document D, we first get the topic dis-
tribution of D via LDA tool. Given a sentence
which contains T terms {t
f
i
}
T
1
in D, our term
translation consistency model TermCons can be
denoted as
TermCons =
T
?
i=1
exp(S
c
(t
f
i
|D)) (6)
where the strength of translation consistency for
t
f
i
given the document D is formulated as fol-
lows:
S
c
(t
f
i
|D) = log(
K
?
k=1
cons(t
f
i
, k) ? p(k|D)) (7)
During decoding, whenever a hypothesis just
translates a source term t
f
i
into t
e
, we check
whether the translation t
e
can be found in Set(t
e
)
of t
f
i
from the reorganized bilingual term bank. If
it can be found, we calculate the strength of trans-
lation consistency for t
f
i
given the document D
according to Eq. (7) and take it as a soft con-
straint. If the S
c
(t
f
i
|D) of t
f
i
is high, the decoder
should translate t
f
i
into the extracted correspond-
ing target terms. Otherwise, the decoder will se-
lect translations from outside of Set(t
e
) for t
f
i
. In
doing so, we encourage terms to be translated in
a topic-dependent consistency pattern in the test
data similar to that in the training data so that we
can control the translation consistency of terms in
the test data.
The term translation consistency model is also
integrated into the log-linear model of SMT as a
feature. Through the feature, we can enable the
decoder to translate terms with a high translation
consistency in a document into corresponding tar-
get terms from our bilingual term bank rather than
other translations in a consistent fashion.
4.3 Term Bracketing Model
The term translation disambiguation model and
consistency model concern the term translation ac-
curacy with domain information. We further pro-
pose a term bracketing model to guarantee the in-
tegrality of term translation. Xiong et al. (2009)
proposed a syntax-driven bracketing model for
phrase-based translation, which predicts whether
a phrase is bracketable or not using rich syntac-
tic constraints. If a source phrase remains con-
tiguous after translation, they refer to this type of
phrase as bracketable phrase, otherwise unbrack-
etable phrase. For multi-word terms, it is also
desirable to be bracketable since a source term
should be translated as a whole unit and its trans-
lation should be contiguous.
In this paper, we adapt Xiong et al. (2009)?s
bracketing approach to term translation and build
a classifier to measure the probability that a source
term should be translated in a bracketable man-
ner. For all source parts of the extracted bilingual
term bank, we find their target counterparts in the
word-aligned training data. If the corresponding
target counterpart remains contiguous, we take the
source term as a bracketable instance, otherwise
an unbracketable instance. With these bracketable
and unbracketable instances, we train a maximum
entropy binary classifier to predict bracketable (b)
probability of a given source term t
f
within par-
ticular contexts c(t
f
). The binary classifier is for-
mulated as follows:
P
b
(b|c(t
f
)) =
exp(
?
j
?
j
h
j
(b, c(t
f
)))
?
b
?
exp(
?
j
?
j
h
j
(b
?
, c(t
f
)))
(8)
where h
j
? {0, 1} is a binary feature function and
?
j
is the weight of h
j
. We use the following fea-
tures: 1) the word sequence of the source term, 2)
the first word of the source term, 3) the last word
of the source term, 4) the preceding word of the
first word of the source term, 5) the succeeding
word of the last word of the source term, and 6)
the number of words in the source term.
Given a source sentence which contains T terms
{t
f
i
}
T
1
, our term bracketing model TermBrack
can be denoted as
TermBrack =
T
?
i=1
P
b
(b|c(t
f
i
)) (9)
Whenever a hypothesis just covers a source term
t
f
i
, we calculate the bracketable probability of t
f
i
according to Eq. (8).
The term bracketing model is integrated into the
log-linear model of SMT as a feature. Through the
feature, we want the decoder to translate source
terms with a high bracketable probability as a
whole unit.
550
Source Target D M
F?angy`u X`?t?ong defence mechanisms
F?angy`u X`?t?ong defence systems
F?angy`u X`?t?ong defense programmes 470 56
F?angy`u X`?t?ong prevention systems
... ...
Zh`anlu`e D?aod`an F?angy`u X`?t?ong strategic missile defense system 7 0
Table 1: Examples of bilingual terms extracted from the training data. ?D? means the total number of
documents in which the corresponding source term occurs and ?M? denotes the number of documents in
which the corresponding source term is translated into different target terms. The source side is Chinese
Pinyin. To save space, we do not list all the 23 different translations of the source term ?F?angy`u X`?t?ong?.
5 Experiments
In this section, we conducted experiments to an-
swer the following three questions.
1. Are our term translation disambiguation,
consistency and bracketing models able to
improve translation quality in BLEU?
2. Does the combination of the three models
provide further improvements?
3. To what extent do the proposed models affect
the translations of test sets?
5.1 Setup
Our training data consist of 4.28M sentence pairs
extracted from LDC
1
data with document bound-
aries explicitly provided. The bilingual training
data contain 67,752 documents, 124.8M Chinese
words and 140.3M English words. We chose
NIST MT05 as the MERT (Och, 2003) tuning set,
NIST MT06 as the development test set, and NIST
MT08 as the final test set. The numbers of docu-
ments/sentences in NIST MT05, MT06 and MT08
are 100/1082, 79/1664 and 109/1357 respectively.
The word alignments were obtained by running
GIZA++ (Och and Ney, 2003) on the corpora in
both directions and using the ?grow-diag-final-
and? balance strategy (Koehn et al., 2003). We
adopted SRI Language Modeling Toolkit (Stol-
cke and others, 2002) to train a 4-gram language
model with modified Kneser-Ney smoothing on
the Xinhua portion of the English Gigaword cor-
pus. For the topic model, we used the open source
1
The corpora include LDC2003E07, LDC2003E14,
LDC2004T07, LDC2004E12, LDC2005E83, LDC2005T06,
LDC2005T10, LDC2006E24, LDC2006E34, LDC2006E85,
LDC2006E92, LDC2007E87, LDC2007E101,
LDC2008E40, LDC2008E56, LDC2009E16 and
LDC2009E95.
LDA tool GibbsLDA++
2
with the default setting
for training and inference. We performed 100 it-
erations of the L-BFGS algorithm implemented in
the MaxEnt toolkit
3
with both Gaussian prior and
event cutoff set to 1 to train the term bracketing
prediction model (Section 4.3).
We performed part-of-speech tagging for mono-
lingual term extraction (C-value/NC-vaule method
in Section 3) of the source and target languages
with the Stanford NLP toolkit
4
. The bilingual term
bank was extracted based on the following param-
eter settings of term extraction methods. Empiri-
cally, we set the maximum length of a term to 6
words
5
. For both the C-value/NC-value and LLR-
based extraction methods, we set the context win-
dow size to 5 words, which is a widely-used set-
ting in previous work. And we set C-value/NC-
value score threshold to 0 and LLR score threshold
to 10 according to the training corpora.
We used the case-insensitive 4-gram BLEU
6
as
our evaluation metric. In order to alleviate the im-
pact of the instability of MERT (Och, 2003), we
ran it three times for all our experiments and pre-
sented the average BLEU scores on the three runs
following the suggestion by Clark et al. (2011).
We used an in-house hierarchical phrase-based
decoder to verify our proposed models. Although
the decoder translates a document in a sentence-
by-sentence fashion, it incorporates document-
informed information for sentence translation via
the proposed term translation models trained on
documents.
2
http://sourceforge.net/projects/gibbslda/
3
http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html
4
http://nlp.stanford.edu/software/tagger.shtml
5
We determine the maximum length of a term by testing
{5, 6, 7, 8} in our preliminary experiments. We find that
length 6 produces a slightly better performance than other
values.
6
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
551
Zh?y?u W?iyu?nhu? Ch?ngyu?n C?i  K? C?nji? W?iyu?nhu? Sh?ny? ?
Only members of the commission shall take part  in the commission deliberations .
?
He these proposals
T? Ji?ng Zh?xi? Ji?ny? Ji?o Y?u Y? G? B?zh?ngj? W?iyu?nhu? Sh?ny?
submit for approval to a committee of ministers .
(a)
(b)
Figure 1: An example of unbracketable source term in the training data. In (a), ?W?eiyu?anhu`? Sh?eny`?? is
bracketable while in (b) it is unbracketable. The solid lines connect bilingual phrases. The source side is
Chinese Pinyin.
5.2 Bilingual Term Bank
Before reporting the results of the proposed mod-
els, we provide some statistics of the bilingual
term bank extracted from the training data.
According to our statistics, about 1.29M bilin-
gual terms are extracted from the training data.
65.07% of the sentence pairs contain bilingual
terms in the training data. And on average, a
source term has about 1.70 different translations.
These statistics indicate that terms are frequently
used in real-world data and that a source term can
be translated into different target terms.
We also present some examples of bilingual
terms extracted from the training data in Table 1.
Accordingly, we show the total number of doc-
uments in which the corresponding source term
occurs and the number of documents in which
the corresponding source term is translated into
different target terms. The source term ?F?angy`u
X`?t?ong? has 23 different translations in total. They
are distributed in 470 documents in the training
data. In 414 documents, ?F?angy`u X`?t?ong? has
only one single translation. However, in the other
56 documents it has different translations. This
indicates that ?F?angy`u X`?t?ong? is not consistently
translated in these 56 documents. Different from
this, the source term ?Zh`anlu`e D?aod`an F?angy`u
X`?t?ong? only has one translation. And it is trans-
lated consistently in all 7 documents where it oc-
curs. In fact, according to our statistics, there are
about 5.19% source terms whose translations are
not consistent even in the same document.
These examples and statistics suggest 1) that
source terms have domain-specific translations
and 2) that source terms are not necessarily trans-
lated in a consistent manner even in the same doc-
ument. These are exactly the reasons why we pro-
pose the term translation disambiguation and con-
sistency model based on domain information rep-
resented by topic distributions.
Actually, 36.13% of the source terms are not
necessarily translated into target strings as a whole
unit. We show an example of such terms in Fig-
ure 1. In Figure 1-(a), ?W?eiyu?anhu`? Sh?eny`?? is a
term, and is translated into ?commission deliber-
ations? as a whole unit. Therefore ?W?eiyu?anhu`?
Sh?eny`?? is bracketable in this sentence. How-
ever, in Figure 1-(b), ?W?eiyu?anhu`?? and ?Sh?eny`??
are translated separately. Therefore ?W?eiyu?anhu`?
Sh?eny`?? is an unbracketable term in this sentence.
This is the reason why we propose a bracketing
model to predict whether a source term is brack-
etable or not.
5.3 Effect of the Proposed Models
In this section, we validate the effectiveness of the
proposed term translation disambiguation model,
consistency model and bracketing model respec-
tively. In addition to the traditional hiero (Chi-
ang, 2007) system, we also compare against the
?CountFeat? method in Ren et al. (2009) who use
a binary feature to indicate whether a bilingual
phrase contains a term pair. Although Ren et al.
(2009)?s experiments are conducted in a phrase-
based system, the idea can be easily applied to a
hierarchical phrase-based system.
We carried out experiments to investigate the ef-
fect of the term translation disambiguation model
(Dis-Model) and report the results in Table 2. In
order to find the topic number setting with which
our model has the best performance, we ran exper-
iments using the MT06 as the development test set.
From Table 2, we observe that the Dis-Model ob-
tains steady improvements over the baseline and
?CountFeat? method with the topic number K
552
Models MT06 MT08 Avg
Baseline 32.43 24.14 28.29
CountFeat 32.77 24.29 28.53
Dis-Model
K = 50 32.94* 24.53 28.74
K = 100 33.10* 24.57 28.84
K = 150 33.16* 24.67* 28.92
K = 200 33.08* 24.55 28.81
Cons-Model
K = 50 33.09* 24.59 28.84
K = 100 33.13* 24.74* 28.94
K = 150 33.32*+ 24.84*+ 29.08
K = 200 33.02* 24.73* 28.88
Brack-Model 33.09* 24.66* 28.88
Combined-Model 33.59*+ 24.99*+ 29.29
Table 2: BLEU-4 scores (%) of the term translation disambiguation model (Dis-Model), the term transla-
tion consistency model (Cons-Model), the term bracketing model (Brack-Model), and the combination of
the three models, on the development test set MT06 and the final test set MT08. K ? {50, 100, 150, 200}
which is the number of topics for the Dis-Model and the Cons-Model. ?Combined-Model? is the combi-
nation of the three single modes with topic number 150 for the Dis-Model and the Cons-Model. ?Base-
line? is the traditional hierarchical phrase-based system. ?CountFeat? is the method that adds a counting
feature to reward translation hypotheses containing bilingual term pairs. The ?*? and ?+? denote that the
results are significantly (Clark et al., 2011) better than those of the baseline system and the CountFeat
method respectively (p<0.01).
ranging from 50 to 150. However, when we set K
to 200, the performance drops. The highest BLEU
scores 33.16 and 24.67 are obtained at the topic
setting K = 150. In fact, our Dis-Model gains
higher performance in BLEU than both the tradi-
tional hiero baseline and the ?CountFeat? method
with all topic settings. The ?CountFeat? method
rewards translation hypotheses containing bilin-
gual term pairs. However it does not explore any
domain information. Our Dis-Model incorporates
domain information to conduct translation disam-
biguation and achieves higher performance. When
the topic number is set to 150, we gain the high-
est BLEU score, which is higher than that of the
baseline by 0.73 and 0.53 BLEU points on MT06
and MT08, respectively. The final gain over the
baseline is on average 0.63 BLEU points.
We conducted the second group of experiments
to study whether the term translation consistency
model (Cons-Model) is able to improve the per-
formance in BLEU, as well as to investigate the
impact of different topic numbers on the Cons-
Model. Results are shown in Table 2, from which
we observe the similar phenomena to what we
have found in the Dis-Model. Our Cons-Model
gains higher BLEU scores than the baseline sys-
tem and the ?CountFeat? method with all topic
settings. Setting topic number to 150 achieves the
highest BLEU score, which is higher than base-
line by 0.89 BLEU points and 0.70 BLEU points
on MT06 and MT08 respectively, and on average
0.79 BLEU points.
We also conducted experiments to verify the ef-
fectiveness of the term bracketing model (Brack-
Model), which conducts bracketing prediction for
source terms. Results in Table 2 show that
our Brack-Model gains higher BLEU scores than
those of the baseline system and the ?CountFeat?
method. The final gain of Brack-Model over the
baseline is 0.66 BLEU points and 0.52 points on
MT06 and MT08 respectively, and on average
0.59 BLEU points.
5.4 Combination of the Three Models
As shown in the previous subsection, the term
translation disambiguation model, consistency
model and bracketing model substantially outper-
form the baseline. Now, we investigate whether
using these three models simultaneously can lead
to further improvements. The last row in Table 2
shows that the combination of the three models
(Combined-Model) achieves higher BLEU score
than all single models, when we set the topic num-
ber to 150 for the term translation disambigua-
tion model and consistency model. The final gain
553
Models MT06 MT08
Best-Dis-Model 30.89 30.14
Best-Cons-Model 38.04 36.70
Brack-Model 60.46 55.78
Combined-Model 54.39 50.85
Table 3: Percentage (%) of 1-best translations
which are generated by the Combined-Model and
the three single models with best settings on the
development test set MT06 and the final test set
MT08. The topic number is 150 for Best-Dis-
Model and Best-Cons-Model.
of the Combined-Model over the baseline is 1.16
BLEU points and 0.85 points on MT06 and MT08
respectively, and on average 1.00 BLEU points.
5.5 Analysis
In this section, we investigate to what extent the
proposed models affect the translations of test sets.
In Table 3, we show the percentage of 1-best trans-
lations affected by the Combined-Model and the
three single models with best settings on test sets
MT06 and MT08. For single models, if the corre-
sponding feature (disambiguation, consistency or
bracketing) is activated in the 1-best derivation,
the corresponding model has impact on the 1-best
translation. For the Combined-Model, if any of
the corresponding features is activated in the 1-
best derivation, the Combined-Model affects the
1-best translation.
From Table 3, we can see that 1-best transla-
tions of source sentences affected by any of the
proposed models account for a high proportion
(30%?60%) on both MT06 and MT08. This in-
dicates that all proposed models play an important
role in the translation of both test sets. Among
the three proposed models, the Brack-Model is the
one that affects the largest number of 1-best trans-
lations in both test sets. And the percentage is
60.46% and 55.78% on MT06 and MT08 respec-
tively. The Brack-Model only considers source
terms during decoding, while the Dis-Model and
Cons-Model need to match both source and target
terms. The Brack-Model is more likely to be acti-
vated. Hence the percentage of 1-best translations
affected by this model is higher than those of the
other two models. Since we only investigate the
1-best translations generated by the Combined-
Model and single models, the translations gener-
ated by some single models (e.g., Brack-Model)
may not be generated by the Combined-Model.
Therefore it is hard to say that the numbers of 1-
best translations affected by the Combined-Model
must be greater than those of single models.
6 Conclusion and Future Work
We have studied the three issues of term trans-
lation and proposed three different term trans-
lation models for document-informed SMT. The
term translation disambiguation model enables
the decoder to favor the most suitable domain-
specific translations with domain information for
source terms. The term translation consistency
model encourages the decoder to translate source
terms with a high domain translation consistency
strength into target terms rather than other new
strings. Finally, the term bracketing model re-
wards hypotheses that translate bracketable terms
into continuous target strings as a whole unit.
We integrate the three models into a hierarchical
phrase-based SMT system
7
and evaluate their ef-
fectiveness on the NIST Chinese-English transla-
tion task with large-scale training data. Experi-
ment results show that all three models achieve
significant improvements over the baseline. Ad-
ditionally, combining the three models achieves a
further improvement. For future work, we would
like to evaluate our models on term translation
across a range of different domains.
Acknowledgments
This work was supported by National Key Tech-
nology R&D Program (No. 2012BAH39B03) and
CAS Action Plan for the Development of Western
China (No. KGZD-EW-501). Deyi Xiong?s work
was supported by Natural Science Foundation of
Jiangsu Province (Grant No. BK20140355). Qun
Liu?s work was partially supported by Science
Foundation Ireland (Grant No. 07/CE/I1142) as
part of the CNGL at Dublin City University. Sin-
cere thanks to the anonymous reviewers for their
thorough reviewing and valuable suggestions. The
corresponding author of this paper, according to
the meaning given to this role by University of
Chinese Academy of Sciences and Soochow Uni-
versity, is Deyi Xiong.
7
Our models are not limited to hierarchical phrase-based
SMT. They can be easily applied to other SMT formalisms,
such as phrase- and syntax-based SMT.
554
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Houda Bouamor, Aur?elien Max, and Anne Vilnat.
2012. Validation of sub-sentential paraphrases ac-
quired from parallel monolingual corpora. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 716?725. Association for Computa-
tional Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, pages 176?181.
B?eatrice Daille. 1996. Study and implementation of
combined techniques for automatic extraction of ter-
minology. Journal of The balancing act: Combin-
ing symbolic and statistical approaches to language,
1:49?66.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers-Volume 2, pages 115?119.
Association for Computational Linguistics.
Xiaorong Fan, Nobuyuki Shimizu, and Hiroshi Nak-
agawa. 2009. Automatic extraction of bilin-
gual terms from a chinese-japanese parallel corpus.
In Proceedings of the 3rd International Universal
Communication Symposium, pages 41?45. ACM.
Katerina T Frantzi, Sophia Ananiadou, and Junichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Re-
search and Advanced Technology for Digital Li-
braries, pages 585?604. Springer.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 909?919.
Liane Guillou. 2013. Analysing lexical consistency
in translation. In Proceedings of the Workshop on
Discourse in Machine Translation, pages 10?18.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014a. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, Gothenburg, Sweden.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2014b.
Dynamic topic adaptation for smt using distribu-
tional profiles. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, pages 445?
456, Baltimore, Maryland, USA, June. Association
for Computational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of 16th Nordic Conference of Computational Lin-
guistics Nodalida, pages 97?104.
Masaki Itagaki and Takako Aikawa. 2008. Post-mt
term swapper: Supplementing a statistical machine
translation system with a user dictionary. In LREC.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology trans-
lation consistency with statistical method. Proceed-
ings of MT summit XI, pages 269?274.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54.
Els Lefever, Lieve Macken, and Veronique Hoste.
2009. Language-independent bilingual terminology
extraction from a multilingual parallel corpus. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 496?504.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of 16th Nordic Conference
of Computational Linguistics Nodalida, pages 349?
354.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167.
Scott SL Piao, Guangfan Sun, Paul Rayson, and
Qi Yuan. 2006. Automatic extraction of chi-
nese multiword expressions with a statistical tool.
In Workshop on Multi-word-expressions in a Mul-
tilingual Context held in conjunction with the 11th
EACL, Trento, Italy, pages 17?24.
555
Pinis and Skadins. 2012. Mt adaptation for under-
resourced domains?what works and what not. In
Human Language Technologies?The Baltic Perspec-
tive: Proceedings of the Fifth International Confer-
ence Baltic HLT 2012, volume 247, page 176. IOS
Press.
Zhixiang Ren, Yajuan L?u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, pages 47?54.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901?904.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 459?468.
J?org Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing, pages 8?15.
Ferhan Ture, Douglas W Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426. Association for Computational Lin-
guistics.
Muriel Vasconcellos, Brian Avey, Claudia Gdaniec,
Laurie Gerber, Marjorie Le?on, and Teruko Mita-
mura. 2001. Terminology and machine translation.
Handbook of Terminology Management, 2:697?723.
Thuy Vu, Ai Ti Aw, and Min Zhang. 2008. Term ex-
traction through unithood and termhood unification.
In Proceedings of the third international joint con-
ference on natural language processing.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 1060?1068.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for hi-
erarchical phrase-based translation. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume 1,
pages 750?758.
Deyi Xiong and Min Zhang. 2013. A topic-based
coherence model for statistical machine translation.
In Proceedings of the Twenty-Seventh AAAI Confer-
ence on Artificial Intelligence (AAAI-13), Bellevue,
Washington, USA, July.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L?u,
and Qun Liu. 2013a. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, pages 2183?2189. AAAI
Press.
Deyi Xiong, Yang Ding, Min Zhang, and Chew Lim
Tan. 2013b. Lexical chain based cohesion mod-
els for document-level statistical machine transla-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1563??1573.
Bing Zhao and Eric P Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 969?976.
556
Linguistically Annotated Reordering:
Evaluation and Analysis
Deyi Xiong?
Institute for Infocomm Research
Min Zhang??
Institute for Infocomm Research
Aiti Aw?
Institute for Infocomm Research
Haizhou Li?
Institute for Infocomm Research
Linguistic knowledge plays an important role in phrase movement in statistical machine trans-
lation. To efficiently incorporate linguistic knowledge into phrase reordering, we propose a new
approach: Linguistically Annotated Reordering (LAR). In LAR, we build hard hierarchical skele-
tons and inject soft linguistic knowledge from source parse trees to nodes of hard skeletons during
translation. The experimental results on large-scale training data show that LAR is comparable
to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very compet-
itive lexicalized reordering approach. When combined with BWR, LAR provides complementary
information for phrase reordering, which collectively improves the BLEU score significantly.
To further understand the contribution of linguistic knowledge in LAR to phrase reordering,
we introduce a syntax-based analysis method to automatically detect constituent movement in
both reference and system translations, and summarize syntactic reordering patterns that are
captured by reordering models. With the proposed analysis method, we conduct a comparative
analysis that not only provides the insight into how linguistic knowledge affects phrase move-
ment but also reveals new challenges in phrase reordering.
1. Introduction
The phrase-based approach is a widely accepted formalism in statistical machine trans-
lation (SMT). It segments the source sentence into a sequence of phrases (not necessarily
syntactic phrases), then translates and reorders these phrases in the target. The reason
for the popularity of phrasal SMT is its capability of non-compositional translations and
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: dyxiong@i2r.a-star.edu.sg.
?? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: mzhang@i2r.a-star.edu.sg.
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: aaiti@i2r.a-star.edu.sg.
? 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: hli@i2r.a-star.edu.sg.
Submission received: 24 October 2008; revised submission received: 12 March 2010; accepted for publication:
21 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
local word reorderings within phrases. Unfortunately, reordering at the phrase level is
still problematic for phrasal SMT. The default distortion-based reordering model simply
penalizes phrase movement according to the jump distance, without considering any
linguistic contexts (morphological, lexical, or syntactic) around phrases.
In order to utilize lexical information for phrase reordering, Tillman (2004) and
Koehn et al (2005) propose lexicalized reordering models which directly condition
phrase movement on phrases themselves. One problem with such lexicalized reordering
models is that they are restricted only to reorderings of phrases seen in training data.
To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words
of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering
evidence. Although these lexicalized reordering models significantly outperform the
distortion-based reordering model as reported, only using lexical information (e.g.,
boundary words) is not adequate to move phrases to appropriate positions.
Consider the following Chinese example with its English translation:
[VP [PP(while)(develop)(related)(legislation)] [VP [VV
(consider)] [NP [DNP [NP(this) (referendum)] [DEG(of)]] [NP
(results)]]]]1
consider the results of this referendum while developing related legislation
In this example, boundary words and are able to decide that the translation of the
PP phrase ... should be postponed until some phrase that succeeds it is translated.
But they cannot provide further information about exactly which succeeding phrase
should be translated first. If high-level linguistic knowledge, such as the syntactic
context VP?PP VP, is given, the position of the PP phrase can be easily determined
since the pre-verbal modifier PP in Chinese is frequently translated into a post-verbal
counterpart in English.
In this article, we focus on linguistically motivated phrase reordering, which in-
tegrates high-level linguistic knowledge in phrase reordering. We adopt a two-step
strategy. In the first step, we establish a hierarchical skeleton in phrasal SMT by in-
corporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In
the second step, we inject soft linguistic information into nodes of the skeleton.
There are two significant advantages to using BTG in phrasal SMT. First, BTG is able
to generate hierarchical structures.2 This not only enhances phrasal SMT?s capability
for hierarchical and long-distance reordering but also establishes a platform for phrasal
SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is
restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight
or inverted) of nodes in any binary branching structure, it is broadly verified that the
ITG constraint has good coverage of word reorderings on various language pairs (Wu,
Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable
task.
After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject
soft linguistic information into the nodes of the skeleton. We annotate each BTG node
1 In this article, we use Penn Chinese Treebank phrase labels (Xue et al 2000).
2 Chiang (2005) also generates hierarchical structures in phrasal SMT. One difference is that Chiang?s
hierarchical grammar is lexicon-sensitive because the model requires at least one pair of aligned words in
each rule except for the ?glue rule.? The other difference is that his grammar allows multiple
nonterminals. These two differences make Chiang?s grammar more expressive than the BTG but at the
cost of learning a larger model.
536
Xiong et al Linguistically Annotated Reordering
with syntactic and lexical elements by projecting the source parse tree onto the BTG
binary tree. The challenge, of course, is that BTG hierarchical structures are not always
aligned with the linguistic structures in the source language parse tree. To address this
issue, we propose an annotation algorithm. The algorithm is able to label any BTG
nodes during decoding with very little overhead, regardless of whether the BTG nodes
are aligned with syntactic constituent nodes in the source parse tree. The annotated
linguistic elements are then used to guide phrase reordering under the ITG constraint.
We call this two-step phrase reordering strategy linguistically annotated reorder-
ing (LAR) (Xiong et al 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reorder-
ing strategy based on BTG. However, they use boundary words as reordering features
at the second step. To distinguish this from our work, we call their approach boundary
word?based reordering (BWR). LAR and BWR can be considered as two reordering
variants for BTG-based phrasal SMT, which have similar training procedures. Further-
more, they can be combined.
We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al 2002).
The BLEU scores show that LAR is comparable to BWR and significantly improves
phrase reordering when combined with BWR.
We want to further study what happens when we combine BWR with LAR. In
particular, we want to investigate to what extent the integrated linguistic knowledge
(from LAR) changes phrase movement in an actual SMT system, and in what direction
the change takes place. The investigations will enable us to have a better understanding
of the relationship between phrase movement and linguistic context, and therefore to
explore linguistic knowledge more effectively in phrasal SMT.
Because syntactic constituents are often moved together across languages during
translation (Fox 2002), we particularly study how linguistic knowledge affects syntactic
constituent movement. To that end, we introduce a syntax-based analysis method. We
parse source sentences, and align the parse trees with reference translations as well as
system translations. We then summarize syntactic reordering patterns using context-
free grammar (CFG) rules from the obtained tree-to-string alignments. The extracted
reordering patterns clearly show the trace of syntactic constituent movement in both
reference translations and system translations.
With the proposed analysis method, we analyze the combination of BWR and LAR
vs. BWR alone. There are essentially three issues that are addressed in this syntax-based
comparative analysis.
1. The first issue concerns syntactic constituent movement in human/
machine translations. Fox (2002) investigates syntactic constituent
movement in human translations. We study syntactic constituent
movement in both human translations and machine translations that are
generated by an actual SMT system and compare them.
2. The second issue concerns the change of phrase movement after rich
linguistic knowledge is integrated into phrase reordering. To gain a better
insight into this issue, we study phrase movement patterns for 13 specific
syntactic constituents.
3. The last issue concerns which constituents remain difficult to reorder even
though rich linguistic knowledge is employed.
The rest of the article is structured as follows. Section 2 introduces background
information about BTG-based phrasal SMT and phrase reordering under the ITG
537
Computational Linguistics Volume 36, Number 3
constraint. Section 3 describes algorithms which extract training instances for reorder-
ing models of BWR and LAR. Section 4 introduces the BWR model as our baseline
reordering model. Section 5 describes LAR and the combination of LAR with BWR.
Section 6 elaborates the syntax-based analysis method. Section 7 reports our evaluation
results on large-scale data. Section 8 demonstrates our analysis results and addresses the
various issues discussed above. Section 9 discusses related work. And finally, Section 10
summarizes our main conclusions.
2. Background
2.1 BTG-Based Phrasal SMT
We establish a unified framework for BTG-based phrasal SMT in this section. There
are two kinds of rules in BTG, lexical rules (denoted as rl) and merging rules (denoted
as rm):3
rl : Ap ? x/y
rm : Ap ? [Al, Ar]|?Al, Ar? (1)
A lexical rule translates a source phrase x into a target phrase y and generates a leaf
node A in the BTG tree. Merging rules combine left and right neighboring phrases Al
and Ar into a larger phrase Ap in an order o ? {straight, inverted}. In this article, we use
?[]? to denote a straight order and ???? an inverted order.
We define a BTG derivation D as a sequence of independent applications of lexical
and merging rules (D = ?rl1..nl , r
m
1..nm?). Given a source sentence, the decoding task of
BTG-based SMT is to find a best derivation, which yields the best translation.4
We assign a probability to each rule using a log-linear model with different features
and corresponding weights ?, then multiply them to obtain P(D). To keep in line with
the common understanding of standard phrasal SMT (Koehn, Och, and Marcu 2003),
here we re-organize these features into a translation model (PT), a reordering model
(PR), and a target language model (PL) as follows:
P(D) = PT(r
l
1..nl
) ? PR(rm1..nm )
?R ? PL(e)?L ? exp(|e|)?w (2)
where exp(|e|) is the word penalty.
The translation model is defined as
PT(r
l
1..nl
) =
nl
?
i=1
P(rli)
P(rl) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3 ? plex(y|x)?4 ? exp(1)?5 (3)
where p(?) represents the phrase translation probabilities in both directions, plex(?) de-
notes the lexical translation probabilities in both directions, and exp(1) is the phrase
penalty.
3 The subscripts l, r, p of A do not mean that we categorize A into three different nonterminals. We use them
to represent the left node, right node, and parent node.
4 In this article, we use c to denote a source sentence and e a target sentence.
538
Xiong et al Linguistically Annotated Reordering
Similarly, the reordering model is defined on the merging rules as follows:
PR(r
m
1..nm ) =
nm
?
i=1
P(rmi ) (4)
One of the most important and challenging tasks in building a BTG-based phrasal SMT
system is to define P(rm).
2.2 Reordering Under the ITG Constraint
Under the ITG constraint, three nodes {Al, Ar, Ap} are involved when we consider the
order o between the two children {Al, Ar} in any binary subtrees. Therefore it is natural
to define the ITG reordering P(rm) as a function as follows:
P(rm) = f (Al, Ar, Ap, o) (5)
where o ? {straight, inverted}.
Based on this function, various reordering models are built according to different
assumptions. For example, the flat reordering model in the original BTG (Wu 1996)
assigns prior probabilities for the straight and inverted order assuming the order is
highly related to the properties of language pairs. It is formulated as
P(rm) =
{
ps, o = straight
1 ? ps, o = inverted
(6)
Supposing French and English are the source and target language, respectively, the
value of ps can be set as high as 0.8 to prefer monotone orientations because the two
languages have similar word orders in most cases.
The main problem of the flat reordering model is also the problem of the standard
distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic
contexts. To be context-dependent, the ITG reordering might directly model the condi-
tional probability P(o|Al, Ar). This probability could be calculated using the maximum
likelihood estimate (MLE) by taking counts from training data, in the manner of the
lexicalized reordering model (Tillman 2004; Koehn et al 2005):
P(o|Al, Ar) =
Count(o, Al, Ar)
Count(Al, Ar)
(7)
Unfortunately this lexicalized reordering method usually leads to a serious data sparse-
ness problem under the ITG constraint because Al and Ar become larger and larger due
to the merging rules, and are finally unseen in the training data.
To avoid the data sparseness problem yet be contextually informative, attributes
of Al and Ar, instead of nodes themselves, are used as reordering evidence in a new
perspective of the ITG reordering (Xiong, Liu, and Lin 2006). The new perspective treats
the ITG reordering as a binary-classification problem where the possible order straight
or inverted between two children nodes is the target class which the reordering model
predicts given Al, Ar, and Ap.
539
Computational Linguistics Volume 36, Number 3
3. Reordering Example Extraction
Because we consider the ITG reordering as a classification problem, we need to obtain
training instances to build a classifier. Here we refer to a training instance as a reorder-
ing example, which is formally defined as a triple of (o, bl, br) where bl and br are two
neighboring blocks and o ? {straight, inverted} is the order between them.
The block is a pair of aligned source phrase and target phrase
b = (ci2i1 , e
j2
j1
) (8)
where b must be consistent with the word alignment M
?(i, j) ? M, i1 ? i ? i2 ? j1 ? j ? j2 (9)
By this, we require that no words inside the source phrase ci2i1 are aligned to words
outside the target phrase ej2j1 and that no words outside the source phrase are aligned to
words inside the target phrase. This definition is similar to that of the bilingual phrase
except that there is no length limitation over blocks. Figure 1 shows a word alignment
matrix between a Chinese sentence and English sentence. In the matrix, each block can
be represented as a rectangle, for example, blocks (c44, e
4
4), (c
5
4, e
5
4), (c
7
4, e
9
4) in red rectangles,
and (c32, e
3
3), (c
3
1, e
3
1) in blue rectangles.
In this section, we discuss two algorithms for extracting reordering examples from
word-aligned bilingual data. The first algorithm AExtractor (described in Section 3.1)
extracts reordering examples directly from word alignments by extending the bilin-
gual phrase extraction algorithm. The second algorithm TExtractor (described in Sec-
tion 3.2) extracts reordering examples from BTG-style trees which are built from word
alignments.
Figure 1
A word alignment matrix between a Chinese sentence and English sentence. Bold dots represent
junctions which connect two neighboring blocks. Red and blue rectangles are blocks which are
connected by junction J2.
540
Xiong et al Linguistically Annotated Reordering
3.1 AExtractor: Extracting Reordering Examples from Word Alignments
Before we describe this algorithm, we introduce the concept of junction in the word
alignment matrix. We define a junction as a vertex shared by two neighboring blocks.
There are two types of junctions: a straight junction, which connects two neighboring
blocks in a straight order (e.g., black dots J1 ? J4 in Figure 1) and an inverted junction,
which connects two neighboring blocks in an inverted order (e.g., the red dot J5 in
Figure 1).
The algorithm for AExtractor is shown in Figure 2. This completes three sub-tasks
as follows.
1. Find blocks (lines 4 and 5). This is similar to the standard phrase extraction
algorithm (Och 2002) except that we find blocks with arbitrary length.
2. Detect junctions and store blocks in the arrays of detected junctions (lines 7
and 8). Junctions that are included the current block can be easily detected
by looking at the previous and next blocks. A junction can connect
multiple blocks on its left and right sides. For example, the second
junction J2 in Figure 1 connects two blocks on the left side and three blocks
on the right side. To store these blocks, we maintain two arrays (left and
right) for each junction.
3. Select block pairs from each detected junction as reordering examples
(lines 12?16). This is the most challenging task for AExtractor. Because a
junction may have n blocks on its left side and m blocks on its right side,
we will obtain nm reordering examples if we enumerate all block pairs.
This will quickly increase the number of reordering examples, especially
Figure 2
AExtractor.
541
Computational Linguistics Volume 36, Number 3
those with the straight order. To keep the number of reordering examples
tractable, we define various selection rules r to heuristically select special
block pairs as reordering examples.
We define four selection rules as follows.
1. strINV: We select the smallest blocks (in terms of the target length) for
straight junctions, and the largest blocks for inverted junctions. Take the
straight junction J2 in Figure 1 as an example, the extracted reordering
example is (straight, | five,| flights).
2. STRinv: We select the largest blocks (in terms of the target length) for
straight junctions, and the smallest blocks for inverted junctions. Still
taking the straight junction J2 as an example, this time the extracted
reordering example is (straight, | The last five,|
flights all fail due to accidents).
3. RANDOM: For any junction, we randomly select one block pair from its
arrays.
4. COMBO: For each junction, we first select two block pairs using selection
rule strINV and STRinv. If there are unselected blocks, we randomly select
one block pair from the remaining blocks.
3.2 TExtractor: Extracting Reordering Examples from BTG-Style Trees
A potential problem for AExtractor is caused by the use of heuristic selection rules:
keeping some block pairs as reordering examples while abandoning other block pairs.
The kept block pairs are not necessarily the best training instances for tuning an ITG
order predictor. To avoid this problem we can extract reordering examples from the
BTG trees of sentence pairs. Reordering examples extracted in this way are naturally
suitable for BTG order prediction.
There are various ways to build BTG trees over sentence pairs. One can use BTG to
produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu
(1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models
BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse
trees and a better reordering model is therefore induced from the extracted reordering
examples. Using the better reordering model, the bilingual sentences are parsed again.
This procedure is run iteratively until no performance gain is obtained in terms of
translation or parsing accuracy. Formally, we can use expectation-maximization (EM)
training in this procedure. In the expectation step, we first estimate the likelihood of all
BTG trees of sentence pairs with the current BTG model. Then we extract reordering
examples and collect counts for them, weighted with the probability of the BTG tree
where they occur. In the maximization step, we can train a more accurate reordering
model with updated reordering examples. Unfortunately, this method is at high com-
putational cost.
Instead, here we adopt a less expensive alternative method to produce BTG trees
over sentence pairs. Supposing we have word alignments produced by GIZA++, we
then use the shift-reduce algorithm (SRA) introduced by Zhang, Gildea, and Chiang
(2008) to decompose word alignments into hierarchical trees. The SRA can guarantee
that each node is a bilingual phrase in the decomposition tree. If the fan-out of a node
is larger than two, we binarize it from left to right: for two neighboring child nodes, if
542
Xiong et al Linguistically Annotated Reordering
they are also neighboring on both the source and target sides, we combine them and
create a new node to dominate them. In this way, we can transform the decomposition
tree into a BTG-style tree. Note that not all multi-branching nodes can be binarized. We
extract reordering examples only from binary nodes.
Figure 3 shows the BTG-style tree which is built from the word alignment in Figure 1
according to the method mentioned here. From this tree, we can easily extract four re-
ordering examples in a straight order and one reordering example in an inverted order.
4. Boundary Word-Based Reordering
Following the binary-classification perspective of the ITG reordering, Xiong, Liu, and
Lin (2006) propose a reordering model which exploits the maximum entropy (MaxEnt)
classifier for BTG order prediction
PRb (r
m) = P?(o|Al, Ar, Ap) =
exp(
?
i ?ihi(o, Al, Ar, Ap))
?
o? exp(
?
i ?ihi(o
?, Al, Ar, Ap))
(10)
where the functions hi ? {0, 1} are reordering features and the ?i are the weights of these
features.
Xiong, Liu, and Lin (2006) define reordering features using the boundary words of
the source/target sides of both children {Al, Ar}. Supposing that we have a reordering
example (inverted, 7 15 | on July 15, | held its presidential and
parliament elections), leftmost/rightmost source words {, 15,,} and target
words {on, 15, held, elections} will be extracted as boundary words. Each boundary word
will form a reordering feature as follows
hi(o, Al, Ar, Ap) =
{
1, fn = bval, o = inverted
0, otherwise
where fn denotes the feature name, and bval is the corresponding boundary word.
There are two reasons why boundary words are used as important clues for
reordering:
1. Phrases frequently cohere across languages (Fox 2002). In cohesive phrase
movement, boundary words directly interact with the external contexts of
Figure 3
The BTG-style tree built from the word alignment in Figure 1. We use ([i, j], [p, q]) to denote a tree
node, where i, j and p, q are the beginning and ending indices in the source and target language,
respectively.
543
Computational Linguistics Volume 36, Number 3
phrases. This suggests that boundary words might contain information for
phrase reordering.
2. The quantitative analysis in Xiong, Liu, and Lin (2006, page 525) further
shows that boundary words indeed contain information for order
prediction.
To train a BWR model, we follow three steps. First, we extract reordering examples
from word-aligned bilingual data as described in the last section, then generate reorder-
ing features using boundary words from the reordering examples, and finally estimate
feature weights.
5. Linguistically Annotated Reordering
In order to employ more linguistic knowledge in the ITG reordering, we annotate each
BTG node involved in reordering using linguistic elements from the source-side parse
trees. The linguistic elements include: (1) the head word hw, (2) the part-of-speech (POS)
tag ht of the head word, and (3) its syntactic category sc. In this section, we describe the
annotation algorithm and the LAR model, as well as the combination of LAR and BWR.
5.1 Annotation Algorithm
There are two steps to annotating a BTG node using source-side parse tree information:
(1) determining the sequence on the source side which is exactly covered by the node,
then (2) annotating the sequence according to the source-side parse tree. If the sequence
is exactly covered by a single subtree in the source-side parse tree, it is called a syntactic
sequence, otherwise it is a non-syntactic sequence. One of the challenges in this an-
notation is that phrases (BTG nodes) do not always cover syntactic sequences; in other
words, they are not always aligned to constituent nodes in the source-side tree. To solve
this problem, we generate a pseudo head word and composite category which consists
of the syntactic categories of three relevant constituents for the non-syntactic sequences.
In this way, our annotation is capable of labelling both syntactic and non-syntactic
phrases and therefore providing linguistic information for any phrase reordering.
The annotation algorithm is shown in Figure 4. For a syntactic sequence, the an-
notation is trivial. Annotation elements directly come from the subtree that covers the
sequence exactly. For a non-syntactic sequence, the process is more complicated. Firstly,
we need to locate the smallest subtree c? covering the sequence (line 6). Secondly, we
try to identify the head word/tag of the sequence (lines 7?12) by using its head word
directly if it is within the sequence. Otherwise, the word within the sequence which is
nearest to hw will be assigned as the head word of the sequence. Finally, we determine
the composite category of the sequence (lines 13?15), which is formulated as L-C-R.
L/R refers to the syntactic category of the left/right boundary node of s, which is the
highest leftmost/rightmost sub-node of c? not overlapping the sequence. If there is no
such boundary node (the sequence s is exactly aligned to the left/right boundary of c?),
L/R will be set to NULL. C is the syntactic category of c?. L, R, and C together describe
the external syntactic context of s. The composite category we define for non-syntactic
phrases is similar to the CCG-style category in Zollmann, Venugopal, and Vogel (2008).
Figure 5 shows a syntactic parse tree for a Chinese sentence, with the head word
annotated for each internal node. Some sample annotations are given in Table 1.
544
Xiong et al Linguistically Annotated Reordering
Figure 4
The Annotation Algorithm.
Figure 5
A syntactic parse tree with the head word annotated for each internal node. The superscripts on
leaf nodes denote their surface positions from left to right.
5.2 Reordering Model
The linguistically annotated reordering model PRa is a MaxEnt-based classification
model, which can be formulated as
PRa (r
m) = p?(o|A
ap
p , A
al
l , A
ar
r ) =
exp(
?
i ?ihi(o, A
ap
p , A
al
l , A
ar
r ))
?
o? exp(
?
i ?ihi(o
?, A
ap
p , A
al
l , A
ar
r ))
(11)
where the feature functions hi ? {0, 1} are defined using annotated linguistic elements
of each BTG node. Here we use the superscripts al, ar, and ap to stress that the BTG nodes
are linguistically annotated.
545
Computational Linguistics Volume 36, Number 3
Table 1
Annotation samples according to the tree shown in Figure 5.
sequence hw ht sc
?1, 2?  NN NULL-NP-NN
?2, 3?  NN NP
?2, 4?  VV NP-IP-NP
?3, 4?  VV NP-IP-NP
hw/ht = the head word/tag, respectively; sc = syntactic category.
Each merging rule involves three nodes (A
ap
p , A
al
l , A
ar
r ) and each node has three
linguistic elements (hw, ht, sc). Therefore, the model has nine features in total. Taking
the left node Aall as an example, the model could use its head word w as a feature as
follows:
hi(o, A
ap
p , A
al
l , A
ar
r ) =
{
1, Aall .hw = w, o = straight
0, otherwise
Training an LAR model also takes three steps. Firstly, we extract annotated reorder-
ing examples from source-side parsed, word-aligned bilingual data using the reordering
example extraction algorithm and the annotation algorithm. We then generate features
using the linguistic elements of these examples. Finally we tune feature weights to build
the MaxEnt model.
5.3 Combining LAR and BWR
LAR and BWR can be combined at two different levels:
1. Feature level. Because both LAR and BWR are trained under the
maximum entropy principle, we can combine linguistically annotated
features from LAR and boundary word features from BWR together and
train a single MaxEnt model. We call this method All-in-One combination.
2. Model level. We can also train two reordering models separately and
integrate them into BTG-based SMT
P(D) = PT(rl1..nl ) ?PRb (r
m
1..nm )
?Rb ?
PRa (r
m
1..nm )
?Ra ?PL(e)?L ? exp(|e|)?w (12)
where PRb is the BWR reordering model and PRa is the LAR reordering
model. We call this combination BWR+LAR.
We will empirically compare these two combination methods in Section 7.4.
6. A New Syntax-Based Reordering Analysis Method
In order to understand the influence of linguistic knowledge on phrase reordering, we
propose a syntax-based method to analyze phrase reordering. In this analysis method,
546
Xiong et al Linguistically Annotated Reordering
we leverage the alignments between source-side parse trees and reference/system
translations to summarize syntactic reordering patterns and calculate syntax-based
measures of precision and recall for each syntactic constituent.
6.1 Overview
The alignment between a source parse tree and a target string is a collection of rela-
tionships between parse tree nodes and their corresponding target spans.5 A syntactic
reordering pattern (SRP) is defined as
?? ? ?1...?n ? [i1]...[in]?
The first part of an SRP is a CFG structure on the source side and the second part
[i1]...[in] indicates the order of target spans ?T1 ...?
T
n of nonterminals ?1...?n on the target
side.6
Let?s take the VP structure VP ? PP1VP2 as an example to explain how the pre-
cision and recall can be obtained. On the target side, the order of PPT1 and VP
T
2
might be [1][2] or [2][1]. Therefore we have two syntactic reordering patterns for this
structure:
?VP ? PP1VP2 ? [1][2]? and ?VP ? PP1VP2 ? [2][1]?
Suppose that the two reordering patterns occur a times in the alignments between
source parse trees and reference translations, b times in the alignments between source
parse trees and system translations, and c times in both alignments. Then the reordering
precision/recall for this structure is c/b and c/a, respectively. We can further calculate
the F1-score as 2 ? c/(a + b). These syntax-based metrics intuitively show how well the
reordering model can reorder this structure. By summarizing all reordering patterns of
all constituents, we can obtain an overall precision, recall, and F1-score for the tested
reordering model.
This new syntax-based analysis for reordering is motivated in part by recent work
which transforms the order of nodes in the source-side parse tree before translation
(Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Li et al 2007; Wang,
Collins, and Koehn 2007). Here we focus on the order transformation of syntactic con-
stituents performed by reordering models during translation. In addition to aligning
parse trees with reference translations, we also align parse trees with system transla-
tions so that we can learn the movement of syntactic constituents carried out by the
reordering models and investigate the performance of the reordering models by com-
paring both alignments.
For notational convenience, we denote syntactic reordering patterns that are ex-
tracted from the alignments between source parse trees and reference translations as
REF-SRP and those from the alignments between source parse trees and system trans-
lations as SYS-SRP. We refer to those present in both alignments under some conditions
5 We adopt the definition of span from Fox (2002): Given a node n that covers a word sequence sp...si...sq
and a word alignment matrix M, the target words aligned to n are {ti : ti ? M(si )}. We define the target
span of node n as nT = (min({ti}), max({ti})). Note that nT may contain words that are not in {ti}.
6 Please note that the order of structures may not be defined in some cases (see Section 6.3).
547
Computational Linguistics Volume 36, Number 3
that will be described in Section 6.4 as MATCH-SRP. To conduct a thorough analysis on
the reorderings, we carry out the following steps on the test corpus (source sentences +
reference translations):
1. Parse source sentences.
2. Generate word alignments between source sentences and reference
translations as well as word alignments between source sentences and
system translations.
3. According to the word alignments of Step 2, for each multi-branching
node ? ? ?1...?n in the source parse tree generated in Step 1, find the
target spans ?T1 ...?
T
n and their order [i1]...[in] in the reference and system
translations, respectively.
4. Generate REF-SRPs, SYS-SRPs, and MATCH-SRPs according to the target
orders generated in Step 3 for each multi-branching node.
5. Summarize all SRPs and calculate the precision and recall as described
above.
We further elaborate Steps 2?4 in the Sections 6.2?6.4.
6.2 Generating Word Alignments
To obtain word alignments between source sentences and multiple reference transla-
tions, we pair the source sentences with each of the reference translations and include
the created sentence pairs in our bilingual training corpus. Then we run GIZA++ on the
new corpus in both directions, and apply the ?grow-diag-final? refinement rule (Koehn
et al 2005) to produce the final word alignments.
To obtain word alignments between source sentences and system translations, we
store the word alignments within each phrase pair in our phrase table. When we output
the system translation for a source sentence, we trace back the original source phrase
for each target phrase in the system translation. This will generate a phrase alignment
between the source sentence and system translation. Given the phrase alignment and
word alignments within the phrase stored in the phrase table, we can easily obtain word
alignments between the whole source sentence and system translation.
6.3 Generating Target Spans and Orders
Given the source parse tree and the word alignment between a source sentence and
a reference/system translation, for each multi-branching node ? ? ?1...?n, we firstly
determine the target span ?Ti for each child node ?i following Fox (2002). If one child
node is aligned to NULL, we define a special target span for it. The order for this special
target span will remain the same as the child node occurring in ?1...?n.
Two target spans may overlap with each other because of inherent divergences
between two languages or noise in the word alignment. When this happens on two
neighboring nodes ?i and ?i+1, we combine these two nodes together and redefine a
target span ?Ti&i+1 for the combined node. This process will be repeated until no more
neighboring nodes can be combined. For example, the target span of nodes a and b in
548
Xiong et al Linguistically Annotated Reordering
Figure 6
An example source parse tree with the word alignment between the source sentence and the
target translation. Dotted lines show the word alignment.
Figure 6 overlap ((1, 3) vs. (2, 2)). Therefore these two nodes are to be combined into a
new node, whose target span is (1, 3).
After performing all necessary node combinations, if there are no more overlaps, we
call the multi-branching node reorderable, otherwise non-reorderable. To get a clearer
picture of the reorderable nodes, we divided them into two categories:
 fully reorderable if all target spans of child nodes don?t overlap;
 partially reorderable if some child nodes are combined due to
overlapping.
In Figure 6, both nodes a and c are fully reorderable nodes.7 Node d is a partially
reorderable node. Node g is a non-reorderable node because (1) the target spans of its
child nodes d and f overlap, and (2) child nodes d and f cannot be combined because
they are not neighbors.
Because we have multiple reference translations for each source sentence, we can
define multiple orders for {?Ti }n1. If one node is non-reorderable in all reference trans-
lations, we call it REF-non-reorderable, otherwise REF-reorderable. To specify the
reorderable attribute of a node in the system translation, we prefix ?SYS-? to {non-
reorderable, reorderable, fully-reorderable, partially-reorderable}.
6.4 Generating SRPs
After we obtain the orders of the child nodes for each multi-branching node, we gener-
ate REF-SRPs and SYS-SRPs from the fully/partially reorderable nodes. We obtain the
7 Their target translations are interrupted by the other node?s translation. We will discuss this situation in
Section 8.5.
549
Computational Linguistics Volume 36, Number 3
MATCH-SRP for each multi-branching node by comparing the obtained SYS-SRP with
the REF-SRPs for this node under the following conditions:
1. Because we have multiple reference translations, we may have different
REF-SRPs. We compare the SYS-SRP with the REF-SRP where the
reference translation for this node (the sequence within the target span of
the node defined by the REF-SRP) has the shortest Levenshtein distance
(Navarro 2001) to that of the system translation.
2. If there are combined nodes in SYS/REF-SRPs, they are treated as a unit
when comparing, without considering the order within each combined
node. If the order of the SYS-SRP and the selected REF-SRP matches, we
have one MATCH-SRP for the node.
Let?s give an example to explain these conditions. Suppose that we are processing
the structure VP ? PP1ADVP2VP3. We obtain four REF-SRPs from four different ref-
erence translations and one SYS-SRP from the system output. Here we only show the
orders:
Ref.a : [3][1][2]
Ref.b : [3][2][1]
Ref.c&d : [2][3][1]
SYS : [3][1&2]
References c and d have the same order. Therefore we have three different REF-SRPs
for this structure. In the SYS-SRP, PP1 and ADVP2 are combined and moved to the right
side of VP3. Supposing that the system translation for this structure has the shortest edit
distance to that of Reference b, we use the order of Reference b to compare the system
order. In the Reference b order, both PP1 and ADVP2 are also moved to the right side of
VP3. Therefore the two orders of Reference b and SYS match. We have one matched SRP
for this structure.
7. Evaluation
Our system is a BTG-based phrasal SMT system, developed following Section 2. We
integrate the boundary word?based reordering model and the linguistically annotated
reordering model into our system according to our reordering configuration. We car-
ried out various experiments to evaluate the reordering example extraction algorithms
of Section 3, the linguistically annotated reordering model vs. boundary word?based
reordering model, and the effects of linguistically annotated features on the Chinese-to-
English translation task of the NIST MT-05 using large scale training data.
7.1 Experimental Setup
We ran GIZA++ (Och and Ney 2000) on the parallel corpora (consisting of 101.93M
Chinese words and 112.78M English words) listed in Table 2 in both directions and
then applied the ?grow-diag-final? refinement rule (Koehn, Och, and Marcu 2003) to
550
Xiong et al Linguistically Annotated Reordering
Table 2
Corpora used.
Corpus LDC catalog Chinese words English words
United Nations LDC2004E12 68.63M 76.99M
Hong Kong News LDC2004T08 15.07M 15.89M
Sinorama Magazine LDC2005T10 10.26M 9.64M
FBIS LDC2003E14 7.09M 9.28M
Xinhua LDC2002E18 0.40M 0.43M
Chinese News Translation LDC2005T06 0.28M 0.31M
Chinese Treebank LDC2003E07 0.10M 0.13M
Multiple Translation Chinese LDC2004T07 0.10M 0.11M
Total ?? 101.93M 112.78M
obtain many-to-many word alignments. From the word-aligned corpora, we extracted
bilingual phrases.
We used all corpora listed in Table 2 except for the United Nations corpus to train
our reordering models, which consist of 33.3M Chinese words and 35.79M English
words. We ran the reordering example extractor AExtractor and TExtractor of Section 3
on the chosen word-aligned corpora. We then extracted boundary word features from
the reordering examples. To extract linguistically annotated features, we parsed the
Chinese side of the chosen parallel text using a Chinese parser (Xiong, Liu, and Lin
2005) which was trained on the Penn Chinese Treebank with an F1-score of 79.4%. We
ran the off-the-shelf MaxEnt toolkit8 to tune the reordering feature weights with the
iteration number set to 100 and Gaussian prior to 1 to avoid overfitting.
We built our 4-gram language model using the SRILM toolkit (Stolcke 2002), which
was trained on the Xinhua section of the English Gigaword corpus (181.1M words).
We selected 580 short sentences (not exceeding 50 characters per sentence) from the
NIST MT-02 evaluation test data as our development set (18 words/31 characters per
sentence). The NIST MT-05 test set includes 1,082 sentences with an average of 27.4
words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set
contains four translations per source sentence. Both the development and test sets were
also parsed using the parser mentioned above.
Our evaluation metric is the case-insensitive BLEU-4 (Papineni et al 2002) using the
shortest reference sentence length for the brevity penalty. The model feature weights are
tuned on the development set to maximize BLEU using MERT (Och 2003). Statistical
significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn
2004).
7.2 Bias in AExtractor
As described in Section 3, AExtractor selectively extracts reordering examples. This
selective extraction raises three questions:
1. Is it necessary to extract all reordering examples?
8 Available at: http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
551
Computational Linguistics Volume 36, Number 3
2. If it is not necessary, do the heuristic selection rules impose any bias on
the reordering model? For example, if we use the strINV selection rule,
meaning that we always extract the largest block pairs for inverted
reordering examples, does the reordering model prefer swappings on
larger blocks to those on smaller blocks?
3. Does the bias have a strong impact on the performance in terms of BLEU
score?
The answer to the first question is no. Firstly, it is practically undesirable to extract
all reordering examples because even a very small training set will produce millions of
reordering examples if we enumerate all block pair combinations. Secondly, extracting
all reordering examples introduces a great amount of noise into training and therefore
undermines the final reordering model. In Table 3, we show the number of reorder-
ing examples extracted using different extraction algorithms and selection rules. The
AExtractor with the COMBO selection rule extracts the largest number of reordering
examples. However, it does not obtain the highest BLEU score compared with other
selection rules which extract a smaller number of reordering examples. This empirically
suggests that there is no need to extract all reordering examples.
To answer the second question, we trace the best BTG trees produced on the test set
by our system. The BWR reordering model is trained on reordering examples which are
extracted using different selection rules. Then we calculate the average number of words
on the target side which are covered by binary nodes in a straight order. We refer to this
number as straight average length. Similarly, inverted average length is calculated on
all binary nodes in an inverted order. The third and fourth columns of Table 3 show the
two average variables. Comparing these average numbers, we clearly observe that two
selection rules indeed impose noticeable bias on the reordering model.
 The strINV selection rule, which always extracts the largest block pairs for
inverted reordering examples, has the largest inverted average length.
This indicates that the strINV rule biases the reordering model towards
larger swappings.
 On the contrary, the STRinv selection rule, which extracts the largest block
pairs for straight reordering examples and smallest pairs for inverted
reordering examples, has the largest straight average length and a
Table 3
Comparison of reordering example extraction algorithms and selection rules. We only use BWR
as the reordering model for this comparison.
Ext. Alg. Reordering Straight Inverted BLEU
(Sel. rule) Examples Avg. Len. Avg. Len.
AExtractor (strINV) 10.06M 15.8 14.5 32.37
AExtractor (STRinv) 10.06M 17.3 12.8 32.47
AExtractor (RANDOM) 10.06M 14.7 11.8 32.24
AExtractor (COMBO) 23.27M 13.8 13.5 32.10
TExtractor 14.30M 15.0 14.1 29.95
552
Xiong et al Linguistically Annotated Reordering
relatively much smaller inverted average length. This suggests that the
STRinv rule makes the reordering model prefer smaller swappings.
Note that the selection rules RANDOM and COMBO do not impose bias on the length
of extracted reordering examples compared with strINV and STRinv. The latter two se-
lection rules have special preferences on the length of reordering examples and transfer
these preferences to the reordering models as shown in Table 3.
Because the preference for reordering larger/smaller blocks is imposed by the
reordering example extraction algorithm with special selection rules, one might wonder
whether we can allow the decoder to decide its own reordering preference. We add two
new features to our translation model: reordering count penalty (rc) and reordering
length penalty (rl). We accumulate rc whenever two neighboring BTG nodes are re-
ordered. And at the same time we add the number of words which are covered by these
two neighboring nodes to rl. Their weights are tuned using MERT to maximize BLEU
score on the development set with other model feature weights. These two features
are similar to the widely used word/phrase penalty features. Tuning the weights of
the word/phrase penalty features, we can allow the decoder to favor shorter or longer
phrases. Similarly, with the two new features rc and rl, we can allow the decoder to
favor shorter or longer reorderings.
We conducted experiments using reordering examples which are extracted with the
RANDOM and COMBO selection rules because these two rules do not impose bias on
the length of reordering examples. Observing the optimized weights of rc and rl on
the development set, we find that the decoder rewards larger rc but smaller rl. This
means that the decoder prefers shorter reorderings to longer reorderings. However, the
BLEU scores on the test set are 31.89 and 32.0 for RANDOM and COMBO, respectively,
which are worse than the BLEU scores of RANDOM and COMBO without using rc
and rl in Table 3, and also worse than the performance of strINV and STRinv which
impose preferences on reordering examples. This seems to suggest that the preference
for shorter/longer reorderings imposed by the reordering example extraction algorithm
is better than that decided by the decoder itself.
Finally, for the last question, we observe from Table 3 that BLEU scores are not that
much different although we have quite the opposite bias imposed by different selection
rules. The changes in BLEU score, which happen when we shift from one selection rule
to the other, are limited to a maximum of 1.2%. Among the four selection rules, the
STRinv rule achieves the highest BLEU score. The reason might be that the bias towards
smaller swappings imposed by this rule helps the decoder to reduce incorrect long-
distance swappings (Xiong et al 2008b).
7.3 AExtractor vs. TExtractor
We further compared the two algorithms for reordering example extraction. In Table 3,
we find that TExtractor significantly underperforms in comparison to AExtractor. This
is because the transformation from decomposition trees to BTG trees is not complete.
Many crossing links due to errors and noise in word alignments generated by GIZA++
make it impossible to build BTG nodes over the corresponding words. It would be better
to use alignments induced by the ITG and EM procedure described in Section 3.2 but
this has a very high cost.
Given the comparison in Table 3, we use AExtractor with the STRinv selection rule
to extract reordering examples for both BWR and LAR in all experiments described
below.
553
Computational Linguistics Volume 36, Number 3
7.4 LAR vs. BWR
Table 4 shows the results of the different integration of BWR and LAR into our systems.
Only using LAR achieves a BLEU score of 32.17, which is comparable to that of BWR.
This suggests that LAR is promising given that:
 LAR uses many fewer features than BWR does. According to our statistics,
LAR contains only 166.1k linguistically annotated features whereas BWR
has 451.4k boundary word features.
 Syntactic divergences between the source and target languages as well as
parse errors prevent the effective use of syntactic knowledge for phrase
reordering (see the in-depth analysis in Section 8.2.2).
Although BWR marginally outperforms LAR (32.47 vs. 32.17), simple boundary
word features are not adequate to move phrases to appropriate positions because
they cannot recognize syntactic contexts which are very relevant to phrase reordering.
Therefore the best way to reorder a phrase is to combine BWR and LAR so that we
can use syntactic information on the one hand and not worry too much about syntactic
divergences on the other hand.
As described in Section 5.3, we can combine BWR and LAR at two levels: the feature
level and the model level. When we combine them at the model level, we achieve an
absolute improvement of 0.83 and 1.13 BLEU points over BWR and LAR, respectively,
which are both statistically significant (p < 0.01). This shows that LAR and BWR are
complementary to each other and in particular that using linguistic knowledge can
significantly improve a very competitive lexicalized reordering model (BWR).
The other combination method All-in-One (at the feature level) also obtains signif-
icant improvements over BWR and LAR but marginally underperforms compared to
BWR+LAR. In our later experiments we use the combination method BWR+LAR.
7.5 Varying Training Data Size
To investigate how LAR improves BWR when we vary our training data size, we carried
out experiments on three different training data sets: FBIS (7.09M Chinese words, 9.28M
English words); Large1, which includes all corpora listed in Table 2 except for the United
Nations corpus (33.3M Chinese words, 35.79M English words); and Large2, which
Table 4
BLEU scores for LAR, BWR, and their combinations.
Reordering Configuration BLEU
BWR 32.47
LAR 32.17
All-in-One 33.03**++
BWR+LAR 33.30**++
** = Significantly better than BWR (p < 0.01); ++ = significantly better than LAR (p < 0.01).
554
Xiong et al Linguistically Annotated Reordering
Table 5
BLEU scores on different training data sets. Large1 refers to the corpora listed in Table 2 except
for the United Nations corpus. Large2 includes all corpora listed in Table 2.
Training Data BWR BWR+LAR Improvement
FBIS 24.97 26.52 1.55
Large1 29.96 30.78 0.82
Large2 32.47 33.30 0.83
consists of Large1 and the United Nations corpus (101.93M Chinese words, 112.78
English words). The language model remains the same for these three data sets because
it is trained on a much larger data set (181.1M words).
Table 5 shows the results. We observe that BWR+LAR is able to achieve a larger
improvement of 1.55 BLEU points over BWR on smaller training data. When we enlarge
the training data set from FBIS to Large1, both BWR and BWR+LAR improve quite a
bit. The difference between them is narrower, 0.82 BLEU points, but still significant.
When we continue to use more training data (Large2), the improvement obtained by
integrating LAR becomes stable at the 0.8 level.
7.6 Effects of Linguistically Annotated Features
We conducted further experiments to evaluate the effects of individual linguistically an-
notated features. Using the reordering configuration of BWR+LAR, we augment LAR?s
feature pool incrementally: firstly using only syntactic categories9(sc) as features (170
features in total), then constructing composite categories (cc) for non-syntactic phrases
(sc + cc) (8.6K features), and finally introducing head words and their POS tags into
the feature pool (sc + cc + hw + ht) (166.1K features). This series of experiments demon-
strates the impact and degree of contribution made by each feature for reordering.
The experimental results are presented in Table 6, from which we have the following
observations:
1. Syntactic category alone improves the performance statistically
significantly. The baseline feature set sc with only 170 features improves
the BLEU score from 32.47 to 32.87.
2. Other linguistic information, provided by the categories of boundary
nodes (cc) and head word/tag pairs (hw + ht), also improves phrase
reordering. Producing composite categories for non-syntactic BTG nodes
and integrating head word/tag pairs into LAR as reordering features are
both effective, indicating that context information complements syntactic
category for capturing reordering patterns.
9 For a non-syntactic node, we only use the single category C, without constructing the composite category
L-C-R.
555
Computational Linguistics Volume 36, Number 3
Table 6
The effect of the linguistically annotated reordering model. (sc) is the baseline feature set,
(sc + cc) and (sc + cc + hw + ht) are extended feature sets for LAR.
Reordering Configuration BLEU
BWR 32.47
BWR + LAR (sc) 32.87*
BWR + LAR (sc + cc) 33.06**
BWR + LAR (sc + cc + hw + ht) 33.30**++
* = almost significantly better than BWR (p < 0.075); ** = significantly better than BWR (p < 0.01); ++ =
significantly better than BWR + LAR (sc) (p < 0.01).
8. Analysis
We first obtain system translations of the test corpus. We generate word alignments
between source sentences and system/reference translations as described in Section 6.2.
Then we follow the analysis steps of Section 6.1 to investigate syntactic constituent
movement in the reference translations and system translations which are generated
using two different reordering configurations: BWR+LAR vs. BWR. In LAR, we use the
best reordering feature set (sc + cc + hw + ht).
8.1 Syntactic Constituent Movement: Overview
If a syntactic constituent is fully reorderable or partially reorderable, it is considered to
be movable as a unit. To denote the proportion of syntactic constituents to be moved as
a unit, we introduce two variables REF-R-rate and SYS-R-rate, which are defined as
SYS-R-rate =
count(SYS-reorderable nodes)
count(multi-branching nodes)
(13)
REF-R-rate =
count(REF-reorderable nodes)
count(multi-branching nodes)
(14)
Table 7 shows the statistics of REF/SYS-reorderable nodes on the test corpus. From
this table, we have the following observations:
1. A large number of nodes are REF-reorderable, accounting for 79.82% of all
the multi-branching nodes. This number shows that, in reference
translations, a majority of syntactic constituent movement across
Chinese?English can be performed by directly permuting constituents in a
sub-tree.
2. The R-rates of BWR and BWR+LAR are 77.46% and 81.79%, respectively.
The R-rate of BWR+LAR is obviously higher than that of BWR, which
suggests that BWR+LAR tends towards moving more syntactic
constituents together than BWR does. We will discuss this further later.
556
Xiong et al Linguistically Annotated Reordering
Table 7
Statistics of multi-branching and REF/SYS-reorderable nodes per sentence.
BWR BWR+LAR
multi-branching node 18.68
REF-reorderable node 14.91
REF-R-rate 79.82%
SYS-fully-reorderable node 13.16 14.01
SYS-partially-reorderable node 1.31 1.26
SYS-R-rate 77.46% 81.79%
8.2 Syntactic Constituent Movement among Multiple Reference Translations
8.2.1 Differences in Movement Orientation. Because each source sentence is translated by
four different human experts, we would like to analyze the differences among reference
translations, especially on the orders of constituents being translated. Table 8 shows
the overall distribution over the number of different orders for each multi-branching
constituent among the reference translations.
In most cases (75.4%), four reference translations have completely the same order
for syntactic constituents. This makes it easier for our analysis to compare the system
order with the reference order. However, there are 22% cases where two different orders
are provided, which shows the flexibility of translation. According to our study, noun
phrases taking DNP or CP modifiers, as well as DNPs and CPs themselves, are more
likely to be translated in two different orders. Table 9 shows the percentages in which
two different orders for these constituents are observed in the reference corpus.
DNP and CP are always used as pre-modifiers of noun phrases in Chinese. They
often include the particle word  (of ) at the ending position. The difference is that
DNP constructs a phrasal modifier whereas CP constructs a relative-clause modifier.
There is no fixed reordering pattern for DNP and CP and therefore for NP which takes
DNP/CP as a pre-modifier. In the DNP ? NP DEG structure, the DEG () can be
Table 8
Distribution of number of different orders by which syntactic constituents are translated in
references.
Number of different orders 1 2 3 4
Percentage 75.40 22 2.33 0.33
Table 9
Two-order translation distribution of 4 NP-related constituents.
Constituent 2-order translation percentage
NP ? DNP NP 16.93
NP ? CP NP 9.43
CP ? IP DEC 24.79
DNP ? NP DEG 34.58
557
Computational Linguistics Volume 36, Number 3
translated into ?s or of, which are both appropriate in most cases, depending on the
translator?s preference. If the former is chosen, the order of DNP and therefore the
order for NP ? DNP NP will both be straight: [1][2]. Otherwise, the two orders will be
inverted: [2][1]. Similarly, there are also different translation patterns for CP ? IP DEC
and NP ? CP NP. CP can be translated into ?that + clause? or adjective-like phrases
in English. Figure 7 shows an example where the CP constituent is translated into an
adjective-like phrase. Although the ?that + clause? must be placed behind the noun
phrase which it modifies, the order for adjective-like phrases is flexible (see Figure 7).
For those constituents with different reference orders, we compare the order of
the system translation to that of the reference translation which has the shortest edit
distance to the system translation as described herein so that we can take into account
the potential influence of different translations on the order of syntactic constituents.
8.2.2 REF-Non-Reorderable Constituents. We also study REF-R-rates for the 13 most fre-
quent constituents listed in Table 10. We find that two constituents, VP1 ? PP VP2 and
NP1 ? CP NP2, have the lowest REF-R-rates, 58.20% and 61.77%, respectively. This
means that about 40% of them are REF-non-reorderable. In order to understand the
reasons why they are non-reorderable in reference translations, we further investigate
REF-non-reorderable cases for the constituent type VP1 ? PP VP2 and roughly classify
the reasons into three categories as follows.
1. Outside interruption. The reordering of PP and VP2 is interrupted by
other constituents outside VP1. For example, the Chinese sentence [NP
/somebody ] [VP1 [PP.../when...] [VP2 [/say NP[...] ] ] ] is translated
into when..., somebody said .... Here the translation of the first NP which is
outside VP1 is inserted between the translations of PP and VP2 and
therefore interrupts their reordering. Outside interruption accounts for
21.65% of REF-non-reorderable cases.
2. Inside interruption. The reordering of PP and VP2 is interrupted by the
combination of PP?s subnodes with VP2?s subnodes. Inside interruption
accounts for 48.45% of REF-non-reorderable cases, suggesting that it is the
major factor which decreases the reorderability of VP ? PP VP. Because
both PP and VP have their own complex sub-structures, the inside
Figure 7
An example of the translation of NP ? CP NP. This constituent can be translated in two
different orders: 1) the recently adopted statistical method (straight order); 2) the statistical
method recently adopted (inverted order).
558
Xiong et al Linguistically Annotated Reordering
Table 10
F1-scores ( BWR+LAR vs. BWR) for the 13 most frequent constituents in the test corpus.
Constituents indicated in bold have relatively lower F1 score for reordering.
Type Constituent Percent. (%) SYS-R-rate (%) F1-score (%)
BWR BWR+LAR BWR BWR+LAR
VP
VP ? VV NP 8.12 79.22 84.10 76.97 80.53
VP ? ADVP VP 4.30 63.45 65.86 70.83 73.67
VP ? PP VP 1.87 60.32 70.37 39.29 40.33
VP ? VV IP 1.82 79.35 86.14 77.16 82.26
NP
NP ? NN NN 6.88 84.68 85.18 76.17 79.10
NP ? NP NP 5.12 82.13 84.93 69.25 72.17
NP ? DNP NP 2.14 69.75 74.83 56.68 56.61
NP ? CP NP 2.12 59.67 73.43 48.75 54.48
Misc.
IP ? NP VP 6.78 71.99 79.80 63.22 65.79
PP ? P NP 3.63 80.63 85.95 82.75 84.93
CP ? IP DEC 3.51 83.94 87.89 69.91 72.24
QP ? CD CLP 2.74 66 65 67.52 68.47
DNP ? NP DEG 2.43 85.98 89.84 67.5 68.75
interruption is very complicated and includes a variety of cases, some of
which are quite unexpected. Here we show two frequent examples of
inside interruption:
a. The preposition in the PP and the verb word/phrase of VP2 are
aligned to only one target word or one continuous phrase. For
example,.../pressure,.../be confident of,.../
suffer from, and so on. This is caused by the lexical divergence
problem.
b. The PP is first combined with the verb word of VP2 in an inverted
order, then combined with the remainder of VP2 in a straight order.
For example, [PP [P] [omission1]] [VP [VV	] [omission2]]
might be translated into learned from omission1 that omission2.
3. Parse error. This accounts for 29.90% of REF-non-reorderable cases.
Although these reasons are summarized from our analysis on the constituent type
VP ? PP VP, they can be used to explain other REF-non-reorderable constituents, such
as NP ? CP NP.
8.3 Syntactic Constituent Movement in System Translations
8.3.1 Overall Reordering Precision and Recall of Syntactic Constituents. By summarizing all
syntactic reordering patterns (REF-SRP, SYS-SRP, and Match-SRP) for all constituents,
we can calculate the overall reordering precision and recall of syntactic constituents.
Table 11 shows the results for both BWR+LAR and BWR, where BWR+LAR clearly
outperforms BWR.
559
Computational Linguistics Volume 36, Number 3
Table 11
Syntactic reordering precision and recall of BWR+LAR vs. BWR on the test corpus.
Precision Recall F1
BWR 70.89 68.79 69.83
BWR+LAR 71.32 73.08 72.19
8.3.2 The Effect of Linguistic Knowledge on Phrase Movement. To understand the change
in phrase movement caused by linguistic knowledge, we further investigate how well
BWR and BWR+LAR reorder certain constituents, especially those with high distribu-
tion probability. Table 10 lists the 13 most frequent constituents, which jointly account
for 51.46% of all multi-branching constituents. Except for NP ? DNP NP, the reorder-
ing F1 score of all these constituents in BWR+LAR is better than that in BWR.
Our hypothesis for the phrase movement change in BWR+LAR is that the integrated
linguistic knowledge makes phrase movement in BWR+LAR pay more respect to syn-
tactic constituent boundaries. The overall R-rates of BWR+LAR vs. BWR described in
Section 8.1 indicate that BWR+LAR tends towards moving more syntactic constituents
together than BWR does. We want to know whether this is also true for a specific
constituent type. The fourth and fifth columns in Table 10 present the R-rate for each
individual constituent type that we have analyzed. It is obvious that the R-rate of
BWR+LAR is much higher than that of BWR for almost all constituents. This indicates
that higher R-rate is one of the reasons for the higher performance of BWR+LAR.
To gain a more concrete understanding of this change, we show two examples for
the reordering of VP ? PP VP in Figure 8. In both examples, BWR fails to move the PP
constituent to the right of the VP constituent, whereas BWR+LAR does it successfully.
By tracing the binary BTG trees generated by the decoder, we find that BWR generated
a very different BTG tree from the source parse tree whereas the BTG tree in BWR+LAR
almost matches the source parse tree. In the first example, BWR combines the VP phrase
Figure 8
Two examples for the translation of VP ? PP VP. Square brackets indicate combinations in a
straight order and angular brackets represent combinations in an inverted order.
560
Xiong et al Linguistically Annotated Reordering

 with  and then combines . The preposition word  is combined with
the NP phrase NHK, which makes the translation of NHK interrupt the reordering of
VP ? PP VP in this example. The BWR tree in the second example is even worse. The
non-syntactic phrase   in the VP phrase is first combined with  ,
which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of
the VP phrase is then merged. This merging process continues regardless of the source
parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples
suggests that reordering models should respect syntactic structures in order to capture
reorderings under these structures.
Our observation on phrase movement change resonates with the recent efforts in
phrasal SMT that allow the decoder to prefer translations which show more respect
for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto,
Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other
words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early
syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has
receded in more powerful syntax-based models (Galley et al 2004; Chiang 2005) and
non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008)
and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses
which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose
this as a hard constraint on the ITG constraint to allow reorderings which respect the
source parse tree. They all report significant improvements on different language pairs,
which indicates that syntactic cohesion is very useful for phrasal SMT. Our analysis
demonstrates that linguistically annotated reordering provides an alternative way to
incorporate syntactic cohesion into phrasal SMT.
8.4 Challenges in Phrase Reordering and Suggestions
We highlight three constituent types in Table 10 (indicated in bold) which are much
more difficult to reorder, as indicated by their relatively lower F1 scores. The lower F1
scores indicate that BWR+LAR is not fully sufficient for reordering these constituents
although it performs much better than BWR. We find two main reasons for the lower F1
scores and provide suggestions accordingly as follows.
1. Constrained decoding. We observe that in reorderable constituents which
involve long-distance reorderings, their boundaries are easily violated by
phrases outside them. To prohibit boundary violations, we propose
constrained decoding. In constrained decoding, we define special zones
in source sentences. Reorderings and translations within the zones cannot
be interrupted by fragments outside the zones. We can also define other
constrained operations on the zones. For example, we can prohibit
swappings in any zones which contain punctuation (Xiong et al 2008b).
The beginning and ending positions of a zone are automatically learned.
To be more flexible, they are not necessarily constituent boundaries.
Constrained decoding is different from both soft constraints (Cherry 2008;
Marton and Resnik 2008) and hard constraints (Yamamoto, Okuma, and
Sumita 2008). It can be considered as in between both of these because it is
harder than the former but softer than the latter.
2. Integrating special reordering rules. Some constituents are indeed
non-reorderable as we discussed in Section 8.2.2. Inside or outside
561
Computational Linguistics Volume 36, Number 3
interruptions have to be allowed to obtain fluent translations for these
constituents. However, the allowance of interruptions is sometimes
beyond the representability of BTG rules. For example, to solve the lexical
divergence problem, bilingual rules with aligned lexicons have to be
introduced. To capture reorderings of these constituents, we propose to
integrate special reordering rules with richer contextual information into
BTG to extend BTG?s ability to deal with interruptions. Completely
replacing BTG with richer formalisms, such as hierarchical phrase
(Chiang 2005) and tree-to-string (Liu, Liu, and Lin 2006) or string-to-tree
(Marcu et al 2006), introduces a huge extra cost. Instead, integrating a
small number of reordering rules into BTG to model reorderings of
non-reorderable constituents would be more desirable.
8.5 Discussion
In the definition of syntactic reordering patterns, we only consider the relative order
of individual constituents on the target side. We do not consider whether or not they
remain contiguous on the target side. It is possible that other words are inserted be-
tween spans of two contiguous constituents. We use the term gap to refer to when
this happens. The absence of a gap in the definition of syntactic reordering patterns
may produce more matched SRPs and therefore lead to higher precision and recall.
Table 12 shows the revised overall precision and recall of syntactic reordering patterns
when we also compare gaps. The revised results show that BWR+LAR still significantly
outperforms BWR. This also applies to the 13 constituents identified in Table 10. The
analysis results obtained before are still valid when we consider gaps.
9. Related Work
9.1 Linguistically Motivated Phrase Reordering
There are various approaches which are devoted to incorporating linguistic knowledge
into phrase reordering. Generally, these approaches can be roughly divided into three
categories: (1) reordering the source language in a preprocessing step before decoding
begins; (2) estimating phrase movement with reordering models; and (3) capturing
reorderings by synchronous grammars. The preprocessing approach applies manual or
automatically extracted reordering knowledge from linguistic structures to transform
the source language sentence into a word order that is closer to the target sentence.
The second reordering approach moves phrases under certain reordering constraints
and estimates the probabilities of movement with linguistic information. In the third
Table 12
Revised overall precision and recall of BWR+LAR vs. BWR on the test corpus when we consider
the gap in syntactic reordering patterns.
Precision Recall F1
BWR (gap) 46.28 44.91 45.58
BWR+LAR (gap) 48.80 50 49.39
562
Xiong et al Linguistically Annotated Reordering
approach, reordering knowledge is included in synchronous rules. The last two cate-
gories reorder the source sentence during decoding, which distinguishes them from the
first approach. Note that some researchers integrate multiple reordering approaches in
one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni
2008).
9.1.1 The Preprocessing Approach. In early work, Brown et al (1992) describe an approach
to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a
preprocessing approach which automatically learns reordering patterns based on CFG
productions. Since then, the preprocessing approach seems to have been more popular.
Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types
of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees
using fine-grained human-written rules, mostly concentrating on VP and NP structures.
Li et al (2007) improve the preprocessing approach by generating n-best reordered
source sentences with reordering knowledge automatically learned from the alignments
between source parse trees and target translations. The approach proposed in Li et al
also enhances the connection between the preprocessing and decoding by adding a
source reordering probability feature. Other approaches introduced in Nie?n and Ney
(2001), Popovic? and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological,
POS, and chunk knowledge in the preprocessing approach, respectively.
9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM con-
straint (Zens and Ney 2003), the early work uses a distortion-based reordering model
to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG
constraint, the corresponding model is the flat model which assigns a prior probability
to the straight or inverted order (Wu 1996). These two models don?t respect the content
of phrases which are moved. To address this issue, lexicalized reordering models which
are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn
et al 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and
Lin (2006) introduce a more flexible reordering model under the ITG constraint using
discriminative features which are automatically learned from a training corpus. Zhang
et al (2007) propose a model for syntactic phrase reordering which uses syntactic
knowledge from source parse trees. Our reordering approach is most similar to those in
Xiong, Liu, and Lin (2006) and Zhang et al but extends them further by using syntactic
knowledge and allowing non-syntactic phrase reordering.
9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use
synchronous grammars to capture reorderings between two languages. Chiang (2005)
introduces formal synchronous grammars for phrase-based translation. In his work,
hierarchical reordering knowledge is included in synchronous rules which are automat-
ically learned from word-aligned corpus. In linguistically syntax-based models, string-
to-tree (Marcu et al 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and
Lin 2006), and tree-to-tree (Zhang et al 2008) translation rules, just to name a few, are
explored. Linguistical reordering knowledge is naturally included in these syntax-based
translation rules.
9.2 Automatic Analysis of Reordering
Although there is a variety of work on phrase reordering, automatic analysis of phrase
reordering is not widely explored in the SMT literature. Chiang et al (2005) propose
563
Computational Linguistics Volume 36, Number 3
an automatic method to compare different system outputs in a fine-grained manner
with regard to reordering. In their method, common word n-grams occurring in both
reference translations and system translations are extracted and generalized to part-of-
speech tag sequences. A recall is calculated for each certain tag sequence to indicate the
ability of reordering models to capture this tag sequence in system translations. Popovic
et al (2006) use the relative difference between WER (word error rate) and PER (position
independent word error rate) to indicate reordering errors. The larger the difference, the
more reordering errors there are.
Callison-Burch et al (2007) propose a constituent-based evaluation that is very simi-
lar to our method in Steps (1)?(3). They also parse the source sentence and automatically
align the parse tree with the reference/system translations. The difference is that they
highlight constituents from the parse tree to enable human evaluation of the translations
of these constituents, rather than automatically analyzing constituent movement. They
use this method for human evaluation in the shared translation task of the 2007 and
2008 ACL Workshop on Statistical Machine Translation.
Fox (2002) systematically studies syntactic cohesion between French and English
using human translations and alignments. Compared with her work, our analysis here
includes, but is not limited to, an investigation of syntactic cohesion in an actual MT
system.
10. Conclusion
We have presented a novel linguistically motivated phrase reordering approach:
Linguistically Annotated Reordering. The LAR approach incorporates soft linguistic
knowledge from the source parse tree into hard hierarchical skeletons generated by
BTG in phrasal SMT. To automatically learn reordering features, we have introduced
algorithms for reordering example extraction and linguistic annotation. We have also
proposed a new syntax-based analysis method to detect syntactic constituent movement
in human/machine translations.
We have conducted experiments on large-scale training data to evaluate LAR and
BWR as well as the reordering example extraction algorithms. Our evaluation results
show that:
1. Extracting reordering examples directly from word alignments is much
better than from BTG-style trees which are built from word alignments.
2. Selection rules which bias the reordering model towards smaller
swappings improve translation quality.
3. BWR+LAR significantly outperforms BWR, which suggests that the
integration of linguistic knowledge improves reordering; and tuning two
separate reordering models is better than the All-in-One combination
method.
We have further analyzed the outputs of BWR+LAR vs. BWR using the proposed
syntax-based analysis method. Our analysis results show that:
1. BWR+LAR achieves a significantly higher reordering precision and recall
than BWR does with regard to syntactic constituent movement.
564
Xiong et al Linguistically Annotated Reordering
2. For most reorderable constituents, integrating source-side linguistic
knowledge into the reordering model can significantly improve
reorderings by guiding reordering models to prefer hypotheses that pay
more respect to constituent boundaries.
3. For non-reorderable constituents or constituents involving long-distance
reorderings, integrating source-side linguistic knowledge into the
reordering model is not sufficient to avoid illegal boundary violations or to
capture reordering patterns.
To avoid illegal boundary violations in long-span constituents, we suggest con-
strained decoding, which protects special zones in the source sentence from being
interrupted by phrases outside the zones. Beginning and ending positions of the zones
are automatically learned using lexical and syntactic knowledge. To capture complex re-
orderings which cross constituent boundaries, phrasal SMT should integrate reordering
rules with richer contextual information.
Acknowledgments
We would like to thank the three anonymous
reviewers for their helpful comments and
suggestions.
References
Al-Onaizan, Yaser and Kishore Papineni.
2006. Distortion models for statistical
machine translation. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th
Annual Meeting of the Association for
Computational Linguistics, pages 529?536,
Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, John D. Lafferty,
and Robert L. Mercer. 1992. Analysis,
statistical transfer, and synthesis in
machine translation. In Proceedings
of the Fourth International Conference on
Theoretical and Methodological Issues in
Machine Translation, pages 83?100,
Montreal.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2007. (Meta-) evaluation of
machine translation. In Proceedings of the
Second Workshop on Statistical Machine
Translation, pages 136?158, Prague.
Cherry, Colin. 2008. Cohesive phrase-based
decoding for statistical machine
translation. In Proceedings of ACL-08: HLT,
pages 72?80, Columbus, OH.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 263?270,
Ann Arbor, MI.
Chiang, David, Adam Lopez, Nitin
Madnani, Christof Monz, Philip Resnik,
and Michael Subotin. 2005. The hiero
machine translation system: Extensions,
evaluation, and analysis. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 779?786,
Vancouver.
Collins, Michael, Philipp Koehn, and Ivona
Kucerova. 2005. Clause restructuring for
statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 531?540, Ann Arbor, MI.
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation. In
The Companion Volume to the Proceedings of
41st Annual Meeting of the Association for
Computational Linguistics, pages 205?208,
Sapporo.
Fox, Heidi. 2002. Phrasal cohesion and
statistical machine translation. In
Proceedings of the 2002 Conference on
Empirical Methods in Natural Language
Processing, pages 304?311,
Philadelphia, PA.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics:
HLT-NAACL 2004, pages 273?280,
Boston, MA.
Ge, Niyu, Abe Ittycheriah, and Kishore
Papineni. 2008. Multiple reorderings in
phrase-based machine translation. In
Proceedings of the ACL-08: HLT Second
Workshop on Syntax and Structure in
565
Computational Linguistics Volume 36, Number 3
Statistical Translation (SSST-2), pages 61?68,
Columbus, OH.
Huang, Liang, Kevi Knight, and Aravind
Joshi. 2006. Statistical syntax-directed
translation with extended domain of
locality. In Proceedings of the 7th Conference
of the Association for Machine Translation
of the Americas, pages 66?73,
Cambridge, MA.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of EMNLP 2004, pages 388?395,
Barcelona.
Koehn, Philipp, Amittai Axelrod, Alexandra
Birch Mayne, Chris Callison-Burch,
Miles Osborne, and David Talbot. 2005.
Edinburgh system description for the
2005 IWSLT speech translation
evaluation. In Proceedings of the
International Workshop on Spoken
Language Translation 2005, pages 78?85,
Pittsburgh, PA.
Koehn, Philipp, Franz Joseph Och, and
Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings of
the 2003 Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 58?54, Edmonton.
Kumar, Shankar and William Byrne. 2005.
Local phrase reordering models for
statistical machine translation. In
Proceedings of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing,
pages 161?168, Vancouver.
Li, Chi-Ho, Minghui Li, Dongdong Zhang,
Mu Li, Ming Zhou, and Yi Guan. 2007. A
probabilistic approach to syntax-based
reordering for statistical machine
translation. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 720?727,
Prague.
Lin, Dekang. 2004. A path-based transfer
model for machine translation. In
Proceedings of the 20th International
Conference on Computational Linguistics
(Coling 2004), pages 625?630, Geneva.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006.
Tree-to-string alignment template for
statistical machine translation. In
Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 609?616,
Sydney.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 44?52, Sydney.
Marton, Yuval and Philip Resnik. 2008. Soft
syntactic constraints for hierarchical
phrased-based translation. In Proceedings
of ACL-08: HLT, pages 1003?1011,
Columbus, OH.
Navarro, Gonzalo. 2001. A guided tour to
approximate string matching. ACM
Computing Surveys, 33(1):31?88.
Nie?n, Sonja and Hermann Ney. 2001.
Morpho-syntactic analysis for
reordering in statistical machine
translation. In Proceedings of MT Summit
VIII, pages 247?252, Santiago de
Compostela.
Och, Franz Josef. 2002. Statistical Machine
Translation: From Single-Word Models to
Alignment Templates. Ph.D. thesis, RWTH
Aachen University, Germany.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 160?167, Sapporo.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics,
pages 440?447, Hong Kong.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of 40th
Annual Meeting of the Association for
Computational Linguistics, pages 311?318,
Philadelphia, PA.
Popovic, Maja, Adria` de Gispert, Deepa
Gupta, Patrik Lambert, Hermann Ney,
Jose? B. Marin?o, Marcello Federico, and
Rafael Banchs. 2006. Morpho-syntactic
information for automatic error analysis
of statistical machine translation output.
In Proceedings on the Workshop on
Statistical Machine Translation, pages 1?6,
New York, NY.
Popovic?, Maja and Hermann Ney. 2006.
Pos-based word reorderings for statistical
machine translation. In Proceedings of the
Fifth International Conference on Language
Resources and Evaluation (LREC 2006),
pages 1278?1283, Genoa.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translations: Syntactically informed
phrasal smt. In Proceedings of the 43rd
566
Xiong et al Linguistically Annotated Reordering
Annual Meeting of the ACL, pages 271?279,
Ann Arbor, MI.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit.
In Proceedings of the 7th International
Conference on Spoken Language
Processing (ICSLP 2002), pages 901?904,
Denver, CO.
Tillman, Christoph. 2004. A unigram
orientation model for statistical machine
translation. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL
2004): Short Papers, pages 101?104,
Boston, MA.
Wang, Chao, Michael Collins, and Philipp
Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In
Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL),
pages 737?745, Prague.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 152?158,
Santa Cruz, CA.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Wu, Dekai, Marine Carpuat, and Yihai Shen.
2006. Inversion transduction grammar
coverage of Arabic-English word
alignment for tree-structured statistical
machine translation. In Proceeding
of the IEEE/ACL 2006 Workshop on
Spoken Language Technology (SLT 2006),
pages 234?237, Aruba.
Xia, Fei and Michael McCord. 2004.
Improving a statistical MT system
with automatically learned rewrite
patterns. In Proceedings of the 20th
International Conference on Computational
Linguistics (Coling 2004), pages 508?514,
Geneva.
Xiong, Deyi, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank
with semantic knowledge. In Proceedings of
The 2nd International Joint Conference on
Natural Language Processing (IJCNLP-05),
pages 70?81, Jeju Island.
Xiong, Deyi, Qun Liu, and Shouxun Lin.
2006. Maximum entropy based phrase
reordering model for statistical machine
translation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 521?528, Sydney.
Xiong, Deyi, Min Zhang, Aiti Aw, and
Haizhou Li. 2008a. A linguistically
annotated reordering model for
BTG-based statistical machine
translation. In Proceedings of ACL-08:
HLT, Short Papers, pages 149?152,
Columbus, OH.
Xiong, Deyi, Min Zhang, Aiti Aw, Haitao Mi,
Qun Liu, and Shouxun Lin. 2008b.
Refinements in BTG-based statistical
machine translation. In Proceedings
of the Third International Joint Conference
on Natural Language Processing,
pages 505?512, Hyderabad.
Xue, Nianwen, Fei Xia, Shizhe Huang, and
Anthony Kroch. 2000. The bracketing
guidelines for the Penn Chinese treebank
(3.0). Technical report IRCS 00-07,
University of Pennsylvania Institute for
Research in Cognitive Science,
Philadelphia.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics,
pages 523?530, Toulouse.
Yamamoto, Hirofumi, Hideo Okuma, and
Eiichiro Sumita. 2008. Imposing
constraints from the source tree on ITG
constraints for SMT. In Proceedings of the
ACL-08: HLT Second Workshop on Syntax
and Structure in Statistical Translation
(SSST-2), pages 1?9, Columbus, OH.
Zens, Richard, and Hermann Ney. 2003.
A comparative study on reordering
constraints in statistical machine
translation. In Proceedings of the 41st
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 144?151, Sapporo, Japan.
Zhang, Dongdong, Mu Li, Chi-Ho Li, and
Ming Zhou. 2007. Phrase reordering
model integrating syntactic knowledge
for SMT. In Proceedings of the 2007 Joint
Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language
Learning (EMNLP-CoNLL), pages 533?540,
Prague.
Zhang, Hao and Daniel Gildea. 2005.
Stochastic lexicalized inversion
transduction grammar for alignment. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 475?482, Ann Arbor, MI.
567
Computational Linguistics Volume 36, Number 3
Zhang, Hao, Daniel Gildea, and David
Chiang. 2008. Extracting synchronous
grammar rules from word-level
alignments in linear time. In Proceedings of
the 22nd International Conference on
Computational Linguistics (Coling 2008),
pages 1081?1088, Manchester.
Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou
Li, Chew Lim Tan, and Sheng Li. 2008. A
tree sequence alignment-based tree-to-tree
translation model. In Proceedings of ACL-08:
HLT, pages 559?567, Columbus, OH.
Zhang, Yuqi, Richard Zens, and Hermann
Ney. 2007. Chunk-level reordering of
source language sentences with
automatically learned rules for statistical
machine translation. In Proceedings
of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in
Statistical Translation, pages 1?8,
Rochester, NY.
Zollmann, Andreas, Ashish Venugopal, and
Stephan Vogel. 2008. The CMU
syntax-augmented machine translation
system: SAMT on hadoop with N-best
alignments. In Proceedings of International
Workshop on Spoken Language Translation
(IWSLT), pages 18?25, Honolulu, HI.
568
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 136?144,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Translation Boundaries for Phrase-Based Decoding
Deyi Xiong, Min Zhang, Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632.
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
Abstract
Constrained decoding is of great importance
not only for speed but also for translation qual-
ity. Previous efforts explore soft syntactic con-
straints which are based on constituent bound-
aries deduced from parse trees of the source
language. We present a new framework to es-
tablish soft constraints based on a more nat-
ural alternative: translation boundary rather
than constituent boundary. We propose sim-
ple classifiers to learn translation boundaries
for any source sentences. The classifiers are
trained directly on word-aligned corpus with-
out using any additional resources. We report
the accuracy of our translation boundary clas-
sifiers. We show that using constraints based
on translation boundaries predicted by our
classifiers achieves significant improvements
over the baseline on large-scale Chinese-to-
English translation experiments. The new
constraints also significantly outperform con-
stituent boundary based syntactic constrains.
1 Introduction
It has been known that phrase-based decoding
(phrase segmentation/translation/reordering (Chi-
ang, 2005)) should be constrained to some extent not
only for transferring the NP-hard problem (Knight,
1999) into a tractable one in practice but also for im-
proving translation quality. For example, Xiong et
al. (2008) find that translation quality can be signif-
icantly improved by either prohibiting reorderings
around punctuation or restricting reorderings within
a 15-word window.
Recently, more linguistically motivated con-
straints are introduced to improve phrase-based de-
coding. (Cherry, 2008) and (Marton and Resnik,
2008) introduce syntactic constraints into the stan-
dard phrase-based decoding (Koehn et al, 2003) and
hierarchical phrase-based decoding (Chiang, 2005)
respectively by using a counting feature which ac-
cumulates whenever hypotheses violate syntactic
boundaries of source-side parse trees. (Xiong et al,
2009) further presents a bracketing model to include
thousands of context-sensitive syntactic constraints.
All of these approaches achieve their improvements
by guiding the phrase-based decoder to prefer trans-
lations which respect source-side parse trees.
One major problem with such constituent bound-
ary based constraints is that syntactic structures of
the source language do not necessarily reflect trans-
lation structures where the source and target lan-
guage correspond to each other. In this paper,
we investigate building classifiers that directly ad-
dress the problem of translation boundary, rather
than extracting constituent boundary from source-
side parsers built for a different purpose. A trans-
lation boundary is a position in the source sequence
which begins or ends a translation zone 1 spanning
multiple source words. In a translation zone, the
source phrase is translated as a unit. Reorderings
which cross translation zones are not desirable.
Inspired by (Roark and Hollingshead, 2008)
which introduces classifiers to decide if a word can
begin/end a multi-word constituent, we build two
discriminative classifiers to tag each word in the
source sequence with a binary class label. The first
classifier decides if a word can begin a multi-source-
word translation zone; the second classifier decides
if a word can end a multi-source-word translation
1We will give a formal definition of translation zone in Sec-
tion 2.
136
zone. Given a partial translation covering source se-
quence (i, j) with start word ci and end word cj 2,
this translation can be penalized if the first classifier
decides that the start word ci can not be a beginning
translation boundary or the second classifier decides
that the end word cj can not be an ending translation
boundary. In such a way, we can guide the decoder
to boost hypotheses that respect translation bound-
aries and therefore the common translation structure
shared by the source and target language, rather than
the syntactic structure of the source language.
We report the accuracy of such classifiers by com-
paring their outputs with ?gold? translation bound-
aries obtained from reference translations on the de-
velopment set. We integrate translation boundary
based constraints into phrase-based decoding and
display that they improve translation quality signif-
icantly in large-scale experiments. Furthermore, we
confirm that they also significantly outperform con-
stituent boundary based syntactic constraints.
2 Beginning and Ending Translation Zones
To better understand the particular task that we ad-
dress in this paper, we study the distribution of
classes of translation boundaries in real-world data.
First, we introduce some notations. Given a source
sentence c1...cn, we will say that a word ci (1 < i <
n) is in the class By if there is a translation zone ?
spanning ci...cj for some j > i; and ci ? Bn oth-
erwise. Similarly, we will say that a word cj is in
the class Ey if there is a translation zone spanning
ci...cj for some j > i; and cj ? En otherwise.
Here, a translation zone ? is a pair of aligned
source phrase and target phrase
? = (cji , e
q
p)
where ? must be consistent with the word alignment
M
?(u, v) ? M, i ? u ? j ? p ? v ? q
By this, we require that no words inside the source
phrase cji are aligned to words outside the target
phrase eqp and that no words outside the source
phrase are aligned to words inside the target phrase.
2In this paper, we use c to denote the source language and e
the target language.
Item Count (M) P (%)
Sentences 3.8 ?
Words 96.9 ?
Words ? By 22.7 23.4
Words ? Ey 41.0 42.3
Words /? By and /? Ey 33.2 34.3
Table 1: Statistics on word classes from our bilingual
training data. All numbers are calculated on the source
side. P means the percentage.
This means, in other words, that the source phrase
cji is mapped as a unit onto the target phrase eqp.
When defining the By and Ey class, we also re-
quire that the source phrase cji in the translation zone
must contain multiple words (j > i). Our interest
is the question of whether a sequence of consecu-
tive source words can be translated as a unit (i.e.
whether there is a translation zone covering these
source words). For a single-word source phrase, if
it can be translated separately, it is always translated
as a unit in the context of phrase-based decoding.
Therefore this question does not exist.
Note that the first word c1 and the last word cn
are unambiguous in terms of whether they begin or
end a translation zone. The first word c1 must begin
a translation zone spanning the whole source sen-
tence. The last word cn must end a translation zone
spanning the whole source sentence. Therefore, our
classifiers only need to predict the other n?2 words
for a source sentence of length n.
Table 1 shows statistics of word classes from our
training data which contain nearly 100M words in
approximately 4M sentences. Among these words,
only 22.7M words can begin a translation zone
which covers multiple source words. 41M words
can end a translation zone spanning multiple source
words, which accounts for more than 42% in all
words. We still have more than 33M words, ac-
counting for 34.3%, which neither begin nor end
a multi-source-word translation zone. Apparently,
translations that begin/end on words ? By/? Ey are
preferable to those which begin/end on other words.
Yet another interesting study is to compare trans-
lation boundaries with constituent boundaries de-
duced from source-side parse trees. In doing so,
we can know further how well constituent boundary
137
Classification Task Avg. Accuracy (%)
By/Bn 46.9
Ey/En 52.2
Table 2: Average classification accuracy on the develop-
ment set when we treat constituent boundary deducer (ac-
cording to source-side parse trees) as a translation bound-
ary classifier.
based syntactic constraints can improve translation
quality. We pair the source sentences of our devel-
opment set with each of the reference translations
and include the created sentence pairs in our bilin-
gual training corpus. Then we obtain word align-
ments on the new corpus (see Section 5.1 for the de-
tails of learning word alignments). From the word
alignments we obtain translation boundaries (see de-
tails in the next section). We parse the source sen-
tences of our development set and obtain constituent
boundaries from parse trees.
To make a clear comparison with our transla-
tion boundary classifiers (see Section 3.3), we treat
constituent boundaries deduced from source-side
parse trees as output from beginning/ending bound-
ary classifiers: the constituent beginning boundary
corresponds to By; the constituent ending boundary
corresponds to Ey. We have four reference transla-
tions for each source sentence. Therefore we have
four translation boundary sets, each of which is pro-
duced from word alignments between source sen-
tences and one reference translation set. Each of
the four translation boundary sets will be used as a
gold standard. We calculate classification accuracy
for our constituent boundary deducer on each gold
standard and average them finally.
Table 2 shows the accuracy results. The average
accuracies on the four gold standard sets are very
low, especially for the By/Bn classification task. In
section 3.3, we will show that our translation bound-
ary classifiers achieve higher accuracy than that of
constituent boundary deducer. This suggests that
pure constituent boundary based constraints are not
the best choice to constrain phrase-based decoding.
3 Learning Translation Boundaries
In this section, we investigate building classifiers
to predict translation boundaries. First, we elabo-
rate the acquisition of training instances from word
alignments. Second, we build two classifiers with
simple features on the obtained training instances.
Finally, we evaluate our classifiers on the develop-
ment set using the ?gold? translation boundaries ob-
tained from reference translations.
3.1 Obtaining Translation Boundaries from
Word Alignments
We can easily obtain constituent boundaries from
parse trees. Similarly, if we have a tree covering
both source and target sentence, we can easily get
translation boundaries from this tree. Fortunately,
we can build such a tree directly from word align-
ments. We use (Zhang et al, 2008)?s shift-reduce al-
gorithm (SRA) to decompose word alignments into
hierarchical trees.
Given an arbitrary word-level alignment as an in-
put, SRA is able to output a tree representation of the
word alignment (a.k.a decomposition tree). Each
node of the tree is a translation zone as we defined
in the Section 2. Therefore the first word on the
source side of each multi-source-word node is a be-
ginning translation boundary (? By); the last word
on the source side of each multi-source-word node
is an ending translation boundary (? Ey).
Figure 1a shows an example of many-to-many
alignment, where the source language is Chinese
and the target language is English. Each word is
indexed with their occurring position from left to
right. Figure 1b is the tree representation of the word
alignment after hierarchical analysis using SRA. We
use ([i, j], [p, q]) to denote a tree node, where i, j
and p, q are the beginning and ending index in the
source and target language, respectively. By check-
ing nodes which cover multiple source words, we
can easily decide that the source words {??, ?,
??} are in the class By and any other words are
in the class Bn if we want to train a By/Bn classi-
fier with class labels {By, Bn}. Similarly, the source
words {?,??,?,??} are in the class Ey and
any other words are in the class En when we train a
Ey/En classifier with class labels {Ey, En}.
By using SRA on each word-aligned bilingual
sentence, as described above, we can tag each source
word with two sets of class labels: {By, Bn} and
{Ey, En}. The tagged source sentences will be used
to train our two translation boundary classifiers.
138
?? ??? ? ?? ??
The last five flights all failed due to accidents
?
1 2 3 4 5 6 7
1 2 3 4 5 6 7 8 9
([1, 7], [1, 9])
([6, 7], [6, 9])
([6, 6], [7, 9]) ([7, 7], [6, 6])
([1, 5], [1, 5])
([1, 4], [1, 4]) ([5, 5], [5, 5])
([1, 3], [1, 3]) ([4, 4], [4, 4])
([1, 1], [1, 2]) ([2, 3], [3, 3])
a) b)
Figure 1: An example of many-to-many word alignment and its tree representation produced by (Zhang et al, 2008)?s
shift-reduce algorithm.
3.2 Building Translation Boundary Classifiers
We build two discriminative classifiers based on
Maximum Entropy Markov Models (MEMM) (Mc-
Callum et al, 2000). One classifier is to predict the
word class ? ? {By, Bn} for each source word. The
other is to predict the word class ? ? {Ey, En}.
These two classifiers are separately trained using
training instances obtained from our word-aligned
training data as demonstrated in the last section.
We use features from surrounding words, includ-
ing 2 before and 2 after the current word position
(c?2, c?1, c+1, c+2). We also use class features to
train models with Markov order 1 (including class
feature ?c?1), and Markov order 2 (including class
features ?c?1 , ?c?2).
3.3 Evaluating Translation Boundary
Classifiers
How well can we perform these binary classifica-
tion tasks using the classifiers described above? Can
we obtain better translation boundary predictions
than extracting constituent boundary from source-
side parse trees? To investigate these questions, we
evaluate our MEMM based classifiers. We trained
them on our 100M-word word-aligned corpus. We
ran the two trained classifiers on the development
set separately to obtain the By/Bn words and Ey/En
words. Then we built our four gold standards using
four reference translation sets as described in Sec-
Avg. Accuracy (%)
Classification Task MEMM 1 MEMM 2
By/Bn 71.7 70.2
Ey/En 59.2 58.8
Table 3: Average classification accuracy on the develop-
ment set for our MEMM based translation boundary clas-
sifiers with various Markov orders.
tion 2. The average classification accuracy results
are shown in Table 3.
Comparing Table 3 with Table 2, we find that our
MEMM based classifiers significantly outperform
constituent boundary deducer in predicting transla-
tion boundaries, especially in the By/Bn classifi-
cation task, where our MEMM based By/Bn clas-
sifier (Markov order 1) achieves a relative increase
of 52.9% in accuracy over the constituent bound-
ary deducer. In the Ey/En classification task, our
classifiers also perform much better than constituent
boundary deducer.
Then are our MEMM based translation boundary
classifiers good enough? The accuracies are still low
although they are higher than those of constituent
boundary deducer. One reason why we have low
accuracies is that our gold standard based evalua-
tion is not established on real gold standards. In
other words, we don?t have gold standards in terms
of translation boundary since different translations
139
Classification Task Avg. Accuracy (%)
By/Bn 80.6
Ey/En 75.7
Table 4: Average classification accuracy on the develop-
ment set when treating each reference translation set as a
boundary classifier.
generate very different translation boundaries. We
can measure these differences in reference transla-
tions using the same evaluation metric (classification
accuracy). We treat each reference translation set
as a translation boundary classifier while the other
three reference translation sets as gold standards.
We calculate the classification accuracy for the cur-
rent reference translation set and finally average all
four accuracies. Table 4 presents the results.
Comparing Table 4 with Table 3, we can see that
the accuracy of our translation boundary classifica-
tion approach is not that low when considering vast
divergences of reference translations. The question
now becomes, how can classifier output be used to
constrain phrase-based decoding, and what is the
impact on the system performance of using such
constraints.
4 Integrating Translation Boundaries into
Decoding
By running the two trained classifiers on the source
sentence separately, we obtain two classified word
sets: By/Bn words, and Ey/En words. We can pro-
hibit any translations or reorderings spanning ci...cj
(j > i) where ci /? By according to the first classi-
fier or cj /? Ey according to the second classifier. In
such a way, we integrate translation boundaries into
phrase-based decoding as hard constraints, which,
however, is at the risk of producing no translation
covering the whole source sentence.
Alternatively, we introduce soft constraints based
on translation boundary that our classifiers pre-
dict, similar to constituent boundary based soft con-
straints in (Cherry, 2008) and (Marton and Resnik,
2008). We add a new feature to the decoder?s log-
linear model: translation boundary violation count-
ing feature. This counting feature accumulates
whenever hypotheses have a partial translation span-
ning ci...cj (j > i) where ci /? By or cj /? Ey. The
LDC ID Description
LDC2004E12 United Nations
LDC2004T08 Hong Kong News
LDC2005T10 Sinorama Magazine
LDC2003E14 FBIS
LDC2002E18 Xinhua News V1 beta
LDC2005T06 Chinese News Translation
LDC2003E07 Chinese Treebank
LDC2004T07 Multiple Translation Chinese
Table 5: Training corpora.
weight ?v of this feature is tuned via minimal error
rate training (MERT) (Och, 2003) with other feature
weights.
Unlike hard constraints, which simply prevent
any hypotheses from violating translation bound-
aries, soft constraints allow violations of translation
boundaries but with a penalty of exp(??vCv) where
Cv is the violation count. By using soft constraints,
we can enable the model to prefer hypotheses which
are consistent with translation boundaries.
5 Experiment
Our baseline system is a phrase-based system us-
ing BTGs (Wu, 1997), which includes a content-
dependent reordering model discriminatively trained
using reordering examples (Xiong et al, 2006). We
carried out various experiments to evaluate the im-
pact of integrating translation boundary based soft
constraints into decoding on the system performance
on the Chinese-to-English translation task of the
NIST MT-05 using large scale training data.
5.1 Experimental Setup
Our training corpora are listed in Table 5. The
whole corpora consist of 96.9M Chinese words and
109.5M English words in 3.8M sentence pairs. We
ran GIZA++ (Och and Ney, 2000) on the par-
allel corpora in both directions and then applied
the ?grow-diag-final? refinement rule (Koehn et al,
2005) to obtain many-to-many word alignments.
From the word-aligned corpora, we extracted bilin-
gual phrases and trained our translation model.
We used all corpora in Table 5 except for the
United Nations corpus to train our MaxEnt based
reordering model (Xiong et al, 2006), which con-
140
sist of 33.3M Chinese words and 35.8M English
words. We built a four-gram language model us-
ing the SRILM toolkit (Stolcke, 2002), which was
trained on Xinhua section of the English Gigaword
corpus (181.1M words).
To train our translation boundary classifiers, we
extract training instances from the whole word-
aligned corpora, from which we obtain 96.9M train-
ing instances for the By/Bn and Ey/En classifier.
We ran the off-the-shelf MaxEnt toolkit (Zhang,
2004) to tune classifier feature weights with Gaus-
sian prior set to 1 to avoid overfitting.
We used the NIST MT-03 evaluation test data as
our development set (919 sentences in total, 27.1
words per sentence). The NIST MT-05 test set in-
cludes 1082 sentences with an average of 27.4 words
per sentence. Both the reference corpus for the NIST
MT-03 set and the reference corpus for the NIST
MT-05 set contain 4 translations per source sen-
tence. To compare with constituent boundary based
constraints, we parsed source sentences of both the
development and test sets using a Chinese parser
(Xiong et al, 2005) which was trained on the Penn
Chinese Treebank with an F1-score of 79.4%.
Our evaluation metric is case-insensitive BLEU-4
(Papineni et al, 2002) using the shortest reference
sentence length for the brevity penalty. Statistical
significance in BLEU score differences was tested
by paired bootstrap re-sampling (Koehn, 2004).
5.2 Using Translation Boundaries from
Reference Translations
The most direct way to investigate the impact on the
system performance of using translation boundaries
is to integrate ?right? translation boundaries into de-
coding which are directly obtained from reference
translations. For both the development set and test
set, we have four reference translation sets, which
are named ref1, ref2, ref3 and ref4, respectively.
For the development set, we used translation bound-
aries obtained from ref1. Based on these boundaries,
we built our translation boundary violation counting
feature and tuned its feature weight with other fea-
tures using MERT. When we obtained the best fea-
ture weights ?s, we evaluated on the test set using
translation boundaries produced from ref1, ref2, ref3
and ref4 of the test set respectively.
Table 6 shows the results. We clearly see that us-
System BLEU-4 (%)
Base 33.05
Ref1 33.99*
Ref2 34.17*
Ref3 33.93*
Ref4 34.21*
Table 6: Results of using translation boundaries obtained
from reference translations. *: significantly better than
baseline (p < 0.01).
ing ?right? translation boundaries to build soft con-
straints significantly improve the performance mea-
sured by BLEU score. The best result comes from
ref4, which achieves an absolute increase of 1.16
BLEU points over the baseline. We believe that the
best result here only indicates the lower bound of
potential improvement when using right translation
boundaries. If we have consistent translation bound-
aries on the development and test set (for example,
we have the same 4 translators build reference trans-
lations for both the development and test set), the
performance improvement will be higher.
5.3 Using Automatically Learned Translation
Boundaries
The success of using translation boundaries from
reference translations inspires us to pursue trans-
lation boundaries predicted by our MEMM based
classifiers. We ran our MEMM1 (Markov order 1)
and MEMM2 (Markov order 2) By/Bn and Ey/En
classifiers on both the development and test set.
Based on translation boundaries output by MEMM1
and MEMM2 classifiers, we built our translation
boundary violation feature and tuned it on the de-
velopment set. The evaluation results on the test set
are shown in Table 7.
From Table 7 we observe that using soft con-
straints based on translation boundaries from both
our MEMM 1 and MEMM 2 significantly outper-
form the baseline. Impressively, when using outputs
from MEMM 2, we achieve an absolute improve-
ment of almost 1 BLEU point over the baseline. This
result is also very close to the best result of using
translation boundaries from reference translations.
To compare with constituent boundary based syn-
tactic constraints, we also carried out experiments
using two kinds of such constraints. One is the
141
System BLEU-4 (%)
Base 33.05
Condeducer 33.18
XP+ 33.58*
BestRef 34.21*+
MEMM 1 33.70*
MEMM 2 34.04*+
Table 7: Results of using automatically learned trans-
lation boundaries. Condeducer means using pure con-
stituent boundary based soft constraint. XP+ is another
constituent boundary based soft constraint but with dis-
tinction among special constituent types (Marton and
Resnik, 2008). BestRef is the best result using reference
translation boundaries in Table 6. MEMM 1 and MEMM
2 are our MEMM based translation boundary classifiers
with Markov order 1 and 2. *: significantly better than
baseline (p < 0.01). +: significantly better than XP+
(p < 0.01).
Condeducer which uses pure constituent bound-
ary based syntactic constraint: any partial transla-
tions which cross any constituent boundaries will
be penalized. The other is the XP+ from (Marton
and Resnik, 2008) which only penalizes hypotheses
which violate the boundaries of a constituent with
a label from {NP, VP, CP, IP, PP, ADVP, QP, LCP,
DNP}. The XP+ is the best syntactic constraint
among all constraints that Marton and Resnik (2008)
use for Chinese-to-English translation.
Still in Table 7, we find that both syntactic con-
straint Condeducer and XP+ are better than the base-
line. But only XP+ is able to obtain significant im-
provement. Both our MEMM 1 and MEMM 2 out-
perform Condeducer. MEMM 2 achieves significant
improvement over XP+ by approximately 0.5 BLEU
points. This comparison suggests that translation
boundary is a better option than constituent bound-
ary when we build constraints to restrict phrase-
based decoding.
5.4 One Classifier vs. Two Classifiers
Revisiting the classification task in this paper, we
can also consider it as a sequence labeling task
where the first source word of a translation zone
is labeled ?B?, the last source word of the trans-
lation zone is labeled ?E?, and other words are la-
beled ?O?. To complete such a sequence labeling
task, we built only one classifier which is still based
on MEMM (with Markov order 2) with the same
features as described in Section 3.2. We built soft
constraints based on the outputs of this classifier and
evaluated them on the test set. The case-insensitive
BLEU score is 33.62, which is lower than the per-
formance of using two separate classifiers (34.04).
We calculated the accuracy for class ?B? by map-
ping ?B? to By and ?E? and ?O? to Bn. The result is
67.9%. Similarly, we obtained the accuracy of class
?E?, which is as low as 48.6%. These two accura-
cies are much lower than those of using two separate
classifiers, especially the accuracy of ?E?. This sug-
gests that the By and Ey are not interrelated tightly.
It is better to learn them separately with two classi-
fiers.
Another advantage of using two separate classi-
fiers is that we can explore more constraints. A word
ck can be possibly labeled asBy by the first classifier
and Ey by the second classifier. Therefore we can
build soft constraints on span (ci, ck) (ci ? By, ck ?
Ey) and span (ck, cj) (ck ? By, cj ? Ey). This is
impossible if we use only one classifier since each
word can have only one class label. We can build
only one constraint on span (ci, ck) or span (ck, cj).
6 Related Work
Various approaches incorporate constraints into
phrase-based decoding in a soft or hard manner. Our
introduction has already briefly mentioned (Cherry,
2008) and (Marton and Resnik, 2008), which utilize
source-side parse tree boundary violation counting
feature to build soft constraints for phrase-based de-
coding, and (Xiong et al, 2009), which calculates a
score to indicate to what extent a source phrase can
be translated as a unit using a bracketing model with
richer syntactic features. More previously, (Chi-
ang, 2005) rewards hypotheses whenever they ex-
actly match constituent boundaries of parse trees on
the source side.
In addition, hard linguistic constraints are also ex-
plored. (Wu and Ng, 1995) employs syntactic brack-
eting information to constrain search in order to im-
prove speed and accuracy. (Collins et al, 2005) and
(Wang et al, 2007) use hard syntactic constraints to
perform reorderings according to source-side parse
trees. (Xiong et al, 2008) prohibit any swappings
142
which violate punctuation based constraints.
Non-linguistic constraints are also widely used
in phrase-based decoding. The IBM and ITG con-
straints (Zens et al, 2004) are used to restrict re-
orderings in practical phrase-based systems.
(Berger et al, 1996) introduces the concept of rift
into a machine translation system, which is similar
to our definition of translation boundary. They also
use a maximum entropy model to predict whether a
source position is a rift based on features only from
source sentences. Our work differs from (Berger et
al., 1996) in three major respects.
1) We distinguish a segment boundary into two
categories: beginning and ending boundary due
to their different distributions (see Table 1).
However, Berger et al ignore this difference.
2) We train two classifiers to predict beginning
and ending boundary respectively while Berger
et al build only one classifier. Our experiments
show that two separate classifiers outperform
one classifier.
3) The last difference is how segment bound-
aries are integrated into a machine transla-
tion system. Berger et al use predicted
rifts to divide a long source sentence into a
series of smaller segments, which are then
translated sequentially in order to increase de-
coding speed (Brown et al, 1992; Berger
et al, 1996). This can be considered as a
hard integration, which may undermine trans-
lation accuracy given wrongly predicted rifts.
We integrate predicted translation boundaries
into phrase-based decoding in a soft manner,
which improves translation accuracy in terms
of BLEU score.
7 Conclusion and Future Work
In this paper, we have presented a simple approach
to learn translation boundaries on source sentences.
The learned translation boundaries are used to con-
strain phrase-based decoding in a soft manner. The
whole approach has several properties.
? First, it is based on a simple classification task
that can achieve considerably high accuracy
when taking translation divergences into ac-
count using simple models and features.
? Second, the classifier output can be straightfor-
wardly used to constrain phrase-based decoder.
? Finally, we have empirically shown that, to
build soft constraints for phrase-based decod-
ing, translation boundary predicted by our clas-
sifier is a better choice than constituent bound-
ary deduced from source-side parse tree.
Future work in this direction will involve trying
different methods to define more informative trans-
lation boundaries, such as a boundary to begin/end
a swapping. We would also like to investigate new
methods to incorporate automatically learned trans-
lation boundaries more efficiently into decoding in
an attempt to further improve search in both speed
and accuracy.
References
Adam L. Berger, Stephen A. Della Pietra and Vincent J.
Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistics, 22(1):39-71.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Robert L. Mercer, and Surya Mohanty.
1992. Dividing and Conquering Long Sentences in a
Translation System. In Proceedings of the workshop
on Speech and Natural Language, Human Language
Technology.
Colin Cherry. 2008. Cohesive Phrase-based Decoding
for Statistical Machine Translation. In Proceedings of
ACL.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL, pages 263?270.
Michael Collins, Philipp Koehn and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL.
Kevin Knight. 1999. Decoding Complexity in Word Re-
placement Translation Models. In Computational Lin-
guistics, 25(4):607? 615.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
143
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrase-Based Translation.
In Proceedings of ACL.
Andrew McCallum, Dayne Freitag and Fernando Pereira
2000. Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation. In Proceedings
of the Seventeenth International Conference on Ma-
chine Learning 2000.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL
2000.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatically
Evaluation of Machine Translation. In Proceedings of
ACL 2002.
Brian Roark and Kristy Hollingshead. 2008. Classifying
Chart Cells for Quadratic Complexity Context-Free In-
ference. In Proceedings of COLING 2008.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Chao Wang, Michael Collins and Philipp Koehn 2007.
Chinese Syntactic Reordering for Statistical Machine
Translation. In Proceedings of EMNLP.
Dekai Wu and Cindy Ng. 1995. Using Brackets to Im-
prove Search for Statistical Machine Translation In
Proceedings of PACLIC-IO, Pacific Asia Conference
on Language, Information and Computation.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
bank with Semantic Knowledge. In Proceedings of
IJCNLP, Jeju Island, Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase ReorderingModel for Sta-
tistical Machine Translation. In Proceedings of ACL-
COLING 2006.
Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, Qun Liu
and Shouxun Lin. 2008. Refinements in BTG-based
Statistical Machine Translation. In Proceedings of
IJCNLP 2008.
Deyi Xiong, Min Zhang, Ai Ti Aw, and Haizhou Li.
2009. A Syntax-Driven Bracketing Model for Phrase-
Based Translation. In Proceedings of ACL-IJCNLP
2009.
Richard Zens, Hermann Ney, TaroWatanabe and Eiichiro
Sumita 2004. Reordering Constraints for Phrase-
Based Statistical Machine Translation. In Proceedings
of COLING.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting Synchronous Grammars Rules from Word-
Level Alignments in Linear Time. In Proceeding of
COLING 2008.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
144
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604?611,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Error Detection for Statistical Machine
Translation Using Linguistic Features
Deyi Xiong, Min Zhang, Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632.
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
Abstract
Automatic error detection is desired in
the post-processing to improve machine
translation quality. The previous work is
largely based on confidence estimation us-
ing system-based features, such as word
posterior probabilities calculated from N -
best lists or word lattices. We propose to
incorporate two groups of linguistic fea-
tures, which convey information from out-
side machine translation systems, into er-
ror detection: lexical and syntactic fea-
tures. We use a maximum entropy clas-
sifier to predict translation errors by inte-
grating word posterior probability feature
and linguistic features. The experimen-
tal results show that 1) linguistic features
alone outperform word posterior probabil-
ity based confidence estimation in error
detection; and 2) linguistic features can
further provide complementary informa-
tion when combined with word confidence
scores, which collectively reduce the clas-
sification error rate by 18.52% and im-
prove the F measure by 16.37%.
1 Introduction
Translation hypotheses generated by a statistical
machine translation (SMT) system always contain
both correct parts (e.g. words, n-grams, phrases
matched with reference translations) and incor-
rect parts. Automatically distinguishing incorrect
parts from correct parts is therefore very desir-
able not only for post-editing and interactive ma-
chine translation (Ueffing and Ney, 2007) but also
for SMT itself: either by rescoring hypotheses in
the N -best list using the probability of correct-
ness calculated for each hypothesis (Zens and Ney,
2006) or by generating new hypotheses using N -
best lists from one SMT system or multiple sys-
tems (Akibay et al, 2004; Jayaraman and Lavie,
2005).
In this paper we restrict the ?parts? to words.
That is, we detect errors at the word level for SMT.
A common approach to SMT error detection at the
word level is calculating the confidence at which a
word is correct. The majority of word confidence
estimation methods follows three steps:
1) Calculate features that express the correct-
ness of words either based on SMT model
(e.g. translation/language model) or based on
SMT system output (e.g. N -best lists, word
lattices) (Blatz et al, 2003; Ueffing and Ney,
2007).
2) Combine these features together with a clas-
sification model such as multi-layer percep-
tron (Blatz et al, 2003), Naive Bayes (Blatz
et al, 2003; Sanchis et al, 2007), or log-
linear model (Ueffing and Ney, 2007).
3) Divide words into two groups (correct trans-
lations and errors) by using a classification
threshold optimized on a development set.
Sometimes the step 2) is not necessary if only one
effective feature is used (Ueffing and Ney, 2007);
and sometimes the step 2) and 3) can be merged
into a single step if we directly output predicting
results from binary classifiers instead of making
thresholding decision.
Various features from different SMT models
and system outputs are investigated (Blatz et al,
2003; Ueffing and Ney, 2007; Sanchis et al, 2007;
Raybaud et al, 2009). Experimental results show
that they are useful for error detection. However,
it is not adequate to just use these features as dis-
cussed in (Shi and Zhou, 2005) because the infor-
mation that they carry is either from the inner com-
ponents of SMT systems or from system outputs.
To some extent, it has already been considered by
SMT systems. Hence finding external information
604
sources from outside SMT systems is desired for
error detection.
Linguistic knowledge is exactly such a good
choice as an external information source. It has al-
ready been proven effective in error detection for
speech recognition (Shi and Zhou, 2005). How-
ever, it is not widely used in SMT error detection.
The reason is probably that people have yet to find
effective linguistic features that outperform non-
linguistic features such as word posterior proba-
bility features (Blatz et al, 2003; Raybaud et al,
2009). In this paper, we would like to show an
effective use of linguistic features in SMT error
detection.
We integrate two sets of linguistic features into
a maximum entropy (MaxEnt) model and develop
aMaxEnt-based binary classifier to predict the cat-
egory (correct or incorrect) for each word in a
generated target sentence. Our experimental re-
sults show that linguistic features substantially im-
prove error detection and even outperform word
posterior probability features. Further, they can
produce additional improvements when combined
with word posterior probability features.
The rest of the paper is organized as follows. In
Section 2, we review the previous work on word-
level confidence estimation which is used for error
detection. In Section 3, we introduce our linguistic
features as well as the word posterior probability
feature. In Section 4, we elaborate our MaxEnt-
based error detection model which combine lin-
guistic features and word posterior probability fea-
ture together. In Section 5, we describe the SMT
system which we use to generate translation hy-
potheses. We report our experimental results in
Section 6 and conclude in Section 7.
2 Related Work
In this section, we present an overview of confi-
dence estimation (CE) for machine translation at
the word level. As we are only interested in error
detection, we focus on work that uses confidence
estimation approaches to detect translation errors.
Of course, confidence estimation is not limited to
the application of error detection, it can also be
used in other scenarios, such as translation predic-
tion in an interactive environment (Grandrabur and
Foster, 2003) .
In a JHU workshop, Blatz et al (2003) investi-
gate using neural networks and a naive Bayes clas-
sifier to combine various confidence features for
confidence estimation at the word level as well as
at the sentence level. The features they use for
word level CE include word posterior probabil-
ities estimated from N -best lists, features based
on SMT models, semantic features extracted from
WordNet as well as simple syntactic features, i.e.
parentheses and quotation mark check. Among all
these features, the word posterior probability is the
most effective feature, which is much better than
linguistic features such as semantic features, ac-
cording to their final results.
Ueffing and Ney (2007) exhaustively explore
various word-level confidence measures to label
each word in a generated translation hypothe-
sis as correct or incorrect. All their measures
are based on word posterior probabilities, which
are estimated from 1) system output, such as
word lattices or N -best lists and 2) word or
phrase translation table. Their experimental re-
sults show that word posterior probabilities di-
rectly estimated from phrase translation table are
better than those from system output except for the
Chinese-English language pair.
Sanchis et al (2007) adopt a smoothed naive
Bayes model to combine different word posterior
probability based confidence features which are
estimated from N -best lists, similar to (Ueffing
and Ney, 2007).
Raybaud et al (2009) study several confi-
dence features based on mutual information be-
tween words and n-gram and backward n-gram
language model for word-level and sentence-level
CE. They also explore linguistic features using in-
formation from syntactic category, tense, gender
and so on. Unfortunately, such linguistic features
neither improve performance at the word level nor
at the sentence level.
Our work departs from the previous work in two
major respects.
? We exploit various linguistic features and
show that they are able to produce larger im-
provements than widely used system-related
features such as word posterior probabilities.
This is in contrast to some previous work. Yet
another advantage of using linguistic features
is that they are system-independent, which
therefore can be used across different sys-
tems.
? We treat error detection as a complete bi-
nary classification problem. Hence we di-
605
rectly output prediction results from our dis-
criminatively trained classifier without opti-
mizing a classification threshold on a distinct
development set beforehand.1 Most previous
approaches make decisions based on a pre-
tuned classification threshold ? as follows
class =
{
correct, ?(correct, ?) > ?
incorrect, otherwise
where ? is a classifier or a confidence mea-
sure and ? is the parameter set of ?. The per-
formance of these approaches is strongly de-
pendent on the classification threshold.
3 Features
We explore two sets of linguistic features for each
word in a machine generated translation hypoth-
esis. The first set of linguistic features are sim-
ple lexical features. The second set of linguistic
features are syntactic features which are extracted
from link grammar parse. To compare with the
previously widely used features, we also investi-
gate features based on word posterior probabili-
ties.
3.1 Lexical Features
We use the following lexical features.
? wd: word itself
? pos: part-of-speech tag from a tagger trained
on WSJ corpus. 2
For each word, we look at previous n
words/tags and next n words/tags. They together
form a word/tag sequence pattern. The basic idea
of using these features is that words in rare pat-
terns are more likely to be incorrect than words
in frequently occurring patterns. To some extent,
these two features have similar function to a tar-
get language model or pos-based target language
model.
3.2 Syntactic Features
High-level linguistic knowledge such as syntac-
tic information about a word is a very natural and
promising indicator to decide whether this word is
syntactically correct or not. Words occurring in an
1This does not mean we do not need a development set.
We do validate our feature selection and other experimental
settings on the development set.
2Available via http://www-tsujii.is.s.u-tokyo.ac.jp/
?tsuruoka/postagger/
ungrammatical part of a target sentence are prone
to be incorrect. The challenge of using syntac-
tic knowledge for error detection is that machine-
generated hypotheses are rarely fully grammati-
cal. They are mixed with grammatical and un-
grammatical parts, which hence are not friendly
to traditional parsers trained on grammatical sen-
tences because ungrammatical parts of a machine-
generated sentence could lead to a parsing failure.
To overcome this challenge, we select the Link
Grammar (LG) parser 3 as our syntactic parser to
generate syntactic features. The LG parser pro-
duces a set of labeled links which connect pairs of
words with a link grammar (Sleator and Temper-
ley, 1993).
The main reason why we choose the LG parser
is that it provides a robustness feature: null-link
scheme. The null-link scheme allows the parser to
parse a sentence even when the parser can not fully
interpret the entire sentence (e.g. including un-
grammatical parts). When the parser fail to parse
the entire sentence, it ignores one word each time
until it finds linkages for remaining words. After
parsing, those ignored words are not connected to
any other words. We call them null-linked words.
Our hypothesis is that null-linked words are
prone to be syntactically incorrect. We hence
straightforwardly define a syntactic feature for a
word w according to its links as follows
link(w) =
{
yes, w has links
no, otherwise
In Figure 1 we show an example of a generated
translation hypothesis with its link parse. Here
links are denoted with dotted lines which are an-
notated with link types (e.g., Jp, Op). Bracketed
words, namely ?,? and ?including?, are null-linked
words.
3.3 Word Posterior Probability Features
Our word posterior probability is calculated onN -
best list, which is first proposed by (Ueffing et al,
2003) and widely used in (Blatz et al, 2003; Ueff-
ing and Ney, 2007; Sanchis et al, 2007).
Given a source sentence f , let {en}N1 be theN -
best list generated by an SMT system, and let ein is
the i-th word in en. The major work of calculating
word posterior probabilities is to find the Leven-
shtein alignment (Levenshtein, 1966) between the
best hypothesis e1 and its competing hypothesis
3Available at http://www.link.cs.cmu.edu/link/
606
Figure 1: An example of Link Grammar parsing results.
en in the N -best list {en}N1 . We denote the align-
ment between them as ?(e1, en). The word in the
hypothesis en which ei1 is Levenshtein aligned to
is denoted as ?i(e1, en).
The word posterior probability of ei1 is then cal-
culated by summing up the probabilities over all
hypotheses containing ei1 in a position which is
Levenshtein aligned to ei1.
pwpp(ei1) =
?
en: ?i(e1,en)=ei1
p(en)
?N
1 p(en)
To use the word posterior probability in our er-
ror detection model, we need to make it discrete.
We introduce a feature for a word w based on its
word posterior probability as follows
dwpp(w) = ??log(pwpp(w))/df?
where df is the discrete factor which can be set to
1, 0.1, 0.01 and so on. ?? ?? is a rounding oper-
ator which takes the largest integer that does not
exceed ?log(pwpp(w))/df . We optimize the dis-
crete factor on our development set and find the
optimal value is 1. Therefore a feature ?dwpp =
2? represents that the logarithm of the word poste-
rior probability is between -3 and -2;
4 Error Detection with a Maximum
Entropy Model
As mentioned before, we consider error detec-
tion as a binary classification task. To formal-
ize this task, we use a feature vector ? to rep-
resent a word w in question, and a binary vari-
able c to indicate whether this word is correct or
not. In the feature vector, we look at 2 words
before and 2 words after the current word posi-
tion (w?2, w?1, w, w1, w2). We collect features
{wd, pos, link, dwpp} for each word among these
words and combine them into the feature vector
? for w. As such, we want the feature vector to
capture the contextual environment, e.g., pos se-
quence pattern, syntactic pattern, where the word
w occurs.
For classification, we employ the maximum
entropy model (Berger et al, 1996) to predict
whether a word w is correct or incorrect given its
feature vector ?.
p(c|?) = exp(
?
i ?ifi(c, ?))
?
c? exp(
?
i ?ifi(c?, ?))
where fi is a binary model feature defined on c
and the feature vector ?. ?i is the weight of fi.
Table 1 shows some examples of our binary model
features.
In order to learn the model feature weights ? for
probability estimation, we need a training set of
m samples {?i, ci}m1 . The challenge of collect-
ing training instances is that the correctness of a
word in a generated translation hypothesis is not
intuitively clear (Ueffing and Ney, 2007). We will
describe the method to determine the correctness
of a word in Section 6.1, which is broadly adopted
in previous work.
We tune our model feature weights using an
off-the-shelf MaxEnt toolkit (Zhang, 2004). To
avoid overfitting, we optimize the Gaussian prior
on the development set. During test, if the proba-
bility p(correct|?) is larger than p(incorrect|?)
according the trained MaxEnt model, the word is
labeled as correct otherwise incorrect.
5 SMT System
To obtain machine-generated translation hypothe-
ses for our error detection, we use a state-of-the-art
phrase-based machine translation system MOSES
(Koehn et al, 2003; Koehn et al, 2007). The
translation task is on the official NIST Chinese-
to-English evaluation data. The training data con-
sists of more than 4 million pairs of sentences (in-
cluding 101.93MChinese words and 112.78M En-
glish words) from LDC distributed corpora. Table
2 shows the corpora that we use for the translation
task.
We build a four-gram language model using the
SRILM toolkit (Stolcke, 2002), which is trained
607
Feature Example
wd f(c, ?) =
{
1, ?.w.wd = ?.?, c = correct
0, otherwise
pos f(c, ?) =
{
1, ?.w2.pos = ?NN?, c = incorrect
0, otherwise
link f(c, ?) =
{
1, ?.w.link = no, c = incorrect
0, otherwise
dwpp f(c, ?) =
{
1, ?.w?2.dwpp = 2, c = correct
0, otherwise
Table 1: Examples of model features.
LDC ID Description
LDC2004E12 United Nations
LDC2004T08 Hong Kong News
LDC2005T10 Sinorama Magazine
LDC2003E14 FBIS
LDC2002E18 Xinhua News V1 beta
LDC2005T06 Chinese News Translation
LDC2003E07 Chinese Treebank
LDC2004T07 Multiple Translation Chinese
Table 2: Training corpora for the translation task.
on Xinhua section of the English Gigaword cor-
pus (181.1M words). For minimum error rate tun-
ing (Och, 2003), we use NIST MT-02 as the de-
velopment set for the translation task. In order
to calculate word posterior probabilities, we gen-
erate 10,000 best lists for NIST MT-02/03/05 re-
spectively. The performance, in terms of BLEU
(Papineni et al, 2002) score, is shown in Table 4.
6 Experiments
We conducted our experiments at several levels.
Starting with MaxEnt models with single linguis-
tic feature or word posterior probability based fea-
ture, we incorporated additional features incre-
mentally by combining features together. In do-
ing so, we would like the experimental results not
only to display the effectiveness of linguistic fea-
tures for error detection but also to identify the ad-
ditional contribution of each feature to the task.
6.1 Data Corpus
For the error detection task, we use the best trans-
lation hypotheses of NIST MT-02/05/03 generated
by MOSES as our training, development, and test
corpus respectively. The statistics about these cor-
pora is shown in Table 3. Each translation hypoth-
esis has four reference translations.
Corpus Sentences Words
Training MT-02 878 24,225
Development MT-05 1082 31,321
Test MT-03 919 25,619
Table 3: Corpus statistics (number of sentences
and words) for the error detection task.
To obtain the linkage information, we run the
LG parser on all translation hypotheses. We find
that the LG parser can not fully parse 560 sen-
tences (63.8%) in the training set (MT-02), 731
sentences (67.6%) in the development set (MT-05)
and 660 sentences (71.8%) in the test set (MT-03).
For these sentences, the LG parser will use the the
null-link scheme to generate null-linked words.
To determine the true class of a word in a gen-
erated translation hypothesis, we follow (Blatz et
al., 2003) to use the word error rate (WER). We
tag a word as correct if it is aligned to itself in
the Levenshtein alignment between the hypothesis
and the nearest reference translation that has min-
imum edit distance to the hypothesis among four
reference translations. Figure 2 shows the Lev-
enshtein alignment between a machine-generated
hypothesis and its nearest reference translation.
The ?Class? row shows the label of each word ac-
cording to the alignment, where ?c? and ?i? repre-
sent correct and incorrect respectively.
There are several other metrics to tag single
words in a translation hypothesis as correct or in-
correct, such as PER where a word is tagged as
correct if it occurs in one of reference translations
with the same number of occurrences, Setwhich is
a less strict variant of PER, ignoring the number of
occurrences per word. In Figure 2, the two words
?last year? in the hypothesis will be tagged as cor-
rect if we use the PER or Set metric since they do
not consider the occurring positions of words. Our
608
China Unicom net profit rose up 38% last year
China Unicom net profit rose up 38%last yearHypothesis
Reference
China/c Unicom/c net/c profit/c rose/c up/c 38%/clast/i year/iClass
Figure 2: Tagging a word as correct/incorrect according to the Levenshtein alignment.
Corpus BLEU (%) RCW (%)
MT-02 33.24 47.76
MT-05 32.03 47.85
MT-03 32.86 47.57
Table 4: Case-insensitive BLEU score and ratio
of correct words (RCW) on the training, develop-
ment and test corpus.
metric corresponds to the m-WER used in (Ueff-
ing and Ney, 2007), which is stricter than PER and
Set. It is also stricter than normal WER metric
which compares each hypothesis to all references,
rather than the nearest reference.
Table 4 shows the case-insensitive BLEU score
and the percentage of words that are labeled as cor-
rect according to the method described above on
the training, development and test corpus.
6.2 Evaluation Metrics
To evaluate the overall performance of the error
detection, we use the commonly used metric, clas-
sification error rate (CER) to evaluate our classi-
fiers. CER is defined as the percentage of words
that are wrongly tagged as follows
CER = # of wrongly tagged words
Total # of words
The baseline CER is determined by assuming
the most frequent class for all words. Since the ra-
tio of correct words in both the development and
test set is lower than 50%, the most frequent class
is ?incorrect?. Hence the baseline CER in our ex-
periments is equal to the ratio of correct words as
these words are wrongly tagged as incorrect.
We also use precision and recall on errors to
evaluate the performance of error detection. Let
ng be the number of words of which the true class
is incorrect, nt be the number of words which are
tagged as incorrect by classifiers, and nm be the
number of words tagged as incorrect that are in-
deed translation errors. The precision Pre is the
percentage of words correctly tagged as transla-
tion errors.
Pre = nm
nt
The recall Rec is the proportion of actual transla-
tion errors that are found by classifiers.
Rec = nm
ng
F measure, the trade-off between precision and re-
call, is also used.
F = 2 ? Pre?Rec
Pre+Rec
6.3 Experimental Results
Table 5 shows the performance of our experiments
on the error detection task. To compare with pre-
vious work using word posterior probabilities for
confidence estimation, we carried out experiments
using wpp estimated from N -best lists with the
classification threshold ? , which was optimized on
our development set to minimize CER. A relative
improvement of 9.27% is achieved over the base-
line CER, which reconfirms the effectiveness of
word posterior probabilities for error detection.
We conducted three groups of experiments us-
ing the MaxEnt based error detection model with
various feature combinations.
? The first group of experiments uses single
feature, such as dwpp, pos. We find the
most effective feature is pos, which achieves
a 16.12% relative improvement over the base-
line CER and 7.55% relative improvement
over the CER of word posterior probabil-
ity thresholding. Using discrete word pos-
terior probabilities as features in the Max-
Ent based error detection model is marginally
better than word posterior probability thresh-
olding in terms of CER, but obtains a 13.79%
relative improvement in F measure. The syn-
tactic feature link also improves the error de-
tection in terms of CER and particularly re-
call.
609
Combination Features CER (%) Pre (%) Rec (%) F (%)
Baseline - 47.57 - - -
Thresholding wpp - 43.16 58.98 58.07 58.52
MaxEnt (dwpp) 44 43.07 56.12 81.86 66.59
MaxEnt (wd) 19,164 41.57 58.25 73.11 64.84
MaxEnt (pos) 199 39.90 58.88 79.23 67.55
MaxEnt (link) 19 44.31 54.72 89.72 67.98
MaxEnt (wd+ pos) 19,363 39.43 59.36 78.60 67.64
MaxEnt (wd+ pos+ link) 19,382 39.79 58.74 80.97 68.08
MaxEnt (dwpp+ wd) 19,208 41.04 57.18 83.75 67.96
MaxEnt (dwpp+ wd+ pos) 19,407 38.88 59.87 78.38 67.88
MaxEnt (dwpp+ wd+ pos+ link) 19,426 38.76 59.89 78.94 68.10
Table 5: Performance of the error detection task.
? The second group of experiments concerns
with the combination of linguistic features
without word posterior probability feature.
The combination of lexical features improves
both CER and precision over single lexical
feature (wd, pos). The addition of syntactic
feature link marginally undermines CER but
improves recall by a lot.
? The last group of experiments concerns about
the additional contribution of linguistic fea-
tures to error detection with word posterior
probability. We added linguistic features in-
crementally into the feature pool. The best
performance was achieved by using all fea-
tures, which has a relative of improvement of
18.52% over the baseline CER.
The first two groups of experiments show that
linguistic features, individually (except for link)
or by combination, are able to produce much better
performance than word posterior probability fea-
tures in both CER and F measure. The best com-
bination of linguistic features achieves a relative
improvement of 8.64% and 15.58% in CER and
F measure respectively over word posterior prob-
ability thresholding.
The Table 5 also reveals how linguistic fea-
tures improve error detection. The lexical features
(pos, wd) improve precision when they are used.
This suggests that lexical features can help the sys-
tem find errors more accurately. Syntactic features
(link), on the other hand, improve recall whenever
they are used, which indicates that they can help
the system find more errors.
We also show the number of features in each
combination in Table 5. Except for the wd feature,
0 200 400 600 800 1000
38.6
38.8
39.0
39.2
39.4
39.6
39.8
40.0
40.2
40.4
40.6
 

 
CE
R 
(%
)
Number of Training Sentences
Figure 3: CER vs. the number of training sen-
tences.
the pos has the largest number of features, 199,
which is a small set of features. This suggests that
our error detection model can be learned from a
rather small training set.
Figure 3 shows CERs for the feature combina-
tion MaxEnt (dwpp + wd + pos + link) when
the number of training sentences is enlarged incre-
mentally. CERs drop significantly when the num-
ber of training sentences is increased from 100 to
500. After 500 sentences are used, CERs change
marginally and tend to converge.
7 Conclusions and Future Work
In this paper, we have presented a maximum en-
tropy based approach to automatically detect er-
rors in translation hypotheses generated by SMT
610
systems. We incorporate two sets of linguistic
features together with word posterior probability
based features into error detection.
Our experiments validate that linguistic features
are very useful for error detection: 1) they by
themselves achieve a higher improvement in terms
of both CER and F measure than word posterior
probability features; 2) the performance is further
improved when they are combined with word pos-
terior probability features.
The extracted linguistic features are quite com-
pact, which can be learned from a small train-
ing set. Furthermore, The learned linguistic fea-
tures are system-independent. Therefore our ap-
proach can be used for other machine translation
systems, such as rule-based or example-based sys-
tem, which generally do not produce N -best lists.
Future work in this direction involve detect-
ing particular error types such as incorrect po-
sitions, inappropriate/unnecessary words (Elliott,
2006) and automatically correcting errors.
References
Yasuhiro Akibay, Eiichiro Sumitay, Hiromi Nakaiway,
Seiichi Yamamotoy, and Hiroshi G. Okunoz. 2004.
Using a Mixture of N-best Lists from Multiple MT
Systems in Rank-sum-based Confidence Measure
for MT Outputs. In Proceedings of COLING.
Adam L. Berger, Stephen A. Della Pietra andVincent
J. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1): 39-71.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, Nicola Ueffing. 2003. Confidence estima-
tion for machine translation. final report, jhu/clsp
summer workshop.
Debra Elliott. 2006 Corpus-based Machine Transla-
tion Evaluation via Automated Error Detection in
Output Texts. Phd Thesis, University of Leeds.
Simona Gandrabur and George Foster. 2003. Confi-
dence Estimation for Translation Prediction. In Pro-
ceedings of HLT-NAACL.
S. Jayaraman and A. Lavie. 2005. Multi-engine Ma-
chine Translation Guided by Explicit Word Match-
ing. In Proceedings of EAMT.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constrantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session.
V. I. Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet
Physics Doklady, Feb.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: aMethod for Automatically
Evaluation of Machine Translation. In Proceedings
of ACL 2002.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
Kamel Sma??li. 2009. Word- and Sentence-level
Confidence Measures for Machine Translation. In
Proceedings of EAMT 2009.
Alberto Sanchis, Alfons Juan and Enrique Vidal. 2007.
Estimation of Confidence Measures for Machine
Translation. In Procedings of Machine Translation
Summit XI.
Daniel Sleator and Davy Temperley. 1993. Parsing En-
glish with a Link Grammar. In Proceedings of Third
International Workshop on Parsing Technologies.
Yongmei Shi and Lina Zhou. 2005. Error Detec-
tion Using Linguistic Features. In Proceedings of
HLT/EMNLP 2005.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence Measures for Statistical Machine
Translation. In Proceedings. of MT Summit IX.
Nicola Ueffing and Hermann Ney. 2007. Word-
Level Confidence Estimation for Machine Transla-
tion. Computational Linguistics, 33(1):9-40.
Richard Zens and Hermann Ney. 2006. N-gram Pos-
terior Probabilities for Statistical Machine Transla-
tion. In HLT/NAACL: Proceedings of the Workshop
on Statistical Machine Translation.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
611
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1288?1297,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Enhancing Language Models in Statistical Machine Translation
with Backward N-grams and Mutual Information Triggers
Deyi Xiong, Min Zhang, Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
Abstract
In this paper, with a belief that a language
model that embraces a larger context provides
better prediction ability, we present two ex-
tensions to standard n-gram language mod-
els in statistical machine translation: a back-
ward language model that augments the con-
ventional forward language model, and a mu-
tual information trigger model which captures
long-distance dependencies that go beyond
the scope of standard n-gram language mod-
els. We integrate the two proposed models
into phrase-based statistical machine transla-
tion and conduct experiments on large-scale
training data to investigate their effectiveness.
Our experimental results show that both mod-
els are able to significantly improve transla-
tion quality and collectively achieve up to 1
BLEU point over a competitive baseline.
1 Introduction
Language model is one of the most important
knowledge sources for statistical machine transla-
tion (SMT) (Brown et al, 1993). The standard
n-gram language model (Goodman, 2001) assigns
probabilities to hypotheses in the target language
conditioning on a context history of the preceding
n ? 1 words. Along with the efforts that advance
translation models from word-based paradigm to
syntax-based philosophy, in recent years we have
also witnessed increasing efforts dedicated to ex-
tend standard n-gram language models for SMT. We
roughly categorize these efforts into two directions:
data-volume-oriented and data-depth-oriented.
In the first direction, more data is better. In or-
der to benefit from monolingual corpora (LDC news
data or news data collected from web pages) that
consist of billions or even trillions of English words,
huge language models are built in a distributed man-
ner (Zhang et al, 2006; Brants et al, 2007). Such
language models yield better translation results but
at the cost of huge storage and high computation.
The second direction digs deeply into monolin-
gual data to build linguistically-informed language
models. For example, Charniak et al (2003) present
a syntax-based language model for machine transla-
tion which is trained on syntactic parse trees. Again,
Shen et al (2008) explore a dependency language
model to improve translation quality. To some ex-
tent, these syntactically-informed language models
are consistent with syntax-based translation models
in capturing long-distance dependencies.
In this paper, we pursue the second direction with-
out resorting to any linguistic resources such as a
syntactic parser. With a belief that a language model
that embraces a larger context provides better pre-
diction ability, we learn additional information from
training data to enhance conventional n-gram lan-
guage models and extend their ability to capture
richer contexts and long-distance dependencies. In
particular, we integrate backward n-grams and mu-
tual information (MI) triggers into language models
in SMT.
In conventional n-gram language models, we look
at the preceding n ? 1 words when calculating the
probability of the current word. We henceforth call
the previous n ? 1 words plus the current word
as forward n-grams and a language model built
1288
on forward n-grams as forward n-gram language
model. Similarly, backward n-grams refer to the
succeeding n ? 1 words plus the current word. We
train a backward n-gram language model on back-
ward n-grams and integrate the forward and back-
ward language models together into the decoder. In
doing so, we attempt to capture both the preceding
and succeeding contexts of the current word.
Different from the backward n-gram language
model, the MI trigger model still looks at previous
contexts, which however go beyond the scope of for-
ward n-grams. If the current word is indexed as wi,
the farthest word that the forward n-gram includes
is wi?n+1. However, the MI triggers are capable of
detecting dependencies between wi and words from
w1 to wi?n. By these triggers ({wk ? wi}, 1 ?
k ? i?n), we can capture long-distance dependen-
cies that are outside the scope of forward n-grams.
We integrate the proposed backward language
model and the MI trigger model into a state-of-
the-art phrase-based SMT system. We evaluate
the effectiveness of both models on Chinese-to-
English translation tasks with large-scale training
data. Compared with the baseline which only uses
the forward language model, our experimental re-
sults show that the additional backward language
model is able to gain about 0.5 BLEU points, while
the MI trigger model gains about 0.4 BLEU points.
When both models are integrated into the decoder,
they collectively improve the performance by up to
1 BLEU point.
The paper is structured as follows. In Section 2,
we will briefly introduce related work and show how
our models differ from previous work. Section 3 and
4 will elaborate the backward language model and
the MI trigger model respectively in more detail, de-
scribe the training procedures and explain how the
models are integrated into the phrase-based decoder.
Section 5 will empirically evaluate the effectiveness
of these two models. Section 6 will conduct an in-
depth analysis. In the end, we conclude in Section
7.
2 Related Work
Previous work devoted to improving language mod-
els in SMT mostly focus on two categories as we
mentioned before1: large language models (Zhang
et al, 2006; Emami et al, 2007; Brants et al, 2007;
Talbot and Osborne, 2007) and syntax-based lan-
guage models (Charniak et al, 2003; Shen et al,
2008; Post and Gildea, 2008). Since our philoso-
phy is fundamentally different from them in that we
build contextually-informed language models by us-
ing backward n-grams and MI triggers, we discuss
previous work that explore these two techniques
(backward n-grams and MI triggers) in this section.
Since the context ?history? in the backward lan-
guage model (BLM) is actually the future words
to be generated, BLM is normally used in a post-
processing where all words have already been gener-
ated or in a scenario where sentences are proceeded
from the ending to the beginning. Duchateau et al
(2002) use the BLM score as a confidence measure
to detect wrongly recognized words in speech recog-
nition. Finch and Sumita (2009) use the BLM in
their reverse translation decoder where source sen-
tences are proceeded from the ending to the begin-
ning. Our BLM is different from theirs in that we ac-
cess the BLM during decoding (rather than after de-
coding) where source sentences are still proceeded
from the beginning to the ending.
Rosenfeld et al (1994) introduce trigger pairs
into a maximum entropy based language model as
features. The trigger pairs are selected accord-
ing to their mutual information. Zhou (2004) also
propose an enhanced language model (MI-Ngram)
which consists of a standard forward n-gram lan-
guage model and an MI trigger model. The latter
model measures the mutual information of distance-
dependent trigger pairs. Our MI trigger model is
mostly inspired by the work of these two papers, es-
pecially by Zhou?s MI-Ngram model (2004). The
difference is that our model is distance-independent
and, of course, we are interested in an SMT problem
rather than a speech recognition one.
Raybaud et al (2009) useMI triggers in their con-
fidence measures to assess the quality of translation
results after decoding. Our method is different from
theirs in the MI calculation and trigger pair selec-
tion. Mauser et al (2009) propose bilingual triggers
where two source words trigger one target word to
1Language model adaptation is not very related to our work
so we ignore it.
1289
improve lexical choice of target words. Our analysis
(Section 6) show that our monolingual triggers can
also help in the selection of target words.
3 Backward Language Model
Given a sequence of words wm1 = (w1...wm), a
standard forward n-gram language model assigns a
probability Pf (wm1 ) to wm1 as follows.
Pf (wm1 ) =
m
?
i=1
P (wi|wi?11 ) ?
m
?
i=1
P (wi|wi?1i?n+1) (1)
where the approximation is based on the nth order
Markov assumption. In other words, when we pre-
dict the current word wi, we only consider the pre-
ceding n ? 1 words wi?n+1...wi?1 instead of the
whole context history w1...wi?1.
Different from the forward n-gram language
model, the backward n-gram language model as-
signs a probability Pb(wm1 ) to wm1 by looking at the
succeeding context according to
Pb(wm1 ) =
m
?
i=1
P (wi|wmi+1) ?
m
?
i=1
P (wi|wi+n?1i+1 ) (2)
3.1 Training
For the convenience of training, we invert the or-
der in each sentence in the training data, i.e., from
the original order (w1...wm) to the reverse order
(wm...w1). In this way, we can use the same toolkit
that we use to train a forward n-gram language
model to train a backward n-gram language model
without any other changes. To be consistent with
training, we also need to reverse the order of trans-
lation hypotheses when we access the trained back-
ward language model2. Note that the Markov con-
text history of Eq. (2) is wi+n?1...wi+1 instead of
wi+1...wi+n?1 after we invert the order. The words
are the same but the order is completely reversed.
3.2 Decoding
In this section, we will present two algorithms
to integrate the backward n-gram language model
into two kinds of phrase-based decoders respec-
tively: 1) a CKY-style decoder that adopts bracket-
ing transduction grammar (BTG) (Wu, 1997; Xiong
2This is different from the reverse decoding in (Finch and
Sumita, 2009) where source sentences are reversed in the order.
et al, 2006) and 2) a standard phrase-based decoder
(Koehn et al, 2003). Both decoders translate source
sentences from the beginning of a sentence to the
ending. Wu (1996) introduce a dynamic program-
ming algorithm to integrate a forward bigram lan-
guage model with inversion transduction grammar.
His algorithm is then adapted and extended for inte-
grating forward n-gram language models into syn-
chronous CFGs by Chiang (2007). Our algorithms
are different from theirs in two major aspects
1. The string input to the algorithms is in a reverse
order.
2. We adopt a different way to calculate language
model probabilities for partial hypotheses so
that we can utilize incomplete n-grams.
Before we introduce the integration algorithms,
we define three functions P , L, and R on strings (in
a reverse order) over the English terminal alphabet
T . The function P is defined as follows.
P(wk...w1) =P (wk)...P (wk?n+2|wk...wk?n+3)
? ?? ?
a
?
?
1?i?k?n+1
P (wi|wi+n?1...wi+1)
? ?? ?
b
(3)
This function consists of two parts:
? The first part (a) calculates incomplete n-gram
language model probabilities for word wk to
wk?n+2. That means, we calculate the uni-
gram probability for wk (P (wk)), bigram prob-
ability for wk?1 (P (wk?1|wk)) and so on un-
til we take n ? 1-gram probability for wk?n+2
(P (wk?n+2|wk...wk?n+3)). This resembles
the way in which the forward language model
probability in the future cost is computed in
the standard phrase-based SMT (Koehn et al,
2003).
? The second part (b) calculates complete n-
gram backward language model probabilities
for word wk?n+1 to w1.
The function is different from Chiang?s p func-
tion in that his function p only calculates language
model probabilities for the complete n-grams. Since
1290
we calculate backward language model probabilities
during a beginning-to-ending (left-to-right) decod-
ing process, the succeeding context for the current
word is either yet to be generated or incomplete in
terms of n-grams. The P function enables us to
utilize incomplete succeeding contexts to approxi-
mately predict words. Once the succeeding con-
texts are complete, we can quickly update language
model probabilities in an efficient way in our algo-
rithms.
The other two functions L and R are defined as
follows
L(wk...w1) =
{
wk...wk?n+2, if k ? n
wk...w1, otherwise
(4)
R(wk...w1) =
{
wn?1...w1, if k ? n
wk...w1, otherwise
(5)
The L and R function return the leftmost and right-
most n ? 1 words from a string in a reverse order
respectively.
Following Chiang (2007), we describe our algo-
rithms in a deductive system. We firstly show the
algorithm3 that integrates the backward language
model into a BTG-style decoder (Xiong et al, 2006)
in Figure 1. The item [A, i, j; l|r] indicates that a
BTG node A has been constructed spanning from i
to j on the source side with the leftmost|rightmost
n? 1 words l|r on the target side. As mentioned be-
fore, all target strings assessed by the defined func-
tions (P , L, and R) are in an inverted order (de-
noted by e). We only display the backward lan-
guage model probability for each item, ignoring all
other scores such as phrase translation probabilities.
The Eq. (8) in Figure 1 shows how we calculate
the backward language model probability for the ax-
iom which applies a BTG lexicon rule to translate
a source phrase c into a target phrase e. The Eq.
(9) and (10) show how we update the backward lan-
guage model probabilities for two inference rules
which combine two neighboring blocks in a straight
and inverted order respectively. The fundamental
theories behind this update are
P(e1e2) = P(e1)P(e2)
P(R(e2)L(e1))
P(R(e2))P(L(e1))
(6)
3It can also be easily adapted to integrate the forward n-
gram language model.
Function Value
e1 a1a2a3
e2 b1b2b3
R(e2) b2b1
L(e1) a3a2
P(R(e2)) P (b2)P (b1|b2)
P(L(e1)) P (a3)P (a2|a3)
P(e1) P (a3)P (a2|a3)P (a1|a3a2)
P(e2) P (b3)P (b2|b3)P (b1|b3b2)
P(R(e2)L(e1))
P (b2)P (b1|b2)
P (a3|b2b1)P (a2|b1a3)
P(e1e2)
P (b3)P (b2|b3)P (b1|b3b2)
P (a3|b2b1)P (a2|b1a3)P (a1|a3a2)
Table 1: Values of P , L, and R in a 3-gram example .
P(e2e1) = P(e1)P(e2)
P(R(e1)L(e2))
P(R(e1))P(L(e2))
(7)
Whenever two strings e1 and e2 are concatenated
in a straight or inverted order, we can reuse their
P values (P(e1) and P(e2)) in terms of dynamic
programming. Only the probabilities of boundary
words (e.g., R(e2)L(e1) in Eq. (6)) need to be re-
calculated since they have complete n-grams after
the concatenation. Table 1 shows values of P , L,
and R in a 3-gram example which helps to verify
Eq. (6). These two equations guarantee that our
algorithm can correctly compute the backward lan-
guage model probability of a sentence stepwise in a
dynamic programming framework.4
The theoretical time complexity of this algorithm
is O(m3|T |4(n?1)) because in the update parts in
Eq. (6) and (7) both the numerator and denomina-
tor have up to 2(n?1) terminal symbols. This is the
same as the time complexity of Chiang?s language
model integration (Chiang, 2007).
Figure 2 shows the algorithm that integrates the
backward language model into a standard phrase-
based SMT (Koehn et al, 2003). V denotes a cover-
age vector which records source words translated so
far. The Eq. (11) shows how we update the back-
ward language model probability for a partial hy-
pothesis when it is extended into a longer hypothesis
by a target phrase translating an uncovered source
4The start-of-sentence symbol ?s? and end-of-sentence sym-
bol ?/s? can be easily added to update the final language model
probability when a translation hypothesis covering the whole
source sentence is completed.
1291
A ? c/e
[A, i, j;L(e)|R(e)] : P(e)
(8)
A ? [A1, A2] [A1, i, k;L(e1)|R(e1)] : P(e1) [A2, k + 1, j;L(e2)|R(e2)] : P(e2)
[A, i, j;L(e1e2)|R(e1e2)] : P(e1)P(e2) P(R(e2)L(e1))P(R(e2))P(L(e1))
(9)
A ? ?A1, A2? [A1, i, k;L(e1)|R(e1)] : P(e1) [A2, k + 1, j;L(e2)|R(e2)] : P(e2)
[A, i, j;L(e2e1)|R(e2e1)] : P(e1)P(e2) P(R(e1)L(e2))P(R(e1))P(L(e2))
(10)
Figure 1: Integrating the backward language model into a BTG-style decoder.
[V;L(e1)] : P(e1) c/e2 : P(e2)
[V ?;L(e1e2)] : P(e1)P(e2) P(R(e2)L(e1))P(R(e2))P(L(e1))
(11)
Figure 2: Integrating the backward language model into
a standard phrase-based decoder.
segment. This extension on the target side is simi-
lar to the monotone combination of Eq. (9) in that a
newly translated phrase is concatenated to an early
translated sequence.
4 MI Trigger Model
It is well-known that long-distance dependencies be-
tween words are very important for statistical lan-
guage modeling. However, n-gram language models
can only capture short-distance dependencies within
an n-word window. In order to model long-distance
dependencies, previous work such as (Rosenfeld et
al., 1994) and (Zhou, 2004) exploit trigger pairs. A
trigger pair is defined as an ordered 2-tuple (x, y)
where word x occurs in the preceding context of
word y. It can also be denoted in a more visual man-
ner as x ? y with x being the trigger and y the
triggered word5.
We use pointwise mutual information (PMI)
(Church and Hanks, 1990) to measure the strength
of the association between x and y, which is defined
as follows
PMI(x, y) = log( P (x, y)
P (x)P (y)
) (12)
5In this paper, we require that word x and y occur in the
same sentence.
Zhou (2004) proposes a new language model en-
hanced with MI trigger pairs. In his model, the prob-
ability of a given sentence wm1 is approximated as
P (wm1 ) ?(
m
?
i=1
P (wi|wi?1i?n+1))
?
m
?
i=n+1
i?n
?
k=1
exp(PMI(wk, wi, i? k ? 1))
(13)
There are two components in his model. The first
component is still the standard n-gram language
model. The second one is the MI trigger model
which multiples all exponential PMI values for trig-
ger pairs where the current word is the triggered
word and all preceding words outside the n-gram
window of the current word are triggers. Note that
his MI trigger model is distance-dependent since
trigger pairs (wk, wi) are sensitive to their distance
i? k? 1 (zero distance for adjacent words). There-
fore the distance between word x and word y should
be taken into account when calculating their PMI.
In this paper, for simplicity, we adopt a distance-
independent MI trigger model as follows
MI(wm1 ) =
m
?
i=n+1
i?n
?
k=1
exp(PMI(wk, wi)) (14)
We integrate the MI trigger model into the log-
linear model of machine translation as an additional
knowledge source which complements the standard
n-gram language model in capturing long-distance
dependencies. By MERT (Och, 2003), we are even
able to tune the weight of the MI trigger model
against the weight of the standard n-gram language
model while Zhou (2004) sets equal weights for both
models.
1292
4.1 Training
We can use the maximum likelihood estimation
method to calculate PMI for each trigger pair by tak-
ing counts from training data. Let C(x, y) be the
co-occurrence count of the trigger pair (x, y) in the
training data. The joint probability of (x, y) is cal-
culated as
P (x, y) = C(x, y)?
x,y C(x, y)
(15)
The marginal probabilities of x and y can be de-
duced from the joint probability as follows
P (x) =
?
y
P (x, y) (16)
P (y) =
?
x
P (x, y) (17)
Since the number of distinct trigger pairs is
O(|T |2), the question is how to select valuable trig-
ger pairs. We select trigger pairs according to the
following three steps
1. The distance between x and y must not be less
than n? 1. Suppose we use a 5-gram language
model and y = wi , then x ? {w1...wi?5}.
2. C(x, y) > c. In all our experiments we set c =
10.
3. Finally, we only keep trigger pairs whose PMI
value is larger than 0. Trigger pairs whose PMI
value is less than 0 often contain stop words,
such as ?the?, ?a?. These stop words have very
large marginal probabilities due to their high
frequencies.
4.2 Decoding
The MI trigger model of Eq. (14) can be directly
integrated into the decoder. For the standard phrase-
based decoder (Koehn et al, 2003), whenever a par-
tial hypothesis is extended by a new target phrase,
we can quickly retrieve the pre-computed PMI value
for each trigger pair where the triggered word lo-
cates in the newly translated target phrase and the
trigger is outside the n-word window of the trig-
gered word. It?s a little more complicated to in-
tegrate the MI trigger model into the CKY-style
phrase-based decoder. But we still can handle it by
dynamic programming as follows
MI(e1e2) = MI(e1)MI(e2)MI(e1 ? e2) (18)
where MI(e1 ? e2) represents the PMI values in
which a word in e1 triggers a word in e2. It is defined
as follows
MI(e1 ? e2) =
?
wi?e2
?
wk?e1
i?k?n
exp(PMI(wk, wi))
(19)
5 Experiments
In this section, we conduct large-scale experiments
on NIST Chinese-to-English translation tasks to
evaluate the effectiveness of the proposed backward
language model and MI trigger model in SMT. Our
experiments focus on the following two issues:
1. How much improvements can we achieve by
separately integrating the backward language
model and the MI trigger model into our
phrase-based SMT system?
2. Can we obtain a further improvement if we
jointly apply both models?
5.1 System Overview
Without loss of generality6, we evaluate our models
in a phrase-based SMT system which adapts brack-
eting transduction grammars to phrasal translation
(Xiong et al, 2006). The log-linear model of this
system can be formulated as
w(D) =MT (rl1..nl) ?MR(r
m
1..nm)
?R
? PfL(e)?fL ? exp(|e|)?w
(20)
where D denotes a derivation, rl1..nl are the BTG
lexicon rules which translate source phrases to tar-
get phrases, and rm1..nm are the merging rules which
combine two neighboring blocks into a larger block
in a straight or inverted order. The translation
model MT consists of widely used phrase and lex-
ical translation probabilities (Koehn et al, 2003).
6We have discussed how to integrate the backward language
model and the MI trigger model into the standard phrase-based
SMT system (Koehn et al, 2003) in Section 3.2 and 4.2 respec-
tively.
1293
The reordering model MR predicts the merging or-
der (straight or inverted) by using discriminative
contextual features (Xiong et al, 2006). PfL is the
standard forward n-gram language model.
If we simultaneously integrate both the backward
language model PbL and the MI trigger model MI
into the system, the new log-linear model will be
formulated as
w(D) =MT (rl1..nl) ?MR(r
m
1..nm)
?R ? PfL(e)?fL
? PbL(e)?bL ?MI(e)?MI ? exp(|e|)?w
(21)
5.2 Experimental Setup
Our training corpora7 consist of 96.9M Chinese
words and 109.5M English words in 3.8M sentence
pairs. We used all corpora to train our translation
model and smaller corpora without the United Na-
tions corpus to build a maximum entropy based re-
ordering model (Xiong et al, 2006).
To train our language models and MI trigger
model, we used the Xinhua section of the En-
glish Gigaword corpus (306 million words). Firstly,
we built a forward 5-gram language model using
the SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney smoothing. Then we trained a back-
ward 5-gram language model on the same monolin-
gual corpus in the way described in Section 3.1. Fi-
nally, we trained our MI trigger model still on this
corpus according to the method in Section 4.1. The
trained MI trigger model consists of 2.88M trigger
pairs.
We used the NIST MT03 evaluation test data as
the development set, and the NIST MT04, MT05 as
the test sets. We adopted the case-insensitive BLEU-
4 (Papineni et al, 2002) as the evaluation metric,
which uses the shortest reference sentence length for
the brevity penalty. Statistical significance in BLEU
differences is tested by paired bootstrap re-sampling
(Koehn, 2004).
5.3 Experimental Results
The experimental results on the two NIST test sets
are shown in Table 2. When we combine the back-
ward language model with the forward language
7LDC2004E12, LDC2004T08, LDC2005T10,
LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07
and LDC2004T07.
Model MT-04 MT-05
Forward (Baseline) 35.67 34.41
Forward+Backward 36.16+ 34.97+
Forward+MI 36.00+ 34.85+
Forward+Backward+MI 36.76+ 35.12+
Table 2: BLEU-4 scores (%) on the two test sets for dif-
ferent language models and their combinations. +: better
than the baseline (p < 0.01).
model, we obtain 0.49 and 0.56 BLEU points over
the baseline on theMT-04 andMT-05 test set respec-
tively. Both improvements are statistically signifi-
cant (p < 0.01). The MI trigger model also achieves
statistically significant improvements of 0.33 and
0.44 BLEU points over the baseline on the MT-04
and MT-05 respectively.
When we integrate both the backward language
model and the MI trigger model into our system,
we obtain improvements of 1.09 and 0.71 BLEU
points over the single forward language model on
the MT-04 and MT-05 respectively. These improve-
ments are larger than those achieved by using only
one model (the backward language model or the MI
trigger model).
6 Analysis
In this section, we will study more details of the two
models by looking at the differences that they make
on translation hypotheses. These differences will
help us gain some insights into how the presented
models improve translation quality.
Table 3 shows an example from our test set. The
italic words in the hypothesis generated by using the
backward language model (F+B) exactly match the
reference. However, the italic words in the base-
line hypothesis fail to match the reference due to
the incorrect position of the word ?decree? (??).
We calculate the forward/backward language model
score (the logarithm of language model probability)
for the italic words in both the baseline and F+B hy-
pothesis according to the trained language models.
The difference in the forward language model score
is only 1.58, which may be offset by differences in
other features in the log-linear translation model. On
the other hand, the difference in the backward lan-
guage model score is 3.52. This larger difference
may guarantee that the hypothesis generated by F+B
1294
Source ??????? ,??????
?????????????
?
Baseline Beijing Youth Daily reported that
Beijing Agricultural decree recently
issued a series of control and super-
vision
F+B Beijing Youth Daily reported that
Beijing Bureau of Agriculture re-
cently issued a series of prevention
and control laws
Reference Beijing Youth Daily reported that
Beijing Bureau of Agriculture re-
cently issued a series of preventative
and monitoring ordinances
Table 3: Translation example from the MT-04 test set,
comparing the baseline with the backward language
model. F+B: forward+backward language model .
is better enough to be selected as the best hypothe-
sis by the decoder. This suggests that the backward
language model is able to provide useful and dis-
criminative information which is complementary to
that given by the forward language model.
In Table 4, we present another example to show
how the MI trigger model improves translation qual-
ity. The major difference in hypotheses of this ex-
ample is the word choice between ?is? and ?was?.
The new system enhanced with the MI trigger model
(F+M) selects the former while the baseline selects
the latter. The forward language model score for the
baseline hypothesis is -26.41, which is higher than
the score of the F+M hypothesis -26.67. This could
be the reason why the baseline selects the word
?was? instead of ?is?. As can be seen, there is an-
other ?is? in the preceding context of the word ?was?
in the baseline hypothesis. Unfortunately, this word
?is? is located just outside the scope of the preceding
5-gram context of ?was?. The forward 5-gram lan-
guage model is hence not able to take it into account
when calculating the probability of ?was?. However,
this is not a problem for the MI trigger model. Since
?is? and ?was? rarely co-occur in the same sentence,
the PMI value of the trigger pair (is, was)8 is -1.03
8Since we remove all trigger pairs whose PMI value is neg-
ative, the PMI value of this pair (is, was) is set 0 in practice in
the decoder.
Source ???????????? ,?
?????????????
?
Baseline Self-Defense Force ?s trip is remark-
able , because it was not an isolated
incident .
F+M Self-Defense Force ?s trip is remark-
able , because it is not an isolated in-
cident .
Reference The Self-Defense Forces? trip
arouses attention because it is not an
isolated incident.
Table 4: Translation example from the MT-04 test set,
comparing the baseline with the MI trigger model. Both
system outputs are not detokenized so that we can see
how language model scores are calculated. The un-
derlined words highlight the difference between the en-
hanced models and the baseline. F+M: forward language
model + MI trigger model.
while the PMI value of the trigger pair (is, is) is as
high as 0.32. Therefore our MI trigger model selects
?is? rather than ?was?.9 This example illustrates that
the MI trigger model is capable of selecting correct
words by using long-distance trigger pairs.
7 Conclusion
We have presented two models to enhance the abil-
ity of standard n-gram language models in captur-
ing richer contexts and long-distance dependencies
that go beyond the scope of forward n-gram win-
dows. The two models have been integrated into
the decoder and have shown to improve a state-of-
the-art phrase-based SMT system. The first model
is the backward language model which uses back-
ward n-grams to predict the current word. We in-
troduced algorithms that directly integrate the back-
ward language model into a CKY-style and a stan-
dard phrase-based decoder respectively. The sec-
ond model is the MI trigger model that incorporates
long-distance trigger pairs into language modeling.
Overall improvements are up to 1 BLEU point on
the NIST Chinese-to-English translation tasks with
large-scale training data. Further study of the two
9The overall MI trigger model scores (the logarithm of Eq.
(14)) of the baseline hypothesis and the F+M hypothesis are
2.09 and 2.25 respectively.
1295
models indicates that backward n-grams and long-
distance triggers provide useful information to im-
prove translation quality.
In future work, we would like to integrate the
backward language model into a syntax-based sys-
tem in a way that is similar to the proposed algo-
rithm shown in Figure 1. We are also interested in
exploring more morphologically- or syntactically-
informed triggers. For example, a verb in the past
tense triggers another verb also in the past tense
rather than the present tense.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867, Prague, Czech Republic, June. Association for
Computational Linguistics.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of MT Summit IX.
Intl. Assoc. for Machine Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Jacques Duchateau, Kris Demuynck, and Patrick
Wambacq. 2002. Confidence scoring based on back-
ward language models. In Proceedings of ICASSP,
pages 221?224, Orlando, FL, April.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP, pages 37?40, Honolulu, HI,
April.
Andrew Finch and Eiichiro Sumita. 2009. Bidirectional
phrase-based statistical machine translation. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1124?
1132, Singapore, August. Association for Computa-
tional Linguistics.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling extended version. Technical report,
Microsoft Research.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 58?54, Edmon-
ton, Canada, May-June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210?218, Singa-
pore, August. Association for Computational Linguis-
tics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of AMTA.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Sma??li. 2009. New confidence measures
for statistical machine translation. In Proceedings of
the International Conference on Agents and Artificial
Intelligence, pages 61?68, Porto, Portugal, January.
Roni Rosenfeld, Jaime Carbonell, and Alexander Rud-
nicky. 1994. Adaptive statistical language model-
ing: A maximum entropy approach. Technical report,
Carnegie Mellon University.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado, USA, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 512?519,
1296
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics, pages 152?158, Santa Cruz, California,
USA, June.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521?528, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 216?223, Sydney, Australia, July. Association
for Computational Linguistics.
GuoDong Zhou. 2004. Modeling of long distance con-
text dependency. In Proceedings of Coling, pages 92?
98, Geneva, Switzerland, Aug 23?Aug 27. COLING.
1297
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 750?758,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Topic Similarity Model
for Hierarchical Phrase-based Translation
Xinyan Xiao? Deyi Xiong? Min Zhang?? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Human Language Technology
Institute of Computing Technology Institute for Infocomm Research
Chinese Academy of Sciences
{xiaoxinyan, liuqun, sxlin}@ict.ac.cn {dyxiong, mzhang?}@i2r.a-star.edu.sg
Abstract
Previous work using topic model for statis-
tical machine translation (SMT) explore top-
ic information at the word level. Howev-
er, SMT has been advanced from word-based
paradigm to phrase/rule-based paradigm. We
therefore propose a topic similarity model to
exploit topic information at the synchronous
rule level for hierarchical phrase-based trans-
lation. We associate each synchronous rule
with a topic distribution, and select desirable
rules according to the similarity of their top-
ic distributions with given documents. We
show that our model significantly improves
the translation performance over the baseline
on NIST Chinese-to-English translation ex-
periments. Our model also achieves a better
performance and a faster speed than previous
approaches that work at the word level.
1 Introduction
Topic model (Hofmann, 1999; Blei et al, 2003) is
a popular technique for discovering the underlying
topic structure of documents. To exploit topic infor-
mation for statistical machine translation (SMT), re-
searchers have proposed various topic-specific lexi-
con translation models (Zhao and Xing, 2006; Zhao
and Xing, 2007; Tam et al, 2007) to improve trans-
lation quality.
Topic-specific lexicon translation models focus
on word-level translations. Such models first esti-
mate word translation probabilities conditioned on
topics, and then adapt lexical weights of phrases
?Corresponding author
by these probabilities. However, the state-of-the-
art SMT systems translate sentences by using se-
quences of synchronous rules or phrases, instead of
translating word by word. Since a synchronous rule
is rarely factorized into individual words, we believe
that it is more reasonable to incorporate the topic
model directly at the rule level rather than the word
level.
Consequently, we propose a topic similari-
ty model for hierarchical phrase-based translation
(Chiang, 2007), where each synchronous rule is as-
sociated with a topic distribution. In particular,
? Given a document to be translated, we cal-
culate the topic similarity between a rule and
the document based on their topic distributions.
We augment the hierarchical phrase-based sys-
tem by integrating the proposed topic similarity
model as a new feature (Section 3.1).
? As we will discuss in Section 3.2, the similarity
between a generic rule and a given source docu-
ment computed by our topic similarity model is
often very low. We don?t want to penalize these
generic rules. Therefore we further propose a
topic sensitivity model which rewards generic
rules so as to complement the topic similarity
model.
? We estimate the topic distribution for a rule
based on both the source and target side topic
models (Section 4.1). In order to calculate sim-
ilarities between target-side topic distributions
of rules and source-side topic distributions of
given documents during decoding, we project
750
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(a) ?? ?? ? opera-
tional capability
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(b) ??X1 ? grandsX1
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(c) ??X1 ? giveX1
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(d) X1 ?? ?? X2 ?
held talksX1 X2
Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its topic distribution,
where the X-axis means topic index and the Y-axis means the topic probability. Notably, the rule (b) and rule (c) shares
the same source Chinese string, but they have different topic distributions due to the different English translations.
the target-side topic distributions of rules into
the space of source-side topic model by one-to-
many projection (Section 4.2).
Experiments on Chinese-English translation tasks
(Section 6) show that, our method outperforms the
baseline hierarchial phrase-based system by +0.9
BLEU points. This result is also +0.5 points high-
er and 3 times faster than the previous topic-specific
lexicon translation method. We further show that
both the source-side and target-side topic distribu-
tions improve translation quality and their improve-
ments are complementary to each other.
2 Background: Topic Model
A topic model is used for discovering the topics
that occur in a collection of documents. Both La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
and Probabilistic Latent Semantic Analysis (PLSA)
(Hofmann, 1999) are types of topic models. LDA
is the most common topic model currently in use,
therefore we exploit it for mining topics in this pa-
per. Here, we first give a brief description of LDA.
LDA views each document as a mixture pro-
portion of various topics, and generates each word
by multinomial distribution conditioned on a topic.
More specifically, as a generative process, LDA first
samples a document-topic distribution for each doc-
ument. Then, for each word in the document, it sam-
ples a topic index from the document-topic distribu-
tion and samples the word conditioned on the topic
index according the topic-word distribution.
Generally speaking, LDA contains two types of
parameters. The first one relates to the document-
topic distribution, which records the topic distribu-
tion of each document. The second one is used for
topic-word distribution, which represents each topic
as a distribution over words. Based on these param-
eters (and some hyper-parameters), LDA can infer a
topic assignment for each word in the documents. In
the following sections, we will use these parameters
and the topic assignments of words to estimate the
parameters in our method.
3 Topic Similarity Model
Sentences should be translated in consistence with
their topics (Zhao and Xing, 2006; Zhao and Xing,
2007; Tam et al, 2007). In the hierarchical phrase
based system, a synchronous rule may be related to
some topics and unrelated to others. In terms of
probability, a rule often has an uneven probability
distribution over topics. The probability over a topic
is high if the rule is highly related to the topic, other-
wise the probability will be low. Therefore, we use
topic distribution to describe the relatedness of rules
to topics.
Figure 1 shows four synchronous rules (Chiang,
2007) with topic distributions, some of which con-
tain nonterminals. We can see that, although the
source part of rule (b) and (c) are identical, their top-
ic distributions are quite different. Rule (b) contains
a highest probability on the topic about ?China-U.S.
relationship?, which means rule (b) is much more
related to this topic. In contrast, rule (c) contains
an even distribution over various topics. Thus, giv-
en a document about ?China-U.S. relationship?, we
hope to encourage the system to apply rule (b) but
penalize the application of rule (c). We achieve this
by calculating similarity between the topic distribu-
tions of a rule and a document to be translated.
More formally, we associate each rule with a rule-
topic distribution P (z|r), where r is a rule, and z is
a topic. Suppose there are K topics, this distribution
751
can be represented by a K-dimension vector. The
k-th component P (z = k|r) means the probability
of topic k given the rule r. The estimation of such
distribution will be described in Section 4.
Analogously, we represent the topic information
of a document d to be translated by a document-
topic distribution P (z|d), which is also a K-
dimension vector. The k-th dimension P (z = k|d)
means the probability of topic k given document d.
Different from rule-topic distribution, the document-
topic distribution can be directly inferred by an off-
the-shelf LDA tool.
Consequently, based on these two distribution-
s, we select a rule for a document to be translat-
ed according to their topic similarity (Section 3.1),
which measures the relatedness of the rule to the
document. In order to encourage the application
of generic rules which are often penalized by our
similarity model, we also propose a topic sensitivity
model (Section 3.2).
3.1 Topic Similarity
By comparing the similarity of their topic distribu-
tions, we are able to decide whether a rule is suitable
for a given source document. The topic similarity
computes the distance of two topic distributions. We
calculate the topic similarity by Hellinger function:
Similarity(P (z|d), P (z|r))
=
K
?
k=1
(
?
P (z = k|d) ?
?
P (z = k|r)
)2
(1)
Hellinger function is used to calculate distribution
distance and is popular in topic model (Blei and Laf-
ferty, 2007).1 By topic similarity, we aim to encour-
age or penalize the application of a rule for a giv-
en document according to their topic distributions,
which then helps the SMT system make better trans-
lation decisions.
3.2 Topic Sensitivity
Domain adaptation (Wu et al, 2008; Bertoldi and
Federico, 2009) often distinguishes general-domain
data from in-domain data. Similarly, we divide the
rules into topic-insensitive rules and topic-sensitive
1We also try other distance functions, including Euclidean
distance, Kullback-Leibler divergence and cosine function.
They produce similar results in our preliminary experiments.
rules according to their topic distributions. Let?s
revisit Figure 1. We can easily find that the topic
distribution of rule (c) distribute evenly. This in-
dicates that it is insensitive to topics, and can be
applied in any topics. We call such a rule a topic-
insensitive rule. In contrast, the distributions of the
rest rules peak on a few topics. Such rules are called
topic-sensitive rules. Generally speaking, a topic-
insensitive rule has a fairly flat distribution, while a
topic-sensitive rule has a sharp distribution.
A document typically focuses on a few topics, and
has a sharp topic distribution. In contrast, the distri-
bution of topic-insensitive rule is fairly flat. Hence,
a topic-insensitive rule is always less similar to doc-
uments and is punished by the similarity function.
However, topic-insensitive rules may be more
preferable than topic-sensitive rules if neither of
them are similar to given documents. For a doc-
ument about the ?military? topic, the rule (b) and
(c) in Figure 1 are both dissimilar to the document,
because rule (b) relates to the ?China-U.S. relation-
ship? topic and rule (c) is topic-insensitive. Never-
theless, since rule (c) occurs more frequently across
various topics, it may be better to apply rule (c).
To address such issue of the topic similarity mod-
el, we further introduce a topic sensitivity model to
describe the topic sensitivity of a rule using entropy
as a metric:
Sensitivity(P (z|r))
= ?
K
?
k=1
P (z = k|r) ? log (P (z = k|r)) (2)
According to the Eq. (2), a topic-insensitive rule has
a large entropy, while a topic-sensitive rule has a s-
maller entropy. By incorporating the topic sensitivi-
ty model with the topic similarity model, we enable
our SMT system to balance the selection of these t-
wo types of rules. Given rules with approximately
equal values of Eq. (1), we prefer topic-insensitive
rules.
4 Estimation
Unlike document-topic distribution that can be di-
rectly learned by LDA tools, we need to estimate the
rule-topic distribution according to our requirement.
In this paper, we try to exploit the topic information
752
of both source and target language. To achieve this
goal, we use both source-side and target-side mono-
lingual topic models, and learn the correspondence
between the two topic models from word-aligned
bilingual corpus.
Specifically, we use two types of rule-topic dis-
tributions: one is source-side rule-topic distribution
and the other is target-side rule-topic distribution.
These two rule-topic distributions are estimated by
corresponding topic models in the same way (Sec-
tion 4.1). Notably, only source language documents
are available during decoding. In order to compute
the similarity between the target-side topic distribu-
tion of a rule and the source-side topic distribution
of a given document?we need to project the target-
side topic distribution of a synchronous rule into the
space of the source-side topic model (Section 4.2).
A more principle way is to learn a bilingual topic
model from bilingual corpus (Mimno et al, 2009).
However, we may face difficulty during decoding,
where only source language documents are avail-
able. It requires a marginalization to infer the mono-
lingual topic distribution using the bilingual topic
model. The high complexity of marginalization pro-
hibits such a summation in practice. Previous work
on bilingual topic model avoid this problem by some
monolingual assumptions. Zhao and Xing (2007)
assume that the topic model is generated in a mono-
lingual manner, while Tam et al, (2007) construct
their bilingual topic model by enforcing a one-to-
one correspondence between two monolingual topic
models. We also estimate our rule-topic distribution
by two monolingual topic models, but use a differ-
ent way to project target-side topics onto source-side
topics.
4.1 Monolingual Topic Distribution Estimation
We estimate rule-topic distribution from word-
aligned bilingual training corpus with documen-
t boundaries explicitly given. The source and tar-
get side distributions are estimated in the same way.
For simplicity, we only describe the estimation of
source-side distribution in this section.
The process of rule-topic distribution estimation
is analogous to the traditional estimation of rule
translation probability (Chiang, 2007). In addition
to the word-aligned corpus, the input for estimation
also contains the source-side topic-document distri-
bution of every documents inferred by LDA tool.
We first extract synchronous rules from training
data in a traditional way. When a rule r is extracted
from a document d with topic distribution P (z|d),
we collect an instance (r, P (z|d), c), where c is the
fraction count of an instance as described in Chiang,
(2007). After extraction, we get a set of instances
I = {(r, P (z|d), c)} with different document-topic
distributions for each rule. Using these instances,
we calculate the topic probability P (z = k|r) as
follows:
P (z = k|r) =
?
I?I c? P (z = k|d)
?K
k?=1
?
I?I c? P (z = k?|d)
(3)
By using both source-side and target-side
document-topic distribution, we obtain two rule-
topic distributions for each rule in total.
4.2 Target-side Topic Distribution Projection
As described in the previous section, we also esti-
mate the target-side rule-topic distribution. How-
ever, only source document-topic distributions are
available during decoding. In order to calculate
the similarity between the target-side rule-topic dis-
tribution of a rule and the source-side document-
topic distribution of a source document, we need to
project target-side topics into the source-side topic
space. The projection contains two steps:
? In the first step, we learn the topic-to-topic cor-
respondence probability p(zf |ze) from target-
side topic ze to source-side topic zf .
? In the second step, we project the target-side
topic distribution of a rule into source-side top-
ic space using the correspondence probability.
In the first step, we estimate the correspondence
probability by the co-occurrence of the source-side
and the target-side topic assignment of the word-
aligned corpus. The topic assignments are output
by LDA tool. Thus, we denotes each sentence pair
by (zf , ze,a), where zf and ze are the topic as-
signments of source-side and target-side sentences
respectively, and a is a set of links {(i, j)}. A
link (i, j) means a source-side position i aligns to
a target-side position j. Thus, the co-occurrence of
a source-side topic with index kf and a target-side
753
e-topic f-topic 1 f-topic 2 f-topic 3
enterprises ??(agricultural) ??(enterprise) ??(develop)
rural ??(rural) ??(market) ??(economic)
state ??(peasant) ??(state) ??(technology )
agricultural ??(reform) ??(company) ??(China)
market ??(finance) ??(finance) ??(technique)
reform ??(social) ??(bank) ??(industry)
production ??(safety) ??(investment) ??(structure)
peasants ??(adjust) ??(manage) ??(innovation)
owned ??(policy) ??(reform) ??(accelerate)
enterprise ??(income) ??(operation) ??(reform)
p(zf |ze) 0.38 0.28 0.16
Table 1: Example of topic-to-topic correspondence. The
last line shows the correspondence probability. Each col-
umnmeans a topic represented by its top-10 topical word-
s. The first column is a target-side topic, while the rest
three columns are source-side topics.
topic ke is calculated by:
?
(zf ,ze,a)
?
(i,j)?a
?(zfi , kf ) ? ?(zej , ke) (4)
where ?(x, y) is the Kronecker function, which is 1
if x = y and 0 otherwise. We then compute the
probability of P (z = kf |z = ke) by normalizing
the co-occurrence count. Overall, after the first step,
we obtain an correspondence matrix MKe?Kf from
target-side topic to source-side topic, where the item
Mi,j represents the probability P (zf = i|ze = j).
In the second step, given the correspondence ma-
trix MKe?Kf , we project the target-side rule-topic
distribution P (ze|r) to the source-side topic space
by multiplication as follows:
T (P (ze|r)) = P (ze|r) ?MKe?Kf (5)
In this way, we get a second distribution for a rule
in the source-side topic space, which we called pro-
jected target-side topic distribution T (P (ze|r)).
Obviously, our projection method allows one
target-side topic to align to multiple source-side top-
ics. This is different from the one-to-one correspon-
dence used by Tam et al, (2007). From the training
result of the correspondence matrix MKe?Kf , we
find that the topic correspondence between source
and target language is not necessarily one-to-one.
Typically, the probability P (z = kf |z = ke) of a
target-side topic mainly distributes on two or three
source-side topics. Table 1 shows an example of
a target-side topic with its three mainly aligned
source-side topics.
5 Decoding
We incorporate our topic similarity model as a
new feature into a traditional hiero system (Chi-
ang, 2007) under discriminative framework (Och
and Ney, 2002). Considering there are a source-
side rule-topic distribution and a projected target-
side rule-topic distribution, we add four features in
total:
? Similarity (P (zf |d), P (zf |r))
? Similarity(P (zf |d), T (P (ze|r)))
? Sensitivity(P (zf |r))
? Sensitivity(T (P (ze|r))
To calculate the total score of a derivation on each
feature listed above during decoding, we sum up the
correspondent feature score of each applied rule.2
The source-side and projected target-side rule-
topic distribution are calculated before decoding.
During decoding, we first infer the topic distribution
P (zf |d) for a given document on source language.
When applying a rule, it is straightforward to calcu-
late these topic features. Obviously, the computa-
tional cost of these features is rather small.
In the topic-specific lexicon translation model,
given a source document, it first calculates the topic-
specific translation probability by normalizing the
entire lexicon translation table, and then adapts the
lexical weights of rules correspondingly. This makes
the decoding slower. Therefore, comparing with the
previous topic-specific lexicon translation method,
our method provides a more efficient way for incor-
porating topic model into SMT.
6 Experiments
We try to answer the following questions by experi-
ments:
1. Is our topic similarity model able to improve
translation quality in terms of BLEU? Further-
more, are source-side and target-side rule-topic
distributions complementary to each other?
2Since glue rule and rules of unknown words are not extract-
ed from training data, here, we just ignore the calculation of the
four features for them.
754
System MT06 MT08 Avg Speed
Baseline 30.20 21.93 26.07 12.6
TopicLex 30.65 22.29 26.47 3.3
SimSrc 30.41 22.69 26.55 11.5
SimTgt 30.51 22.39 26.45 11.7
SimSrc+SimTgt 30.73 22.69 26.71 11.2
Sim+Sen 30.95 22.92 26.94 10.2
Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the
traditional hierarchical system (?Baseline?) and the topic-specific lexicon translation method (?TopicLex?). ?SimSrc?
and ?SimTgt? denote similarity by source-side and target-side rule-distribution respectively, while ?Sim+Sen? acti-
vates the two similarity and two sensitivity features. ?Avg? is the average BLEU score on the two test sets. Scores
marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01).
2. Is it helpful to introduce the topic sensitivi-
ty model to distinguish topic-insensitive and
topic-sensitive rules?
3. Is it necessary to project topics by one-to-many
correspondence instead of one-to-one corre-
spondence?
4. What is the effect of our method on various
types of rules, such as phrase rules and rules
with non-terminals?
6.1 Data
We present our experiments on the NIST Chinese-
English translation tasks. The bilingual training da-
ta contains 239K sentence pairs with 6.9M Chinese
words and 9.14M English words, which comes from
the FBIS portion of LDC data. There are 10,947
documents in the FBIS corpus. The monolingual da-
ta for training English language model includes the
Xinhua portion of the GIGAWORD corpus, which
contains 238M English words. We used the NIST
evaluation set of 2005 (MT05) as our development
set, and sets of MT06/MT08 as test sets. The num-
bers of documents in MT05, MT06, MT08 are 100,
79, and 109 respectively.
We obtained symmetric word alignments of train-
ing data by first running GIZA++ (Och and Ney,
2003) in both directions and then applying re-
finement rule ?grow-diag-final-and? (Koehn et al,
2003). The SCFG rules are extracted from this
word-aligned training data. A 4-gram language
model was trained on the monolingual data by the
SRILM toolkit (Stolcke, 2002). Case-insensitive
NIST BLEU (Papineni et al, 2002) was used to mea-
sure translation performance. We used minimum er-
ror rate training (Och, 2003) for optimizing the fea-
ture weights.
For the topic model, we used the open source L-
DA tool GibbsLDA++ for estimation and inference.3
GibssLDA++ is an implementation of LDA using
gibbs sampling for parameter estimation and infer-
ence. The source-side and target-side topic models
are estimated from the Chinese part and English part
of FBIS corpus respectively. We set the number of
topic K = 30 for both source-side and target-side,
and use the default setting of the tool for training and
inference.4 During decoding, we first infer the top-
ic distribution of given documents before translation
according to the topic model trained on Chinese part
of FBIS corpus.
6.2 Effect of Topic Similarity Model
We compare our method with two baselines. In addi-
tion to the traditional hiero system, we also compare
with the topic-specific lexicon translation method in
Zhao and Xing (2007). The lexicon translation prob-
ability is adapted by:
p(f |e,DF ) ? p(e|f,DF )P (f |DF ) (6)
=
?
k
p(e|f, z = k)p(f |z = k)p(z = k|DF ) (7)
However, we simplify the estimation of p(e|f, z =
k) by directly using the word alignment corpus with
3http://gibbslda.sourceforge.net/
4We determine K by testing {15, 30, 50, 100, 200} in our
preliminary experiments. We find that K = 30 produces a s-
lightly better performance than other values.
755
Type Count Src% Tgt%
Phrase-rule 3.9M 83.4 84.4
Monotone-rule 19.2M 85.3 86.1
Reordering-rule 5.7M 85.9 86.8
All-rule 28.8M 85.1 86.0
Table 3: Percentage of topic-sensitive rules of various
types of rule according to source-side (?Src?) and target-
side (?Tgt?) topic distributions. Phrase rules are fully
lexicalized, while monotone and reordering rules contain
nonterminals (Section 6.5).
topic assignment that is inferred by the GibbsL-
DA++. Despite the simplification of estimation, the
improvement of our implementation is comparable
with the improvement in Zhao et al,(2007). Given a
new document, we need to adapt the lexical transla-
tion weights of the rules based on topic model. The
adapted lexicon translation model is added as a new
feature under the discriminative framework.
Table 2 shows the result of our method compar-
ing with the traditional system and the topic-lexicon
specific translation method described as above. By
using all the features (last line in the table), we im-
prove the translation performance over the baseline
system by 0.87 BLEU point on average. Our method
also outperforms the topic-lexicon specific transla-
tion method by 0.47 points. This verifies that topic
similarity model can improve the translation quality
significantly.
In order to gain insights into why our model is
helpful, we further investigate how many rules are
topic-sensitive. As described in Section 3.2, we use
entropy to measure the topic sensitivity. If the en-
tropy of a rule is smaller than a certain threshold,
then the rule is topic sensitive. Since documents of-
ten focus on some topics, we use the average entropy
of document-topic distribution of all training docu-
ments as the threshold. We compare both source-
side and target-side distribution shown in Table 3.
We find that more than 80 percents of the rules are
topic-sensitive, thus provides us a large space to im-
prove the translation by exploiting topics.
We also compare these methods in terms of the
decoding speed (words/second). The baseline trans-
lates 12.6 words per second, while the topic-specific
lexicon translation method only translates 3.3 word-
s in one second. The overhead of the topic-specific
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
One-to-One 30.27 22.12 26.20
One-to-Many 30.51 22.39 26.45
Table 4: Effects of one-to-one and one-to-many topic pro-
jection.
lexicon translation method mainly comes from the
adaptation of lexical weights. It takes 72.8% of
the time to do the adaptation, despite only lexical
weights of the used rules are adapted. In contrast,
our method has a speed of 10.2 words per second for
each sentence on average, which is three times faster
than the topic-specific lexicon translation method.
Meanwhile, we try to separate the effects of
source-side topic distribution from the target-side
topic distribution. From lines 4-6 of Table 2. We
clearly find that the two rule-topic distributions im-
prove the performance by 0.48 and 0.38 BLEU
points over the baseline respectively. It seems that
the source-side topic model is more helpful. Fur-
thermore, when combine these two distributions, the
improvement is increased to 0.64 points. This indi-
cates that the effects of source-side and target-side
distributions are complementary.
6.3 Effect of Topic Sensitivity Model
As described in Section 3.2, because the similari-
ty features always punish topic-insensitive rules, we
introduce topic sensitivity features as a complemen-
t. In the last line of Table 2, we obtain a fur-
ther improvement of 0.23 points, when incorporat-
ing topic sensitivity features with topic similarity
features. This suggests that it is necessary to dis-
tinguish topic-insensitive and topic-sensitive rules.
6.4 One-to-One Vs. One-to-Many Topic
Projection
In Section 4.2, we find that source-side topic and
target-side topics may not exactly match, hence we
use one-to-many topic correspondence. Yet anoth-
er method is to enforce one-to-one topic projection
(Tam et al, 2007). We achieve one-to-one projection
by aligning a target topic to the source topic with the
largest correspondence probability as calculated in
Section 4.2.
Table 4 compares the effects of these two method-
756
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
Phrase-rule 30.53 22.29 26.41
Monotone-rule 30.72 22.62 26.67
Reordering-rule 30.31 22.40 26.36
All-rule 30.95 22.92 26.94
Table 5: Effect of our topic model on three types of rules.
Phrase rules are fully lexicalized, while monotone and
reordering rules contain nonterminals.
s. We find that the enforced one-to-one topic method
obtains a slight improvement over the baseline sys-
tem, while one-to-many projection achieves a larger
improvement. This confirms our observation of the
non-one-to-one mapping between source-side and
target-side topics.
6.5 Effect on Various Types of Rules
To get a more detailed analysis of the result, we
further compare the effect of our method on differ-
ent types of rules. We divide the rules into three
types: phrase rules, which only contain terminal-
s and are the same as the phrase pairs in phrase-
based system; monotone rules, which contain non-
terminals and produce monotone translations; re-
ordering rules, which also contain non-terminals but
change the order of translations. We define the
monotone and reordering rules according to Chiang
et al, (2008).
Table 5 show the results. We can see that our
method achieves improvements on all the three type-
s of rules. Our topic similarity method on mono-
tone rule achieves the most improvement which is
0.6 BLEU points, while the improvement on reorder-
ing rules is the smallest among the three types. This
shows that topic information also helps the selec-
tions of rules with non-terminals.
7 Related Work
In addition to the topic-specific lexicon transla-
tion method mentioned in the previous sections,
researchers also explore topic model for machine
translation in other ways.
Foster and Kunh (2007) describe a mixture-model
approach for SMT adaptation. They first split a
training corpus into different domains. Then, they
train separate models on each domain. Finally, they
combine a specific domain translation model with a
general domain translation model depending on var-
ious text distances. One way to calculate the dis-
tance is using topic model.
Gong et al (2010) introduce topic model for fil-
tering topic-mismatched phrase pairs. They first as-
sign a specific topic for the document to be translat-
ed. Similarly, each phrase pair is also assigned with
one specific topic. A phrase pair will be discarded if
its topic mismatches the document topic.
Researchers also introduce topic model for cross-
lingual language model adaptation (Tam et al, 2007;
Ruiz and Federico, 2011). They use bilingual topic
model to project latent topic distribution across lan-
guages. Based on the bilingual topic model, they ap-
ply the source-side topic weights into the target-side
topic model, and adapt the n-gram language model
of target side.
Our topic similarity model uses the document top-
ic information. From this point, our work is related
to context-dependent translation (Carpuat and Wu,
2007; He et al, 2008; Shen et al, 2009). Previous
work typically use neighboring words and sentence
level information, while our work extents the con-
text into the document level.
8 Conclusion and Future Work
We have presented a topic similarity model which
incorporates the rule-topic distributions on both the
source and target side into traditional hierarchical
phrase-based system. Our experimental results show
that our model achieves a better performance with
faster decoding speed than previous work on topic-
specific lexicon translation. This verifies the advan-
tage of exploiting topic model at the rule level over
the word level. Further improvement is achieved by
distinguishing topic-sensitive and topic-insensitive
rules using the topic sensitivity model.
In the future, we are interesting to find ways to
exploit topic model on bilingual data without docu-
ment boundaries, thus to enlarge the size of training
data. Furthermore, our training corpus mainly focus
on news, it is also interesting to apply our method on
corpus with more diverse topics. Finally, we hope to
apply our method to other translation models, espe-
cially syntax-based models.
757
Acknowledgement
The authors were supported by High-Technology
R&D Program (863) Project No 2011AA01A207
and 2012BAH39B03. This work was done dur-
ing Xinyan Xiao?s internship at I2R. We would like
to thank Yun Huang, Zhengxian Gong, Wenliang
Chen, Jun lang, Xiangyu Duan, Jun Sun, Jinsong
Su and the anonymous reviewers for their insightful
comments.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proc of WMT 2009.
David M. Blei and John D. Lafferty. 2007. A correlated
topic model of science. AAS, 1(1):17?35.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. In Proceedings of the MT Sum-
mit XI.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proc. of the Second Work-
shop on Statistical Machine Translation, pages 128?
135, Prague, Czech Republic, June.
Zhengxian Gong, Yu Zhang, and Guodong Zhou. 2010.
Statistical machine translation based on lda. In Proc.
IUCS 2010, page 286?290, Oct.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proc. EMNLP 2008.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of UAI 1999, pages 289?296.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proc. of EMNLP 2009.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.
Nick Ruiz and Marcello Federico. 2011. Topic adapta-
tion for lecture translation through bilingual latent se-
mantic models. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, July.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. ICSLP 2002.
Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proc. Coling 2008.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Proc.
ACL 2006.
Bin Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation. In
Proc. NIPS 2007.
758
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 902?911,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modeling the Translation of Predicate-Argument Structure for SMT
Deyi Xiong, Min Zhang?, Haizhou Li
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
Abstract
Predicate-argument structure contains rich se-
mantic information of which statistical ma-
chine translation hasn?t taken full advantage.
In this paper, we propose two discriminative,
feature-based models to exploit predicate-
argument structures for statistical machine
translation: 1) a predicate translation model
and 2) an argument reordering model. The
predicate translation model explores lexical
and semantic contexts surrounding a verbal
predicate to select desirable translations for
the predicate. The argument reordering model
automatically predicts the moving direction
of an argument relative to its predicate af-
ter translation using semantic features. The
two models are integrated into a state-of-the-
art phrase-based machine translation system
and evaluated on Chinese-to-English transla-
tion tasks with large-scale training data. Ex-
perimental results demonstrate that the two
models significantly improve translation accu-
racy.
1 Introduction
Recent years have witnessed increasing efforts to-
wards integrating predicate-argument structures into
statistical machine translation (SMT) (Wu and Fung,
2009b; Liu and Gildea, 2010). In this paper, we take
a step forward by introducing a novel approach to in-
corporate such semantic structures into SMT. Given
a source side predicate-argument structure, we at-
tempt to translate each semantic frame (predicate
and its associated arguments) into an appropriate tar-
get string. We believe that the translation of predi-
cates and reordering of arguments are the two central
?Corresponding author
issues concerning the transfer of predicate-argument
structure across languages.
Predicates1 are essential elements in sentences.
Unfortunately they are usually neither correctly
translated nor translated at all in many SMT sys-
tems according to the error study by Wu and Fung
(2009a). This suggests that conventional lexical and
phrasal translation models adopted in those SMT
systems are not sufficient to correctly translate pred-
icates in source sentences. Thus we propose a
discriminative, feature-based predicate translation
model that captures not only lexical information
(i.e., surrounding words) but also high-level seman-
tic contexts to correctly translate predicates.
Arguments contain information for questions of
who, what, when, where, why, and how in sentences
(Xue, 2008). One common error in translating ar-
guments is about their reorderings: arguments are
placed at incorrect positions after translation. In or-
der to reduce such errors, we introduce a discrim-
inative argument reordering model that uses the
position of a predicate as the reference axis to es-
timate positions of its associated arguments on the
target side. In this way, the model predicts moving
directions of arguments relative to their predicates
with semantic features.
We integrate these two discriminative models into
a state-of-the-art phrase-based system. Experimen-
tal results on large-scale Chinese-to-English transla-
tion show that both models are able to obtain signif-
icant improvements over the baseline. Our analysis
on system outputs further reveals that they can in-
deed help reduce errors in predicate translations and
argument reorderings.
1We only consider verbal predicates in this paper.
902
The paper is organized as follows. In Section 2,
we will introduce related work and show the signif-
icant differences between our models and previous
work. In Section 3 and 4, we will elaborate the pro-
posed predicate translation model and argument re-
ordering model respectively, including details about
modeling, features and training procedure. Section
5 will introduce how to integrate these two models
into SMT. Section 6 will describe our experiments
and results. Section 7 will empirically discuss how
the proposed models improve translation accuracy.
Finally we will conclude with future research direc-
tions in Section 8.
2 Related Work
Predicate-argument structures (PAS) are explored
for SMT on both the source and target side in some
previous work. As PAS analysis widely employs
global and sentence-wide features, it is computa-
tionally expensive to integrate target side predicate-
argument structures into the dynamic programming
style of SMT decoding (Wu and Fung, 2009b).
Therefore they either postpone the integration of tar-
get side PASs until the whole decoding procedure is
completed (Wu and Fung, 2009b), or directly project
semantic roles from the source side to the target side
through word alignments during decoding (Liu and
Gildea, 2010).
There are other previous studies that explore only
source side predicate-argument structures. Komachi
and Matsumoto (2006) reorder arguments in source
language (Japanese) sentences using heuristic rules
defined on source side predicate-argument structures
in a pre-processing step. Wu et al (2011) automate
this procedure by automatically extracting reorder-
ing rules from predicate-argument structures and ap-
plying these rules to reorder source language sen-
tences. Aziz et al (2011) incorporate source lan-
guage semantic role labels into a tree-to-string SMT
system.
Although we also focus on source side predicate-
argument structures, our models differ from the pre-
vious work in two main aspects: 1) we propose two
separate discriminative models to exploit predicate-
argument structures for predicate translation and ar-
gument reordering respectively; 2) we consider ar-
gument reordering as an argument movement (rel-
ative to its predicate) prediction problem and use
a discriminatively trained classifier for such predic-
tions.
Our predicate translation model is also related to
previous discriminative lexicon translation models
(Berger et al, 1996; Venkatapathy and Bangalore,
2007; Mauser et al, 2009). While previous models
predict translations for all words in vocabulary, we
only focus on verbal predicates. This will tremen-
dously reduce the amount of training data required,
which usually is a problem in discriminative lexi-
con translation models (Mauser et al, 2009). Fur-
thermore, the proposed translation model also dif-
fers from previous lexicon translation models in that
we use both lexical and semantic features. Our ex-
perimental results show that semantic features are
able to further improve translation accuracy.
3 Predicate Translation Model
In this section, we present the features and the train-
ing process of the predicate translation model.
3.1 Model
Following the context-dependent word models in
(Berger et al, 1996), we propose a discriminative
predicate translation model. The essential compo-
nent of our model is a maximum entropy classifier
pt(e|C(v)) that predicts the target translation e for
a verbal predicate v given its surrounding context
C(v). The classifier can be formulated as follows.
pt(e|C(v)) =
exp(?i ?ifi(e, C(v)))
?
e? exp(
?
i ?ifi(e?, C(v)))
(1)
where fi are binary features, ?i are weights of these
features. Given a source sentence which contains
N verbal predicates {vi}N1 , our predicate translation
model Mt can be denoted as
Mt =
N
?
i=1
pt(evi |C(vi)) (2)
Note that we do not restrict the target translation
e to be a single word. We allow e to be a phrase
of length up to 4 words so as to capture multi-word
translations for a verbal predicate. For example, a
Chinese verb ?u1(issue)? can be translated as ?to
be issued? or ?have issued? with modality words.
903
This will increase the number of classes to be pre-
dicted by the maximum entropy classifier. But ac-
cording to our observation, it is still computation-
ally tractable (see Section 3.3). If a verbal predicate
is not translated, we set e = NULL so that we can
also capture null translations for verbal predicates.
3.2 Features
The apparent advantage of discriminative lexicon
translation models over generative translation mod-
els (e.g., conventional lexical translation model as
described in (Koehn et al, 2003)) is that discrim-
inative models allow us to integrate richer contexts
(lexical, syntactic or semantic) into target translation
prediction. We use two kinds of features to predict
translations for verbal predicates: 1) lexical features
and 2) semantic features. All features are in the fol-
lowing binary form.
f(e, C(v)) =
{
1, if e = ? and C(v).? = ?
0, else
(3)
where the symbol ? is a placeholder for a possible
target translation (up to 4 words), the symbol ? indi-
cates a contextual (lexical or semantic) element for
the verbal predicate v, and the symbol ? represents
the value of ?.
Lexical Features: The lexical element ? is
extracted from the surrounding words of verbal
predicate v. We use the preceding 3 words and
the succeeding 3 words to define the lexical con-
text for the verbal predicate v. Therefore ? ?
{w?3, w?2, w?1, v, w1, w2, w3}.
Semantic Features: The semantic element ? is
extracted from the surrounding arguments of ver-
bal predicate v. In particular, we define a seman-
tic window centered at the verbal predicate with
6 arguments {A?3, A?2, A?1, A1, A2, A3} where
A?3 ? A?1 are arguments on the left side of v
while A1 ? A3 are those on the right side. Differ-
ent verbal predicates have different number of argu-
ments in different linguistic scenarios. We observe
on our training data that the number of arguments for
96.5% verbal predicates on each side (left/right) is
not larger than 3. Therefore the defined 6-argument
semantic window is sufficient to describe argument
contexts for predicates.
For each argument Ai in the defined seman-
f(e, C(v)) = 1 if and only if
e = adjourn and C(v).Ah?3 =Sn?
e = adjourn and C(v).Ar?1 = ARGM-TMP
e = adjourn and C(v).Ah1 =U
e = adjourn and C(v).Ar2 = null
e = adjourn and C(v).Ah3 = null
Table 1: Semantic feature examples.
tic window, we use its semantic role (i.e., ARG0,
ARGM-TMP and so on) Ari and head word Ahi to
define semantic context elements ?. If an argument
Ai does not exist for the verbal predicate v 2, we set
the value of both Ari and Ahi to null.
Figure 1 shows a Chinese sentence with its
predicate-argument structure and English transla-
tion. The verbal predicate ?>?/adjourn? (in bold)
has 4 arguments: one in an ARG0 agent role, one
in an ARGM-ADV adverbial modifier role, one in
an ARGM-TMP temporal modifier role and the last
one in an ARG1 patient role. Table 1 shows several
semantic feature examples of this verbal predicate.
3.3 Training
In order to train the discriminative predicate transla-
tion model, we first parse source sentences and la-
beled semantic roles for all verbal predicates (see
details in Section 6.1) in our word-aligned bilingual
training data. Then we extract all training events for
verbal predicates which occur at least 10 times in
the training data. A training event for a verbal predi-
cate v consists of all contextual elements C(v) (e.g.,
w1, Ah1 ) defined in the last section and the target
translation e. Using these events, we train one max-
imum entropy classifier per verbal predicate (16,121
verbs in total) via the off-the-shelf MaxEnt toolkit3.
We perform 100 iterations of the L-BFGS algorithm
implemented in the training toolkit for each verbal
predicate with both Gaussian prior and event cutoff
set to 1 to avoid overfitting. After event cutoff, we
have an average of 140 classes (target translations)
per verbal predicate with the maximum number of
classes being 9,226. The training takes an average of
52.6 seconds per verb. In order to expedite the train-
2For example, the verb v has only two arguments on its left
side. Thus argument A?3 doest not exist.
3Available at: http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
904
The [Security Council] will adjourn for [4 days] [starting Thursday]
Sn?1 ?2 [g3 ?o4 m?5] >?6 [o7 U8]
ARG0
ARGM-ADV
ARGM-TMP
ARG1
Figure 1: An example of predicate-argument structure in Chinese and its aligned English translation. The bold word in
Chinese is the verbal predicate. The subscripts on the Chinese sentence show the indexes of words from left to right.
ing, we run the training toolkit in a parallel manner.
4 Argument Reordering Model
In this section we introduce the discriminative ar-
gument reordering model, features and the training
procedure.
4.1 Model
Since the predicate determines what arguments are
involved in its semantic frame and semantic frames
tend to be cohesive across languages (Fung et al,
2006), the movements of predicate and its arguments
across translations are like the motions of a planet
and its satellites. Therefore we consider the reorder-
ing of an argument as the motion of the argument
relative to its predicate. In particular, we use the po-
sition of the predicate as the reference axis. The mo-
tion of associated arguments relative to the reference
axis can be roughly divided into 3 categories4: 1) no
change across languages (NC); 2) moving from the
left side of its predicate to the right side of the predi-
cate after translation (L2R); and 3) moving from the
right side of its predicate to the left side of the pred-
icate after translation (R2L).
Let?s revisit Figure 1. The ARG0, ARGM-ADV
and ARG1 are located at the same side of their predi-
cate after being translated into English, therefore the
reordering category of these three arguments is as-
signed as ?NC?. The ARGM-TMP is moved from
the left side of ?>?/adjourn? to the right side of
?adjourn? after translation, thus its reordering cate-
gory is L2R.
In order to predict the reordering category for
an argument, we propose a discriminative argu-
ment reordering model that uses a maximum en-
4Here we assume that the translations of arguments are not
interrupted by their predicates, other arguments or any words
outside the arguments in question. We leave for future research
the task of determining whether arguments should be translated
as a unit or not.
tropy classifier to calculate the reordering category
m ? {NC, L2R, R2L} for an argument A as fol-
lows.
pr(m|C(A)) =
exp(?i ?ifi(m, C(A)))
?
m? exp(
?
i ?ifi(m?, C(A)))
(4)
where C(A) indicates the surrounding context of A.
The features fi will be introduced in the next sec-
tion. We assume that motions of arguments are in-
dependent on each other. Given a source sentence
with labeled arguments {Ai}N1 , our discriminative
argument reordering model Mr is formulated as
Mr =
N
?
i=1
pr(mAi |C(Ai)) (5)
4.2 Features
The features fi used in the argument reordering
model still takes the binary form as in Eq. (3). Table
2 shows the features that are used in the argument
reordering model. We extract features from both the
source and target side. On the source side, the fea-
tures include the verbal predicate, the semantic role
of the argument, the head word and the boundary
words of the argument. On the target side, the trans-
lation of the verbal predicate, the translation of the
head word of the argument, as well as the boundary
words of the translation of the argument are used as
features.
4.3 Training
To train the argument reordering model, we first ex-
tract features defined in the last section from our
bilingual training data where source sentences are
annotated with predicate-argument structures. We
also study the distribution of argument reordering
categories (i.e.,NC, L2R and R2L) in the training
data, which is shown in Table 3. Most arguments,
accounting for 82.43%, are on the same side of their
verbal predicates after translation. The remaining
905
Features of an argument A for reordering
src
its verbal predicate Ap
its semantic role Ar
its head word Ah
the leftmost word of A
the rightmost word of A
tgt
the translation of Ap
the translation of Ah
the leftmost word of the translation of A
the rightmost word of the translation of A
Table 2: Features adopted in the argument reordering
model.
Reordering Category Percent
NC 82.43%
L2R 11.19%
R2L 6.38%
Table 3: Distribution of argument reordering categories
in the training data.
arguments (17.57%) are moved either from the left
side of their predicates to the right side after transla-
tion (accounting for 11.19%) or from the right side
to the left side of their translated predicates (ac-
counting for 6.38%).
After all features are extracted, we use the maxi-
mum entropy toolkit in Section 3.3 to train the maxi-
mum entropy classifier as formulated in Eq. (4). We
perform 100 iterations of L-BFGS.
5 Integrating the Two Models into SMT
In this section, we elaborate how to integrate the two
models into phrase-based SMT. In particular, we in-
tegrate the models into a phrase-based system which
uses bracketing transduction grammars (BTG) (Wu,
1997) for phrasal translation (Xiong et al, 2006).
Since the system is based on a CKY-style decoder,
the integration algorithms introduced here can be
easily adapted to other CKY-based decoding sys-
tems such as the hierarchical phrasal system (Chi-
ang, 2007).
5.1 Integrating the Predicate Translation
Model
It is straightforward to integrate the predicate trans-
lation model into phrase-based SMT (Koehn et al,
2003; Xiong et al, 2006). We maintain word
alignments for each phrase pair in the phrase ta-
ble. Given a source sentence with its predicate-
argument structure, we detect all verbal predicates
and load trained predicate translation classifiers for
these verbs. Whenever a hypothesis covers a new
verbal predicate v, we find the target translation e
for v through word alignments and then calculate its
translation probability pt(e|C(v)) according to Eq.
(1).
The predicate translation model (as formulated in
Eq. (2)) is integrated into the whole log-linear model
just like the conventional lexical translation model
in phrase-based SMT (Koehn et al, 2003). The
two models are independently estimated but comple-
mentary to each other. While the lexical translation
model calculates the probability of a verbal predi-
cate being translated given its local lexical context,
the discriminative predicate translation model is able
to employ both lexical and semantic contexts to pre-
dict translations for verbs.
5.2 Integrating the Argument Reordering
Model
Before we introduce the integration algorithm for
the argument reordering model, we define two
functions A and N on a source sentence and its
predicate-argument structure ? as follows.
? A(i, j, ?): from the predicate-argument struc-
ture ? , the function finds all predicate-argument
pairs which are completely located within the
span from source word i to j. For example, in
Figure 1, A(3, 6, ?) = {(>?, ARGM-TMP)}
while A(2, 3, ?) = {}, A(1, 5, ?) = {} because
the verbal predicate ?>?? is located outside
the span (2,3) and (1,5).
? N (i, k, j, ?): the function finds all predicate-
argument pairs that cross the two neighboring
spans (i, k) and (k+1, j). It can be formulated
as A(i, j, ?)? (A(i, k, ?)?A(k + 1, j, ?)).
We then define another function Pr to calculate
the argument reordering model probability on all ar-
guments which are found by the previous two func-
tions A and N as follows.
Pr(B) =
?
A?B
pr(mA|C(A)) (6)
906
where B denotes either A or N .
Following (Chiang, 2007), we describe the algo-
rithm in a deductive system. It is shown in Figure
2. The algorithm integrates the argument reordering
model into a CKY-style decoder (Xiong et al, 2006).
The item [X, i, j] denotes a BTG node X spanning
from i to j on the source side. For notational con-
venience, we only show the argument reordering
model probability for each item, ignoring all other
sub-model probabilities such as the language model
probability. The Eq. (7) shows how we calculate the
argument reordering model probability when a lex-
ical rule is applied to translate a source phrase c to
a target phrase e. The Eq. (8) shows how we com-
pute the argument reordering model probability for a
span (i, j) in a dynamic programming manner when
a merging rule is applied to combine its two sub-
spans in a straight (X ? [X1, X2]) or inverted or-
der (X ? ?X1, X2?). We directly use the probabili-
ties Pr(A(i, k, ?)) and Pr(A(k + 1, j, ?)) that have
been already obtained for the two sub-spans (i, k)
and (k + 1, j). In this way, we only need to calcu-
late the probability Pr(N (i, k, j, ?)) for predicate-
argument pairs that cross the two sub-spans.
6 Experiments
In this section, we present our experiments on
Chinese-to-English translation tasks, which are
trained with large-scale data. The experiments are
aimed at measuring the effectiveness of the proposed
discriminative predicate translation model and argu-
ment reordering model.
6.1 Setup
The baseline system is the BTG-based phrasal sys-
tem (Xiong et al, 2006). Our training corpora5
consist of 3.8M sentence pairs with 96.9M Chinese
words and 109.5M English words. We ran GIZA++
on these corpora in both directions and then applied
the ?grow-diag-final? refinement rule to obtain word
alignments. We then used all these word-aligned
corpora to generate our phrase table. Our 5-gram
language model was trained on the Xinhua section
of the English Gigaword corpus (306 million words)
5The corpora include LDC2004E12, LDC2004T08,
LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06,
LDC2003E07 and LDC2004T07.
using the SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing.
To train the proposed predicate translation model
and argument reordering model, we first parsed all
source sentences using the Berkeley Chinese parser
(Petrov et al, 2006) and then ran the Chinese se-
mantic role labeler6 (Li et al, 2010) on all source
parse trees to annotate semantic roles for all verbal
predicates. After we obtained semantic roles on the
source side, we extracted features as described in
Section 3.2 and 4.2 and used these features to train
our two models as described in Section 3.3 and 4.3.
We used the NIST MT03 evaluation test data as
our development set, and the NIST MT04, MT05
as the test sets. We adopted the case-insensitive
BLEU-4 (Papineni et al, 2002) as the evaluation
metric. Statistical significance in BLEU differences
was tested by paired bootstrap re-sampling (Koehn,
2004).
6.2 Results
Our first group of experiments is to investigate
whether the predicate translation model is able to
improve translation accuracy in terms of BLEU and
whether semantic features are useful. The experi-
mental results are shown in Table 4. From the table,
we have the following two observations.
? The proposed predicate translation models
achieve an average improvement of 0.57 BLEU
points across the two NIST test sets when all
features (lex+sem) are used. Such an improve-
ment is statistically significant (p < 0.01). Ac-
cording to our statistics, there are 5.07 verbal
predicates per sentence in NIST04 and 4.76
verbs per sentence in NIST05, which account
for 18.02% and 16.88% of all words in NIST04
and 05 respectively. This shows that not only
verbal predicates are semantically important,
they also form a major part of the sentences.
Therefore, whether verbal predicates are trans-
lated correctly or not has a great impact on the
translation accuracy of the whole sentence 7.
6Available at: http://nlp.suda.edu.cn/?jhli/.
7The example in Table 6 shows that the translations of
verbs even influences reorderings and translations of neighbor-
ing words.
907
X ? c/e
[X, i, j] : Pr(A(i, j, ?))
(7)
X ? [X1, X2] or ?X1, X2? [X1, i, k] : Pr(A(i, k, ?)) [X2, k + 1, j] : Pr(A(k + 1, j, ?))
[X, i, j] : Pr(A(i, k, ?)) ? Pr(A(k + 1, j, ?)) ? Pr(N (i, k, j, ?))
(8)
Figure 2: Integrating the argument reordering model into a BTG-style decoder.
Model NIST04 NIST05
Base 35.52 33.80
Base+PTM (lex) 35.71+ 34.09+
Base+PTM (lex+sem) 36.10++** 34.35++*
Table 4: Effects of the proposed predicate translation
model (PTM). PTM (lex): predicate translation model
with lexical features; PTM (lex+sem): predicate transla-
tion model with both lexical and semantic features; +/++:
better than the baseline (p < 0.05/0.01). */**: better
than Base+PTM (lex) (p < 0.05/0.01).
Model NIST04 NIST05
Base 35.52 33.80
Base+ARM 35.82++ 34.29++
Base+ARM+PTM 36.19++ 34.72++
Table 5: Effects of the proposed argument reordering
model (ARM) and the combination of ARM and PTM.
++: better than the baseline (p < 0.01).
? When we integrate both lexical and semantic
features (lex+sem) described in Section 3.2, we
obtain an improvement of about 0.33 BLEU
points over the system where only lexical fea-
tures (lex) are used. Such a gain, which is sta-
tistically significant, confirms the effectiveness
of semantic features.
Our second group of experiments is to validate
whether the argument reordering model is capable
of improving translation quality. Table 5 shows the
results. We obtain an average improvement of 0.4
BLEU points on the two test sets over the base-
line when we incorporate the proposed argument re-
ordering model into our system. The improvements
on the two test sets are both statistically significant
(p < 0.01).
Finally, we integrate both the predicate translation
model and argument reordering model into the final
system. The two models collectively achieve an im-
provement of up to 0.92 BLEU points over the base-
line, which is shown in Table 5.
7 Analysis
In this section, we conduct some case studies to
show how the proposed models improve translation
accuracy by looking into the differences that they
make on translation hypotheses.
Table 6 displays a translation example which
shows the difference between the baseline and
the system enhanced with the predicate translation
model. There are two verbal predicates ?` /head
to? and ??\/attend? in the source sentence. In
order to get the most appropriate translations for
these two verbal predicates, we should adopt differ-
ent ways to translate them. The former should be
translated as a corresponding verb word or phrase
while the latter into a preposition word ?for?. Unfor-
tunately, the baseline incorrectly translates the two
verbs. Furthermore, such translation errors even re-
sult in undesirable reorderings of neighboring words
??|?/Bethlehem and ??g/mass?. This indi-
cates that verbal predicate translation errors may
lead to more errors, such as inappropriate reorder-
ings or lexical choices for neighboring words. On
the contrary, we can see that our predicate transla-
tion model is able to help select appropriate words
for both verbs. The correct translations of these two
verbs also avoid incorrect reorderings of neighbor-
ing words.
Table 7 shows another example to demonstrate
how the argument reordering model improve re-
orderings. The verbal predicate ??1/carry out?
has three arguments, ARG0, ARG-ADV and ARG1.
The ARG1 argument should be moved from the
right side of the predicate to its left side after trans-
lation. The ARG0 argument can either stay on the
left side or move to right side of the predicate. Ac-
908
Base
[?Z] &? ` ?|? ?\ [?S?] ?g
[thousands of] followers to Mass in Bethlehem [Christmas Eve]
Base+PTM
[?Z] &? ` ?|? ?\ [?S?] ?g
[thousands of] devotees [rushed to] Bethlehem for [Christmas Eve] mass
Ref thousands of worshippers head to Bethlehem for Christmas Midnight mass
Table 6: A translation example showing the difference between the baseline and the system with the predicate transla-
tion model (PTM). Phrase alignments in the two system outputs are shown with dashed lines. Chinese words in bold
are verbal predicates.
PAS [k'?@/J?wX?] ?? ?1 [??????]
ARG0
ARGM-ADV
ARG1
Base
[k'?] @ /J [?wX?] ?? [?1??] [????]
the more [important consultations] also set disaster [warning system]
Base+ARM
k' [?@] /J [?wX?] [???1] [??] [????]
more [important consultations] on [such a] disaster [warning system] [should be carried out]
Ref more important discussions will be held on the disaster warning system
Table 7: A translation example showing the difference between the baseline and the system with the argument re-
ordering model (ARM). The predicate-argument structure (PAS) of the source sentence is also displayed in the first
row.
cording to the phrase alignments of the baseline,
we clearly observe three serious translation errors:
1) the ARG0 argument is translated into separate
groups which are not adjacent on the target side;
2) the predicate is not translated at all; and 3) the
ARG1 argument is not moved to the left side of the
predicate after translation. All of these 3 errors are
avoided in the Base+ARM system output as a re-
sult of the argument reordering model that correctly
identifies arguments and moves them in the right di-
rections.
8 Conclusions and Future Work
We have presented two discriminative models to
incorporate source side predicate-argument struc-
tures into SMT. The two models have been inte-
grated into a phrase-based SMT system and evalu-
ated on Chinese-to-English translation tasks using
large-scale training data. The first model is the pred-
icate translation model which employs both lexical
and semantic contexts to translate verbal predicates.
The second model is the argument reordering model
which estimates the direction of argument move-
ment relative to its predicate after translation. Ex-
perimental results show that both models are able to
significantly improve translation accuracy in terms
of BLEU score.
In the future work, we will extend our predicate
translation model to translate both verbal and nom-
inal predicates. Nominal predicates also frequently
occur in Chinese sentences and thus accurate trans-
lations of them are desirable for SMT. We also want
to address another translation issue of arguments as
shown in Table 7: arguments are wrongly translated
into separate groups instead of a cohesive unit (Wu
and Fung, 2009a). We will build an argument seg-
mentation model that follows (Xiong et al, 2011) to
determine whether arguments should be translated
as a unit or not.
909
References
Wilker Aziz, Miguel Rios, and Lucia Specia. 2011. Shal-
low semantic trees for smt. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
316?322, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Pascale Fung, Wu Zhaojun, Yang Yongsheng, and Dekai
Wu. 2006. Automatic learning of chinese english se-
mantic structure mapping. In IEEE/ACL 2006 Work-
shop on Spoken Language Technology (SLT 2006),
Aruba, December.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 58?54, Edmon-
ton, Canada, May-June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Mamoru Komachi and Yuji Matsumoto. 2006. Phrase
reordering for statistical machine translation based on
predicate-argument structure. In In Proceedings of the
International Workshop on Spoken Language Trans-
lation: Evaluation Campaign on Spoken Language
Translation, pages 77?82.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108?
1117, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), pages 716?724, Beijing,
China, August. Coling 2010 Organizing Committee.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210?218, Singa-
pore, August. Association for Computational Linguis-
tics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm?an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado, USA, September.
Sriram Venkatapathy and Srinivas Bangalore. 2007.
Three models for discriminative machine translation
using global lexical selection and sentence reconstruc-
tion. In Proceedings of SSST, NAACL-HLT 2007 /
AMTA Workshop on Syntax and Structure in Statisti-
cal Translation, pages 96?102, Rochester, New York,
April. Association for Computational Linguistics.
Dekai Wu and Pascale Fung. 2009a. Can semantic
role labeling improve smt. In Proceedings of the
13th Annual Conference of the EAMT, pages 218?225,
Barcelona, May.
Dekai Wu and Pascale Fung. 2009b. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 13?16, Boulder, Colorado,
June. Association for Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting pre-
ordering rules from predicate-argument structures. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 29?37, Chiang
Mai, Thailand, November. Asian Federation of Natu-
ral Language Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521?528, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011. A
maximum-entropy segmentation model for statistical
machine translation. IEEE Transactions on Audio,
Speech and Language Processing, 19(8):2494?2505.
910
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225?255.
911
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382?386,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bilingual Lexical Cohesion Trigger Model for Document-Level
Machine Translation
Guosheng Ben? Deyi Xiong?? Zhiyang Teng? Yajuan Lu?? Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{benguosheng,tengzhiyang,lvyajuan,liuqun}@ict.ac.cn
?School of Computer Science and Technology,Soochow University
{dyxiong}@suda.edu.cn
?Centre for Next Generation Localisation, Dublin City University
{qliu}@computing.dcu.ie
Abstract
In this paper, we propose a bilingual lexi-
cal cohesion trigger model to capture lex-
ical cohesion for document-level machine
translation. We integrate the model into
hierarchical phrase-based machine trans-
lation and achieve an absolute improve-
ment of 0.85 BLEU points on average over
the baseline on NIST Chinese-English test
sets.
1 Introduction
Current statistical machine translation (SMT) sys-
tems are mostly sentence-based. The major draw-
back of such a sentence-based translation fash-
ion is the neglect of inter-sentential dependencies.
As a linguistic means to establish inter-sentential
links, lexical cohesion ties sentences together in-
to a meaningfully interwoven structure through
words with the same or related meanings (Wong
and Kit, 2012).
This paper studies lexical cohesion devices and
incorporate them into document-level machine
translation. We propose a bilingual lexical cohe-
sion trigger model to capture lexical cohesion for
document-level SMT. We consider a lexical co-
hesion item in the source language and its corre-
sponding counterpart in the target language as a
trigger pair, in which we treat the source language
lexical cohesion item as the trigger and its target
language counterpart as the triggered item. Then
we use mutual information to measure the strength
of the dependency between the trigger and trig-
gered item.
We integrate this model into a hierarchical
phrase-based SMT system. Experiment results
?Corresponding author
show that it is able to achieve substantial improve-
ments over the baseline.
The remainder of this paper proceeds as fol-
lows: Section 2 introduces the related work and
highlights the differences between previous meth-
ods and our model. Section 3 elaborates the pro-
posed bilingual lexical cohesion trigger model, in-
cluding the details of identifying lexical cohesion
devices, measuring dependency strength of bilin-
gual lexical cohesion triggers and integrating the
model into SMT. Section 4 presents experiments
to validate the effectiveness of our model. Finally,
Section 5 concludes with future work.
2 Related Work
As a linguistic means to establish inter-sentential
links, cohesion has been explored in the literature
of both linguistics and computational linguistics.
Cohesion is defined as relations of meaning that
exist within the text and divided into grammatical
cohesion that refers to the syntactic links between
text items and lexical cohesion that is achieved
through word choices in a text by Halliday and
Hasan (1976). In order to improve the quality of
machine translation output, cohesion has served as
a high level quality criterion in post-editing (Vas-
concellos, 1989). As a part of COMTIS project,
grammatical cohesion is integrated into machine
translation models to capture inter-sentential links
(Cartoni et al, 2011). Wong and Kit (2012) in-
corporate lexical cohesion to machine translation
evaluation metrics to evaluate document-level ma-
chine translation quality. Xiong et al (2013) inte-
grate various target-side lexical cohesion devices
into document-level machine translation. Lexical
cohesion is also partially explored in the cache-
based translation models of Gong et al (2011) and
translation consistency constraints of Xiao et al
382
(2011).
All previous methods on lexical cohesion for
document-level machine translation as mentioned
above have one thing in common, which is that
they do not use any source language information.
Our work is mostly related to the mutual infor-
mation trigger based lexical cohesion model pro-
posed by Xiong et al (2013). However, we sig-
nificantly extend their model to a bilingual lexical
cohesion trigger model that captures both source
and target-side lexical cohesion items to improve
target word selection in document-level machine
translation.
3 Bilingual Lexical Cohesion Trigger
Model
3.1 Identification of Lexical Cohesion Devices
Lexical cohesion can be divided into reiteration
and collocation (Wong and Kit, 2012). Reitera-
tion is a form of lexical cohesion which involves
the repetition of a lexical item. Collocation is a
pair of lexical items that have semantic relation-
s, such as synonym, near-synonym, superordinate,
subordinate, antonym, meronym and so on. In
the collocation, we focus on the synonym/near-
synonym and super-subordinate semantic relation-
s 1. We define lexical cohesion devices as content
words that have lexical cohesion relations, namely
the reiteration, synonym/near-synonym and super-
subordinate.
Reiteration is common in texts. Take the fol-
lowing two sentences extracted from a document
for example (Halliday and Hasan, 1976).
1. There is a boy climbing the old elm.
2. That elm is not very safe.
We see that word elm in the first sentence is re-
peated in the second sentence. Such reiteration de-
vices are easy to identify in texts. Synonym/near-
synonym is a semantic relationship set. We can
use WordNet (Fellbaum, 1998) to identify them.
WordNet is a lexical resource that clusters words
with the same sense into a semantic group called
synset. Synsets in WordNet are organized ac-
cording to their semantic relations. Let s(w) de-
note a function that defines all synonym words of
w grouped in the same synset in WordNet. We
can use the function to compute all synonyms and
near-synonyms for word w. In order to represen-
t conveniently, s0 denotes the set of synonyms in
1Other collocations are not used frequently, such as
antonyms. So we we do not consider them in our study.
s(w). Near-synonym set s1 is defined as the union
of all synsets that are defined by the function s(w)
where w? s0. It can be formulated as follows.
s1 =
?
w?s0
s(w) (1)
s2 =
?
w?s1
s(w) (2)
s3 =
?
w?s2
s(w) (3)
Similarly sm can be defined recursively as follows.
sm =
?
w?sm?1
s(w) (4)
Obviously, We can find synonyms and near-
synonyms for word w according to formula (4).
Superordinate and subordinate are formed by
words with an is-a semantic relation in WordNet.
As the super-subordinate relation is also encoded
in WordNet, we can define a function that is simi-
lar to s(w) identify hypernyms and hyponyms.
We use rep, syn and hyp to represent the lex-
ical cohesion device reiteration, synonym/near-
synonym and super-subordinate respectively here-
after for convenience.
3.2 Bilingual Lexical Cohesion Trigger
Model
In a bilingual text, lexical cohesion is present in
the source and target language in a synchronous
fashion. We use a trigger model capture such a
bilingual lexical cohesion relation. We define xRy
(R?{rep, syn, hyp}) as a trigger pair where x is
the trigger in the source language and y the trig-
gered item in the target language. In order to cap-
ture these synchronous relations between lexical
cohesion items in the source language and their
counterparts in the target language, we use word
alignments. First, we identify a monolingual lexi-
cal cohesion relation in the target language in the
form of tRy where t is the trigger, y the triggered
item that occurs in a sentence succeeding the sen-
tence of t, and R?{rep, syn, hyp}. Second, we
find word x in the source language that is aligned
to t in the target language. We may find multiple
words xk1 in the source language that are aligned
to t. We use all of them xiRt(1?i?k) to define
bilingual lexical cohesion relations. In this way,
we can create bilingual lexical cohesion relations
xRy (R?{rep, syn, hyp}): x being the trigger and
y the triggered item.
383
The possibility that y will occur given x is equal
to the chance that x triggers y. Therefore we mea-
sure the strength of dependency between the trig-
ger and triggered item according to pointwise mu-
tual information (PMI) (Church and Hanks, 1990;
Xiong et al, 2011).
The PMI for the trigger pair xRy where x is the
trigger, y the triggered item that occurs in a target
sentence succeeding the target sentence that aligns
to the source sentence of x, and R?{rep, syn, hyp}
is calculated as follows.
PMI(xRy) = log( p(x, y,R)p(x,R)p(y,R) ) (5)
The joint probability p(x, y,R) is:
p(x, y,R) = C(x, y,R)?
x,y C(x, y,R)
(6)
where C(x, y,R) is the number of aligned bilin-
gual documents where both x and y occur
with the relation R in different sentences, and?
x,y C(x, y,R) is the number of bilingual docu-
ments where this relation R occurs. The marginal
probabilities of p(x,R) and p(y,R) can be calcu-
lated as follows.
p(x,R) =
?
y
C(x, y,R) (7)
p(y,R) =
?
x
C(x, y,R) (8)
Given a target sentence ym1 , our bilingual lexical
cohesion trigger model is defined as follows.
MIR(ym1 ) =
?
yi
exp(PMI(?Ryi)) (9)
where yi are content words in the sentence ym1 and
PMI(?Ryi)is the maximum PMI value among all
trigger words xq1 from source sentences that have
been recently translated, where trigger words xq1
have an R relation with word yi.
PMI(?Ryi) = max1?j?qPMI(xjRyi) (10)
Three models MIrep(ym1 ), MIsyn(ym1 ),
MIhyp(ym1 ) for the reiteration device, the
synonym/near-synonym device and the super-
subordinate device can be formulated as above.
They are integrated into the log-linear model of
SMT as three different features.
3.3 Decoding
We incorporate our bilingual lexical cohesion trig-
ger model into a hierarchical phrase-based system
(Chiang, 2007). We add three features as follows.
? MIrep(ym1 )
? MIsyn(ym1 )
? MIhyp(ym1 )
In order to quickly calculate the score of each fea-
ture, we calculate PMI for each trigger pair be-
fore decoding. We translate document one by one.
During translation, we maintain a cache to store
source language sentences of recently translated
target sentences and three sets Srep, Ssyn, Shyp
to store source language words that have the re-
lation of {rep, syn, hyp} with content words gen-
erated in target language. During decoding, we
update scores according to formula (9). When one
sentence is translated, we store the corresponding
source sentence into the cache. When the whole
document is translated, we clear the cache for the
next document.
4 Experiments
4.1 Setup
Our experiments were conducted on the NIST
Chinese-English translation tasks with large-scale
training data. The bilingual training data contain-
s 3.8M sentence pairs with 96.9M Chinese word-
s and 109.5M English words from LDC2. The
monolingual data for training data English lan-
guage model includes the Xinhua portion of the
Gigaword corpus. The development set is the
NIST MT Evaluation test set of 2005 (MT05),
which contains 100 documents. We used the sets
of MT06 and MT08 as test sets. The numbers of
documents in MT06, MT08 are 79 and 109 respec-
tively. For the bilingual lexical cohesion trigger
model, we collected data with document bound-
aries explicitly provided. The corpora are select-
ed from our bilingual training data and the whole
Hong Kong parallel text corpus3, which contains
103,236 documents with 2.80M sentences.
2The corpora include LDC2002E18, LDC2003E07, LD-
C2003E14,LDC2004E12,LDC2004T07,LDC2004T08(Only
Hong Kong News), LDC2005T06 and LDC2005T10.
3They are LDC2003E14, LDC2004T07, LDC2005T06,
LDC2005T10 and LDC2004T08 (Hong Kong Hansard-
s/Laws/News).
384
We obtain the word alignments by running
GIZA++ (Och and Ney, 2003) in both direction-
s and applying ?grow-diag-final-and? refinemen-
t (Koehn et al, 2003). We apply SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram language model with Kneser-Ney smooth-
ing. Case-insensitive NIST BLEU (Papineni et
al., 2002) was used to measure translation per-
formance. We used minimum error rate training
MERT (Och, 2003) for tuning the feature weights.
4.2 Distribution of Lexical Cohesion Devices
in the Target Language
Cohesion Device Percentage(%)
rep 30.85
syn 17.58
hyp 18.04
Table 1: Distributions of lexical cohesion devices
in the target language.
In this section we want to study how these
lexical cohesion devices distribute in the train-
ing data before conducting our experiments on
the bilingual lexical cohesion model. Here
we study the distribution of lexical cohesion in
the target language (English). Table 1 shows
the distribution of percentages that are counted
based on the content words in the training da-
ta. From Table 1, we can see that the reitera-
tion cohesion device is nearly a third of all con-
tent words (30.85%), synonym/near-synonym and
super-subordinate devices account for 17.58% and
18.04%. Obviously, lexical cohesion devices are
frequently used in real-world texts. Therefore cap-
turing lexical cohesion devices is very useful for
document-level machine translation.
4.3 Results
System MT06 MT08 Avg
Base 30.43 23.32 26.88
rep 31.24 23.70 27.47
syn 30.92 23.71 27.32
hyp 30.97 23.48 27.23
rep+syn+hyp 31.47 23.98 27.73
Table 2: BLEU scores with various lexical co-
hesion devices on the test sets MT06 and MT08.
?Base? is the traditonal hierarchical system, ?Avg?
is the average BLEU score on the two test sets.
Results are shown in Table 2. From the table,
we can see that integrating a single lexical cohe-
sion device into SMT, the model gains an improve-
ment of up to 0.81 BLEU points on the MT06 test
set. Combining all three features rep+syn+hyp to-
gether, the model gains an improvement of up to
1.04 BLEU points on MT06 test set, and an av-
erage improvement of 0.85 BLEU points on the
two test sets of MT06 and MT08. These stable
improvements strongly suggest that our bilingual
lexical cohesion trigger model is able to substan-
tially improve the translation quality.
5 Conclusions
In this paper we have presented a bilingual lex-
ical cohesion trigger model to incorporate three
classes of lexical cohesion devices, namely the
reiteration, synonym/near-synonym and super-
subordinate devices into a hierarchical phrase-
based system. Our experimental results show
that our model achieves a substantial improvement
over the baseline. This displays the advantage of
exploiting bilingual lexical cohesion.
Grammatical and lexical cohesion have often
been studied together in discourse analysis. In
the future, we plan to extend our model to cap-
ture both grammatical and lexical cohesion in
document-level machine translation.
Acknowledgments
This work was supported by 863 State Key Project
(No.2011AA01A207) and National Key Technol-
ogy R&D Program(No.2012BAH39B03). Qun
Liu was also partially supported by Science Foun-
dation Ireland (Grant No.07/CE/I1142) as part of
the CNGL at Dublin City University. We would
like to thank the anonymous reviewers for their in-
sightful comments.
References
Bruno Cartoni, Andrea Gesmundo, James Hender-
son, Cristina Grisot, Paola Merlo, Thomas Mey-
er, Jacques Moeschler, Sandrine Zufferey, Andrei
Popescu-Belis, et al 2011. Improving mt coher-
ence through text-level processing of input texts:
the comtis project. http://webcast. in2p3. fr/videos-
the comtis project.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201?228.
Kenneth Ward Church and Patrick Hanks. 1990. Word
385
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Christine Fellbaum. 1998. Wordnet: An electronic
lexical database.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion
in english. English language series, 9.
Philipp Koehn, Franz Josef Och, and Daniel Mar-
cu. 2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, S-
apporo, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of the internation-
al conference on spoken language processing, vol-
ume 2, pages 901?904.
Muriel Vasconcellos. 1989. Cohesion and coherence
in the presentation of machine translation products.
Georgetown University Round Table on Languages
and Linguistics, pages 89?105.
Billy T. M. Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lex-
ical cohesion to document level. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1060?1068, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Machine Translation Sum-
mit, volume 13, pages 131?138.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011.
Enhancing language models in statistical machine
translation with backward n-grams and mutual in-
formation triggers. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1288?1297, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, Beijing,China.
386
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1459?1469,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Sense-Based Translation Model for Statistical Machine Translation
Deyi Xiong and Min Zhang
?
Provincial Key Laboratory for Computer Information Processing Technology
Soochow University, Suzhou, China 215006
{dyxiong, minzhang}@suda.edu.cn
Abstract
The sense in which a word is used deter-
mines the translation of the word. In this
paper, we propose a sense-based transla-
tion model to integrate word senses into
statistical machine translation. We build
a broad-coverage sense tagger based on
a nonparametric Bayesian topic model
that automatically learns sense clusters for
words in the source language. The pro-
posed sense-based translation model en-
ables the decoder to select appropriate
translations for source words according to
the inferred senses for these words us-
ing maximum entropy classifiers. Our
method is significantly different from pre-
vious word sense disambiguation reformu-
lated for machine translation in that the lat-
ter neglects word senses in nature. We test
the effectiveness of the proposed sense-
based translation model on a large-scale
Chinese-to-English translation task. Re-
sults show that the proposed model sub-
stantially outperforms not only the base-
line but also the previous reformulated
word sense disambiguation.
1 Introduction
One of very common phenomena in language is
that a plenty of words have multiple meanings.
In the context of machine translation, such dif-
ferent meanings normally produce different target
translations. Therefore a natural assumption is that
word sense disambiguation (WSD) may contribute
to statistical machine translation (SMT) by provid-
ing appropriate word senses for target translation
selection with context features (Carpuat and Wu,
2005).
?
Corresponding author
This assumption, however, has not been em-
pirically verified in the early days. Carpuat and
Wu (2005) adopt a standard formulation of WSD:
predicting word senses that are defined on an
ontology for ambiguous words. As they apply
WSD to Chinese-to-English translation, they pre-
dict word senses from a Chinese ontology HowNet
and project the predicted senses to English glosses
provided by HowNet. These glosses, used as the
sense predictions of their WSD system, are inte-
grated into a word-based SMT system either to
substitute for translation candidates of their trans-
lation model or to postedit the output of their SMT
system. They report that WSD degenerates the
translation quality of SMT.
In contrast to the standard WSD formulation,
Vickrey et al (2005) reformulate the task of WSD
for SMT as predicting possible target translations
rather than senses for ambiguous source words.
They show that such a reformulated WSD can im-
prove the accuracy of a simplified word translation
task.
Following this WSD reformulation for SMT,
Chan et al (2007) integrate a state-of-the-art
WSD system into a hierarchical phrase-based sys-
tem (Chiang, 2005). Carpuat and Wu (2007) also
use this reformulated WSD and further adapt it to
multi-word phrasal disambiguation. They both re-
port that the redefined WSD can significantly im-
prove SMT.
Although this reformulated WSD has proved
helpful for SMT, one question is not answered
yet: are pure word senses useful for SMT? The
early WSD for SMT (Carpuat and Wu, 2005)
uses projected word senses while the reformu-
lated WSD sidesteps word senses. In this pa-
per we would like to re-investigate this question
by resorting to word sense induction (WSI) that
is related to but different from WSD.
1
We use
1
We will discuss the relation and difference between WSI
and WSD in Section 2.
1459
WSI to obtain word senses for large-scale data.
With these word senses, we study in particular: 1)
whether word senses can be directly integrated to
SMT to improve translation quality and 2) whether
WSI-based model can outperform the reformu-
lated WSD in the context of SMT.
In order to incorporate word senses into SMT,
we propose a sense-based translation model that
is built on maximum entropy classifiers. We use a
nonparametric Bayesian topic model basedWSI to
infer word senses for source words in our training,
development and test set. We collect training in-
stances from the sense-tagged training data to train
the proposed sense-based translation model. Spe-
cially,
? Instead of predicting target translations for
ambiguous source words as the previous re-
formulated WSD does, we first predict word
senses for ambiguous source words. The pre-
dicted word senses together with other con-
text features are then used to predict possible
target translations for these words.
? Instead of using word senses defined by a
prespecified sense inventory as the standard
WSD does, we incorporate word senses that
are automatically learned from data into our
sense-based translation model.
We integrate the proposed sense-based transla-
tion model into a state-of-the-art SMT system and
conduct experiments on Chines-to-English trans-
lation using large-scale training data. Results
show that automatically learned word senses are
able to improve translation quality and the sense-
based translation model is better than the previous
reformulated WSD.
The remainder of this paper proceeds as fol-
lows. Section 2 introduces how we obtain word
senses for our large-scale training data via a WSI-
based broad-coverage sense tagger. Section 3
presents our sense-based translation model. Sec-
tion 4 describes how we integrate the sense-based
translation model into SMT. Section 5 elaborates
our experiments on the large-scale Chinese-to-
English translation task. Section 6 introduces re-
lated studies and highlights significant differences
from them. Finally, we conclude in Section 7 with
future directions.
2 WSI-Based Broad-Coverage Sense
Tagger
In order to obtain word senses for any source
words, we build a broad-coverage sense tagger
that relies on the nonparametric Bayesian model
based word sense induction. We first describe
WSI, especially WSI based on the Hierarchical
Dirichlet Process (HDP) (Teh et al, 2004), a non-
parametric version of Latent Dirichlet Allocation
(LDA) (Blei et al, 2003). We then elaborate how
we use the HDP-based WSI to predict sense clus-
ters and to annotate source words in our train-
ing/development/test sets with these sense clus-
ters.
2.1 Word Sense Induction
Before we introduce WSI, we differentiate word
type from word token. A word type refers to a
unique word as a vocabulary entry while a word
token is an occurrence of a word type. Take the
first sentence of this paragraph as an example, it
has 11 word tokens but 9 word types as there are
two word tokens of the word type ?we? and two
tokens of the word type ?word?.
Word sense induction is a task of automatically
inducing the underlying senses of word tokens
given the surrounding contexts where the word
tokens occur. The biggest difference from word
sense disambiguation lies in that WSI does not
rely on a predefined sense inventory. Such a pre-
specified list of senses is normally assumed by
WSD which predicts senses of word tokens using
this given inventory. From this perspective, WSI
can be treated as a clustering problem while WSD
a classification one.
Various clustering algorithms, such as k-means,
have been previously used for WSI. Recently, we
have also witnessed that WSI is cast as a topic
modeling problem where the sense clusters of a
word type are considered as underlying topics
(Brody and Lapata, 2009; Yao and Durme, 2011;
Lau et al, 2012). We follow this line to tailor a
topic modeling framework to induce word senses
for our large-scale training data.
In the topic-based WSI, surrounding context of
a word token is considered as a pseudo document
of the corresponding word type. A pseudo docu-
ment is composed of either a bag of neighboring
words of a word token, or the Part-to-Speech tags
of neighboring words, or other contextual infor-
mation elements. In this paper, we define a pseudo
1460
document as ?N neighboring words centered on
a given word token. Table 1 shows examples of
pseudo documents for a Chinese word ?w?nglu??
(network). These two pseudo documents are ex-
tracted from a sentence listed in the first row of Ta-
ble 1. Here we set N = 5. We can extract as many
pseudo documents as the number of word tokens
of a given word type that occur in training data.
The collection of all these extracted pseudo docu-
ments of the given word type forms a corpus. We
can induce topics on this corpus for each pseudo
document via topic modeling approaches.
Figure 1(a) shows the LDA-based WSI for a
given word type W . The outer plate represents
replicates of pseudo documents which consist of
N neighboring words centered on the tokens of
the given word type W . w
j,i
is the i-th word of
the j-th pseudo document of the given word type
W . s
j,i
is the sense assigned to the word w
j,i
.
The conventional topic distribution ?
j
for the j-
th pseudo document is taken as the the distribu-
tion over senses for the given word type W . The
LDA generative process for sense induction is as
follows: 1) for each pseudo document D
j
, draw a
per-document sense distribution ?
j
from a Dirich-
let distribution Dir(?); 2) for each item w
j,i
in the
pseudo document D
j
, 2.1) draw a sense cluster
s
j,i
? Multinomial(?
j
); and 2.2) draw a word
w
j,i
? ?
s
j,i
where ?
s
j,i
is the distribution of
sense s
j,i
over words drawn from a Dirichlet dis-
tribution Dir(?).
As LDA needs to manually specify the num-
ber of senses (topics), a better idea is to let the
training data automatically determine the number
of senses for each word type. Therefore we re-
sort to the HDP, a natural nonparametric gener-
alization of LDA, for the inference of both sense
clusters and the number of sense clusters follow-
ing Lau et al (2012) and Yao and Durme (2011).
The HDP for WSI is shown in Figure 1(b). The
HDP generative process for word sense induction
is as follows: 1) sample a base distribution G
0
from a Dirichlet process DP(?,H) with a con-
centration parameter ? and a base distribution H;
2) for each pseudo document D
j
, sample a dis-
tribution G
j
? DP(?
0
, G
0
); 3) for each item
w
j,i
in the pseudo document D
j
, 3.1) sample a
sense cluster s
j,i
? G
j
; and 3.2) sample a word
w
j,i
? ?
s
j,i
. Here G
0
is a global distribution
over sense clusters that are shared by all G
j
. G
j
is
a per-document sense distribution over these sense
wj,i
?
?j
sj,i
j ? [1, J]
?k
k ? [1,K]
?
G0
Gj
sj,i
j ? [1, J]
wj,i
H
?
?0
(a) (b)
i ? [1, Nj ]
i ? [1, Nj ]
Figure 1: Graphical model representations of (a)
Latent Dirichlet Allocation for WSI, (b) Hierar-
chical Dirichlet Process for WSI.
clusters, which has its own document-specific pro-
portions of these sense clusters. The hyperparam-
eter ?, ?
0
in the HDP are both concentration pa-
rameters which control the variability of senses in
the global distribution G
0
and document-specific
distribution G
j
.
The HDP/LDA-based WSI complies with the
distributional hypothesis that states that words oc-
curring in the same contexts tend to have similar
meanings. We want to extend this hypothesis to
machine translation by building sense-based trans-
lation model upon the HDP-based word sense in-
duction: words with the same meanings tend to be
translated in the same way.
2.2 Word Sense Tagging
We adopt the HDP-based WSI to automatically
predict word senses and use these predicted senses
to annotate source words. We individually build a
HDP-based WSI model per word type and train
these models on the training data. The sense for a
word token is defined as the most probable sense
according to the per-document sense distribution
G
j
estimated for the corresponding pseudo doc-
ument that represents the surrounding context of
the word token. In particular, we take the follow-
ing steps.
1461
t? t?x?ng w?gu? w?nglu? y?ny?ng zh? zh?y? f?ngf?n h?ik? g?ngj? ? qu?b?o w?nglu? ?nqu?n?
Pseudo Documents for word ?w?nglu??
t? t?x?ng w?gu? w?nglu? y?ny?ng zh? zh?y? f?ngf?n h?ik?
f?ngf?n h?ik? g?ngj? ? qu?b?o w?nglu? ?nqu?n?
Table 1: Examples of pseudo documents extracted from a Chinese sentence (written in Chinese Pinyin).
? Data preprocessing We preprocess the
source side of our bilingual training data as
well as development and test set by removing
stop words and rare words.
? Training Data Sense Annotation From the
preprocessed training data, we extract all
possible pseudo documents for each source
word type. The collection of these extracted
pseudo documents is used as a corpus to train
a HDP-based WSI model for the source word
type. In this way, we can train as many HDP-
based WSI models as the number of word
types kept after preprocessing. The sense
with the highest probability output by the
HDP-based WSI model for each pseudo doc-
ument is used as the sense cluster to label the
corresponding word token.
? Test/Dev Data Sense Annotation From the
preprocessed test data, we can also extract
pseudo documents for each source word type
that occur in the test/dev set. Using the
trained HDP-based WSI model that corre-
spond to the source word type in question, we
can obtain the best sense assignment for each
pseudo document of the word type, which
in turn is used to annotate the corresponding
word token in the test/dev data.
3 Sense-Based Translation Model
In this section we present our sense-based transla-
tion model and describe the features that we use as
well as the training process of this model.
3.1 Model
The sense-based translation model estimates the
probability that a source word c is translated into a
target phrase e? given contextual information, in-
cluding word senses that are obtained using the
HDP-based WSI as described in the last section.
We allow the target phrase e? to be either a phrase
of length up to 3 words or NULL so that we can
capture both multi-word and null translations. The
essential component of the model is a maximum
entropy (MaxEnt) based classifier that is used to
predict the translation probability p(e?|C(c)). The
MaxEnt classifier can be formulated as follows.
p(e?|C(c)) =
exp(
?
i
?
i
h
i
(e?, C(c)))
?
e?
?
exp(
?
i
?
i
h
i
(e?
?
, C(c)))
(1)
where h
i
s are binary features, ?
i
s are weights of
these features, C(c) is the surrounding context of
c.
We define two groups of binary features: 1) lex-
icon features and 2) sense features. All these fea-
tures take the following form.
h(e?, C(c)) =
{
1, if e? = 2 and C(c).? = ?
0, else
(2)
where 2 is a placeholder for a possible target
translation (up to 3 words or NULL), ? is the name
of a contextual (lexicon or sense) feature for the
source word c, and the symbol ? represents the
value of the feature ?.
We extract both the lexicon and sense features
from a ?k-word window centered on the word c.
The lexicon features are defined as the preceding
k words, the succeeding k words and the word c
itself: {c
?k
, ..., c
?1
, c, c
1
, ..., c
k
}. The sense fea-
tures are defined as the predicted senses for these
words: {s
c
?k
, ..., s
c
?1
, s
c
, s
c
1
, ..., s
c
k
}.
As we also use these neighboring words to pre-
dict word senses in the HDP-based WSI, the infor-
mation provided by the lexicon and sense features
may overlap. This is not a issue for the MaxEnt
classifier as it can deal with arbitrary overlapping
features (Berger et al, 1996). One may also won-
der whether the sense features can contribute to
SMT new information that can NOT be obtained
from the lexicon features. First, we believe that
the senses induced by the HDP-based WSI pro-
vide a different view of data than that of the lex-
icon features. Second, the sense features contain
semantic distributional information learned by the
HDP across contexts where lexical words occur.
Third, we empirically investigate this doubt by
comparing two MaxEnt-based translation models
1462
in Section 5. One model only uses the lexicon fea-
tures while the other integrates both the lexicon
and sense features. The former model can be con-
sidered as a reformulated WSD for SMT as we de-
scribed in Section 1.
Given a source sentence {c
i
}
I
1
, the proposed
sense-based translation model M
s
can be denoted
as
M
s
=
?
c
i
?W
(e?
i
|C(c
i
)) (3)
where W is a set of words for which we build
MaxEnt classifiers (see the next subsection for the
discussion on how we build MaxEnt classifiers for
our sense-based translation model).
3.2 Training
The training of the proposed sense-based transla-
tion model is a process of estimating the feature
weights ?s in the equation (1). There are two
strategies that we can use to obtain these weights.
We can either build an all-in-one MaxEnt clas-
sifier that integrates all source word types c and
their possible target translations e? or build multi-
ple MaxEnt classifiers. If we train the all-in-one
classifier, we have to predict millions of classes
(target translations of length up to 3 words). This
is normally intractable in practice. Therefore we
take the second strategy: building multiple Max-
Ent classifiers with one classifier per source word
type.
In order to train these classifiers, we have to col-
lect training events from our word-aligned bilin-
gual training data where source words are anno-
tated with their corresponding sense clusters pre-
dicted by the HDP-based WSI as described in
Section 2. A training event for a source word c
consists of all contextual elements in the form of
C(c).? = ? defined in the last subsection and the
target translation e?. Using these collected events,
we can train our multiple classifiers. In prac-
tice, we do not build MaxEnt classifiers for source
words that occur less than 10 times in the train-
ing data and run the MaxEnt toolkit in a parallel
manner in order to expedite the training process.
4 Decoding with Sense-Based
Translation Model
The sense-based translation model described
above is integrated into the log-linear translation
model of SMT as a sense-based knowledge source.
The weight of this model is tuned by the minimum
source sentences HDP-basedWSI
sense-taggedsource sentences
MaxEntclassifiers
sense-basedtranslation modeldecoder
target sentences
othermodels
Figure 2: Architecture of SMT system with the
sense-based translation model.
error rate training (MERT) (Och, 2003) together
with other models such as the language model.
Figure 2 shows the architecture of the SMT
system enhanced with the sense-based translation
model. Before we translate a source sentence, we
use the HDP-based WSI models trained on the
training data to predict senses for word tokens oc-
curring in the source sentence as discussed in Sec-
tion 2.2. Note that the HDP-based WSI does not
predict senses for all words due to the following
two reasons.
? We do not train HDP-based WSI models for
word types for which we extract more than T
pseudo documents.
2
? In the test/dev set, there are some words that
are unseen in the training data. These un-
seen words, of course, do not have their HDP-
based WSI models.
For these words, we set a default sense (i.e. s
c
=
s
1
).
Sense tagging on test sentences can be done in
a preprocessing step. Once we get sense clus-
ters for word tokens in test sentences, we load
pre-trained MaxEnt classifiers of the correspond-
ing word types. During decoding, we keep word
alignments for each translation rule. Whenever a
new source word c is translated, we find its trans-
lation e? via the kept word alignments. We then
calculate the translation probability p(e?|C(c)) ac-
cording to the equation (1) using the correspond-
ing loaded classifier. In this way, we can easily
calculate the sense-based translation model score.
2
we set T = 20, 000.
1463
5 Experiments
In this section, we carried out a series of ex-
periments on Chinese-to-English translation us-
ing large-scale bilingual training data. In order to
build the proposed sense-based translation model,
we annotated the source part of the bilingual train-
ing data with word senses induced by the HDP-
based WSI. With the trained sense-based transla-
tion model, we would like to investigate the fol-
lowing two questions:
? Do word senses automatically induced by the
HDP-basedWSI improve translation quality?
? Does the sense-based translation model out-
perform the reformulated WSD for SMT?
5.1 Setup
Our baseline system is a state-of-the-art SMT
system which adapts Bracketing Transduction
Grammars (Wu, 1997) to phrasal translation and
equips itself with a maximum entropy based
reordering model (Xiong et al, 2006). We used
LDC corpora LDC2004E12, LDC2004T08,
LDC2005T10, LDC2003E14, LDC2002E18,
LDC2005T06, LDC2003E07, LDC2004T07 as
our bilingual training data which consists of
3.84M bilingual sentences, 109.5M English word
tokens and 96.9M Chinese word tokens. We ran
Giza++ on the training data in two directions
and applied the ?grow-diag-final? refinement
rule (Koehn et al, 2003) to obtain word align-
ments. From the word-aligned data, we extracted
weighted phrase pairs to generate our phrase
table. We trained a 5-gram language model on the
Xinhua section of the English Gigaword corpus
(306 million words) using the SRILM toolkit
(Stolcke, 2002) with the modified Kneser-Ney
smoothing (Chen and Goodman, 1996).
We trained our HDP-based WSI models via the
C++ HDP toolkit
3
(Wang and Blei, 2012). We
set the hyperparameters ? = 0.1 and ?
0
= 1.0
following Lau et al (2012).We extracted pseudo
documents from a ?10-word window centered on
the corresponding word token for each word type
following Brody and Lapata (2009). As described
in Section 2.2, we preprocessed the source part
of our bilingual training data by removing stop
words and infrequent words that occurs less than
3
http://www.cs.cmu.edu/
?
chongw/
resource.html
Training Test
# Word Types 67,723 4,348
# Total Pseudo Documents 27.73M 11,777
# Avg Pseudo Documents 427.79 2.71
# Total Senses 271,770 24,162
# Avg Senses 4.01 5.56
Table 2: Statistics of the HDP-based word sense
induction on the training and test data.
10 times in the training data. From the prepro-
cessed data, we extracted pseudo documents for
each word type to train a HDP-based WSI model
per word type. Note that we do not build WSI
models for highly frequent words that occur more
than 20,000 times in order to expedite the HDP
training process.
We trained our MaxEnt classifiers with the off-
the-shelf MaxEnt tool.
4
We performed 100 iter-
ations of the L-BFGS algorithm implemented in
the training toolkit on the collected training events
from the sense-annotated data as described in Sec-
tion 3.2. We set the Gaussian prior to 1 to avoid
overfitting. On average, we obtained 346 classes
(target translations) per source word type with the
maximum number of classes being 256,243. It
took an average of 57.5 seconds for training a
Maxent classifier.
We used the NIST MT03 evaluation test data as
our development set, and the NIST MT05 as the
test set. We evaluated translation quality with the
case-insensitive BLEU-4 (Papineni et al, 2002)
and NIST (Doddington, 2002). In order to al-
leviate the impact of MERT (Och, 2003) insta-
bility, we followed the suggestion of Clark et al
(2011) to run MERT three times and report aver-
age BLEU/NIST scores over the three runs for all
our experiments.
5.2 Statistics and Examples of Word Senses
Before we present our experiment results of the
sense-based translation model, we study some
statistics of the HDP-based WSI on the training
and test data. We show these statistics in Table 2.
There are 67,723 and 4,348 unique word types in
the training and test data after the preprocessing
step. For these word types, we extract 27.73M and
11,777 pseudo documents from the training and
test set respectively. On average, there are 427.79
4
http://homepages.inf.ed.ac.uk/
lzhang10/maxenttoolkit.html
1464
System BLEU(%) NIST
STM (?5w) 34.64 9.4346
STM (?10w) 34.76 9.5114
STM (?15w) - -
Table 4: Experiment results of the sense-based
translation model (STM) with lexicon and sense
features extracted from a window of size varying
from ?5 to ?15 words on the development set.
pseudo documents per word type in the training
data and 2.71 in the test set. The HDP-based
WSI learns 271,770 word senses in total using the
pseudo documents collected from the training data
and infers 24,162 word senses using the pseudo
documents extracted from the test set. There are
4.01 different senses per word type in the training
data and 5.56 in the test set on average.
Table 3 illustrates six different senses of the
word ??? (operate)? learned by the HDP-based
WSI in the training data. We also show the most
probable 10 words for each sense cluster. Sense s
1
represents the operations of company or organi-
zation, sense s
2
denotes country/institution/inter-
nation operations, sense s
3
refers to market opera-
tions, sense s
4
corresponds to business operations,
sense s
5
to public facility operations, and finally
s
6
to economy operations.
5.3 Impact of Window Size k used in MaxEnt
Classifiers
Our first group of experiments were conducted to
investigate the impact of the window size k on
translation performance in terms of BLEU/NIST
on the development set. We extracted both the lex-
icon and sense features from a ?k-word window
for our MaxEnt classifiers. We varied k from 5
to 15. Experiment results are shown in Table 4.
We achieve the best performance when k = 10.
This suggests that a ?10-word window context is
sufficient for predicting target translations for am-
biguous source words. We therefore set k = 10
for all experiments thereafter.
5.4 Effect of the Sense-Based Translation
Model
Our second group of experiments were carried out
to investigate whether the sense-base translation
model is able to improve translation quality by
comparing the system enhanced with our sense-
based translation model against the baseline. We
also studied the impact of word senses induced by
System BLEU(%) NIST
Base 33.53 9.0561
STM (sense) 34.15 9.2596
STM (sense+lexicon) 34.73 9.4184
Table 5: Experiment results of the sense-based
translation model (STM) against the baseline.
System BLEU(%) NIST
Base 33.53 9.0561
Reformulated WSD 34.16 9.3820
STM 34.73 9.4184
Table 6: Comparison results of the sense-based
translation model vs. the reformulated WSD for
SMT.
the HDP-based WSI on translation performance
by enforcing the sense-based translation model to
use only sense features. Table 5 shows the experi-
ment results. From the table, we can observe that
? Our sense-based translation model achieves
a substantial improvement of 1.2 BLEU
points over the baseline. This indicates that
the sense-based translation model is able to
help select correct translations for ambiguous
source words.
? If we only integrate sense features into
the sense-based translation model, we can
still outperform the baseline by 0.62 BLEU
points. This suggests that automatically in-
duced word senses alone are indeed useful for
machine translation.
5.5 Comparison to Word Sense
Disambiguation
As we mentioned in Section 3.1, our sense-based
translation model can be degenerated to a reformu-
lated WSD model for SMT if we only use lexicon
features in MaxEnt classifiers. This allows us to
directly compare our method against the reformu-
lated WSD for SMT. Table 6 shows the compari-
son result.
From the table, we can find that the sense-
based translation model outperforms the reformu-
lated WSD by 0.57 BLEU points. This suggests
that the HDP-based word sense induction is bet-
ter than the reformulated WSD in the context of
SMT. Furthermore, as the reformulated WSD is
a degenerated version of our sense-based transla-
tion model which only uses the lexicon features,
1465
s1
s
2
s
3
?? (operate) ?? (operate) ?? (operate)
?? (facility) ?? (satellite) ?? (market)
?? (plan) ?? (system) ?? (enterprise)
?? (foundation) ?? (country) ?? (competition)
?? (project) ?? (supply) ?? (assets)
?? (company) ?? (inter-nation) ?? (profit)
?? (structure) ?? (institution) ?? (cause)
?? (service) ?? (proceed) ?? (cost)
?? (organization) ?? (center) ?? (capital)
?? (supply) ?? (cooperate) ?? (business)
s
4
s
5
s
6
?? (cost) ?? (city) ?? (lie)
?? (share price) ?? (process) ?? (photograph)
27000 ??? (tap-water) 119
??? (Kosovo) ?? (factory) DPRK
?? (extra) ?? (car) ?? (insurance)
?? (wage) ?? (railway) ?? (overspend)
?? (dollar) ?? (sewage) ?? (position)
?? (commerce) ??? (office) ?? (economy)
?? (income) ?? (break-even) ??? (competitor)
??? (railway administration) ?? (component) ?? (balance)
Table 3: Six different senses learned for the word ???? from the training data.
the sense features used in our model do provide
new information that can not be obtained by the
lexicon features.
6 Related Work
In this section we introduce previous studies that
are related to our work. For ease of comparison,
we roughly divide them into 4 categories: 1) WSD
for SMT, 2) topic-based WSI, 3) topic model for
SMT and 4) lexical selection.
WSD for SMT As we mentioned in Section
1, WSD has been successfully reformulated and
adapted to SMT (Vickrey et al, 2005; Carpuat and
Wu, 2007; Chan et al, 2007). Rather than predict-
ing word senses for ambiguous words, the refor-
mulated WSD directly predicts target translations
for source words with context information. Our
sense-based translation model also predicts target
translations for SMT. The significant difference is
that we predict word senses automatically learned
from data and incorporate these predicted senses
into SMT. Our experiments show that such word
senses are able to improve translation quality.
Topic-based WSI Topic-based WSI can be
considered as the foundation of our work as we
use it to obtain broad-coverage word senses to an-
notate our large-scale training data. Brody and La-
pata (2009)?s work is the first attempt to approach
WSI via topic modeling. They adapt LDA to word
sense induction by building one topic model per
word type. According to them, there are 3 sig-
nificant differences between topic-based WSI and
generic topic modeling.
? First, the goal of topic-based WSI is to di-
vide contexts of a word type into different
categories, each representing a sense cluster.
However generic topic models aim at topic
distributions of documents.
? Second, generic topic modeling explores
whole documents for topic inference while
topic-based WSI uses much smaller units in
a document (e.g., surrounding words of a tar-
get word) for word sense induction.
? Finally, the number of induced word senses
in WSI is usually less than 10 while the num-
ber of inferred topics in generic topic model-
ing is tens or hundreds.
As LDA-based WSI needs to manually spec-
ify the number of word senses, Yao and Durme
(2011) propose HDP-based WSI that is capable of
1466
determining the number of senses for each word
type according to training data. Lau et al (2012)
adopt the HDP-based WSI for novel sense de-
tection and empirically show that the HDP-based
WSI is better than the LDA-based WSI. We follow
them to set the hyperparameters of HDP for train-
ing and incorporate automatically induced word
senses into SMT in our work.
Topic model for SMT Generic topic models
are also explored for SMT. Zhao and Xing (2007)
propose a bilingual topic model and integrate a
topic-specific lexicon translation model into SMT.
Tam et al (2007) also explore a bilingual topic
model for translation and language model adapta-
tion. Foster and Kunh (2007) introduce a mixture
model approach for translation model adaptation.
Xiao et al (2012) propose a topic-based similar-
ity model for rule selection in hierarchical phrase-
based translation. Xiong and Zhang (2013) em-
ploy a sentence-level topic model to capture co-
herence for document-level machine translation.
The difference between our work and these pre-
vious studies on topic model for SMT lies in that
we adopt topic-based WSI to obtain word senses
rather than generic topics and integrate induced
word senses into machine translation.
Lexical selection Our work is also related to
lexical selection in SMT where appropriate target
lexical items for source words are selected by a
statistical model with context information (Banga-
lore et al, 2007; Mauser et al, 2009). The refor-
mulated WSD discussed before can also be con-
sidered as a lexical selection model. The signif-
icant difference from these studies is that we per-
form lexical selection using automatically induced
word senses by the HDP on the source side.
7 Conclusion
We have presented a sense-based translation
model that integrates word senses into machine
translation. We capitalize on the broad-coverage
word sense induction system that is built on the
nonparametric Bayesian HDP to learn sense clus-
ters for words in the source language. We gen-
erate pseudo documents for word tokens in the
training/test data for the HDP-based WSI system
to infer topics. The most probable topic inferred
for a pseudo document is taken as the sense of
the corresponding word token. We incorporate
these learned word senses as translation evidences
into maximum entropy classifiers which form the
foundation of the proposed sense-based translation
model.
We carried out a series of experiments to vali-
date the effectiveness of the sense-based transla-
tion by comparing the model against the baseline
and the previous reformulated WSD. Our experi-
ment results show that
? The sense-based translation model is able to
substantially improve translation quality in
terms of both BLEU and NIST.
? The sense-based translation model is also
better than the previous reformulated WSD
for SMT.
? Word senses automatically induced by the
HDP-based WSI on large-scale training data
are very useful for machine translation. To
the best of our knowledge, this is the first at-
tempt to empirically verify the positive im-
pact of word senses on translation quality.
Comparing with macro topics of documents in-
ferred by LDA with bag of words from the whole
documents, word senses inferred by the HDP-
based WSI can be considered as micro topics. In
the future, we would like to explore both the micro
and macro topics for machine translation. Addi-
tionally, we also want to induce sense clusters for
words in the target language so that we can build
sense-based language model and integrate it into
SMT. We would like to investigate whether auto-
matically learned senses of proceeding words are
helpful for predicting succeeding words.
Acknowledgement
The work was sponsored by the National Natu-
ral Science Foundation of China under projects
61373095 and 61333018. We would like to thank
three anonymous reviewers for their insightful
comments.
References
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical Machine Translation through
Global Lexical Selection and Sentence Reconstruc-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
152?159, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
1467
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1):39?71.
David M. Blei, Andrew Y. Ng, Michael I. Jordan,
and John Lafferty. 2003. Latent Dirichlet Al-
location. Journal of Machine Learning Research,
3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian
Word Sense Induction. In Proceedings of the
12th Conference of the European Chapter of the
ACL (EACL 2009), pages 103?111, Athens, Greece,
March. Association for Computational Linguistics.
Marine Carpuat and Dekai Wu. 2005. Word Sense
Disambiguation vs. Statistical Machine Transla-
tion. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 387?394, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Marine Carpuat and Dekai Wu. 2007. Improving
Statistical Machine Translation Using Word Sense
Disambiguation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Sta-
tistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association of Com-
putational Linguistics, pages 33?40, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Stanley F. Chen and Joshua Goodman. 1996. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. In Proceedings of the 34th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?96, pages 310?318, Stroudsburg, PA,
USA. Association for Computational Linguistics.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better Hypothesis Testing for
Statistical Machine Translation: Controlling for Op-
timizer Instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176?181, Portland, Oregon, USA, June.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
occurrence Statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In Proc. of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
58?54, Edmonton, Canada, May-June.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word Sense
Induction for Novel Sense Detection. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 591?601, Avignon, France, April. Association
for Computational Linguistics.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
210?218, Singapore, August. Association for Com-
putational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz.
2007. Bilingual LSA-based adaptation for statis-
tical machine translation. Machine Translation,
21(4):187?207.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2004. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101.
David Vickrey, Luke Biewald, Marc Teyssier, and
Daphne Koller. 2005. Word-Sense Disambiguation
for Machine Translation. In HLT/EMNLP. The As-
sociation for Computational Linguistics.
C. Wang and D. M. Blei. 2012. A Split-Merge MCMC
Algorithm for the Hierarchical Dirichlet Process.
ArXiv e-prints, January.
1468
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A Topic Similarity Model for
Hierarchical Phrase-based Translation. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 750?758, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Deyi Xiong and Min Zhang. 2013. A Topic-Based Co-
herence Model for Statistical Machine Translation.
In Proceedings of the Twenty-Seventh AAAI Confer-
ence on Artificial Intelligence (AAAI-13), Bellevue,
Washington, USA, July.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 521?528,
Sydney, Australia, July.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric Bayesian Word Sense Induction. In
Proceedings of TextGraphs-6: Graph-based Meth-
ods for Natural Language Processing, pages 10?14,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Bin Zhao and Eric P. Xing. 2007. HM-BiTAM:
Bilingual Topic Exploration, Word Alignment, and
Translation. In Proc. NIPS 2007.
1469
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 11?12,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
Semantics, Discourse and Statistical Machine Translation
Deyi Xiong and Min Zhang
Provincial Key Laboratory for Computer Information Processing Technology
Soochow University, Suzhou, China 215006
{dyxiong, minzhang}@suda.edu.cn
1 Description
In the past decade, statistical machine translation
(SMT) has been advanced from word-based SMT
to phrase- and syntax-based SMT. Although this
advancement produces significant improvements
in BLEU scores, crucial meaning errors and lack
of cross-sentence connections at discourse level
still hurt the quality of SMT-generated transla-
tions. More recently, we have witnessed two ac-
tive movements in SMT research: one towards
combining semantics and SMT in attempt to gen-
erate not only grammatical but also meaning-
preserved translations, and the other towards ex-
ploring discourse knowledge for document-level
machine translation in order to capture inter-
sentence dependencies.
The emergence of semantic SMT are due to the
combination of two factors: the necessity of se-
mantic modeling in SMT and the renewed interest
of designing models tailored to relevant NLP/SMT
applications in the semantics community. The
former is represented by recent numerous studies
on exploring word sense disambiguation, semantic
role labeling, bilingual semantic representations as
well as semantic evaluation for SMT. The latter
is reflected in CoNLL shared tasks, SemEval and
SenEval exercises in recent years.
The need of capturing cross-sentence dependen-
cies for document-level SMT triggers the resur-
gent interest of modeling translation from the per-
spective of discourse. Discourse phenomena, such
as coherent relations, discourse topics, lexical co-
hesion that are beyond the scope of conventional
sentence-level n-grams, have been recently con-
sidered and explored in the context of SMT.
This tutorial aims at providing a timely and
combined introduction of such recent work along
these two trends as discourse is inherently con-
nected with semantics. The tutorial has three parts.
The first part critically reviews the phrase- and
syntax-based SMT. The second part is devoted to
the lines of research oriented to semantic SMT,
including a brief introduction of semantics, lex-
ical and shallow semantics tailored to SMT, se-
mantic representations in SMT, semantically mo-
tivated evaluation as well as advanced topics on
deep semantic learning for SMT. The third part
is dedicated to recent work on SMT with dis-
course, including a brief review on discourse stud-
ies from linguistics and computational viewpoints,
discourse research from monolingual to multilin-
gual, discourse-based SMT and a few advanced
topics.
The tutorial is targeted for researchers in the
SMT, semantics and discourse communities. In
particular, the expected audience comes from two
groups: 1) Researchers and students in the SMT
community who want to design cutting-edge mod-
els and algorithms for semantic SMT with various
semantic knowledge and representations, and who
would like to advance SMT from sentence-by-
sentence translation to document-level translation
with discourse information; 2) Researchers and
students from the semantics and discourse com-
munity who are interested in developing models
and methods and adapting them to SMT.
2 Outline
1. SMT Overall Review (30 minutes)
? SMT architecture
? phrase- and syntax-based SMT
2. Semantics and SMT (1 hour and 15 minutes)
? Brief introduction of semantics
? Lexical semantics for SMT
? Semantic representations in SMT
? Semantically Motivated Evaluation
? Advanced topics: deep semantic learn-
ing for SMT
? Future directions
11
3. Discourse and SMT (1 hour and 15 minutes)
? Introduction of discourse: linguistics,
computational and bilingual discourse
? Discourse-based SMT: modeling, train-
ing, decoding and evaluation
? Future directions
3 Bios of Presenters
Dr. Deyi Xiong is a professor at Sochoow Uni-
versity. His research interests are in the area of
natural language processing, particularly statisti-
cal machine translation and parsing. Previously
he was a research scientist at the Institute for
Infocomm Research of Singapore. He received
the B.Sc degree from China University of Geo-
sciences (Wuhan, China) in 2002, the Ph.D.degree
from the Institute of Computing Technology (Bei-
jing, China) in 2007, both in computer science. He
has published papers in prestigious journals and
conferences on statistical machine translation, in-
cluding Computational Linguistics, IEEE TASLP,
JAIR, NLE, ACL, EMNLP, AAAI and IJCAI. He
was the program co-chair of IALP 2012 and CLIA
workshop 2011.
Dr. Min Zhang, a distinguished professor and
Director of the Research Center of Human Lan-
guage Technology at Soochow University (China),
received his Bachelor degree and Ph.D. degree in
computer science from Harbin Institute of Tech-
nology in 1991 and 1997, respectively. From
1997 to 1999, he worked as a postdoctoral re-
search fellow in Korean Advanced Institute of Sci-
ence and Technology in South Korea. He began
his academic and industrial career as a researcher
at Lernout & Hauspie Asia Pacific (Singapore) in
Sep. 1999. He joined Infotalk Technology (Singa-
pore) as a researcher in 2001 and became a senior
research manager in 2002. He joined the Institute
for Infocomm Research (Singapore) as a research
scientist in Dec. 2003. He joined the Soochow
University as a distinguished professor in 2012.
His current research interests include machine
translation, natural language processing, informa-
tion extraction, social network computing and In-
ternet intelligence. He has co-authored more than
150 papers in leading journals and conferences,
and co-edited 10 books/proceedings published by
Springer and IEEE. He was the recipient of several
awards in China and oversea. He is the vice pres-
ident of COLIPS (2011-2013), the elected vice
chair of SIGHAN/ACL (2014-2015), a steering
committee member of PACLIC (2011-now), an
executive member of AFNLP (2013-2014) and
a member of ACL (since 2006). He supervises
Ph.D students at National University of Singapore,
Harbin Institute of Technology and Soochow Uni-
versity.
12
