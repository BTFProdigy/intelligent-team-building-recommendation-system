Proceedings of the Eighteenth Conference on Computational Language Learning, pages 30?38,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Automatic Transliteration of Romanized Dialectal Arabic
Mohamed Al-Badrashiny
?
, Ramy Eskander, Nizar Habash and Owen Rambow
?
Department of Computer Science, The George Washington University, Washington, DC
?
badrashiny@gwu.edu
Center for Computational Learning Systems, Columbia University, NYC, NY
{reskander,habash,rambow}@ccls.columbia.edu
Abstract
In this paper, we address the problem
of converting Dialectal Arabic (DA) text
that is written in the Latin script (called
Arabizi) into Arabic script following the
CODA convention for DA orthography.
The presented system uses a finite state
transducer trained at the character level
to generate all possible transliterations for
the input Arabizi words. We then filter
the generated list using a DA morpholog-
ical analyzer. After that we pick the best
choice for each input word using a lan-
guage model. We achieve an accuracy of
69.4% on an unseen test set compared to
63.1% using a system which represents a
previously proposed approach.
1 Introduction
The Arabic language is a collection of varieties:
Modern Standard Arabic (MSA), which is used
in formal settings and has a standard orthogra-
phy, and different forms of Dialectal Arabic (DA),
which are commonly used informally and with in-
creasing presence on the web, but which do not
have standard orthographies. While both MSA
and DA are commonly written in the Arabic script,
DA (and less so MSA) is sometimes written in
the Latin script. This happens when using an Ara-
bic keyboard is dispreferred or impossible, for ex-
ample when communicating from a mobile phone
that has no Arabic script support. Arabic written
in the Latin script is often referred to as ?Arabizi?.
Arabizi is not a letter-based transliteration from
the Arabic script as is, for example, the Buck-
walter transliteration (Buckwalter, 2004). Instead,
roughly speaking, writers use sound-to-letter rules
inspired by those of English
1
as well as informally
1
In different parts of the Arab World, the basis for the
Latin script rendering of DA may come from different lan-
established conventions to render the sounds of the
DA sentence. Because the sound-to-letter rules
of English are very different from those of Ara-
bic, we obtain complex mappings between the two
writing systems. This issue is compounded by the
underlying problem that DA itself does not have
any standard orthography in the Arabic script. Ta-
ble 1 shows different plausible ways of writing an
Egyptian Arabic (EGY) sentence in Arabizi and
in Arabic script.
Arabizi poses a problem for natural language
processing (NLP). While some tools have recently
become available for processing EGY input, e.g.,
(Habash et al., 2012b; Habash et al., 2013; Pasha
et al., 2014), they expect Arabic script input (or a
Buckwalter transliteration). They cannot process
Arabizi. We therefore need a tool that converts
from Arabizi to Arabic script. However, the lack
of standard orthography in EGY compounds the
problem: what should we convert Arabizi into?
Our answer to this question is to use CODA, a
conventional orthography created for the purpose
of supporting NLP tools (Habash et al., 2012a).
The goal of CODA is to reduce the data sparseness
that comes from the same word form appearing in
many spontaneous orthographies in data (be it an-
notated or unannotated). CODA has been defined
for EGY as well as Tunisian Arabic (Zribi et al.,
2014), and it has been used as part of different ap-
proaches for modeling DA morphology (Habash
et al., 2012b), tagging (Habash et al., 2013; Pasha
et al., 2014) and spelling correction (Eskander et
al., 2013; Farra et al., 2014).
This paper makes two main contributions. First,
we clearly define the computational problem of
transforming Arabizi to CODA. This improves
over previous work by unambiguously fixing the
guages that natively uses the Latin script, such as English
or French. In this paper, we concentrate on Egyptian Arabic,
which uses English as its main source of sound-to-letter rules.
30
target representation for the transformation. Sec-
ond, we perform experiments using different com-
ponents in a transformation pipeline, and show
that a combination of character-based transduc-
tion, filtering using a morphological analyzer, and
using a language model outperforms other archi-
tectures, including the state-of-the-art system de-
scribed in Darwish (2013). Darwish (2013) pre-
sented a conversion tool, but did not discuss con-
version into a conventionalized orthography, and
did not investigate different architectures. We
show in this paper that our proposed architecture,
which includes an EGY morphological analyzer,
improves over Darwish?s architecture.
This paper is structured as follows. We start out
by presenting relevant linguistic facts (Section 2)
and then we discuss related work. We present our
approach in Section 4 and our experiments and re-
sults in Section 5.
2 Linguistic Facts
2.1 EGY Spontaneous Orthography
An orthography is a specification of how to use
a particular writing system (script) to write the
words of a particular language. In cases where
there is no standard orthography, people use a
spontaneous orthography that is based on dif-
ferent criteria. The main criterion is phonol-
ogy: how to render a word pronunciation in
the given writing system. This mainly de-
pends on language-specific assumptions about the
grapheme-to-phoneme mapping. Another crite-
rion is to use cognates in a related language (sim-
ilar language or a language variant), where two
words represent a cognate if they are related et-
ymologically and have the same meaning. Ad-
ditionally, a spontaneous orthography may be af-
fected by speech effects, which are the lengthen-
ing of specific syllables to show emphasis or other
effects (such as
Q



J


J






J? ktyyyyr
2
?veeeery?).
EGY has no standard orthography. Instead,
it has a spontaneous orthography that is related
to the standard orthography of Modern Standard
Arabic. Table 1 shows an example of writing a
sentence in EGY spontaneous orthography in dif-
ferent variants.
2
Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al., 2007): (in alphabetical
order) Abt?jHxd?rzs?SDT
?
D??fqklmnhwy and the additional
symbols: ? Z, ?

@,
?
A @

,
?
A

@, ?w

?', ?y Z?', h?

?, ? ?.
2.2 Arabizi
Arabizi is a spontaneous orthography used to write
DA using the Latin script, the so-called Arabic
numerals, and other symbols commonly found on
various input devices such as punctuation. Arabizi
is commonly used by Arabic speakers to write in
social media and SMS and chat applications.
The orthography decisions made for writing
in Arabizi mainly depend on a phoneme-to-
grapheme mapping between the Arabic pronunci-
ation and the Latin script. This is largely based
on the phoneme-to-grapheme mapping used in En-
glish. Crucially, Arabizi is not a simple transliter-
ation of Arabic, under which each Arabic letter in
some orthography is replaced by a Latin letter (as
is the case in the Buckwalter transliteration used
widely in natural language processing but nowhere
else). As a result, it is not straightforward to con-
vert Arabizi to Arabic. We discuss some specific
aspects of Arabizi.
Vowels While EGY orthography omits vocalic
diacritics representing short vowels, Arabizi uses
the Latin script symbols for vowels (a, e, i, o, u, y)
to represent EGY?s short and long vowels, making
them ambiguous. In some cases, Arabizi words
omit short vowels altogether as is done in Arabic
orthography.
Consonants Another source of ambiguity is the
use of a single Latin letter to refer to multiple Ara-
bic phonemes. For example, the Latin letter "d" is
used to represent the sounds of the Arabic letters
X d and
	
? D. Additionally, some pairs of Arabizi
letters can ambiguously map to a single Arabic let-
ter or pairs of letters: "sh" can be use to represent

? ? or ?? sh. Arabizi also uses digits to repre-
sent some Arabic letters. For example, the dig-
its 2, 3, 5, 6, 7 and 9 are used to represent the
Hamza (glottal stop), and the sounds of the letters
? ? , p x, ? T, h H and ? S, respectively. How-
ever, when followed by "?", the digits 3, 6, 7 and
9 change their interpretations to the dotted version
of the Arabic letter:
	
? ?,
	
?
?
D, p x and
	
? D, re-
spectively. Moreover, "?" (as well as "q") may also
refer to the glottal stop.
Foreign Words Arabizi contains a large num-
ber of foreign words, that are either borrowings
such as mobile or instances of code switching such
as I love you.
Abbreviations Arabizi may also include some
abbreviations such as isa which means ?<? @ Z A

?
	
?@

?
An ?A? Allh ?God willing?.
31
Orthography Example
CODA
hPAJ
.
?@
	
?? ?


G
.
Am

?

?


	
?

? A?
mA ?ft? SHAby mn AmbArH
Non-CODA
hPAJ
.
?@
	
?? ?G
.
Ag??

?


	
??

?A?
Arabic Script mA?wft? SwHAb? mn AmbArH
hPAJ
.
	
K @

	
?? ?G
.
Am

?

?


	
?

??
m?ft? SHAb? mn
?
AnbArH
hPAJ
.
?@

	
?? ?


G
.
Am

?

?



J
	
?

? A?
mA ?fty? SHAby mn
?
AmbArH
Arabizi
mashoftesh sohaby men embare7
ma shftesh swhabi mn imbareh
mshwftish swhaby min ambare7
Table 1: The different spelling variants in EGY and Arabizi for writing the sentence "I have not seen my
friends since yesterday" versus its corresponding CODA form.
2.3 CODA
CODA is a conventionalized orthography for Di-
alectal Arabic (Habash et al., 2012a). In CODA,
every word has a single orthographic representa-
tion. CODA has five key properties (Eskander
et al., 2013). First, CODA is an internally con-
sistent and coherent convention for writing DA.
Second, CODA is primarily created for computa-
tional purposes, but is easy to learn and recognize
by educated Arabic speakers. Third, CODA uses
the Arabic script as used for MSA, with no ex-
tra symbols from, for example, Persian or Urdu.
Fourth, CODA is intended as a unified framework
for writing all dialects. CODA has been defined
for EGY (Habash et al., 2012a) as well as Tunisian
Arabic (Zribi et al., 2014). Finally, CODA aims
to maintain a level of dialectal uniqueness while
using conventions based on similarities between
MSA and the dialects. For a full presentation of
CODA and a justification and explanation of its
choices, see (Habash et al., 2012a).
CODA has been used as part of different ap-
proaches for modeling DA morphology (Habash
et al., 2012b), tagging (Habash et al., 2013; Pasha
et al., 2014) and spelling correction (Eskander et
al., 2013; Farra et al., 2014). Converting Dialec-
tal Arabic (written using a spontaneous Arabic or-
thography or Arabizi) to CODA is beneficial to
NLP applications that better perform on standard-
ized data with less sparsity (Eskander et al., 2013).
Table 1 shows the CODA form corresponding
to spontaneously written Arabic.
3 Related Work
Our proposed work has some similarities to Dar-
wish (2013). His work is divided into two sec-
tions: language identification and transliteration.
He used word and sequence-level features to iden-
tify Arabizi that is mixed with English. For Arabic
words, he modeled transliteration from Arabizi to
Arabic script, and then applied language model-
ing on the transliterated text. This is similar to our
proposed work in terms of transliteration and lan-
guage modeling. However, Darwish (2013) does
not target a conventionalized orthography, while
our system targets CODA. Additionally, Darwish
(2013) transliterates Arabic words only after filter-
ing out non-Arabic words, while we transliterate
the whole input Arabizi. Finally, he does not use
any morphological information, while we intro-
duce the use of a morphological analyzer to sup-
port the transliteration pipeline.
Chalabi and Gerges (2012) presented a hybrid
approach for Arabizi transliteration. Their work
relies on the use of character transformation rules
that are either handcrafted by a linguist or au-
tomatically generated from training data. They
also employ word-based and character-based lan-
guage models for the final transliteration choice.
Like Darwish (2013), the work done by Chalabi
and Gerges (2012) is similar to ours except that
it does not target a conventionalized orthography,
and does not use deep morphological information,
while our system does.
There are three commercial products that con-
32
vert Arabizi to Arabic, namely: Microsoft Maren,
3
Google Ta3reeb
4
and Yamli.
5
However, since
these products are for commercial purposes, there
is not enough information about their approaches.
But given their output, it is clear that they do
not follow a well-defined standardized orthogra-
phy like we do. Furthermore, these tools are pri-
marily intended as input method support, not full
text transliteration. As a result, their users? goal
is to produce Arabic script text not Arabizi text.
We expect, for instance, that users of these input
method support systems will use less or no code
switching to English, and they may employ char-
acter sequences that help them arrive at the target
Arabic script form, which otherwise they would
not write if they are targeting Arabizi.
Eskander et al. (2013) introduced a system
to convert spontaneous EGY to CODA, called
CODAFY. The difference between CODAFY and
our proposed system is that CODAFY works on
spontaneous text written in Arabic script, while
our system works on Arabizi, which involves a
higher degree of ambiguity. However, we use
CODAFY as a black-box module in our prepro-
cessing.
Additionally, there is some work on convert-
ing from dialectal Arabic to MSA, which is sim-
ilar to our work in terms of processing a dialec-
tal input. However, our final output is in EGY
and not MSA. Shaalan et al. (2007) introduced a
rule-based approach to convert EGY to MSA. Al-
Gaphari and Al-Yadoumi (2010) also used a rule-
based method to transform from Sanaani dialect to
MSA. Sawaf (2010), Salloum and Habash (2011)
and Salloum and Habash (2013) used morpholog-
ical analysis and morphosyntactic transformation
rules for processing EGY and Levantine Arabic.
There has been some work on machine translit-
eration by Knight and Graehl (1997). Al-Onaizan
and Knight (2002) introduced an approach for ma-
chine transliteration of Arabic names. Freeman
et al. (2006) also introduced a system for name
matching between English and Arabic, which
Habash (2008) employed as part of generating
English transliterations from Arabic words in the
context of machine translation. This work is sim-
ilar to ours in terms of text transliteration. How-
ever, our work is not restricted to names.
3
http://www.getmaren.com
4
http://www.google.com/ta3reeb
5
http://www.yamli.com/
4 Approach
4.1 Defining the Task
Our task is as follows: for each Arabizi word in
the input, we choose the Arabic script word which
is the correct CODA spelling of the input word
and which carries the intended meaning (as deter-
mined in the context of the entire available text).
We do not merge two or more input words into
a single Arabic script word. If CODA requires
two consecutive input Arabizi words to be merged,
we indicate this by attaching a plus to the end of
the first word. On the other hand, if CODA re-
quires an input Arabizi word to be broken into two
or more Arabic script words, we indicate this by
inserting a dash between the words. We do this
to maintain the bijection between input and out-
put words, i.e., to allow easy tracing of the Arabic
script back to the Arabizi input.
4.2 Transliteration Pipeline
The proposed system in this paper is called 3AR-
RIB.
6
Using the context of an input Arabizi word,
3ARRIB produces the word?s best Arabic script
CODA transliteration. Figure 1 illustrates the dif-
ferent components of 3ARRIB in both the train-
ing and processing phases. We summarize the full
transliteration process as follows. Each Arabizi
sentence input to 3ARRIB goes through a pre-
processing step of lowercasing (de-capitalization),
speech effects handling, and punctuation split-
ting. 3ARRIB then generates a list of all possi-
ble transliterations for each word in the input sen-
tence using a finite-state transducer that is trained
on character-level alignment from Arabizi to Ara-
bic script. We then experiment with different com-
binations of the following two components:
Morphological Analyzer We use CALIMA
(Habash et al., 2012b), a morphological analyzer
for EGY. For each input word, CALIMA provides
all possible morphological analyses, including the
CODA spelling for each analysis. All generated
candidates are passed through CALIMA. If CAL-
IMA has no analysis for a candidate, then that
candidate gets filtered out; otherwise, the CODA
spellings of the analyses from CALIMA become
the new candidates in the rest of the transliteration
pipeline. For some words, CALIMA may sug-
gest multiple CODA spellings that reflect different
analyses of the word.
6
3ARRIB (pronounced /ar-rib/) means ?Arabize!?.
33
FSM 
Candidates
FSM
CALIMA
(+tokenization)
Best 
Selections
LMFST model
SRILM
Arabizi ? Arabic 
Parallel Data
Giza++
Training phase
 Input Arabizi 
Script
FST
Egyptian Corpus
CALIMA 
Output
A* Search
Preprocessing
MADAMIRA
 Output Arabic 
Script
Figure 1: An illustration of the different components of the 3ARRIB system in both the training and
processing phases. FST: finite-state Transducer; LM: Language Model; CALIMA: Morphological Ana-
lyzer for Dialectal Arabic; MADAMIRA: Morphological Tagger for Arabic.
Language Model We disambiguate among the
possibilities for all input words (which consti-
tute a ?sausage? lattice) using an n-gram language
model.
4.3 Preprocessing
We apply the following preprocessing steps to the
input Arabizi text:
? We separate all attached emoticons such as
(:D, :p, etc.) and punctuation from the words.
We only keep the apostrophe because it is
used in Arabizi to distinguish between dif-
ferent sounds. 3ARRIB keeps track of any
word offset change, so that it can reconstruct
the same number of tokens at the end of the
pipeline.
? We tag emoticons and punctuation to protect
them from any change through the pipeline.
? We lowercase all letters.
? We handle speech effects by replacing any
sequence of the same letter whose length is
greater than two by a sequence of exactly
length two; for example, iiiii becomes ii.
4.4 Character-Based Transduction
We use a parallel corpus of Arabizi-Arabic words
to learn a character-based transduction model.
The parallel data consists of two sources. First,
we use 2,200 Arabizi-to-Arabic script pairs from
the training data used by (Darwish, 2013). We
manually revised the Arabic side to be CODA-
compliant. Second, we use about 6,300 pairs
of proper names in Arabic and English from
the Buckwalter Arabic Morphological Analyzer
(Buckwalter, 2004). Since proper names are typ-
ically transliterated, we expect them to be a rich
source for learning transliteration mappings.
The words in the parallel data are turned into
space-separated character tokens, which we align
using Giza++ (Och and Ney, 2003). We then use
the phrase extraction utility in the Moses statistical
machine translation system (Koehn et al., 2007) to
extract a phrase table which operates over char-
acters. The phrase table is then used to build a
finite-state transducer (FST) that maps sequences
of Arabizi characters into sequences of Arabic
script characters. We use the negative logarithmic
conditional probabilities of the Arabizi-to-Arabic
pairs in the phrase tables as costs inside the FST.
We use the FST to transduce an input Arabizi word
to one or more words in Arabic script, where ev-
ery resulting word in Arabic script is given a prob-
abilistic score.
As part of the preprocessing of the parallel data,
we associate all Arabizi letters with their word
location information (beginning, middle and end-
ing letters). This is necessary since some Arabizi
34
mapping phenomena happen only at specific loca-
tions. For example, the Arabizi letter "o" is likely
to be transliterated into

@ ? in Arabic if it appears
at the beginning of the word, but almost never so
if it appears in the middle of the word.
For some special Arabizi cases, we directly
transliterate input words to their correct Arabic
form using a table, without going through the FST.
For example, isa is mapped to ?

<? @ Z A

?
	
?@

?
An ?A?
Allh ?God willing?. There are currently 32 entries
in this table.
4.5 Morphological Analyzer
For every word in the Arabizi input, all the candi-
dates generated by the character-based transduc-
tion are passed through the CALIMA morpholog-
ical analyzer. For every candidate, CALIMA pro-
duces a list of all the possible morphological anal-
yses. The CODA for these analyses need not be
the same. For example, if the output from the char-
acter based transducer is Aly, then CALIMA pro-
duces the following CODA-compliant spellings:
??@

?
Al? ?to?, ?


?@

?
Al? ?to me? and ?


?

@
?
Aly ?automatic?
or ?my family?. All of these CODA spellings are
the output of CALIMA for that particular input
word. The output from CALIMA then becomes
the set of final candidates of the input Arabizi in
the rest of the transliteration pipeline. If a word
is not recognized by CALIMA, it gets filtered out
from the transliteration pipeline. However, if all
the candidates of some word are not recognized
by CALIMA, then we retain them all since there
should be an output for every input word.
We additionally run a tokenization step that
makes use of the generated CALIMA morphologi-
cal analysis. The tokenization scheme we target is
D3, which separates all clitics associated with the
word (Habash, 2010). For every word, we keep
a list of the possible tokenized and untokenized
CODA-compliant pairs. We use the tokenized or
untokenized forms as inputs to either a tokenized
or untokenized language model, respectively, as
described in the next subsection. The untokenized
form is necessary to retain the surface form at the
end of the transliteration process.
Standalone clitics are sometimes found in Ara-
bizi such as lel ragel (which corresponds to
?g
.
@P +?? ll+ rAjl ?for the man?). Since CALIMA
does not handle most standalone clitics, we keep
a lookup table that associates them with their tok-
enization information.
4.6 Language Model
We then use an EGY language model that is
trained on CODA-compliant text. We investi-
gate two options: a language model that has stan-
dard CODA white-space word tokenization con-
ventions (?untokenized?), and a language model
that has a D3 tokenized form of CODA in which
all clitics are separated (?tokenized?). The output
of the morphological analyzer (which is the input
to the LM component) is processed to match the
tokenization used in the LM.
The language models are built from a large
corpus of 392M EGY words.
7
The corpus is
first processed using CODAFY (Eskander et al.,
2013), a system for spontaneous text convention-
alization into CODA. This is necessary so that
our system remains CODA-compliant across the
whole transliteration pipeline. Eskander et al.
(2013) states that the best conventionalization re-
sults are obtained by running the MLE component
of CODAFY followed by an EGY morphological
tagger, MADA-ARZ (Habash et al., 2013). In the
work reported here, we use the newer version of
MADA-ARZ, named MADAMIRA (Pasha et al.,
2014). For the tokenized language model, we run
a D3 tokenization step on top of the processed text
by MADAMIRA. The processed data is used to
build a language model with Kneser-Ney smooth-
ing using the SRILM toolkit (Stolcke, 2002).
We use A* search to pick the best transliteration
for each word given its context. The probability of
any path in the A* search space combines the FST
probability of the words with the probability from
the language model. Thus, for any certain path of
selected Arabic possibilities A
0,i
= {a
0
, a
1
, ...a
i
}
given the corresponding input Arabizi sequence
W
0,i
= {w
0
, w
1
, ...w
i
}, the transliteration prob-
ability can be defined by equation (1).
P (A
0,i
|W
0,i
) =
i?
j=0
(P (a
j
|w
j
) ? P (a
j
|a
j?N+1,j?1
)) (1)
Where, N is the maximum affordable n-
gram length in the LM, P (a
j
|w
j
) is the
FST probability of transliterating the Ara-
bizi word w
j
into the Arabic word a
j
, and
P (a
j
|a
j?N+1,j?1
) is the LM probability of the se-
quence {a
j?N+1
, a
j?N+2
, ...a
j
}.
7
All of the resources we use are available from the Lin-
guistic Data Consortium: www.ldc.upenn.edu.
35
5 Experiments and Results
5.1 Data
We use two in-house data sets for development
(Dev; 502 words) and blind testing (Test; 1004
words). The data contains EGY Arabizi SMS
conversations that are mapped to Arabic script in
CODA by a CODA-trained EGY native speaker.
5.2 Experiments
We conducted a suite of experiments to evaluate
the performance of our approach and identify op-
timal settings on the Dev set. The optimal result
and the baseline are then applied to the blind Test
set. During development, the following settings
were explored:
? INV-Selection: The training data of the finite
state transducer is used to generate the list of
possibilities for each input Arabizi word. If
the input word cannot be found in the FST
training data, the word is kept in Arabizi.
? FST-ONLY: Pick the top choice from the list
generated by the finite state transducer.
? FST-CALIMA: Pick the top choice from the
list after the CALIMA filtering.
? FST-CALIMA-Tokenized-LM-5: Run the
full pipeline of 3ARRIB with a 5-gram to-
kenized LM.
8
? FST-CALIMA-Tokenized-LM-5-MLE:
The same as FST-CALIMA-Tokenized-
LM-5, but for an Arabizi word that appears
in training, force its most frequently seen
mapping directly instead of running the
transliteration pipeline for that word.
? FST-CALIMA-Untokenized-LM-5: Run
the full pipeline of 3ARRIB with a 5-gram
untokenized LM.
? FST-Untokenized-LM-5: Run the full
pipeline of 3ARRIB minus the CALIMA fil-
tering with a 5-gram untokenized LM. This
setup is analogous to the transliteration ap-
proach proposed by (Darwish, 2013). Thus
we use it as our baseline.
Each of the above experiments is evaluated
with exact match, and with Alif/Ya normalization
(El Kholy and Habash, 2010; Habash, 2010).
8
3, 5, and 7-gram LMs have been tested. The 3 and 5-
gram LMs give the same performance while the 7-gram LM
is the worst.
5.3 Results
Table 2 summarizes the results on the Dev set.
Our best performing setup is FST-CALIMA-
Tokenized-LM-5 which has 77.5% accuracy and
79.1% accuracy with normalization. The baseline
system, FST-Untokenized-LM-5, gives 74.1% ac-
curacy and 74.9 % accuracy with normalization.
This highlights the value of morphological filter-
ing as well as sparsity-reducing tokenization.
Table 3 shows how we do (best system and best
baseline) on a blind Test set. Although the accu-
racy drops overall, the gap between the best sys-
tem and the baseline increases.
5.4 Error Analysis
We conducted two error analyses for the best per-
forming transliteration setting on the Dev set. We
first analyze in which component the Dev set er-
rors occur. About 29% of the errors are cases
where the FST does not generate the correct an-
swer. An additional 15% of the errors happen be-
cause the correct answer is not covered by CAL-
IMA. The language model does not include the
correct answer in an additional 8% of the errors.
The rest of the errors (48%) are cases where the
correct answer is available in all components but
does not get selected.
Motivated by the value of Arabizi transliteration
for machine translation into English, we distin-
guish between two types of words: words that re-
main the same when translated into English, such
as English words, proper nouns, laughs, emoti-
cons, punctuations and digits (EN-SET) versus
EGY-only words (EGY-SET). Examples of words
in EN-SET are: love you very much (code switch-
ing), Peter (proper noun), haha (laugh), :D (emoti-
con), ! (punctuation) and 123 (digits).
While the overall performance of our best set-
tings is 77.5%, the accuracy of the EGY-SET by
itself is 84.6% as opposed to 46.2% for EN-SET.
This large difference reflects the fact that we do
not target English word transliteration into Arabic
script explicitly.
We now perform a second error analysis only on
the errors in the EGY-SET, in which we categorize
the errors by their linguistic type. About 25% of
the errors are non-CODA-compliant system out-
put, where the answer is a plausible non-CODA
form, i.e., a form that may be written or read eas-
ily by a native speaker who is not aware of CODA.
For example, the system generates the non-CODA
36
System Exact-Matching A/Y-normalization
INV-Selection 37.1 40.6
FST-ONLY (pick top choice) 63.1 65.1
FST-CALIMA (pick top choice) 66.1 68.9
FST-CALIMA-Tokenized-LM-5 77.5 79.1
FST-CALIMA-Tokenized-LM-5-MLE 68.7 73.5
FST-CALIMA-Untokenized-LM-5 77.3 78.9
FST-Untokenized-LM-5 74.1 74.9
Table 2: Results on the Dev set in terms of accuracy (%).
System Exact-Matching A/Y-normalization
FST-CALIMA-Tokenized-LM-5 69.4 73.9
FST-Untokenized-LM-5 63.1 65.4
Table 3: Results on the blind Test set in terms of accuracy (%).
form

??
	
?
	
JJ


? mynf?? instead of the correct CODA
form

??
	
?
	
JK


A? mA ynf?? ?it doesn?t work?. Ignor-
ing the CODA-related errors increases the overall
accuracy by about 3.0% to become 80.5%. The ac-
curacy of the EGY-SET rises to 88.3% as opposed
to 84.6% when considering CODA compliance.
Ambiguous Arabizi input contributes to an ad-
ditional 27% of the errors, where the system as-
signs a plausible answer that is incorrect in con-
text. For example, the word matar in the input
Arabizi fel matar ?at the airport? has two plausi-
ble out-of-context solutions: PA?? mTAr ?airport?
(contextually correct) and Q?? mTr ?rain? (contex-
tually incorrect).
In about 2% of the errors, the Arabizi input con-
tains a typo making it impossible to produce the
gold reference. For example, the input Arabizi
ba7bet contains a typo where the final t should turn
into k, so that it means ?J
.
kAK
.
bAHbk ?I love you
[2fs]?.
In the rest of the errors (about 46%), the sys-
tem fails to come up with the correct answer. In-
stead, it assigns a completely different word or
even an impossible word. For example, the cor-
rect answer for the input Arabizi sora ?picture? is

?P?? Swrh?, while the system produces the word
P?? swr ?wall?. Another example is the input Ara-
bizi talabt ?I asked for?, where the output from the
system is

?J
.
? A? TAlbh? ?student?, while the correct
answer is

IJ
.
?? tlbt ?I asked for, ordered? instead.
6 Conclusion and Future Work
We presented a method for converting dialectal
Arabic (specifically, EGY) written in Arabizi to
Arabic script following the CODA convention for
DA orthography. We achieve a 17% error reduc-
tion over our implementation of a previously pub-
lished work (Darwish, 2013) on a blind test set.
In the future, we plan to improve several aspects
of our models, particularly FST character map-
ping, the morphological analyzer coverage, and
language models. We also plan to work on the
problem of automatic identification of non-Arabic
words. We will extend the system to work on other
Arabic dialects. We also plan to make the 3AR-
RIB system publicly available.
Acknowledgement
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-
0014. Any opinions, findings and conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reflect the
views of DARPA.
References
G. Al-Gaphari and M. Al-Yadoumi. 2010. A method
to convert Sana?ani accent to Modern Standard Ara-
bic. International Journal of Information Science
and Management, pages 39?49.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 Workshop on Computational Ap-
proaches to Semitic Languages.
37
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Achraf Chalabi and Hany Gerges. 2012. Romanized
Arabic Transliteration. In Proceedings of the Sec-
ond Workshop on Advances in Text Input Methods
(WTIM 2012).
Kareem Darwish. 2013. Arabizi Detection and Con-
version to Arabic. CoRR.
Ahmed El Kholy and Nizar Habash. 2010. Techniques
for Arabic Morphological Detokenization and Or-
thographic Denormalization. In Proceedings of the
seventh International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing Spontaneous Or-
thography. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), Atlanta, GA.
Noura Farra, Nadi Tomeh, Alla Rozovskaya, and
Nizar Habash. 2014. Generalized Character-Level
Spelling Error Correction. In Proceedings of the
Conference of the Association for Computational
Linguistics (ACL), Baltimore, Maryland, USA.
Andrew Freeman, Sherri Condon, and Christopher
Ackerman. 2006. Cross linguistic name matching
in English and Arabic. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 471?478, New York City,
USA.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalu-
ation Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In Proceedings of the Twelfth Meeting of the
Special Interest Group on Computational Morphol-
ogy and Phonology, pages 1?9, Montr?al, Canada.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-08: HLT, Short Papers, pages 57?
60, Columbus, Ohio.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of the European
chapter of the Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Association for Computational
Linguistics, Prague, Czech Republic.
Franz Joseph Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan M. Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
Wael Salloum and Nizar Habash. 2011. Dialectal to
Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Pro-
ceedings of the First Workshop on Algorithms and
Resources for Modelling of Dialects and Language
Varieties, pages 10?21, Edinburgh, Scotland.
Wael Salloum and Nizar Habash. 2013. Dialectal
Arabic to English Machine Translation: Pivoting
through Modern Standard Arabic. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT), At-
lanta, GA.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in
the Americas (AMTA), Denver, Colorado.
Khaled Shaalan, Hitham Abo Bakr, and Ibrahim
Ziedan. 2007. Transferring Egyptian Colloquial
into Modern Standard Arabic. In International Con-
ference on Recent Advances in Natural Language
Processing (RANLP), Borovets, Bulgaria.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Ines Zribi, Rahma Boujelbane, Abir Masmoudi,
Mariem Ellouze, Lamia Belguith, and Nizar Habash.
2014. A Conventional Orthography for Tunisian
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
38
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 148?154,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
GWU-HASP: Hybrid Arabic Spelling and Punctuation Corrector1 
  Mohammed Attia, Mohamed Al-Badrashiny, Mona Diab Department of Computer Science The George Washington University {Mohattia;badrashiny;mtdiab}@gwu.edu 
 
    Abstract In this paper, we describe our Hybrid Ar-abic Spelling and Punctuation Corrector (HASP). HASP was one of the systems participating in the QALB-2014 Shared Task on Arabic Error Correction. The system uses a CRF (Conditional Random Fields) classifier for correcting punctua-tion errors, an open-source dictionary (or word list) for detecting errors and gener-ating and filtering candidates, an n-gram language model for selecting the best candidates, and a set of deterministic rules for text normalization (such as re-moving diacritics and kashida and con-verting Hindi numbers into Arabic nu-merals). We also experiment with word alignment for spelling correction at the character level and report some prelimi-nary results. 1 Introduction In this paper1 we describe our system for Arabic spelling error detection and correction, Hybrid Arabic Spelling and Punctuation Corrector (HASP). We participate with HASP in the QALB-2014 Shared Task on Arabic Error Cor-rection (Mohit et al., 2014) as part of the Arabic Natural Language Processing Workshop (ANLP) taking place at EMNLP 2014.      The shared task data deals with ?errors? in the general sense which comprise: a) punctuation errors; b) non-word errors; c) real-word spelling errors; d) grammatical errors; and, e) orthograph-ical errors such as elongation (kashida) and speech effects such as character multiplication                                                 1 This work was supported by the Defense Advanced Research Projects Agency (DARPA) Contract No. HR0011-12-C-0014, BOLT program with subcontract from Raytheon BBN. 
for emphasis. HASP in its current stage only handles types (a), (b), and (e) errors. We assume that the various error types are too distinct to be treated with the same computational technique. Therefore, we treat each problem separately, and for each problem we select the approach that seems most efficient, and ultimately all compo-nents are integrated in a single framework.  1.1 Previous Work Detecting spelling errors in typing is one of the earliest NLP applications, and it has been re-searched extensively over the years, particularly for English (Damerau, 1964; Church and Gale, 1991; Kukich, 1992; Brill and Moore, 2000; Van Delden et al., 2004; Golding, 1995; Golding and Roth, 1996; Fossati and Di Eugenio, 2007; Islam in Inkpen, 2009; Han and Baldwin, 2011; Wu et al., 2013).  The problem of Arabic spelling error correc-tion has been investigated in a number of papers (Haddad and Yaseen, 2007; Alfaifi and Atwell, 2012; Hassan et al., 2008; Shaalan et al., 2012; Attia et al., 2012; Alkanhal et al., 2012).  In our research, we address the spelling error detection and correction problem with a focus on non-word errors. Our work is different from pre-vious work on Arabic in that we cover punctua-tion errors as well. Furthermore, we fine-tune a Language Model (LM) disambiguator by adding probability scores for candidates using forward-backward tracking, which yielded better results than the default Viterbi. We also develop a new and more efficient splitting algorithm for merged words. 1.2 Arabic Morphology, Orthography and Punctuation Arabic has a rich and complex morphology as it applies both concatenative and non-concatenative morphotactics (Ratcliffe, 1998; Beesley, 1998; Habash, 2010), yielding a wealth of morphemes that express various morpho-
148
syntactic features, such as tense, person, number, gender, voice and mood.      Arabic has a large array of orthographic varia-tions, leading to what is called ?typographic er-rors? or ?orthographic variations? (Buckwalter, 2004a), and sometimes referred to as sub-standard spellings, or spelling soft errors. These errors are basically related to the possible over-lap between orthographically similar letters in three categories: a) the various shapes of ham-zahs (?? A2, ?? ,' ? ,{ ?? ,| ?? ,> ?? ,< ?? &); b) taa mar-boutah and haa ?? p, ?? h); and c) yaa and alif maqsoura (?? y, ?? Y).        Ancient Arabic manuscripts were written in scriptura continua, meaning running words without punctuation marks. Punctuation marks were introduced to Arabic mainly through bor-rowing from European languages via translation (Alqinai, 2013). Although punctuation marks in Arabic are gaining popularity and writers are becoming more aware of their importance, yet many writers still do not follow punctuation con-ventions as strictly and consistently as English writers. For example, we investigated contempo-raneous same sized tokenized (simple tokeniza-tion with separation of punctuation) English and Modern Standard Arabic Gigaword edited newswire corpora, we found that 10% of the to-kens in the English Gigaword corresponded to punctuation marks, compared to only 3% of the tokens in the Arabic counterpart.    Train. % Dev. % Word Count 925,643 -- 48,471 -- Total Errors 306,757 33.14 16,659 34.37 Word errors 187,040 60.97 9,878 59.30 Punc. errors 618,886 39.03 6,781 40.70 Split 10,869 3.48 612 3.67 Add_before 99,258 32.36 5,704 34.24 Delete 6,778 2.21 338 2.03 Edit 169,769 55.34 8,914 53.51 Merge 18,267 5.95 994 5.97 Add_after 20 0.01 2 0.01 Move 427 0.14 13 0.08 Table 1. Distribution Statistics on Error Types 1.3 Data Analysis In our work, we use the QALB corpus (Zag-houani et al. 2014), and the training and devel-opment set provided in the QALB shared task (Mohit et. al 2014). The shared task addresses a large array of errors, and not just typical spelling                                                 2 In this paper, we use the Buckwalter Transliteration Scheme as described in www.qamus.com. 
errors. For instance, as Table 1 illustrates punc-tuation errors make up to 40% of all the errors in the shared task. For further investigation, we annotated 1,100 words from the development set for error types, and found that 85% of the word errors (excluding punctuation marks) are typical spelling errors (or non-word errors), while 15% are real-word er-rors, or lexical ambiguities (that is, they are valid words outside of their context), and they range between dialectal words, grammatical errors, semantic errors, speech effects and elongation, examples shown in Table 2.  Error Type Example Correction dialectal words bhAy ??????  ?by this? [Syrian] bh*h ??????  ?by this? [MSA] grammatical errors kbyr ?????  ?big.masc? kbyrp ???????  ?big.fem? semantic  errors |tyh ???????  ?come to him? |typ ??????  ?coming? speech  effects ??????????????   AlrjAAAAl ?men? ????????  AlrjAl ?men? elongation dm__A' ??????  ?blood? dmA' ?????  ?blood? Table 2. Examples of real word errors  2 Our Methodology Due to the complexity and variability of errors in the shared task, we treat each problem individu-ally and use different approaches that prove to be most appropriate for each problem. We specifi-cally address three subtypes of errors: ortho-graphical errors; punctuation errors; and non-word errors. 2.1 Orthographical Errors There are many instances in the shared task?s data that can be treated using simple and straight-forward conversion via regular expression re-place rules. We estimate that these instances cover 10% of the non-punctuation errors in the development set. In HASP we use deterministic heuristic rules to normalize the text, including the following: 1. Hindi numbers (?????????????????) are converted into Arabic numerals [0-9] (occurs 495 in the training data times); 2. Speech effects are removed. For example, ?????????????? AlrjAAAAl ?men? is converted to ???????? AlrjAl. As a general rule letters repeated three times or more are reduced to one letter (715 times); 3. Elongation or kashida is removed. For ex-ample, ???????  dm__A' ?blood? is converted to 
149
????? dmA' (906 times); 4. Special character U+06CC, the Farsi yeh: ?? is converted to U+0649, the visually similar Arabic alif maqsoura ?? Y (293 times). 2.2 Punctuation Errors Punctuation errors constitute 40% of the errors in the QALB Arabic data. It is worth noting that by comparison, punctuation errors only constituted 4% of the English data in CoNLL 2013 Shared Task on English Grammatical Error Correction (Ng et al., 2013) and were not evaluated or han-dled by any participant. In HASP, we focus on 6 punctuation marks: comma, colon, semi-colon, exclamation mark, question mark and period. The ?column? file in the QALB shared task da-ta comes preprocessed with the MADAMIRA morphological analyzer version 04092014-1.0-beta (Pasha et al., 2014). The features that we utilize in our punctuation classification experi-ments are all extracted from the ?column? file, and they are as follows: (1) The original word, that is the word as it ap-pears in the text without any further pro-cessing, (e.g., ????????? llt$Awr ?for consulting?); (2) The tokenized word using the Penn Arabic Treebank (PATB) tokenization (e.g., ?????????? +?? l+Alt$Awr); (3) Kulick POS tag (e.g., IN+DT+NN). (4) Buckwalter POS tag (e.g., PREP+DET+ NOUN+CASE_DEF_GN) as produced by MADAMIRA; (5) Classes to be predicted: colon_after, com-ma_after, exclmark_after, period_after, qmark_after, semicolon_after and NA (when no punctuation marks are used);  Window Size Recall Precision F-measure 4 36.24 54.09 43.40 5 37.95 59.61 46.37 6 36.65 59.99 45.50 7 34.50 59.53 43.68 Table 3. Yamcha results on the development set       For classification, we experiment with Sup-port Vector Machines (SVM) as implemented in Yamcha (Kudo and Matsumoto, 2003) and Con-ditional Random Field (CRF++) classifiers  (Laf-ferty et al. 2001). In our investigation, we vary the context window size from 4 to 8 and we use all 5 features listed for every word in the win-dow. As Tables 3 and 4 show, we found that window size 5 gives the best f-score by both Yamcha and CRF. When we strip clitics from 
tokenized tag, reducing it to stems only, the per-formance of the system improved. Overall CRF yields significantly higher results using the same experimental setup. We assume that the perfor-mance advantage of CRF is a result of the way words in the context and their features are inter-connected in a neat grid in the template file.  # Window Size Recall Precision f-measure 1 4 44.03 74.33 55.31 2 5 44.50 75.49 55.99 3 6 44.22 74.93 55.62 4 7 43.81 75.09 55.34 5 8 43.49 75.41 55.17 6 8* 43.31 75.37 55.00 Table 4. CRF results on the development set * with full tokens; other experiments use stems only, i.e., clitics are removed. 2.3. Non Word Errors This type of errors comprises different subtypes: merges where two or more words are merged together; splits where a space is inserted within a single word; or misspelled words (which under-went substitution, deletion, insertion or transpo-sition) that should be corrected. We handle these problems as follows. 2.3.1. Word Merges Merged words are when the space(s) between two or more words is deleted, such as  ???????????????h*AAlnZAm ?this system?, which should be  ??????????????? h*A AlnZAm. They constitute 3.67% and 3.48% of the error types in the shared task?s de-velopment and training data, respectively. Attia et al. (2012) used an algorithm for dealing with merged words in Arabic, that is, ? ? 3, where l is the length of a word. For a 7-letter word, their algorithm generates 4 candidates as it allows on-ly a single space to be inserted in a string. Their algorithm, however, is too restricted. By contrast Alkanhal et al. (2012) developed an algorithm with more generative power, that is 2???. Their algorithm, however, is in practice too general and leads to a huge fan out. For a 7-letter word, it generates 64 solutions. We develop a splitting algorithm by taking into account that the mini-mum length of words in Arabic is two. Our mod-ified algorithm is 2???, which creates an effec-tive balance between comprehensiveness and compactness. For the 7-letter word, it generates 8 candidates. However, from Table 5 on merged words and their gold splits, one would question 
150
the feasibility of producing more than two splits for any given string. Our splitting algorithm is evaluated in 2.3.3.1.c and compared to Attia et al.?s (2012) algorithm.   Development Training Total Count 631 11,054 1 split 611 10,575 2 splits 15 404 3 splits 3 57 4 splits 1 13 5 splits 1 5 Table 5. Merged words and their splits 2.3.2. Word Splits Beside the problem of merged words, there is also the problem of split words, where one or more spaces are inserted within a word, such as ?? ???? Sm Am ?valve? (correction is ????? SmAm). This error constitutes 6% of the shared task?s both training and development set. We found that the vast majority of instances of this type of error involve the clitic conjunction waw ?and?, which should be represented as a word prefix. Among the 18,267 splits in the training data 15,548 of them involved the waw, corresponding to 85.12%. Similarly among the 994 splits in the development data, 760 of them involved the waw (76.46%).     Therefore, we opted to handle this problem in our work in a partial and shallow manner using deterministic rules addressing specifically the following two phenomena:  1. Separated conjunction morpheme waw ?? w ?and? is attached to the succeeding word (oc-curs 15,915 times in the training data); 2. Literal strings attached to numbers are sepa-rated with space(s). For example, ?????????2000?????? ?dmA'2000$hydF? ?blood of 2000 martyrs? is converted to ????????? 2000 ?????? ?dmA' 2000 $hydF? (824 times). 2.3.3. Misspelled Word Errors This is more akin to the typical spelling correc-tion problem where a word has the wrong letters, rendering it a non-word. We address this prob-lem using two approaches: Dictionary-LM Cor-rection, and Alignment Based Correction.  2.3.3.1. Dictionary-LM Correction Spelling error detection and correction mainly consists of three phases: a) error detection; b) candidate generation; and c) error correction, or best candidate selection.   
a. Error Detection For non-word spelling error detection and candi-date generation we use AraComLex Extended, an open-source reference dictionary (or word list) of full-form words. The dictionary is devel-oped by Attia et al. (2012) through an amalgama-tion of various resources, such as a wordlist from the Arabic Gigaword corpus, wordlist generated from the Buckwalter morphological analyzer, and AraComLex (Attia et al., 2011), a finite-state morphological transducer. AraComLex Extended consists of 9.2M words and, as far as we know, is the largest wordlist for Arabic reported in the literature to date. We enhance the AraComLex Extended dic-tionary by utilizing the annotated data in the shared task?s training data. We add 776 new val-id words to the dictionary and remove 4,810 mis-spelt words, leading to significant improvement in the dictionary?s ability to make decisions on words. Table 6 shows the dictionary?s perfor-mance on the training and development set in the shared task as applied only to non-words and excluding grammatical, semantic and punctua-tion errors.  data set R P F Training 98.84 96.34 97.57 Development 98.72 96.04 97.36 Table 6. Results of dictionary error detection  b. Candidate Generation For candidate generation we use Foma (Hulden, 2009), a finite state compiler that is capable of producing candidates from a wordlist (compiled as an FST network) within a certain edit distance from an error word. Foma allows the ranking of candidates according to customizable transfor-mation rules.   # Error Type Count Ratio % 1.  ?? > typed as ?? A 59,507 31.82 2.  Insert 28.945 15.48 3.  ?? < typed as ?? A 25.392 13.58 4.  Delete 18.246 9.76 5.  ?? p typed as ?? h 14.639 7.83 6.  Split 11.419 6.11 7.  ?? y typed as ?? Y 6.419 3.43 Table 7. Error types in the training set  We develop a re-ranker based on our observa-tion of the error types in the shared task?s train-ing data (as shown in Table 7) and examining the character transformations between the misspelt words and their gold corrections. Our statistics 
151
shows that soft errors (or variants as explained in Section 1.2) account for more than 62% of all errors in the training data.  c. Error Correction For error correction, namely selecting the best solution among the list of candidates, we use an n-gram language model (LM), as implemented in the SRILM package (Stolcke et al., 2011). We use the ?disambig? tool for selecting candidates from a map file where erroneous words are pro-vided with a list of possible corrections. We also use the ?ngram? utility in post-processing for de-ciding on whether a split-word solution has a better probability than a single word solution. Our bigram language model is trained on the Gi-gaword Corpus 4th edition (Parker et al., 2009).     For the LM disambiguation we use the ??fb? option (forward-backward tracking), and we pro-vide candidates with probability scores. We gen-erate these probability scores by converting the edit distance scores produced by the Foma FST re-ranker explained above. Both of the forward-backward tracking and the probability scores in in tandem yield better results than the default values. We evaluate the performance of our sys-tem against the gold standard using the Max-Match (M2) method for evaluating grammatical error correction by Dahlmeier and Ng (2012).     The best f-score achieved in our system is ob-tained when we combine the CRF punctuation classifier (merged with the original punctuations found in data), knowledge-based normalization (norm), dictionary-LM disambiguation and split-1, as shown in Table 8. The option split-1 refers to using the splitting algorithm ? ? 3  as ex-plained in Section 2.3.1, while split-2 refers to using the splitting algorithm 2???.  # Experiment R P F 1 LM+split-1 33.32 73.71 45.89 2 +CRF_punc+split-1 49.74 65.38 56.50 3 + norm+split-1 38.81 69.08 49.70 4 +CRF_punc+norm +split-1 54.79 67.65 60.55 5 +CRF_punc+norm +orig_punc+split-1 53.18 73.15 61.59 6 +CRF_punc+norm +orig_punc+split-2 53.13 73.01 61.50 Table 8. LM correction with 3 candidates       In the QALB Shared Task evaluation, we submit two systems: System 1 is configuration 5 in Table 8, and System 2 corresponds to configu-ration 6, and the results on the test set are shown 
in Table 9. As Table 9 shows, the best scores are obtained by System 1, which is ranked 5th among the 9 systems participating in the shared task.  # Experiment R P F 1 System 1 52.98 75.47 62.25 2 System 2 52.99 75.34 62.22 Table 9. Final official results on the test set pro-vided by the Shared Task 2.3.3.2. Alignment-Based Correction We formatted the data for alignment using a window of 4 words: one word to each side (forming the contextual boundary) and two words in the middle. The two words in the mid-dle are split into characters so that character transformations can be observed and learned by the aligner. The alignment tool we use is Giza++ (Och and Ney, 2003). Results are reported in Ta-ble 10. 	 ? # Experiment  R P F 1 for all error types 36.05 45.13 37.99 2 excluding punc 32.37 54.65 40.66 3 2 + CRF_punc+norm 46.11 62.02 52.90 Table 10. Results of character-based alignment 	 ?Although these preliminary results from Align-ment are significantly below results yielded from the Dictionary-LM approach, we believe that there are several potential improvements that need to be explored:  ? Using LM on the output of the alignment; ? Determining the type of errors that the alignment is most successful at handling: punctuation, grammar, non-words, etc; ? Parsing training data errors with the Diction-ary-LM disambiguation and retraining, so in-stead of training data consisting of errors and gold corrections, it will consist of corrected errors and gold corrections. 3 Conclusion We have described our system HASP for the au-tomatic correction of spelling and punctuation mistakes in Arabic. To our knowledge, this is the first system to handle punctuation errors. We utilize and improve on an open-source full-form dictionary, introduce better algorithm for hand-ing merged word errors, tune the LM parameters, and combine the various components together, leading to cumulative improved results. 
152
References Alfaifi, A., and Atwell, E. (2012) Arabic Learner Corpora (ALC): a taxonomy of coding errors. In Proceedings of the 8th International Computing Conference in Arabic (ICCA 2012), Cairo, Egypt. Alkanhal, Mohamed I., Mohamed A. Al-Badrashiny, Mansour M. Alghamdi, and Abdulaziz O. Al-Qabbany. (2012) Automatic Stochastic Arabic Spelling Correction With Emphasis on Space In-sertions and Deletions. IEEE Transactions on Au-dio, Speech, and Language Processing, Vol. 20, No. 7, September 2012. Alqinai, Jamal. (2013) Mediating punctuation in Eng-lish Arabic translation. Linguistica Atlantica.  Vol. 32. Attia, M., Pecina, P., Tounsi, L., Toral, A., and van Genabith, J. (2011) An Open-Source Finite State Morphological Transducer for Modern Standard Arabic. International Workshop on Finite State Methods and Natural Language Processing (FSMNLP). Blois, France. Attia, Mohammed, Pavel Pecina, Younes Samih, Khaled Shaalan, Josef van Genabith. 2012. Im-proved Spelling Error Detection and Correction for Arabic. COLING 2012, Bumbai, India.  Beesley, Kenneth R. (1998). Arabic Morphology Us-ing Only Finite-State Operations. In The Workshop on Computational Approaches to Semitic lan-guages, Montreal, Quebec, pp. 50?57. Ben Othmane Zribi, C. and Ben Ahmed, M. (2003) Efficient Automatic Correction of Misspelled Ara-bic Words Based on Contextual Information, Lec-ture Notes in Computer Science, Springer, Vol. 2773, pp.770?777. Brill, Eric and Moore, Robert C. (2000) An improved error model for noisy channel spelling correction. Proceedings of the 38th Annual Meeting of the As-sociation for Computational Linguistics, Hong Kong, pp. 286?293. Brown, P. F., Della Pietra, V. J., de Souza, P. V., Lai, J. C. and Mercer, R. L. (1992) Class-Based n-gram Models of Natural Language. Computational Lin-guistics, 18(4), 467?479. Buckwalter, T. (2004b) Buckwalter Arabic Morpho-logical  Analyzer (BAMA) Version 2.0. Linguistic Data Consortium (LDC) catalogue number: LDC2004L02,  ISBN1-58563-324-0. Buckwalter, Tim. (2004a) Issues in Arabic orthogra-phy and morphology analysis. Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages. Pages 31-34. Association for Computational Linguistics Stroudsburg, PA, USA. 
Church, Kenneth W. and William A. Gale. (1991) Probability scoring for spelling correction. Statis-tics and Computing, 1, pp. 93?103. Dahlmeier, Daniel and Ng, Hwee Tou. 2012. Better evaluation for grammatical error correction. In Proceedings of NAACL. Damerau, Fred J. (1964) A Technique for Computer Detection and Correction of Spelling Errors. Communications of the ACM, Volum 7, issue 3, pp. 171?176. Gao, Jianfeng, Xiaolong Li, Daniel Micol, Chris Quirk, and Xu Sun. (2010) A large scale ranker-based system for search query spelling correction. Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 358?366, Beijing, China Golding, Andrew R. A Bayesian Hybrid Method for Context-sensitive Spelling Correction. In Proceed-ings of the Third Workshop on Very Large Corpo-ra. MIT, Cambridge, Massachusetts, USA. 1995, pp.39?53. Golding, Andrew R., and Dan Roth. (1996) Applying Winnow to Context-Sensitive Spelling Correction. In Proceedings of the Thirteenth International Con-ference on Machine Learning, Stroudsburg, PA, USA, pp. 182?190 Habash, Nizar Y. (2010) Introduction to Arabic Natu-ral Language Processing. Synthesis Lectures on Human Language Technologies 3.1: 1-187. Haddad, B., and Yaseen, M. (2007) Detection and Correction of Non-Words in Arabic: A Hybrid Ap-proach. International Journal of Computer Pro-cessing of Oriental Languages. Vol. 20, No. 4. Han, Bo and Timothy Baldwin. (2011) Lexical Nor-malisation of Short Text Messages: Makn Sens a #twitter. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368?378, Portland, Oregon, June 19-24, 2011 Hassan, A, Noeman, S., and Hassan, H. (2008) Lan-guage Independent Text Correction using Finite State Automata. IJCNLP. Hyderabad, India. Hulden, M. (2009) Foma: a Finite-state compiler and library. EACL '09 Proceedings of the 12th Confer-ence of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics Stroudsburg, PA, USA Islam, Aminul, Diana Inkpen. (2009) Real-Word Spelling Correction using Google Web 1T n-gram with Backoff. International Conference on Natural Language Processing and Knowledge Engineering, Dalian, China, pp. 1?8. 
153
Kiraz, G. A. (2001) Computational Nonlinear Mor-phology: With Emphasis on Semitic Languages. Cambridge University Press. Kudo, Taku, Yuji Matsumoto. (2003) Fast Methods for Kernel-Based Text Analysis. 41st Annual Meeting of the Association for Computational Lin-guistics (ACL-2003), Sapporo, Japan. Kukich, Karen. (1992) Techniques for automatically correcting words in text. Computing Surveys, 24(4), pp. 377?439. Lafferty, John, Andrew McCallum, and Fernando Pereira. (2001) Conditional random fields: Proba-bilistic models for segmenting and labeling se-quence data, In Proceedings of the International Conference on Machine Learning (ICML 2001), , MA, USA, pp. 282-289. Levenshtein, V. I. (1966) Binary codes capable of correcting deletions, insertions, and reversals. In: Soviet Physics Doklady, pp. 707-710. Magdy, W., and Darwish, K. (2006) Arabic OCR er-ror correction using character segment correction, language modeling, and shallow morphology. EMNLP '06 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Pro-cessing. Mohit, Behrang, Alla Rozovskaya, Nizar Habash, Wajdi Zaghouani, and Ossama Obeid, 2014.  The First QALB Shared Task on Automatic Text Cor-rection for Arabic. In Proceedings of EMNLP workshop on Arabic Natural Language Processing. Doha, Qatar. Ng, Hwee Tou, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. (2013) The CoNLL-2013 Shared Task on Grammatical Error Correction. Proceedings of the Seventeenth Con-ference on Computational Natural Language Learning: Shared Task, pages 1?12, Sofia, Bulgar-ia, August 8-9 2013. Norvig, P. (2009) Natural language corpus data. In Beautiful Data, edited by Toby Segaran and Jeff Hammerbacher, pp. 219-?-242. Sebastopol, Ca-lif.: O'Reilly. Och, Franz Josef, Hermann Ney. (2003) A Systematic Comparison of Various Statistical Alignment Models. In Computational Linguistics, volume 29, number 1, pp. 19-51 March 2003. Parker, R., Graff, D., Chen, K., Kong, J., and Maeda, K. (2009) Arabic Gigaword Fifth Edition. LDC Catalog No.: LDC2009T30, ISBN: 1-58563-532-4. Parker, R., Graff, D., Chen, K., Kong, J., and Maeda, K. (2011) Arabic Gigaword Fifth Edition. LDC Catalog No.: LDC2011T11, ISBN: 1-58563-595-2. Pasha, Arfath, Mohamed Al-Badrashiny, Ahmed El Kholy, Ramy Eskander, Mona Diab, Nizar Habash, 
Manoj Pooleery, Owen Rambow, Ryan Roth. (2014) Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of Ar-abic. In Proceedings of the 9th International Con-ference on Language Resources and Evaluation, Reykjavik, Iceland. Ratcliffe, Robert R. (1998) The Broken Plural Prob-lem in Arabic and Comparative Semitic: Allo-morphy and Analogy in Non-concatenative Mor-phology. Amsterdam studies in the theory and his-tory of linguistic science. Series IV, Current issues in linguistic theory ; v. 168. Amsterdam ; Philadel-phia: J. Benjamins. Roth, R. Rambow, O., Habash, N., Diab, M., and Rudin, C. (2008) Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking. Proceedings of ACL-08: HLT, Short Papers, pp. 117?120. Shaalan, K., Samih, Y., Attia, M., Pecina, P., and van Genabith, J. (2012) Arabic Word Generation and Modelling for Spell Checking. Language Re-sources and Evaluation (LREC). Istanbul, Turkey. pp. 719?725. Stolcke, A., Zheng, J., Wang, W., and Abrash, V. (2011) SRILM at sixteen: Update and outlook. in Proc. IEEE Automatic Speech Recognition and Understanding Workshop. Waikoloa, Hawaii. van Delden, Sebastian, David B. Bracewell, and Fer-nando Gomez. (2004) Supervised and Unsuper-vised Automatic Spelling Correction Algorithms. In proceeding of Information Reuse and Integration (IRI). Proceedings of the 2004 IEEE International Conference on Web Services, pp. 530?535. Wu, Jian-cheng, Hsun-wen Chiu, and Jason S. Chang. (2013) Integrating Dictionary and Web N-grams for Chinese Spell Checking. Computational Lin-guistics and Chinese Language Processing. Vol. 18, No. 4, December 2013, pp. 17?30. Zaghouani, Wajdi, Behrang Mohit, Nizar Habash, Ossama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014. Large Scale Arabic Error Annotation: Guide-lines and Framework. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC?14), Reykjavik, Iceland.  
154
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 1?12,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Foreign Words and the Automatic Processing
of Arabic Social Media Text Written in Roman Script
Ramy Eskander, Mohamed Al-Badrashiny
?
, Nizar Habash
?
and Owen Rambow
Center for Computational Learning Systems, Columbia University
{reskander,rambow}@ccls.columbia.edu
?
Department of Computer Science, The George Washington University
?
badrashiny@gwu.edu
?
Computer Science Department, New York University Abu Dhabi
?
nizar.habash@nyu.edu
Abstract
Arabic on social media has all the prop-
erties of any language on social media
that make it tough for natural language
processing, plus some specific problems.
These include diglossia, the use of an
alternative alphabet (Roman), and code
switching with foreign languages. In this
paper, we present a system which can
process Arabic written in Roman alpha-
bet (?Arabizi?). It identifies whether each
word is a foreign word or one of an-
other four categories (Arabic, name, punc-
tuation, sound), and transliterates Arabic
words and names into the Arabic alphabet.
We obtain an overall system performance
of 83.8% on an unseen test set.
1 Introduction
Written language used in social media shows dif-
ferences from that in other written genres: the
vocabulary is informal (and sometimes the syn-
tax is as well); there are intentional deviations
from standard orthography (such as repeated let-
ters for emphasis); there are typos; writers use
non-standard abbreviations; non-linguistic sounds
are written (haha); punctuation is used creatively;
non-linguistic signs such as emoticons often com-
pensate for the absence of a broader communica-
tion channel in written communication (which ex-
cludes, for example, prosody or visual feedback);
and, most importantly for this paper, there fre-
quently is code switching. These facts pose a well-
known problem for natural language processing of
social media texts, which has become an area of
interest as applications such as sentiment analy-
sis, information extraction, and machine transla-
tion turn to this genre.
This situation is exacerbated in the case of Ara-
bic social media. There are three principal rea-
sons. First, the Arabic language is a collection of
varieties: Modern Standard Arabic (MSA), which
is used in formal settings, and different forms of
Dialectal Arabic (DA), which are commonly used
informally. This situation is referred to as ?diglos-
sia?. MSA has a standard orthography, while
the dialects do not. What is used in Arabic so-
cial media is typically DA. This means that there
is no standard orthography to begin with, result-
ing in an even broader variation in orthographic
forms found. Diglossia is seen in other linguistic
communities as well, including German-speaking
Switzerland, in the Czech Republic, or to a some-
what lesser extent among French speakers. Sec-
ond, while both MSA and DA are commonly writ-
ten in the Arabic script, DA is sometimes writ-
ten in the Roman script. Arabic written in Roman
is often called ?Arabizi?. It is common in other
linguistic communities as well to write informal
communication in the Roman alphabet rather than
in the native writing system, for example, among
South Asians. And third, educated speakers of
Arabic are often bilingual or near-bilingual speak-
ers of another language as well (such as English
or French), and will code switch between DA and
the foreign language in the same utterance (and
sometimes MSA as well). As is well known, code
switching is common in many linguistic commu-
nities, for example among South Asians.
In this paper, we investigate the issue of pro-
cessing Arabizi input with code switching. There
are two tasks: identification of tokens that are
not DA or MSA (and should not be transliterated
into Arabic script for downstream processing), and
then the transliteration into Arabic script of the
parts identified as DA or MSA. In this paper, we
1
use as a black box an existing component that we
developed to transliterate from Arabizi to Arabic
script (Al-Badrashiny et al., 2014). This paper
concentrates on the task of identifying which to-
kens should be transliterated. A recent release
of annotated data by the Linguistic Data Consor-
tium (LDC, 2014c; Bies et al., 2014) has enabled
novel research on this topic. The corpus pro-
vides each token with a tag, as well as a translit-
eration if appropriate. The tags identify foreign
words, as well as Arabic words, names, punctua-
tion, and sounds. Only Arabic words and names
are transliterated. (Note that code switching is not
distinguished from borrowing.) Emoticons, which
may be isolated or part of an input token, are also
identified, and converted into a conventional sym-
bol (#). This paper presents taggers for the tags,
and an end-to-end system which takes Arabizi in-
put and produces a complex output which consists
of a tag for each input token and a transliteration
of Arabic words and names into the Arabic script.
To our knowledge, this is the first system that han-
dles the complete task as defined by the LDC data.
This paper focuses on the task of identifying for-
eign words (as well as the other tags), on creating
a single system, and on evaluating the system as a
whole.
This paper makes three main contributions.
First, we clearly define the computational prob-
lem of dealing with social media Arabizi, and pro-
pose a new formulation of the evaluation metric
for the LDC corpus. Second, we present novel
modules for the detection of foreign words as well
as of emoticons, sounds, punctuation marks, and
names in Arabizi. Third, we compose a single sys-
tem from the various components, and evaluate the
complete system.
This paper is structured as follows. We start by
presenting related work (Section 2), and then we
present relevant linguistic facts and explain how
the data is annotated (Section 3). After summariz-
ing our system architecture (Section 4) and exper-
imental setup (Section 5), we present our systems
for tagging in Sections 6, 7 and 8. The evaluation
results are presented in Section 9.
2 Related Work
While natural language processing for English in
social media has attracted considerable attention
recently (Clark and Araki, 2011; Gimpel et al.,
2011; Gouws et al., 2011; Ritter et al., 2011; Der-
czynski et al., 2013), there has not been much
work on Arabic yet. We give a brief summary of
relevant work on Arabic.
Darwish et al. (2012) discuss NLP problems in
retrieving Arabic microblogs (tweets). They dis-
cuss many of the same issues we do, notably the
problems arising from the use of DA such as the
lack of a standard orthography. However, they do
not deal with DA written in the Roman alphabet
(though they do discuss non-Arabic characters).
There is some work on code switching be-
tween Modern Standard Arabic (MSA) and di-
alectal Arabic (DA). Zaidan and Callison-Burch
(2011) are interested in this problem at the inter-
sentence level. They crawl a large dataset of
MSA-DA news commentaries. They use Ama-
zon Mechanical Turk to annotate the dataset at the
sentence level. Then they use a language model-
ing approach to predict the class (MSA or DA)
for an unseen sentence. There is other work on
dialect identification, such as AIDA (Elfardy et
al., 2013; Elfardy et al., 2014). In AIDA, some
statistical and morphological analyses are applied
to capture code switching between MSA and DA
within the same sentence. Each word in the sen-
tence is tagged to be either DA or MSA based
on the context. The tagging process mainly de-
pends on the language modeling (LM) approach,
but if a word is unknown in the LM, then its tag is
assigned through MADAMIRA, a morphological
disambiguator Pasha et al. (2014).
Lui et al. (2014) proposed a system that does
language identification in multilingual documents,
using a generative mixture model that is based
on supervised topic modeling algorithms. This is
similar to our work in terms of identifying code
switching. However, our system deals with Ara-
bizi, a non-standard orthography with high vari-
ability, making the identification task much harder.
Concerning specifically NLP for Arabizi, Dar-
wish (2013) (published in an updated version as
(Darwish, 2014)) is similar to our work in that
he identifies English in Arabizi text and he also
transliterates Arabic text from Arabizi to Arabic
script. We compare our transliteration method to
his in Al-Badrashiny et al. (2014). For identifi-
cation of non-Arabic words in Arabizi, Darwish
(2013) uses word and sequence-level features with
CRF modeling; while we use SVMs and decision
trees. Darwish (2013) identifies three tags: Ara-
bic, foreign and others (such as email addresses
and URLs). In contrast, we identify a bigger
set: Arabic, foreign, names, sounds, punctuation
2
and emoticons. Furthermore, Darwish (2013) uses
around 5K words for training his taggers and 3.5K
words for testing; this is considerably smaller than
our training and test sets of 113K and 32K words,
respectively.
Chalabi and Gerges (2012) presented a hybrid
approach for Arabizi transliteration. Their work
does not address the detection of English words,
punctuation, emoticons, and so on. They also do
not handle English when mixed with Arabizi.
Voss et al. (2014) deal with exactly the prob-
lem of classifying tokens in Arabizi as Arabic or
not. More specifically, they deal with Moroccan
Arabic, and with both French and English, mean-
ing they do a three-way classification. There are
many differences between our work and theirs:
they have noisy training data, and they have a
much more balanced test set. They also only deal
with foreignness, and do not address the other tags
we deal with, nor do they actually discuss translit-
eration itself.
3 Linguistic Facts and Data Annotation
3.1 Arabizi
Arabizi refers to Arabic written using the Roman
script (Darwish, 2013; Voss et al., 2014). Ara-
bizi orthography is spontaneous and has no stan-
dard references, although there are numerous com-
monly used conventions making specific usage of
the so-called Arabic numerals and punctuation in
addition to Roman script letters. Arabizi is com-
monly used by Arabic speakers to write mostly in
dialectal Arabic in social media, SMS and chat ap-
plications.
Arabizi orthography decisions mainly depend
on a phoneme-to-grapheme mapping between the
Arabic pronunciation and the Roman script. This
is largely based on the phoneme-to-grapheme
mapping used in English (in Middle Eastern Arab
countries) or French (in Western North African
Arab countries). Since there is no standard or-
thography for Arabizi, it is not a simple translit-
eration of Arabic. For example, in Arabizi, words
omit vowels far less frequently than is done when
writers follow standard Arabic orthography. Fur-
thermore, there are several cases of many-to-many
mappings between Arabic phonemes and Roman
script letters: for example, the letter ?t? is used to
represent the sound of the Arabic letters

H t
1
and
1
Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al., 2007): (in alphabetical
? T (which itself can be also be represented using
the digit ?6?).
Text written in Arabizi also tends to have a large
number of foreign words, that are either borrow-
ings such as telephone, or code switching, such
as love you!. Note that Arabizi often uses the
source language orthography for borrowings (es-
pecially recent borrowings), even if the Arabic
pronunciation is somewhat modified. As a re-
sult, distinguishing borrowings from code switch-
ing is, as is usually the case, hard. And, as in any
language used in social media and chat, Arabizi
may also include abbreviations, such as isa which
means ?<? @ Z A

?
	
?@

?
An ?A? Allh ?God willing? and
lol ?laugh out loud?.
The rows marked with Arabizi in Figure 1
demonstrate some of the salient features of Ara-
bizi. The constructed example in the figure is of
an SMS conversation in Egyptian Arabic.
3.2 Data Annotation
The data set we use in this paper was created by
the Linguistic Data Consortium (Bies et al., 2014;
LDC, 2014a; LDC, 2014b; LDC, 2014c). We
summarize below the annotation decisions. The
system we present in this paper aims at predicting
exactly this annotation automatically. The input
text is initially segregated into Arabic script and
Arabizi. Arabic script text is not modified in any
way. Arabizi text undergoes two sets of annotation
decisions: Arabizi word tagging and Arabizi-to-
Arabic transliteration. All of the Arabizi annota-
tions are initially done using an automatic process
(Al-Badrashiny et al., 2014) and then followed by
manual correction and validation.
Arabizi Word Tagging Each Arabizi word re-
ceives one of the following five tags:
? Foreign All words from languages other than
Arabic are tagged as Foreign if they would
be kept in the same orthographic form when
translated into their source language (which
in our corpus is almost always English).
Thus, non-Arabic words that include Arabic
affixes are not tagged as Foreign. The defini-
tion of ?foreign? thus means that uninflected
borrowings spelled as in the source language
orthography are tagged as ?foreign?, while
borrowings that are spelled differently, as
well as borrowing that have been inflected
order) Abt?jHxd?rzs?SDT
?
D??fqklmnhwy and the additional
symbols: ? Z, ?

@,
?
A @

,
?
A

@, ?w

?', ?y Z?', h?

?, ? ?.
3
(1) Arabizi Youmna i need to know anti gaya wala la2 ?
Tag Name Foreign Foreign Foreign Foreign Arabic Arabic Arabic Arabic Punct
Arabic ?
	
??
?

?


@ YJ


	
K ?

K ?
	
K ?



?
	
K @

?K


Ag
.
B?

B ?
ymn? Ay nyd tw nw Anty jAyh? wlA l? ?
English Youmna I need to know you coming or not ?
(2) Arabizi Mmmm ok ana 7aseb el sho3?l now w ageelk isa :-)
Tag Sound Foreign Arabic Arabic Arabic Arabic Foreign Arabic Arabic Arabic Arabic
Arabic ??
?
?J


??@ A
	
K @ I
.



?Ag [+]?@ ?
	
?

? ?A
	
K [+]? ??[-]?


k
.
@ ?<? @[-]ZA

?[-]
	
?@ #
mmm Awkyh AnA HAsyb Al[+] ?gl nAw w[+] Ajy[-]lk An[-]?A?[-]Allh #
English mmm OK I will-leave the work now and I-come-to-you God-willing :-)
(3) Arabizi qishta!:D
Tag Arabic
Arabic #[-]!

??

?

?
q?Th?![-]#
English cream!:D (slang for cool!)
Figure 1: A short constructed SMS conversation written in Arabizi together with annotation of word
tags and transliteration into Arabic script. A Romanized transliteration of the Arabic script and English
glosses are provided for clarity. The cells with gray background are the parts of the output that we
evaluate.
following Arabic morphology, are not tagged
as ?foreign? (even if the stem is spelled as in
the source language, such as Almobile). The
Arabic transliterations of these words are not
manually corrected.
? Punct Punctuation marks are a set of conven-
tional signs that are used to aid interpretation
by indicating division of text into sentences
and clauses, etc. Examples of punctuation
marks are the semicolon ;, the exclamation
mark ! and the right brace }. Emoticons are
not considered punctuation and are handled
as part of the transliteration task discussed
below.
? Sound Sounds are a list of interjections that
have no grammatical meaning, but mimic
non-linguistic sounds that humans make, and
that often signify emotions. Examples of
sounds are hahaha (laughing), hmm (wonder-
ing) and eww (being disgusted). It is common
to stretch sounds out to make them stronger,
i.e., to express more intense emotions. For
example, hmm could be stretched out into
hmmmmm to express a stronger feeling of
wondering. The Arabic transliterations of
these words are not manually corrected.
? Name Proper names are tagged as such and
later manually corrected.
? Arabic All other words are tagged as Arabic
and are later manually corrected.
See the rows marked with Tag in Figure 1 for
examples of these different tags. It is impor-
tant to point out that the annotation of this data
was intended to serve a project focusing on ma-
chine translation from dialectal Arabic into En-
glish. This goal influenced some of the annotation
decisions and was part of the reason for this selec-
tion of word tags.
Arabizi-to-Arabic Transliteration The second
annotation task is about converting Arabizi to an
Arabic-script-based orthography. Since, dialectal
Arabic including Egyptian Arabic has no standard
orthography in Arabic script, the annotation uses a
conventionalized orthography for Dialectal Arabic
called CODA (Habash et al., 2012a; Eskander et
al., 2013; Zribi et al., 2014). Every word has a
single orthographic representation in CODA.
In the corpus we use, only words tagged as
Arabic or Name are manually checked and cor-
rected. The transliteration respects the white-
space boundaries of the original Arabizi words. In
cases where an Arabizi word represents a prefix
or suffix that should be joined in CODA to the
next or previous word, a [+] symbol is added to
mark this decision. Similarly, for Arabizi words
that should be split into multiple CODA words,
the CODA words are written with added [-] sym-
bol delimiting the word boundaries.
The Arabic transliteration task also includes
handling emoticons. Emoticons are digital icons
or sequences of keyboard symbols serving to rep-
resent facial expressions or to convey the writer?s
emotions. Examples of emoticons are :d, :-(, O.O
and ? used to represent laughing, sadness, being
surprised and positive emotion, respectively. All
emoticons, whether free-standing or attached to a
4
word, are replaced by a single hash symbol (#).
Free-standing emoticons are tagged as Arabic. At-
tached emoticons are not tagged separately; the
word they are attached to is tagged according to
the usual rules. See Figure 1 for examples of these
different decisions.
Since words tagged as Foreign, Punct, or Sound
are not manually transliterated in the corpus, in
our performance evaluation we combine the de-
cisions of tags and transliteration. For foreign
words, punctuation and sounds, we only consider
the tags for accuracy computations; in contrast, for
names and Arabic words, we consider both the tag
and transliteration.
4 System Architecture
Figure 2 represent the overall architecture of our
system. We distinguish below between existing
components that we use and novel extensions that
we contribute in this paper.
4.1 Existing Arabization System
For the core component of Arabizi-to-Arabic
transliteration, we use a previously published sys-
tem (Al-Badrashiny et al., 2014), which converts
Arabizi into Arabic text following CODA conven-
tions (see Section 3). The existing system uses a
finite state transducer trained on 8,500 Arabizi-to-
Arabic transliteration pairs at the character level to
obtain a large number of possible transliterations
for the input Arabizi words. The generated list is
then filtered using a dialectal Arabic morphologi-
cal analyzer. Finally, the best choice for each input
word is selected using a language model. We use
this component as a black box except that we re-
train it using additional training data. In Figure 2,
this component is represented using a central black
box.
4.2 Novel Extension
In this paper, we add Word Type Tagging as a
new set of modules. We tag the Arabizi words into
five categories as discussed above: Arabic, For-
eign, Names, Sounds, and Punctuation. Figure 2
illustrates the full proposed system. First, we pro-
cess the Arabizi input to do punctuation and sound
tagging, along with emoticon detection. Then we
run the transliteration system to produce the cor-
responding Arabic transliteration. The Arabizi in-
put and Arabic output are then used together to
do name tagging and foreign word tagging. The
Arabic tag is assigned to all untagged words, i.e.,
words not tagged as Foreign, Names, Sounds, or
Punctuation. The outputs from all steps are then
combined to produce the final Arabic translitera-
tion along with the tag.
5 Experimental Setup
5.1 Data Sets
We define the following sets of data:
? Train-S: A small size dataset that is used to
train all taggers in all experiments to deter-
mine the best performing setup (feature engi-
neering).
? Train-L: A larger size dataset that is used to
train the best performing setup.
? Dev: The development set that is used to
measure the system performance in all exper-
iments
? Test: A blind set that is used to test the best
system (LDC, 2014a).
The training and development sets are extracted
from (LDC, 2014b). Table 1 represents the tags
distribution in each dataset. Almost one of every
five words is not Arabic text and around one of
every 10 words is foreign.
5.2 Arabizi-to-Arabic Transliteration
Accuracy
For the Arabizi-to-Arabic transliteration system,
we report on using the two training data sets
with two modifications. First, we include the
8,500 word pairs from Al-Badrashiny et al. (2014),
namely 2,200 Arabizi-to-Arabic script pairs from
the training data used by Darwish (2013) (man-
ually revised to be CODA-compliant) and about
6,300 pairs of proper names in Arabic and En-
glish from the Buckwalter Arabic Morphologi-
cal Analyzer (Buckwalter, 2004). (Since these
pairs are not tagged, we do not use them to train
the taggers.) Second, we exclude all the foreign
tagged words from training the transliteration sys-
tem since they were not manually corrected.
Table 2 shows the overall transliteration accu-
racy of Arabic words and names only, using dif-
ferent training data sets and evaluating on Dev
(as determined by the gold standard). The ac-
curacy when using the original Arabizi-to-Arabic
transliteration system from Al-Badrashiny et al.
(2014) gives an accuracy of 68.6%. Retraining it
on Train-S improves the accuracy to 76.9%. The
accuracy goes up further to 79.5% when using the
5
Arabizi-to-Arabic 
Transliteration Arabizi 
Punctuation, 
Sound & 
Emoticon 
Detection 
Name Tagger 
Foreign Word 
Tagger 
Combiner 
Arabic  
& 
Tagged 
Arabizi 
Figure 2: The architecture of our complete Arabizi processing system. The "Punctuation, Sound and
Emoticon Detection" component does labeling that is read by the "Name" and "Foreign Word" taggers,
While the actual Aribizi-to-Arabic transliteration system is used as a black box.
Data # Words Arabic Foreign Name Sound Punct Emoticon
Train-S 21,950 80.5% 12.1% 2.8% 1.7% 1.3% 1.6%
Train-L 113,490 82.3% 9.8% 2.4% 1.8% 1.1% 2.6%
Dev 5,061 76.3% 16.2% 2.9% 1.8% 1.2% 1.5%
Test 31,717 86.1% 6.0% 2.7% 1.6% 0.9% 2.8%
Table 1: Dataset Statistics
Data Translit. Acc.
Al-Badrashiny et al. (2014) 68.6%
Train-S 76.9%
Train-L 79.5%
Table 2: Transliteration accuracy of Arabic words
and names when using different training sets and
evaluating on Dev
bigger training set Train-L. The overall transliter-
ation accuracy of Arabic words and names on Test
using the bigger training set Train-L is 83.6%.
6 Tagging Punctuation, Emoticons and
Sounds
6.1 Approach
We start the tagging process by detecting three
types of closed classes: punctuation, sounds and
emoticons. Simple regular expressions perform
very well at detecting their occurrence in text. The
regular expressions are applied to the Arabizi in-
put, word by word, after lower-casing, since both
emoticons and sounds could contain either small
or capital letters.
Since emoticons can be composed of just con-
catenated punctuation marks, their detection is re-
quired before punctuation is tagged. Once de-
tected, emoticons are replaced by #. Then punctu-
ation marks are detected. If a non-emoticon word
is only composed of punctuation marks, then it
gets tagged as Punct. Sounds are targeted next.
A word gets tagged as Sound if it matches the
sound detection expression, after stripping out any
attached punctuation marks and/or emoticons.
6.2 Results
Table 6 in Section 9 shows the accuracy, recall,
precision and F-score for the classification of the
Punct and Sound tags and detection of emoticons.
Since emoticons can be part of another word, and
in that case do not receive a specific tag (as spec-
ified in the annotation guidelines by the LDC),
emoticon evaluation is concerned with the num-
ber of detected emoticons within an Arabizi word,
as opposed to a binary tagging decision. In other
words, emoticon identification is counted as cor-
rect (?positive?) if the number of detected emoti-
cons in a word is correct in the test token. The
Punct and Sound tags represent standard binary
classification tasks and are evaluated in the usual
way.
7 Tagging Names
7.1 Approach
We consider the following set of binary features
for learning a model of name tagging. The fea-
tures are used either separately or combined using
a modeling classifier implemented with decision
trees.
? Capitalization A word is considered a name
if the first letter in Arabizi is capitalized.
6
? MADAMIRA MADAMIRA is a system for
morphological analysis and disambiguation
of Arabic (Pasha et al., 2014). We run
MADAMIRA on the Arabic output after run-
ning the Arabizi-to-Arabic transliteration. If
the selected part-of-speech (POS) of a word
is proper noun (NOUN_PROP), then the
word is tagged as Name.
? CALIMA CALIMA is a morphological an-
alyzer for Egyptian Arabic (Habash et al.,
2012b). If the Arabic transliteration of a
given Arabizi word has a possible proper
noun analysis in CALIMA, then the word is
tagged as Name.
? Maximum Likelihood Estimate (MLE) An
Arabizi word gets assigned the Name tag if
Name is the most associated tag for that word
in the training set.
? Tharwa Tharwa is a large scale Egyptian
Arabic-MSA-English lexicon that includes
POS tag information (Diab et al., 2014). If
an Arabizi word appears in Tharwa as an En-
glish gloss with a proper noun POS, then it is
tagged as Name.
? Name Language Model We use a list of
280K unique lower-cased English words as-
sociated with their probability of appearing
capitalized (Habash, 2009). When using this
feature, any probability that is not equal to
one is rounded to zero.
All the features above are modeled after case-
lowering the Arabizi input, and removing speech
effects. Any attached punctuation marks and/or
emoticons are stripped out. One exception is the
capitalization feature, where the case of the first
letter of the Arabizi word is preserved. The tech-
niques above are then combined together using de-
cision trees. In this approach, the words tagged as
Name are given a weight that balances their infre-
quent occurrence in the data.
7.2 Results
Table 3 shows the performance of the Name tag-
ging on Dev using Train-S. The best results are
obtained when looking up the MLE value in the
training data, with an accuracy and F-score of
97.8% and 56.0%, respectively. When using
Train-L, the accuracy and F-score given by MLE
go up to 98.1% and 63.9%, respectively. See Ta-
ble 6. The performance of the combined approach
Feature Accuracy Recall Precision F-Score
Capitalization 85.6 28.3 6.4 10.4
MADAMIRA 95.9 24.8 28.3 26.5
CALIMA 86.3 50.3 10.9 17.9
MLE 97.8 46.9 69.4 56.0
THARWA 96.3 22.8 33.0 26.9
NAME-LM 84.5 30.3 6.3 10.4
All Combined 97.7 49.7 63.2 55.6
(Decision Trees)
Table 3: Name tagging results on Dev with Train-S
does not outperform the most effective single clas-
sifier, MLE. This is because adding other features
decreases the precision by an amount that exceeds
the increase in the recall.
8 Tagging Foreign Words
As shown earlier, around 10% of all words in Ara-
bizi text are foreign, mostly English in our data set.
Tagging foreign words is challenging since there
are many words that can be either Arabic (in Ara-
bizi) or a word in a foreign languages. For exam-
ple the Arabizi word mesh can refer to the English
reading or the Arabic word

?? m? ?not?. There-
fore, simple dictionary lookup is not sufficient to
determine whether a word is Arabic or Foreign.
Our target in this section is to identify the foreign
words in the input Arabizi text .
8.1 Baseline Experiments
We define a foreignness index formula that gives
each word a score given its unigram probabili-
ties against Arabic and English language models
(LMs).
? (w) = ?P
E
(w) + (1? ?) (1? P
A
(w
t
)) (1)
?(w) is the foreignness score of the Arabizi word
w. P
E
(w) is the unigram probability of w in the
English LM, and P
A
(w
t
) is the unigram proba-
bility in the Arabic LM of the transliteration into
Arabic (w
t
) proposed by our system for the Ara-
bizi word w. ? is a tuning parameter varying from
zero to one. From equation 1 we define the mini-
mum and maximum ? values as follows:
?
min
= ?P
E
min
+ (1? ?) (1? P
A
max
)
?
max
= ?P
E
max
+ (1? ?) (1? P
A
min
)
(2)
Where P
E
min
and P
E
max
are the minimum and
maximum uni-gram probabilities in the English
LM. And P
A
min
and P
A
max
are the minimum
7
and maximum uni-gram probabilities in the Ara-
bic LM. The foreignness index Foreignness(w)
is the normalized foreignness score derived using
equations 1 and 2 as follow:
Foreignness (w) =
? (w)? ?
min
?
max
? ?
min
(3)
If the foreignness index of a word is higher than
a certain threshold ?, we consider the word For-
eign. We define three baseline experiments as fol-
lows:
? FW-index-manual: Use brute force search
to find the best ? and ? that maximize the
foreign words tagging on Dev.
? FW-index-SVM: Use the best ? from above
and train an SVM model using the foreign-
ness index as sole feature. Then use this
model to classify each word in Dev.
? LM-lookup: The word is said to be Foreign
if it exists in the English LM and does not
exist in the Arabic LM.
8.2 Machine Learning Experiments
We conducted a suite of experiments by train-
ing different machine learning techniques using
WEKA (Hall et al., 2009) on the following groups
of features. We performed a two-stage feature ex-
ploration, where we did an exhaustive search over
all features in each group in the first phase, and
then exhaustively searched over all retained fea-
ture groups. In addition, we also performed an ex-
haustive search over all features in the first three
groups.
? Word n-gram features: Run the input Ara-
bizi word through an English LM and the cor-
responding Arabic transliteration through an
Arabic LM to get the set of features that are
defined in "Group1" in Table 4. Then find the
best combination of features that maximizes
the F-score on Dev.
? FW-char-n-gram features: Run the input
Arabizi word through a character-level n-
gram LM of the Arabizi words that are tagged
as foreign in the training data. We get the set
of features that are defined in "Group2" in Ta-
ble 4. Then find the best feature combination
from this group that maximizes the F-score
on Dev.
? AR-char-n-gram features: Run the input
Arabizi word through a character-level n-
gram LM of the Arabizi words that are tagged
Group Description
Group1 Uni and bi-grams probabilities from English and
Arabic LMs
Group2 1,2,3,4, and 5 characters level n-grams of foreign
words
Group3 1,2,3,4, and 5 characters level n-grams of Arabic
words
Use the Arabizi word itself as a feature
Group4 Was the input Arabizi word tagged as foreign in
the gold training data?
Was the input Arabizi word tagged as Arabic in the
gold training data?
Does the input word has speech effects?
Group5 Word length
Is the Arabizi word capitalized?
Table 4: List of the different features that are used
in the foreign word tagging
as non-foreign in the training data. We get the
set of features that are defined in "Group3" in
Table 4. Then find the best feature that maxi-
mizes the F-score on Dev.
? Word identity: Use the input Arabizi word
to get all features that are defined in "Group4"
in Table 4. Then find the best combination of
features that maximizes the F-score on Dev.
? Word properties: Use the input Arabizi
word to get all features that are defined in
"Group5" in Table 4. Then find the best com-
bination of features that maximizes the F-
score on Dev.
? Best-of-all-groups: Use the best selected set
of features from each of the above experi-
ments. Then find the best combination of
these features that maximizes the F-score on
Dev.
? All-features: Use all features from all
groups.
? Probabilistic-features-only: Find the best
combination of features from "Group1",
"Group2", and "Group3" in Table 4 that max-
imizes the F-score on Dev.
8.3 Results
Table 5 shows the results on Dev using Train-S.
It can be seen that the decision tree classifier is
doing better than the SVM except in the "Word
properties" and "All-features" experiments. The
best performing setup is "Probabilistic-features-
only" with decision trees which has 87.3% F-
score. The best selected features are EN-Unigram,
AR-char-2-grams, FW-char-1-grams, FW-char-2-
grams, FW-char-5-grams.
8
Experiment Recall Precision F-Score Classifier Selected Features
LM-lookup 7.6 95.4 14.1
FW-index-manual 75.0 51.0 60.7 ? =0.8 , ? = 0.23
FW-index-SVM 4.0 89.0 7.7 SVM
Word n-gram features 76.7 73.2 74.9 AR-unigram, EN-unigram
AR-char-n-gram features 55.4 34.8 42.8 AR-char-4-grams
FW-char-n-gram features 42.4 52.2 46.8 FW-char-3-grams
Word properties 2.4 28.6 4.5 Has-speech-effect, Word-length, Is-capitalized
Word identity 70.3 63.0 66.4 SVM FW-tagged-list
Best-of-all-groups 82.1 76.1 79.0 AR-unigram, EN-unigram, Word-length
All-features 69.4 87.7 77.5 All features from all groups
Probabilistic-features-only 84.5 80.6 82.5 AR-unigram, EN-unigram, AR-char-3-grams, FW-
char-3-grams
Word n-gram features 82.8 80.5 81.6 AR-unigram, EN-unigram
AR-char-n-gram features 80.6 63.2 70.8 AR-char-5-grams
FW-char-n-gram features 73.8 76.3 75.0 FW-char-3-grams
Word properties 1.9 25.4 3.6 Has-speech-effect, Word-length
Word identity 73.2 60.9 66.5 Decision-Tree FW-tagged-list
Best-of-all-groups 87.0 81.5 84.1 AR-unigram, EN-unigram, AR-char-5-grams, FW-
char-3-grams
All-features 92.0 53.4 67.6 All features from all groups
Probabilistic-features-
only
89.9 84.9 87.3 EN-Unigram, AR-char-2-grams, FW-char-1-
grams, FW-char-2-grams, FW-char-5-grams
Table 5: Foreign words tagging results on Dev in terms of F-score (%).
9 System Evaluation
9.1 Development and Blind Test Results
We report the results on Dev using Train-L and
with the best settings determined in the previous
three sections. Table 6 summarizes the recall, pre-
cision and F-score results for the classification of
the Punct, Sound, Foreign, Name and Arabic tags,
in addition to emoticon detection.
We report our results on Test, our blind test
set, using Train-L and with the best settings de-
termined in the previous three sections in Table 7.
The punctuation, sounds and emoticons have
high F-scores but lower than expected. This is
likely due to the limitations of the regular expres-
sions used. The performance on these tags drops
further on the test set. A similar drop is seen for
the Foreign tag. Name is the hardest tag overall.
But it performs slightly better in test compared to
the development set, and so does the Arabic tag.
Tag Accuracy Recall Precision F-Score
Punct 99.8 100.0 88.7 94.0
Sound 99.4 93.5 78.9 85.6
Foreign 95.8 91.6 84.0 87.6
Name 98.1 57.5 71.8 63.9
Arabic 94.5 95.6 97.3 96.4
Emoticon 100.0 97.5 98.7 98.1
Detection
Table 6: Tagging results on Dev using Train-L
Tag Accuracy Recall Precision F-Score
Punct 99.8 98.2 80.1 88.3
Sound 99.3 87.4 74.2 80.3
Foreign 96.5 92.3 64.3 75.8
Name 98.6 53.7 90.2 67.3
Arabic 95.4 96.3 98.5 97.4
Emoticon 99.2 85.3 93.6 89.3
Detection
Table 7: Tagging results on Test using Train-L
9.2 Overall System Evaluation
In this subsection we report on evaluating the over-
all system accuracy. This includes the correct tag-
ging and Arabizi to Arabic transliteration. How-
ever, since there is no manually annotated gold
transliteration for foreign words, punctuation, or
sounds into Arabic, we cannot compare the system
transliteration of foreign words to the gold translit-
eration. Thus, we define the following metric to
judge the overall system accuracy.
Overall System Accuracy Metric A word is
said to be correctly transliterated according to the
following rules:
1. If the gold tag is anything other than Arabic
and Name, the produced tag must match the
gold tag.
2. If the gold tag is either Arabic or Name, the
produced tag and the produced transliteration
must both match the gold.
9
Data Baseline Accuracy System Accuracy
Dev 65.7% 82.5%
Test 76.8% 83.8%
Table 8: Baseline vs. System Accuracy
Tag
Gold Errors System Errors
Typos
Not Over Not Over
Tagged generated Tagged generated
Punct 100.0 0.0 0.0 0.0 0.0
Sound 79.3 10.3 10.3 0.0 0.0
Foreign 47.2 1.9 12.3 20.3 18.4
Name 26.3 13.7 45.3 8.4 6.3
Table 9: Error Analysis of tag classification errors
As a baseline, we use the most frequent tag,
which is Arabic in our case, along with the translit-
eration of the word using our black box system.
Then we apply the above evaluation metric on both
Dev and Test. The results are shown in table 8. The
baseline accuracies on Dev and Test are 65.7% and
76.8% respectively. By considering the actual out-
put of our system, the accuracy on the Dev and Test
data increases to 82.5% and 83.8% respectively.
9.3 Error Analysis
We conducted an error analysis for tag classifica-
tion on the development set. The analysis is done
for the tags that we built models for, which are
Punct, Sound, Foreign and Name.
2
Table 9 shows
the different error types for classifying the tags.
Tagging errors could be either gold errors or sys-
tem errors. These errors could be either due to
tag over-generation or because the correct tag is
not detected. Additionally, there are typos in the
input Arabizi that sometimes prevent the system
from assigning the correct tags. Gold errors con-
tribute to a large portion of the tagging errors, rep-
resenting 100.0%, 89.6%, 49.1% and 40.0% for
the Punct, Sound, Foreign and Name tags, respec-
tively.
10 Conclusion and Future Work
We presented a system for automatic processing of
Arabic social media text written in Roman script,
or Arabizi. Our system not only transliterates the
Arabizi text in the Egyptian Arabic dialect but also
classifies input Arabizi tokens as sounds, punc-
tuation marks, names, foreign words, or Arabic
words, and detects emoticons. We define a new
2
As mentioned in Section 4, the Arabic tag is assigned to
any remaining untagged words after running the classification
models.
task-specific metric for evaluating the complete
system. Our best setting achieves an overall per-
formance accuracy of 83.8% on a blind test set.
In the future, we plan to extend our work to
other Arabic dialects and other language contexts
such as Judeo-Arabic (Arabic written in Hebrew
script with code switching between Arabic and
Hebrew). We plan to explore the use of this com-
ponent in the context of specific applications such
as machine translation from Arabizi Arabic to En-
glish, and sentiment analysis in social media. We
also plan to make the system public so it can be
used by other people working on Arabic NLP tasks
related to Arabizi.
Acknowledgement
This paper is based upon work supported by
DARPA Contract No. HR0011-12-C-0014. Any
opinions, findings and conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of
DARPA. Nizar Habash performed most of his con-
tribution to this paper while he was at the Center
for Computational Learning Systems at Columbia
University.
References
Mohamed Al-Badrashiny, Ramy Eskander, Nizar
Habash, and Owen Rambow. 2014. Automatic
Transliteration of Romanized Dialectal Arabic. In
Proceedings of the Eighteenth Conference on Com-
putational Natural Language Learning, pages 30?
38, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Ann Bies, Zhiyi Song, Mohamed Maamouri, Stephen
Grimes, Haejoong Lee, Jonathan Wright, Stephanie
Strassel, Nizar Habash, Ramy Eskander, and Owen
Rabmow. 2014. Transliteration of Arabizi into Ara-
bic Orthography: Developing a Parallel Annotated
Arabizi-Arabic Script SMS/Chat Corpus. In Arabic
Natural Language Processing Workshop, EMNLP,
Doha, Qatar.
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Achraf Chalabi and Hany Gerges. 2012. Romanized
Arabic Transliteration. In Proceedings of the Sec-
ond Workshop on Advances in Text Input Methods
(WTIM 2012).
Eleanor Clark and Kenji Araki. 2011. Text normal-
ization in social media: Progress, problems and ap-
plications for a pre-processing system of casual en-
glish. Procedia - Social and Behavioral Sciences,
27(0):2 ? 11. Computational Linguistics and Re-
lated Fields.
10
Kareem Darwish, Walid Magdy, and Ahmed Mourad.
2012. Language Processing for Arabic Microblog
Retrieval. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management, CIKM ?12, pages 2427?2430, New
York, NY, USA. ACM.
Kareem Darwish. 2013. Arabizi Detection and Con-
version to Arabic. CoRR.
Kareem Darwish. 2014. Arabizi Detection and Con-
version to Arabic . In Arabic Natural Language Pro-
cessing Workshop, EMNLP, Doha, Qatar.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In
Proceedings of the International Conference Recent
Advances in Natural Language Processing RANLP
2013, pages 198?206, Hissar, Bulgaria, September.
INCOMA Ltd. Shoumen, BULGARIA.
Mona Diab, Mohamed Al-Badrashiny, Maryam
Aminian, Mohammed Attia, Pradeep Dasigi, Heba
Elfardy, Ramy Eskander, Nizar Habash, Abdelati
Hawwari, and Wael Salloum. 2014. Tharwa: A
Large Scale Dialectal Arabic - Standard Arabic - En-
glish Lexicon. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Reyk-
javik, Iceland.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code Switch Point Detection in Arabic.
In Proceedings of the 18th International Conference
on Application of Natural Language to Information
Systems (NLDB2013), MediaCity, UK, June.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2014. AIDA: Identifying Code Switching
in Informal Arabic Text. In Workshop on Compu-
tational Approaches to Linguistic Code Switching,
EMNLP, Doha, Qatar, October.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing Spontaneous Or-
thography. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), Atlanta, GA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2,
HLT ?11, pages 42?47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Stephan Gouws, Donald Metzler, Congxing Cai, and
Eduard Hovy. 2011. Contextual bearing on lin-
guistic variation in social media. In Proceedings of
the Workshop on Languages in Social Media, LSM
?11, pages 20?29, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalu-
ation Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In Proceedings of the Twelfth Meeting of the
Special Interest Group on Computational Morphol-
ogy and Phonology, pages 1?9, Montr?al, Canada.
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine trans-
lation. In Khalid Choukri and Bente Maegaard, ed-
itors, Proceedings of the Second International Con-
ference on Arabic Language Resources and Tools.
The MEDAR Consortium, April.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
LDC. 2014a. BOLT Phase 2 SMS and Chat Ara-
bic DevTest Data ? Source Annotation, Translit-
eration and Translation. LDC catalog number
LDC2014E28.
LDC. 2014b. BOLT Phase 2 SMS and Chat Ara-
bic Training Data ? Source Annotation, Translit-
eration and Translation R1. LDC catalog number
LDC2014E48.
LDC. 2014c. BOLT Program: Romanized Arabic
(Arabizi) to Arabic Transliteration and Normaliza-
tion Guidelines. Version 3. Linguistic Data Consor-
tium.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. In Proceedings of LREC.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan M. Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Clare Voss, Stephen Tratz, Jamal Laoudi, and Dou-
glas Briesch. 2014. Finding Romanized Arabic
Dialect in Code-Mixed Tweets. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Hrafn Loftsson, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios
11
Piperidis, editors, Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC?14), Reykjavik, Iceland, may. Euro-
pean Language Resources Association (ELRA).
Omar F Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of ACL, pages 37?41.
Ines Zribi, Rahma Boujelbane, Abir Masmoudi,
Mariem Ellouze, Lamia Belguith, and Nizar Habash.
2014. A Conventional Orthography for Tunisian
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
12
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 94?101,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
AIDA: Identifying Code Switching in Informal Arabic Text
Heba Elfardy
Department of Computer Science
Columbia University
New York, NY
heba@cs.columbia.edu
Mohamed Al-Badrashiny, Mona Diab
Department of Computer Science
The George Washington University
Washington, DC
{badrashiny, mtdiab}@gwu.edu
Abstract
In this paper, we present the latest version
of our system for identifying linguistic
code switching in Arabic text. The system
relies on Language Models and a tool for
morphological analysis and disambigua-
tion for Arabic to identify the class of each
word in a given sentence. We evaluate
the performance of our system on the test
datasets of the shared task at the EMNLP
workshop on Computational Approaches
to Code Switching (Solorio et al., 2014).
The system yields an average token-level
F
?=1
score of 93.6%, 77.7% and 80.1%,
on the first, second, and surprise-genre
test-sets, respectively, and a tweet-level
F
?=1
score of 4.4%, 36% and 27.7%, on
the same test-sets.
1 Introduction
Most languages exist in some standard form while
also being associated with informal regional vari-
eties. Some languages exist in a state of diglos-
sia (Ferguson, 1959). Arabic is one of those
languages comprising a standard form known as
Modern Standard Arabic (MSA), that is used in
education, formal settings, and official scripts; and
dialectal variants (DA) corresponding to the na-
tive tongue of Arabic speakers. While these vari-
ants have no standard orthography, they are com-
monly used and have become pervasive across
web-forums, blogs, social networks, TV shows,
and normal daily conversations. Arabic dialects
may be divided into five main groups: Egyptian
(including Libyan and Sudanese), Levantine (in-
cluding Lebanese, Syrian, Palestinian and Jorda-
nian), Gulf, Iraqi and Moroccan. Sub-dialectal
variants also exist within each dialect (Habash,
2010). Speakers of a specific Arabic Dialect
typically code switch between their dialect and
MSA, and less frequently between different di-
alects, both inter and intra-sententially. The iden-
tification and classification of these dialects in
diglossic text can enhance semantic predictability.
In this paper we modify an existing system
AIDA (Elfardy and Diab, 2012b), (Elfardy et al.,
2013) that identifies code switching between MSA
and Egyptian DA (EDA). We apply the modified
system to the datasets used for evaluating systems
participating at the EMNLP Workshop on Com-
putational Approaches to Linguistic Code Switch-
ing.
1
2 Related Work
Dialect Identification in Arabic is crucial for al-
most all NLP tasks, and has recently gained in-
terest among Arabic NLP researchers. One of
the early works is that of (Biadsy et al., 2009)
where the authors present a system that identifies
dialectal words in speech through acoustic signals.
Zaidan and Callison-Burch (2011) crawled a large
dataset of MSA-DA news commentaries and an-
notated part of the dataset for sentence-level di-
alectalness employing Amazon Mechanical Turk.
Cotterell and Callison-Burch (2014) extended the
previous work by handling more dialects. In (Cot-
terell et al., 2014), the same authors collect and
annotate on Amazon Mechanical Turk a large set
of tweets and user commentaries pertaining to five
Arabic dialects. Bouamor et al. (2014) select a set
of 2,000 Egyptian Arabic sentences and have them
translated into four other Arabic dialects to present
the first multidialectal Arabic parallel corpus.
Eskander et al. (2014) present a system for han-
dling Arabic written in Roman script ?Arabizi?.
Using decision trees; the system identifies whether
each word in the given text is a foreign word or
not and further divides non foreign words into four
1
Another group in our lab was responsible for the organi-
zation of the task, hence we did not officially participate in
the task.
94
classes: Arabic, Named Entity, punctuation, and
sound.
In the context of machine-translation, Salloum
and Habash (2011) tackle the problem of DA to
English Machine Translation (MT) by pivoting
through MSA. The authors present a system that
uses DA to MSA transfer rules before applying
state of the art MSA to English MT system to
produce an English translation. In (Elfardy and
Diab, 2012a), we present a set of guidelines for
token-level identification of DA while in (Elfardy
and Diab, 2012b), (Elfardy et al., 2013) we tackle
the problem of token-level dialect-identification
by casting it as a code-switching problem. El-
fardy and Diab (2013) presents our solution for the
sentence-level dialect identification problem.
3 Shared Task Description
The shared task for ?Language Identification
in Code-Switched Data? (Solorio et al., 2014)
aims at allowing participants to perform word-
level language identification in code-switched
Spanish-English, MSA-DA, Chinese-English and
Nepalese-English data. In this work, we only fo-
cus on MSA-DA data. The dataset has six tags:
1. lang1: corresponds to an MSA word, ex.
	
??@Q? @, AlrAhn
2
meaning ?the current?;
2. lang2: corresponds to a DA word, ex. ?K


	
P@,
ezyk meaning ?how are you?;
3. mixed: corresponds to a word with mixed
morphology, ex.
	
??

?? ?

A ?
?
@ , Alm>lw$wn
meaning ?the ones that were excluded or re-
jected?;
4. other: corresponds to punctuation, numbers
and words having punctuation or numbers at-
tached to them;
5. ambig: corresponds to a word where the
class cannot be determined given the current
context, could either be lang1 or lang2; ex.
the phrase ?A?

?
???, klh tmAm meaning ?all is
well? is ambiguous if enough context is not
present since it can be used in both MSA and
EDA.
6. NE: corresponds to a named-entity, ex. Q???,
mSr meaning ?Egypt?.
2
We use Buckwalter transliteration scheme
http://www.qamus.org/transliteration.htm
4 Approach
We use a variant of the system that was pre-
sented in (Elfardy et al., 2013) to identify the
tag of each word in a given Arabic sentence.
The original approach relies on language mod-
els and a morphological analyzer to assign tags
to words in an input sentence. In this new vari-
ant, we use MADAMIRA (Pasha et al., 2014);
a tool for morphological analysis and disam-
biguation for Arabic. The advantage of using
MADAMIRA over using a morphological ana-
lyzer is that MADAMIRA performs contextual
disambiguation of the analyses produced by the
morphological analyzer, hence reducing the pos-
sible options for analyses per word. Figures 1 il-
lustrates the pipeline of the proposed system.
4.1 Preprocessing
We experiment with two preprocessing tech-
niques:
1. Basic: In this scheme, we only perform a ba-
sic clean-up of the text by separating punc-
tuation and numbers from words, normal-
izing word-lengthening effects, and replac-
ing all punctuation, URLs, numbers and non-
Arabic words with PUNC, URL, NUM, and
LAT keywords, respectively
2. Tokenized: In this scheme, in addition to
basic preprocessing, we use MADAMIRA
toolkit to tokenize clitics and affixes by ap-
plying the D3-tokenization scheme (Habash
and Sadat, 2006). For example, the word Ym
.
