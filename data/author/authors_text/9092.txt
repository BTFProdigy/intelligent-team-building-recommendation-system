Proceedings of the 43rd Annual Meeting of the ACL, pages 491?498,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Simple Algorithms for Complex Relation Extraction
with Applications to Biomedical IE
Ryan McDonald1 Fernando Pereira1 Seth Kulick2
1CIS and 2IRCS, University of Pennsylvania, Philadelphia, PA
{ryantm,pereira}@cis.upenn.edu, skulick@linc.cis.upenn.edu
Scott Winters Yang Jin Pete White
Division of Oncology, Children?s Hospital of Pennsylvania, Philadelphia, PA
{winters,jin,white}@genome.chop.edu
Abstract
A complex relation is any n-ary relation
in which some of the arguments may be
be unspecified. We present here a simple
two-stage method for extracting complex
relations between named entities in text.
The first stage creates a graph from pairs
of entities that are likely to be related, and
the second stage scores maximal cliques
in that graph as potential complex relation
instances. We evaluate the new method
against a standard baseline for extracting
genomic variation relations from biomed-
ical text.
1 Introduction
Most research on text information extraction (IE)
has focused on accurate tagging of named entities.
Successful early named-entity taggers were based
on finite-state generative models (Bikel et al, 1999).
More recently, discriminatively-trained models have
been shown to be more accurate than generative
models (McCallum et al, 2000; Lafferty et al, 2001;
Kudo and Matsumoto, 2001). Both kinds of mod-
els have been developed for tagging entities such
as people, places and organizations in news mate-
rial. However, the rapid development of bioinfor-
matics has recently generated interest on the extrac-
tion of biological entities such as genes (Collier et
al., 2000) and genomic variations (McDonald et al,
2004b) from biomedical literature.
The next logical step for IE is to begin to develop
methods for extracting meaningful relations involv-
ing named entities. Such relations would be ex-
tremely useful in applications like question answer-
ing, automatic database generation, and intelligent
document searching and indexing. Though not as
well studied as entity extraction, relation extraction
has still seen a significant amount of work. We dis-
cuss some previous approaches at greater length in
Section 2.
Most relation extraction systems focus on the spe-
cific problem of extracting binary relations, such
as the employee of relation or protein-protein in-
teraction relation. Very little work has been done
in recognizing and extracting more complex rela-
tions. We define a complex relation as any n-ary
relation among n typed entities. The relation is
defined by the schema (t1, . . . , tn) where ti ? T
are entity types. An instance (or tuple) in the rela-
tion is a list of entities (e1, . . . , en) such that either
type(ei) = ti, or ei =? indicating that the ith ele-
ment of the tuple is missing.
For example, assume that the entity types
are T = {person, job, company} and we are
interested in the ternary relation with schema
(person, job, company) that relates a person
to their job at a particular company. For
the sentence ?John Smith is the CEO at Inc.
Corp.?, the system would ideally extract the tu-
ple (John Smith, CEO, Inc. Corp.). However, for
the sentence ?Everyday John Smith goes to his
office at Inc. Corp.?, the system would extract
(John Smith,?, Inc. Corp.), since there is no men-
tion of a job title. Hence, the goal of complex re-
lation extraction is to identify all instances of the
relation of interest in some piece of text, including
491
incomplete instances.
We present here several simple methods for ex-
tracting complex relations. All the methods start by
recognized pairs of entity mentions, that is, binary
relation instances, that appear to be arguments of the
relation of interest. Those pairs can be seen as the
edges of a graph with entity mentions as nodes. The
algorithms then try to reconstruct complex relations
by making tuples from selected maximal cliques in
the graph. The methods are general and can be ap-
plied to any complex relation fitting the above def-
inition. We also assume throughout the paper that
the entities and their type are known a priori in the
text. This is a fair assumption given the current high
standard of state-of-the-art named-entity extractors.
A primary advantage of factoring complex rela-
tions into binary relations is that it allows the use of
standard classification algorithms to decide whether
particular pairs of entity mentions are related. In ad-
dition, the factoring makes training data less sparse
and reduces the computational cost of extraction.
We will discuss these benefits further in Section 4.
We evaluated the methods on a large set of anno-
tated biomedical documents to extract relations re-
lated to genomic variations, demonstrating a consid-
erable improvement over a reasonable baseline.
2 Previous work
A representative approach to relation extraction is
the system of Zelenko et al (2003), which attempts
to identify binary relations in news text. In that
system, each pair of entity mentions of the correct
types in a sentence is classified as to whether it is
a positive instance of the relation. Consider the bi-
nary relation employee of and the sentence ?John
Smith, not Jane Smith, works at IBM?. The pair
(John Smith, IBM) is a positive instance, while the
pair (Jane Smith, IBM) is a negative instance. In-
stances are represented by a pair of entities and their
position in a shallow parse tree for the containing
sentence. Classification is done by a support-vector
classifier with a specialized kernel for that shallow
parse representation.
This approach ? enumerating all possible en-
tity pairs and classifying each as positive or nega-
tive ? is the standard method in relation extraction.
The main differences among systems are the choice
of trainable classifier and the representation for in-
stances.
For binary relations, this approach is quite
tractable: if the relation schema is (t1, t2), the num-
ber of potential instances is O(|t1| |t2|), where |t| is
the number of entity mentions of type t in the text
under consideration.
One interesting system that does not belong to
the above class is that of Miller et al (2000), who
take the view that relation extraction is just a form
of probabilistic parsing where parse trees are aug-
mented to identify all relations. Once this augmen-
tation is made, any standard parser can be trained
and then run on new sentences to extract new re-
lations. Miller et al show such an approach can
yield good results. However, it can be argued that
this method will encounter problems when consid-
ering anything but binary relations. Complex re-
lations would require a large amount of tree aug-
mentation and most likely result in extremely sparse
probability estimates. Furthermore, by integrating
relation extraction with parsing, the system cannot
consider long-range dependencies due to the local
parsing constraints of current probabilistic parsers.
The higher the arity of a relation, the more likely
it is that entities will be spread out within a piece
of text, making long range dependencies especially
important.
Roth and Yih (2004) present a model in which en-
tity types and relations are classified jointly using a
set of global constraints over locally trained classi-
fiers. This joint classification is shown to improve
accuracy of both the entities and relations returned
by the system. However, the system is based on con-
straints for binary relations only.
Recently, there has also been many results from
the biomedical IE community. Rosario and Hearst
(2004) compare both generative and discriminative
models for extracting seven relationships between
treatments and diseases. Though their models are
very flexible, they assume at most one relation per
sentence, ruling out cases where entities participate
in multiple relations, which is a common occurrence
in our data. McDonald et al (2004a) use a rule-
based parser combined with a rule-based relation
identifier to extract generic binary relations between
biological entities. As in predicate-argument extrac-
tion (Gildea and Jurafsky, 2002), each relation is
492
always associated with a verb in the sentence that
specifies the relation type. Though this system is
very general, it is limited by the fact that the design
ignores relations not expressed by a verb, as the em-
ployee of relation in?John Smith, CEO of Inc. Corp.,
announced he will resign?.
Most relation extraction systems work primarily
on a sentential level and never consider relations that
cross sentences or paragraphs. Since current data
sets typically only annotate intra-sentence relations,
this has not yet proven to be a problem.
3 Definitions
3.1 Complex Relations
Recall that a complex n-ary relation is specified by
a schema (t1, . . . , tn) where ti ? T are entity types.
Instances of the relation are tuples (e1, . . . , en)
where either type(ei) = ti, or ei =? (missing ar-
gument). The only restriction this definition places
on a relation is that the arity must be known. As we
discuss it further in Section 6, this is not required by
our methods but is assumed here for simplicity. We
also assume that the system works on a single rela-
tion type at a time, although the methods described
here are easily generalizable to systems that can ex-
tract many relations at once.
3.2 Graphs and Cliques
An undirected graph G = (V,E) is specified by a
set of vertices V and a set of edges E, with each
edge an unordered pair (u, v) of vertices. G? =
(V ?, E?) is a subgraph of G if V ? ? V and E? =
{(u, v) : u, v ? V ?, (u, v) ? E}. A clique C of G is
a subgraph of G in which there is an edge between
every pair of vertices. A maximal clique of G is a
clique C = (VC , EC) such that there is no other
clique C ? = (VC? , EC?) such that VC ? VC? .
4 Methods
We describe now a simple method for extracting
complex relations. This method works by first fac-
toring all complex relations into a set of binary re-
lations. A classifier is then trained in the standard
manner to recognize all pairs of related entities. Fi-
nally a graph is constructed from the output of this
classifier and the complex relations are determined
from the cliques of this graph.
a. All possible
relation instances
(John, CEO, Inc. Corp.)
(John,?, Inc. Corp.)
(John, CEO, Biz. Corp.)
(John,?, Biz. Corp.)
(John, CEO,?)
(Jane, CEO, Inc. Corp.)
(Jane,?, Inc. Corp.)
(Jane, CEO, Biz. Corp.)
(Jane,?, Biz. Corp.)
(Jane, CEO,?)
(?, CEO, Inc. Corp.)
(?, CEO, Biz. Corp.)
b. All possible
binary relations
(John, CEO)
(John, Inc. Corp.)
(John, Biz. Corp.)
(CEO, Inc. Corp.)
(CEO, Biz. Corp.)
(Jane, CEO)
(Jane, Inc. Corp.)
(Jane, Biz. Corp.)
Figure 1: Relation factorization of the sentence:
John and Jane are CEOs at Inc. Corp. and Biz.
Corp. respectively.
4.1 Classifying Binary Relations
Consider again the motivating example of the
(person, job, company) relation and the sentence
?John and Jane are CEOs at Inc. Corp. and Biz.
Corp. respectively?. This sentence contains two
people, one job title and two companies.
One possible method for extracting the rela-
tion of interest would be to first consider all 12
possible tuples shown in Figure 1a. Using all
these tuples, it should then be possible to train
a classifier to distinguish valid instances such as
(John, CEO, Inc. Corp.) from invalid ones such as
(Jane, CEO, Inc. Corp.). This is analogous to the
approach taken by Zelenko et al (2003) for binary
relations.
There are problems with this approach. Computa-
tionally, for an n-ary relation, the number of possi-
ble instances is O(|t1| |t2| ? ? ? |tn|). Conservatively,
letting m be the smallest |ti|, the run time is O(mn),
exponential in the arity of the relation. The second
problem is how to manage incomplete but correct
instances such as (John,?, Inc. Corp.) when train-
ing the classifier. If this instance is marked as neg-
ative, then the model might incorrectly disfavor fea-
tures that correlate John to Inc. Corp.. However,
if this instance is labeled positive, then the model
may tend to prefer the shorter and more compact in-
complete relations since they will be abundant in the
positive training examples. We could always ignore
instances of this form, but then the data would be
heavily skewed towards negative instances.
493
Instead of trying to classify all possible relation
instances, in this work we first classify pairs of en-
tities as being related or not. Then, as discussed in
Section 4.2, we reconstruct the larger complex rela-
tions from a set of binary relation instances.
Factoring relations into a set of binary decisions
has several advantages. The set of possible pairs is
much smaller then the set of all possible complex
relation instances. This can be seen in Figure 1b,
which only considers pairs that are consistent with
the relation definition. More generally, the num-
ber of pairs to classify is O((?i |ti|)2) , which is
far better than the exponentially many full relation
instances. There is also no ambiguity when label-
ing pairs as positive or negative when constructing
the training data. Finally, we can rely on previous
work on classification for binary relation extraction
to identify pairs of related entities.
To train a classifier to identify pairs of related
entities, we must first create the set of all positive
and negative pairs in the data. The positive in-
stances are all pairs that occur together in a valid
tuple. For the example sentence in Figure 1, these
include the pairs (John, CEO), (John, Inc. Corp.),
(CEO, Inc. Corp.), (CEO, Biz. Corp.), (Jane, CEO)
and (Jane, Biz. Corp.). To gather negative in-
stances, we extract all pairs that never occur to-
gether in a valid relation. From the same exam-
ple these would be the pairs (John, Biz. Corp.) and
(Jane, Inc. Corp.).
This leads to a large set of positive and negative
binary relation instances. At this point we could em-
ploy any binary relation classifier and learn to iden-
tify new instances of related pairs of entities. We
use a standard maximum entropy classifier (Berger
et al, 1996) implemented as part of MALLET (Mc-
Callum, 2002). The model is trained using the fea-
tures listed in Table 1.
This is a very simple binary classification model.
No deep syntactic structure such as parse trees is
used. All features are basically over the words sepa-
rating two entities and their part-of-speech tags. Of
course, it would be possible to use more syntactic
information if available in a manner similar to that
of Zelenko et al (2003). However, the primary pur-
pose of our experiments was not to create a better
binary relation extractor, but to see if complex re-
lations could be extracted through binary factoriza-
Feature Set
entity type of e1 and e2
words in e1 and e2
word bigrams in e1 and e2
POS of e1 and e2
words between e1 and e2
word bigrams between e1 and e2
POS between e1 and e2
distance between e1 and e2
concatenations of above features
Table 1: Feature set for maximum entropy binary
relation classifier. e1 and e2 are entities.
a. Relation graph G
John Jane
CEO
Inc. Corp. Biz. Corp.
b. Tuples from G
(John, CEO,?)
(John,?, Inc. Corp.)
(John,?, Biz. Corp.)
(Jane, CEO,?)
(?, CEO, Inc. Corp.)
(?, CEO, Biz. Corp.)
(John, CEO, Inc. Corp.)
(John, CEO, Biz. Corp.)
Figure 2: Example of a relation graph and tuples
from all the cliques in the graph.
tion followed by reconstruction. In Section 5.2 we
present an empirical evaluation of the binary relation
classifier.
4.2 Reconstructing Complex Relations
4.2.1 Maximal Cliques
Having identified all pairs of related entities in the
text, the next stage is to reconstruct the complex re-
lations from these pairs. Let G = (V,E) be an undi-
rected graph where the vertices V are entity men-
tions in the text and the edges E represent binary
relations between entities. We reconstruct the com-
plex relation instances by finding maximal cliques
in the graphs.
The simplest approach is to create the graph
so that two entities in the graph have an edge
if the binary classifier believes they are related.
For example, consider the binary factoriza-
tion in Figure 1 and imagine the classifier
identified the following pairs as being related:
(John, CEO), (John, Inc. Corp.), (John, Biz. Corp.),
(CEO, Inc. Corp.), (CEO, Biz. Corp.) and
(Jane, CEO). The resulting graph can be seen
in Figure 2a.
Looking at this graph, one solution to construct-
494
ing complex relations would be to consider all the
cliques in the graph that are consistent with the def-
inition of the relation. This is equivalent to having
the system return only relations in which the binary
classifier believes that all of the entities involved are
pairwise related. All the cliques in the example are
shown in Figure 2b. We add ? fields to the tuples to
be consistent with the relation definition.
This could lead to a set of overlapping
cliques, for instance (John, CEO, Inc. Corp.) and
(John, CEO,?). Instead of having the system re-
turn all cliques, our system just returns the maximal
cliques, that is, those cliques that are not subsets of
other cliques. Hence, for the example under con-
sideration in Figure 2, the system would return the
one correct relation, (John, CEO, Inc. Corp.), and
two incorrect relations, (John, CEO, Biz. Corp.) and
(Jane, CEO,?). The second is incorrect since it
does not specify the company slot of the relation
even though that information is present in the text.
It is possible to find degenerate sentences in which
perfect binary classification followed by maximal
clique reconstruction will lead to errors. One such
sentence is, ?John is C.E.O. and C.F.O. of Inc. Corp.
and Biz. Corp. respectively and Jane vice-versa?.
However, we expect such sentences to be rare; in
fact, they never occur in our data.
The real problem with this approach is that an ar-
bitrary graph can have exponentially many cliques,
negating any efficiency advantage over enumerating
all n-tuples of entities. Fortunately, there are algo-
rithms for finding all maximal cliques that are effi-
cient in practice. We use the algorithm of Bron and
Kerbosch (1973). This is a well known branch and
bound algorithm that has been shown to empirically
run linearly in the number of maximal cliques in the
graph. In our experiments, this algorithm found all
maximal cliques in a matter of seconds.
4.2.2 Probabilistic Cliques
The above approach has a major shortcom-
ing in that it assumes the output of the bi-
nary classifier to be absolutely correct. For
instance, the classifier may have thought with
probability 0.49, 0.99 and 0.99 that the fol-
lowing pairs were related: (Jane, Biz. Corp.),
(CEO, Biz. Corp.) and (Jane, CEO) respectively.
The maximal clique method would not produce the
tuple (Jane, CEO, Biz. Corp.) since it never consid-
ers the edge between Jane and Biz. Corp. However,
given the probability of the edges, we would almost
certainly want this tuple returned.
What we would really like to model is a belief
that on average a clique represents a valid relation
instance. To do this we use the complete graph
G = (V,E) with edges between all pairs of entity
mentions. We then assign weight w(e) to edge e
equal to the probability that the two entities in e are
related, according to the classifier. We define the
weight of a clique w(C) as the mean weight of the
edges in the clique. Since edge weights represent
probabilities (or ratios), we use the geometric mean
w(C) =
?
?
?
e?EC
w(e)
?
?
1/|EC |
We decide that a clique C represents a valid tuple if
w(C) ? 0.5. Hence, the system finds all maximal
cliques as before, but considers only those where
w(C) ? 0.5, and it may select a non-maximal clique
if the weight of all larger cliques falls below the
threshold. The cutoff of 0.5 is not arbitrary, since it
ensures that the average probability of a clique rep-
resenting a relation instance is at least as large as
the average probability of it not representing a rela-
tion instance. We ran experiments with varying lev-
els of this threshold and found that, roughly, lower
thresholds result in higher precision at the expense
of recall since the system returns fewer but larger
tuples. Optimum results were obtained for a cutoff
of approximately 0.4, but we report results only for
w(C) ? 0.5.
The major problem with this approach is that
there will always be exponentially many cliques
since the graph is fully connected. However, in our
experiments we pruned all edges that would force
any containing clique C to have w(C) < 0.5. This
typically made the graphs very sparse.
Another problem with this approach is the as-
sumption that the binary relation classifier outputs
probabilities. For maximum entropy and other prob-
abilistic frameworks this is not an issue. However,
many classifiers, such as SVMs, output scores or
distances. It is possible to transform the scores from
those models through a sigmoid to yield probabili-
495
ties, but there is no guarantee that those probability
values will be well calibrated.
5 Experiments
5.1 Problem Description and Data
We test these methods on the task of extracting ge-
nomic variation events from biomedical text (Mc-
Donald et al, 2004b). Briefly, we define a varia-
tion event as an acquired genomic aberration: a spe-
cific, one-time alteration at the genomic level and
described at the nucleic acid level, amino acid level
or both. Each variation event is identified by the re-
lationship between a type of variation, its location,
and the corresponding state change from an initial-
state to an altered-state. This can be formalized as
the following complex schema
(var-type, location, initial-state, altered-state)
A simple example is the sentence
?At codons 12 and 61, the occurrence of
point mutations from G/A to T/G were observed?
which gives rise to the tuples
(point mutation, codon 12, G, T)
(point mutation, codon 61, A, G)
Our data set consists of 447 abstracts selected
from MEDLINE as being relevant to populating a
database with facts of the form: gene X with vari-
ation event Y is associated with malignancy Z. Ab-
stracts were randomly chosen from a larger corpus
identified as containing variation mentions pertain-
ing to cancer.
The current data consists of 4691 sentences that
have been annotated with 4773 entities and 1218 re-
lations. Of the 1218 relations, 760 have two ? ar-
guments, 283 have one ? argument, and 175 have
no ? arguments. Thus, 38% of the relations tagged
in this data cannot be handled using binary relation
classification alone. In addition, 4% of the relations
annotated in this data are non-sentential. Our sys-
tem currently only produces sentential relations and
is therefore bounded by a maximum recall of 96%.
Finally, we use gold standard entities in our exper-
iments. This way we can evaluate the performance
of the relation extraction system isolated from any
kind of pipelined entity extraction errors. Entities in
this domain can be found with fairly high accuracy
(McDonald et al, 2004b).
It is important to note that just the presence of two
entity types does not entail a relation between them.
In fact, 56% of entity pairs are not related, due either
to explicit disqualification in the text (e.g. ?... the
lack of G to T transversion ...?) or ambiguities that
arise from multiple entities of the same type.
5.2 Results
Because the data contains only 1218 examples of re-
lations we performed 10-fold cross-validation tests
for all results. We compared three systems:
? MC: Uses the maximum entropy binary classi-
fier coupled with the maximal clique complex
relation reconstructor.
? PC: Same as above, except it uses the proba-
bilistic clique complex relation reconstructor.
? NE: A maximum entropy classifier that naively
enumerates all possible relation instances as
described in Section 4.1.
In training system NE, all incomplete but correct
instances were marked as positive since we found
this had the best performance. We used the same
pairwise entity features in the binary classifier of
the above two systems. However, we also added
higher order versions of the pairwise features. For
this system we only take maximal relations,that is,
if (John, CEO, Inc. Corp.) and (John,?, Inc. Corp.)
are both labeled positive, the system would only re-
turn the former.
Table 2 contains the results of the maximum en-
tropy binary relation classifier (used in systems MC
and PC). The 1218 annotated complex relations pro-
duced 2577 unique binary pairs of related entities.
We can see that the maximum entropy classifier per-
forms reasonably well, although performance may
be affected by the lack of rich syntactic features,
which have been shown to help performance (Miller
et al, 2000; Zelenko et al, 2003).
Table 3 compares the three systems on the real
problem of extracting complex relations. An ex-
tracted complex relation is considered correct if and
only if all the entities in the relation are correct.
There is no partial credit. All training and clique
finding algorithms took under 5 minutes for the en-
tire data set. Naive enumeration took approximately
26 minutes to train.
496
ACT PRD COR
2577 2722 2101
Prec Rec F-Meas
0.7719 0.8153 0.7930
Table 2: Binary relation classification results for the
maximum entropy classifier. ACT: actual number of
related pairs, PRD: predicted number of related pairs
and COR: correctly identified related pairs.
System Prec Rec F-Meas
NE 0.4588 0.6995 0.5541
MC 0.5812 0.7315 0.6480
PC 0.6303 0.7726 0.6942
Table 3: Full relation classification results. For a
relation to be classified correctly, all the entities in
the relation must be correctly identified.
First we observe that the maximal clique method
combined with maximum entropy (system MC) re-
duces the relative error rate over naively enumer-
ating and classifying all instances (system NE) by
21%. This result is very positive. The system based
on binary factorization not only is more efficient
then naively enumerating all instances, but signifi-
cantly outperforms it as well. The main reason naive
enumeration does so poorly is that all correct but
incomplete instances are marked as positive. Thus,
even slight correlations between partially correct en-
tities would be enough to classify an instance as cor-
rect, which results in relatively good recall but poor
precision. We tried training only with correct and
complete positive instances, but the result was a sys-
tem that only returned few relations since negative
instances overwhelmed the training set. With fur-
ther tuning, it may be possible to improve the per-
formance of this system. However, we use it only as
a baseline and to demonstrate that binary factoriza-
tion is a feasible and accurate method for extracting
complex relations.
Furthermore, we see that using probabilistic
cliques (system PC) provides another large im-
provement, a relative error reduction of 13%
over using maximal cliques and 31% reduction
over enumeration. Table 4 shows the breakdown
of relations returned by type. There are three
types of relations, 2-ary, 3-ary and 4-ary, each
with 2, 1 and 0 ? arguments respectively, e.g.
System 2-ary 3-ary 4-ary
NE 760:1097:600 283:619:192 175:141:60
MC 760:1025:601 283:412:206 175:95:84
PC 760:870:590 283:429:223 175:194:128
Table 4: Breakdown of true positive relations by
type that were returned by each system. Each cell
contains three numbers, Actual:Predicted:Correct,
which represents for each arity the actual, predicted
and correct number of relations for each system.
(point mutation, codon 12,?,?) is a 2-ary relation.
Clearly the probabilistic clique method is much
more likely to find larger non-binary relations, veri-
fying the motivation that there are some low proba-
bility edges that can still contribute to larger cliques.
6 Conclusions and Future Work
We presented a method for complex relation extrac-
tion, the core of which was to factorize complex re-
lations into sets of binary relations, learn to identify
binary relations and then reconstruct the complex re-
lations by finding maximal cliques in graphs that
represent relations between pairs of entities. The
primary advantage of this method is that it allows
for the use of almost any binary relation classifier,
which have been well studied and are often accu-
rate. We showed that such a method can be suc-
cessful with an empirical evaluation on a large set
of biomedical data annotated with genomic varia-
tion relations. In fact, this approach is both signifi-
cantly quicker and more accurate then enumerating
and classifying all possible instances. We believe
this work provides a good starting point for contin-
ued research in this area.
A distinction may be made between the factored
system presented here and one that attempts to clas-
sify complex relations without factorization. This
is related to the distinction between methods that
learn local classifiers that are combined with global
constraints after training and methods that incorpo-
rate the global constraints into the learning process.
McCallum and Wellner (2003) showed that learning
binary co-reference relations globally improves per-
formance over learning relations in isolation. How-
ever, their model relied on the transitive property in-
herent in the co-reference relation. Our system can
be seen as an instance of a local learner. Punyakanok
497
et al (2004) argued that local learning actually out-
performs global learning in cases when local deci-
sions can easily be learnt by the classifier. Hence, it
is reasonable to assume that our binary factorization
method will perform well when binary relations can
be learnt with high accuracy.
As for future work, there are many things that we
plan to look at. The binary relation classifier we em-
ploy is quite simplistic and most likely can be im-
proved by using features over a deeper representa-
tion of the data such as parse trees. Other more pow-
erful binary classifiers should be tried such as those
based on tree kernels (Zelenko et al, 2003). We also
plan on running these algorithms on more data sets
to test if the algorithms empirically generalize to dif-
ferent domains.
Perhaps the most interesting open problem is how
to learn the complex reconstruction phase. One pos-
sibility is recent work on supervised clustering. Let-
ting the edge probabilities in the graphs represent a
distance in some space, it may be possible to learn
how to cluster vertices into relational groups. How-
ever, since a vertex/entity can participate in one or
more relation, any clustering algorithm would be re-
quired to produce non-disjoint clusters.
We mentioned earlier that the only restriction of
our complex relation definition is that the arity of
the relation must be known in advance. It turns out
that the algorithms we described can actually handle
dynamic arity relations. All that is required is to
remove the constraint that maximal cliques must be
consistent with the structure of the relation. This
represents another advantage of binary factorization
over enumeration, since it would be infeasible to
enumerate all possible instances for dynamic arity
relations.
Acknowledgments
The authors would like to thank Mark Liberman,
Mark Mandel and Eric Pancoast for useful discus-
sion, suggestions and technical support. This work
was supported in part by NSF grant ITR 0205448.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1).
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning Journal Special Issue on Natural Language
Learning, 34(1/3):221?231.
C. Bron and J. Kerbosch. 1973. Algorithm 457: finding
all cliques of an undirected graph. Communications of
the ACM, 16(9):575?577.
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting
the names of genes and gene products with a hidden
Markov model. In Proc. COLING.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proc. NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on In-
formation Integration on the Web.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. ICML.
A. K. McCallum. 2002. MALLET: A machine learning
for language toolkit.
D.M. McDonald, H. Chen, H. Su, and B.B. Marshall.
2004a. Extracting gene pathway relations using a hy-
brid grammar: the Arizona Relation Parser. Bioinfor-
matics, 20(18):3370?78.
R.T. McDonald, R.S. Winters, M. Mandel, Y. Jin, P.S.
White, and F. Pereira. 2004b. An entity tagger for
recognizing acquired genomic variations in cancer lit-
erature. Bioinformatics, 20(17):3249?3251.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proc. NAACL.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Learning via inference over structurally constrained
output. In Workshop on Learning Structured with Out-
put, NIPS.
Barbara Rosario and Marti A. Hearst. 2004. Classifying
semantic relations in bioscience texts. In ACL.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proc. CoNLL.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. JMLR.
498
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 41?48,
New York City, June 2006. c?2006 Association for Computational Linguistics
Human Gene Name Normalization using Text Matching with Automatically
Extracted Synonym Dictionaries
Haw-ren Fang
Department of Computer Science, University of Maryland
College Park, MD 20742, USA
hrfang@cs.umd.edu
Kevin Murphy and Yang Jin and Jessica S. Kim and Peter S. White?
Division of Oncology, Children?s Hospital of Philadelphia
Philadelphia, PA 19104, USA
{murphy,jin,kim,white}@genome.chop.edu
Abstract
The identification of genes in biomedi-
cal text typically consists of two stages:
identifying gene mentions and normaliza-
tion of gene names. We have created an
automated process that takes the output
of named entity recognition (NER) sys-
tems designed to identify genes and nor-
malizes them to standard referents. The
system identifies human gene synonyms
from online databases to generate an ex-
tensive synonym lexicon. The lexicon is
then compared to a list of candidate gene
mentions using various string transforma-
tions that can be applied and chained in
a flexible order, followed by exact string
matching or approximate string matching.
Using a gold standard of MEDLINE ab-
stracts manually tagged and normalized
for mentions of human genes, a com-
bined tagging and normalization system
achieved 0.669 F-measure (0.718 preci-
sion and 0.626 recall) at the mention level,
and 0.901 F-measure (0.957 precision and
0.857 recall) at the document level for
documents used for tagger training.
1 Introduction
Gene and protein name identification and recogni-
tion in biomedical text are challenging problems.
A recent competition, BioCreAtIvE, highlighted the
? To whom correspondence should be addressed.
two tasks inherent in gene recognition: identifying
gene mentions in text (task 1A) (Yeh et al, 2005)
and normalizing an identified gene list (task 1B)
(Hirschman et al, 2005). This competition resulted
in many novel and useful approaches, but the results
clearly identified that more important work is neces-
sary, especially for normalization, the subject of the
current work.
Compared with gene NER, gene normalization
is syntactically easier because identification of the
textual boundaries of each mention is not required.
However, gene normalization poses significant se-
mantic challenges, as it requires detection of the ac-
tual gene intended, along with reporting of the gene
in a standardized form (Crim et al, 2005). Several
approaches have been proposed for gene normal-
ization, including classification techniques (Crim et
al., 2005; McDonald et al, 2004), rule-based sys-
tems (Hanisch et al, 2005), text matching with dic-
tionaries (Cohen, 2005), and combinations of these
approaches. Integrated systems for gene identifica-
tion typically have three stages: identifying candi-
date mentions in text, identifying the semantic in-
tent of each mention, and normalizing mentions by
associating each mention with a unique gene identi-
fier (Morgan et al, 2004). In our current work, we
focus upon normalization, which is currently under-
explored for human gene names. Our objective is
to create systems for automatically identifying hu-
man gene mentions with high accuracy that can be
used for practical tasks in biomedical literature re-
trieval and extraction. Our current approach relies
on a manually created and tuned set of rules.
41
2 Automatically Extracted Synonym
Dictionaries
Even when restricted to human genes, biomedical
researchers mention genes in a highly variable man-
ner, with a minimum of adherence to the gene nam-
ing standard provided by the Human Gene Nomen-
clature Committee (HGNC). In addition, frequent
variations in spelling and punctuation generate ad-
ditional non-standard forms. Extracting gene syn-
onyms automatically from online databases has sev-
eral benefits (Cohen, 2005). First, online databases
contain highly accurate annotations from expert
curators, and thus serve as excellent information
sources. Second, refreshing of specialized lexicons
from online sources provides a means to obtain new
information automatically and with no human in-
tervention. We thus sought a way to rapidly col-
lect as many human gene identifiers as possible.
All the statistics used in this section are from on-
line database holdings last extracted on February 20,
2006.
2.1 Building the Initial Dictionaries
Nineteen online websites and databases were ini-
tially surveyed to identify a set of resources that col-
lectively contain a large proportion of all known hu-
man gene identifiers. After examination of the 19 re-
sources with a limited but representative set of gene
names, we determined that only four databases to-
gether contained all identifiers (excluding resource-
specific identifiers used for internal tracking pur-
poses) used by the 19 resources. We then built an
automated retrieval agent to extract gene synonyms
from these four online databases: The HGNC Ge-
new database, Entrez Gene, Swiss-Prot, and Stan-
ford SOURCE. The results were collected into a sin-
gle dictionary. Each entry in the dictionary con-
sists of a gene identifier and a corresponding offi-
cial HGNC symbol. For data from HGNC, with-
drawn entries were excluded. Retrieving gene syn-
onyms from SOURCE required a list of gene identi-
fiers to query SOURCE, which was compiled by the
retrieval agent from the other sources (i.e., HGNC,
Entrez Gene and Swiss-Prot). In total, there were
333,297 entries in the combined dictionary.
2.2 Rule-Based Filter for Purification
Examination of the initial dictionary showed that
some entries did not fit our definition of a gene iden-
tifier, usually because they were peripheral (e.g., a
GenBank sequence identifier) or were describing a
gene class (e.g., an Enzyme Commission identifier
or a term such as ?tyrosine kinase?). A rule-based
filter was imposed to prune these uninformative syn-
onyms. The rules include removing identifiers under
these conditions:
1. Follows the form of a GenBank or EC acces-
sion ID (e.g., 1-2 letters followed by 5-6 dig-
its).
2. Contains at most 2 characters and 1 letter but
not an official HGNC symbol (e.g., P1).
3. Matches a description in the OMIM morbid
list1 (e.g., Tangier disease).
4. Is a gene EC number.2
5. Ends with ?, family ??, where ? is a capital letter
or a digit.
6. Follows the form of a DNA clone (e.g., 1-4 dig-
its followed by a single letter, followed by 1-2
digits).
7. Starts with ?similar to? (e.g., similar to zinc fin-
ger protein 533).
Our filter pruned 9,384 entries (2.82%).
2.3 Internal Update Across the Dictionaries
We used HGNC-designated human gene symbols as
the unique identifiers. However, we found that cer-
tain gene symbols listed as ?official? in the non-
HGNC sources were not always current, and that
other assigned symbols were not officially desig-
nated as such by HGNC. To remedy these issues, we
treated HGNC as the most reliable source and Entrez
Gene as the next most reliable, and then updated our
dictionary as follows:
1ftp://ftp.ncbi.nih.gov/repository/OMIM/morbidmap
2EC numbers are removed because they often represent gene
classes rather than specific instances.
42
? In the initial dictionary, some synonyms are
associated with symbols that were later with-
drawn by HGNC. Our retrieval agent extracted
a list of 5,048 withdrawn symbols from HGNC,
and then replaced any outdated symbols in the
dictionary with the official ones. Sixty with-
drawn symbols were found to be ambiguous,
but we found none of them appearing as sym-
bols in our dictionary.
? If a symbol used by Swiss-Prot or SOURCE
was not found as a symbol in HGNC or En-
trez Gene, but was a non-ambiguous synonym
in HGNC or Entrez Gene, then we replaced
it by the corresponding symbol of the non-
ambiguous synonym.
Among the 323,913 remaining entries, 801 entries
(0.25%) had symbols updated. After removing du-
plicate entries (42.19%), 187,267 distinct symbol-
synonym pairs representing 33,463 unique genes
were present. All tasks addressed in this section
were performed automatically by the retrieval agent.
3 Exact String Matching
We initially invoked several string transformations
for gene normalization, including:
1. Normalization of case.
2. Replacement of hyphens with spaces.
3. Removal of punctuation.
4. Removal of parenthesized materials.
5. Removal of stop words3.
6. Stemming, where the Porter stemmer was em-
ployed (Porter, 1980).
7. Removal of all spaces.
The first four transformations are derived from
(Cohen et al, 2002). Not all the rules we ex-
perimented with demonstrated good results for hu-
man gene name normalization. For example, we
found that stemming is inappropriate for this task.
To amend potential boundary errors of tagged men-
tions, or to match the variants of the synonyms, four
3ftp://ftp.cs.cornell.edu/pub/smart/English.stop
mention reductions (Cohen et al, 2002) were also
applied to the mentions or synonyms:
1. Removal of the first character.
2. Removal of the first word.
3. Removal of the last character.
4. Removal of the last word.
To provide utility, a system was built to allow
for transformations and reductions to be invoked
flexibly, including chaining of rules in various se-
quences, grouping of rules for simultaneous invo-
cation, and application of transformations to ei-
ther or both the candidate mention input and the
dictionary. For example, the mention ?alpha2C-
adrenergic receptor? in PMID 8967963 matches
synonym ?Alpha-2C adrenergic receptor? of gene
ADRA2C after normalizing case, replacing hyphens
by spaces, and removing spaces. Each rule can be
built into an invoked sequence deemed by evaluation
to be optimal for a given application domain.
A normalization step is defined here as the pro-
cess of finding string matches after a sequence of
chained transformations, with optional reductions
of the mentions or synonyms. We call a normal-
ization step safe if it generally makes only minor
changes to mentions. On the contrary, a normaliza-
tion step is called aggressive if it often makes sub-
stantial changes. However, a normalization step safe
for long mentions may not be safe for short ones.
Hence, our system was designed to allow a user to
set optional parameters factoring the minimal men-
tion length and/or the minimal normalized mention
length required to invoke a match.
A normalization system consists of multiple nor-
malization steps in sequence. Transformations are
applied sequentially and a match searched for; if
no match is identified for a particular step, the al-
gorithm proceeds to the next transformation. The
normalization steps and the optional conditions are
well-encoded in our program, which allows for a
flexible system specified by the sequences of the step
codes. Our general principle is to design a normal-
ization system that invokes safe normalization steps
first, and then gradually moves to more aggressive
43
ones. As the process lengthens, the precision de-
creases while the recall increases. The balance be-
tween precision and recall desired for a particular
application can be defined by the user.
Specifically, given string s, we use T (s) to de-
note the transformed string. All the 7 transformation
rules listed at the beginning of this subsection are
idempotent, since T (T (s)) = T (s). Two transfor-
mations, denoted by T1 and T2, are called commuta-
tive, if T1(T2(s)) = T2(T1(s)). The first four trans-
formations listed form a set of commutative rules.
Knowledge of these properties helps design a nor-
malization system.
Recall that NER systems, such as those required
for BioCreAtIvE task 1B, consist of two stages. For
our applications of interest, the normalization in-
put is generated by a gene tagger (McDonald and
Pereira, 2005), followed by the normalization sys-
tem described here as the second stage. In the sec-
ond stage, more synonyms do not necessarily imply
better performance, because less frequently used or
less informative synonyms may result in ambigu-
ous matches, where a match is called ambiguous
if it associates a mention with multiple gene iden-
tifiers. For example, from the Swiss-Prot dictio-
nary we know the gene mention ?MDR1? in PMID
8880878 is a synonym uniquely representing the
ABCB1 gene. However, if we include synonyms
from HGNC, it results in an ambiguous match be-
cause the TBC1D9 gene also uses the synonym
?MDR1?.
We investigated the rules separately, designed the
initial normalization procedure, and tuned our sys-
tem at the end. To evaluate the efficacy of our com-
piled dictionary and its sources, we determined the
accuracy of our system with all transformations and
reductions invoked sequentially, and without any ef-
forts to optimize the sequence (see section 6 for eval-
uation details). The goal in this experiment was to
evaluate the effectiveness of each vocabulary source
alone and in combination. Our experimental re-
sults at the mention level are summarized in Ta-
ble 1. The best two-staged system achieved a preci-
sion of 0.725 and recall of 0.704 with an F-measure
of 0.714, by using only HGNC and Swiss-Prot en-
tries.
As errors can be derived from the tagger or the
normalization alone or in combination, we also as-
Table 1: Results of Gene Normalization Using Exact
String Matching
Steps Recall Precision F-measure
(1) HGNC 0.762 0.511 0.611
(2) Entrez Gene 0.686 0.559 0.616
(3) Swiss-Prot 0.722 0.622 0.669
(4) SOURCE 0.743 0.431 0.545
(1)+(2) 0.684 0.564 0.618
(1)+(3) 0.725 0.704 0.714
(2)+(3) 0.665 0.697 0.681
(1)+(2)+(3) 0.667 0.702 0.684
(1)+(2)+(3)+(4) 0.646 0.707 0.675
sessed the performance of our normalization pro-
gram alone by directly normalizing the mentions in
the gold standard file used for evaluation (i.e., as-
suming the tagger is perfect). Our normalization
system achieved 0.824 F-measure (0.958 precision
and 0.723 recall) in this evaluation.
4 Approximate String Matching
Approximate string matching techniques have been
well-developed for entity identification. Given two
strings, a distance metric generates a score that re-
flects their similarity. Various string distance met-
rics have been developed based upon edit-distance,
string tokenization, or a hybrid of the two ap-
proaches (Cohen et al, 2003). Given a gene men-
tion, we consider the synonym(s) with the high-
est score to be a match if the score is higher than
a defined threshold. Our program also allows op-
tional string transformations and provides a user-
defined parameter for determining the minimal men-
tion length for approximate string matching. The
decision on the method chosen may be affected by
several factors, such as the application domain, fea-
tures of the strings representing the entity class, and
the particular data sets used. For gene NER, vari-
ous scoring methods have been favored (Crim et al,
2005; Cohen et al, 2003; Wellner et al, 2005).
Approximate string matching is usually consid-
ered more aggressive than exact string matching
with transformations; hence, we applied it as the last
step of our normalization sequence. To assess the
usefulness of approximate string matching, we be-
gan with our best dictionary subset in Subsection 3
44
(i.e., using HGNC and SwissProt), and applied ap-
proximate string matching as an additional normal-
ization step.
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
q-gram Match Ratio
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
q-gram Match Ratio
Jaro
JaroWinkler
SmithWaterman
TFIDF
UnsmoothedJS
Jaccard
Figure 1: Performance of Approximate String
Matching for Gene Normalization.
We selected six existing distance metrics that ap-
peared to be useful for human gene normalization:
Jaro, JaroWinkler, SmithWaterman, TFIDF, Un-
smoothedJS, and Jaccard. Our experiment showed
that TFIDF, UnsmoothedJS and Jaccard outper-
formed the others for human gene normalization in
our system, as shown in Figure 1. By incorpo-
rating approximate string matching using either of
these metrics into our system, overall performance
was slightly improved to 0.718 F-measure (0.724
precision and 0.713 recall) when employing a high
threshold (0.95). However, in most scenarios, ap-
proximate matching did not considerably improve
recall and had a non-trivial detrimental effect upon
precision.
5 Ambiguation Analysis
Gene identifier ambiguity is inherent in synonym
dictionaries as well as being generated during nor-
malization steps that transform mention strings.
5.1 Ambiguity in Synonym Dictionaries
If multiple gene identifiers share the same synonym,
it results in ambiguity. Table 2 shows the level of
ambiguity between and among the four sources of
gene identifiers used by our dictionary. The rate
of ambiguity ranges from 0.89% to 2.83%, which
is a rate comparable with that of mouse (1.5%)
and Drosophila (3.6%) identifiers (Hirschman et al,
2005).
 1
 10
 100
 1000
 10000
 100000
 1e+06
 1  10
# 
Sy
no
ny
m
s
Degree of Ambiguity
HGNC
Entrez Gene
Swiss-Prot
SOURCE
Total
Figure 2: Distribution of ambiguous synonyms in
the human gene dictionary.
Figure 2 is a log-log plot showing the distribu-
tion of ambiguous synonyms, where the degree is
the number of gene identifiers that a synonym is as-
sociated with. Comparing Figure 2 with (Hirschman
et al, 2005, Figure 3), we noted that on average, hu-
man gene synonyms are less ambiguous than those
of the three model organisms.
Another type of ambiguity is caused by gene sym-
bols or synonyms being common English words or
other biological terms. Our dictionary contains 11
gene symbols identical to common stop words4: T,
AS, DO, ET, IF, RD, TH, ASK, ITS, SHE and
WAS.
5.2 Ambiguous Matches in Gene
Normalization
We call a match ambiguous if it associates a men-
tion with multiple gene identifiers. Although the
4ftp://ftp.cs.cornell.edu/pub/smart/English.stop
45
Table 2: Statistics for Dictionary Sources
Dictionary # Symbols # Synonyms Ratio Max. # of Synonyms # with One Ambiguity
per Gene Definition Rate
HGNC 22,838 78,706 3.446 10 77,389 1.67%
Entrez Gene 33,007 109,127 3.306 22 106,034 2.83%
Swiss-Prot 12,470 61,743 4.951 17 60,536 1.95%
SOURCE 17,130 66,682 3.893 13 66,086 0.89%
Total 33,469 181,061 5.410 22 176,157 2.71%
normalization procedure may create ambiguity, if a
mention matches multiple synonyms, it may not be
strictly ambiguous. For example, the gene mention
?M creatine kinase? in PMID 1690725 matches the
synonyms ?Creatine kinase M-type? and ?Creatine
kinase, M chain? in our dictionary using the TFIDF
scoring method (with score 0.866). In this case, both
synonyms are associated with the CKM gene, so the
match is not ambiguous. However, even if a mention
matches only one synonym, it can be ambiguous, be-
cause the synonym is possibly ambiguous.
Figure 3 shows the result of an experiment con-
ducted upon 200,000 MEDLINE abstracts, where
the degree of ambiguity is the number of gene iden-
tifiers that a mention is associated with. The maxi-
mum, average, and standard deviation of the ambi-
guity degrees are 20, 1.129 and 0.550, respectively.
The overall ambiguity rate of all matched mentions
was 8.16%, and the rate of ambiguity is less than
10% at each step. Successful disambiguation can
increase the true positive match rate and therefore
improve performance but is beyond the scope of the
current investigation.
 1
 10
 100
 1000
 10000
 100000
 1e+06
 2  4  6  8  10  12  14  16  18  20
# 
M
en
tio
ns
# Matched Genes
Figure 3: Distribution of Ambiguous Genes in
200,000 MEDLINE Abstracts.
6 Application and Evaluation of an
Optimized Normalizer
Finally, we were interested in determining the effec-
tiveness of an optimized system based upon the gene
normalization system described above, and also cou-
pled with a state-of-the-art gene tagger. To de-
termine the optimal results of such a system, we
created a corpus of 100 MEDLINE abstracts that
together contained 1,094 gene mentions for 170
unique genes (also used in the evaluations above).
These documents were a subset of those used to train
the tagger, and thus measure optimal, rather than
typical MEDLINE, performance (data for a gener-
alized evaluation is forthcoming). This corpus was
manually annotated to identify human genes, ac-
cording to a precise definition of gene mentions that
an NER gene system would be reasonably expected
to tag and normalize correctly. Briefly, the definition
included only human genes, excluded multi-protein
complexes and antibodies, excluded chained men-
tions of genes (e.g., ?HDAC1- and -2 genes?), and
excluded gene classes that were not normalizable
to a specific symbol (e.g., tyrosine kinase). Docu-
ments were dual-pass annotated in full and then ad-
judicated by a 3rd expert. Adjudication revealed a
very high level of agreement between annotators.
To optimize the rule set for human gene normal-
ization, we evaluated up to 200 cases randomly cho-
sen from all MEDLINE files for each rule, where
invocation of that specific rule alone resulted in a
match. Most of the transformations worked per-
fectly or very well. Stemming and removal of the
first or last word or character each demonstrated
poor performance, as genes and gene classes were
often incorrectly converted to other gene instances
(e.g., ?CAP? and ?CAPS? are distinct genes). Re-
46
moval of stop words generated a high rate of false
positives. Rules were ranked according to their pre-
cision when invoked separately. A high-performing
sequence was ?0 01 02 03 06 016 026 036?, with 0
referring to case-insensitivity, 1 being replacement
of hyphens with spaces, 2 being removal of punc-
tuation, 3 being removal of parenthesized materials,
and 6 being removal of spaces; grouped digits indi-
cate simultaneous invocation of each specified rule
in the group. Table 3 indicates the cumulative accu-
racy achieved at each step5. A formalized determi-
nation of an optimal sequence is in progress. Ap-
proximate matching did not considerably improve
recall and had a non-trivial detrimental effect upon
precision.
Table 3: Results of Gene Normalization after Each
Step of Exact String Matching
Steps Recall Precision F-measure
0 0.628 0.698 0.661
01 0.649 0.701 0.674
02 0.654 0.699 0.676
03 0.665 0.702 0.683
06 0.665 0.702 0.683
016 0.718 0.685 0.701
026 0.718 0.685 0.701
036 0.718 0.685 0.701
The normalization sequence ?0 01 02 03 06 016
026 036? was then utilized for two separate evalua-
tions. First, we used the actual textual mentions of
each gene from the gold standard files as input into
our optimized normalization sequence, in order to
determine the accuracy of the normalization process
alone. We also used a previously developed CRF
gene tagger (McDonald and Pereira, 2005) to tag the
gold standard files, and then used the tagger?s output
as input for our normalization sequence. This sec-
ond evaluation determined the accuracy of a com-
bined NER system for human gene identification.
Depending upon the application, evaluation can
be determined more significant at either at the men-
tion level (redundantly), where each individual men-
tion is evaluated independently for accuracy, or as in
5The last two steps did not generate new matches using our
gold standard file and therefore the scores were unchanged.
These rule sets may improve performance in other cases.
the case of BioCreAtIvE task 1B, at the document
level (non-redundantly), where all mentions within a
document are considered to be equivalent. For pure
information extraction tasks, mention level accuracy
is a relevant performance indicator. However, for ap-
plications such as information extraction-based in-
formation retrieval (e.g., the identification of docu-
ments mentioning a specific gene), document-level
accuracy is a relevant gauge of system performance.
For normalization alone, at the mention level
our optimized normalization system achieved 0.882
precision, 0.704 recall, and 0.783 F-measure. At
the document level, the normalization results were
1.000 precision, 0.994 recall, and 0.997 F-measure.
For the combined NER system, the performance
was 0.718 precision, 0.626 recall, and 0.669 F-
measure at the mention level. At the document level,
the NER system results were 0.957 precision, 0.857
recall, and 0.901 F-measure. The lower accuracy of
the combined system was due to the fact that both
the tagger and the normalizer introduce error rates
that are multiplicative in combination.
7 Conclusions and Future Work
In this article we present a gene normalization sys-
tem that is intended for use in human gene NER, but
that can also be readily adapted to other biomedi-
cal normalization tasks. When optimized for human
gene normalization, our system achieved 0.783 F-
measure at the mention level.
Choosing the proper normalization steps depends
on several factors, such as (for genes) the organism
of interest, the entity class, the accuracy of identify-
ing gene mentions, and the reliability of the under-
lying dictionary. While the results of our normalizer
compare favorably with previous efforts, much fu-
ture work can be done to further improve the perfor-
mance of our system, including:
1. Performance of identifying gene mentions.
Only approximately 50 percent of gene men-
tions identified by our tagger were normaliz-
able. While this is mostly due to the fact that
the tagger identifies gene classes that cannot
be normalized to a gene instance, a significant
subset of gene instance mentions are not being
normalized.
2. Reliability of the dictionary. Though we have
47
investigated a sizable number of gene identifier
sources, the four representative sources used
for compiling our gene dictionary are incom-
plete and often not precise for individual terms.
Some text mentions were not normalizable due
the the incompleteness of our dictionary, which
limited the recall.
3. Disambiguation. A small portion (typi-
cally 7%-10%) of the matches were ambigu-
ous. Successful development of disambigua-
tion tools can improve the performance.
4. Machine-learning. It is likely possible that op-
timized rules can be used as probabilistic fea-
tures for a machine-learning-based version of
our normalizer.
Gene normalization has several potential applica-
tions, such as for biomedical information extraction,
database curation, and as a prerequisite for relation
extraction. Providing a proper synonym dictionary,
our normalization program is amenable to generaliz-
ing to other organisms, and has already proven suc-
cessful in our group for other entity normalization
tasks. An interesting future study would be to deter-
mine accuracy for BioCreAtIvE data once mouse,
Drosophila, and yeast vocabularies are incorporated
into our system.
Acknowledgment
This work was supported in part by NSF grant
EIA-0205448, funds from the David Lawrence
Altschuler Chair in Genomics and Computational
Biology, and the Penn Genomics Institute. The au-
thors acknowledge Shannon Davis and Jeremy Laut-
man for gene dictionary assessment, Steven Carroll
for gene tagger implementation and results, Penn
BioIE annotators for annotation of the gold standard,
and Monica D?arcy and members of the Penn BioIE
team for helpful comments.
References
K. B. Cohen, A. E. Dolbey, G. K. Acquaah-Mensah, and
L. Hunter. 2002. Contrast and variability in gene
names. In ACL Workshop on Natural Language Pro-
cessing in the Biomedical Domain, pages 14?20.
W. W. Cohen, P. Ravikumar, and S. E. Fienberg. 2003.
A comparison of string distance metrices for name-
matching tasks. In Proceedings of IIWeb Workshop.
A. M. Cohen. 2005. Unsupervised gene/protein entity
normalization using automatically extracted dictionar-
ies. In Linking Biological Literature, Ontologies and
Databases: Mining Biological Semantics, Proceed-
ings of the BioLINK2005 Workshop, pages 17?24. MI:
Association for Computational Linguistics, Detroit.
J. Crim, R. McDonald, and F. Pereira. 2005. Automati-
cally annotating documents with normalized gene lists.
BMC Bioinformatics, 6(Suppl 1)(S13).
D. Hanisch, K. Fundel, H.-T. Mevissen, R. Zimmer, and
J. Fluck. 2005. Prominer: Rule-based protein and
gene entity recognition. BMC Bioinformatics, 6(Suppl
1)(S14).
L. Hirschman, M. Colosimo, A. Morgan, and A. Yeh.
2005. Overview of biocreative task 1b: Normalized
gene lists. BMC Bioinformatics, 6(Suppl 1)(S11).
R. McDonald and F. Pereira. 2005. Identifying gene
and protein mentions in text using conditional random
fields. BMC Bioinformatics, 6(Suppl 1)(S6).
R. McDonald, R. S. Winters, M. Mandel, Y. Jin, P. S.
White, and F. Pereira. 2004. An entity tagger for rec-
ognizing acquired genomic variations in cancer litera-
ture. Journal of Bioinformatics, 20(17):3249?3251.
A. A. Morgan, L. Hirschman, M. Colosimo, A. S. Yeh,
and J. B. Colombe. 2004. Gene name identification
and normalization using a model organism database.
Journal of Biomedical Informatics, 37(6):396?410.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
B Wellner, J. Castan?o, and J. Pustejovsky. 2005. Adap-
tive string similarity metrics for biomedical reference
resolution. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontologies and
Databases: Mining Biological Semantics, pages 9?16,
Detroit. Association for Computational Linguistics.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. Biocreative task 1a: Gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl 1)(S2).
48
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 913?920
Manchester, August 2008
 
	
	ffSYSTRAN?s Chinese Word Segmentation 
Jin Yang Jean Senellart Remi Zajac 
SYSTRAN Software, Inc. 
9333 Genesee Ave. 
San Diego, CA 92121, USA 
jyang@systransoft.com 
SYSTRAN S.A. 
1, rue du Cimeti?re 
95230 Soisy-sous-Montmorency, France 
senellart@systran.fr 
 
SYSTRAN Software, Inc. 
9333 Genesee Ave. 
San Diego, CA 92121, USA 
zajac@systransoft.com 
Abstract 
SYSTRAN?s Chinese word segmentation 
is one important component of its 
Chinese-English machine translation 
system. The Chinese word segmentation 
module uses a rule-based approach, based 
on a large dictionary and fine-grained 
linguistic rules. It works on general-
purpose texts from different Chinese-
speaking regions, with comparable 
performance. SYSTRAN participated in 
the four open tracks in the First 
International Chinese Word Segmentation 
Bakeoff. This paper gives a general 
description of the segmentation module, 
as well as the results and analysis of its 
performance in the Bakeoff. 
1 Introduction 
Chinese word segmentation is one of the pre-
processing steps of the SYSTRAN Chinese-
English Machine Translation (MT) system. The 
development of the Chinese-English MT system 
began in August 1994, and this is where the 
Chinese word segmentation issue was first 
addressed. The algorithm of the early version of 
the segmentation module was borrowed from 
SYSTRAN?s Japanese segmentation module. The 
program ran on a large word list, which contained 
600,000 entries at the time1. The basic strategy was 
to list all possible matches for an entire linguistic 
unit, then solve the overlapping matches via 
linguistic rules. The development was focused on 
technical domains, and high accuracy was 
achieved after only three months of development. 
Since then, development has shifted to other areas 
of Chinese-English MT, including the enrichment 
of the bi-lingual word lists with part-of-speech, 
syntactic and semantic features. In 2001, the 
development of a prototype Chinese-Japanese MT 
system began. Although the project only lasted for 
three months, some important changes were made 
in the segmentation convention, regarding the 
distinction between words and phrases 2 . Along 
with new developments of the SYSTRAN MT 
engine, the segmentation engine has recently been 
re-implemented. The dictionary and the general 
approach remain unchanged, but dictionary lookup 
and rule matching were re-implemented using 
finite-state technology, and linguistic rules for the 
segmentation module are now expressed using a 
context-free-based formalism, improving 
maintainability. The re-implementation generates 
multiple segmentation results with associated 
probabilities. This will allow for disambiguation at 
a later stage of the MT process, and will widen the 
possibility of word segmentation for other 
applications. 
2 System Description 
2.1 Segmentation Standard 
Our definition of words and our segmentation 
conventions are based on available standards, 
modified for MT purposes. The PRC standard (Liu 
et al, 1993) was initially used. Sample differences 
are listed as follows: 
 
Type PRC SYSTRAN 
NP ???? 
??????? 
??   ?? 
??  ??  ??? 
CD 31? 31  ? 
CD + M ?? ??? ?  ?   ?  ?  ? 
DI4 + CD ?? ?  ? 
Name ?  ? ?  ?? ??   ??? 
 
Table 1. Segmentation Divergences with the PRC Guidelines 
2.2 Methodology 
The SYSTRAN Chinese word segmentation 
module uses a rule-based approach and a large 
dictionary. The dictionary is derived from the 
Chinese-English MT dictionary. It currently 
includes about 400,000 words. The basic 
segmentation strategy is to list all possible matches 
for a translation unit (typically, a sentence), then to 
solve overlapping matches via linguistic rules. The 
same segmentation module and the same 
dictionary are used to segment different types of 
text with comparable performance. 
All dictionary lookup and rule matching are 
performed using a low level Finite State 
Automaton library. The segmentation speed is 
3,500 characters per second using a Pentium 4 
2.4GHZ processor. 
Dictionary 
The Chinese-English MT dictionary currently 
contains 400,000 words (e.g., ??), and 200,000 
multi-word expressions (e.g., ??  ??  ??? ). 
Only words are used for the segmentation. 
Specialized linguistic rules are associated with the 
dictionary. The dictionary is general purpose, with 
good coverage on several domains. Domain-
specific dictionaries are also available, but were 
not used in the Bakeoff. 
The dictionary contains words from different 
Chinese-speaking regions, but the representation is 
mostly in simplified Chinese. The traditional 
characters are considered as ?variants?, and they 
are not physically stored in the dictionary. For 
example, ???  and ???  are stored in the 
dictionary, and ??? can also be found via the 
character matching ?? ?. 
The dictionary is encoded in Unicode (UTF8), 
and all internal operations manipulate UTF8 
strings. Major encoding conversions are supported, 
including GB2312-80, GB13000, BIG-5, BIG5-
HKSCS, etc. 
Training 
The segmentation module has been tested and fine-
tuned on general texts, and on texts in the technical 
and military domains (because of specific customer 
requirements for the MT system). Due to the wide 
availability of news texts, the news domain has 
also recently been used for training and testing. 
The training process is merely reduced to the 
customization of a SYSTRAN MT system. In the 
current version of the MT system, customization is 
achieved by building a User Dictionary (UD). A 
UD supplements the main dictionary: any word 
that is not found in the main MT system dictionary 
is added in a User Dictionary.  
Name-Entity Recognition and Unknown Words 
Name entity recognition is still under development. 
Recognition of Chinese persons? names is done via 
linguistic rules. Foreign name recognition is not 
yet implemented due to the difficulty of obtaining 
translations. 
Due to the unavailability of translations, even 
when an unknown word has been successfully 
recognized, we consider the unknown word 
recognition as part of the terminology extraction 
process. This feature was not integrated for the 
Bakeoff. 
2.3 Evaluation 
Our internal evaluation has been focused on the 
accuracy of segmentation using our own 
segmentation standard. Our evaluation process 
includes large-scale bilingual regression testing for 
the Chinese-English system, as well as regression 
testing of the segmenter itself using a test database 
of over 5MB of test items. Two criteria are used: 
1. Overlapping Ambiguity Strings (OAS): the 
reference segmentation and the segmenter 
segmentation overlap for some string, e.g., 
AB-C and A-BC. As shown below, this 
typically indicates an error from our 
segmenter. 
2. Covering Ambiguity Strings (CAS): the test 
strings that cover the reference strings 
(CAS-T: ABC and AB-C), and the reference 
strings that cover the test strings (CAS-R: 
AB-C and ABC). These cases arise mostly 
from a difference between equally valid 
segmentation standards. 
No evaluation with other standards had been done 
before the Bakeoff. 
 
Test Reference Type 
???  ?? ??  ??? OAS 
???? ? ? ? ? CAS-T 
???? ?? ? ? CAS-T 
?? ? ? CAS-T 
?? ?? ???? CAS-R 
1994  ? 1994 ? CAS-R 
? ? ?? CAS-R 
Table 2. Types of Segmentation Differences 
3 Discussion of the Bakeoff 
3.1 Results 
SYSTRAN participated in the four open tracks in 
the First International Chinese Word Segmentation 
Bakeoff http://www.sighan.org/bakeoff2003/. Each 
track corresponds to one corpus with its own word 
segementation standard. Each corpus had its own 
segmentation standard that was significantly 
different from the others. The training process 
included building a User Dictionary that contains 
words found in the training corpora, but not in the 
SYSTRAN dictionary.  Although each of these 
corpora was segmented according to its own 
standard, we made a single UD containing all the 
words gathered in all corpora.  
Although the ranking of the SYSTRAN 
segmenter is different in the four open tracks, 
SYSTRAN?s segmentation performance is quite 
comparable across the four corpora. This is to be 
compared to the scores obtained by other 
participants, where good performance was 
typically obtained on one corpus only. SYSTRAN 
scores for the 4 tracks are shown in Table 3 (Sproat 
and Emerson, 2003). 
 
Track R P F Roov Riv 
ASo 0.915 0.894 0.904 0.426 0.926 
CTBo 0.891 0.877 0.884 0.733 0.925 
HKo 0.898 0.860 0.879 0.616 0.920 
PKo 0.905 0.869 0.886 0.503 0.934 
Table 3. SYSTRAN?s Scores in the Bakeoff 
3.2 Discussions 
The segmentation differences between the 
reference corpora and SYSTRAN?s results are 
further analyzed. Table 4 shows the  partition of 
divergences between OAS, CAS-T, and CAS-R 
strings:3 
 
 Total Same OAS CAS-T CAS-R 
ASo 11,985 10,970 76 448 491 
CTBo 39,922 35,561 231 2,419 1,711 
HKo 34,959 31,397 217 1,436 1,909 
PKo 17,194 15,554 82 615 943 
Table 4. Count of OAS and CAS Divergence 
The majority of OAS divergences show incorrect 
segmentation from SYSTRAN. However, 
differences in CAS do not necessarily indicate 
incorrect segmentation results. The reasons can be 
categorized as follows: a) different segmentation 
standards, b) unknown word problem, c) name 
entity recognition problem, and d) miscellaneous4. 
The distributions of the differences are further 
analyzed in Table 5 and 6 for the ASo and PKo 
corpora, respectively. 
 
CAS-R: Unique Strings=334 (total=491) 
Type Count Percent Examples 
Different 
Standards 
184 55% ???  
??  
????  
???? 
Unknown  
Words 
116 35% ??  
??  
??  
?? 
Name  
Entity 
30 9% ??  
???  
?? 
Misc. 4 1% ???? 
CAS-T: Unique Strings=137 (total=448) 
Type Count Percent Examples 
Different  
Standard 
134 98% ??  
??? 
???? 
True 
Covering 
3 2% ?? 
?? 
Table 5. Distribution of Divergences in the ASo Track  
 
 
CAS-R: Unique Strings=508 (total=943) 
Type Count Percent Examples 
Different 
Standards 
294 58% ???? 
?? 
?? 
?? 
?? 
2001 ? 
Unknown  
Words 
90 18% ?? 
?? 
?? 
Name  
Entity 
61 12% ??? 
??? 
Misc. 63 12% 20% 
3.9? 
CAS-T: Unique Strings=197 (total=615) 
Type Count Percent Examples 
Different  
Standards 
194 98% ??? 
?? 
??? 
??? 
True 
Covering 
3 2% ?? 
?? 
Table 6. Distribution of Divergences in the PKo Track  
 
This analysis shows that the segmentation results 
are greatly impacted by the difference in the 
segmentation standards. Other problems include 
for example the encoding of numbers using single 
bytes instead of the standard double-byte encoding 
in the PKo corpus, which account for about 12% of 
differences in the PKo track scores.  
4 Conclusion 
For an open track segmentation competition like 
the Bakeoff, we need to achieve a balance between 
the following aspects:  
? Segmentation standards: differences between 
one?s own standard and the reference standard. 
? Adaptation to the other standards: whether one 
should adapt to other standards. 
? Dictionary coverage: the coverage of one?s 
own dictionary and the dictionary obtained by 
training. 
? Algorithm: combination of segmentation, 
unknown word identification, and name entity 
recognition. 
? Speed: the time needed to segment the corpora. 
? Training: time and manpower used for training 
each corpus and track 
Few systems participated in all open tracks: 
only SYSTRAN and one university participated in 
all four. We devoted about 2 person/week for this 
evaluation. We rank in the top three of three open 
tracks, and only the PKo track scores are lower, 
probably because of encoding problems for 
numbers for this corpus (we did not adjust our 
segmenter to cope with this corpus-specific 
problem). Our results are very consistent for all 
open tracks, indicating a very robust approach to 
Chinese segmentation.  
Analysis of results shows that SYSTRAN?s 
Chinese word segmentation excels in the area of 
dictionary coverage, robustness, and speed. The 
vast majority of divergences with the test corpora 
originate from differences in segmentation 
standards (over 55% for CAS-R and about 98% for 
CAS-T). True errors range between 0% and 2% 
only, the rest being assigned to either the lack of 
unknown word processing or the lack of a name 
entity recognizer. Although not integrated, the 
unknown word identification and name entity 
recognition are under development as part of a 
terminology extraction tool. 
For future Chinese word segmentation 
evaluations, some of the issues that arose in this 
Bakeoff would need to be addressed to obtain even 
more significant results, including word 
segmentation standards and encoding problems for 
example. We would also welcome the introduction 
of a surprise track, similar to the surprise track of 
the DARPA MT evaluations that would require 
participants to submit results within 24 hours on an 
unknown corpus. 
References 
Liu, Y, Tan Q. & Shen, X. 1993. Segmentation 
Standard for Modern Chinese Information Processing 
and Automatic Segmentation Methodology. 
Sproat, R., & Emerson T. 2003.  The First International 
Chinese Word Segmentation Bakeoff.  In the 
Proceedings of the Second SIGHAN Workshop on 
Chinese Language Processing. ACL03. 
 
                                                          
1  The word list only contained Chinese-English 
bilingual dictionary without any syntactic or semantic 
features. It also contained many compound nouns, e.g. 
????. 
2 Compound nouns are no longer considered as words. 
They were moved to the expression dictionary. For 
example, ???? has become ?? ??. 
3 The number of words in the reference strings is used 
when counting OAS and CAS divergences. For 
example, ?????s CAS count is three because the 
number of words in the reference string ?? ? ? is 
three. 
4 Word segmentation in SYSTRAN MT systems occurs 
after sentence identification and normalization. During 
word segmentation, Chinese numbers are converted into 
Arabic numbers. 
