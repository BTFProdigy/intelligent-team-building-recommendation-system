Coling 2008: Companion volume ? Posters and Demonstrations, pages 35?38
Manchester, August 2008
Underspecified Modelling of Complex Discourse Constraints
Markus Egg
egg@let.rug.nl
University of Groningen
Michaela Regneri
regneri@coli.uni-sb.de
Saarland University
Abstract
We introduce a new type of discourse con-
straints for the interaction of discourse re-
lations with the configuration of discourse
segments. We examine corpus-extracted
examples as soft constraints. We show how
to use Regular Tree Gramamrs to process
such constraints, and how the representa-
tion of some constraints depends on the ex-
pressive power of this formalism.
1 Introduction
Discourse structures cannot always be described
completely, either because they are ambiguous
(Stede, 2004), or because a discourse parser fails
to analyse them completely. In either case, un-
derspecification formalisms (UFs) can be used to
represent partial information on discourse struc-
ture. UFs are used in semantics to model structural
ambiguity without disjunctive enumeration of the
readings (van Deemter and Peters, 1996).
Underspecified descriptions of discourse must
handle two kinds of incomplete information, on
the configuration of discourse segments (how
they combine into larger units), and on the dis-
course relations that bring about this configura-
tion: Our corpus studies on the RST Discourse
Treebank (Carlson et al, 2002) showed interde-
pendencies between relations and configuration, a
phenomenon first noted by (Corston-Oliver, 1998).
These interdependencies can be formulated as con-
straints that contribute to the disambiguation of un-
derspecified descriptions of discourse structure.
E.g., in discourse segments constituted by the
relation Condition, the premiss tends to be a dis-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
course atom (or at least, maximally short).
1
Simi-
larly, there is evidence for an interdependency con-
straint for the relation Purpose(1)
2
. In most cases,
Purpose(1) has a discourse atom as its nucleus.
The corpus evaluation furthermore shows that
those patterns never occur exclusively but only as
tendencies. Realised as soft constraints, such ten-
dencies can help to sort the set of readings ac-
cording to the established preferences, which al-
lows to focus on the best reading or the n-best
readings. This is of high value for an UF-based
approach to discourse structure, which must cope
with extremely high numbers of readings. To
model interdependency constraints, we will use
Regular Tree Grammars (RTGs) (Comon et al,
2007). RTGs can straightforwardly be extended
to weighted Regular Tree Grammars (wRTGs),
which can represent both soft and hard constraints.
Apart from our corpus-extracted examples, we
also consider a hard interdependency constraint
similar to the Right Frontier Constraint. We show
that we can integrate this attachment constraint
with our formalism, and how its representation de-
pends on the expressiveness of RTGs.
2 Underspecified Discourse Structure
We describe (partial) information on discourse
structure by expressions of a suitable UF, here,
dominance graphs (Althaus et al, 2003). Consider
e.g. Fig. 1(a), the dominance graph for (1):
(1) [C
1
I try to read a novel] [C
2
if I feel bored]
[C
3
because the TV programs disappoint me]
[C
4
but I can?t concentrate on anything.]
1
Following Rhetorical Structure Theory (Mann and
Thompson, 1988), most discourse relations have a central nu-
cleus argument, and a peripheral satellite argument. For Con-
dition, the premiss is the satellite, the nucleus, the conclusion.
2
?(n)? as part of a relation name indicates that the nucleus
is its n-th argument; relations with names without such an
affix are multinuclear, i.e., link two segments of equal promi-
nence. We sometimes omit the numbers where the position of
the nucleus is clear from the context.
35
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
C
1
C
2
C
3
C
4
C
2
C
3
C
4
C
1
C
4
C
1
C
2
C
3
Condition
(1)
Condition
(1)
Condition
(1)
Condition
(1)
Cause
(2)Cause
(2)
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
Condition
(1)
Cause
(2)
Contrast
Contrast
Contrast
Condition
(1)
1
2
3 5
4
7
6
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
(a) (b) (c) (d) (e) (f)
Figure 1: An underspecified discourse structure and its five configurations
{1-7}? Condition({1}, {3-7}) [1] {1-7}? Cause({1-3}, {5-7}) [1] {3-7}? Contrast({3-5}, {7}) [1]
{3-5}? Cause({3}, {5}) [1] {1-7}? Contrast({1-5}, {7}) [1] {1-5}? Cause({1-3}, {5}) [1]
{5-7}? Contrast({5}, {7}) [1] {1-5}? Condition({1}, {3-5}) [3] {1-3}? Condition({1}, {3}) [9]
{3-7}? Cause({3}, {5-7}) [1] {1} ? C
1
[1] {3} ? C
2
[1] {5} ? C
3
[1] {7} ? C
4
[1]
Figure 2: A wRTG modelling the interdependency constraint for Fig. 1
Such constraints describe a set of discourse
structures (formalised as binary tree structures).
Their key ingredient are (reflexive, transitive and
antisymmetric) dominance relations, which are in-
dicated by dotted lines. Dominance of X
1
over X
2
means that X
2
is part of the structure below (and
including) X
1
, but there might be additional mate-
rial intervening between X
1
and X
2
.
Fig. 1(a) states that C
1
is linked to a part of the
following discourse (including at leastC
2
) byCon-
dition, Cause(2) connects two discourse segments
(comprising at least C
2
and C
3
, respectively), and
Contrast links a discourse segment to its left (in-
cluding at least C
3
) to C
4
.
This constraint describes (is compatible with)
exactly the five tree structures in Fig. 1(b-f), if
described tree structures may only comprise ma-
terial that is already introduced in the constraint.
They model the potential discourse structures for
(1) (see Webber (2004)). Dominance graphs like
Fig. 1a. are pure chains. Pure chains describe all
binary trees with the same leaf language, here the
discourse segments, in their textual order. Pure
chains define a left-to-right order, in that not only
the leaves always form the same sequence, but also
the inner nodes: If a labelled node X is further to
the left in the chain than another node Y, in every
described tree, X will either be Y?s left child, or Y
will be X?s right child, or there will be a fragment
F of which X is a successor on the left and Y is a
right successor. Henceforth we will refer to frag-
ments with their index in the chain (indicated by
encircled numbers in Fig. 1a).
3 Representing Soft Interdependencies
The interdependency constraint for Condition(1) is
that its satellite tends to be maximally short, i.e.,
mostly consists of only one discourse atom, and
in most remaining cases, of two atoms. Thus, (b)
and (d) are preferred among the configurations in
Fig. 1, (c) is less preferred, and (e) and (f) are the
least preferred. Regular Tree Grammars (RTGs) as
UF (Koller et al, 2008) can express such complex
constraints straightforwardly, and provide a con-
venient framework to process them. They allow
to extract a best configuration with standard algo-
rithms very efficiently.
Koller et al (2008) show how to generate an
RTG describing the same set of trees as a domi-
nance graph. Similar to a context free grammar, an
RTG uses production rules with terminal symbols
and nonterminal symbols (NTs), whereby the left-
hand side (LHS) is always a nonterminal and the
right-hand side (RHS) contains at least one termi-
nal symbol. One NT is the start symbol. A tree
is accepted by the grammar if the grammar con-
tains a derivation for it. An example for an RTG is
given in Fig. 2, which describes the same trees as
the dominance graph in Fig. 1a. The start symbol
is {1-7}. To derive e.g. the tree in Fig. 1d, we first
select the rule {1-7} ? Cause({1-3}, {5-7}) that
determines Condition as root for the whole tree.
The left child of Condition is then derived from
{1-7}, and the right child from {5-7} respectively.
To emphasize the association with the dominance
graph, we mark nonterminals as the subgraphs they
represent, e.g., {1-7} denotes the whole graph.
The terminal in the RHS of a grammar rule deter-
mines the root of the LHS subgraph.
Koller et al (2008) also use weighted RTGs
(wRTGs, an extension of RTG with weights) to
express soft dominance constraints (which, unlike
hard constraints, do not restrict but rather rank the
set of configurations). We use wRTGs to model
the soft interdependency constraints. The gram-
mar in Fig. 2 is also a wRTG that assigns a weight
to each derived tree: Its weight is the product over
all weights of all rules used for the derivation.
Weights appear in squared brackets after the rules.
36
The (merely expository) weights in our example
encode the preference of Condition for a maxi-
mally short right child: There are three grammar
rules that establish Condition as the root of a sub-
graph (shaded in Fig. 2), which are distinguished
by the size of the right child of the root (one ({3}),
three ({3-5}) or five ({3-7}) nodes). The shorter
the right child, the higher the weight associated
with the rule. (1 is a neutral weight by definition.)
The grammar thus assigns different weights to the
trees in Fig. 1; (b) and (d) get the maximum weight
of 9, (b), a medium weight of 3, and (e) and (f), the
lowest weight of 1.
4 Expressive Power of RTGs
As Koller et al (2008) show, the expressive power
of RTGs is superior to other common underspec-
ification formalism. We show an important appli-
cation of the increased expressiveness with Ex. 2,
where a. can be continued by b. but not by c:
(2) a. [C
1
Max and Mary are falling apart.]
[C
2
They no longer meet for lunch.]
[C
3
And, last night, Max went to the
pub] [C
4
but Mary visited her parents.]
b. [C
5a
She complained bitterly about his
behaviour.]
c. [C
5b
He left after his fifth pint of lager.]
Segment C
5a
continues the preceding clause
about Mary?s visit with additional information
about the visit, it thus attaches directly to C
4
. To
find a coherent integration of C
5b
, we would have
to connect it to C
3
, as it provides more details
about Max? night at the pub. However, in the given
constellation of C
3
and C
4
, that form a Contrast
together, C
3
is not available any longer for attach-
ment of further discourse units. (This constraint is
reminiscent of the Right Frontier Constraint, as it
is used by Asher and Lascarides (2003). However,
it is unclear how the Right Frontier Constraint in
its exact definition can carry over to binary trees.)
The given attachment constraint is not express-
ible with dominance graphs: it excludes the config-
urations of its dominance graph (Fig. 3) in which
Contrast shows up as a direct left child, e.g.,
(3b/e/f) as opposed to (3c/d). For instance, the
excluded structure emerges in (3e/f) by choosing
Cause as root of the the subgraph 5-9 (i.e., includ-
ing the Contrast- and Sequence-fragments). For
convenience, we will talk about this constraint as
the ?left child constraint? (LCC).
S ? Contrast(S, S) L ? Evid(S, S)
S ? Sequ(L, S) L ? List(S, S)
S ? L
L ? C
1
L ? C
2
L ? C
3
L ? C
4
L ? C
5
Figure 5: A filter RTG corresponding to Ex. 2
This additional constraint, however, can be ex-
pressed by an RTG like Fig. 4. We explicitly
distinguish between subgraphs (referred to with
numbers) and their associated NTs here. Cru-
cially, some subgraphs can be processed in dif-
ferent derivations here, e.g., {5-9} (as right child
of List, irrespective of the relative scope of Ev-
idence and List), or {3-7} (in the expansions of
both {EvLiCo} and {LiCoSe}, like in (3c) as
opposed to (3d)). Sometimes this derivation his-
tory is irrelevant, like in the case of {5-9} (here,
only Contrast may be chosen as root anyway), but
there are cases where it matters: If {3-7} is the left
child of Sequence, as in (3b/d), the choice of Con-
trast as its root is excluded, since this would make
Contrast the left child of Sequence, as in (3b). In
contrast, {3-7} as the right child of Evidence, like
in (3c), allows both Contrast and List as root, be-
cause Contrast emerges as a right child in either
case. Thus, the two occurrences of {3-7} are dis-
tinguished in terms of different NTs in the gram-
mar, and only in the NT for the latter occurrence is
there more than one further expansion rule.
Regular tree languages are closed under inter-
section. Thus, one can derive a grammar like Fig. 4
by intersecting a completely underspecified RTG
(here, the one derived from Fig. 3a) with a suitable
filter grammar, e.g., Fig. 4. The filter grammar
produces an infinite language, containing the frag-
ments of Fig. 3a and excluding any derivation in
which Sequence is the direct parent of Contrast.
This is guaranteed by introducing the nonterminal
L (the left child NT for Sequence), for which there
is no derivation with Contrast as its root.
For an arbitrary pure chain with n fragments, the
filter grammar generating the LCC is constructed
as follows: S is the start symbol. For every frag-
ment i s.t. 0 < i < n, there is a derivation rule
with S as its LHS and i in its RHS, thus either
S ? i, for singleton fragments, or S ? i(A,S),
for binary fragments. If i is binary, we must de-
termine A: If there is at least one fragment f < i
s.t. the LCC is assumed for f , we create a new
NT L
i
; every derivation rule with i on its RHS fol-
lows the pattern X ? i(L
i
, S) (thus A = L
i
in
particular). If there is no LCC fragment to the left
37
C5
C
2
C
3
C
4
Evidence
(1)
SequenceContrast
C
1
List
C
1
C
5
C
3
C
4
Evid
Contr
Sequ
C
2
List
C
1
C
3
C
4
Evid
Contr
C
2
List
C
5
Sequ
C
1
C
5
C
4
Evid
Contr
Sequ
C
3
C
2
List
List
C
1
Evid
C
2
C
4
C
5
Contr
C
3
(a) (b)
(c) (d) (e)
C
1
C
5
C
4
Evid
Contr
List
C
3
C
2
Sequ
Sequ
(f)
1
2
3
5
4
7
6
8
9
Figure 3: An underspecified discourse structure for Ex. 2 and five of its configurations
{EvLiCoSe} ? Evid({C
1
}, {LiCoSe}) {EvLiCo} ? List({Ev}, {Co}) {Ev} ? Evid({C
1
}, {C
2
})
{EvLiCoSe} ? List({Ev}, {CoSe}) {CoSe} ? Cont({C
3
}, {Se}) {Li} ? List({C
2
}, {C
3
})
{EvLiCoSe} ? Cont({EvLi}, {Se}) {EvLi} ? Evid({C
1
}, {Li}) {Co} ? Cont({C
3
}, {C
4
})
{EvLiCoSe} ? Sequ({EvLiCo}, {C
5
}) {EvLi} ? List({Ev}, {C
3
}) {Se} ? Sequ({C
4
}, {C
5
})
{LiCoSe} ? Sequ({LiCo}
L
, {C
5
}) {LiCo}
L
? List({C
2
}, {Co})
{LiCoSe} ? List({C
2
}, {CoSe}) {LiCo}
S
? Cont({Li}, {C
4
}) {C
1
} ? C
1
{C
2
} ? C
2
{LiCoSe} ? Cont({Li}, {Se}) {LiCo}
S
? Li({Li}, {C
4
}) {C
3
} ? C
3
{EvLiCo} ? Evid({C
1
}, {LiCo}
S
) {C
4
} ? C
4
{C
5
} ? C
5
Figure 4: A RTG integrating the attachment constraint for Contrast from Ex. 2 into Fig. 3
of i, A = S. If a new NT L
i
was created, we
need to create its RHSs: For every fragment h s.t.
0 < h < i and there is no LCC for h, there is a
rewrite rule directly deriving h from L
i
. If h is a
singleton fragment, the rule is L
i
? h. Otherwise
the rule is L
i
? h(A
?
, S), whereby A
?
= S, if
there is no L
h
, or A
?
= L
h
if there is some LCC
fragment on the left of h.
3
The grammar in Fig. 4 can be generated with
that scheme; it has been reduced afterwards in that
a general rule S ? L substitutes for all rules of the
form S ? NT for which there is a corresponding
rule L ? NT (e.g., S ? Evid(S, S)).
5 Conclusion
Interdependency constraints that arise from the in-
teraction of discourse relations and their surround-
ing structures are introduced as a new technique
for disambiguating discourse structure. We inte-
grate those constraints in underspecified discourse
structures by exploiting the expressive power of
Regular Tree Grammars as UF. As the corpus anal-
ysis yields in many cases only soft interdepen-
dency constraints, we use the weighted extension
of RTGs, which allows to sort the readings of an
underspecified representation and to identify pre-
ferred discourse structures. We then showed that
the representation of some discourse constraints
depend on the expressive power of RTGs. For
notes on implementation and tractability of our ap-
proach, see Regneri et al (2008).
3
To model this as a preference rather than as a hard con-
straint, no rules for the L-NTs are omitted, but rather weighted
low. An intersection with a preference-neutral wRTG would
rank the configurations violating the constraint low, and all
others with neutral weights.
References
Althaus, Ernst, Denys Duchier, Alexander Koller, Kurt
Mehlhorn, Joachim Niehren, and Sven Thiel. 2003.
An efficient graph algorithm for dominance con-
straints. Journal of Algorithms, 48:194?219.
Asher, Nicholas and Alex Lascarides. 2003. Logics of
Conversation. Cambridge UP, Cambridge.
Carlson, Lynn, Daniel Marcu, and Mary Ellen
Okurowski. 2002. RST Discourse Treebank. LDC.
Comon, H., M. Dauchet, R. Gilleron, C. L?oding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree Automata Techniques and Ap-
plications. Available on: http://www.grappa.
univ-lille3.fr/tata. Release 12-10-2007.
Corston-Oliver, Simon H. 1998. Computing Represen-
tations of Discourse Structure. Ph.D. thesis, Dept. of
Linguistics, University of California, Santa Barbara.
van Deemter, Kees and Stanley Peters, editors. 1996.
Semantic ambiguity and underspecification. CSLI,
Stanford.
Koller, Alexander, Michaela Regneri, and Stefan
Thater. 2008. Regular tree grammars as a formal-
ism for scope underspecification. In Proceedings of
the ACL 08.
Mann, William C. and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8:243?281.
Regneri, Michaela, Markus Egg, and Alexander Koller.
2008. Efficient Processing of Underspecified Dis-
course Representations. In Proceedings of the ACL
08 (Short Papers).
Stede, Manfred. 2004. The Potsdam Commentary Cor-
pus. In Webber, Bonnie and Donna K. Byron, edi-
tors, ACL 2004 Workshop on Discourse Annotation.
Webber, Bonnie. 2004. D-LTAG: extending lexicalized
TAG to discourse. Cognitive Science, 28:751?779.
38
Proceedings of ACL-08: HLT, pages 218?226,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Regular tree grammars as a formalism for scope underspecification
Alexander Koller?
a.koller@ed.ac.uk
? University of Edinburgh
Michaela Regneri? ?
regneri@coli.uni-sb.de
? University of Groningen
Stefan Thater?
stth@coli.uni-sb.de
? Saarland University
Abstract
We propose the use of regular tree grammars
(RTGs) as a formalism for the underspecified
processing of scope ambiguities. By applying
standard results on RTGs, we obtain a novel
algorithm for eliminating equivalent readings
and the first efficient algorithm for computing
the best reading of a scope ambiguity. We also
show how to derive RTGs from more tradi-
tional underspecified descriptions.
1 Introduction
Underspecification (Reyle, 1993; Copestake et al,
2005; Bos, 1996; Egg et al, 2001) has become the
standard approach to dealing with scope ambiguity
in large-scale hand-written grammars (see e.g. Cope-
stake and Flickinger (2000)). The key idea behind
underspecification is that the parser avoids comput-
ing all scope readings. Instead, it computes a single
compact underspecified description for each parse.
One can then strengthen the underspecified descrip-
tion to efficiently eliminate subsets of readings that
were not intended in the given context (Koller and
Niehren, 2000; Koller and Thater, 2006); so when
the individual readings are eventually computed, the
number of remaining readings is much smaller and
much closer to the actual perceived ambiguity of the
sentence.
In the past few years, a ?standard model? of scope
underspecification has emerged: A range of for-
malisms from Underspecified DRT (Reyle, 1993)
to dominance graphs (Althaus et al, 2003) have
offered mechanisms to specify the ?semantic mate-
rial? of which the semantic representations are built
up, plus dominance or outscoping relations between
these building blocks. This has been a very suc-
cessful approach, but recent algorithms for elimi-
nating subsets of readings have pushed the expres-
sive power of these formalisms to their limits; for
instance, Koller and Thater (2006) speculate that
further improvements over their (incomplete) redun-
dancy elimination algorithm require a more expres-
sive formalism than dominance graphs. On the theo-
retical side, Ebert (2005) has shown that none of
the major underspecification formalisms are expres-
sively complete, i.e. supports the description of an
arbitrary subset of readings. Furthermore, the some-
what implicit nature of dominance-based descrip-
tions makes it difficult to systematically associate
readings with probabilities or costs and then com-
pute a best reading.
In this paper, we address both of these shortcom-
ings by proposing regular tree grammars (RTGs)
as a novel underspecification formalism. Regular
tree grammars (Comon et al, 2007) are a standard
approach for specifying sets of trees in theoretical
computer science, and are closely related to regu-
lar tree transducers as used e.g. in recent work on
statistical MT (Knight and Graehl, 2005) and gram-
mar formalisms (Shieber, 2006). We show that the
?dominance charts? proposed by Koller and Thater
(2005b) can be naturally seen as regular tree gram-
mars; using their algorithm, classical underspecified
descriptions (dominance graphs) can be translated
into RTGs that describe the same sets of readings.
However, RTGs are trivially expressively complete
because every finite tree language is also regular. We
exploit this increase in expressive power in present-
ing a novel redundancy elimination algorithm that is
simpler and more powerful than the one by Koller
and Thater (2006); in our algorithm, redundancy
elimination amounts to intersection of regular tree
languages. Furthermore, we show how to define a
PCFG-style cost model on RTGs and compute best
readings of deterministic RTGs efficiently, and illus-
trate this model on a machine learning based model
218
of scope preferences (Higgins and Sadock, 2003).
To our knowledge, this is the first efficient algorithm
for computing best readings of a scope ambiguity in
the literature.
The paper is structured as follows. In Section 2,
we will first sketch the existing standard approach
to underspecification. We will then define regular
tree grammars and show how to see them as an un-
derspecification formalism in Section 3. We will
present the new redundancy elimination algorithm,
based on language intersection, in Section 4, and
show how to equip RTGs with weights and compute
best readings in Section 5. We conclude in Section 6.
2 Underspecification
The key idea behind scope underspecification is to
describe all readings of an ambiguous expression
with a single, compact underspecified representation
(USR). This simplifies semantics construction, and
current algorithms (Koller and Thater, 2005a) sup-
port the efficient enumeration of readings from an
USR when it is necessary. Furthermore, it is possible
to perform certain semantic processing tasks such
as eliminating redundant readings (see Section 4) di-
rectly on the level of underspecified representations
without explicitly enumerating individual readings.
Under the ?standard model? of scope underspeci-
fication, readings are considered as formulas or trees.
USRs specify the ?semantic material? common to
all readings, plus dominance or outscopes relations
between these building blocks. In this paper, we con-
sider dominance graphs (Egg et al, 2001; Althaus
et al, 2003) as one representative of this class. An
example dominance graph is shown on the left of
Fig. 1. It represents the five readings of the sentence
?a representative of a company saw every sample.?
The (directed, labelled) graph consists of seven sub-
trees, or fragments, plus dominance edges relating
nodes of these fragments. Each reading is encoded
as one configuration of the dominance graph, which
can be obtained by ?plugging? the tree fragments
into each other, in a way that respects the dominance
edges: The source node of each dominance edge
must dominate (i.e., be an ancestor of) the target
node in each configuration. The trees in Fig. 1a?e
are the five configurations of the example graph.
An important class of dominance graphs are hy-
pernormally connected dominance graphs, or dom-
inance nets (Niehren and Thater, 2003). The pre-
cise definition of dominance nets is not important
here, but note that virtually all underspecified de-
scriptions that are produced by current grammars are
nets (Flickinger et al, 2005). For the rest of the pa-
per, we restrict ourselves to dominance graphs that
are hypernormally connected.
3 Regular tree grammars
We will now recall the definition of regular tree
grammars and show how they can be used as an un-
derspecification formalism.
3.1 Definition
Let ? be an alphabet, or signature, of tree construc-
tors { f ,g,a, . . .}, each of which is equipped with an
arity ar( f )? 0. A finite constructor tree t is a finite
tree in which each node is labelled with a symbol of
?, and the number of children of the node is exactly
the arity of this symbol. For instance, the configura-
tions in Fig. 1a-e are finite constructor trees over the
signature {ax|2,ay|2,compz|0, . . .}. Finite construc-
tor trees can be seen as ground terms over ? that
respect the arities. We write T (?) for the finite con-
structor trees over ?.
A regular tree grammar (RTG) is a 4-tuple G =
(S,N,?,R) consisting of a nonterminal alphabet N,
a terminal alphabet ?, a start symbol S ? N, and a
finite set of production rules R of the form A? ? ,
where A ? N and ? ? T (??N); the nonterminals
count as zero-place constructors. Two finite con-
structor trees t, t ? ? T (? ? N) stand in the deriva-
tion relation, t ?G t ?, if t ? can be built from t by
replacing an occurrence of some nonterminal A by
the tree on the right-hand side of some production
for A. The language generated by G, L(G), is the set
{t ? T (?) | S??G t}, i.e. all terms of terminal sym-
bols that can be derived from the start symbol by a
sequence of rule applications. Note that L(G) is a
possibly infinite language of finite trees. As usual,
we write A? t1 | . . . | tn as shorthand for the n pro-
duction rules A? ti (1 ? i ? n). See Comon et al
(2007) for more details.
The languages that can be accepted by regular tree
grammars are called regular tree languages (RTLs),
and regular tree grammars are equivalent to regular
219
every
y
sample
y
see
x,y
a
x
repr-of
x,z
a
z
comp
z
12 3
4 5 6
7
every
y
a
x
sample
y
see
x,y
repr-of
x,z
a
z
comp
z
(a)
every
y
a
z
a
x
sample
y
see
x,y
comp
z
repr-of
x,z
(c)
every
y
a
z
a
x
sample
y
see
x,y
comp
z
repr-of
x,z
(d)(b)
every
y
sample
y
see
x,y
a
x
repr-of
x,z
a
z
comp
z
(e)
every
y
sample
y
a
x
repr-of
x,z
see
x,y
a
z
comp
z
Figure 1: A dominance graph (left) and its five configurations.
tree automata, which are defined essentially like the
well-known regular string automata, except that they
assign states to the nodes in a tree rather than the po-
sitions in a string. Tree automata are related to tree
transducers as used e.g. in statistical machine trans-
lation (Knight and Graehl, 2005) exactly like finite-
state string automata are related to finite-state string
transducers, i.e. they use identical mechanisms to ac-
cept rather than transduce languages. Many theoreti-
cal results carry over from regular string languages
to regular tree languages; for instance, membership
of a tree in a RTL can be decided in linear time,
RTLs are closed under intersection, union, and com-
plement, and so forth.
3.2 Regular tree grammars in
underspecification
We can now use regular tree grammars in underspeci-
fication by representing the semantic representations
as trees and taking an RTG G as an underspecified
description of the trees in L(G). For example, the
five configurations in Fig. 1 can be represented as
the tree language accepted by the following gram-
mar with start symbol S.
S ? ax(A1,A2) | az(B1,A3) | everyy(B3,A4)
A1 ? az(B1,B2)
A2 ? everyy(B3,B4)
A3 ? ax(B2,A2) | everyy(B3,A5)
A4 ? ax(A1,B4) | az(B1,A5)
A5 ? ax(B2,B4)
B1 ? compz B2 ? repr-ofx,z
B3 ? sampley B4 ? seex,y
More generally, every finite set of trees can be
written as the tree language accepted by a non-
recursive regular tree grammar such as this. This
grammar can be much smaller than the set of trees,
because nonterminal symbols (which stand for sets
of possibly many subtrees) can be used on the right-
hand sides of multiple rules. Thus an RTG is a com-
pact representation of a set of trees in the same way
that a parse chart is a compact representation of the
set of parse trees of a context-free string grammar.
Note that each tree can be enumerated from the RTG
in linear time.
3.3 From dominance graphs to tree grammars
Furthermore, regular tree grammars can be system-
atically computed from more traditional underspeci-
fied descriptions. Koller and Thater (2005b) demon-
strate how to compute a dominance chart from a
dominance graph D by tabulating how a subgraph
can be decomposed into smaller subgraphs by re-
moving what they call a ?free fragment?. If D is
hypernormally connected, this chart can be read as
a regular tree grammar whose nonterminal symbols
are subgraphs of the dominance graph, and whose
terminal symbols are names of fragments. For the
example graph in Fig. 1, it looks as follows.
{1,2,3,4,5,6,7} ? 1({2,4,5},{3,6,7})
{1,2,3,4,5,6,7} ? 2({4},{1,3,5,6,7})
{1,2,3,4,5,6,7} ? 3({6},{1,2,4,5,7})
{1,3,5,6,7} ? 1({5},{3,6,7}) | 3({6},{1,5,7})
{1,2,4,5,7} ? 1({2,4,5},{7}) | 2({4},{1,5,7})
{1,5,7} ? 1({5},{7})
{2,4,5} ? 2({4},{5}) {4} ? 4 {6}? 6
{3,6,7} ? 3({6},{7}) {5} ? 5 {7}? 7
This grammar accepts, again, five different trees,
whose labels are the node names of the dominance
graph, for instance 1(2(4,5),3(6,7)). If f : ?? ??
is a relabelling function from one terminal alpha-
bet to another, we can write f (G) for the grammar
(S,N,??,R?), where R? = {A ? f (a)(B1, . . . ,Bn) |
A? a(B1, . . . ,Bn) ? R}. Now if we choose f to be
the labelling function of D (which maps node names
to node labels) and G is the chart of D, then L( f (G))
will be the set of configurations of D. The grammar
in Section 3.2 is simply f (G) for the chart above (up
to consistent renaming of nonterminals).
In the worst case, the dominance chart of a dom-
inance graph with n fragments has O(2n) produc-
tion rules (Koller and Thater, 2005b), i.e. charts may
be exponential in size; but note that this is still an
220
1,0E+00
1,0E+04
1,0E+08
1,0E+12
1,0E+16
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
#fragments
#
c
o
n
f
i
g
u
r
a
t
i
o
n
s
/
r
u
l
e
s
0
10
20
30
40
50
60
70
80
#
s
e
n
t
e
n
c
e
s
#sentences
#production rules in chart
#configurations
Figure 2: Chart sizes in the Rondane corpus.
improvement over the n! configurations that these
worst-case examples have. In practice, RTGs that
are computed by converting the USR computed by a
grammar remain compact: Fig. 2 compares the aver-
age number of configurations and the average num-
ber of RTG production rules for USRs of increasing
sizes in the Rondane treebank (see Sect. 4.3); the
bars represent the number of sentences for USRs of a
certain size. Even for the most ambiguous sentence,
which has about 4.5?1012 scope readings, the domi-
nance chart has only about 75 000 rules, and it takes
only 15 seconds on a modern consumer PC (Intel
Core 2 Duo at 2 GHz) to compute the grammar from
the graph. Computing the charts for all 999 MRS-
nets in the treebank takes about 45 seconds.
4 Expressive completeness and
redundancy elimination
Because every finite tree language is regular, RTGs
constitute an expressively complete underspecifica-
tion formalism in the sense of Ebert (2005): They
can represent arbitrary subsets of the original set of
readings. Ebert shows that the classical dominance-
based underspecification formalisms, such as MRS,
Hole Semantics, and dominance graphs, are all
expressively incomplete, which Koller and Thater
(2006) speculate might be a practical problem for al-
gorithms that strengthen USRs to remove unwanted
readings. We will now show how both the expres-
sive completeness and the availability of standard
constructions for RTGs can be exploited to get an
improved redundancy elimination algorithm.
4.1 Redundancy elimination
Redundancy elimination (Vestre, 1991; Chaves,
2003; Koller and Thater, 2006) is the problem of de-
riving from an USR U another USR U ?, such that
the readings of U ? are a proper subset of the read-
ings of U , but every reading in U is semantically
equivalent to some reading in U ?. For instance, the
following sentence from the Rondane treebank is an-
alyzed as having six quantifiers and 480 readings by
the ERG grammar; these readings fall into just two
semantic equivalence classes, characterized by the
relative scope of ?the lee of? and ?a small hillside?.
A redundancy elimination would therefore ideally re-
duce the underspecified description to one that has
only two readings (one for each class).
(1) We quickly put up the tents in the lee of a
small hillside and cook for the first time in the
open. (Rondane 892)
Koller and Thater (2006) define semantic equiva-
lence in terms of a rewrite system that specifies un-
der what conditions two quantifiers may exchange
their positions without changing the meaning of the
semantic representation. For example, if we assume
the following rewrite system (with just a single rule),
the five configurations in Fig. 1a-e fall into three
equivalence classes ? indicated by the dotted boxes
around the names a-e ? because two pairs of read-
ings can be rewritten into each other.
(2) ax(az(P,Q),R)? az(P,ax(Q,R))
Based on this definition, Koller and Thater (2006)
present an algorithm (henceforth, KT06) that deletes
rules from a dominance chart and thus removes sub-
sets of readings from the USR. The KT06 algorithm
is fast and quite effective in practice. However, it es-
sentially predicts for each production rule of a dom-
inance chart whether each configuration that can be
built with this rule is equivalent to a configuration
that can be built with some other production for the
same subgraph, and is therefore rather complex.
4.2 Redundancy elimination as language
intersection
We now define a new algorithm for redundancy elim-
ination. It is based on the intersection of regular tree
languages, and will be much simpler and more pow-
erful than KT06.
Let G = (S,N,?,R) be an RTG with a linear or-
der on the terminals ?; for ease of presentation, we
assume ? ? N. Furthermore, let f : ?? ?? be a re-
labelling function into the signature ?? of the rewrite
221
system. For example, G could be the dominance
chart of some dominance graph D, and f could be
the labelling function of D.
We can then define a tree language LF as follows:
LF contains all trees over ? that do not contain a sub-
tree of the form q1(x1, . . . ,xi?1,q2(. . .),xi+1, . . . ,xk)
where q1 > q2 and the rewrite system contains a rule
that has f (q1)(X1, . . . ,Xi?1, f (q2)(. . .),Xi+1, . . . ,Xk)
on the left or right hand side. LF is a regular tree lan-
guage, and can be accepted by a regular tree gram-
mar GF with O(n) nonterminals and O(n2) rules,
where n = |??|. A filter grammar for Fig. 1 looks
as follows:
S ? 1(S,S) | 2(S,Q1) | 3(S,S) | 4 | . . . | 7
Q1 ? 2(S,Q1) | 3(S,S) | 4 | . . . | 7
This grammar accepts all trees over ? except ones
in which a node with label 2 is the parent of a node
with label 1, because such trees correspond to config-
urations in which a node with label az is the parent of
a node with label ax, az and ax are permutable, and
2 > 1. In particular, it will accept the configurations
(b), (c), and (e) in Fig. 1, but not (a) or (d).
Since regular tree languages are closed under in-
tersection, we can compute a grammar G? such that
L(G?) = L(G)?LF . This grammar has O(nk) nonter-
minals and O(n2k) productions, where k is the num-
ber of production rules in G, and can be computed
in time O(n2k). The relabelled grammar f (G?) ac-
cepts all trees in which adjacent occurrences of per-
mutable quantifiers are in a canonical order (sorted
from lowest to highest node name). For example, the
grammar G? for the example looks as follows; note
that the nonterminal alphabet of G? is the product of
the nonterminal alphabets of G and GF .
{1,2,3,4,5,6,7}S ? 1({2,4,5}S,{3,6,7}S)
{1,2,3,4,5,6,7}S ? 2({4}S,{1,3,5,6,7}Q1)
{1,2,3,4,5,6,7}S ? 3({6}S,{1,2,4,5,7}S)
{1,3,5,6,7}Q1 ? 3({6}S,{1,5,7}S)
{1,2,4,5,7}S ? 1({2,4,5}S,{7}S)
{1,2,4,5,7}S ? 2({4}S,{1,5,7}Q1)
{2,4,5}S ? 2({4}S,{5}Q1) {4}S ? 4
{3,6,7}S ? 3({6}S,{7}S) {5}S ? 5
{1,5,7}S ? 1({5}S,{7}S) {5}Q1 ? 5
{6}S ? 6 {7}S ? 7
Significantly, the grammar contains no produc-
tions for {1,3,5,6,7}Q1 with terminal symbol 1, and
no production for {1,5,7}Q1 . This reduces the tree
language accepted by f (G?) to just the configura-
tions (b), (c), and (e) in Fig. 1, i.e. exactly one
representative of every equivalence class. Notice
that there are two different nonterminals, {5}Q1 and
{5}S, corresponding to the subgraph {5}, so the in-
tersected RTG is not a dominance chart any more.
As we will see below, this increased expressivity in-
creases the power of the redundancy elimination al-
gorithm.
4.3 Evaluation
The algorithm presented here is not only more trans-
parent than KT06, but also more powerful; for exam-
ple, it will reduce the graph in Fig. 4 of Koller and
Thater (2006) completely, whereas KT06 won?t.
To measure the extent to which the new algo-
rithm improves upon KT06, we compare both algo-
rithms on the USRs in the Rondane treebank (ver-
sion of January 2006). The Rondane treebank is a
?Redwoods style? treebank (Oepen et al, 2002) con-
taining MRS-based underspecified representations
for sentences from the tourism domain, and is dis-
tributed together with the English Resource Gram-
mar (ERG) (Copestake and Flickinger, 2000).
The treebank contains 999 MRS-nets, which we
translate automatically into dominance graphs and
further into RTGs; the median number of scope read-
ings per sentence is 56. For our experiment, we con-
sider all 950 MRS-nets with less than 650 000 con-
figurations. We use a slightly weaker version of the
rewrite system that Koller and Thater (2006) used in
their evaluation.
It turns out that the median number of equivalence
classes, computed by pairwise comparison of all con-
figurations, is 8. The median number of configu-
rations that remain after running our algorithm is
also 8. By contrast, the median number after run-
ning KT06 is 11. For a more fine-grained compari-
son, Fig. 3 shows the percentage of USRs for which
the two algorithms achieve complete reduction, i.e.
retain only one reading per equivalence class. In the
diagram, we have grouped USRs according to the
natural logarithm of their numbers of configurations,
and report the percentage of USRs in this group on
which the algorithms were complete. The new algo-
rithm dramatically outperforms KT06: In total, it re-
duces 96% of all USRs completely, whereas KT06
was complete only for 40%. This increase in com-
pleteness is partially due to the new algorithm?s abil-
ity to use non-chart RTGs: For 28% of the sentences,
222
0%
20%
40%
60%
80%
100%
1 3 5 7 9 11 13
KT06 RTG
Figure 3: Percentage of USRs in Rondane for which the
algorithms achieve complete reduction.
it computes RTGs that are not dominance charts.
KT06 was only able to reduce 5 of these 263 graphs
completely.
The algorithm needs 25 seconds to run for the
entire corpus (old algorithm: 17 seconds), and it
would take 50 (38) more seconds to run on the 49
large USRs that we exclude from the experiment.
By contrast, it takes about 7 hours to compute the
equivalence classes by pairwise comparison, and it
would take an estimated several billion years to com-
pute the equivalence classes of the excluded USRs.
In short, the redundancy elimination algorithm pre-
sented here achieves nearly complete reduction at a
tiny fraction of the runtime, and makes a useful task
that was completely infeasible before possible.
4.4 Compactness
Finally, let us briefly consider the ramifications of
expressive completeness on efficiency. Ebert (2005)
proves that no expressively complete underspecifi-
cation formalism can be compact, i.e. in the worst
case, the USR of a set of readings become exponen-
tially large in the number of scope-bearing operators.
In the case of RTGs, this worst case is achieved by
grammars of the form S? t1 | . . . | tn, where t1, . . . , tn
are the trees we want to describe. This grammar is as
big as the number of readings, i.e. worst-case expo-
nential in the number n of scope-bearing operators,
and essentially amounts to a meta-level disjunction
over the readings.
Ebert takes the incompatibility between compact-
ness and expressive completeness as a fundamental
problem for underspecification. We don?t see things
quite as bleakly. Expressions of natural language it-
self are (extremely underspecified) descriptions of
sets of semantic representations, and so Ebert?s ar-
gument applies to NL expressions as well. This
means that describing a given set of readings may
require an exponentially long discourse. Ebert?s def-
inition of compactness may be too harsh: An USR,
although exponential-size in the number of quanti-
fiers, may still be polynomial-size in the length of
the discourse in the worst case.
Nevertheless, the tradeoff between compactness
and expressive power is important for the design
of underspecification formalisms, and RTGs offer a
unique answer. They are expressively complete; but
as we have seen in Fig. 2, the RTGs that are derived
by semantic construction are compact, and even in-
tersecting them with filter grammars for redundancy
elimination only blows up their sizes by a factor of
O(n2). As we add more and more information to
an RTG to reduce the set of readings, ultimately to
those readings that were meant in the actual context
of the utterance, the grammar will become less and
less compact; but this trend is counterbalanced by
the overall reduction in the number of readings. For
the USRs in Rondane, the intersected RTGs are, on
average, 6% smaller than the original charts. Only
30% are larger than the charts, by a maximal factor
of 3.66. Therefore we believe that the theoretical
non-compactness should not be a major problem in
a well-designed practical system.
5 Computing best configurations
A second advantage of using RTGs as an under-
specification formalism is that we can apply exist-
ing algorithms for computing the best derivations
of weighted regular tree grammars to compute best
(that is, cheapest or most probable) configurations.
This gives us the first efficient algorithm for comput-
ing the preferred reading of a scope ambiguity.
We define weighted dominance graphs and
weighted tree grammars, show how to translate the
former into the latter and discuss an example.
5.1 Weighted dominance graphs
A weighted dominance graph D = (V,ET unionmulti ED unionmulti
WDunionmultiWI) is a dominance graph with two new types
of edges ? soft dominance edges, WD, and soft dis-
jointness edges, WI ?, each of which is equipped
with a numeric weight. Soft dominance and dis-
jointness edges provide a mechanism for assigning
weights to configurations; a soft dominance edge ex-
223
every
y
sample
y
see
x,y
a
x
repr-of
x,z
a
z
comp
z
1
2
3
4 5 6
7
9
8
Figure 4: The graph of Fig. 1 with soft constraints
presses a preference that two nodes dominate each
other in a configuration, whereas a soft disjointness
edge expresses a preference that two nodes are dis-
joint, i.e. neither dominates the other.
We take the hard backbone of D to be the ordinary
dominance graph B(D) = (V,ET unionmultiED) obtained by
removing all soft edges. The set of configurations
of a weighted graph D is the set of configurations
of its hard backbone. For each configuration t of
D, we define the weight c(t) to be the product of
the weights of all soft dominance and disjointness
edges that are satisfied in t. We can then ask for
configurations of maximal weight.
Weighted dominance graphs can be used to en-
code the standard models of scope preferences
(Pafel, 1997; Higgins and Sadock, 2003). For exam-
ple, Higgins and Sadock (2003) present a machine
learning approach for determining pairwise prefer-
ences as to whether a quantifier Q1 dominates an-
other quantifier Q2, Q2 dominates Q1, or neither (i.e.
they are disjoint). We can represent these numbers
as the weights of soft dominance and disjointness
edges. An example (with artificial weights) is shown
in Fig. 4; we draw the soft dominance edges as
curved dotted arrows and the soft disjointness edges
as as angled double-headed arrows. Each soft edge
is annotated with its weight. The hard backbone
of this dominance graph is our example graph from
Fig. 1, so it has the same five configurations. The
weighted graph assigns a weight of 8 to configura-
tion (a), a weight of 1 to (d), and a weight of 9 to (e);
this is also the configuration of maximum weight.
5.2 Weighted tree grammars
In order to compute the maximal-weight configura-
tion of a weighted dominance graph, we will first
translate it into a weighted regular tree grammar. A
weighted regular tree grammar (wRTG) (Graehl and
Knight, 2004) is a 5-tuple G = (S,N,?,R,c) such
that G? = (S,N,?,R) is a regular tree grammar and
c : R? R is a function that assigns each production
rule a weight. G accepts the same language of trees
as G?. It assigns each derivation a cost equal to the
product of the costs of the production rules used in
this derivation, and it assigns each tree in the lan-
guage a cost equal to the sum of the costs of its
derivations. Thus wRTGs define weights in a way
that is extremely similar to PCFGs, except that we
don?t require any weights to sum to one.
Given a weighted, hypernormally connected dom-
inance graph D, we can extend the chart of B(D) to
a wRTG by assigning rule weights as follows: The
weight of a rule D0 ? i(D1, . . . ,Dn) is the product
over the weights of all soft dominance and disjoint-
ness edges that are established by this rule. We say
that a rule establishes a soft dominance edge from
u to v if u = i and v is in one of the subgraphs
D1, . . . ,Dn; we say that it establishes a soft disjoint-
ness edge between u and v if u and v are in different
subgraphs D j and Dk ( j 6= k). It can be shown that
the weight this grammar assigns to each derivation
is equal to the weight that the original dominance
graph assigns to the corresponding configuration.
If we apply this construction to the example graph
in Fig. 4, we obtain the following wRTG:
{1, ...,7} ? ax({2,4,5},{3,6,7}) [9]
{1, ...,7} ? az({4},{1,3,5,6,7}) [1]
{1, ...,7} ? everyy({6},{1,2,4,5,7}) [8]
{2,4,5} ? az({4},{5}) [1]
{3,6,7} ? everyy({6},{7}) [1]
{1,3,5,6,7} ? ax({5},{3,6,7}) [1]
{1,3,5,6,7} ? everyy({6},{1,5,7}) [8]
{1,2,4,5,7} ? ax({2,4,5},{7}) [1]
{1,2,4,5,7} ? az({4},{1,5,7}) [1]
{1,5,7} ? ax({5},{7}) [1]
{4} ? compz [1] {5} ? repr?o f x,z [1]
{6} ? sampley [1] {7} ? seex,y [1]
For example, picking ?az? as the root of a con-
figuration (Fig. 1 (c), (d)) of the entire graph has
a weight of 1, because this rule establishes no soft
edges. On the other hand, choosing ?ax? as the root
has a weight of 9, because this establishes the soft
disjointness edge (and in fact, leads to the derivation
of the maximum-weight configuration in Fig. 1 (e)).
5.3 Computing the best configuration
The problem of computing the best configuration of
a weighted dominance graph ? or equivalently, the
224
best derivation of a weighted tree grammar ? can
now be solved by standard algorithms for wRTGs.
For example, Knight and Graehl (2005) present an
algorithm to extract the best derivation of a wRTG in
time O(t + n logn) where n is the number of nonter-
minals and t is the number of rules. In practice, we
can extract the best reading of the most ambiguous
sentence in the Rondane treebank (4.5? 1012 read-
ings, 75 000 grammar rules) with random soft edges
in about a second.
However, notice that this is not the same problem
as computing the best tree in the language accepted
by a wRTG, as trees may have multiple deriva-
tions. The problem of computing the best tree is NP-
complete (Sima?an, 1996). However, if the weighted
regular tree automaton corresponding to the wRTG
is deterministic, every tree has only one derivation,
and thus computing best trees becomes easy again.
The tree automata for dominance charts are always
deterministic, and the automata for RTGs as in Sec-
tion 3.2 (whose terminals correspond to the graph?s
node labels) are also typically deterministic if the
variable names are part of the quantifier node labels.
Furthermore, there are algorithms for determinizing
weighted tree automata (Borchardt and Vogler, 2003;
May and Knight, 2006), which could be applied as
preprocessing steps for wRTGs.
6 Conclusion
In this paper, we have shown how regular tree gram-
mars can be used as a formalism for scope under-
specification, and have exploited the power of this
view in a novel, simpler, and more complete algo-
rithm for redundancy elimination and the first effi-
cient algorithm for computing the best reading of a
scope ambiguity. In both cases, we have adapted
standard algorithms for RTGs, which illustrates the
usefulness of using such a well-understood formal-
ism. In the worst case, the RTG for a scope ambigu-
ity is exponential in the number of scope bearers in
the sentence; this is a necessary consequence of their
expressive completeness. However, those RTGs that
are computed by semantic construction and redun-
dancy elimination remain compact.
Rather than showing how to do semantic construc-
tion for RTGs, we have presented an algorithm that
computes RTGs from more standard underspecifica-
tion formalisms. We see RTGs as an ?underspecifi-
cation assembly language? ? they support efficient
and useful algorithms, but direct semantic construc-
tion may be inconvenient, and RTGs will rather be
obtained by ?compiling? higher-level underspecified
representations such as dominance graphs or MRS.
This perspective also allows us to establish a
connection to approaches to semantic construc-
tion which use chart-based packing methods rather
than dominance-based underspecification to manage
scope ambiguities. For instance, both Combinatory
Categorial Grammars (Steedman, 2000) and syn-
chronous grammars (Nesson and Shieber, 2006) rep-
resent syntactic and semantic ambiguity as part of
the same parse chart. These parse charts can be
seen as regular tree grammars that accept the lan-
guage of parse trees, and conceivably an RTG that
describes only the semantic and not the syntactic
ambiguity could be automatically extracted. We
could thus reconcile these completely separate ap-
proaches to semantic construction within the same
formal framework, and RTG-based algorithms (e.g.,
for redundancy elimination) would apply equally to
dominance-based and chart-based approaches. In-
deed, for one particular grammar formalism it has
even been shown that the parse chart contains an
isomorphic image of a dominance chart (Koller and
Rambow, 2007).
Finally, we have only scratched the surface of
what can be be done with the computation of best
configurations in Section 5. The algorithms gen-
eralize easily to weights that are taken from an ar-
bitrary ordered semiring (Golan, 1999; Borchardt
and Vogler, 2003) and to computing minimal-weight
rather than maximal-weight configurations. It is also
useful in applications beyond semantic construction,
e.g. in discourse parsing (Regneri et al, 2008).
Acknowledgments. We have benefited greatly
from fruitful discussions on weighted tree grammars
with Kevin Knight and Jonathan Graehl, and on dis-
course underspecification with Markus Egg. We
also thank Christian Ebert, Marco Kuhlmann, Alex
Lascarides, and the reviewers for their comments on
the paper. Finally, we are deeply grateful to our for-
mer colleague Joachim Niehren, who was a great fan
of tree automata before we even knew what they are.
225
References
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn,
J. Niehren, and S. Thiel. 2003. An efficient graph
algorithm for dominance constraints. J. Algorithms,
48:194?219.
B. Borchardt and H. Vogler. 2003. Determinization of
finite state weighted tree automata. Journal of Au-
tomata, Languages and Combinatorics, 8(3):417?463.
J. Bos. 1996. Predicate logic unplugged. In Proceedings
of the Tenth Amsterdam Colloquium, pages 133?143.
R. P. Chaves. 2003. Non-redundant scope disambigua-
tion in underspecified semantics. In Proceedings of
the 8th ESSLLI Student Session, pages 47?58, Vienna.
H. Comon, M. Dauchet, R. Gilleron, C. Lo?ding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tommasi.
2007. Tree automata techniques and applications.
Available on: http://www.grappa.univ-lille3.fr/tata.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Confer-
ence on Language Resources and Evaluation.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag. 2005.
Minimal recursion semantics: An introduction. Re-
search on Language and Computation, 3:281?332.
C. Ebert. 2005. Formal investigations of underspecified
representations. Ph.D. thesis, King?s College, Lon-
don.
M. Egg, A. Koller, and J. Niehren. 2001. The Constraint
Language for Lambda Structures. Logic, Language,
and Information, 10:457?485.
D. Flickinger, A. Koller, and S. Thater. 2005. A new
well-formedness criterion for semantics debugging. In
Proceedings of the 12th HPSG Conference, Lisbon.
J. S. Golan. 1999. Semirings and their applications.
Kluwer, Dordrecht.
J. Graehl and K. Knight. 2004. Training tree transducers.
In HLT-NAACL 2004, Boston.
D. Higgins and J. Sadock. 2003. A machine learning ap-
proach to modeling scope preferences. Computational
Linguistics, 29(1).
K. Knight and J. Graehl. 2005. An overview of proba-
bilistic tree transducers for natural language process-
ing. In Computational linguistics and intelligent text
processing, pages 1?24. Springer.
A. Koller and J. Niehren. 2000. On underspecified
processing of dynamic semantics. In Proceedings of
COLING-2000, Saarbru?cken.
A. Koller and O. Rambow. 2007. Relating dominance
formalisms. In Proceedings of the 12th Conference on
Formal Grammar, Dublin.
A. Koller and S. Thater. 2005a. Efficient solving and
exploration of scope ambiguities. Proceedings of the
ACL-05 Demo Session.
A. Koller and S. Thater. 2005b. The evolution of dom-
inance constraint solvers. In Proceedings of the ACL-
05 Workshop on Software.
A. Koller and S. Thater. 2006. An improved redundancy
elimination algorithm for underspecified descriptions.
In Proceedings of COLING/ACL-2006, Sydney.
J. May and K. Knight. 2006. A better n-best list: Prac-
tical determinization of weighted finite tree automata.
In Proceedings of HLT-NAACL.
R. Nesson and S. Shieber. 2006. Simpler TAG semantics
through synchronization. In Proceedings of the 11th
Conference on Formal Grammar.
J. Niehren and S. Thater. 2003. Bridging the gap be-
tween underspecification formalisms: Minimal recur-
sion semantics as dominance constraints. In Proceed-
ings of ACL 2003.
S. Oepen, K. Toutanova, S. Shieber, C. Manning,
D. Flickinger, and T. Brants. 2002. The LinGO Red-
woods treebank: Motivation and preliminary applica-
tions. In Proceedings of the 19th International Con-
ference on Computational Linguistics (COLING?02),
pages 1253?1257.
J. Pafel. 1997. Skopus und logische Struktur: Studien
zum Quantorenskopus im Deutschen. Habilitationss-
chrift, Eberhard-Karls-Universita?t Tu?bingen.
M. Regneri, M. Egg, and A. Koller. 2008. Efficient pro-
cessing of underspecified discourse representations. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT) ? Short Papers,
Columbus, Ohio.
U. Reyle. 1993. Dealing with ambiguities by underspec-
ification: Construction, representation and deduction.
Journal of Semantics, 10(1).
S. Shieber. 2006. Unifying synchronous tree-adjoining
grammars and tree transducers via bimorphisms. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), Trento, Italy.
K. Sima?an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree-grammars. In
Proceedings of the 16th conference on Computational
linguistics, pages 1175?1180, Morristown, NJ, USA.
Association for Computational Linguistics.
M. Steedman. 2000. The syntactic process. MIT Press.
E. Vestre. 1991. An algorithm for generating non-
redundant quantifier scopings. In Proc. of EACL,
pages 251?256, Berlin.
226
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 245?248,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Efficient Processing of Underspecified Discourse Representations
Michaela Regneri? ?
regneri@coli.uni-sb.de
? Saarland University
Markus Egg?
egg@let.rug.nl
? University of Groningen
Alexander Koller?
a.koller@ed.ac.uk
? University of Edinburgh
Abstract
Underspecification-based algorithms for pro-
cessing partially disambiguated discourse
structure must cope with extremely high num-
bers of readings. Based on previous work on
dominance graphs and weighted tree gram-
mars, we provide the first possibility for com-
puting an underspecified discourse description
and a best discourse representation efficiently
enough to process even the longest discourses
in the RST Discourse Treebank.
1 Introduction
Discourse processing has emerged as a highly rele-
vant source of information for applications such as
information extraction and automatic summarisation
(Taboada and Mann (2006) outline this and further
applications). But discourse structures cannot al-
ways be described completely, either due to genuine
ambiguity (Stede, 2004) or to the limitations of a
discourse parser. In either case, only partial infor-
mation on discourse structure is available. To han-
dle such information, underspecification formalisms
can be used. Underspecification was originally in-
troduced in computational semantics to model struc-
tural ambiguity without disjunctively enumerating
the readings, and later applied to discourse parsing
(Gardent and Webber, 1998; Schilder, 2002).
However, while the existing algorithms for un-
derspecification processing work well for seman-
tic structures, they were not designed for discourse
structures, which can be much larger. Indeed, it
has never been shown that underspecified discourse
reprentations (UDRs) can be processed efficiently,
since the general-purpose implementations are too
slow for that task.
In this paper, we present a new way to imple-
ment and process discourse underspecification in
terms of regular tree grammars (RTGs). RTGs are
used as an underspecification formalism in seman-
tics (Koller et al, 2008). We show how to compute
RTGs for discourse from dominance-based under-
specified representations more efficiently (by a typ-
ical factor of 100) than before. Furthermore, we
show how weighted RTGs can be used to represent
constraints and preferences on the discourse struc-
ture. Taking all these results together, we show for
the first time how the globally optimal discourse rep-
resentation based on some preference model can be
computed efficiently from an UDR.
2 Underspecified Discourse Representation
Following annotation schemes like the one of Stede
(2004), we model discourse structures by binary
trees. Fig. (1b-f) represent the potential structures of
(1). We write each elementary discourse unit (EDU)
in square brackets.
(1) [C1 I try to read a novel] [C2 if I feel bored]
[C3 because the TV programs disappoint me]
[C4 but I can?t concentrate on anything.]
Underspecification formalisms such as domi-
nance graphs (Althaus et al, 2003) can model par-
tial information about such trees; see Fig. (1a) for
the underspecified discourse representation (UDR)
of (1). These graphs consist of labelled roots and
unlabelled holes; the solid edges indicate that a
node must be the parent of another, and the dashed
edges indicate (transitive) dominance requirements.
A configuration of a dominance graph is an arrange-
ment of the (labelled) graph nodes into a tree that
satisfies all (immediate and transitive) dominance
requirements. Subgraphs that are connected by solid
edges are called fragments and must be tree-shaped.
Using UDRs, discourse parsing can be modu-
larised into three separate steps. First, a discourse
parser segments the text and generates an UDR from
it. The node labels in the UDR aren?t necessarily
fully specified (Egg and Redeker, 2007; Schilder,
245
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
C
1
C
2
C
3
C
4
C
2
C
3
C
4
C
1
C
4
C
1
C
2
C
3
Condition
(1)
Condition
(1)
Condition
(1)
Condition
(1)
Cause
(2)Cause
(2)
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
Condition
(1)
Cause
(2)
Contrast
Contrast
Contrast
Condition
(1)
1
2
3 5
4
7
6
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
(a) (b) (c) (d) (e) (f)
Figure 1: An underspecified discourse structure and its five configurations
2002); here we pretend that they are to simplify the
presentation, as nothing in this paper hinges on it.
Then weights are added to the UDR that incorporate
preferences for discourse structures based on lin-
guistic cues. Finally, the weighted UDR can either
be processed directly by other applications, or, if a
tree structure is required, we can compute the best
configuration. In this paper, we show how an UDR
dominance graph can be converted into a regular tree
grammar efficiently. This simplifies the specifica-
tion of weights in Step 2; we also show how to ef-
ficiently compute a best tree from a weighted RTG
(Step 3). We do not discuss Step 1 in this paper.
The dominance graphs used in discourse under-
specification are constrained chains. A constrained
chain of length n consists of n upper fragments with
two holes each and n+ 1 lower fragments with no
holes. There must also be a numbering 1, . . . ,2n+1
of the fragments such that for every 1? i? n, frag-
ment 2i is an upper fragment, fragments 2i? 1 and
2i+1 are lower fragments, and there are dominance
edges from the left hole of 2i to the root of 2i?1 and
from the right hole of 2i to the root of 2i+ 1 (and
possibly further dominance edges). These numbers
are shown in circles in Fig. (1a). In discourse dom-
inance graphs, upper fragments correspond to dis-
course relations, and lower fragments correspond to
EDUs; the EDUs are ordered according to their ap-
pearance in the text, and the upper fragments con-
nect the two text spans to which they are adjacent.
3 Underspecified Processing for Discourses
Recently, Koller et al (2008) showed how to pro-
cess dominance graphs with regular tree grammars
(Comon et al, 2007, RTGs). RTGs are a grammar
formalism that describes sets of trees using produc-
tion rules which rewrite non-terminal symbols (NTs)
into terms consisting of tree constructors and possi-
bly further NTs. A tree (without NTs) is accepted
by the grammar if it can be derived by a sequence
of rule applications from a given start symbol. An
example RTG is shown in Fig. 2; its start symbol
is {1;7}, and it describes exactly the five trees in
{1;7} ? Cond({1},{3;7}) [1] {5;7} ? Contr({5},{7}) [1]
{3;7} ? Contr({3;5},{7}) [1] {3;5} ? Cause({3},{5}) [1]
{1;7} ? Contr({1;5},{7}) [1] {1;3} ? Cond({1},{3}) [5]
{1;7} ? Cause({1;3},{5;7}) [1] {1;5} ? Cond({1},{3;5}) [3]
{1;5} ? Cause({1;3},{5}) [1] {3;7} ? Cause({3},{5;7}) [1]
{1} ? C1 [1] {3} ? C2 [1] {5} ? C3 [1] {7} ? C4 [1]
Figure 2: A wRTG modelling Fig. 1
Fig. (1b-f). For example, Fig. (1e) is derived by ex-
panding the start symbol with the first rule in Fig. 2.
This determines that the tree root is labelled with
Condition; we then derive the left subtree from the
NT {1} and the right subtree from the NT {3;7}.
The NTs in the grammar correspond to subgraphs
in the dominance graph: The NT {1;7} repre-
sents the subgraph {1,2,3,4,5,6,7} (i.e. the whole
graph); the NT {1} represents the subgraph contain-
ing only the fragment 1; and so forth. The trees that
can be derived from each nonterminal correspond
exactly to the configurations of the subgraph.
Koller and Thater (2005b) presented an algorithm
for generating, from a very general class of dom-
inance graphs, an RTG that describes exactly the
same trees. For each subgraph S that is to be the
LHS of a rule, the algorithm determines the free
fragments of S, i.e. the fragments that may serve
as the root of one of its configurations, by a certain
graph algorithm. For every free fragment in S with
n holes and a root label f , the algorithm generates a
new rule of the form S? f (S1, . . . ,Sn), where each
Si corresponds to the remaining subgraph under the
i-th hole. The procedure calls itself recursively on
the subgraphs until it reaches singleton subgraphs.
While this algorithm works well with underspec-
ified semantic representations in semantics, it is too
slow for the larger discourse graphs, as we will see in
Section 5. However, we will now optimise it for the
special case of constrained chains. First, we observe
that all subgraphs ever visited by the algorithm are
connected subchains. A subchain is uniquely identi-
fiable by the positions of the first and last fragment
in the left-to-right order of the chain; we can thus
read the nonterminal {i; j} simply as a pair of inte-
gers that identifies the subchain from the i-th to the
246
Algorithm 1: GenerateRules({i; j},G,C)
if G contains rules for {i; j} then return1
if i=j then G.add({ {i; j}? Label(i) } ) else2
/* Loop over upper fragments */
for k = i+1 to j-1 step 2 do3
if ?? edge=(s,t) ? C s.t. (i ? s < k ? t ? j) ? (i ? t4
? k < s ? j) then
lSub?{i;k-1}, rSub?{k+1; j}5
G.add({i; j}? Label(i)(lSub, rSub))6
GenerateRules(lSub, G, C)7
GenerateRules(rSub, G, C)8
j-th fragment (rather than an abbreviation for a set
of fragments). i and j will generally represent lower
fragments. In the grammar in Fig. 2, {i} is an abbre-
viation of {i; i}.
We can now rephrase the Koller & Thater algo-
rithm in our terms (Algorithm 1). The most impor-
tant change is that we can now test whether an up-
per fragment k in a subgraph {i; j} is free simply by
checking whether there is no dominance edge from
some upper fragment l to some upper fragment r
such that i? l < k ? r ? j, and no dominance edge
from r to l such that i? l ? k < r ? j. For instance,
if there was a dominance edge from the right hole of
2 to the root of 6 in Fig. (1a), then 4 and 6 would
not be free, but 2 would be; and indeed, all config-
urations of this graph would have to have 2 as their
roots. Hence we can replace the graph algorithm for
freeness by a simple comparison of integers. The
general structure of the algorithm remains the same
as in (Koller and Thater, 2005b): It takes a domi-
nance graphC as its input, and recursively calls itself
on pairs {i; j} representing subgraphs while adding
rules and NTs to an RTG G.
4 Soft Discourse Constraints
RTGs can be extended to weighted regular tree
grammars (Knight and Graehl, 2005, wRTGs) by
adding numeric weights to the rules. WRTG deriva-
tions assign weights to each tree: The weight of a
tree is the product of the weights of all rules that
were used in its derivation.
Egg and Regneri (2008) motivate the use of
wRTGs in discourse processing. They assign rule
weights based on corpus-extracted constraints which
express the interdependencies between discourse re-
lations and their surrounding tree structure. One
such constraint states that the right subtree of a Con-
1.00
15.65
244.95
3833.66
60000.00
0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230
new total utool total
Figure 3: Runtime Comparison
dition node should be of minimal size, which ranks
the readings of Fig. 1 (a): (b), (d) > (c) > (e), (f).
In order to state this constraint in a wRTG, we
annotate the grammar in Fig. 2 with the weights
shown in brackets. The Condition rules get higher
weights if the second NT on the RHS represents a
smaller subgraph. The grammar assigns the maxi-
mum weight of 5 to (b) and (d) (fragment 2 has a
leaf as right child), the medium weight 3 to (c) (the
right subgraph of fragment 2 contains two EDUs),
and the minimum weight 1 to (e) and (f). i.e. it ranks
the readings as intended.
Based on our implementation of nonterminals as
integer pairs, we can efficiently compute a con-
figuration with maximal weight using a version of
Knight and Graehl?s (2005) algorithm for comput-
ing the best derivation of a wRTG that is specialised
to the grammars we use.
5 Evaluation
We compare our runtimes with those of Utool
(Koller and Thater, 2005a), the fasted known solver
for general dominance graphs; it implements the
Koller & Thater algorithm. Utool runs very fast for
underspecified representations in semantics, but the
representations for discourse parsing are consider-
ably larger: The largest underspecified semantic rep-
resentation found in the Rondane treebank analysed
with the English Resource Grammar (Copestake and
Flickinger, 2000, ERG) has 4.5? 1012 structural
scope readings, but for 59% of the discourses in the
RST Discourse Treebank (Carlson et al, 2002, RST-
DT), there are more ways of configuring all EDUs
into a binary tree than that.
We evaluate the efficiency of our algorithm on 364
texts from the RST-DT, by converting each discourse
247
into a chain with one lower fragment for each EDU
and one upper fragment labelled with each anno-
tated discourse relation. We use our algorithm and
Utool to generate the RTG from the chain, assign
all soft constraints of Egg and Regneri (2008) to the
grammar, and finally compute the best configuration
according to this model. The evaluation results are
shown in Fig. 3. The horizontal axis shows the chain
length (= number of EDUs minus 1), rounded down
to multiples of ten; the (logarithmic) vertical axis
shows the average runtime in milliseconds for dis-
courses of that length. Both algorithms spend a bit
over half the runtime on computing the RTGs.
As the diagram shows, our algorithm is up to 100
times faster than Utool for the same discourses. It
is capable of computing the best configuration for
every tested discourse ? in less than one second for
86% of the texts. Utool exceeded the OS memory
limit on 77 discourses, and generally couldn?t pro-
cess any text with more than 100 EDUs. The longest
text in the RST-DT has 304 EDUs, so the UDR has
about 2.8?10178 different configurations. Our algo-
rithm computes the best configuration for this UDR
in about three minutes.
6 Conclusion
We presented the first solver for underspecified dis-
course representations that is efficient enough to
compute the globally best configurations of every
discourse in the RST discourse treebank, by exploit-
ing the fact that UDRs are very large but obey very
strong structural restrictions. Our solver converts
a dominance graph into an RTG, adds weights to
the RTG to represent discourse constraints, and then
computes the globally optimal configuration.
It takes about three minutes to compute a best
configuration with a given probability model for the
longest discourse in the treebank, out of 10178 pos-
sible configurations. For comparison, an algorithm
that enumerates a billion configurations per second
to find the best one could have inspected only about
1026 within the estimated age of the universe. So our
algorithm is useful and necessary to process real-
world underspecified discourse representations.
We have thus demonstrated that discourse pro-
cessing based on underspecification is computation-
ally feasible. Nothing in our algorithm hinges on
using RST in particular; it is compatible with any
approach that uses binary trees. In future research,
it would be interesting to complete our system into
a full-blown discourse parser by adding a module
that computes an UDR for a given text, and evaluate
whether its ability to delay decisions about discourse
structure would improve accuracy.
References
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn,
J. Niehren, and S. Thiel. 2003. An efficient graph
algorithm for dominance constraints. Journal of Algo-
rithms, 48:194?219.
L. Carlson, D. Marcu, and M. E. Okurowski. 2002. RST
Discourse Treebank. LDC.
H. Comon, M. Dauchet, R. Gilleron, C. Lo?ding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree Automata Techniques and Ap-
plications. Available on: http://www.grappa.
univ-lille3.fr/tata. Release 12-10-2007.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Confer-
ence on Language Resources and Evaluation.
M. Egg and G. Redeker. 2007. Underspecified discourse
representation. In A. Benz and P. Ku?hnlein, editors,
Constraints in Discourse, Amsterdam. Benjamins.
M. Egg and M. Regneri. 2008. Underspecified Mod-
elling of Complex Discourse Constraints. Submitted.
C. Gardent and B. Webber. 1998. Describing Discourse
Semantics. In Proceedings of the 4th TAG+Workshop,
University of Pennsylvania, Philadelphia.
K. Knight and J. Graehl. 2005. An overview of proba-
bilistic tree transducers for natural language process-
ing. In Computational linguistics and intelligent text
processing, pages 1?24. Springer.
A. Koller and S. Thater. 2005a. Efficient solving and
exploration of scope ambiguities. Proceedings of the
ACL-05 Demo Session.
A. Koller and S. Thater. 2005b. The evolution of dom-
inance constraint solvers. In Proceedings of the ACL-
05 Workshop on Software, Ann Arbor.
A. Koller, M. Regneri, and S. Thater. 2008. Regular tree
grammars as a formalism for scope underspecification.
In Proceedings of ACL-08: HLT.
F. Schilder. 2002. Robust discourse parsing via discourse
markers, topicality and position. Natural Language
Engineering, 8:235?255.
M. Stede. 2004. The Potsdam Commentary Corpus. In
B. Webber and D. Byron, editors, ACL-04 Workshop
on Discourse Annotation.
M. Taboada andW.Mann. 2006. Applications of Rhetor-
ical Structure Theory. Discourse Studies, 8:567?588.
248
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 916?927, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Using Discourse Information for Paraphrase Extraction
Michaela Regneri
Dept. of Computational Linguistics
Saarland University
Saarbr?cken, Germany
regneri@coli.uni-saarland.de
Rui Wang
Language Technology Lab
DFKI GmbH
Saarbr?cken, Germany
ruiwang@dfki.de
Abstract
Previous work on paraphrase extraction us-
ing parallel or comparable corpora has gener-
ally not considered the documents? discourse
structure as a useful information source. We
propose a novel method for collecting para-
phrases relying on the sequential event or-
der in the discourse, using multiple sequence
alignment with a semantic similarity measure.
We show that adding discourse information
boosts the performance of sentence-level para-
phrase acquisition, which consequently gives
a tremendous advantage for extracting phrase-
level paraphrase fragments from matched sen-
tences. Our system beats an informed baseline
by a margin of 50%.
1 Introduction
It is widely agreed that identifying paraphrases is a
core task for natural language processing, including
applications like document summarization (Barzilay
et al1999), Recognizing Textual Entailment (Da-
gan et al2005), natural language generation (Zhao
et al2010; Ganitkevitch et al2011), and machine
translation (Marton et al2009). As a consequence,
many methods have been proposed for generating
large paraphrase resources (Lin and Pantel, 2001;
Szpektor et al2004; Dolan et al2004). One of
the intuitively appropriate data sources for such col-
lections are parallel or comparable corpora: if two
texts are translations of the same foreign document,
or if they describe the same underlying scenario,
they should contain a reasonable number of sentence
pairs that convey the same meaning.
Most approaches that extract paraphrases from
parallel texts employ some type of pattern match-
ing: sentences with the same meaning are assumed
to share many n-grams (Barzilay and Lee, 2003;
Callison-Burch, 2008, among others), many words
in their context (Barzilay and McKeown, 2001) or
certain slots in a dependency path (Lin and Pantel,
2001; Szpektor et al2004). Discourse structure
has only marginally been considered for this task:
For example, Dolan et al2004) extract the first
sentences from comparable articles and take them
as paraphrases. Another approach (Del?ger and
Zweigenbaum, 2009) matches similar paragraphs in
comparable texts, creating smaller comparable doc-
uments for paraphrase extraction.
We believe that discourse structure delivers im-
portant information for the extraction of para-
phrases. Sentences that play the same role in a cer-
tain discourse and have a similar discourse context
can be paraphrases, even if a semantic similarity
model does not consider them very similar. This ex-
tends the widely applied distributional hypothesis to
the discourse level: According to the distributional
hypothesis, entities are similar if they share similar
contexts. In our case, entities are whole sentences,
and contexts are discourse units.
Based on this assumption, we propose a novel
method for collecting paraphrases from parallel texts
using discourse information. We create a new type
of parallel corpus by collecting multiple summaries
for several TV show episodes. The discourse struc-
tures of those summaries are easy to compare: they
all contain the events in the same order as they
have appeared on the screen. This allows us to
take sentence order as event-based discourse struc-
ture, which is highly parallel for recaps of the same
episode.
In its first step, our system uses a sequence align-
916
ment algorithm combined with a state-of-the-art
similarity measure. The approach outperforms in-
formed baselines on the task of sentential paraphrase
identification. The usage of discourse information
even contributes more to the final performance than
the sentence similarity measure.
As second step, we extract phrase-level para-
phrase fragments from the matched sentences. This
step relies on the alignment algorithm?s output, and
we show that discourse information makes a big dif-
ference for the precision of the extraction. We then
add more discourse-based information by prepro-
cessing the text with a coreference resolution sys-
tem, which results in additional performance im-
provement.
The paper is structured as follows: first we sum-
marize related work (Sec. 2), and then we give an
overview over our perspective on the task and sketch
our system pipeline (Sec. 3). The following two sec-
tions describe the details of the sentence matching
step (Sec. 4) and the subsequent paraphrase frag-
ment extraction (Sec. 5). We present both automatic
and manual evaluation of the two system compo-
nents (Sec. 6). Finally, we conclude the paper and
give some hints for future work (Sec. 7).
2 Related Work
Previous paraphrase extraction approaches can be
roughly characterized under two aspects: 1) data
source and 2) granularity of the output.
Both parallel corpora and comparable corpora
have been quite well studied. Barzilay and McK-
eown (2001) use different English translations of
the same novels (i.e., monolingual parallel corpora),
while others (Quirk et al2004) experiment on mul-
tiple sources of the same news/events, i.e., mono-
lingual comparable corpora. Commonly used (can-
didate) comparable corpora are news articles writ-
ten by different news agencies within a limited time
window (Wang and Callison-Burch, 2011). Other
studies focus on extracting paraphrases from large
bilingual parallel corpora, which the machine trans-
lation (MT) community provides in many varieties.
Bannard and Callison-Burch (2005) as well as Zhao
et al2008) take one language as the pivot and
match two possible translations in the other lan-
guages as paraphrases if they share a common pivot
phrase. As parallel corpora have many alternative
ways of expressing the same foreign language con-
cept, large quantities of paraphrase pairs can be ex-
tracted.
The paraphrasing task is also strongly related to
cross-document event coreference resolution, which
is tackled by similar techniques used by the available
paraphrasing systems (Bagga and Baldwin, 1999;
Tomadaki and Salway, 2005).
Most work in paraphrase acquisition has dealt
with sentence-level paraphrases, e.g., (Barzilay and
McKeown, 2001; Barzilay and Lee, 2003; Dolan et
al., 2004; Quirk et al2004). Our approach for sen-
tential paraphrase extraction is related to the one in-
troduced by Barzilay and Lee (2003), who also em-
ploy multiple sequence alignment (MSA). However,
they use MSA at the sentence level rather than at the
discourse level.
We take some core ideas from our previous work
on mining script information (Regneri et al2010).
In this earlier work, we focused on event structures
and their possible realizations in natural language.
The corpus used in those experiments were short
crowd-sourced descriptions of everyday tasks writ-
ten in bullet point style. We aligned them with a
hand-crafted similarity measure that was specifically
designed for this text type. In this current work,
we target the general task of extracting paraphrases
for events rather than the much more specific script-
related task. The current approach uses a domain-
independent similarity measure instead of a specific
hand-crafted similarity score and is thus applicable
to standard texts.
From an applicational point of view, senten-
tial paraphrases are difficult to use in other NLP
tasks. At the phrasal level, interchangeable patterns
(Shinyama et al2002; Shinyama and Sekine, 2003)
or inference rules (Lin and Pantel, 2001) are ex-
tracted. In both cases, each pattern or rule contains
one or several slots, which are restricted to certain
type of words, e.g., named entities (NE) or content
words. They are quite successful in NE-centered
tasks, like information extraction, but their level of
generalization or coverage is insufficient for appli-
cations like Recognizing Textual Entailment (Dinu
and Wang, 2009).
The research on general paraphrase fragment ex-
traction at the sub-sentential level is mainly based
917
on phrase pair extraction techniques from the MT
literature. Munteanu and Marcu (2006) extract sub-
sentential translation pairs from comparable corpora
using the log-likelihood-ratio of word translation
probability. Quirk et al2007) extract fragments
using a generative model of noisy translations. Our
own work (Wang and Callison-Burch, 2011) extends
the first idea to paraphrase fragment extraction on
monolingual parallel and comparable corpora. Our
current approach also uses word-word alignment,
however, we use syntactic dependency trees to com-
pute grammatical fragments. Our use of dependency
trees is inspired by the constituent-tree-based exper-
iments of Callison-Burch (2008).
3 Paraphrases and Discourse
Previous approaches have shown that comparable
texts provide a good basis for paraphrase extrac-
tion. We want to show that discourse structure is
highly useful for precise and high-yield paraphrase
collection from such corpora. Consider the follow-
ing (made-up) example:
(1) [House keeps focusing on his aching leg.1.1.]
[The psychiatrist suggests him to get a hobby
1.2.] [House joins a cooking class.1.3]
(2) [He tells him that the Ibuprofen is not helping
with the pain.2.1.] [Nolan tells House to take up
a hobby.2.2] [Together with Wilson he goes to a
cookery course.2.3]
Read as a whole, it is clear that the two texts de-
scribe the same three events, in the same order, and
thus, e.g., 1.2 and 2.2 are paraphrases. However,
they share very few n-grams, nor named entities. We
determine three factors that can help to identify such
paraphrases:
1. Consider the sequence of events. A system
which recognizes that the three sentence pairs
occur in the same sequential event order would
have a chance of actually matching the sen-
tences.
2. Do coreference resolution. To determine
which sentence parts actually carry the same
meaning, pronoun resolution is essential (e.g.,
to match ?suggest him? and ?tells House?).
recaps 
of House 
M.D.
parallel corpus 
with parallel 
discourse 
structures
The psychiatrist suggests 
him to get a hobby 
Nolan tells House to take 
up a hobby.
sentence-level paraphrases
 + discourse information
 + semantic similarity 
 + word alignments 
 + coref. resolution
 + dependency trees 
 get a hobby 
take up a hobby
paraphrase 
fragments
1
2
3
Figure 1: System pipeline
3. Try a generic sentence similarity model. Pat-
tern matching or n-gram overlap might not be
sufficient to solve this problem.
Our system pipeline is sketched in Fig. 1:
1. Create a corpus: First, we create a compara-
ble corpus of texts with highly comparable dis-
course structures. Complete discourse struc-
tures like in the RST Discourse Treebank (Carl-
son et al2002) may be very useful for para-
phrase computation, however, they are hard to
obtain. Discourse annotation is difficult and
work-intensive, and full-blown automatic dis-
course parsers are neither robust nor very pre-
cise. To circumvent this problem, we assemble
documents that have parallel discourse struc-
tures by default: We compile multiple plot
summaries of TV show episodes. The textual
order of those summaries typically mirrors the
underlying event order of the episodes, in the
same sequence they happened on screen. We
take sentence sequences of recaps as parallel
discourse structures.
2. Extract sentence-level paraphrases: Our sys-
tem finds sentence pairs that are either para-
phrases themselves, or at least contain para-
phrase fragments. This procedure crucially re-
lies on discourse knowledge: A Multiple Se-
quence Alignment (MSA) algorithm matches
sentences if both their inherent semantic sim-
ilarities and the overall similarity score of their
discourse contexts are high enough.
3. Extract paraphrase fragments: Sentence-
level paraphrases may be too specific for fur-
ther domain-independent applications, as they
918
row recap 1 recap 2 recap 3 recap 4 recap 5
34
She gives Fore-
man one shot.
Cuddy tells Fore-
man he has one
chance to prove to
her he can run the
team.

Cuddy agrees
to give him one
chance to prove
himself.
Foreman insists he de-
serves a chance and
Cuddy gives in, warn-
ing him he gets one
shot.
35   
Foreman, Hadley,
and Taub get the
conference room
ready and Foreman
explains that he?ll
be in charge.
Foreman gives the
news to Thirteen
and Taub and they
unpack the conference
room and go with a
diagnosis of CRPS.
36
They decide that
it might be CRPS
and Foreman or-
ders a spinal stim-
ulation.

Foreman says to
treat him for com-
plex regional pain
syndrome with a
spinal stimulation.
 
Figure 2: Excerpt from an alignment table for 5 exemplary recaps of Episode 2 (Season 6).
contain specific NEs (e.g. ?House?) or time ref-
erences. Thus we take a necessary second step
and extract finer-grained paraphrase fragments
from the sentence pairs matched in step 2. The
resulting matched phrases should be grammat-
ical and interchangeable regardless of context.
We propose and compare different fragment ex-
traction algorithms.
The remainder of the paper shows how both of
the paraphrasing steps benefit from using a corpus
with highly parallel discourse structures: The sys-
tem components employ discourse information ei-
ther directly by using MSA (step 1) or coreference
resolution (step 2), or indirectly, because using MSA
in step 1 results in a high precision gain for the sub-
sequent second step.
4 Sentence Matching with MSA
This section explains how we apply MSA to ex-
tract sentence-level paraphrases from a comparable
corpus. As our input data, we manually collect re-
caps for House M.D. episodes from different sources
on the web1. House episodes have an intermediate
length (?45 min), which results in recaps of a con-
1e.g. http://house.wikia.com ? for a detailed list of
URLs, please check the supplementary material or contact the
authors.
venient size (40 to 150 sentences). The result is one
comparable document collection per episode. We
applied a sentence splitter (Gillick, 2009) to the doc-
uments and treat them as sequences of sentences for
further processing.
Sequence alignment takes as its input two se-
quences consisting of elements of some alphabet,
and an alphabet-specific score function cm over
pairs of sequence elements. For insertions and dele-
tions, the algorithm additionally takes gap costs
(cgap). Multiple Sequence Alignment generalizes
pairwise alignment to arbitrarily many sequences.
MSA has its main application area in bioinformat-
ics, where it is used to identify equivalent parts of
DNA (Durbin et al1998). Our alphabet consists of
sentences, and a sequence is an ordered sentence list
constituting a recap.
A Multiple Sequence Alignment results in a table
like Fig. 2. Each column contains the sentences of
one recap, possibly intermitted with gaps (??), and
each row contains at least one non-gap. If two sen-
tences end up in the same row, they are aligned; we
take aligned sentence to be paraphrases. Aligning a
sentence with a gap can be thought of as an insertion
or deletion. Each alignment has a score which is the
sum of all scores for substitutions and all costs for
insertions and deletions. Informally, the alignment
919
score is the sum of all scores for each pair of cells
(c1, c2), if c1 and c2 are in the same row. If either c1
or c2 is a gap, the pair?s score is cgap. If both cells
contain sentences, the score is cm(c1, c2).
Fern and Stevenson (2009) showed that sophis-
ticated similarity measures improve paraphrasing,
so we apply a state-of-the-art vector space model
(Thater et al2011) as our score function. The vec-
tor space model provides contextualized similarities
of words, i.e. the vector of each word is disam-
biguated by the context the current instance occurs
in. cm(c1, c2) returns the model?s similarity score
for c1 and c2.
We re-implement a standard MSA algorithm
(Needleman and Wunsch, 1970) which approxi-
mates the best MSA given the input sequences, cm
and cgap. This algorithm recursively aligns two se-
quences at a time, treating the resulting alignment
as a new sequence. This does not necessarily result
in the globally optimal alignment, because the order
in which sequences are aligned can change the final
output. Given this constraint, the algorithm finds the
best alignment, which - in our case - is the alignment
with the maximal score. Intuitively, we are looking
for the alignment where the most similar sentences
with the most similar preceding and trailing contexts
end up as paraphrases.
5 Paraphrase Fragment Extraction
Taking the output of the sentence alignment as in-
put, we next extract shorter phrase-level paraphrases
(paraphrase fragments) from the matched sentence
pairs. We try different algorithms for this step, all
relying on word-word alignments.
5.1 Preprocessing
Before extracting paraphrase fragments, we first pre-
process all documents as follows:
Stanford CoreNLP 2 provides a set of natural lan-
guage analysis tools. We use the part-of-
speech (POS) tagger, the named-entity recog-
nizer, the parser (Klein and Manning, 2003),
and the coreference resolution system (Lee et
al., 2011). In particular, the dependency struc-
tures of the parser?s output are used for VP-
2http://nlp.stanford.edu/software/
corenlp.shtml
fragment extraction (Sec. 5.3). The output from
the coreference resolution system is used to
cluster all mentions referring to the same en-
tity and to select one as the representative men-
tion. If the representative mention is not a pro-
noun, we modify the original texts by replac-
ing all pronoun mentions in the cluster with the
syntactic head of the representative mention.
Note that the coreference resolution system is
applied to each recap as a whole.
GIZA++ (Och and Ney, 2003) is a widely used
word aligner for MT systems. We amend the
input data by copying identical word pairs 10
times and adding them as additional ?sentence?
pairs (Byrne et al2003), in order to emphasize
the higher alignment probability between iden-
tical words. We run GIZA++ for bi-directional
word alignment and obtain a lexical translation
table.
5.2 Fragment Extraction
As mentioned in Sec. 2, we choose to use alignment-
based approaches to this task, which allows us to use
many existing MT techniques and tools. We mainly
follow our previous approach (Wang and Callison-
Burch, 2011), which is a modified version of an ap-
proach by Munteanu and Marcu (2006) on trans-
lation fragment extraction. We briefly review the
three-step procedure here and refer the reader to the
original paper for more details:
1. Establish word-word alignment between each
sentence pair using GIZA++;
2. Smooth the alignment based on lexical occur-
rence likelihood;
3. Extract fragment pairs using different heuris-
tics, e.g., non-overlapping n-grams, chunk
boundaries, or dependency trees.
After obtaining a lexical translation table by run-
ning GIZA++, for each word pair, w1 and w2, we
use both positive and negative lexical associations
for the alignment, which are defined as the condi-
tional probabilities p(w1|w2) and p(w1|?w2), re-
spectively. The resulting alignment can be further
constrained by a modified longest common sub-
string (LCS) algorithm, which takes sequences of
920
words instead of letters as input. Smoothing (step 2)
is done for each word by taking the average score of
it and its four neighbor words. All the word align-
ments (excluding stop-words) with positive scores
are selected as candidate fragment elements.
Provided with the candidate fragment elements,
we previously (Wang and Callison-Burch, 2011)
used a chunker3 to finalize the output fragments, in
order to follow the linguistic definition of a (para-)
phrase. We extend this step in the current system
by applying a dependency parser to constrain the
boundary of the fragments (Sec. 5.3). Finally, we
filter out trivial fragment pairs, such as identical or
the original sentence pairs.
5.3 VP-fragment Extraction
To obtain more grammatical output fragments, we
add another layer of linguistic information to our
input sentences. Based on the dependency parses
produced during preprocessing, we extract phrases
containing verbs and their complements. More pre-
cisely, we match two phrases if their respective sub-
trees t1 and t2 satisfy the following conditions:
? The subtrees mirror a complete subset of
the GIZA++ word alignment, i.e., all words
aligned to a given word in t1 are contained in
t2, and vice versa. For empty alignments, we
require an overlap of at least one lemma (ig-
noring stop words).
? The root nodes of t1 and t2 have the same
roles within their trees, e.g., we match clauses
with an xcomp-label only with other xcomp-
labelled clauses.
? Both t1 and t2 contain at least one verb with
at least one complement. To enhance recall,
we additionally extract complete prepositional
phrases.
? We exclude trivial fragment pairs that are pre-
fixes or suffixes of each other (or identical).
The main advantage of this approach lies in the out-
put?s grammaticality, because the subtrees always
match complete phrases. This method also functions
as a filtering mechanism for mistakenly aligned sen-
tences: If only the two sentence nodes are returned
3We use the same OpenNLP chunker (http:
//opennlp.sourceforge.net/) for consistency.
as possible matching partners, the pair is discarded
from the results.
6 Evaluation
We evaluate both sentential paraphrase matching
and paraphrase fragment extraction using manually
labelled gold standards (provided in the supplemen-
tary material). We collect recaps for all 20 episodes
of season 6 of House M.D., taking 8 summaries per
episode (the supplementary material contains a list
of all URLs). This results in 160 documents con-
taining 14735 sentences. For evaluation, we use all
episodes except no. 2, which is held out for parame-
ter optimizations and other development purposes.
6.1 Sentential Paraphrase Evaluation
To evaluate sentence matching, we adapt the base-
lines from our earlier work (Regneri et al2010) and
create a new gold standard. We compute precision,
recall and accuracy of our main system and suggest
baselines that separately show the influence of both
the MSA and the semantic scoring function.
Gold-Standard
We aim to create an evaluation set that contains
a sufficient amount of genuine paraphrases. Find-
ing such sentence pairs with random sampling and
manual annotation is infeasible: There are more than
200, 000, 000 possible sentence pairs, and we ex-
pect less than 1% of them to be paraphrases. We
thus sample pairs that either the system or the base-
lines recognized as paraphrases and try to create an
evaluation set that is not biased towards the actual
system or any of the baselines. The evaluation set
consists of 2000 sentence pairs: 400 that the system
recognized as paraphrases, 400 positively labelled
pairs for each of the three baselines (described in the
following section) and 400 randomly selected pairs.
For the final evaluation, we compute precision, re-
call, f-score and accuracy for our main system and
each baseline on this set.
Two annotators labelled each sentence pair
(S1, S2) with one of the following labels:
1. paraphrases: S1 and S2 refer to exactly the
same event(s).
2. containment: S1 contains all the event infor-
mation mentioned in S2, but refers to at least
921
one additional event, or vice versa.
3. related: S1 and S2 overlap in at least one event
reference, but both refer to at least one addi-
tional event.
4. unrelated: S1 and S2 do not overlap at all.
This scheme has a double purpose: The main objec-
tive is judging whether two sentences contain para-
phrases (1-3) or if they are unrelated (4). We use
this coarser distinction for system evaluation by col-
lapsing the categories 1-3 in one paraphrasecoll cat-
egory. Secondly, the annotation shows how well the
sentences fit each other?s content (1 vs. 2&3), and
how much work needs to be done to extract the sen-
tence parts with the same meaning (2 vs. 3).
The inter-annotator agreement according to Co-
hen?s Kappa (Cohen, 1960) is ? = 0.55 (?mod-
erate agreement?). The distinction between unre-
lated cases and elements of paraphrasecoll reaches
? = 0.71 (?substantial agreement?). For the final
gold standard, a third annotator resolved all conflict
cases.
Among all gold standard sentence pairs, we find
158 paraphrases, 238 containment cases, 194 re-
lated ones and 1402 unrelated. We had to discard 8
sentence pairs because one of the items was invalid
or empty. The high proportion of ?unrelated? cases
results from the 400 random pairs and the low pre-
cision of the baselines. Looking at the paraphrases,
27% of the 590 instances in the paraphrasecoll cate-
gory are proper paraphrases, and 73% of them con-
tain additional information that does not belong to
the paraphrased part.
Experimental Setup
We compute precision, recall and f-score with re-
spect to the gold standard (paraphrases are members
of paraphrasecoll), taking f-score as follows:
f -score =
2 ? precision ? recall
precision+ recall
We also compute accuracy as the overall fraction of
correct labels (negative and positive ones).
Our main system uses MSA (denoted by MSA af-
terwards) with vector-based similarities (VEC) as a
scoring function. The gap costs are optimized for
f-score, resulting in cgap = 0.4
To show the contribution of MSA?s structural
component and compare it to the vector model?s
contribution, we create a second MSA-based sys-
tem that uses MSA with BLEU scores (Papineni et
al., 2002) as scoring function (MSA+BLEU). BLEU
establishes the average 1-to-4-gram overlap of two
sentences. The gap costs for this baseline were opti-
mized separately, ending up with cgap = 1.
In order to quantify the contribution of the align-
ment, we create a discourse-unaware baseline by
dropping the MSA and using a state-of-the-art clus-
tering algorithm (Noack, 2007) fed with the vec-
tor space model scores (CLUSTER+VEC). The algo-
rithm partitions the set of sentences into paraphrase
clusters such that the most similar sentences end up
in one cluster. This does not require any parameter
tuning.
We also show a baseline that uses the cluster-
ing algorithm with BLEU scores (CLUSTER+BLEU).
The comparison of this baseline with the other
clustering-baseline that uses vector similarities helps
to underline the sentence similarities? advantage
compared to pure word overlap. Note that the CLUS-
TER+BLEU system resembles popular n-gram over-
lap measures for paraphrase classification.
We also show the results completely random label
assignment, which constitutes a lower bound for the
baselines and the system.
Results
Overall, our system extracts 20379 paraphrase
pairs. Tab. 1 shows the evaluation results on our
gold-standard.
The MSA based system variants outperform the
two clustering baselines significantly (all levels refer
to p = 0.01 and were tested with a resampling test
(Edgington, 1986)).
The clustering baselines perform significantly
better than a random baseline, especially consider-
ing recall. The more elaborated vector-space mea-
sure even gives 10% more in precision and accu-
racy, and overall 14% more in f-score. This is al-
4Gap costs directly influence precision and recall: ?cheap?
gaps lead to a more restrictive system with higher precision, and
more expensive gaps give more recall. We chose f-score as our
objective.
922
System Prec. Recall F-score Acc.
RANDOM 0.30 0.49 0.37 0.51
CLUSTER+BLEU 0.35 0.63 0.45 0.54
CLUSTER+VEC 0.40 0.68 0.51 0.61
MSA+BLEU 0.73 0.74 0.73 0.84
MSA+VEC 0.79 0.66 0.72 0.85
Table 1: Results for sentence matching.
ready a remarkable improvement compared to the
random baseline, and still a significant one com-
pared to CLUSTER+BLEU.
Adding structural knowledge with MSA im-
proves the clustering?s accuracy performance by
24% (CLUSTER+VEC vs. MSA+VEC), precision
even goes up by 39%.
Intuitively we expected the MSA-based systems
to end up with a higher recall than the clustering
baselines, because sentences can be matched even
if their similarity is moderate or low, but their dis-
course context is highly similar. However, this is
only the case for the system using BLEU scores, but
not for the system based on the vector space model.
One possible explanation lies in picking f-score as
objective for the optimization of the gap costs for
MSA: For the naturally more restrictive word over-
lap measure, this leads to a more recall-oriented
system with a low threshold for aligning sentences,
whereas the gap costs for the vector-based system
favors a more restrictive alignment with more pre-
cise results.
The comparison of the two MSA-based sys-
tems highlights the great benefit of using structural
knowledge: Both MSA+BLEU and MSA+VEC have
comparable f-scores and accuracy. The advantage
from using the vector-space model that is still obvi-
ous for the clustering baselines is nearly evened out
when adding discourse knowledge as a backbone.
However, the vector model still results in nominally
higher precision and accuracy.
It is hard to do a direct comparison with state-
of-the-art paraphrase recognition systems, because
most are evaluated on different corpora, e.g., the
Microsoft paraphrase corpus (Dolan and Brockett,
2005, MSR). We cannot apply our system to the
MSR corpus, because we take complete texts as in-
put, while the MSR corpus solely delivers sentence
pairs. While the MSR corpus is larger than our
collection, the wording variations in its paraphrase
pairs are usually lower than for our examples. Thus
the final numbers of previous approaches might be
vaguely comparable with our results: Das and Smith
(2009) present two systems reaching f-scores of 0.82
and 0.83, with a precision of 0.75 and 0.80. Both
precision and f-scores of our msa-based systems lie
within the same range. Heilman and Smith (2010)
introduce a recall-oriented system, which reaches an
f-score of 0.81 by a precision of 0.76. Compared to
this system, our approach results in better precision
values.
All further computations bases on the system us-
ing MSA and the vector space model (MSA+VEC),
because it achieves the highest precision and accu-
racy values.
6.2 Paraphrase Fragment Evaluation
We also manually evaluate precision on paraphrase
fragments, and additionally describe the productiv-
ity of the different setups, providing some intuition
about the methods? recall.
Gold-Standard
We randomly collect 150 fragment pairs for each
of the five system configurations (explained in the
following section). Each fragment pair (f1, f2) is
annotated with one of the following categories:
1. paraphrases: f1 and f2 convey the same
meaning, i.e., they are well-formed and good
matches on the content level.
2. related: f1 and f2 overlap in their meaning, but
one or both phrases have additional unmatched
information.
3. irrelevant: f1 and f2 are unrelated.
This labeling scheme again assesses precision as
well as paraphrase granularity. For precision rating,
we collapse categories 1&2 into one paraphrasecoll
category. Each pair is labelled by two annotators,
who were shown both the fragments and the whole
sentences they originate from. Overall, the raters
had an agreement of ? = 0.67 (?substantial agree-
ment?), which suggests that the task was easier than
sentence level annotation. The agreement for the
923
distinction between the paraphrasecoll categories
and irrelevant instances reaches a level of ? = 0.88
(also ?substantial agreement?). All conflicts were
again adjudicated by a third annotator. Overall, the
gold standard contains 190 paraphrases, 258 related
pairs and 302 irrelevant instances. Unlike previ-
ous approaches to fragment extraction, we do not
evaluate grammaticality, given that the VP-fragment
method implicitly constrains the output fragments to
be complete phrases.
Configurations & Results
We take the output of the sentence matching sys-
tem MSA+VEC as input for paraphrase fragment ex-
traction. As detailed in Sec. 5, our core fragment
module uses the word-word alignments provided by
GIZA++ and uses a chunker for fragment extrac-
tion. We successively enrich this core module with
more information, either by longest common sub-
string (LCS) matching or by operating on depen-
dency trees (VP). In addition, we evaluate the in-
fluence of coreference resolution by preprocessing
the input to the best performing configuration with
pronoun resolution (COREF).
We mainly compute precision for this task, as the
recall of paraphrase fragments is difficult to define.
However, we do include a measure we call produc-
tivity to indicate the algorithm?s completeness. It is
defined as the ratio between the number of result-
ing fragment pairs and the number of sentence pairs
used as input.
Extraction Method Precision Productivity
MSA 0.57 0.76
MSA+LCS 0.45 0.30
MSA+VP 0.81 0.42
MSA+VP+COREF 0.84 0.45
Table 2: Results of paraphrase fragment extraction.
Tab. 2 shows the evaluation results. We reach
our best precision by using the VP-fragment heuris-
tics, which is still more productive than the LCS
method. The grammatical filter gives us a higher
precision compared to the purely alignment-based
approaches. Enhancing the system with corefer-
ence resolution raises the score even further. We
cannot directly compare this performance to other
systems, as all other approaches have different data
sources. However, precision is usually manually
evaluated, so the figures are at least indicative for
a comparison with previous work: One state-of-the-
art system introduced by Zhao et al2008) extracts
paraphrase fragments from bilingual parallel cor-
pora and reaches a precision of 0.67. We found the
same number using our previous approach (Wang
and Callison-Burch, 2011), which is roughly equiv-
alent to our core module. Our approach outperforms
both by 17% with similar estimated productivity.
As a final comparison, we show how the perfor-
mance of the sentence matching methods directly af-
fects the fragment extraction. We use the VP-based
fragment extraction system (VP), and compare the
performances by using either the outputs from our
main system (MSA+VP) or alternatively the base-
line that replaces MSA with a clustering algorithm
(CLUSTER+VP). Both variants use the vector-based
semantic similarity measure.
Sentence matching Precision Productivity
CLUSTER+VP 0.31 0.04
MSA+VP 0.81 0.42
Table 3: Impact of MSA on fragment extraction
As shown in Tab. 3, the precision gain from using
MSA becomes tremendous during further process-
ing: We beat the baseline by 50% here, and produc-
tivity increases by a factor of 10. This means that the
baseline produces on average 0.01 good fragment
pairs per matched sentence pair, and the final sys-
tem extracts 0.3 of them. Those numbers show that
for any application that acquires paraphrases of arbi-
trary granularity, sequential event information pro-
vides an invaluable source to achieve a lean para-
phrasing method with high precision.
6.3 Example output
Fig. 3 shows exemplary results from our system
pipeline, using the VP?FRAGMENTS method with
full coreference resolution on the sentence pairs ex-
tracted by MSA. The results reflect the importance
of discourse information for this task: Sentences are
correctly matched in spite of not having common de-
924
Sentence 1 [with fragment 1] Sentence 2 [with fragment 2]
1 Taub meets House for dinner and claims [that
Rachel had a pottery class].
Taub shows up for his dinner with House without
Rachel, explaining [that she?s at a ceramics class].
2 House doesn?t want her to go and she doesn?t want
to go either, but [she can?t leave her family.]
Lydia admits that she doesn?t want to leave House but
[she has to stay with her family].
3 Thirteen is in a cab to the airport when she finds
out that [her trip had been canceled].
Hadley discovers that [her reservation has been can-
celled].
4 Nash asks House [for the extra morphine]. The patient is ready [for more morphine].
5 House comes in to tell Wilson that Tucker has can-
cer and [shows him the test results].
House comes in and [informs Wilson that the tests have
proven positive]: Tucker has cancer.
6 Foreman tells him [to confide in Cameron]. When Chase points out they can?t move Donny with-
out alerting Cameron, Foreman tells Chase [to be honest
with his wife].
7 Thirteen breaks [into the old residence] and tells
Taub that she realizes that he?s been with Maya.
Taub and Thirteen break [into Ted?s former residence].
8 He finds [a darkened patch on his right foot near
the big toe].
House finally finds [a tumorous mole on his toe].
Figure 3: Example results; fragments extracted from aligned sentences are bracketed and emphasized.
pendency patterns (e.g., Example 4) or sharing many
n-grams (6-8). Additionally, the coreference resolu-
tion allows us to match Rachel (1) and Wilson (5) to
the correct corresponding pronouns. All examples
show that this technique of matching sentence could
even help to make coreference resolution better, be-
cause we can easily identify Cameron with his wife,
Lydia with the respective pronouns, Nash with The
Patient or the nickname Thirteen with Hadley, the
character?s actual name.
7 Conclusion and Future Work
We presented our work on paraphrase extraction us-
ing discourse information, on a corpus consisting
of recaps of TV show episodes. Our approach first
uses MSA to extract sentential paraphrases, which
are then further processed to compute finer-grained
paraphrase fragments using dependency trees and
pronoun resolution. The experimental results show
great advantages from using discourse information,
beating informed baselines and performing compet-
itively with state-of-the-art systems.
For future work, we plan to use MSA to align
single clauses rather than whole sentences. This
can also help to define the fragment boundaries
more clearly. Additionally, we plan to generalize
the method for other parallel texts by preprocessing
them with a temporal classifier. In a more advanced
step, we will also use the aligned paraphrases to help
resolving discourse structure, e.g. for coreference
resolution, which could lead to a high-performance
bootstrapping system. In a long-term view, it would
be interesting to see how aligned discourse trees
could help to extract paraphrases from arbitrary par-
allel text.
Acknowledgements
The first author was funded by the Cluster
of Excellence ?Multimodal Computing and In-
teraction? in the German Excellence Initiative.
The second Author was funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
No. 287923 (EXCITEMENT, http://www.
excitement-project.eu/). ? We want to
thank Stefan Thater for supplying the semantic sim-
ilarity scores of his algorithm for our data. We are
grateful to Manfred Pinkal, Alexis Palmer and three
anonymous reviewers for their helpful comments on
previous versions of this paper.
925
References
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: annotations, experiments, and ob-
servations. In Proceedings of the Workshop on Coref-
erence and its Applications.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL 2005.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of HLT-NAACL 2003.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Proc.
of ACL 2001.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of
ACL 1999.
W. Byrne, S. Khudanpur, W. Kim, S. Kumar, P. Pecina,
P. Virga, P. Xu, and D. Yarowsky. 2003. The Johns
Hopkins University 2003 Chinese-English machine
translation system. In Proceedings of the MT Summit
IX.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP 2008.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2002. RST Discourse Treebank. LDC.
J. Cohen. 1960. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In MLCW, pages 177?190.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
Proceedings of ACL-IJCNLP 2009.
Louise Del?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the ACL-IJCNLP BUCC-2009 Workshop.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entailment.
In Proceedings of EACL 2009.
W. B. Dolan and C. Brockett. 2005. Automatically con-
structing a corpus of sentential paraphrases. In Pro-
ceedings of the third International Workshop on Para-
phrasing.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING 2004.
Richard Durbin, Sean Eddy, Anders Krogh, and Graeme
Mitchison. 1998. Biological Sequence Analysis.
Cambridge University Press.
Eugene S Edgington. 1986. Randomization tests. Mar-
cel Dekker, Inc., New York, NY, USA.
Samuel Fern and Mark Stevenson. 2009. A semantic
similarity approach to paraphrase detection. In Pro-
ceedings of the Computational Linguistics UK (CLUK
2008) 11th Annual Research Colloquium.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP
2011.
Dan Gillick. 2009. Sentence boundary detection and the
problem with the u.s. In Proceedings of HLT-NAACL
2009: Companion Volume: Short Papers.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL-HLT 2010.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL 2003.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the CoNLL-2011 Shared Task.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proceedings of the
ACM SIGKDD.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved Statistical Machine Translation Using
Monolingually-Derived Paraphrases. In Proceedings
of EMNLP 2009.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting Parallel Sub-Sentential Fragments from Non-
Parallel Corpora. In Proceedings of ACL 2006.
Saul B. Needleman and Christian D. Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
molecular biology, 48(3), March.
Andreas Noack. 2007. Energy models for graph cluster-
ing. Journal of Graph Algorithms and Applications,
11(2):453?480.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002.
926
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of EMNLP 2004.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning Script Knowledge with Web
Experiments. In Proceedings of ACL 2010.
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase
acquisition for information extraction. In Proceedings
of the ACL PARAPHRASE ?03 Workshop.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of HLT 2002.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition
of Entailment Relations. In Proceedings of EMNLP
2004.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2011. Word Meaning in Context: A Simple and Effec-
tive Vector Model. In Proceedings of IJCNLP 2011.
Eleftheria Tomadaki and Andrew Salway. 2005. Match-
ing verb attributes for cross-document event corefer-
ence. In Proc. of the Interdisciplinary Workshop on
the Identification and Representation of Verb Features
and Verb Classes.
Rui Wang and Chris Callison-Burch. 2011. Para-
phrase fragment extraction from monolingual compa-
rable corpora. In Proc. of the ACL BUCC-2011 Work-
shop.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase Pat-
terns from Bilingual Corpora. In Proceedings of ACL
2008.
Shiqi Zhao, Haifeng Wang, Xiang Lan, and Ting Liu.
2010. Leveraging Multiple MT Engines for Para-
phrase Generation. In Proceedings of COLING 2010.
927
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 979?988,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Script Knowledge with Web Experiments
Michaela Regneri Alexander Koller
Department of Computational Linguistics and Cluster of Excellence
Saarland University, Saarbru?cken
{regneri|koller|pinkal}@coli.uni-saarland.de
Manfred Pinkal
Abstract
We describe a novel approach to unsuper-
vised learning of the events that make up
a script, along with constraints on their
temporal ordering. We collect natural-
language descriptions of script-specific
event sequences from volunteers over the
Internet. Then we compute a graph rep-
resentation of the script?s temporal struc-
ture using a multiple sequence alignment
algorithm. The evaluation of our system
shows that we outperform two informed
baselines.
1 Introduction
A script is ?a standardized sequence of events that
describes some stereotypical human activity such
as going to a restaurant or visiting a doctor? (Barr
and Feigenbaum, 1981). Scripts are fundamental
pieces of commonsense knowledge that are shared
between the different members of the same cul-
ture, and thus a speaker assumes them to be tac-
itly understood by a hearer when a scenario re-
lated to a script is evoked: When one person says
?I?m going shopping?, it is an acceptable reply
to say ?did you bring enough money??, because
the SHOPPING script involves a ?payment? event,
which again involves the transfer of money.
It has long been recognized that text under-
standing systems would benefit from the implicit
information represented by a script (Cullingford,
1977; Mueller, 2004; Miikkulainen, 1995). There
are many other potential applications, includ-
ing automated storytelling (Swanson and Gordon,
2008), anaphora resolution (McTear, 1987), and
information extraction (Rau et al, 1989).
However, it is also commonly accepted that the
large-scale manual formalization of scripts is in-
feasible. While there have been a few attempts at
doing this (Mueller, 1998; Gordon, 2001), efforts
in which expert annotators create script knowledge
bases clearly don?t scale. The same holds true of
the script-like structures called ?scenario frames?
in FrameNet (Baker et al, 1998).
There has recently been a surge of interest in
automatically learning script-like knowledge re-
sources from corpora (Chambers and Jurafsky,
2008b; Manshadi et al, 2008); but while these
efforts have achieved impressive results, they are
limited by the very fact that a lot of scripts ? such
as SHOPPING ? are shared implicit knowledge, and
their events are therefore rarely elaborated in text.
In this paper, we propose a different approach
to the unsupervised learning of script-like knowl-
edge. We focus on the temporal event structure of
scripts; that is, we aim to learn what phrases can
describe the same event in a script, and what con-
straints must hold on the temporal order in which
these events occur. We approach this problem by
asking non-experts to describe typical event se-
quences in a given scenario over the Internet. This
allows us to assemble large and varied collections
of event sequence descriptions (ESDs), which are
focused on a single scenario. We then compute a
temporal script graph for the scenario by identify-
ing corresponding event descriptions using a Mul-
tiple Sequence Alignment algorithm from bioin-
formatics, and converting the alignment into a
graph. This graph makes statements about what
phrases can describe the same event of a scenario,
and in what order these events can take place. Cru-
cially, our algorithm exploits the sequential struc-
ture of the ESDs to distinguish event descriptions
that occur at different points in the script storyline,
even when they are semantically similar. We eval-
uate our script graph algorithm on ten unseen sce-
narios, and show that it significantly outperforms
a clustering-based baseline.
The paper is structured as follows. We will
first position our research in the landscape of re-
lated work in Section 2. We will then define how
979
we understand scripts, and what aspect of scripts
we model here, in Section 3. Section 4 describes
our data collection method, and Section 5 explains
how we use Multiple Sequence Alignment to com-
pute a temporal script graph. We evaluate our sys-
tem in Section 6 and conclude in Section 7.
2 Related Work
Approaches to learning script-like knowledge are
not new. For instance, Mooney (1990) describes
an early attempt to acquire causal chains, and
Smith and Arnold (2009) use a graph-based algo-
rithm to learn temporal script structures. However,
to our knowledge, such approaches have never
been shown to generalize sufficiently for wide
coverage application, and none of them was rig-
orously evaluated.
More recently, there have been a number of ap-
proaches to automatically learning event chains
from corpora (Chambers and Jurafsky, 2008b;
Chambers and Jurafsky, 2009; Manshadi et al,
2008). These systems typically employ a method
for classifying temporal relations between given
event descriptions (Chambers et al, 2007; Cham-
bers and Jurafsky, 2008a; Mani et al, 2006).
They achieve impressive performance at extract-
ing high-level descriptions of procedures such as
a CRIMINAL PROCESS. Because our approach in-
volves directly asking people for event sequence
descriptions, it can focus on acquiring specific
scripts from arbitrary domains, and we can con-
trol the level of granularity at which scripts are
described. Furthermore, we believe that much
information about scripts is usually left implicit
in texts and is therefore easier to learn from our
more explicit data. Finally, our system automat-
ically learns different phrases which describe the
same event together with the temporal ordering
constraints.
Jones and Thompson (2003) describe an ap-
proach to identifying different natural language re-
alizations for the same event considering the tem-
poral structure of a scenario. However, they don?t
aim to acquire or represent the temporal structure
of the whole script in the end.
In its ability to learn paraphrases using Mul-
tiple Sequence Alignment, our system is related
to Barzilay and Lee (2003). Unlike Barzilay and
Lee, we do not tackle the general paraphrase prob-
lem, but only consider whether two phrases de-
scribe the same event in the context of the same
script. Furthermore, the atomic units of our align-
ment process are entire phrases, while in Barzilay
and Lee?s setting, the atomic units are words.
Finally, it is worth pointing out that our work
is placed in the growing landscape of research
that attempts to learn linguistic information out of
data directly collected from users over the Inter-
net. Some examples are the general acquisition of
commonsense knowledge (Singh et al, 2002), the
use of browser games for that purpose (von Ahn
and Dabbish, 2008), and the collaborative anno-
tation of anaphoric reference (Chamberlain et al,
2009). In particular, the use of the Amazon Me-
chanical Turk, which we use here, has been evalu-
ated and shown to be useful for language process-
ing tasks (Snow et al, 2008).
3 Scripts
Before we delve into the technical details, let us
establish some terminology. In this paper, we dis-
tinguish scenarios, as classes of human activities,
from scripts, which are stereotypical models of the
internal structure of these activities. Where EAT-
ING IN A RESTAURANT is a scenario, the script
describes a number of events, such as ordering and
leaving, that must occur in a certain order in order
to constitute an EATING IN A RESTAURANT activ-
ity. The classical perspective on scripts (Schank
and Abelson, 1977) has been that next to defin-
ing some events with temporal constraints, a script
also defines their participants and their causal con-
nections.
Here we focus on the narrower task of learning
the events that a script consists of, and of model-
ing and learning the temporal ordering constraints
that hold between them. Formally, we will spec-
ify a script (in this simplified sense) in terms of a
directed graph Gs = (Es, Ts), where Es is a set
of nodes representing the events of a scenario s,
and Ts is a set of edges (ei, ek) indicating that the
event ei typically happens before ek in s. We call
Gs the temporal script graph (TSG) for s.
Each event in a TSG can usually be expressed
with many different natural-language phrases. As
the TSG in Fig. 3 illustrates, the first event in the
script for EATING IN A FAST FOOD RESTAURANT
can be equivalently described as ?walk to the
counter? or ?walk up to the counter?; even phrases
like ?walk into restaurant?, which would not usu-
ally be taken as paraphrases of these, can be ac-
cepted as describing the same event in the context
980
  1. walk into restaurant
  2. find the end of the line
  3. stand in line
  4. look at menu board
  5. decide on food and drink
  6. tell cashier your order
  7. listen to cashier repeat order
  8. listen for total price
  9. swipe credit card in scanner
 10. put up credit card
 11. take receipt
 12. look at order number
 13. take your cup
 14. stand off to the side
 15. wait for number to be called
 16. get your drink
 1. look at menu
 2. decide what you want
 3. order at counter
 4. pay at counter
 5. receive food at counter 
 6. take food to table
 7. eat food
 1. walk to the counter
 2. place an order
 3. pay the bill
 4. wait for the ordered food
 5. get the food
 6. move to a table
 7. eat food
 8. exit the place
Figure 1: Three event sequence descriptions
of this scenario. We call a natural-language real-
ization of an individual event in the script an event
description, and we call a sequence of event de-
scriptions that form one particular instance of the
script an event sequence description (ESD). Ex-
amples of ESDs for the FAST FOOD RESTAURANT
script are shown in Fig. 1.
One way to look at a TSG is thus that its nodes
are equivalence classes of different phrases that
describe the same event; another is that valid ESDs
can be generated from a TSG by randomly select-
ing phrases from some nodes and arranging them
in an order that respects the temporal precedence
constraints in Ts. Our goal in this paper is to take
a set of ESDs for a given scenario as our input
and then compute a TSG that clusters different de-
scriptions of the same event into the same node,
and contains edges that generalize the temporal in-
formation encoded in the ESDs.
4 Data Acquisition
In order to automatically learn TSGs, we selected
22 scenarios for which we collect ESDs. We de-
liberately included scenarios of varying complex-
ity, including some that we considered hard to
describe (CHILDHOOD, CREATE A HOMEPAGE),
scenarios with highly variable orderings between
events (MAKING SCRAMBLED EGGS), and sce-
narios for which we expected cultural differences
(WEDDING).
We used the Amazon Mechanical Turk1 to col-
lect the data. For every scenario, we asked 25 peo-
ple to enter a typical sequence of events in this sce-
nario, in temporal order and in ?bullet point style?.
1http://www.mturk.com/
We required the annotators to enter at least 5 and
at most 16 events. Participants were allowed to
skip a scenario if they felt unable to enter events
for it, but had to indicate why. We did not restrict
the participants (e.g. to native speakers).
In this way, we collected 493 ESDs for the 22
scenarios. People used the possibility to skip a
form 57 times. The most frequent explanation for
this was that they didn?t know how a certain sce-
nario works: The scenario with the highest pro-
portion of skipped forms was CREATE A HOME-
PAGE, whereas MAKING SCRAMBLED EGGS was
the only one in which nobody skipped a form. Be-
cause we did not restrict the participants? inputs,
the data was fairly noisy. For the purpose of this
study, we manually corrected the data for orthog-
raphy and filtered out forms that were written in
broken English or did not comply with the task
(e.g. when users misunderstood the scenario, or
did not list the event descriptions in temporal or-
der). Overall we discarded 15% of the ESDs.
Fig. 1 shows three of the ESDs we collected
for EATING IN A FAST-FOOD RESTAURANT. As
the example illustrates, descriptions differ in their
starting points (?walk into restaurant? vs. ?walk to
counter?), the granularity of the descriptions (?pay
the bill? vs. event descriptions 8?11 in the third
sequence), and the events that are mentioned in
the sequence (not even ?eat food? is mentioned in
all ESDs). Overall, the ESDs we collected con-
sisted of 9 events on average, but their lengths var-
ied widely: For most scenarios, there were sig-
nificant numbers of ESDs both with the minimum
length of 5 and the maximum length of 16 and ev-
erything in between. Combined with the fact that
93% of all individual event descriptions occurred
only once, this makes it challenging to align the
different ESDs with each other.
5 Temporal Script Graphs
We will now describe how we compute a temporal
script graph out of the collected data. We proceed
in two steps. First, we identify phrases from dif-
ferent ESDs that describe the same event by com-
puting a Multiple Sequence Alignment (MSA) of
all ESDs for the same scenario. Then we postpro-
cess the MSA and convert it into a temporal script
graph, which encodes and generalizes the tempo-
ral information contained in the original ESDs.
981
row s1 s2 s3 s4
1  walk into restaurant  enter restaurant
2   walk to the counter go to counter
3  find the end of the line  
4  stand in line  
5 look at menu look at menu board  
6 decide what you want decide on food and drink  make selection
7 order at counter tell cashier your order place an order place order
8  listen to cashier repeat order  
9 pay at counter  pay the bill pay for food
10  listen for total price  
11  swipe credit card in scanner  
12  put up credit card  
13  take receipt  
14  look at order number  
15  take your cup  
16  stand off to the side  
17  wait for number to be called wait for the ordered food 
18 receive food at counter get your drink get the food pick up order
19    pick up condiments
20 take food to table  move to a table go to table
21 eat food  eat food consume food
22    clear tray
22   exit the place 
Figure 2: A MSA of four event sequence descriptions
5.1 Multiple Sequence Alignment
The problem of computing Multiple Sequence
Alignments comes from bioinformatics, where it
is typically used to find corresponding elements in
proteins or DNA (Durbin et al, 1998).
A sequence alignment algorithm takes as its in-
put some sequences s1, . . . , sn ? ?? over some al-
phabet ?, along with a cost function cm : ????
R for substitutions and gap costs cgap ? R for in-
sertions and deletions. In bioinformatics, the ele-
ments of ? could be nucleotides and a sequence
could be a DNA sequence; in our case, ? contains
the individual event descriptions in our data, and
the sequences are the ESDs.
A Multiple Sequence Alignment A of these se-
quences is then a matrix as in Fig. 2: The i-th col-
umn of A is the sequence si, possibly with some
gaps (??) interspersed between the symbols of
si, such that each row contains at least one non-
gap. If a row contains two non-gaps, we take these
symbols to be aligned; aligning a non-gap with a
gap can be thought of as an insertion or deletion.
Each sequence alignment A can be assigned a
cost c(A) in the following way:
c(A) = cgap ? ? +
n?
i=1
m?
j=1,
aji 6=
m?
k=j+1,
aki 6=
cm(aji, aki)
where ? is the number of gaps in A, n is the
number of rows and m the number of sequences.
In other words, we sum up the alignment cost for
any two symbols from ? that are aligned with
each other, and add the gap cost for each gap.
There is an algorithm that computes cheapest pair-
wise alignments (i.e. n = 2) in polynomial time
(Needleman and Wunsch, 1970). For n > 2, the
problem is NP-complete, but there are efficient al-
gorithms that approximate the cheapest MSAs by
aligning two sequences first, considering the result
as a single sequence whose elements are pairs, and
repeating this process until all sequences are incor-
porated in the MSA (Higgins and Sharp, 1988).
5.2 Semantic similarity
In order to apply MSA to the problem of aligning
ESDs, we choose ? to be the set of all individ-
ual event descriptions in a given scenario. Intu-
itively, we want the MSA to prefer the alignment
of two phrases if they are semantically similar, i.e.
it should cost more to align ?exit? with ?eat? than
?exit? with ?leave?. Thus we take a measure of se-
mantic (dis)similarity as the cost function cm.
The phrases to be compared are written in
bullet-point style. They are typically short and
elliptic (no overt subject), they lack determiners
and use infinitive or present progressive form for
the main verb. Also, the lexicon differs consider-
ably from usual newspaper corpora. For these rea-
sons, standard methods for similarity assessment
are not straightforwardly applicable: Simple bag-
of-words approaches do not provide sufficiently
good results, and standard taggers and parsers can-
not process our descriptions with sufficient accu-
racy.
We therefore employ a simple, robust heuristics,
which is tailored to our data and provides very
982
get in line
enter restaurant
stand in line
wait in line
look at menu board
wait in line to order my food
examine menu board
look at the menu
look at menu
go to cashier
go to ordering counter
go to counter
i decide what i want
decide what to eat
decide on food and drink
decide on what to order
make selection
decide what you want
order food
i order it
tell cashier your order
order items from wall menu
order my food
place an order
order at counter
place order
pay at counter
pay for the food
pay for food
give order to the employee
pay the bill
pay
pay for the food and drinks
pay for order
collect utensils
pay for order
pick up order
make payment
keep my receipt
take receipt
wait for my order
look at prices
wait
look at order number
wait for order to be done
wait for food to be ready
wait for order
wait for the ordered food
expect order
wait for food
pick up condiments
take your cup
receive food
take food to table
receive tray with order
get condiments
get the food
receive food at counter
pick up food when ready
get my order
get food
move to a table
sit down
wait for number to be called
seat at a table
sit down at table
leave
walk into the reasturant
walk up to the counter
walk into restaurant
go to restaurant
walk to the counter
Figure 3: An extract from the graph computed for EATING IN A FAST FOOD RESTAURANT
shallow dependency-style syntactic information.
We identify the first potential verb of the phrase
(according to the POS information provided by
WordNet) as the predicate, the preceding noun (if
any) as subject, and all following potential nouns
as objects. (With this fairly crude tagging method,
we also count nouns in prepositional phrases as
?objects?.)
On the basis of this pseudo-parse, we compute
the similarity measure sim:
sim = ? ? pred+ ? ? subj + ? ? obj
where pred, subj, and obj are the similarity val-
ues for predicates, subjects and objects respec-
tively, and ?, ?, ? are weights. If a constituent
is not present in one of the phrases to compare,
we set its weight to zero and redistribute it over
the other weights. We fix the individual simi-
larity scores pred, subj, and obj depending on
the WordNet relation between the most similar
WordNet senses of the respective lemmas (100 for
synonyms, 0 for lemmas without any relation, and
intermediate numbers for different kind of Word-
Net links).
We optimized the values for pred, subj, and
obj as well as the weights ?, ? and ? using a
held-out development set of scenarios. Our exper-
iments showed that in most cases, the verb con-
tributes the largest part to the similarity (accord-
ingly, ? needs to be higher than the other factors).
We achieved improved accuracy by distinguishing
a class of verbs that contribute little to the meaning
of the phrase (i.e., support verbs, verbs of move-
ment, and the verb ?get?), and assigning them a
separate, lower ?.
5.3 Building Temporal Script Graphs
We can now compute a low-cost MSA for each
scenario out of the ESDs. From this alignment, we
extract a temporal script graph, in the following
way. First, we construct an initial graph which has
one node for each row of the MSA as in Fig. 2. We
interpret each node of the graph as representing
a single event in the script, and the phrases that
are collected in the node as different descriptions
of this event; that is, we claim that these phrases
are paraphrases in the context of this scenario. We
then add an edge (u, v) to the graph iff (1) u 6=
v, (2) there was at least one ESD in the original
data in which some phrase in u directly preceded
some phrase in v, and (3) if a single ESD contains
a phrase from u and from v, the phrase from u
directly precedes the one from v. In terms of the
MSA, this means that if a phrase from u comes
from the same column as a phrase from v, there
are at most some gaps between them. This initial
graph represents exactly the same information as
the MSA, in a different notation.
The graph is automatically post-processed in
a second step to simplify it and eliminate noise
that caused MSA errors. At first we prune spu-
rious nodes which contain only one event descrip-
tion. Then we refine the graph by merging nodes
whose elements should have been aligned in the
first place but were missed by the MSA. We merge
two nodes if they satisfy certain structural and se-
mantic constraints.
The semantic constraints check whether the
event descriptions of the merged node would be
sufficiently consistent according to the similarity
measure from Section 5.2. To check whether we
can merge two nodes u and v, we use an unsuper-
vised clustering algorithm (Flake et al, 2004) to
983
first cluster the event descriptions in u and v sep-
arately. Then we combine the event descriptions
from u and v and cluster the resulting set. If the
union has more clusters than either u or v, we as-
sume the nodes to be too dissimilar for merging.
The structural constraints depend on the graph
structure. We only merge two nodes u and v if
their event descriptions come from different se-
quences and one of the following conditions holds:
? u and v have the same parent;
? u has only one parent, v is its only child;
? v has only one child and is the only child of
u;
? all children of u (except for v) are also chil-
dren of v.
These structural constraints prevent the merg-
ing algorithm from introducing new temporal re-
lations that are not supported by the input ESDs.
We take the output of this post-processing step
as the temporal script graph. An excerpt of the
graph we obtain for our running example is shown
in Fig. 3. One node created by the node merg-
ing step was the top left one, which combines one
original node containing ?walk into restaurant? and
another with ?go to restaurant?. The graph mostly
groups phrases together into event nodes quite
well, although there are some exceptions, such as
the ?collect utensils? node. Similarly, the tempo-
ral information in the graph is pretty accurate. But
perhaps most importantly, our MSA-based algo-
rithm manages to keep similar phrases like ?wait
in line? and ?wait for my order? apart by exploiting
the sequential structure of the input ESDs.
6 Evaluation
We evaluated the two core aspects of our sys-
tem: its ability to recognize descriptions of the
same event (paraphrases) and the resulting tem-
poral constraints it defines on the event descrip-
tions (happens-before relation). We compare our
approach to two baseline systems and show that
our system outperforms both baselines and some-
times even comes close to our upper bound.
6.1 Method
We selected ten scenarios which we did not use
for development purposes, five of them taken from
the corpus described in Section 4, the other five
from the OMICS corpus.2 The OMICS corpus is a
freely available, web-collected corpus by the Open
Mind Initiative (Singh et al, 2002). It contains
several stories (? scenarios) consisting of multi-
ple ESDs. The corpus strongly resembles ours in
language style and information provided, but is re-
stricted to ?indoor activities? and contains much
more data than our collection (175 scenarios with
more than 40 ESDs each).
For each scenario, we created a paraphrase set
out of 30 randomly selected pairs of event de-
scriptions which the system classified as para-
phrases and 30 completely random pairs. The
happens-before set consisted of 30 pairs classified
as happens-before, 30 random pairs and addition-
ally all 60 pairs in reverse order. We added the
reversed pairs to check whether the raters really
prefer one direction or whether they accept both
and were biased by the order of presentation.
We presented each pair to 5 non-experts, all
US residents, via Mechanical Turk. For the para-
phrase set, an exemplary question we asked the
rater looks as follows, instantiating the Scenario
and the two descriptions to compare appropriately:
Imagine two people, both telling a story
about SCENARIO. Could the first one
say event2 to describe the same part of
the story that the second one describes
with event1 ?
For the happens-before task, the question template
was the following:
Imagine somebody telling a story about
SCENARIO in which the events event1
and event2 occur. Would event1 nor-
mally happen before event2?
We constructed a gold standard by a majority deci-
sion of the raters. An expert rater adjudicated the
pairs with a 3:2 vote ratio.
6.2 Upper Bound and Baselines
To show the contributions of the different system
components, we implemented two baselines:
Clustering Baseline: We employed an unsu-
pervised clustering algorithm (Flake et al, 2004)
and fed it all event descriptions of a scenario. We
first created a similarity graph with one node per
event description. Each pair of nodes is connected
2http://openmind.hri-us.com/
984
SCENARIO
PRECISION RECALL F-SCORE
sys basecl baselev sys basecl baselev sys basecl baselev upper
M
T
U
R
K
pay with credit card 0.52 0.43 0.50 0.84 0.89 0.11 0.64 0.58 ? 0.17 0.60
eat in restaurant 0.70 0.42 0.75 0.88 1.00 0.25 0.78 ? 0.59 ? 0.38 ? 0.92
iron clothes I 0.52 0.32 1.00 0.94 1.00 0.12 0.67 ? 0.48 ? 0.21 ? 0.82
cook scrambled eggs 0.58 0.34 0.50 0.86 0.95 0.10 0.69 ? 0.50 ? 0.16 ? 0.91
take a bus 0.65 0.42 0.40 0.87 1.00 0.09 0.74 ? 0.59 ? 0.14 ? 0.88
O
M
IC
S
answer the phone 0.93 0.45 0.70 0.85 1.00 0.21 0.89 ? 0.71 ? 0.33 0.79
buy from vending machine 0.59 0.43 0.59 0.83 1.00 0.54 0.69 0.60 0.57 0.80
iron clothes II 0.57 0.30 0.33 0.94 1.00 0.22 0.71 ? 0.46 ? 0.27 0.77
make coffee 0.50 0.27 0.56 0.94 1.00 0.31 0.65 ? 0.42 ? 0.40 ? 0.82
make omelette 0.75 0.54 0.67 0.92 0.96 0.23 0.83 ? 0.69 ? 0.34 0.85
AVERAGE 0.63 0.40 0.60 0.89 0.98 0.22 0.73 0.56 0.30 0.82
Figure 4: Results for paraphrasing task; significance of difference to sys: ? : p ? 0.01, ? : p ? 0.1
with a weighted edge; the weight reflects the se-
mantic similarity of the nodes? event descriptions
as described in Section 5.2. To include all input in-
formation on inequality of events, we did not allow
for edges between nodes containing two descrip-
tions occurring together in one ESD. The underly-
ing assumption here is that two different event de-
scriptions of the same ESD always represent dis-
tinct events.
The clustering algorithm uses a parameter
which influences the cluster granularity, without
determining the exact number of clusters before-
hand. We optimized this parameter automatically
for each scenario: The system picks the value that
yields the optimal result with respect to density
and distance of the clusters (Flake et al, 2004),
i.e. the elements of each cluster are as similar as
possible to each other, and as dissimilar as possi-
ble to the elements of all other clusters.
The clustering baseline considers two phrases
as paraphrases if they are in the same cluster. It
claims a happens-before relation between phrases
e and f if some phrase in e?s cluster precedes
some phrase in f ?s cluster in the original ESDs.
With this baseline, we can show the contribution
of MSA.
Levenshtein Baseline: This system follows the
same steps as our system, but using Levenshtein
distance as the measure of semantic similarity for
MSA and for node merging (cf. Section 5.3). This
lets us measure the contribution of the more fine-
grained similarity function. We computed Leven-
shtein distance as the character-wise edit distance
on the phrases, divided by the phrases? character
length so as to get comparable values for shorter
and longer phrases. The gap costs for MSA with
Levenshtein were optimized on our development
set so as to produce the best possible alignment.
Upper bound: We also compared our system
to a human-performance upper bound. Because no
single annotator rated all pairs of ESDs, we con-
structed a ?virtual annotator? as a point of com-
parison, by randomly selecting one of the human
annotations for each pair.
6.3 Results
We calculated precision, recall, and f-score for our
system, the baselines, and the upper bound as fol-
lows, with allsystem being the number of pairs la-
belled as paraphrase or happens-before, allgold as
the respective number of pairs in the gold standard
and correct as the number of pairs labeled cor-
rectly by the system.precision = correctallsystem recall = correctallgold
f -score = 2 ? precision ? recallprecision+ recall
The tables in Fig. 4 and 5 show the results of our
system and the reference values; Fig. 4 describes
the paraphrasing task and Fig. 5 the happens-
before task. The upper half of the tables describes
the test sets from our own corpus, the remainder
refers to OMICS data. The columns labelled sys
contain the results of our system, basecl describes
the clustering baseline and baselev the Levenshtein
baseline. The f-score for the upper bound is in the
column upper. For the f-score values, we calcu-
lated the significance for the difference between
our system and the baselines as well as the upper
bound, using a resampling test (Edgington, 1986).
The values marked with ? differ from our system
significantly at a level of p ? 0.01, ?marks a level
of p ? 0.1. The remaining values are not signifi-
cant with p ? 0.1. (For the average values, no sig-
985
SCENARIO
PRECISION RECALL F-SCORE
sys basecl baselev sys basecl baselev sys basecl baselev upper
M
T
U
R
K
pay with credit card 0.86 0.49 0.65 0.84 0.74 0.45 0.85 ? 0.59 ? 0.53 0.92
eat in restaurant 0.78 0.48 0.68 0.84 0.98 0.75 0.81 ? 0.64 0.71 ? 0.95
iron clothes I 0.78 0.54 0.75 0.72 0.95 0.53 0.75 0.69 ? 0.62 ? 0.92
cook scrambled eggs 0.67 0.54 0.55 0.64 0.98 0.69 0.66 0.70 0.61 ? 0.88
take a bus 0.80 0.49 0.68 0.80 1.00 0.37 0.80 ? 0.66 ? 0.48 ? 0.96
O
M
IC
S
answer the phone 0.83 0.48 0.79 0.86 1.00 0.96 0.84 ? 0.64 0.87 0.90
buy from vending machine 0.84 0.51 0.69 0.85 0.90 0.75 0.84 ? 0.66 ? 0.71 0.83
iron clothes II 0.78 0.48 0.75 0.80 0.96 0.66 0.79 ? 0.64 0.70 0.84
make coffee 0.70 0.55 0.50 0.78 1.00 0.55 0.74 0.71 ? 0.53 ? 0.83
make omelette 0.70 0.55 0.79 0.83 0.93 0.82 0.76 ? 0.69 0.81 ? 0.92
AVERAGE 0.77 0.51 0.68 0.80 0.95 0.65 0.78 0.66 0.66 0.90
Figure 5: Results for happens-before task; significance of difference to sys: ? : p ? 0.01, ? : p ? 0.1
nificance is calculated because this does not make
sense for scenario-wise evaluation.)
Paraphrase task: Our system outperforms
both baselines clearly, reaching significantly
higher f-scores in 17 of 20 cases. Moreover, for
five scenarios, the upper bound does not differ sig-
nificantly from our system. For judging the pre-
cision, consider that the test set is slightly biased:
Labeling all pairs with the majority category (no
paraphrase) would result in a precision of 0.64.
However, recall and f-score for this trivial lower
bound would be 0.
The only scenario in which our system doesn?t
score very well is BUY FROM A VENDING MA-
CHINE, where the upper bound is not significantly
better either. The clustering system, which can?t
exploit the sequential information from the ESDs,
has trouble distinguishing semantically similar
phrases (high recall, low precision). The Leven-
shtein similarity measure, on the other hand, is too
restrictive and thus results in comparatively high
precisions, but very low recall.
Happens-before task: In most cases, and on
average, our system is superior to both base-
lines. Where a baseline system performs better
than ours, the differences are not significant. In
four cases, our system does not differ significantly
from the upper bound. Regarding precision, our
system outperforms both baselines in all scenarios
except one (MAKE OMELETTE).
Again the clustering baseline is not fine-grained
enough and suffers from poor precision, only
slightly better than the majority baseline. The Lev-
enshtein baseline gets mostly poor recall, except
for ANSWER THE PHONE: to describe this sce-
nario, people used very similar wording. In such a
scenario, adding lexical knowledge to the sequen-
tial information makes less of a difference.
On average, the baselines do much better here
than for the paraphrase task. This is because once
a system decides on paraphrase clusters that are
essentially correct, it can retrieve correct informa-
tion about the temporal order directly from the
original ESDs.
Both tables illustrate that the task complexity
strongly depends on the scenario: Scripts that al-
low for a lot of variation with respect to ordering
(such as COOK SCRAMBLED EGGS) are particu-
larly challenging for our system. This is due to the
fact that our current system can neither represent
nor find out that two events can happen in arbitrary
order (e.g., ?take out pan? and ?take out bowl?).
One striking difference between the perfor-
mance of our system on the OMICS data and on
our own dataset is the relation to the upper bound:
On our own data, the upper bound is almost al-
ways significantly better than our system, whereas
significant differences are rare on OMICS. This
difference bears further analysis; we speculate it
might be caused either by the increased amount of
training data in OMICS or by differences in lan-
guage (e.g., fewer anaphoric references).
7 Conclusion
We conclude with a summary of this paper and
some discussion along with hints to future work
in the last part.
7.1 Summary
In this paper, we have described a novel approach
to the unsupervised learning of temporal script in-
formation. Our approach differs from previous
work in that we collect training data by directly
asking non-expert users to describe a scenario, and
986
then apply a Multiple Sequence Alignment algo-
rithm to extract scenario-specific paraphrase and
temporal ordering information. We showed that
our system outperforms two baselines and some-
times approaches human-level performance, espe-
cially because it can exploit the sequential struc-
ture of the script descriptions to separate clusters
of semantically similar events.
7.2 Discussion and Future Work
We believe that we can scale this approach to
model a large numbers of scenarios represent-
ing implicit shared knowledge. To realize this
goal, we are going to automatize several process-
ing steps that were done manually for the cur-
rent study. We will restrict the user input to lex-
icon words to avoid manual orthography correc-
tion. Further, we will implement some heuristics
to filter unusable instances by matching them with
the remaining data. As far as the data collection is
concerned, we plan to replace the web form with a
browser game, following the example of von Ahn
and Dabbish (2008). This game will feature an
algorithm that can generate new candidate scenar-
ios without any supervision, for instance by identi-
fying suitable sub-events of collected scripts (e.g.
inducing data collection for PAY as sub-event se-
quence of GO SHOPPING)
On the technical side, we intend to address the
question of detecting participants of the scripts and
integrating them into the graphs, Further, we plan
to move on to more elaborate data structures than
our current TSGs, and then identify and repre-
sent script elements like optional events, alterna-
tive events for the same step, and events that can
occur in arbitrary order.
Because our approach gathers information from
volunteers on the Web, it is limited by the knowl-
edge of these volunteers. We expect it will per-
form best for general commonsense knowledge;
culture-specific knowledge or domain-specific ex-
pert knowledge will be hard for it to learn. This
limitation could be addressed by targeting spe-
cific groups of online users, or by complementing
our approach with corpus-based methods, which
might perform well exactly where ours does not.
Acknowledgements
We want to thank Dustin Smith for the OMICS
data, Alexis Palmer for her support with Amazon
Mechanical Turk, Nils Bendfeldt for the creation
of all web forms and Ines Rehbein for her effort
with several parsing experiments. In particular, we
thank the anonymous reviewers for their helpful
comments. ? This work was funded by the Cluster
of Excellence ?Multimodal Computing and Inter-
action? in the German Excellence Initiative.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics, pages 86?90, Morristown, NJ,
USA. Association for Computational Linguistics.
Avron Barr and Edward Feigenbaum. 1981. The
Handbook of Artificial Intelligence, Volume 1.
William Kaufman Inc., Los Altos, CA.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL 2003.
Jon Chamberlain, Massimo Poesio, and Udo Kru-
schwitz. 2009. A demonstration of human compu-
tation using the phrase detectives annotation game.
In KDD Workshop on Human Computation. ACM.
Nathanael Chambers and Dan Jurafsky. 2008a. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of EMNLP 2008.
Nathanael Chambers and Dan Jurafsky. 2008b. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of ACL-IJCNLP 2009.
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of ACL-07: Interactive
Poster and Demonstration Sessions.
Richard Edward Cullingford. 1977. Script applica-
tion: computer understanding of newspaper stories.
Ph.D. thesis, Yale University, New Haven, CT, USA.
Richard Durbin, Sean Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis. Cambridge University Press.
Eugene S Edgington. 1986. Randomization tests.
Marcel Dekker, Inc., New York, NY, USA.
Gary W. Flake, Robert E. Tarjan, and Kostas Tsiout-
siouliklis. 2004. Graph clustering and minimum cut
trees. Internet Mathematics, 1(4).
Andrew S. Gordon. 2001. Browsing image collec-
tions with representations of common-sense activi-
ties. JASIST, 52(11).
987
Desmond G. Higgins and Paul M. Sharp. 1988.
Clustal: a package for performing multiple sequence
alignment on a microcomputer. Gene, 73(1).
Dominic R. Jones and Cynthia A. Thompson. 2003.
Identifying events using similarity and context. In
Proceedings of CoNNL-2003.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006.
Machine learning of temporal relations. In
COLING/ACL-2006.
Mehdi Manshadi, Reid Swanson, and Andrew S. Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Proceed-
ings of the 21st FLAIRS Conference.
Michael McTear. 1987. The Articulate Computer.
Blackwell Publishers, Inc., Cambridge, MA, USA.
Risto Miikkulainen. 1995. Script-based inference and
memory retrieval in subsymbolic story processing.
Applied Intelligence, 5(2), 04.
Raymond J. Mooney. 1990. Learning plan schemata
from observation: Explanation-based learning for
plan recognition. Cognitive Science, 14(4).
Erik T. Mueller. 1998. Natural Language Processing
with Thought Treasure. Signiform.
Erik T. Mueller. 2004. Understanding script-based sto-
ries using commonsense reasoning. Cognitive Sys-
tems Research, 5(4).
Saul B. Needleman and Christian D. Wunsch. 1970.
A general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of molecular biology, 48(3), March.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization using
linguistic knowledge acquisition. Information Pro-
cessing and Management, 25(4):419 ? 428.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding. Lawrence Erl-
baum, Hillsdale, NJ.
Push Singh, Thomas Lin, Erik T. Mueller, Grace Lim,
Travell Perkins, and Wan L. Zhu. 2002. Open
mind common sense: Knowledge acquisition from
the general public. In On the Move to Meaningful
Internet Systems - DOA, CoopIS and ODBASE 2002,
London, UK. Springer-Verlag.
Dustin Smith and Kenneth C. Arnold. 2009. Learning
hierarchical plans by reading simple english narra-
tives. In Proceedings of the Commonsense Work-
shop at IUI-09.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP 2008.
Reid Swanson and Andrew S. Gordon. 2008. Say any-
thing: A massively collaborative open domain story
writing companion. In Proceedings of ICIDS 2008.
Luis von Ahn and Laura Dabbish. 2008. Designing
games with a purpose. Commun. ACM, 51(8).
988
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 109?114,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
lex4all: A language-independent tool for building and evaluating
pronunciation lexicons for small-vocabulary speech recognition
Anjana Vakil, Max Paulus, Alexis Palmer, and Michaela Regneri
Department of Computational Linguistics, Saarland University
{anjanav,mpaulus,apalmer,regneri}@coli.uni-saarland.de
Abstract
This paper describes lex4all, an open-
source PC application for the generation
and evaluation of pronunciation lexicons
in any language. With just a few minutes
of recorded audio and no expert knowl-
edge of linguistics or speech technology,
individuals or organizations seeking to
create speech-driven applications in low-
resource languages can build lexicons en-
abling the recognition of small vocabular-
ies (up to 100 terms, roughly) in the target
language using an existing recognition en-
gine designed for a high-resource source
language (e.g. English). To build such lex-
icons, we employ an existing method for
cross-language phoneme-mapping. The
application also offers a built-in audio
recorder that facilitates data collection, a
significantly faster implementation of the
phoneme-mapping technique, and an eval-
uation module that expedites research on
small-vocabulary speech recognition for
low-resource languages.
1 Introduction
In recent years it has been demonstrated that
speech recognition interfaces can be extremely
beneficial for applications in the developing world
(Sherwani and Rosenfeld, 2008; Sherwani, 2009;
Bali et al., 2013). Typically, such applications
target low-resource languages (LRLs) for which
large collections of speech data are unavailable,
preventing the training or adaptation of recogni-
tion engines for these languages. However, an ex-
isting recognizer for a completely unrelated high-
resource language (HRL), such as English, can
be used to perform small-vocabulary recognition
tasks in the LRL, given a pronunciation lexicon
mapping each term in the target vocabulary to a
sequence of phonemes in the HRL, i.e. phonemes
which the recognizer can model.
This is the motivation behind lex4all,
1
an open-
source application that allows users to automati-
cally create a mapped pronunciation lexicon for
terms in any language, using a small number of
speech recordings and an out-of-the-box recog-
nition engine for a HRL. The resulting lexicon
can then be used with the HRL recognizer to add
small-vocabulary speech recognition functionality
to applications in the LRL, without the need for
the large amounts of data and expertise in speech
technologies required to train a new recognizer.
This paper describes the lex4all application and
its utility for the rapid creation and evaluation of
pronunciation lexicons enabling small-vocabulary
speech recognition in any language.
2 Background and related work
Several commercial speech recognition systems
offer high-level Application Programming Inter-
faces (APIs) that make it extremely simple to add
voice interfaces to an application, requiring very
little general technical expertise and virtually no
knowledge of the inner workings of the recogni-
tion engine. If the target language is supported by
the system ? the Microsoft Speech Platform,
2
for
example, supports over 20 languages ? this makes
it very easy to create speech-driven applications.
If, however, the target language is one of the
many thousands of LRLs for which high-quality
recognition engines have not yet been devel-
oped, alternative strategies for developing speech-
recognition interfaces must be employed. Though
tools for quickly training recognizers for new lan-
guages exist (e.g. CMUSphinx
3
), they typically
require many hours of training audio to produce
effective models, data which is by definition not
1
http://lex4all.github.io/lex4all/
2
http://msdn.microsoft.com/en-us/library/hh361572
3
http://www.cmusphinx.org
109
available for LRLs. In efforts to overcome this
data scarcity problem, recent years have seen
the development of techniques for rapidly adapt-
ing multilingual or language-independent acoustic
and language models to new languages from rela-
tively small amounts of data (Schultz and Waibel,
2001; Kim and Khudanpur, 2003), methods for
building resources such as pronunciation dictio-
naries from web-crawled data (Schlippe et al.,
2014), and even a web-based interface, the Rapid
Language Adaptation Toolkit
4
(RLAT), which al-
lows non-expert users to exploit these techniques
to create speech recognition and synthesis tools
for new languages (Vu et al., 2010). While they
greatly reduce the amount of data needed to build
new recognizers, these approaches still require
non-trivial amounts of speech and text in the target
language, which may be an obstacle for very low-
or zero-resource languages. Furthermore, even
high-level tools such as RLAT still demand some
understanding of linguistics/language technology,
and thus may not be accessible to all users.
However, many useful applications (e.g. for ac-
cessing information or conducting basic transac-
tions by telephone) only require small-vocabulary
recognition, i.e. discrimination between a few
dozen terms (words or short phrases). For ex-
ample, VideoKheti (Bali et al., 2013), a text-
free smartphone application that delivers agricul-
tural information to low-literate farmers in In-
dia, recognizes 79 Hindi terms. For such small-
vocabulary applications, an engine designed to
recognize speech in a HRL can be used as-is to
perform recognition of the LRL terms, given a
grammar describing the allowable combinations
and sequences of terms to be recognized, and a
pronunciation lexicon mapping each target term to
at least one pronunciation (sequence of phonemes)
in the HRL (see Fig. 1 for an example).
This is the thinking behind Speech-based Auto-
mated Learning of Accent and Articulation Map-
ping, or ?Salaam? (Sherwani, 2009; Qiao et al.,
2010; Chan and Rosenfeld, 2012), a method of
cross-language phoneme-mapping that discovers
accurate source-language pronunciations for terms
in the target language. The basic idea is to discover
the best pronunciation (phoneme sequence) for a
target term by using the source-language recog-
nition engine to perform phone decoding on one
or more utterances of the term. As commercial
4
http://i19pc5.ira.uka.de/rlat-dev
<lexicon version="1.0" xmlns="http://www
.w3.org/2005/01/pronunciation-
lexicon" xml:lang="en-US" alphabet
="x-microsoft-ups">
<lexeme>
<grapheme>beeni</grapheme>
<phoneme>B E NG I</phoneme>
<phoneme>B EI N I I</phoneme>
</lexeme>
</lexicon>
Figure 1: Sample XML lexicon mapping the
Yoruba word beeni (?yes?) to two possible se-
quences of American English phonemes.
recognizers such as Microsoft?s are designed for
word-decoding, and their APIs do not usually al-
low users access to the phone-decoding mode, the
Salaam approach uses a specially designed ?super-
wildcard? recognition grammar to mimic phone
decoding and guide pronunciation discovery (Qiao
et al., 2010; Chan and Rosenfeld, 2012). This al-
lows the recognizer to identify the phoneme se-
quence best matching a given term, without any
prior indication of how many phonemes that se-
quence should contain.
Given this grammar and one or more audio
recordings of the term, Qiao et al. (2010) use an it-
erative training algorithm to discover the best pro-
nunciation(s) for that term, one phoneme at a time.
Compared to pronunciations hand-written by a lin-
guist, pronunciations generated automatically by
this algorithm yield substantially higher recog-
nition accuracy: Qiao et al. (2010) report word
recognition accuracy rates in the range of 75-95%
for vocabularies of 50 terms. Chan and Rosen-
feld (2012) improve accuracy on larger vocabu-
laries (up to approximately 88% for 100 terms)
by applying an iterative discriminative training al-
gorithm, identifying and removing pronunciations
that cause confusion between word types.
The Salaam method is fully automatic, demand-
ing expertise neither in speech technology nor
in linguistics, and requires only a few recorded
utterances of each word. At least two projects
have successfully used the Salaam method to add
voice interfaces to real applications: an Urdu
telephone-based health information system (Sher-
wani, 2009), and the VideoKheti application men-
tioned above (Bali et al., 2013). What has not ex-
isted before now is an interface that makes this ap-
proach accessible to any user.
110
Given the established success of the Salaam
method, our contribution is to create a more time-
efficient implementation of the pronunciation-
discovery algorithm and integrate it into an easy-
to-use graphical application. In the following sec-
tions, we describe this application and our slightly
modified implementation of the Salaam method.
3 System overview
We have developed lex4all as a desktop applica-
tion for Microsoft Windows,
5
since it relies on the
Microsoft Speech Platform (MSP) as explained in
Section 4.1. The application and its source code
are freely available via GitHub.
6
The application?s core feature is its lexicon-
building tool, the architecture of which is illus-
trated in Figure 2. A simple graphical user in-
terface (GUI) allows users to type in the written
form of each term in the target vocabulary, and
select one or more audio recordings (.wav files)
of that term. Given this input, the program uses
the Salaam method to find the best pronuncia-
tion(s) for each term. This requires a pre-trained
recognition engine for a HRL as well as a series
of dynamically-created recognition grammars; the
engine and grammars are constructed and man-
aged using the MSP. We note here that our imple-
mentation of Salaam deviates slightly from that of
Qiao et al. (2010), improving the time-efficiency
and thus usability of the system (see Sec. 4).
Once pronunciations for all terms in the vocab-
ulary have been generated, the application outputs
a pronunciation lexicon for the given terms as an
XML file conforming to the Pronunciation Lexi-
con Specification.
7
This lexicon can then be di-
rectly included in a speech recognition application
built using the MSP API or a similar toolkit.
4 Pronunciation mapping
4.1 Recognition engine
For the HRL recognizer we use the US English
recognition engine of the MSP. The engine is used
as-is, with no modifications to its underlying mod-
els. We choose the MSP for its robustness and
ease of use, as well as to maintain comparability
with the work of Qiao et al. (2010) and Chan and
Rosenfeld (2012). Following these authors, we
use an engine designed for server-side recognition
5
Windows 7 or 8 (64-bit).
6
http://github.com/lex4all/lex4all
7
http://www.w3.org/TR/pronunciation-lexicon/
Figure 2: Overview of the core components of the
lex4all lexicon-building application.
of low-quality audio, since we aim to enable the
creation of useful applications for LRLs, includ-
ing those spoken in developing-world communi-
ties, and such applications should be able to cope
with telephone-quality audio or similar (Sherwani
and Rosenfeld, 2008).
4.2 Implementation of the Salaam method
Pronunciations (sequences of source-language
phonemes) for each term in the target vocabu-
lary are generated from the audio sample(s) of
that term using the iterative Salaam algorithm
(Sec. 2), which employs the source-language rec-
ognizer and a special recognition grammar. In
the first pass, the algorithm finds the best candi-
date(s) for the first phoneme of the sample(s), then
the first two phonemes in the second pass, and so
on until a stopping criterion is met. In our im-
plementation, we stop iterations if the top-scoring
sequence for a term has not changed for three con-
secutive iterations (Chan and Rosenfeld, 2012), or
if the best sequence from a given pass has a lower
confidence score than the best sequence from the
111
previous pass (Qiao et al., 2010). In both cases, at
least three passes are required.
After the iterative training has completed, the n-
best pronunciation sequences (with n specified by
users ? see Sec. 5.2) for each term are written to
the lexicon, each in a <phoneme> element corre-
sponding to the <grapheme> element containing
the term?s orthographic form (see Fig. 1).
4.3 Running time
A major challenge we faced in engineering a user-
friendly application based on the Salaam algo-
rithm was its long running time. The algorithm
depends on a ?super-wildcard? grammar that al-
lows the recognizer to match each sample of a
given term to a ?phrase? of 0-10 ?words?, each
word comprising any possible sequence of 1, 2, or
3 source-language phonemes (Qiao et al., 2010).
Given the 40 phonemes of US English, this gives
over 65,000 possibilities for each word, resulting
in a huge training grammar and thus a long pro-
cessing time. For a 25-term vocabulary with 5
training samples per term, the process takes ap-
proximately 1-2 hours on a standard modern lap-
top. For development and research, this long train-
ing time is a serious disadvantage.
To speed up training, we limit the length of each
?word? in the grammar to only one phoneme, in-
stead of up to 3, giving e.g. 40 possibilities in-
stead of tens of thousands. The algorithm can still
discover pronunciation sequences of an arbitrary
length, since, in each iteration, the phonemes dis-
covered so far are prepended to the super-wildcard
grammar, such that the phoneme sequence of the
first ?word? in the phrase grows longer with each
pass (Qiao et al., 2010). However, the new imple-
mentation is an order of magnitude faster: con-
structing the same 25-term lexicon on the same
hardware takes approximately 2-5 minutes, i.e.
less than 10% of the previous training time.
To ensure that the new implementation?s vastly
improved running time does not come at the cost
of reduced recognition accuracy, we evaluate and
compare word recognition accuracy rates using
lexicons built with the old and new implementa-
tions. The data we use for this evaluation is a
subset of the Yoruba data collected by Qiao et al.
(2010), comprising 25 Yoruba terms (words) ut-
tered by 2 speakers (1 male, 1 female), with 5
samples of each term per speaker. To determine
same-speaker accuracy for each of the two speak-
Old New p
Female average 72.8 73.6 0.75
Male average 90.4 90.4 1.00
S
a
m
e
-
s
p
e
a
k
e
r
Overall average 81.6 82 0.81
Trained on male 70.4 66.4 ?
Trained on female 76.8 77.6 ?
C
r
o
s
s
-
s
p
e
a
k
e
r
Average 73.6 72 0.63
Table 1: Word recognition accuracy for Yoruba us-
ing old (slower) and new (faster) implementations,
with p-values from t-tests for significance of dif-
ference in means. Bold indicates highest accuracy.
ers, we perform a leave-one-out evaluation on the
five samples recorded per term per speaker. Cross-
speaker accuracy is evaluated by training the sys-
tem on all five samples of each term recorded by
one speaker, and testing on all five samples from
the other speaker. We perform paired two-tailed t-
tests on the results to assess the significance of the
differences in mean accuracy.
The results of our evaluation, given in Table 1,
indicate no statistically significant difference in
accuracy between the two implementations (all p-
values are above 0.5 and thus clearly insignifi-
cant). As our new, modified implementation of the
Salaam algorithm is much faster than the original,
yet equally accurate, lex4all uses the new imple-
mentation by default, although for research pur-
poses we leave users the option of using the origi-
nal (slower) implementation (see Section 5.2).
4.4 Discriminative training
Chan and Rosenfeld (2012) achieve increased ac-
curacy (gains of up to 5 percentage points) by
applying an iterative discriminative training al-
gorithm. This algorithm takes as input the set
of mapped pronunciations generated using the
Salaam algorithm; in each iteration, it simulates
recognition of the training audio samples using
these pronunciations, and outputs a ranked list of
the pronunciations in the lexicon that best match
each sample. Pronunciations that cause ?confu-
sion? between words in the vocabulary, i.e. pro-
nunciations that the recognizer matches to sam-
ples of the wrong word type, are thus identified
and removed from the lexicon, and the process is
repeated in the next iteration.
We implement this accuracy-boosting algorithm
in lex4all, and apply it by default. To enable fine-
112
tuning and experimentation, we leave users the
option to change the number of passes (4 by de-
fault) or to disable discriminative training entirely,
as mentioned in Section 5.2.
5 User interface
As mentioned above, we aim to make the creation
and evaluation of lexicons simple, fast, and above
all accessible to users with no expertise in speech
or language technologies. Therefore, the applica-
tion makes use of a simple GUI that allows users
to quickly and easily specify input and output file
paths, and to control the parameters of the lexicon-
building algorithms.
Figure 3 shows the main interface of the lex4all
lexicon builder. This window displays the terms
that have been specified and the number of audio
samples that have been selected for each word.
Another form, accessed via the ?Add word? or
?Edit? buttons, allows users to add to or edit the
vocabulary by simply typing in the desired ortho-
graphic form of the word and selecting the audio
sample(s) to be used for pronunciation discovery
(see Sec. 5.1 for more details on audio input).
Once the target vocabulary and training audio
have been specified, and the additional options
have been set if desired, users click the ?Build
Lexicon? button and specify the desired name and
target directory of the lexicon file to be saved, and
pronunciation discovery begins. When all pronun-
ciations have been generated, a success message
displaying the elapsed training time is displayed,
and users may either proceed to the evaluation
module to assess the newly created lexicon (see
Sec. 6), or return to the main interface to build an-
other lexicon.
5.1 Audio input and recording
The GUI allows users to easily browse their file
system for pre-recorded audio samples (.wav
files) to be used for lexicon training. To simplify
data collection and enable the development of lexi-
cons even for zero-resource languages, lex4all also
offers a simple built-in audio recorder to record
new speech samples.
The recorder, built using the open-source library
NAudio,
8
takes the default audio input device as
its source and records one channel with a sampling
rate of 8 kHz, as the recognition engine we employ
is designed for low-quality audio (see Section 4.1).
8
http://naudio.codeplex.com/
Figure 3: Screenshot of the lexicon builder.
5.2 Additional options
As seen in Figure 3, lex4all includes optional con-
trols for quick and easy fine-tuning of the lexicon-
building process (the default settings are pictured).
First of all, users can specify the maxi-
mum number of pronunciations (<phoneme> el-
ements) per word that the lexicon may contain;
allowing more pronunciations per word may im-
prove recognition accuracy (Qiao et al., 2010;
Chan and Rosenfeld, 2012). Secondly, users may
train the lexicon using our modified, faster imple-
mentation of the Salaam algorithm or the origi-
nal implementation. Finally, users may choose
whether or not discriminative training is applied,
and if so, how many passes are run (see Sec. 4.4).
6 Evaluation module for research
In addition to its primary utility as a lexicon-
building tool, lex4all is also a valuable research
aide thanks to an evaluation module that allows
users to quickly and easily evaluate the lexicons
they have created. The evaluation tool allows users
to browse their file system for an XML lexicon file
that they wish to evaluate; this may be a lexicon
created using lex4all, or any other lexicon in the
same format. As in the main interface, users then
select one or more audio samples (.wav files)
for each term they wish to evaluate. The system
then attempts to recognize each sample using the
given lexicon, and reports the counts and percent-
ages of correct, incorrect, and failed recognitions.
113
Users may optionally save this report, along with
a confusion matrix of word types, as a comma-
separated values (.csv) file.
The evaluation module thus allows users to
quickly and easily assess different configurations
of the lexicon-building tool, by simply changing
the settings using the GUI and evaluating the re-
sulting lexicons. Furthermore, as the applica-
tion?s source code is freely available and modifi-
able, researchers may even replace entire modules
of the system (e.g. use a different pronunciation-
discovery algorithm), and use the evaluation mod-
ule to quickly assess the results. Therefore, lex4all
facilitates not only application development but
also further research into small-vocabulary speech
recognition using mapped pronunciation lexicons.
7 Conclusion and future work
We have presented lex4all, an open-source appli-
cation that enables the rapid automatic creation
of pronunciation lexicons in any (low-resource)
language, using an out-of-the-box commercial
recognizer for a high-resource language and the
Salaam method for cross-language pronunciation
mapping (Qiao et al., 2010; Chan and Rosen-
feld, 2012). The application thus makes small-
vocabulary speech recognition interfaces feasible
in any language, since only minutes of training au-
dio are required; given the built-in audio recorder,
lexicons can be constructed even for zero-resource
languages. Furthermore, lex4all?s flexible and
open design and easy-to-use evaluation module
make it a valuable tool for research in language-
independent small-vocabulary recognition.
In future work, we plan to expand the selection
of source-language recognizers; at the moment,
lex4all only uses US English as the source lan-
guage, but any of the 20+ other HRLs supported
by the MSP could be added. This would enable in-
vestigation of the target-language recognition ac-
curacy obtained using different source languages,
though our initial exploration of this issue sug-
gests that phonetic similarity between the source
and target languages might not significantly affect
accuracy (Vakil and Palmer, 2014). Another future
goal is to improve and extend the functionality of
the audio-recording tool to make it more flexible
and user-friendly. Finally, as a complement to the
application, it would be beneficial to create a cen-
tral online data repository where users can upload
the lexicons they have built and the speech sam-
ples they have recorded. Over time, this could be-
come a valuable collection of LRL data, enabling
developers and researchers to share and re-use data
among languages or language families.
Acknowledgements
The first author was partially supported by a
Deutschlandstipendium scholarship sponsored by
IMC AG. We thank Roni Rosenfeld, Hao Yee
Chan, and Mark Qiao for generously sharing their
speech data and valuable advice, and Dietrich
Klakow, Florian Metze, and the three anonymous
reviewers for their feedback.
References
Kalika Bali, Sunayana Sitaram, Sebastien Cuendet,
and Indrani Medhi. 2013. A Hindi speech recog-
nizer for an agricultural video search application. In
ACM DEV ?13.
Hao Yee Chan and Roni Rosenfeld. 2012. Discrimi-
native pronunciation learning for speech recognition
for resource scarce languages. In ACM DEV ?12.
Woosung Kim and Sanjeev Khudanpur. 2003. Lan-
guage model adaptation using cross-lingual infor-
mation. In Eurospeech.
Fang Qiao, Jahanzeb Sherwani, and Roni Rosenfeld.
2010. Small-vocabulary speech recognition for
resource-scarce languages. In ACM DEV ?10.
Tim Schlippe, Sebastian Ochs, and Tanja Schultz.
2014. Web-based tools and methods for rapid pro-
nunciation dictionary creation. Speech Communica-
tion, 56:101?118.
Tanja Schultz and Alex Waibel. 2001. Language-
independent and language-adaptive acoustic model-
ing for speech recognition. Speech Communication,
35(1-2):31 ? 51.
Jahanzeb Sherwani and Roni Rosenfeld. 2008. The
case for speech technology for developing regions.
In HCI for Community and International Develop-
ment Workshop, Florence, Italy.
Jahanzeb Sherwani. 2009. Speech interfaces for in-
formation access by low literate users. Ph.D. thesis,
Carnegie Mellon University.
Anjana Vakil and Alexis Palmer. 2014. Cross-
language mapping for small-vocabulary ASR in
under-resourced languages: Investigating the impact
of source language choice. In SLTU ?14.
Ngoc Thang Vu, Tim Schlippe, Franziska Kraus, and
Tanja Schultz. 2010. Rapid bootstrapping of five
Eastern European languages using the Rapid Lan-
guage Adaptation Toolkit. In Interspeech.
114
Transactions of the Association for Computational Linguistics, 1 (2013) 25?36. Action Editor: Hal Daume? III.
Submitted 10/2012; Published 3/2013. c?2013 Association for Computational Linguistics.
Grounding Action Descriptions in Videos
Michaela Regneri ?, Marcus Rohrbach , Dominikus Wetzel ?,
Stefan Thater ?, Bernt Schiele  and Manfred Pinkal ?
? Department of Computational Linguistics, Saarland University, Saarbru?cken, Germany
(regneri|dwetzel|stth|pinkal)@coli.uni-saarland.de
 Max Planck Institute for Informatics, Saarbru?cken, Germany
(rohrbach|schiele)@mpi-inf.mpg.de
Abstract
Recent work has shown that the integration of
visual information into text-based models can
substantially improve model predictions, but
so far only visual information extracted from
static images has been used. In this paper, we
consider the problem of grounding sentences
describing actions in visual information ex-
tracted from videos. We present a general
purpose corpus that aligns high quality videos
with multiple natural language descriptions of
the actions portrayed in the videos, together
with an annotation of how similar the action
descriptions are to each other. Experimental
results demonstrate that a text-based model of
similarity between actions improves substan-
tially when combined with visual information
from videos depicting the described actions.
1 Introduction
The estimation of semantic similarity between
words and phrases is a basic task in computational
semantics. Vector-space models of meaning are one
standard approach. Following the distributional hy-
pothesis, frequencies of context words are recorded
in vectors, and semantic similarity is computed as a
proximity measure in the underlying vector space.
Such distributional models are attractive because
they are conceptually simple, easy to implement and
relevant for various NLP tasks (Turney and Pan-
tel, 2010). At the same time, they provide a sub-
stantially incomplete picture of word meaning, since
they ignore the relation between language and extra-
linguistic information, which is constitutive for lin-
guistic meaning. In the last few years, a growing
amount of work has been devoted to the task of
grounding meaning in visual information, in par-
ticular by extending the distributional approach to
jointly cover texts and images (Feng and Lapata,
2010; Bruni et al, 2011). As a clear result, visual
information improves the quality of distributional
models. Bruni et al (2011) show that visual infor-
mation drawn from images is particularly relevant
for concrete common nouns and adjectives.
A natural next step is to integrate visual infor-
mation from videos into a semantic model of event
and action verbs. Psychological studies have shown
the connection between action semantics and videos
(Glenberg, 2002; Howell et al, 2005), but to our
knowledge, we are the first to provide a suitable data
source and to implement such a model.
The contribution of this paper is three-fold:
? We present a multimodal corpus containing
textual descriptions aligned with high-quality
videos. Starting from the video corpus of
Rohrbach et al (2012b), which contains high-
resolution video recordings of basic cooking
tasks, we collected multiple textual descrip-
tions of each video via Mechanical Turk. We
also provide an accurate sentence-level align-
ment of the descriptions with their respective
videos. We expect the corpus to be a valu-
able resource for computational semantics, and
moreover helpful for a variety of purposes, in-
cluding video understanding and generation of
text from videos.
? We provide a gold-standard dataset for the
evaluation of similarity models for action verbs
and phrases. The dataset has been designed
as analogous to the Usage Similarity dataset of
25
Erk et al (2009) and contains pairs of natural-
language action descriptions plus their associ-
ated video segments. Each of the pairs is an-
notated with a similarity score based on several
manual annotations.
? We report an experiment on similarity model-
ing of action descriptions based on the video
corpus and the gold standard annotation, which
demonstrates the impact of scene information
from videos. Visual similarity models outper-
form text-based models; the performance of
combined models approaches the upper bound
indicated by inter-annotator agreement.
The paper is structured as follows: We first place
ourselves in the landscape of related work (Sec. 2),
then we introduce our corpus (Sec. 3). Sec. 4 re-
ports our action similarity annotation experiment
and Sec. 5 introduces the similarity measures we ap-
ply to the annotated data. We outline the results of
our evaluation in Sec. 6, and conclude the paper with
a summary and directions for future work (Sec. 7).
2 Related Work
A large multimodal resource combining language
and visual information resulted from the ESP game
(von Ahn and Dabbish, 2004). The dataset contains
many images tagged with several one-word labels.
The Microsoft Video Description Corpus (Chen
and Dolan, 2011, MSVD) is a resource providing
textual descriptions of videos. It consists of multiple
crowd-sourced textual descriptions of short video
snippets. The MSVD corpus is much larger than our
corpus, but most of the videos are of relatively low
quality and therefore too challenging for state-of-
the-art video processing to extract relevant informa-
tion. The videos are typically short and summarized
with a single sentence. Our corpus contains coher-
ent textual descriptions of longer video sequences,
where each sentence is associated with a timeframe.
Gupta et al (2009) present another useful re-
source: their model learns the alignment of
predicate-argument structures with videos and uses
the result for action recognition in videos. However,
the corpus contains no natural language texts.
The connection between natural language sen-
tences and videos has so far been mostly explored
by the computer vision community, where dif-
ferent methods for improving action recognition
by exploiting linguistic data have been proposed
(Gupta and Mooney, 2010; Motwani and Mooney,
2012; Cour et al, 2008; Tzoukermann et al, 2011;
Rohrbach et al, 2012b, among others). Our resource
is intended to be used for action recognition as well,
but in this paper, we focus on the inverse effect of
visual data on language processing.
Feng and Lapata (2010) were the first to enrich
topic models for newspaper articles with visual in-
formation, by incorporating features from article il-
lustrations. They achieve better results when in-
corporating the visual information, providing an en-
riched model that pairs a single text with a picture.
Bruni et al (2011) used the ESP game data to cre-
ate a visually grounded semantic model. Their re-
sults outperform purely text-based models using vi-
sual information from pictures for the task of mod-
eling noun similarities. They model single words,
and mostly visual features lead only to moderate im-
provements, which might be due to the mixed qual-
ity and random choice of the images. Dodge et al
(2012) recently investigated which words can actu-
ally be grounded in images at all, producing an au-
tomatic classifier for visual words.
An interesting in-depth study by Mathe et al
(2008) automatically learnt the semantics of motion
verbs as abstract features from videos. The study
captures 4 actions with 8-10 videos for each of the
actions, and would need a perfect object recognition
from a visual classifier to scale up.
Steyvers (2010) and later Silberer and Lapata
(2012) present an alternative approach to incorpo-
rating visual information directly: they use so-called
feature norms, which consist of human associations
for many given words, as a proxy for general percep-
tual information. Because this model is trained and
evaluated on those feature norms, it is not directly
comparable to our approach.
The Restaurant Game by Orkin and Roy (2009)
grounds written chat dialogues in actions carried out
in a computer game. While this work is outstanding
from the social learning perspective, the actions that
ground the dialogues are clicks on a screen rather
than real-world actions. The dataset has successfully
been used to model determiner meaning (Reckman
et al, 2011) in the context of the Restaurant Game,
26
but it is unclear how this approach could scale up to
content words and other domains.
3 The TACOS Corpus
We build our corpus on top of the ?MPII Cook-
ing Composite Activities? video corpus (Rohrbach
et al, 2012b, MPII Composites), which contains
videos of different activities in the cooking domain,
e.g., preparing carrots or separating eggs. We ex-
tend the existing corpus with multiple textual de-
scriptions collected by crowd-sourcing via Amazon
Mechanical Turk1 (MTurk). To facilitate the align-
ment of sentences describing activities with their
proper video segments, we also obtained approxi-
mate timestamps, as described in Sec. 3.2.
MPII Composites comes with timed gold-
standard annotation of low-level activities and par-
ticipating objects (e.g. OPEN [HAND,DRAWER] or
TAKE OUT [HAND,KNIFE,DRAWER]). By adding
textual descriptions (e.g., The person takes a knife
from the drawer) and aligning them on the sentence
level with videos and low-level annotations, we pro-
vide a rich multimodal resource (cf. Fig. 2), the
?Saarbru?cken Corpus of Textually Annotated Cook-
ing Scenes? (TACOS). In particular, the TACOS cor-
pus provides:
? A collection of coherent textual descrip-
tions for video recordings of activities of
medium complexity, as as a basis for empiri-
cal discourse-related research, e.g., the selec-
tion and granularity of action descriptions in
context
? A high-quality alignment of sentences with
video segments, supporting the grounding of
action descriptions in visual information
? Collections of paraphrases describing the same
scene, which result as a by-product from the
text-video alignment and can be useful for text
generation from videos (among other things)
? The alignment of textual activity descriptions
with sequences of low-level activities, which
may be used to study the decomposition of ac-
tion verbs into basic activity predicates
1mturk.com
We expect that our corpus will encourage and en-
able future work on various topics in natural lan-
guage and video processing. In this paper, we will
make use of the second aspect only, demonstrating
the usefulness of the corpus for the grounding task.
After a more detailed description of the basic
video corpus and its annotation (Sec. 3.1) we de-
scribe the collection of textual descriptions with
MTurk (Sec. 3.2), and finally show the assembly and
some benchmarks of the final corpus (Sec. 3.3).
3.1 The video corpus
MPII Composites contains 212 high resolution video
recordings of 1-23 minutes length (4.5 min. on av-
erage). 41 basic cooking tasks such as cutting a cu-
cumber were recorded, each between 4 and 8 times.
The selection of cooking tasks is based on those pro-
posed at ?Jamie?s Home Cooking Skills?.2 The cor-
pus is recorded in a kitchen environment with a total
of 22 subjects. Each video depicts a single task exe-
cuted by an individual subject.
The dataset contains expert annotations of low-
level activity tags. Annotations are provided for seg-
ments containing a semantically meaningful cook-
ing related movement pattern. The action must go
beyond single body part movements (such as move
arm up) and must have the goal of changing the state
or location of an object. 60 different activity labels
are used for annotation (e.g. PEEL, STIR, TRASH).
Each low-level activity tag consists of an activity
label (PEEL), a set of associated objects (CARROT,
DRAWER,...), and the associated timeframe (start-
ing and ending points of the activity). Associated
objects are the participants of an activity, namely
tools (e.g. KNIFE), patient (CARROT) and location
(CUTTING-BOARD). We provide the coarse-grained
role information for patient, location and tool in the
corpus data, but we did not use this information in
our experiments. The dataset contains a total of
8818 annotated segments, on average 42 per video.
3.2 Collecting textual video descriptions
We collected textual descriptions for a subset of the
videos in MPII Composites, restricting collection to
tasks that involve manipulation of cooking ingredi-
ents. We also excluded tasks with fewer than four
2www.jamieshomecookingskills.com
27
video recordings in the corpus, leaving 26 tasks to be
described. We randomly selected five videos from
each task, except the three tasks for which only four
videos are available. This resulted in a total of 127
videos. For each video, we collected 20 different
textual descriptions, leading to 2540 annotation as-
signments. We published these assignments (HITs)
on MTurk, using an adapted version3 of the annota-
tion tool Vatic (Vondrick et al, 2012).
In each assignment, the subject saw one video
specified with the task title (e.g. How to prepare an
onion), and then was asked to enter at least five and
at most 15 complete English sentences to describe
the events in the video. The annotation instructions
contained example annotations from a kitchen task
not contained in our actual dataset.
Annotators were encouraged to watch each video
several times, skipping backward and forward as
they wished. They were also asked to take notes
while watching, and to sketch the annotation before
entering it. Once familiarized with the video, sub-
jects did the final annotation by watching the entire
video from beginning to end, without the possibil-
ity of further non-sequential viewing. Subjects were
asked to enter each sentence as soon as the action de-
scribed by the sentence was completed. The video
playback paused automatically at the beginning of
the sentence input. We recorded pause onset for
each sentence annotation as an approximate ending
timestamp of the described action. The annotators
resumed the video manually.
The tasks required a HIT approval rate of 75%
and were open only to workers in the US, in order
to increase the general language quality of the En-
glish annotations. Each task paid 1.20 USD. Before
paying we randomly inspected the annotations and
manually checked for quality. The total costs of col-
lecting the annotations amounted to 3,353 USD. The
data was obtained within a time frame of 3.5 weeks.
3.3 Putting the TACOS corpus together
Our corpus is a combination of the MTurk data and
MPII Composites, created by filtering out inappro-
priate material and computing a high-quality align-
ment of sentences and video segments. The align-
ment is done by matching the approximate times-
3github.com/marcovzla/vatic/tree/bolt
l1
l2
l3
l4
s1
l5
s3
s2
s1
s3
s2
e
l
e
m
e
n
t
a
r
y
 
t
i
m
e
f
r
a
m
e
s
s
e
n
t
e
n
c
e
s
l1
l2
l3
l4
l5
Figure 1: Aligning action descriptions with the video.
tamps of the MTurk data to the accurate timestamps
in MPII Composites.
We discarded text instances if people did not time
the sentences properly, taking the association of sev-
eral (or even all) sentences to a single timestamp as
an indicator. Whenever we found a timestamp asso-
ciated with two or more sentences, we discarded the
whole instance. Overall, we had to filter out 13%
of the text instances, which left us with 2206 textual
video descriptions.
For the alignment of sentence annotations and
video segments, we assign a precise timeframe to
each sentence in the following way: We take the
timeframes given by the low-level annotation in
MPII Composites as a gold standard micro-event
segmentation of the video, because they mark all
distinct frames that contain activities of interest. We
call them elementary frames. The sequence of el-
ementary frames is not necessarily continuous, be-
cause idle time is not annotated.
The MTurk sentences have end points that con-
stitute a coarse-grained, noisy video segmentation,
assuming that each sentence spans the time between
the end of the previous sentence and its own end-
ing point. We refine those noisy timeframes to gold
frames as shown in Fig. 1: Each elementary frame
(l1-l5) is mapped to a sentence (s1-s3) if its noisy
timeframe covers at least half of the elementary
frame. We define the final gold sentence frame then
as the timespan between the starting point of the first
and the ending point of the last elementary frame.
The alignment of descriptions with low-level ac-
tivities results in a table as given in Fig. 3. Columns
contain the textual descriptions of the videos; rows
28
Top 10
Verbs
cut, take, get, put, wash, place,
rinse, remove, *pan, peel
Top 10
Activities
move, take out, cut, wash, take
apart, add, shake, screw, put in, peel
Figure 4: 10 most frequent verbs and low-level actions in
the TACOS corpus. pan is probably often mis-tagged.
correspond to low-level actions, and each sentence
is aligned with the last of its associated low-level ac-
tions. As a side effect, we also obtain multiple para-
phrases for each sentence, by considering all sen-
tences with the same associated time frame as equiv-
alent realizations of the same action.
The corpus contains 17,334 action descrip-
tions (tokens), realizing 11,796 different sentences
(types). It consists of 146,771 words (tokens),
75,210 of which are content word instances (i.e.
nouns, verbs and adjectives). The verb vocabulary
comprises 28,292 verb tokens, realizing 435 lem-
mas. Since verbs occurring in the corpus typically
describe actions, we can note that the linguistic vari-
ance for the 58 different low-level activities is quite
large. Fig. 4 gives an impression of the action re-
alizations in the corpus, listing the most frequent
verbs from the textual data, and the most frequent
low-level activities.
On average, each description covers 2.7 low-level
activities, which indicates a clear difference in gran-
ularity. 38% of the descriptions correspond to ex-
actly one low-level activity, about a quarter (23%)
covers two of them; 16% have 5 or more low-level
elements, 2% more than 10. The corpus shows how
humans vary the granularity of their descriptions,
measured in time or number of low-level activities,
and it shows how they vary the linguistic realization
of the same action. For example, Fig. 3 contains dice
and chop into small pieces as alternative realizations
of the low-level activity sequence SLICE - SCRATCH
OFF - SLICE.
The descriptions are of varying length (9 words
on average), reaching from two-word phrases to de-
tailed descriptions of 65 words. Most sentences are
short, consisting of a reference to the person in the
video, a participant and an action verb (The person
rinses the carrot, He cuts off the two edges). People
often specified an instrument (from the faucet), or
the resulting state of the action (chop the carrots in
small pieces). Occasionally, we find more complex
constructions (support verbs, coordinations).
As Fig. 3 indicates, the timestamp-based align-
ment is pretty accurate; occasional errors occur like
He starts chopping the carrot... in NL Sequence 3.
The data contains some typos and ungrammatical
sentences (He washed carrot), but for our own ex-
periments, the small number of such errors did not
lead to any processing problems.
4 The Action Similarity Dataset
In this section, we present a gold standard dataset,
as a basis for the evaluation of visually grounded
models of action similarity. We call it the ?Action
Similarity Dataset? (ASim) in analogy to the Usage
Similarity dataset (USim) of Erk et al (2009) and
Erk et al (2012). Similarly to USim, ASim con-
tains a collection of sentence pairs with numerical
similarity scores assigned by human annotators. We
asked the annotators to focus on the similarity of the
activities described rather than on assessing seman-
tic similarity in general. We use sentences from the
TACOS corpus and record their timestamps. Thus
each sentence comes with the video segment which
it describes (these were not shown to the annotators).
4.1 Selecting action description pairs
Random selection of annotated sentences from the
corpus would lead to a large majority of pairs which
are completely dissimilar, or difficult to grade (e.g.,
He opens the drawer ? The person cuts off the ends
of the carrot). We constrained the selection pro-
cess in two ways: First, we consider only sentences
describing activities of manipulating an ingredient.
The low-level annotation of the video corpus helps
us identify candidate descriptions. We exclude rare
and special activities, ending up with CUT, SLICE,
CHOP, PEEL, TAKE APART, and WASH, which oc-
cur reasonably frequently, with a wide distribution
over different scenarios. We restrict the candidate
set to those sentences whose timespan includes one
of these activities. This results in a conceptually
more focussed repertoire of descriptions, and at the
same time admits full linguistic variation (wash an
apple under the faucet ? rinse an apple, slice the
cucumber ? cut the cucumber into slices).
29
 896 -1137 wash      [hand,carrot]
1145 -1212 shake     [hand,carrot]
1330 -1388 close     [hand,drawer]
1431 -1647 take out  [hand,knife,drawer]
1647 -1669 move      [hand,cutting board,counter]
1673 -1705 move      [hand,carrot,bowl,cutting board]
1736 -1818 cut       [knife,carrot,cutting board]
1919 -3395 slice     [knife,carrot,cutting board]
>  890: The man takes out a cutting board.
> 1300: He washes a carrot.
> 1500: He takes out a knife.
> 4000: He slices the carrot.
Videos of basic kitchen tasks
Low level annotations with timestamps, actions and objects
Natural language descriptions 
with ending times of the actions
manual low-level annotation
Mechanical Turk data collection
timestamp-based alignment
Figure 2: Corpus Overview
Sample frame Start End Action Participants NL Sequence 1 NL Sequence 2 NL Sequence 3
743 911 wash hand, carrot He washed carrot The person rinses the
carrot.
He rinses the carrot from
the faucet.
982 1090 cut knife, carrot,
cutting board
He cut off ends of
carrots
The person cuts off
the ends of the carrot.
He cuts off the two edges.
1164 1257 open hand, drawer
1679 1718 close hand, drawer He searches for some-
thing in the drawer, failed
attempt, he throws away
the edges in trash.
1746 1799 trash hand, carrot The person searches
for the trash can, then
throws the ends of
the carrot away.
1854 2011 wash hand, carrot He rinses the carrot again.
2011 2045 shake hand, carrot He washed carrot The person rinses the
carrot again.
He starts chopping the
carrot in small pieces.
2083 2924 slice knife, carrot,
cutting board
2924 2959 scratch
off
hand, carrot,
knife, cutting
board
3000 3696 slice knife, carrot,
cutting board
He diced carrots He finished chopping the
carrots in small pieces.
Figure 3: Excerpt from the corpus for a video on PREPARING A CARROT. Example frames, low-level annotation
(Action and Participants) is shown along with three of the MTurk sequences (NL Sequence 1-3).
30
Second, we required the pairs to share some lexi-
cal material, either the head verb or the manipulated
ingredient (or both).4 More precisely, we composed
the ASim dataset from three different subsets:
Different activity, same object: This subset con-
tains pairs describing different types of actions car-
ried out on the same type of object (e.g. The man
washes the carrot. ? She dices the carrot.). Its fo-
cus is on the central task of modeling the semantic
relation between actions (rather than the objects in-
volved in the activity), since the object head nouns
in the descriptions are the same, and the respective
video segments show the same type of object.
Same activity, same object: Description pairs of
this subset will in many cases, but not always, agree
in their head verbs. The dataset is useful for explor-
ing the degree to which action descriptions are un-
derspecified with respect to the precise manner of
their practical realization. For example, peeling an
onion will mostly be done in a rather uniform way,
while cut applied to carrot can mean that the carrot
is chopped up, or sliced, or cut in halves.
Same activity & verb, different object: Descrip-
tion pairs in this subset share head verb and low-
level activity, but have different objects (e.g. The
man washes the carrot. ? A girl washes an apple un-
der the faucet.). This dataset enables the exploration
of the objects? meaning contribution to the complete
action, established by the variation of equivalent ac-
tions that are done to different objects.
We assembled 900 action description pairs for anno-
tation: 480 pairs share the object; 240 of which have
different activities, and the other 240 pairs share the
same activity. We included paraphrases describing
the same video segment, but we excluded pairs of
identical sentences. 420 additional pairs share their
head verb, but have different objects.
4.2 Manual annotation
Three native speakers of English were asked to judge
the similarity of the action pairs with respect to how
4We refer to the latter with the term object; we don?t require
the ingredient term to be the actual grammatical object in the
action descriptions, we rather use ?object? in its semantic role
sense as the entity affected by an action.
Part of Gold Standard Sim ? ?
DIFF. ACTIVITY, SAME OBJECT 2.20 1.07 0.73
SAME ACTIVITY, SAME OBJECT 4.19 1.04 0.73
ALL WITH SAME OBJECT 3.20 1.44 0.84
SAME VERB, DIFF. OBJECT 3.34 0.69 0.43
COMPLETE DATASET 3.27 1.15 0.73
Figure 5: Average similarity ratings (Sim), their standard
deviation (?)) and annotator agreement (?) for ASim.
they are carried out, rating each sentence pair with
a score from 1 (not similar at all) to 5 (the same or
nearly the same). They did not see the respective
videos, but we noted the relevant kitchen task (i.e.
which vegetable was prepared). We asked the an-
notators explicitly to ignore the actor of the action
(e.g. whether it is a man or a woman) and score
the similarities of the underlying actions rather than
their verbalizations. Each subject rated all 900 pairs,
which were shown to them in completely random or-
der, with a different order for each subject.
We compute inter-annotator agreement (and the
forthcoming evaluation scores) using Spearman?s
rank correlation coefficient (?), a non-parametric
test which is widely used for similar evaluation tasks
(Mitchell and Lapata, 2008; Bruni et al, 2011; Erk
and McCarthy, 2009). Spearman?s ? evaluates how
the samples are ranked relative to each other rather
than the numerical distance between the rankings.
Fig. 5 shows the average similarity ratings in the
different settings and the inter-annotator agreement.
The average inter-rater agreement was ? = 0.73 (av-
eraged over pairwise rater agreements), with pair-
wise results of ? = 0.77, 0.72, and 0.69, respec-
tively, which are all highly significant at p < 0.001.
As expected, pairs with the same activity and ob-
ject are rated very similar (4.19) on average, while
the similarity of different activities on the same ob-
ject is the lowest (2.2). For both subsets, inter-rater
agreement is high (? = 0.73), and even higher for
both SAME OBJECT subsets together (0.84).
Pairs with identical head verbs and different ob-
jects have a small standard deviation, at 0.69. The
inter-annotator agreement on this set is much lower
than for pairs from the SAME OBJECT set. This indi-
cates that similarity assessment for different variants
of the same activity is a hard task even for humans.
31
5 Models of Action Similarity
In the following, we demonstrate that visual infor-
mation contained in videos of the kind provided by
the TACOS corpus (Sec. 3) substantially contributes
to the semantic modeling of action-denoting expres-
sions. In Sec. 6, we evaluate several methods for
predicting action similarity on the task provided by
the ASim dataset. In this section, we describe the
models considered in the evaluation. We use two
different models based on visual information, and in
addition two text based models. We will also explore
the effect of combining linguistic and visual infor-
mation and investigate which mode is most suitable
for which kinds of similarity.
5.1 Text-based models
We use two different models of textual similarity
to predict action similarity: a simple word-overlap
measure (Jaccard coefficient) and a state-of-the-art
model based on ?contextualized? vector representa-
tions of word meaning (Thater et al, 2011).
Jaccard coefficient. The Jaccard coefficient gives
the ratio between the number of (distinct) words
common to two input sentences and the total num-
ber of (distinct) words in the two sentences. Such
simple surface-oriented measures of textual similar-
ity are often used as baselines in related tasks such as
recognizing textual entailment (Dagan et al, 2005)
and are known to deliver relatively strong results.
Vector model. We use the vector model of Thater
et al (2011), which ?contextualizes? vector repre-
sentations for individual words based on the particu-
lar sentence context in which the target word occurs.
The basic intuition behind this approach is that the
words in the syntactic context of the target word in a
given input sentence can be used to refine or disam-
biguate its vector. Intuitively, this allows us to dis-
criminate between different actions that a verb can
refer to, based on the different objects of the action.
We first experimented with a version of this vec-
tor model which predicts action similarity scores of
two input sentences by computing the cosine simi-
larity of the contextualized vectors of the verbs in the
two sentences only. We achieved better performance
with a variant of this model which computes vectors
for the two sentences by summing over the contex-
tualized vectors of all constituent content words.
In the experiments reported below, we only use
the second variant. We use the same experimental
setup as Thater et al (2011), as well as the parameter
settings that are reported to work best in that paper.
5.2 Video-based models
We distinguish two approaches to compute the sim-
ilarity between two video segments. In the first, un-
supervised approach we extract a video descriptor
and compute similarities between these raw features
(Wang et al, 2011). The second approach builds
upon the first by additionally learning higher level
attribute classifiers (Rohrbach et al, 2012b) on a
held out training set. The similarity between two
segments is then computed between the classifier re-
sponses. In the following we detail both approaches:
Raw visual features. We use the state-of-the-art
video descriptor Dense Trajectories (Wang et al,
2011) which extracts visual video features, namely
histograms of oriented gradients, flow, and motion
boundary histograms, around densely sampled and
tracked points.
This approach is especially suited for this data as
it ignores non-moving parts in the video: we are
interested in activities and manipulation of objects,
and this type of feature implicitly uses only infor-
mation in relevant image locations. For our setting
this feature representation has been shown to be su-
perior to human pose-based approaches (Rohrbach
et al, 2012a). Using a bag-of-words representation
we encode the features using a 16,000 dimensional
codebook. Features and codebook are provided with
the publicly available video dataset.
We compute the similarity between two encoded
features by computing the intersection of the two
(normalized) histograms.
Visual classifiers. Visual raw features tend to have
several dimensions in the feature space which pro-
vide unreliable, noisy values and thus degrade the
strength of the similarity measure. Intermediate
level attribute classifiers can learn which feature di-
mensions are distinctive and thus significantly im-
prove performance over raw features. Rohrbach et
al. (2012b) showed that using such an attribute clas-
sifier representation can significantly improve per-
32
MODEL SAME OBJECT SAME VERB OVERALL
TE
XT
JACCARD 0.28 0.25 0.25
TEXTUAL VECTORS 0.30 0.25 0.27
TEXT COMBINED 0.39 0.35 0.36
VI
DE
O VISUAL RAW VECTORS 0.53 -0.08 0.35
VISUAL CLASSIFIER 0.60 0.03 0.44
VIDEO COMBINED 0.61 -0.04 0.44
M
IX ALL UNSUPERVISED 0.58 0.32 0.48
ALL COMBINED 0.67 0.28 0.55
UPPER BOUND 0.84 0.43 0.73
Figure 6: Evaluation results in Spearman?s ?. All values > 0.11 are significant at p < 0.001.
formance for composite activity recognition. The
relevant attributes are all activities and objects an-
notated in the video data (cf. Section 3.1). For the
experiments reported below we use the same setup
as Rohrbach et al (2012b) and use all videos in
MPII Composites and MPII Cooking (Rohrbach et
al., 2012a), excluding the 127 videos used during
evaluation. The real-valued SVM-classifier output
provides a confidence how likely a certain attribute
appeared in a given video segment. This results in a
218-dimensional vector of classifier outputs for each
video segment. To compute the similarity between
two vectors we compute the cosine between them.
6 Evaluation
We evaluate the different similarity models intro-
duced in Sec. 5 by calculating their correlation with
the gold-standard similarity annotations of ASim
(cf. Sec. 4). For all correlations, we use Spear-
man?s ? as a measure. We consider the two textual
measures (JACCARD and TEXTUAL VECTORS) and
their combination, as well as the two visual mod-
els (VISUAL RAW VECTORS and VISUAL CLAS-
SIFIER) and their combination. We also combined
textual and visual features, in two variants: The
first includes all models (ALL COMBINED), the sec-
ond only the unsupervised components, omitting the
visual classifier (ALL UNSUPERVISED). To com-
bine multiple similarity measures, we simply aver-
age their normalized scores (using z-scores).
Figure 6 shows the scores for all of these mea-
sures on the complete ASim dataset (OVERALL),
along with the two subparts, where description pairs
share either the object (SAME OBJECT) or the head
verb (SAME VERB). In addition to the model re-
sults, the table also shows the average human inter-
annotator agreement as UPPER BOUND.
On the complete set, both visual and textual mea-
sures have a highly significant correlation with the
gold standard, whereas the combination of both
clearly leads to the best performance (0.55). The
results on the SAME OBJECT and SAME VERB sub-
sets shed light on the division of labor between the
two information sources. While the textual mea-
sures show a comparable performance over the two
subsets, there is a dramatic difference in the contri-
bution of visual information: On the SAME OBJECT
set, the visual models clearly outperform the textual
ones, whereas the visual information has no positive
effect on the SAME VERB set. This is clear evidence
that the visual model does not capture the similar-
ity of the participating objects but rather genuine ac-
tion similarity, which the visual features (Wang et
al., 2011) we employ were designed for. A direction
for future work is to learn dedicated visual object de-
tectors to recognize and capture similarities between
objects more precisely.
The numbers shown in Figure 7 support this hy-
pothesis, showing the two groups in the SAME OB-
JECT class: For sentence pairs that share the same
activity, the textual models seem to be much more
suitable than the visual ones. In general, visual mod-
els perform better on actions with different activity
types, textual models on closely related activities.
33
MODEL (SAME OBJECT) same action diff. action
TE
XT
JACCARD 0.44 0.14
TEXT VECTORS 0.42 0.05
TEXT COMBINED 0.52 0.14
VI
DE
O VIS. RAW VECTORS 0.21 0.23
VIS. CLASSIFIER 0.21 0.45
VIDEO COMBINED 0.26 0.38
M
IX ALL UNSUPERVISED 0.49 0.24
ALL COMBINED 0.48 0.41
UPPER BOUND 0.73 0.73
Figure 7: Results for sentences with the same object, with
either the same or different low-level activity.
Overall, the supervised classifier contributes a
good part to the final results. However, the supervi-
sion is not strictly necessary to arrive at a significant
correlation; the raw visual features alone are suffi-
cient for the main performance gain seen with the
integration of visual information.
7 Conclusion
We presented the TACOS corpus, which provides
coherent textual descriptions for high-quality video
recordings, plus accurate alignments of text and
video on the sentence level. We expect the corpus
to be beneficial for a variety of research activities in
natural-language and visual processing.
In this paper, we focused on the task of grounding
the meaning of action verbs and phrases. We de-
signed the ASim dataset as a gold standard and eval-
uated several text- and video-based semantic simi-
larity models on the dataset, both individually and
in different combinations.
We are the first to provide semantic models for
action-describing expressions, which are based on
information extracted from videos. Our experimen-
tal results show that these models are of considerable
quality, and that predictions based on a combination
of visual and textual information even approach the
upper bound given by the agreement of human an-
notators.
In this work we used existing similarity models
that had been developed for different applications.
We applied these models without any special train-
ing or optimization for the current task, and we com-
bined them in the most straightforward way. There
is room for improvement by tuning the models to
the task, or by using more sophisticated approaches
to combine modality-specific information (Silberer
and Lapata, 2012).
We built our work on an existing corpus of high-
quality video material, which is restricted to the
cooking domain. As a consequence, the corpus cov-
ers only a limited inventory of activity types and ac-
tion verbs. Note, however, that our models are fully
unsupervised (except the Visual Classifier model),
and thus can be applied without modification to ar-
bitrary domains and action verbs, given that they are
about observable activities. Also, corpora contain-
ing information comparable to the TACOS corpus but
with wider coverage (and perhaps a bit noisier) can
be obtained with a moderate amount of effort. One
needs videos of reasonable quality and some sort of
alignment with action descriptions. In some cases
such alignments even come for free, e.g. via subti-
tles, or descriptions of short video clips that depict
just a single action.
For future work, we will further investigate the
compositionality of action-describing phrases. We
also want to leverage the multimodal information
provided by the TACOS corpus for the improvement
of high-level video understanding, as well as for
generation of natural-language text from videos.
The TACOS corpus and all other data described in
this paper (videos, low-level annotation, aligned tex-
tual descriptions, the ASim-Dataset and visual fea-
tures) are publicly available. 5
Acknowledgements
We?d like to thank Asad Sayeed, Alexis Palmer and
Prashant Rao for their help with the annotations.
We?re indebted to Carl Vondrick and Marco An-
tonio Valenzuela Escrcega for their extensive sup-
port with the video annotation tool. Further we
thank Alexis Palmer and in particular three anony-
mous reviewers for their helpful comments on this
paper. ? This work was funded by the Cluster of Ex-
cellence ?Multimodal Computing and Interaction?
of the German Excellence Initiative and the DFG
project SCHI989/2-2.
5http://www.coli.uni-saarland.de/
projects/smile/page.php?id=tacos
34
References
Luis von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of
SIGCHI 2004.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of GEMS 2011.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of ACL 2011.
Timothee Cour, Chris Jordan, Eleni Miltsakaki, and Ben
Taskar. 2008. Movie/script: Alignment and parsing
of video and text transcription. In Computer Vision
? ECCV 2008, volume 5305 of Lecture Notes in Com-
puter Science, pages 158?171. Springer Berlin Heidel-
berg.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of MLCW 2005.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi,
Yejin Choi, Hal Daume? III, Alexander C. Berg, and
Tamara L. Berg. 2012. Detecting visual text. In HLT-
NAACL, pages 762?772.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of EMNLP 2009.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word usages.
In Proceedings of ACL/AFNLP 2009.
Katrin Erk, Diana McCarthy, and Nick Gaylord. 2012.
Measuring word meaning in context. CL.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings of
HLT-NAACL 2010.
A. M. Glenberg. 2002. Grounding language in action.
Psychonomic Bulletin & Review.
Sonal Gupta and Raymond J. Mooney. 2010. Us-
ing closed captions as supervision for video activ-
ity recognition. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence (AAAI-
2010), pages 1083?1088, Atlanta, GA, July.
Abhinav Gupta, Praveen Srinivasan, Jianbo Shi, and
Larry S. Davis. 2009. Understanding videos, con-
structing plots learning a visually grounded storyline
model from annotated videos. In Proceedings of
CVPR 2009.
Steve R. Howell, Damian Jankowicz, and Suzanna
Becker. 2005. A model of grounded language ac-
quisition: Sensorimotor features improve lexical and
grammatical learning. JML.
S. Mathe, A. Fazly, S. Dickinson, and S. Stevenson.
2008. Learning the abstract motion semantics of verbs
from captioned videos. pages 1?8.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL 2008.
Tanvi S. Motwani and Raymond J. Mooney. 2012. Im-
proving video activity recognition using object recog-
nition and text mining. In Proceedings of the 20th
European Conference on Artificial Intelligence (ECAI-
2012), pages 600?605, August.
Jeff Orkin and Deb Roy. 2009. Automatic learning and
generation of social behavior from collective human
gameplay. In Proceedings of AAMAS 2009.
Hilke Reckman, Jeff Orkin, and Deb Roy. 2011. Ex-
tracting aspects of determiner meaning from dialogue
in a virtual world environment. In Proceedings of CCS
2011, IWCS ?11.
Marcus Rohrbach, Sikandar Amin, Mykhaylo Andriluka,
and Bernt Schiele. 2012a. A database for fine grained
activity detection of cooking activities. In Proceedings
of CVPR 2012.
Marcus Rohrbach, Michaela Regneri, Micha Andriluka,
Sikandar Amin, Manfred Pinkal, and Bernt Schiele.
2012b. Script data for attribute-based recognition of
composite activities. In Proceedings of ECCV 2012.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings of
EMNLP-CoNLL 2012.
Mark Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234 ? 243. ?ce:title?Formal modeling of se-
mantic concepts?/ce:title?.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of IJCNLP 2011.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning. vector space models for semantics.
JAIR.
E. Tzoukermann, J. Neumann, J. Kosecka, C. Fermuller,
I. Perera, F. Ferraro, B. Sapp, R. Chaudhry, and
G. Singh. 2011. Language models for semantic ex-
traction and filtering in video action recognition. In
AAAI Workshop on Language-Action Tools for Cogni-
tive Artificial Agents.
Carl Vondrick, Donald Patterson, and Deva Ramanan.
2012. Efficiently scaling up crowdsourced video an-
notation. IJCV.
Heng Wang, Alexander Kla?ser, Cordelia Schmid, and
Cheng-Lin Liu. 2011. Action Recognition by Dense
Trajectories. In Proceedings of CVPR 2011.
35
36
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 77?85,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
SeedLing: Building and using a seed corpus
for the Human Language Project
Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer, and Michaela Regneri
Universit?at des Saarlandes
66123 Saarbr?ucken, Germany
{emerson, liling, susfert, apalmer, regneri}
@coli.uni-saarland.de
Abstract
A broad-coverage corpus such as the Hu-
man Language Project envisioned by Ab-
ney and Bird (2010) would be a powerful
resource for the study of endangered lan-
guages. Existing corpora are limited in
the range of languages covered, in stan-
dardisation, or in machine-readability. In
this paper we present SeedLing, a seed
corpus for the Human Language Project.
We first survey existing efforts to compile
cross-linguistic resources, then describe
our own approach. To build the foundation
text for a Universal Corpus, we crawl and
clean texts from several web sources that
contain data from a large number of lan-
guages, and convert them into a standard-
ised form consistent with the guidelines
of Abney and Bird (2011). The result-
ing corpus is more easily-accessible and
machine-readable than any of the underly-
ing data sources, and, with data from 1451
languages covering 105 language fami-
lies, represents a significant base corpus
for researchers to draw on and add to in
the future. To demonstrate the utility of
SeedLing for cross-lingual computational
research, we use our data in the test appli-
cation of detecting similar languages.
1 Introduction
At the time of writing, 7105 living languages
are documented in Ethnologue,
1
but Simons and
Lewis (2011) calculated that 37% of extant lan-
guages were at various stages of losing trans-
misson to new generations. Only a fraction
of the world?s languages are well documented,
fewer have machine-readable resources, and fewer
again have resources with linguistic annotations
1
http://www.ethnologue.com
(Maxwell and Hughes, 2006) - so the time to work
on compiling these resources is now.
Several years ago, Abney and Bird (2010; 2011)
posed the challenge of building a Universal Cor-
pus, naming it the Human Language Project. Such
a corpus would include data from all the world?s
languages, in a consistent structure, facilitating
large-scale cross-linguistic processing. The chal-
lenge was issued to the computational linguistics
community, from the perspective that the language
processing, machine learning, and data manipula-
tion and management tools well-known in com-
putational linguistics must be brought to bear on
the problems of documentary linguistics, if we
are to make any serious progress toward build-
ing such a resource. The Universal Corpus as
envisioned would facilitate broadly cross-lingual
natural language processing (NLP), in particular
driving innovation in research addressing NLP for
low-resource languages, which in turn supports
the language documentation process.
We have accepted this challenge and have be-
gun converting existing resources into a format
consistent with Abney and Bird?s specifications.
We aim for a collection of resources that includes
data: (a) from as many languages as possible, and
(b) in a format both in accordance with best prac-
tice archiving recommendations and also readily
accessible for computational methods. Of course,
there are many relevant efforts toward producing
cross-linguistic resources, which we survey in sec-
tion 2. To the best of our knowledge, though, no
existing effort meets these two desiderata to the
extent of our corpus, which we name SeedLing: a
seed corpus for the Human Language Project.
To produce SeedLing, we have drawn on four
web sources, described in section 3.2. To bring
the four resources into a common format and
data structure (section 3.1), each required differ-
ent degrees and types of cleaning and standardis-
ation. We describe the steps required in section 4,
77
presenting each resource as a separate mini-case
study. We hope that the lessons we learned in
assembling our seed corpus can guide future re-
source conversion efforts. To that end, many of the
resources described in section 2 are candidates for
inclusion in the next stage of building a Universal
Corpus.
We believe the resulting corpus, which at
present covers 1451 languages from 105 language
families, is the first of its kind: large enough and
consistent enough to allow broadly multilingual
language processing. To test this claim, we use
SeedLing in a sample application (section 5): the
task of language clustering. With no additional
pre-processing, we extract surface-level features
(frequencies of character n-grams and words) to
estimate the similarity of two languages. Unlike
most previous approaches to the task, we make
no use of resources curated for linguistic typol-
ogy (e.g. values of typological features as in
WALS (Dryer and Haspelmath, 2013), Swadesh
word lists). Despite our approach being highly
dependent on orthography, our clustering perfor-
mance matches the results obtained by Georgi
et al. (2010) using typolological features, which
demonstrates SeedLing?s utility in cross-linguistic
research.
2 Related Work
In this section, we review existing efforts to com-
pile multilingual machine-readable resources. Al-
though some commercial resources are available,
we restrict attention to freely accessible data.
2
Traditional archives. Many archives exist to
store the wealth of traditional resources produced
by the documentary linguistics community. Such
documents are increasingly being digitised, or
produced in a digital form, and there are a number
of archives which now offer free online access.
Some archives aim for a universal scope, such
as The Language Archive (maintained by the
Max Planck Institute of Psycholinguistics), Col-
lection Pangloss (maintained by LACITO), and
The Endangered Languages Archive (maintained
by SOAS). Most archives are regional, including
AILLA, ANLA, PARADISEC, and many others.
However, there are two main problems common
to all of the above data sources. Firstly, the data
2
All figures given below were correct at the time of writ-
ing, but it must be borne in mind that most of these resources
are constantly growing.
is not always machine readable. Even where the
data is available digitally, these often take the form
of scanned images or audio files. While both can
provide invaluable information, they are extremely
difficult to process with a computer, requiring an
impractical level of image or video pre-processing
before linguistic analysis can begin. Even textual
data, which avoids these issues, may not be avail-
able in a machine-readable form, being stored as
pdfs or other opaque formats. Secondly, when data
is machine readable, the format can vary wildly.
This makes automated processing difficult, espe-
cially if one is not aware of the details of each
project. Even when metadata standards and en-
codings agree, there can be idiosyncractic markup
or non-linguistic information, such as labels for
speakers in the transcript of a conversation.
We can see that there is still much work to be
done by individual researchers in digitising and
standardising linguistic data, and it is outside of
the scope of this paper to attempt this for the above
archives. Guidelines for producing new materi-
als are available from the E-MELD project (Elec-
tronic Metastructure for Endangered Languages
Data), which specifically aimed to deal with the
expanding number of standards for linguistic data.
It gives best practice recommendations, illustrated
with eleven case studies, and provides input tools
which link to the GOLD ontology language, and
the OLAC metadata set. Further recommenda-
tions are given by Bird and Simons (2003), who
describe seven dimensions along which the porta-
bility of linguistic data can vary. Various tools are
available from The Language Archive at the Max
Planck Institute for Psycholinguistics.
Many archives are part of the Open Language
Archive Community (OLAC), a subcommunity
of the Open Archives Initiative. OLAC main-
tains a metadata standard, based on the 15-element
Dublin Core, which allows a user to search
through all participating archives in a unified fash-
ion. However, centralising access to disparate re-
sources, while of course extremely helpful, does
not solve the problem of inconsistent standards.
Indeed, it can even be hard to answer simple ques-
tions like ?how many languages are represented??
In short, while traditional archives are invalu-
able for many purposes, for large-scale machine
processing, they leave much to be desired.
Generic corpus collections. Some corpus col-
lections exist which do not focus on endangered
78
languages, but which nonetheless cover an in-
creasing number of languages.
MetaShare (Multilingual Europe Technology
Alliance) provides data in a little over 100 lan-
guages. While language codes are used, they have
not been standardised, so that multiple codes are
used for the same language. Linguistic Data Con-
sortium (LDC) and the European Language Re-
sources Association (ELRA) both offer data in
multiple languages. However, while large in size,
they cover only a limited number of languages.
Furthermore, the corpora they contain are stored
separately, making it difficult to access data ac-
cording to language.
Parallel corpora. The Machine Translation
community has assembled a number of parallel
corpora, which are crucial for statistical machine
translation. The OPUS corpus (Tiedemann, 2012)
subsumes a number of other well-known parallel
corpora, such as Europarl, and covers documents
from 350 languages, with various language pairs.
Web corpora. There has been increasing inter-
est in deriving corpora from the web, due to the
promise of large amounts of data. The majority
of web corpora are however aimed at either one or
a small number of languages, which is perhaps to
be expected, given that the majority of online text
is written in a handful of high-resource languages.
Nonetheless, there have been a few efforts to apply
the same methods to a wider range of languages.
HC Corpora currently provides download of
corpora in 68 different language varieties, which
vary in size from 2M to 150M words. The cor-
pora are thus of a respectable size, but only 1% of
the world?s languages are represented. A further
difficulty is that languages are named, without the
corresponding ISO language codes.
The Leipzig Corpora Collection (LCC)
3
(Bie-
mann et al., 2007) provides download of corpora
in 117 languages, and dictionaries in a number of
others, bringing the total number of represented
languages up to 230. The corpora are large, read-
ily available, in plain-text, and labelled with ISO
language codes.
The Cr?ubad?an Project aims to crawl the web for
text in low-resource languages, and data is cur-
rently available for 1872 languages. This rep-
resents a significant portion of the world?s lan-
guages; unfortunately, due to copyright restric-
3
http://corpora.uni-leipzig.de
tions, only lists of n-grams and their frequencies
are publically available, not the texts themselves.
While the breadth of languages covered makes this
a useful resource for cross-linguistic research, the
lack of actual texts means that only a limited range
of applications are possible with this data.
Cross-linguistic projects. Responding to the
call to document and preserve the world?s lan-
guages, highly cross-linguistic projects have
sprung up, striving towards the aim of universality.
Of particular note are the Endangered Languages
Project, and the Rosetta Project. These projects
are to be praised for their commitment to univer-
sality, but in their current forms it is difficult to use
their data to perform large-scale NLP.
3 The Data
3.1 Universal Corpus and Data Structure
Building on their previous paper, Abney and Bird
(2011) describe the data structure they envisage
for a Universal Corpus in more detail, and we aim
to adopt this structure where possible. Two types
of text are distinguished:
Aligned texts consist of parallel documents,
aligned at the document, sentence, or word level.
Note that monolingual documents are viewed as
aligned texts only tied to a single language.
Analysed texts, in addition to the raw text, con-
tain more detailed annotations including parts of
speech, morphological information, and syntactic
relations. This is stored as a table, where rows rep-
resent words, and columns represent: document
ID, language code, sentence ID, word ID, word-
form, lemma, morphological information, part of
speech, gloss, head/governor, and relation/role.
Out of our data sources, three can be straight-
forwardly represented in the aligned text struc-
ture. However, ODIN contains richer annotations,
which are in fact difficult to fit into Abney and
Bird?s proposal, and which we discuss in section
3.2 below.
3.2 Data Sources
Although data size matters in general NLP, uni-
versality is the top priority for a Universal Corpus.
We focus on the following data sources, because
they include a large number of languages, include
several parallel texts, and demonstrate a variety of
data types which a linguist might encounter (struc-
tured, semi-structured, unstructured): the Online
79
Langs. Families Tokens Size
ODIN 1,270 100 351,161 39 MB
Omniglot 129 20 31,318 677 KB
UDHR 352 46 640,588 5.2 MB
Wikipedia 271 21 37 GB
Combined 1,451 105
Table 1: Corpus Coverage
Database of Interlinear Text (ODIN), the Om-
niglot website, the Universal Declaration of Hu-
man Rights (UHDR), and Wikipedia.
Our resulting corpus runs the full gamut of text
types outlined by Abney and Bird, ranging from
single-language text (Wikipedia) to parallel text
(UDHR and Omniglot) to IGTs (ODIN). Table 1
gives some coverage statistics, and we describe
each source in the following subsections. For 332
languages, the corpus contains data from more
than one source.
Universal Declaration of Human Rights. The
Universal Declaration of Human Rights (UDHR)
is a document released by the United Nations in
1948, and represents the first global expression of
human rights. It consists of 30 articles, amounting
to about four pages of text. This is a useful doc-
ument for NLP, since it has been translated into a
wide variety of languages, providing a highly par-
allel text.
Wikipedia. Wikipedia is a collaboratively-
edited encyclopedia, appealing to use for NLP
because of its large size and easy availability.
At the time of writing, it contained 30.8 million
articles in 286 languages, which provides a
sizeable amount of monolingual text in a fairly
wide range of languages. Text dumps are made
regularly available, and can be downloaded from
http://dumps.wikimedia.org.
Omniglot. The Omniglot website
4
is an online
encyclopedia of writing systems and languages.
We extract information from pages on ?Useful for-
eign phrases? and the ?Tower of Babel? story, both
of which give us parallel data in a reasonably large
number of languages.
ODIN. ODIN (The Online Database of Inter-
linear Text) is a repository of interlinear glossed
texts (IGTs) extracted from scholarly documents
(Lewis, 2006; Lewis and Xia, 2010). Compared to
other resources, it is notable for the breadth of lan-
4
http://www.omniglot.com
guages included and the level of linguistic annota-
tion. An IGT canonically consists of three lines:
(i) the source, a sentence in a target language, (ii)
the gloss, an analysis of each source element, and
(iii) the translation, done at the sentence level. The
gloss line can additionally include a number of lin-
guistic terms, which means that the gloss is written
in metalanguage rather than natural language. In
ODIN, translations are into English, and glosses
are written in an English-based metalanguage. An
accepted set of guidelines are given by the Leipzig
Glossing Rules,
5
where morphemes within words
are separated by hyphens (or equal signs, for cli-
tics), and the same number of hyphens should ap-
pear in each word of the source and gloss.
The data from ODIN poses the first obstacle to
straightforwardly adopting Abney and Bird?s pro-
posal. The suggested data structure is aligned at
the word level, and includes a specific list of rel-
evant features which should be used to annotate
words. When we try to adapt IGTs into this for-
mat, we run into certain problems. Firstly, there
is the problem that the most fundamental unit of
analysis according to the Leipzig Glossing Rules
is the morpheme, not the word. Ideally, we should
encode this information explicitly in a Universal
Corpus, assigning a unique identifier to each mor-
pheme (instead of, or in addition to each word).
Indeed, Haspelmath (2011) argues that there is no
cross-linguistically valid definition of word, which
undermines the central position of words in the
proposed data structure.
Secondly, it is unclear how to represent the
gloss. Since the gloss line is not written in a natu-
ral language, we cannot treat it as a simple trans-
lation. However, it is not straightforward to incor-
porate it into the proposed structure for analysed
texts, either. One possible resolution is to move
all elements of the gloss written in capital letters to
the MORPH field (as functional elements are usu-
ally annotated in this way), and all remaining el-
ements to the GLOSS field. However, this loses
information, since we no longer know which mor-
pheme has which meaning. To keep all informa-
tion encoded in the IGT, we need to modify Abney
and Bird (2011)?s proposal.
The simplest solution we can see is to allow
morphemes to be a level of structure in the Uni-
versal Corpus, just as documents, sentences, and
5
http://www.eva.mpg.de/lingua/
resources/glossing-rules.php
80
Figure 1: Heatmap of languages in SeedLing according to endangerment status
words already are. The overall architecture re-
mains unchanged. We must then decide how to
represent the glosses.
Even though glosses in ODIN are based on
English, having been extracted from English-
language documents, this is not true of IGTs in
general. For example, it is common for documen-
tary linguists working on indigenous languages of
the Americas to provide glosses and translations
based on Spanish. For this reason, we believe it
would be wise to specify the language used to pro-
duce the gloss. Since it is not quite the language
itself, but a metalanguage, one solution would be
to use new language codes that make it clear both
that a metalanguage is being used, and also what
natural language it is based on. The five-letter
code gloss cannot be confused with any code
in any version of ISO 639 (with codes of length
two to four). Following the convention that sub-
varieties of a language are indicated with suffixes,
we can append the code of the natural language.
For example, glosses into English and Spanish-
based metalanguages would be given the codes
gloss-eng and gloss-spa, respectively.
One benefit of this approach is that glossed texts
are treated in exactly the same way as parallel
texts. There is a unique identifier for each mor-
pheme, and glosses are stored under this identifier
and the corresponding gloss code. Furthermore,
to motivate the important place of parallel texts in
a Universal Corpus, Abney and Bird view trans-
lations into a high-resource reference language as
a convenient surrogate of meaning. By the same
reasoning, we can use glosses to provide a more
detailed surrogate of meaning, only written in a
metalanguage instead of a natural one.
3.3 Representation and Universality
According to Ethnologue, there are 7105 liv-
ing languages, and 147 living language families.
Across all our data sources, we manage to cover
1451 languages in 105 families, which represents
19.0% of the world?s languages. To get a bet-
ter idea of the kinds of languages represented,
we give a breakdown according to their EGIDS
scores (Expanded Graded Intergenerational Dis-
ruption Scale) (Lewis and Simons, 2010) in Fig-
ure 1. The values in each cell have been colored
according to proportion of languages represented,
with green indicating good coverage and red poor.
It?s interesting to note that vigorous languages (6a)
are poorly represented across all data sources, and
worse than more endangered categories. In terms
of language documentation, vigorous languages
are less urgent goals than those in categories 6b
and up, but this highlights an unexpected gap in
linguistic resources.
4 Data Clean-Up, Consistency, and
Standardisation
Consistency in data structures and formatting is
essential to facilitate use of data in computational
linguistics research (Palmer et al., 2010). In the
following subsections, we describe the process-
ing required to convert the data into a standardised
form. We then discuss standardisation of language
codes and file formats.
81
4.1 Case Studies
UDHR. We used the plain-text UDHR files
available from the Unicode website
6
which uses
UTF-8 encoding for all languages. The first four
lines of each file record metadata, and the rest is
the translation of the UDHR. This dataset is ex-
tremely clean, and simply required segmentation
into sentences.
Wikipedia. One major issue with using the
Wikipedia dump is the problem of separating text
from abundant source-specific markup. To con-
vert compressed Wikipedia dumps to textfiles, we
used the WikiExtractor
7
tool. After conversion
into textfiles, we used several regular expressions
to delete residual Wikipedia markup and so-called
?magic words?.
8
Omniglot. The main issue with extracting the
Omniglot data is that the pages are designed to
be human-readable, not machine-readable. Clean-
ing this data required parsing the HTML source,
and extracting the relevant content, which required
different code for the two types of page we con-
sidered (?Useful foreign phrases? and ?Tower of
Babel?). Even after automatic extraction, some
noise in the data remained, such as explanatory
notes given in parentheses, which are written in
English and not the target language. Even though
the total amount of data here is small compared to
our other sources, the amount of effort required
to process it was not, because of these idiosyn-
cracies. We expect that researchers seeking to
convert data from human-readable to machine-
readable formats will encounter similar problems,
but unfortunately there is unlikely to be a one-size-
fits-all solution to this problem.
ODIN. The ODIN data is easily accessible in
XML format from the online database
9
. Data
for each language is saved in a separate XML
file and the IGTs are encoded in tags of the form
<igt><example>...</example></igt>.
For example, the IGT in Figure 2 is represented
by the XML snippet in Figure 3.
The primary problem in extracting the data is a
lack of consistency in the IGTs. In the above ex-
6
http://unicode.org/udhr/d
7
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
8
http://en.wikipedia.org/wiki/Help:
Magic_words
9
http://odin.linguistlist.org/download
21 a. o lesu mai
2sg return here
?You return here.?
Figure 2: Fijian IGT from ODIN
<igt>
<example>
<line>21 a. o lesu mai</line>
<line>2sg return here</line>
<line>?You return here.?</line>
</example>
</igt>
Figure 3: Fijian IGT in ODIN?s XML format
amples, the sentence is introduced by a letter or
number, which needs to be removed; however, the
form of such indexing elements varies. In addi-
tion, the source line in Figure 4 includes two types
of metadata: the language name, and a citation,
both of which introduce noise. Finally, extrane-
ous punctuation such as the quotation marks in the
translation line need to be removed. We used regu-
lar expressions for cleaning lines within the IGTs.
4.2 Language Codes
As Xia et al. (2010) explain, language names do
not always suffice to identify languages, since
many names are ambiguous. For this reason, sets
of language codes exist to more accurately identify
languages. We use ISO 639-3
10
as our standard set
of codes, since it aims for universal coverage, and
has widespread acceptance in the community. The
data from ODIN and the UDHR already used this
standard.
To facilitate the standardization of language
codes, we have written a python API that can be
used to query information about a language or a
code, fetching up-to-date information from SIL
International (which maintains the ISO 639-3 code
set), as well as from Ethnologue.
Wikipedia uses its own set of language codes,
most of which are in ISO 639-1 or ISO 639-3.
The older ISO 639-1 codes are easy to recognise,
being two letters long instead of three, and can
be straightforwardly converted. However, a small
number of Wikipedia codes are not ISO codes at
all - we converted these to ISO 639-3, following
10
http://www-01.sil.org/iso639-3/
default.asp
82
<igt>
<example>
<line>(69) na-Na-tmi-kwalca-t
Yimas (Foley 1991)</line>
<line>3sgA-1sgO-say-rise-PERF
</line>
<line>?She woke me up?
(by verbal action)</line>
</example>
</igit>
Figure 4: Yimas IGT in ODIN?s XML format
documentation from the Wikimedia Foundation.
11
Omniglot does not give codes at all, but only the
language name. To resolve this issue, we automat-
ically converted language names to codes using in-
formation from the SIL website.
Some languages have more than one orthog-
raphy. For example, Mandarin Chinese is writ-
ten with either traditional or simplified charac-
ters; Serbian is written with either the Cyrillic or
the Roman alphabet. For cross-linguistic NLP, it
could be helpful to have standard codes to identify
orthographies, but at present none exist.
4.3 File Formats
It is important to make sure that the data we have
compiled will be available to future researchers,
regardless of how the surrounding infrastructure
changes. Bird and Simons (2003) describe a set of
best practices for maintaining portability of digi-
tal information, outlining seven dimensions along
which this can vary. Following this advice, we
have ensured that all our data is available as plain-
text files, with UTF-8 encoding, labelled with the
relevant ISO 639-3 code. Metadata is stored sepa-
rately. This allows users to easily process the data
using the programming language or software of
their choice.
To allow access to the data following Abney
and Bird?s guidelines, as discussed in section 3,
we have written an API, which we distribute along
with the data. Abney and Bird remain agnostic
to the specific file format used, but if an alterna-
tive format would be preferred, the data would
be straightfoward to convert since it can be ac-
cessed according to these guidelines. As exam-
ples of functionality, our API allows a user to fetch
all sentences in a given language, or all sentences
from a given source.
11
http://meta.wikimedia.org/wiki/
Special_language_codes
5 Detecting Similar Languages
To exemplify the use of SeedLing for compu-
tational research on low-resource languages, we
experiment with automatic detection of similar
languages. When working on endangered lan-
guages, documentary and computational linguists
alike face a lack of resources. It is often helpful to
exploit lexical, syntactic or morphological knowl-
edge of related languages. For example, similar
high-resource languages can be used in bootstrap-
ping approaches, such as described by Yarowsky
and Ngai (2001) or Xia and Lewis (2007).
Language classification can be carried out in
various ways. Two common approaches are ge-
nealogical classification, mapping languages onto
family trees according to their historical related-
ness (Swadesh, 1952; Starostin, 2010); and ty-
pological classification, grouping languages ac-
cording to linguistic features (Georgi et al., 2010;
Daum?e III, 2009). Both of these approaches re-
quire linguistic analysis. By contrast, we use
surface features (character n-gram and word uni-
gram frequencies) extracted from SeedLing, and
apply an off-the-shelf hierarchical clustering al-
gorithm.
12
Specifically, each language is repre-
sented as a vector of frequencies of character bi-
grams, character trigrams, and word unigrams.
Each of these three components is normalised to
unit length. Data was taken from ODIN, Om-
niglot, and the UDHR.
Experimental Setup. We first perform hierar-
chical clustering, which produces a tree structure:
each leaf represents a language, and each node
a cluster. We use linkage methods, which recur-
sively build the tree starting from the leaves. Ini-
tially, each language is in a separate cluster, then
we iteratively find the closest two clusters and
merge them. Each time we do this, we take the
two corresponding subtrees, and introduce a new
node to join them.
We define the distance between two clusters by
considering all possible pairs of languages, with
one from each cluster, and taking the largest dis-
tance. We experimented with other ways to de-
fine the distance between clusters, but results were
poor and we omit results for brevity.
To ease evaluation, we produce a partitional
clustering, by stopping when we reach a certain
number of clusters, set in advance.
12
http://www.scipy.org
83
Precision Recall F-score
SeedLing 0.255 0.205 0.150
Base. 1: random 0.184 0.092 0.068
Base. 2: together 0.061 1.000 0.112
Base. 3: separate 1.000 0.086 0.122
Table 2: Clustering compared with baselines
Figure 5: Performance against number of clusters
Evaluation. We compare our clustering to the
language families in Ethnologue. However, there
are many ways to evaluate clustering quality.
Amig?o et al. (2009) propose a set of criteria which
a clustering evaluation metric should satisfy, and
demonstrate that most popular metrics fail to sat-
isfy at least one of these criteria. However, they
prove that all criteria are satisfied by the BCubed
metric, which we therefore adopt. To calculate the
BCubed score, we take the induced cluster and
gold standard class for each language, and cal-
culate the F-score of the cluster compared to the
class. These F-scores are then averaged across all
languages.
In Table 2, we set the number of clusters to be
105, the number of language families in our data,
and compare this with three baselines: a random
baseline (averaged over 20 runs); putting all lan-
guages in a single cluster; and putting each lan-
guage in a separate cluster. Our clustering outper-
forms all baselines. It is worth noting that pre-
cision is higher than recall, which is perhaps ex-
pected, given that related languages using wildly
differing orthographies will appear distinct.
To allow a closer comparison with Georgi et al.
(2010), we calculate pairwise scores - i.e. consid-
ering if pairs of languages are in the same cluster
or the same class. For 105 clusters, we achieve
a pairwise f-score of 0.147, while Georgi et al.
report 0.140. The figures are not quite compa-
rable since we are evaluating over a different set
of languages; nonetheless, we only use surface
features, while Georgi et al. use typological fea-
tures from WALS. This suggests the possibility for
cross-linguistic research to be conducted based on
shallow features.
In Figure 5, we vary the number of clusters. The
highest f-score is obtained for 199 clusters. There
is a notable jump in performance between 98 and
99, just before the true number of families, 105.
Interpreting the clusters directly is difficult, be-
cause they are noisy. However, the distribution of
cluster sizes mirrors the true distribution - for 105
clusters, we have 48 clusters of size 1 or 2, with
the largest cluster of size 130; while in our gold
standard, there are 51 families with only 1 or 2
languages in the data, with the largest of size 150.
6 Conclusion and Outlook
In this paper, we have described the creation of
SeedLing, a foundation text for a Universal Cor-
pus, following the guidelines of Abney and Bird
(2010; 2011). To do this, we cleaned and standard-
ised data from several multilingual data sources:
ODIN, Omniglot, the UDHR, Wikipedia. The
resulting corpus is more easily machine-readable
than any of the underlying data sources, and has
been stored according to the best practices sug-
gested by Bird and Simons (2003). At present,
SeedLing has data from 19% of the world?s liv-
ing languages, covering 72% of language families.
We believe that a corpus with such diversity of lan-
guages, uniformity of format, cleanliness of data,
and ease of access provides an excellent seed for a
Universal Corpus. It is our hope that taking steps
toward creating this resource will spur both further
data contributions and interesting computational
research with cross-linguistic or typological per-
spectives; we have here demonstrated SeedLing?s
utility for such research by using the data to per-
form language clustering, with promising results.
SeedLing (data, API and documentation) is cur-
rently available via a GitHub repository.
13
We
have yet to fully address questions of long-term
access, and we welcome ideas or collaborations
along these lines.
13
https://github.com/alvations/SeedLing
84
Acknowledgements
We thank the three anonymous reviewers for their
helpful comments. This research was supported
in part by the Cluster of Excellence ?Multi-modal
Computing and Interaction? in the German Excel-
lence Initiative.
References
Steven Abney and Steven Bird. 2010. The Hu-
man Language Project: Building a Universal Cor-
pus of the world?s languages. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 88?97. Association for
Computational Linguistics.
Steven Abney and Steven Bird. 2011. Towards a data
model for the Universal Corpus. In Proceedings of
the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 120?127. Association for Computational Lin-
guistics.
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information retrieval, 12(4):461?486.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig Corpora
Collection-monolingual corpora of standard size.
Proceedings of Corpus Linguistic 2007.
Steven Bird and Gary Simons. 2003. Seven dimen-
sions of portability for language documentation and
description. Language, pages 557?582.
Hal Daum?e III. 2009. Non-parametric bayesian areal
linguistics. In Proceedings of human language tech-
nologies: The 2009 annual conference of the north
american chapter of the association for computa-
tional linguistics, pages 593?601. Association for
Computational Linguistics.
Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, pages 385?393. Association for
Computational Linguistics.
Martin Haspelmath. 2011. The indeterminacy of word
segmentation and the nature of morphology and syn-
tax. Folia Linguistica, 45(1):31?80.
M Paul Lewis and Gary F Simons. 2010. Assessing
endangerment: expanding fishman?s GIDS. Revue
roumaine de linguistique, 2:103?119.
William D Lewis and Fei Xia. 2010. Developing
ODIN: A multilingual repository of annotated lan-
guage data for hundreds of the world?s languages.
Literary and Linguistic Computing, 25(3):303?319.
William D Lewis. 2006. ODIN: A model for adapt-
ing and enriching legacy infrastructure. In e-Science
and Grid Computing, 2006. e-Science?06. Second
IEEE International Conference on, pages 137?137.
IEEE.
Mike Maxwell and Baden Hughes. 2006. Frontiers in
linguistic annotation for lower-density languages. In
Proceedings of the workshop on frontiers in linguis-
tically annotated corpora 2006, pages 29?37. Asso-
ciation for Computational Linguistics.
Alexis Palmer, Taesun Moon, Jason Baldridge, Katrin
Erk, Eric Campbell, and Telma Can. 2010. Compu-
tational strategies for reducing annotation effort in
language documentation. Linguistic Issues in Lan-
guage Technology, 3.
Gary F Simons and M Paul Lewis. 2011. The world?s
languages in crisis: A 20-year update. In 26th
Linguistic Symposium: Language Death, Endanger-
ment, Documentation, and Revitalization. Univer-
sity of Wisconsin, Milwaukee, pages 20?22.
George Starostin. 2010. Preliminary lexicostatistics as
a basis for language classification: a new approach.
Journal of Language Relationship, 3:79?117.
Morris Swadesh. 1952. Lexico-statistic dating of pre-
historic ethnic contacts: with special reference to
north american indians and eskimos. Proceedings of
the American philosophical society, pages 452?463.
J?org Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In LREC, pages 2214?2218.
Fei Xia and William D Lewis. 2007. Multilingual
structural projection across interlinear text. In HLT-
NAACL, pages 452?459.
Fei Xia, Carrie Lewis, and William D Lewis. 2010.
The problems of language identification within
hugely multilingual data sets. In LREC.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of NAACL-2001, pages 200?207.
85
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 86?90,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Short-term projects, long-term benefits:
Four student NLP projects for low-resource languages
Alexis Palmer and Michaela Regneri
Department of Computational Linguistics
Saarland University
Saarbr?ucken, Germany
{apalmer,regneri}@coli.uni-saarland.de
Abstract
This paper describes a local effort to
bridge the gap between computational and
documentary linguistics by teaching stu-
dents and young researchers in computa-
tional linguistics about doing research and
developing systems for low-resource lan-
guages. We describe four student software
projects developed within one semester.
The projects range from a front-end for
building small-vocabulary speech recogni-
tion systems, to a broad-coverage (more
than 1000 languages) language identifi-
cation system, to language-specific sys-
tems: a lemmatizer for the Mayan lan-
guage Uspanteko and named entity recog-
nition systems for both Slovak and Per-
sian. Teaching efforts such as these are an
excellent way to develop not only tools for
low-resource languages, but also computa-
tional linguists well-equipped to work on
endangered and low-resource languages.
1 Introduction
There is a strong argument to be made for bring-
ing together computational and documentary lin-
guistics in order to support the documentation and
description of endangered languages (Abney and
Bird, 2010; Bird, 2009). Documentation, de-
scription, and revitalization work for endangered
languages, as well as efforts to produce digi-
tal and machine-readable resources for languages
currently lacking such data, benefit from techno-
logical support in many different ways. Here we
focus on support via (a) tools facilitating more effi-
cient development of resources, with easy learning
curves, and (b) linguistic analysis tools.
Various meetings and workshops in recent years
have helped to bring the two fields closer to-
gether, but a sizeable gap remains. We?ve come
far enough to, for example, have a relevant work-
shop at a major computational linguistics confer-
ence, but not so far that issues around language en-
dangerment are well-known to even a large subset
of the computational linguistics community. One
way to get computational linguists thinking about
issues related to endangered languages is for them
to get their hands dirty ? to work directly on re-
lated projects. In this paper we describe our own
local effort to bridge this gap: a course for Mas-
ter?s and Bachelor?s students in computational lin-
guistics in which small teams of students each pro-
duced working, non-trivial natural language pro-
cessing (NLP) tools for low-resource languages
(LRLs) over the span of a single semester. The
individual projects are described in Section 3.
Such a course benefits the students in a num-
ber of ways. They get hands-on experience in
system building, they learn about a new subfield
within computational linguistics, with a different
set of concerns (some of these are discussed in
Section 2), and, in some cases, they get the op-
portunity to develop tools for their own native lan-
guages. From the perspective of computational
work on endangered languages, the positive out-
comes are not only a new set of NLP tools, but
also a group of students and young researchers
armed with experience working on low-resource
languages and better equipped to take on similar
projects in the future.
2 Teaching NLP for LRLs
Working on LRLs from a computational perspec-
tive requires training beyond the typical compu-
tational linguistics curriculum. It is not the case
that the most widely-used methods from computa-
tional linguistics can be straightforwardly adapted
for any arbitrarily-selected language. Thus an im-
portant part of our teaching agenda in this context
is to familiarize students with the challenges inher-
ent to NLP for LRLs as well as some of the main
86
approaches for addressing these same challenges.
This section briefly surveys some of the relevant
issues, with pointers to representative studies.
The first and most obvious concern is data spar-
sity. Many of the most successful and widely-
taught methods and models in computational lin-
guistics rely on either large amounts of labeled
data or massive amounts of unlabeled data. Meth-
ods and models explicitly addressing LRLs need
to maximize the utility of available data. Ap-
proaches for addressing data sparsity range from
data collection proposals (Abney and Bird, 2010)
to leveraging high-resource languages (Xia and
Lewis, 2007) to maximizing annotation effort
(Garrette and Baldridge, 2013). A second con-
cern is model suitability. Many existing models
in computational linguistics implicitly encode or
expect characteristics of high-resource languages
(Bender, 2011); for example, much work on com-
putational syntax uses models that exploit linear
ordering of elements in utterances. Such models
are not straightforwardly applicable for languages
with free or flexible word order, nor for highly
agglutinative languages where, for example, com-
plete utterances are encoded as single words. Ap-
proaches to this issues include adaptation of mod-
els using linguistic knowledge and/or universals
(Boonkwan and Steedman, 2011; Naseem et al.,
2010). The third issue to note is the difficulty
of evaluation. The output of systems or tools
performing automated analysis are predictions of
analyses for new data; these predictions must
be evaluated against a ground truth or human-
supplied analysis of the same data. Evaluation
is difficult in the low-resource setting, both be-
cause of limited availability of expert-labeled data
and because, in some cases, the ground truth
isn?t known, or analyses are shifting as knowledge
about the language develops.
We began the course with a discussion of these
issues, as well as an introduction to a range of ex-
isting tools, projects and resources. We did not
explicitly teach programming skills in the course,
but we also did not require extensive program-
ming background. Rather, we aimed to balance
the teams such that each contained a mix of back-
grounds: a bit more than half of the students
had previous experience with software develop-
ment, and the rest had at least taken one intro-
ductory programming course. The projects were
scoped such that there were clear ways for stu-
dents without programming experience to con-
tribute. For example, in some cases, students with
extensive background in linguistics performed lin-
guistic analysis of the data which informed the de-
sign of the system.
Evaluation of students was designed to empha-
size three objectives: production of a working sys-
tem, communication of challenges faced and so-
lutions to those challenges, and personal devel-
opment of professionally-relevant skills. Students
were graded on their weekly progress (more detail
in Section 3), one 15-20 minute talk per student,
individual written reports detailing specific contri-
butions to the project, and a conference-style end-
of-semester poster and demo session. Systems
were required to be working and demonstratable
both at the midway point of the semester (as a sim-
plified prototype) and at the end of the semester.
3 Four projects in four months
The course described here (?NLP tools for Low-
Resource Languages?) was offered as part of the
regular curriculum for undergraduate and gradu-
ate students in the Computational Linguistics de-
partment at Saarland University. We started with
10 students and formed four teams (based on pref-
erences for general topics and programming lan-
guages). The teams could choose their own project
or select from a set of proposed topics.
During the teaching period, we regularly moni-
tored the student?s progress by using some meth-
ods of agile software development.
1
For each
weekly meeting, each team had to set three goals
which constituted their homework. Goals could be
minor tasks (fixing a certain bug), bigger chunks
(choosing and implementing a strategy for data
standardization) or course requirements (prepar-
ing a talk). Not fulfilling a (project-related) goal
was acceptable, but students had to analyze why
they missed the goal and to learn from the experi-
ence. They were expected over the course of the
semester to become better both at setting reach-
able goals and at estimating how long they would
need to meet each goal. Under this obligation to
make continuous, weekly progress, each team had
a working system within three months. At the end
of month four, systems were suitable for demon-
stration at the poster session.
The projects differ according to their scopes and
goals, as well as their immediate practical utility.
1
http://en.wikipedia.org/wiki/Agile_software_development
87
One project (3.1) makes previous research accessi-
ble to users by developing an easy-to-use frontend;
a second project (3.2) aims to extend the num-
ber of languages addressed for an existing multi-
lingual classification task; and the remaining two
(3.3 and 3.4) implement language-specific solu-
tions for individual language processing tasks. We
additionally required that each project be open-
source; the public code repositories are linked in
the respective sections.
3.1 Small-vocabulary ASR for any language
This project
2
builds on existing research for small-
vocabulary (up to roughly 100 distinct words)
speech recognition. Such technology is desirable
for, among other things, developing speech inter-
faces to mobile applications (e.g. to deliver med-
ical information or weather reports; see Sherwani
(2009)), but dedicated speech recognition engines
are available only for a relatively small number
of languages. For small-vocabulary applications,
though, an existing recognizer for a high-resource
language can be used to do recognition in the tar-
get language, given a pronunciation lexicon map-
ping the relevant target language words into se-
quences of sounds in the high-resource language.
This project produces the required lexicon.
Building on the algorithms developed by Qiao
et al. (2010) and Chan and Rosenfeld (2012), two
students developed an easy-to-use interface that
allows a user with no knowledge of speech tech-
nologies to build and test a system to recognize
words spoken in the target language. In its cur-
rent implementation, the system uses the English-
language recognizer from the freely-available Mi-
crosoft Speech Platform;
3
for this reason, the sys-
tem is available for Windows only. To build a rec-
ognizer for a target language, a user needs only
to specify a written form and upload one or more
audio samples for each word in the vocabulary;
generally, the more audio samples per word, the
better the performance. The students additionally
implemented a built-in recorder; this means a user
can spontaneously make recordings for the desired
words. Finally, the system includes implementa-
tions of two different variants of the algorithm and
an evaluation module, thus facilitating use for both
research and development purposes.
The main challenges for this project involved
managing the interaction between the algorithm
2
https://github.com/lex4all/lex4all
3
http://msdn.microsoft.com/en-us/library/hh361572
and the Microsoft speech recognition platform, as
well as getting familiar with development in Win-
dows. The practical utility of this project is imme-
diately evident: any user with a Windows machine
can install the necessary components and have a
working small-vocabulary recognizer within sev-
eral hours. Of course, more time and data may
be required to improve performance of the rec-
ognizer, which currently reaches in the mid-70s
with five audio samples per word. These results,
as well as further details about the system (includ-
ing where to download the code, and discussion
of substituting other high-resource language rec-
ognizers), are described in Vakil et al. (2014).
3.2 Language ID for many languages
This project
4
addresses the task of language iden-
tification. Given a string of text in an arbitrary lan-
guage, can we train a system to recognize what
language the text is written in? Excellent classifi-
cation rates have been achieved in previous work,
but for a relatively small number of languages, and
the task becomes noticeably more difficult as the
number of languages increases (Baldwin and Lui,
2010; Lui and Baldwin, 2012, for example). With
few exceptions (Brown, 2013; Xia et al., 2010; Xia
et al., 2009), existing systems have only attempted
to distinguish between fewer than 200 of the thou-
sands of written languages currently in use. This
team of three students aimed to expand coverage
of language identification systems as much as pos-
sible given existing sources of data.
To do this, they first needed to gather and stan-
dardize data from various sources. They targeted
three sources of data: the Universal Declaration
of Human Rights, Wikipedia,
5
ODIN (Lewis and
Xia, 2010), and some portions of the data avail-
able from Omniglot.
5
The challenges faced by this
group lay primarily in two areas: issues involv-
ing data and those involving classification. In the
first area, they encountered expected and well-
known issues such as clean-up and standardization
of data, dealing with encoding issues, and manag-
ing large amounts of data. The second set of chal-
lenges have to do with the high degree of skew
in the data collected. Though their system covers
over 1000 languages, the amount of data per lan-
guage ranges from a single sentence to hundreds
of thousands of words. Along the way, the stu-
dents realized that this collection of data in a stan-
4
https://github.com/alvations/SeedLing
5
http://www.wikipedia.com,http://www.omniglot.com
88
dard, machine-readable form is useful for many
other purposes. The corpus and how to access it
are described in Emerson et al. (2014). A second
paper presenting the language identification re-
sults (including those for low-resource languages)
is planned for later this year.
3.3 A lemmatizer for Uspanteko
The third project
6
involved implementing a lem-
matizer for the Mayan language Uspanteko. Us-
ing data that had been cleaned, standardized (as
described in Palmer et al. (2010)), and made avail-
able through the Archive of Indigenous Languages
of Latin America,
7
these three students imple-
mented a tool to identify the citation form for in-
flected word forms in texts. The lemmatization
algorithm is based on longest common substring
matching: the closest match for an inflected form
is returned as the lemma. Additionally, a table for
irregular verb inflections was generated using the
annotated source corpus (roughly 50,000 words)
and an Uspanteko-Spanish dictionary (Can Pix-
abaj et al., 2007), to map inflected forms translated
with the same Spanish morpheme.
This group more than any other faced the chal-
lenge of evaluation. Not all lemmas covered in
the texts appear in the dictionary, and the Uspan-
teko texts, though fully analyzed with morphologi-
cal segmentation and glossing, part of speech tags,
and translation into Spanish, do not include cita-
tion forms. Manual evaluation of 100 sentences,
for which a linguist on the team with knowledge
of Spanish determined citation forms, showed ac-
curacy of 59% for the lemmatization algorithm.
3.4 NER for Slovak & Persian
Finally, the fourth project
8
(two students) chose
to tackle the task of named entity recognition
(NER): identifying instances of named entities
(NEs, e.g. people, locations, geopolitical entities)
in texts and associating them with appropriate la-
bels. The students developed a single platform to
do NER in both Slovak and Persian, their native
languages. The approach is primarily based on us-
ing gazetteers (for person names and locations), as
well as regular expressions (for temporal expres-
sions). The students collected the gazeteers for the
two languages as part of the project. Their sys-
tem builds on a modular design; one can swap out
6
https://code.google.com/p/mayan-lemmatizer/
7
http://www.ailla.utexas.org
8
https://code.google.com/p/named\-entity\-tagger/
gazetteers and a few language-specific heuristic
components to perform NER in a new language.
In this project, resource acquisition and evalua-
tion were the main challenges. The students used
some existing resources for both languages, but
also devoted quite some time to producing new
gazetteers. For Slovak, additional challenges were
presented by the language?s large number of in-
flectional cases and resulting variability in form.
For example, some inflected forms used to re-
fer to people from a given location are string-
identical to the names of the locations with a dif-
ferent case inflection. In Persian, the main chal-
lenges were detection of word boundaries (many
names are multi-word expressions) and frequent
NE/proper noun ambiguities. For evaluation, the
students hand-labeled over 35,000 words of Slo-
vak (with 545 NE instances) and about 600 para-
graphs of Persian data (306 NE instances). Perfor-
mace varies across named entity category: tempo-
ral expression matching is most reliable (f-score
0.96 for Slovak, 0.89 for Persion), followed by
locations (0.78 Slovak, 0.92 Persian) and person
names (0.63 Slovak, 0.87 Persian). Note that for
Persian, only NEs with correctly matched bound-
aries are counted (which are 50% for persons).
4 Conclusion
In this paper we have presented four student soft-
ware projects, each one addressing a different
NLP task relevant for one or more low-resource
languages. The successful outcomes of the four
projects show that much progress can be made
even with limited time and limited prior expe-
rience developing such systems. Local teach-
ing efforts such as these can be highly success-
ful in building a group of young researchers who
are both familiar with issues surrounding low-
resource and endangered languages and prepared
to do research and development in this area in the
future. We think of this as planting seeds for an
early harvest: with one semester?s combined effort
between instructors and students, we reap the re-
wards of both new tools and new researchers who
can continue to work on closing the gap between
computational and documentary linguistics.
Course materials are publicly available from the
course homepage,
9
and from the project reposito-
ries linked from the descriptions in Section 3.
9
http://www.coli.uni-saarland.de/courses/cl4lrl-swp/
89
Acknowledgements
First of all, we want to thank the students who par-
ticipated in our course and put so much effort and
passion in their projects. They are (in alphabeti-
cal order): Christine Bocionek, Guy Emerson, Su-
sanne Fertmann, Liesa Heuschkel, Omid Moradi-
annasab, Michal Petko, Maximilian Paulus, Alek-
sandra Piwowarek, Liling Tan and Anjana Vakil.
Further, we want to thank the anonymous review-
ers for their helpful comments. The second author
was funded by the Cluster of Excellence ?Multi-
modal Computing and Interaction? in the German
Excellence Initiative.
References
Steven Abney and Steven Bird. 2010. The Human
Language Project: Building a universal corpus of the
world?s languages. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 88?97. Association for Computa-
tional Linguistics.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the matter.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 229?237, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Emily M Bender. 2011. On achieving and evaluating
language-independence in NLP. Linguistic Issues in
Language Technology, 6(3):1?26.
Steven Bird. 2009. Natural language processing
and linguistic fieldwork. Computational Linguis-
tics, 35(3):469?474.
Prachya Boonkwan and Mark Steedman. 2011. Gram-
mar induction from text using small syntactic proto-
types. In IJCNLP, pages 438?446.
Ralf D Brown. 2013. Selecting and weighting n-grams
to identify 1100 languages. In Text, Speech, and Di-
alogue, pages 475?483. Springer.
Telma Angelina Can Pixabaj, Oxlajuuj Keej Maya?
Ajtz?iib? (Group) Staff, and Centro Educativo y Cul-
tural Maya Staff. 2007. Jkemiix yalaj li uspanteko.
Cholsamaj Fundacion, Guatemala.
Hao Yee Chan and Roni Rosenfeld. 2012. Discrimi-
native pronunciation learning for speech recognition
for resource scarce languages. In Proceedings of the
2nd ACM Symposium on Computing for Develop-
ment, page 12. ACM.
Guy Emerson, Liling Tan, Susanne Fertmann, Alexis
Palmer, and Michaela Regneri. 2014. SeedLing:
Building and using a seed corpus for the Human
Language Project. In Proceedings of ACL Workshop
on the use of computational methods in the study of
endangered languages (ComputEL).
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL-HLT, pages 138?147.
William D Lewis and Fei Xia. 2010. Developing
ODIN: A multilingual repository of annotated lan-
guage data for hundreds of the world?s languages.
Literary and Linguistic Computing, 25(3):303?319.
Marco Lui and Timothy Baldwin. 2012. Langid.py:
An off-the-shelf language identification tool. In
Proceedings of the ACL 2012 System Demonstra-
tions, ACL ?12, pages 25?30, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234?1244. Asso-
ciation for Computational Linguistics.
Alexis Palmer, Taesun Moon, Jason Baldridge, Katrin
Erk, Eric Campbell, and Telma Can. 2010. Compu-
tational strategies for reducing annotation effort in
language documentation. Linguistic Issues in Lan-
guage Technology, 3.
Fang Qiao, Jahanzeb Sherwani, and Roni Rosenfeld.
2010. Small-vocabulary speech recognition for
resource-scarce languages. In Proceedings of the
First ACM Symposium on Computing for Develop-
ment, page 3. ACM.
Jahanzeb Sherwani. 2009. Speech interfaces for in-
formation access by low literate users. Ph.D. thesis,
SRI International.
Anjana Vakil, Max Paulus, Alexis Palmer, and
Michaela Regneri. 2014. lex4all: A language-
independent tool for building and evaluating pronun-
ciation lexicons for small-vocabulary speech recog-
nition. In Proceedings of ACL2014 Demo Session.
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proceed-
ings of HLT/NAACL 2007, Rochester, NY.
Fei Xia, William D Lewis, and Hoifung Poon. 2009.
Language ID in the context of harvesting language
data off the web. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 870?878. Associ-
ation for Computational Linguistics.
Fei Xia, Carrie Lewis, and William D Lewis. 2010.
The problems of language identification within
hugely multilingual data sets. In LREC.
90
