Extraposition: A case study in German sentence realization 
Michael GAMON?, Eric RINGGER?, Zhu ZHANG?, 
Robert MOORE?, Simon CORSTON-OLIVER? 
 
?Microsoft Research 
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, bobmoore, 
simonco}@microsoft.com 
 
 
?University of Michigan 
Ann Arbor, MI 48109 
zhuzhang@umich.edu 
 
Abstract 
We profile the occurrence of clausal 
extraposition in corpora from different 
domains and demonstrate that extraposition 
is a pervasive phenomenon in German that 
must be addressed in German sentence 
realization. We present two different 
approaches to the modeling of extraposition, 
both based on machine learned decision tree 
classifiers. The two approaches differ in their 
view of the movement operation: one 
approach models multi-step movement 
through intermediate nodes to the ultimate 
target node, while the other approach models 
one-step movement to the target node. We 
compare the resulting models, trained on data 
from two domains and discuss the 
differences between the two types of models 
and between the results obtained in the 
different domains. 
Introduction 
Sentence realization, the last stage in natural 
language generation, derives a surface string 
from a more abstract representation. Numerous 
complex operations are necessary to produce 
fluent output, including syntactic aggregation, 
constituent ordering, word inflection, etc. We 
argue that for fluent output from German 
sentence realization, clausal extraposition needs 
to be included. We show how to accomplish this 
task by applying machine learning techniques. 
A comparison between English and German 
illustrates that it is possible in both languages to 
extrapose clausal material to the right periphery 
of a clause, as the following examples show: 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave 
the country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumor has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Unlike obligatory movement phenomena such as 
Wh-movement, extraposition is subject to 
pragmatic variability. A widely-cited factor 
influencing extraposition is clausal heaviness; in 
general, extraposition of heavy clauses is 
preferred over leaving them in place. Consider 
the following example from the technical 
domain: 
German: Es werden Datenstrukturen 
verwendet, die f?r die Benutzer nicht 
sichtbar sind. 
English: Data structures are used which 
are not visible to the user. 
This perfectly fluent sentence contains an 
extraposed relative clause. If the relative clause is 
left in place, as in the following example, the 
result is less fluent, though still grammatical: 
? Es werden Datenstrukturen, die f?r die 
Benutzer nicht sichtbar sind, verwendet. 
Data structures which are not visible to 
the users are used. 
Table 1 presents a quantitative analysis of the 
frequency of extraposition in different corpora in 
both English and German. This analysis is based 
on automatic data profiling using the NLPWin 
system (Heidorn 2000). The technical manual 
corpus consists of 100,000 aligned 
English-German sentence pairs from Microsoft 
technical manuals. The Encarta corpora consist 
of 100,000 randomly selected sentences from the 
Encarta encyclopedia in both English and 
German. The output of the parser was 
post-processed to identify relative clauses 
(RELCL), infinitival clauses (INFCL), and 
complement clauses (COMPCL) that have been 
moved from a position adjacent to the term they  
modify. According to this data profile, 
approximately one third of German relative 
clauses are extraposed in technical writing, while 
only 0.22% of English relative clauses are 
extraposed in the corresponding sentence set. The 
high number of extraposed relative clauses in 
German is corroborated by numbers from the 
German hand-annotated NEGRA corpus. In 
NEGRA, 26.75% of relative clauses are 
extraposed. Uszkoreit et al (1998) report 24% of 
relative clauses being extraposed in NEGRA, but 
their number is based on an earlier version of 
NEGRA, which is about half the size of the 
current NEGRA corpus. 
We also used the NEGRA corpus to verify the 
accuracy of our data profiling with NLPWin. 
These results are presented in Table 2. We only 
took into account sentences that received a 
complete parse in NLPWin. Of the 20,602 
sentences in NEGRA, 17,756 (86.19%) fell into 
that category. The results indicate that NLPWin 
is sufficiently reliable for the identification of 
relative clauses to make our conclusions 
noteworthy and to make learning from 
NLPWin-parsed data compelling. 
Extraposition is so rare in English that a sentence 
realization module may safely ignore it and still 
yield fluent output. The fluency of sentence 
realization for German, however, will suffer from 
the lack of a good extraposition mechanism.
 
 
German  
technical 
manuals 
English  
technical 
manuals 
German  
Encarta 
English  
Encarta 
RELCL 34.97% 0.22% 18.97% 0.30% 
INFCL 3.2% 0.53% 2.77% 0.33% 
COMPCL 1.50% 0.00% 2.54% 0.15% 
Table 1: Percentage of extraposed clauses in English and German corpora 
Relative clause 
identification overall 
Identification of 
extraposed relative clauses 
Identification of non-
extraposed relative clauses 
Recall Precision Recall Precision Recall Precision 
94.55 93.40 74.50 90.02 94.64 87.76 
Table 2: NLPWin recall and precision for relative clauses on the NEGRA corpus 
 
This evidence makes it clear that any serious 
sentence realization component for German 
needs to be able to produce extraposed relative 
clauses in order to achieve reasonable fluency. In 
the German sentence realization module, 
code-named Amalgam (Gamon et al 2002, 
Corston-Oliver et al 2002), we have successfully 
implemented both extraposition models as 
described here. 
1 Two strategies for modeling 
extraposition 
The linguistic and pragmatic factors involved in 
clause extraposition are inherently complex. We 
use machine learning techniques to leverage large 
amounts of data for discovering the relevant 
conditioning features for extraposition. As a 
machine learning technique for the problem at 
hand, we chose decision tree learning, a practical 
approach to inductive inference in widespread 
use. We employ decision tree learning to 
approximate discrete-valued functions from large 
feature sets that are robust to noisy data. Decision 
trees provide an easily accessible inventory of the 
selected features and some indication of their 
relative importance in predicting the target 
features in question. The particular tool we used 
to build our decision trees is the WinMine toolkit 
(Chickering et al, 1997, n.d.). Decision trees 
built by WinMine predict a probability 
distribution over all possible target values. 
We consider two different strategies for the 
machine-learned modeling of extraposition. The 
two strategies are a series of movements versus a 
single reattachment. 
1.1 Multi-step movement 
In the multi-step movement approach, the 
question to model for each potential attachment 
site of an extraposable clause is whether the 
clause should move up to its grandparent (a ?yes? 
answer) or remain attached to its current parent (a 
?no? answer). In other words, we have cast the 
problem as a staged classification task. At 
generation runtime, for a given extraposable 
clause, the movement question is posed, and if 
the DT classifier answers ?yes?, then the clause is 
reattached one level up, and the question is posed 
again. The final attachment site is reached when 
the answer to the classification task is ?no?, and 
hence further movement is barred. Figure 1 
illustrates the multi-step movement of a clause 
(lower triangle) through two steps to a new 
landing site (the reattached clause is the upper 
triangle). Note that in both Figure 1 and Figure 2 
linear order is ignored; only the hierarchical 
aspects of extraposition are represented. 
 
Figure 1: Multi-step movement 
1.2 One-step movement 
Modeling extraposition as a one-step movement 
involves a classification decision for each node in 
the parent chain of an extraposable clause. The 
classification task can be formulated as ?should 
the extraposable clause move up to this target 
from its base position??. Figure 2 shows the 
one-step movement approach to extraposition in 
the same structural configuration as in Figure 1. 
In this example, out of the three potential landing 
sites, only one qualifies. At generation runtime, if 
more than one node in the parent chain qualifies 
as a target for extraposition movement, the node 
with the highest probability of being a target is 
chosen. In the event of equally likely target nodes, 
the target node highest in the tree is chosen. 
 
Figure 2: One-step movement 
2 Data and features 
We employed two different sets of data to build 
the models for German: the 100,000 sentence 
technical manual corpus, and the 100,000 
sentence Encarta corpus. The data were split 
70/30 for training and parameter tuning purposes, 
respectively. We extracted features for each data 
point, using the syntactic and semantic analysis 
provided by the Microsoft NLPWin system (see 
Gamon et al 2002 for more details). We only 
considered sentences for feature extraction which 
received a complete spanning parse in NLPwin. 
85.14% of the sentences in the technical domain, 
and 88.37% of the sentences in the Encarta 
corpus qualified. The following features were 
extracted: 
? syntactic label of the node under 
consideration (i.e., the starting node for a 
single-step movement), its parent and 
grandparent, and the extraposable clause 
? semantic relation to the parent node of 
the node under consideration, the parent 
and the grandparent, and the 
extraposable clause 
? status of the head of the node under 
consideration as a separable prefix verb, 
the same for the parent and the 
grandparent 
? verb position information (verb-second 
versus verb-final) for the node under 
consideration, the parent and grandparent 
? all available analysis features and 
attributes in NLPWin (see Gamon et al 
2002 for a complete list of the currently 
used features and attributes) on the node 
under consideration, the parent and 
grandparent, and on the extraposable 
clause and its parent and grandparent 
? two features indicating whether the 
extraposable node has any verbal 
ancestor node with verb-final or 
verb-second properties 
? ?heaviness? of extraposable clause as 
measured in both number of words and 
number of characters 
? ?heaviness? of the whole sentence as 
measured in both number of words and 
number of characters 
A total of 1397 features were extracted for the 
multi-step movement model. For the single-step 
movement model, we extracted an additional 21 
features. Those features indicate for each of the 
21 labels for non-terminal nodes whether a node 
with that label intervenes between the parent of 
the extraposable clause and the putative landing 
site. 
Another linguistic feature commonly cited as 
influencing extraposition is the length and 
complexity of the part of the structure between 
the original position and the extraposed clause. 
Since in the Amalgam generation module 
extraposition is applied before word and 
constituent order is established, length of 
intervening strings is not accessible as a feature.  
For each training set, we built decision trees at 
varying levels of granularity (by manipulating the 
prior probability of tree structures to favor 
simpler structures) and selected the model with 
maximal accuracy on the corresponding 
parameter tuning data set. 
Since the syntactic label of the extraposable 
clause is one of the extracted features, we decided 
to build one general extraposition model, instead 
of building separate models for each of the three 
extraposable clause types (complement clause 
COMPCL, infinitival clause INFCL, and relative 
clause RELCL). If different conditions apply to 
the three types of extraposition, the decision tree 
model is expected to pick up on the syntactic 
label of the extraposable clause as a predictive 
feature. If, on the other hand, conditions for 
extraposition tend to be neutral with respect to the 
type of extraposable clause, the modeling of 
INFCL and COMPCL extraposition can greatly 
benefit from the much larger set of data points in 
relative clause extraposition. 
3 Comparison  
To compare the one-step and multi-step models, 
we processed a new blind test set of 10,000 
sentences from each domain, Microsoft technical 
manuals and Encarta, respectively. These 
sentences were extracted randomly from data in 
these domains that were neither included in the 
training nor in the parameter tuning set. For each 
extraposable clause, three different outputs were 
computed: the observed behavior, the prediction 
obtained by iteratively applying the multi-step 
model as described in Section 1.1, and the 
prediction obtained by applying the one-step 
model. The values for these outputs were either 
?no extraposition? or a specific target node. If 
either the general extraposition prediction or the 
predicted specific target node did not match the 
observed behavior, this was counted as an error. 
3.1 One-step versus multi-step in the 
technical domain 
Accuracy data on a blind set of 10,000 sentences 
from the technical manuals domain are presented 
in Table 3. 
 One-step Multi-step Baseline 
RELCL 81.56% 83.87% 60.93% 
INFCL 93.70% 92.02% 93.70% 
COMPCL 98.10% 98.57% 94.29% 
Overall 84.42% 86.12% 67.58% 
Table 3: Accuracy numbers for the two models in 
the technical domain 
The baseline score is the accuracy for a system 
that never extraposes. Both models outperform 
the overall baseline by a large margin; the 
multi-step movement model achieves an 
accuracy 1.7% higher than the one-step model. 
The baselines in INFCL and COMPCL 
extraposition are very high. In the test set there 
were only 15 cases of extraposed INFCLs and 12 
cases of extraposed COMPCLs, making it 
impossible to draw definite conclusions. 
3.2 One-step versus multi-step in the 
Encarta domain 
Results from a blind test set of 10,000 sentences 
from the Encarta domain are presented in Table 
4. 
 One-step Multi-step Baseline 
RELCL 87.59% 88.45% 80.48% 
INFCL 97.73% 97.48% 95.72% 
COMPCL 97.32% 97.32% 95.97% 
Overall 89.99% 90.61% 84.15% 
Table 4: Accuracy numbers for the two models in 
the Encarta domain 
As in the technical domain, the multi-step model 
outperforms the one-step model, and both 
outperform the baseline significantly. Again, 
extraposed COMPCLs and INFCLs are rare in 
the dataset (there were only 17 and 6 instances, 
respectively), making the results on these types of 
clauses inconclusive. 
3.3 Domain-specificity of the models 
Since we have data from two very different 
domains we considered the extent to which the 
domain-specific models overlapped. This is a 
linguistically interesting question: from a 
linguistic perspective one would expect both 
universal properties of extraposition as well as 
domain specific generalizations to emerge from 
such a comparison. 
3.3.1 Feature selection in the technical domain 
versus Encarta 
Of the 1397 features that were extracted for the 
multi-step model, the best model for the technical 
domain was created by the WinMine tools by 
selecting 60 features. In the Encarta domain, 49 
features were selected. 27 features are shared by 
the two models. This overlap in selected features 
indicates that the models indeed capture 
linguistic generalizations that are valid across 
domains. The shared features fall into the 
following categories (where node refers to the 
starting node for multi-step movement): 
? features relating to verbal properties of 
the node 
o a separable prefix verb as 
ancestor node 
o tense and mood of ancestor 
nodes 
o presence of a verb-final or 
verb-second VP ancestor 
o presence of Modals attribute 
(indicating the presence of a 
modal verb) on ancestors 
o verb-position in the current node 
and ancestors 
? ?heaviness?-related features on the 
extraposable clause and the whole 
sentence: 
o sentence length in characters 
o number of words in the 
extraposable clause 
? syntactic labels 
? the presence of a prepositional relation 
? the presence of semantic subjects and 
objects on the node and ancestors 
? definiteness features 
? the presence of modifiers on the parent 
? person and number features 
? some basic subcategorization features 
(e.g., transitive versus intransitive) 
Interestingly, the features that are not shared (33 
in the model for the technical domain and 27 in 
the model for the Encarta domain) fall roughly 
into the same categories as the features that are 
shared. To give some examples: 
? The Encarta model refers to the presence 
of a possessor on the parent node, the 
technical domain model does not. 
? The technical domain model selects more 
person and number features on ancestors 
of the node and ancestors of the 
extraposable clause than the Encarta 
model. 
For the one-step model, 1418 total features were 
extracted. Of these features, the number of 
features selected as being predictive is 49 both in 
the Encarta and in the technical domain. 
Twenty-eight of the selected features are shared 
by the models in the two domains. Again, this 
overlap indicates that the models do pick up on 
linguistically relevant generalizations. 
The shared features between the one-step models 
fall into the same categories as the shared features 
between the multi-step models. 
The results from these experiments suggest that 
the categories of selected features are 
domain-independent, while the choice of 
individual features from a particular category 
depends on the domain. 
3.3.2 Model complexity 
In order to assess the complexity of the models, 
we use the simple metric of number of branching 
nodes in the decision tree. The complexity of the 
models clearly differs across domains. Table 5 
illustrates that for both multi-step and one-step 
movement the model size is considerably smaller 
in the Encarta domain versus the technical 
domain. 
 One-step Multi-step 
Encarta 68 82 
Technical 87 116 
Table 5: Number of branching nodes in the 
decision trees 
We hypothesize that this difference in model 
complexity may be attributable to the fact that 
NLPWin assigns a higher percentage of spanning 
parses to the Encarta data, indicating that in 
general, the Encarta data may yield more reliable 
parsing output. 
3.3.3 Cross-domain accuracy 
The results in Table 3 and Table 4 above show 
that the models based on the Encarta domain 
achieve a much higher overall accuracy (89.99% 
and 90.61%) than the models based on the 
technical domain (84.42% and 86.12%), but they 
are also based on a much higher baseline of 
non-extraposed clauses (84.15% versus 67.58% 
in the technical domain). To quantify the domain 
specificity of the models, we applied the models 
across domains; i.e., we measured the 
performance of the Encarta models on the 
technical domain and vice versa. The results 
contrasted with the in-domain overall accuracy 
from Table 3 and Table 4 are given in Table 6. 
Encarta Model Technical Model  
1-step Multi 1-step Multi 
On 
Enc. 
89.99% 90.61% 84.42% 86.12% 
On 
Tech. 
79.39% 83.03% 88.54% 89.20% 
Table 6: Cross-domain accuracy of the models 
The results show that for both one-step and 
multi-step models, the models trained on a given 
domain will outperform the models trained on a 
different domain. These results are not surprising; 
they confirm domain-specificity of the 
phenomenon. Viewed from a linguistic 
perspective, this indicates that the generalizations 
governing clausal extraposition cannot be 
formulated independently of the text domain. 
Conclusion 
We have shown that it is possible to model 
extraposition in German using decision tree 
classifiers trained on automatic linguistic 
analyses of corpora. This method is particularly 
effective for extraposed relative clauses, which 
are pervasive in German text in domains as 
disparate as news, technical manuals, and 
encyclopedic text. Both one-step and multi-step 
models very clearly outperform the baseline in 
the two domains in which we experimented. This 
in itself is a significant result, given the 
complexity of the linguistic phenomenon of 
clausal extraposition. The machine learning 
approach to extraposition has two clear 
advantages: it eliminates the need for 
hand-coding of complex conditioning 
environments for extraposition, and it is 
adaptable to new domains. The latter point is 
supported by the cross-domain accuracy 
experiment and the conclusion that extraposition 
is governed by domain-specific regularities. 
We have shown that across domains, the 
multi-step model outperforms the one-step model. 
In the German sentence realization system 
code-named Amalgam (Corston-Oliver et al 
2002, Gamon et al 2002), we have experimented 
with implementations of both the one-step and 
multi-step extraposition models, and based on the 
results reported here we have chosen the 
multi-step model for inclusion in the end-to-end 
system. 
As we have shown, extraposed relative clauses 
outnumber other extraposed clause types by a 
large margin. Still, the combined model for 
clausal extraposition outperforms the baseline 
even for infinitival clauses and complement 
clauses, although the conclusions here are not 
very firm, given the small number of relevant 
data points in the test corpus. Since the syntactic 
label of the extraposed clause is one of the 
features extracted from the training data, 
however, the setup that we have used will adapt 
easily once more training data (especially for 
infinitival and complement clauses) become 
available. The models will automatically pick up 
distinctions between the generalizations covering 
relative clauses versus infinitival/complement 
clauses when they become relevant, by selecting 
the syntactic label feature as predictive. 
Finally, evaluation of the types of features that 
were selected by the extraposition models show 
that besides the ?heaviness? of the extraposed 
clause, a number of other factors from the 
structural context enter the determination of 
likelihood of extraposition. This, in itself, is an 
interesting result: it shows how qualitative 
inspection of a machine learned model can yield 
empirically based linguistic insights. 
Acknowledgements 
Our thanks go to Max Chickering for his 
assistance with the WinMine toolkit and to the 
anonymous reviewers for helpful comments. 
References  
Chickering D. M., Heckerman D. and Meek C. (1997). 
A Bayesian approach to learning Bayesian 
networks with local structure. In "Uncertainty in 
Artificial Intelligence: Proceedings of the 
Thirteenth Conference", D. Geiger and P. Punadlik 
Shenoy, ed., Morgan Kaufman, San Francisco, 
California,  pp. 80-89. 
Chickering, D. Max. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/To
oldoc.htm 
Corston-Oliver S., Gamon M., Ringger E. and Moore 
R. (2002). An overview of Amalgam: a 
machine-learned generation module. To appear in 
Proceedings of the Second International Natural 
Language Generation Conference 2002, New York. 
Gamon M., Ringger E., Corston-Oliver S.. (2002). 
Amalgam: A machine-learned generation module. 
Microsoft Technical Report MSR-TR-2002-57. 
Heidorn, G. E. (2000): Intelligent Writing Assistance. 
In "A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text", R. Dale, H. Moisl, and H. 
Somers (ed.), Marcel Dekker, New York, pp. 
181-207. 
Uszkoreit, H., Brants T., Duchier D., Krenn B., 
Konieczny L., Oepen S. and Skut W. (1998). 
Aspekte der Relativsatzextraposition im Deutschen. 
Claus-Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
Linguistically Informed Statistical Models of Constituent Structure for 
Ordering in Sentence Realization 
Eric RINGGER1, Michael GAMON1, Robert C. MOORE1, 
David ROJAS2, Martine SMETS1, Simon CORSTON-OLIVER1 
 
1Microsoft Research 
One Microsoft Way 
Redmond, Washington 98052, USA 
{ringger, mgamon, bobmoore, msmets, 
simonco}@microsoft.com 
2Butler Hill Group, LLC 
& Indiana University Linguistics Dept. 
1021 East 3rd Street, MM 322 
Bloomington, Indiana 47405, USA 
drojas@indiana.edu 
 
 
Abstract 
We present several statistical models of syntactic 
constituent order for sentence realization. We 
compare several models, including simple joint 
models inspired by existing statistical parsing 
models, and several novel conditional models. The 
conditional models leverage a large set of linguistic 
features without manual feature selection. We apply 
and evaluate the models in sentence realization for 
French and German and find that a particular 
conditional model outperforms all others. We 
employ a version of that model in an evaluation on 
unordered trees from the Penn TreeBank. We offer 
this result on standard data as a reference-point for 
evaluations of ordering in sentence realization. 
1 Introduction 
Word and constituent order play a crucial role in 
establishing the fluency and intelligibility of a 
sentence. In some systems, establishing order 
during the sentence realization stage of natural 
language generation has been accomplished by 
hand-crafted generation grammars in the past (see 
for example, Aikawa et al (2001) and Reiter and 
Dale (2000)). In contrast, the Nitrogen (Langkilde 
and Knight, 1998a, 1998b) system employs a word 
n-gram language model to choose among a large 
set of word sequence candidates which vary in 
constituent order, word order, lexical choice, and 
morphological inflection. Nitrogen?s model does 
not take into consideration any non-surface 
linguistic features available during realization.  
The Fergus system (Bangalore and Rambow, 
2000) employs a statistical tree model to select 
probable trees and a word n-gram model to rank 
the string candidates generated from the best trees. 
Like Nitrogen, the HALogen system (Langkilde, 
2000; Langkilde-Geary, 2002a, 2002b) uses word 
n-grams, but it extracts the best-scoring surface 
realizations efficiently from a packed forest by 
constraining the search first within the scope of 
each constituent.  
Our research is carried out within the Amalgam 
broad coverage sentence realization system. 
Amalgam generates sentence strings from abstract 
predicate-argument structures (Figure 1), using a 
pipeline of stages, many of which employ 
machine-learned models to predict where to 
perform specific linguistic operations based on the 
linguistic context (Corston-Oliver et al, 2002; 
Gamon et al, 2002a, 2002b; Smets et al, 2003). 
Amalgam has an explicit ordering stage that 
determines the order of constituents and their 
daughters. The input for this stage is an unordered 
tree of constituents; the output is an ordered tree of 
constituents or a ranked list of such trees. For 
ordering, Amalgam leverages tree constituent 
structure and, importantly, features of those 
constituents and the surrounding context. By 
separately establishing order within constituents, 
Amalgam heavily constrains the possible 
alternatives in later stages of the realization 
process.  The design allows for interaction between 
ordering choices and other realization decisions, 
such as lexical choice (not considered in the 
present work), through score combinations from 
distinct Amalgam pipeline stages. 
Most previous research into the problem of 
establishing order for sentence realization has 
focused on English, a language with fairly strict 
word and constituent order. In the experiments 
described here we first focus on German and 
French which present novel challenges.1 We also 
describe an English experiment involving data 
from the Penn Treebank. Our ultimate goal is to 
develop a model that handles all ordering 
phenomena in a unified and elegant way across 
typologically diverse languages. In the present 
paper, we explore the space of possible models and 
examine some of these closely. 
                                                          
1
 For an overview of some of the issues in 
determining word and constituent order in German and 
French, see (Ringger et al, 2003).  
 Figure 1: Abstract predicate-argument structure (NLPWin logical form) for the German sentence: 
In der folgenden Tabelle werden die Optionen sowie deren Funktionen aufgelistet. 
(The options and their functions are listed in the following table.) 
2 Models of Constituent Order 
In order to develop a model of constituent 
structure that captures important order phenomena, 
we will consider the space of possible joint and 
conditional models in increasing complexity. For 
each of the models, we will survey the 
independence assumptions and the feature set used 
in the models. 
Our models differ from the previous statistical 
approaches in the range of input features. Like the 
knowledge-engineered approaches, the models 
presented here incorporate lexical features, parts-
of-speech, constituent-types, constituent 
boundaries, long-distance dependencies, and 
semantic relations between heads and their 
modifiers. 
Our experiments do not cover the entire space of 
possible models, but we have chosen significant 
points in the space for evaluation and comparison. 
2.1 Joint Models 
We begin by considering joint models of 
constituent structure of the form ( ),P ? ?  over 
ordered syntax trees ?  and unordered syntax trees 
? . An ordered tree ?  contains non-terminal 
constituents C, each of which is the parent of an 
ordered sequence of daughters ( 1,..., nD D ), one of 
which is the head constituent H.2 Given an ordered 
tree ? , the value of the function 
_ ( )unordered tree ?  is an unordered tree ?  
corresponding to ?  that contains a constituent B 
for each C in ? , such that 
( ) ( )
1
_ ( )
{ ,..., }n
unordered set daughters Cdaughters B
D D
=
=
 
again with iH D=  for some i in ( )1..n . The 
hierarchical structure of ?  is identical to that of 
? . 
We employ joint models for scoring alternative 
ordered trees as follows: given an unordered 
syntax tree ? , we want the ordered syntax tree ??  
that maximizes the joint probability: 
                                                          
2 All capital Latin letters denote constituents, and 
corresponding lower-case Latin letters denote their 
labels (syntactic categories). 
( ) ( )
: _ ( )
? arg max , arg max
unordered tree
P P
? ? ? ?
? ? ? ?
=
= =    (1) 
As equation (1) indicates, we can limit our search 
to those trees ?  which are alternative orderings of 
the given tree ? . 
Inter-dependencies among ordering decisions 
within different constituents (e.g., for achieving 
parallelism) make the global sentence ordering 
problem challenging and are certainly worth 
investigating in future work.  For the present, we 
constrain the possible model types considered here 
by assuming that the ordering of any constituent is 
independent of the ordering within other 
constituents in the tree, including its daughters; 
consequently, 
( ) ( )
( )C constits
P P C
?
?
?
= ?  
Given this independence assumption, the only 
possible ordered trees are trees built with non-
terminal constituents computed as follows: for 
each ( )B constits ?? , 
( )
: _ ( )
* arg max
C B unordered set C
C P C
=
=  
In fact, we can further constrain our search for the 
best ordering of each unordered constituent B, 
since C?s head must match B?s head: 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C
=
=
=  
Thus, we have reduced the problem to finding the 
best ordering of each constituent of the unordered 
tree. 
Now if we wish to condition on some feature ( )x f ?= , then we must first predict it as follows: 
( ) ( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P x P C x
=
=
=  
If x is truly a feature of ?  and does not depend on 
any particular ordering of any constituent in ? , 
then ( )P x  is constant, and we do not need to 
compute it in practice. In other words, 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C x
=
=
=       (2) 
Hence, even for a joint model ( )P C , we can 
condition on features that are fixed in the given 
unordered tree ?  without first predicting them. 
The joint models described here are of this form. 
For this reason, when we describe a distribution ( )P C x , unless we explicitly state otherwise, we 
are actually describing the part of the joint model 
that is of interest. As justified above, we do not 
need to compute ( )P x  and will simply present 
alternative forms of ( )P C x . 
We can factor a distribution ( )P C x  in many 
different ways using the chain rule. As our starting 
point we adopt the class of models called Markov 
grammars.3 We first consider a left-to-right 
Markov grammar of order j that expands C by 
predicting its daughters 1,..., nD D  from left-to-
right, one at a time, as shown in Figure 2: in the 
figure. iD  depends only on ( i jD ? , ?, 1iD ? ), and 
the parent category C ., according to the 
distribution in equation (3). 
 
i?
Figure 2: Left-to-right Markov grammar. 
( ) ( )1
1
,..., , ,
n
i i i j
i
P C h P d d d c h
? ?
=
= ?  (3) 
In order to condition on another feature of each 
ordered daughter iD , such as its semantic relation 
i? to the head constituent H, we also first predict 
it, according to the chain rule. The result is the 
semantic Markov grammar in equation (4):  
( ) ( )( )
1 1
1 1 1
, ,..., , , ,
, , ,..., , , ,
n i i i i j i j
i i i i i i j i j
P d d c h
P C h
P d d d c h
? ? ?
? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (4) 
Thus, the model predicts semantic relation i? and 
then the label id  in the context of that semantic 
relation. We will refer to this model as Type 1 
(T1). 
As an extension to model Type 1, we include 
features computed by the following functions on 
the set i?  of daughters of C already ordered (see 
Figure 2): 
? Number of daughters already ordered (size 
of i? ) 
? Number of daughters in i?  having a 
particular label for each of the possible 
constituent labels {NP, AUXP, VP, etc.} 
(24 for German, 23 for French) 
We denote that set of features in shorthand as ( )if ? . With this extension, a model of Markov 
                                                          
3
 A ?Markov grammar? is a model of constituent 
structure that starts at the root of the tree and assigns 
probability to the expansion of a non-terminal one 
daughter at a time, rather than as entire productions 
(Charniak, 1997 & 2000). 
order j can potentially have an actual Markov order 
greater than j. Equation (5) is the extended model, 
which we will refer to as Type 2 (T2): 
( ) ( )( )( )( )
1 1
1 1 1
, ,..., , , , ,
, , ,..., , , , ,
n i i i i j i j i
i i i i i i j i j i
P d d c h f
P C h
P d d d c h f
? ? ? ?
? ? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (5) 
As an alternative to a left-to-right expansion, we 
can also expand a constituent in a head-driven 
fashion. We refer the reader to (Ringger et al, 
2003) for details and evaluations of several head-
driven models (the missing ?T3?, ?T4?, and ?T6? 
in this discussion). 
2.2 Conditional Models 
We now consider more complex models that use 
additional features. We define a function ( )g X on 
constituents, where the value of ( )g X represents a 
set of many lexical, syntactic, and semantic 
features of X (see section 5.2 for more details). No 
discourse features are included for the present 
work. We condition on 
? ( )g B , where B is the unordered constituent 
being ordered 
? ( )g H , where H is the head of B 
? ( )Bg P , where BP  is the parent of B, and 
? ( )Bg G , where BG  is the grandparent of B. 
These features are fixed in the given unordered tree 
? , as in the discussion of equation (2), hence the 
resulting complex model is still a joint model.   
Up until this point, we have been describing joint 
generative models that describe how to generate an 
ordered tree from an unordered tree. These models 
require extra effort and capacity to accurately 
model the inter-relations among all features. Now 
we move on to truly conditional models by 
including features that are functions on the set i?  
of daughters of C yet to be ordered. In the 
conditional models we do not need to model the 
interdependencies among all features. We include 
the following: 
? Number of daughters remaining to be 
ordered (size of i? ) 
? Number of daughters in i?  having a 
particular label 
As before, we denote these feature sets in 
shorthand as ( )if ? . The resulting distribution is 
represented in equation (6), which we will refer to 
as Type 5 (T5): 
( )
( )
( )
1 1
1 1 1
( ), ( ), ( ), ( )
, ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
, , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
B B
i i i j i j
i
n B B i i
i i i i i j i j
i
B B i i
P C g H g B g P g G
d d c h
P
g H g B g P g G f f
d d c h
P d
g H g B g P g G f f
? ?
?
? ?
? ? ?
? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
=
? ?
? ?
? ?
?
? ?
? ?
? ?
? ?
? ?
? ?
?
   (6) 
All models in this paper are nominally Markov 
order 2, although those models incorporating the 
additional feature functions ( )if ?  and ( )if ?  
defined in Section 2.2 can be said to have higher 
order. 
2.3 Binary conditional model 
We introduce one more model type called the 
binary conditional model. It estimates a much 
simpler distribution over the binary variable ?  
called ?sort-next? with values in {yes, no} 
representing the event that an as-yet unordered 
member D of i?  (the set of as-yet unordered 
daughters of parent C, as defined above) should be 
?sorted? next, as illustrated in Figure 3. 
i?i?
?
 
Figure 3: Binary conditional model. 
The conditioning features are almost identical to 
those used in the left-to-right conditional models 
represented in equation (6) above, except that id  
and i?  (the semantic relation of D with head H) 
appear in the conditional context and need not first 
be predicted. In its simple form, the model 
estimates the following distribution: 
( )
1 1, , , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
i i i i i j i j
i
B B i i
d d d c h
P
g H g B g P g G f f
? ? ?
?
? ?
? ? ? ?
? ?
? ?
? ?
? ?
   (7) 
In our shorthand, we will call this Type 7 (T7). We 
describe how to apply this model directly in a left-
to-right ?sorting? search later in the section on 
search. 
3 Estimation 
We estimate a model?s distributions with 
probabilistic decision trees (DTs).4 We build 
decision trees using the WinMine toolkit 
(Chickering, 2002). WinMine-learned decision 
trees are not just classifiers; each leaf is a 
conditional probability distribution over the target 
random variable, given all features available in 
training; hence the tree as a whole is an estimate of 
the conditional distribution of interest. The primary 
advantage of using decision trees, is the automatic 
feature selection and induction from a large pool of 
features. 
We train four models for German and French 
each. One model is joint (T1); one is joint with 
additional features on the set of daughters already 
ordered (T2); one is conditional (T5). In addition, 
we employ one binary conditional DT model (T7), 
both with and without normalization (see equation 
(8)). 
                                                          
4
 Other approaches to feature selection, feature 
induction, and distribution estimation are certainly 
possible, but they are beyond the scope of this paper. 
One experiment using interpolated language modeling 
techniques is described in (Ringger et al, 2003) 
4 Search 
4.1 Exhaustive search 
Given an unordered tree ?  and a model of 
constituent structure O of any of the types already 
presented, we search for the best ordered tree ?  
that maximizes ( )OP ?  or ( )OP ? ? , as 
appropriate, with the context varying according to 
the complexity of the model. Each of our models 
(except the binary conditional model) estimates the 
probability of an ordering of any given constituent 
C in ? , independently of the ordering inside other 
constituents in ? . The complete search is a 
dynamic programming (DP) algorithm, either left-
to-right in the daughters of C (or head-driven, 
depending on the model type). The search can 
optionally maintain one non-statistical constraint 
we call Input-Output Coordination Consistency 
(IOCC), so that the order of coordinated 
constituents is preserved as they were specified in 
the given unordered tree. For these experiments, 
we employ the constraint. 
4.2 Greedy search for binary conditional 
model 
The binary conditional model can be applied in a 
left-to-right ?sorting? mode (Figure 3). At stage i, 
for each unordered daughter jD , in i? , the model 
is consulted for the probability of j yes? = , 
namely the probability that jD  should be placed to 
the right of the already ordered sister constituents 
i? . The daughter in i?  with the highest 
probability is removed from i?  to produce 1i? +  
and added to the right of i? to produce 1i? + . The 
search proceeds through the remaining unordered 
constituents until all constituents have been 
ordered in this greedy fashion. 
4.3 Exhaustive search for binary conditional 
model 
In order to apply the binary conditional model in 
the exhaustive DP search, we normalize the model 
at every stage of the search and thereby coerce it 
into a probability distribution over the remaining 
daughters in i? . We represent the distribution in 
?equation? (7) in short-hand as ( ), , iP d? ? ? , 
with i?  representing the contextual features for the 
given search hypothesis at search stage i. Thus, our 
normalized distribution for stage i is given by 
equation (8). Free variable j represents an index on 
unordered daughters in i? , as does k. 
( ) ( )
( )
1
, ,
, ,
, ,
i
j j j i
j j j i
k k k i
k
P yes d
P D d
P yes d
?
? ?
?
? ?
=
= ?
? =
= ?
?
 (8) 
This turns out to be the decision tree analogue of a 
Maximum Entropy Markov Model (MEMM) 
(McCallum et al, 2000), which we can refer to as a 
DTMM. 
5 Experiments 
5.1 Training 
We use a training set of 20,000 sentences, both 
for French and German. The data come from 
technical manuals in the computer domain. For a 
given sentence in our training set, we begin by 
analyzing the sentence as a surface syntax tree and 
an abstract predicate argument structure using the 
NLPWin system (Heidorn, 2000). By consulting 
these two linked structures, we produce a tree with 
all of the characteristics of trees seen by the 
Amalgam ordering stage at generation run-time 
with one exception: these training trees are 
properly ordered. The training trees include all 
features of interest, including the semantic 
relations among a syntactic head and its modifiers. 
We train our order models from the constituents of 
these trees. NLPWin parser output naturally 
contains errors; hence, the Amalgam training data 
is imperfect. 
5.2 Selected Features 
A wide range of linguistic features is extracted 
for the different decision tree models. The number 
of selected features for German reaches 280 (out of 
651 possible features) in the binary conditional 
model T7. For the French binary conditional 
model, the number of selected features is 218 (out 
of 550). The binary conditional models draw from 
the full set of available features, including: 
? lexical sub-categorization features such as 
transitivity and compatibility with clausal 
complements 
? lemmas (word-stems) 
? semantic features such as the semantic 
relation and the presence of 
quantificational operators 
? length of constituent in words 
? syntactic information such as the label and 
the presence of syntactic modifiers 
5.3 Evaluation 
To evaluate the constituent order models in 
isolation, we designed our experiments to be 
independent of the rest of the Amalgam sentence 
realization process. We use test sets of 1,000 
sentences, also from technical manuals, for each 
language. To isolate ordering, for a given test 
sentence, we process the sentence as in training to 
produce an ordered tree ?  (the reference for 
evaluation) and from it an unordered tree ? . 
Given ? , we then search for the best ordered tree 
hypothesis ??  using the model in question. 
We then compare ?  and ?? . Because we are 
only ordering constituents, we can compare ? and 
??  by comparing their respective constituents. For 
each C in ? , we measure the per-constituent edit 
distance D, between C and its counterpart C? in ??  
as  follows: 
1. Let d be the edit distance between the 
ordered set of daughters in each, with the 
only possible edit operators being insert and 
delete 
2. Let the number of moves / 2m d= , since 
insertions and deletions can be paired 
uniquely 
3. Divide by the total number of 
daughters: ( )/D m daughters C=  
This metric is like the ?Generation Tree Accuracy? 
metric of Bangalore & Rambow (2000), except 
that there is no need to consider cross-constituent 
moves. The total score for the hypothesis tree ??  is 
the mean of the per-constituent edit distances. 
For each of the models under consideration and 
each language, we report in Table 1 the average 
score across the test set for the given language. The 
first row is a baseline computed from randomly 
scrambling constituents (mean over four 
iterations). 
Model German French 
Baseline (random) 35.14 % 34.36 % 
T1: DT joint 5.948% 3.828% 
T2: DT joint 
with ( )if ?   5.852% 4.008% 
T5: DT conditional 6.053% 4.271% 
T7: DT binary cond., 
greedy search 3.516% 1.986% 
T7: DT normalized 
binary conditional, 
exhaustive search 
3.400% 1.810% 
Table 1: Mean per-constituent edit distances for 
German & French. 
5.4 Discussion 
For both German and French, the binary 
conditional DT model outperforms all other 
models. Normalizing the binary conditional model 
and applying it in an exhaustive search performs 
better than a greedy search. All score differences 
are statistically significant; moreover, manual 
inspection of the differences for the various models 
also substantiates the better quality of those models 
with lower scores. 
With regard to the question of conditional versus 
joint models, the joint models (T1, T2) outperform 
their conditional counterparts (T5). This may be 
due to a lack of sufficient training data for the 
conditional models. At this time, the training time 
of the conditional models is the limiting factor. 
There is a clear disparity between the 
performance of the German models and the 
performance of the French models. The best 
German model is twice as bad as the best French 
model.  (For a discussion of the impact of 
modeling German verb position, please consult 
(Ringger et al, 2003).) 
 Baseline 
(random) 
Greedy, 
IOCC Greedy 
DP,  
IOCC DP 
Total Sentences 2416 2416 2416 2416 2416 
Mean Tokens/Sentence 23.59 23.59 23.59 23.59 23.59 
Time/Input (sec.) n/a 0.01 0.01 0.39 0.43 
Exact Match 0.424% 33.14% 27.53% 33.53% 35.72% 
Coverage 100% 100% 100% 100% 100% 
Mean Per-Const. Edit Dist. 38.3% 6.02% 6.84% 5.25% 4.98% 
Mean NIST SSA -16.75 74.98 67.19 74.65 73.24 
Mean IBM Bleu Score 0.136 0.828 0.785 0.817 0.836 
Table 2: DSIF-Amalgam ordering performance on WSJ section 23. 
6 Evaluation on the Penn TreeBank 
Our goal in evaluating on Penn Tree Bank (PTB) 
data is two-fold: (1) to enable a comparison of 
Amalgam?s performance with other systems 
operating on similar input, and (2) to measure 
Amalgam?s capabilities on less domain-specific 
data than technical software manuals. We derive 
from the bracketed tree structures in the PTB using 
a deterministic procedure an abstract 
representation we refer to as a Dependency 
Structure Input Format (DSIF), which is only 
loosely related to NLPWin?s abstract predicate-
argument structures. 
The PTB to DSIF transformation pipeline 
includes the following stages, inspired by 
Langkilde-Geary?s (2002b) description: 
A. Deserialize the tree 
B. Label heads, according to Charniak?s head 
labeling rules (Charniak, 2000) 
C. Remove empty nodes and flatten any 
remaining empty non-terminals 
D. Relabel heads to conform more closely to the 
head conventions of NLPWin 
E. Label with logical roles, inferred from PTB 
functional roles 
F. Flatten to maximal projections of heads 
(MPH), except in the case of conjunctions 
G. Flatten non-branching non-terminals 
H. Perform dictionary look-up and 
morphological analysis 
I. Introduce structure for material between 
paired delimiters and for any coordination 
not already represented in the PTB 
J. Remove punctuation 
K. Remove function words 
L. Map the head of each maximal projection to 
a dependency node, and map the head?s 
modifiers to the first node?s dependents, 
thereby forming a complete dependency tree. 
To evaluate ordering performance alone, our data 
are obtained by performing all of the steps above 
except for (J) and (K). We employ only a binary 
conditional ordering model, found in the previous 
section to be the best of the models considered. To 
train the order models, we use a set of 10,000 
sentences drawn from the standard PTB training 
set, namely sections 02?21 from the Wall Street 
Journal portion of the PTB (the full set contains 
approx. 40,000 sentences). For development and 
parameter tuning we used a separate set of 2000 
sentences drawn from sections 02?21. 
Decision trees are trained for each of five 
constituent types characterized by their head 
labels: adjectival, nominal, verbal, conjunctions 
(coordinated material), and other constituents not 
already covered. The split DTs can be thought of 
as a single DT with a five-way split at the top 
node. 
Our DSIF test set consists of the blind test set 
(section 23) of the WSJ portion of the PTB. At 
run-time, for each converted tree in the test set, all 
daughters of a given constituent are first permuted 
randomly with one another (scrambled), with the 
option for coordinated constituents to remain 
unscrambled, according to the Input-Output 
Coordination Consistency (IOCC) option. For a 
given unordered (scrambled) constituent, the 
appropriate order model (noun-head, verb-head, 
etc.) is used in the ordering search to order the 
daughters. Note that for the greedy search, the 
input order can influence the final result; therefore, 
we repeat this process for multiple random 
scramblings and average the results. 
We use the evaluation metrics employed in 
published evaluations of HALogen, FUF/SURGE, 
and FERGUS (e.g., Calloway, 2003), although our 
results are for ordering only. Coverage, or the 
percentage of inputs for which a system can 
produce a corresponding output, is uninformative 
for the Amalgam system, since in all cases, it can 
generate an output for any given DSIF. In addition 
to processing time per input, we apply four other 
metrics: exact match, NIST simple string accuracy 
(the complement of the familiar word error rate), 
the IBM Bleu score (Papineni et al, 2001), and the 
intra-constituent edit distance metric introduced 
earlier. 
We evaluate against ideal trees, directly 
computed from PTB bracketed tree structures. The 
results in Table 2 show the effects of varying the 
IOCC parameter. For both trials involving a greedy 
search, the results were averaged across 25 
iterations. As should be expected, turning on the 
input-output faithfulness option (IOCC) improves 
the performance of the greedy search. Keeping 
coordinated material in the same relative order 
would only be called for in applications that plan 
discourse structure before or during generation. 
7 Conclusions and Future Work 
The experiments presented here provide 
conclusive reasons to favor the binary conditional 
model as a model of constituent order. The 
inclusion of linguistic features is of great value to 
the modeling of order, specifically in verbal 
constituents for both French and German. 
Unfortunately space did not permit a thorough 
discussion of the linguistic features used. Judging 
from the high number of features that were 
selected during training for participation in the 
conditional and binary conditional models, the 
availability of automatic feature selection is 
critical. 
Our conditional and binary conditional models 
are currently lexicalized only for function words; 
the joint models not at all. Experiments by Daum? 
et al(2002) and the parsing work of Charniak 
(2000) and others indicate that further 
lexicalization may yield some additional 
improvements for ordering. However, the parsing 
results of Klein & Manning (2003) involving 
unlexicalized grammars suggest that gains may be 
limited. 
For comparison, we encourage implementers of 
other sentence realization systems to conduct 
order-only evaluations using PTB data. 
Acknowledgements 
We wish to thank Irene Langkilde-Geary and 
members of the MSR NLP group for helpful 
discussions.  Thanks also go to the anonymous 
reviewers for helpful feedback. 
References  
Aikawa T., Melero M., Schwartz L. Wu A. 2001. 
Multilingual sentence generation. In Proc. of 8th 
European Workshop on NLG. pp. 57-63. 
Bangalore S. Rambow O. 2000. Exploiting a 
probabilistic hierarchical model for generation. 
In Proc. of COLING. pp. 42-48. 
Calloway, C. 2003. Evaluating Coverage for Large 
Symbolic NLG Grammars.  In Proc. of IJCAI 
2003. pp 811-817. 
Charniak E. 1997. Statistical Techniques for 
Natural Language Parsing, In AI Magazine. 
Charniak E. 2000. A Maximum-Entropy-Inspired 
Parser. In Proc. of ACL. pp.132-139. 
Chickering D. M. 2002. The WinMine Toolkit. 
Microsoft Technical Report 2002-103. 
Corston-Oliver S., Gamon M., Ringger E., Moore 
R. 2002. An overview of Amalgam: a machine-
learned generation module. In Proc. of INLG. 
pp.33-40. 
Daum? III H., Knight K., Langkilde-Geary I., 
Marcu D., Yamada K. 2002. The Importance of 
Lexicalized Syntax Models for Natural 
Language Generation Tasks. In Proc. of INLG. 
pp. 9-16. 
Gamon M., Ringger E., Corston-Oliver S. 2002a. 
Amalgam: A machine-learned generation 
module. Microsoft Technical Report 2002-57. 
Gamon M., Ringger E., Corston-Oliver S., Moore 
R. 2002b. Machine-learned contexts for 
linguistic operations in German sentence 
realization. In Proc. of ACL. pp. 25-32. 
Heidorn G. 2000. Intelligent Writing Assistance. In 
A Handbook of Natural Language Processing,, 
R. Dale, H. Moisl, H. Somers (eds.). Marcel 
Dekker, NY. 
Klein D., Manning C. 2003. "Accurate 
Unlexicalized Parsing." In Proceedings of ACL-
03. 
Langkilde I. 2000. Forest-Based Statistical 
Sentence generation. In Proc. of NAACL. pp. 
170-177. 
Langkilde-Geary I. 2002a. An Empirical 
Verification of Coverage and Correctness for a 
General-Purpose Sentence Generator. In Proc. of 
INLG. pp.17-24. 
Langkilde-Geary, I. 2002b. A Foundation for 
General-purpose Natural Language Generation: 
Sentence Realization Using Probabilistic Models 
of Language. PhD Thesis, University of 
Southern California. 
Langkilde I., Knight K. 1998a. The practical value 
of n-grams in generation. In Proc. of 9th 
International Workshop on NLG. pp. 248-255. 
Langkilde I., Knight K. 1998b. Generation that 
exploits corpus-based statistical knowledge. In 
Proc. of ACL and COLING. pp. 704-710. 
McCallum A., Freitag D., & Pereira F. 2000. 
?Maximum Entropy Markov Models for 
Information Extraction and Segmentation.? In 
Proc. Of ICML-2000. 
Papineni, K.A., Roukos, S., Ward, T., and Zhu, 
W.J. 2001. Bleu: a method for automatic 
evaluation of machine translation. IBM 
Technical Report RC22176 (W0109-022). 
Reiter E. and Dale R. 2000. Building natural 
language generation systems. Cambridge 
University Press, Cambridge. 
Ringger E., Gamon M., Smets M., Corston-Oliver 
S. and Moore R. 2003 Linguistically informed 
models of constituent structure for ordering in 
sentence realization. Microsoft Research 
technical report MSR-TR-2003-54. 
Smets M., Gamon M., Corston-Oliver S. and 
Ringger E. (2003) The adaptation of a machine-
learned sentence realization system to French. 
In Proceedings of EACL. 
323
324
325
326
327
328
329
330
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 160?167,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilingual Dependency Parsing using Bayes Point Machines
Simon Corston-Oliver
Microsoft Research
One Microsoft Way
Redmond, WA 98052
simonco@microsoft.com
Anthony Aue
Microsoft Research
One Microsoft Way
Redmond, WA 98052
anthaue@microsoft.com
Kevin Duh
Dept. of Electrical Eng.
Univ. of Washington
Seattle, WA 98195
duh@ee.washington.edu
Eric Ringger
Computer Science Dept.
Brigham Young Univ.
Provo, UT 84602
ringger@cs.byu.edu
Abstract
We develop dependency parsers for Ara-
bic, English, Chinese, and Czech using
Bayes Point Machines, a training algo-
rithm which is as easy to implement as
the perceptron yet competitive with large
margin methods. We achieve results com-
parable to state-of-the-art in English and
Czech, and report the first directed depen-
dency parsing accuracies for Arabic and
Chinese. Given the multilingual nature of
our experiments, we discuss some issues
regarding the comparison of dependency
parsers for different languages.
1 Introduction
Dependency parsing is an alternative to constituency
analysis with a venerable tradition going back at
least two millenia. The last century has seen at-
tempts to formalize dependency parsing, particu-
larly in the Prague School approach to linguistics
(Tesnie`re, 1959; Melc?uk, 1988).
In a dependency analysis of syntax, words directly
modify other words. Unlike constituency analysis,
there are no intervening non-lexical nodes. We use
the terms child and parent to denote the dependent
term and the governing term respectively.
Parsing has many potential applications, rang-
ing from question answering and information re-
trieval to grammar checking. Our intended ap-
plication is machine translation in the Microsoft
Research Treelet Translation System (Quirk et al,
2005; Menezes and Quirk, 2005). This system ex-
pects an analysis of the source language in which
words are related by directed, unlabeled dependen-
cies. For the purposes of developing machine trans-
lation for several language pairs, we are interested in
dependency analyses for multiple languages.
The contributions of this paper are two-fold: First,
we present a training algorithm called Bayes Point
Machines (Herbrich et al, 2001; Harrington et al,
2003), which is as easy to implement as the per-
ceptron, yet competitive with large margin meth-
ods. This algorithm has implications for anyone
interested in implementing discriminative training
methods for any application. Second, we develop
parsers for English, Chinese, Czech, and Arabic and
probe some linguistic questions regarding depen-
dency analyses in different languages. To the best of
our knowledge, the Arabic and Chinese results are
the first reported results to date for directed depen-
dencies. In the following, we first describe the data
(Section 2) and the basic parser architecture (Section
3). Section 4 introduces the Bayes Point Machine
while Section 5 describes the features for each lan-
guage. We conclude with experimental results and
discussions in Sections 6 and 7.
2 Data
We utilize publicly available resources in Arabic,
Chinese, Czech, and English for training our depen-
dency parsers.
For Czech we used the Prague Dependency Tree-
bank version 1.0 (LDC2001T10). This is a corpus
of approximately 1.6 million words. We divided
the data into the standard splits for training, devel-
160
opment test and blind test. The Prague Czech De-
pendency Treebank is provided with human-edited
and automatically-assigned morphological informa-
tion, including part-of-speech labels. Training and
evaluation was performed using the automatically-
assigned labels.
For Arabic we used the Prague Arabic De-
pendency Treebank version 1.0 (LDC2004T23).
Since there is no standard split of the data into
training and test sections, we made an approxi-
mate 70%/15%/15% split for training/development
test/blind test by sampling whole files. The Ara-
bic Dependency Treebank is considerably smaller
than that used for the other languages, with approx-
imately 117,000 tokens annotated for morphologi-
cal and syntactic relations. The relatively small size
of this corpus, combined with the morphological
complexity of Arabic and the heterogeneity of the
corpus (it is drawn from five different newspapers
across a three-year time period) is reflected in the
relatively low dependency accuracy reported below.
As with the Czech data, we trained and evaluated us-
ing the automatically-assigned part-of-speech labels
provided with the data.
Both the Czech and the Arabic corpora are anno-
tated in terms of syntactic dependencies. For En-
glish and Chinese, however, no corpus is available
that is annotated in terms of dependencies. We there-
fore applied head-finding rules to treebanks that
were annotated in terms of constituency.
For English, we used the Penn Treebank version
3.0 (Marcus et al, 1993) and extracted dependency
relations by applying the head-finding rules of (Ya-
mada and Matsumoto, 2003). These rules are a
simplification of the head-finding rules of (Collins,
1999). We trained on sections 02-21, used section
24 for development test and evaluated on section
23. The English Penn Treebank contains approxi-
mately one million tokens. Training and evaluation
against the development test set was performed us-
ing human-annotated part-of-speech labels. Evalu-
ation against the blind test set was performed us-
ing part-of-speech labels assigned by the tagger de-
scribed in (Toutanova et al, 2003).
For Chinese, we used the Chinese Treebank ver-
sion 5.0 (Xue et al, 2005). This corpus contains
approximately 500,000 tokens. We made an approx-
imate 70%/15%/15% split for training/development
test/blind test by sampling whole files. As with the
English Treebank, training and evaluation against
the development test set was performed using
human-annotated part-of-speech labels. For evalu-
ation against the blind test section, we used an im-
plementation of the tagger described in (Toutanova
et al, 2003). Trained on the same training section
as that used for training the parser and evaluated on
the development test set, this tagger achieved a to-
ken accuracy of 92.2% and a sentence accuracy of
63.8%.
The corpora used vary in homogeneity from the
extreme case of the English Penn Treebank (a large
corpus drawn from a single source, the Wall Street
Journal) to the case of Arabic (a relatively small
corpus?approximately 2,000 sentences?drawn from
multiple sources). Furthermore, each language
presents unique problems for computational analy-
sis. Direct comparison of the dependency parsing
results for one language to the results for another
language is therefore difficult, although we do at-
tempt in the discussion below to provide some basis
for a more direct comparison. A common question
when considering the deployment of a new language
for machine translation is whether the natural lan-
guage components available are of sufficient quality
to warrant the effort to integrate them into the ma-
chine translation system. It is not feasible in every
instance to do the integration work first and then to
evaluate the output.
Table 1 summarizes the data used to train the
parsers, giving the number of tokens (excluding
traces and other empty elements) and counts of sen-
tences.1
3 Parser Architecture
We take as our starting point a re-implementation
of McDonald?s state-of-the-art dependency parser
(McDonald et al, 2005a). Given a sentence x, the
goal of the parser is to find the highest-scoring parse
y? among all possible parses y ? Y :
y? = arg max
y?Y
s(x, y) (1)
1The files in each partition of the Chinese and Arabic data
are given at http://research.microsoft.com/?simonco/
HLTNAACL2006.
161
Language Total Training Development Blind
Tokens Sentences Sentences Sentences
Arabic 116,695 2,100 446 449
Chinese 527,242 14,735 1,961 2,080
Czech 1,595,247 73,088 7,319 7,507
English 1,083,159 39,832 1,346 2,416
Table 1: Summary of data used to train parsers.
For a given parse y, its score is the sum of the scores
of all its dependency links (i, j) ? y:
s(x, y) = ?
(i,j)?y
d(i, j) = ?
(i,j)?y
w ? f(i, j) (2)
where the link (i, j) indicates a head-child depen-
dency between the token at position i and the token
at position j. The score d(i, j) of each dependency
link (i, j) is further decomposed as the weighted
sum of its features f(i, j).
This parser architecture naturally consists of three
modules: (1) a decoder that enumerates all possi-
ble parses y and computes the argmax; (2) a train-
ing algorithm for adjusting the weights w given the
training data; and (3) a feature representation f(i, j).
Two decoders will be discussed here; the training al-
gorithm and feature representation are discussed in
the following sections.
A good decoder should satisfy several proper-
ties: ideally, it should be able to search through all
valid parses of a sentence and compute the parse
scores efficiently. Efficiency is a significant issue
since there are usually an exponential number of
parses for any given sentence, and the discrimina-
tive training methods we will describe later require
repeated decoding at each training iteration. We re-
implemented Eisner?s decoder (Eisner, 1996), which
searches among all projective parse trees, and the
Chu-Liu-Edmonds? decoder (Chu and Liu, 1965;
Edmonds, 1967), which searches in the space of
both projective and non-projective parses. (A pro-
jective tree is a parse with no crossing dependency
links.) For the English and Chinese data, the head-
finding rules for converting from Penn Treebank
analyses to dependency analyses creates trees that
are guaranteed to be projective, so Eisner?s algo-
rithm suffices. For the Czech and Arabic corpora,
a non-projective decoder is necessary. Both algo-
rithms are O(N3), where N is the number of words
in a sentence.2 Refer to (McDonald et al, 2005b)
for a detailed treatment of both algorithms.
4 Training: The Bayes Point Machine
In this section, we describe an online learning al-
gorithm for training the weights w. First, we ar-
gue why an online learner is more suitable than a
batch learner like a Support Vector Machine (SVM)
for this task. We then review some standard on-
line learners (e.g. perceptron) before presenting the
Bayes Point Machine (BPM) (Herbrich et al, 2001;
Harrington et al, 2003).
4.1 Online Learning
An online learner differs from a batch learner in that
it adjusts w incrementally as each input sample is
revealed. Although the training data for our pars-
ing problem exists as a batch (i.e. all input sam-
ples are available during training), we can apply
online learning by presenting the input samples in
some sequential order. For large training set sizes,
a batch learner may face computational difficulties
since there already exists an exponential number of
parses per input sentence. Online learning is more
tractable since it works with one input at a time.
A popular online learner is the perceptron. It ad-
justs w by updating it with the feature vector when-
ever a misclassification on the current input sample
occurs. It has been shown that such updates con-
verge in a finite number of iterations if the data is lin-
early separable. The averaged perceptron (Collins,
2002) is a variant which averages the w across all
iterations; it has demonstrated good generalization
especially with data that is not linearly separable,
as in many natural language processing problems.
2The Chu-Liu-Edmonds? decoder, which is based on a maxi-
mal spanning tree algorithm, can run in O(N2), but our simpler
implementation of O(N3) was sufficient.
162
Recently, the good generalization properties of Sup-
port Vector Machines have prompted researchers to
develop large margin methods for the online set-
ting. Examples include the margin perceptron (Duda
et al, 2001), ALMA (Gentile, 2001), and MIRA
(which is used to train the parser in (McDonald et al,
2005a)). Conceptually, all these methods attempt to
achieve a large margin and approximate the maxi-
mum margin solution of SVMs.
4.2 Bayes Point Machines
The Bayes Point Machine (BPM) achieves good
generalization similar to that of large margin meth-
ods, but is motivated by a very different philoso-
phy of Bayesian learning or model averaging. In
the Bayesian learning framework, we assume a prior
distribution over w. Observations of the training
data revise our belief of w and produce a poste-
rior distribution. The posterior distribution is used
to create the final wBPM for classification:
wBPM = Ep(w|D)[w] =
|V (D)|?
i=1
p(wi|D) wi (3)
where p(w|D) is the posterior distribution of the
weights given the data D and Ep(w|D) is the expec-
tation taken with respect to this distribution. The
term |V (D)| is the size of the version space V (D),
which is the set of weights wi that is consistent with
the training data (i.e. the set of wi that classifies the
training data with zero error). This solution achieves
the so-called Bayes Point, which is the best approx-
imation to the Bayes optimal solution given finite
training data.
In practice, the version space may be large, so we
approximate it with a finite sample of size I . Further,
assuming a uniform prior over weights, we get the
following equation:
wBPM = Ep(w|D)[w] ?
I?
i=1
1
I wi (4)
Equation 4 can be computed by a very simple al-
gorithm: (1) Train separate perceptrons on different
random shuffles of the entire training data, obtaining
a set of wi. (2) Take the average (arithmetic mean)
of the weights wi. It is well-known that perceptron
training results in different weight vector solutions
Input: Training set D = ((x1, y1), (x2, y2), . . . , (xT , yT ))
Output: wBPM
Initialize: wBPM = 0
for i = 1 to I; do
Randomly shuffle the sequential order of samples in D
Initialize: wi = 0
for t = 1 to T; do
y?t = wi ? xt
if (y?t != yt) thenwi = wi + ytxt
done
wBPM = wBPM + 1Iwidone
Figure 1: Bayes Point Machine pseudo-code.
if the data samples are presented sequentially in dif-
ferent orders. Therefore, random shuffles of the data
and training a perceptron on each shuffle is effec-
tively equivalent to sampling different models (wi)
in the version space. Note that this averaging op-
eration should not be confused with ensemble tech-
niques such as Bagging or Boosting?ensemble tech-
niques average the output hypotheses, whereas BPM
averages the weights (models).
The BPM pseudocode is given in Figure 1. The
inner loop is simply a perceptron algorithm, so the
BPM is very simple and fast to implement. The
outer loop is easily parallelizable, allowing speed-
ups in training the BPM. In our specific implemen-
tation for dependency parsing, the line of the pseu-
docode corresponding to [y?t = wi ? xt] is replaced
by Eq. 1 and updates are performed for each in-
correct dependency link. Also, we chose to average
each individual perceptron (Collins, 2002) prior to
Bayesian averaging.
Finally, it is important to note that the definition of
the version space can be extended to include weights
with non-zero training error, so the BPM can handle
data that is not linearly separable. Also, although we
only presented an algorithm for linear classifiers (pa-
rameterized by the weights), arbitrary kernels can be
applied to BPM to allow non-linear decision bound-
aries. Refer to (Herbrich et al, 2001) for a compre-
hensive treatment of BPMs.
5 Features
Dependency parsers for all four languages were
trained using the same set of feature types. The
feature types are essentially those described in (Mc-
Donald et al, 2005a). For a given pair of tokens,
163
where one is hypothesized to be the parent and the
other to be the child, we extract the word of the par-
ent token, the part of speech of the parent token, the
word of the child token, the part of speech of the
child token and the part of speech of certain adjacent
and intervening tokens. Some of these atomic fea-
tures are combined in feature conjunctions up to four
long, with the result that the linear classifiers de-
scribed below approximate polynomial kernels. For
example, in addition to the atomic features extracted
from the parent and child tokens, the feature [Par-
entWord, ParentPOS, ChildWord, ChildPOS] is also
added to the feature vector representing the depen-
dency between the two tokens. Additional features
are created by conjoining each of these features with
the direction of the dependency (i.e. is the parent to
the left or right of the child) and a quantized measure
of the distance between the two tokens. Every token
has exactly one parent. The root of the sentence has
a special synthetic token as its parent.
Like McDonald et al we add features that con-
sider the first five characters of words longer than
five characters. This truncated word crudely approx-
imates stemming. For Czech and English the addi-
tion of these features improves accuracy. For Chi-
nese and Arabic, however, it is clear that we need a
different backoff strategy.
For Chinese, we truncate words longer than a sin-
gle character to the first character.3 Experimental
results on the development test set suggested that an
alternative strategy, truncation of words longer than
two characters to the first two characters, yielded
slightly worse results.
The Arabic data is annotated with gold-standard
morphological information, including information
about stems. It is also annotated with the output
of an automatic morphological analyzer, so that re-
searchers can experiment with Arabic without first
needing to build these components. For Arabic, we
truncate words to the stem, using the value of the
lemma attribute.
All tokens are converted to lowercase, and num-
bers are normalized. In the case of English, Czech
and Arabic, all numbers are normalized to a sin-
3There is a near 1:1 correspondence between characters
and morphemes in contemporary Mandarin Chinese. However,
most content words consist of more than one morpheme, typi-
cally two.
gle token. In Chinese, months are normalized to a
MONTH token, dates to a DATE token, years to a
YEAR token. All other numbers are normalized to a
single NUMBER token.
The feature types were instantiated using all or-
acle combinations of child and parent tokens from
the training data. It should be noted that when the
feature types are instantiated, we have considerably
more features than McDonald et al For example,
for English we have 8,684,328 whereas they report
6,998,447 features. We suspect that this is mostly
due to differences in implementation of the features
that backoff to stems.
The averaged perceptrons were trained on the
one-best parse, updating the perceptron for every
edge and averaging the accumulated perceptrons af-
ter every sentence. Experiments in which we up-
dated the perceptron based on k-best parses tended
to produce worse results. The Chu-Liu-Edmonds al-
gorithm was used for Czech. Experiments with the
development test set suggested that the Eisner de-
coder gave better results for Arabic than the Chu-
Liu-Edmonds decoder. We therefore used the Eisner
decoder for Arabic, Chinese and English.
6 Results
Table 2 presents the accuracy of the dependency
parsers. Dependency accuracy indicates for how
many tokens we identified the correct head. Root ac-
curacy, i.e. for how many sentences did we identify
the correct root or roots, is reported as F1 measure,
since sentences in the Czech and Arabic corpora can
have multiple roots and since the parsing algorithms
can identify multiple roots. Complete match indi-
cates how many sentences were a complete match
with the oracle dependency parse.
A convention appears to have arisen when report-
ing dependency accuracy to give results for English
excluding punctuation (i.e., ignoring punctuation to-
kens in the output of the parser) and to report results
for Czech including punctuation. In order to facil-
itate comparison of the present results with previ-
ously published results, we present measures includ-
ing and excluding punctuation for all four languages.
We hope that by presenting both sets of measure-
ments, we also simplify one dimension along which
published results of parse accuracy differ. A direct
164
Including punctuation Excluding punctuation
Language Dependency Root Complete Dependency Root Complete
Accuracy Accuracy Match Accuracy Accuracy Match
Arabic 79.9 90.0 9.80 79.8 87.8 10.2
Chinese 71.2 66.2 17.5 73.3 66.2 18.2
Czech 84.0 88.8 30.9 84.3 76.2 32.2
English 90.0 93.7 35.1 90.8 93.7 37.6
Table 2: Bayes Point Machine accuracy measured on blind test set.
comparison of parse results across languages is still
difficult for reasons to do with the different nature
of the languages, the corpora and the differing stan-
dards of linguistic detail annotated, but a compar-
ison of parsers for two different languages where
both results include punctuation is at least preferable
to a comparison of results including punctuation to
results excluding punctuation.
The results reported here for English and Czech
are comparable to the previous best published num-
bers in (McDonald et al, 2005a), as Table 3 shows.
This table compares McDonald et al?s results for an
averaged perceptron trained for ten iterations with
no check for convergence (Ryan McDonald, pers.
comm.), MIRA, a large margin classifier, and the
current Bayes Point Machine results. To determine
statistical significance we used confidence intervals
for p=0.95. For the comparison of English depen-
dency accuracy excluding punctuation, MIRA and
BPM are both statistically significantly better than
the averaged perceptron result reported in (McDon-
ald et al, 2005a). MIRA is significantly better
than BPM when measuring dependency accuracy
and root accuracy, but BPM is significantly better
when measuring sentences that match completely.
From the fact that neither MIRA nor BPM clearly
outperforms the other, we conclude that we have
successfully replicated the results reported in (Mc-
Donald et al, 2005a) for English.
For Czech we also determined significance using
confidence intervals for p=0.95 and compared re-
sults including punctuation. For both dependency
accuracy and root accuracy, MIRA is statisticallty
significantly better than averaged perceptron, and
BPM is statistically significantly better than MIRA.
Measuring the number of sentences that match com-
pletely, BPM is statistically significantly better than
averaged perceptron, but MIRA is significantly bet-
ter than BPM. Again, since neither MIRA nor BPM
outperforms the other on all measures, we conclude
that the results constitute a valiation of the results
reported in (McDonald et al, 2005a).
For every language, the dependency accuracy of
the Bayes Point Machine was greater than the ac-
curacy of the best individual perceptron that con-
tributed to that Bayes Point Machine, as Table 4
shows. As previously noted, when measuring
against the development test set, we used human-
annotated part-of-speech labels for English and Chi-
nese.
Although the Prague Czech Dependency Tree-
bank is much larger than the English Penn Treebank,
all measurements are lower than the corresponding
measurements for English. This reflects the fact that
Czech has considerably more inflectional morphol-
ogy than English, leading to data sparsity for the lex-
ical features.
The results reported here for Arabic are, to our
knowledge, the first published numbers for depen-
dency parsing of Arabic. Similarly, the results for
Chinese are the first published results for the depen-
dency parsing of the Chinese Treebank 5.0.4 Since
the Arabic and Chinese numbers are well short of
the numbers for Czech and English, we attempted
to determine what impact the smaller corpora used
for training the Arabic and Chinese parsers might
have. We performed data reduction experiments,
training the parsers on five random samples at each
size smaller than the entire training set. Figure 2
shows the dependency accuracy measured on the
complete development test set when training with
samples of the data. The graph shows the average
4(Wang et al, 2005) report numbers for undirected depen-
dencies on the Chinese Treebank 3.0. We cannot meaningfully
compare those numbers to the numbers here.
165
Language Algorithm DA RA CM
English Avg. Perceptron 90.6 94.0 36.5
(exc punc) MIRA 90.9 94.2 37.5
Bayes Point Machine 90.8 93.7 37.6
Czech Avg. Perceptron 82.9 88.0 30.3
(inc punc) MIRA 83.3 88.6 31.3
Bayes Point Machine 84.0 88.8 30.9
Table 3: Comparison to previous best published results reported in (McDonald et al, 2005a).
Arabic Chinese Czech English
Bayes Point Machine 78.4 83.8 84.5 91.2
Best averaged perceptron 77.9 83.1 83.5 90.8
Worst averaged perceptron 77.4 82.6 83.3 90.5
Table 4: Bayes Point Machine accuracy vs. averaged perceptrons, measured on development test set, ex-
cluding punctuation.
dependency accuracy for five runs at each sample
size up to 5,000 sentences. English and Chinese
accuracies in this graph use oracle part-of-speech
tags. At all sample sizes, the dependency accu-
racy for English exceeds the dependency accuracy
of the other languages. This difference is perhaps
partly attributable to the use of oracle part-of-speech
tags. However, we suspect that the major contribu-
tor to this difference is the part-of-speech tag set.
The tags used in the English Penn Treebank encode
traditional lexical categories such as noun, prepo-
sition, and verb. They also encode morphological
information such as person (the VBZ tag for exam-
ple is used for verbs that are third person, present
tense?typically with the suffix -s), tense, number
and degree of comparison. The part-of-speech tag
sets used for the other languages encode lexical cat-
egories, but do not encode morphological informa-
tion.5 With small amounts of data, the perceptrons
do not encounter sufficient instances of each lexical
item to calculate reliable weights. The perceptrons
are therefore forced to rely on the part-of-speech in-
formation.
It is surprising that the results for Arabic and Chi-
nese should be so close as we vary the size of the
5For Czech and Arabic we followed the convention estab-
lished in previous parsing work on the Prague Czech Depen-
dency Treebank of using the major and minor part-of-speech
tags but ignoring other morphological information annotated on
each node.
training data (Figure 2) given that Arabic has rich
morphology and Chinese very little. One possible
explanation for the similarity in accuracy is that the
rather poor root accuracy in Chinese indicates parses
that have gone awry. Anecdotal inspection of parses
suggests that when the root is not correctly identi-
fied, there are usually cascading related errors.
Czech, a morphologically complex language in
which root identification is far from straightfor-
ward, exhibits the worst performance at small sam-
ple sizes. But (not shown) as the sample size in-
creases, the accuracy of Czech and Chinese con-
verge.
7 Conclusions
We have successfully replicated the state-of-the-art
results for dependency parsing (McDonald et al,
2005a) for both Czech and English, using Bayes
Point Machines. Bayes Point Machines have the ap-
pealing property of simplicity, yet are competitive
with online wide margin methods.
We have also presented first results for depen-
dency parsing of Arabic and Chinese, together with
some analysis of the performance on those lan-
guages.
In future work we intend to explore the discrim-
inative reranking of n-best lists produced by these
parsers and the incorporation of morphological fea-
tures.
166
60
65
70
75
80
85
90
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000Sample size
Depen
dency
 Accu
racy
EnglishChineseArabicCzech
Figure 2: Dependency accuracy at various sample
sizes. Graph shows average of five samples at each
size and measures accuracy against the development
test set.
Acknowledgements
We would like to thank Ryan McDonald, Otakar
Smrz? and Hiroyasu Yamada for help in various
stages of the project.
References
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Processing. Ph.D. the-
sis, University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of EMNLP.
R. O. Duda, P. E. Hart, and D. G. Stork. 2001. Pattern
Classification. John Wiley & Sons, Inc.: New York.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of COLING 1996, pages 340?345.
Claudio Gentile. 2001. A new approximate maximal
margin classification algorithm. Journal of Machine
Learning Research, 2:213?242.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proc. 7th Pacific-Asia
Conference on Knowledge Discovery and Data Min-
ing, pages 241?252.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes point machines. Journal of Machine
Learning Research, pages 245?278.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Assocation for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005b. Online large-margin training of dependency
parsers. Technical Report MS-CIS-05-11, Dept. of
Computer and Information Science, Univ. of Pennsyl-
vania.
Igor A. Melc?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Arul Menezes and Chris Quirk. 2005. Microsoft re-
search treelet translation system: IWSLT evaluation.
In Proceedings of the International Workshop on Spo-
ken Language Translation.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd annual meet-
ing of the Association for Computational Linguistics.
Lucien Tesnie`re. 1959. E?le?ments de syntaxe structurale.
Librairie C. Klincksieck.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2005.
Strictly lexical dependency parsing. In Proceedings
of the Ninth International Workshop on Parsing Tech-
nologies, pages 152?159.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2).
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
167
A machine learning approach
to the automatic evaluation of machine translation
Simon Corston-Oliver, Michael Gamon and Chris Brockett
Microsoft Research
One Microsoft Way
Redmond WA 98052, USA
{simonco, mgamon, chrisbkt}@microsoft.com
Abstract
We present a machine learning
approach to evaluating the well-
formedness of output of a machine
translation system, using classifiers that
learn to distinguish human reference
translations from machine translations.
This approach can be used to evaluate
an MT system, tracking improvements
over time; to aid in the kind of failure
analysis that can help guide system
development; and to select among
alternative output strings. The method
presented is fully automated and
independent of source language, target
language and domain.
1 Introduction
Human evaluation of machine translation (MT)
output is an expensive process, often
prohibitively so when evaluations must be
performed quickly and frequently in order to
measure progress. This paper describes an
approach to automated evaluation designed to
facilitate the identification of areas for
investigation and improvement. It focuses on
evaluating the wellformedness of output and
does not address issues of evaluating content
transfer.
Researchers are now applying automated
evaluation in MT and natural language
generation tasks, both as system-internal
goodness metrics and for the assessment of
output. Langkilde and Knight (1998), for
example, employ n-gram metrics to select
among candidate outputs in natural language
generation, while Ringger et al (2001) use n-
gram perplexity to compare the output of MT
systems. Su et al (1992), Alshawi et al (1998)
and Bangalore et al (2000) employ string edit
distance between reference and output sentences
to gauge output quality for MT and generation.
To be useful to researchers, however,
assessment must provide linguistic information
that can guide in identifying areas where work is
required. (See Nyberg et al, 1994 for useful
discussion of this issue.)
The better the MT system, the more its
output will resemble human-generated text.
Indeed, MT might be considered a solved
problem should it ever become impossible to
distinguish automated output from human
translation. We have observed that in general
humans can easily and reliably categorize a
sentence as either machine- or human-generated.
Moreover, they can usually justify their
decision. This observation suggests that
evaluation of the wellformedness of output
sentences can be treated as a classification
problem: given a sentence, how accurately can
we predict whether it has been translated by
machine? In this paper we cast the problem of
MT evaluation as a machine learning
classification task that targets both linguistic
features and more abstract features such as n-
gram perplexity.
2 Data
Our corpus consists of 350,000 aligned Spanish-
English sentence pairs taken from published
computer software manuals and online help
documents. We extracted 200,000 English
sentences for building language models to
evaluate per-sentence perplexity. From the
remainder of the corpus, we extracted 100,000
aligned sentence pairs. The Spanish sentences in
this latter sample were then translated by the
Microsoft machine translation system, which
was trained on documents from this domain
(Richardson et al, 2001). This yielded a set of
200,000 English sentences, one half of which
were English reference sentences, and the other
half of which were MT output. (The Spanish
sentences were not used in building or
evaluating the classifiers). We split the 200,000
English sentences 90/10, to yield 180,000
sentences for training classifiers and 20,000
sentences that we used as held-out test data.
Training and test data were evenly divided
between reference English sentences and
Spanish-to-English translations.
3 Features
The selection of features used in our
classification task was motivated by failure
analysis of system output. We were particularly
interested in those linguistic features that could
aid in qualitative analysis, as we discuss in
section 5. For each sentence we automatically
extracted 46 features by performing a syntactic
parse using the Microsoft NLPWin natural
language processing system (Heidorn, 2000) and
language modeling tools. The features extracted
fall into two broad categories:
(i) Perplexity measures were extracted using the
CMU-Cambridge Statistical Language Modeling
Toolkit (Clarkson and Rosenfeld, 1997). We
calculated two sets of values: lexicalized trigram
perplexity, with values discretized into deciles
and part of speech (POS) trigram perplexity. For
the latter we used the following sixteen POS
tags: adjective, adverb, auxiliary, punctuation,
complementizer, coordinating conjunction,
subordinating conjunction, determiner,
interjection, noun, possessor, preposition,
pronoun, quantifier, verb, and other.
(ii) Linguistic features fell into several
subcategories: branching properties of the parse;
function word density, constituent length, and
other miscellaneous features
We employed a selection of features to
provide a detailed assessment of the branching
properties of the parse tree. The linguistic
motivation behind this was twofold. First, it had
become apparent from failure analysis that MT
system output tended to favor right-branching
structures over noun compounding. Second, we
hypothesized that translation from languages
whose branching properties are radically
different from English (e.g. Japanese, or a verb-
second language like German) might pollute the
English output with non-English characteristics.
For this reason, assessment of branching
properties is a good candidate for a language-
pair independent measure. The branching
features we employed are given below. Indices
are scalar counts; other measures are normalized
for sentence length.
? number of right-branching nodes across
all constituent types
? number of right-branching nodes for
NPs only
? number of left-branching nodes across
all constituent types
? number of left-branching nodes for NPs
only
? number of premodifiers across all
constituent types
? number of premodifiers within NPs only
? number of postmodifiers across all
constituent types
? number of postmodifiers within NPs
only
? branching index across all constituent
types, i.e. the number of right-branching
nodes minus number of left-branching
nodes
? branching index for NPs only
? branching weight index: number of
tokens covered by right-branching
nodes minus number of tokens covered
by left-branching nodes across all
categories
? branching weight index for NPs only
? modification index, i.e. the number of
premodifiers minus the number of
postmodifiers across all categories
? modification index for NPs only
? modification weight index: length in
tokens of all premodifiers minus length
in tokens of all postmodifiers across all
categories
? modification weight index for NPs only
? coordination balance, i.e. the maximal
length difference in coordinated
constituents
We considered the density of function words,
i.e. the ratio of function words to content words,
because of observed problems in WinMT
output. Pronouns received special attention
because of frequent problems detected in failure
analysis. The density features are:
? overall function word density
? density of determiners/quantifiers
? density of pronouns
? density of prepositions
? density of punctuation marks,
specifically commas and semicolons
? density of auxiliary verbs
? density of conjunctions
? density of different pronoun types: Wh,
1st, 2nd, and 3rd person pronouns
We also measured the following constituent
sizes:
? maximal and average NP length
? maximal and average AJP length
? maximal and average PP length
? maximal and average AVP length
? sentence length
On a lexical level, the presence of out of
vocabulary (OOV) words is frequently caused
by the direct transfer of source language words
for which no translation could be found. The
top-level syntactic template, i.e. the labels of the
immediate children of the root node of a
sentence, was also used, as was subject-verb
disagreement. The final five features are:
? number of OOV words
? the presence of a word containing a non-
English letter, i.e. an extended ASCII
character. This is a special case of the
OOV problem.
? label of the root node of the sentence
(declarative, imperative, question, NP,
or "FITTED" for non-spanning parses)
? sentence template, i.e. the labels of the
immediate children of the root node.
? subject-verb disagreement
4 Decision Trees
We used a set of automated tools to construct
decision trees (Chickering et al, 1997) based on
the features extracted from the reference and
MT sentences. To avoid overfitting, we
specified that nodes in the decision tree should
not be split if they accounted for fewer than fifty
cases. In the discussion below we distinguish the
perplexity features from the linguistic features.
4.1 Decision trees built using all
training data
Table 1 gives the accuracy of the decision trees,
when trained on all 180,000 training sentences
and evaluated against the 20,000 held-out test
sentences. Since the training data and test data
contain an even split between reference human
translations and machine translations, the
baseline for comparison is 50.00%. As Table 1
shows, the decision trees dramatically
outperform this baseline. Using only perplexity
features or only linguistic features yields
accuracy substantially above this baseline.
Combining the two sets of features yields the
highest accuracy, 82.89%.
Features used Accuracy (%)
All features 82.89
Perplexity features only 74.73
Linguistic features only 76.51
Table 1 Accuracy of the decision trees
Notably, most of the annotated features
were selected by the decision tree tools. Two
features were found not to be predictive. The
first non-selected feature is the presence of a
word containing an extended ASCII character,
suggesting that general OOV features were
sufficient and subsume the effect of this
narrower feature. Secondly, subject-verb
disagreement was also not predictive, validating
the consistent enforcement of agreement
constraints in the natural language generation
component of the MT system. In addition, only
eight of approximately 5,200 observed sentence
templates turned out to be discriminatory.
For a different use of perplexity in
classification, see Ringger et al (2001) who
compare the perplexity of a sentence using a
language model built solely from reference
translations to the perplexity using a language
model built solely from machine translations.
The output of such a classifier could be used as
an input feature in building decision trees.
Effect of training data size
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
0
10
,00
0
20
,00
0
30
,00
0
40
,00
0
50
,00
0
60
,00
0
70
,00
0
80
,00
0
90
,00
0
10
0,0
00
11
0,0
00
12
0,0
00
13
0,0
00
14
0,0
00
15
0,0
00
16
0,0
00
17
0,0
00
18
0,0
00
Training cases
A
v
g
be
st
ac
cu
ra
cy
All features Perplexity only Linguistic only
Figure 1 Accuracy with varying amounts of training data
4.2 Varying the amount of training data
For our experiments, we had access to several
hundred thousand sentences from the target
domain. To measure the effect of reducing the
size of the training data set on the accuracy of
the classifier, we built classifiers using samples
of the training data and evaluating against the
same held-out sample of 20,000 sentences. We
randomly extracted ten samples containing the
following numbers of sentences: {1,000, 2,000,
3,000, 4,000, 5,000, 6,000, 12,000, 25,000,
50,000, 100,000, 150,000}. Figure 1 shows the
effect of varying the size of the training data.
The data point graphed is the average accuracy
over the ten samples at a given sample size, with
error bars showing the range from the least
accurate decision tree at that sample size to the
most accurate.
As Figure 1 shows, the models built using
only perplexity features do not benefit from
additional training data. The models built using
linguistic features, however, benefit
substantially, with accuracy leveling off after
150,000 training cases. With only 2,000 training
cases, the classifiers built using all features
range in accuracy from 75.06% to 78.84%,
substantially above the baseline accuracy of
50%.
5 Discussion
As the results in section 4 show, it is possible to
build classifiers that can distinguish human
reference translations from the output of a
machine translation system with high accuracy.
We thus have an automatic mechanism that can
perform the task that humans appear to do with
ease, as noted in section 1. The best result, a
classifier with 82.89% accuracy, is achieved by
combining perplexity calculations with a set of
finer-grained linguistic features. Even with as
few as 2,000 training cases, accuracy exceeded
75%. In the discussion below we consider the
advantages and possible uses of this automatic
evaluation methodology.
5.1 Advantages of the approach
Once an appropriate set of features has been
selected and tools to automatically extract those
features are in place, classifiers can be built and
evaluated quickly. This overcomes the two
problems associated with traditional manual
evaluation of MT systems: manual evaluation is
both costly and time-consuming. Indeed, an
automated approach is essential when dealing
with an MT system that is under constant
development in a collaborative research
environment. The output of such a system may
change from day to day, requiring frequent
feedback to monitor progress.
The methodology does not crucially rely on
any particular set of features. As an MT system
matures, more and more subtle cues might be
necessary to distinguish between human and
machine translations. Any linguistic feature that
can be reliably extracted can be proposed as a
candidate feature to the decision tree tools.
The methodology is also not sensitive to the
domain of the training texts. All that is needed
to build classifiers for a new domain is a
sufficient quantity of aligned translations.
5.2 Possible applications of the
approach
The classifiers can be used for evaluating a
system overall, providing feedback to aid in
system development, and in evaluating
individual sentences.
Evaluating an MT system overall
Evaluating the accuracy of the classifier against
held-out data is equivalent to evaluating the
fluency of the MT system. As the MT system
improves, its output will become more like the
human reference translations. To measure
improvement over time, we would hold the set
of features constant and build and evaluate new
classifiers using the human reference
translations and the output of the MT system at a
given point in time. Using the same set of
features, we expect the accuracy of the
classifiers to go down over time as the MT
output becomes more like human translations.
Feedback to aid system development
Our primary interest in evaluating an MT system
is to identify areas that require improvement.
This has been the motivation for using linguistic
features in addition to perplexity measures.
From the point of view of system development,
perplexity is a rather opaque measure. This can
be viewed as both a strength and a weakness. On
the one hand, it is difficult to tune a system with
the express goal of causing perplexity to
improve, rendering perplexity a particularly
good objective measurement. On the other hand,
given a poor perplexity score, it is not clear how
to improve a system without additional failure
analysis.
We used the DNETVIEWER tool (Heckerman
et al, 2000), a visualization tool for viewing
decision trees and Bayesian networks, to explore
the decision trees and identify problem areas in
our MT system. In one visualization, shown in
Figure 2, DNETVIEWER allows the user to adjust
a slider to see the order in which the features
were selected during the heuristic search that
guides the construction of decision trees. The
most discriminatory features are those which
cause the MT translations to look most awful, or
are characteristics of the reference translations
that ought to be emulated by the MT system. For
the coarse model shown in Figure 2, the distance
between pronouns (nPronDist) is the highest
predictor, followed by the number of second
person pronouns (n2ndPersPron), the number of
function words (nFunctionWords), and the
distance between prepositions (nPrepDist).
Using DNETVIEWER we are able to explore
the decision tree, as shown in Figure 3. Viewing
the leaf nodes in the decision tree, we see a
probability distribution over the possible states
of the target variable. In the case of the binary
classifier here, this is the probability that a
sentence will be a reference translation. In
Figure 3, the topmost leaf node shows that
p(Human translation) is low. We modified
DNETVIEWER so that double-clicking on the leaf
node would display reference translations and
MT sentences from the training data. We display
a window showing the path through the decision
tree, the probability that the sentence is a
reference translation given that path, and the
sentences from the training data identified by the
features on the path. This visualization allows
the researcher to view manageable groups of
similar problem sentences with a view to
identifying classes of problems within the
groups. A goal for future research is to select
additional linguistic features that will allow us to
pinpoint problem areas in the MT system and
thereby further automate failure analysis.
Figure 2 Using the slider to view the best predictors
Figure 3 Examining sentences at a leaf node in the decision tree
Figure 4 Examining sentences at a leaf node in the decision tree
Decision trees are merely one form of
classifier that could be used for the automated
evaluation of an MT system. In preliminary
experiments, the accuracy of classifiers using
support vector machines (SVMs) (Vapnik, 1998;
Platt et al, 2000) exceeded the accuracy of the
decision tree classifiers by a little less than one
percentage point using a linear kernel function,
and by a slightly greater margin using a
polynomial kernel function of degree three. We
prefer the decision tree classifiers because they
allow a researcher to explore the classification
system and focus on problem areas and
sentences. We find this method for exploring the
data more intuitive than attempting to visualize
the location of sentences in the high-
dimensional space of the corresponding SVM.
Evaluating individual sentences
In addition to system evaluation and failure
analysis, classifiers could be used on a per-
sentence basis to guide the output of an MT
system by selecting among multiple candidate
strings. If no candidate is judged sufficiently
similar to a human reference translation, the
sentence could be flagged for human post-
editing.
6 Conclusion
We have presented a method for evaluating the
fluency of MT, using classifiers based on
linguistic features to emulate the human ability
to distinguish MT from human translation. The
techniques we have described are system- and
language-independent. Possible applications of
our approach include system evaluation, failure
analysis to guide system development, and
selection among alternative possible outputs.
We have focused on structural aspects of a
text that can be used to evaluate fluency. A full
evaluation of MT quality would of course need
to include measurements of idiomaticity and
techniques to verify that the semantic and
pragmatic content of the source language had
been successfully transferred to the target
language.
Acknowledgements
Our thanks go to Eric Ringger and Max
Chickering for programming assistance with the
tools used in building and evaluating the
decision trees, and to Mike Carlson for help in
sampling the initial datasets. Thanks also to
John Platt for helpful discussion on parameter
setting for the SVM tools, and to the members
of the MSR NLP group for feedback on the uses
of the methodology presented here.
References
Alshawi, H., S. Bangalore, and S. Douglas. 1998.
Automatic acquisition of hierarchical transduction
models for machine translation. In Proceedings of
the 36th Annual Meeting of the Association for
Computational Linguistics, Montreal Canada, Vol.
I: 41-47.
Bangalore, S., O. Rambow, and S. Whittaker. 2000.
Evaluation Metrics for Generation. In Proceedings
of the International Conference on Natural
Language Generation (INLG 2000), Mitzpe
Ramon, Israel. 1-13.
Chickering, D. M., D. Heckerman, and C. Meek.
1997. A Bayesian approach to learning Bayesian
networks with local structure. In Geiger, D. and P.
Punadlik Shenoy (Eds.), Uncertainty in Artificial
Intelligence: Proceedings of the Thirteenth
Conference. 80-89.
Clarkson, P. and R. Rosenfeld. 1997. Statistical
Language Modeling Using the CMU-Cambridge
Toolkit. Proceedings of Eurospeech97. 2707-
2710.
Heckerman, D., D. M. Chickering, C. Meek, R.
Rounthwaite, and C. Kadie. 2000. Dependency
networks for inference, collaborative filtering and
data visualization. Journal of Machine Learning
Research 1:49-75.
Heidorn, G. E., 2000. Intelligent writing assistance.
In R. Dale, H. Moisl and H. Somers (Eds.).
Handbook of Natural Language Processing. New
York, NY. Marcel Dekker. 181-207.
Langkilde, I., and K. Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics, and
17th International Conference on Computational
Linguistics, Montreal, Canada. 704-710.
Nyberg, E. H., T. Mitamura, and J. G. Carbonnell.
1994. Evaluation Metrics for Knowledge-Based
Machine Translation. In Proceedings of the 15th
International Conference on Computational
Linguistics, Kyoto, Japan (Coling 94). 95-99.
Platt, J., N. Cristianini, J. Shawe-Taylor. 2000. Large
margin DAGs for multiclass classification. In
Advances in Neural Information Processing
Systems 12, MIT Press. 547-553.
Richardson, S., B. Dolan, A. Menezes, and J.
Pinkham. 2001. Achieving commercial-quality
translation with example-based methods.
Submitted for review.
Ringger, E., M. Corston-Oliver, and R. Moore. 2001.
Using Word-Perplexity for Automatic Evaluation
of Machine Translation. Manuscript.
Su, K., M. Wu, and J. Chang. 1992. A new
quantitative quality measure for machine
translation systems. In Proceedings of COLING-
92, Nantes, France. 433-439.
Vapnik, V. 1998. Statistical Learning Theory, Wiley-
Interscience, New York.
Machine-learned contexts for linguistic operations 
in German sentence realization 
 
Michael GAMON, Eric RINGGER, Simon CORSTON-OLIVER, Robert MOORE 
Microsoft Research  
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, simonco, bobmoore}@microsoft.com 
 
Abstract 
We show that it is possible to learn the 
contexts for linguistic operations which 
map a semantic representation to a 
surface syntactic tree in sentence 
realization with high accuracy. We cast 
the problem of learning the contexts for 
the linguistic operations as 
classification tasks, and apply 
straightforward machine learning 
techniques, such as decision tree 
learning. The training data consist of 
linguistic features extracted from 
syntactic and semantic representations 
produced by a linguistic analysis 
system. The target features are extracted 
from links to surface syntax trees. Our 
evidence consists of four examples from 
the German sentence realization system 
code-named Amalgam: case 
assignment, assignment of verb position 
features, extraposition, and syntactic 
aggregation 
1 Introduction 
The last stage of natural language generation, 
sentence realization, creates the surface string 
from an abstract (typically semantic) 
representation. This mapping from abstract 
representation to surface string can be direct, or it 
can employ intermediate syntactic representations 
which significantly constrain the output. 
Furthermore, the mapping can be performed 
purely by rules, by application of statistical 
models, or by a combination of both techniques. 
Among the systems that use statistical or 
machine learned techniques in sentence 
realization, there are various degrees of 
intermediate syntactic structure. Nitrogen 
(Langkilde and Knight, 1998a, 1998b) produces a 
large set of alternative surface realizations of an 
input structure (which can vary in abstractness). 
This set of candidate surface strings, represented 
as a word lattice, is then rescored by a word-
bigram language model, to produce the best-
ranked output sentence. FERGUS (Bangalore and 
Rambow, 2000), on the other hand, employs a 
model of syntactic structure during sentence 
realization. In simple terms, it adds a tree-based 
stochastic model to the approach taken by the 
Nitrogen system. This tree-based model chooses a 
best-ranked XTAG representation for a given 
dependency structure. Possible linearizations of 
the XTAG representation are generated and then 
evaluated by a language model to pick the best 
possible linearization, as in Nitrogen. 
In contrast, the sentence realization system 
code-named Amalgam (A Machine Learned 
Generation Module) (Corston-Oliver et al, 2002; 
Gamon et al, 2002b) employs a series of 
linguistic operations which map a semantic 
representation to a surface syntactic tree via 
intermediate syntactic representations. The 
contexts for most of these operations in Amalgam 
are machine learned. The resulting syntactic tree 
contains all the necessary information on its leaf 
nodes from which a surface string can be read.  
The goal of this paper is to show that it is 
possible to learn accurately the contexts for 
linguistically complex operations in sentence 
realization. We propose that learning the contexts 
for the application of these linguistic operations 
can be viewed as per-operation classification 
problems. This approach combines advantages of 
a linguistically informed approach to sentence 
realization with the advantages of a machine 
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 25-32.
                         Proceedings of the 40th Annual Meeting of the Association for
learning approach. The linguistically informed 
approach allows us to deal with complex linguistic 
phenomena, while machine learning automates the 
discovery of contexts that are linguistically 
relevant and relevant for the domain of the data. 
The machine learning approach also facilitates 
adaptation of the system to a new domain or 
language. Furthermore, the quantitative nature of 
the machine learned models permits finer 
distinctions and ranking among possible solutions. 
To substantiate our claim, we provide four 
examples from Amalgam: assignment of case, 
assignment of verb position features, 
extraposition, and syntactic aggregation. 
2 Overview of Amalgam 
Amalgam takes as its input a sentence-level 
semantic graph representation with fixed lexical 
choices for content words (the logical form graph 
of the NLPWin system ? see (Heidorn, 2000)). 
This representation is first degraphed into a tree, 
and then gradually augmented by the insertion of 
function words, assignment of case and verb 
position features, syntactic labels, etc., and 
transformed into a syntactic surface tree. A 
generative statistical language model establishes 
linear order in the surface tree (Ringger et al, in 
preparation), and a surface string is generated 
from the leaf nodes. Amalgam consists of eight 
stages. We label these ML (machine-learned 
context) or RB (rule-based). 
Stage 1 Pre-processing (RB): 
? degraphing of the semantic representation 
? retrieval of lexical information 
Stage 2 Flesh-out (ML): 
? assignment of syntactic labels 
? insertion of function words 
? assignment of case and verb position 
features 
Stage 3 Conversion to syntactic tree (RB): 
? introduction of syntactic representation 
for coordination 
? splitting of separable prefix verbs based 
on both lexical information and 
previously assigned verb position features 
? reversal of heads (e.g., in quantitative 
expressions) (ML) 
Stage 4 Movement: 
? extraposition (ML) 
? raising, wh movement (RB) 
Stage 5 Ordering (ML): 
? ordering of constituents and leaf nodes in 
the tree 
Stage 6 Surface cleanup (ML): 
? lexical choice of determiners and relative 
pronouns 
? syntactic aggregation 
Stage 7 Punctuation (ML) 
Stage 8 Inflectional generation (RB) 
All machine learned components, with the 
exception of the generative language model for 
ordering of constituents (stage 5), are decision tree 
classifiers built with the WinMine toolkit 
(Chickering et al, 1997; Chickering, nd.). There 
are a total of eighteen decision tree classifiers in 
the system. The complexity of the decision trees 
varies with the complexity of the modeled task. 
The number of branching nodes in the decision 
tree models in Amalgam ranges from 3 to 447. 
3 Data and feature extraction 
The data for all of the models were drawn from a 
set of 100,000 sentences from technical software 
manuals and help files. The sentences are 
analyzed by the NLPWin system, which provides 
a syntactic and logical form analysis. Nodes in the 
logical form representation are linked to the 
corresponding syntactic nodes, allowing us to 
learn contexts for the mapping from the semantic 
representation to a surface syntax tree. The data is 
split 70/30 for training versus model parameter 
tuning. For each set of data we built decision trees 
at several different levels of granularity (by 
manipulating the prior probability of tree 
structures to favor simpler structures) and selected 
the model with the maximal accuracy as 
determined on the parameter tuning set. All 
models are then tested on data extracted from a 
separate blind set of 10,000 sentences from the 
same domain. For both training and test, we only 
extract features from sentences that have received 
a complete, spanning parse: 85.14% of the 
sentences in the training and parameter tuning set, 
and 84.59% in the blind test set fall into that 
category. Most sentences yield more than one 
training case. 
We attempt to standardize as much as possible 
the set of features to be extracted. We exploit the 
full set of features and attributes available in the 
analysis, instead of pre-determining a small set of 
potentially relevant features (Gamon et al, 
2002b). This allows us to share the majority of 
code between the individual feature extraction 
tasks. More importantly, it enables us to discover 
new linguistically interesting and/or domain-
specific generalizations from the data. Typically, 
we extract the full set of available analysis 
features of the node under investigation, its parent 
and its grandparent, with the only restriction being 
that these features need to be available at the stage 
where the model is consulted at generation run-
time. This provides us with a sufficiently large 
structural context for the operations. In addition, 
for some of the models we add a small set of 
features that we believe to be important for the 
task at hand, and that cannot easily be expressed 
as a combination of analysis features/attributes on 
constituents. Most features, such as lexical 
subcategorization features and semantic features 
such as [Definite] are binary. Other features, such 
as syntactic label or semantic relation, have as 
many as 25 values. Training time on a standard 
500MHz PC ranges from one hour to six hours. 
4 Assignment of case 
In German sentence realization, proper 
assignment of morphological case is essential for 
fluent and comprehensible output. German is a 
language with fairly free constituent order, and the 
identification of functional roles, such as subject 
versus object, is not determined by position in the 
sentence, as in English, but by morphological 
marking of one of the four cases: nominative, 
accusative, genitive or dative. In Amalgam, case 
assignment is one of the last steps in the Flesh-out 
stage (stage 2). Morphological realization of case 
can be ambiguous in German (for example, a 
feminine singular NP is ambiguous between 
accusative and nominative case). Since the 
morphological realization of case depends on the 
gender, number and morphological paradigm of a 
given NP, we chose to only consider NP nodes 
with unambiguous case as training data for the 
model1. As the target feature for this model is 
                                                     
1
 Ideally, we should train the case assignment model on 
a corpus that is hand-disambiguated for case. In the 
absence of such a corpus, though, we believe that our 
approach is linguistically justified. The case of an NP 
depends solely on the syntactic context it appears in. 
morphological case, it has four possible values for 
the four cases in German. 
4.1 Features in the case assignment 
model 
For each data point, a total of 712 features was 
extracted. Of the 712 features available to the 
decision tree building tools, 72 were selected as 
having predictive value in the model. The selected 
features fall into the following categories: 
? syntactic label of the node, its parent and 
grandparent 
? lemma (i.e., citation form) of the parent, 
and lemma of the governing preposition 
? subcategorization information, including 
case governing properties of governing 
preposition and parent 
? semantic relation of the node itself to its 
parent, of the parent to its grandparent, 
and of the grandparent to its great-
grandparent 
? number information on the parent and 
grandparent 
? tense and mood on the parent and 
grandparent 
? definiteness on the node, its parent and 
grandparent 
? the presence of various semantic 
dependents such as subject, direct and 
indirect objects, operators, attributive 
adjuncts and unspecified modifiers on the 
node and its parent and grandparent 
? quantification, negation, coordination on 
the node, the parent and grandparent 
? part of speech of the node, the parent and 
the grandparent 
? miscellaneous semantic features on the 
node itself and the parent 
4.2 The case assignment model 
The decision tree model for case assignment 
has 226 branching nodes, making it one of the 
most complex models in Amalgam. For each 
nominal node in the 10,000 sentence test set, we 
compared the prediction of the model to the 
                                                                                  
Since we want to learn the syntactically determining 
factors for case, using unambiguously case marked NPs 
for training seems justified. 
morphological case compatible with that node. 
The previously mentioned example of a singular 
feminine NP, for example, would yield a ?correct? 
if the model had predicted nominative or 
accusative case (because the NP is 
morphologically ambiguous between accusative 
and nominative), and it would yield an ?incorrect? 
if the model had predicted genitive or dative. This 
particular evaluation setup was a necessary 
compromise because of the absence of a hand-
annotated corpus with disambiguated case in our 
domain. The caveat here is that downstream 
models in the Amalgam pipeline that pick up on 
case as one of their features rely on the absolute 
accuracy of the assigned case, not the relative 
accuracy with respect to morphological 
ambiguity. Accuracy numbers for each of the four 
case assignments are given in Table 1. Note that it 
is impossible to give precision/recall numbers, 
without a hand-disambiguated test set. The 
baseline for this task is 0.7049 (accuracy if the 
most frequent case (nominative) had been 
assigned to all NPs). 
Table 1. Accuracy of the case assignment model. 
Value Accuracy 
Dat 0.8705 
Acc 0.9707 
Gen 0.9457 
Nom 0.9654 
overall 0.9352 
5 Assignment of verb position 
features 
One of the most striking properties of German is 
the distributional pattern of verbs in main and 
subordinate clauses. Most descriptive accounts of 
German syntax are based on a topology of the 
German sentence that treats the position of the 
verb as the fixed frame around which other 
syntactic constituents are organized in relatively 
free order (cf. Eisenberg, 1999; Engel, 1996). The 
position of the verb in German is non-negotiable; 
errors in the positioning of the verb result in 
gibberish, whereas most permutations of other 
constituents only result in less fluent output. 
Depending on the position of the finite verb, 
German sentences and verb phrases are classified 
as being ?verb-initial?, ?verb-second? or ?verb-
final?. In verb-initial clauses (e.g., in imperatives), 
the finite verb is in initial position. Verb-second 
sentences contain one constituent preceding the 
finite verb, in the so-called ?pre-field?. The finite 
verb is followed by any number of constituents in 
the ?middle-field?, and any non-finite verbs are 
positioned at the right periphery of the clause, 
possibly followed by extraposed material or 
complement clauses (the ?post-field?). Verb-final 
clauses contain no verbal element in the verb-
second position: all verbs are clustered at the right 
periphery, preceded by any number of constituents 
and followed only by complement clauses and 
extraposed material. 
During the Flesh-out stage in Amalgam, a 
decision tree classifier is consulted to make a 
classification decision among the four verb 
positions: ?verb-initial?, ?verb-second?, ?verb-
final?, and ?undefined?. The value ?undefined? 
for the target feature of verb position is extracted 
for those verbal constituents where the local 
syntactic context is too limited to make a clear 
distinction between initial, second, or final 
position of the verb. The number of ?undefined? 
verb positions is small compared to the number of 
clearly established verb positions: in the test set, 
there were only 690 observed cases of 
?undefined? verb position out of a total of 15,492 
data points. At runtime in Amalgam, verb position 
features are assigned based on the classification 
provided by the decision tree model. 
5.1 Features in the verb position model 
For each data point, 713 features were extracted. 
Of those features, 41 were selected by the decision 
tree algorithm. The selected features fall into the 
following categories: 
? syntactic label of the node and the parent 
? subcategorization features 
? semantic relations of the node to its parent 
and of the parent node to its parent 
? tense and mood features 
? presence of empty, uncontrolled subject 
? semantic features on the node and the 
parent 
5.2 The verb position model 
The decision tree model for verb position has 
115 branching nodes. Precision, recall and F-
measure for the model are given in Table 2. As a 
point of reference for the verb position classifier, 
assigning the most frequent value (second) of the 
target feature yields a baseline score of 0.4240. 
Table 2. Precision, recall, and F-measure for the verb 
position model. 
Value Precision Recall F-measure 
Initial 0.9650 0.9809 0.9729 
Second 0.9754 0.9740 0.9743 
Final 0.9420 0.9749 0.9581 
Undefined 0.5868 0.3869 0.4663 
Overall 
accuracy 
0.9491 
6 Extraposition 
In both German and English it is possible to 
extrapose clausal material to the right periphery of 
the sentence (extraposed clauses underlined in the 
examples below): 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave the 
country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumour has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Extraposition is not obligatory like other types 
of movement (such as Wh-movement). Both 
extraposed and non-extraposed versions of a 
sentence are acceptable, with varying degrees of 
fluency. 
The interesting difference between English and 
German is the frequency of this phenomenon. 
While it can easily be argued that English 
sentence realization may ignore extraposition and 
still result in very fluent output, the fluency of 
sentence realization for German will suffer much 
more from the lack of a good extraposition 
mechanism. We profiled data from various 
domains (Gamon et al 2002a) to substantiate this 
linguistic claim (see Uszkoreit et al 1998 for 
similar results). In the technical domain, more 
than one third of German relative clauses are 
extraposed, as compared to a meagre 0.22% of 
English relative clauses. In encyclopaedia text 
(Microsoft Encarta), approximately every fifth 
German relative clause is extraposed, compared to 
only 0.3% of English relative clauses. For 
complement clauses and infinitival clauses, the 
differences are not as striking, but still significant: 
in the technical and encyclopaedia domains, 
extraposition of infinitival and complement 
clauses in German ranges from 1.5% to 3.2%, 
whereas English only shows a range from 0% to 
0.53%. 
We chose to model extraposition as an iterative 
movement process from the original attachment 
site to the next higher node in the tree (for an 
alternative one-step solution and a comparison of 
the two approaches see (Gamon et al, 2002a)). 
The target feature of the model is the answer to 
the yes/no question ?Should the clause move from 
node X to the parent of node X??. 
6.1 Features in the extraposition model 
The tendency of a clause to be extraposed depends 
on properties of both the clause itself (e.g., some 
notion of ?heaviness?) and the current attachment 
site. Very coarse linguistic generalizations are that 
a relative clause tends to be extraposed if it is 
sufficiently ?heavy? and if it is followed by verbal 
material in the same clause. Feature extraction for 
this model reflects that fact by taking into 
consideration features on the extraposition 
candidate, the current attachment site, and 
potential next higher landing site. This results in a 
total of 1168 features. Each node in the parent 
chain of an extraposable clause, up to the actual 
attachment node, constitutes a single data point 
During the decision tree building process, 60 
features were selected as predictive. They can be 
classified as follows: 
General feature: 
? overall sentence length 
Features on the extraposable clause: 
? presence of verb-final and verb-second 
ancestor nodes 
? ?heaviness? both in number of characters 
and number of tokens 
? various linguistic features in the local 
context (parent node and grandparent 
node): number and person, definiteness, 
voice, mood, transitivity, presence of 
logical subject and object, presence of 
certain semantic attributes, coordination, 
prepositional relations 
? syntactic label 
? presence of modal verbs 
? prepositional relations 
? transitivity 
Features on the attachment site 
? presence of logical subject 
? status of the parent and grandparent as a 
separable prefix verb 
? voice and presence of modal verbs on the 
parent and grandparent 
? presence of arguments and transitivity 
features on the parent and grandparent 
? number, person and definiteness; the same 
on parent and grandparent 
? syntactic label; the same on the parent and 
grandparent 
? verb position; the same on the parent 
? prepositional relation on parent and 
grandparent 
? semantic relation that parent and 
grandparent have to their respective 
parent node 
6.2 The extraposition model 
During testing of the extraposition model, the 
model was consulted for each extraposable clause 
to find the highest node to which that clause could 
be extraposed. In other words, the target node for 
extraposition is the highest node in the parent 
chain for which the answer to the classification 
task ?Should the clause move from node X to the 
parent of node X?? is ?yes? with no interceding 
?no? answer. The prediction of the model was 
compared with the actual observed attachment site 
of the extraposable clause to yield the accuracy 
figures shown in Table 3. The model has 116 
branching nodes. The baseline for this task is 
calculated by applying the most frequent value for 
the target feature (?don't move?) to all nodes. The 
baseline for extraposition of infinitival and 
complement clauses is very high. The number of 
extraposed clauses of both types in the test set 
(fifteen extraposed infinitival clauses and twelve 
extraposed complement clauses) is very small, so 
it comes as no surprise that the model accuracy 
ranges around the baseline for these two types of 
extraposed clauses. 
Table 3. Accuracy of the extraposition model. 
Extraposable clause Accuracy Baseline 
RELCL 0.8387 0.6093 
INFCL 0.9202 0.9370 
COMPCL 0.9857 0.9429 
Overall 0.8612 0.6758 
7 Syntactic aggregation 
Any sentence realization component that 
generates from an abstract semantic representation 
and strives to produce fluent output beyond simple 
templates will have to deal with coordination and 
the problem of duplicated material in 
coordination. This is generally viewed as a sub-
area of aggregation in the generation literature 
(Wilkinson, 1995; Shaw, 1998; Reape and 
Mellish, 1999; Dalianis and Hovy, 1993). In 
Amalgam, the approach we take is strictly intra-
sentential, along the lines of what has been called 
conjunction reduction in the linguistic literature 
(McCawley, 1988). While this may seem a fairly 
straightforward task compared to inter-sentential, 
semantic and lexical aggregation, it should be 
noted that the cross-linguistic complexity of the 
phenomenon makes it much less trivial than a first 
glance at English would suggest. In German, for 
example, position of the verb in the coordinated 
VPs plays an important role in determining which 
duplicated constituent can be omitted. 
The target feature for the classification task is 
formulated as follows: ?In which coordinated 
constituent is the duplicated constituent to be 
realized??. There are three values for the target 
feature: ?first?, ?last?, and ?middle?. The third 
value (?middle?) is a default value for cases where 
neither the first, nor the last coordinated 
constituent can be identified as the location for the 
realization of duplicated constituents. At 
generation runtime, multiple realizations of a 
constituent in coordination are collected and the 
aggregation model is consulted to decide on the 
optimal position in which to realize that 
constituent. The constituent in that position is 
retained, while all other duplicates are removed 
from the tree. 
7.1 Features in the syntactic aggregation 
model 
A total of 714 features were extracted for the 
syntactic aggregation model. Each instance of 
coordination which exhibits duplicated material at 
the semantic level without corresponding 
duplication at the syntactic level constitutes a data 
point. 
Of these features, 15 were selected as 
predictive in the process of building the decision 
tree model: 
? syntactic label and syntactic label of the 
parent node 
? semantic relation to the parent of the 
duplicated node, its parent and grandparent 
? part of speech of the duplicated node 
? verb position across the coordinated node 
? position of the duplicated node in 
premodifiers or postmodifiers of the parent 
? coordination of the duplicated node and 
the grandparent of the duplicated node 
? status of parent and grandparent as a 
proposition 
? number feature on the parent 
? transitivity and presence of a direct object 
on the parent 
7.2 The syntactic aggregation model 
The syntactic aggregation model has 21 branching 
nodes. Precision, recall and F-measure for the 
model are given in Table 4. As was to be expected 
on the basis of linguistic intuition, the value 
?middle? for the target feature did not play any 
role. In the test set there were only 2 observed 
instances of that value. The baseline for this task 
is 0.8566 (assuming ?first? as the default value). 
Table 4. Precision, recall, and F-measure for the 
syntactic aggregation model. 
Value Precision Recall F-measure 
last 0.9191 0.9082 0.9136 
first 0.9837 0.9867 0.9851 
middle 0.0000 0.0000 0.0000 
overall 
accuracy 
0.9746 
8 Conclusion and future research 
We have demonstrated on the basis of four 
examples that it is possible to learn the contexts 
for complex linguistic operations in sentence 
realization with high accuracy. We proposed to 
standardize most of the feature extraction for the 
machine learning tasks to all available linguistic 
features on the node, and its parent and 
grandparent node. This generalized set of features 
allows us to rapidly train on new sets of data and 
to experiment with new machine learning tasks. 
Furthermore, it prevents us from focusing on a 
small set of hand-selected features for a given 
phenomenon; hence, it allows us to learn new (and 
unexpected) generalizations from new data. 
We have found decision trees to be useful for 
our classification problems, but other classifiers 
are certainly applicable. Decision trees provided 
an easily accessible inventory of the selected 
features and some indication of their relative 
importance in predicting the target features in 
question. Although our exposition has focused on 
the preferred value (the mode) predicted by the 
models, decision trees built by WinMine predict a 
probability distribution over all possible target 
values. For a system such as Amalgam, built as a 
pipeline of stages, this point is critical, since 
finding the best final hypothesis requires the 
consideration of multiple hypotheses and the 
concomitant combination of probabilities assigned 
by the various models in the pipeline to all 
possible target values. For example, our 
extraposition model presented above depends 
upon the value of the verb-position feature, which 
is predicted upstream in the pipeline. Currently, 
we greedily pursue the best hypothesis, which 
includes only the mode of the verb-position 
model?s prediction. However, work in progress 
involves a search that constructs multiple 
hypotheses incorporating each of the predictions 
of the verb-position model and their scores, and 
likewise for all other models. 
We have found the combination of knowledge-
engineered linguistic operations with machine-
learned contexts to be advantageous. The 
knowledge-engineered choice of linguistic 
operations, allows us to deal with complex 
linguistic phenomena. Machine learning, on the 
other hand, automates the discovery of general 
and domain-specific contexts. This facilitates 
adaptation of the system to a new domain or even 
to a new language. 
It should also be noted that none of the learned 
models can be easily replaced by a rule. While 
case assignment, for example, depends to a high 
degree on the lexical properties of the governing 
preposition or governing verb, other factors such 
as semantic relations, etc., play a significant role, 
so that any rule approaching the accuracy of the 
model would have to be quite complex.  
We are currently adapting Amalgam to the task 
of French sentence realization, as a test of the 
linguistic generality of the system. Initial results 
are encouraging. It appears that much of the 
feature extraction and many of the linguistic 
operations are reusable. 
Acknowledgements 
Our thanks go to Max Chickering for assistance 
with the WinMine decision tree tools and to Zhu 
Zhang who made significant contributions to the 
development of the extraposition models. 
References 
S. Bangalore and O. Rambow 2000. Exploiting a 
probabilistic hierarchical model for generation. 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING 2000). 
Saarbr?cken, Germany. 42-48. 
D. M. Chickering. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/Tool
doc.htm 
D. M. Chickering, D. Heckerman and C. Meek. 1997. 
A Bayesian approach to learning Bayesian networks 
with local structure. In ?Uncertainty in Artificial 
Intelligence: Proceedings of the Thirteenth 
Conference?, D. Geiger and P. Punadlik Shenoy, 
ed., Morgan Kaufman, San Francisco, California, 
pp. 80-89. 
S. Corston-Oliver, M. Gamon, E. Ringger, and R. 
Moore. 2002. An overview of Amalgam: A machine-
learned generation module. To be presented at 
INLG 2002. 
H. Dalianis and E. Hovy 1993 Aggregation in natural 
language generation. Proceedings of the 4th 
European Workshop on Natural Language 
Generation, Pisa, Italy. 
P. Eisenberg 1999. Grundriss der deutschen 
Grammatik. Band2: Der Satz. Metzler, 
Stuttgart/Weimar. 
U. Engel. 1996. Deutsche Grammatik. Groos, 
Heidelberg. 
M. Gamon, E. Ringger, Z. Zhang, R. Moore and S. 
Corston-Oliver. 2002a. Extraposition: A case study 
in German sentence realization. To be presented at 
the 19th International Conference on Computational 
Linguistics (COLING) 2002. 
M. Gamon, E. Ringger, S. Corston-Oliver. 2002b. 
Amalgam: A machine-learned generation module. 
Microsoft Research Technical Report, to appear. 
G. E. Heidorn. 2002. Intelligent Writing Assistance. In 
?A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text?, R. Dale, H. Moisl, and H. 
Somers (ed.), Marce Dekker, New York. 
I. Langkilde. and K. Knight. 1998a. The practical value 
of n-grams in generation. Proceedings of the 9th 
International Workshop on Natural Language 
Generation, Niagara-on-the-Lake, Canada. pp. 248-
255. 
I. Langkilde and K. Knight. 1998b. Generation that 
exploits corpus-based statistical knowledge. 
Proceedings of the 36th ACL and 17th COLING 
(COLING-ACL 1998). Montr?al, Qu?bec, Canada. 
704-710. 
J. D. McCawley. 1988 The Syntactic Phenomena of 
English. The University of Chicago Press, Chicago 
and London. 
M. Reape. and C. Mellish. 1999. Just what is 
aggregation anyway? Proceedings of the 7th 
European Workshop on Natural Language 
Generation, Toulouse, France. 
E. Ringger, R. Moore, M. Gamon, and S. Corston-
Oliver. In preparation. A Linguistically Informed 
Generative Language Model for Intra-Constituent 
Ordering during Sentence Realization. 
J. Shaw. 1998 Segregatory Coordination and Ellipsis in 
Text Generation. Proceedings of COLING-ACL, 
1998, pp 1220-1226. 
H. Uszkoreit, T. Brants, D. Duchier, B. Krenn, L. 
Konieczny, S. Oepen and W. Skut. 1998. Aspekte 
der Relativsatzextraposition im Deutschen. Claus-
Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
J. Wilkinson 1995 Aggregation in Natural Language 
Generation: Another Look. Co-op work term report, 
Department of Computer Science, University of 
Waterloo. 
Using decision trees to select 
the gran natical relation of a noun phrase 
Simon CORSTON-OLIVER 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052, USA 
simonco@microsoft.com 
Abstract  
We present a machine-learning approach to 
modeling the distribution of noun phrases 
(NPs) within clauses with respect o a fine- 
grained taxonomy of grammatical relations. We 
demonstrate that a cluster of superficial 
linguistic features can function as a proxy for 
more abstract discourse features that are not 
observable using state-of-the-art natural 
language processing. The models constructed 
for actual texts can be used to select among 
alternative linguistic expressions of the same 
propositional content when generating 
discourse. 
1. Introduction 
Natural language generation involves a number of 
processes ranging from planning the content o be 
expressed through making encoding decisions 
involving syntax, the lexicon and morphology. The 
present study concerns decisions made about he form 
and distribution of each "mention" of a discourse 
entity: should reference be made with a lexical NP, a 
pronominal NP or a zero anaphor (i.e. an elided 
mention)? Should a given mention be expressed as 
the subject of its clause or in some other grammatical 
relation? 
If all works well, a natural anguage generation 
system may end up proposing a mmaber of possible 
well-formed expressions of the same propositional 
content. Although these possible formulations would 
all be judged to be valid sentences of the target 
language, it is not the ease that they are all equally 
likely to occur. 
Research in the area of Preferred Argument 
Structure (Corston 1996, Du Bois 1987) has 
established that in discourse in many languages, 
including English, NPs are distributed across 
grammatical relations in statistically significant ways. 
For example, transitive clauses tend not to contain 
lexical NPs in both subject and object positions and 
subjects of transitives tend not to be lexical NPs nor 
to be discourse-new. 
Unfortunately, the models used in PAS have 
involved only simple chi-squared tests to identify 
statistically significant patterns in the distribution of 
NPs with respect o pairs of features (e.g. part of 
speech and grammatical relation). A further problem 
from the point of view of computational discourse 
analysis is that many of the features used in empirical 
studies are not observable in texts using state-of-the 
art natural anguage processing. Such non-observable 
features include animacy, the information status of a 
referent, and the identification of the gender of a 
referent based on world knowledge. 
In the present study, we treat the task of 
determining the appropriate distribution of mentions 
in text as a machine learning classification problem: 
what is the probability that a mention will have a 
certain grammatical relation given a deh set of 
linguistic features? In particular, how accurately can 
we select appropriate grammatical relations using 
only superficial linguistic features? 
2. Data 
A total of 5,252 mentions were annotated from the 
Encarta electronic encyclopedia and 4,937 mentions 
from the Wall Street Journal (WSJ). Sentences were 
parsed using the Microsoft English Grammar 
(Heidorn 1999) to extract mentions and linguistic 
features. These analyses were then hand-corrected to 
eliminate noise in the training data caused by 
inaccurate parses, allowing us to determine the upper 
bound on accuracy for the classification task if the 
computational nalysis were perfect. Zero anaphors 
were annotated only when they occurred as subjects 
of coordinated clauses. They have been excluded 
66 
from the present study since they are invariably 
discourse-given subjects. 
3. Features 
Nineteen linguistic features were annotated, along 
with information about the referent of each mention. 
On the basis of the reference information we 
extracted the feature \[InformationStatus\], 
distinguishing "discourse-new" versus "discourse- 
old". All mentions without a prior coreferential 
mention in the text were classified as discourse-new, 
even if  they would not traditionally be considered 
referential. \[InformationStatus\] is not directly 
observable since it requires the analyst to make 
decisions about he referent of a mention. 
In addition to the feature \[InformafionStatus\], the 
following eighteen observable features were 
annotated. These are all features that we can 
reasonably expect syntactic parsers to extract with 
sufficient accuracy today or in the near future. 
? \[ClausalStatus\]: Does the mention occur in a 
main clause ("M"), complement clause ("C"), 
or subordinate clause ("S")? 
? \[Coordinated\] The mention is coordinated 
with at least one sibling. 
? \[Definite\] The mention is marked with the 
definite article or a demonstrative pronoun. 
\[Fem\] The mention is unambiguously 
feminine. 
? \[GrRel\] The grammatical relation of the 
mention (see below, this section). 
? \[HasPossessive\] Modified by a possessive 
pronoun or a possessive NP with the elit ic's 
ors'. 
? \[HasPP\] Contains a postmodifying pre- 
positional phrase. 
? \[HasRelC1\] Contains a postmodifying relative 
clause. 
? \[InQuotes\] The mention occurs in quoted 
material. 
? \[Lex\] The specific inflected form of a 
pronoun, e.g. he, him. 
? \[Mase\] The mention is unambiguously 
masculine. 
? \[NounClass\] We distinguish common nouns 
versus proper names. Within proper names, 
we distinguish the name of a place ("Geo") 
versus other proper names ("ProperName"). 
? \[Plural\] The head of the mention is 
morphologically marked as plural. 
? \[POS\] The part of speech of the head of the 
mention. 
? \[Prep\] The governing preposition, if any. 
? \[RelC1\] The mention is a child of a relative 
clause. 
? \[TopLevel\] The mention is not embedded 
within another mention. 
? \[Words\] The total number of words in the 
mention, discretized to the following values: 
{0, 1, 2, 3, 4, 5, 6to10, 1 lto15, abovel5}. 
Gender (\[Fern\], \[Mast\]) was only annotated for 
common ouns whose default word sense is gendered 
(e.g. "mother", "father"), for common nouns with 
specific morphology (e.g. with the -ess suffix) and 
for gender-marked proper names (e.g. "John", 
"Mary"). Gender was not marked for pronouns, to 
avoid difficult encoding decisions uch as the use of 
genetic "he". ~ Gender was also not marked for cases 
that would require world knowledge. 
The feature \[GrRel\] was given a much finer- 
grained analysis than is usual in computational 
linguistics. Studies in PAS have demonstrated the 
need to distinguish finer-grained categories than the 
traditional grammatical relations of English grammar 
("subject", "object" ere) in order to account for 
distributional phenomena in discourse. For example, 
subjects of intransitive verbs pattern with the direct 
objects of transitive verbs as being the preferred locus 
for introducing new mentions. Subjects of transitives, 
however, are strongly dispreferred slots for the 
expression of new information. The use of fine- 
grained grammatical relations enables us to make 
rather specific claims about the distribution of 
mentions. The taxonomy of fine-grained grammatical 
relations is given below in Figure 1. 
1 The feature \[Lex\] was sufficient for the decision tree tools 
to learn idiosyncratic uses of gendered pronouns. 
67 
Subject of 
transitive (S.r) 
Subject 
Subject of 
intransitive 
Object of transitive 
Subject of copula 
(Sc) 
Subject of 
intransitive 
(non-copula) (Si) 
Grammatical 
Relation 
/ / / L  (PN) 
Po so.Po    PCP e I 
, / / ' t ,  NPCPP. I 
~! adjective(PP,) } 
., .. I ~ PP complement ofJ ' 
N?un (NA~=ve I \[ verb (PPv) I 
Other (Oth) J 
Figure 1 The taxonomy of grammatical relations 
4. Decision trees 
For a set of annotated examples, we used decision- 
tree tools to construct the conditional probability of a 
specific grammatical relation, given other features in 
the domain, zThe decision trees are constructed using 
a Bayesian learning approach that identifies tree 
structures with high posterior probability (Chickefing 
et al 1997). In particular, a candidate tree structure 
(S) is evaluated against data (D) using Bayes' rule as 
follows: 
P(SID) = constant- p(DIS) ? p(S) 
For simplicity, we specify a prior distribution 
over tree structures using a single parameter kappa 
(k). Assuming that N(S) probabilities are needed to 
parameterize a tree with structure S, we use: 
p(S) = c .  k 
2 Comparison experiments were also done with Support 
Vector Machines (Platt 2000, Vapnik 1998) using a 
where 0 < k _< 1, and c is a constant such that p(S) 
sums to one. Note that smaller values of kappa cause 
simpler structures to be favored. As kappa grows 
closer to one (k = 1 corresponds to a uniform prior 
over all possible tree structures), the learned ecision 
trees become more elaborate. Decision trees were 
built for k~ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 
0.95, 0.99, 0.999}. 
Having selected a decision tree, we use the 
posterior means of the parameters to specify a 
probability distribution over the grammatical 
relations. To avoid overfitting, nodes containing 
fewer than fifty examples were not split during the 
learning process. In building decision trees, 70% of 
the data was used for training and 30% for held-out 
evaluation. 
The decision trees constructed can be rather complex, 
making them difficult o present visually. Figure 2 
gives a simpler decision tree that predicts the 
grammatical relation of  a mention for Enearta t 
variety of kernel functions. The results obtained were 
indistinguishable from those reported here. 
68 
,=:..,?IY" 
~)=o.~ I ,- "-. I t~,te, n}=o.o~ . _ . . . . .2  ?~'---. It~=:,=o.o~ 
p(sc)=O.09 I I p(na)=0.O~ \] I p(nay=u..26 J p(ppv)=O.~ I P(~) =0-07 1 y ~ -~,  
p(sc}=0.1~l P~PPW~.-3 
p(st)=o,os ~ .;~..~ Zo I~(st)=o.o3 I I p(ppa)=o.o5 ~(poss)=o.: 
p(srn)=O.Ol 
p(so)=o.o2 
Figure 2 Decision tree for Enearta, at k=0.7 
k=0.7. The tree was constructed using a subset of the 
morphological nd syntactic features: \[Coordinated\], 
\[HasPP\], \[Lex\], \[NounClass\], \[Plural\], \[POS\], \[Prep\], 
\[RelC1\], \[TopLevel\], [Words\]. Grammatical relations 
with only a residual probability are omitted for the 
sake of clarity. The top-ranked grammatical relation 
at each leaf node appears in bold type. Selecting the 
top-ranked grammatical relation at each node results 
in a correct decision 58.82% of the time in the held- 
out test data. By way of comparison, the best decision 
tree for Enema computed using all morphological 
and syntactic features yields 66.05% accuracy at k = 
0.999. 
The distributional facts about the pronoun "he" 
represented in Figure 2 illustrate the utility of the 
\[me-grained taxonomy of grammatical relations. The 
pronoun "he" in embedded NPs (\[Prep\] = "-", 
\[TopLevel\] = No) and when not in a relative clause 
(\[RelC1\] = No) favors ST and SI. Other grammatical 
relations have only residual probabilities. The use of 
the traditional notion of subject would fail to capture 
the fact that, in this syntactic ontext, the pronoun 
"he" tends not to occur as Sc, the subject of a copula. 
5. Evaluating decision trees 
Decision trees were constructed and evaluated for 
each corpus. We were particularly interested in the 
accuracy of models built using only observable 
features. If accurate modeling were to require more 
abstract discourse features such as 
\[InformationStatus\], a feature that is not directly 
observable, then a machine-learning approach to 
modeling the distribution of mentions would not be 
computationally feasible. Also of interest was the 
generality of the models. 
5.1 Using Observable Features Only 
Decision trees were built for Encarta and the Wall 
Street Journal using all features except the non- 
observable discourse feature \[InformationStatus\]. The 
best accuracy when evaluated against held-out test 
data and selecting the top-ranked grammatical 
relation at each leaf node was 66.05% for Encarta at 
k=0.999 and 65.18% for Wall Street Journal at 
k=0.99. Previous studies in Preferred Argument 
Structure (Corston 1996, Du Bois 1987) have 
69 
Table 1 Accuracy using only morphological nd syntactic features 
Grammatical relations in Accuracy in held-out est data 
training data (decision tree accuracy in parentheses) 
Corpus Top-ranked i Top two Using top-ranked Using top two 
i 
Encarta PPN PPN, PPv 20.88% (66.05%) 41.37% (81.92%) 
WSJ Or Or, PPN 19.91% (66.16%) 35.56% (80.70%) 
established pairings of free-grained grammatical 
relations with respect o abstract discourse factors. 
New mentions in discourse, for example, tend to be 
introduced as the subjects of intransitive verbs or as 
direct objects, and are extremely unlikely to occur as 
the subj~ts of transitive verbs. Some languages even 
give the same morphological nd syntactic treatment 
to subjects of intransitives and direct objects, marking 
them (so called "absolutive" case marking) in 
opposition to subjects of transitives (so called 
"ergative" marking). Human referents, on the other 
hand, tend to occur as the subjects of transitive verbs 
and as the subjects of intransitive verbs, rather than as 
objects. Such discourse tendencies perhaps motivate 
the Use of one set of pronouns (the so called 
"nominative" pronouns {"he", "she", "we", "r', 
"they"}) in a language like English :for subjects and a 
different set of pronouns for objects (the so called 
"accusative" set {"him", "her", "us", "me", "them"}). 
Thus, we can see that distributional facts about 
mentions in discourse sometimes cross-cut the 
morphological nd syntactic encoding strategies of a 
language. With a free-grained set of grammatical 
relations, we can allow the decision trees to discover 
such groupings of relations, rather than attempting to 
specify the groupings in advance. 
We evaluated the accuracy of the decision trees 
by counting as a correct decision a grammatical 
relation that matched the top-ranked grammatical 
relation for a leaf node or the second ranked 
gamrnatieal relation for that leaf node. With this 
evaluation criterion, the accuracy for Enearta is 
81.92% at k=0.999 and for Wall Street Journal, 
80.70% at k=0.9. 
It is clearly naive to assume a baseline for 
comparison in which all grammatical relations have 
an equal probability of occurrence, i.e. 1/12 or 0.083. 
Rather, in Table 1 we compare the accuracy to that 
obtained by predicting the most frequent grammatical 
relations observed in the training data. The decision 
trees perform substantially above this baseline. The 
top two grammatical relations in the two corpora do 
not form a natural class. In the Wall Street Journal 
texts, for example, the top two grammatical relations 
are Or (object of transitive verb) and PPN 
(prepositional phrase complement of a NP). It is 
difficult to see how mentions in these two 
grammatical relations might be related. Objects of 
transitive verbs, for example, are typically entities 
affected by the action of the verb. Prepositional 
phrase complements of NPs, however, are 
prototypically used to express attributes of the NP, 
e.g. "the man with the red hat". The grammatical 
relations paired by taking the top two predictions at 
each leaf node in the decision trees constructed for 
the Wall Street Journal and Encarta, however, 
frequently correspond to classes that have been 
previously observed in the literature on Preferred 
Argument Structure. The groupings {Or, Si}, {Or, 
Sc} and {Si, ST}, for example, occur on multiple leaf 
nodes in the decision trees for both corpora. 
5.2 Using All Features 
Decision trees were built for Encarta and the Wall 
Street Journal using all features including the 
discourse feature \[InformationStatus\]. As it turned 
out, the feature \[InformationStatus\] wasnot selected 
during the automatic onstruction of the decision tree 
for the Wall Street Journal. The performance of the  
70 
decision trees on held-out est data from the Wall 
Street Journal therefore remained the same as that 
given in section 5.1. For Encarta, the addition of 
\[InformationStatus\] yielded only a modest 
improvement in accuracy. Selecting the top-ranked 
grammatical relation rose from 66.05% at k=0.999 to 
67.32% at k = 0.999. Applying a paired t-test, this is 
statistically significant at the 0.01 level. Selecting the 
top two grammatical relations caused accuracy to rise 
from 81.92% at k=0.999 to 82.23% at k=0.999, not a 
statistically significant improvement. 
The fact that the discourse feature 
\[InformationStatus\] does not make a marked impact 
on accuracy is not surprising. The information status 
of an NP is an important factor in determining 
elements of form, such as the decision to use a 
pronoun versus a lexical NP, or the degree of 
elaboration (e.g. by means of adjectives, post- 
modifying PPs and relative clauses). Those elements 
of form can be viewed as proxies for the feature 
\[informationStatus\]. Pronouns and definite NPs, for 
example, typically refer to given entities, and 
therefore are compatible with the grammatical 
relation ST. Similarly, long indefinite lexical NPs are 
likely to be new mentions. 
In a separate set of experiments conducted on the 
same data, we built decision trees to predict the 
information status of the referent of a noun phrase 
using the other linguistic features (grammatical 
relation, clausal status, definiteness and so on.) Zero 
anaphors were excluded, yielding 4,996 noun phrases 
for Encarta and 4,758 noun phrases for the Wall 
Street Journal. The accuracy of the decision trees was 
80.45% for Encarta and 78.36% for the Wall Street 
Journal. To exclude the strong associations between 
personal pronouns and information status, we also 
built decision trees for only the lexical noun phrases 
in the two corpora, a total of 4,542 noun phrases for 
Enema and 4,153 noun phrases for the Wall Street 
Journal. The accuracy of the decision trees was 
78.14% for Encarta and 77.45% for the Wall Street 
Journal. The feature \[informationStatus\] can thus be 
seen to be highly inferrable given the other features 
used. 
5.3 Domain-specificity of the Decision Trees 
The decision trees built for the Encarta and Wall 
Street Journal corpora differ considerably, as is to be 
expected for such distinct genres. To measure the 
specificity of the decision trees, we built models 
using all the data for one corpus and evaluated on all 
the data in the other corpus, using all features except 
\[informationStatus\]. Table 2 gives the baseline 
figures for this cross-domain evaluation, selecting the 
most frequent grammatical relations in the training 
data. The peak accuracy from the decision trees is 
given in parentheses for comparison. The decision 
trees perform well above the baseline. 
Table 3 provides a comparison of the accuracy of 
decision trees applied across domains compared to 
those constructed and evaluated within a given 
domain. The extremely specialized sublanguage of 
Encarta does not generalize well to the Wall Street 
Journal. In particular, when selecting the top-ranked 
grammatical relation, the most severe evaluation of 
the accuracy of the decision trees, training on Encarta 
and evaluating on the Wall Street Journal results in a 
drop in accuracy of 7.54% compared to the Wall 
Street Journal within-corpus model. By way of 
contrast, decision trees built from the Wall Street 
Journal data do generalize well to Enearta, even 
yielding a modest 0.41% improvement in accuracy 
over the model built for Encarta. Since the Encarta 
data contains more mentions (5,252 mentions) than 
the Wall Street Journal data (4,937 mentions), this 
effect is not simply due to differences in the size of 
the training set. 
71 
Table 2 Cross-domain evaluation of the decision trees 
Train- 
Test 
WSJ- 
Encarta 
Encarta- 
Grammatical 
training data 
Top-ranked 
Or 
relations in 
Top two 
OT, PPN 
Accuracy in held-out est data 
(decision tree accuracy in parentheses) 
Using top-ranked 
15.90% (66.32%) 
Using top two 
36.58% (79.51%) 
PPN PPN, PPv 15.98% (61.17%) 31.90% (77.64%) 
WSJ 
Table 3 Comparison of cross-domain accuracy to 
. . . . .  within-domain accuracy 
Top-ranked 
Train on Encarta, evaluate on WSJ 61.17% 
Train on WSJ, evaluate on WSJ 66.16% 
Relative difference in accuracy -7.54% 
Train on WSJ, evaluate on Encarta 
Train on Encarta, evaluate on Enema 
Relative difference in accuracy 
66.32% 
66.05% 
+0.41% 
Top two 
Train on Encarta, evaluate on WSJ 
Train on WSJ, evaluate on WSJ 
Relative difference in accuracy 
77.64% 
80.70% 
-3.74% 
Train on WSJ, evaluate on Encarta 
Train on Enema, evaluate on Encarta 
Relative difference in accuracy 
79.51% 
81.92% 
-2.94% 
5.4 Combining the Data 
Combining the Wall Street Journal and Encarta data 
into one dataset and using 70% of the data for 
training and 30% for testing yielded mixed results. 
Selecting the top-ranked grammatical relation for the 
combined ata yielded 66.01% at lc~0.99, compared 
to the Encarta-specific accuracy of 66.05% and the 
Wall Street Journal-specific peak accuracy of 
66.16%. Selecting the top two grammatical relations, 
the peak accuracy for the combined ata was 81.39% 
at k=0.99, a result approximately midway between the 
corpus-specific results obtained in section 5.1, 
namely 81.92% for Encarta and 80.70% for Wall 
Street Journal. 
The Wall Street Journal corpus contains a diverse 
range of articles, including op-ed pieces, mundane 
financial reporting, and world news. The addition of 
the relatively homogeneous Encarta articles appears 
to result in models that are even more robust than 
those constructed solely on the basis of the Wall 
Street Journal data. The addition of the heterogeneous 
Wall Street Journal articles, however, dilutes the 
focus of the model constructed for Encarta. This 
perhaps explains the fact that the peak accuracy of the 
combined model lies above that for the Wall Street 
Journal but below that of Encarta. 
6. Conclusion 
Natural language generation is typically done under 
one of two scenarios. In the first scenario, language is 
generated eex nihilo: a planning component formulates 
propositions on the basis of a database query, a 
system event, or some other non-linguistic stimulus. 
Under such a scenario, the discourse status of 
referents is known, since the planning component has 
selected the discourse ntities to be expressed. More 
abstract discourse features like \[informationStatus\] 
can therefore be used to guide the linguistic encoding 
decisions. 
In the second, more typical scenario, natural 
language generation involves reformulating existing 
text, e.g. for summarization ormachine translation. In 
this scenario, analysis of the linguistic stimulus will 
most likely have resulted in only a partial 
understanding of the source text. Coreferenee 
relations (e.g. between a pronoun and its antecedent) 
72 
may not be fully resolved, discourse relations may be 
unspecified, and the information status of mentions i
unlikely to have been determined. As was shown in 
section 5.2, the accuracy of the decision trees 
constructed without he feature \[InformationStatus\] is 
comparable to the accuracy that results from using 
this feature, since superficial elements of the 
linguistic form of a mention are motivated by the 
information status of the mention. 
The decision trees that were constructed tomodel 
the distribution of NPs in real texts can be used to 
guide the generation of natural language, specially to 
guide the selection among alternative grammatical 
ways of expressing the same propositional content. 
Sentences in which mentions occur in positions that 
are unlikely given a set of linguistic features hould 
be avoided. 
One interesting problem remains for future 
research: why do writers occasionally place mentions 
in statistically unlikely positions? One possibility is 
that writers do so for stylistic variation. Another 
intriguing possibility is that statistically unusual 
occurrences reflect pragmatic markedness, i.e. that 
writers place NPs in certain positions in order to 
signal discourse information. Fox (1987), for 
example, demonstrates that lexical NPs may be used 
for previously mentioned iscourse ntities where a 
pronoun might be expected instead if there is an 
episode boundary in the discourse. For example, a 
prot~igonist in a novel may be reintroduced by name 
at the beginning of a chapter. In future research we 
propose to examine the mentions that occur in places 
not predicted by the models. It may be that this 
approach to modeling the distribution of mentions, 
essentially a machine-learning approach that seeks to 
mine an abstract property of texts, will provide useful 
insights into issues of discourse structure. 
References 
Chickering, D. M., D. Heckerman, and C. Meek, 1997, "A 
Bayesian approach to learning Bayesian etworks with 
local structure," In Geiger, D. and P. Punadlik Shenoy 
(eds.), Uncertainty inArtificial Intelligence: Proceedings 
of the Thirteenth Conference, 80-89. 
Corston, S. H., 1996, Ergativity in Roviana, Solomon 
Islands, Pacific Linguistics, Series B-113, Australia 
National University Press: Canberra. 
Du Bois, J. W., 1987, "The discourse basis of ergativity," 
Language 63:805-855. 
Fox, B.A., 1987, Discourse structure and anaphora, 
Cambridge Studies in Linguistics 48, Cambridge 
University Press, Cambridge. 
Heidorn, G., 1999, "Intelligent writing assistance," To 
appear in Dale, R., H. Moisl and H. Somers (eds.), A 
Handbook of Natural Language Processing Teclmiques, 
Marcel Dekker. 
Platt, J., N. Cfistianini, J. Shawe-Taylor, 2000, "Large 
margin DAGs for multiclass classification," In Advances 
in Neural Information Processing Systems 12, MIT Press. 
Vapnik, V., 1998, Statistical Learning Theory, Wiley- 
Interscience, New York. 
73 
        Task-focused Summarization of Email 
Simon Corston-Oliver, Eric Ringger, Michael Gamon and Richard Campbell 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{simonco, ringger, mgamon, richcamp}@microsoft.com 
 
 
 
 
Abstract 
 We describe SmartMail, a prototype system for 
automatically identifying action items (tasks) in 
email messages. SmartMail presents the user with 
a task-focused summary of a message. The 
summary consists of a list of action items extracted 
from the message. The user can add these action 
items to their ?to do? list. 
1 Introduction 
Email for many users has evolved from a mere 
communication system to a means of organizing 
workflow, storing information and tracking tasks 
(i.e. ?to do? items) (Bellotti et al, 2003; Cadiz et 
al., 2001). Tools available in email clients for 
managing this information are often cumbersome 
or even so difficult to discover that users are not 
aware that the functionality exists. For example, in 
one email client, Microsoft Outlook, a user must 
switch views and fill in a form in order to create a 
task corresponding to the current email message. 
By automatically identifying tasks that occur in the 
body of an email message, we hope to simplify the 
use of email as a tool for task creation and 
management. 
In this paper we describe SmartMail, a prototype 
system that automatically identifies tasks in email, 
reformulates them, and presents them to the user in 
a convenient interface to facilitate adding them to a 
?to do? list.  
SmartMail performs a superficial analysis of an 
email message to distinguish the header, message 
body (containing the new message content), and 
forwarded sections. 1  SmartMail breaks the 
                                                                 
1  This simple division into header, message body, and 
forwarded sections was sufficient for the corpus of email 
messages we considered. Messages containing original 
messages interleaved with new content were extremely 
message body into sentences, then determines 
the speech act of each sentence in the message 
body by consulting a machine-learned classifier. 
If the sentence is classified as a task, SmartMail 
performs additional linguistic processing to 
reformulate the sentence as a task description. 
This task description is then presented to the 
user. 
2 Data 
We collected a corpus of 15,741 email 
messages. The messages were divided into 
training, development test and blind test. The 
training set contained 106,700 sentences in 
message bodies from 14,535 messages. To 
avoid overtraining to individual writing styles, 
we limited the number of messages from a 
given sender to 50. To ensure that our 
evaluations are indicative of performance on 
messages from previously unencountered 
senders, we selected messages from 3,098 
senders, assigning all messages from a given 
sender to either the training or the test sets. 
Three human annotators labeled the message 
body sentences, selecting one tag from the 
following set: Salutation, Chit-chat (i.e., social 
discussion unrelated to the main purpose of the 
message), Task, Meeting (i.e., a proposal to 
meet), Promise, Farewell, various components 
of an email signature (Sig_Name, Sig_Title, 
Sig_Affiliation, Sig_Location, Sig_Phone, 
Sig_Email, Sig_URL, Sig_Other), and the 
default category ?None of the above?. The set of 
tags can be considered a set of application-
specific speech acts analogous to the rather 
particular tags used in the Verbmobil project, 
such as ?Suggest_exclude_date? and 
                                                                                                
uncommon in our corpus. Most senders were using 
Microsoft Outlook, which places the insertion point for 
new content at the top of the message. 
?Motivate_appointment? (Warnke et al, 1997; 
Mast et al, 1996) or the form-based tags of Stolcke 
et al (1998). 
All three annotators independently labeled 
sentences in a separate set of 146 messages not 
included in the training, development or blind test 
sets. We measured inter-annotator agreement for 
the assignment of tags to sentences in the message 
bodies using Cohen?s Kappa. Annotator 1 and 
annotator 2 measured 85.8%; annotator 1 and 
annotator 3 measured 82.6%; annotator 2 and 
annotator 3 measured 82.3%. We consider this 
level of inter-annotator agreement good for a novel 
set of application-specific tags. 
The development test and blind test sets of 
messages were tagged by all three annotators, and 
the majority tag for each sentence was taken. If any 
sentence did not have a majority tag, the entire 
message was discarded, leaving a total of 507 
messages in the development test set and 699 
messages in the blind test set. 
The set of tags was intended for a series of 
related experiments concerning linguistic 
processing of email. For example, greetings and 
chit-chat could be omitted from messages 
displayed on cell phones, or the components of an 
email signature could be extracted and stored in a 
contact database. In the current paper we focus 
exclusively on the identification of tasks. 
Annotators were instructed to mark a sentence 
as containing a task if it looked like an appropriate 
item to add to an on-going ?to do? list. By this 
criterion, simple factual questions would not 
usually be annotated as tasks; merely responding 
with an answer fulfills any obligation. Annotators 
were instructed to consider the context of an entire 
message when deciding whether formulaic endings 
to email such as Let me know if you have any 
questions were to be interpreted as mere social 
convention or as actual requests for review and 
comment. The following are examples of actual 
sentences annotated as tasks in our data: 
Since Max uses a pseudo-
random number generator, you 
could possibly generate the 
same sequence of numbers to 
select the same cases. 
 
Sorry, yes, you would have to 
retrain. 
 
An even fast [sic] thing 
would be to assign your own 
ID as a categorical feature. 
 
Michael, it?d be great if 
you could add some stuff re 
MSRDPS. 
 
Could you please remote 
desktop in and try running 
it on my machine. 
 
If CDDG has its own notion 
of what makes for good 
responses, then we should 
use that. 
 
3 Features 
Each sentence in the message body is described 
by a vector of approximately 53,000 features. 
The features are of three types: properties of the 
message (such as the number of addressees, the 
total size of the message, and the number of 
forwarded sections in the email thread), 
superficial features and linguistic features. 
The superficial features include word 
unigrams, bigrams and trigrams as well as 
counts of special punctuation symbols (e.g. @, 
/, #), whether the sentence contains words with 
so-called ?camel caps? (e.g., SmartMail), 
whether the sentence appears to contain the 
sender?s name or initials, and whether the 
sentence contains one of the addressees? names. 
The linguistic features were obtained by 
analyzing the given sentence using the NLPWin 
system (Heidorn 2000). The linguistic features 
include abstract lexical features, such as part-of-
speech bigrams and trigrams, and structural 
features that characterize the constituent 
structure in the form of context-free phrase 
structure rewrites (e.g., DECL:NP-VERB-NP; 
i.e., a declarative sentence consisting of a noun 
phrase followed by a verb and another noun 
phrase). Deeper linguistic analysis yielded 
features that describe part-of-speech 
information coupled with grammatical relations 
(e.g., Verb-Subject-Noun indicating a nominal 
subject of a verb) and features of the logical 
form analysis such as transitivity, tense and 
mood. 
 
4 Results 
We trained support vector machines (SVMs) 
(Vapnik, 1995) using an implementation of the 
sequential minimal optimization algorithm 
(Platt, 1999). We trained linear SVMs, which 
have proven effective in text categorization with 
large feature vectors (Joachims, 1998; Dumais et 
al., 1998).  
Figure 1 illustrates the precision-recall curve for 
the SVM classifier trained to distinguish tasks vs. 
non-tasks measured on the blind test set. 
We conducted feature ablation experiments on 
the development test set to assess the contribution 
of categories of features to overall classification 
performance. In particular we were interested in 
the role of linguistic analysis features compared to 
using only surface features. Within the linguistic 
features, we distinguished deep linguistic features 
(phrase structure features and semantic features) 
from POS n-gram features. We conducted 
experiments with three feature sets: 
1. all features (message level features + word 
unigram, bigram and trigram  
2. features + POS bigram and trigram 
features + linguistic analysis features) 
3. no deep linguistic features (no phrase 
structure or semantic features) 
4. no linguistic features at all (no deep 
linguistic features and no POS n-gram 
features) 
Based on these experiments on the development 
test set, we chose the feature set used for our run-
time applications.  
 
Figure 1 shows final results for these feature 
sets on the blind test set: for recall between 
approximately 0.2 and 0.4 and between 
approximately 0.5 and 0.6 the use of all features 
produces the best results. The distinction 
between the ?no linguistic features? and ?no 
deep linguistic features? scenarios is negligible; 
word n-grams appear to be highly predictive. 
Based on these results, we expect that for 
languages where we do not have an NLPWin 
parser, we can safely exclude the deeper 
linguistic features and still expect good 
classifier performance. 
 
 
Figure 2 illustrates the accuracy of 
distinguishing messages that contain tasks from 
those that do not, using all features. A message 
was marked as containing a task if it contained 
at least one sentence classified as a task. Since 
only one task has to be found in order for the 
entire message to be classified as containing a 
task, accuracy is substantially higher than on a 
per-sentence basis. In section 6, we discuss the 
scenarios motivating the distinction between 
sentence classification and message 
classification. 
 
 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
Pr
ec
is
io
n
All features
No deep linguistic features
No linguistic features
 
 
Figure 1: Precision-Recall curves for ablation experiments 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
P
re
ci
si
on
Per sentence
Per message
 
 
Figure 2: Precision-Recall curves comparing message classification and sentence classification 
 
 
5 Reformulation of Tasks 
SmartMail performs post-processing of sentences 
identified as containing a task to reformulate them 
as task-like imperatives. The process of 
reformulation involves four distinct knowledge-
engineered steps:  
1. Produce a logical form (LF) for the 
extracted sentence (Campbell and Suzuki, 
2001). The nodes of the LF correspond to 
syntactic constituents. Edges in the LF 
represent semantic and deep syntactic 
relations among nodes. Nodes bear 
semantic features such as tense, number 
and mood. 
2. Identify the clause in the logical form that 
contains the task; this may be the entire 
sentence or a subpart. We consider such 
linguistic properties as whether the clause 
is imperative, whether its subject is second 
person, and whether modality words such 
as please or a modal verb are used. All 
parts of the logical form not subsumed by 
the task clause are pruned. 
3. Transform the task portion of the LF to 
exclude extraneous words (e.g. please, 
must, could), extraneous subordinate 
clauses, adverbial modifiers, and vocative 
phrases. We replace certain deictic 
elements (i.e., words or phrases whose 
denotation varies according to the writer or 
the time and place of utterance) with non-
deictic expressions. For example, first 
person pronouns are replaced by either the 
name of the sender of the email or by a 
third person pronoun, if such a pronoun 
would unambiguously refer to the sender. 
Similarly, a temporal expression such as 
Thursday, which may refer to a different 
date depending on the week in which it is 
written, is replaced by an absolute date 
(e.g., 4/1/2004). 
4. Pass the transformed LF to a sentence 
realization module to yield a string 
(Aikawa et al, 2001). 
Below we illustrate the reformulation of tasks with 
some examples from our corpus. 
 
Example 1: 
On the H-1 visa issue, I am 
positive that you need to go 
to the Embassy in London to 
get your visa stamped into 
your passport. 
Reformulation: 
Go to the Embassy in London to 
get your visa stamped into 
your passport. 
 
In this example, the embedded sentential 
complement, that is, the part of the sentence 
following positive, is selected as the part of the 
sentence containing the task, because of the modal 
verb need and the second person subject; only that 
part of the sentence gets reformulated. The modal 
verb and the second person subject are deleted to 
form an imperative sentence. 
 
Example 2: 
Can you please send me the 
follow up information for the 
demo(s) listed in this Email 
ASAP. 
Reformulation: 
Send Kendall the follow up 
information for the demo 
listed in this Email ASAP. 
 
In this example, the whole sentence is selected 
as containing the task (modal verb, second person 
subject); modal elements including please are 
deleted along with the second person subject to 
form an imperative. In addition, the first person 
pronoun me is replaced by a reference to the 
sender, Kendall in this instance. 
 
Example 3: 
I've been Wednesday at the 
lecture on Amalgam you gave in 
the 113/1021 Room (which I 
really liked), and I've been 
wondering how feasible would 
it be to use Amalgam for 
learning requirements or code 
corpus structures and rules 
(and eventually rephrase them 
in some way). 
Reformulation: 
On June 5, 2002 Pablo wrote: 
?I've been Wednesday at the 
lecture on Amalgam you gave in 
the 113/1021 Room (which I 
really liked), and I've been 
wondering how feasible would 
it be to use Amalgam for 
learning requirements or code 
corpus structures and rules 
(and eventually rephrase them 
in some way).' 
 
This example illustrates what happens when 
NLPWin is unable to produce a spanning parse and 
hence a coherent LF; in this case NLPWin 
misanalyzed the clause following wondering as a 
main clause, instead of correctly analyzing it as a 
complement clause. SmartMail?s back-off strategy 
for non-spanning parses is to enclose the entire 
original sentence in quotes, prefixed with a matrix 
sentence indicating the date and the name of the 
sender. 
 
 
6 Task-Focused Summarization 
We have considered several scenarios for 
presenting the tasks that SmartMail identifies. 
Under the most radical scenario, SmartMail would 
automatically add extracted tasks to the user?s ?to 
do? list. This scenario has received a fairly 
negative reception when we have suggested it to 
potential users of a prototype. From an application 
perspective, this scenario is ?fail hard?; i.e., 
classification errors might result in garbage being 
added to the ?to do? list, with the result that the 
user would have to manually remove items. Since 
our goal is to reduce the workload on the user, this 
outcome would seem to violate the maxim ?First, 
do no harm?. 
 
Figure 3 and Figure 4 illustrate several ideas for 
presenting tasks to the user of Microsoft Outlook. 
Messages that contain tasks are flagged, using the 
existing flag icons in Outlook for proof of concept. 
Users can sort mail to see all messages containing 
tasks. This visualization amounts to summarizing 
the message down to one bit, i.e., +/- Task, and is 
conceptually equivalent to performing document 
classification. 
The right-hand pane in Figure 3 is magnified as 
Figure 4 and shows two more visualizations. At the 
top of the pane, the tasks that have been identified 
are presented in one place, with a check box beside 
them. Checking the box adds the task to the Tasks 
or ?to do? list, with a link back to the original 
message. This presentation is ?fail soft?: the user 
can ignore incorrectly classified tasks, or tasks that 
were correctly identified but which the user does 
not care to add to the ?to do? list. This list of tasks 
amounts to a task-focused summary of the 
document. This summary is intended to be read as 
a series of disconnected sentences, thus side-
stepping the issue of producing a coherent text 
from a series of extracted sentences. In the event 
that users prefer to view these extracted sentences 
as a coherent text, it may prove desirable to 
attempt to improve the textual cohesion by using 
anaphoric links, cue phrases and so on. 
Finally, Figure 3 also shows tasks highlighted in 
context in the message, allowing the user to skim 
the document and read the surrounding text. 
In the prototype we allow the user to vary the 
precision and recall of the classifier by adjusting a 
slider (not illustrated here) that sets the probability 
threshold on the probability of Task. 
 
Figure 3 and Figure 4 illustrate a convention that 
we observed in a handful of emails: proper names 
occur as section headings. These names have scope 
over the tasks enumerated beneath them, i.e. there 
is a list of tasks assigned to Matt, a list assigned to 
Eric or Mo, and a list assigned to Mo. SmartMail 
does not currently detect this explicit assignment 
of tasks to individuals. 
Important properties of tasks beyond the text of 
the message could also be automatically extracted. 
For example, the schema for tasks in Outlook 
includes a field that specifies the due date of the 
task. This field could be filled with date and time 
information extracted from the sentence containing 
the task. Similarly the content of the sentence 
containing the task or inferences about social 
relationships of the email interlocutors could be 
used to mark the priority of tasks as High, Low, or 
Normal in the existing schema. 
7 Conclusion 
In this paper we have presented aspects of 
SmartMail, which provides a task-oriented 
summary of email messages. This summary is 
produced by identifying the task-related sentences 
in the message and then reformulating each task-
related sentence as a brief (usually imperative) 
summation of the task. The set of tasks extracted 
and reformulated from a given email message is 
thus a task-focused summary of that message. 
We plan to conduct user studies by distributing 
the prototype as an Outlook add-in to volunteers 
who would use it to read and process their own 
mail over a period of several weeks. We intend to 
measure more than the precision and recall of our 
classifier by observing how many identified tasks 
users actually add to their ?to do? list and by 
administering qualitative surveys of user 
satisfaction. 
The ability to reformulate tasks is in principle 
separate from the identification of tasks. In our 
planned usability study we will distribute variants 
of the prototype to determine the effect of 
reformulation. Do users prefer to be presented with 
the extracted sentences with no additional 
processing, the tasks reformulated as described in 
Section 5, or an even more radical reformulation to 
a telegraphic form consisting of a verb plus object, 
such as Send information or Schedule subjects? 
 
 
 
 
 
 
 
Figure 3: Prototype system showing ways of visualizing tasks 
 
 
Figure 4: Magnified view of prototype system showing message with enumerated tasks 
 
 
 
8  Acknowledgements 
Many of the ideas presented here were formulated 
in discussion with Bob Atkinson, Dave Reed and 
Malcolm Pearson. Our thanks go to Jeff 
Stevenson, Margaret Salome and Kevin Gaughen 
for annotating the data. 
References 
Aikawa, Takako, Maite Melero, Lee Schwartz and 
Andi Wu. 2001. Multilingual natural language 
generation. EAMT. 
Bellotti, Victoria, Nicolas Ducheneaut, Mark 
Howard , Ian Smith. 2003. Taking email to 
task: the design and evaluation of a task 
management centered email tool. Proceedings 
of the conference on human factors in 
computing systems, pages 345-352. 
Cadiz, J. J., Dabbish, L., Gupta, A., & Venolia, G. 
D. 2001. Supporting email workflow. MSR-TR-
2001-88: Microsoft Research. 
Campbell, Richard and Hisami Suzuki. 2002. 
Language neutral representation of syntactic 
structure. Proceedings of SCANALU 2002. 
Dumais, Susan, John Platt, David Heckerman, 
Mehran Sahami 1998: Inductive learning 
algorithms and representations for text 
categorization. Proceedings of CIKM-98, pages 
148-155. 
Heidorn, George. 2000. Intelligent writing 
assistance. In R. Dale, H. Moisl and H. Somers, 
(eds.), Handbook of Natural Language 
Processing. Marcel Dekker. 
Joachims, Thorsten. 1998. Text categorization 
with support vector machines: Learning with 
many relevant features. Proceedings of ECML 
1998, pages 137-142. 
Mast, M., Kompe, R., Harbeck, S., Kiessling, A., 
Niemann, H., N?th, E., Schukat-Talamazzini, 
E. G. and Warnke., V. 1996. Dialog act 
classification with the help of prosody. ICSLP 
96. 
Platt, John. 1999. Fast training of SVMs using 
sequential minimal optimization. In B. 
Schoelkopf, C. Burges and A. Smola (eds.) 
Advances in Kernel Methods: Support Vector 
Learning, pages 185-208, MIT Press, 
Cambridge, MA.  
Stolcke, A., E. Shriberg, R. Bates, N. Coccaro, D. 
Jurafsky, R. Martin, M. Meteer, K. Ries, P. 
Taylor and C. Van Ess-Dykema. 1998. Dialog 
act modeling for conversational speech. 
Proceedings of the AAAI-98 Spring Symposium 
on Applying Machine Learning to Discourse 
Processing.  
Vapnik, V. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Warnke, V., R. Kompe, H. Niemann and E. N?th. 
1997. Integrated dialog act segmentation and 
classification using prosodic features and 
language models. Proc. European Conf. on 
Speech Communication and Technology, vol 1, 
pages 207?210. 
 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62?69,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The impact of parse quality on syntactically-informed statistical machine
translation
Chris Quirk and Simon Corston-Oliver
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
{chrisq,simonco}@microsoft.com
Abstract
We investigate the impact of parse quality
on a syntactically-informed statistical ma-
chine translation system applied to techni-
cal text. We vary parse quality by vary-
ing the amount of data used to train the
parser. As the amount of data increases,
parse quality improves, leading to im-
provements in machine translation output
and results that significantly outperform a
state-of-the-art phrasal baseline.
1 Introduction
The current study is a response to a question
that proponents of syntactically-informed machine
translation frequently encounter: How sensitive is
a syntactically-informed machine translation sys-
tem to the quality of the input syntactic analysis?
It has been shown that phrasal machine translation
systems are not affected by the quality of the in-
put word alignments (Koehn et al, 2003). This
finding has generally been cast in favorable terms:
such systems are robust to poor quality word align-
ment. A less favorable interpretation of these re-
sults might be to conclude that phrasal statistical
machine translation (SMT) systems do not stand
to benefit from improvements in word alignment.
In a similar vein, one might ask whether con-
temporary syntactically-informed machine trans-
lation systems would benefit from improvements
in parse accuracy. One possibility is that cur-
rent syntactically-informed SMT systems are de-
riving only limited value from the syntactic anal-
yses, and would therefore not benefit from im-
proved analyses. Another possibility is that syn-
tactic analysis does indeed contain valuable infor-
mation that could be exploited by machine learn-
ing techniques, but that current parsers are not of
sufficient quality to be of use in SMT.
With these questions and concerns, let us be-
gin. Following some background discussion we
describe a set of experiments intended to elucidate
the impact of parse quality on SMT.
2 Background
We trained statistical machine translation systems
on technical text. In the following sections we
provide background on the data used for training,
the dependency parsing framework used to pro-
duce treelets, the treelet translation framework and
salient characteristics of the target languages.
2.1 Dependency parsing
Dependency analysis is an alternative to con-
stituency analysis (Tesnie`re, 1959; Melc?uk, 1988).
In a dependency analysis of syntax, words di-
rectly modify other words, with no intervening
non-lexical nodes. We use the terms child node
and parent node to denote the tokens in a depen-
dency relation. Each child has a single parent, with
the lexical root of the sentence dependent on a syn-
thetic ROOT node.
We use the parsing approach described in
(Corston-Oliver et al, 2006). The parser is trained
on dependencies extracted from the English Penn
Treebank version 3.0 (Marcus et al, 1993) by
using the head-percolation rules of (Yamada and
Matsumoto, 2003).
Given a sentence x, the goal of the parser is to
find the highest-scoring parse y? among all possible
parses y ? Y :
y? = argmax
y?Y
s(x,y) (1)
The score of a given parse y is the sum of the
62
scores of all its dependency links (i, j) ? y:
s(x,y) = ?
(i, j)?y
d(i, j) = ?
(i, j)?y
w ? f(i, j) (2)
where the link (i, j) indicates a parent-child de-
pendency between the token at position i and the
token at position j. The score d(i, j) of each de-
pendency link (i, j) is further decomposed as the
weighted sum of its features f(i, j).
The feature vector f(i, j) computed for each
possible parent-child dependency includes the
part-of-speech (POS), lexeme and stem of the par-
ent and child tokens, the POS of tokens adjacent
to the child and parent, and the POS of each to-
ken that intervenes between the parent and child.
Various combinations of these features are used,
for example a new feature is created that combines
the POS of the parent, lexeme of the parent, POS
of the child and lexeme of the child. Each feature
is also conjoined with the direction and distance
of the parent, e.g. does the child precede or follow
the parent, and how many tokens intervene?
To set the weight vector w, we train twenty
averaged perceptrons (Collins, 2002) on different
shuffles of data drawn from sections 02?21 of the
Penn Treebank. The averaged perceptrons are then
combined to form a Bayes Point Machine (Her-
brich et al, 2001; Harrington et al, 2003), result-
ing in a linear classifier that is competitive with
wide margin techniques.
To find the optimal parse given the weight vec-
tor w and feature vector f(i, j) we use the decoder
described in (Eisner, 1996).
2.2 Treelet translation
For syntactically-informed translation, we fol-
low the treelet translation approach described
in (Quirk et al, 2005). In this approach, trans-
lation is guided by treelet translation pairs. Here,
a treelet is a connected subgraph of a dependency
tree. A treelet translation pair consists of a source
treelet S, a target treelet T , and a word alignment
A ? S? T such that for all s ? S, there exists a
unique t ? T such that (s, t)? A, and if t is the root
of T , there is a unique s ? S such that (s, t) ? A.
Translation of a sentence begins by parsing
that sentence into a dependency representation.
This dependency graph is partitioned into treelets;
like (Koehn et al, 2003), we assume a uniform
probability distribution over all partitions. Each
source treelet is matched to a treelet translation
pair; together, the target language treelets in those
treelet translation pairs will form the target trans-
lation. Next the target language treelets are joined
to form a single tree: the parent of the root of each
treelet is dictated by the source. Let tr be the root
of the target language treelet, and sr be the source
node aligned to it. If sr is the root of the source
sentence, then tr is made the root of the target lan-
guage tree. Otherwise let sp be the parent of sr,
and tp be the target node aligned to sp: tr is at-
tached to tp. Finally the ordering of all the nodes
is determined, and the target tree is specified, and
the target sentence is produced by reading off the
labels of the nodes in order.
Translations are scored according to a log-linear
combination of feature functions, each scoring dif-
ferent aspects of the translation process. We use a
beam search decoder to find the best translation T ?
according to the log-linear combination of models:
T ? = argmax
T
{
?
f?F
? f f (S,T,A)
}
(3)
The models include inverted and direct channel
models estimated by relative frequency, lexical
weighting channel models following (Vogel et al,
2003), a trigram target language model using mod-
ified Kneser-Ney smoothing (Goodman, 2001),
an order model following (Quirk et al, 2005),
and word count and phrase count functions. The
weights for these models are determined using the
method described in (Och, 2003).
To estimate the models and extract the treelets,
we begin from a parallel corpus. First the cor-
pus is word-aligned using GIZA++ (Och and Ney,
2000), then the source sentence are parsed, and
finally dependencies are projected onto the target
side following the heuristics described in (Quirk et
al., 2005). This word aligned parallel dependency
tree corpus provides training material for an order
model and a target language tree-based language
model. We also extract treelet translation pairs
from this parallel corpus. To limit the combina-
torial explosion of treelets, we only gather treelets
that contain at most four words and at most two
gaps in the surface string. This limits the number
of mappings to be O(n3) in the worst case, where
n is the number of nodes in the dependency tree.
2.3 Language pairs
In the present paper we focus on English-to-
German and English-to-Japanese machine transla-
63
you can set this property using Visual Basic
Sie k?nnen diese Eigenschaft auch mit Visual Basic festlegen
Figure 1: Example German-English and Japanese-English sentence pairs, with word alignments.
tion. Both German and Japanese differ markedly
from English in ways that we believe illumi-
nate well the strengths of a syntactically-informed
SMT system. We provide a brief sketch of the lin-
guistic characteristics of German and Japanese rel-
evant to the present study.
2.3.1 German
Although English and German are closely re-
lated ? they both belong to the western branch of
the Germanic family of Indo-European languages
? the languages differ typologically in ways that
are especially problematic for current approaches
to statistical machine translation as we shall now
illustrate. We believe that these typological differ-
ences make English-to-German machine transla-
tion a fertile test bed for syntax-based SMT.
German has richer inflectional morphology than
English, with obligatory marking of case, num-
ber and lexical gender on nominal elements and
person, number, tense and mood on verbal ele-
ments. This morphological complexity, combined
with pervasive, productive noun compounding is
problematic for current approaches to word align-
ment (Corston-Oliver and Gamon, 2004).
Equally problematic for machine translation is
the issue of word order. The position of verbs is
strongly determined by clause type. For exam-
ple, in main clauses in declarative sentences, finite
verbs occur as the second constituent of the sen-
tence, but certain non-finite verb forms occur in fi-
nal position. In Figure 1, for example, the English
?can? aligns with German ?ko?nnen? in second po-
sition and ?set? aligns with German ?festlegen? in
final position.
Aside from verbs, German is usually charac-
terized as a ?free word-order? language: major
constituents of the sentence may occur in various
orders, so-called ?separable prefixes? may occur
bound to the verb or may detach and occur at a
considerable distance from the verb on which they
depend, and extraposition of various kinds of sub-
ordinate clause is common. In the case of extrapo-
sition, for example, more than one third of relative
clauses in human-translated German technical text
are extraposed. For comparable English text the
figure is considerably less than one percent (Ga-
mon et al, 2002).
2.3.2 Japanese
Word order in Japanese is rather different from
English. English has the canonical constituent or-
der subject-verb-object, whereas Japanese prefers
subject-object-verb order. Prepositional phrases
in English generally correspond to postpositional
phrases in Japanese. Japanese noun phrases are
strictly head-final whereas English noun phrases
allow postmodifiers such as prepositional phrases,
relative clauses and adjectives. Japanese has lit-
tle nominal morphology and does not obligatorily
mark number, gender or definiteness. Verbal mor-
phology in Japanese is complex with morphologi-
cal marking of tense, mood, and politeness. Top-
icalization and subjectless clauses are pervasive,
and problematic for current SMT approaches.
The Japanese sentence in Figure 1 illustrates
several of these typological differences. The
sentence-initial imperative verb ?move? in the En-
glish corresponds to a sentence-final verb in the
Japanese. The Japanese translation of the object
noun phrase ?the camera slider switch? precedes
the verb in Japanese. The English preposition ?to?
aligns to a postposition in Japanese.
3 Experiments
Our goal in the current paper is to measure the
impact of parse quality on syntactically-informed
statistical machine translation. One method for
producing parsers of varying quality might be to
train a parser and then to transform its output, e.g.
64
by replacing the parser?s selection of the parent for
certain tokens with different nodes.
Rather than randomly adding noise to the
parses, we decided to vary the quality in ways that
more closely mimic the situation that confronts us
as we develop machine translation systems. An-
notating data for POS requires considerably less
human time and expertise than annotating syntac-
tic relations. We therefore used an automatic POS
tagger (Toutanova et al, 2003) trained on the com-
plete training section of the Penn Treebank (sec-
tions 02?21). Annotating syntactic dependencies
is time consuming and requires considerable lin-
guistic expertise.1 We can well imagine annotat-
ing syntactic dependencies in order to develop a
machine translation system by annotating first a
small quantity of data, training a parser, training a
system that uses the parses produced by that parser
and assessing the quality of the machine transla-
tion output. Having assessed the quality of the out-
put, one might annotate additional data and train
systems until it appears that the quality of the ma-
chine translation output is no longer improving.
We therefore produced parsers of varying quality
by training on the first n sentences of sections 02?
21 of the Penn Treebank, where n ranged from 250
to 39,892 (the complete training section). At train-
ing time, the gold-standard POS tags were used.
For parser evaluation and for the machine transla-
tion experiments reported here, we used an auto-
matic POS tagger (Toutanova et al, 2003) trained
on sections 02?21 of the Penn Treebank.
We trained English-to-German and English-to-
Japanese treelet translation systems on approxi-
mately 500,000 manually aligned sentence pairs
drawn from technical computer documentation.
The sentence pairs consisted of the English source
sentence and a human-translation of that sentence.
Table 1 summarizes the characteristics of this data.
Note that German vocabulary and singleton counts
are slightly more than double the corresponding
English counts due to complex morphology and
pervasive compounding (see section 2.3.1).
3.1 Parser accuracy
To evaluate the accuracy of the parsers trained on
different samples of sentences we used the tradi-
1Various people have suggested to us that the linguistic
expertise required to annotate syntactic dependencies is less
than the expertise required to apply a formal theory of con-
stituency like the one that informs the Penn Treebank. We
tend to agree, but have not put this claim to the test.
75%
80%
85%
90%
95%
0 10,000 20,000 30,000 40,000
Sample size
D
ep
en
de
n
c
y 
ac
cu
ra
c
y.
PTB Section 23
Technical text
Figure 2: Unlabeled dependency accuracy of
parsers trained on different numbers of sentences.
The graph compares accuracy on the blind test sec-
tion of the Penn Treebank to accuracy on a set of
250 sentences drawn from technical text. Punctu-
ation tokens are excluded from the measurement
of dependency accuracy.
tional blind test section of the Penn Treebank (sec-
tion 23). As is well-known in the parsing commu-
nity, parse quality degrades when a parser trained
on the Wall Street Journal text in the Penn Tree-
bank is applied to a different genre or semantic do-
main. Since the technical materials that we were
training the translation system on differ from the
Wall Street Journal in lexicon and syntax, we an-
notated a set of 250 sentences of technical material
to use in evaluating the parser. Each of the authors
independently annotated the same set of 250 sen-
tences. The annotation took less than six hours for
each author to complete. Inter-annotator agree-
ment excluding punctuation was 91.8%. Differ-
ences in annotation were resolved by discussion,
and the resulting set of annotations was used to
evaluate the parsers.
Figure 2 shows the accuracy of parsers trained
on samples of various sizes, excluding punctua-
tion tokens from the evaluation, as is customary
in evaluating dependency parsers. When mea-
sured against section 23 of the Penn Treebank,
the section traditionally used for blind evaluation,
the parsers range in accuracy from 77.8% when
trained on 250 sentences to 90.8% when trained
on all of sections 02?21. As expected, parse accu-
racy degrades when measured on text that differs
greatly from the training text. A parser trained on
250 Penn Treebank sentences has a dependency
65
English German English Japanese
Training Sentences 515,318 500,000
Words 7,292,903 8,112,831 7,909,198 9,379,240
Vocabulary 59,473 134,829 66,731 68,048
Singletons 30,452 66,724 50,381 52,911
Test Sentences 2,000 2,000
Words 28,845 31,996 30,616 45,744
Table 1: Parallel data characteristics
accuracy of 76.6% on the technical text. A parser
trained on the complete Penn Treebank training
section has a dependency accuracy of 84.3% on
the technical text.
Since the parsers make extensive use of lexi-
cal features, it is not surprising that the perfor-
mance on the two corpora should be so similar
with only 250 training sentences; there were not
sufficient instances of each lexical item to train re-
liable weights or lexical features. As the amount
of training data increases, the parsers are able to
learn interesting facts about specific lexical items,
leading to improved accuracy on the Penn Tree-
bank. Many of the lexical items that occur in the
Penn Treebank, however, occur infrequently or not
at all in the technical materials so the lexical infor-
mation is of little benefit. This reflects the mis-
match of content. The Wall Street Journal articles
in the Penn Treebank concern such topics as world
affairs and the policies of the Reagan administra-
tion; these topics are absent in the technical mate-
rials. Conversely, the Wall Street Journal articles
contain no discussion of such topics as the intrica-
cies of SQL database queries.
3.2 Translation quality
Table 2 presents the impact of parse quality on a
treelet translation system, measured using BLEU
(Papineni et al, 2002). Since our main goal is to
investigate the impact of parser accuracy on trans-
lation quality, we have varied the parser training
data, but have held the MT training data, part-of-
speech-tagger, and all other factors constant. We
observe an upward trend in BLEU score as more
training data is made available to the parser; the
trend is even clearer in Japanese.2 As a baseline,
we include right-branching dependency trees, i.e.,
trees in which the parent of each word is its left
2This is particularly encouraging since various people
have remarked to us that syntax-based SMT systems may
be disadvantaged under n-gram scoring techniques such as
BLEU.
EG EJ
Phrasal decoder 31.7?1.2 32.9?0.9
Treelet decoder
Right-branching 31.4?1.3 28.0?0.7
250 sentences 32.8?1.4 34.1?0.9
2,500 sentences 33.0?1.4 34.6?1.0
25,000 sentences 33.7?1.5 35.7?0.9
39,892 sentences 33.6?1.5 36.0?1.0
Table 2: BLEU score vs. decoder and parser vari-
ants. Here sentences refer to the amount of parser
training data, not MT training data.
neighbor and the root of a sentence is the first
word. With this analysis, treelets are simply sub-
sequences of the sentence, and therefore are very
similar to the phrases of Phrasal SMT. In English-
to-German, this result produces results very com-
parable to a phrasal SMT system (Koehn et al,
2003) trained on the same data. For English-to-
Japanese, however, this baseline performs much
worse than a phrasal SMT system. Although
phrases and treelets should be nearly identical
under this scenario, the decoding constraints are
somewhat different: the treelet decoder assumes
phrasal cohesion during translation. This con-
straint may account for the drop in quality.
Since the confidence intervals for many pairs
overlap, we ran pairwise tests for each system to
determine which differences were significant at
the p < 0.05 level using the bootstrap method de-
scribed in (Zhang and Vogel, 2004); Table 3 sum-
marizes this comparison. Neither language pair
achieves a statistically significant improvement
from increasing the training data from 25,000
pairs to the full training set; this is not surprising
since the increase in parse accuracy is quite small
(90.2% to 90.8% on Wall Street Journal text).
To further understand what differences in de-
pendency analysis were affecting translation qual-
ity, we compared a treelet translation system that
66
Pharaoh Right-branching 250 2,500 25,000 39,892
Pharaoh ? > > > >
Right-branching > > > >
250 ? > >
2,500 > >
25,000 ?
(a) English-German
Pharaoh Right-branching 250 2,500 25,000 39,892
Pharaoh < ? > > >
Right-branching > > > >
250 > > >
2,500 > >
25,000 ?
(b) English-Japanese
Table 3: Pairwise statistical significance tests. > indicates that the system on the top is significantly better
than the system on the left; < indicates that the system on top is significantly worse than the system on
the left; ? indicates that difference between the two systems is not statistically significant.
32
33
34
35
36
37
100 1000 10000 100000
Parser training sentences
B
LE
U
 
sc
o
re
Japanese
German
Figure 3: BLEU score vs. number of sentences
used to train the dependency parser
used a parser trained on 250 Penn Treebank sen-
tences to a treelet translation system that used
a parser trained on 39,892 Treebank sentences.
From the test data, we selected 250 sentences
where these two parsers produced different anal-
yses. A native speaker of German categorized the
differences in machine translation output as either
improvements or regressions. We then examined
and categorized the differences in the dependency
analyses. Table 4 summarizes the results of this
comparison. Note that this table simply identifies
correlations between parse changes and translation
changes; it does not attempt to identify a causal
link. In the analysis, we borrow the term ?NP
[Noun Phrase] identification? from constituency
analysis to describe the identification of depen-
dency treelets spanning complete noun phrases.
There were 141 sentences for which the ma-
chine translated output improved, 71 sentences for
which the output regressed and 38 sentences for
which the output was identical. Improvements in
the attachment of prepositions, adverbs, gerunds
and dependent verbs were common amongst im-
proved translations, but rare amongst regressed
translations. Correct identification of the depen-
dent of a preposition3 was also much more com-
mon amongst improvements.
Certain changes, such as improved root identifi-
cation and final punctuation attachment, were very
common across the corpus. Therefore their com-
mon occurrence amongst regressions is not very
surprising. It was often the case that improve-
ments in root identification or final punctuation at-
tachment were offset by regressions elsewhere in
the same sentence.
Improvements in the parsers are cases where
the syntactic analysis more closely resembles the
analysis of dependency structure that results from
applying Yamada and Matsumoto?s head-finding
rules to the Penn Treebank. Figure 4 shows dif-
ferent parses produced by parsers trained on dif-
3In terms of constituency analysis, a prepositional phrase
should consist of a preposition governing a single noun
phrase
67
You can manipulate Microsoft Access objects from another application that also supports automation .ROOT
You can manipulate Microsoft Access objects from another application that also supports automation .ROOT
(a) Dependency analysis produced by parser trained on 250 Wall Street Journal sentences.
(b) Dependency analysis produced by parser trained on 39,892 Wall Street Journal sentences.Figure 4: Parses produced by parsers trained on different numbers of sentences.
ferent numbers of sentences. The parser trained
on 250 sentences incorrectly attaches the prepo-
sition ?from? as a dependent of the noun ?ob-
jects? whereas the parser trained on the complete
Penn Treebank training section correctly attaches
the preposition as a dependent of the verb ?ma-
nipulate?. These two parsers also yield different
analyses of the phrase ?Microsoft Access objects?.
In parse (a), ?objects? governs ?Office? and ?Of-
fice? in turn governs ?Microsoft?. This analy-
sis is linguistically well-motivated, and makes a
treelet spanning ?Microsoft Office? available to
the treelet translation system. In parse (b), the
parser has analyzed this phrase so that ?objects?
directly governs ?Microsoft? and ?Office?. The
analysis more closely reflects the flat branching
structure of the Penn Treebank but obscures the
affinity of ?Microsoft? and ?Office?.
An additional measure of parse utility for MT
is the amount of translation material that can be
extracted from a parallel corpus. We increased the
parser training data from 250 sentences to 39,986
sentences, but held the number of aligned sentence
pairs used train other modules constant. The count
of treelet translation pairs occurring at least twice
in the English-German parallel corpus grew from
1,895,007 to 2,010,451.
4 Conclusions
We return now to the questions and concerns
raised in the introduction. First, is a treelet SMT
system sensitive to parse quality? We have shown
that such a system is sensitive to the quality of
Error category Regress Improve
Attachment of prep 1% 22%
Root identification 13% 28%
Final punctuation 18% 30%
Coordination 6% 16%
Dependent verbs 14% 32%
Arguments of verb 6% 15%
NP identification 24% 33%
Dependent of prep 0% 7%
Other attachment 3% 22%
Table 4: Error analysis, showing percentage of
regressed and improved translations exhibiting a
parse improvement in each specified category
the input syntactic analyses. With the less accu-
rate parsers that result from training on extremely
small numbers of sentences, performance is com-
parable to state-of-the-art phrasal SMT systems.
As the amount of data used to train the parser in-
creases, both English-to-German and English-to-
Japanese treelet SMT improve, and produce re-
sults that are statistically significantly better than
the phrasal baseline.
In the introduction we mentioned the concern
that others have raised when we have presented
our research: syntax might contain valuable infor-
mation but current parsers might not be of suffi-
cient quality. It is certainly true that the accuracy
of the best parser used here falls well short of what
we might hope for. A parser that achieves 90.8%
dependency accuracy when trained on the Penn
Treebank Wall Street Journal corpus and evalu-
68
ated on comparable text degrades to 84.3% accu-
racy when evaluated on technical text. Despite the
degradation in parse accuracy caused by the dra-
matic differences between the Wall Street Journal
text and the technical articles, the treelet SMT sys-
tem was able to extract useful patterns. Research
on syntactically-informed SMT is not impeded by
the accuracy of contemporary parsers.
One significant finding is that as few as 250
sentences suffice to train a dependency parser for
use in the treelet SMT framework. To date our
research has focused on translation from English
to other languages. One concern in applying the
treelet SMT framework to translation from lan-
guages other than English has been the expense
of data annotation: would we require 40,000 sen-
tences annotated for syntactic dependencies, i.e.,
an amount comparable to the Penn Treebank, in
order to train a parser that was sufficiently accu-
rate to achieve the machine translation quality that
we have seen when translating from English? The
current study gives hope that source languages can
be added with relatively modest investments in
data annotation. As more data is annotated with
syntactic dependencies and more accurate parsers
are trained, we would hope to see similar improve-
ments in machine translation output.
We challenge others who are conducting re-
search on syntactically-informed SMT to verify
whether or to what extent their systems are sen-
sitive to parse quality.
References
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Simon Corston-Oliver and Michael Gamon. 2004.
Normalizing German and English inflectional mor-
phology to improve statistical word alignment. In
R. E. Frederking and K. B. Taylor, editors, Machine
translation: From real users to research. Springer
Verlag.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using Bayes Point Machines. In Proceedings of
HLT/NAACL.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of COLING, pages 340?345.
Michael Gamon, Eric Ringger, Zhu Zhang, Robert
Moore, and Simon Corston-Oliver. 2002. Extrapo-
sition: A case study in German sentence realization.
In Proceedings of COLING, pages 301?307.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling, extended version. Technical Re-
port MSR-TR-2001-72, Microsoft Research.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proc. 7th Pacific-Asia
Conference on Knowledge Discovery and Data Min-
ing, pages 241?252.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes Point Machines. Journal of Machine
Learning Research, pages 245?278.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Igor A. Melc?uk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL, pages 440?447, Hongkong, China, October.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311?318, Philadelpha, Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the ACL.
Lucien Tesnie`re. 1959. ?Ele?ments de syntaxe struc-
turale. Librairie C. Klincksieck.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT/EMNLP, pages 252?259.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings of the MT Summit.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195?206.
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for mt evaluation metrics. In Pro-
ceedings of TMI.
69
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 196?200, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing with Reference to Slovene, Spanish and Swedish
Simon Corston-Oliver
Natural Language Processing
Microsoft Research
One Microsoft Way
Redmond WA 98052
simonco@microsoft.com
Anthony Aue
Natural Language Processing
Microsoft Research
One Microsoft Way
Redmond WA 98052
anthaue@microsoft.com
Abstract
We describe a parser used in the CoNLL
2006 Shared Task, ?Multingual Depen-
dency Parsing.? The parser first identi-
fies syntactic dependencies and then labels
those dependencies using a maximum en-
tropy classifier. We consider the impact of
feature engineering and the choice of ma-
chine learning algorithm, with particular
focus on Slovene, Spanish and Swedish.
1 Introduction
The system that we submitted for the CoNLL 2006
Shared Task, ?Multingual Dependency Parsing,?
(Buchholz et al, 2006) is a two stage pipeline. The
first stage identifies unlabeled directed dependen-
cies using an extension of the parser described in
(Corston-Oliver et al, 2006). The second stage is a
maximum entropy classifier that labels the directed
dependencies. The system was trained on the twelve
obligatory languages, as well as the optional lan-
guage, Bulgarian (Hajic? et al, 2004; Simov et al,
2005; Simov and Osenova, 2003; Chen et al, 2003;
Bo?hmova? et al, 2003; Kromann, 2003; van der Beek
et al, 2002; Brants et al, 2002; Kawata and Bar-
tels, 2000; Afonso et al, 2002; Dz?eroski et al, 2006;
Civit Torruella and Mart?? Anton??n, 2002; Nilsson et
al., 2005; Oflazer et al, 2003; Atalay et al, 2003).
Table 1 presents the results of the system de-
scribed in the current paper on the CoNLL shared
task, including the optional evaluation on Bulgar-
ian. For Slovene, we ranked second with a labeled
Language Unlabeled Labeled
Attachment Attachment
Arabic 78.40 63.53
Bulgarian 90.09 83.36
Chinese 90.00 79.92
Czech 83.02 74.48
Danish 87.94 81.74
Dutch 74.83 71.43
German 87.20 83.47
Japanese 92.84 89.95
Portugese 88.96 84.59
Slovene 81.77 72.42
Spanish 84.87 80.36
Swedish 89.54 79.69
Turkish 73.11 61.74
Table 1: Results on CoNLL 2006 shared task.
dependency accuracy of 72.42%. This was not sta-
tistically significantly different from the top-ranked
score of 73.44%. For Spanish, our labeled depen-
dency accuracy of 80.36% is within 0.1% of the
third-ranked score of 80.46%. Our unlabeled de-
pendency accuracy for Swedish was the best of all
the systems at 89.54%. Our labeled accuracy for
Swedish, however, at 79.69%, fell far short of the
third-best score of 82.31%. We therefore focus on
Swedish when considering the impact of our choice
of learning algorithm on our label accuracy.
2 Data
We divided the shared data into training and devel-
opment test sets, using larger development test sets
196
for the languages supplied with more data. The de-
velopment test set consisted of 250 sentences for
Arabic, Slovene, Spanish and Turkish, 500 sen-
tences for Danish and Portuguese, and 1,000 sen-
tences for the other languages.
3 The Parser
The baseline parser predicts unlabeled directed de-
pendencies. As described in (Corston-Oliver et al,
2006), we reimplemented the parser described in
(McDonald et al, 2005) and validated their results
for Czech and English.
The parser finds the highest-scoring parse y?
among all possible parses y ? Y for a given sen-
tence:
y? = argmax
y?Y
s(y) (1)
The score s of a given parse y is the sum of the
scores of all the dependency links (i,j) ? y:
s(y) =
?
(i,j)?y
d(i, j) =
?
(i,j)?y
w ? f(i, j) (2)
where the link (i,j) indicates a parent-child depen-
dency between the token at position i and the token
at position j. The score d(i, j) of each dependency
link (i,j) is further decomposed as the weighted sum
of its features f(i, j).
To set w, we trained twenty averaged perceptrons
on different shuffles of the training data, using the
development test set to determine when the percep-
trons had converged. The averaged perceptrons were
then combined to make a Bayes Point Machine (Har-
rington et al, 2003). At both training and run time,
edges are scored independently, and Eisner?s O(N3)
decoder (Eisner, 1996) is used to find the optimal
parse. This decoder produces only projective analy-
ses, although it does allow for analyses with multiple
roots.
The features used for scoring the edges prior to
applying Eisner?s algorithm are extracted from each
possible parent-child dependency. The features in-
clude the case-normalized original form and lemma1
of each token , the part of speech (POS) tag of each
token, the POS tag of each intervening token and
1If no lemma was specified, we truncated the original form
by taking the first two characters for Chinese words consisting
of two characters or more and the first five characters for words
consisting of five characters or more in the other languages.
of each token to the left and right of the parent and
child. Additional features are created by combining
these atomic features, as described in (McDonald et
al., 2005). All features are in turn combined with
the direction of attachment and the distance between
tokens. Distance was discretized, with individual
buckets for distances 0-4, a single bucket for 5-9,
and a single bucket for 10+. In sections 3.1 and 3.2
we discuss the feature engineering we performed.
3.1 Part of Speech Features
We experimented with using the coarse POS tag and
the fine POS tag. In our official submission, we
used fine POS tags for all languages except Dutch
and Turkish. For Dutch and Turkish, using the fine
POS tag resulted in a reduction in unlabeled depen-
dency accuracy of 0.12% and 0.43% respectively
on the development test sets, apparently because of
the sparsity of the fine POS tags. For German and
Swedish, the fine and coarse POS tags are the same
so using the fine POS tag had no effect. For other
languages, using the fine POS tag showed modest
improvements in unlabeled dependency accuracy.
For Swedish, we performed an additional manipu-
lation on the POS tags, normalizing the distinct POS
tags assigned to each verbal auxiliary and modal to
a single tag ?aux?. For example, in the Swedish
data all inflected forms of the verb ?vara? (?be?) are
tagged as AV, and all inflected forms of the modal
?ma?ste? (?must?) are tagged as MV. This normaliza-
tion caused unlabeled dependency accuracy on the
Swedish development set to improve from 89.23%
to 89.45%.
3.2 Features for Root Identification
Analysis of the baseline parser?s errors suggested
the need for additional feature types to improve the
identification of the root of the sentence. In particu-
lar, the parser was frequently making errors in iden-
tifying the root of periphrastic constructions involv-
ing an auxiliary verb or modal and a participle. In
Germanic languages, for example, the auxiliary or
modal typically occurs in second position in declar-
ative main clauses or in initial position in cases of
subject-aux inversion. We added a collection of fea-
tures intended to improve the identification of the
root. The hope was that improved root identifica-
tion would have a positive cascading effect in the
197
identification of other dependencies, since a failure
to correctly identify the root of the sentence usually
means that the parse will have many other errors.
We extracted four feature types, the original form
of the first and last tokens in the sentence and the
POS of the first and last tokens in the sentence.
These features were intended to identify declarative
vs. interrogative sentences.
For each child and parent token being scored, we
also noted the following four features: ?child/parent
is first non-punctuation token in sentence?,
?child/parent is second non-punctuation token in
sentence?. The features that identify the second
token in the sentence were intended to improve
the identification of verb-second phenomena. Of
course, this is a linguistic oversimplification. Verb-
second phenomena are actually sensitive to the order
of constituents, not words. We therefore added four
feature types that considered the sequence of POS
tags to the left of the child or parent if they occurred
within ten tokens of the beginning of the sentence
and the sequence of POS tags to the right of the
child or parent if they occurred within ten tokens of
the end of the sentence.
We also added features intended to improve the
identification of the root in sentences without a fi-
nite verb. For example, the Dutch training data
contained many simple responses to a question-
answering task, consisting of a single noun phrase.
Four simple features were used ?Child/Parent is the
leftmost noun in the sentence?, ?Child/Parent is a
noun but not the leftmost noun in the sentence?.
These features were combined with an indicator
?Sentence contains/does not contain a finite verb?.
Child or parent tokens that were finite verbs were
flagged as likely candidates for being the root of
the sentence if they were the leftmost finite verb in
the sentence and not preceded by a subordinating
conjunction or relative pronoun. Finite verbs were
identified by POS tags and morphological features,
e.g. in Spanish, verbs without the morphological
feature ?mod=n? were identified as finite, while in
Portuguese the fine POS tag ?v-fin? was used.
Similarly, various sets of POS tags were used to
identify subordinating conjunctions or relative pro-
nouns for different languages. For example, in Bul-
garian the fine POS tag ?pr? (relative pronoun) and
?cs? (subordinating conjunction) were used. For
Dutch, the morphological features ?onder?, ?betr?
and ?voorinf? were used to identify subordinating
conjunctions and relative pronouns.
These features wreaked havoc with Turkish, a
verb-final language. For certain other languages,
dependency accuracy measured on the develop-
ment test set improved by a modest amount, with
more dramatic improvements in root accuracy (F1
measure combining precision and recall for non-
punctuation root tokens).
Since the addition of these features had been mo-
tivated by verb-second phenomena in Germanic lan-
guages, we were surprised to discover that the only
Germanic language to demonstrate a marked im-
provement in unlabeled dependency accuracy was
Danish, whose accuracy on the development set rose
from 87.51% to 87.72%, while root accuracy F1
rose from 94.12% to 94.72%. Spanish showed a
modest improvement in unlabeled dependency accu-
racy, from 85.08% to 85.13%, but root F1 rose from
80.08% to 83.57%.
The features described above for identifying the
leftmost finite verb not preceded by a subordinat-
ing conjunction or relative pronoun did not im-
prove Slovene unlabeled dependency accuracy, and
so were not included in the set of root-identifying
features in our Slovene CoNLL submission. Closer
examination of the Slovene corpus revealed that pe-
riphrastic constructions consisting of one or more
auxiliaries followed by a participle were annotated
with the participle as the head, whereas for other
languages in the shared task the consensus view ap-
pears to be that the auxiliary should be annotated
as the head. Singling out the leftmost finite verb in
Slovene when a participle ought to be selected as the
root of the sentence is therefore counter-productive.
The other root identification features did improve
root F1 in Slovene. Root F1 on the development test
set rose from 45.82% to 46.43%, although overall
unlabeled dependency accuracy on the development
test set fell slightly from 80.24% to 79.94%.
3.3 Morphological Features
As the preceding discussion shows, morphological
information was occasionally used to assist in mak-
ing finer-grained POS distinctions than were made
in the POS tags, e.g., for distinguishing subordi-
nating vs. coordinating conjunctions. Aside from
198
these surgical uses of the morphological information
present in the CoNLL data, morphology was not ex-
plicitly used by the baseline parser. For example,
there were no features that considered subject-verb
agreement nor agreement of an adjective with the
number or lexical gender of the noun it modified.
However, it is possible that morphological informa-
tion influenced the training of edge weights if the
information was implicit in the POS tags.
4 The Dependency Labeler
4.1 Classifier
We used a maximum entropy classifier (Berger et al,
1996) to assign labels to the unlabeled dependen-
cies produced by the Bayes Point Machine. We used
the same training and development test split that was
used to train the dependency parser. We chose to use
maximum entropy classifiers because they can be
trained relatively quickly while still offering reason-
able classification accuracy and are robust in the face
of large numbers of superfluous features, a desirable
property given the requirement that the same parser
handle multiple languages. Furthermore, maximum
entropy classifiers provide good probability distribu-
tions over class labels. This was important to us be-
cause we had initially hoped to find the optimal set
of dependency labels for the children of a given node
by modeling the probability of each set of labels
conditioned on the lemma and POS of the parent.
For example, labeling each dependant of a parent
node independently might result in three OBJECT
relations dependent on a single verb; modeling sets
of relations ought to prevent this. Unfortunately, this
approach did not outperform labeling each node in-
dependently.
Therefore, the system we submitted labeled each
dependency independently, using the most probable
label from the maximum entropy classifier. We have
noted in previous experiments that our SVM imple-
mentation often gives better one-best classification
accuracy than our maximum entropy implementa-
tion, but did not have time to train SVM classifiers.
To see how much the choice of classification al-
gorithm affected our official results, we trained a lin-
ear SVM classifier for Swedish after the competition
had ended, tuning parameters on the development
test set. As noted in section 1, our system scored
highest for Swedish in unlabeled dependency accu-
racy at 89.54% but fell well short of the third-ranked
system when measuring labeled dependency accu-
racy. Using an SVM classifier instead of a maxi-
mum entropy classifier, Swedish label accuracy rose
from 82.33% to 86.06%, and labeled attachment ac-
curacy rose from 79.69% to 82.95%, which falls
between the first-ranked score of 84.58% and the
second-ranked score of 82.55%. Similarly, Japanese
label accuracy rose from 93.20% to 93.96%, and
labeled attachment accuracy rose from 89.95% to
90.77% when we replaced a maximum entropy clas-
sifier with an SVM. This labeled attachment result
of 90.77% is comparable to the official second place
result of 90.71% for Japanese. We conclude that a
two stage pipeline such as ours, in which the sec-
ond stage labels dependencies in isolation, is greatly
impacted by the choice of classifier.
4.2 Features Used for Labeling
We extracted features from individual nodes in the
dependency tree, parent-child features and features
that took nodes other than the parent and child into
account.
The features extracted from each individual par-
ent and child node were the original surface form,
the lemma (see footnote 1 above), the coarse and fine
POS tags and each morphological feature.
The parent-child features are the direction of
modification, the combination of the parent and
child lemmata, all combinations of parent and child
lemma and coarse POS tag (e.g. child lemma com-
bined with coarse POS tag of the parent) and all pair-
wise combinations of parent and child morphology
features (e.g. parent is feminine and child is plural).
Additional features were verb position (whether
the parent or child is the first or last verb in the sen-
tence), coarse POS and lemma of the left and right
neighbors of the parent and child, coarse POS and
lemma of the grandparent, number and coarse POS
tag sequence of siblings to the left and to the right of
the child, total number of siblings of the child, num-
ber of tokens governed by child, whether the par-
ent has a verbal ancestor, lemma and morphological
features of the verb governing the child (if any), and
coarse POS tag combined with relative offset of each
sibling (e.g., the sibling two to the left of the child is
a determiner).
199
For Slovene, the label accuracy using all of the
features above was 81.91%. We retrained our max-
imum entropy classifier by removing certain classes
of features in order to determine their contribu-
tion. Removing the weight features caused a notable
drop, with label accuracy on the development test set
falling 0.52% to 81.39%. Removing the grandpar-
ent features (but including weight features) caused
an even greater drop of 1.03% to 80.88%. One place
where the grandparent features were important was
in distinguishing between Adv and Atr relations. It
appears that the relation between a noun and its gov-
erning preposition or between a verb and its govern-
ing conjunction is sensitive to the part of speech of
the grandparent. For example, we observed a num-
ber of cases where the relation between a noun and
its governing preposition had been incorrectly la-
beled as Adv when it should have been Atr. The
addition of grandparent features allowed the classi-
fier to make the distinction by looking at the POS of
the grandparent; when the POS was noun, the clas-
sifier tended to correctly choose the Atr label.
5 Conclusion
We have described a two stage pipeline that first pre-
dicts directed unlabeled dependencies and then la-
bels them. The system performed well on Slovene,
Spanish and Swedish. Feature engineering played
an important role both in predicting dependencies
and in labeling them. Finally, replacing the maxi-
mum entropy classifier used to label dependencies
with an SVM improves upon our official results.
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. In Proc. of the Tenth Conf. on Com-
putational Natural Language Learning (CoNLL-X).
SIGNLL.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency parsing
using bayes point machines. In Proc. of HLT-NAACL
2006.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
the 16th Intern. Conf. on Computational Linguistics
(COLING), pages 340?345.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proceedings of Seventh
Pacific-Asia Conference on Knowledge Discovery and
Data Mining, pages 241?252.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting
of the Assocation for Computational Linguistics.
200
