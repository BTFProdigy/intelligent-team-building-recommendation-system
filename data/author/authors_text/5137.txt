Frameworks, Implementation and Open Problems for the 
Collaborative Building of a Multilingual Lexical Database 
Mathieu MANGEOT-LEREBOURS(1), Gilles S?RASSET(2) and Fr?d?ric ANDR?S(1) 
(1) Software Research Division, NII 
Hitotsubashi, 2-1-2-1913 Chiyoda-ku 
101-8430 Tokyo, Japan 
{mangeot,andres}@nii.ac.jp 
(2) GETA-CLIPS-IMAG 
185, rue de la biblioth?que, BP 53 
F-38041 GRENOBLE CEDEX 9, France 
Gilles.Serasset@imag.fr 
 
Abstract 
Many NLP systems are based on lexical 
data. The development costs of such data 
are a major drawback in such NLP systems. 
In order to cut these costs, we adopt a 
strategy inspired from "open-source" 
projects to allow volunteers to collaborate 
in the creation of a multilingual lexical 
database. 
For this, we had to specify and develop 
tools to manage a lexical database 
containing information complete and 
detailed enough to be usable for a wide 
range of applications. 
This paper presents our project and details 
the tools, frameworks and structures used 
to manage such a database. We will also 
show some research problems still to be 
addressed in this context. 
R?sum? 
La connaissance linguistique reste une 
constituante importante de nombreux 
syst?mes de traitement automatique des 
langues (TAL). Le co?t de cr?ation d?un 
dictionnaire est l?un des freins majeurs 
dans le d?veloppement de ces syst?mes. 
Afin de r?duire les co?ts de cr?ation de 
cette connaissance lexicale, nous adoptons 
une m?thode inspir?e des projets 
"open-source" afin de cr?er une base 
lexicale multilingue. 
 
Pour cela, nous avons sp?cifi? et 
d?velopp? des outils de gestion d'une base 
lexicale contenant des informations 
suffisamment compl?tes et d?taill?es pour 
?tres utilis?es dans de nombreuses 
applications diff?rentes. 
Cet article pr?sente notre projet et d?taille 
les outils, les cadres et les structures 
utilis?es pour la gestion de cette base. 
Nous montrons aussi certains probl?mes 
de recherche ouverts qu'il nous faut 
aborder dans ce contexte. 
Introduction 
Many NLP systems are based on lexical data. 
The development costs of such data are a major 
drawback in such NLP systems. Furthermore, the 
existing lexical data have generally been 
developed for a specific purpose and can?t be 
reused easily in other applications. 
The Papillon project applies some tools and 
methods to develop multipurpose, multilingual 
lexical data collaboratively on Internet. This data 
is complete and detailed enough to be eventually 
used either by NLP systems (MT engines for 
example) or by human users (language learners, 
translators?).  
After presenting the motivations of the Papillon 
project, we will show the management of existing 
data. Then we will describe the structure of the 
Papillon dictionary, and the tools that are used to 
allow contributions from Internet volunteers. 
1 The Papillon Project 
1.1 Motivations 
The Papillon project is the result of the gathering 
of different people sharing common problems 
and solutions. 
1.1.1 A Lack of Resources 
On the Internet, a lot of free dictionaries are 
available but very few of them imply more than 2 
languages. Most of these dictionaries include 
English as one of their languages. 
Furthermore, the existing dictionaries often lack 
information essential for beginners or NLP 
systems.  
Another point contributing to this lack: the high 
costs of development of large lexical resources 
for NLP involves also a high price, dissuasive for 
the end-user. 
1.1.2 Existing Structures and Tools for 
Multilingual Dictionaries 
Some partners of the Papillon project have been 
involved in research on the definition of 
structures and tools to handle multilingual lexical 
databases. 
They were looking for an opportunity to apply 
their research results on real scale lexical data. 
1.1.3 Collaborative Development on the 
Internet 
Most partners were participating, as computer 
scientists, in the development of open source 
products. With the democratisation of Internet 
access in a lot of countries, came the opportunity 
to apply the open source principles to the 
development of a multipurpose, multilingual 
lexical database. 
Cooperation projects for bilingual dictionaries 
are already going on such as EDICT, a 
Japanese-English dictionary lead by Jim Breen 
(2001) for more than 10 years and more recently, 
SAIKAM, a Japanese-Thai dictionary (see 
Ampornaramveth (2000)).  
With the Papillon project, the dictionary is 
extended to a multilingual lexical database. 
Volunteers will find lexicons developed by 
others and some tools to complete or correct the 
Papillon multilingual dictionary. Users will also 
be able to define their own personal views of the 
database. 
1.2 Dictionary Markup Language 
Framework 
Mathieu Mangeot-Lerebours (2001) defines a 
complete framework for the consultation and the 
construction of dictionaries. The framework is 
completely generic in order to manage 
heterogeneous dictionaries with their own proper 
structures. This framework is extensively used in 
Papillon project. 
1.2.1 Dictionary Markup Language (DML) 
 The framework consists in the definition of an 
XML namespace1 called DML   (Dictionary  
Markup Language). All lexical data of a lexical 
database can be described with DML elements. 
The entire hierarchy of the XML files, elements 
and attributes is described using XML schemata 
and grouped into the DML namespace. Figure 1 
describes the organisation of the main DML 
elements.  
Database
Entry
Dictionary
Client
API
Supplier
API
Volume
User
History
Group
CDM set
?headword
?pos
?pronunciation
?translation
?example
?idiom
Basic Types
?boolean
?integer
?date
function
tree
graph
automaton
link
Figure 1. The DML Framework 
The XML schemata are available online. This 
allows users to edit and validate their files online 
with an XML schema validator. 
1.2.2 Common Dictionary Markup (CDM) 
The DML framework may be used to encode 
many different dictionary structures. Indeed, two 
dictionary structures can be radically different. In 
order to handle such heterogeneous structures 
with the same tools, we have defined a subset of 
DML element and attributes that are used to 
 
                                                     
1  http://www-clips.imag.fr/geta/services/dml 
identify which part of the different structures 
represent the same lexical information. This 
subset is called Common Dictionary Markup 
(CDM). This set is in constant evolution. If the 
same kind of information is found in several 
dictionaries then a new element representing this 
piece of information is added to the CDM set. It 
allows tools to have access to common 
information in heterogeneous dictionaries by way 
of pointers into the structures of the dictionaries. 
1.3 Three Layers for the Lexical Data 
The lexical data repository of the Papillon project 
is divided into 4 subdirectories:  
Administration contains guidelines and 
administrative files 
? 
? 
? 
? 
Hell (data in original format) 
Purgatory (data in XML & UTF-8) 
Paradise (data in Papillon format) 
The name of the files and directories is 
normalised in order to allow easy navigation into 
the repository. 
All lexical data stored in the repository is free of 
rights or protected by a GPL-like licence. 
1.3.1 Hell Directory 
This directory contains lexical data in their 
original format. When a dictionary is received, it 
is first stored there while waiting to be ?recycled?. 
For each dictionary, we create a metadata file 
containing all available information concerning 
the dictionary (name, languages covered, 
creation date, size, authors, domain, etc.). It is 
then used to evaluate the quality of the dictionary 
and to guide the recycling process. These 
dicitonaries are freely downloadable as they are. 
1.3.2 Purgatory Directory 
The Purgatory directory receives the lexical data 
once the recuperation process is over. This 
process consists in converting the lexical data 
from its original format into XML encoded in 
UTF-8. To perform this task, we use the 
RECUPDIC methodology described in 
Doan-Nguyen (1998) regular expression tools 
like Perl scripts. 
 
If a dictionary is already encoded in XML, the 
recuperation process consists in mapping the 
elements of information into CDM elements and 
storing the correspondence into the metadata file.  
Internet users access these dictionaries as 
classical online dictionaries, retrieving individual 
entries by way of requests on the Papillon web 
site. 
1.3.3 Paradise Directory 
The Paradise directory contains only one 
dictionary often called the "Papillon dictionary". 
This dictionary has a particular DML structure. 
Internet users access entries of this dictionary by 
way of requests to the Papillon web site. 
It is possible to retrieve only one entry, or any 
subset of entries in any available output format. 
The ?native? format is the Papillon textual XML 
DML format in UTF-8. Users also have ways to 
add new entries or correct existing ones online.  
Other purgatory dictionaries may be integrated 
into the Papillon dictionary with the help of the 
CDM elements. 
2 The Papillon Multilingual 
Dictionary 
2.1 Macrostructure 
The architecture of the Papillon multilingual 
dictionary is based on Gilles S?rasset (1994) and  
has been prototyped by Blanc (1999). This 
architecture uses a pivot structure based on 
multiple monolingual volumes linked to an 
interlingual acception volume.  
Each entry of a monolingual volume represents a 
word sense. In this document, we use the term of 
?lexie? as in the Explanatory and Combinatory 
Dictionary to name a monolingual entry. The 
meaning of ?lexie? is not the same as ?lexeme?. 
A lexie is a complete monolingual entry. 
Figure 2. Illustration of Papillon's macrostructure. 
 
The interlingual volume gathers all the 
interlingual acceptions. An interlingual acception 
represents the union of word-senses or ?lexies? 
considered as ?equivalent? among different 
monolingual volumes. This equivalence is 
calculated from translation links. In this 
document, we use the term of ?axie? to name an 
interlingual acception. 
Real contrastive problems in lexical equivalence 
(not to be confused with monolingual polysemy, 
homonymy or synonymy as clearly explained in 
Mel'cuk and Wanner (2001) are 
handled by way of a special kind 
of link between axies. Figure 2 
illustrates this architecture using a 
classical example involving 
"Rice" in 4 languages. In this 
example, we used the word senses 
as given by the "Petit Robert" 
dictionary for French and the 
"Longman Dictionary of 
Contemporary English" for 
English. As shown, the French and 
English dictionaries do not make 
any word sense distinction 
between cooked and uncooked 
rice seeds. However, this 
distinction is clearly made in 
Japanese and Malay. No axie may 
be used to denote the union of the 
word senses for Malay "nasi" and 
"beras" unless we want to consider 
them as true synonyms in Malay 
(which would be false). Hence, we 
have to create 3 different axies: 
one for the union of "nasi" and ?
? (gohan), the other for the union 
of "beras" and ? (kome) and one 
for the union of "rice" and "riz". A 
link (non-continuous line in Figure 
1 has to be added between the third 
axies and the others in order to 
keep the translation equivalence 
between the word-senses. 
Note that the links between axies 
do not bear any particular 
semantics and should not be 
confused with some kind of 
ontological links. 
 
Bilingual dictionaries can be obtained from the 
multilingual dictionary. 
2.2 Microstructure 
The structure of the lexies (units of the 
monolingual dictionaries) is based on Polgu?re 
(2000) and Mel'cuk?s work on the combinatorial 
and explanatory lexicography, a part of the 
meaning-text theory. An XML schema using the 
DML framework has been defined to represent 
this structure as accurately as possible. 
This structure is common to all the monolingual 
dictionaries. In order to cope with language 
<lexie xmlns="http://www-clips.imag.fr/geta/services/dml" 
       xmlns:d="http://www-clips.imag.fr/geta/services/dml" 
       xmlns:xlink="http://www.w3.org/1999/xlink"  
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
       basic="true" d:id="meurtre$1" frequency="0.3"  
       name="Papillon-fra" source-language="fra"  
       ...> 
<headword hn="1">meurtre</headword> 
  <pronunciation encoding="GETA">meu+rtr(e)</pronunciation> 
  <pos>n.m.</pos> 
  <semantic-for action de tuer: ~ PAR L' mula>
    <sem-label>individu</sem-label><actor>X</actor> DE L' 
    <sem-label>individu</sem-label><actor>Y</actor> 
  </semantic-formula> 
  <government-pattern> 
   <mod nb="1"> 
    <actor> 
      <sem-actant>X</sem-actant><synt-actant>I</synt-actant> 
      <surface-group> 
       <surface>de N</surface> 
       <surface>A-poss</surface></surface-group></actor> 
    <actor> 
     <sem-actant>Y</sem-actant><synt-actant>II</synt-actant> 
      <surface-group> 
       <surface>de N surface> </
       <surface>A-poss</surface></surface-group></actor> 
   </mod></government-pattern> 
  <lexical-functions> 
   <function name="Qsyn"> 
     <valgroup> 
      <value> 
       <reflexie xlink:href="#assassinat$1">assassinat 
       </reflexie></value> 
     <value> 
      <reflexie xlink:href="#homicide$2">homicide</reflexie> 
     </value><value> 
      <reflexie xlink:href="#crime$1">crime</reflexie> 
     </value></valgroup></function> 
   <function name="V0"> 
    <valgroup> 
     <value><reflexie xlink:href="#tuer$1">tuer</reflexie> 
     </value></valgroup></function> 
    ... 
  </lexical-functions> 
  <examples> 
    <example d:id="#meurtre$1-e1"> 
    C'est ici que le double meurtre a ?t? commis.</example>   
    ...</examples> 
  ... 
</lexie> 
Figure 3. XML encoding of the French entry "meurtre" (excerpt) 
 
differences, small variations are authorised for 
each monolingual lexicon. Up to now, these 
variations have been used to define the parts of 
speech for each language and to add information 
specific to each language, such as level of 
politeness and counters for Japanese. 
Figure 3 presents an excerpt of the XML 
encoding of the French entry "meurtre" (murder) 
and Figure 4 shows a DEC-like view. 
The general schema has been presented in detail 
in Gilles S?rasset & Mathieu Mangeot-Lerebours 
(2001). 
3 Implementation of the 
Collaborative Web Site 
For the external user, the Papillon project is 
viewed as a dynamic web site providing access 
the existing dictionaries and giving ways to 
contribute to the Papillon dictionary. 
3.1 General Architecture 
The Papillon web site is built with a Java based 
open source framework called Enhydra2. It is 
designed around a standard 3-tier architecture 
a presentation layer in charge of the 
interface with the user. We currently use 
classical HTML/CSS rendering, but plan to 
integrate WML access to the dictionaries 
(for mobile phones), 
? 
a business layer in charge of data 
manipulation and transformation. We 
currently use XML data (in UTF-8) and 
XSL transformations for data manipulation, 
? 
a data layer in charge of the communication 
with the database via a JDBC driver. The 
data layer should be managed by an XML 
database allowing language dependent 
sorting. For the moment, XML databases are 
still in an early stage. In order to advance in 
the project, a mapping system for DML has 
been defined in order to store the XML data 
into conventional relational databases. 
PostgreSQL is used at this point. 
? 
3.2 Particular features 
As different users may have different needs 
(translators, learners?) we define different 
views of the Papillon dictionary. Each view is 
encoded as a XSL stylesheet that is applied on the 
result of each user query. In the future, we will 
also allow users to define their own custom views 
and store them on the server. All these 
transformations are done on the server in order to 
allow users to use their preferred browser (even if 
it is not XML aware). Figure 4 shows an example 
of the French entry "MEURTRE" (murder) 
viewed as in Mel'cuk's DEC dictionary. 
                                                     
2 available at www.enhydra.org 
 
 
Figure 4. French entry "meurtre" dynamically 
displayed using Mel'cuk's classical view 
 
To avoid the unintentional pollution of the 
database by erroneous data, the contributions of a 
user are to be validated by a central group of 
trusted users. In the mean time, the contributions 
are stored as XSL stylesheets in the cntributor?s 
private space.  
Each time a user requests a corresponding entry, 
the request is performed in the main database and 
in the user space. The results from the user space 
are used to modify results from the main database. 
This way, the contribution is immediately visible 
to the user exactly as if it had been integrated into 
the main database. 
While contributions are waiting to be validated 
and integrated into the common space, The 
contributors may choose to share them with other 
users or groups of users.  
Every user can contribute at his/her level. For 
example, a linguist specialist of lexical functions  
will enter values of lexico-semantic functions, a 
phonologist pronunciations and a professional 
bilingual translator will enter new interlingual 
links or check the semi-automatically generated 
ones. For this, different interfaces will be 
 
developed to accommodate the various user 
profiles. 
3.3 Annex Tools 
As the web site hosts a rather complex 
collaborative work, we have added some tools 
that are not related to lexicography, but that have 
to work in a multilingual context. 
First, there is a tool to archive our Papillon 
mailing list. Such a tool is very common on 
Internet sites. However, as we found out, these 
tools may not be used in our multilingual context, 
where mails may contain discussion in different 
languages, written with different tools, and 
encoded using different standards. Hence we 
patched an existing tool so that it archives all 
mail in UTF-8, regardless of its original 
encoding. 
To avoid the considerable work of the webmaster 
and to facilitate the communication and the 
exchange of informations between the users of 
the database, we are developing tools to facilitate 
the use of a document repository. 
After registration and login, users can easily 
upload online a file in whatever format. It will 
immediately be stored into the document 
repository and made accessible online on the 
web. 
4 Actual Research and Development 
Directions 
The Papillon project is a extremely interesting 
experimentation platform. We are currently 
working on validation of monolingual data, 
management of axies and acquisition of new 
data. 
4.1 Validation of the Monolingual Data 
A team of trusted lexicographers validates user 
contributions before they are integrated into the 
main database. 
This validation is a time consuming process and 
implies a good level in linguistics and 
lexicography. Moreover, we may not find enough 
specialists volunteering for such a work and we 
may have to pay a core team for this. 
This task is essential and should be conducted as 
quickly as possible lest the users will be 
discouraged by the delays implied by the central 
team. 
Hence, even in this validation process, we wish to 
enroll users as much as possible. For this task, we 
plan to implement tools for indirect validation of 
information using vote mechanisms and 
generating questions answerable without any 
special knowledge in linguistics. 
As a first experiment, we will use a French 
generator in order to produce a lot of examples 
using the word to be validated and a set of known 
words (already validated). These examples will 
be presented to native speakers and they will 
simply have to accept or reject them. This 
strategy is very interesting in our context, as it 
will help validating the lexical functions. 
4.2 Management of the Interlingual Links 
The use of a pivot dictionary to represent 
translation equivalence is challenging. This 
macrostructure is very satisfying on a theoretical 
level, but introduces a high complexity of 
management. 
In S?rasset (1994), we envisaged that these 
interlingual acceptions would be created and 
managed by hand by a team of specialists, helped 
by tools that would detect inconsistencies and 
propagate decisions among the different 
languages. This appeared to be unrealistic. 
However, we now have means to manage these 
acceptions automatically. For this, we use the fact 
that the interlingual acceptions volume does not, 
in any way, represent a semantic pivot. It is not 
related to an ontology. 
In fact, the only relevant purpose of this 
interlingual volume is to factorise the bilingual 
links we find in classical bilingual dictionaries 
(or the ones that will be specified by the users). 
Hence, given a set of translation equivalences 
between monolingual acceptions of different 
languages, it is possible to compute a minimal set 
of acceptions (and their links) that conforms to a 
set of well-formedness criteria. 
One of the difficult tasks is to obtain bilingual 
translation equivalences between monolingual 
acceptions when bilingual dictionaries often 
provide bilingual links between mere lemmas. 
For this, we will use aligned corpora and 
translations memories to add contextual 
information to the translation pairs. 
 
4.3 Acquisition of new data 
To depend entirely on volunteer work is of course 
unrealistic, especially while beginning to build 
the lexical database. That is why we first reuse 
existing dictionaries in order to build the kernelof 
the database.  
Contributors will come in later, filling in missing 
informationin existing entries and creating partial 
or complete new entries as well as links. 
However, as we are using a rather complex 
structure which require some skills that are not 
shared by all Internet users, we will have to help 
them help us. 
In particular, we are beginning to use 
corpus-based techniques to extract lemmas that 
will be candidates as a value of a lexical function. 
Determining the appropriate lexical function is 
one of the jobs of our contributors, but they will 
be helped in this task by tools that will provide 
them with questions and candidate paraphrases. 
For a complement of information or to help the 
contributors in their task, the database should 
also propose the consultation of other 
dictionaries stored locally or available online on 
the web. 
Moreover, to be really useful for the reader, and 
especially to the learners, the examples found in 
the dictionaries will be translated in other 
languages literally and semantically. Some of 
these translations will be extracted from aligned 
corpora. 
Conclusion 
The theoretical frameworks for the whole 
database, the macrostructure and the 
microstructure are very well defined. It 
constitutes a solid basis for the implementation. 
A lot of open problems still have to be addressed 
for the Papillon project to be a success. In this 
respect, the Papillon project appears to be a very 
interesting experimentation platform for a lot of 
NLP research as data acquisition or human access 
to lexical data, among others. 
All this research will improve the attraction of 
such a project to the Internet users. This attraction 
is necessary for the project to go on, as it is highly 
dependent on its users motivations. 
This way, we will be able to provide a very 
interesting multilingual lexical database that we 
hope useful for a lot of persons.  
References  
Vutichai Ampornaramveth, Akiko Aizawa, Keizo 
Oyama & Tanasee Methapisit (2000) An 
Internet-Based Collaborative Dictionary 
Development Project: SAIKAM., Proc. AdInfo 2000, 
9-10 March 2000, NACSIS, Tokyo, Japan, 4 p. 
Etienne Blanc (1999) PARAX-UNL: a Large Scale 
Hypertextual Multilingual Lexical Database. Proc. 
NLPRS 1999, Tsinghua University Press, Beijing, 
1999, pp. 507-510.  
Jim W. Breen (2001) A WWW Dictionary and Word 
Translator: Threat or Aid to Language Acquisition?, 
in R Gitsaki-Taylor and P Lewis (eds), Proc. 
JALT-CALL 2001, Gunma, Japan, 26-27 May 2001, 
10 pp. 
Ha? Doan-Nguyen (1998) Accumulation of Lexical 
Sets: Acquisition of Dictionary Resources and 
Production of New Lexical Sets. COLING-ACL'98, 
Montr?al, 10-14 August 1998, pp 330-335.  
Mathieu Mangeot-Lerebours (2001) Environnements 
centralis?s et distribu?s pour lexicographes et 
lexicologues en contexte multilingue. PhD Thesis in 
Computer Sciences Universit? Joseph Fourier 
Grenoble I, 27 September 2001, 280 p.  
Igor Melc?uk & Leo Wanner (2001) Towards a 
Lexicographic Approach to Lexical Transfer in 
Machine Translation (Illustrated by the 
German?Russian Language Pair). Machine 
Translation 16: 21?87, 2001. ? 2001 Kluwer 
Academic Publishers. Printed in the Netherlands. 
Alain Polgu?re (1998) La th?orie 
Sens-Texte .Dialangue, Vol. 8-9, Universit? du 
Qu?bec ? Chicoutimi, pp 9-30. 
Alain Polgu?re (2000) Towards a 
theoretically-motivated general public dictionary of 
semantic derivations and collocations for French. 
Proc. EURALEX'2000, Stuttgart, pp 517-527. 
Gilles S?rasset (1994) Interlingual Lexical 
Organisation for Multilingual Lexical Databases in 
NADIA. In Proc. COLING-94, Kyoto, 5-9 August 
1994, M. Nagao ed. vol. 1/2 : pp. 278-282. 
Gilles S?rasset & Mathieu Mangeot-Lerebours (2001) 
Papillon Lexical Database Project: Monolingual 
Dictionaries & Interlingual Links. Proc. 
NLPRS'2001, Hitotsubashi Memorial Hall, 
National Center of Sciences, Tokyo, Japan, 27-30 
November 2001, vol 1/1, pp. 119-125. 
Mutsuko Tomokiyo et al (2000) Papillon : a Project 
of Lexical Database for English, French and 
Japanese, using Interlingual Links. Journ?es 
Science et Technologie de l'ambassade de France au 
Japon, 13 November 2000, Tokyo, Japan, 3 p. 
 
The PAPILLON project: cooperatively building a multilingual lexical
data-base to derive open source dictionaries & lexicons
Christian BOITET(1), Mathieu MANGEOT(2) & Gilles S?RASSET(1)
 (1) GETA, CLIPS, IMAG
385, av. de la biblioth?que, BP 53
F-38041 Grenoble cedex 9, France
Christian.Boitet@imag.fr
(2) National Institute of Informatics (NII)
2-1-2-1314, Hitotsubashi
Chiyoda-ku Tokyo 101-8430, Japan
Mathieu.Mangeot@imag.fr
Abstract
The PAPILLON project aims at creating a cooperative, free, permanent, web-oriented and personalizable environment for the
development and the consultation of a multilingual lexical database. The initial motivation is the lack of dictionaries, both for
humans and machines, between French and many Asian languages. In particular, although there are large F-J paper usage
dictionaries, they are usable only by Japanese literates, as they never contain both original (kanji/kana) and romaji writing.
This applies as well to Thai, Vietnamese, Lao, etc.
Introduction
The project was initiated in 2000 and launched
with the support of the French Embassy and NII
(Tokyo) in July 2000, and took really shape in
2001, with a technical seminar in July 2001 at
Grenoble, and concrete work (data gathering,
tool building, etc.).
The macrostructure of Papillon is a set of
monolingual dictionaries (one for each language)
of word senses, called "lexies", linked through a
central set of interlingual links, called "axies".
This pivot macrostructure has been defined by
S?rasset (1994) and experimented by Blanc
(1994) in the PARAX mockup.
The microstructure of the monolingual
dictionaries is the "DiCo" structure, which is a
simplification of Mel'tchuk's (1981;1987;1995)
DEC (Explanatory and Combinatory
Dictionary) designed by Polgu?re (2000) &
Mel'tchuk !"# make it possible to construct
large, detailed and principled dictionaries in
tractable time.
1. Languages included in the project
In 2000, the initial languages of the Papillon
project were English, French, Japanese and Thai.
Thai was included because there had been a
successful project, SAIKAM (Ampornaramveth
et al 1998; 2000), supported by NII and
NECTEC, of building a Japanese-Thai lexicon by
volunteers on the web. Lao, Vietnamese, and
Malay have been added in 2001 because of active
interest of labs and individuals.
The star-like macrostructure of Papillon makes
it easy to add a new language. Also, the DiCo
microstructure of each monolingual dictionary is
defined by an XML schema, containing a large
common core and a small specialization part
(morphosyntactic categories, language usage).
2. Interlingual links
Axies, also called "interlingual acceptions", are
not concepts, but simply interlingual links
between lexies, motivated by translations found
in existing dictionaries or proposed by the
contributors.
In case of discrepancies, 1 axie may be linked
with lexies of some languages only, e.g.
FR(mur#1), EN(wall#1), RU(stena#1), and t o
other axies by refinement links:
Example:
Axie#234 --lg--> FR(mur#1),
EN(wall#1), RU(stena#1)
--rf--> Axie#235, Axie#236
Axie#235 --lg--> DE(Wand#2),
IT(muro#1), ES(muro#1)
Axie#236 --lg--> DE(Mauer#2),
IT(parete#1), ES(pared#1)
It is also possible to have 2 axies for the same
"concept" at a certain stage in the life of the
database, because the monolingual information is
not yet detailed enough.
Suppose the level of language (usual, specialized,
vulgar, familiar?) is not yet given for
FR(maladie#1), FR(affection#2), EN(disease#1),
EN(affection#3).
Then we might have, for translational reasons:
Axie#456
--lg-->
FR(maladie#1),
EN(disease#1)
Axie#457
--lg-->
FR(affection#2),
EN(affection#3)
When this information will be put in each of the
above 4 monolingual entries, we may merge the
2 axies and get:
Axie#500
--lg-->
FR(maladie#1, affection#2),
EN(disease#1, affection#3)
Axies may also be linked to "external" systems
of semantic description. Each axie contains a
(possibly empty) list for each such system, and
the list of systems is open. The following are
included at this stage; UNL UWs (universal
words), ONTOS concepts, WordNet synsets,
NTT semantic categories.
3. Building the content
3.1. Recuperating existing resources
Building the content of the data base has several
aspects. To initiate it, the project starts from
open source computerized data, called "raw
dictionaries", which may be monolingual (4,000
French DiCo entries from UdM, 10,000 Thai
entries from Kasetsart Univ.), bilingual (70,000
Japanese-English entries and 10,000 Japanese-
French entries in J.Breen's JDICT XML format,
8000 Japanese-Thai entries in SAIKAM XML
format, 120,000 English-Japanese entries in
KDD-KATE LISP format), or multilingual
(50,000 French-English-Malay entries in FEM
XML format).
3.2. Integrating the data into Papillon
In the second phase, the "raw dictionaries" are
transformed into a "lexical soup" in
M.Mangeot's (2001) intermediary DML format
(an XML schema and namespace). The
transformation into almost empty DiCo entries
and the creation of axies for the translational
equivalences is semi-automatic. A tool has been
programmed at NII for that task.
3.3. Enriching the data with contributions
After that, it is hoped that many contributors
will "fill in" the missing information. The basis
for that third and continuous phase is a server
for cooperative building of the data base, where
each contributor has his/her own space, so that
contributions can be validated and integrated
into the DB by a group experts. Users can
establish user groups with specific read and write
access rights on their spaces.
4. Consultation of the resulting data
4.1. Online consultation
Consultation is meant to be free for anybody,
and open source. Requests produce personalizable
views of the data base, the most classical of
which are fragments of bilingual dictionaries.
However, it is also possible to produce
multitarget entries, on the fly and offline. Users
(consumers) are encouraged to become
contributors. To contribute, one may propose a
new word sense, a definition, an example of use,
a translation, the translation of an example, a
correction, etc., or an annotation on any
accessible information: Every user can
contribute with his own knowledge level.
4.2. Download of entire files
Users can also retrieve files, and can contribute
to define new output formats. The files retrieved
can contain structural, content-oriented tags.
This open source orientation contrasts with the
current usage of allowing users to retrieve files
containing only presentation-oriented tags.
4.3. Coverage of the dictionary
An interesting point is that the project wants t o
cover both general terms and terminological
terms.
Another one is that it contains a translation
subproject, because definitions, examples,
citations, etc. have to be translated into all
languages of the collection. For this, the notion
of complex lexie, already present to account for
lexical collocations such as compound predicates
(e.g. "to kick the bucket"), is extended to cover
full sentences. Axies relating them are special
because they can't in general relate them t o
external semantic systems such as WordNet. An
exception is UNL: the UNL list for an axie may
contain one UNL graph, produced automatically,
manually, or semi-automatically.  This graph
may be automatically sent to available UNL
"deconverters" to get draft translations.
5. Project organisation
In the current stage, the project has no legal
implementation as a fundation, association,
company, etc., although many participants have
already established official MOUs and other
types of agreements on which to base their
cooperative work.
There is a steering committee of about 10-12
members, who represent Papillon where they
are, and not the converse. There is a set of
tasks, and for each task a working group and an
advisory committee. One of the tasks is the
management of the project. In between, there is
a coordinating group containing the heads of the
tasks and chaired by the head of the
management task.
Sponsors may not donate money to the project,
which has no bank account. Rather, they are
encouraged to donate data, to assign personal
part time to the project, and to fund
participating organizations and persons as they
see fit.
Conclusion
The theoretical frameworks for the whole
database, the macrostructure and the
microstructure are very well defined. I t
constitutes a solid basis for the implementation.
A lot of open problems still have to be addressed
for the Papillon project to be a success. In this
respect, the Papillon project appears to be a
very interesting experimentation platform for a
lot of NLP research as data acquisition or human
access to lexical data, among others.
All these research will improve the attraction of
such a project to the Internet users. This
attraction is necessary for the project to go on,
as it is highly dependent on its users
motivations.
This way, we will be able to provide a very
interesting multilingual lexical database that we
hope useful for a lot of persons.
Rerefences
Ampornaramveth V., Aizawa A. & Oyama K. (2000)
An Internet-based Collaborative Dictionary
Development Project: SAIKAM. Proc. of 7th Intl.
Workshop on Academic Information Networks and
Systems (WAINS'7), Bangkok, 7-8 December 2000,
Kasetsart University.
Blanc ?., S?rasset G. & Tch?ou F. (1994) Designing
an Acception-Based Multilingual Lexical Data Base
under HyperCard: PARAX. Research Report, GETA,
IMAG (UJF & CNRS), Aug. 1994, 10 p.
Connolly, Dan (1997) XML Principles, Tools and
Techniques World Wide Web Journal, Volume 2,
Issue 4, Fall 1997, O'REILLY & Associates, 250 p.
Ide, N. & Veronis, J. (1995) Text Encoding Initiative,
background and context. Kluwer Academic
Publishers, 242 p.
Mangeot-Lerebours M. (2000) Papillon Lexical
Database Project: Monolingual Dictionaries &
Interlingual Links. Proc. of 7th Workshop on
Advanced Information Network and System Pacific
Association for Computational Linguistics 1997
Conference (WAINS'7), Bangkok, Thailande, 7-8
d?cembre 2000, Kasetsart University, 6 p.
Mangeot-Lerebours M. (2001) Environnements
centralis?s et distribu?s pour lexicographes et
lexicologues en contexte multilingue. Nouvelle th?se,
Universit? Joseph Fourier (Grenoble I), 27 September
2001, 280 p.
Mel?tchuk I., Clas A. & Polgu?re A. (1995)
Introduction ? la lexicologie explicative et
combinatoire. AUPELF-UREF/Duculot, Louvain-la-
Neuve, 256 p.
Polgu?re, A. (2000) Towards a theoretically-motivated
general public dictionary of semantic derivations
and collocations for French. Proc. EURALEX'2000,
Stuttgart, pp 517-527.
S?rasset G. (1994a) Interlingual Lexical Organisation
for Multilingual Lexical Databases. Proc. of 15th
International Conference on Computational
Linguistics, COLING-94, 5-9 Aug. 1994, 6 p.
S?rasset G. (1994b) SUBLIM, un syst?me universel de
bases lexicales multilingues; et NADIA, sa
sp?cialisation aux bases lexicales interlingues par
acceptions. Nouvelle th?se, UJF (Grenoble 1), d?c.
1994.
S?rasset G. (1997) Le projet NADIA-DEC : vers un
dictionnaire explicatif et combinatoire informatis? ?
Proc. of La m?moire des mots, 5?me journ?es
scientifiques du r?seau LTT, Tunis, 25-27 septembre
1997, AUPELF?UREF, 7 p.
S?rasset G. & Mangeot-Lerebours M. (2001) Papillon
Lexical Database Project: Monolingual Dictionaries
& Interlingual Links. Proc. NLPRS'2001,
Hitotsubashi Memorial Hall, National Center of
Sciences, Tokyo, Japan, 27-30 November 2001, vol
1/1, pp. 119-125.
Tomokiyo M., Mangeot-Lerebours M. & Planas E.
(2000) Papillon : a Project of Lexical Database for
English, French and Japanese, using Interlingual
Links. Proc. of Journ?es des Sciences et Techniques
de l'Ambassade de France au Japon, Tokyo, Japon,
13-14 novembre 2000, Ambassade de France au
Japon, 3 p.
-o-o-o-o-o-o-o-o-o-
Online Generic Editing of Heterogeneous Dictionary Entries in
Papillon Project
Mathieu MANGEOT
Unit Terjemahan Melalui Komputer
Universiti Sains Malaysia,
11800, Pulau Pinang
Malaysia
mathieu@mangeot.org
David THEVENIN
National Institute of Informatics
Hitotsubashi 2-1-2-1913 Chiyoda-ku
JP-101-8430 Tokyo
Japan
thevenin@nii.ac.jp
Abstract
The Papillon project is a collaborative
project to establish a multilingual dictio-
nary on the Web. This project started 4
years ago with French and Japanese. The
partners are now also working on English,
Chinese, Lao, Malay, Thai and Vietnamese.
It aims to apply the LINUX cooperative
construction paradigm to establish a broad-
coverage multilingual dictionary. Users can
contribute directly on the server by adding
new data or correcting existing errors. Their
contributions are stored in the user space
until checked by a specialist before being
fully integrated into the database. The re-
sulting data is then publicly available and
freely distributable. An essential condition
for the success of the project is to find a
handy solution for all the participants to be
able to contribute online by editing dictio-
nary entries.In this paper, we describe our
solution for an online generic editor of dic-
tionary entries based on the description of
their structure.
1 Introduction
The Papillon Project (Se?rasset and Mangeot,
2001) is a cooperative project for a multilin-
gual dictionary on the Web with the following
languages: English, Chinese, French, Japanese,
Lao, Malay, Thai and Vietnamese. The dic-
tionary structure makes it very simple to add
a new language at any time. It aims to ap-
ply the LINUX construction paradigm to estab-
lish a multilingual usage dictionary with broad-
coverage.
This project is based on the participation of
voluntary contributors. In order to be really
attractive, this project must imperatively find
a convenient solution so that contributors can
easily edit the dictionary entries. Since Papillon
dictionary is available on a web server and the
contributors are located all around the world,
the obvious solution is to implement an editor
available online. Unfortunately, the existing so-
lutions (HTML forms, java applets) have impor-
tant limitations. Thus, we propose an entirely
generic solution that can adapt very easily not
only the interfaces to the various entry struc-
tures needing to be edited but also to the user
needs and competences.
Firstly, we outline the issue addressed in this
paper; and draw up an overview of the existing
methods for dictionary entry edition. A presen-
tation of the chosen method follows detailing
its integration in the Papillon server. Finally,
we show an example of the online edition of a
dictionary entry.
2 Addressed Issue and Requirements
In this paper, the addressed issue is how to
edit online dictionary entries with heteroge-
neous structures.
2.1 Online Edition
In order to build a multilingual dictionary that
covers a lot of languages, we need large compe-
tences in those languages. It may be possible
to find an expert with enough knowledge of 3
or 4 languages but when that number reaches
10 languages (like now), it is almost impossi-
ble. Thus, we need contributors from all over
the world.
Furthermore, in order to avoid pollution of
the database, we plan a two-step integration of
the contributions in the database. When a con-
tributor finishes a new contribution, it is stored
into his/her private user space until it is revised
by a specialist and integrated into the database.
Then, each data needs to be revised although
the revisers may not work in the same place of
the initial contributors.
Thus, the first requirement for the editor is
to work online on the Web.
2.2 Heterogeneous Entry Structures
The Papillon platform is built for generic pur-
poses. Thus, it can manipulate not only the
Papillon dictionary but also any kind of dictio-
nary encoded in XML (Mangeot, 2002). The
lexical data is organized in 3 layers:
? Limbo contains dictionaries in their origi-
nal format and structure;
? Purgatory contains dictionaries in their
original format but encoded in XML;
? Paradise contains the target dictionary, in
our case Papillon dictionary.
The Purgatory data can be reused for building
the Paradise dictionary.
We would like then to be able to edit different
dictionaries structures from Paradise but also
from Purgatory. Furthermore, being Papillon
a research project, entry structures may evolve
during the life of the project, since they are not
fixed from the beginning.
Hence, the second requirement is that the ed-
itor must deal with heterogeneous and evolving
entry structures.
2.3 Extra Requirements
Previous requirements must be fulfilled, whilst
the following ones are optional.
The contributors will have various compe-
tences and use the editor for different purposes
(a specialist in speech may add the pronuncia-
tion, a linguist may enter grammatical informa-
tion, a translator would like to add interlingual
links, and a reviewer will check the existing con-
tributions, etc.).
The second optional requirement concerns
the adaptation to the user platform. The
increasing number of smart mobile phones
and PDAs makes real the following scenarios:
adding an interlingual link with a mobile phone,
adding small parts of information with a PDA
and revising the whole entry with a workstation.
It would then be very convenient if the editor
could adapt itself both to the user and to the
platform.
2.4 Final Aim
Guided by these requirements, our final aim is
to generate, as much automatically as possible,
online interfaces for editing dictionary entries.
It has to be taken into account the fact that en-
try structures are heterogeneous and may vary
and to try to adapt as much as possible these in-
terfaces to the different kinds of users and plat-
forms.
3 Overview of Existing Editing
Methods
3.1 Local and Ad Hoc
The best way to implement a most comfort-
able editor for the users is to implement an ad-
hoc application like the one developed for the
NADIA-DEC project: DECID (Se?rasset, 1997).
It was conceived to edit entries for the ECD
(Mel?c?uk et al, 1984889296). The Papillon mi-
crostructure is based on a simplification of this
structure. We were indeed very interested by
such software. It is very convenient - for exam-
ple - for editing complex lexical functions.
But several drawbacks made it impossible to
use in our project. First, the editor was de-
veloped ad hoc for a particular entry structure.
If we want to change that structure, we must
reimplement changes in the editor.
Second, the editor is platform-dependent
(here written and compiled for MacOs). The
users have to work locally and cannot contribute
online.
3.2 Distributed and Democratic
This solution implemented for the construc-
tion of the French-UNL dictionary (Se?rasset and
Mangeot, 1998) project is called ?democratic?
because it uses common and widespread appli-
cations (works on Windows and MacOs) such
as Microsoft Word.
The first step is to prepare pre-existing data
on the server (implemented here in Macintosh
Common Lisp). Then, the data is converted
into rtf by using a different Word style for each
part of information (the style ?headword? for
the headword, the style ?pos? for the part-of-
speech, etc.) and exported. The clients can
open the resulting rtf files locally with their
Word and edit the entries. Finally, the Word
rtf files are reintegrated into the database via a
reverse conversion program.
This solution leads to the construction of
20,000 entries with 50,000 word senses. It was
considered as a very convenient method, never-
theless, two important drawbacks prevented us
to reuse this solution. The first is that in or-
der to convert easily from the database to rtf
and vice-versa, the dictionary entry structure
cannot be too complex. Furthermore, when the
user edits the entry with Word, it is very dif-
ficult to control the syntax of the entry, even
if some Word macros can partially remedy this
problem.
The second is the communication between the
users and the database. The Word files have
to be sent to the users, for example via email.
It introduces inevitably some delay. Further-
more, during the time when the file is stored
on the user machine, no other user can edit the
contents of the file. It was also observed that
sometimes, users abandon their job and forget
to send their files back to the server.
3.3 Online and HTML Forms
In order to work online, we should then use ei-
ther HTML forms, or a Java applet. The use of
HTML forms is interesting at a first glance, be-
cause the implementation is fast and all HTML
browsers can use HTML forms.
On the other hand, the simplicity of the forms
leads to important limitations. The only exist-
ing interactors are: buttons, textboxes, pop-up
menus, and checkboxes.
JavaScripts offer the possibility to enrich the
interactors by verifying for example the content
of a textbox, etc. However, very often they raise
compatibility problems and only some browsers
can interpret them correctly. Thus, we will
avoid them as much as possible.
One of the major drawbacks of this solution
is our need to modify the source code of the
HTML form each time we want to modify the
entry structure. We also need to write as many
HTML forms as there are different entry struc-
tures.
3.4 Online and Java Applets
In order to remedy the limitations of the HTML
forms and to continue to work online, there is
the possibility to use a java applet that will be
executed on the client side. Theoretically, it
is possible to develop an ad hoc editor for any
complicated structure, like the 3.1 solution.
Nevertheless, the problems linked to the use
of a java applet are numerous: the client ma-
chine must have java installed, and it must be
the same java version of the applet. Further-
more, the execution is made on the client ma-
chine, which can be problematic for not very
powerful machines. Moreover, nowadays there
is a strong decrease of java applets usage on the
Web mainly due to the previous compatibility
problems.
3.5 Conclusion
As a result, none of these existing solutions can
fully fulfil our requirements: online edition and
heterogeneous entry structures. We might then
use other approaches that are more generic like
the ones used in interface conception in order
to build our editor. In the remainder of this
paper, we will detail how we used an interface
generation module in Papillon server in order to
generate semi-automatically editing interfaces.
4 Using an Interface Generation
Module
This Papillon module has to generate graphic
user interfaces for consulting and editing dic-
tionary entries. We base our approach on
the work done on Plasticity of User interfaces
(Thevenin and Coutaz, 1999) and the tool ART-
Studio (Calvary et al, 2001). They propose
frameworks and mechanisms to generate semi-
automatically graphic user interfaces for differ-
ent targets. Below we present the design frame-
work and models used.
4.1 Framework for the UI generation
Our approach (Calvary et al, 2002) is based
on four-generation steps (Figure 1). The first
is a manual design for producing initial mod-
els. It includes the application description with
the data, tasks and instances models, and the
description of the context of use. This latter
generally includes the platform where the inter-
action is done, the user who interacts and the
environment where the user is. In our case we
do not describe the environment, since it is too
difficult and not really pertinent for Papillon.
From there, we are able to generate the Abstract
User Interface (AUI). This is a platform inde-
pendent UI. It represents the basic structure of
the dialogue between a user and a computer. In
the third step, we generate the Concrete User
Interface (CUI) based on the Abstract User In-
terface (AUI). It is an instantiation of the AUI
for a given platform. Once the interactor (wid-
get) and the navigation in UI have been chosen,
it is a prototype of the executable UI. The last
stage is the generation of Final User Interface
(FUI). This is the same as concrete user inter-
face (CUI) but it can be executed.
We will now focus on some models that de-
scribe the application.
4.2 Application Models: Data & Task
The Data model describes the concepts that the
user manipulates in any context of use. When
considering plasticity issues, the data model
should cover all usage contexts, envisioned for
the interactive system. By doing so, designers
obtain a global reusable reference model that
can be specialized according to user needs or
Task Concept
Instance
Abstract UI
Concrete
 UI
Concrete
 UI
Final
 UI
Final
 UI
Platform
User
Environment
Platform
User
Environment
Initial description
Transit description
Final description
Figure 1: Multitarget Generation Framework
C_entry
C_head 
word
C_pos
C_list
examples
C_example
I_head 
word
PopUp Menu
I_pos
example1
example2
example3
List
?
I_list
examples
I_entry
TextBox
I_
examples
Legend:
Concept
Instance
Link to the interactor used by the concept
Link to a child concept
Link to the instance
TextBox
Figure 2: Data Model Structure
more generally to context of use. A similar de-
sign rationale holds for tasks modeling. For the
Papillon project, the description of data model
corresponds to the XML Schema description
of dictionary and request manipulation. The
tasks? model is the set of all tasks that will be
implemented independently of the type of user.
It includes modification of the lexical database
and visualization of dictionaries.
As showed on Figure 2, the model of concepts
will drive the choice of interactors and the struc-
ture of the interface.
4.3 Instance Model
It describes instances of the concepts manipu-
lated by the user interface and the dependence
graph between them. For example there is the
concept ?Entry? and one of its instances ?sci-
entifique?. (cf. Figure 3).
I_head
word
I_pos
I_examples 
list
I_entry
I_example I_example
scientifique adj
journ?es
scientifiques
journal
scientifique
<entry><hv>scientifique</hv>
<pos>adj</pos>
<ex>journ?es scientifiques</ex>
<ex>journal scientifique</ex></entry>
<ex>journ?es scientifiques</ex>
<ex>journal scientifique</ex>
Legend:
Instance
Link to the instance value
Link to a child instance
Figure 3: Relation Between the XML Entry and
its Corresponding Instance
This model is described at design time, be-
fore generation, and linked with the task model
(a task uses a set of instances). Each instance
will be effectively created at run-time with data
coming from the Papillon database.
4.4 Platform and Interactors Models
A platform is described by interaction capacity
(for example, screen size, mouse or pen, key-
board, speech recognition, etc.). These capaci-
ties will influence the choice of interactors, pre-
sentation layouts or the navigation in the user
interface.
Associated to the platform there are the inter-
actors (widgets) proposed by the graphic tools-
box of the targeted language (for example Swing
or AWT for Java). In this project interac-
tors are coming from HMTL Forms (textBox,
comboBox, popup menu, button, checkBox, ra-
dioButton) and HTML tags. We also had to
build more complex interactors by a combina-
tion of HTML Forms and HTML Tags.
4.5 User Model
Previous research has shown the difficulty to de-
scribe the cognitive aspects of user behavior.
Therefore, we will simplify by defining differ-
ent user classes (tourist, student, business man,
etc.). Each class will be consisting of a set of de-
sign preferences. Depending on the target class,
the generator will use appropriate design rules.
The model is not yet implemented; it is im-
plicitly used in the data & task models. We
defined different views of data according to the
target:
? all data is rendered for the workstation
editing interface for lexicographers,
? only headword and grammatical class are
rendered and examples are browsable on
the mobile phone interface for a ?normal?
dictionary user.
4.6 Concrete User Interface Model
This model, based on an independent user in-
terface language, describes the graphic user in-
terface, as the final device will render it. It is
target-dependent.
4.7 Final User Interface
From the CUI model, the generator produces
a final interface that will be executed by the
targeted device, and links it with the Papillon
database. In our case we produce:
? HTML code for the workstation,
Figure 4: Generated GUI
? Tiny XHTML code for AU mobile phones,
? and CGI links for the communication with
the database.
Figure 4 shows a simple example of a final
generated UI.
5 Integrating the Module in
Papillon Server
5.1 Implementation
The Papillon server is based on Enhydra, a web
server of Java dynamic objects. The data is
stored as XML objects into an SQL database:
PostgresQL.
ARTStudio tool is entirely written in Java.
For its integration into the Papillon/Enhydra
server, we created a java archive for the codes
to stay independent.
The Papillon/Enhydra server can store java
objects during a user session. When the user
connects to the Papillon server with a browser,
a session is created and the user is identified
thanks to a cookie. When the user opens the
dictionary entry editor, the java objects needed
for the editor will be kept until the end of the
session.
5.2 A Working Session
When the editor is launched, the models cor-
responding to the entry structure are loaded.
Then, if an entry is given as a parameter (edit-
ing an existing entry), the entry template is in-
stantiated with the data contained in that entry.
If no entry is given, the template is instantiated
with an empty entry. Finally, the instantiated
models and entry templates are stored into the
session data and the result is displayed embed-
ded in an HTML form, through a Web page
(Figure 4).
Then, after a user modification (e.g. adding
an item to the examples list), the HTML form
entry
head
word
pos example example
scientifique adj journ?es
scientifiques
journal
scientifique
Legend:
XML
Element
Link to a child element
Link to the element valuetextual content
Figure 5: Abstract View of an Entry
sends the data to the server via a CGI mecha-
nism. The server updates the models and tem-
plate stored in the session data and sends back
the modified result in the HTML page.
At the end of the session, the modified entry is
extracted from the session data and then stored
as a contribution in the database.
6 An Editing Example
6.1 A Dictionary Entry
Figure 5 shows an abstract view of a simple dic-
tionary entry. It is the entry ?scientifique? (sci-
entific) of a French monolingual dictionary. The
entry has been simplified on purpose. The en-
tries are stored as XML text into the database.
6.2 Entry Structure
The generation of the graphic interface is mostly
based on the dictionary microstructure. In the
Papillon project, we describe them with XML
schemata. We chose XML schemata instead of
DTDs because they allow for a more precise
description of the data structure and handled
types. For example, it is possible to describe the
textual content of an XML element as a closed
value list. In this example, the French part-of-
speech type is a closed list of ?nom?, ?verb?,
and ?adj?.
Figure 6 is an abstract view of the structure
corresponding to the previous French monolin-
gual dictionary entry.
6.3 Entry Displayed in the Editor
The dictionary entry of Figure 5 is displayed in
the HTML editor as in Figure 4. In the follow-
ing one (Figure 7), an example has been added
in the list by pushing the + button.
6.4 A More Complex Entry
In the following figure (Figure 8), we show the
entry ?y? (taberu, to eat) of the Papillon
Japanese monolingual volume. The entry struc-
ture comes from the DiCo structure (Polgue`re,
entry
head 
word
pos example
Occurences: 0 to ?Occurence: 1Occurence: 1
Legend:
XML 
Element
Type
Link to a child element
Link to the element type
text
list:
 nom
 verbe
 adj
text
Figure 6: Structure of an Entry
Figure 7: Entry Displayed in the Editor
2000), a light simplification of the ECD by
Mel?c?uk & al.
Two interesting points may be highlighted.
You can note that not only the content of the
entry is in Japanese, but also the text labels
of the information. For example, the first one,
??W? (midashigo) means headword. The
interface generator is multitarget: it generates
the whole HTML content. It is then possible to
redefine the labels for each language.
The second point is the complexity of the en-
try structure. There is a list of lexical functions.
Each lexical function consists of a name and a
list of valgroups (group of values), and in turn,
each valgroup consists of a list of values. Finally,
each value is a textbox. The lists are nested the
one in the other one and it is possible to use the
lists + and - operators at any level.
7 Evaluation
7.1 Preamble
This paper focuses on one particular function-
ality of the Papillon platform: the generic edi-
tor. Its purpose is not to present the building of
Papillon dictionary or the progress of Papillon
Project as a whole. The evaluation will then
focus on the editor.
Figure 8: Papillon Entry Displayed in the Edi-
tor
The contribution phase on Papillon project
has not begun yet. Thus, for the moment, very
few users tested the editor. We have not yet
enough data to evaluate seriously the usability
of the interface. Then, the evaluation will be
driven on the technical aspects of the editor.
In order to evaluate the genericity and the
usability of the editor, we generated interfaces
for two other dictionary structures: the GDEF
Estonian-French dictionary and the WaDoku-
JiTen Japanese-German dictionary.
7.2 Edition of the GDEF dictionary
The GDEF project (Big Estonian-French Dic-
tionary) is managed by Antoine Chalvin from
INALCO, Paris. The dictionary microstructure
is radically different from the Papillon dictio-
nary as you will see in Figure 9 compared to
figure 8. You may notice the 6 levels of recur-
sion embedded in the entry structure.
It took about one week to write the interface
description files for the new dictionary structure
in order to generate properly a complete inter-
face for the GDEF dictionary.
7.3 Edition of the WaDokuJiTen
The WaDokuJiTen project is managed by Ul-
rich Apel, now invited researcher at NII, Tokyo.
The dictionary is originally stored in a File-
Maker database. It has more than 200,000 en-
tries. It took four days to export integrate the
dictionary in Papillon platform and to write the
Figure 9: GDEF Entry Displayed in the Editor
Figure 10: WaDokuJiTen Entry Displayed in
the Editor
files needed for the generation of the editor in-
terface. The integration was done in 4 steps:
export the dictionary from FileMaker into an
XML file, tag the implicit structure with a perl
script, write the metadata files and upload the
dictionary on the Papillon server.
The dictionary microstructure is simpler than
the previous one (see figure 10). It took only two
days to write the files needed for the generation
of the editor interface.
8 Conclusion
The implementation of ARTStudio and Papil-
lon plateform started separately four years ago.
The development of the HTML generation mod-
ule in ARTStudio and its integration into Papil-
lon platform took about a year from the first
specifications and the installation of a com-
plete and functional version on the Papillon
server. The collaboration between a specialist of
computational lexicography and a specialist of
the adaptability of interfaces has produced very
original and interesting work. Furthermore, the
evaluation and the feedback received from the
users is very positive. Now, we want to further
pursue this work following several paths.
First of all, only a specialist can use the ex-
isting interface for Papillon entry since it is too
complex for a beginner. We plan to generate dif-
ferent interface types adapted to the varied user
needs and competences. Thanks to the modu-
larity of the editor, we need only to describe the
tasks and instance models corresponding to the
desired interface.
For the moment, the interface generation is
not fully automatic; some of the model descrip-
tions used by the editor have to be written ?by
hand?. This is why we are working now on
automating the whole generation process and
the implementation of graphical editors allow-
ing users to post-edit or modify a generated in-
terface description.
References
Gae?lle Calvary, Joe?lle Coutaz, and David Thevenin. 2001.
Unifying reference framework for the development of plas-
tic user interfaces. In EHCI?01, IFIP WG2.7 (13.2) Work-
ing Conference, pages 173?192, Toronto, Canada.
Gae?lle Calvary, Joe?lle Coutaz, David Thevenin, Quentin Lim-
bourg, Nathalie Souchon, Laurent Bouillon, and Jean
Vanderdonckt. 2002. Plasticity of user interfaces: A re-
vised reference framework. In Proc. TAMODIA 2002,
pages 127?134, Bucharest, Romania. INFOREC Publish-
ing House.
Mathieu Mangeot. 2002. An xml markup language frame-
work for lexical databases environments: the dictionary
markup language. In International Standards of Termi-
nology and Language Resources Management, pages 37?
44, Las Palmas, Spain, May.
Igor Mel?c?uk, Nadia Arbatchewsky-Jumarie, Le?o Eltnisky,
Lidija Iordanskaja, Ade`le Lessard, Suzanne Mantha, and
Alain Polgue`re. 1984,88,92,96. DEC : Dictionnaire expli-
catif et combinatoire du franc?ais contemporain, recherches
lexico-se?mantiques I,II,III et IV. Presses de l?universite? de
Montre?al, Canada.
Alain Polgue`re. 2000. Towards a theoretically-motivated gen-
eral public dictionary of semantic derivations and col-
locations for french. In Proceeding of EURALEX?2000,
Stuttgart, pages 517?527.
Gilles Se?rasset and Mathieu Mangeot. 1998. L?e?dition lexi-
cographique dans un syste`me ge?ne?rique de gestion de bases
lexicales multilingues. In NLP-IA, volume 1, pages 110?
116, Moncton, Canada.
Gilles Se?rasset and Mathieu Mangeot. 2001. Papillon lexical
database project: Monolingual dictionaries and interlin-
gual links. In NLPRS-2001, pages 119?125, Tokyo, 27-30
November.
Gilles Se?rasset. 1997. Le projet nadia-dec : vers un diction-
naire explicatif et combinatoire informatise? ? In LTT?97,
volume 1, pages 149?160, Tunis, Septembre. Actualite? sci-
entifique, AUPELF-UREF.
David Thevenin and Joe?lle Coutaz. 1999. Plasticity of user
interfaces: Framework and research agenda. In Interact?99
Seventh IFIP Conference on Human-Computer Interac-
tion, volume 1, pages 110?117, Edinburgh, Scotland.
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 19?27,
Beijing, August 2010
Multilingual Lexical Network from the Archives of the Digital 
Silk Road 
Mohammad Daoud  
LIG, GETALP 
Universit? Joseph Fourier 
Mohammad.Daoud@imag.fr 
Kyo Kageura 
Graduate School of Education 
The University of Tokyo  
kyo@p.u-tokyo.ac.jp 
Christian Boitet 
LIG, GETALP 
Universit? Joseph Fourier 
Christian.Boitet@imag.fr 
Asanobu Kitamoto 
The National Institute of Informat-
ics (Tokyo) 
Kitamoto@nii.ac.jp 
Mathieu Mangeot 
LIG, GETALP 
Universit? Joseph Fourier 
Mathieu.Mangeot@imag.fr 
 
 
Abstract 
We are describing the construction 
process of a specialized multilingual 
lexical resource dedicated for the ar-
chive of the Digital Silk Road DSR. The 
DSR project creates digital archives of 
cultural heritage along the historical Silk 
Road; more than 116 of basic references 
on Silk Road have been digitized and 
made available online. These books are 
written in various languages and attract 
people from different linguistic back-
ground, therefore, we are trying to build 
a multilingual repository for the termi-
nology of the DSR to help its users, and 
increase the accessibility of these books. 
The construction of a terminological da-
tabase using a classical approach is dif-
ficult and expensive. Instead, we are in-
troducing specialized lexical resources 
that can be constructed by the commu-
nity and its resources; we call it Multi-
lingual Preterminological Graphs 
MPGs.  We build such graphs by ana-
lyzing the access log files of the website 
of the Digital Silk Road. We aim at 
making this graph as a seed repository 
so multilingual volunteers can contrib-
ute.  We have used the access log files 
of the DSR since its beginning in 2003, 
and obtained an initial graph of around 
116,000 terms. As an application, We 
have used this graph to obtain a preter-
minological multilingual database that 
has a number of applications. 
1 Introduction 
This paper describes the design and develop-
ment of a specialized multilingual lexical re-
source for the archive constructed and main-
tained by the Digital Silk Road project. The 
Digital Silk Road project (NII 2003) is an initia-
tive started by the National Institute of Infor-
matics (Tokyo/Japan) in 2002, to archive cul-
tural historical resources along the Silk Road, 
by digitizing them and making them available 
and accessible online.  
One of the most important sub-projects is the 
Digital Archive of Toyo Bunko Rare Books 
(NII 2008) where 116 (30,091 pages) of old rare 
books available at Toyo Bunko library have 
been digitized using OCR (Optical Character 
Recognition) technology. The digitized collec-
tion contains books from nine languages includ-
ing English. The website of the project attracts 
visitors from the domain of history, archeology, 
and people who are interested in cultural heri-
tage. It provides services of reading and search-
ing the books of Toyo Bunko, along with vari-
ety of services. Table 1 shows the countries 
from which DSR is being accessed. The table 
19
shows that around 60% of visitors are coming 
from countries other than Japan. The diversity 
of the visitors? linguistic backgrounds suggests 
two things: 1) Monolingual translation service is 
not enough. 2) It shows that we can benefit from 
allowing them to contribute to a multilingual 
repository. So we design and build a collabora-
tive multilingual terminological database and 
seed using the DSR project and its resources 
(Daoud, Kitamoto et al 2008). However, Dis-
covering and translating domain specific termi-
nology is a very complicated and expensive 
task, because (1) traditionally, it depends on 
human terminologists (Cabre and Sager 1999) 
which increases the cost, (2) terminology is dy-
namic (Kageura 2002), thousands of terms are 
coined each year, and (3) it is difficult to in-
volve domain experts in the construction proc-
ess. That will not only increase the cost, but it 
will reduce the quality, and the coverage (num-
ber of languages and size). Databases like (UN-
Geo 2002; IATE 2008; UN 2008) are built by 
huge organizations, and it is difficult for a 
smaller community to produce its own multilin-
gual terminological database. 
Country Visitors language Books in the same language  
Japan 117782 JA 2 books 
China 30379 CH 5 books 
USA 15626 EN 44 books 
Germany 8595 GE 14 books 
Spain 7076 SP - 
Australia 5239 EN See USA  
  Italy  4136 IT 1 book 
  France  3875 FR 14 books 
  Poland  2236  PO - 
  Russia  1895  RU 7 books 
other  87573 Other There are many books in 
different language 
Total 284412 
Table 1. Countries of the DSR visitors (from 
jan/2007 to dec/2008) 
In the next section we will give definitions 
for the basic concepts presented in this article, 
in particular, the preterminology and its lexical 
network (graph). Then, in the third section we 
will show the automatic approach to seed the 
multilingual preterminological graph based on 
the resources of the DSR. And then, we will 
discuss the human involvement in the develop-
ment of such a resource by providing a study of 
the possible contributors through analyzing the 
multilinguality and loyalty of the DSR visitors. 
In the fifth section we will show the experimen-
tal results. And finally, we will draw some con-
clusions.   
2 Multilingual Preterminological 
Graphs 
2.1 Preterminology 
Terminological sphere of a domain is the set of 
terms related to that domain. A smaller set of 
that sphere is well documented and available in 
dictionaries and terminological databases such 
as (FAO 2008; IEC 2008; IDRC 2009)... How-
ever, the majority of terms are not multilingual-
ized, nor stored into a database, even though, 
they may be used and translated by the commu-
nity and domain experts. This situation is shown 
in Figure 1, where the majority of terms are in 
area B. Preterminological sphere (area B) of a 
domain is a set of terms (preterms) related to 
the domain and used by the community but it 
might not be documented and included in tradi-
tional lexical databases. 
Multilingual Terminological Sphere
Preterminology
MTDB
B
A
C
 
Figure  1. Preterminological sphere 
Every year thousands of terms are coined and 
introduced in correspondence to new concepts, 
scientific discoveries or social needs. Most of 
these terms are produced in the top dominant 
languages, i.e. English. Interested people from 
different linguistic backgrounds would find 
suitable translations to new terms and use it 
amongst them. For example, the term ?status 
update? is used by people who visit social net-
working websites like facebook.com. Transla-
tion of this term to Arabic might not be avail-
able in area A of Figure 1. However the Arabic 
community found a translation that is acceptable 
which is  ????? ??????. So this term is in the area B. 
We are trying to use what is in area A, and what 
can be contributed from B to build preterminol-
ogy (Daoud, Boitet et al 2009).  
20
2.2 Structure of MPG 
We are building preterminological resource as a 
lexical network (graph) to handle the diversity 
of the resources that we use. A multilingual pre-
terminological graph MPG(N,E) is a finite non-
empty set N={n1,n2, ?} of objects called 
Nodes together with a set E={e1,e2, ?} of un-
ordered pairs of distinct nodes of MPG called 
edges. This definition is based on the general 
definition of a graph at the following references 
(Even 1979; Loerch 2000).  MPG of domain X, 
contains possible multilingual terms related to 
that domain connected to each other with rela-
tions. A multilingual lexical unit and its transla-
tions in different languages are represented as 
connected nodes with labels.  
In an MPG the set of nodes N consists of p,l, 
s, occ, where p is the string of the preterm, l is 
the language, s is the code of the first source of 
the preterm, and occ is the number of occur-
rences. Note that l could be undefined. For ex-
ample: N={[silk road, en, log],[Great Wall of China, en, 
,wikipedia, 5], [?????, ar, contributorx,6]}, here we have 
three nodes, 2 of them are English and one in 
Arabic, each term came from a different source. 
Note that English and Arabic terms belong to 
the same N thus, the same MPG. 
An Edge e={n, v} is a pair of nodes adjacent in 
an MPG. An edge represents a relation between 
two preterms represented by their nodes. The 
nature of the relation varies. However, edges are 
weighted with several weights (described be-
low) to indicate the possible nature of this rela-
tion. 
The following are the weights that label the 
edges on an MPG: Relation Weights rw: For an 
edge e={[p1,l1,s1], [p2,l2,s2]}, rw indicates 
that there is a relation between the preterm p1 
and p2. The nature of the relation could not be 
assumed by rw. Translation Weights tw: For an 
edge e={[p1,l1,s1], [p2,l2,s2]}, tw suggests that 
p1 in language l1 is a translation of p2 in lan-
guage l2. Synonym Weights sw: For an edge 
e={[p1,l1,s1], [p2,l1,s2]}, sw suggests that p1 
and p2 are synonyms. 
3 Automatic Initialization of DSR-
MPG  
Basically we seeded DSR-MPG, through two 
steps, the firs one is the automatic seeding, 
which consists of the following: 1) Initialization 
by finding interesting terms used to search the 
website of the DSR. 2) Multilingualization, us-
ing online resources. 3) Graph Expansion using 
the structure of the graph it self. The second 
step is the progressive enhancement, by receiv-
ing contributions from users, through set of use-
ful applications. In this section we will discuss 
the first three steps. In section 4, we will discuss 
the human factor in the development of DSR-
MPG. 
3.1 Analyzing Access Log Files 
We analyze two kinds of access requests that 
can provide us with information to enrich the 
MPG: (1) requests made to the local search en-
gine of DSR (2) requests from web-based 
search engine (like Google, Yahoo!?). These 
requests provide the search terms that visitors 
used to access the website. Moreover, we can 
understand the way users interpret a concept 
into lexical units. For example, if we find that 
five different users send two search requests t1 
and t2, then there is a possibility that t1 and t2 
have a relation. The graph constructor analyzes 
the requests to make the initial graph by creat-
ing edges between terms in the same session. 
rw(x,y), is set to the number of sessions contain-
ing x and y within the log file. 
For example, rw(x,y) = 10 means that 10 
people thought about x and y within the same 
search session. Figure 2 shows an example of a 
produced graph. The method did not discover 
the kind of relation between the terms. But it 
discovered that there is a relation, for example, 
three users requested results for ?yang? fol-
lowed by ?yin? within the same session. Hence, 
edge with weight of 2 was constructed based on 
this. 
21
 Figure  2. Example of constructing an MPG 
from an access log file 
3.2 Multilingualization Using Online Re-
sources 
Many researchers focused on the usage of dic-
tionaries in digital format to translate lexical 
resources automatically (Gopestake, Briscoe et 
al. 1994) (Etzioni, Reiter et al 2007). We are 
concerned with the automatic utilization of 
these resources to acquire multilingual preter-
minological resources through the following: 1) 
Wikipedia 2) online MT systems 3) online dic-
tionaries. 
Wikipedia (Wikipedia-A 2008) is a rich 
source of preterminology, it has good linguistic 
and lexical coverage. As of December, 2009, 
there are 279 Wikipedias in different languages, 
and 14,675,872 articles. There are 29 Wikipe-
dias with more that 100000 articles and 91 lan-
guages have more than 10,000 articles. Beside, 
Wikipedia is built by domain experts. We ex-
ploit the structure of Wikipedia to seed an 
MPG, by selecting a root set of terms, for each 
one of them we fetch its wikipedia article, and 
then we use the language roll of the article. For 
example, we fetch the article (Cuneiform script) 
En: http://en.wikipedia.org/wiki/Cuneiform_script, to reach its 
translation in Arabic from this url:  
http://ar.wikipedia.org/wiki/ ???????_?????  
We use also online machine translation sys-
tems as general purpose MRDs. One of the 
main advantages of MT systems is the good 
coverage even for multiword terms. The agree-
ment of some MT systems with other resources 
on the translation of one term enhanced the con-
fidence of the translation. Another positive 
point is that the results of MT provide a first 
draft to be post edited later. We used 3 MT sys-
tems: 
? Google Translate (Google 2008) (50 
languages) 
? Systran (Systran 2009) (14 languages) 
? Babylon (Babylon 2009) (26 languages) 
Here is an example of translating the term 
?great wall of China? into Arabic. 
 
Figure  3. MPG sample nodes 
In a similar way, we used several online re-
positories; to make good use of what is avail-
able and standardized, to initializing the MPG 
with various resources, and to construct a meta-
system to call online dictionaries automatically. 
We used IATE (IATE 2008)  as an example of a 
terminological db, and Google dictionary 
(Google 2008). The concept is similar to the 
concept of using online translations, where we 
construct an http request, to receive the result as 
html page. 
3.3 Graph Expansion 
 And then, the Graph is expanded by finding the 
synonyms according to formula (1) described at 
(Daoud, Boitet et al 2009). After finding syno-
nyms we assume that synonyms share the same 
translations. As Figure 4 shows, X1 and X2 have 
translations overlaps, and relatively high rw, so 
that suggest they are synonyms. Therefore we 
constructed heuristic edges between the transla-
tions of X1 and X2. 
Systran 
wight=1 
Wikipedia 
Google 
 Babylon 
wight=3 
great wall 
of China 
 ??? ?????
?????? 
 ?????? ????
????? 
22
 Figure  4. Graph expansion 
4 Human Involvement in the Develop-
ment of DSR-MPG 
After initializing the graph, we target contribu-
tions from the visitors to the DSR website. In 
this section we will start by analyzing the possi-
bility of receiving contributions from the visi-
tors, and then we will introduce some useful 
applications on the DSR-MPG that can help the 
visitors and attract them to get involved. 
4.1 Analyzing Possible Contributors of the 
DSR 
We are trying to analyze access log files to find 
out the possible contributors to a pretermi-
nological multilingual graph dedicated to an 
online community. This kind of information is 
necessary for the following reasons: 1) it pro-
vide feasibility analysis predicting the possibil-
ity of receiving contribution to a multilingual 
preterminological repository. 2) it gives infor-
mation that can be used by the collaborative 
environment to personalize the contribution 
process for those who prove to be able to con-
tribute. 
In the analysis process we are using the fol-
lowing information that can be easily extracted 
the access records: 
? Key terms to access the historical resources of 
the Digital Silk Road, whether it is the local 
search engine, or any external search engine. 
? Access frequency: number of access requests 
by a visitor over a period of time. 
? Language preferences 
? Period of visits 
Knowing these points helps determining the 
possible users who might be willing to contrib-
ute. A contributor should satisfy the following 
characteristics: 1) Loyalty 2) Multilinguality.  A 
multilingual user is a visitor who uses multilin-
gual search terms to access the online resources. 
We rank users based on their linguistic compe-
tence, we measure that by tracking users? search 
requests, and matching them with the multilin-
gual preterminological graph, users with higher 
matches in certain pair of languages are ranked 
higher. A loyal user is a user who visits the web 
site frequently and stays longer than other users. 
Users based on how many months they accessed 
the website more that k times. 
4.2 DSR-MPG Applications 
For a historical archive like the DSR, we find 
that reading and searching where the most im-
portant for users. Log files since 2003 shows 
that 80% of the project visitors were interested 
in reading the historical records. Moreover, 
around 140000 search requests have been sent 
to the internal search engine. So we imple-
mented two applications (1) ?contribute-while-
reading? and (2) ?contribute-while-searching?. 
4.2.1 Contribute While Searching 
Physical books have been digitized and indexed 
into a search engine. We expect users to send 
monolingual search requests in any language 
supported by our system to get multilingual an-
swers. Having a term base of multilingual 
equivalences could achieve this (Chen 2002). A 
bilingual user who could send a bilingual search 
request could be a valid candidate to contribute. 
We plan that users who use our search engine 
will use the DSR-pTMDB to translate their re-
quests and will contribute to the graph sponta-
neously. As Figure 5 shows, a user would trans-
late the search request, during the searching 
process; the user can ask to add new translation 
if s/he was not happy with the suggested transla-
tion, by clicking on ?Add Suggestions? to view 
a contribution page. 
 
Figure  5. A Japanese user translating his re-
quest 
23
4.2.2 Contribute While Reading 
The other application is trying to help users 
from different linguistic backgrounds to trans-
late some of the difficult terms into their lan-
guages while they are reading, simply by select-
ing a term from the screen. As shown in Figure 
6, readers will see a page from a book as an im-
age, with its OCR text. Important terms will be 
presented with yellow background. Once a term 
is clicked, a small child contribution/lookup 
window will be open, similar. Also user can 
lookup/translate any term from the screen by 
selecting it. This application helps covering all 
the important terms of each book. 
 
Figure 6. Translate while reading 
5 Experimental Results 
In this section present we will present the ex-
periment of seeding DSR-MPG, and the results 
of discovering possible contributors from the 
visitors of the DSR. 
5.1 DSR-MPG Initialization 
To build the initial DSR-MPG, we used the ac-
cess log files of the DSR website (dsr.nii.ac.jp) 
from December 2003 to January 2009. The ini-
tial graph after normalization contained 89,076 
nodes.  Also we extracted 81,204 terms using 
Yahoo terms. 27,500 of them were not discov-
ered from the access files. So, the total number 
of nodes in the initial graph was 116,576 nodes, 
see Figure 7 for sample nodes. 
After multilingualization, the graph has 210,781 
nodes containing terms from the most important 
languages. The graph has now 779,765 edges 
with tw > 0.  The important languages are the 
languages of the majority of the visitors, the 
languages of the archived books, and represen-
tative languages a long the Silk Road. DSR-
MPG achieved high linguistic coverage as 20 
languages have more than 1000 nodes on the 
graph. To evaluate the produced graph, we ex-
tracted 350 English terms manually from the 
index pages of the following books: 
Ancient Khotan, vol.1: 
http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-7/V-1/ 
On Ancient Central-Asian Tracks, 
vol.1:http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-
19/V-1 
Memoir on Maps of Chinese Turkistan and 
Kansu, vol.1: 
http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-11/V-1 
0
5 0
10 0
15 0
2 0 0
2 5 0
3 0 0
DS R- M P G 2 D S R - M P G 1 P a n Ima g e s W i ki t io n a ry B i- d i c t io n a ry DS R1
En-Ar (only correct tranlstions) En-Fr (only correct translations)
 
Figure  7. A comparison between DSR-MPG, 
and other dictionaries. The En-Ar bi-dictionary 
is Babylon (Babylon 2009), and the En-Fr bi-
dictionary was IATE. 
We assume that the terms available in these 
books are strongly related to the DSR. Hence, 
we tried to translate them into Arabic and 
French. Figure 7 compares between DSR-MPG, 
and various general purpose dictionaries. Out of 
the 350 terms, we found 189 correct direct 
translations into Arabic. However, the number 
reached 214 using indirect translations.  On the 
other hand, the closest to our result was PanI-
mages, which uses Wikitionaries and various 
dictionaries, with only 83 correct translations. 
DSR-MPG1 is the translations obtained from 
formula 1, DSR-MPG2 represents the transla-
tions obtained from indirect translations, which 
increased the amount of correct translation by 
24
25 terms in the case of En-Ar. The result can be 
progressively enhanced by accepting contribu-
tions from volunteers through the applications 
we described in the section three and the generic 
nature of MPG makes it easy to accept contribu-
tions from any dictionary or terminological da-
tabase. 
Around 55200 root English terms were used 
as a seed set of terms; these terms were selected 
from the initial DSR-MPG. Around 35000 
terms have been translated from Wikipedia into 
at least 1 language, mostly in French, German. 
Wikipedia increased the density of the graph by 
introducing around 113,000 edges (with tw). 
Translations
0
2000
4000
6000
8000
10000
12000
fr de ja it zh es ru ar
 
Figure 8. Number of translated terms in sam-
ple languages using Wikipedia 
Naturally MT would achieve better coverage; 
we checked the results for Arabic, we selected 
60 terms randomly from the root set, around 25 
terms were translated correctly. 13 terms needed 
slight modification to be correct. 
0
2000
4000
6000
8000
10000
12000
fr de ja it zh es ru ar
Wikipedia
Google Translate confirmations
 
Figure 9. Terms translated by Google MT 
and matched the translation of Wikipedia 
5.2 DSR Possible Contributors 
With K=2, meaning that a multilinguality com-
petence is counted only if the two terms sent by 
a user has to have more than 2 points of transla-
tion weight on the MPG. 
The highest score was 33, achieved by this 
IP: p27250-adsao05douji-acca.osaka.ocn.ne.jp. 
That means that this user sent 33 multilingual 
search requests. We have another 115 users with 
score higher than 5.  
For example, the following two request, sent by 
one user: 
p27250-adsao05douji-acca.osaka.ocn.ne.jp
 &input=peshawar 
p27250-adsao05douji-acca.osaka.ocn.ne.jp
  &input=?????? 
On the DSR-MPG the translation weight be-
tween peshawer and ?????? = 5, thus 
this IP earned a point. With k=10, means that a 
user should send 10 requests to earn a loyalty 
point, only 309 users earned 12 point (for 12 
months), 43 of them has more than 3 points. 
6 Conclusions 
We presented our work in constructing a new 
lexical resource that can handle multilingual 
terms based on the historical archive of the 
Digital Silk Road. Multilingual Preterminologi-
cal Graphs (MPGs) are constructed based on 
domain dedicated resources, and based on vol-
unteer contributions.  
DSR Terminology
DSR-MPG (200,000 nodes)
previous DSR
dictionary (500
entries)
 
Figure  10. DSR preterminology 
It compiles terms available in the pretermi-
nological sphere of a domain. In this article we 
defined the framework of the construction of 
preterminology, and we described the approach 
for using access log files to initialize such pre-
terminological resource by finding the trends in 
the search requests used to access the resources 
of an online community. Aiming at a standard-
ized multilingual repository is very expensive 
25
and difficult.  Instead of that, MPGs tries to use 
all available contributions.  This way will en-
hance the linguistic and informational coverage, 
and tuning the weights (tw, rw, and sw) will 
give indications for the confidence of the trans-
lation equivalences, as the tedges accumulate 
the agreements of the contributors and MDRs 
(online resources). 
We used the resources of the Digital Silk 
Road Project to construct a DSR-MPG and 
some applications that attract further contribu-
tion to the MPG.  DSR-MPG achieved high lin-
guistic and informational coverage compared to 
other general purpose dictionaries, Figure 10. 
Furthermore, the generic structure of the MPG 
makes it possible to accept volunteer contribu-
tions, and it facilitates further study of comput-
ing more lexical functions and ontological rela-
tions between the terms. We made a study on 
the possibility of receiving contributions from 
users, by analyzing the access log file to find 
multilinguality and loyalty of the DSR visitors; 
we found 115 users with the needed linguistic 
capacity 43 of them scored high loyalty points. 
This gives an indication of the future of the con-
tributions. These measures are just estimations 
and expected to go high with the help of the 
MPG-DSR applications. 
References 
Babylon. (2009). "Babylon Dictionary."   Retrieved 
5/5/2009, 2009, from 
http://www.babylon.com/define/98/English-
Arabic-Dictionary.html. 
Cabre, M. T. and J. C. Sager (1999). Terminology: 
Theory, methods, and applications, J. Benjamins 
Pub. Co. 
Chen, A. (2002). "Cross-Language Retrieval Ex-
periments at CLEF 2002." in CLEF-2002 working 
notes,. 
Daoud, M., C. Boitet, et al (2009). Constructing 
multilingual preterminological graphs using vari-
ous online-community resources. the Eighth In-
ternational Symposium on Natural Language 
Processing (SNLP2009), Thailand. 
Daoud, M., C. Boitet, et al (2009). Building a 
Community-Dedicated Preterminological Multi-
lingual Graphs from Implicit and Explicit User In-
teractions. Second International Workshop on 
REsource Discovery (RED 2009), co-located with 
VLDB 2009, Lyon, France. 
Daoud, M., A. Kitamoto, et al (2008). A CLIR-
Based Collaborative Construction of Multilingual 
Terminological Dictionary for Cultural Resources. 
Translating and the Computer 30, London-UK. 
Etzioni, O., K. Reiter, et al (2007). Lexical transla-
tion with application to image searching on the 
web. MT Summit XI, Copenhagen, Denmark. 
Even, S. (1979). Graph Algorithms, Computer Sci-
ence Press. 
FAO. (2008). "FAO TERMINOLOGY."   Retrieved 
1/9/2008, 2008, from http://www.fao.org/faoterm. 
Google. (2008). "Google Dictionary."   Retrieved 
1/9/2008, 2008, from 
http://www.google.com/dictionary. 
Google. (2008). "Google Translate."   Retrieved 1 
June 2008, 2008, from http://translate.google.com. 
Gopestake, A., T. Briscoe, et al (1994). "Acquisition 
of lexical translation relations from MRDS." Ma-
chine Translation Volume 9, Numbers 3-4 / Sep-
tember, 1994: 183-219. 
IATE. (2008). "Inter-Active Terminology for 
Europe."   Retrieved 10/10/2008, 2008, from 
http://iate.europa.eu. 
IDRC. (2009, 10 January 2009). "The Water De-
mand Management Glossary (Second Edition)." 
from 
http://www.idrc.ca/WaterDemand/IDRC_Glossar
y_Second_Edition/index.html. 
IEC. (2008). "Electropedia."   Retrieved 10/10/2008, 
2008, from 
http://dom2.iec.ch/iev/iev.nsf/welcome?openform. 
Kageura, K. (2002). The Dynamics of Terminology: 
A descriptive theory of term formation and termi-
nological growth. 
Loerch, U. (2000). An Introduction to Graph Algo-
rithms Auckland, New Zealand, University of 
Auckland. 
NII. (2003). "Digital Silk Road."   Retrieved 
1/9/2008, 2008, from 
http://dsr.nii.ac.jp/index.html.en. 
NII. (2008). "Digital Archive of Toyo Bunko Rare 
Books."   Retrieved 1 June 2008, 2008, from 
http://dsr.nii.ac.jp/toyobunko/. 
Systran. (2009). "Systran Web Tranlstor."   Re-
trieved 20/12/2009, 2009, from 
www.systransoft.com/. 
UN-Geo (2002). Glossary of Terms for the Stan-
dardization of Geographical Names, UN, New 
York. 
26
UN. (2008). "United Nations Multilingual Terminol-
ogy Database."   Retrieved 10/10/2008, 2008, 
from http://unterm.un.org/. 
Wikipedia-A. (2008). "Wikipedia."   Retrieved 1 
June 2008, 2008, from http://www.wikipedia.org/. 
 
 
 
27
Proceedings of the 3rd Workshop on the People?s Web Meets NLP, ACL 2012, pages 10?14,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Serious Game for Building a Portuguese Lexical-Semantic Network
Mathieu Mangeot? Carlos Ramisch??
? GETALP ? LIG, University of Grenoble (France)
? Federal University of Rio Grande do Sul (Brazil)
FirstName.LastName@imag.fr
Abstract
This paper presents a game with a purpose
for the construction of a Portuguese lexical-
semantic network. The network creation is
implicit, as players collaboratively create links
between words while they have fun. We de-
scribe the principles and implementation of
the platform. As this is an ongoing project,
we discuss challenges and long-term goals.We
present the current network in terms a quanti-
tative and qualitative analysis, comparing it to
other resources. Finally, we describe our tar-
get applications.
1 Introduction
The creation of lexical resources like wordnets is
time consuming and very costly in terms of man-
power. Funding agencies and publishing houses are
very reluctant to launch new projects. Ironically, in
our globalized nowadays world, the need of such re-
sources for communication is growing. In this con-
text, there is hope for building resources via commu-
nities of voluntary contributors. But is it possible to
use the Wikipedia paradigm for building a rich and
broad-coverage lexical resource reusable by humans
and machines in NLP projects? Wordnets are very
interesting resources, but they suffer of several limi-
tations. First, even if the English wordnet (Miller et
al., 1990) is open source and freely available, this is
not the case of the EuroWordnets. Second, wordnets
and other manually built thesauri are based on lin-
guists? intuition. Information about up-to-date en-
tities (Facebook, Costa Concordia, etc.) and real-
world facts are missing. Third, relations between the
synsets of wordnets are of limited semantic kinds.
We would like to build other relations at the syntac-
tic and lexical level (e.g. collocations).
Our first goal is to build a rich lexical network
for the Portuguese language. The relations between
nodes (words) is represented in a sophisticated way,
by using lexical-semantical functions ? la Mel?c?uk
(Mel?c?uk, 1995) such as the Magn function repre-
senting the notion of intensifier: Magn(smoker) =
heavy, Magn(bachelor) = confirmed. The resulting
network represents the usage of the language, not the
norm. Thus, it may contain frequent spelling mis-
takes or neologisms. This resource is open-source
and freely available. It can be used in several appli-
cations: lexicography, printed dictionary, text gen-
eration, semantic information extraction, ontology
learning, etc. The construction of the resource is
done indirectly by contributors through a game.
In the next section, the concept of using serious
games for building NLP resources will be explained
(? 2). The following section will detail the construc-
tion of the Portuguese version of the game (? 3). Af-
terwards, we will discuss some preliminary results
(? 4) and finally we present future work (? 5).
2 Serious Games and NLP
The concept of human contribution, collaboration
and computation has been utilized in many applica-
tions and scenarios. The work of Luis von Ahn made
a breakthrough, especially in ESP game (von Ahn,
2006; von Ahn and Dabbish, 2008). Human com-
putation (crowdsourcing, volunteer contribution) is
now seriously considered to be able to solve large
computational problems (Speer, 2007). The idea
of collecting massive contributions from volunteers
through an online game took off recently. Nowa-
10
days, many serious games or GWAP ?Game With
A Purpose? (von Ahn and Dabbish, 2008) projects
exist in different domains, like Open Mind Com-
mon Sense (Singh et al, 2002), ESP games, Learner
(Chklovski and Gil, 2005), or CYC project 1. Con-
cerning more specifically lexical networks, similar
projects exist like ?small world of words?2 launched
in 2003 by KU Leuwen. For the moment, this
project is limited to building relations of only one
kind: associated ideas.
Looking at the Wikipedia project, the idea of
building lexical resources with the help voluntary
contributors comes to mind. Unfortunately, the
Wikipedia paradigm cannot be easily applied to
build a dictionary with rich lexical information. In
Wikipedia, articles do not need to follow the same
structure, while in a dictionary, the same structure
and linguistic theory must be applied to all the arti-
cles. Moreover, while it is easy to contribute to an
encyclopedia entry, not everyone has the linguistic
knowledge to contribute to a dictionary. On read-
ing Wiktionary entries, one realizes that the quality
cannot be compared to existing paper dictionaries.
When looking at people playing online games
through the Internet, one could think that it would
be interesting to use this time for playing a game that
would build lexical data in the background, specifi-
cally data that is difficult to find in existing dictionar-
ies. In this context, the idea of a serious lexical game
emerged. The first version was launched for French
in 2007 (Lafourcade and Joubert, 2008), which has
now around 250,000 nodes and 1,330,000 relations.
Our game aims at building a rich and evolving
lexical network comparable to the famous English
wordnet (Miller et al, 1990). The principle is as
follows: a player A initiates a game, an instruc-
tion is displayed concerning a type of competency
corresponding to a lexical relation (e.g. synonym,
antonym, domain, intensifier) and a word W is cho-
sen randomly in the database. Player A has then a
limited amount of time for giving propositions that
answer the instruction applied to the word W .
The same word W with the same instruction is
proposed to another player B and the process is the
same. The two half-games of player A and player
1http://game.cyc.com/
2http://www.smallworldofwords.com/
B are asynchronous. For each common answer in
A and B?s propositions, the two players earn a cer-
tain amount of points and credits. For the word W ,
the common answers of A and B players are entered
into the database. This process participates in the
construction of a lexical network linking terms with
typed and weighted relations, validated by pairs of
players. The relations are typed by the instructions
given to the players and weighted with the number
of pair players that proposed them. A more detailed
description of the game in French is provided by
Lafourcade and Zampa (2009).
3 Portuguese Version
The game interface was translated by a native Por-
tuguese speaker. A preliminary step was to interna-
tionalize the text messages by separating them from
the interface and storing them in an array, allowing
for easy translation in any other language. Simul-
taneously, we developed, and tested an easy step-
by-step installer which makes the deployment of the
game as easy as installing a content management
system software on a server.
A list of seed words must be provided from which
the game will chose the proposed terms at the be-
ginning. As the game evolves, people suggest new
words not necessarily in the initial dictionary, thus
helping the vocabulary to grow. Two resources
were used to compose this list of seed words. The
first is the DELAS?PB dictionary from NILC (Mu-
niz, 2004). All nouns, verbs, adjectives and ad-
verbs were extracted, resulting in 67,062 words. As
these include a large number of rare words, pilot
tests showed that the game became annoying when
the player ignored the meaning of most of the pro-
posed words. Therefore, the number of Google
hits for every word was obtained and only the 20%
most common ones were kept, resulting in a list of
13,413 words. To this, the entries of the Brazilian
Open Mind Common Sense network (Anacleto et
al., 2008) were added, in order to allow future com-
parison with this resource. Apertium?s lt-toolbox3
was used in order to obtain the most frequent POS
tag for each entry, resulting in 5,129 nouns, 3,672
verbs, 1,176 adjectives, and 201 adverbs. The union
with the preceding dictionary resulted in a final seed
3http://wiki.apertium.org/wiki/Lttoolbox
11
list of 20,854 words.
Once the game is deployed, one of the big chal-
lenges is to gather volunteer players. We gave pre-
sentations about the game in the academic context
and spread the word among Portuguese teachers, ar-
guing that the game could be used to enrich the vo-
cabulary of their students. We also created a Face-
book page and linked it in our website. One way to
motivate subscribed players to come back is to offer
gift words. Each player can offer a friend a game
with a specific word. For example, if I have a friend
fan of baseball, I will offer him/her this word.
Once the first challenge of gathering a commu-
nity of players is overcome, the main difficulty is
to keep the motivation going. For succeeding, the
project needs a person that will animate the com-
munity, motivate gamers and publicize the game for
recruiting new contributors. Games were launched
in other languages, but due to social factors (lack of
community animator), they are in a sleeping state.4
Internally, each word is represented as a node in
a graph. The directed edges are the lexico-syntactic
relations created by the game. Each edge has a type
(associated idea, hypernym, hyponym, typical ob-
ject, etc.) and a weight, corresponding to the num-
ber of times the two words co-occurred. Each node
has also a weight corresponding to its popularity
(proportional to its degree). Part of speech is en-
coded as edges going from a term to special POS
nodes. In addition to the standard attributes, each
edge also contains counters that represent the coun-
try of players who contributed to its creation. There-
fore, we would like to investigate dialectal varia-
tions of Portuguese in Portugal, Brasil and other lu-
sophone countries. This information can be impor-
tant for using the resource in semantic extraction,
according to the variation of the analyzed text.
4 Preliminary Evaluation
To date, 61 players participated in the game. In this
preliminary quantitative and qualitative evaluation,
we consider only the nodes for which some rela-
tion was created, thus excluding all the seed words
that were not connected to other words yet. Figure 1
shows a fragment of the network. Green edges rep-
resent associated words, red edges represent hypo-
4http://jeuxdemots.liglab.fr/
Figure 1: Overview of part of the network.
and hypernyms. Most relations created are standard,
like feij?, andr?, am?lia and jean are associated with
name. However, non-standard relations are also cre-
ated, like Cuba is the antonym of United States or
tatoo is associated to eternal. While purists may
consider these as noise, we regard it as relations rep-
resenting real-world semantics and language use.
The network contains 19,473 word nodes and
20,854 occurrences of POS relations (a word may
have several POS). Among those, 347 nodes do
not contain POS edges, meaning that they are new
words. A sample of the 20 most popular terms is
presented in Table 1. They include common hyper-
nym nodes like thing and person and animal but also
everyday language words like drink, car and sea.
From all the nodes in the network, only 1,408
(7.23%) have a degree greater than 1 (excluding
POS edges). For the remaining 18,065 nodes, no
relation was created. Figure 2 shows user activity
in number of games played per day. The number of
games is unsteady but it does not seem to increase
nor decrease. Analysis of the log files show that
players tend to participate a lot and the beginning
and then, after one or two weeks, they stop. Thus, it
is important not only to attract new players but also
to keep them active.
Word w Word w Word w
comida 110 hotel 80 pintura 74
*** 100 bebida 80 ?gua 72
pessoa 96 mulher 78 porta 72
dinheiro 92 casa 78 mar 72
carne 82 carro 78 empresa 72
nome 80 animal 76 coisa 72
Table 1: Top-20 most connected words and weights (w).
12
25/12 02/01 10/01 18/01 26/01 03/02 11/02 19/02 27/02 06/03 14/03 22/03 30/03 07/04Date
0
10
20
30
40
50
Numb
er of 
game
s
Figure 2: User activity from Dec 27, 2011 to Apr 4, 2012.
For the moment, the most common edge is of the
associated idea type. It corresponds to more than
80% of the edges. Some players bought compe-
tences in hypo- and hypernym, which together ac-
count for 15.39% of the edges. As these relations
are dual, it would be easy to infer new edges.The
total number of edges acquired is 1,344.
There is one large connected component in the
graph and a large number of small connected com-
ponents with two or three nodes. The total number
of connected components in the graph is 281 (ig-
noring disconnected nodes), yielding a high mod-
ularity of 0.898. The average degree of a node is
0.955, as more than 750 nodes have only one rela-
tion and around 200 have 2 relations, and the degree
decreases exponentially.
The trend is that, as more relations are added,
the current small components will be attached to
the larger ones, but also more smaller unconnected
graphs will be created. However, we expect that
once a large proportion of the nodes has been cov-
ered, the network will converge to a single large con-
nected component.
For the moment, the coverage of our resource is
limited. But a previous comparison done for the
French game versus the French Euro WordNet (F-
EWN) was very promising. The french game con-
tains 10 times more terms than F-EWN (240,000 vs
Relation type Count %
Associated 1,126 83.78
Hypernym 115 8.56
Hyponym 81 6.83
Domain 12 0.89
Antonym 10 0.74
Total 1,344 100
Table 2: Number of edges according to types.
23,000) and relations (1,359,000 vs 100,000). On a
sample of 100 terms frequently played in the game,
3% of them contain errors (spelling mistakes or con-
fusions). Data collected with the French game bring
a lot of originality but the precision rate is much
lower that data collected manually in F-EWN.
Our resource now only contains as much as 0.91%
of the nodes in English Wordnet. As precise num-
bers about the size of the Portuguese wordnet (Mar-
rafa et al, 2011) are not available, we also queried
the online service for the nodes in our network. We
found out that 35.87% of the nodes are covered by
the Portuguese wordnet. Thus, we believe that col-
laborative methods can considerably speed up the
creation of lexical resources, as in only three months
we already have some information complimentary to
a 13-years old project.
5 Future work
We presented the deployment and a preliminary
evaluation of JeuxDeMots?pt,5 a game that aims
at the construction of a Portuguese lexical-semantic
network. The coverage of the resource is still lim-
ited, but the network keeps growing.
For the moment, we have made available a simple
interface in which the user can query for a word and
retrieve all the words related to it. For instance, if
one searches for the word loja (store), the result is:
? store is a place
? cell phone store is a store
? store is associated to buy shirt
? store is associated to sell toys
? store is associated to manager
? store is associated to clothes
The creation of the network is much less onerous
and faster (and more entertaining) than traditional
thesauri construction, that can take years of the work
of many experts. Our long-term goal is the creation
of a large network comparable to existing resources
for English. This resource would be extremely use-
ful in many NLP tasks. Once we will have enough
data, our goal is to apply it to many other applica-
tions like information extraction, WSD, semantic in-
ference and textual entailment. This would help to
bridge the gap of missing lexical resources for NLP
applications dealing with Portuguese language.
5http://jeuxdemots.liglab.fr/por
13
Acknowledgements
This work was partly funded by the CAMELEON
project (CAPES?COFECUB 707-11).
References
Junia Coutinho Anacleto, Aparecido Fabiano P. de Car-
valho, Alexandre M. Ferreira, Eliane N. Pereira, and
Alessandro J. F. Carlos. 2008. Common sense based
applications to advance personalized learning. In
PROC of the IEEE International Conference on Sys-
tems, Man and Cybernetics (SMC 2008), pages 3244?
3249, Singapore.
Timothy Chklovski and Yolanda Gil. 2005. An anal-
ysis of knowledge collected from volunteer contribu-
tors. In Twentieth National Conference on Artificial
Intelligence (AAAI-05), Pittsburgh, Pennsylvania.
Mathieu Lafourcade and Alain Joubert. 2008. Jeuxde-
mots : un prototype ludique pour l??mergence de re-
lations entre termes. In JADT 2008 : 9es Journ?es
internationales d?Analyse statistique des Donn?es
Textuelles, pages 657?666, Lyon, France, 12-14 mars.
Mathieu Lafourcade and Virginie Zampa. 2009. Jeuxde-
mots and pticlic: games for vocabulary assessment and
lexical acquisition. In Computer Games, Multimedia
& Allied technology 09 (CGAT?09), Singapore, 11th-
13th May.
Palmira Marrafa, Raquel Amaro, and Sara Mendes.
2011. Wordnet.pt global ? extending wordnet.pt to
portuguese varieties. In Proc. of the First Workshop
on Algorithms and Resources for Modelling of Di-
alects and Language Varieties, pages 70?74, Edin-
burgh, Scotland, July. ACL.
Igor Mel?c?uk. 1995. Lexical functions: A tool for
the description of lexical relations in the lexicon. In
Leo Wanner, editor, Lexical Functions in Lexicogra-
phy and Natural Language Processing, pages 37?102.
John Benjamins, Amsterdam/Philadelphia.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990. In-
troduction to wordnet: an on-line lexical database. In-
ternational Journal of Lexicography, 3(4):235?244.
Marcelo C. M. Muniz. 2004. A constru??o de recursos
linguistico-computacionais para o portugues do brasil:
o projeto de unitex-pb. Master?s thesis, Instituto de
Ciencias Matematicas de Sao Carlos, USP, S?o Carlos,
SP, Brazil.
Push Singh, Thomas Lin, et al 2002. Open mind
common sense: Knowledge acquisition from the gen-
eral public. In Proceedings of the First International
Conference on Ontologies, Databases, and Applica-
tions of Semantics for Large Scale Information Sys-
tems, Irvine, CA, USA.
Robert Speer. 2007. Open mind commons: An inquis-
itive approach to learning common sense. In Work-
shop on Common Sense and Intelligent User Inter-
faces, Honolulu, Hawaii, USA., January 28-31.
Luis von Ahn and Laura Dabbish. 2008. General tech-
niques for designing games with a purpose. Commu-
nications of the ACM, pages 58?67.
Luis von Ahn. 2006. Games with a purpose. IEEE Com-
puter Magazine, pages 96?98.
14
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 87?98,
Dublin, Ireland, August 23, 2014.
Jibiki-LINKS: a Tool between Traditional Dictionaries and Lexical Networks for Modelling Lexical Resources  
ZHANG Ying1, 2 Mathieu MANGEOT1 Val?rie BELLYNCK1 Christian BOITET1 1. GETALP-LIG, 41 rue des Math?matiques BP53, 38041 Grenoble Cedex 2. SAS Lingua et Machina, Domaine de Voluceau, Rocquencourt, 78153 Le Chesnay  {ying.zhang, mathieu.mangeot, valerie.bellynck, christian.boitet}@imag.fr  
 Abstract  Between simple electronic dictionaries such as the TLFi (computerized French Language Treasure)1 and lexical networks like WordNet2 (Diller et al., 1990; Vossen, 1998), the lexical databases are growing at high speed. Our work is about the addition of rich links to lexical databases, in the context of the parallel development of lexical networks. Current research on management tools for lexical databases is strongly influenced by the field of massive data ("big data") and by the Web of data ("linked data"). In lexical networks, one can build and use arbitrary links, but possible queries cannot model all the usual interactions with lexicographers-developers and users, that are needed, and derive from the paper world. Our work aims to find a solution that allows for the main advantages of lexical networks, while providing the equivalent of paper dictionaries by doing the lexicographic work in lexical DBs.  1 Introduction  The growing importance of IT in all human activities extends and expands the needs and usages of all key digital resources that include lexical resources. Thus, while applications valuing the linguistic processes rely on increasingly abstract representations, modelled for computer operations, it remains that models coming from the historical construction of resources foster human understanding, and therefore, the building of tools for studies centring on the humanities.  In this this section, we place the emergence of the concept of lexical database between electronic dictionaries and lexical networks. We show that this concept is still valid, that it is still necessary to enrich it, and that our work on improving tools for lexical databases helps solve real problems.  To do this, we analyse in the second section the evolution of lexical resources in 4 main steps (simple electronic dictionaries, simple lexical databases, multilevel and multiversion lexical databases, and lexical networks) and present the associated problems. In the third section, we present Jibiki-LINKS, a platform for building multilingual lexical databases that enriches the Jibiki generic platform by introducing the concept of rich link between the components it manages (dictionary entries and dictionary volumes). Finally, we show that it allows the construction of lexical databases such as Pivax-UNL, which support scaling up.  2 From computerized dictionaries to lexical databases with rich links  The first computerized lexical resources are electronic versions of printed dictionaries, mainly monolingual or bilingual. The use of computers has helped to overcome the constraints of the paper form. The impossibility to inverse bilingual dictionaries led to a model having a "pivot" consisting of axies3. Lexical pivot-based databases are invertible and transitive, but rooted on the form of the 
                                                This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 http://atilf.atilf.fr 2 http://wordnet.princeton.edu 3 "Axie" = "interlingual meaning," by analogy with "lexie". 
87
symbols, while the lexical networks allow a move towards the direct manipulation of semantic tokens, regardless of their surface form, and thus of the language.  In this section, we present the evolution of approaches, distinguishing four main types of lexical resources, the limitations that motivated this evolution, and the remaining hard problems.  2.1 Simple electronic dictionaries  A simple electronic dictionary is an electronic version of a printed dictionary, or the computer representation of a new kind of the same type of dictionary, for example, the TLFi4, the morphological and bilingual dictionaries of Apertium5, etc. A simple electronic dictionary contains either one volume or two volumes. The electronic version of a monolingual paper dictionary is (usually implicitly) based on its microstructure, that is to say, on the organization of its entries in the form of a small tree organizing the information it contains. In a paper dictionary, the presentation of an entry reflects the microstructure, but the microstructure is not always directly retrievable from it (for example, parts in italics can correspond to different types of information units, such as idiom or example of use). In absolute terms, it is always possible to represent the information specified in each entry of a dictionary according to a common structure. In reality, the structures of paper dictionaries are less rigorous than what would be required for automatic processing, so that manual editing is required.  A bilingual paper dictionary is generally based on a structure in two volumes, one for each language pair, each volume conforming to the same microstructure. There are therefore generally one volume from language A (Lg A) to language B (Lg B) and a mirror volume from Lg B to Lg A. We define the macrostructure of a dictionary as the organization of the volumes that make up its structure. These macrostructures constitute the bulk of the printed dictionaries.  2.2 Lexical databases  A lexical database is a tool for unifying any set of dictionaries, where each dictionary can be monolingual, bilingual or multitarget. A multilingual lexical database is composed of volumes that are monolingual, direct multilingual, or indirect multilingual, i.e. connecting the entries of different languages via a pivot structure. It has an overall macrostructure, and a microstructure for each of its volumes. A link between 2 entries is realized by the software tool as a direct link, or as 2 links going through an intermediate language, or as a semantic link, etc.  The lack of symmetry of the correspondence between the entries of bilingual dictionaries (from word senses to words, not word senses) led to the concept of interlingual pivot. In the pivot macrostructure developed and used for the Papillon-NADIA multilingual lexical database (S?rasset and Mangeot, 2001), there is only one monolingual volume for each language. Lexies are word senses (of a lexeme or an idiom) and make up the entries of these volumes. To group the lexies of different languages together, there is a pivot volume of axies (interlingual acceptions). An axie connects synonymous lexies. The links are established only between lexies and axies. This is the simplest macrostructure for a pivot-based multilingual lexical resource that allows for the extraction of usage dictionaries for all pairs in all directions. The concept of axie-based pivot structure has been validated by the Papillon project and then included in the Lexical Markup Framework standard (Francopoulo et al. 2009).  2.3 Multilevel and multiversion databases  In this type of lexical database, several monolingual volumes are allowed for each lexical space6, A volume of axemes (monolingual acceptions) is introduced to link synonymous lexies of the considered lexical space. Also, various levels are introduced to tag entries according to different points of view (sublanguage, version, type of link, reliability, preference). The simple links of previous versions are replaced by rich links that can be established not only between lexies, axemes and axies, but also between entries and subentries, monolingually (lexicosemantic functions) or bilingually (translations).                                                  4 Tr?sor de la Langue Fran?aise informatis?, http://atilf.atilf.fr/ 5 http://wiki.apertium.org/wiki/User:Alessiojr/Easy_dictionary_-_Application-GSOC2010 6 A lexical space of a natural language contains various levels (wordform, lemma, lexie, prolexeme, proaxeme); it can also contain the lexical symbols of an artificial semantic representation language (e.g., the UWs of UNL). 
88
For example, there is a 3-level macrostructure (lexie, axeme, axie) in PIVAX (Nguyen & al., 2007) and a 4-level macrostructure (lexie, prolexeme, proaxie, axie) in ProAxie (Zhang & Mangeot, 2013), described in more detail in section 4.1. Both allow us to manage one or more monolingual volumes for each lexical space. That has been quite useful in the ANR Traouiero GBDLex-UW++ subproject, during which we stored the UNL part of many UNL-Li dictionaries (the UW interlingual lexemes, built with slightly different conventions by different UNL groups for their languages), and tried then to unify them in a new monolingual UNL dictionary (using a set of "UW++" built from WordNet and from the previous UNL dictionaries).  2.4 Lexical network  A lexical network brings together the set of words that denote ideas or realities that refer to the same theme, as well as all the words that, because of the context and certain aspects of their meaning, also evoke this theme7. The theme may possibly be very broad. It is possible to represent the full vocabulary of a language in a lexical network, such as, for French, the JeuxDeMots network (Lafourcade and Joubert, 2010) or RFL (Lexical Network of French (Lux-Pogodalla, Polgu?re 2011)).  Lexical networks are traditionally represented as graphs. Nodes represent the lexemes of one or more languages, and links represent the relationships between these lexemes (translation, synonymy, etc.). A lexical network can be monolingual or multilingual. One can create syntactic, morphological and semantic relations between lexemes.  Although lexical networks have many advantages, they are not suitable for all usages. For example, lexical networks like WordNet (Diller & al., 1990; Vossen, 1998), HowNet (Dong et al., 2010) and MindNet (Dolan and Richardson, 1996) (Richardson et al., 1998) are not browsable in alphabetical order. But we need that possibility to have an idea of the content of a lexical repository, whatever its nature, or to play word games, or to find a word one has on the tip of the tongue8. On the other hand, in a lexical network, the concept of volume is missing, which prevents to create a resource in a simple way when studying a new language.  For example, the lexical network DBNary (S?rasset, 2012), which is based on the Lemon model (McCrae et al., 2011), contains millions of terms, but does not allow labelling the links. To navigate in this system, one must write SPARQL queries, which is not within the reach of everyone.  2.5 Conclusion: features, limitations and hard problems  Research efforts focus today mainly on lexical networks, but much remains to be done on the preceding types (pivot, multilevel). In particular, the import of lexical databases in lexical networks causes a loss of information, especially information born by the attributes of rich links. For example, what concerns the history, the etymology or the evolution of word senses is not systematically imported into lexical networks. They therefore cannot meet the needs of the humanities, nor allow the transition to "digital humanities."  A lexical network is actually the type of structure that enables the greatest freedom of representation. Indeed, we can create entries and links arbitrarily. But the possible queries cannot model all the usual interactions with lexicographers-developers and users, which come from the world of paper, and are felt necessary. They allow us to represent all categories of lexical resources, but the analogy with the real world is lost. Thus, the practical expertise of linguists-lexicographers is lost.  We must continue to equip lexical databases, because that is the right level to transfer the techniques used by lexicographers-linguists. Also, modelling by a volume-based macrostructure allows keeping a link to the original paper world. Moreover, there are already reusable resources of these types. That is why we focus on the management of resources having multiversion and multilevel macrostructures.  3 Reuse of rich links  In this section, we present an improvement that consists in introducing into lexical databases relational                                                 7 http://ddata.over-blog.com/xxxyyy/3/12/82/15/GRAMMAIRE/champs-et-reseaux-lexicaux.pdf 8 For that kind of functionality, multiple sorting on subsets of inflected forms and on arbitray types of information seems to be a necessary first level of computer aid. 
89
information in the form of rich links that will bring them closer to lexical networks. An important point is that these links may bear arbitrary labels.  3.1 Presentation of the Jibiki platform  Jibiki is a generic platform that enables the construction of contributive websites dedicated to the construction of multilingual lexical databases. That platform has been developed mainly by Mathieu Mangeot (Mangeot & Chalvin, 2006) and Gilles S?rasset (S?rasset & Mangeot, 2001). It has been used in various projects (EU LexALP project, Papillon project, GDEF project, etc.). The code is available in open source, and freely downloadable by SVN from ligforge.imag.fr. With this platform, one can perform import, export, edit and search operations in lexical databases. One can also manage the contributions. Jibiki allows handling almost all lexical resources of XML type, by using different microstructures and macrostructures.  In the Jibiki approach, resources are organized in volumes, which makes it easier to achieve the equivalent of paper dictionaries, keeping the mental image of the representation of the dictionary, while offering new interactions allowed in the digital world. Usages of dictionaries in Jibiki are also similar to those of paper dictionaries. For example, one can consult a database in alphabetical order, indicate a source and/or target language, group lexies in vocables, navigate in a volume, etc.  3.2 Classical Common Dictionary Markup  Version 1 of Jibiki uses "CDM pointers" (Common Dictionary Markup (Mangeot, 2002)) to import, view and edit any type of microstructure without modifying it. CDM pointers are also used to index specific parts of the information, and then allow a multi-criteria search.  Each CDM pointer indicates the path (XPath) to the corresponding element in the XML microstructure of the described resource (see Figure 1). Its description is stored in a XML metadata file. When the resource is imported in the Jibiki platform, the pointers are computed, and the result is stored in a table of the (postgresql) database, for each volume. This table is considered as an indexing table.  
 Figure 1: CDM pointers for the French volume of the GDEF9 resource (Mangeot and Chalvin, 2006) 
CDM tags FeM10 (Gut et al., 1996) OHD11  JMdict12 (Breen, 2004) Volume /volume /volume /JMdict Entry /volume/entry /volume/se /JMdict/entry Entry ID /volume/entry/@id  /JMdict/entry/ent_seq/text() Headword /volume/entry/headword/text() /volume/se/hw/text() /JMdict/entry/k_ele/keb/text() Pron /volume/entry/prnc/text() /volume/se/pr/ph/text()  PoS //sense-list/sense/pos-list/text() /volume/se/hg/ps/text() /JMdict/entry/sense/pos/text() Domain  //u/text()  Example //sense1/expl-list/expl/fra //le/text() /JMdict/entry/sense/gloss/text() Table 1: Examples of Common Dictionary Markup                                                  9 GDEF is a large Estonian-French dictionary that is being created by the Franco-Estonian lexicography association (see http://estfra.ee/GDEF.po).  10 FeM is a French-English-Malay dictionary (30000 entries, 50000 lexies, 8000 idioms, 10000 examples of use). 11 OHD is abbreviation of Oxford-Hachette Dictionary, which is a French-English dictionary. 12 JMdict is a Japanese-multilingual dictionary. 
90
The translation links are treated at this stage with conventional CDM pointers, as classical information elements. It is not possible to index other information carried by the links, such as weights or labels.  Hence, multilevel macrostructures cannot be modelled in a generic manner with Jibiki-v1 and traditional CDM pointers. For example, it is not possible to link the same volume to several volumes at different levels. This has forced us initially to use palliatives that did not scale up. It became necessary to modify the conceptual model. We addressed these shortcomings in a new version, Jibiki-LINKS.  Table 1 above is an example of CDM for the different resources.  3.3 New version of Jibiki with CDM LINKS  To manage multilevel macrostructures, we enriched the CDM with a richer description of the links (see Figure 2). For each link, more information can be indexed:  ? the identifier of the source entry.  ? the identifier of the target entry.  ? the identifier of the XML element of the source entry containing the link. For example, the sense number in a polysemous entry having a translation link for each translation direction. That allows us to precisely retrieve the origin of the link.  ? the link name. It is used to distinguish between different types of links in a single entry, such as a translation link and a synonymy link.  ? the target language (three-letter code ISO 639-2 / T).  ? the target volume.  ? the type of link. Some types are predefined, because they are used by the algorithms that compute the rich links (translation, axeme, axie), but it is possible to use other types of links.  ? a label whose text is arbitrary.  ? a weight whose value must be a real number.  These links can be established between two entries of the same volume or between two different volumes. The same volume may group entries connected to several volumes.  To realize the implementation of rich links, we separated the module processing the links from the module processing other CDM pointers. It means we have two CDM tables in the database associated to each volume. The first stores CDM traditional pointers, and the second CDM LINKS. All information of LINKS can be found in this table.  
 Figure 2: CDM-LINKS for the English volume of the CommonUNLDict resource  3.4 Approach by rich links in searching in a complex lexical network  To explain how we create arbitrary links, let us give an example. A free label is available for each link. For example, in a lexical resource including SMS, in French "A+" has a link to "Over" with a "SMS" label, in English "L8R" corresponds to "later" with a "SMS" label, and the label of the link between "Over" and "later" is "translation."  A ProAxie macrostructure (Zhang & Mangeot, 2013) has been implemented on the Jibiki-Links platform. We present another example of rich links for semantic search in section 4.1.  
91
3.5 Algorithms for computing rich links  The computer implementation is based on two algorithms. The first collects the links, and the second builds the result. More precisely, the first looks for all possible links in the set of all rich links of all volumes, for a desired entry. The second recursively performs the following steps: (1) selection of the start entry; (2) search of the links to other entries; (3) treatment of labels; (4) recursive call of the algorithm on the connected entry; (5) integration of the XML code of the entry connected to the start entry; (6) display.  4 Experimentation  4.1 Examples of multilevel macrostructures  We have already installed several multilevel macrostructures on Jibiki-LINKS. Here are 3 examples.  Mot?Mot: trilingual lexical database with a pivot structure (Mangeot & Touche, 2010) This project (2009-2012) has computerized a French-Khmer classical dictionary, initially in Word, into a Jibiki database (see http://jibiki.univ-savoie.fr/motamot/). The macrostructure is composed of a monolingual volume for each language and a central pivot volume. However, in order not to confuse users, the contributing interface shows a classic view of a bilingual dictionary. Each bilingual link language A ? language B added via this interface is actually translated into the background by creating two interlingual links as well as an axie link representing the original translation, to finally get: language A ? pivot axie ? language B (see Figure 3). If a contributor wants to add a translation link between a vocable Va of language A and a vocable Vb of language B, s/he can establish this link at different levels. The ideal solution is to connect a word meaning (lexie) La of the vocable Va to another word meaning Lb of the vocable Vb. In this case, the link is bijective and Lb is also connected to La. If the contributor cannot choose between word meanings, s/he can connect directly the word meaning La to the vocable Vb and the link is tagged for refinement. With the pivot macrostructure, if two links language A ? language B and language B ? language C exist, then it will automatically create a link language A ? language C tagged for refinement. 
 Figure 3: Example of Mot?Mot ProAxie: multilingual extension of ProxlexBase (Tran, 2006) The ProAxie macrostructure aims at solving the problem of linking several terms that refer to one and the same referent, in particular for the management of acronyms (Zhang et Mangeot, 2013). In this macrostructure, there are two different layers. The base layer consists of two types of volume: volumes of lexies and volumes of axies. The axies are used to connect the lexies that match each other exactly. For example, one translates "ONU" by "UN" (see Figure 4) from French into English.  The "Pro" layer allows us to propose to users translations having the same referential meanings. This layer includes the volumes of prolexemes (Tran, 2006) and one volume of proaxies. A prolexeme entry links lexies having the same meaning with a label (aka, acronym, definition, etc.). A proaxie entry connects prolexemes of different languages. If one cannot find the translations directly using the lower layer, one will get the translations proposed by the "Pro" layer.  For example, for "Nations-Unies", translations by "United Nations" and "UN" will be proposed, with the "alias" label.   
92
 Figure 4: Example of ProAxie  For each natural language, there are one or more volumes of lexies, and a single volume of prolexemes. For each dictionary, there is a volume of axies and a volume of proaxies.  This gives three levels of translation, classified according to the precision obtained.  (1) The system finds a lexie directly, using the volume of axies. That is the first and most accurate level of translation.  (2) The system searches a link to the prolexemes volume of the source language with a certain label. When it finds the link in the proaxies volume, it follows the prolexeme link of the target language, and finally arrives at the volume of lexies in the target language, and finds a lexie that has the same label. That is the second, intermediate level.  (3) The system finds the lexies going through prolexemes and proaxies, without a corresponding label. These proposed lexies constitute the third and least accurate level.  Pivax: lexical multilingual multiversion database with 3 levels  The Pivax macrostructure has three levels: lexie, axeme and axie (Nguyen & al., 2007). Axemes are monolingual acceptions, and group monolingual lexies having the same meaning. Axies group synonymous axemes of different languages in a central "hub". In some situations, a lexical database has several volumes for a single language. For example, when there are several editions, or when the lexical resource is created for a machine translation system: one may have one volume coming from Systran, one from Ariane/H?lo?se, one from IATE13, etc. This macrostructure allows us to manage multiple volumes in the same language. Given a language, there are one or more volumes of lexies and a single volume of axemes. For any Pivax database, there is only one volume of axies. The links between the lexies and the axemes and between the axemes and the axies are rich links with attributes such as type, target volume, target language, free label, weight, etc.  4.2 CommonUNLDict: toward scaling up with a resource of Pivax type  In this section, we present the CommonUNLDict resource that uses the Pivax macrostructure. We have implemented this resource on the Pivax-UNL platform, which is an instance of Jibiki-Links. Users can easily use this resource via the link http://getalp.imag.fr/pivax/Home.po.  Resource created by linguists  Thanks to CDM-LINKS, all types of XML formats can be used in an instance of Jibiki-LINKS without modification. One needs only simple knowledge about XML to create a resource for Jibiki-LINKS. In addition, very useful available tools can be used to create an XML file, such as oXygen14 that allows the creation of a DTD using a graphical interface.   
                                                13 "A single database for all EU-related terminology (InterActive Terminology for Europe) in 23 languages opens to the public", 2007) 14 http://www.oxygenxml.com 
93
The CommonUNLDict resource has been created by the Russian lexicographer  and linguist Viacheslav Dikonov (Dikonov & Boguslavsky, 2009).  Figure 5 shows the graph of a monolingual volume structure using oXygen. In this example, each volume contains a large quantity of vocables, and each vocable includes one or more lexie. We will explain this structure in section 3.2.3. 
 Figure 5: Structure of a monolingual volume 
 Figure 6: Macrostructure of CommonUNLDict   
94
Macrostructure of CommonUNLDict  CommonUNLDict contains 8 languages (7 natural languages, French, English, Hindi, Malay, Russian, Spanish, Vietnamese, and the UNL language) and 17 volumes (8 volumes of monolingual data, 8 volumes of monolingual axemes, and 1 volume of axies ("interlingual meanings"). The macrostructure of CommonUNLDict is diagrammed in Figure 6. For each language, there is only one volume of monolingual data (vocables and lexical items) and a single volume of axemes. For the whole CommonUNLDict, there is only one volume of axies.  Microstructure of CommonUNLDict  The microstructure is the structure of the entries (Mangeot, 2001). In the CommonUNLDict resource, there are three types of entries (vocables, axemes and axies) and 720 K entries in total.  See Table 2.  Volume  Language  Entries  CommonUNLDict_axi  axi  82804  CommonUNLDict_eng  English  45471  CommonUNLDict_eng-axemes  English  82069  CommonUNLDict_esp  Spanish  7080  CommonUNLDict_esp-axemes  Spanish  22254  CommonUNLDict_fra  French  27537  CommonUNLDict_fra-axemes  French  48312  CommonUNLDict_hin  Hindi  31255  CommonUNLDict_hin-axemes  Hindi  50380  CommonUNLDict_msa  Malay  37342  CommonUNLDict_msa-axemes  Malay  31699  CommonUNLDict_rus  Russian  28475  CommonUNLDict_rus-axemes  Russian  45020  CommonUNLDict_unl  unl  82804  CommonUNLDict_unl-axemes  unl  82804  CommonUNLDict_vie  Vietnamese  6585  CommonUNLDict_vie-axemes  Vietnamese  8819  Table 2: Number of entries of CommonUNLDict  All volumes of the same type have the same microstructure. The example below (see Figure 7) shows the microstructure of a volume of vocables. Each entry of vocable type allows us to describe all detailed information, such as part of speech (POS), pronunciation, etc. Each vocable includes one or more lexies (word senses). Figure 2 shows an example. Therefore the number of axemes is greater than or equal to the number of vocables. In this microstructure, the "entryref" attribute allows us to manage the links between lexies and the entries of axeme type.  
 Figure 7: Microstructure of a volume of lexies  ? In this example, the value of "type" is the type of link, the value of "volume" is the target volume, the value of "idref" is the identifier of the axeme entry, the value of "lang" is the target language, and the value of "relationship-mono" is the label.  
95
? The microstructure of the entries of axeme type allows us to describe the links with entries of lexie type and the links with entries of axie type. The microstructure of the axies allows us to describe the links with the entries of axeme type.  Response time and use case The tests were performed with an instance of Jibiki-LINKS installed on a machine with an Intel Core i3 processor at 3.3 GHz with 8 GB of RAM.  The tool used to perform queries is wget. The command is run directly on the server to avoid the latency due to the network. We give three examples in Table 3, which show the number of links computed by the system, of entries displayed, of queries, of different languages, and the average response time. The response time, less than 1 second in these cases, is generally satisfactory. For better understanding, there is some details about the example "manger" (see Figure 8). We search "manger" in French, and find one entry with id "fra.manger.v" in the French vocable volume. The search direction is "up". This entry links to another entry of the volume of French axemes, whose id is CommonUNLDict.axeme.fra.eat(icl>consume>do, agt>living_thing, obj>concrete_thing, ins>thing) 15. This axeme entry links with one axie entry and the vocable entry fra.manger.v. Because the search direction is "up", we just go to the axie entry. When we arrive in the volume of axies, the search direction is changed to "down". The axie entry links to 6 different axeme entries. We search each axeme entry and its links. Because the search direction is "down", we only take into account vocable entries links. For each axeme entry, we find at least one vocable entry. In other cases, one vocable entry has more than one lexie, so it links to one or several axeme entries, and there are more links.  Search argument Links  Displayed entries  Number of requests  Different languages  Average time (ms)  French vocable "manger"  14  6  10  6  19.7  French vocable "recherche"  66  27  10  6  73.5  UNL "search(icl>action)"  51  20  10  6  56  Table 3: Response time on three examples  
 Figure 8: Links in the case of "manger"  Figure 9 shows the display of the interface for a classical search in a Web browser.  
                                                15 In order to better display figure, we have simplified the id in figure 8. 
96
 Figure 9: Display of the interface for a classical search  5 Conclusion and perspectives  In this article, we analysed the different types of lexical resource and presented a method of modelling lexical resources using volumes. This method allows us to manage complex resources while providing facilities for manipulation and treatment equivalent to those of a paper dictionary.  Jibiki-LINKS is a new version of the Jibiki platform, which can manage resources based on multilevel macrostructures using rich links, bearing attributes such as target volume, weight, type, language, open label, etc. To realize the implementation of rich links, we separated the module processing the links from the module processing other CDM pointers. Jibiki-LINKS has been used to implement the Mot?Mot, ProAxie and Pivax macrostructures.  On the Pivax-UNL platform, another instance of the Jibiki-LINKS-based Pivax macrostructure, we have installed the volumes corresponding to the CommonUNLDict resource of V. Dikonov, and tested our platform with that resource.  There is also a UW (UNL interlingual lexemes) resource of 8G entries that was created from DBpedia by David Rouquet. In that resource, there are several volumes for the same language. As links were poorly structured, we are currently working on this resource in order to recompute them. We hope to be able to import this resource, and to make tests at that very large scale in the near future.  To sum up, lexical databases equipped with rich links allow for importing XML-based electronic dictionaries without loss of information, whether they have been elaborated from source or printable forms (such as Word, rtf, ps, pdf) or directly produced in XML from a relational database, or using a dedicated editor knowing their microstructures. They also allow us to automatically produce from them a pivot-based macrostructure organised in volumes, and after that to edit and improve them, using a mixed textual and graphical interface to merge or split lexies, axemes or axies, or to enrich the links with appropriate labels. The introduction of rich links to multilevel lexical databases enhances them with a very interesting aspect of the lexical networks while keeping the classical ways of using dictionaries and of performing lexicographic work. References  EU-IATE (2007) A single database for all EU-related terminology (InterActiveTerminology for Europe) in 23 languages opens to the public. Press release. Brussels. 2007-06-28. Breen, J. W., (2004) JMdict : a Japanese-Multilingual Dictionary. In Gilles S?rasset, Susan Armstrong, Christian Boitet, Andrei Pospescu-Belis, and Dan Tufis, editors, post COLING Workshop on Multilingual Linguistic Resources, Geneva, Switzerland, 28th August. International Committee on Computational Linguistics.  
97
Dikonov V., Boguslavsky I., (2009) Semantic Network of the UNL Dictionary of Concepts. Proceedings of the SENSE Workshop on conceptual Structures for Extracting Natural language SEmantics Moscow, Russia, July 2009, 7 p. Diller, G.A., Beckwith, R., Fellbaum, C., Gross, D., and Miller, K.J. (1990) Introduction to WordNet: an on-line lexical database, International Journal of Lexicography 3(4), pp. 235-244. Dolan, W.B. & Richardson, S.D., (1996) Interactive Lexical Priming for Disambiguation. Proc. MIDDIM'96, Post-COLING seminar on Interactive Disambiguation, C. Boitet ed. Le Col de Porte, Is?re, France. 12-14 ao?t 1996. vol. 1/1 : pp. 54-56. Dong, Z.D., Dong, Q., Hao, C.L., (2010). HowNet and Its Computation of Meaning. In Actes de COLING-2010, Beijing, 4 p. Francopoulo, G., Bel, N., George, M., Calzolari, N., Monachini, M., Pet, M. and Soria, C. (2009). Multilingual resources for NLP in the lexical markup framework (LMF). In journal de Language Resources and Evaluation, March 2009, Volume 43, pp. 55-57. Gut, Y., Ramli, P. R. M., Yusoff, Z., Kim, Ch. Ch., Samat, S. A., Boitet, Ch., N?dobejkine, N., Lafourcade, M., Gaschler, J. and Levenbach, D. (1996). Kamus Perancis-Melayu Dewan, dictionnaire fran?ais-malais. Dewan Bahasa Dan Pustaka, Kuala Lumpur, 667 p. Lafourcade, M., Joubert, A. (2010). Computing trees of named word usages from a crowdsourced lexical network. Investigationes Linguisticae, vol. XXI, pp. 39-56  Lux-Pogodalla, V., Polgu?re, A. (2011) Construction of a French Lexical Network: Methodological Issues. Proceedings of the First International Workshop on Lexical Resources, WoLeR 2011. An ESSLLI-2011 Workshop. Ljubljana, 2011, pp. 54-61.  Mangeot, M. (2002). An XML Markup Language Framework for Lexical Databases Environments: the Dictionary Markup Language. In Actes de LREC-2002, pp. 37-44. Mangeot, M & Chalvin, A.  (2006). Dictionary Building with the Jibiki Platform: the GDEF case. In Actes de LREC-2006, Genoa, pp. 1666-1669.  Mangeot, M. & Touch, S., (2010) Mot?Mot project: building a multilingual lexical system via bilingual dictionaries. Proc. SLTU 2010: Second International Workshop on Spoken Languages Technologies for Under-Resourced Languages, Penang, Malaysia, 2010, 6 p. McCrae, J., Spohr, D. and Cimiano, P., (2011)  Linking lexical resources and ontologies on the semantic web with lemon. Proc. ESWC?11, Berlin, pp. 245-259. Nguyen, H.T., Boitet, C. and S?rasset, G. (2007). PIVAX, an online contributive lexical data base for heterogeneous MT systems using a lexical pivot. In Actes de SNLP-2007, Bangkok, 6 p. Richardson, S.D., Dolan, W.B. and Vanderwende, L. (1998) MindNet: acquiring and structuring semantic information from text, no. MSR-TR-98-23. S?rasset, G. (2012) Dbnary: Wiktionary as a LMF-based Multilingual RDF network. In Actes de LREC-2012, Istanbul, 7 p. S?rasset, G. & Mangeot, M. (2001). Papillon Lexical Database Project: Monolingual Dictionaries and Interlingual Links. In Proc. NLPRS-2011, Tokyo, pp. 119-125. Tran, M. (2006). Prolexbase : Un dictionnaire relationnel multilingue de noms propres : conception, impl?mentation et gestion en ligne. Th?se de doctorat, Tours, pp. 54-57. Vossen, P., (1998) EuroWordNet: A Multilingual Database with Lexical Semantic Networks, Computers and the Humanities, 32(2-3). Zhang, Y. & Mangeot, M., (2013).  Gestion des terminologies riches : l'exemple des acronymes. In Actes de TALN-2013, Les Sables d?Olonne, 8 p.  
98
