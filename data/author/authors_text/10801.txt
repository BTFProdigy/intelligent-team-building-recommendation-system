Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940?949,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Mixing Multiple Translation Models in Statistical Machine Translation
Majid Razmara1 George Foster2 Baskaran Sankaran1 Anoop Sarkar1
1 Simon Fraser University, 8888 University Dr., Burnaby, BC, Canada
{razmara,baskaran,anoop}@sfu.ca
2 National Research Council Canada, 283 Alexandre-Tache? Blvd, Gatineau, QC, Canada
george.foster@nrc.gc.ca
Abstract
Statistical machine translation is often faced
with the problem of combining training data
from many diverse sources into a single trans-
lation model which then has to translate sen-
tences in a new domain. We propose a novel
approach, ensemble decoding, which com-
bines a number of translation systems dynam-
ically at the decoding step. In this paper,
we evaluate performance on a domain adap-
tation setting where we translate sentences
from the medical domain. Our experimental
results show that ensemble decoding outper-
forms various strong baselines including mix-
ture models, the current state-of-the-art for do-
main adaptation in machine translation.
1 Introduction
Statistical machine translation (SMT) systems re-
quire large parallel corpora in order to be able to
obtain a reasonable translation quality. In statisti-
cal learning theory, it is assumed that the training
and test datasets are drawn from the same distribu-
tion, or in other words, they are from the same do-
main. However, bilingual corpora are only available
in very limited domains and building bilingual re-
sources in a new domain is usually very expensive.
It is an interesting question whether a model that is
trained on an existing large bilingual corpus in a spe-
cific domain can be adapted to another domain for
which little parallel data is present. Domain adap-
tation techniques aim at finding ways to adjust an
out-of-domain (OUT) model to represent a target do-
main (in-domain or IN).
Common techniques for model adaptation adapt
two main components of contemporary state-of-the-
art SMT systems: the language model and the trans-
lation model. However, language model adapta-
tion is a more straight-forward problem compared to
translation model adaptation, because various mea-
sures such as perplexity of adapted language models
can be easily computed on data in the target domain.
As a result, language model adaptation has been well
studied in various work (Clarkson and Robinson,
1997; Seymore and Rosenfeld, 1997; Bacchiani and
Roark, 2003; Eck et al, 2004) both for speech recog-
nition and for machine translation. It is also easier to
obtain monolingual data in the target domain, com-
pared to bilingual data which is required for transla-
tion model adaptation. In this paper, we focused on
adapting only the translation model by fixing a lan-
guage model for all the experiments. We expect do-
main adaptation for machine translation can be im-
proved further by combining orthogonal techniques
for translation model adaptation combined with lan-
guage model adaptation.
In this paper, a new approach for adapting the
translation model is proposed. We use a novel sys-
tem combination approach called ensemble decod-
ing in order to combine two or more translation
models with the goal of constructing a system that
outperforms all the component models. The strength
of this system combination method is that the sys-
tems are combined in the decoder. This enables
the decoder to pick the best hypotheses for each
span of the input. The main applications of en-
semble models are domain adaptation, domain mix-
ing and system combination. We have modified
Kriya (Sankaran et al, 2012), an in-house imple-
mentation of hierarchical phrase-based translation
system (Chiang, 2005), to implement ensemble de-
coding using multiple translation models.
We compare the results of ensemble decoding
with a number of baselines for domain adaptation.
In addition to the basic approach of concatenation of
in-domain and out-of-domain data, we also trained
a log-linear mixture model (Foster and Kuhn, 2007)
940
as well as the linear mixture model of (Foster et al,
2010) for conditional phrase-pair probabilities over
IN and OUT. Furthermore, within the framework of
ensemble decoding, we study and evaluate various
methods for combining translation tables.
2 Baselines
The natural baseline for model adaption is to con-
catenate the IN and OUT data into a single paral-
lel corpus and train a model on it. In addition to
this baseline, we have experimented with two more
sophisticated baselines which are based on mixture
techniques.
2.1 Log-Linear Mixture
Log-linear translation model (TM) mixtures are of
the form:
p(e?|f?) ? exp
( M?
m
?m log pm(e?|f?)
)
where m ranges over IN and OUT, pm(e?|f?) is an
estimate from a component phrase table, and each
?m is a weight in the top-level log-linear model, set
so as to maximize dev-set BLEU using minimum
error rate training (Och, 2003). We learn separate
weights for relative-frequency and lexical estimates
for both pm(e?|f?) and pm(f? |e?). Thus, for 2 compo-
nent models (from IN and OUT training corpora),
there are 4 ? 2 = 8 TM weights to tune. Whenever
a phrase pair does not appear in a component phrase
table, we set the corresponding pm(e?|f?) to a small
epsilon value.
2.2 Linear Mixture
Linear TM mixtures are of the form:
p(e?|f?) =
M?
m
?mpm(e?|f?)
Our technique for setting ?m is similar to that
outlined in Foster et al (2010). We first extract a
joint phrase-pair distribution p?(e?, f?) from the de-
velopment set using standard techniques (HMM
word alignment with grow-diag-and symmeteriza-
tion (Koehn et al, 2003)). We then find the set
of weights ?? that minimize the cross-entropy of the
mixture p(e?|f?) with respect to p?(e?, f?):
?? = argmax
?
?
e?,f?
p?(e?, f?) log
M?
m
?mpm(e?|f?)
For efficiency and stability, we use the EM algo-
rithm to find ??, rather than L-BFGS as in (Foster et
al., 2010). Whenever a phrase pair does not appear
in a component phrase table, we set the correspond-
ing pm(e?|f?) to 0; pairs in p?(e?, f?) that do not appear
in at least one component table are discarded. We
learn separate linear mixtures for relative-frequency
and lexical estimates for both p(e?|f?) and p(f? |e?).
These four features then appear in the top-level
model as usual ? there is no runtime cost for the lin-
ear mixture.
3 Ensemble Decoding
Ensemble decoding is a way to combine the exper-
tise of different models in one single model. The
current implementation is able to combine hierar-
chical phrase-based systems (Chiang, 2005) as well
as phrase-based translation systems (Koehn et al,
2003). However, the method can be easily extended
to support combining a number of heterogeneous
translation systems e.g. phrase-based, hierarchical
phrase-based, and/or syntax-based systems. This
section explains how such models can be combined
during the decoding.
Given a number of translation models which are
already trained and tuned, the ensemble decoder
uses hypotheses constructed from all of the models
in order to translate a sentence. We use the bottom-
up CKY parsing algorithm for decoding. For each
sentence, a CKY chart is constructed. The cells of
the CKY chart are populated with appropriate rules
from all the phrase tables of different components.
As in the Hiero SMT system (Chiang, 2005), the
cells which span up to a certain length (i.e. the max-
imum span length) are populated from the phrase-
tables and the rest of the chart uses glue rules as de-
fined in (Chiang, 2005).
The rules suggested from the component models
are combined in a single set. Some of the rules may
be unique and others may be common with other
component model rule sets, though with different
scores. Therefore, we need to combine the scores
of such common rules and assign a single score to
941
them. Depending on the mixture operation used for
combining the scores, we would get different mix-
ture scores. The choice of mixture operation will be
discussed in Section 3.1.
Figure 1 illustrates how the CKY chart is filled
with the rules. Each cell, covering a span, is popu-
lated with rules from all component models as well
as from cells covering a sub-span of it.
In the typical log-linear model SMT, the posterior
probability for each phrase pair (e?, f?) is given by:
p(e? | f?) ? exp
(
?
i
wi?i(e?, f?)
? ?? ?
w??
)
Ensemble decoding uses the same framework for
each individual system. Therefore, the score of a
phrase-pair (e?, f?) in the ensemble model is:
p(e? | f?) ? exp
(
w1 ? ?1? ?? ?
1st model
? w2 ? ?2? ?? ?
2nd model
? ? ? ?
)
where? denotes the mixture operation between two
or more model scores.
3.1 Mixture Operations
Mixture operations receive two or more scores
(probabilities) and return the mixture score (prob-
ability). In this section, we explore different options
for mixture operation and discuss some of the char-
acteristics of these mixture operations.
? Weighted Sum (wsum): in wsum the ensemble
probability is proportional to the weighted sum
of all individual model probabilities (i.e. linear
mixture).
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component mod-
els, M is the total number of them and ?i is the
weight for component i.
? Weighted Max (wmax): where the ensemble
score is the weighted max of all model scores.
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
? Model Switching (Switch): in model switch-
ing, each cell in the CKY chart gets populated
only by rules from one of the models and the
other models? rules are discarded. This is based
on the hypothesis that each component model
is an expert on certain parts of sentence. In this
method, we need to define a binary indicator
function ?(f? ,m) for each span and component
model to specify rules of which model to retain
for each span.
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each cell,
?(f? , n), could be based on:
? Max: for each cell, the model that has the
highest weighted best-rule score wins:
?(f? , n) = ?n max
e
(wn ? ?n(e?, f?))
? Sum: Instead of comparing only the
scores of the best rules, the model with
the highest weighted sum of the probabil-
ities of the rules wins. This sum has to
take into account the translation table limit
(ttl), on the number of rules suggested by
each model for each cell:
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
computed as:
p(e? | f?) =
M?
m
?(f? ,m) pm(e? | f?)
? Product (prod): in Product models or Prod-
uct of Experts (Hinton, 1999), the probability
of the ensemble model or a rule is computed as
the product of the probabilities of all compo-
nents (or equally the sum of log-probabilities,
i.e. log-linear mixture). Product models can
also make use of weights to control the contri-
bution of each component. These models are
942
Figure 1: The cells in the CKY chart are populated using rules from all component models and sub-span cells.
generally known as Logarithmic Opinion Pools
(LOPs) where:
p(e? | f?) ? exp
(
M?
m
?m (wm ? ?m)
)
Product models have been used in combining
LMs and TMs in SMT as well as some other
NLP tasks such as ensemble parsing (Petrov,
2010).
Each of these mixture operations has a specific
property that makes it work in specific domain adap-
tation or system combination scenarios. For in-
stance, LOPs may not be optimal for domain adapta-
tion in the setting where there are two or more mod-
els trained on heterogeneous corpora. As discussed
in (Smith et al, 2005), LOPs work best when all the
models accuracies are high and close to each other
with some degree of diversity. LOPs give veto power
to any of the component models and this perfectly
works for settings such as the one in (Petrov, 2010)
where a number of parsers are trained by changing
the randomization seeds but having the same base
parser and using the same training set. They no-
ticed that parsers trained using different randomiza-
tion seeds have high accuracies but there are some
diversities among them and they used product mod-
els for their advantage to get an even better parser.
We assume that each of the models is expert in some
parts and so they do not necessarily agree on cor-
rect hypotheses. In other words, product models (or
LOPs) tend to have intersection-style effects while
we are more interested in union-style effects.
In Section 4.2, we compare the BLEU scores of
different mixture operations on a French-English ex-
perimental setup.
3.2 Normalization
Since in log-linear models, the model scores are
not normalized to form probability distributions, the
scores that different models assign to each phrase-
pair may not be in the same scale. Therefore, mixing
their scores might wash out the information in one
(or some) of the models. We experimented with two
different ways to deal with this normalization issue.
A practical but inexact heuristic is to normalize the
scores over a shorter list. So the list of rules coming
from each model for a cell in CKY chart is normal-
ized before getting mixed with other phrase-table
rules. However, experiments showed changing the
scores with the normalized scores hurts the BLEU
score radically. So we use the normalized scores
only for pruning and the actual scores are intact.
We could also globally normalize the scores to ob-
tain posterior probabilities using the inside-outside
algorithm. However, we did not try it as the BLEU
scores we got using the normalization heuristic was
not promissing and it would impose a cost in de-
coding as well. More investigation on this issue has
been left for future work.
A more principled way is to systematically find
the most appropriate model weights that can avoid
this problem by scaling the scores properly. We
used a publicly available toolkit, CONDOR (Van-
den Berghen and Bersini, 2005), a direct optimizer
based on Powell?s algorithm, that does not require
943
explicit gradient information for the objective func-
tion. Component weights for each mixture operation
are optimized on the dev-set using CONDOR.
4 Experiments & Results
4.1 Experimental Setup
We carried out translation experiments using the Eu-
ropean Medicines Agency (EMEA) corpus (Tiede-
mann, 2009) as IN, and the Europarl (EP) corpus1 as
OUT, for French to English translation. The dev and
test sets were randomly chosen from the EMEA cor-
pus.2 The details of datasets used are summarized in
Table 1.
Dataset Sents
Words
French English
EMEA 11770 168K 144K
Europarl 1.3M 40M 37M
Dev 1533 29K 25K
Test 1522 29K 25K
Table 1: Training, dev and test sets for EMEA.
For the mixture baselines, we used a standard
one-pass phrase-based system (Koehn et al, 2003),
Portage (Sadat et al, 2005), with the following 7
features: relative-frequency and lexical translation
model (TM) probabilities in both directions; word-
displacement distortion model; language model
(LM) and word count. The corpus was word-aligned
using both HMM and IBM2 models, and the phrase
table was the union of phrases extracted from these
separate alignments, with a length limit of 7. It
was filtered to retain the top 20 translations for each
source phrase using the TM part of the current log-
linear model.
For ensemble decoding, we modified an in-house
implementation of hierarchical phrase-based sys-
tem, Kriya (Sankaran et al, 2012) which uses the
same features mentioned in (Chiang, 2005): for-
ward and backward relative-frequency and lexical
TM probabilities; LM; word, phrase and glue-rules
penalty. GIZA++(Och and Ney, 2000) has been used
for word alignment with phrase length limit of 7.
In both systems, feature weights were optimized
using MERT (Och, 2003) and with a 5-gram lan-
1www.statmt.org/europarl
2Please contact the authors to access the data-sets.
guage model and Kneser-Ney smoothing was used
in all the experiments. We used SRILM (Stolcke,
2002) as the langugage model toolkit. Fixing the
language model allows us to compare various trans-
lation model combination techniques.
4.2 Results
Table 2 shows the results of the baselines. The first
group are the baseline results on the phrase-based
system discussed in Section 2 and the second group
are those of our hierarchical MT system. Since the
Hiero baselines results were substantially better than
those of the phrase-based model, we also imple-
mented the best-performing baseline, linear mixture,
in our Hiero-style MT system and in fact it achieves
the hights BLEU score among all the baselines as
shown in Table 2. This baseline is run three times
the score is averaged over the BLEU scores with
standard deviation of 0.34.
Baseline PBS Hiero
IN 31.84 33.69
OUT 24.08 25.32
IN + OUT 31.75 33.76
LOGLIN 32.21 ?
LINMIX 33.81 35.57
Table 2: The results of various baselines implemented in
a phrase-based (PBS) and a Hiero SMT on EMEA.
Table 3 shows the results of ensemble decoding
with different mixture operations and model weight
settings. Each mixture operation has been evalu-
ated on the test-set by setting the component weights
uniformly (denoted by uniform) and by tuning the
weights using CONDOR (denoted by tuned) on a
held-out set. The tuned scores (3rd column in Ta-
ble 3) are averages of three runs with different initial
points as in Clark et al (2011). We also reported the
BLEU scores when we applied the span-wise nor-
malization heuristic. All of these mixture operations
were able to significantly improve over the concate-
nation baseline. In particular, Switching:Max could
gain up to 2.2 BLEU points over the concatenation
baseline and 0.39 BLEU points over the best per-
forming baseline (i.e. linear mixture model imple-
mented in Hiero) which is statistically significant
based on Clark et al (2011) (p = 0.02).
Prod when using with uniform weights gets the
944
Mixture Operation Uniform Tuned Norm.
WMAX 35.39 35.47 (s=0.03) 35.47
WSUM 35.35 35.53 (s=0.04) 35.45
SWITCHING:MAX 35.93 35.96 (s=0.01) 32.62
SWITCHING:SUM 34.90 34.72 (s=0.23) 34.90
PROD 33.93 35.24 (s=0.05) 35.02
Table 3: The results of ensemble decoding on EMEA for Fr2En when using uniform weights, tuned weights and
normalization heuristic. The tuned BLEU scores are averaged over three runs with multiple initial points, as in (Clark
et al, 2011), with the standard deviations in brackets .
lowest score among the mixture operations, how-
ever after tuning, it learns to bias the weights to-
wards one of the models and hence improves by
1.31 BLEU points. Although Switching:Sum outper-
forms the concatenation baseline, it is substantially
worse than other mixture operations. One explana-
tion that Switching:Max is the best performing op-
eration and Switching:Sum is the worst one, despite
their similarities, is that Switching:Max prefers more
peaked distributions while Switching:Sum favours a
model that has fewer hypotheses for each span.
An interesting observation based on the results in
Table 3 is that uniform weights are doing reasonably
well given that the component weights are not opti-
mized and therefore model scores may not be in the
same scope (refer to discussion in ?3.2). We suspect
this is because a single LM is shared between both
models. This shared component controls the vari-
ance of the weights in the two models when com-
bined with the standard L-1 normalization of each
model?s weights and hence prohibits models to have
too varied scores for the same input. Though, it may
not be the case when multiple LMs are used which
are not shared.
Two sample sentences from the EMEA test-set
along with their translations by the IN, OUT and En-
semble models are shown in Figure 2. The boxes
show how the Ensemble model is able to use n-
grams from the IN and OUT models to construct
a better translation than both of them. In the first
example, there are two OOVs one for each of the
IN and OUT models. Our approach is able to re-
solve the OOV issues by taking advantage of the
other model?s presence. Similarly, the second exam-
ple shows how ensemble decoding improves lexical
choices as well as word re-orderings.
5 Related Work
5.1 Domain Adaptation
Early approaches to domain adaptation involved in-
formation retrieval techniques where sentence pairs
related to the target domain were retrieved from the
training corpus using IR methods (Eck et al, 2004;
Hildebrand et al, 2005). Foster et al (2010), how-
ever, uses a different approach to select related sen-
tences from OUT. They use language model per-
plexities from IN to select relavant sentences from
OUT. These sentences are used to enrich the IN
training set.
Other domain adaptation methods involve tech-
niques that distinguish between general and domain-
specific examples (Daume? and Marcu, 2006). Jiang
and Zhai (2007) introduce a general instance weight-
ing framework for model adaptation. This approach
tries to penalize misleading training instances from
OUT and assign more weight to IN-like instances
than OUT instances. Foster et al (2010) propose a
similar method for machine translation that uses fea-
tures to capture degrees of generality. Particularly,
they include the output from an SVM classifier that
uses the intersection between IN and OUT as pos-
itive examples. Unlike previous work on instance
weighting in machine translation, they use phrase-
level instances instead of sentences.
A large body of work uses interpolation tech-
niques to create a single TM/LM from interpolating
a number of LMs/TMs. Two famous examples of
such methods are linear mixtures and log-linear mix-
tures (Koehn and Schroeder, 2007; Civera and Juan,
2007; Foster and Kuhn, 2007) which were used as
baselines and discussed in Section 2. Other meth-
ods include using self-training techniques to exploit
monolingual in-domain data (Ueffing et al, 2007;
945
SOURCE ame?norrhe?e , menstruations irre?gulie`res
REF amenorrhoea , irregular menstruation
IN amenorrhoea , menstruations irre?gulie`res
OUT ame?norrhe?e , irregular menstruation
ENSEMBLE amenorrhoea , irregular menstruation
SOURCE le traitement par naglazyme doit e?tre supervise? par un me?decin ayant l? expe?rience de
la prise en charge des patients atteints de mps vi ou d? une autre maladie me?tabolique
he?re?ditaire .
REF naglazyme treatment should be supervised by a physician experienced in the manage-
ment of patients with mps vi or other inherited metabolic diseases .
IN naglazyme treatment should be supervise? by a doctor the with
in the management of patients with mps vi or other hereditary metabolic disease .
OUT naglazyme ?s treatment must be supervised by a doctor with the experience of the care
of patients with mps vi. or another disease hereditary metabolic .
ENSEMBLE naglazyme treatment should be supervised by a physician experienced
in the management of patients with mps vi or other hereditary metabolic disease .
Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems.
Bertoldi and Federico, 2009). In this approach, a
system is trained on the parallel OUT and IN data
and it is used to translate the monolingual IN data
set. Iteratively, most confident sentence pairs are se-
lected and added to the training corpus on which a
new system is trained.
5.2 System Combination
Tackling the model adaptation problem using sys-
tem combination approaches has been experimented
in various work (Koehn and Schroeder, 2007; Hilde-
brand and Vogel, 2009). Among these approaches
are sentence-based, phrase-based and word-based
output combination methods. In a similar approach,
Koehn and Schroeder (2007) use a feature of the fac-
tored translation model framework in Moses SMT
system (Koehn and Schroeder, 2007) to use multiple
alternative decoding paths. Two decoding paths, one
for each translation table (IN and OUT), were used
during decoding. The weights are set with minimum
error rate training (Och, 2003).
Our work is closely related to Koehn and
Schroeder (2007) but uses a different approach to
deal with multiple translation tables. The Moses
SMT system implements (Koehn and Schroeder,
2007) and can treat multiple translation tables in
two different ways: intersection and union. In in-
tersection, for each span only the hypotheses would
be used that are present in all phrase tables. For
each set of hypothesis with the same source and
target phrases, a new hypothesis is created whose
feature-set is the union of feature sets of all corre-
sponding hypotheses. Union, on the other hand, uses
hypotheses from all the phrase tables. The feature
set of these hypotheses are expanded to include one
feature set for each table. However, for the corre-
sponding feature values of those phrase-tables that
did not have a particular phrase-pair, a default log
probability value of 0 is assumed (Bertoldi and Fed-
erico, 2009) which is counter-intuitive as it boosts
the score of hypotheses with phrase-pairs that do not
belong to all of the translation tables.
Our approach is different from Koehn and
Schroeder (2007) in a number of ways. Firstly, un-
like the multi-table support of Moses which only
supports phrase-based translation table combination,
our approach supports ensembles of both hierarchi-
cal and phrase-based systems. With little modifica-
tion, it can also support ensemble of syntax-based
systems with the other two state-of-the-art SMT sys-
946
tems. Secondly, our combining method uses the
union option, but instead of preserving the features
of all phrase-tables, it only combines their scores
using various mixture operations. This enables us
to experiment with a number of different opera-
tions as opposed to sticking to only one combination
method. Finally, by avoiding increasing the number
of features we can add as many translation models
as we need without serious performance drop. In
addition, MERT would not be an appropriate opti-
mizer when the number of features increases a cer-
tain amount (Chiang et al, 2008).
Our approach differs from the model combina-
tion approach of DeNero et al (2010), a generaliza-
tion of consensus or minimum Bayes risk decoding
where the search space consists of those of multi-
ple systems, in that model combination uses forest
of derivations of all component models to do the
combination. In other words, it requires all compo-
nent models to fully decode each sentence, compute
n-gram expectations from each component model
and calculate posterior probabilities over transla-
tion derivations. While, in our approach we only
use partial hypotheses from component models and
the derivation forest is constructed by the ensemble
model. A major difference is that in the model com-
bination approach the component search spaces are
conjoined and they are not intermingled as opposed
to our approach where these search spaces are inter-
mixed on spans. This enables us to generate new
sentences that cannot be generated by component
models. Furthermore, various combination methods
can be explored in our approach. Finally, main tech-
niques used in this work are orthogonal to our ap-
proach such as Minimum Bayes Risk decoding, us-
ing n-gram features and tuning using MERT.
Finally, our work is most similar to that of
Liu et al (2009) where max-derivation and max-
translation decoding have been used. Max-
derivation finds a derivation with highest score and
max-translation finds the highest scoring translation
by summing the score of all derivations with the
same yield. The combination can be done in two
levels: translation-level and derivation-level. Their
derivation-level max-translation decoding is similar
to our ensemble decoding with wsum as the mixture
operation. We did not restrict ourself to this par-
ticular mixture operation and experimented with a
number of different mixing techniques and as Ta-
ble 3 shows we could improve over wsum in our
experimental setup. Liu et al (2009) used a mod-
ified version of MERT to tune max-translation de-
coding weights, while we use a two-step approach
using MERT for tuning each component model sep-
arately and then using CONDOR to tune component
weights on top of them.
6 Conclusion & Future Work
In this paper, we presented a new approach for do-
main adaptation using ensemble decoding. In this
approach a number of MT systems are combined at
decoding time in order to form an ensemble model.
The model combination can be done using various
mixture operations. We showed that this approach
can gain up to 2.2 BLEU points over its concatena-
tion baseline and 0.39 BLEU points over a powerful
mixture model.
Future work includes extending this approach to
use multiple translation models with multiple lan-
guage models in ensemble decoding. Different
mixture operations can be investigated and the be-
haviour of each operation can be studied in more
details. We will also add capability of support-
ing syntax-based ensemble decoding and experi-
ment how a phrase-based system can benefit from
syntax information present in a syntax-aware MT
system. Furthermore, ensemble decoding can be ap-
plied on domain mixing settings in which develop-
ment sets and test sets include sentences from dif-
ferent domains and genres, and this is a very suit-
able setting for an ensemble model which can adapt
to new domains at test time. In addition, we can
extend our approach by applying some of the tech-
niques used in other system combination approaches
such as consensus decoding, using n-gram features,
tuning using forest-based MERT, among other pos-
sible extensions.
Acknowledgments
This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Fac-
ulty Award to the last author. We would like to
thank Philipp Koehn and the anonymous reviewers
for their valuable comments. We also thank the de-
velopers of GIZA++ and Condor which we used for
our experiments.
947
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In Acoustics, Speech, and
Signal Processing, 2003. Proceedings. (ICASSP ?03).
2003 IEEE International Conference on, volume 1,
pages I?224 ? I?227 vol.1, april.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 182?189, Stroudsburg, PA, USA. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. ACL.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Mor-
ristown, NJ, USA. ACL.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, StatMT ?07, pages
177?180, Stroudsburg, PA, USA. ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume 2,
HLT ?11, pages 176?181. ACL.
P. Clarkson and A. Robinson. 1997. Language model
adaptation using mixtures and an exponentially decay-
ing cache. In Proceedings of the 1997 IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP ?97)-Volume 2 - Volume 2,
ICASSP ?97, pages 799?, Washington, DC, USA.
IEEE Computer Society.
Hal Daume?, III and Daniel Marcu. 2006. Domain
adaptation for statistical classifiers. J. Artif. Int. Res.,
26:101?126, May.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 975?983, Stroudsburg, PA, USA. ACL.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In In Pro-
ceedings of LREC.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for smt. In Proceedings of the Second
Workshop on Statistical Machine Translation, StatMT
?07, pages 128?135, Stroudsburg, PA, USA. ACL.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 451?
459, Stroudsburg, PA, USA. ACL.
Almut Silja Hildebrand and Stephan Vogel. 2009. CMU
system combination for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 47?50, Stroudsburg, PA, USA.
ACL.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th EAMT
2005, Budapest, Hungary, May.
Geoffrey E. Hinton. 1999. Products of experts. In Artifi-
cial Neural Networks, 1999. ICANN 99. Ninth Interna-
tional Conference on (Conf. Publ. No. 470), volume 1,
pages 1?6.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264?271, Prague, Czech
Republic, June. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 224?
227, Stroudsburg, PA, USA. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, pages 127?133, Edmonton, May.
NAACL.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ?09, pages
576?584, Stroudsburg, PA, USA. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual Meet-
ing of the ACL, pages 440?447, Hongkong, China, Oc-
tober.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the ACL, Sapporo, July. ACL.
948
Slav Petrov. 2010. Products of random latent variable
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 19?27, Stroudsburg, PA, USA. ACL.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Joel Martin, and Aaron Tikuisis. 2005.
Portage: A phrase-based machine translation system.
In In Proceedings of the ACL Worskhop on Building
and Using Parallel Texts, Ann Arbor. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, 97(97), April.
Kristie Seymore and Ronald Rosenfeld. 1997. Us-
ing story topics for language model adaptation. In
George Kokkinakis, Nikos Fakotakis, and Evangelos
Dermatas, editors, EUROSPEECH. ISCA.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 18?25, Stroudsburg, PA, USA. ACL.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286.
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 25?32, Prague, Czech Republic, June. ACL.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175,
September.
949
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1105?1115,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Graph Propagation for Paraphrasing Out-of-Vocabulary Words in
Statistical Machine Translation?
Majid Razmara1 Maryam Siahbani1 Gholamreza Haffari2 Anoop Sarkar1
1 Simon Fraser University, Burnaby, BC, Canada
{razmara,msiahban,anoop}@sfu.ca
2 Monash University, Clayton, VIC, Australia
reza@monash.edu
Abstract
Out-of-vocabulary (oov) words or phrases
still remain a challenge in statistical machine
translation especially when a limited amount of
parallel text is available for training or when
there is a domain shift from training data to
test data. In this paper, we propose a novel
approach to finding translations for oov words.
We induce a lexicon by constructing a graph on
source language monolingual text and employ
a graph propagation technique in order to find
translations for all the source language phrases.
Our method differs from previous approaches
by adopting a graph propagation approach that
takes into account not only one-step (from oov
directly to a source language phrase that has a
translation) but multi-step paraphrases from oov
source language words to other source language
phrases and eventually to target language transla-
tions. Experimental results show that our graph
propagation method significantly improves per-
formance over two strong baselines under intrin-
sic and extrinsic evaluation metrics.
1 Introduction
Out-of-vocabulary (oov) words or phrases still re-
main a challenge in statistical machine translation.
SMT systems usually copy unknown words verba-
tim to the target language output. Although this is
helpful in translating a small fraction of oovs such
as named entities for languages with same writ-
ing systems, it harms the translation in other types
of oovs and distant language pairs. In general,
copied-over oovs are a hindrance to fluent, high
quality translation, and we can see evidence of this
in automatic measures such as BLEU (Papineni
et al, 2002) and also in human evaluation scores
such as HTER. The problem becomes more se-
vere when only a limited amount of parallel text is
available for training or when the training and test
data are from different domains. Even noisy trans-
lation of oovs can aid the language model to better
?This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant. The third author was sup-
ported by an early career research award from Monash Uni-
versity to visit Simon Fraser University.
re-order the words in the target language (Zhang
et al, 2012).
Increasing the size of the parallel data can re-
duce the number of oovs. However, there will al-
ways be some words or phrases that are new to the
system and finding ways to translate such words
or phrases will be beneficial to the system. Re-
searchers have applied a number of approaches to
tackle this problem. Some approaches use pivot
languages (Callison-Burch et al, 2006) while oth-
ers use lexicon-induction-based approaches from
source language monolingual corpora (Koehn and
Knight, 2002; Garera et al, 2009; Marton et al,
2009).
Pivot language techniques tackle this problem
by taking advantage of available parallel data be-
tween the source language and a third language.
Using a pivot language, oovs are translated into a
third language and back into the source language
and thereby paraphrases to those oov words are
extracted (Callison-Burch et al, 2006). For each
oov, the system can be augmented by aggregating
the translations of all its paraphrases and assign
them to the oov. However, these methods require
parallel corpora between the source language and
one or multiple pivot languages.
Another line of work exploits spelling and mor-
phological variants of oov words. Habash (2008)
presents techniques for online handling of oov
words for Arabic to English such as spelling ex-
pansion and morphological expansion. Huang et
al. (2011) proposes a method to combine sub-
lexical/constituent translations of an oov word or
phrase to generate its translations.
Several researchers have applied lexicon-
induction methods to create a bilingual lexicon
for those oovs. Marton et al (2009) use a mono-
lingual text on the source side to find paraphrases
to oov words for which the translations are avail-
able. The translations for these paraphrases are
1105
then used as the translations of the oov word.
These methods are based on the distributional hy-
pothesis which states that words appearing in the
same contexts tend to have similar meaning (Har-
ris, 1954). Marton et al (2009) showed that this
method improves over the baseline system where
oovs are untranslated.
We propose a graph propagation-based exten-
sion to the approach of Marton et al (2009) in
which a graph is constructed from source language
monolingual text1 and the source-side of the avail-
able parallel data. Nodes that have related mean-
ings are connected together and nodes for which
we have translations in the phrase-table are an-
notated with target-side translations and their fea-
ture values. A graph propagation algorithm is then
used to propagate translations from labeled nodes
to unlabeled nodes (phrases appearing only in the
monolingual text and oovs). This provides a gen-
eral purpose approach to handle several types of
oovs, including morphological variants, spelling
variants and synonyms2.
Constructing such a huge graph and propagat-
ing messages through it pose severe computational
challenges. Throughout the paper, we will see how
these challenges are dealt with using scalable algo-
rithms.
2 Collocational Lexicon Induction
Rapp (1995) introduced the notion of a distribu-
tional profile in bilingual lexicon induction from
monolingual data. A distributional profile (DP) of
a word or phrase type is a co-occurrence vector
created by combining all co-occurrence vectors of
the tokens of that phrase type. Each distributional
profile can be seen as a point in a |V |-dimensional
space where V is the vocabulary where each word
type represents a unique axis. Points (i.e. phrase
types) that are close to one another in this high-
dimensional space can represent paraphrases. This
approach has also been used in machine trans-
lation to find in-vocabulary paraphrases for oov
words on the source side and find a way to trans-
late them.
2.1 Baseline System
Marton et al (2009) was the first to successfully
integrate a collocational approach to finding trans-
1Here on by monolingual data we always mean monolin-
gual data on the source language
2Named entity oovs may be handled properly by copying
or transliteration.
lations for oov words into an end-to-end SMT sys-
tem. We explain their method in detail as we will
compare against this approach. The method re-
lies on monolingual distributional profiles (DPs)
which are numerical vectors representing the con-
text around each word. The goal is to find words or
phrases that appear in similar contexts as the oovs.
For each oov a distributional profile is created by
collecting all words appearing in a fixed distance
from all occurrences of the oov word in the mono-
lingual text. These co-occurrence counts are con-
verted to an association measure (Section 2.2) that
encodes the relatedness of each pair of words or
phrases.
Then, the most similar phrases to each oov are
found by measuring the similarity of their DPs to
that of the oov word. Marton et al (2009) uses
a heuristic to prune the search space for finding
candidate paraphrases by keeping the surrounding
context (e.g. L R) of each occurrences of the
oov word. All phrases that appear in any of such
contexts are collected as candidate paraphrases.
For each of these paraphrases, a DP is constructed
and compared to that of the oov word using a sim-
ilarity measure (Section 2.2).
The top-k paraphrases that have translations in
the phrase-table are used to assign translations and
scores to each oov word by marginalizing transla-
tions over paraphrases:
p(t|o) =
?
s
p(t|s)p(s|o)
where t is a phrase on the target side, o is the oov
word or phrase, and s is a paraphrase of o. p(s|o)
is estimated using a similarity measure over DPs
and p(t|s) is coming from the phrase-table.
We reimplemented this collocational approach
for finding translations for oovs and used it as a
baseline system.
Alternative ways of modeling and comparing
distributional profiles have been proposed (Rapp,
1999; Fung and Yee, 1998; Terra and Clarke,
2003; Garera et al, 2009; Marton et al, 2009).
We review some of them here and compare their
performance in Section 4.3.
2.2 Association Measures
Given a word u, its distributional profile DP (u)
is constructed by counting surrounding words (in
a fixed window size) in a monolingual corpus.
DP (u) = {?A(u,wi)? | wi ? V }
1106
The counts can be collected in positional3 (Rapp,
1999) or non-positional way (count all the word
occurrences within the sliding window). A(?, ?)
is an association measure and can simply be de-
fined as co-occurrence counts within sliding win-
dows. Stronger association measures can also be
used such as:
Conditional probability: the probability for the
occurrence of each word in DP given the occur-
rence of u: CP(u,wi) = P (wi|u) (Schu?tze and
Pedersen, 1997)
Pointwise Mutual Information: this measure is
a transformation of the independence assumption
into a ratio. Positive values indicate that words
co-occur more than what we expect under the in-
dependence assumption (Lin, 1998):
PMI(u,wi) = log2 P (u,wi)P (u)P (wi)
Likelihood ratio: (Dunning, 1993) uses the like-
lihood ratio for word similarity:
?(u,wi) =
L(P (wi|u); p) ? L(P (wi|?u); p)
L(P (wi|u); p1) ? L(P (wi|?u); p2)
where L is likelihood function under the assump-
tion that word counts in text have binomial distri-
butions. The numerator represents the likelihood
of the hypothesis that u and wi are independent
(P (wi|u) = P (wi|?u) = p) and the denomina-
tor represents the likelihood of the hypothesis that
u and wi are dependent (P (wi|u) 6= P (wi|?u) ,
P (wi|u) = p1, P (wi|?u) = p2 )4.
Chi-square test: is a statistical hypothesis testing
method to evaluate independence of two categori-
cal random variables, e.g. whether the occurrence
of u and wi (denoted by x and y respectively) are
independent. The test statistics ?2(u,wi) is the
deviation of the observed counts fx,y from their
expected values Ex,y:
?2(u,wi) :=
?
x?{wi,?wi}
?
y?{u,?u}
(fx,y ? Ex,y)2
Ex,y
2.3 Similarity Measures
Various functions have been used to estimate
the similarity between distributional profiles.
3e.g., position 1 is the word immediately after, position -1
is the word immediately before etc.
4Binomial distribution B(k;n, ?) gives the probability of
observing k heads in n tosses of a coin where the coin pa-
rameter is ?. In our context, p, p1 and p2 are parameters of
Binomial distributions estimated using maximum likelihood.
Given two distributional profiles DP (u) and
DP (v), some similarity functions can be defined
as follows. Note that A(?, ?) stands for the various
association measures defined in Sec. 2.2.
Cosine coefficient is the cosine the angle between
two vectors DP (u) and DP (v):
cos(DP (u), DP (v)) =?
wi?V A(u,wi)A(v, wi)??
wi?V A(u,wi)2
??
wi?V A(v, wi)2
L1-Norm computes the accumulated distance
between entries of two distributional profiles
(L1(?, ?)). It has been used as word similarity mea-
sure in language modeling (Dagan et al, 1999).
L1(DP (u), DP (v)) =
?
wi?V
|A(u,wi)?A(v, wi)|
Jensen-Shannon Divergence is a symmetric ver-
sion of contextual average mutual information
(KL) which is used by (Dagan et al, 1999) as
word similarity measure.
JSD(DP (u), DP (v)) =KL(DP (u), AV GDP (u, v))+
KL(DP (v), AV GDP (u, v))
AV GDP (u, v) =
{
A(u,wi) +A(v, wi)
2 | wi ? V
}
KL(DP (u), DP (v)) =
?
wi?V
A(u,wi)log
A(u,wi)
A(v, wi)
3 Graph-based Lexicon Induction
We propose a novel approach to alleviate the oov
problem. Given a (possibly small amount of) par-
allel data between the source and target languages,
and a large monolingual data in the source lan-
guage, we construct a graph over all phrase types
in the monolingual text and the source side of the
parallel corpus and connect phrases that have sim-
ilar meanings (i.e. appear in similar context) to one
another. To do so, the distributional profiles of
all source phrase types are created. Each phrase
type represents a vertex in the graph and is con-
nected to other vertices with a weight defined by a
similarity measure between the two profiles (Sec-
tion 2.3). There are three types of vertices in the
graph: i) labeled nodes which appear in the par-
allel corpus and for which we have the target-side
1107
translations5; ii) oov nodes from the dev/test set
for which we seek labels (translations); and iii) un-
labeled nodes (words or phrases) from the mono-
lingual data which appear usually between oov
nodes and labeled nodes. When a relatively small
parallel data is used, unlabeled nodes outnumber
labeled ones and many of them lie on the paths
between an oov node to labeled ones.
Marton et al (2009)?s approach ignores these
bridging nodes and connects each oov node to the
k-nearest labeled nodes. One may argue that these
unlabeled nodes do not play a major role in the
graph and the labels will eventually get to the oov
nodes from the labeled nodes by directly connect-
ing them. However based on the definition of the
similarity measures using context, it is quite possi-
ble that an oov node and a labeled node which are
connected to the same unlabeled node do not share
any context words and hence are not directly con-
nected. For instance, consider three nodes, u (un-
labeled), o (oov) and l (labeled) where u has the
same left context words with o but share the right
context with l. o and l are not connected since they
do not share any context word.
Once a graph is constructed based on simi-
larities of phrases, graph propagation is used to
propagate the labels from labeled nodes to unla-
beled and oov nodes. The approach is based on
the smoothness assumption (Chapelle et al, 2006)
which states if two nodes are similar according to
the graph, then their output labels should also be
similar.
The baseline approach (Marton et al, 2009) can
be formulated as a bipartite graph with two types
of nodes: labeled nodes (L) and oov nodes (O).
Each oov node is connected to a number of labeled
nodes, and vice versa and there is no edge between
nodes of the same type. In such a graph, the sim-
ilarity of each pair of nodes is computed using
one of the similarity measures discussed above.
The labels are translations and their probabilities
(more specifically p(e|f)) from the phrase-table
extracted from the parallel corpus. Translations
get propagated to oov nodes using a label prop-
agation technique. However beside the difference
in the oov label assignment, there is a major differ-
ence between our bipartite graph and the baseline
(Marton et al, 2009): we do not use a heuristic to
5It is possible that a phrase appears in the parallel corpus,
but not in the phrase-table. This happens when the word-
alignment module is not able to align the phrase to a target
side word or words.
reduce the number of neighbor candidates and we
consider all possible candidates that share at least
one context word. This makes a significant differ-
ence in practice as shown in Section 4.3.1.
We also take advantage of unlabeled nodes to
help connect oov nodes to labeled ones. The dis-
cussed bipartite graph can easily be expanded to a
tripartite graph by adding unlabeled nodes. Fig-
ure 1 illustrate a tripartite graph in which unla-
beled nodes are connected to both labeled and oov
nodes. Again, there is no edge between nodes
of the same type. We also created the full graph
where all nodes can be freely connected to nodes
of any type including the same type. However,
constructing such graph and doing graph propa-
gation on it is computationally very expensive for
large n-grams.
3.1 Label Propagation
Let G = (V,E,W ) be a graph where V is the set
of vertices,E is the set of edges, andW is the edge
weight matrix. The vertex set V consists of la-
beled VL and unlabeled VU nodes, and the goal of
the labeling propagation algorithm is to compute
soft labels for unlabeled vertices from the labeled
vertices. Intuitively, the edge weight W (u, v) en-
codes the degree of our belief about the similarity
of the soft labeling for nodes u and v. A soft label
Y?v ? ?m+1 is a probability vector in (m + 1)-
dimensional simplex, where m is the number of
possible labels and the additional dimension ac-
counts for the undefined ? label6.
In this paper, we make use of the modified Ad-
sorption (MAD) algorithm (Talukdar and Cram-
mer, 2009) which finds soft label vectors Y?v to
solve the following unconstrained optimization
problem:
min
Y?
?1
?
v?VL
p1,v||Yv ? Y?v||22 + (1)
?2
?
v,u
p2,vWv,u||Y?v ? Y?u||22 + (2)
?3
?
v
p3,v||Y?v ?Rv||22 (3)
where ?i and pi,v are hyper-parameters (?v :?
i pi,v = 1)7, and Rv ? ?m+1 encodes our prior
belief about the labeling of a node v. The first
6Capturing those cases where the given data is not enough
to reliably compute a soft labeling using the initial m real
labels.
7The values of these hyper-parameters are set to their de-
faults in the Junto toolkit (Talukdar and Crammer, 2009).
1108
o1
o2
o3
l1
l2
l3
u1 u2 u3 u4 u5
t11 : p11
t12 : p12
t13 : p13
t21 : p21
t22 : p22
t23 : p23
t31 : p31
t32 : p32
t33 : p33
O : oov nodes L : labeled nodes
U : unlabeled nodes
sim(o1, l1)
Figure 1: A tripartite graph between oov, labeled and unlabeled nodes. Translations propagate either directly from labeled
nodes to oov nodes or indirectly via unlabeled nodes.
term (1) enforces the labeling of the algorithm to
match the seed labeling Yv with different extent
for different labeled nodes. The second term (2)
enforces the smoothness of the labeling according
to the graph structure and edge weights. The last
term (3) regularizes the soft labeling for a vertex
v to match a priori label Rv, e.g. for high-degree
unlabeled nodes (hubs in the graph) we may be-
lieve that the neighbors are not going to produce
reliable label and hence the probability of unde-
fined label ? should be higher. The optimiza-
tion problem can be solved with an efficient iter-
ative algorithm which is parallelized in a MapRe-
duce framework (Talukdar et al, 2008; Rao and
Yarowsky, 2009). We used the Junto label prop-
agation toolkit (Talukdar and Crammer, 2009) for
label propagation.
3.2 Efficient Graph Construction
Graph-based approaches can easily become com-
putationally very expensive as the number of
nodes grow. In our case, we use phrases in the
monolingual text as graph vertices. These phrases
are n-grams up to a certain value, which can re-
sult in millions of nodes. For each node a distribu-
tional profile (DP) needs to be created. The num-
ber of possible edges can easily explode in size
as there can be as many as O(n2) edges where n
is the number of nodes. A common practice to
control the number of edges is to connect each
node to at most k other nodes (k-nearest neigh-
bor). However, finding the top-k nearest nodes to
each node requires considering its similarity to all
the other nodes which requires O(n2) computa-
tions and since n is usually very large, doing such
is practically intractable. Therefore, researchers
usually resort to an approximate k-NN algorithms
such as locality-sensitive hashing (?; Goyal et al,
2012).
Fortunately, since we use context words as cues
for relating their meaning and since the similar-
ity measures are defined based on these cues, the
number of neighbors we need to consider for each
node is reduced by several orders of magnitude.
We incorporate an inverted-index-style data struc-
ture which indicates what nodes are neighbors
based on each context word. Therefore, the set
of neighbors of a node consists of union of all the
neighbors bridged by each context word in the DP
of the node. However, the number of neighbors to
be considered for each node even after this dras-
tic reduction is still large (in order of a few thou-
sands).
In order to deal with the computational chal-
lenges of such a large graph, we take advantage of
the Hadoop?s MapReduce functionality to do both
graph construction and label propagation steps.
4 Experiments & Results
4.1 Experimental Setup
We experimented with two different domains for
the bilingual data: Europarl corpus (v7) (Koehn,
1109
Dataset Domain Sents TokensFr En
Bitext Europarl 10K 298K 268KEMEA 1M 16M 14M
Monotext Europarl 2M 60M ?
Dev-set WMT05 2K 67K 58K
Test-set WMT05 2K 66K 58K
Table 1: Statistics of training sets in different domains.
2005), and European Medicines Agency docu-
ments (EMEA) (Tiedemann, 2009) from French
to English. For the monolingual data, we used
French side of the Europarl corpus and we used
ACL/WMT 20058 data for dev/test sets. Table 1
summarizes statistics of the datasets used.
From the dev and test sets, we extract all source
words that do not appear in the phrase-table con-
structed from the parallel data. From the oovs, we
exclude numbers as well as named entities. We
apply a simple heuristic to detect named entities:
basically words that are capitalized in the original
dev/test set that do not appear at the beginning of
a sentence are named entities. Table 2 shows the
number of oov types and tokens for Europarl and
EMEA systems in both dev and test sets.
Dataset Dev Testtypes tokens types tokens
Europarl 1893 2229 1830 2163
EMEA 2325 4317 2294 4190
Table 2: number of oovs in dev and test sets for Europarl and
EMEA systems.
For the end-to-end MT pipeline, we used
Moses (Koehn et al, 2007) with these stan-
dard features: relative-frequency and lexical trans-
lation model (TM) probabilities in both direc-
tions; distortion model; language model (LM)
and word count. Word alignment is done using
GIZA++ (Och and Ney, 2003). We used distortion
limit of 6 and max-phrase-length of 10 in all the
experiments. For the language model, we used the
KenLM toolkit (Heafield, 2011) to create a 5-gram
language model on the target side of the Europarl
corpus (v7) with approximately 54M tokens with
Kneser-Ney smoothing.
4.1.1 Phrase-table Integration
Once the translations and their probabilities for
each oov are extracted, they are added to the
8http://www.statmt.org/wpt05/mt-shared-task/
phrase-table that is induced from the parallel text.
The probability for new entries are added as a
new feature in the log-linear framework to be
tuned along with other features. The value of
this newly introduced feature for original entries
in the phrase-table is set to 1. Similarly, the value
of original four probability features in the phrase-
table for the new entries are set to 1. The entire
training pipeline is as follows: (i) a phrase table is
constructed using parallel data as usual, (ii) oovs
for dev and test sets are extracted, (iii) oovs are
translated using graph propagation, (iv) oovs and
translations are added to the phrase table, intro-
ducing a new feature type, (v) the new phrase table
is tuned (with a LM) using MERT (Och, 2003) on
the dev set.
4.2 Evaluation
If we have a list of possible translations for oovs
with their probabilities, we become able to eval-
uate different methods we discussed. We word-
aligned the dev/test sets by concatenating them to
a large parallel corpus and running GIZA++ on
the whole set. The resulting word alignments are
used to extract the translations for each oov. The
correctness of this gold standard is limited to the
size of the parallel data used as well as the quality
of the word alignment software toolkit, and is not
100% precise. However, it gives a good estimate
of how each oov should be translated without the
need for human judgments.
For evaluating our baseline as well as graph-
based approaches, we use both intrinsic and
extrinsic evaluations. Two intrinsic evaluation
metrics that we use to evaluate the possible
translations for oovs are Mean Reciprocal Rank
(MRR) (Voorhees, 1999) and Recall. Intrinsic
evaluation metrics are faster to apply and are used
to optimize different hyper-parameters of the ap-
proach (e.g. window size, phrase length, etc.).
Once we come up with the optimized values for
the hyper-parameters, we extrinsically evaluate
different approaches by adding the new transla-
tions to the phrase-table and run it through the MT
pipeline.
4.2.1 MRR
MRR is an Information Retrieval metric used to
evaluate any process that produces a ranked list of
possible candidates. The reciprocal rank of a list
is the inverse of the rank of the correct answer in
the list. Such score is averaged over a set, oov set
1110
in our case, to get the mean-reciprocal-rank score.
MRR = 1|O|
|O|?
i=1
1
ranki
O = {oov}
In a few cases, there are multiple translations for
an oov word (i.e. appearing more than once in the
parallel corpus and being assigned to multiple dif-
ferent phrases), we take the average of reciprocal
ranks for each of them.
4.2.2 Recall
MRR takes the probabilities of oov translations
into account in sorting the list of candidate trans-
lations. However, in an MT pipeline, the language
model is supposed to rerank the hypotheses and
move more appropriate translations (in terms of
fluency) to the top of the list. Hence, we also
evaluate our candidate translation regardless of the
ranks. Since Moses uses a certain number of trans-
lations per source phrase (called the translation ta-
ble limit or ttl which we set to 20 in our experi-
ments) , we use the recall measure to evaluate the
top ttl translations in the list. Recall is another In-
formation Retrieval measure that is the fraction of
correct answers that are retrieved. For example, it
assigns score of 1 if the correct translation of the
oov word is in the top-k list and 0 otherwise. The
scores are averaged over all oovs to compute re-
call.
Recall = |{gold standard} ? {candidate list}||{gold standard}|
4.3 Intrinsic Results
In Section 2.2 and 2.3, different types of associa-
tion measures and similarity measures have been
explained to build and compare distributional pro-
files. Table 3 shows the results on Europarl when
using different similarity combinations. The mea-
sures are evaluated by fixing the window size to
4 and maximum candidate paraphrase length to 2
(e.g. bigram). First column shows the association
measures used to build DPs. As the results show,
the combination of PMI as association measure
and cosine as DP similarity measure outperforms
the other possible combinations. We use these two
measures throughout the rest of the experiments.
Figure 2 illustrates the effects of different win-
dow sizes and paraphrase lengths on MRR. As the
figure shows, the best MRR is reached when using
window size of 4 and trigram nodes. Going from
trigram to 4-gram results in a drop in MRR. One
Assoc cosine(%) L1norm(%) JSD(%)MRR RCL MRR RCL MRR RCL
CP 1.66 4.16 2.18 5.55 2.33 6.32
LLR 1.79 4.26 0.13 0.37 0.5 1.00
PMI 3.91 7.75 0.50 1.17 0.59 1.21
Chi 1.66 4.16 0.26 0.55 0.03 0.05
Table 3: Results of intrinsic evaluations (MRR and Recall)
on Europarl, window size 4 and paraphrase length 2
3.5	 ?
3.7	 ?
3.9	 ?
4.1	 ?
4.3	 ?
2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ?
MR
R	 ?(%
)	 ?
Window	 ?Size	 ?
unigram	 ? bigram	 ? trigram	 ? quadgram	 ?
Figure 2: Effects of different window sizes and paraphrase
length on the MRR of the dev set.
reason would be that distributional profiles for 4-
grams are very sparse and that negatively affects
the stability of similarity measures.
Figure 3 illustrates the effect of increasing the
size of monolingual text on both MRR and recall.
1? refers to the case of using 125k sentences for
the monolingual text and the 16? indicates using
the whole Europarl text on the source side (? 2M
sentences). As shown, there is a linear correla-
tion between the logarithm of the data size and
the MRR and recall ratios. Interestingly, MRR is
growing faster than recall by increasing the mono-
lingual text size, which means that the scoring
function gets better when more data is available.
The figure also indicates that a much bigger mono-
lingual text data can be used to further improve the
quality of the translations, however, at the expense
of more computational resources.
MRR	 ?Ra?o	 ?
Recall	 ?Ra?o	 ?
0 
1 
2 
3 
4 
5 
0 1x 2x 4x 8x 16x 
Mono-text Size Ratio 
Figure 3: Effect of increasing the monolingual text size on
MRR and Recall.
1111
Graph Neighbor MRR % RCL %
Bipartite 20 5.2 12.5
Tripartite 15+5 5.9 12.6
Full 20 5.1 10.9
Baseline 20 3.7 7.2
Table 4: Intrinsic results of different types of graphs when
using unigram nodes on Europarl.
Type Node MRR % RCL %
Bipartite unigram 5.2 12.5bigram 6.8 15.7
Tripartite unigram 5.9 12.6bigram 6.9 15.9
Baseline bigram 3.9 7.7
Table 5: Results on using unigram or bigram nodes.
4.3.1 Graph-based Results
Table 4 shows the intrinsic results on the Eu-
roparl corpus when using unigram nodes in each
of the graphs. The results are evaluated on the
dev-set based on the gold alignment created us-
ing GIZA++. Each node is connected to at most
20 other nodes (same as the max-paraphrase-limit
in the baseline). For the tripartite graph, each
node is connected to 15 labeled nodes and 5 un-
labeled ones. The tripartite graph gets a slight im-
provement over the bipartite one, however, the full
graph failed to have the same increase. One rea-
son is that allowing paths longer than 2 between
oov and labeled nodes causes more noise to prop-
agate into the graph. In other words, a paraphrase
of a paraphrase of a paraphrase is not necessarily
a useful paraphrase for an oov as the translation
may no longer be a valid one.
Table 5 also shows the effect of using bigrams
instead of unigrams as graph nodes. There is an
improvement by going from unigrams to bigrams
in both bipartite and tripartite graphs. We did not
use trigrams or larger n-grams in our experiments.
4.4 Extrinsic Results
The generated candidate translations for the oovs
can be added to the phrase-table created using
the parallel corpus to increase the coverage of the
phrase-table. This aggregated phrase-table is to be
tuned along with the language model on the dev
set, and run on the test set. BLEU (Papineni et
al., 2002) is still the de facto evaluation metric for
machine translation and we use that to measure
the quality of our proposed approaches for MT.
In these experiments, we do not use alignment in-
formation on dev or test sets unlike the previous
section.
Table 6 reports the Bleu scores for different do-
mains when the oov translations from the graph
propagation is added to the phrase-table and com-
pares them with the baseline system (i.e. Moses).
Results for our approach is based on unigram tri-
partite graphs and show that we improve over the
baseline in both the same-domain (Europarl) and
domain adaptation (EMEA) settings.
Table 7 shows some translations found by our
system for oov words.
oov gold standard candiate list
spe?cialement
undone
particularly
especially
special
particular
particularly
specific
only
particular
should
and
especially
assentiment approval
support
agreement
approval
accession
will approve
endorses
Table 7: Two examples of oov translations found by our
method.
5 Related work
There has been a long line of research on learning
translation pairs from non-parallel corpora (Rapp,
1995; Koehn and Knight, 2002; Haghighi et al,
2008; Garera et al, 2009; Marton et al, 2009;
Laws et al, 2010). Most have focused on ex-
tracting a translation lexicon by mining monolin-
gual resources of data to find clues, using prob-
abilistic methods to map words, or by exploit-
ing the cross-language evidence of closely related
languages. Most of them evaluated only high-
frequency words of specific types (nouns or con-
tent words) (Rapp, 1995; Koehn and Knight, 2002;
Haghighi et al, 2008; Garera et al, 2009; Laws et
al., 2010) In contrast, we do not consider any con-
straint on our test data and our data includes many
low frequency words. It has been shown that trans-
lation of high-frequency words is easier than low
frequency words (Tamura et al, 2012).
Some methods have used a third language(s)
as pivot or bridge to find translation pairs (Mann
and Yarowsky, 2001; Schafer and Yarowsky, 2002;
Callison-Burch et al, 2006).
1112
Corpus System MRR Recall Dev Bleu Test Bleu
Europarl Baseline ? ? 28.53 28.97Our approach 5.9 12.6 28.76 29.40*
EMEA Baseline ? ? 20.05 20.34Our approach 3.6 7.4 20.54 20.80*
* Statistically significant with p < 0.02 using the bootstrap resampling significance test (in Moses).
Table 6: Bleu scores for different domains with or without using oov translations.
Context similarity has been used effectively in
bilingual lexicon induction (Rapp, 1995; Koehn
and Knight, 2002; Haghighi et al, 2008; Gar-
era et al, 2009; Marton et al, 2009; Laws et al,
2010). It has been modeled in different ways: in
terms of adjacent words (Rapp, 1999; Fung and
Yee, 1998), or dependency relations (Garera et al,
2009). Laws et al (2010) used linguistic analy-
sis in the form of graph-based models instead of a
vector space. But all of these researches used an
available seed lexicon as the basic source of simi-
larity between source and target languages unlike
our method which just needs a monolingual cor-
pus of source language which is freely available
for many languages and a small bilingual corpora.
Some methods tried to alleviate the lack of seed
lexicon by using orthographic similarity to extract
a seed lexicon (Koehn and Knight, 2002; Fiser and
Ljubesic, 2011). But it is not a practical solution
in case of unrelated languages.
Haghighi et al (2008) and Daume? and Jagarla-
mudi (2011) proposed generative models based on
canonical correlation analysis to extract transla-
tion lexicons for non-parallel corpora by learning a
matching between source and target lexicons. Us-
ing monolingual features to represent words, fea-
ture vectors are projected from source and target
words into a canonical space to find the appropri-
ate matching between them. Their method relies
on context features which need a seed lexicon and
orthographic features which only works for phylo-
genetically related languages.
Graph-based semi-supervised methods have
been shown to be useful for domain adaptation in
MT as well. Alexandrescu and Kirchhoff (2009)
applied a graph-based method to determine simi-
larities between sentences and use these similari-
ties to promote similar translations for similar sen-
tences. They used a graph-based semi-supervised
model to re-rank the n-best translation hypothe-
sis. Liu et al (2012) extended Alexandrescu?s
model to use translation consensus among simi-
lar sentences in bilingual training data by devel-
oping a new structured label propagation method.
They derived some features to use during decoding
process that has been shown useful in improving
translation quality. Our graph propagation method
connects monolingual source phrases with oovs to
obtain translation and so is a very different use of
graph propagation from these previous works.
Recently label propagation has been used for
lexicon induction (Tamura et al, 2012). They used
a graph based on context similarity as well as co-
occurrence graph in propagation process. Similar
to our approach they used unlabeled nodes in la-
bel propagation process. However, they use a seed
lexicon to define labels and comparable corpora to
construct graphs unlike our approach.
6 Conclusion
We presented a novel approach for inducing oov
translations from a monolingual corpus on the
source side and a parallel data using graph prop-
agation. Our results showed improvement over
the baselines both in intrinsic evaluations and on
BLEU. Future work includes studying the effect
of size of parallel corpus on the induced oov trans-
lations. Increasing the size of parallel corpus on
one hand reduces the number of oovs. But, on
the other hand, there will be more labeled para-
phrases that increases the chance of finding the
correct translation for oovs in the test set.
Currently, we find paraphrases for oov words.
However, oovs can be considered as n-grams
(phrases) instead of unigrams. In this scenario,
we also can look for paraphrases and translations
for phrases containing oovs and add them to the
phrase-table as new translations along with the
translations for unigram oovs.
We also plan to explore different graph propa-
gation objective functions. Regularizing these ob-
jective functions appropriately might let us scale
to much larger data sets with an order of magni-
tude more nodes in the graph.
1113
References
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL ?09, pages 119?127,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 17?24. Association for
Computational Linguistics.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Mach. Learn., 34(1-3):43?69,
February.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 407?412, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
Darja Fiser and Nikola Ljubesic. 2011. Bilingual lexi-
con extraction from comparable corpora for closely
related languages. In RANLP, pages 125?131.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1, ACL ?98, pages 414?
420. Association for Computational Linguistics.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?09,
pages 129?137, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Amit Goyal, Hal Daume III, and Raul Guerra. 2012.
Fast Large-Scale Approximate Graph Construction
for NLP. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?12.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57?60. Association for
Computational Linguistics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197.
Chung-Chi Huang, Ho-Ching Yen, Ping-Che Yang,
Shih-Ting Huang, and Jason S Chang. 2011. Us-
ing sublexical translations to handle the oov prob-
lem in machine translation. ACM Transactions on
Asian Language Information Processing (TALIP),
10(3):16.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 workshop on Unsuper-
vised lexical acquisition - Volume 9, ULA ?02, pages
9?16, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Florian Laws, Lukas Michelbacher, Beate Dorow,
Christian Scheible, Ulrich Heid, and Hinrich
Schu?tze. 2010. A linguistically grounded graph
model for bilingual lexicon extraction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Posters, COLING ?10, pages
614?622, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
768?774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
1114
Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou. 2012.
Learning translation consensus with structured la-
bel propagation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ?12, pages
302?310, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In Proceedings of the second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language technolo-
gies, NAACL ?01, pages 1?8, Stroudsburg, PA,
USA.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, pages 381?390, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
the 41th Annual Meeting of the ACL, Sapporo, July.
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Delip Rao and David Yarowsky. 2009. Ranking
and semi-supervised classification on large scale
graphs using map-reduce. In Proceedings of the
2009 Workshop on Graph-based Methods for Nat-
ural Language Processing, TextGraphs-4. Associa-
tion for Computational Linguistics.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?95, pages 320?322. Association for
Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526. Association for Computational Linguistics.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity mea-
sures and bridge languages. In proceedings of the
6th conference on Natural language learning - Vol-
ume 20, COLING-02, pages 1?7, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hinrich Schu?tze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications
to information retrieval. Inf. Process. Manage.,
33(3):307?318, May.
Partha Pratim Talukdar and Koby Crammer. 2009.
New Regularized Algorithms for Transductive
Learning. In European Conference on Machine
Learning (ECML-PKDD).
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas?ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In EMNLP-
CoNLL, pages 24?36.
Egidio L. Terra and Charles L. A. Clarke. 2003. Fre-
quency estimates for statistical word similarity mea-
sures. In HLT-NAACL.
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natu-
ral Language Processing, volume V, pages 237?248.
John Benjamins, Amsterdam/Philadelphia.
Ellen M. Voorhees. 1999. TREC-8 Question Answer-
ing Track Report. In Proceedings of the 8th Text
Retrieval Conference, pages 77?82.
Jiajun Zhang, Feifei Zhai, and Chengqing Zong. 2012.
Handling unknown words in statistical machine
translation from a new perspective. In Natural Lan-
guage Processing and Chinese Computing, pages
176?187. Springer.
1115
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 334?339,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Stacking for Statistical Machine Translation?
Majid Razmara and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
{razmara,anoop}@sfu.ca
Abstract
We propose the use of stacking, an ensem-
ble learning technique, to the statistical machine
translation (SMT) models. A diverse ensem-
ble of weak learners is created using the same
SMT engine (a hierarchical phrase-based sys-
tem) by manipulating the training data and a
strong model is created by combining the weak
models on-the-fly. Experimental results on two
language pairs and three different sizes of train-
ing data show significant improvements of up
to 4 BLEU points over a conventionally trained
SMT model.
1 Introduction
Ensemble-based methods have been widely used
in machine learning with the aim of reduc-
ing the instability of classifiers and regressors
and/or increase their bias. The idea behind
ensemble learning is to combine multiple mod-
els, weak learners, in an attempt to produce a
strong model with less error. It has also been
successfully applied to a wide variety of tasks in
NLP (Tomeh et al, 2010; Surdeanu and Man-
ning, 2010; F. T. Martins et al, 2008; Sang, 2002)
and recently has attracted attention in the statis-
tical machine translation community in various
work (Xiao et al, 2013; Song et al, 2011; Xiao
et al, 2010; Lagarda and Casacuberta, 2008).
In this paper, we propose a method to adopt
stacking (Wolpert, 1992), an ensemble learning
technique, to SMT. We manipulate the full set of
training data, creating k disjoint sets of held-out
and held-in data sets as in k-fold cross-validation
and build a model on each partition. This creates
a diverse ensemble of statistical machine transla-
tion models where each member of the ensemble
has different feature function values for the SMT
log-linear model (Koehn, 2010). The weights of
model are then tuned using minimum error rate
training (Och, 2003) on the held-out fold to pro-
vide k weak models. We then create a strong
?This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Faculty Award
to the second author.
model by stacking another meta-learner on top of
weak models to combine them into a single model.
The particular second-tier model we use is a model
combination approach called ensemble decoding
which combines hypotheses from the weak mod-
els on-the-fly in the decoder.
Using this approach, we take advantage of the
diversity created by manipulating the training data
and obtain a significant and consistent improve-
ment over a conventionally trained SMT model
with a fixed training and tuning set.
2 Ensemble Learning Methods
Two well-known instances of general framework
of ensemble learning are bagging and boosting.
Bagging (Breiman, 1996a) (bootstrap aggregat-
ing) takes a number of samples with replacement
from a training set. The generated sample set
may have 0, 1 or more instances of each origi-
nal training instance. This procedure is repeated
a number of times and the base learner is ap-
plied to each sample to produce a weak learner.
These models are aggregated by doing a uniform
voting for classification or averaging the predic-
tions for regression. Bagging reduces the vari-
ance of the base model while leaving the bias rela-
tively unchanged and is most useful when a small
change in the training data affects the prediction
of the model (i.e. the model is unstable) (Breiman,
1996a). Bagging has been recently applied to
SMT (Xiao et al, 2013; Song et al, 2011)
Boosting (Schapire, 1990) constructs a strong
learner by repeatedly choosing a weak learner
and applying it on a re-weighted training set. In
each iteration, a weak model is learned on the
training data, whose instance weights are modi-
fied from the previous iteration to concentrate on
examples on which the model predictions were
poor. By putting more weight on the wrongly
predicted examples, a diverse ensemble of weak
learners is created. Boosting has also been used in
SMT (Xiao et al, 2013; Xiao et al, 2010; Lagarda
334
Algorithm 1: Stacking for SMT
Input: D = {?fj , ej?}Nj=1 . A parallel corpusInput: k . # of folds (i.e. weak learners)
Output: STRONGMODEL s
1: D1, . . . ,Dk ? SPLIT(D, k)
2: for i = 1? k do
3: T i ? D ?Di . Use all but current partition as
training set.
4: ?i? TRAIN(T i) . Train feature functions.
5: Mi? TUNE(?i, Di) . Tune the model on thecurrent partition.
6: end for
7: s? COMBINEMODELS(M1 , . . .,Mk) . Combine all
the base models to produce a strong stacked model.
and Casacuberta, 2008).
Stacking (or stacked generalization) (Wolpert,
1992) is another ensemble learning algorithm that
uses a second-level learning algorithm on top of
the base learners to reduce the bias. The first
level consists of predictors g1, . . . , gk where gi :
Rd ? R, receiving input x ? Rd and produc-
ing a prediction gi(x). The next level consists
of a single function h : Rd+k ? R that takes
?x, g1(x), . . . , gk(x)? as input and produces an en-
semble prediction y? = h(x, g1(x), . . . , gk(x)).
Two categories of ensemble learning are ho-
mogeneous learning and heterogeneous learning.
In homogeneous learning, a single base learner
is used, and diversity is generated by data sam-
pling, feature sampling, randomization and pa-
rameter settings, among other strategies. In het-
erogeneous learning different learning algorithms
are applied to the same training data to create a
pool of diverse models. In this paper, we focus on
homogeneous ensemble learning by manipulating
the training data.
In the primary form of stacking (Wolpert,
1992), the training data is split into multiple dis-
joint sets of held-out and held-in data sets using
k-fold cross-validation and k models are trained
on the held-in partitions and run on held-out par-
titions. Then a meta-learner uses the predictions
of all models on their held-out sets and the actual
labels to learn a final model. The details of the
first-layer and second-layer predictors are consid-
ered to be a ?black art? (Wolpert, 1992).
Breiman (1996b) linearly combines the weak
learners in the stacking framework. The weights
of the base learners are learned using ridge regres-
sion: s(x) = ?k ?kmk(x), where mk is a base
model trained on the k-th partition of the data and
s is the resulting strong model created by linearly
interpolating the weak learners.
Stacking (aka blending) has been used in the
system that won the Netflix Prize1, which used a
multi-level stacking algorithm.
Stacking has been actively used in statistical
parsing: Nivre and McDonald (2008) integrated
two models for dependency parsing by letting one
model learn from features generated by the other;
F. T. Martins et al (2008) further formalized the
stacking algorithm and improved on Nivre and
McDonald (2008); Surdeanu and Manning (2010)
includes a detailed analysis of ensemble models
for statistical parsing: i) the diversity of base
parsers is more important than the complexity of
the models; ii) unweighted voting performs as well
as weighted voting; and iii) ensemble models that
combine at decoding time significantly outperform
models that combine multiple models at training
time.
3 Our Approach
In this paper, we propose a method to apply stack-
ing to statistical machine translation (SMT) and
our method is the first to successfully exploit
stacking for statistical machine translation. We
use a standard statistical machine translation en-
gine and produce multiple diverse models by par-
titioning the training set using the k-fold cross-
validation technique. A diverse ensemble of weak
systems is created by learning a model on each
k?1 fold and tuning the statistical machine trans-
lation log-linear weights on the remaining fold.
However, instead of learning a model on the output
of base models as in (Wolpert, 1992), we combine
hypotheses from the base models in the decoder
with uniform weights. For the base learner, we
use Kriya (Sankaran et al, 2012), an in-house hier-
archical phrase-based machine translation system,
to produce multiple weak models. These mod-
els are combined together using Ensemble Decod-
ing (Razmara et al, 2012) to produce a strong
model in the decoder. This method is briefly ex-
plained in next section.
3.1 Ensemble Decoding
SMT Log-linear models (Koehn, 2010) find the
most likely target language output e given the
source language input f using a vector of feature
functions ?:
p(e|f) ? exp
(
w ? ?
)
1http://www.netflixprize.com/
335
Ensemble decoding combines several models
dynamically at decoding time. The scores are
combined for each partial hypothesis using a
user-defined mixture operation ? over component
models.
p(e|f) ? exp
(
w1 ? ?1 ?w2 ? ?2 ? . . .
)
We previously successfully applied ensemble
decoding to domain adaptation in SMT and
showed that it performed better than approaches
that pre-compute linear mixtures of different mod-
els (Razmara et al, 2012). Several mixture oper-
ations were proposed, allowing the user to encode
belief about the relative strengths of the compo-
nent models. These mixture operations receive
two or more probabilities and return the mixture
probability p(e? | f?) for each rule e?, f? used in the
decoder. Different options for these operations
are:
? Weighted Sum (wsum) is defined as:
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component
models, M is the total number of them and
?m is the weight for component m.
? Weighted Max (wmax) is defined as:
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
? Prod or log-wsum is defined as:
p(e? | f?) ? exp
( M?
m
?m (wm ? ?m)
)
? Model Switching (Switch): Each cell in the
CKY chart is populated only by rules from
one of the models and the other models? rules
are discarded. Each component model is con-
sidered as an expert on different spans of the
source. A binary indicator function ?(f? ,m)
picks a component model for each span:
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each
cell, ?(f? , n), could be based on max
Train size Src tokens Tgt tokens
Fr - En
0+dev 67K 58K
10k+dev 365K 327K
100k+dev 3M 2.8M
Es - En
0+dev 60K 58K
10k+dev 341K 326K
100k+dev 2.9M 2.8M
Table 1: Statistics of the training set for different systems and
different language pairs.
(SW:MAX), i.e. for each cell, the model that
has the highest weighted score wins:
?(f? , n) = ?n maxe (wn ? ?n(e?, f?))
Alternatively, we can pick the model with
highest weighted sum of the probabilities of
the rules (SW:SUM). This sum has to take into
account the translation table limit (ttl), on the
number of rules suggested by each model for
each cell:
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
then:
p(e? | f?) =
M?
m
?(f? ,m) pm(e? | f?)
4 Experiments & Results
We experimented with two language pairs: French
to English and Spanish to English on the Europarl
corpus (v7) (Koehn, 2005) and used ACL/WMT
2005 2 data for dev and test sets.
For the base models, we used an in-house
implementation of hierarchical phrase-based sys-
tems, Kriya (Sankaran et al, 2012), which uses
the same features mentioned in (Chiang, 2005):
forward and backward relative-frequency and lex-
ical TM probabilities; LM; word, phrase and glue-
rules penalty. GIZA++ (Och and Ney, 2003) has
been used for word alignment with phrase length
limit of 10. Feature weights were optimized using
MERT (Och, 2003). We built a 5-gram language
model on the English side of Europarl and used the
Kneser-Ney smoothing method and SRILM (Stol-
cke, 2002) as the language model toolkit.
2http://www.statmt.org/wpt05/mt-shared-task/
336
Direction k-fold Resub Mean WSUM WMAX PROD SW:MAX SW:SUM
Fr - En
2 18.08 19.67 22.32 22.48 22.06 21.70 21.81
4 18.08 21.80 23.14 23.48 23.55 22.83 22.95
8 18.08 22.47 23.76 23.75 23.78 23.02 23.47
Es - En
2 18.61 19.23 21.62 21.33 21.49 21.48 21.51
4 18.61 21.52 23.42 22.81 22.91 22.81 22.92
8 18.61 22.20 23.69 23.89 23.51 22.92 23.26
Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
Direction Corpus k-fold Baseline BMA WSUM WMAX PROD SW:MAX SW:SUM
Fr - En 10k+dev 6 28.75 29.49 29.87 29.78 29.21 29.69 29.59100k+dev 11 / 51 29.53 29.75 34.00 34.07 33.11 34.05 33.96
Es - En 10k+dev 6 28.21 28.76 29.59 29.51 29.15 29.10 29.21100k+dev 11 / 51 33.25 33.44 34.21 34.00 33.17 34.19 34.22
Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
4.1 Training on devset
We first consider the scenario in which there is
no parallel data between a language pair except
a small bi-text used as a devset. We use no spe-
cific training data and construct a SMT system
completely on the devset by using our approach
and compare to two different baselines. A natu-
ral baseline when having a limited parallel text is
to do re-substitution validation where the model
is trained on the whole devset and is tuned on the
same set. This validation process suffers seriously
from over-fitting. The second baseline is the mean
of BLEU scores of all base models.
Table 2 summarizes the BLEU scores on the
testset when using stacking only on the devset on
two different language pairs. As the table shows,
increasing the number of folds results in higher
BLEU scores. However, doing such will generally
lead to higher variance among base learners.
Figure 1 shows the BLEU score of each of the
base models resulted from a 20-fold partitioning
of the devset alng with the strong models? BLEU
scores. As the figure shows, the strong models are
generally superior to the base models whose mean
is represented as a horizontal line.
4.2 Training on train+dev
When we have some training data, we can use
the cross-validation-style partitioning to create k
splits. We then train a system on k ? 1 folds and
tune on the devset. However, each system eventu-
ally wastes a fold of the training data. In order to
take advantage of that remaining fold, we concate-
nate the devset to the training set and partition the
whole union. In this way, we use all data available
to us. We experimented with two sizes of train-
ing data: 10k sentence pairs and 100k, that with
the addition of the devset, we have 12k and 102k
sentence-pair corpora.
Table 1 summarizes statistics of the data sets
used in this scenario. Table 3 reports the BLEU
scores when using stacking on these two corpus
sizes. The baselines are the conventional systems
which are built on the training-set only and tuned
on the devset as well as Bayesian Model Averaging
(BMA, see ?5). For the 100k+dev corpus, we sam-
pled 11 partitions from all 51 possible partitions
by taking every fifth partition as training data. The
results in Table 3 show that stacking can improve
over the baseline BLEU scores by up to 4 points.
Examining the performance of the different
mixture operations, we can see that WSUM and
WMAX typically outperform other mixture oper-
ations. Different mixture operations can be domi-
nant in different language pairs and different sizes
of training sets.
5 Related Work
Xiao et al (2013) have applied both boosting
and bagging on three different statistical machine
translation engines: phrase-based (Koehn et al,
2003), hierarchical phrase-based (Chiang, 2005)
and syntax-based (Galley et al, 2006) and showed
SMT can benefit from these methods as well.
Duan et al (2009) creates an ensemble of mod-
els by using feature subspace method in the ma-
chine learning literature (Ho, 1998). Each mem-
ber of the ensemble is built by removing one non-
LM feature in the log-linear framework or varying
the order of language model. Finally they use a
sentence-level system combination on the outputs
of the base models to pick the best system for each
337
ws
um
wm
ax
sw
:m
ax
sw
:su
m
prod

testset BLEU
Mo
dels
 
Mea
n
Bas
e M
ode
ls
Stro
ng M
ode
ls
Figure 1: BLEU scores for all the base models and stacked models on the Fr-En devset with 20-fold cross validation. The
horizontal line shows the mean of base models? scores.
sentence. Though, they do not combine the hy-
potheses search spaces of individual base models.
Our work is most similar to that of Duan et
al. (2010) which uses Bayesian model averaging
(BMA) (Hoeting et al, 1999) for SMT. They used
sampling without replacement to create a num-
ber of base models whose phrase-tables are com-
bined with that of the baseline (trained on the full
training-set) using linear mixture models (Foster
and Kuhn, 2007).
Our approach differs from this approach in a
number of ways: i) we use cross-validation-style
partitioning for creating training subsets while
they do sampling without replacement (80% of the
training set); ii) in our approach a number of base
models are trained and tuned and they are com-
bined on-the-fly in the decoder using ensemble de-
coding which has been shown to be more effective
than offline combination of phrase-table-only fea-
tures; iii) in Duan et al (2010)?s method, each sys-
tem gives up 20% of the training data in exchange
for more diversity, but in contrast, our method not
only uses all available data for training, but pro-
motes diversity through allowing each model to
tune on a different data set; iv) our approach takes
advantage of held out data (the tuning set) in the
training of base models which is beneficial espe-
cially when little parallel data is available or tun-
ing/test sets and training sets are from different do-
mains.
Empirical results (Table 3) also show that our
approach outperforms the Bayesian model averag-
ing approach (BMA).
6 Conclusion & Future Work
In this paper, we proposed a novel method on ap-
plying stacking to the statistical machine transla-
tion task. The results when using no, 10k and 100k
sentence-pair training sets (along with a develop-
ment set for tuning) show that stacking can yield
an improvement of up to 4 BLEU points over con-
ventionally trained SMT models which use a fixed
training and tuning set.
Future work includes experimenting with larger
training sets to investigate how useful this ap-
proach can be when having different sizes of train-
ing data.
References
Leo Breiman. 1996a. Bagging predictors. Machine
Learning, 24(2):123?140, August.
Leo Breiman. 1996b. Stacked regressions. Machine
Learning, 24(1):49?64, July.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263?
270, Morristown, NJ, USA. ACL.
Nan Duan, Mu Li, Tong Xiao, and Ming Zhou. 2009.
The feature subspace method for smt system combi-
nation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1096?
1104, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Nan Duan, Hong Sun, and Ming Zhou. 2010. Transla-
tion model generalization using probability averag-
ing for machine translation. In Proceedings of the
338
23rd International Conference on Computational
Linguistics, COLING ?10, pages 304?312, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
157?166, Honolulu, Hawaii, October. Association
for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ?07, pages 128?135, Stroudsburg, PA, USA.
ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 961?968, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tin Kam Ho. 1998. The random subspace method for
constructing decision forests. IEEE Trans. Pattern
Anal. Mach. Intell., 20(8):832?844, August.
Jennifer A. Hoeting, David Madigan, Adrian E.
Raftery, and Chris T. Volinsky. 1999. Bayesian
Model Averaging: A Tutorial. Statistical Science,
14(4):382?401.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, pages 127?133, Edmonton,
May. NAACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Antonio Lagarda and Francisco Casacuberta. 2008.
Applying boosting to statistical machine translation.
In Annual Meeting of European Association for Ma-
chine Translation (EAMT), pages 88?96.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
the 41th Annual Meeting of the ACL, Sapporo, July.
ACL.
Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple transla-
tion models in statistical machine translation. In The
50th Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Conference,
July 8-14, 2012, Jeju Island, Korea - Volume 1: Long
Papers, pages 940?949. The Association for Com-
puter Linguistics.
Erik F. Tjong Kim Sang. 2002. Memory-based shal-
low parsing. J. Mach. Learn. Res., 2:559?594,
March.
Baskaran Sankaran, Majid Razmara, and Anoop
Sarkar. 2012. Kriya an end-to-end hierarchical
phrase-based mt system. The Prague Bulletin of
Mathematical Linguistics, 97(97), April.
Robert E. Schapire. 1990. The strength of weak learn-
ability. Mach. Learn., 5(2):197?227, July.
Linfeng Song, Haitao Mi, Yajuan Lu?, and Qun Liu.
2011. Bagging-based system combination for do-
main adaption. In Proceedings of the 13th Machine
Translation Summit (MT Summit XIII), pages 293?
299. International Association for Machine Transla-
tion, September.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing,
pages 257?286.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 649?652, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nadi Tomeh, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc?ois Yvon. 2010. Refining word
alignment with discriminative training. In Proceed-
ings of The Ninth Conference of the Association for
Machine Translation in the Americas (AMTA 2010).
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5:241?259.
Tong Xiao, Jingbo Zhu, Muhua Zhu, and Huizhen
Wang. 2010. Boosting-based system combina-
tion for machine translation. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 739?748,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Tong Xiao, Jingbo Zhu, and Tongran Liu. 2013. Bag-
ging and boosting statistical machine translation sys-
tems. Artificial Intelligence, 195:496?527, Febru-
ary.
339
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 356?361,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Kriya - The SFU System for Translation Task at WMT-12
Majid Razmara and Baskaran Sankaran and Ann Clifton and Anoop Sarkar
School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby BC. V5A 1S6. Canada
{razmara, baskaran, aca69, anoop}@cs.sfu.ca
Abstract
This paper describes our submissions for the
WMT-12 translation task using Kriya - our hi-
erarchical phrase-based system. We submitted
systems in French-English and English-Czech
language pairs. In addition to the baseline sys-
tem following the standard MT pipeline, we
tried ensemble decoding for French-English.
The ensemble decoding method improved the
BLEU score by 0.4 points over the baseline
in newstest-2011. For English-Czech, we seg-
mented the Czech side of the corpora and
trained two different segmented models in ad-
dition to our baseline system.
1 Baseline Systems
Our shared task submissions are trained in the hier-
archical phrase-based model (Chiang, 2007) frame-
work. Specifically, we use Kriya (Sankaran et al,
2012) - our in-house Hiero-style system for training
and decoding. We now briefly explain the baseline
systems in French-English and English-Czech lan-
guage pairs.
We use GIZA++ for word alignments and the
Moses (Koehn et al, 2007) phrase-extractor for ex-
tracting the initial phrases. The translation models
are trained using the rule extraction module in Kriya.
In both cases, we pre-processed the training data by
running it through the usual pre-processing pipeline
of tokenization and lowercasing.
For French-English baseline system, we trained
a simplified hierarchical phrase-based model where
the right-hand side can have at most one non-
terminal (denoted as 1NT) instead of the usual two
non-terminal (2NT) model. In our earlier experi-
ments we found the 1NT model to perform com-
parably to the 2NT model for close language pairs
such as French-English (Sankaran et al, 2012) at the
same time resulting in a smaller model. We used the
shared-task training data consisting of Europarl (v7),
News commentary and UN documents for training
the translation models having a total of 15 M sen-
tence pairs (we did not use the Fr-En Giga paral-
lel corpus for the training). We trained a 5-gram
language model for English using the English Gi-
gaword (v4).
For English-Czech, we trained a standard Hiero
model that has up to two non-terminals on the right-
hand side. We used the Europarl (v7), news com-
mentary and CzEng (v0.9) corpora having 7.95M
sentence pairs for training translation models. We
trained a 5-gram language model using the Czech
side of the parallel corpora and did not use the Czech
monolingual corpus.
The baseline systems use the following 8 stan-
dard Hiero features: rule probabilities p(e|f) and
p(f |e); lexical weights pl(e|f) and pl(f |e); word
penalty, phrase penalty, language model and glue
rule penalty.
1.1 LM Integration in Kriya
The kriya decoder is based on a modified CYK al-
gorithm similar to that of Chiang (2007). We use
a novel approach in computing the language model
(LM) scores in Kriya, which deserves a mention
here.
The CKY decoder in Hiero-style systems can
freely combine target hypotheses generated in inter-
356
mediate cells with hierarchical rules in the higher
cells. Thus the generation of the target hypotheses
are fragmented and out of order in Hiero, compared
to the left to right order preferred by n-gram lan-
guage models.
This leads to challenges in estimating LM scores
for partial target hypotheses and this is typically ad-
dressed by adding a sentence initial marker (<s>)
to the beginning of each derivation path.1 Thus the
language model scores for the hypothesis in the in-
termediate cell are approximated, with the true lan-
guage model score (taking into account sentence
boundaries) being computed in the last cell that
spans the entire source sentence.
Kriya uses a novel idea for computing LM scores:
for each of the target hypothesis fragment, it finds
the best position for the fragment in the final sen-
tence and uses the corresponding score. Specifi-
cally, we compute three different scores correspond-
ing to the three states where the fragment can end
up in the final sentence, viz. sentence initial, middle
and final and choose the best score. Thus given a
fragment tf consisting of a sequence of target to-
kens, we compute LM scores for (i) <s> tf , (ii)
tf and (iii) tf </s> and use the best score (only)
for pruning.2 While this increases the number of
LM queries, we exploit the language model state in-
formation in KenLM (Heafield, 2011) to optimize
the queries by saving the scores for the unchanged
states. Our earlier experiments showed significant
reduction in search errors due to this approach, in
addition to a small but consistent increase in BLEU
score (Sankaran et al, 2012).
2 French-English System
In addition to the baseline system, we also trained
separate systems for News and Non-News genres
for applying ensemble decoding (Razmara et al,
2012). The news genre system was trained only us-
ing the news-commentary corpus (about 137K sen-
1Alternately systems add sentence boundary markers (<s>
and </s>) to the training data so that they are explicitly present
in the translation and language models. While this can speed
up the decoding as the cube pruning is more aggressive, it also
limits the applicability of rules having the boundary contexts.
2This ensures the the LM score estimates are never underes-
timated for pruning. We retain the LM score for fragment (case
ii) for estimating the score for the full candidate sentence later.
tence pairs) and the non-news genre system was
trained on the Europarl and UN documents data
(14.8M sentence pairs). The ensemble decoding
framework combines the models of these two sys-
tems dynamically when decoding the testset. The
idea is to effectively use the small amount of news
genre data in order to maximize the performance on
the news-based testsets. In the following sections,
we explain in broader detail how this system combi-
nation technique works as well as the details of this
experiment and the evaluation results.
2.1 Ensemble Decoding
In the ensemble decoding framework we view trans-
lation task as a domain mixing problem involving
news and non-news genres. The official training
data is from two major sources: news-commentary
data and Europarl/UN data and we hope to exploit
the distinctive nature of the two genres. Given that
the news data is smaller comparing to parliamen-
tary proceedings data, we could tune the ensemble
decoding to appropriately boost the weight for the
news genre mode during decoding. The ensemble
decoding approach (Razmara et al, 2012) takes ad-
vantage of multiple translation models with the goal
of constructing a system that outperforms all the
component models. The key strength of this system
combination method is that the systems are com-
bined dynamically at decode time. This enables the
decoder to pick the best hypotheses for each span of
the input.
In ensemble decoding, given a number of transla-
tion systems which are already trained and tuned, all
of the hypotheses from component models are used
in order to translate a sentence. The scores of such
rules are combined in the decoder (i.e. CKY) using
various mixture operations to assign a single score to
them. Depending on the mixture operation used for
combining the scores, we would get different mix-
ture scores.
Ensemble decoding extends the log-linear frame-
work which is found in state-of-the-art machine
translation systems. Specifically, the probability of
a phrase-pair (e?, f?) in the ensemble model is:
p(e? | f?) ? exp
(
w1 ? ?1? ?? ?
1st model
? w2 ? ?2? ?? ?
2nd model
? ? ? ?
)
357
where? denotes the mixture operation between two
or more model scores.
Mixture operations receive two or more scores
(probabilities) and return the mixture score (prob-
ability). In this section, we explore different options
for this mixture operation.
Weighted Sum (wsum): in wsum the ensemble
probability is proportional to the weighted sum
of all individual model probabilities.
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component mod-
els, M is the total number of them and ?i is the
weight for component i.
Weighted Max (wmax): where the ensemble score
is the weighted max of all model scores.
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
Product (prod): in prod, the probability of the en-
semble model or a rule is computed as the prod-
uct of the probabilities of all components (or
equally the sum of log-probabilities). When
using this mixture operation, ensemble de-
coding would be a generalization of the log-
linear framework over multiple models. Prod-
uct models can also make use of weights to
control the contribution of each component.
These models are generally known as Logarith-
mic Opinion Pools (LOPs) where:
p(e? | f?) ? exp
(
M?
m
?m wm ? ?m
)
Model Switching: in model switching, each cell in
the CKY chart gets populated only by rules
from one of the models and the other mod-
els? rules are discarded. This is based on the
hypothesis that each component model is an
expert on different parts of sentence. In this
method, we need to define a binary indicator
function ?(f? ,m) for each span and component
model.
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each cell,
?(f? , n), could be based on:
Max: for each cell, the model that has the
highest weighted top-rule score wins:
?(f? , n) = ?n max
e
(wn ? ?n(e?, f?))
Sum: Instead of comparing only the score of
the top rules, the model with the high-
est weighted sum of the probability of
the rules wins (taking into account the
ttl(translation table limit) limit on the
number of rules suggested by each model
for each cell):
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
computed as:
p(e? | f?) =
?
m
?(f? ,m) pm(e? | f?)
Since log-linear models usually look for the best
derivation, they do not need to normalize the scores
to form probabilities. Therefore, the scores that dif-
ferent models assign to each phrase-pair may not be
in the same scale. Therefore, mixing their scores
might wash out the information in one (or some)
of the models. We applied a heuristic to deal with
this problem where the scores are normalized over
a shorter list. So the list of rules coming from each
model for a certain cell in the CKY chart is normal-
ized before getting mixed with other phrase-table
rules. However, experiments showed using normal-
ized scores hurts the BLEU score radically. So we
use the normalized scores only for pruning and for
mixing the actual scores are used.
As a more principled way, we used a toolkit,
CONDOR (Vanden Berghen and Bersini, 2005), to
optimize the weights of our component models on
a dev-set. CONDOR, which is publicly available, is
a direct optimizer based on Powell?s algorithm that
does not require explicit gradient information for the
objective function.
2.2 Experiments and Results
As mentioned earlier all the experiments reported
for French-English use a simpler Hiero translation
358
Method Devset Test-11 Test-12
Baseline Hiero 26.03 27.63 28.15
News data 24.02 26.47 26.27
Non-news data 26.09 27.87 28.15
Ensemble PROD 25.66 28.25 28.09
Table 1: French-English BLEU scores. Best performing
setting is shown in Boldface.
model having at most one non-terminal (1NT) on the
right-hand side. We use 7567 sentence pairs from
news-tests 2008 through 2010 for tuning and use
news-test 2011 for testing in addition to the 2012
test data. The feature weights were tuned using
MERT (Och, 2003) and we report the devset (IBM)
BLEU scores and the testset BLEU scores computed
using the official evaluation script (mteval-v11b.pl).
The results for the French-English experiments
are reported in Table 1. We note that both baseline
Hiero model and the model trained from the non-
news genre get comparable BLEU scores. The news
genre model however gets a lesser BLEU score and
this is to be expected due to the very small training
data available for this genre.
Table 2 shows the results of applying various mix-
ture operations on the devset and testset, both in nor-
malized (denoted by Norm.) and un-normalized set-
tings (denoted by Base). We present results for these
mixture operations using uniform weights (i.e. un-
tuned weights) and for PROD we also present the
results using the weights optimized by CONDOR.
Most of the mixture operations outperform the Test-
11 BLEU of the baseline models (shown in Table 1)
even with uniform (untuned) weights. We took the
best performing operation (i.e. PROD) and tuned its
component weights using our optimizer which lead
to 0.26 points improvement over its uniform-weight
version.
The last row in Table 1 reports the BLEU score
for this mixture operation with the tuned weights
on the Test-12 dataset and it is marginally less than
the baseline model. While this is disappointing, this
also runs counter to our empirical results from other
datasets. We are currently investigating this aspect
as we hope to improve the robustness and applicabil-
ity of our ensemble approach for different datasets
and language pairs.
Mix. Operation Weights Base Norm.
WMAX uniform 27.67 27.94
WSUM uniform 27.72 27.95
SWITCHMAX uniform 27.96 26.21
SWITCHSUM uniform 27.98 27.98
PROD uniform 27.99 28.09
PROD optimized 28.25 28.11
Table 2: Applying ensemble decoding with different mix-
ture operations on the Test-11 dataset. Best performing
setting is shown in Boldface.
3 English-Czech System
3.1 Morpheme Segmented Model
For English-Czech, we additionally experimented
using morphologically segmented versions of the
Czech side of the parallel data, since previous
work (Clifton and Sarkar, 2011) has shown that seg-
mentation of morphologically rich languages can
aid translation. To derive the segmentation, we
built an unsupervised morphological segmentation
model using the Morfessor toolkit (Creutz and La-
gus, 2007).
Morfessor uses minimum description length cri-
teria to train a HMM-based segmentation model.
Varying the perplexity threshold in Morfessor does
not segment more word types, but rather over-
segments the same word types. We hand tuned the
model parameters over training data size and per-
plexity; these control the granularity and coverage of
the segmentations. Specifically, we trained different
segmenter models on varying sets of most frequent
words and different perplexities and identified two
sets that performed best based on a separate held-
out set. These two sets correspond to 500k most fre-
quent words and a perplexity of 50 (denoted SM1)
and 10k most frequent words and a perplexity of 20
(denoted SM2). We then used these two models to
segment the entire data set and generate two differ-
ent segmented training sets. These models had the
best combination of segmentation coverage of the
training data and largest segments, since we found
empirically that smaller segments were less mean-
ingful in the translation model. The SM2 segmenta-
tion segmented more words than SM1, but more fre-
quently segmented words into single-character units.
359
For example, the Czech word ?dlaebn??? is broken
into the useful components ?dlaeb + n??? by SM1, but
is oversegmented into ?dl + a + e + b + n??? by SM2.
However, SM1 fails to find a segmentation at all for
the related word ?dlaebn??mi?, while SM2 breaks it
up similiarly with an additional suffix: ?dl + a + e +
b + n?? + mi?.
With these segmentation models, we segmented
the target side of the training and dev data before
training the translation model. Similarly, we also
train segmented language models corresponding to
the two sets SM1 and SM2. The MERT tuning step
uses the segmented dev-set reference to evaluate the
segmented hypotheses generated by the decoder for
optimizing the weights for the BLEU score. How-
ever for evaluating the test-set, we stitched the seg-
ments in the decoder output back into unsegmented
forms in a post-processing step, before performing
evaluation against the original unsegmented refer-
ences. The hypotheses generated by the decoder
can have incomplete dangling segments where one
or more prefixes and/or suffixes are missing. While
these dangling segments could be handled in a dif-
ferent way, we use a simple heuristic of ignoring the
segment marker ?+? by just removing the segment
marker. In next section, we report the results of us-
ing the unsegmented model as well as its segmented
counterparts.
3.2 Experiments and Results
In the English-Czech experiments, we used the same
datasets for the dev and test sets as in French-
English experiments (dev: news-tests 2008, 2009,
2010 with 7567 sentence pairs and test: news-
test2011 with 3003 sentence pairs). Similarly,
MERT (Och, 2003) has been used to tune the feature
weights and we report the BLEU scores of two test-
sets computed using the official evaluation script
(mteval-v11b.pl).
Table 3.2 shows the results of different segmenta-
tion schemes on the WMT-11 and WMT-12 test-sets.
SM1 slightly outperformed the other two models in
Test-11, however the unsegmented model performed
best in Test-12, though marginally. We are currently
investigating this and are also considering the pos-
sibility employing the idea of morpheme prediction
in the post-decoding step in combination with this
morpheme-based translation as suggested by Clifton
Segmentation Test-11 Test-12
Baseline Hiero 14.65 12.40
SM1 : 500k-ppl50 14.75 12.34
SM2 : 10k-ppl20 14.57 12.34
Table 3: The English-Czech results for different segmen-
tation settings. Best performing setting is shown in Bold-
face.
and Sarkar (2011).
4 Conclusion
We submitted systems in two language pairs French-
English and English-Czech for WMT-12 shared
task. In French-English, we experimented the en-
semble decoding framework that effectively utilizes
the small amount of news genre data to improve the
performance in the testset belonging to the same
genre. We obtained a moderate gain of 0.4 BLEU
points with the ensemble decoding over the baseline
system in newstest-2011. For newstest-2012, it per-
forms comparably to that of the baseline and we are
presently investigating the lack of improvement in
newstest-2012. For Cz-En, We found that the BLEU
scores do not substantially differ from each other
and also the minor differences are not consistent for
Test-11 and Test-12.
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, pages 32?42.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1):3:1?3:34, February.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
360
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of Association of Computational Lin-
guistics, pages 160?167.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics, Jeju, Republic of Korea,
July. Association for Computational Linguistics. To
appear.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, 97(97):83?98, April.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175,
September.
361
