Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Building parallel corpora for eContent professionals 
M. Gavrilidou, P. Labropoulou, E. Desipri, V. Giouli, V. Antonopoulos, S. Piperidis 
Institute for Language and Speech Processing  
Epidavrou & Artemidos 6 
151 25 Maroussi, Greece. 
{maria, penny, elina, voula, vantonop, spip} @ilsp.gr 
 
Abstract 
This paper reports on completed work carried 
out in the framework of the INTERA project, 
and specifically, on the production of 
multilingual resources (LRs) for eContent 
purposes. The paper presents the methodology 
adopted for the development of the corpus 
(acquisition and processing of the textual 
data), discusses the divergence of the initial 
assumptions from the actual situation met 
during this procedure, and concludes with a 
summarization of the problems attested which 
undermine the viability of multilingual parallel 
corpora construction.  
1 Introduction 
INTERA (Integrated European language data 
Repository Area, Contract 22076Y2C2DMAL2) is 
an EU-funded project within the eContent 
framework, aiming at  
? building an integrated European Language 
Resources (LRs) area by connecting existing 
data centers at regional, national and 
international level, and 
? at proposing "ways and techniques for LRs 
packaging to make it a profitable and attractive 
task to eContent professionals"; as an 
application of this task, the production of 
multilingual resources, namely parallel corpora 
and multilingual terminologies extracted from 
these, is undertaken (INTERA Technical 
Annex). 
This paper focuses on the second aim of the 
project, presenting the work carried out in the area 
of parallel corpus production, identifying the steps 
followed in this process, in order to point out the 
problematic areas involved in the task and suggest 
ways of encompassing them. 
2 Methodology and specifications  
The process usually followed in the LRs 
production involves the following tasks: (a) 
identification of user needs and requirements, (b) 
specifications for the selection, construction and 
packaging of the LRs, (c) identification of potential 
sources, (d) construction of the LRs per se, (e) 
promotion and distribution of the LRs. 
Given that INTERA is an eContent project, the 
target user group defined by the Technical Annex 
of the project was eContent professionals and 
users; furthermore, it was decided that the LRs to 
be produced (which would be of interest to this 
group) would be parallel corpora and multilingual 
terminological lists. Finally, the most important 
objective of the LRs production was the definition 
of a business model which would be attractive to 
the abovementioned target group. 
The following sections discuss the actual steps 
taken for the implementation of these 
requirements.  
The target group of eContent players addressed 
by the project has been further defined as 
consisting of professionals involved with the:  
? production of digital content (authors or 
publishers)  
? Globalization, Internationalization, Localiz-
ation and Translation (GILT) processes, and  
? development of Human Language 
Technology (HLT) software, ranging from 
multilingual information retrieval and 
extraction tools, to content management and 
Computer-Assisted Translation or Machine 
Translation solutions. 
The next step concerned the identification of 
user needs and requirements on the basis of the 
professionals? working habits and processes. This 
was achieved by exploiting the results of a number 
of previous initiatives to roadmap the state-of-the-
art in multilingual LRs, in combination with new 
initiatives undertaken in the framework of the 
project and targeted to the eContent world.  
The surveys conducted in the framework of the 
ENABLER project (Maegaard et al 2003, 
Gavrilidou & Desipri 2003) provided insights as to 
the existence and availability of different types of 
LRs, language demand, domains of interest, 
standards, etc. Although ENABLER focused on 
the LRs developer?s point of view, a number of 
valuable results were elicited. Other surveys, such 
as those conducted by ELRA and its distribution 
agency ELDA aiming at determining the needs of 
users with respect to available and potentially 
available LRs (http://www.elra.info/), or surveys 
available over the Internet through the sites of 
international organizations such as LISA and IDC 
or consultancy firms (http://www.globalsight.com, 
LISA 2001, LISA/AIIM 2001, LISA/OSCAR 
2003) shed a light as to the availability of 
resources and relevant tools.  
The information elicited from these surveys was 
coupled by a study of the activities of the eContent 
professionals as regards LRs, conducted in the 
framework of INTERA (Gavrilidou et al 2004) 
through the circulation of a questionnaire 
distributed to potential users, as well as through 
personal contacts with a number of actors in the 
relevant fields. The main areas of the study 
concerned the types of LRs the eContent 
professionals are interested in, domains and 
languages of interest, and, most important, policies 
concerning the way they acquire, use and exploit 
LRs and tools. 
The study of the target group yielded the 
following specifications: 
? domains: it is obvious that eContent users are 
more interested in specialized domains than in 
general language resources; moreover, the 
survey results showed health/medicine, 
tourism, education, law, automotive industry 
and IT/telecommunications, as being the 
prevailing ones. In the framework of the 
INTERA project, however, we decided to 
focus on the prevailing domains as long as 
they promote multilingual and multicultural 
content. The selected domains are: health, 
tourism, education and law, which correspond 
to the predominant digital activities, namely, 
eTourism, eHealth, eLearning, eGovernment 
and eCommerce. 
? languages: the focus of eContent and the needs 
of the users pointed towards the less widely 
spoken languages, including Balkan and 
Central and Eastern European languages (i.e 
the languages of the new EU countries).  
The project aims at the construction of a 
multilingual parallel corpus of 12 million 
words in total. The ideal scenario for the 
intended application of term extraction would 
be that of having a corpus with a source or 
pivot language and translations of the same 
texts in a number of target languages; 
however, given that the project aims at 
proposing realistic solutions to be adopted in 
the future by prospective LRs creators, real-life 
drawbacks should be taken into account; 
therefore, the limitations in the availability of 
existing resources (see section 3.1) dictated the 
decision to collect resources for four pairs of 
languages: Greek-English, Bulgarian-English, 
Slovene-English and Serbian-English. 
The specifications for the processing of the 
corpus have been based on the requirements of its 
intended application, which is the extraction of 
terminology, and involve the following tasks: 
? alignment of the texts: for the specific 
application purposes, alignment at sentence 
level has been deemed sufficient; however, the 
quality of the output is considered crucial; 
therefore, automatic processing is followed by 
human validation by language experts; 
? external and internal structural annotation: the 
minimal requirements include segmentation at 
sentence level for the alignment task and 
metadata information that will be required for 
the distribution and re-use of the corpus; 
? linguistic processing: below-Part of Speech 
(PoS) tagging and lemmatization is the 
minimum information required for the 
automatic term extraction task. 
To ensure re-usability of the collected and 
processed material, compliance with the following 
internationally accredited standards was decided: 
? the aligned material conforms to the TMX 
standard (Translation Memory eXchange, 
http://www.lisa.org/tmx/), which is XML-
compliant. Being a vendor-neutral, open 
standard for storing and exchanging translation 
memories created by Computer Aided 
Translation (CAT) and localization tools, 
TMX standard was identified as a requirement 
for the eContent professionals. It allows easier 
exchange of translation memory data between 
tools and/or translation vendors with little or 
no loss of critical data during the process; 
? for the external annotation, the IMDI metadata 
schema (IMDI, Metadata Elements for Session 
Descriptions, Version 3.0.4, Sept. 2003, 
http://www.mpi.nl/world/ISLE/schemas/schem
as_frame.html) has been selected; the internal 
structural annotation adheres to the XCES 
standard, i.e. the XML version of the Corpus 
Encoding Standard (XCES, 
http://www.cs.vassar.edu/XCES/ and CES, 
http://www.cs.vassar.edu/CES/CES1-0.html). 
? the linguistic annotation of the texts also 
adheres to the XCES standard, which 
incorporates the EAGLES guidelines for 
morphosyntactic annotation 
(http://www.ilc.cnr.it/EAGLES96/home.html). 
3 Corpus construction 
3.1 Text collection 
In order to construct the parallel corpus, the first 
step consisted in the identification of potential 
sources, i.e. existing parallel corpora and, 
alternatively or additionally, textual material that 
could be used for the creation from scratch of the 
INTERA corpus. 
Previous surveys (see section 2) that identify 
existing LRs as well as a search over the Internet 
attested the scarcity of available resources in the 
selected languages and domains, and so, the idea of 
re-using existing corpora was abandoned in favour 
of the construction of a new corpus from scratch. 
The identification process of potential sources 
had to take into consideration the following 
requirements: 
? to obtain texts from a variety of sources of 
interest to the eContent society, 
? to ensure that the material was free of 
Intellectual Property Rights problems, either 
through the arrangement of specific 
agreements or by obtaining them from public 
sources. 
The ideal candidates, in this respect, mainly 
consist of texts available over the Internet, 
provided by organizations/institutions that wish to 
make their own material available in more than one 
language, such as international organizations (e.g. 
United Nations, European Union, World Health 
Organization, Non-Governmental Organizations, 
etc.), multinational companies, companies with 
activities outside their own country (e.g. data 
describing company profiles & activities, product 
catalogues, etc.), public administration services 
(e.g. regarding bilateral agreements, regulations for 
immigrants, etc.), news agencies (targeting 
international broadcasting or for foreign language 
audience within their own country), official 
national government sites, national tourism 
organizations, etc. In all the above cases, the 
material consists of either web content per se (i.e. 
mainly bilingual web sites, rarely trilingual or 
quadrilingual) or of texts (official documents, 
technical reports, etc.) included in the web sites. 
A more careful investigation, however, of web 
texts showed that although Internet is rapidly 
becoming multilingual, it is not yet parallel, 
especially as regards the languages involved in the 
project: most international bodies include original 
and translated texts but only in the more widely 
spoken languages. Moreover, a closer inspection of 
web texts that "seem" parallel, on the basis of 
structural similarities (e.g. similar size, paragraph 
segmentation, possible "anchors", such as list 
enumerators, etc.) showed that only sporadic parts 
of them were parallel. More problems arise from 
the fact that texts may contain large parts of 
foreign language material (e.g. EU regulations that 
include amendments to previous regulations by 
including the replacement text of specific 
paragraphs in all EU languages). 
Given the above observations, cooperation with 
other data centers, with proven expertise in the 
area of LRs production for the specific project 
languages was sought; this would ensure content 
quality of the corpus, both during the selection (i.e. 
native speakers are better qualified to recognize 
true parallel material) and the encoding and 
validation processes, especially as regards the 
alignment validation and the linguistic processing. 
ILSP remains responsible for the construction of 
the Greek-English corpus, the collection and 
harmonization of the four subcorpora, the 
linguistic processing of the English texts and the 
addition of the IMDI metadata. 
3.2 Text processing 
Depending on the source that provided the 
original material (e.g. web site content, publishing 
house, translation company, etc.), different 
processing was required in order to arrive at the 
desired format adhering to the specifications set by 
the INTERA project; such as, indicatively: 
? conversion of the original PDF/RTF/HTML 
etc. files into the format required by the 
various tools (tokenizer, aligner, tagger), 
? cleanup of the texts from unwanted material 
(e.g. tables, figures, foreign language material, 
etc.) 
? re-structuring of the original monolingual texts 
from the TMX file, when the source was the 
output of a Translation Memory, 
? manual or semi-automatic annotation of 
metadata. 
 
Each language team undertook the processing of 
the collected material (i.e. alignment and human 
validation, structural and linguistic annotation 
without human validation), using their own tools, 
thus ensuring that no time is lost over training with 
new tools and that the required language-
dependent tools (especially taggers) used in the 
project are the most appropriate ones. The material 
to be delivered, however, at the end of all 
processes must be conformant to the selected 
standards. 
The intervention of ILSP takes place only at the 
end of this process, with the purpose of validating 
the conformance of the results and of harmonizing 
any problematic issues. The most important point 
of this process is the linguistic annotation and, 
specifically, the harmonization of the different 
tagsets used. In conformance with the 
methodology adopted in the project, i.e. of re-using 
existing material, whenever possible, with the least 
possible interventions, so as to ensure time and 
cost efficiency, it was decided to re-use only 
existing tools for each language, without making 
any modifications to the tools themselves but only 
conversion(s) of their output. Therefore, the task of 
harmonizing the output with regard to the 
morphosyntactic tags employed by each tagger is 
the last stage of the procedure, where all tagsets are 
mapped to one, based on the EAGLES guidelines.  
4 Conclusions  
In this paper, we described the methodology 
followed in the construction of a multilingual 
parallel corpus; this task has been interpreted as a 
test application endeavor in the process of defining 
a business model for the LRs production. The 
effort was to identify gaps and shortcomings in the 
process usually employed by LRs producers (or 
users who might wish to create their own LRs) and 
to suggest ways of remedying them. Our findings 
include: 
? problems faced during the acquisition phase: 
although an increasing supply of raw data (e.g. 
over Internet) and tools capable of exploiting 
this data (e.g. web crawlers that can identify 
and download texts in a given language) is 
attested, there is also a need for the 
enhancement of these tools with more 
intelligent techniques (e.g. incorporation of 
alignment techniques during the acquisition 
process in order to spot potential parallel texts, 
identification and mark-up of large foreign 
language excerpts), 
? problems faced during the processing phase: 
in order to enhance the LRs production effort, 
the re-use of existing tools is considered 
crucial. It is true that an increasing number of 
tools are available for text processing; 
however, this is oriented mainly towards the 
major languages. Moreover, information 
concerning the existence, availability and 
operation of existing tools is not easy to locate 
? a gap that the other pillar of INTERA tries to 
remedy through the building of an integrated 
European Language Resources area. 
Additionally, tools must be enhanced with 
respect to two directions: improvement of the 
tools themselves (e.g. more robust alignment 
techniques) and interoperability of all relevant 
tools currently used at different phases of 
processing. The issue of interoperability is 
closely related with the issue of standards. The 
promotion and deployment of existing 
standards as well as the creation of new 
standards, when these are lacking, is important 
to ensure viability and re-use of LRs, given the 
cost of their production. 
References  
Gavrilidou, M., E. Desipri. 2003. Final Version of 
the Survey, ENABLER Deliverable 2.1.  
Gavrilidou, M. E. Desipri, P. Labropoulou, S. 
Piperidis, N. Calzolari, M. Monachini & C. 
Soria. 2004. Technical specifications for the 
selection and encoding of multilingual resources, 
INTERA (Integrated European language data 
Repository Area), Deliverable 5.1. 
IMDI, Metadata Elements for Session 
Descriptions, Version 3.0.4, Sept. 2003. 
INTERA ? eContent  2002 Integrated European 
languages data Repository Area, Technical 
Annex. 
LISA. 2001. The LISA Globalization Strategies 
Awareness Survey. 
LISA/AIIM. 2001. The Black Hole in the Internet: 
LISA/AIIM Globalization Survey. 
LISA/OSCAR. 2003. Translation Memory Survey. 
Maegaard, B., K. Choukri, V. Mapelli, M. 
Nikkhou & C. Povlsen. 2003. Language 
resources-Industrial needs, ENABLER 
Deliverable 4.2. 
 
 
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, page 1,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
Machine Translation and its philosophical accounts
Stelios Piperidis
Institute for Language and Speech Processing,
?Athena? Research Centre
spip@ilsp.gr
Abstract
This paper attempts to explore the interrelation 
between  philosophical  accounts  of  language 
and respective technological  developments in 
the field of human language technologies.  In 
doing so, it focuses on the interaction between 
analytical philosophy and machine translation 
development,  trying  to  draw  the  emerging 
methodological analogies.
1 Introduction
Philosophical accounts of science and respective 
technological development bear a tight interrela-
tion  and  continuous  interplay.  Likewise,  philo-
sophical  investigations  of  language  bear  their 
own implications on how technology processing 
language,  in  monolingual  or  multilingual  set-
tings, evolves. 
In  the  multilingual  setting,  machine  translation 
feasibility,  its  presuppositions and implications, 
brings forth a range of questions, applicable to 
human translation as well, including, but not lim-
ited to, linguistic and ontological relativity, inde-
terminacy of translation,  inscrutability of  refer-
ence,  representational  function of language,  the 
problem of meaning. 
Bar  Hillel?s  claims  on  the  infeasibility  of  ma-
chine translation and the Sapir-Whorf hypothesis 
with  the  linguistic  determinism  and  relativity 
principles, partly backed up by Quine?s ontologi-
cal relativity and later Wittgenstein?s private lan-
guage  and  variability  of  language  games  have 
haunted the way of thinking in machine transla-
tion development. Indeterminacy,  relativity, and 
the consequent abolition of the one gold transla-
tion idea, however, as well as the necessity for 
frameworks  integrating  pragmatic  and  be-
havioural  data  in  translation  have  played  their 
role  in  advancing  machine  translation  design 
paving the way for observed paradigm shifts at 
all stages of development.  
In broadly dividing machine translation history 
in the rule-based and corpus-based eras,  in  the 
50?s  and  80?s  respectively,  one  can  draw  the 
analogies that would rather point to a tight inter-
action between philosophical  accounts and ma-
chine translation paradigms.
Early contemporary analytic philosophy, through 
conceptual,  reference-bound  analysis  and  com-
positionality principles, provided the foundation 
for  representational,  rule  and  knowledge-based 
approaches  of  early  machine  translation,  from 
50?s through the 80?s. The turn, in analytic phi-
losophy, to an understanding of meaning though 
use,  to  pragmatics  and  behaviourism,  may  be 
paired and seen as laying the foundation for the 
machine  translation paradigm shift  observed in 
the 80?s. In this pairing, it is the use of the much 
required parallel (or comparable) translation data 
that could be seen as constituting the behavioural 
data  base,  with  each  alignment  function  being 
conceived of as the result of a radical translation 
process, where a source language sentence pro-
vides the sensory data and a target language sen-
tence provides the linguistic observation. In such 
a framework, this aligned data source does pro-
vide the ?translation manual?, which after a se-
ries of  inductive operations does converge to a 
potentially usable set of translation relations. 
Along this line, we will discuss, in this talk, the 
continuous  relations  between  analytic  philoso-
phy, linguistic science and human language tech-
nologies. Such relations, direct or indirect, can be 
bi-directional and can possibly work towards bet-
ter understanding and facilitating the virtuous cy-
cle between language technology and its theoreti-
cal underpinnings.        
1
Application of Analogical Modelling to Example Based Machine Translation 
Chr i s tos  Ma lavazos  l' 2 Ste l ios P iper id is  !'2 
%stitute for Language and Speech Processing, 2National Technical University of Athens 
6 Artemidos & Epidavrou, 151 25 Marousi, Athens, Greece 
{christos, pip}@ilsp.gr 
Abstract 
This paper describes a self-modelling, incremental gorithm for learning translation rules from existing 
bilingual corpora. The notions of supracontext and subcontext are extended to encompass bilingual 
information through simultaneous analogy on both source and target sentences and juxtaposition of 
corresponding results. Analogical modelling is performed uring the learning phase and translation 
patterns are projected in a multi-dimensional analogical network. The proposed fi'amework was evaluated 
on a small training corpus providing promising results. Suggestions to improve system performance are 
1. Introduction 
Ideally, an EBMT system must determine 
correspondences at a sub-sentence l vel if optimal 
adaptation of matching fragments i to be achieved 
(Collins, B., & Cunningham, P. 1995). In practice, 
EBMT systems that operate at sub-sentence l vel 
involve the dynamic derivation of the optimum 
length of segments of the input sentence by 
analysing the available parallel corpora. This 
requires a procedure for determining the best 
"cover" of an input text by segments of sentences 
contained in the database (Nirenburg, S. 
Domashnev, C., Grannes, D. 1993), (Cranias, L. et 
al 1994), (Frederking, R., Nirenburg, S., 1994), 
(Sato, S. 1995). What is needed is a procedure for 
aligning parallel texts at sub-sentence level, 
(Sadler, V., Vendehnans, R. 1990), (Boutsis, S., 
Piperidis, S. 1998). If sub-sentence alignment is 
available, the approach is fully automated but is 
quite vulnerable to the problem of low quality, as 
well as to translational mbiguity problems when 
the produced segments are rather small. 
Several approaches aim at proceeding a step 
further, by attempting to build a transfer-rule base 
in the form of abstract representations through 
different ypes of generalization processes applied 
on the available corpora relying on different levels 
of linguistic information and processing (Kaji et al 
92), (Juola, P. 1994), (Furuse, O., Iida, H. 1996), 
(Veale, T. and Way, A. 1997), (McTait, K., et al 
1999), thus providing more complete "context" 
information to the translation phase. The deeper the 
linguistic analysis involved in such a process, the 
more flexible the final translation structures will be 
and the better the quality of the results. However, 
tiffs kind of analysis unquestionably leads to more 
computationally expensive and difficult to obtain 
systems. Our approach consists in a fully modular 
analogical fiamework, which can cope with lack of 
resources, and will perform even better when these 
are available. 
Analogical Modelling (AM) has been proposed as 
an alternative model of language usage. The main 
assumption underlying this approach is that many 
aspects of speaker performance are better 
accounted for in terms of "analogy", i.e. the 
identification of similarities and differences with 
forms in memory (the lexicon), than by referring to 
explicit and inaccessible rules. By "analogy" we 
mean the process of matching between an input 
pattern and a database of stored examples 
(exemplars). The result of this matching process is 
a collection of examples called the "analogical set" 
and classification of the input pattern is achieved 
through extrapolation fi'om this set. At any given 
time, the main source of knowledge consists in a 
database of stored translation examples. These 
examples themselves are used to classify new 
items, without intermediate abstraction i the form 
of rules. In order to achieve this exhaustive 
database search is needed, and during this search, 
less relevant examples need to be discarded. All 
text features are equally important initially, and 
serve to partition the database into several disjoint 
classes of examples. 
In contrast o most of the analogy-based systems 
our approach applies tile same principles during the 
learning phase in an attempt to extract appropriate 
generalizations (translatiou rules) based on 
similarities and differences between input 
exemplars. In this way, analogy is treated as more 
516 
than simple pairwisc simila,ity between input and 
database xemplars, rather it is conside,'ed as the 
main relation underlying a more complex network 
of relations between database xemplars. 
2. General 
The main idea behind otu" approach is based o,1 the 
observation that given any source and target 
language sentence pair, any alteration of the source 
sentence will most likely result in one or more 
changes in the respective target, while it is also 
highly likely that constant and variable units of the 
source sentence correspond to constant and variable 
target units respectively. Apart from cases of so 
called "translational divergences" (Dorr, B. 1994) 
as well as cases of idiomatic expressions, in most 
eases the above assumption hokts true. Especially 
in the case of technical sublanguagcs, where rather 
literal and accurate translation is expected, 
? y '~ "translational divergences are limited while 
idiomatic expressions can be captured and finally 
rejected fiom the main process, through certain 
constraints, as this will be explained later on. 
The matching process as this is described by 
(Daelemans W., et al 1997) based on Skousen's 
analogical modelling algorithm (Skousen, R. 1989), 
consists of two subsequent s ages. The first stage of 
the matching process is the construction of 
"subcontexts", these are sets o1' examples and they 
are obtained by matching the input tmttern, feature 
by feature, to each database item on an equal/not- 
equal base, and classify the database examples 
accordingly. Taking the input pattern ABC as an 
example ight (=2 3) different and mutually disjoint 
subcontexts would be constructed: 
ABC, ~,BC, ABC, ABC, ABC, ABC, ABC, ABC 
where the macron denotes complementation. Thus 
exemplars in the second class share only the second 
and third feature with the input pattern. 
in the following stage "supraeontexts" are 
constructed by generalising over specific feature 
values. This is done by systematically discarding 
features fi'om the input pattern and taking the union 
of the subcontexts that are subsumed by this new 
pattern. Supracontexts can be ordered with respect 
to generality, so that most specific supracontext 
contains items that share all features with the input 
pattern while the less specific ones those items that 
share at least one feature. The most general 
supracontext contains all database examples 
whether or not they share any features with the 
input pattern. 
Some exemplary supracontexts together with the 
respective subeontexts for the input pattern ABC 
are I)rcsented in the following table? 
A B - ABe, ABC, 
A-  C ABC, ABC 
- B C ABe, ~,BC 
A- -  ABC, ABC, AB(~, ABC 
1,1 addition, our approach introduces a second 
dimension to tile above described process, that of 
language, by simultaneously performing the 
matching process to target language equivalents 
and aligning individual results, based on the 
principles described earlier. Therefore, what we are 
ultimately searching for, are source and target 
sentence pairs for which evidence of 
correspondence between any or all of respective 
subcontcxts within the available training corpora is 
available. This will subsequently lead to links 
between respective supracontexts. For example : 
\[As BsCs\] o \[At Bt Ct \ ] - '~  
AND __~>-=> \[As Bs-\] ~ \[At Bt-\] 
\[As B~Cs\] o \[At Bt Ct \ ] . . f l  
Subcontexts Supracontexts 
(Where s = Source Language, t = Target Language ) 
3. The learning mechanism 
3.1 Translation Templates 
Supracontexts and translation templates can be 
viewed as two sides of the same coin. 
Generalization through unification on feature 
values of neigbbouring sentences, if these satisfy, 
certain criteria, leads to more abstract expressions 
of bilingual pairs of "pseudo-sentences", consisting 
of sequences of constant and variable elements, 
517 
m~ 
.+ 
m~ 
I 
(application)41- . . . . . . . . . . .  T . . . . . . . . . . . . . . . . . . .  ~ I ~?0appoy/l~ 
u + ; ' b v I v 
+ l++ 1 +- ' -+  ) + ,  x,+, I 
( 1 . . . . . . . . . .  ;{  ?.+u - - I I '+ network ~. . . . . . . . . . . .  I . . . . . . . . . .  
Syntagmatic Relations 
Customizing I + 
(1):Customizing application settings <=:> Flpoaappoyq puepicr~tov ~(oaPllOg/lf, and 
(2): Customizing network settings ~> npo<~cxppoyq puBpicr~tov 51KrOOU is" 
where ? \[ Xs l \ ]  ~ \[ Xt l \ ]  and, 
application ~ ~q)appoyfl(; 
network ~ 61KTt)OU 
Figure I 
where variable elements are represented by special 
symbols ("Xi") and constant-fixed elements act as 
the context in each case. 
3.2 Translation Units 
Discarded features (represented by the "-" symbol) 
of corresponding supracontexts, rising from 
variable elements of the matching sentences, 
correspond to the translation units of the respective 
translation patterns. As a result, single or multi- 
word elements (translation units) of source and 
target language appearing within corresponding 
supracontext positions, are linked and stored, 
comprising the bilingual translation unit lexicon. 
3.3 The Analogical Network 
The main linguistic object for which matching is 
performed is not the sentence but pairs of source 
and target sentences/exemplars. Therefore, 
matching between linguistic objects is performed in 
two dimensions imultaneously, that is between 
source and target sentences of matching pairs 
respectively. The result of the process, if certain 
conditions are met, are stored in an "analogical 
network" (Federici, S. & Pirrelli V., 1994) of inter- 
sentence and intrasentence r lations between these 
exemplars and their generalizations. A rather 
simple example of this is presented ill Figure 1. 
Different parts of matching sentences are replaced 
by corresponding variables, and are consequently 
assigned the role of translation units, while 
similar/constant parts are considered to be the 
context under which variable units are instantiated. 
The union of context and variables establishes the 
"generalized" translation (paradigmatic) patterns 
between source and target language. The similar 
(constant) and different (variable) parts between 
source and target sentences are factored out and 
presented as separate nodes in the above diagram. 
For each sentence we can view its constituent 
single or multi-word, constant or variable units as 
separate nodes, where links between these nodes 
indicate the syntagmatic relations between them, 
that is, the way they actually appear and are ordered 
in the respective sentence. The vertical axis 
represents the paradigmatic dimension of available 
alternants, that is, the information concerning 
which substrings are in complementary distribution 
with respect o the same syntagmatic context i.e. 
with respect o the same context '?Customizing __ 
518 
settings". Syntagmatic links constitute the 
intrasentence relations/links between sentence 
constituents Mille paradigmatic ones correspond to 
the interscntential relations. Furthermore, a third 
dhnension is added to the whole framework, that of 
the "l'mguage", since all principles are applied 
simultaneously to both source sentences and their 
target equivalents. In case, linguistic annotations 
are available, they are appropriately incorporated in
the respective nodes. 
At this point no conflicts are resolved. All possible 
patterus are stored in the network including 
conflicting as well as overlapping patterns. 
However, all links both paradigmatic and 
syntagmatic are weighted by frequency 
information. Tiffs will eventually provide the 
necessary informatiou to disable and even discard 
certain false or useless variables or templates. 
3.4 The Algorithm 
Translation templates as well as translation units 
are treated as paradigmatic flexible structures that 
depend on the available evidence. As new data 
come into the system, rules can be extended or even 
replaced by other more general ones. It is usually 
assulned that there is only one fixed way to assign a 
structural representation to a symbolic object either 
be a translation unit or a translation template. 
14owever, it is obvious that in our approach there is 
no initial fixed definition of this particular 
structure, rather it is left up to the training corpus 
and the learning mechanism. As was expected, 
under this kind of analogy-based approach, 
linguistic objects were determined based on the 
paradiglnatic context hey appeared in, resulting in 
a more flexible and also corpus dependent 
definition of translation units. 
Search Space Reduction 
In general, if sentence matching were 
unconstrained and all resulting matches were stored 
in tile analogical network, then the number of all 
links (inter/intra-sentential) for N equal to the 
number of translation patterns learned through the 
process and L equal to the number of words in a 
sentence (template) would be : 
while the complexity of the learning phase is also 
increased by tile fact that each candidate rule needs 
to be verified against the available corpus, 
introducing an additional parameter S, that of the 
size of tile training corpora (in number of 
sentences). 
Moreover, if a rather straightforward approach in 
matching was to be followed, the complexity 
involved for each individual candidate senteuce 
would be enormous. In such an approach, for each 
candidate sentence, all corresponding subcontexts 
would have to be identified and verified against he 
available corpora. For instance, a sentence of length 
L would generate 2 ~' subcontexts, thus resulting in 
0(2 L) required search actions against he available 
corpora. Even if constraints would be set upon the 
length of possible ignore (variable) areas, for 
example = 5 words, the process would still be too 
complex. For example for a sentence of length L = 
10 and for variables of length up to 5 words, the 
possible subcontexts that have to be matched 
against the corpus would be (,,,)+/:)_,_ (,;)+ (:)+ (,;)--,o+,, 
210 + 36 =- 421, where terms of tile previous 
cquation correspond to the subcontexts with 
variables of length 1 to 5 respectively. 
The SSR methodology, depends on the specific 
needs of the particular task. Run-time pruning of 
possible matches can speed up the learning process, 
however it also reduces ystem recall & coverage. 
On the other hand, constraints on paradigmatic 
relations are more reliable providing better esults 
but cannot contribute to the speed of the learning 
process. SSR was based on an efficient indexing 
and retrieval mechanism (Wilhnan, N. 1994) 
allowing fast identification of "relevant" sentences 
based on comlnon single/multi-word units. In this 
way, the search space for each individual candidate 
was significantly reduced to a smaller set of 
possible matching sentences. 
Distance Metric 
The main objects of knowledge generated by the 
learning process are the translation patterns and the 
bilingual lexicon of translation units. During the 
learning process, both sources are enriched when 
possible. Sentences are analysed and encoded to 
two-dimensional vectors based on the words (first 
dimension) and the linguistic annotations (second 
dimension) they might contain. Then sentence 
vectors are compared on an equal - not equal basis 
519 
through a Levensthein or Edit distance algorithrn 
(Damerau, F. 1964), (Oflazer, K. 1996). The 
algorithm, implemented through a dynamic 
programming framework (Stephen, G. 1992), 
computes the minimum number of required editing 
actions (insertions, deletions, substitutions, 
movements and transpositions) in order to 
transform one sentence into another through an 
inverse backtracking procedure. The final similarity 
score is computed by assigning appropriate weights 
to these actions. For the time being only insertions 
and deletions were accounted for. More complex 
actions, like transpositions or movements of words 
and their influence in the final translation pattern 
will be the focus of future work. 
Variable Elements 
Diflbrences between matching sentences result in 
coupling of corresponding source and target words, 
as explained earlier in this section, thus enriching 
the lexicon with new information. Coupling is 
restricted to content words. Content words can 
usually be replaced by other words of the same 
category acting as potential variables (Kaji, H. et al
1992). On the other hand fimctional words do 
present an "abnormal" translational behavior, since 
they sometimes act as optional units which do not 
appear in both source and target segments, other 
times have a one-to-one correspondence, yet it is 
not rare that they affect the target pattern 
(especially when they participate in verb 
complementation). "Exclusion lists" were used for 
this purpose in order to reject functional words 
from acting as translation w~riables. 
Workflow 
All sentences are stored as vectors of constituent 
words-annotations. Functional words are marked as 
such. The process runs iteratively tbr all sentences 
starting l'rom sentences of length 1 to the maximum 
length appearing in the training corpus. The process 
terminates in case of an unsuccesslifl loop, meaning 
an iteration where no new information either 
translation units or templates were extracted. The 
learning process consisting of five subsequent 
phases, is depicted in detail in Figure 2 : 
Phasel Search Space Reduction : Extract an 
initial set of possibly relevant sentences tbr the 
current input sentence. 
Phase2 Sentence Matching : Match Input sentence 
against he previous et. Matching candidates are 
SearehS_pace Reduction \] ~;i 
(SSR) 
I Sentence Matchinq~ \] 
(Edit Distance) 
LEARNING PHASE 
Identif~AII Subcontexts 1 -~-~h Target Equivalents 
Resolve Differences 
Identify Variables/Tunits 
_. Enrich Bilingual Lexicon 
Identi_fffy_A/l_S u p raco ntexts 
Unify Variable Feature Values \] 
Extract Translation Patterns 
Sup~x~a~erns  
L Fnrinh/llndat~ Ana~nical N~.twnrk 
Figure. 2 
sorted based on distance score. Matches with fewer 
differences are examined first. 
Phase3 Identification of Subcontexts :For each 
naatching candidate, identify the respective 
subcontext of the input sentence that it adheres to. 
Examine target language equivalents. Resolve 
differences between source and target language 
matching candidates based on already existing 
intbrmation contained in the bilingual exicon. 
During this process, the bilingual translation unit 
lexicon is enriched with any successfldly resolved 
difference (even if the particular candidate will not 
finally lead to a new translation pattern). 
520 
Phase4 Identification of Supracontcxts : Based 
on tile ah'eady identified subcontexts 
produce tile respective supraeontexts through 
unification of respective variable feature values. 
Phase5 Extraction of Translation Patterns : 
Construct corresponding translation patterns from 
existing supraeontexts. Update analogical network. 
In case a pattern ahcady exists, update the weight 
of its constituent links. 
At the end of the learning process the analogical 
network has been enriched with all possible 
translation patterns and variables/units extracted 
fi'om the available corpora. Conflict resolution and 
network refinement in general is performed on the 
final results, where all information is available as 
described in the next section. 
3.5 Network Refinement 
As mentioned earlier, tile analogical network 
contains all translation alternatives For individual 
translation units as well as all translation patterns 
resulting fiom the learning process, l lowever, link 
weight information is also included in the above 
framework representing the validily of a particular 
relation against he training corpus. 
Translation alternatives of individual units (in 
our case words) are implicitly classified through 
their context, that is the constant part of the 
translation patterns they participate in. These will 
constitute the main selection criterion during 
translation, l lowevcr, fi'equency inlimnation is 
also used in order to disable and finally discard 
obsolete or erroneous translation unit alternatives. 
Translation templates are compared with respect 
to their source and target language constituent 
patterns: (a) Conflicting templates, that is 
templates haring only one of the two patterns are 
subsequcntly checked in terms of weight 
information. Templates of equivalent weights are 
considered equally effective. This is usually tile 
case where different translations are produced fi'om 
the same source pattern due to semantic difliarences 
on the variables it contains. Conflicting templates 
with significantly low weights (under a predefined 
threshold), are judged ineffective or "exceptional" 
(Nomiyama, It. 1992) and are flagged as such in 
order to receive a special treatment during the 
translation phase (Watanabe, 1t. 1994). These can 
even be disabled or discarded fiom the network 
depending on their significance weight lhrough a 
dynamic 'Torgetting and remembering" process 
(Streiter, O. et al 1999). (b) Overlapping 
templates, where both source and target patterns of 
one template can be generated from the other by 
coupling words of the constant part of the template 
through valid translation alternatives included in 
the network, are identified and the more general 
ones are preferred. A basic requirement is that tile 
set of all translation alternatives instantiating the 
variables of the more general template is a superset 
of those instantiating the less general one. In any 
other case, both templates are retained. And finally, 
(c) complementary templates, are also identified 
and replaced by their union. 
4. Evaluation 
The training set consisted of a bilingual (EN-GR) 
technical corpus (automotive industry) of 5K 
sentences, -20K wordl'orms on each language. The 
process resulted in ~550 translation rules, and 350 
translation units (~50 multi-word ones). The 
precision estimated through manual evaluation was 
~75%. More than 23% of the erroneous rules were 
due to idiomatic expressions. The rest of tile errors 
was caused by imprecise translation patterns found 
in the corpus. However, these errors being rather 
exceptional, received a very low weight of 
effectiveness atlhe end of the process. No straight 
forward approach to measure the recall of the 
learning process was devised, since it was not easy 
to a-priori determine the number off rules that 
should be extracted fiom tile training corpus. 
Howcvcr, coverage of the final translation rule set 
against the corpus was measured and found equal to 
38%. More specifically, the set of 500 rules could 
tlu'ough an inverse process generate 38% of tile 
corpus sentences, subsequently interpreted in a 
significant gain in terms of storage space. Another 
obvious benefit is the subsentential alignment 
information that is, the source and target ranslation 
units learned at tile end of the process. 
5. Conclusion & Future Work 
We have presented a self-modelling, incremental 
analogical algorithm for extracting translatiou 
patterns fi'om existing bilingual corpora s well as a 
method for efficient storage and representation f
extracted relations between various units of text. 
Not surprisingly, the quality of the results depends 
on the available information in terms of quantity as 
521 
well as quality and depth. Lack of any kind of 
linguistic information will consequently result in 
translation rules based only owl "shallow" evidence. 
Similarly, information of low quality will generate 
erroneous rules. However, this is a basic 
presupposition of any EBMT system: "what you 
give is what you get...". 
Tile proposed fi'amework was initially evaluated 
based only owl string form information. However, 
tile model can easily take into account "deeper" 
linguistic knowledge during the learning phase, 
thus improving tile quality of the final results. 
Evaluation of learning performance in this case is 
the main object of ctn'rent work. 
Another, interesting issue is how the current 
fi'amework can constrain acceptable multi-word 
variables in order to reduce computational 
complexity. In present, accepting or rqiecting 
candidate variables extracted from the sentence 
matching process, is based on a simple heuristic of 
length in content words. This type of approach 
would presumably require some kind of clue on 
what could be an acceptable translation unit pattern 
(Juola, P. 1994), (Furuse, O., lida, 14. 1996). 
Finally, future work will mainly fbcus on how the 
system can invoke all existing information in order 
to generate new translations, mainly aiming at 
automatic and (senti-) automatic methods for 
"recursive" as well as "parallel" utilization of 
multiple translation rules towards optimal 
"coverage" of new incoming sentences. 
7. References 
(Boutsis, S., Piperidis, S. t998) Aligning Clauses in 
Parallel Texts. 3ld Conference on Empirical Methods in 
Natural Language Processing, June 1998 
(Collins, B., & Cunningham, P. 1995) A Methodology 
th for EBMT. 4 International Conference on the Cognitive 
Science of Natural Language Processing, Dublin 1995. 
(Cranias, L., Papageorgiou, H. and Piperidis, S. 1994). 
A matching technique in Example-Based Machine 
Translation, Proc. of COLING-94, pp 100-105, 
(Daelemans, W., Gillis, S. & Durieux, G., 1997) 
Skousen's analogical modelling algorithm: a comparison 
with lazy learning. New Methods in Language 
Processing: Edited by Daniel Jones & Harold Somers, 
UCL Press, p.3-15. 
(Damerau, F. 1964) A Technique for Computer Detection 
and Correction of Spelling Errors. Communications of the 
ACM 7, p. 171-176, 1964. 
(Dorr, B. 1994) Machine Translation Divergences: A 
Formal Description and Proposed Solution. Association 
for Computational Linguistics, Vol. 20, 1994. 
(Federici, S. & Pirrelli, V. 1994). The compilation of 
large pronunciation lexica: the elicitation of letter to sound 
patterns through analogy based networks. Papers in 
Computational Lexicography, Complex '94, Budapest, 
59-67. 
(Frederking, R., Nirenburg, S., 1994)Three Heads are 
Better then One. Proceedings of the fourth Conference 
on Appfied Natural Language Processing, ANLP-94, 
Stuttgart, GernTany 
(Furuse, O., lida, H. 1996) Incremental Translation 
Utilizing Constituent Boundary Patterns. Proc. Coling-96, 
pp 412-417. 
(Juola, P. 1994) Self-Organizing Machine Translation: 
Example-Driven Induction of Transfer Functions. 
University of Colorado at Boulder, Technical Report CU- 
CS-722-94. 
(Kaji, H., Kida, Y., and Morimoto, Y., 1992) Learning 
Translation Templates from Bilingual TexL Proc. Coling., 
p. 672-678, 1992. 
(McTait, K., Olohan, M., Trujillo, A. 1999) A Building 
Blocks Approach to Translation Memory. Proc. From the 
21 st ASLIB Conference, London, 1999. 
(Nirenburg, S. Domashnev, C., Grannes, D. 1993) Two 
Approaches to Matching in Example-Based Machine 
Translation. Proc. of TMI-93, Kyoto, Japan, 1993. 
(Nomiyama, H. 1992) Machine Translation by Case 
Generalization. Proceedings of the \[sic\] International 
Conference on Computational Linguistics, COLING-92, 
Nantes, p. 714-720. 
(Oflazer, K. t996) Error-tolerant Finite State Recognition 
with Applications to Morphological Analysis and Spelling 
Correction. Association for Computational Linguistics, 
Vol. 22, (1), 1996 
(Sadler, V., Vendelmans, R. 1990} Pilot Implementation 
of a Bilingual Knowledge Bank. Proc. of Coling, pp 449- 
451, 1990. 
(Sato, S. 1995). MBT2: A Method for Combining 
Fragments of Examples in Example-Based Machine 
Translation. Artificial Intelligence 75, 31-49. 
(Skousen, R. 1989} Analogical Modelling of language. 
Dordrecht: Kluwe~. 
(Stephen, G. 1992) String Search. University College of 
North Wales, Technical Report TR-92-gas-01. 
(Streiter, O., Iomdin,L., Hong,M., Hauck, U., 1999) 
IAI CA T2 Publications, www.iai, uni-sb.de 
(Veale, T. and Way, A. 1997) Gaijin: A Bootstrapping 
Approach to Example-Based Machine Translation. 
International Conf., Recent Advances in Natural 
Language Processing, Tzigov Chark, Bulgaria, 239-244. 
(Watanabe, H. 1994) A Method for Distinguishing 
Exceptional or General Examples in Example-Based 
Transfer Systems. The 15 t~' International Conference on 
Computational LhTguistics, COLING-94, Kyoto, p.39-44. 
(Willman, N. 1994) A Prototype Information Retrieval 
System to Perform a Best-Match Search for Names. 
Conference Proceeding of RIA 0 '94. 
522 
