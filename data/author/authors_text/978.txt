Non-locality all the way through:  
Emergent Global Constraints in the Italian Morphological Lexicon 
Vito Pirrelli 
Istituto di Linguistica Computazionale 
CNR, Pisa, Italy 
vito.pirrelli@ilc.cnr.it 
Basilio Calderone 
Laboratorio di Linguistica  
Scuola Normale Superiore, Pisa, Italy 
b.calderone@sns.it 
Ivan Herreros 
Istituto di Linguistica Computazionale 
CNR, Pisa, Italy 
ivan.herreros@ilc.cnr.it 
Michele Virgilio 
Dipartimento di Fisica 
Universit? degli Studi di Pisa, Italy 
virgilio@df.unipi.it 
 
Abstract 
The paper reports on the behaviour of a Koho-
nen map of the mental lexicon, monitored 
through different phases of acquisition of the 
Italian verb system. Reported experiments ap-
pear to consistently reproduce emergent global 
ordering constraints on memory traces of in-
flected verb forms, developed through princi-
ples of local interactions between parallel 
processing neurons. 
1 Introduction 
Over the last 15 years, considerable evidence has 
accrued on the critical role of paradigm-based rela-
tions as an order-principle imposing a non-local 
organising structure on word forms memorised in 
the speaker?s mental lexicon, facilitating their re-
tention, accessibility and use, while permitting the 
spontaneous production and analysis of novel 
words. A number of theoretical models of the men-
tal lexicon have been put forward to deal with the 
role of these global constraints in i) setting an up-
per bound on the number of possible forms a 
speaker is ready to produce (Stemberger and Car-
stairs, 1988), ii) accounting for reaction times in 
lexical decision and related tasks (Baayen et al 
1997; Orsolini and Marslen-Wilson, 1997 and oth-
ers), iii) explaining production errors by both 
adults and children (Bybee and Slobin, 1982; By-
bee and Moder; 1983; Orsolini et al, 1998) and iv) 
accounting for human acceptability judgements 
and generalisations over nonce verb stems (Say 
and Clahsen, 2001). While most of these models 
share some core assumptions, they appear to 
largely differ on the role played by lexical relations 
in word storage, access and processing. According 
to the classical view (e.g. Taft, 1988) the relation-
ship between regularly inflected forms is directly 
encoded as lexical procedures linking inflectional 
affixation to separately encoded lexical roots. Ir-
regular word forms, on the other hand, are stored 
in full (Prasada and Pinker, 1993). In contrast to 
this view, associative models of morphological 
processing claim that words in the mental lexicon 
are always listed as full forms, establishing an in-
terconnected network of largely redundant linguis-
tic data reflecting similarities in meaning and form 
(Bybeee, 1995). 
Despite the great deal of experimental evidence 
now available, however, we still seem to know too 
little of the dynamic interplay between morpho-
logical learning and the actual working of the 
speaker?s lexicon to draw conclusive inferences 
from experimental findings. Associative models, 
for example, are generally purported to be unable 
to capture morpheme-based effects of morphologi-
cal storage and access. Thus, if humans are shown 
to access the mental lexicon through morphemes, 
so the argument goes, then associative models of 
the mental lexicon cannot be true. In fact, if asso-
ciative models can simulate emergent morpheme-
based effects of lexical organisation through stor-
age of full forms, then this conclusion is simply 
unwarranted.  
We believe that computer simulations of mor-
phology learning can play a role in this dispute. 
However, there have been comparatively few at-
tempts to model the way global ordering principles 
of lexical organisation interact with (local) proc-
essing strategies in morphology learning. In the 
present paper, we intend to simulate a biologically-
inspired process of paradigm-based self-
organisation of inflected verb forms in a Kohonen 
map of the Italian mental lexicon, built on the basis 
of local processes of memory access and updating. 
Before we go into that, we briefly overview rele-
vant machine learning work from this perspective. 
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
2 Background 
Lazy learning methods such as the nearest 
neighbour algorithm (van den Bosch et al, 1996) 
or the analogy-based approach (Pirrelli and 
Federici, 1994; Pirrelli and Yvon, 1999) require 
full storage of supervised data, and make on-line 
use of them with no prior or posterior lexical struc-
turing. This makes this class of algorithms flexible 
and efficient, but comparatively noise-sensitive 
and rather poor in simulating emergent learning 
phenomena. There is no explicit sense in which the 
system learns how to map new exemplars to al-
ready memorised ones, since the mapping function 
does not change through time and the only incre-
mental pay-off lies in the growing quantity of in-
formation stored in the exemplar data-base.   
Decision tree algorithms (Quinlan, 1986), on the 
other hand, try to build the shortest hierarchical 
structure that best classifies the training data, using 
a greedy heuristics to select the most discrimina-
tive attributes near the root of the hierarchy. As 
heuristics are based on a locally optimal splitting 
of all training data, adding new training data may 
lead to a dramatic reorganisation of the hierarchy, 
and nothing is explicitly learned from having built 
a decision tree at a previous learning stage (Ling 
and Marinov, 1993). 
To tackle the issue of word structure more 
squarely, there has been a recent upsurge of inter-
est in global paradigm-based constraints on mor-
phology learning, as a way to minimise the range 
of inflectional or derivational endings heuristically 
inferred from raw training data (Goldsmith, 2001; 
Gaussier, 1999; Baroni, 2000). It should be noted, 
however, that global, linguistically-inspired con-
straints of this sort do not interact with morphology 
learning in any direct way. Rather, they are typi-
cally used as global criteria for optimal conver-
gence on an existing repertoire of minimally re-
dundant sets of paradigmatically related mor-
phemes. Candidate morpheme-like units are ac-
quired independently of paradigm-based con-
straints, solely on the basis of local heuristics. 
Once more, there is no clear sense in which global 
constraints form integral part of learning. 
Of late, considerable attention has been paid to 
aspects of emergent morphological structure and 
continuous compositionality in multi-layered per-
ceptrons. Plaut et al (1996) show how a neural 
network comes to be sensitive to degrees of com-
positionality on the basis of exposure to examples 
of inputs and outputs from a word-reading task. 
Systematic input-output pairs tend to establish a 
clear one-to-one correlation between parts of input 
and parts of output representations, thus develop-
ing strongly compositional analyses. By the same 
token, a network trained on inputs with graded 
morphological structure develops representations 
with corresponding degrees of compositionality 
(Rueckl and Raveh, 1999). It must be appreciated 
that most such approaches to incremental com-
postionality are task-oriented and highly super-
vised. Arguably, a better-motivated and more ex-
planatory approach should be based on self-
organisation of input tokens into morphologically 
natural classes and their time-bound specialisation 
as members of one such class, with no external su-
pervision. Kohonen?s Self-Organising Maps 
(SOMs) (Kohonen, 1995) simulate self-
organisation by structuring input knowledge on a 
(generally) two-dimensional grid of neurons, 
whose activation values can be inspected by the 
researcher both instantaneously and through time. 
In the remainder of this paper we show that we can 
use SOMs to highlight interesting aspects of global 
morphological organisation in the learning of Ital-
ian conjugation, incrementally developed through 
local interactions between parallel processing neu-
rons.   
3 SOMs 
SOMs can project input tokens, represented as 
data points of an n-dimensional input space, onto a 
generally two-dimensional output space (the map 
grid) where similar input tokens are mapped onto 
nearby output units. Each output unit in the map is 
associated with a distinct prototype vector, whose 
dimensionality is equal to the dimensionality of in-
put vectors. As we shall see, a prototype vector is 
an approximate memory trace of recurring inputs, 
and plays the role of linking its corresponding out-
put unit to a position in the input space. Accord-
ingly, each output unit takes two positions: one in 
the input space (through its prototype vector) and 
one in the output space (its co-ordinates on the 
map grid).  
SOMs were originally conceived of as computer 
models of somatotopic brain maps. This explains 
why output units are also traditionally referred to 
as neurons. Intuitively, a prototype vector repre-
sents the memorised input pattern to which its as-
sociated neuron is most sensitive. Through learn-
ing, neurons gradually specialise in selectively be-
ing associated with specific input patterns. More-
over, memorised input patterns tend to cluster on 
the map grid so as to reflect natural classes in the 
input space.  
These interesting results are obtained through it-
erative unsupervised exposure to input tokens. At 
each learning step, a SOM is exposed to a single 
input token and goes through the following two 
stages: a) competitive neuron selection, and b) 
adaptive adjustment of prototype vectors. As we 
shall see in more detail in the remainder of this 
section, both stages are local and incremental in 
some crucial respects.1  
3.1 Stage 1: competitive selection 
Let vx be the n-dimension vector representation 
of the current input. At this stage, the distance be-
tween each prototype vector and vx is computed. 
The output unit b that happens to be associated 
with the prototype vector vb closest to vx is selected 
as the best matching unit. More formally: 
 { }ixbx vvvv ??? min ,  
 
where   is also known as the quantization error 
scored by vb relative to vx. Intuitively, this is to say 
that, although b is the map neuron reacting most 
sensitively to the current stimulus, b is not (yet) 
perfectly attuned to vx.  
Notably, the quantization error is a local distance 
function, as it involves two vector representations 
at a time. Hence, competitive selection is blind to 
general structural properties of the input space, 
such as the comparative role of each dimension in 
discriminating input tokens. This makes competi-
tive selection prone to errors due to accidental or 
spurious similarity between the input vector and 
SOM prototype vectors.     
3.2 Stage 2: adaptive adjustment 
After the winner unit b is selected at time t, the 
SOM locally adapts prototype vectors to the cur-
rent stimulus. Vector adaptation applies locally, 
within a kernel area of radius r, centred on the po-
sition of b on the map grid. Both vb(t) (vb at time t) 
and the prototype vectors associated with b?s ker-
nel units are adjusted to make them more similar to 
vx(t) (vx at time t). In particular, for each prototype 
vector vi in b?s kernel and the input vector vx, the 
following adaptive function is used 
 [ ])()()()1( tvtvhtvtv ixbiii ?+=+  , 
 
where hbi is the neighbourhood kernel centred 
around the winner unit b at time t, a non-increasing 
function of both time and the distance between the 
input vi and the winner vector vb. As learning time 
progresses, however, hbi decreases, and prototype 
vector updates become less sensitive to input con-
ditions, according to the following: 
 
                                                     
1 This marks a notable difference between SOMs and 
other classical projection techniques such as Vector 
Analysis or Multi-dimensional Scaling, which typically 
work on the basis of global constraints on the overall 
distribution of input data (e.g. by finding the space pro-
jection that maximizes data variance/co-variance). 
 )(),()( ttllhth ibbi ???= , 
 
where lb and li are, respectively, the position of b 
and its kernel neurons on the map grid, and ?(t) is 
the learning rate at time t, a monotonically decreas-
ing function of t. Interaction of these functions 
simulates effects of memory entrenchment and 
proto-typicality of early input data. 
3.3 Summary  
The dynamic interplay between locality and in-
crementality makes SOMs plausible models of 
neural computation and data compression. Their 
sensitivity to frequency effects in the distribution 
of input data allows the researcher to carefully test 
their learning behaviour in different time-bound 
conditions. Learning makes output units increas-
ingly more reactive to already experienced stimuli 
and thus gradually more competitive for selection. 
If an output unit is repeatedly selected by system-
atically occurring input tokens, it becomes associ-
ated with a more and more faithful vector represen-
tation of a stimulus or class of stimuli, to become 
an attractor for its neighbouring area on the map. 
As a result, the most parsimonious global organisa-
tion of input data emerges that is compatible with 
a) the size of the map grid, b) the dimensionality of 
output units and c) the distribution of input data.  
This intriguing dynamics persuaded us to use 
SOMs to simulate the emergence of non-local lexi-
cal constraints from local patterns of interconnec-
tivity between vector representations of full word 
forms. The Italian verb system offers a particularly 
rich material to put this hypothesis to the challeng-
ing test of a computer simulation. 
4 The Italian Verb System  
The Italian conjugation is a complex inflectional 
system, with a considerable number of classes of 
regular, subregular and irregular verbs exhibiting 
different probability densities (Pirrelli, 2000; Pir-
relli and Battista, 2000). Traditional descriptive 
grammars (e.g. Serianni, 1988) identify three main 
conjugation classes (or more simply conjugations), 
characterised by a distinct thematic vowel (TV), 
which appears between the verb root and the in-
flectional endings. First conjugation verbs have the 
TV -a- (parl-a-re 'speak'), second conjugation 
verbs have the TV -e- (tem-e-re 'fear'), and third 
conjugation verbs -i- (dorm-i-re 'sleep'). The first 
conjugation is by far the largest class of verbs 
TYPE EXAMPLE ENGLISH GLOSS 
[isk]-insertion + palatalization fi"nisko/fi"niSSi/fi"njamo (I)/(you)/(we) end 
[g]-insertion + diphthongization "vEngo/"vjEni/ve"njamo (I)/(you)/(we) come 
ablauting + velar palatalization "Esko/"ESSi/uS"Samo (I)/(you)/(we) go out 
[r]-drop + diphthongization "mwojo/"mwori/mo"rjamo (I)/(you)/(we) die 
Table 1. Variable stem alternations in the Italian present indicative. 
(73% of all verbs listed in De Mauro et al, 1993), 
almost all of which are regular. Only very few 1st 
conjugation verbs have irregularly inflected verb 
forms: andare 'go', dare 'give', stare 'stay' and fare 
?do, make?. It is also the only truly productive 
class. Neologisms and foreign loan words all fall 
into it. The second conjugation has far fewer 
members (17%), which are for the most part ir-
regular (around 95%). The third conjugation is the 
smallest class (10%). It is mostly regular (around 
10% of its verbs are irregular) and only partially 
productive. 
Besides this macro-level of paradigmatic or-
ganisation, Italian subregular verbs also exhibit 
ubiquitous patterns of stem alternations, whereby 
a change in paradigm slot triggers a simultaneous 
change of verb stem and inflectional ending, as 
illustrated in Table 1 for the present indicative ac-
tive. Pirrelli and Battista (2000) show that phe-
nomena of Italian stem alternation, far from being 
accidental inconsistencies of the Italian morpho-
phonology, define stable and strikingly conver-
gent patterns of variable stem formation (Aronoff, 
1994) throughout the entire verb system. The pat-
terns partition subregular Italian verbs into 
equivalence micro-classes. In turn, this can be in-
terpreted as suggesting that inter-class consistency 
plays a role in learning and may have exerted a 
convergent pressure in the history of the Italian 
verb system. If a speaker has heard a verb only in 
ambiguous inflections (i.e. inflections that are in-
dicators of more than one verb micro-class), (s)he 
will need to guess, in order to produce unambigu-
ous forms. Guesses are made on the basis of fre-
quently attested verb micro-classes (Albright, 
2002). 
5 Computer simulations 
The present experiments were carried out using 
the SOM toolbox (Vesanto et al, 2000), devel-
oped at the Neural Networks Research Centre of 
Helsinki University of Technology. The toolbox 
partly forced some standard choices in the training 
protocol, as discussed in more detail in the follow-
ing sections. In particular, we complied with Ko-
honen?s view of SOM training as consisting of 
two successive phases: a) rough training and b) 
fine-tuning. The implications of this view will be 
discussed in more detail later in the paper.  
5.1 Input data 
Our input data are inflected verb forms written 
in standard Italian orthography. Since Italian or-
thography is, with a handful of exceptions, consis-
tently phonological, we expect to replicate the 
same results with phonologically transcribed verb 
forms.  
Forms are incrementally sampled from a train-
ing data set, according to their probability densi-
ties in a free text corpus of about 3 million words. 
Input data cover a fragment of Italian verb inflec-
tion, including, among others, present indicative 
active, future indicative active, infinitive and past 
participle forms, for a total of 10 different inflec-
tions. The average length of training forms is 8.5, 
with a max value of 18.  
Following Plunkett and Marchman (1993), we 
assume than the map is exposed to a gradually 
growing lexicon. At epoch 1, the map learns in-
flected forms of the 5 most frequent verb types. At 
each ensuing epoch, five more verb types are 
added to the training data, according to their rank 
in a list of decreasingly frequent verb types. As an 
overall learning session consists of 100 epochs, 
the map is eventually exposed to a lexicon of 500 
verb types, each seen in ten different inflections. 
Although forms are sampled according to their 
corpus distributions, we hypothesise that the range 
of inflections in which verb tokens are seen by the 
map remains identical across verb types. This is 
done to throw paradigmatic effects in sharper re-
lief and responds to the (admittedly simplistic) as-
sumption that the syntactic patterns forming the 
linguistic input to the child do not vary across 
verb types. 
Each input token is localistically encoded as an 
8*16 matrix of values drawn from the set {1, -1}. 
Column vectors represent characters, and rows 
give the random encoding of each character, en-
suring maximum independence of character vec-
tor representations. The first eight columns in the 
matrix represent the first left-aligned characters of 
the form in question. The remaining eight col-
umns stand for the eight (right-aligned) final char-
acters of the input form.   
 a) b) 
Figure 1. Early self-organisation of a SOM for roots (a) and endings (b) of Italian verbs (epoch 10). 
 
 
a) b) 
Figure 2. Late self-organization of a SOM for roots (a) and endings (b) of Italian verbs (epoch 100).  
 
5.2 Training protocol 
At each training epoch, the map is exposed to a 
total of 3000 input tokens. As the range of 
different inflected forms from which input tokens 
are sampled is fairly limited (especially at early 
epochs), forms are repeatedly shown to the map. 
Following Kohonen (1995), a learning epoch 
consists of two phases. In the first rough training 
phase, the SOM is exposed to the first 1500 
tokens. In this phase, values of ? (the learning 
rate) and neighbourhood kernel radius r are made 
vary as a linear decreasing function of the time 
epoch, from max ? = 0.1 and r = 20 (epoch 1), to 
? = 0.02 and r = 10 (epoch 100). In the second 
fine-tuning phase of each epoch, on the other 
hand, ? is kept to 0.02 and r = 3. 
5.3 Simulation 1: Critical transitions in lexi-
cal organisation 
Figures 1 and 2 contain snapshots of the Italian 
verb map taken at the beginning and the end of 
training (epochs 1 and 100). The snapshots are 
Unified distance matrix (U-matrix, Ultsch and 
Siemon, 1990) representations of the Italian SOM. 
They are used to visualise distances between neu-
rons. In a U-matrix representation, the distance 
between adjacent neurons is calculated and pre-
sented with different colourings between adjacent 
positions on the map. A dark colouring between 
neurons signifies that their corresponding proto-
type vectors are close to each other in the input 
space. Dark colourings thus highlight areas of the 
map whose units react consistently to the same 
stimuli. A light colouring between output units, on 
the other hand, corresponds to a large distance (a 
gap) between their corresponding prototype vec-
tors. In short, dark areas can be viewed as clusters, 
and light areas as chaotically reacting cluster 
separators. This type of pictorial presentation is 
useful when one wants to inspect the state of 
knowledge developed by the map through learn-
ing.  
For each epoch, we took two such snapshots: i) 
one of prototype vector dimensions representing 
the initial part of a verb form (approximately its 
verb root, Figures 1.a and 2.a), and ii) one of pro-
totype vector dimensions representing the verb fi-
nal part (approximately, its inflectional endings, 
Figure 1.b and 2.b). 
5.3.1 Discussion 
Data storage on a Kohonen map is a dynamic 
process whereby i) output units tend to consis-
tently become more reactive to classes of input 
data, and ii) vector prototypes which are adjacent 
in the input space tend to cluster in topologically 
connected subareas of the map. 
Self-organisation is thus an emergent property, 
based on local (both in time and space) principles 
of prototype vector adaptation. At the outset, the 
map is a tabula rasa, i.e. it has no notion whatso-
ever of Italian inflectional morphology. This has 
two implications. First, before training sets in, 
output units are associated with randomly initial-
ised sequences of characters. Secondly, prototype 
vectors are randomly associated with map neu-
rons, so that two contiguous neurons on the map 
may be sensitive to very different stimulus pat-
terns.  
Figure 1 shows that, after the first training ep-
och, the map started by organising memorised in-
put patterns lexically, grouping them around their 
(5) roots. Each root is an attractor of lexically re-
lated stimuli, that nonetheless exhibit fairly het-
erogeneous endings (see Figure 1.b). 
At learning epoch 100, on the other hand, the 
topological organisation of the verb map is the 
mirror image of that at epoch 10 (Figures 2.a and 
2.b). In the course of learning, root attractors are 
gradually replaced by ending attractors. Accord-
ingly, vector prototypes that used to cluster 
around their lexical root appear now to stick to-
gether by morpho-syntactic categories such as 
tense, person and number. One can conceive of 
each connected dark area of map 2.b as a slot in 
an abstract inflectional paradigm, potentially as-
sociated with many forms that share an inflec-
tional ending but differ in their roots.  
 
 
root 
ending 
Figure 3. Average quantization error for an increasing number of input verbs  
 
The main reason for this morphological organisa-
tion to emerge at a late learning stage rests in the 
distribution of training data. At the beginning, the 
map is exposed to a small set of verbs, each of 
which is inflected in 10 different forms. Forms 
with the same ending tend to be fewer than forms 
with the same root. As the verb vocabulary grows 
(say of the order of about 50 different verbs), 
however, the principles of morphological (as op-
posed to lexical) organisation allow for more 
compact and faithful data storage, as reflected by 
a significant reduction in the map average quanti-
zation error (Figure 3). Many different forms can 
be clustered around comparatively few endings, 
and the latter eventually win out as local paradig-
matic attractors.  
Figure 4 (overleaf) is a blow-up of the map area 
associated with infinitive and past participle end-
ings. The map shows the content of the last three 
characters of each prototype vector. Since past 
participle forms occur in free texts more often 
than infinitives, they have a tendency to take a 
proportionally larger area of the map (due to the 
so-called magnification factor). Interestingly 
enough, past participles ending in -ato occupy one 
third of the whole picture, witnessing the promi-
nent role played by regular first conjugation verbs 
in the past participle inflection. 
Another intriguing feature of the map is the way 
the comparatively connected area of the past par-
ticiple is carved out into tightly interconnected 
micro-areas, corresponding to subregular verb 
forms (e.g. corso ?run?, scosso ?shaken? and chie-
sto ?answered?). Rather than lying outside of the 
morpho-phonological realm (as exceptions to the 
?TV + to? default rule), subregular forms of this 
kind seem here to draw the topological borders of 
the past participle domain, thus defining a con-
tinuous chain of morphological family resem-
blances. Finally, by analogy-based continuity, the 
map comes to develop a prototype vector for the 
non existing (but paradigmatically consistent) past 
participle ending -eto.2  This ?spontaneous? over-
generalization is the by-product of graded, over-
lapping morpheme-based memory traces. 
In general, stem frequency may have had a re-
tardatory effect on the critical transition from a 
lexical to a paradigm-based organisation. For the 
same reason, high-frequency forms are eventually 
memorised as whole words, as they can success-
fully counteract the root blurring effect produced 
by the chaotic overlay of past participle forms of 
different verbs, which are eventually attracted to 
the same map area. This turns out to be the case 
for very frequent past participles such as stato 
?been? and fatto ?done?. As a final point, a more 
detailed analysis of memory traces in the past par-
ticiple area of the map is likely to highlight sig-
nificant stem patterns in the subregular micro-
classes. If confirmed, this should provide fresh 
evidence supporting the existence of prototypical 
morphonological stem patterns consistently select-
ing specific subregular endings (Albright, 2002). 
5.4 Simulation 2: Second level map 
A SOM projects n-dimensional data points onto 
grid units of reduced dimensionality (usually 2). 
We can take advantage of this data compression to 
train a new SOM with complex representations 
consisting of the output units of a previously 
trained SOM. The newly trained SOM is a second 
level projection of the original data points.  
To test the consistency of the paradigm-based 
organisation of the map in Figure 2, we trained a 
                                                     
2 While Italian regular 1st  and 3rd conjugation verbs 
present a thematic vowel in their past participle end-
ings (-ato and  -ito respectively), regular 2 conjugation 
past participles (TV -e-) end, somewhat unexpectedly, 
in -uto. 
novel SOM with verb type vectors. Each such 
vector contains all 10 inflected forms of the same 
verb type, encoded through the co-ordinates of 
their best-matching units in the map grid of Figure 
2. The result of the newly trained map is given in 
Figure 5. 
 
 
Figure 4. The past participle and infinitive areas 
 
5.4.1 Discussion  
Figure 5 consistently pictures the three-fold 
macrostructure of the Italian verb system (section 
2) as three main horizontal areas going across the 
map top-to-bottom.  
 
 
Figure 5: A second level map 
 
Besides, we can identify other micro-areas, 
somewhat orthogonal to the main ones.The most 
significant such micro-class (circled by a dotted 
line) contains so-called [g]-inserted verbs (Pirrelli, 
2000; Fanciullo, 1998), whose forms exhibit a 
characteristic [g]/0 stem alternation, as in 
vengo/venite ?I come, you come (plur.)? and 
tengo/tenete ?I have/keep, you have/keep (plur.)?. 
The class straddles the 2nd and 3rd conjugation 
areas, thus pointing to a convergent phenomenon 
affecting a portion of the verb system (the present 
indicative and subjunctive) where the distinction 
between 2nd and 3rd conjugation inflections is 
considerably (but not completely) blurred. All in 
all, Italian verbs appear to fall not only into 
equivalence classes based on the selection of 
inflectional endings (traditional conjugations), but 
also into homogeneous micro-classes reflecting 
processes of variable stem formation. 
Identification of the appropriate micro-class is a 
crucial problem in Italian morphology learning. 
Our map appears to be in a position to tackle it 
reliably. 
Note finally the very particular position of the 
verb stare ?stay? on the grid. Although stare is a 
1st conjugation verb, it selects some 2nd conjuga-
tion endings (e.g. stessimo ?that we stayed (subj.)? 
and stette ?(s)he stayed?). This is captured in the 
map, where the verb is located halfway between 
the 1st and 2nd conjugation areas. 
6 Conclusion and future work 
The paper offered a series of snapshots of the 
dynamic behaviour of a Kohonen map of the men-
tal lexicon taken in different phases of acquisition 
of the Italian verb system. The snapshots consis-
tently portray the emergence of global ordering 
constraints on memory traces of inflected verb 
forms, at different levels of linguistic granularity.  
Our simulations highlight not only morphologi-
cally natural classes of input patterns (reminiscent 
of the hierarchical clustering of perceptron input 
units on the basis of their hidden layer activation 
values) and selective specialisation of neurons and 
prototype vector dimensions in the map, but also 
other non-trivial aspects of memory organisation. 
We observe that the number of neighbouring units 
involved in the memorisation of a specific mor-
phological class is proportional to both type fre-
quency of the class and token frequency of its 
members. Token frequency also affects the en-
trenchment of memory areas devoted to storing 
individual forms, so that highly frequent forms are 
memorised in full, rather than forming part of a 
morphological cluster.  
In our view, the solid neuro-physiological basis 
of SOMs? processing strategies and the consider-
able psycho-linguistic and linguistic evidence in 
favour of global constraints in morphology learn-
ing make the suggested approach an interesting 
medium-scale experimental framework, mediating 
between small-scale neurological structures and 
large-scale linguistic evidence. In the end, it 
would not be surprising if more in-depth computa-
tional analyses of this sort will give strong indica-
tions that associative models of the morphological 
lexicon are compatible with a ?realistic? interpre-
tation of morpheme-based decomposition and ac-
cess of inflected forms in the mental lexicon. Ac-
cording to this view, morphemes appear to play a 
truly active role in lexical indexing, as they ac-
quire an increasingly dominant position as local 
attractors through learning. This may sound trivial 
to the psycholinguistic community. Nonetheless, 
only very few computer simulations of morphol-
ogy learning have so far laid emphasis on the im-
portance of incrementally acquiring structure from 
morphological data (as opposed ? say ? to simply 
memorising more and more input examples) and 
on the role of acquired structure in lexical organi-
sation. Most notably for our present concerns, the 
global ordering constraints imposed by morpho-
logical structure in a SOM are the by-product of 
purely local strategies of memory access, process-
ing and updating, which are entirely compatible 
with associative models of morphological learn-
ing. After all, the learning child is not a linguist 
and it has no privileged perspective on all relevant 
data. It would nonetheless be somewhat reassur-
ing to observe that its generalisations and ordering 
constraints come very close to a linguist?s ontol-
ogy. 
The present work also shows some possible 
limitations of classical SOM architectures. The 
propensity of SOMs to fully memorise input data 
only at late learning stages (in the fine-tuning 
phase) is not fully justified in our context. Like-
wise, the hypothesis of a two-staged learning 
process, marked by a sharp discontinuity at the 
level of kernel radius length, has little psycholin-
guistic support. Furthermore, multiple classifica-
tions are only minimally supported by SOMs. As 
we saw, a paradigm-based organisation actually 
replaces the original lexical structure. This is not 
entirely desirable when we deal with complex 
language tasks. In order to tackle these potential 
problems, the following changes are currently be-
ing implemented:  
 
? endogenous modification of radius length as 
a function of the local distance between the 
best matching prototype vector and the cur-
rent stimulus; the smaller the distance the 
smaller the effect of adaptive updating on 
neighbouring vectors  
? adaptive vector-distance function; as a neu-
ron becomes more sensitive to an input pat-
tern, it also develops a sensitivity to specific 
input dimensions; differential sensitivity, 
however, is presently not taken into account 
when measuring the distance between two 
vectors; we suggest weighting vector di-
mensions, so that distances on some dimen-
sions are valued higher than distances on 
other dimensions   
?  ?self-feeding? SOMs for multiple classifi-
cation tasks; when an incoming stimulus 
has been matched by the winner unit only 
partially, the non matching part of the same 
stimulus is fed back to the map; this is in-
tended to allow ?recognition? of more than 
one morpheme within the same input form    
? more natural input representations, address-
ing the issue of time and space-invariant 
features in character sequences. 
References  
Albright, Adam. 2002. Islands of reliability for regular 
morphology: Evidence from Italian. Language, 
78:684-709. 
Aronoff, Mark. 1994. Morphology by Itself. M.I.T. 
Press, Cambridge, USA. 
Baayen, Harald, Ton Dijkstra and Robert Schreuder. 
1997. Singulars and Plurals in Dutch: Evidence for a 
Parallel Dual Route Model. Journal of Memory and 
Language, 36:94-117. 
Baroni, Marco. 2000. Distributional cues in morpheme 
discovery: A computational model and empirical 
evidence. Ph.D. dissertation, UCLA. 
Bosch van den, Antal, Walter Daelemans, Ton Wei-
jters. 1996. Morphological Analysis as Classifica-
tion: an Inductive-learning approach. In Proceedings 
of NEMLAP II , K. Oflazer and H. Somers, eds., 
pages 79-89, Ankara. 
Bybee, Joan. 1995. Regular Morphology and the Lexi-
con. Language and Cognitive Processes, 10 (5): 
425-455. 
Bybee, Joan and Dan I. Slobin. 1982. Rules and Sche-
mas in the Development and Use of the English Past 
Tense. Language, 58:265-289. 
Bybee, Joan and Carol Lynn Moder. 1983. Morpholo-
gical Classes as Natural Categories. Language, 
59:251-270. 
De Mauro, Tullio, Federico Mancini, Massimo Vedo-
velli and Miriam Voghera. 1993. Lessico di frequen-
za dell'italiano parlato. Etas Libri, Milan. 
Fanciullo, Franco. 1998. Per una interpretazione dei 
verbi italiani a ?inserto? velare. Archivio Glottologi-
co Italiano, LXXXIII(II):188-239. 
Gaussier, Eric. 1999. Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In 
Proceedings of the Workshop on Unsupervised 
Learning in Natural Language Processing, pages 
24-30, University of Maryland.  
Goldsmith, John. 2001. Unsupervised Learning of the 
Morphology of a Natural Language. Computational 
Linguistics, 27(2):153-198. 
Kohonen, Teuvo. 1995. Self-Organizing Maps. 
Springer, Berlin. 
Ling, Charles X. and Marin Marinov. 1993. Answering 
the Connectionist Challenge: a Symbolic Model of 
Learning the Past Tense of English Verbs. Cogni-
tion, 49(3):235-290. 
Orsolini, Margherita and William Marslen-Wilson.  
1997. Universals in Morphological Representations: 
Evidence from Italian. Language and Cognitive 
Processes, 12(1):1-47. 
Orsolini, Margherita, Rachele Fanari and Hugo Bo-
wles. 1998. Acquiring regular and irregular inflec-
tion in a language with verbal classes. Language 
and Cognitive Processes, 13(4):452-464. 
Pirrelli, Vito. 2000. Paradigmi in Morfologia. Istituti 
Editoriali e Poligrafici Internazionali, Pisa. 
Pirrelli, Vito and Federici Stefano. 1994. "Deriva-
tional" Paradigms in Morphonology. In Proceedings 
of Coling 94, pages 234-240,  Kyoto. 
Pirrelli, Vito and Fran?ois Yvon. 1999. The hidden di-
mension: a paradigmatic view of data driven NLP. 
Journal of Experimental and Theoretical Artificial 
Intelligence, 11:391-408. 
Pirrelli, Vito and Marco Battista. 2000. The Paradig-
matic Dimension of Stem Allomorphy in Italian In-
flection. Italian Journal of Linguistics, 12(2):307-
380. 
Plaut, David C., James L. McClelland, Mark S. Sei-
denberg and Karalyn Patterson. 1996. Understand-
ing Normal and Impaired Word Reading: Computa-
tional Principles in Quasi-regular Domains. Psycho-
logical Review , 103:56-115. 
Plunkett, Kim and Virginia Marchman. 1993. From 
rote learning to system building: Acquiring verb 
morphology in children and connectionist nets. 
Cognition, 48:21-69. 
Prasada, Sandeep and Steven Pinker. 1993. Generaliza-
tions of regular and irregular morphology. Language 
and Cognitive Processes, 8:1-56. 
Rueckl, Jay G. and Michal Raveh. 1999. The Influence 
of Morphological Regularities on the Dynamics of 
Connectionist Networks. Brain and Language, 
68:110-117. 
Say, Tessa and Harald Clahsen. 2001. Words, Rules 
and Stems in the Italian Mental Lexicon. In ?Storage 
and computation in the language faculty?, S. Noote-
boom, F. Weerman and  F. Wijnen, eds., pages 75-
108, Kluwer Academic Publishers, Dordrecht. 
Serianni, Luca. 1988. Grammatica italiana: italiano 
comune e lingua letteraria. UTET, Turin. 
Stemberger, Joseph P. and Andrew Carstairs. 1988. A 
Processing Constraint on Inflectional Homonymy. 
Linguistics, 26:601-61. 
Taft, Marcus. 1988. A morphological-decomposition 
model of lexical representation. Linguistics, 26:657-
667. 
Ultsch, Alfred and H. Peter Siemon. 1990. Kohonen's 
Self-Organizing Feature Maps for Exploratory Data 
Analysis. In ?Proceedings of INNC'90. International 
Neural Network Conference1990?, pages 305-308, 
Dordrecht 
Vesanto, Juha, Johan Himberg, Esa Alhoniemi, and 
Juha Parhankangas. 2000. SOM Toolbox for Matlab 
5 . Report A57, Helsinki University of Technology, 
Neural Networks Research Centre, Espoo, Finland. 
 
 
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 33?37,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
PHACTS about activation-based word similarity effects
Basilio Calderone
CLLE-ERSS (UMR 5263) CNRS &
Universit? de Toulouse-Le Mirail
31058 Toulouse Cedex 9, France
basilio.calderone@univ-tlse2.fr
Chiara Celata
Scuola Normale Superiore
Laboratorio di Linguistica
56126 Pisa, Italy
c.celata@sns.it
Abstract
English phonotactic learning is modeled by
means of the PHACTS algorithm, a topo-
logical neuronal receptive field implement-
ing a phonotactic activation function aimed
at capturing both local (i.e., phonemic) and
global (i.e., word-level) similarities among
strings. Limits and merits of the model are
presented.
1 Introduction
Categorical rules and probabilistic constraints of
phonotactic grammar affect speakers? intuitions
about the acceptability of word-level units in a
number of experimental tasks, including con-
tinuous speech segmentation and word similar-
ity judgment. Several sources of information
contribute to phonotactic generalization, includ-
ing sub-segmental properties, segment transition
probabilities, lexical neighborhood effects; all
these factors have been independently or jointly
modeled in several recent accounts of phonotac-
tics and phonotactic learning (Coady and Aslin,
2004; Vitevitch, 2003; Vitevitch and Luce, 2005;
Hayes and Wilson, 2008; Albright, 2009; Coet-
zee, 2009).
In this study, we explore the word level phono-
tactics in terms of a function of ?phonotactic ac-
tivation? within a PHACTS environment (Celata
et al, 2011). PHACTS is a topological neu-
ronal receptive field implementing an n-gram
sampling estimate of the frequency distribution of
phonemes and a sub- lexical chunking of recur-
rent sequences of phonemes. Once this phono-
tactic knowledge has been developed, the model
generalizes it to novel stimuli to derive activation-
based representations of full lexical forms, thus
mirroring the contribution of lexical neighbor-
hood effects. Then the similarity values for pairs
of words and non-words can be calculated.
2 PHACTS: the model
PHACTS (for PHonotactic ACTivation System) is
based on the principles of a Self-Organizing Map
(SOM) (Kohonen, 2000), an associative memory
algorithm which realizes low-dimensional (gener-
ally, bi-dimensional) representations of a multidi-
mensional input space.
PHACTS simulates the formation of phonotactic
knowledge in the mind of a speaker, who is ex-
posed to a stream of phonological words and grad-
ually develops a mental representation of the sta-
tistical regularities shaping the phonotactics of a
given language. The model also performs lexi-
cal generalizations on the basis of the phonotactic
knowledge developed in the training phase.
The physical structure of PHACTS is defined
by a set S (with finite cardinality) of neurons njk
with 1 ? j ? J and 1 ? k ? K arranged in
a bi-dimensional grid of S = {n11, n12, . . . n},
?S? = JK. Each neuron in the grid corresponds
to a vector (the so-called prototype vector) whose
dimension is equal to the dimension of the input
data vector. At the beginning of the learning pro-
cess, the prototype vectors assume random values
while, as learning progresses, they change their
values to fit the input data.
PHACTS works according to the two follow-
ing phases: i) the training phase, where language-
specific phonotactic knowledge is acquired; ii) the
lexical generalization phase.
33
2.1 Training phase: the acquisition of
phonotactic knowledge
At the beginning, each input word iteratively hits
the system. For any iteration, the algorithm
searches for the best matching unit (BMU), that
is, the neuron which is topologically the closest to
the input vector i and which is a good candidate
to represent the input data through the prototype
vector. The search for the BMU is given by maxi-
mizing the dot product of i and ujk in the t-th step
of the iteration:
BMU((i)t) = arg max
jk
(i(t) ? ujk) (1)
In other terms, the BMU((i)t) is the best aligned
prototype vector with respect to the input i. Af-
ter the BMU is selected for each i at time t,
PHACTS adapts the prototype vector ujk to the
current input according to the topological adapta-
tion equation given in (2):
?ujk(t) = ?(t)?(t)[i(t)? ujk(t? 1)] (2)
where ?(t) is a learning rate and ?(t) is the so-
called neighborhood function. The neighborhood
function is a function of time and distance be-
tween the BMU and each of its neighbors on the
bi-dimensional map. It defines a set of neurons
around the that would receive training, while neu-
rons outside this set would not be changed. In our
model the neighborhood function is defined as a
Gaussian function.
The ? parameter controls for the elasticity of
the network, and ? roughly controls for the area
around each best matching where the neurons are
modified. The initial value of both parameters is
set heuristically and in general decreases as long
as the learning progresses. In order to facilitate a
training convergence, we set ? ? 0 and ? ? 0
as t ? 0. PHACTS performs a vector map-
ping of the data space in input to the output space
defined by the prototype vectors ujk on the bi-
dimensional grid of neurons S.
2.1.1 The data: Type and token frequency in
PHACTS
For the present simulations, PHACTS was
trained on a portion of the CELEX English
database (Baayen et al, 1995), and specifically
on 8266 English word types phonologically tran-
scribed and provided with their frequency of oc-
currence (only the words with token frequency
> 100 were selected). Each phoneme was phono-
logically encoded according to a binary vector
specifying place, manner of articulation and voic-
ing for consonants, roundedness, height and ante-
riority for vowels. The bi-dimensional map was
25 X 35 neurons, and thus S = 875. Input words
were sampled according to i for PHACTS is con-
stituted by the input training words with a n-gram
sampling window (with n spanning up the length
of the longest word).
During the training phase, the map takes into
account the global distribution of the n-grams
in order to realize the topological activations
of the phonotactic patterns (?phonotactic activa-
tion?). Both token frequency (i.e., the number
of occurrences of specific n-grams) and type fre-
quency (i.e., the number of all members of an
n-gram type as defined by phonological features
shared; for instance, /tan/ and /dim/ are two re-
alizations of the trigram type stop+vowel+nasal)
play a key role in phonotactic activation. By
virtue of being repeatedly inputted to the map, a
high token frequency n-gram will exhibit high ac-
tivation state in the map. Low token frequency
n-grams, however, will exhibit activation on the
SOM only if they share phonological material
(namely, phonemes or features) with high token
frequency n-grams. Type frequency generates
entrenchment effects in the map; high type fre-
quency n-grams will occupy adjacent positions
on the bi-dimensional map, thus defining clear
phonotactic clusters. For these reasons, PHACTS
differ sharply from current models of phonotac-
tic learning, where only type frequencies are as-
sumed to play a role in phonotactic generalization
(and formalized accordingly). (Albright, 2009)
2.2 N-gram generalization and lexical
generalizations
Once PHACTS has been exposed to an input of
phonologically-encoded n-grams , an activation-
based representation of unseen words can be
derived. This phase implements a linear thresh-
olded function d in which each neuron T?firesT?
as a function of its activation with respect to
the (unseen) n-grams. In this sense each neuron
acts as a ?transfer function?T? of an activation
weight depending on the alignment between
the unseen n-gram vector and the best aligned
n-gram prototype vector.
34
Lexical generalization in PHACTS is therefore
a word-level transfer process whereby the activa-
tion values of each word n-gram are summed ac-
cording to equation [4]:
FPHACTS(x) =
?
jk
?(x) (3)
The cumulative action of n-gram activations re-
alizes a distributed representation of the word in
which both phonological similarity (at the string
level), and token frequency effects for phonotac-
tic patterns are taken into account.
Being based on an associative memory learn-
ing of phonological words inputted by a n-gram
sampling window, PHACTS develops topolog-
ical cumulative memory traces of the learned
words in which phonotactic activations emerge
as the results of repeated mnemonic superim-
positions of n-grams. This aspect is crucial
for a distributional analysis of the morphotactic
salience in a given language. In this direction,
PHACTS was successfully implemented in the
modeling of the micro- and macro-phonotactics
in Italian (Calderone and Celata, 2010). By
micro-phonotactics we mean sequential informa-
tion among segments (e.g., the fact that, in the
specific language, a phonological sequence, such
as /ato/, differs from similar sequences, such as
/uto/, /rto/, and /atu/ ). By macro-phonotactics we
mean positional information within the word, i.e.,
sub-lexical (or chunk) effects (e.g., the fact that
word-initial /#ato/ is different from word-medial
/-ato-/, as well as from word-final /ato#/ ). In En-
glish language as well, PHACTS seems to distri-
butionally distinguish a positional relevance for
highly attested phonological sequences such as
/ing/. Figure 1 reports the phonotactic activation
states outputted for the sequence /ing/ in initial
and final word position (training corpus and pa-
rameters described in 2.1.1).
3 The experiments
According to the literature, the speakers in judg-
ing the wordlikeness of isolated non-words rely
mainly on a grammar-based phonotactic knowl-
edge and enhance the correspondence among
types of strings (e.g., segmental features and onset
and coda constituency). In doing so, they estab-
lish connections between each non-word and the
#ing-
-ing#
Figure 1: Phonotactic activation states for the se-
quence #ing- (initial word position) and -ing# (final
word position)
neighborhood of all attested and unattested (but
phonotactically legal, i.e., potentially attested)
strings of their language. This must be a com-
putationally hard task to accomplish even when
no time restrictions are imposed, as in traditional
wordlikeness experiments (since (Scholes, 1966)
onward). In this experiment, we want to verify
whether such task can be modeled in PHACTS
and whether the vector representation of words
outputted by PHACTS may represent a solid basis
for this type of phonotactic evaluation. To evalu-
ate PHACTS?s ability to reproduce the typicality
patterns produced by the speakers in judging the
?Englishness? of isolated strings, we had to derive
a similarity value among each string and some
counterpart in the English lexicon, as explained
with more details below. We used 150 non-words,
which were randomly selected from the list of
272 non-words of Bailey and Hahn (2001, B &
H henceforth).
In that study, pronounceable non-words were
created, either 4- or 5-phoneme long, differing
from their nearest real word neighbor by either
one or two phonemes (in terms of substitution,
addition or subtraction). In the former case they
were called near misses, in the latter case they
were called isolates. 22 isolates and 250 near
misses around the isolates were used in the B
& H?s study; 24 English speakers were asked
to judge the ?Englishness? of the non-words that
were individually presented in their orthographic
and auditory form. The 150 non-words used in
the present experiment were selected from among
the near misses only. PHACTS was asked to de-
rive the cosine value between the vector represen-
tations of each non- word and the corresponding
real English words composing its neighbor fam-
ily (according to the lists provided in B & H).
The total number of string pairs was 1650 (the
average number of neighbors for each non-word
35
being 11). Then, an average cosine value was
calculated for each of the 150 non-words. The
average cosine value was assumed to reflect the
phonotactic acceptability of each non-word with
respect to their real word neighbors and therefore,
to approximate the speakers? typicality judgment
of isolated non-words. An edit distance calcula-
tion (normalized by the length of the two strings)
was performed for the same 1650 pairs of non-
words. Since the neighbors were all selected by
adding, subtracting or modifying one phoneme
from their reference non-words, the edit distance
values were expected not to vary to a large ex-
tent. In the edit distance algorithm, values range
from 0 to 1 according to the degree of the sim-
ilarity between the two strings As expected, the
distribution of the edit distance values was not
uniform and the 1650 string pairs elicited a very
small range of edit distance values. In total, 96%
of cases elicited only four different edit distance
values (namely, 0.83, 0.87, 0.93 and 0.97); the re-
maining 4% elicited three different values which
were all higher than 0.7.
The cosine values outputted by PHACTS for
the same string pairs were evaluated with respect
to the calculated edit distances. As in the case
of the edit distance algorithm, cosine values close
to 1 indicate high similarity while values close
to 0 indicate low similarity. As in the case of
the edit distances, the cosine values were asym-
metrically distributed, highly skewed to the right
(for high similarity values). The global range of
the distribution of values was similar for the two
algorithms (spanning from 0.7 to 0.99). How-
ever, compared to the sharpness of the edit dis-
tance results (see Figure 2), PHACTS?s output
included subtler variations across comparisons,
with fine distinctions distributed over a continu-
ous range of values. The edit distance and the
cosine values turned out to be correlated with
r = 0.465. Although the nature of the differ-
ence between PHACTS?s output and the edit dis-
tance algorithm should be better evaluated with
respect to a more varied data set, also including
pairs of very dissimilar strings, we could prelimi-
narily conclude that the cosine value calculated by
PHACTS for pairs of activation-based string rep-
resentations did not correspond to an edit distance
calculation.
We further verified whether PHACTS cosine
values could approximate the perceived phonotac-
0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 10.65
0.7
0.75
0.8
0.85
0.9
0.95
1
PHACTS
(NOR
MAL
IZED
) ED
IT DI
STAN
CE
 
 
0.840.860.880.90.920.940.960.9811.02 0
20
40
60
80
100
120
140
160
0.7
0.75
0.8
0.85
0.9
0.95
1
0100200300400500600
Figure 2: Correlation scatterplot and distribution his-
tograms of the edit distance and PHACTS values for
the B & H?s materials
tic distance between two strings, as it is calculated
by the speaker when (s)he is asked to judge the
phonotactic acceptability of an isolated non-word.
To test this hypothesis, the average cosine value
of each non-word was correlated with the corre-
sponding acceptability rating produced by the En-
glish subjects in the B & H?s work. The Spear-
man?s rank correlation between speakers? ratings
and the (exp-transformed) cosine values was ? =
.216, p < .01. Although statistically significant,
the correlation coefficient was rather low and re-
vealed that the observed and simulated behaviors
overlapped only to a limited extent. In particu-
lar, PHACTS did not reach a span of phonotactic
acceptability as large as the speakers appeared to
produce (with ratings comprised between 2.1 and
6.5).
In conclusion, PHACTS-based word similar-
ity calculation appeared not to produce a reliable
ranking of strings according to their phonotactic
wellformedness. On the other hand, it did pro-
duce a fine-grained distributed representation of
word in which both phonological similarity and
token frequency effects for full forms seemed to
define phonotactic activations of highly attested
phonological sequences. This kind of representa-
tion differed from raw calculations of the number
of operations required to transform a string into
another.
Experimental protocols for modeling word simi-
larity in PHACTS are currently under investiga-
tion.
36
References
Adam Albright. 2009. Feature-based generalisation
as a source of gradient acceptability. Phonology,
26(1):9?41.
Harald R. Baayen, Richard Piepenbrock, and Leon
Gulikers. 1995. The celex lexical database. release
2 (cd-rom). Philadelphia: Linguistic Data Consor-
tium, University of Philadelphia: Linguistic Data
Consortium, University of Pennsylvania.
Basilio Calderone and Chiara Celata. 2010.
The morphological impact of micro- and macro-
phonotactics. computational and behavioral analy-
sis (talk given). In 14th International Morphology
Meeting, Budapest, 13-16 May.
Chiara Celata, Basilio Calderone, and Fabio Mon-
termini. 2011. Enriched sublexical representa-
tions to access morphological structures. a psycho-
computational account. TAL-Traitement Automa-
tique du Langage, 2(52):123?149.
Jeffry A. Coady and Richard N. Aslin. 2004. Young
children?s sensitivity to probabilistic phonotactics
in the developing lexicon. Journal of Experimen-
tal Child Psychology, 89:183?213.
Andries W. Coetzee. 2009. Grammar is both categor-
ical and gradient. In S. Parker, editor, Phonologi-
cal Argumentation: Essays on Evidence and Moti-
vation. Equinox.
Bruce Hayes and Colin Wilson. 2008. A maxi-
mum entropy model of phonotactics and phonotac-
tic learning. Linguistic Inquiry, 39(3):379?440.
Teuvo Kohonen. 2000. Self-Organizing Maps.
Springer, Heidelberg.
Robert J. Scholes. 1966. Phonotactic Grammatical-
ity. Mouton.
Michael S. Vitevitch and Paul A. Luce. 2005. In-
creases in phonotactic probability facilitate spoken
nonword repetition. Journal of Memory and Lan-
guage, 52(2):193?204.
Michael S. Vitevitch. 2003. The influence of sub-
lexical and lexical representations on the processing
of spoken words in english. Clinical Linguistics &
Phonetics, 17:487?499.
37
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 65?74,
Coling 2014, Dublin, Ireland, August 24 2014.
Acquisition and enrichment of morphological and morphosemantic
knowledge from the French Wiktionary
Nabil Hathout Franck Sajous Basilio Calderone
CLLE-ERSS (CNRS & Universit?e de Toulouse 2)
Abstract
We present two approaches to automatically acquire morphologically related words from Wik-
tionary. Starting with related words explicitly mentioned in the dictionary, we propose a method
based on orthographic similarity to detect new derived words from the entries? definitions with
an overall accuracy of 93.5%. Using word pairs from the initial lexicon as patterns of formal
analogies to filter new derived words enables us to rise the accuracy up to 99%, while extending
the lexicon?s size by 56%. In a last experiment, we show that it is possible to semantically type
the morphological definitions, focusing on the detection of process nominals.
1 Introduction
Around the 1980s the computational exploitation of machine-readable dictionaries (MRDs) for the au-
tomatic acquisition of lexical and semantic information enjoyed a great favor in NLP (Calzolari et al.,
1973; Chodorow et al., 1985). MRDs? definitions provided robust and structured knowledge from which
semantic relations were automatically extracted for linguistic studies (Markowitz et al., 1986) and lin-
guistic resources development (Calzolari, 1988). Today the scenario has changed as corpora have become
the main source for semantic knowledge acquisition. However, dictionaries are regaining some interest
thanks to the availability of public domain dictionaries, especially Wiktionary.
In the present work, we describe a method to create a morphosemantic and morphological French
lexicon from Wiktionary?s definitions. This type of large coverage resource is not available for almost
all languages, with the exception of the CELEX database (Baayen et al., 1995) for English, German and
Dutch, a paid resource distributed by the LDC.
The paper is organized as follows. Section 2 reports related work on semantic and morphological
acquisition from MRDs. In Section 3, we describe how we converted Wiktionnaire, the French language
edition of Wiktionary, into a structured XML-tagged MRD which contains, among other things, defini-
tions and morphological relations. In Section 4, we explain how we used Wiktionnaire?s morphological
sections to create a lexicon of morphologically related words. The notion of morphological definitions
and their automatic identification are introduced in Section 5. In Section 6, we show how these defini-
tions enable us to acquire new derived words and enrich the initial lexicon. Finally, Section 7 describes
an experiment where we semantically typed process nouns definitions.
2 Related work
Semantic relations are usually acquired using corpora (Curran and Moens, 2002; van der Plas and Bouma,
2005; Heylen et al., 2008) but may also be acquired from MRDs. MRDs-based approaches are bound
to the availability of such resources. However, for some languages including French, no such resource
exists. Recent years have seen the development of large resources built automatically by aggregating
and/or translating data originating from different sources. For example, Sagot and Fi?ser (2008) have
built WOLF, ?a free French Wordnet? and Navigli and Ponzetto (2010) BabelNet, a large multilingual
semantic network. Such resources tend to favor coverage over reliability and may contain errors and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
65
inaccuracy, or be incomplete. Pierrel (2013), while criticizing these resources, describes the digitization
process of the Tr?esor de la Langue Franc?aise, a large printed French dictionary. The first impulse of this
long-course reverse-engineering project is described in (Dendien, 1994) and resulted in the TLFi, a fine-
grained XML-structured dictionary. Pierrel advocates mutualization, recommends resources sharing and
underlines how the use of the TLFi would be relevant for NLP. Though we totally agree on this assertion,
we deplore that the resource, being only available for manual use and not for download, prevents its use
for NLP.
Crowdsourcing has recently renewed the field of lexical resources development. For example Lafour-
cade (2007) designed JeuxDeMots, a game with a purpose, to collect a great number of relations between
words. Other works use the content of wikis produced by crowds of contributors. Initially in the shadow
of Wikipedia, the use of Wiktionary tends to grow in NLP studies since its exploitation by Zesch et al.
(2008). Its potential as an electronic lexicon was first studied by Navarro et al. (2009) for English and
French. The authors leverage the dictionary to build a synonymy network and perform random walks to
find missing links. Other works tackled data extraction: Anton P?erez et al. (2011) for instance, describe
the integration of the Portuguese Wiktionary and Onto.PT; S?erasset (2012) built Dbnary, a multilingual
network containing ?easily extractable? entries. If the assessment of Wiktionary?s quality from a lex-
icographic point of view has not been done yet, Zesch and Gurevych (2010) have shown that lexical
resources built by crowds lead to results comparable to those obtained with resources designed by pro-
fessionals, when used to compute semantic relatedness of words. In Sajous et al. (2013a), we created an
inflectional and phonological lexicon from Wiktionary and showed that its quality is comparable to those
of reference lexicons, while the coverage is much wider.
Comparatively little effort has been reported in literature on the exploitation of semantic relations to
automatically identify morphological relations. Schone and Jurafsky (2000) learn morphology with a
method based on semantic similarity extracted by latent semantic analysis. Baroni et al. (2002) combine
orthographic (string edit distances) and semantic similarity (words? contextual information) in order to
discover morphologically related words. Along the same line, Zweigenbaum and Grabar (2003) ac-
quire semantic information from a medical corpus and use it to detect morphologically derived words.
More recently, Hathout (2008) uses the TLFi to discover morphologically related words by combining
orthographic and semantic similarity with formal analogy.
I another work, Pentheroudakis and Vanderwende (1993) present a method to automatically extract
morphological relations from the definitions of MRDs. The authors automatically identify classes of
morphologically related words by comparing the semantic information in the entry of the derivative
with the information stored in the candidate base form. This effort shows the crucial importance and the
potential of the MRDs? definitions to acquire and discover morphological relationships of derived words.
3 Turning the French Wiktionary into a Machine-Readable Dictionary
As mentioned is section 2, the quality of collaboratively constructed resources has already been assessed
and we will not debate further the legitimacy of leveraging crowdsourced data for NLP purpose. We give
below a brief description of Wiktionary
1
and of the process of converting it into a structured resource.
Wiktionary is divided in language editions. Each language edition is regularly released as a so-called
XML dump.
2
The ?XML? mention is somewhat misleading because it suggests that XML markups
encode the articles? microstructure whereas only the macrostructure (articles? boundaries and titles) is
marked by XML tags. Remaining information is encoded in wikicode, an underspecified format used by
the MediaWiki content-management system. As explained by Sajous et al. (2013b) and S?erasset (2012),
this loose encoding format makes it difficult to extract consistent data. One can choose to either restrict
the extraction to prototypical articles or design a fine-grained parser that collects the maximum of the
available information. The former goal is relatively easily feasible but leads to a resource containing only
a small subset of Wiktionary?s entries. Our belief is that the tedious engineering work of handling all
1
For further details, read Zesch et al. (2008) and Sajous et al. (2013b).
2
The dump used in this work is https://dumps.wikimedia.org/frwiktionary/20140226/
frwiktionary-20140226-pages-articles.xml.bz2
66
== {{langue|fr}} ==
=== {{S|nom|fr}} ===
{{fr-r?eg|kurs}}
???course??? {{pron|kurs|fr}} {{f}}
# [[action|Action]] de [[courir]], [[mouvement]] de celui qui [[court]].
#
*
??[...], il n?est de bruit qu?un ver qui taraude incessamment les boiseries et dans le plafond,
la ???course??? d?un rongeur.?? {{source|{{w|Jean Rogissart}}, ??Passantes d?Octobre??, 1958}}
# {{sport|nocat=1}} Toute [[?epreuve]] [[sportif|sportive]] o`u la [[vitesse]] est en jeu.
#
*
??Nos p`eres ?etaient donc plus sages que nous lorsqu?ils repoussaient l?id?ee des ???courses???.
# {{vieilli|fr}} [[actes|Actes]] d?[[hostilit?e]] que l?on faisait [[courir|en courant]] les mers
ou [[entrer|en entrant]] dans le [[pays]] [[ennemi]].
{{usage}} On dit maintenant [[incursion]], [[reconnaissance]], [[pointe]], etc.
#
*
??Pendant les guerres de la r?evolution, Chausey, trop expos?e aux ???courses??? des corsaires
de Jersey, resta inhabit?e.??
# {{figur?e|fr}} [[marche|Marche]], [[progr`es]] [[rapide]] d?une personne ou d?une chose.
#
*
??Rien ne peut arr?eter ce conqu?erant, ce fl?eau dans sa ???course???.??
==== {{S|d?eriv?es}} ====
*
[[courser]]
*
[[coursier]]
Figure 1: Wikicode extract of the noun course
wikicode particularities is valuable. In our case, it enabled us to design an unprecedented large copylefted
lexicon that has no equivalent for French.
The basic unit of Wiktionary?s articles is the word form: several words from different languages having
the same word form occur in the same page (at the same URL). In such a page, a given language section
may be divided in several parts of speech which may in turn split into several homonyms subsections.
In the French Wiktionary, the course entry, for example, describes both the French and English lexemes.
The French section splits into a noun section (une course ?a run; a race?) and a section related to the
inflected forms of the verb courser ?to pursue?. The noun section distinguishes 11 senses that all have
definitions illustrated by examples. An extract of the noun section?s wikicode is depicted in Figure 1.
As can be seen, some wiki conventions are recurrent (e.g. double-brackets mark hyperlinks) and are
easy to handle. Handling dynamic templates (marked by curly brackets) is more tricky. In definitions,
they mark notes related to particular domains, registers, usages, geographic areas, languages, etc. In
Figure 1, the pattern {{sport}} indicates that the second sense relates to the domain of sport ; the pattern
{{vieilli|fr}} in the following definition denotes a dated usage ; the pattern {{figur?e|fr}} in the last
definition indicates a figurative one. We inventoried about 6,000 such templates and their aliases: for
example, 4 patterns (abbreviated or full form, with or without ligature) signal the domain of enology:
{{?nologie|fr}}, {{oenologie|fr}}, {{?nol|fr}} and {{oenol|fr}}. Unfortunately, the existence of
such patterns does not prevent a contributor to directly write domain name in the page: several versions
of ?hardcoded domains? may be found, e.g. (oenologie) or (?nologie).
Inventorying all these variations enabled us: 1) to remove them from the definitions? text and 2) to
mark them in a formal way. Thus, one can decide to remove or keep, on demand, entries that are marked
as rare or dated, build a sublexicon of a given domain, remove diatopic variations or investigate only
these forms (e.g. words that are used only in Quebec), etc.
The variations observed in the definitions also occur in phonemic transcriptions, inflectional features,
semantic relations, etc. We focus here only on the information used in sections 6 and 7: definitions
and morphological relations. However, we parsed Wiktionnaire?s full content and extracted all kind of
available information, handling the numerous variations that we observed to convert the online dictio-
nary into a structured resource, that we called GLAWI.
3
It contains more than 1.4 million inflected forms
(about 190,000 lemmas) with their definitions, examples, lexicosemantic relations and translations, de-
rived terms and phonemic transcriptions. A shortened extract resulting from the conversion of the noun
section of course is depicted in Figure 2. As can be seen, GLAWI includes both XML structured data
and the initial corresponding wikicode. This version of the resource is intended to remain close to the
Wiktionnaire?s content, whereas other lexicons focused on a particular aspect will be released. Our aim
is to provide ready-to-use lexicons resulting from different post-processing of GLAWI. Post-processing
3
Resulting from the unification of GL
`
AFF and an updated version of WiktionaryX, GLAWI stands for ?GL
`
AFF and Wik-
tionaryX?. This resource is freely available at http://redac.univ-tlse2.fr/lexicons/glawi.html.
67
Figure 2: Extract of the noun subsection of course converted into a workable format
steps will consist in 1) selecting information relevant to a particular need (e.g. phonemic transcriptions,
semantic relations, etc.) and 2) detecting inconsistencies and correcting them. The initial GLAWI re-
source, containing all the initial information, will also be released so that anyone can apply additional
post-processings. GLAWI unburdens such users from the efforts of parsing the wikicode.
Articles from Wiktionnaire may contain morphologically derived terms. Figures 1 and 2 show that
course produces the derived verb courser and noun coursier ?courier?. Such derivational relations are
collected from Wiktionnaire and included in GLAWI. We show below how we leverage this information,
in addition to GLAWI?s definitions, to acquire morphological and morphosemantic knowledge.
4 Acquisition of morphological relations from GLAWI morphological subsections
We first extracted from GLAWI the list of the lexeme headwords that have typographically simple writ-
ten forms (only letters) and that belong to the major POS: noun, verb, adjective, and adverb. This list
(GLAWI-HW) contains 152,567 entries: 79,961 nouns, 22,646 verbs, 47,181 adjective and 2,779 ad-
verbs). In what follows, we only consider these words.
Then we created a morphological lexicon extracted from the morphological subsections
4
of GLAWI
(hereafter GMS). The lexicon consists of all pairs of words (w
1
, w
2
), where w
1
and w
2
belong to
GLAWI-HW and where w
2
is listed in one of the morphological subsections of the article of w
1
or
vice versa. GMS contains 97,058 pairs. The extraction of this lexicon from GLAWI was very simple, all
the variability in Wiktionnaire?s lexicographic descriptions being supported by our parser (see Section 3).
The remainder of the paper presents two methods for extending GMS. In a first experiment, we com-
plement this lexicon with new pairs acquired from GLAWI?s definitions. In a second one, we show how
some of GMS?s morphological pairs can be classified with respect to a given semantic class.
4
The morphological subsections appear under 4 headings in Wiktionnaire: apparent?es; apparent?es ?etymologiques; com-
pos?es; d?eriv?es.
68
w1
w
2
w
1
w
2
bisannuel A an N r?epublicain N r?epublique N
compilation N compilateur A similaire A dissimilitude N
foudroyeur A foudre N tabasser V tabassage N
militance N militer V taxidermie N taxidermiser V
presse N pression N volcan N volcanique A
Figure 3: Excerpt of GMS lexicon. Letters following the underscore indicate the grammatical category.
5 Morphological definitions
Basically, a dictionary definition is a pair composed of a word and a gloss of its meaning. In the follow-
ing, we will use the terms definiendum for the defined word, definiens for the defining gloss and the
notation definiendum = definiens. The definition articulates a number of lexical semantic relations be-
tween the definiendum and some words of the definiens as in (1) where chair is a hyponym of furniture,
is the holonym of seat, legs, back and arm rests and is also the typical instrument of sit on. Some of the
relations are made explicit by lexical markers as used to or comprising.
(1) chair
N
= An item of furniture used to sit on or in comprising a seat, legs, back, and some-
times arm rests, for use by one person.
Martin (1983) uses these relations to characterize the definitions. In his typology, definitions as in (2)
are considered to be (morphological) derivational because the definiendum is defined with respect to
a morphologically related word. In these definitions, the lexical semantic relation only involves two
words that are morphologically related. Being members of the same derivational family, the orthographic
representations of these words show some degree of similarity that can help us identify the morphological
definitions. In (2) for example, the written forms nitrificateur ?nitrifying? and nitrification ?nitrification?
share a 10 letters prefix and only differ by 3 letters. This strong similarity is a reliable indicator of their
morphologically relatedness (Hathout, 2011b). Building on this observation, a definition is likely to be
morphological if its definiens contains a word which is orthographically similar to the definiendum.
(2) nitrificateur
A
= Qui produit, qui favorise la nitrification.
?nitrifying? ?that produces, that favors nitrification?
We used Proxinette, a measure of morphological similarity defined in (Hathout, 2008), to identify the
morphological definitions. Proxinette is designed to reduce the search space for derivational analogies.
The reduction is obtained by bringing closer the words that belong to the same derivational families and
series, since it is precisely within these paradigms that an entry is likely to form analogies (Hathout,
2011a). Proxinette describes the lexemes by all the n-grams of characters that appear in their inflected
forms in order to catch the inflectional stem allomorphy because it tends to also show up in derivation
(Bonami et al., 2009). The n-grams have an additional tag that indicates if they occur at the beginning,
at the end or in the middle of the word. This information is described by adding a # at the beginning
and end of the written forms. For example, in Figure 4, localisation ?localization?, localiser ?localize;
locate? and focalisation ?focalization? share the ions# ending because it occurs in their inflected forms
localisations (plural), localisions (1st person plural, indicative, imperfect) and focalisations (plural). n-
grams of size 1 and 2 are ignored because they occur in too many words and are not discriminant enough.
Proxinette builds a bipartite graph with the words of the lexicon on one side and the features (n-grams)
that characterize them on the other. Each word is linked to all its features and each feature is connected
to the words that own it (see Figure 4). The graph is weighted so that the sum of weights of the outgoing
edges of each node is equal to 1. Morphological similarity is estimated by simulating the spreading of
an activation. For a given entry, an activation is initiated at the node that represents it. This activation is
then propagated towards the features of the entry. In a second step, the activations in the feature nodes
are propagated towards the words that possess them. The words which obtain the highest activations are
the most similar to the entry. The edge weights and the way the graph is traversed brings closer the words
that share the largest number of common features and the most specific ones (i.e. the less frequent).
69
focalisation N
localiser V
#local
ocali
alisat
ation#
list
#foca
localiste A
localisation N
Figure 4: Excerpt of Proxinette bipartite graph. The graph is symmetric.
?
echolocalisation N relocalisation N radiolocalisation N g
?
eolocalisation N glocalisation N d
?
elocalisation N
antid
?
elocalisation A localisateur N localisateur A vocalisation N focalisation N localiser V localisable A
d
?
elocalisateur N localis
?
e A localiste N localiste A localisme N tropicalisation N
Figure 5: The most similar words to the noun localisation. Words in boldface belong to the derivational
family of localisation. Words in light type belong to its derivational series.
We applied Proxinette to GLAWI-HW and calculated for each of them a neighborhood consisting of
the 100 most similar words. Figure 5 shows an excerpt of the neighborhood of the noun localisation.
The occurrence of the verb localiser in this list enables us to identify the morphological definition (3).
(3) localisation
N
= Action de localiser, de se localiser.
?localization? ?the act of localizing, of locating?
The two experiments we conducted use the same data, namely the morphological definitions of GLAWI.
These definitions are selected as follows:
1. We extracted all GLAWI definition glosses (definientia) with their entries and POS (definienda).
2. We syntactically parsed the definientia with the Talismane dependency parser (Urieli, 2013). Figure
6 presents the dependencies syntactic trees for the definientia in (4).
3. We tagged as morphological all definitions where, in the parsed definiens, at least one lemma
(henceforth referred to as morphosemantic head) occurs in the definiendum neighborhood. For
example, in (4), both definitions are tagged as morphological because arr?eter occurs in the neigh-
borhood of arr?et, and d?ecouronner and couronne occur in that of d?ecouronnement.
(4) a. arr?et
N
= Action de la main pour arr?eter le cheval.
?stop? ?action of the hand to stop the horse?
b. d?ecouronnement
N
= L?action de d?ecouronner, d?enlever la couronne.
?uncrowning? ?the act of uncrowning, of removing the crown?
Morphosemantic heads may be the derivational base of the definiendum like d?ecouronner, a more distant
ancestor like couronne or a ?sibling? like in (2) where nitrification is a derivative of the definiendum base
nitrifier ?nitrify?.
Action de la main pour arr?ter le chevalNC P DET NC P VINF DET NC
dep detprep dep prep detobj L'action de d?couronner, d'enlever la couronneNC PDET P VINF DET NCVINF
detdet dep prep dep prep obj
Figure 6: POS-tags and syntactic dependencies of the definientia of (4).
70
6 Acquisition of morphological relations from GLAWI morphological definitions
We extracted from GLAWI?s morphological definitions the pairs of words (w
1
, w
2
) where w
1
is the
definiendum and w
2
the definiens morphosemantic heads (or one of its morphosemantic head if it has
many). After symmetrization, we obtained a lexicon (hereafter GMD) of 107,628 pairs. 32,256 of them
belongs to GMS. A manual check of the 75,372 remaining pairs would enable its addition to GMS.
GMD additional pairs have been evaluated by three judges in two steps. The judges were instructed to
set aside the orthographic variants as desperado N / desp?erado N. We first randomly selected 100 pairs
and had them checked by three judges in order to estimate the inter-annotator agreement. The average
F-measure of the agreement is 0.97 ; Fleiss?s kappa is 0.65. The judges then checked 100 randomly
selected pairs each. 9 out of the 300 pairs were variants and 19 errors were found in the 291 remaining
ones which results in an overall accuracy of 93.5%. This method would lead to an increase of GMS by
more than 70,000 pairs.
The general quality of these acquired pairs can be significantly increased by formal analogy filter-
ing. The idea is to use analogy as a proxy to find pairs of words that are in the same morphological
relation. GMS pairs being provided by Wiktionary contributors, we consider them as correct and use
them as analogical patterns to filter out the pairs acquired from the morphological definitions. By formal
analogy, we mean an analogy between the orthographic representations. For instance, the GMD pair cit-
rique A:citron N form an analogy with ?electrique A:?electron N. The latter being correct, we can assume
that the former is correct too.
(5) a. citrique A : citron N = ?electrique A : ?electron N
b. fragmentation N : d?efragmenter V = concentration N : d?econcentrer V
Analogies between strings are called formal analogies (Lepage, 2003; Stroppa and Yvon, 2005). One
way to check a formal analogy is to find a decomposition (or factorization) of the four strings such that
the differences between the first two are identical to the ones between the second two. In the analogy
in (5a), the ending ique is replaced by on and the POS A by N in both pairs. We applied analogical
filtering to GMS and GMD pairs. 86,228 pairs in GMD form at least one analogy with a pair in GMS;
53,972 of them do not occur in GMS. 300 of these pairs have been checked by three judges. They only
found 3 variants and one error. The obtained accuracy is therefore over 99% (see Table 1).
5
initial analogical
pairs accuracy pairs accuracy
GMS 97,058 ? ? ?
GMD 107,628 95.4% 86,228 99.8%
GMD \ GMS 75,372 93.5% 53,972 99.7%
Table 1: Summary of the quantitative results
GMD morphological relations will not be included into GLAWI. GMS and GMD are made available
as separate resources on the GLAWI web page.
7 Semantic typing of the morphological definitions
The next experiment aims to demonstrate that morphological definitions could easily and quite accurately
be typed semantically. We focus on a particular semantic type, namely definitions of process nominals
such as (6) because they can be evaluated with respect to the Verbaction database (Hathout and Tanguy,
2002). Deverbal nominals have been extensively studied in linguistics (Pustejovsky, 1995) and used
in a number of tools for various tasks. One of their distinctive feature is that they almost have the
same meaning as their base verb. For instance, in (7) the noun and verb phrases are paraphrases of one
another. Verbaction contains 9,393 verb-noun pairs where the noun is morphologically related to the
verb and can be used to express the act denoted by the verb (e.g. verrouiller:verrouillage).
6
It has been
5
Unfortunately, these results could not have been compared with those of Pentheroudakis and Vanderwende (1993) because
their system makes use of a number of lexical and semantic resources that are not available for French. However, a comparison
with Baroni et al. (2002) is underway although their method is corpus-based (and not MRD-based).
6
Verbaction is freely available at: http://redac.univ-tlse2.fr/lexiques/verbaction.html.
71
used in syntactic dependency parsing by Bourigault (2007), in the construction of the French TimeBank
by Bittar et al. (2011), in question answering systems by Bernhard et al. (2011), etc.
(6) verrouillage
N
= Action de verrouiller.
?locking? ?the act of locking.?
(7) nous v
?
erouillons la porte rapidement ?we quickly lock the gate?
le verrouillage de la porte est rapide ?gate locking is quick?
In our experiment, we used the linear SVM classifier liblinear of Fan et al. (2008) to assign a semantic
type to the definitions that have a nominal definiendum and where the morphosemantic head of the
definiens is a verb as in (4) or (6). Verbaction was used to select a corpus of 1,198 of such definitions.
Three judges annotated them. 608 definientia were tagged as processive and 590 ones as non processive.
We then divided the corpus into a test set made up of 100 processive and 100 non processive definitions
and a training set consisting of the remaining definientia.
The classifier is trained to recognize that the definientia in (4) express the same semantic relation
between the morphosemantic head of the definiens and the definiendum. We use the method proposed
by (Hathout, 2008) to capture this semantic similarity. Definientia are described by a large number
of redundant features based on lemmata, POSs and syntactic dependencies. The features are n-grams
calculated from Talismane parses (see figure 6). They are defined as follows:
1. We first collect all the paths that go from one word in the definiens to the syntactic root (e.g. [arr?eter,
pour, action] is a path that starts at arr?eter in (4a)).
2. We extract all the n-grams of consecutive nodes in these paths.
3. Each n-gram yields 3 features: the sequence of the node?s lemmata, the sequence of the nodes POS,
and the sequence of syntactic dependency relations.
We obtained an accuracy of 97% for the semantic typing of the 200 definientia of the test set. The
most immediate application of the classifier is the enrichment of Verbaction. Running the classifier on
all the definitions with a nominal definiendum and a verbal morphosemantic head will provide us with
new couples that could be added to the database. The classifier could also help us type process nouns
that are not morphologically derived such as audition ?hearing? which is defined with respect to the verb
entendre ?hear?. Similar typing could be performed for other semantic types such as agent nouns (in -eur
or -ant), change of state verbs (in -iser or -ifier) or adjectives expressing possibility (in -able), etc. The
experiment also shows that morphological definitions are well suited for semantic analysis because they
express regular semantic relationship between pairs of words that are distinguished by their orthographic
similarity.
8 Conclusion
In this paper, we have presented GLAWI, an XML machine-readable dictionary created from Wiktion-
naire, the French edition of the Wiktionary project. We then showed that GLAWI was well suited for
conducting computational morphology experiments. GLAWI contains morphological subsections which
provide a significant number of valid and varied morphological relations. In addition, morphological re-
lations can also be acquired from GLAWI morphological definitions. We presented a method to identify
these definitions and the words in relation with a fairly good accuracy. We then used formal analogy to
filter out almost all the erroneous pairs acquired from morphological definitions. In a second experiment,
we demonstrate how to assign the morphological definitions to semantic types with a high accuracy.
This work opens several research avenues leading to a formal representation of the different form
and meaning relations that underlie derivational morphology. The next move will be to organize the
morphological relations into a graph similar to D?emonette (Hathout and Namer, 2014) and identify the
paradigms which structure them. We also plan to apply the semantic classification to other semantic
types which could ultimately enable us to explore the intricate interplay between form and meaning.
72
References
Leticia Anton P?erez, Hugo Gonc?alo Oliveira, and Paulo Gomes. 2011. Extracting Lexical-Semantic Knowledge
from the Portuguese Wiktionary. In Proceedings of the 15th Portuguese Conference on Artificial Intelligence,
EPIA 2011, pages 703?717, Lisbon, Portugal.
Rolf Harald Baayen, Richard Piepenbrock, and Leon Gulikers. 1995. The CELEX lexical database (release 2).
CD-ROM. Linguistic Data Consortium, Philadelphia, PA.
Marco Baroni, Johannes Matiasek, and Harald Trost. 2002. Unsupervised discovery of morphologically related
words based on orthographic and semantic similarity. In Proceedings of the Workshop on Morphological and
Phonological Learning of ACL-2002, pages 48?57, Philadelphia, PA, USA.
Delphine Bernhard, Bruno Cartoni, and Delphine Tribout. 2011. A Task-Based Evaluation of French Morpholog-
ical Resources and Tools. Linguistic Issues in Language Technology, 5(2).
Andr?e Bittar, Pascal Amsili, Pascal Denis, et al. 2011. French TimeBank: un corpus de r?ef?erence sur la temporalit?e
en franc?ais. In Actes de la 18e Conf?erence Annuelle sur le Traitement Automatique des Langues Naturelles
(TALN-2011), volume 1, pages 259?270, Montpellier, France.
Olivier Bonami, Gilles Boy?e, and Franc?oise Kerleroux. 2009. L?allomorphie radicale et la relation flexion-
construction. In Bernard Fradin, Franc?oise Kerleroux, and Marc Pl?enat, editors, Aperc?us de morphologie du
franc?ais, pages 103?125. Presses universitaires de Vincennes, Saint-Denis.
Didier Bourigault. 2007. Un analyseur syntaxique op?erationnel : SYNTEX. Habilitation
`a diriger des recherches,
Universit?e Toulouse II-Le Mirail, Toulouse.
Nicoletta Calzolari, Laura Pecchia, and Antonio Zampolli. 1973. Working on the Italian Machine Dictionary:
A Semantic Approach. In Proceedings of the 5th Conference on Computational Linguistics - Volume 2, pages
49?52, Stroudsburg, PA, USA.
Nicoletta Calzolari. 1988. The dictionary and the thesaurus can be combined. In Martha Evens, editor, Relational
Models of the Lexicon, pages 75?96. Cambridge University Press.
Martin S. Chodorow, Roy J. Byrd, and George E. Heidorn. 1985. Extracting semantic hierarchies from a large
on-line dictionary. In Proceedings of the 23rd Annual Meeting on Association for Computational Linguistics,
ACL ?85, pages 299?304, Stroudsburg, PA, USA.
James R. Curran and Marc Moens. 2002. Improvements in Automatic Thesaurus Extraction. In Proceedings of
the ACL Workshop on Unsupervised Lexical Acquisition, pages 59?66, Philadelphia, USA.
Jacques Dendien. 1994. Le projet d?informatisation du TLF. In
?
Eveline Martin, editor, Les textes et
l?informatique, chapter 3, pages 31?63. Didier
?
Erudition, Paris, France.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. The Journal of Machine Learning Research, 9:1871?1874.
Nabil Hathout and Fiammetta Namer. 2014. La base lexicale d?emonette : entre s?emantique constructionnelle et
morphologie d?erivationnelle. In Actes de la 21e conf?erence annuelle sur le traitement automatique des langues
naturelles (TALN-2014), Marseille, France.
Nabil Hathout and Ludovic Tanguy. 2002. Webaffix : Finding and validating morphological links on the WWW.
In Proceedings of the Third International Conference on Language Resources and Evaluation, pages 1799?
1804, Las Palmas de Gran Canaria, Spain.
Nabil Hathout. 2008. Acquisition of the morphological structure of the lexicon based on lexical similarity and
formal analogy. In Proceedings of the Coling workshop Textgraphs-3, pages 1?8, Manchester, England.
Nabil Hathout. 2011a. Morphonette: a paradigm-based morphological network. Lingue e linguaggio,
2011(2):243?262.
Nabil Hathout. 2011b. Une approche topologique de la construction des mots : propositions th?eoriques et appli-
cation `a la pr?efixation en anti-. In Des unit?es morphologiques au lexique (Roch
?e et al., 2011), pages 251?318.
Kris Heylen, Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2008. Modelling Word Similarity: an Eval-
uation of Automatic Synonymy Extraction Algorithms. In Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08), Marrakech, Morocco.
73
Mathieu Lafourcade. 2007. Making People Play for Lexical Acquisition with the JeuxDeMots prototype. In
SNLP?07: 7th International Symposium on Natural Language Processing, Pattaya, Thailand.
Yves Lepage. 2003. De l?analogie rendant compte de la commutation en linguistique. Habilitation `a diriger des
recherches, Universit?e Joseph Fourier, Grenoble.
Judith Markowitz, Thomas Ahlswede, and Martha Evens. 1986. Semantically significant patterns in dictionary
definitions. In Proceedings of the 24th Annual Meeting on Association for Computational Linguistics, pages
112?119, Stroudsburg, PA, USA.
Robert Martin. 1983. Pour une logique du sens. Linguistique nouvelle. Presses universitaires de France, Paris.
Emmanuel Navarro, Franck Sajous, Bruno Gaume, Laurent Pr?evot, ShuKai Hsieh, Ivy Kuo, Pierre Magistry, and
Chu-Ren Huang. 2009. Wiktionary and NLP: Improving synonymy networks. In Proceedings of the 2009
ACL-IJCNLP Workshop on The People?s Web Meets NLP: Collaboratively Constructed Semantic Resources,
pages 19?27, Singapore.
Roberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: Building a Very Large Multilingual Semantic
Network. In Proceedings of ACL?2010, pages 216?225, Uppsala, Sweden.
Joseph Pentheroudakis and Lucy Vanderwende. 1993. Automatically identifying morphological relations in
machine-readable dictionaries. In Proceedings of the Ninth Annual Conference of the UW Centre for the New
OED and Text Research, pages 114?131.
Jean-Marie Pierrel. 2013. Structuration et usage de ressources lexicales institutionnelles sur le franc?ais. Linguis-
ticae investigationes Supplementa, pages 119?152.
James Pustejovsky. 1995. The Generative Lexicon. MIT Press, Cambridge, MA.
Michel Roch?e, Gilles Boy?e, Nabil Hathout, St?ephanie Lignon, and Marc Pl?enat. 2011. Des unit?es morphologiques
au lexique. Herm`es Science-Lavoisier, Paris.
Beno??t Sagot and Darja Fi?ser. 2008. Building a Free French Wordnet from Multilingual Resources. In Proceedings
of OntoLex 2008, Marrakech, Morocco.
Franck Sajous, Nabil Hathout, and Basilio Calderone. 2013a. GL
`
AFF, un Gros Lexique
`
A tout Faire du Franc?ais.
In Actes de la 20e conf?erence sur le Traitement Automatique des Langues Naturelles (TALN?2013), pages 285?
298, Les Sables d?Olonne, France.
Franck Sajous, Emmanuel Navarro, Bruno Gaume, Laurent Pr?evot, and Yannick Chudy. 2013b. Semi-automatic
enrichment of crowdsourced synonymy networks: the WISIGOTH system applied to Wiktionary. Language
Resources and Evaluation, 47(1):63?96.
Patrick Schone and Daniel S. Jurafsky. 2000. Knowledge-free induction of morphology using latent semantic
analysis. In Proceedings of the Conference on Natural Language Learning 2000 (CoNLL-2000), pages 67?72,
Lisbon, Portugal.
Gilles S?erasset. 2012. Dbnary: Wiktionary as a LMF based Multilingual RDF network. In Proceedings of the
Eigth International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey.
Nicolas Stroppa and Franc?ois Yvon. 2005. An analogical learner for morphological analysis. In Procs. of the 9th
Conference on Computational Natural Language Learning (CoNLL-2005), pages 120?127, Ann Arbor, MI.
Assaf Urieli. 2013. Robust French syntax analysis: reconciling statistical methods and linguistic knowledge in
the Talismane toolkit. Th`ese de doctorat, Universit?e de Toulouse-Le Mirail.
Lonneke van der Plas and Gosse Bouma. 2005. Syntactic Contexts for Finding Semantically Related Words. In
Ton van der Wouden, Michaela Po?, Hilke Reckman, and Crit Cremers, editors, Computational Linguistics in
the Netherlands 2004: Selected papers from the fifteenth CLIN meeting, volume 4 of LOT Occasional Series.
Utrecht University.
Torsten Zesch and Iryna Gurevych. 2010. Wisdom of Crowds versus Wisdom of Linguists - Measuring the
Semantic Relatedness of Words. Journal of Natural Language Engineering., 16(01):25?59.
Torsten Zesch, Christof M?uller, and Iryna Gurevych. 2008. Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary. In Proceedings of the Sixth International Conference on Language Resources and
Evaluation (LREC 2008), Marrakech, Morocco.
Pierre Zweigenbaum and Natalia Grabar. 2003. Learning derived words from medical corpora. In 9th Conference
on Artificial Intelligence in Medicine Europe, pages 189?198.
74
