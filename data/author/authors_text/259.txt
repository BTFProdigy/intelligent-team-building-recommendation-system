Learning Argument/Adjunct Distinction for Basque 
Abstract 
This paper presents experiments performed on 
lexical knowledge acquisition in the form of 
verbal argumental information. The system 
obtains the data from raw corpora after the 
application of a partial parser and statistical 
filters. We used two different statistical filters 
to acquire the argumental information: Mutual 
Information, and Fisher?s Exact test. 
Due to the characteristics of agglutinative 
languages like Basque, the usual classification 
of arguments in terms of their syntactic 
category (such as NP or PP) is not suitable. 
For that reason, the arguments will be 
classified in 48 different kinds of case 
markers, which makes the system fine grained 
if compared to equivalent systems that have 
been developed for other languages. 
This work addresses the problem of 
distinguishing arguments from adjuncts, this 
being one of the most significant sources of 
noise in subcategorization frame acquisition. 
Introduction 
In recent years a considerable effort has been done 
on the acquisition of lexical information. As 
several authors point out, this information is useful 
for a wide range of applications. For example, J. 
Carroll et al (1998) show how adding 
subcategorization information improves the 
performance of a parser. 
With this in mind our aim is to obtain a system 
that automatically discriminates between 
subcategorized elements of verbs (arguments) and 
non-subcategorized ones (adjuncts).  
We have evaluated our system in two ways: 
comparing the results to a gold standard and 
estimating the coverage over sentences in the 
corpus. The purpose was to find out which was the 
impact of each approach on this particular task. 
The two methods of evaluation yield significantly 
different results.  
Basque is the subject of this study. A language 
that, in contrast to languages like English, has 
limited resources in the form of digital corpora, 
computational lexicons, grammars or annotated 
treebanks. Therefore, any effort like the one 
presented here, oriented to create lexical resources, 
has to be driven to do as much automatic work as 
possible, minimizing development costs. 
The paper is divided into 4 sections. The first 
section is devoted to explain the theoretical 
motivations underlying the process. The second 
section is a description of the different stages of 
the system. The third section presents the results 
obtained. The fourth section is a review of 
previous work on automatic subcategorization 
acquisition. Finally, we present the main 
conclusions. 
1 The argument/adjunct distinction 
Talking about Subcategorization Frames (SCF), 
means talking about arguments. Many existing 
systems acquire directly a set of possible SCFs 
without any previous filtering of adjuncts. 
However, adjuncts are a substantial source of noise 
and therefore, in order to avoid this problem, our 
approach addresses the problem of the 
argument/adjunct distinction. 
The argument/adjunct distinction is probably 
one of the most unclear issues in linguistics. The 
distinction has being presented, for example, in the 
generativist tradition, in the following way: 
arguments are those elements participating in the 
event and adjuncts are those elements 
contextualizing or locating the event. 
This definition seems to be quite clear, but 
when we deal with concrete examples it is not the 
Izaskun Aldezabal, Maxux Aranzabe, Koldo 
Gojenola , Kepa Sarasola 
Dept. of Computer Languages and Systems, 
University of the Basque Country, 649 P. K., 
E-20080 Donostia,  
Basque Country 
Aitziber Atutxa 
University of Maryland  
College Park 
Maryland, 20740 
jibatsaa@si.ehu.es 
                     July 2002, pp. 42-50.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
case. For example, if we take two verbs, talk and 
play.  
a. Yesterday I talked with Mary. 
b. Yesterday I played soccer with Mary. 
Here Mary is a participant of the event in both 
cases, therefore under the given definition both 
would be arguments. But this is contradictory to 
what traditional views consider in practice. The 
PP, with Mary, is considered an argument of talk 
but not an argument of play. It is true that there are 
differences between both of them because playing 
does not require two participants (though it can 
have them), while talking (under the sense of 
communicating) seems to require two participants. 
Finer argument/adjunct distinction have also 
been proposed differentiating between basic 
arguments, pseudo-arguments and adjuncts. Basic 
arguments are those required by the verb. Pseudo-
arguments are those that even if they are not 
required by the verb, when appearing they extend 
the verbal semantics, for example, adding new 
participants. And finally adjuncts, which would be 
contextualizers of the event. The most radical view 
is to consider the argument/adjunct distinction as a 
continuum where the elements belonging to the 
extremes of this continuum can be easily classified 
as arguments or adjuncts. On the contrary, the 
elements belonging to the central part of the 
continuum can be easily misclassified. For further 
reference see C. Schutze (1995), J.M. Gawron 
(1986), C. Verspoor (1997), J. Grimshaw (1990), 
and N. Chomsky (1995). 
From the different diagnostics proposed in the 
literature some are quite consistent among various 
authors (R. Grishman et al 1994, C. Pollard and I. 
Sag 1987, C. Verspoor 1997). 
1) The Obligatoriness condition. When a verb 
demands obligatorily the appearance of an 
element, this element will be an argument. 
a. John put the book on the table 
b. *John put the book 
2) Frequency. Arguments of a verb occur more 
frequently with that verb than with the other 
verbs. 
a. I came from home (argument). 
b. I heard it from you (adjunct). 
3) Iterability: Several instances of the same 
adjunct can appear together with a verb, while 
several instances of an argument cannot appear 
with a verb. 
a. I saw you in Washington, in the 
Kenedy Center. 
b. *I saw Alice John (being John and 
Alice two persons) 
4) Relative order: Arguments tend to appear closer 
to the verb than adjuncts.  
a. I put the book on the table at three 
b. *I put at three the book on the 
table 
5) Implicational test:  Arguments are semantically 
implied, even when they are optional. 
   a. I came to your house (from x) 
  b. I heard that (from x) 
The third and fourth tests were not very useful 
to us. Iterability test is quite weak because it seems 
to rely more on some other semantic notions such 
as part/whole relation than in the argument/adjunct 
distinction. For example, sentence 3.a would be 
grammatical due to semantic plausibility. The 
Kennedy Center is a part of Washington, therefore 
to see somebody in the Kennedy Center and see 
him in Washington are not semantically 
incompatible, so it is plausible to say it. In the case 
of 3.b John is not a part of  Alice and therefore it is 
not plausible to see Alice John. But for example it 
is plausible to say I saw you the hand. The relative 
order test is difficult to apply on a language like 
Basque which is a free word order language.  
The first and fifth tests are robust enough to be 
useful in practice. But only the two first 
diagnostics can be captured statistically by the 
application of association measures like Mutual 
Information. We did not come out with any  
straightforward way to apply the fifth test 
computationally. 
Before talking about the different measures 
applied, we will present step by step the whole 
process we pursued for achieving the 
argument/adjunct distinction. 
2 The acquisition process  
Our starting point was a raw newspaper corpus 
from of 1,337,445 words, where there were 
instances of 1,412 verbs. From them, we selected 
640 verbs as statistically relevant because they 
appear in more than 10 sentences.  
As we said earlier, our goal was to distinguish 
arguments from adjuncts. When starting from raw 
corpus, like in this case, it is necessary to get 
instances of verbs together with their dependents 
(arguments and adjuncts). We obtained this 
information applying a partial parser (section 2.1) 
to the corpus. Once we had the dependents, 
statistical measures helped us deciding which were 
arguments and which were adjuncts (section 2.2). 
2.1 The parsing phase 
Aiming to obtain the data against which statistical 
filters will be applied, we analyzed the corpus 
using several available linguistic resources: 
? First, we performed morphological analysis of 
the corpus, based on two-level morphology (K. 
Koskenniemi 1983; I. Alegria et al 1996) and 
disambiguation using the Constraint Grammar 
formalism (Karlsson et al 1995, Aduriz et al 
1997).  
? Second, a shallow parser was applied (I. 
Aldezabal et al 2000), which recognizes basic 
syntactic units including noun phrases, 
prepositional phrases and several types of 
subordinate sentences. 
? The third step consisted in linking each verb 
and its dependents. Basque lacks a robust 
parser as in (T. Briscoe & J. Carroll 1997, D. 
Kawahara et al 2001) and, therefore, we used a 
finite state grammar to link the dependents 
(both arguments and adjuncts) with the verb (I. 
I. Aldezabal et al 2001). This grammar was 
developed using the Xerox Finite State Tool (L. 
Karttunen et al 1997). Figure 1 shows the 
result of the parsing phase. In this case, both 
commitative and inessive cases (PPs) are 
adjuncts, while the ergative NP is an argument. 
The linking of dependents to a verb is not 
trivial considering that Basque is a language 
with free order of constituents, and any element 
appearing between two verbs could be, in 
principle, dependent on any of them. Many 
problems must be taken into account, such as 
ambiguity and determination of clause 
boundaries, among others. We evaluated the 
accuracy up to this point, obtaining a precision 
over dependents of 87% and a recall of 66%. 
So the input data to the next phase was 
relatively noisy.  
2.2 The argument selection phase 
In the data resulting from the shallow parsing 
phase we counted up to 65 different cases (types of 
arguments, including postpositions and different 
types of suffixes). These are divided in two main 
groups: 
? 43 correspond to postpositions. Some of them 
can be directly mapped to English prepositions, 
but in many cases several Basque postpositions 
correspond to just one English preposition (see 
Table 1a.). This set alo contains postpositions 
1)? (a) [ EEBBetako lehendakariak] (b) [UEko 15 herrialdeetako merkataritza ministroekin] 
(c) [bazkaldu behar zuen] (d) [negoziazioen bilgunean] ? 
 
2) ? the president of the USA had to eat with the ministers of Commerce of 15 countries of the UE in
the negotiation center ? 
 
(a)  [EEBB-etako lehendakari-a-k]       (b)  [UE-ko    15 herrialde-etako    merkataritza ministro-ekin]  
     [USA-of         president-the-erg.]        [UE-of    15 countries-of          Commerce ministers-with]  
      NP-ergative(president, singular)                PP(with)-commitative(minister, plural)  
 The president of the USA  with the ministers of Commerce of 15 countries of the UE 
 
 
(c) [bazkaldu behar zuen]                  (d)   [negoziazio-en     bilgune-an] 
        [to eat        had]                                  [negotiation-of     center-in]   
          verb(eat)                                        PP(in)-inessive(center, singular) 
 had to eat in the negotiation center 
Figure 1. Example of the output of the shallow parsing phase: 1) Input (in Basque), 2) English translation,. 
Below (c) Verb phrase and (a,b,d) verbal dependents (phrases), and also under the case+head 
that map to categories other than English 
prepositions, such as adverbs (Table 1b). 
Table 1. Correspondence between English 
prepositions and Basque postpositions. 
 English Basque 
a. to dative (suffix) 
alative (suffix) 
final ablative (suffix) 
b. like -en gisa (suffix) 
gisa 
bezala 
legez 
 
? 22 types of sentential complements (For 
instance, English that complementizer 
corresponds to several subordination suffixes:  
-la, -n, -na, -nik). 
This shows to which extent the range of 
arguments is fine grained, in contrast to other 
works where the range is at the categorial level, 
such as NP or PP (M. Brent 1993, C. Manning 
1993, P. Merlo & M. Leybold 2001). 
Due to the complexity carried by having such a 
high number of cases, we decided to gather 
postpositions that are semantically equivalent or 
almost equivalent (for example, English between 
and among). Even if there are some semantic 
differences between them they do not seem to be 
relevant at the syntactic level. Some linguists were 
in charge of completing this grouping task. Even 
considering the risk of making mistakes when 
grouping the cases, we concluded that the loss of 
accuracy due to having too sparse data 
(consequence of having many cases) would be 
worse than the noise introduced by any mistake in 
the grouping. The resulting set contained 48 cases. 
The complexity is reduced but it is still 
considerable.  
Most of the work on automatic acquisition of 
subcategorization information (J. Carroll & T. 
Briscoe 1997, A. Sarkar & D. Zeman 2000, A. 
Korhonen 2001) apply statistical methods 
(hypothesis testing). Basically the idea is the 
following: they get "possible subcategorization 
frames" from automatically parsed data (either 
completely or partially parsed) or from a 
syntactically annotated corpus. Afterwards a 
statistical filter is employed to decide whether 
those "possible frames" are or not real 
subcategorization frames. These statistical 
methods can be problematic mostly because they 
perform badly on sparse data. In order to avoid as 
much as possible data sparseness, we decided to 
design a system that learns which are the 
arguments of a given verb instead of learning 
whole frames. Frames are combinations of 
arguments, and considering that our system deals 
with 48 cases, the number of combinations was 
high, resulting in sparse data. So we decided to 
work at the level of the argument/adjunct 
distinction. Working on this distinction is also very 
useful to avoid noise in the subcategorization 
frame, because in this task adjuncts are synonyms 
of noise. A system that tries to get 
subcategorization frames without previously 
making the argument/adjunct distinction suffers of 
having sparse and noisy data.  
To accomplish the argument/adjunct distinction 
we applied two measures: Mutual Information 
(MI), and Fisher's Exact Test (for more 
information on these measures, see C. Manning & 
H. Sch?tze 1999). MI is a measure coming from 
Information Theory, defined as the logarithm of 
the ratio between the probability of the co-
occurrence of the verb and the case, and the 
probability of the verb and the case appearing 
together calculated from their independent 
probability. So higher Mutual Information values 
correspond to higher associated verb and cases 
(see table 2). 
Table 2. Examples from MI values for verb-case 
pairs 
verb case MI 
atera(to take/go out) ablative(from) 1.830 
atera(to take/go out) instrumental(with) -0.955 
erabili(to use) gisa(as) 2.255 
erabili(to use) instrumental(with) -0.783 
Mutual Information shows higher values for 
atera-ablative(to go/take out), erabili-gisa (to use-
as). These pairs were manually tagged as 
arguments, therefore Mutual information makes 
the right prediction. On the contrary, atera-
instrumental (to go/take out-with), erabili-
instrumental (to use-with) were manually tagged as 
adjuncts. Mutual information values in table 2 go 
along with the manual tagging for these last pairs 
as well, because the Mutual information values are 
low as should correspond to adjuncts.  
Fisher?s Exact Test is a hypothesis testing 
statistical measure1. We used the left-side version 
of the test (see T. Pederssen, 1996). Under this 
version the test tells us how likely it would be to 
perform the same experiment again and be less 
accurate. That is to say, if you were repeating the 
experiment and there were no relation between the 
verb and the case, you would have a big 
probability of finding a lower co-occurrence 
frequency than the one you observed in your 
experiment. So higher left-side Fisher values tell 
us that there is a correlation between the verb and 
the case (see table 3.) 
Table 3. Examples of Fisher?s Exact Test  values for 
verb-case pairs 
verb Case Fisher 
atera(to take/go out) Ablative(from) 1.0000 
atera(to take/go out) instrumental(with) 0.0003 
erabili(to use) gisa(as) 1.0000 
erabili(to use) instrumental(with) 0.0002 
Fisher?s Exact values show higher values for 
atera-ablative(to go/take out), erabili-gisa (to use-
as). These values predict correctly the association 
between the verbs and cases for these examples. 
The low values for the atera-instrumental (to 
go/take out-with), and erabili-instrumental (to use-
with) pairs, should be interpreted as the non-
association between the verbs and the cases in 
these examples, that is to say, they are adjuncts. 
And again, the prediction would be right according 
to the taggers. 
These tests are broadly used to discover 
associations between words, but they show 
different behaviour depending on the nature of the 
data. We did not want to make any a priori 
decision on the measure employed. On the 
contrary, we aimed to check which test behaved 
better on our data.  
3 Evaluation  
We found in the literature two main approaches to 
evaluate a system like the one proposed in this 
paper (T. Briscoe & J. Carroll 1997, A. Sarkar & 
D. Zeman 2000, A. Korhonen 2001): 
                                                     
1 There are two ways of interpreting Fisher?s test, as one 
or two sided test. In the one sided fashion there is still 
another interpretation, as a right or left sided test. 
 
? Comparing the obtained information with a 
gold standard.  
? Calculating the coverage of the obtained 
information on a corpus. This can give  an 
estimate of how well the information obtained 
could help a parser on that corpus. 
Under the former approach a further distinction 
emerges: using a dictionary as a gold standard, or 
performing manual evaluation, where some 
linguists extract the subcategorization frames 
appearing in a corpus and comparing them with the 
set of subcategorization frames obtained 
automatically.  
We decided to evaluate the system both ways, 
that is to say, using a gold standard and calculating 
the coverage over a corpus. The intention was to 
determine, all things being equal, the impact of 
doing it one way or the other. 
3.1 Evaluation 1: comparison of the results with a 
gold standard 
From the 640 analyzed verbs, we selected 10 for 
evaluation. For each of these verbs we extracted 
from the corpus the list of all their dependents. The 
list was a set of bare verb-case pairs, that is, no 
context was involved and, therefore, as the sense 
of the given verb could not be derived, different 
senses of the verb were taken into account.  We 
provided 4 human annotators/taggers with this list 
and they marked each dependent as either 
argument or adjunct. The taggers accomplished the 
task three times. Once, with the simple guideline 
of the implicational test and obligatoriness test, but 
with no further consensus. The inter-tagger 
agreement was low (57%). The taggers gathered 
and realized that the problem came mostly from 
semantics. While some taggers tagged the verb-
case pairs assuming a concrete semantic domain 
the others took into account a wider rage of senses 
(moreover, in some cases the senses did not even 
match). So the tagging was repeated when all of 
them considered the same semantics to the 
different verbs. The inter-tagger agreement raised 
up to a 80%. The taggers gathered again to discuss, 
deciding over the non clear pairs. 
The list obtained from merging2 the 4 lists in 
one is taken to be our gold standard. Notice that 
                                                     
2 Merging was possible once the annotators agreed on 
the marking of each element. 
when the annotators decided whether a possible 
argument was really an argument or not, no 
context was involved. In other words, they were 
deciding over bare pairs of verbs and cases. 
Therefore different senses of the verb were 
considered because there was no way to 
disambiguate the specific meaning of the verb. So 
the evaluation is an approximation of how well 
would the system perform over any corpus. Table 
4 shows the results in terms of Precision and 
Recall. 
Table 4. Results of Evaluation 1 (context 
independent) 
 Precision Recall F-score 
MI 62% 50% 55% 
Fisher 64% 44% 52% 
 
3.2 Evaluation 2: Calculation of the coverage on a 
corpus 
The initial corpus was divided in two parts, one for 
training the system and another one for evaluating 
it. From the fraction reserved for evaluation we 
extracted 200 sentences corresponding to the same 
10 verbs used in the "gold standard" based 
evaluation. In this case, the task carried out by the 
annotators consisted in extracting, for each of the 
200 sentences, the elements (arguments/adjuncts) 
linked to the corresponding verb. Each element 
was marked as argument or adjunct. Note that in 
this case the annotation takes place inside the 
context of the sentence. In other words, the verb 
shows precise semantics.  
We performed a simple evaluation on the 
sentences (see table 5), calculating precision and 
recall over each argument marked by the 
annotators3. For example, if a verb appeared in a 
sentence with two arguments and the statistical 
filters were recognizing them as arguments, both 
precision and recall would be 100%. If, on the 
contrary, only one was found, then precision 
would be 100%, and recall 50%.  
Table 5. Results of Evaluation 2 (inside context) 
 Precision Recall F-score 
MI 93% 97% 95% 
Fisher 93% 93% 93% 
                                                     
3 The inter-tagger agreement in this case was of  97%.  
3.3 Discussion 
It is obvious that the results attained in the first 
evaluation are different than those in the second 
one. The origin of this difference comes mostly, on 
one hand, from semantics and, on the other hand, 
from the nature of statistics: 
? Semantic source. The former evaluation was 
not contextualized, while the latter used the 
sentence context. Our experience showed us 
that broader semantics (non-contextualized 
evaluation) leads to a situation where the 
number of arguments increases with respect to 
narrower (contextualized evaluation) 
semantics. This happens because in many 
cases different senses of the same verb require 
different arguments. So when the meaning of 
the verb is not specified, different meanings 
have to be taken into account and, therefore, 
the task becomes more difficult. 
? Statistical reason. The disagreement in the 
results comes from the nature of the statistics 
themselves. Any statistical measure performs 
better on the most frequent cases than on the 
less frequent ones. In the first experiment all 
possible arguments are evaluated, including 
the less frequent ones, whereas in the second 
experiment only the possible arguments found 
in the piece of corpus used were evaluated. In 
most of the cases, the possible arguments 
found were the most frequent ones. 
At this point it is important to note that the 
system deals with non-structural cases. In Basque 
there are three structural cases (ergative, absolutive 
and dative) which are special because, when they 
appear, they are always arguments. They 
correspond to the subject, direct object and indirect 
object functions. These cases are not very 
conflictive about argumenthood, mainly because in 
Basque the auxiliary bears information about their 
appearance in the sentence. So they are easily 
recognized and linked to the corresponding verb. 
That is the reason for not including them in this 
work. Precision and recall would improve 
considerably if they were included because they 
are the most frequent cases (as statistics perform 
well over frequent data), and also because the 
shallow parser links them correctly using the 
information carried by the auxiliary. Notice that 
we did not incorporate them because in the future 
we would like to use the subcategorization  
information obtained for helping our parser, and 
the non-structural cases are the most problematic 
ones.    
4 Related work  
Concerning the acquisition of verb 
subcategorization information, there are proposals 
ranging from manual examination of corpora (R. 
Grishman et al 1994) to fully automatic 
approaches.  
Table 3, partially borrowed from A. Korhonen 
(2001), summarizes several systems on 
subcategorization frame acquisition. 
C. Manning (1993) presents the acquisition of 
subcategorization frames from unlabelled text 
corpora. He uses a stochastic tagger and a finite 
state parser to obtain instances of verbs with their 
adjacent elements (either arguments or adjuncts), 
and then a statistical filtering phase produces 
subcategorization frames (from a set of previously 
defined 19 frames) for each verb.  
T. Briscoe and J. Carroll (1997) describe a 
grammar based experiment for the extraction of 
subcategorization frames with their associated 
relative frequencies, obtaining 76.6% precision 
and 43.4% recall. Regarding evaluation, they use 
the ANLT and COMLEX Syntax dictionaries as 
gold standard. They also performed evaluation of 
coverage over a corpus. For our work, we could 
not make use of any previous information on 
subcategorization, because there is nothing like a  
subcategorization dictionary for Basque. 
A. Sarkar and D. Zeman (2000) report results 
on the automatic acquisition of subcategorization 
frames for verbs in Czech, a free word order 
language. The input to the system is a set of 
manually annotated sentences from a treebank, 
where each verb is linked with its dependents 
(without distinguishing arguments and adjuncts). 
The task consists in iteratively eliminating 
elements from the possible frames with the aim of 
removing adjuncts. For evaluation, they give an 
estimate of how many of the obtained frames 
appear in a set of 500 sentences where dependents 
were annotated manually, showing an 
improvement from a baseline of 57% (all elements 
are adjuncts) to 88%. 
Comparing this approach to our work, we must 
point out that Sarkar and Zeman's data does not 
come from raw corpus, and thus they do not deal 
with the problem of noise coming from the parsing 
phase. Their main limitation comes by relying on a 
treebank, which is an expensive resource. 
D. Kawahara et al (2001) use a full syntactic 
parser to obtain a case frame dictionary for 
Japanese, where arguments are distinguished by 
their syntactic case, including their headword 
(selectional restrictions). The resulting case frame 
components are selected by a frequency threshold. 
Table 3. Summary of several systems on subcategorization information. 
Method Number 
of frames 
Number 
of verbs 
Linguistic 
resources 
F-Score 
(evaluation 
based on a 
gold standard) 
Coverage on a 
corpus 
C. Manning (1993) 19 200 POS tagger + simple 
finite state parser 
58  
T. Briscoe & J. 
Carroll (1997) 
161 14 Full parser 55  
A. Sarkar & D. 
Zeman (2000) 
137 914 Annotated treebank - 88 
D. Kawahara et al 
(2001) 
- 23,497 Full parser  82 accuracy 
M. Maragoudakis et 
al. (2001) 
- 47 Simple phrase 
chunker 
77  
This paper - 640 Morph. Analyzer + 
Phrase Chunker + 
Finite State Parser 
55 95 
      
M. Maragoudakis et al (2001) apply a 
morphological analyzer and phrase chunking 
module to acquire subcategorization frames for 
Modern Greek. In contrast to this work, they use 
different machine learning techniques. They claim 
that Bayesian Belief Networks are the best 
learning technique. 
P. Merlo and M. Leybold (2001) present 
learning experiments for automatic distinction of 
arguments and adjuncts, applied to the case of 
prepositional phrases attached to a verb. She uses 
decision trees tested on a set of 400 verb instances 
with a single PP, reaching an accuracy of 86.5% 
over a baseline of 74%. 
Note that both Manning and Merlo and 
Leybold's systems learn from contexts with just 
one PP (maximum) per verb (finite state filter). 
Our system learns from contexts with up to 5 PPs. 
Furthermore, we distinguish 48 different kinds of 
cases, hence the number of combinations is 
considerably bigger.  
Regarding the parsing phase, the systems 
presented so far are heterogeneous. While  
Manning, Merlo and Leybold and Maragoudakis et 
al. use very simple parsing techniques, Briscoe and 
Carroll and Kawahara et al use sophisticated 
parsers. Our system can be placed between these 
two approaches. The result of the shallow parsing 
is not simple in that it relies on a robust 
morphological analysis and disambiguation. 
Remember that Basque is an agglutinative 
language with strong morphology and, therefore, 
this stage is particularly relevant. Moreover, the 
finite state filter we used for parsing is very 
sophisticated (L. Karttunen et al 1997, I. 
Aldezabal et al 2001), compared to Manning's. 
Conclusion  
This work describes an initial effort to obtain 
subcategorization information for Basque. To 
successfully perform this task we had to go deeper 
than mere syntactic categories (NP, PP, ?) 
enriching the set of possible arguments to 48 
different classes. This leads to quite sparse data.  
Together with sparseness, another problem 
common to every subcategorization acquisition 
system is that of noise, coming from adjuncts and 
incorrectly parsed elements. For that reason, we 
defined subcategorization acquisition in terms of 
distinguishing between arguments and adjuncts. 
The system presented was applied to a 
newspaper corpus. Subcategorization acquisition is 
highly associated to semantics in that different 
senses of a verb will most of the times show 
different subcategorization information. Thus, the 
task of learning subcategorization information is 
influenced by the corpus. As for the evaluation of 
this work, we carried out two different kinds of 
evaluation. This way, we verified the relevance of 
semantics in this kind of task. 
For the future, we plan to incorporate the 
information resulting from this work in our parsing 
system. We hope that this will lead to better results 
in parsing. Consequently, we would get better 
subcategorization information, in a bootstrapping 
cycle. We also plan to improve the results by using 
semantic information as proposed in A. Korhonen 
(2001).  
Acknowledgements 
This work has been supported by the Department 
of Economy of the Government of Gipuzkoa, The 
University of the Basque Country, the Department 
of Education of the Basque Government and the 
Commission of Science and Technology of the 
Spanish Government.   
References 
I. Aduriz, J. M. Arriola, X. Artola, A. D?az de 
Ilarraza, K. Gojenola and M. Maritxalar (1997) 
Morphosyntactic disambiguation for Basque based on 
the Constraint Grammar Formalism. Conference on 
Recent Advances in Natural Language Processing 
(RANLP).  
I. Alegria, X. Artola, K. Sarasola and M. Urkia (1996) 
Automatic morphological analysis of Basque. Literary 
and Linguistic Computing. 11 (4), Oxford University. 
I. Aldezabal, K. Gojenola and K. Sarasola (2000) A 
Bootstrapping Approach to Parser Development. 
International Workshop on Parsing Technologies 
(IWPT), Trento. 
I. Aldezabal, M. Aranzabe, A. Atutxa, K. Gojenola, 
M. Oronoz M. and Sarasola K. (2001) Application of 
finite-state transducers to the acquisition of verb 
subcategorization information. Finite State Methods 
in Natural Language Processing, ESSLLI Workshop, 
Helsinki. 
M. R. Brent (1993) From Grammar to Lexicon: 
Unsupervised Learning of Lexical Syntax. 
Computational Linguistics, 19:243-262. 
T. Briscoe and J. Carroll  (1997) Automatic Extraction 
of Subcategorization from Corpora. ANLP-97:356-
363. 
J. Carroll, G. Minnen and T. Briscoe (1998) Can 
Subcategorization Probabilities Help a Statistical 
Parser? Proceedings of the 6th ACL/SIGDAT 
Workshop on Very Large Corpora, Montreal. 
N. Chomsky (1995) The Minimalist Program. 
Cambridge MA, MIT Press. 
T. Dunning  (1993) Accurate Methods for the 
Statistics of Surprise and Coincidence. Computational 
Linguistics 19, 1  
J.M. Gawron (1986) Situations and prepositions. 
Linguistics and Philosophy 9(3), 327-382. 
J. Grimshaw (1990) Argument Structure. Cambridge, 
MA, MIT Press. 
R. Grishman, C. Macleod, A. Meyers (1994) Comlex 
Syntax: Building a Computational Lexicon. COLING-
94. 
F. Karlsson, A. Voutilainen, J. Heikkila, A. Anttila 
(1995) Constraint Grammar: A Language-
independent System for Parsing Unrestricted Text. 
Mouton de Gruyter. 
L. Karttunen, J.P. Chanod, G. Grefenstette, A. Schiller 
(1997) Regular Expressions For Language 
Engineering. Natural Language Engineering. 
D. Kawahara, N. Kaji and S. Kurohashi (2000) 
Japanese Case Structure Analysis by Unsupervised 
Construction of a Case Frame Dictionary. COLING-
2000, Saarbrucken. 
A. Korhonen (2001) Subcategorization acquisition. 
Unpublished  PhD Thesis, University of Cambridge. 
K. Koskenniemi (1983) Two-level Morphology: A 
general Computational Model for Word-Form 
Recognition and Production. PhD thesis, University 
of Helsinki. 
J. Kuhn, J. Eckle-Kohlerm and C. Rohrer (1998) 
Lexicon Acquisition with and for Symbolic NLP-
Systems -- a Bootstrapping Approach. First 
International Conference on Language Resources and 
Evaluation (LREC98), Granada. 
C.D. Manning (1993) Automatic Acquisition of a 
Large Subcategorization Dictionary from Corpora. 
Proceedings of the 31th ACL. 
C.D. Manning and H. Sch?tze (1999) Foundations of 
Statistical Natural Language Processing. The MIT 
Press, Cambridge, Massachusetts.  
M. Maragoudakis, K. Kermanidis, N. Fakotakis and 
G. Kokkinakis (2001) Learning Automatic Acquisition 
of Subcategorization Frames using Bayesian 
Inference and Support Vector Machines. The 2001 
IEEE International Conference on Data Mining, 
IMDC'01, San Jos?. 
P. Merlo and M. Leybold (2001) Automatic 
Distinction of Arguments and Modifiers: the Case of 
Prepositional Phrases. EACL-2001, Toulousse. 
T. Pederssen (1996) Fishing for Exactness In the 
Proceeding of the South-Central SAS User Group 
Conference (SCSUG-96). 
C. Pollard and I. Sag (1987) An information based 
Syntax and Semantics, volume 13. CSLI lecture. 
Notes, Standford University. 
A. Sarkar and D. Zeman (2000) Automatic Extraction 
of Subcategorization Frames for Czech. COLING-
2000, Saarbrucken.  
C. Schutze (1995) PP Attachment and Argumenthood. 
MIT Working Papers in Linguistics. 
C. Verspoor (1997) Contextually-Dependent Lexical 
Semantics. PhD thesis, Brandeis University, MA. 
  Towards a Dependency Parser for Basque 
M. J. Aranzabe, J.M. Arriola and A. Diaz de Ilarraza, 
Ixa Group. (http://ixa.si.ehu.es) 
Department of Computer Languages and Systems 
University of the Basque Country 
P.O. box 649, E-20080 Donostia  
jibarurm@si.ehu.es 
 
 
Abstract 
We present the Dependency Parser, 
called Maxuxta, for the linguistic 
processing of Basque, which can serve 
as a representative of agglutinative 
languages that are also characterized by 
the free order of its constituents. The 
Dependency syntactic model is applied 
to establish the dependency-based 
grammatical relations between the 
components within the clause. Such a 
deep analysis is used to improve the 
output of the shallow parsing where 
syntactic structure ambiguity is not fully 
and explicitly resolved. Previous to the 
completion of the grammar for the 
dependency parsing, the design of the 
Dependency Structure-based Scheme 
had to be accomplished; we concentrated 
on issues that must be resolved by any 
practical system that uses such models. 
This scheme was used both to the 
manual tagging of the corpus and to 
develop the parser. The manually tagged 
corpus has been used to evaluate the 
accuracy of the parser. We have 
evaluated the application of the grammar 
to corpus, measuring the linking of the 
verb with its dependents, with 
satisfactory results. 
1 Introduction 
This article describes the steps given for the 
construction of a dependency syntactic parser 
for Basque (Maxuxta ). Our dependency 
analyser follows the constraint-based approach 
advocated by Karlsson (Karlsson, 1995). It 
takes as input the information obtained in the 
shallow parsing process (Abney, 1997). The 
shallow syntax refers to POS tagging and the 
chunking rules which group sequences of 
categories into structures (chunks) to facilitate 
the dependency analysis. The dependency 
parser is considered as the module involved in 
deep parsing (see Fig. 1). In this approach, 
incomplete syntactic structures are produced 
and, thus, the process goes beyond shallow 
parsing to a deeper language analysis in an 
incremental fashion (Aduriz et al, 2004). This 
allows us to tackle unrestricted text parsing 
through descriptions that are organized in 
ordered modules, depending on the depth level 
of the analysis (see Fig. 1).  
In agglutinative languages like Basque, it is 
difficult to separate morphology from syntax. 
That is why we consider morphosyntactic 
parsing for the first phase of the shallow 
syntactic analyser. 
CG
M
or
ph
os
yn
ta
ct
ic
pa
rs
in
g
Sy
nt
ac
tic
 
ta
gg
in
g
C
hu
nk
er
D
ep
en
de
nc
ie
s
EUSLEM 
Morpheus
Disambiguation using linguistic 
information
Disambiguation using statistical 
information
Shallow syntactic parsing
Named Entities
%
CG
PostpositionsCG
xfst
Noun and verb chainsCG
Tagging of syntactic dependenciesCG
Sh
al
lo
w
 
pa
rs
in
g
D
ee
p 
pa
rs
in
g
Raw data
Analysed text
 
Fig. 1. Syntactic processing for Basque. 
The dependency parser has been performed 
in order to improve the syntactic analysis 
 achieved so far, in the sense that, apart from 
the surface structural properties, we have 
added information about deeper structures by 
expressing the relation between the head and 
the dependent in an explicit manner. 
Additionally, we have adopted solutions to 
overcome problems that have emerged in 
doing this analysis (such as discontinuous 
constituents, subordinate clauses, etc. This 
approach has been used in several projects 
(J?rvinen & Tapanainen, 1998; Oflazer, 2003).  
Before carrying out the definition of the 
grammar for the parser, we established the 
syntactic tagging system in linguistic terms. 
We simultaneously have applied it to build the 
treebank for Basque (Eus3LB1) (Aduriz et al, 
2003) as well as to define the Dependency 
Grammar. The treebank would serve to 
evaluate and improve the dependency parser. 
This will enable us to check how robust our 
grammar is.  
The dependency syntactic tagging system is 
based on the framework presented in Carroll et 
al., (1998, 1999): each sentence in the corpus 
is marked up with a set of grammatical 
relations (GRs), specifying the syntactic 
dependency which holds between each head 
and its dependent(s). However, there are 
certain differences: in our system, arguments 
that are not lexicalised may appear in 
grammatical relations  (for example, the 
phonetically empty pro argument, which 
appears in the so-called pro-drop languages). 
The scheme is superficially similar to a 
syntactic dependency analysis in the style of 
Lin (1998). We annotate syntactically the 
Eus3LB corpus following the dependency-
based formalism. The dependencies we have 
defined constitute a hierarchy (see Fig. 2) that 
describes the theoretically and empirically 
relevant dependency tags employed in the 
analysis of the basic syntactic structures of 
Basque.  
                                                
1This work is part of a general project 
(http://www.dlsi.ua.es/projectes/3lb) which objective is to build 
three linguistically annotated corpora with linguistic annotation 
at syntactic, semantic and pragmatic levels: Cat3LB (for 
Catalan), Cast3LB (for Spanish) (Civit & Mart?, 2002) and 
Eus3LB (for Basque). The Catalan and the Spanish corpora 
include 100.000 words each, and the Basque Corpus 50.000 
words. 
This formalism is also used in the Prague 
Dependency Treebank for Czech (Hajic, 1998) 
and in NEGRA corpora for German (Brants et 
al., 2003) among others.  
 
dependant
structurally case
marked
complements
negation
linking-words
modifiers
auxiliary
others
semantics
non clausal
clausal
clausal
non
clausal
determiner
non clausal
clausal
predicative
finite
non finite
clausal
non
clausal
connector
apposition
graduator
particle
interjec.
ncsubj
nczobj
ncobj
ncmod
finite
non finite
detmod
xcomp_obj
xmod
xcomp_subj
cmod
ccomp_obj
ccomp_subj
ncmod
lot
auxmod
ncpred
non finite xpred
finite
non
finite
aponcmod
apocmod
apoxmod
gradmod
prtmod
itj_out
arg_mod
meta
galdemod
ccomp_zobj
xcomp_zobj
 
Fig. 2. Dependency relations hierarchy. 
Section 2 examines the main features of the 
language involved in the analysis in terms of 
dependency relations. Taking into account 
these features, we will explain the reasons for 
choosing the dependency-based formalism. In 
section 3 we briefly describe the general 
parsing system. Section 4 explains the 
dependency relations, the implementation of 
the dependency rules and a preliminary 
evaluation. Finally, some conclusions and 
objectives for future work are presented. 
 
2 A brief description of Basque in order 
to illustrate the adequacy of the adopted 
formalism 
Basque is an agglutinative language, that is, 
for the formation of words the dictionary entry 
independently takes each of the elements 
necessary for the different functions (syntactic 
case included). More specifically, the affixes 
corresponding to the determinant, number and 
declension case are taken in this order and 
independently of each other. These elements 
appear only after the last element in the noun 
phrase. One of the main characteristics of 
 Basque is its declension system with numerous 
cases, which differentiates it from languages 
spoken in the surrounding countries.  
At sentence level, the verb appears as the 
last element in a neutral order. That is, given 
the language typology proposed by Greenberg, 
Basque is a Subject-Object-Verb (SOV) type 
language (Laka, 1998) or a final head type 
language. However, this corresponds to the 
neutral order, but in real sentences any order of 
the sentence elements (NPs, PPs) around the 
verb is possible, that is, Basque can also be 
considered a language with free order of 
sentence constituents.  
These are the principal features that 
characterize the Basque language and, 
obviously, they have influenced us critically in 
our decision:  
 
1. The dependency-based formalism is the one 
that could best deal with the free word order 
displayed by Basque syntax (Skut et al, 
1997). 
2. We consider that the computational tools 
developed so far in our group facilitate 
either achieving dependency relations or 
transforming from dependency-trees to other 
modes of representation.  
3. From our viewpoint, it is less messy to 
evaluate the relation between the elements 
that compose a sentence rather than the 
relation of elements included in parenthesis. 
4. Dependency-based formalism provides a 
way of expressing semantic relations. 
3 Overview of the Syntactic Processing 
of Basque: from shallow parsing to deep 
parsing  
We face the creation of a robust syntactic 
analyser by implementing it in sequential rule 
layers. In most of the cases, these layers are 
realized in grammars defined by the Constraint 
Grammar formalism (Karlsson et al , 1995; 
Tapanainen & Voutilainen, 1994). Each 
analysis layer uses the output of the previous 
layer as its input and enriches it with further 
information. Rule layers are grouped into 
modules depending on the level of depth of 
their analysis. Modularity helps to maintain 
linguistic data and makes the system easily 
customisable or reusable.  
Figure 1 shows the architecture of the 
system, for more details, see Aduriz et al, 
2004. The shallow parsing of the text begins 
with the morphosyntactic analysis and ends 
delimiting noun and verb chains. Finally, the 
deep analysis phase establishes the 
dependency-based grammatical relations 
between the components within the clause.  
The parsing system is based on finite state 
grammars. The Constraint Grammar (CG) 
formalism has been chosen in most cases 
because, on the one hand, it is suitable for 
treating unrestricted texts and, on the other 
hand, it provides a useful methodology and the 
tools to tackle morphosyntax as well as free 
order phrase components in a direct way.  
A series of grammars are implemented 
within the module of the shallow parsing 
which aim:  
1. To be useful for the disambiguation of 
grammatical categories, removing incorrect 
tags based on the context. 
2. To assign and disambiguate partial syntactic 
functions. 
3. To assign the corresponding tags to delimit 
verb and noun chains. 
3.1 Shallow Syntactic Analyser 
The shallow or partial parsing analyser 
produces minimal and incomplete syntactic 
structures. The output of the shallow parser, as 
stated earlier, is the main base for the 
dependency parser. The shallow syntactic 
analyser includes the following modules: 
1. The morphosyntactic analyser MORFEUS. 
The parsing process starts with the outcome 
of the morphosyntactic analyser MORFEUS 
(Alegria et al, 1996), which was created 
following a two-level morphology 
(Koskenniemi, 1983). It deals with the 
parsing of all the lexical units of a text, both 
simple words and multiword units as a 
Complex Lexical Unit (CLU).  
2. The morphosyntactic disambiguation 
module EUSLEM. From the obtained 
results, grammatical categories and lemmas 
are disambiguated. Once morphosyntactic 
disambiguation has been performed, this  
module assigns a single syntactic function to 
each word.  
 3. The ckunk analysis module ZATIAK. This 
module identifies verb and noun chains 
based on the information about syntactic 
functions provided by each word-form. 
Entity names and postpositional phrases are 
also determined.  
We will focus on the last step of the shallow 
analysis because it contains the more 
appropriate information to make explicit the 
dependency relations. Basically, we use the 
syntactic functions and the chunks that are 
determined in the partial analysis. 
Shallow syntactic functions 
The syntactic functions that are determined 
in the partial analysis are based on those given 
in Aduriz et al, 2000. The syntactic functions 
employed basically follow the same approach 
to syntactic tags found in ENGCG 
(Voutilainen et al, 1992), although some 
decisions and a few changes were necessary. 
There are three types of syntactic functions:  
1. Those that represent the dependencies 
within noun chains (@CM>, @NC> etc.). 
2. Non-dependent or main syntactic functions 
(@SUBJ, @OBJ, etc.). 
3. Syntactic functions of the components of 
verb chains (@-FMAINV, @+FMAINV, 
etc.). 
The distinction of these three groups is 
essential when designing the rules that assign 
the function tags for verb and noun chains 
detection. 
Chunker: verb chain and noun chains 
After the morphological analysis and the 
disambiguation are performed (see Figure 1), 
we have the corpus syntactically analysed 
following the CG syntax. In this syntactic 
representation there are not phrase units. But 
on the basis of this representation, the 
identification of various kinds of phrase units 
such as verb chains and noun chains is 
reasonably straightforward.   
Verb chains  
The identification of verb chains is based on 
both the verb function tags (@+FAUXV, @-
FAUXV, @-FMAINV, @+FMAINV, etc.) and 
some particles (the negative particle, modal 
particles, etc.).  
There are two types of verb chains: 
continuous and dispersed verb chains (the 
latter consisting of three components at most). 
The following function tags have been defined: 
? %VCH: this tag is attached to a verb chain 
consisting of a single element. 
? %INIT_VCH: this tag is attached to the 
initial element of a complex verb chain. 
? %FIN_VCH: this tag is attached to the final 
element of a complex verb chain. 
The tags used to mark-up dispersed verb 
chains are: 
? %INIT_NCVCH: this tag is attached to the 
initial element of a non-continuous verb 
chain. 
? %SEC_NCVCH: this tag is attached to the 
second element of a non-continuous verb 
chain. 
? %FIN_NCVCH: this tag is attached to the 
fina l element of a non-continuous verb 
chain. 
Noun chains 
This module is based on the following 
assumption: any word having a modifier 
function tag has to be linked to some word or 
words with a main syntactic function tag. 
Moreover, a word with a main syntactic 
function tag can, by itself, constitute a phrase 
unit (for instance, noun phrases, adverbials and 
prepositional phrases). Taking into account this 
assumption, we recognise simple and 
coordinated noun chains, for which these three 
function tags have been established:  
? %NCH: this tag is attached to words with 
main syntactic function tags that constitute a 
phrase unit by themselves 
? %INIT_NCH: this tag is attached to the 
initial element of a phrase unit.  
? %FIN_NCH: this tag is attached to the final 
element of a phrase unit.  
Figure 3 shows part of the information 
obtained in the process of parsing the sentence 
Defentsako abokatuak desobedientzia 
zibilerako eskubidea aldarrikatu du epaiketan 
(The defense lawyer has claimed the right to 
civil disobedience in the  trial) with its 
corresponding chains tags.  
Let us know the some syntactic tags used in 
fig. 3: @NC>: noun complement; @CM>: 
modifier of the word carrying case in the noun 
 chain; @-FMAINV: non finite main verb; 
@+FAUXV: finite auxiliary verb and 
@ADVL: adverbial. 
"<Defentsako>" <INIT_CAP>"   defense  
     "defentsa" N @NC>  %INIT_NCH 
"<abokatuak>"  the lawyer  
      "abokatu" N @SUBJ  %FIN_NCH 
"<desobedientzia>"                       disobedience  
   "desobedientzia" N @CM> %INIT_NCH 
"<zibilerako>"                                to civil  
       "zibil" ADJ @<NC 
"<eskubidea>"                                the right  
       "eskubide" N @OBJ %FIN_NCH 
"<aldarrikatu>"                              claimed  
   "aldarrikatu" V @-FMAINV %INIT_VCH 
"<du>"                                            has  
   "*edun" AUXV @+FAUXV %FIN_VCH   
"<epaiketan>"                                 in the trial 
        "epaiketa" N @ADVL  %NCH  
"<$.>" <PUNCT_PUNCT>" 
Fig. 3. Analysis of chains. English translation on the 
right 
3.3 Deep Syntactic Analysis  
The aim of the deep syntactic analysis is to 
make explicit the dependency relations 
between words or chunks. For this reason, we 
have designed a Dependency Grammar based 
on the Constraint Grammar Formalism. 
4 The Dependency Grammar for the 
Parser  
In this section we describe in more detail the 
dependency relations defined (see fig. 2), the 
design of the rules and the results obtained. 
The results obtained in the deep parsing of 
sample sentence will help in providing a better 
understanding of the mentioned parsing 
process. This parsing process takes as basis the 
output of the shallow parser (see fig. 3). The 
rules are implemented by means of the CG-2 
parser (www.conexor.com). 
4.1 The dependency relations 
As Lin (2003) says a dependency 
relationship (Hays, 1964; Hudson, 1984; 
Mel?cuk, 1987; B?mov? et al, 2003) is an 
asymmetric binary relationship between a 
word called head (or governor, parent), and 
another word called modifier (or dependent, 
daughter). Dependency grammars represent 
sentence structures as a set of dependency 
relationships. Normally the dependency 
relationships form a tree that connects all the 
words in a sentence. A word in the sentence 
may have several modifiers, but each word 
may modify at most one word. The root of the 
dependency tree does not modify any word. It 
is also called the head of the sentence. 
For example, figure 4 describes the 
dependency structure of the example sentence. 
We use a list of tuples to represent a 
dependency tree. Each tuple represents one 
relation in the dependency tree. For example, a 
structurally case-marked complement when 
complements are nc (non-clausal, Noun 
Phrases, henceforth NP) has the following 
format: 
case : the case-mark by means of what the 
relation is established among the head and the 
modifier. 
head: the modified word head of 
NP/dependent: the modifier. In this case, the 
head of the NP. 
case-marked element within 
NP/dependent: the component of the 
dependent NP that carries the case. 
subj relationship: the label assigned to the 
dependency relationship. 
The syntactic dependencies between the 
components within the sentence are 
represented by tags starting with ?&?. The 
symbols ?>? and ?<? attached to each 
dependency-tag represent the direction in 
which we find the sentence component whose 
dependant is the target word.  
In the example we can see that the noun 
phrase defentsako abokatuak  ?the defense 
lawyer? depends on the verb aldarrikatu ?to 
claim?, which is on its right side. A post-
process will make this link explicit. 
The dependency tree in fig 4 is represented 
by the following tuples: 
 
Modifier Cat Head Type 
Defentsako 
abokatuak 
desobedientzia 
zibilerako 
eskubidea 
aldarrikatu 
du 
epaiketan 
N 
N 
N 
ADJ 
N 
V 
Aux 
N 
abokatuak  
aldarrikatu  
eskubidea  
desobedientzia 
aldarrikatu 
 
aldarrikatu 
aldarrikatu 
&NCMOD> 
&NCSUBJ> 
&NCMOD> 
&<NCMOD 
&NCOBJ> 
 
&<AUXMOD 
&<NCMOD 
 4.2 The dependency grammar rules  
The grammar consists of 255 rules that have 
been defined and distributed in the following 
way: 
 
complements modifiers 
nc2 cc3 det nc cm4 
others 
62 11 19 124 20 19 
 
These rules were formulated, implemented, 
and tested using a part of the manually 
disambiguated corpus (24.000 words). For the 
moment, part of the rest of the corpus was used 
for testing.  
For more details of the rules, we describe 
some examples that illustrate how dependency 
rules can be written to define different types of 
linguistic relations. 
 
1. Verb-subject dependency 
The following rule defines a verb-subject 
dependency relation between 2 words 
aldarrikatu (claimed) and abokatuak   (lawyer) 
of the sentence in the previous example:  
  
 MAP (&NCSUBJ>) TARGET (NOUN)  
   IF (0 (ERG) + (@SUBJ) +(%FIN_NCH)) 
      (*1(@-FMAINV) + (%INIT_VCH)  
       BARRIER (PUNCT_PUNCT)); 
 
The rule assigned the ncsubj tag to the noun 
abokatuak (lawyer) if the following conditions 
are satisfied: a) the noun is declined in ergative 
case; besides, it has assigned the @SUBJ 
syntactic function and, it is the last word of a 
noun chain; b) it has a non-finite main verb 
everywhere on its right before the punctuation 
mark. 
                                                
2 nc: non-clausal complement or modifier 
3 cc:clausal complement 
4 cm: clausal modifier 
 
2. Subordinate clause dependency 
The following rule defines a complement 
subordinate clause dependency relation 
between a subordinate verb and a main verb. 
We illustrate this rule by means of an example 
in which the word egoten (usually stayed) is 
the verb of the complement subordinate clause 
linked to esan (told): 
 
Example: Lehenago aitona egoten zela ni 
EGOTEN naizen tokian esan dit amonak5. 
 
 MAP(&CCOMP>>)TARGET (V)  
 IF(0(@-FMAINV)+ (%INIT_VCH)) 
(1(@+FAUXV_SUB)+ (%FIN_VCH)); 
 
The rule assigned the CCOMP tag to the 
verb egoten  (usually stayed) if the following 
conditions are satisfied: a) the verb is a non-
finite main verb and, it?s the first word-form of 
a verb chain; b) it has an auxiliary verb on its 
immediate right-side which has assigned the 
complement tag and appears as the last part of 
the verb chain.  
 
3. Infinitive control 
The following rule defines that in the 
sentence Jonek Miren etortzea nahi du. (John 
wants to come Mary), etortzea (infinitive 
subordinate clause with object function, "to 
come") is controled by the main verb nahi  ("to 
want"). Taking into account, that etortzea  is 
the controlled object of nahi, if there is another 
non-infinitive object Miren; then we will 
assign to it the subject dependency relation to 
the infinitive verb ("to come").   
  
                                               
5 My grandmother told me my grandfather 
usually stayed  where I am now 
epaiketan Defentsako abokatuak desobedientzia  zibilerako eskubidea aldarrikatu du 
Fig.4. Dependency tree 
 MAP (&NCSUBJ>) TARGET (NOUN)  
IF (0 (ABS) + (@SUBJ) OR (@OBJ)  + (%NCH))  
    (1(@-FMAINV_SUB_@OBJ) ) (2 VTRANS_ -FV )); 
  
4.3 Evaluation 
The system has been manually tested on a 
corpus of newspaper articles (included in 
Eus3LB), containing 302 sentences (3266 
words).  
We have evaluated the precision (correctly 
selected dependent / number of dependant 
returned) and the recall (correctly selected 
dependent / actual dependent in the sentence) 
of the subject (including coordinated subjects), 
and modifier dependency of verbs. For subject, 
precision and recall were respectively 67% and  
69 %, while the figures for verb modifiers were 
73 % and   95%. 
We have detected two main  reasons for 
explaining these figures: 1) the analysis 
strategy is limited because we cannot make use 
of semantic or contextual information for 
resolving uncertainties at an early level; 2) 
errors in previous steps. These errors can be a) 
due either to an incorrect assignment of POS to 
word-forms or to the syncretism of case marks 
(@SUBJ, @OBJ); b) the presence of non-
known word-forms that increases the number 
of possible analysis. At this moment, the head 
and dependent slot fillers are, in all cases, the 
base forms of single head words, so for 
example, ?multi-component? heads, such as 
names, are reduced to a single word; thus the 
slot filler corresponding to Xabier Arzallus 
would be Arzallus.  
5 Conclusions 
We have presented the application of the 
dependency grammar parser for the processing 
of Basque, which can serve as a representative 
of agglutinative languages with free order of 
constituents.  
We have shown how dependency grammar 
approach provides a good solution for deeper 
syntactic analysis, being at this moment the 
best alternative for morphologically complex 
languages.  
We have also evaluated the application of 
the grammar to corpus, measuring the linking 
of the verb with its dependents, with 
satisfactory results. However, the development 
of a full dependency syntactic analyser is still a 
matter of research.  For instance, all kinds of 
constructions without a clear syntactic head are 
difficult to analyse: ellipses, sentences without 
a verb (e.g., copula -less predicative), and 
coordination. All these aspects have been 
treated in our manually annotated Corpus; our 
efforts now are oriented to deal with them 
automatically. 
 
6 Acnowledgments  
This research is supported by the University 
of the Basque Country (9/UPV00141.226-
14601/2002), the Ministry of Industry of the 
Basque Government (project XUXENG, 
OD02UN52). 
References  
Abney S. P. 1997. Part-of-speech tagging and 
partial parsing. S. Young and G. Bloothooft, 
editors,  Corpus -Based Methods in Language 
and Speech Processing, Kluwer, Dordrecht. 
Aduriz I., Aranzabe M.J., Arriola J.M.,  D?az 
de Ilarraza A., Gojenola K., Oronoz M., Ur?a 
L. 2004. A Cascaded Syntactic Analyser for 
Basque. In Gelbukh, A (ed.) Computational 
Linguistics and Intelligent Text Processing. 
SpringerLNCS 2945.  
Aduriz I., Aranzabe M.J., Arriola J.M., Atutxa 
A., D?az de Ilarraza A., Garmendia A., 
Oronoz M. 2003. Construction of a Basque 
Dependency Treebank. Proceedings of the 
Second Workshop on Treebanks and 
Linguistic Theories "TLT 2003", (J. Nivre 
and E. Hinrichs eds.), V?xj? University 
Press. V ?xj?, Suecia   
Aduriz I., Arriola J.M., Artola X., Diaz de 
Illarraza A., Gojenola K., Maritxalar M. 
2000. Euskararako Murriztapen Gramatika: 
mapaketak, erregela morfosintaktikoak eta 
sintaktikoak. UPV/EHU/LSI/TR 12-2000.  
Alegria I., Artola X., Sarasola K., Urkia M. 
1996. Automatic morphological analysis of 
Basque. Literary & Linguistic Computing 
Vol. 11, No. 4, 193-203. Oxford University 
Press. Oxford. 
 B?mov? , A., Haji?c, J., Hajicov?a, E., 
Hladk?a, B. 2003. The Prague 
DependencyTreebank: A Three level 
Annotation Scenario. In Abeill? (ed.) 
Treebanks Building and Using Parsed 
Corpora, Book Series: TEXT, SPEECH 
AND LANGUAGE TECHNOLOGY : 
Volume 20 Kluwer Academic Publisher, 
Dordrecht. 
Brants T., Skut W. & Uszkoreit H. 2003 
"Syntactic Annotation of a German Newspa-  
per Corpus?. In Abeill? (ed.) Treebanks 
Building and Using Parsed Corpora, Book 
Series: TEXT, SPEECH AND LANGUAGE 
TECHNOLOGY : Volume 20 Kluwer 
Academic Publisher, Dordrecht. 
Carroll J., Briscoe E., Sanfilippo A. 1998. 
Parser evaluation: a survey and a new 
proposal. Proceedings of the 1st 
International Conference on Language 
Resources and Evaluation, 447-454. 
Granada, Spain.  
Carroll J., Minnen G., Briscoe T. 1999. Corpus 
Annotation for Parser Evaluation. 
Proceedings of Workshop on Linguistically 
Interpretated Corpora, EACL?99. Bergen. 
Civit M. & Mart? M. 2002. Design Principles 
for a Spanish Treebank. Proceedings of The 
Treebank and Linguistic Theories 
(TLT2002). Sozopol, Bulgaria. 
Hays, D. 1964. Dependency theory: a 
formalism and some observations. 
Language40, p. 511?525. 
Hajic J. 1998. Building a Syntactically 
Annotated Corpus: The Prague Dependency 
Treebank. In  Issues of Valency and 
Meaning, 106-132. Karolinum, Praha. 
Hudson, R. 1984. Word Grammar. Oxford, 
England: Basil Blackwell PublishersLimited. 
J?rvinen T. and Tapanainen P, 1998. Towards 
an implementable dependency grammar. In 
Proceedings of the Workshop "Processing of 
Dependency-Based Grammars", (eds.) 
Sylvain Kahane and Alain Polgu?re, 
Universit? de Montr?al, Quebec, Canada, 
15th August 1998, pp. 1-10. 
Karlsson F., Voutilainen A., Heikkila J., 
Anttila A. 1995. Constraint Grammar: a 
Language-Independent System for Parsing 
Unrestricted Text. Mouton de Gruyter. 
Koskenniemi K 1983. Two-level Morphology: 
A general Computational Model for Word-
Form Recognition and Production. 
University of Helsinki, Department of 
General Linguistics. Publications 11.  
Laka, I. 1998. A Brief Grammar of Euskara, 
the Basque Language. HTML document. 
http://www.ehu.es/grammar. Office of the 
Vice-Dean for the Basque Language. 
University of the Basque Country. 
Lin D. 1998. A Dependency-based Method for 
Evaluating Broad-Coverage Parsers. Natural 
Language Engineering.  
Lin D. 2003. "Dependency-based evaluation of 
MINIPAR" in Building and Using 
syntactically annotated corpora, Abeill?, A. 
Ed. Kluwer, Dordrecht 
Mel?cuk, I. A. 1987. Dependency syntax: 
theory and practice. Albany: StateUniversity 
of New York Press. 
Oflazer K. 2003. Dependency Parsing with an 
Extended Finite-State Approach. ACL 
Journal of Computational Linguistics, Vol. 
29, n?4. 
Skut W., Krenn B., Brants T., Uszkoreit H. 
1997. An Annotation Scheme for Free Word 
Order Languages. In Proceedings of the 
Fifth Conference on Applied Natural 
Language Processing (ANLP-97). 
Washington, DC, USA. 
Tapanainen P. and Voutilainen A. 1994 
Tagging Accurately-Don?t guess if you know. 
In Proceedings of the 4th Conference on  
Applied Natural Language Processing, 
Washington. 
Voutilainen A., Heikkil? J. and Anttila A. 
1992. Constraint Grammar of English. A 
Performance-Oriented Introduction. 
Publications of Department of General 
Linguistics, University of Helsinki, No. 21, 
Helsinki. 
 
