Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 17?23,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Understanding Eggcorns
Sravana Reddy
Department of Computer Science
The University of Chicago
sravana@cs.uchicago.edu
Abstract
An eggcorn is a type of linguistic error where
a word is substituted with one that is seman-
tically plausible ? that is, the substitution is
a semantic reanalysis of what may be a rare,
archaic, or otherwise opaque term. We build
a system that, given the original word and its
eggcorn form, finds a semantic path between
the two. Based on these paths, we derive a ty-
pology that reflects the different classes of se-
mantic reinterpretation underlying eggcorns.
1 Introduction
The term ?eggcorn? was coined in 2003 by Geof-
frey Pullum (Liberman, 2003) to refer to a certain
type of linguistic error where a word or phrase is
replaced with one that is phonetically similar and
semantically justifiable. The eponymous example is
acorn? eggcorn, the meaning of the latter form be-
ing derived from the acorn?s egg-like shape and the
fact that it is a seed (giving rise to corn). These er-
rors are distinct from mere misspellings or mispro-
nunciations in that the changed form is an alternate
interpretation of the original.
The reinterpretation may be related to either the
word?s perceived meaning or etymology (as in the
case of acorn), or some context in which the word is
commonly used. In this sense, eggcorns are similar
to folk etymologies ? errors arising from the misin-
terpretation of borrowed or archaic words ? with the
difference being that the latter are adopted by an en-
tire culture or linguistic community, while eggcorns
are errors made by one or more individual speakers.
The formation of eggcorns and folk etymolo-
gies, mistakes though they are, involves a creative
leap within phonetic and semantic constraints (much
like what is required for puns or certain classes of
jokes). Eggcorns range from simple reshapings of
foreign words (paprika ? pepperika) and substitu-
tions from similar domains (marshal? martial), to
the subtly clever (integrate? intergrade), the tech-
nological (sound bite ? sound byte), or the funny
(stark-raving mad? star-craving mad). The source
of reinterpretation may be a weak imagined link
(wind turbine? wind turban), or an invented myth
(give up the ghost? give up the goat1). And often,
it is not clear what the exact link is between the de-
rived and the original forms, although it is usually
obvious (to the human eye) that there is a connec-
tion.
This paper explores some ways of automatically
tracing the link between a word and its eggcorn.
In reality, we are chiefly concerned with comput-
ing the connections between a word and its rein-
terpreted form. Such pairs may also occur as folk
etymologies, puns, riddles, or get used as a poetic
device. However, we use eggcorns as a testbed
for three main reasons: there are a number of doc-
umented examples, the reanalyses are accidental
(meaning the semantic links are more unpredictable
and tenuous than in the cases of deliberate reshap-
ings), and the errors are idiosyncratic and relatively
modern ? and hence have not been fossilized in the
lexicon ? making them transparent to analysis (as
opposed to many folk etymologies and other histor-
ical errors). That said, much of the work described
here can be potentially applied to other instances of
semantic reinterpretation as well.
1http://eggcorns.lascribe.net/english/
714/goat/
17
The first part of the paper describes an algorithm
(the ?Cornalyzer?) for finding a semantic path be-
tween the original and reinterpreted forms of an
eggcorn pair. We then proceed to use the results
of this algorithm to cluster the eggcorn examples
into 5 classes, with a view to learning a typology
of eggcorns.
2 Related Work
One work related to this area (Nelken and Ya-
mangil, 2008) uses Wikipedia to automatically mine
eggcorns by searching for pairs of phonemically
similar words that occur in the same sentence con-
text in different revisions. However, the mined ex-
amples are reported to contain many false positives
since the algorithm does not include a notion of se-
mantic similarity.
Folk etymologies, the closest cousin to eggcorns,
have been studied from a linguistic point of view, in-
cluding some of the same questions we tackle here
(only, not from a computational side) ? how is an
new word derived from the original, and what are
the different categories of folk etymologies? (Rund-
blad and Kronenfeld, 1998), (Scholfield, 1988). To
the best of our knowledge, there has been no pre-
vious work in inducing or computationally under-
standing properties of neologisms and errors de-
rived through misinterpretation. However, there is
a substantial literature on algorithmic humor, some
of which uses semantic relationships ? (Stock and
Strapparava, 2006), (Manurung et al, 2008), among
others.
3 Data
The list of eggcorns is taken directly from the
Eggcorn Database2 as of the submission date. To
assure soundness of the data, we include only those
examples whose usage is attested and which are con-
firmed to be valid and contemporary reanalyses3,
giving a total of 509 instances. Table 1 shows a sam-
ple of the data.
Every example can be denoted by the tuple
(w, e, c) where c is the list of obligatory contexts in
2http://eggcorns.lascribe.net/
3In other words, all examples that are classified as ?ques-
tionable? (or otherwise indicated as being questionable), ?not an
eggcorn?, ?citational? or ?nearly-mainstream? are eliminated.
Table 1: A few eggcorns. ?X? can be replaced for w or
e to give the original form in context, or the eggcorn in
context respectively.
Original Changed Context
form w form e c
bludgeon bloodgeon X
few view name a X
entree ontray X
praying preying X mantis
jaw jar X-dropping
dissonance dissidence cognitive X
which the reanalysis takes place, w is the original
form, and e is the modified (eggcorned) form.
The Cornalyzer uses WordNet (Fellbaum, 1998)
version 3.0, including the built-in morphological
tools for lemmatization and dictionary definitions4.
4 Automated Understanding of Eggcorn
Generation
Broadly speaking, there are two types of eggcorns:
1. Ones where e or a part of e is semantically re-
lated to the original word w (lost ? loss in
?no love lost?) or the context c (pied ? pipe
in ?pied-piper?).
2. Eggcorns where e is related to an image or ob-
ject that is connected to or evoked by the origi-
nal (like ?song? in lip-sync? lip-sing).
For the first, a database of semantic relations be-
tween words (like WordNet) can be used to find a
semantic connection between w and e. The sec-
ond type is more difficult since external knowledge
is needed to make the connection. To this end, we
make use of the ?glosses? ? dictionary definitions of
word senses ? included in WordNet. For instance,
the ?lip-sing? eggcorn is difficult to analyze using
only semantic relations, since neither ?sync? nor ?lip?
are connected closely to the word ?sing?. However,
the presence of the word song in the gloss of lip-
sync:
move the lips in synchronization
(with recorded speech or song)
4From http://wordnet.princeton.edu/
18
makes the semantic connection fairly transparent.
The Cornalyzer first attempts to analyze an
eggcorn tuple (w, e, c) using semantic relations
(?4.1). If no sufficiently short semantic path is
found, the eggcorn is presumed to be of the second
type, and is analyzed using a combination of seman-
tic relations and dictionary glosses (?4.2).
4.1 Analysis using Word Relations
4.1.1 Building the Semantic Graph
WordNet is a semantic dictionary of English, con-
taining a list of synsets. Each synset consists of a
set of synonymous words or collocations, and its re-
lations (like hypernymy, antonymy, or meronymy)
with other synsets. The dictionary also includes lex-
ical relations ? relations between words rather than
synsets (for instance, a pertainym of a noun is an
adjective that is derived from the noun).
WordNet relations have been used to quantify se-
mantic similarity between words for a variety of ap-
plications (see Budanitsky and Hirst (2001) for a re-
view of similarity measures). The Cornalyzer uses
the same basic idea as most existing measures ? find-
ing the shortest path between the two words ? with
some modifications to fit our problem.
We adopt the convention that two words w1 and
w2 have the relation R if they are in different synsets
S1 and S2, and R(S1, S2) is true. We also define two
new lexical relations that are not directly indicated
in the dictionary: w1 and w2 are synonyms if they
are in the same synset, and homographs if they have
identical orthographic forms and lexical categories
but are in different synsets. 5
This relational network can hence be used to de-
fine a graph Gs over words, where there is an edge
of type tR from w1 to w2 if R(w1, w2) holds. Some
of the relations in WordNet (like antonymy) are ig-
nored, either because they invert semantic similarity,
or are not sufficiently informative. Table 2 summa-
rizes the relations used.
This graph can be used to find the semantic re-
lationships between an original word w and its
5This paper uses ?word? to include sense ? i.e, ?bank? as in
slope beside a body of water and ?bank? as in financial institu-
tion are distinct. When required for disambiguation, the Word-
Net sense number, which is the index of the sense in the list of
the word?s senses, is added in parenthesis; e.g. bank (2) for the
financial institution sense.
Table 2: WordNet relations used to build the semantic
graph.
Relation Parts of Reflexive Example
Speech Relation
Synonym (N, N) Synonym (forest, wood)
(V, V) (move, displace)
(Adj, Adj) (direct, lineal)
(Adv, Adv) (directly, at once)
Homograph (All, All) Homograph (call [greet],
call [order])
Hypernym (V, V) Troponym/ (move, jump)
(N, N) Hyponym (canine, fox)
Meronym (N, N) Holonym (forest, tree)
Has Instance (N, N) Instance Of (city, Dresden)
Cause (V, V) Caused by (affect, feel)
Entails (V, V) not specified (watch, look)
Similar To (Adj, Adj) Similar To (lucid, clear)
Related (V, V) Related
(Adj, Ad) (few, some)
Same Group (V, V) Same Group (displace, travel)
Has Attribute (Adj, N) Attribute Of (few, numerousness)
Derivational (N, V) Derivational (movement, move)
Relation (N, Adj) Relation (movement, motional)
(V, Adj) (move, movable)
Pertainym (Adj, N) not specified (direct, directness)
(Adv, Adj) (directly, direct)
eggcorn form e, if both forms are in the dictionary,
and there exists a path from w to e. However, it is
often the case that e or w are not in the dictionary,
or that a path does not exist. This could be because
one of the forms is an inflected form or compound,
or that some substring of e ? rather than the whole
word or collocation ? is the reinterpreted segment.
It is also essential to consider the strings in c, since
many eggcorns result from semantic reinterpretation
of the contexts.
Hence, three new non-semantic relations are de-
fined: w1 is a substring of w2 if the orthographic
form of w1 is a substring of that of w2, and w1 and
w2 are contextually linked if they occur in the same
collocation or compound. If w2 can be derived from
w1 using WordNet?s lemmatizer, w2 is an inflected
form of w1.
A new graph Ge is constructed by adding edges
of types tsubstring, tcontext, and tinflect to Gs. For
all eggcorn tuples (w, e, c):
1. If e or w are not in the dictionary, add them to
Ge as a vertex
2. Add edges of type inflect between e and its base
form.
3. Add edges of type substring from e to every
19
substring of length ? 3 that is in the dictionary
(except those substrings which are base forms
of e), and edges of type supstring in the other
direction.
4. Extract a set of ?context words? from c by split-
ting it along spaces and hyphenation. Select
those words which are in the dictionary.
5. Add edges of type context from w and e to each
extracted context word.
For example, given the data in table 1, the follow-
ing vertices and edges will be added to Ge:
Vertices bloodgeon, ontray, preying, praying
Substring edges (bloodgeon, blood), (bloodgeon, loo),
(bloodgeon, eon), (view, vie), (entree, tree), (ontray,
ray), (ontray, tray)
Superstring edges above edges in the other direction
Inflectional edges (preying, prey), (praying, pray).
These edges are bidirectional.
Context edges (few, name), (view, name), (few, a),
(view, a), (praying, mantis), (preying, mantis), (jaw,
dropping), (jar, dropping), (dissonance, cognitive),
(dissidence, cognitive). These edges are also bidi-
rectional.
4.1.2 Tracing the Semantic Path
Given the semantic graph, our working assump-
tion is that e is generated from w by following the
shortest path from w to e (denoted by P (w, e, c)).
1. If w and e are both in the dictionary, find
P1(w, e) = the shortest path from w to e in Gs
2. Find P2(w, e, c) = the shortest path using sub-
strings of e and/or c in Ge
(Since the edges are unweighted, the shortest path
from w to e is found simply by performing breadth-
first search starting at w.)
P (w, e, c) is simply the shorter of P1(w, e) (if it
exists) and P2(w, e, c). Note that there may be sev-
eral shortest paths, especially since words that are
synonymous have almost the same incident semantic
edges. Since the candidate shortest paths generally
do not differ much from one another (as far as their
semantic implications), an arbitrary path is chosen
to be P .
Table 3 shows the paths found by the algorithm
for some eggcorns.
4.2 Analysis using Dictionary Definitions
As described in ?4, the source of many eggcorns
is knowledge external to the original word or con-
texts through some concept or object suggested by
the original. In such cases, a semantic network will
not suffice to find the reinterpretation path. One pos-
sible way of accessing the additional information is
to search for w and e in a large corpus, and extract
the key words that appear in conjunction with these
forms.
However, filtering and extracting the represen-
tative information can quickly become a complex
problem beyond the scope of this paper. Hence, as a
first approximation, we use the dictionary definitions
(glosses) that accompany synsets in WordNet. To
optimize efficiency and to avoid having noise added
by the definitions, the Cornalyzer only resorts to this
step if a sufficiently short path ? that is, a path of
length? k for some threshold k ? is not found when
only using word relations. (The results suggest 7
as a good threshold, since most of discovered paths
that are longer than 7 tend not to reflect the semantic
relationships between the eggcorn and the original
form.)
Every gloss from all senses of a lexical item6 x
(for all x in the dictionary) is first tokenized, and
punctuation stripped. All tokens are stemmed using
the built-in lemmatizer. Only those tokens t that are
already present as vertices in Ge are taken into con-
sideration. However, it should be clear that not all
tokens t are equally relevant to x. For instance, con-
sider one gloss of the noun ?move?:
the act of changing location from one
place to another
which gives the tokens act, changing, location,
one, place, another. Clearly, the tokens changing,
location, and place rank higher than the others in
terms of how indicative they are of the meaning of
the noun.
6A lexical item is a word independent of sense, e.g, all
senses of ?bank? constitute a single lexical item.
20
Table 3: A sample of semantic similarity paths. x R?? y means ?y is an R of x?. When relevant, WordNet sense
numbers are indicated.
Eggcorn tuple Path from word to eggcorn
(word, eggcorn, context)
(mince, mix, ?X words?) mince hypernym???????? change hyponym??????? mix
(few, view, ?name a X?) few deriv???? fewness hypernym???????? number hypernym???????? amount
hypernym???????? magnitude hyponym??????? extent hyponym??????? scope hyponym??????? view
(dissonance, dissidence, dissonance synonym??????? disagreement (1) homograph???????? disagreement (3)
cognitive X) hyponym??????? dissidence
(ado, [to-do, to do], [?much X ado synonym??????? stir (3) homograph???????? stir (1) hypernym???????? to-do
about nothing?, ?without further X?])
(jaw, jar, X-dropping) jaw context?????? dropping inflect?????drop hypernym????????displace hyponym???????jar
(ruckus, raucous, X) ruckus homograph???????? din deriv???? cacophonous similar??????raucous
(segue, segway, X) segue hypernym???????? passage (1) homograph???????? passage (3) hypernym????????
way supstring??????? segway
One way of reflecting these distinctions in the
Cornalyzer is to weight these terms appropriately,
with something resembling the TF-IDF (Salton and
Buckley, 1988) measure used in information re-
trieval. Let tf(t, x) = the frequency of the to-
ken t in the glosses of x, and idf(t) = log Ndf(t)where N = the number of lexical items in the dic-
tionary and df(t) = the number of lexical items
in the dictionary whose glosses contain t. Define
W (t, x) = tf(t, x) ? idf(t).
A new graph Gd is constructed from Ge by adding
edges of type hasdef from every lexical item x to
tokens t in its glosses with the edge-weight 1 +
1/W (t, x), and reflexive edges of type indef from
t to x with the same weight. All existing edges in
the original graph Ge are assigned the weight 1.
The semantic path from w to e is found by the
process similar to what was described in ?4.1.2:
first find P1(w, e) and P2(w, e, c) as well as
P3(w, e, c) = the shortest path from w to e in Gd,
and let P (w, e, c) be the shortest of the three. Since
Gd has weighted edges, the shortest path P3 is com-
puted using Dijkstra?s algorithm.
Dictionary-definition-based paths P2 for some
eggcorns are shown in Table 4. The shortest P2 paths
are also shown for comparison. The P3 paths gener-
ally appear to be closer to a human judgment of what
the semantic reinterpretation constitutes. In the case
of (bludgeon? bloodgeon), for example, P2 shows
no indication of the key connection (bleeding due to
being bludgeoned), whereas P3 captures it perfectly.
Of the 509 eggcorns, paths were found for 238
instances by using only Gs or Ge as the relational
graph. Paths for a total of 372 eggcorns were found
when using dictionary glosses in the graph Gd.
5 From Generation to Typology
A quick glance at tables 3 and 4 shows that the paths
vary in shape and structure: some paths move up
and down the hypernym/homonym tree, while oth-
ers move laterally along synonyms and polysemes;
some use no external knowledge, while others make
primary use of context information and dictionary
glosses. A natural next step, therefore, is to group
the eggcorns into some number of classes that rep-
resent general categories of semantic reanalysis. We
can achieve this by clustering eggcorns based on
their semantic shortest paths.
5.1 Clustering of Paths
One natural choice for a feature space is the set of all
24 relations (edge-types) used in Gd. An eggcorn
(w, e, c) is represented as a vector [v1, v2, . . . v24]
where vi = the number of times that relation Ri (or
the reflexive relation of Ri) appears in P (w, e, c).
These vectors are then clustered using k-means
21
Table 4: Some semantic paths using dictionary glosses. As before, x R?? y stands for ?y is an R of x?, and the numbers
in parentheses following a lexical item are the WordNet sense numbers corresponding to that word.
Eggcorn tuple Path from word to eggcorn
(bludgeon, bloodgeon, X) P3 (length 6): bludgeon hypernym???????? hit (3) homograph???????? hit (6) hypernym???????? wound
indef????? gore hypernym???????? blood supstring??????? bloodgeon
P2 (length 11): bludgeon hypernym???????? club hypernym???????? stick hypernym???????? implement
hypernym???????? instrumentality hypernym???????? artefact hyponym??????? structure hyponym??????? area
hyponym???????room hyponym???????lavatory hyponym??????? loo supstring??????? bloodgeon
(entree, [ontray, on-tray], X) P3 (length 4): entree indef????? meal indef????? food hasdef????? tray supstring??????? ontray
P2 (length 8): entree hyponym??????? plate (8) homograph???????? plate (4) hypernym???????? flatware
hypernym???????? tableware hyponym??????? tea set meronym??????? tea tray hypernym???????? tray
supstring??????? on-tray
(praying, preying, X mantis) P3 (length 6): praying context?????? mantis indef????? predacious synonym??????? predatory (3)
homograph???????? predatory (2) indef????? prey inflect????? preying
P2 (length 8): praying context?????? mantis hypernym???????? dictyopterous insect
hypernym???????? insect hypernym???????? arthropod hypernym???????? invertebrate hypernym???????? animal
hyponym??????? prey inflect????? preying
and a Euclidean distance metric. We experimented
with a few different values of k and found that k =
5 produces clusters that are the most semantically
coherent.
5.2 Results
The five clusters roughly correspond to the each of
the following characteristic paths P (w, e, c):
1. Independent of dictionary glosses and of con-
text, and mostly contain synonym, homograph,
related, or similar to types of edges.
2. Contain several hypernym and hyponym edges.
3. Contain several substring, supstring, and inflect
or derivational edges.
4. Heavily dependent on context edges.
5. Heavily dependent on dictionary glosses.
Eggcorns in these clusters can be interpreted to
be (1) Near-synonyms, (2) Semantic cousins ? de-
riving from a common general concept or entity,
(3) Segmentally related ? being linked by morpho-
logical operations, (4) Contextually similar, or (5)
Linked by implication ? deriving from an implicit
concept.
A sample of the cluster membership is shown in
Table 5.
6 Discussion
This paper presents a procedure for computationally
understanding the semantic reanalyses of words. We
identified the two general types of eggcorns, and
built the appropriate networks overlying the Word-
Net graph and dictionary in order to trace the se-
mantic path from a word to its eggcorn.
An obvious drawback to our method stems from
the fact that the semantic dictionary is not perfect,
or fully reflective of human information. Similarly,
dictionary glosses are a limited source of external in-
formation. It would hence be worth exploring data-
driven methods to augment a source like WordNet,
such as building a word graph from co-occurrences
in text, or using corpora to derive distributional sim-
ilarity measures.
The Cornalyzer is only an exploratory first step
? there are a wealth of other possible computa-
tional problems related to eggcorns. Semantic path-
finding can be extended to defining some measure of
eggcorn strength or plausibility. The algorithm can
also be used to mine for new eggcorns ? a thresh-
old or a set of criteria for an ?eggcornish? path can
22
Table 5: A look at the clustered eggcorns.
Cluster Examples
1 (cognitive dissonance? cognitive dissidence), (ado? to-do), (slake thirst? slack thirst),
(ruckus? raucous), (sparkle (protests, etc)? spark), (poise to do? pose to do), ...
2 (sow wild oats? sow wild oaks), (name a few? name a view), (whet, wet),
(curb hunger? curve hunger), (entree? ontray), (mince words? mix words), ...
3 (utmost? upmost), (valedictorian? valevictorian), (quote unquote? quote on quote),
(playwright? playwrite), (no love lost? no love loss), (snub? snob), ...
4 (pied piper? pipe piper), (powerhouse? powerhorse), (jaw-dropping? jar-dropping),
(sell (something) down the river? sail (something) down the river), ...
5 (renowned, reknowned), (praying mantis? preying mantis), (expatriate? expatriot),
(skim milk? skimp milk), (sopping wet? soaping wet), (pique? peak), ...
be set based on the paths found for known eggcorns,
thus helping separate them from false positives (ty-
pos and misspellings).
Another possible line of work is finding general-
izations in pronunciation changes from the original.
?The Eggcorn Database? website includes a partial
catalogue of phonetic changes like t-flapping and
cot/caught merger ? it would be interesting to see if
such patterns and categories can be learnt. The basic
model of the Cornalyzer can potentially also be ex-
tended to applications in other domains of semantic
reanalysis like folk etymologies and puns.
Acknowledgments
We would like to thank the anonymous reviewers for
their excellent and insightful comments.
References
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet:an experimental, application-
oriented evaluation of five measures. In Proceedings
of the ACL Workshop on WordNet and Other Lexical
Resources.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Mark Liberman. 2003. Egg corns: folk et-
ymology, malapropism, mondegreen, ???
http://158.130.17.5/ myl/languagelog/archives/000019.html.
Ruli Manurung, Graeme Ritchie, Helen Pain, Annalu
Waller, Dave O?Mara, and Rolf Black. 2008. The con-
struction of a pun generator for language skills devel-
opment. Applied Artificial Intelligence, 22:841?869.
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedia?s article revision history for training com-
putational linguistics algorithms. In Proceedings of
the AAAI Workshop on Wikipedia and Artificial Intel-
ligence.
Gabriella Rundblad and David B Kronenfeld. 1998.
Folk-etymology: Haphazard perversion or shrewd
analogy? In Julie Coleman and Christian Kay, edi-
tors, Lexicology, Semantics, and Lexicography. John
Benjamins, Manchester.
Gerard Salton and Christopher Buckley. 1988. Term
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Phil Scholfield. 1988. Documenting folk etymological
change in progress. English Studies, 69:341?347.
Oliviero Stock and Carlo Strapparava. 2006. Laughing
with hahacronym, a computational humor system. In
Proceedings of the 21st AAAI Conference on Artificial
Intelligence.
23
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 92?95,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Substring-based Transliteration with Conditional Random Fields
Sravana Reddy and Sonjia Waxmonsky
Department of Computer Science
The University of Chicago
Chicago, IL 60637
{sravana, wax}@cs.uchicago.edu
Abstract
Motivated by phrase-based translation research,
we present a transliteration system where char-
acters are grouped into substrings to be mapped
atomically into the target language. We show how
this substring representation can be incorporated
into a Conditional Random Field model that uses
local context and phonemic information.
1 Introduction
We present a transliteration system that is moti-
vated by research in phrase-based machine trans-
lation. In particular, we borrow the concept of
phrases, which are groups of words that are trans-
lated as a unit. These phrases correspond to multi-
character substrings in our transliteration task.
That is, source and target language strings are
treated not as sequences of characters but as se-
quences of non-overlapping substrings.
We model transliteration as a sequential label-
ing task where substring tokens in the source lan-
guage are labeled with tokens in the target lan-
guage. This is done using Conditional Random
Fields (CRFs), which are undirected graphical
models that maximize the posterior probabilities
of the label sequence given the input sequence. We
use as features both local contexts and phonemic
information acquired from an English pronuncia-
tion dictionary.
2 The Transliteration Process
Our transliteration system has the following steps:
1. Pre-processing of the target language.
2. Substring alphabet generation for both the
source and target. This step also generates
training data for the CRFs in Step 3 and 4.
3. CRF training on aligned data from Step 2.
4. Substring segmentation and translitera-
tion of source language input.
Our training and test data consists of three sets ?
English to Hindi, English to Kannada, and English
to Tamil (Kumaran and Kellner, 2007) ? from the
NEWS 2009 Machine Transliteration Shared Task
(Li et al, 2009).
2.1 Step 1: Pre-Processing
The written words of Hindi, Tamil, and Kannada
correspond almost perfectly to their phonological
forms, with each character mapping to a phoneme.
The only exception to this arises from the implicit
vowel (which may be a schwa /@/ or a central
vowel /5/) that is inserted after consonants that
are not followed by the halanta or ?killer stroke?.
Hence, any mappings of an English vowel to a
target language schwa will not be reflected in the
alignment of the named entity pair.
To minimize misalignments of target language
strings with the English strings during training,
we convert the Indic abugida strings to an in-
ternal phonemic representation. The conversion
maps each unicode character to its correspond-
ing phonemic character and inserts a single sym-
bol (representing the schwa/central vowel) after all
consonants that are not followed by the halanta.
These phoneme sequences are used as the in-
ternal representation of Indic character strings for
all later steps in our system. Once transliteration
is complete, the phonemic symbols are converted
back to unicode by reversing the above process.
2.2 Step 2: Substring alphabet generation
Our decision to use substrings in the transliteration
task is motivated by the differences in orthography
and phonology between the target and source lan-
guages, which prevent trivial one-to-one character
level alignment. We first discuss the cause of the
poor character alignment between English and the
92
Indic languages, and then describe how we trans-
form the input into substring representation.
English uses several digraphs for phonemes
that are represented by single characters in Indic
scripts, which are either part of standard ortho-
graphic convention (oo, ch, etc.), or necessitated
by the lack of a single phoneme that approximates
an Indic one (as in the case of aspirated conso-
nants). Conversely, English sometimes uses a sin-
gle character for a biphone (such as x for /ks/, or
u for /ju/ as in museum), which is represented by
two characters in the target languages. In certain
cases, a digraph in English is transliterated to a di-
graph in the target, as a result of metathesis (le ?
/@l/, in words like temple). Further, all three tar-
get languages often insert vowels between English
consonant clusters; for instance, Hindi inserts a
schwa between s and p in ?transport?, transliter-
ated as ?rAns@por? (V~ A\spoV).
To handle these cases, we borrow the concept of
phrases from machine translation (Och and Ney,
2004), where groups of words are translated as a
unit. In the case of transliteration, the ?phrases?
are commonly occurring substrings ? sequences
of characters ? in one language that map to a
character or a substring in the other. We use the
term ?substrings? after a previous work (Sherif and
Kondrak, 2007) that employs it in a noisy channel
transliteration system. Zhao et al (2007) also use
substrings (which they call ?blocks?) in a bi-stream
HMM.
We bootstrap the induction of substrings by
aligning all named entity pairs in the training data,
using the GIZA++ toolkit (Och and Ney, 2003).
The toolkit performs unidirectional one-to-many
alignments, meaning that a single symbol in its
source string can be aligned to at most one sym-
bol in its target. In order to induce many-to-many
alignments, GIZA++ is run on the data in both di-
rections (source language to target language and
target language to source), and the bidirectional
alignment of a named entity pair is taken to be the
union of the alignments in each direction. Any
inserted characters (maps within the alignment
where the source or target character is null) are
combined with the preceding character within the
string. For example, the initial bidirectional align-
ment of shivlal ? Siv@lAl (E?vlAl) contains
the maps [sh ? S, i ? i, v ? v, null ? @, l ? l,
a ? A, and l ? l]. The null ? @ map is combined
with the preceding map to give v ? v@, and hence
a one-to-one alignment.
Multicharacter units formed by bidirectional
alignments are added to source and target alha-
bets. The above example would add the substrings
?sh? to the source alphabet, and v@ to the target.
Very low frequency substrings in both languages
are removed, giving the final substring alphabets
of single and multicharacter tokens. These alpha-
bets (summarized in Table 1) are used as the token
set for the CRF in Step 3.
We now transform our training data into a
substring-based representation. The original
named entity pairs are replaced by their bidirec-
tional one-to-one alignments described earlier. For
example, the ?s h i v l a l? ? ?S i v @ l
A l? training pair is replaced by ?sh i v l a l? ?
?S i v@ l A l?. A few (less than 3%) of the
pairs are not aligned one-to-one, since their bidi-
rectional alignments contain low-frequency sub-
strings that have not been included in the alpha-
bet.1 These pairs are removed from the training
data, since only one-to-one alignments can be han-
dled by the CRF.
2.3 Step 3: CRF transliteration
With the transformed training data in hand, we can
now train a CRF sequential model that uses sub-
strings rather than characters as the basic token
unit. The CRF algorithm is chosen for its ability
to handle non-independent features of the source
language input sequence. We use the open-source
CRF++ software package (Kudo, 2005).
Ganesh et al (2008) also apply a CRF to the
transliteration task (Hindi to English) but with
different alignment methods than those presented
here. In particular, multicharacter substrings are
only used as tokens on the target (English) side,
and a null token is used to account for deletion.
We train our CRF using unigram, bigram, and
trigram features over the source substrings, as well
as pronunciation information described in ?2.3.1.
Table 2 describes these feature sets.
2.3.1 Phonetic information
Since the CRF model allows us to incorporate non-
independent features, we add pronunciation data
as a token-level feature. Doing so allows the CRF
to use phonetic information for local decision-
making. Word pronunciations were obtained from
1Note that if we did not filter out any of the substrings,
every pair would be aligned one-to-one.
93
Target Language Source Target
# of Tokens Longest Token # of Tokens Longest Token
Hindi 196 augh, ough 141 Aj@ (aAy), ks@ (?s)
Kannada 197 aine 137 Aj@, mjA
Tamil 179 cque 117 mij, Aj@
Table 1: Overview of the substring alphabets generated in Step 2.
Feature Set Description
U Unigram: s?1, s0, and s1
B Bigram: s?1+s0
T Trigram: s?2+s?1+s0,
s?1+s0+s1 and s0+s1+s2
P Phoneme assigned to s0
from dictionary lookup
Table 2: Feature sets used for CRF in Step 3. si is
the substring relative to the current substring s0.
the CMU Pronouncing Dictionary2. Just over a
third of the English named entities have pronun-
ciation information available for some or all the
constituent words.
The CMU dictionary provides a sequence of
phoneme symbols for an English word. We in-
clude these phonemes as CRF features if and
only if a one-to-one correspondence exists be-
tween phonemes and substring tokens. For exam-
ple, the English word simon has the segmentation
?s i m o n? and pronunciation ?S AY M AH N?,
both of length five. Additionally, a check is done
to ensure that vowel phonemes do not align with
consonant characters and vice-versa.
2.4 Step 4: Substring segmentation
In order to apply our trained model to unseen
data, we must segment named entities into non-
overlapping substrings that correspond to tokens
in the source alphabet generated in Step 2. For in-
stance, we need to convert the four character desh
to the three token sequence ?d e sh?.
This is a non-trivial task. We must allow for
the fact that substrings are not inserted every time
the component character sequence appears. For
instance, in our English/Hindi training set, the bi-
gram ti always reduces to a single substring token
when it occurs in the -tion suffix, but does not re-
duce in any other contexts (like martini). There
are also cases where more than one non-trivial seg-
mentation is possible. For example, two possible
2The CMU Pronouncing Dictionary (v0.7a). Available at
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
segmentations of desh are ?d es h? and ?d e sh?,
with the latter being the one that best corresponds
to the three-character Hindi d?eS (d?).
One solution is to greedily choose the most
likely multi-character substring ? in the example
cited, we can choose ?d e sh? because sh reduces
more frequently than es. However, this creates the
problem in cases where no reduction should occur,
as with the ti in martini. Since contextual informa-
tion is necessary to determine the correct substring
segmentation, we model segmentation with a CRF,
using a combination of character unigram, bigram,
and trigram features.
We use an approach motivated by the In-
side/Outside representation of NP-chunking
which treats segmentation as a tagging process
over words (Ramshaw and Marcus, 1995). As
in NP-chunking, our goal is to identify non-
overlapping, non-recursive segments in our input
sequence. Our tagset is {I, O, B} where I
indicates that a character is inside a substring, O
indicates a character is outside a substring, and B
marks a right boundary.
After the test data has been segmented into its
substring representation, it can be passed as input
to the CRF model trained in Step 3 to produce our
final transliteration output.
3 Results
We first report our results on the development data
provided by the NEWS task, for different feature
sets and segmentation methods. We then present
the performance of our system on the test data.3
3.1 Development Data
Table 3 shows the results across feature sets.
Noting that the trigram feature T provides a
sizable improvement, we compare results from
U+B+T+P and U+B+P feature sets. Of the im-
proved cases, 75-84% are a single vowel-to-vowel
3For the development runs, we use the training set for
training, and the development for testing. For the final test
runs, we use both the training and development sets for train-
ing, and the test set for evaluation.
94
Language Feature Set ACC F-Score
U+P 24.6 86.2
Hindi U+B+P 26.2 86.5
U+B+T+P 34.5 88.6
U+B+T 34.2 88.3
U+P 26.7 87.8
Tamil U+B+P 27.6 88.0
U+B+T+P 34.9 89.8
U+B+T 33.1 89.7
U+P 22.5 86.0
Kannada U+B+P 22.6 86.2
U+B+T+P 28.7 88.0
U+B+T 27.5 87.9
Table 3: Accuracy (ACC) and F-score results (in
%) for CRF model on the development data.
Language Feature Set ACC F-Score
Hindi U+B+T+P 34.4 90.2
U+B+T 33.6 89.5
Tamil U+B+T+P 29.1 91.1
U+B+T 25.5 90.6
Kannada U+B+T+P 27.2 89.8
U+B+T 23.4 89.2
Table 4: Results on development data, restricted to
NEs where P is included as a feature.
change, with the majority of the changes involving
a schwa/central vowel.
We see small gains from using the phonetic fea-
ture in both accuracy and F-Score. We further ex-
amine only those named entities where dictionary
information is applied, and as expected, this subset
shows greater improvement (Table 4).
Table 5 compares our the Inside/Outside tag-
ging approach with a greedy approach described
earlier. The greedy approach only inserts a multi-
character substring when that substring reduces
more than 50% of the time in the overall train-
ing corpus. Since the Greedy method uses no
local contextual information, results are signifi-
cantly lower given the same feature set.
Language Segmentation ACC F-Score
Hindi I-O-B 34.5 88.6
Greedy 30.3 86.7
Tamil I-O-B 34.9 89.8
Greedy 28.2 87.5
Kannada I-O-B 28.7 88.0
Greedy 25.0 86.7
Table 5: Comparison of segmentation methods
on development data, using the U+B+T+P feature
set.
3.2 Test Data
Our model produces 10 candidates for each named
entity in the test data, ranked by the probability
that the model assigns the candidate. We filter out
candidates below the rank of 5 whose scores are
less than 0.5 lower than that of the highest rank-
ing candidate. Table 6 shows our results on the
test data, using a CRF trained on the training and
development data, with the feature set U+B+T+P.
Hindi Kannada Tamil
Accuracy 41.8 36.3 43.5
F-Score 87.9 87.0 90.2
MRR 54.6 48.2 57.2
MAPref 41.2 35.5 43.0
MAP10 18.3 16.4 19.5
MAPsys 24.0 21.8 26.5
Table 6: Final results on the test data (in %).
References
Surya Ganesh, Sree Harsh, Prasad Pingali, and Va-
sudeva Varma. 2008. Statistical transliteration for
cross language information retrieval using HMM
alignment model and CRF. In Proceedings of the
2nd Workshop on Cross Lingual Information Access.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Available at http://chasen.org/ taku/software/crf++/.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proceed-
ings of SIGIR-07.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Lance Ramshaw and Mitch Marcus. 1995. Text
chunking using transformation-based learning. In
Proceedings of WVLC-3.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of ACL-07.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Proceedings of
NAACL HLT 2007.
95
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 713?716,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An MDL-based approach to extracting subword units for
grapheme-to-phoneme conversion
Sravana Reddy
Department of Computer Science
The University of Chicago
Chicago, IL 60637
sravana@cs.uchicago.edu
John Goldsmith
Departments of Linguistics
and Computer Science
The University of Chicago
Chicago, IL 60637
goldsmith@uchicago.edu
Abstract
We address a key problem in grapheme-to-
phoneme conversion: the ambiguity in map-
ping grapheme units to phonemes. Rather than
using single letters and phonemes as units, we
propose learning chunks, or subwords, to re-
duce ambiguity. This can be interpreted as
learning a lexicon of subwords that has min-
imum description length. We implement an
algorithm to build such a lexicon, as well as a
simple decoder that uses these subwords.
1 Introduction
A system for converting written words to their pro-
nunciations is an important component of speech-
related applications, especially in large vocabulary
tasks. This problem, commonly termed ?grapheme-
to-phoneme conversion?, or g2p, is non-trivial for
several written languages, including English, since a
given letter (grapheme) may represent one of several
possible phonemes, depending on the context. Be-
cause the length of the context varies throughout the
dictionary, fixed-length contexts may overfit some
words, or inaccurately model others.
We approach this problem by treating g2p as a
function from contiguous sequences of graphemes,
which we call ?grapheme subwords?, to sequences
of phonemes (?phoneme subwords?), so that there is
minimal ambiguity in finding the phoneme subword
that corresponds to a given grapheme subword. That
is, we seek to minimize both these quantities:
1. The conditional entropy of the phoneme sub-
words given a grapheme subword. This di-
rectly tackles the problem of ambiguity ? a per-
fectly unambiguous phoneme subword condi-
tional distribution would have entropy = 0.
2. The entropy of the grapheme subwords. This
prevents the model from getting arbitrarily
complex.
As a toy example, consider the following word-
pronunciation1 pairs:
time T AY M
sting S T IH NG
negation N AH G EY SH AH N
There are at least 5 graphemes whose correspond-
ing phoneme distribution is ambiguous (?i?, ?e?, ?t?,
?n?, ?g?). In the segmentation below, every grapheme
subword corresponds to only one phoneme subword:
t + ime T + AY M
s + t + ing S + T + IH NG
neg + a + tion N AH G + EY + SH AH N
2 Related Work
Many grapheme-to-phoneme algorithms rely on
something resembling subwords; these are mainly
used to account for sequences of letters representing
a single phoneme (?ph? for F), or vice versa (?x? for
K S). Some of the early works that create one-to-
one alignments between a word and its pronuncia-
tion address these cases by allowing a letter to map
to one phoneme, a null phoneme, or 2-3 phonemes.
Jiampojamarn and Kondrak (2009) use
expectation-maximization (EM) to learn many-
to-many alignments between words and pro-
nunciations, effectively obtaining subwords.
1All phonemes are denoted by their Arpabet representations.
713
Joint-sequence models divide a word-pronunciation
pair into a sequence of disjoint graphones or
graphonemes ? tuples containing grapheme and
phoneme subwords. Such segmentations may
include only trivial graphones containing subwords
of length at most 1 (Chen, 2003). Other such
models use EM to learn the maximum likelihood
segmentation into graphones (Deligne and Bimbot,
1995; Bisani and Ney, 2008; Vozilla et al, 2003).
Subwords ? or phrases ? are used widely in ma-
chine translation. There is a large body of work on
phrase extraction starting from word alignments; see
Koehn et al (2003) for a review. Marcu and Wong
(2002) learn phrases directly from sentence pairs us-
ing a joint probability model.
3 Subword Extraction
3.1 Motivation for using MDL
Consider a lexicon of grapheme subwords G and
phoneme subwords P that is extracted from a dic-
tionary of word-pronunciation pairs, along with a
joint probability distribution over G and P . As
stated earlier, our objective is to minimize the en-
tropy of phoneme subwords conditioned on a given
grapheme subword, as well as the entropy of the
grapheme subwords. That is, we would like to min-
imize H(P|G) +H(G), which is
H(G,P) = ?
?
g?G
?
p?P
pr(g, p) log pr(g, p) (1)
This objective can be restated as minimizing the
expected description length of the lexicon, which is
given by its entropy. This is reflected in the MDL
principle (Rissanen, 1978), which seeks to find a
lexicon such that the description length of the lex-
icon (and the compression of the data under the lex-
icon) is minimized.
3.2 Lexicon Induction
We begin with an initial alignment between a word?s
graphemes and the phonemes in its pronunciation
for all word-pronunciation pairs in the training dic-
tionary. These alignments are derived using the stan-
dard string edit distance dynamic programming al-
gorithm (Wagner and Fischer, 1974), giving a list
of tuples t = [(w1, r1), (w2, r2), . . .] for each word-
pronunciation pair.2 The set of all tuple lists t com-
poses the training dictionary T .
The initial lexicon is composed of all singleton
graphemes and phonemes (including null). The
probability pr(g, p) is taken to be the number of
times the tuple (g, p) occurs in T divided by the total
number of tuples over all alignments in T .
Following a procedure similar the word-discovery
algorithm of de Marcken (1996), the lexicon is iter-
atively updated as sketched in Table 1. At no point
do we delete singleton graphemes or phonemes.
The subwords in the final updated lexicon are then
used to decode the pronunciations of unseen words.
4 G2P Decoding
4.1 Joint segmentation and decoding
Finding the pronunciation of a word based on the
induced subword lexicon involves segmenting the
word into a sequence of grapheme subwords, and
mapping it to a sequence of phoneme subwords.
One possibility is carry these steps out sequen-
tially: first parse the word into grapheme subwords,
and then use a sequence labeling algorithm to find
the best corresponding sequence of phoneme sub-
words. However, it is likely that the true pronuncia-
tion of a word is not derived from its best parse into
grapheme units. For example, the best parse of the
word ?gnat? is ?g nat?, which yields the pronuncia-
tion G N AE T, while the parse ?gn at? would give
the correct pronunciation N AE T.
Therefore, we search for the best pronunciation
over all segmentations of the word, adapting the
monotone search algorithm proposed by Zens and
Ney (2004) for phrase-based machine translation.3
4.2 Smoothing
A bigram model is used over both the grapheme
and phoneme subwords. These bigrams need to be
smoothed before the decoding step. Adding an equal
probability mass to unseen bigrams would fail to re-
flect simple phonotactics (patterns that govern sound
2Phoneme insertions and deletions are represented by the
null grapheme and null phoneme respectively.
3The key adaptation is in using a bigram model over both
graphemes and phonemes, rather than only phonemes as in the
original algorithm.
714
Table 1: Concatenative algorithm for building a subword lexicon that minimizes description length. The input is T ,
the set of alignments, and a threshold integer k, which is tuned using a held-out development set.
1 Update pr(g, p) by computing the posterior probabilities of the tuple (g, p) in T ,
using the forward-backward algorithm. Repeat once more to get an intermediate lexicon.
2 Compute the Viterbi parse of each t ? T under the lexicon derived in step 1.
3 Let A, the set of candidate tuples for addition to the lexicon, contain all tuples (wiwi+1, riri+1) such that
(wi, ri) and (wi+1, ri+1) are adjacent more than k times in the computed Viterbi parses. For each (g, p) ? A,
estimate the change in description length of the lexicon if (g, p) is added. If description length decreases,
remove any null symbols within g and p, and add (g, p) to the lexicon.
4 Repeat steps 1 and 2.
5 Delete all tuples that do not occur in any of the Viterbi parses.
6 Compare the description length of the new lexicon with the lexicon at the start of the iteration. If the
difference is sufficiently small, return the new lexicon; else, repeat from step 1.
sequences) in several cases. For example, the bi-
gram L UW K + S is much more likely than L UW
K + Z, since S is more likely than Z to follow K.
To introduce a bias towards phonotacticaly likely
bigrams, we define the smoothed bigram probability
of the subword a following a subword b. Given that
b is made up of a sequence of l phonemes b1b2 . . . bl,
the probability is defined as the interpolation4:
prnew(a|b) = ?1pr(a|b1b2 . . . bl) +
?2pr(a|b1b2 . . . bl?1) + ?3pr(a|b1b2 . . . bl?2)
Both the grapheme and phoneme subword bi-
grams are smoothed as described.
5 Results
We test our algorithm on the CMU Pronouncing
Dictionary5. The dictionary is divided randomly
into a training (90% of the data) and a test set. Per-
formance is evaluated by measuring the phoneme er-
ror rate (PER) and the word error rate (WER).
The subword extraction algorithm converges in 3
iterations.We run the g2p decoder using the lexicon
after 3 iterations, as well as after 0, 1 and 2 itera-
tions. The results are shown in Table 2.
Figure 1 compares the results of our method (de-
noted by ?MDL-Sub?) to two baselines, at different
values of maximum subword length. To evaluate the
quality of our subwords, we substitute another ex-
traction algorithm to create the lexicon ? the grow-
diag-final phrase extraction method (Koehn et al,
4In our experiments, we set ?1 = 0.5, ?2 = 0.3, ?3 = 0.2.
5The CMU Pronouncing Dictionary. Available online at
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
Table 2: Results after each iteration of subword extrac-
tion. While the maximum subword length after iteration
3 is 8, the vast majority of subwords have length 6 or less.
# subwords Max subword WER PER
length
0 |G| : 27, |P| : 40 1 73.16 24.20
1 |G| : 819, |P| : 1254 2 48.39 12.43
2 |G| : 5430, |P| : 4954 4 28.32 7.16
3 |G| : 6417, |P| : 5358 6 26.31 6.29
2005), denoted by ?GD? in the figure. We also run
the implementation of Bisani and Ney (2008) ? de-
noted by ?BN? ? on the same data. BN is an example
of a joint-sequence n-gram model, which uses a joint
distribution pr(G,P) of graphemes and phonemes
(?graphones?), conditioned on the preceding n-1 gra-
phones for context information. Since this algorithm
outperforms most of the existing g2p algorithms, it
serves as a good point of comparison to the state of
the art in g2p. The results of BN using an n-gram
model are compared to MDL-Sub with an n-1 max-
imum subword length6.
The MDL-Sub lexicon does significantly better
than the phrases extracted by GD. While BN starts
off doing better than MDL-Sub, the latter outper-
forms BN at longer subword lengths. Most of the ad-
ditional errors in BN at that stage involve grapheme-
to-phoneme ambiguity ? phonemes like AE, AA, and
AH being confused for one another when mapping
6The contextual information of (n-1)-length subwords with
bigrams is assumed to be roughly comparable to that of very
short subwords over n-grams.
715
the grapheme ?a?, and so on. Far fewer of these er-
rors are produced by our algorithm. However, some
of the longer subwords in MDL-Sub do introduce
additional errors, mainly because the extraction al-
gorithm merges smaller subwords from previous it-
erations. For example, one of the items in the ex-
tracted lexicon is ?icati? ? a product of merging ?ic?
and ?ati? ? corresponding to IH K EY SH, thus
generating incorrect pronunciations for words con-
taining the string ?icating?.
Figure 1: Comparison of error rates.
6 Conclusion
This paper deals with translational ambiguity, which
is a major issue in grapheme-to-phoneme conver-
sion. The core of our system consists of extract-
ing subwords of graphemes and phonemes from the
training data, so that the ambiguity of deriving a
phoneme subword from a grapheme subword is min-
imized. This is achieved by formalizing ambiguity
in terms of the minimum description length princi-
ple, and using an algorithm that reduces the descrip-
tion length of the subword lexicon at each iteration.
In addition, we also introduce a smoothing mech-
anism which retains some of the phonotactic depen-
dencies that may be lost when using subwords rather
than singleton letters and phonemes.
While retaining the basic approach to minimizing
ambiguity, there are some avenues for improvement.
The algorithm that builds the lexicon creates a more
or less hierarchical structure ? subwords tend to be
composed from those extracted at the previous iter-
ation. This appears to be the cause of many of the
errors produced by our method. A subword extrac-
tion algorithm that does not use a strictly bottom-up
process may create a more robust lexicon.
Our method of subword extraction could also be
applied to phrase extraction for machine transla-
tion, or in finding subwords for related problems like
transliteration. It may also be useful in deriving sub-
word units for speech recognition.
References
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50:434?451.
Stanley F Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
Carl G de Marcken. 1996. Unsupervised Language Ac-
quisistion. Ph.D. thesis, MIT.
Sabine Deligne and Frederic Bimbot. 1995. Language
modeling by variable length sequences: theoretical
formulation and evaluation of multigrams. In Pro-
ceedings of ICASSP.
Sittichai Jiampojamarn and Grzegorz Kondrak. 2009.
Online discriminative training for grapheme-to-
phoneme conversion. In Proceedings of Interspeech.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Jorma Rissanen. 1978. Modeling by the shortest data
description. Automatica.
Paul Vozilla, Jeff Adams, Yuliya Lobacheva, and Ryan
Thomas. 2003. Grapheme to phoneme conversion and
dictionary verification using graphonemes. In Pro-
ceedings of Eurospeech.
Robert Wagner and Michael Fischer. 1974. The string-
to-string correction problem. Journal of the ACM.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of HLT-NAACL.
716
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 367?371,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
G2P Conversion of Proper Names Using Word Origin Information
Sonjia Waxmonsky and Sravana Reddy
Department of Computer Science
The University of Chicago
Chicago, IL 60637
{wax, sravana}@cs.uchicago.edu
Abstract
Motivated by the fact that the pronuncia-
tion of a name may be influenced by its
language of origin, we present methods to
improve pronunciation prediction of proper
names using word origin information. We
train grapheme-to-phoneme (G2P) models on
language-specific data sets and interpolate the
outputs. We perform experiments on US sur-
names, a data set where word origin variation
occurs naturally. Our methods can be used
with any G2P algorithm that outputs poste-
rior probabilities of phoneme sequences for a
given word.
1 Introduction
Speakers can often associate proper names with their
language of origin, even when the words have not
been seen before. For example, many English speak-
ers will recognize that Makowski and Masiello are
Polish and Italian respectively, without prior knowl-
edge of either name. Such recognition is important
for language processing tasks since the pronuncia-
tions of out-of-vocabulary (OOV) words may de-
pend on the language of origin. For example, as
noted by Llitjo?s (2001), ?sch? is likely to be pro-
nounced as /sh/ for German-origin names (Schoe-
nenberg) and /sk/ for Italian-origin words (Schi-
avone).
In this work, we apply word origin recognition
to grapheme-to-phoneme (G2P) conversion, the task
of predicting the phonemic representation of a word
given its written form. We specifically study G2P
conversion for personal surnames, a domain where
OOVs are common and expected.
Our goal is to show how word origin information
can be used to train language-specific G2P models,
and how output from these models can be combined
to improve prediction of the best pronunciation of a
name. We deal with data sparsity in rare language
classes by re-weighting the output of the language-
specific and language-independent models.
2 Previous Work
Llitjo?s (2001) applies word origin information to
pronunciation modeling for speech synthesis. Here,
a CART decision tree system is presented for G2P
conversion that maps letters to phonemes using local
context. Experiments use a data set of US surnames
that naturally draws from a diverse set of origin lan-
guages, and show that the inclusion of word origin
features in the model improves pronunciation accu-
racy. We use similar data, as described in ?4.1.
Some works on lexical modeling for speech
recognition also make use of word origin. Here,
the focus is on expanding the vocabulary of an ASR
system rather than choosing a single best pronunci-
ation. Maison et al (2003) train language-specific
G2P models for eight languages and output pronun-
ciations to augment a baseline lexicon. This aug-
mented lexicon outperforms a handcrafted lexicon
in ASR experiments; error reduction is highest for
foreign names spoken by native speakers of the ori-
gin language. Cremelie and ten Bosch (2001) carry
out a similar lexicon augmentation, and make use of
penalty weighting, with different penalties for pro-
nunciations generated by the language-specific and
language-independent G2P models.
The problem of machine transliteration is closely
related to grapheme-to-phoneme conversion. Many
367
transliteration systems (Khapra and Bhattacharyya,
2009; Bose and Sarkar, 2009; Bhargava and Kon-
drak, 2010) use word origin information. The
method described by Hagiwara and Sekine (2011)
is similar to our work, except that (a) we use a data
set where multiple languages of origin occur nat-
urally, rather than creating language-specific lists
and merging them into a single set, and (b) we
consider methods of smoothing against a language-
independent model to overcome the problems of
data sparsity and errors in word origin recognition.
3 Language-Aware G2P
Our methods are designed to be used with any
statistical G2P system that produces the posterior
probability Pr(??|g?) of a phoneme sequence ?? for
a word (grapheme sequence) g? (or a score that
can be normalized to give a probability). The
most likely pronunciation of a word is taken to be
arg max?? Pr(??|g?).
Our baseline is a single G2P model that is trained
on all available training data. We train additional
models on language-specific training subsets and in-
corporate the output of these models to re-estimate
Pr(??|g?), which involves the following steps:
1. Train a supervised word origin classifier to pre-
dict Pr(l|w) for all l ? L, the set of languages
in our hand-labeled word origin training set.
2. Train G2P models for each l ? L. Each model
ml is trained on words with Pr(l|w) greater
than some threshold ?. Here, we use ? = 0.7.
3. For each word w in the test set, generate can-
didate transcriptions from model ml for each
language with nonzero Pr(l|w). Re-estimate
Pr(??|g?) by interpolating the outputs of the
language-specific models. We may also use the
output of the language-independent model.
We elaborate on our approaches to Steps 1 and 3.
3.1 Step 1: Word origin modeling
We apply a sequential conditional model to predict
Pr(l|w), the probability of a language class given
the word. A similar Maximum Entropy model is
presented by Chen and Maison (2003), where fea-
tures are the presence or absence of a given charac-
ter n-gram in w. In our approach, feature functions
are defined at character positions rather than over the
entire word. Specifically, for word wj composed of
character sequence c1 . . . cm of length m (including
start and end symbols), binary features test for the
presence or absence of an n-gram context at each
position m. A context is the presence of a charac-
ter n-gram starting or ending at position m. Model
features are represented as:
fi(w,m, lk) =
?
??
??
1, if lang(w) = lk and context
i is present at position m
0, otherwise
(1)
Then, for wj = ci . . . cm:
Pr(lk|wj) =
exp
?
m
?
i ?ifi(cm, lk)
Z
(2)
where Z =
?
j exp
?
m
?
i ?ifi(cm, lk) is a nor-
malization factor. In practice, we can implement this
model as a CRF, where a language label is applied
at each character position rather than for the word.
While all the language labels in a sequence need
not be the same, we find only a handful of words
where a transition occurs from one language label to
another within a word. For these cases, we take the
label of the last character in the word as the language
of origin. Experiments comparing this sequential
Maximum Entropy method with other word origin
classifiers are described by Waxmonsky (2011).
3.2 Step 3: Re-weighting of G2P output
We test two methods of re-weighting Pr(??|g?) us-
ing the word origin estimation and the output of
language-specific G2P models.
Method A uses only language-specific models:
P?r(??|g?) =
?
l?L
Pr(??|g?, l) Pr(l|g) (3)
where Pr(??|g?, l) is estimated by model ml.
Method B With the previous method, names from
infrequent classes suffer from data sparsity. We
therefore smooth with the output PI of the baseline
language-independent model.
P?r(??|g?) = ?Pr
I
(??|g?)+(1??)
?
l?L
Pr(??|g?, l) Pr(l|g)
(4)
The factor ? is tuned on a development set.
368
Language Train Test Base (A) (B)
Class Count Count -line
British 16.1k 2111 71.8 73.1 73.9
German 8360 1109 75.8 74.2 78.2
Italian 3358 447 61.7 66.2 65.1
Slavic 1658 232 50.9 49.6 51.7
Spanish 1460 246 44.7 41.5 48.0
French 1143 177 42.9 42.4 45.2
Dutch 468 82 70.7 52.4 68.3
Scandin. 393 61 77.1 60.7 72.1
Japanese 116 23 73.9 52.2 78.3
Arabic 68 18 33.3 11.1 38.9
Portug. 34 4 25.0 25.0 50.0
Hungarian 28 3 100.0 66.7 100.0
Other 431 72 55.6 54.2 59.7
All 67.8 67.4 70.0
Table 1: G2P word accuracy for various weighting meth-
ods using a character-based word origin model.
4 Experiments
4.1 Data
We assemble a data set of surnames that occur fre-
quently in the United States. Since surnames are
often ?Americanized? in their written and phone-
mic forms, our goal is to model how a name is
most likely to be pronounced in standard US English
rather than in its language of origin.
We consider the 50,000 most frequent surnames
in the 1990 census1, and extract those entries that
also appear in the CMU Pronouncing Dictionary2,
giving us a set of 45,841 surnames with their
phoneme representations transcribed in the Arpabet
symbol set. We divide this data 80/10/10 into train,
test, and development sets.
To build a word origin classification training set,
we randomly select 3,000 surnames from the same
census lists, and label by hand the most likely lan-
guage of origin of each name when it occurs in the
US. Labeling was done primarily using the Dictio-
nary of American Family Names (Hanks, 2003) and
Ellis Island immigration records.3 We find that, in
many cases, a surname cannot be attributed to a sin-
gle language but can be assigned to a set of lan-
1http://www.census.gov/genealogy/names/
2http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
3http://www.ellisisland.org
guages related by geography and language family.
For example, we discovered several surnames that
could be ambiguously labeled as English, Scottish,
or Irish in origin. For languages that are frequently
confusable, we create a single language group to be
used as a class label. Here, we use groups for British
Isles, Slavic, and Scandinavian languages. Names
of undetermined origin are removed, leaving a final
training set of 2,795 labeled surnames and 33 dif-
ferent language classes. We have made this anno-
tated word origin data publicly available for future
research.4
In these experiments, we use surnames from the
12 language classes that contain at least 10 hand-
labeled words, and merge the remaining languages
into an ?Other? class. Table 1 shows the final lan-
guage classes used. Unlike the training sets, we do
not remove names with ambiguous or unknown ori-
gin from the test set, so our G2P system is also eval-
uated on the ambiguous names.
4.2 Results
The Sequitur G2P algorithm (Bisani and Ney, 2008)
is used for all our experiments.
We use the CMU Dictionary as the gold stan-
dard, with the assumption that it contains the stan-
dard pronunciations in US English. While surnames
may have multiple valid pronunciations, we make
the simplifying assumption that a name has one best
pronunciation. Evaluation is done on the test set of
4,585 names from the CMU Dictionary.
Table 1 shows G2P accuracy for the baseline sys-
tem and Methods A and B. Test data is partitioned
by the most likely language of origin.
We see that Method A, which uses only language-
specific G2P models, has lower overall accuracy
than the baseline. We attribute this to data spar-
sity introduced by dividing the training set by lan-
guage. With the exception of British and German,
language-specific training set sizes are less than 10%
the size of the baseline training set of 37k names.
Another cause of the lowered performance is likely
due to errors made by our word origin model.
Examining results for individual language classes
for Method A, we see that Italian and British are
4The data may be downloaded from http://people.
cs.uchicago.edu/?wax/wordorigin/.
369
Language Surname Baseline Method B
Carcione K AA R S IY OW N IY K AA R CH OW N IY
Cuttino K AH T IY N OW K UW T IY N OW
Italian Lubrano L AH B R AA N OW L UW B R AA N OW
Pesola P EH S AH L AH P EH S OW L AH
Kotula K OW T UW L AH K AH T UW L AH
Slavic Jaworowski JH AH W ER AO F S K IY Y AH W ER AO F S K IY
Lisak L IY S AH K L IH S AH K
Wasik W AA S IH K V AA S IH K
Bencivenga B EH N S IH V IH N G AH B EH N CH IY V EH NG G AH
Spanish Vivona V IH V OW N AH V IY V OW N AH
Zavadil Z AA V AA D AH L Z AA V AA D IY L
Table 2: Sample G2P output from the Baseline (language-independent) and Method B systems. Language labels
shown here are the arg maxl P (l|w) using the character-based word origin model. Phoneme symbols are from an
Arpabet-based alphabet, as used in the CMU Pronouncing Dictionary.
the only language classes where accuracy improves.
For Italian, we attribute this to two factors: high
divergence in pronunciation from US English, and
the availability of enough training data to build a
successful language-specific model. In the case of
British, a language-specific model removes foreign
words but leaves enough training data to model the
language sufficiently.
Method B shows accuracy gains of 2.2%, with
gains for almost all language classes except Dutch
and Scandinavian. This is probably because names
in these two classes have almost standard US En-
glish pronunciations, and are already well-modeled
by a language-independent model.
We next look at some sample outputs from our
G2P systems. Table 2 shows names where Method
B generated the gold standard pronunciation and the
baseline system did not. For the Italian and Span-
ish sets, we see that the letter-to-phoneme mappings
produced by Method B are indicative of the lan-
guage of origin: (c ? /CH/) in Carcione, (u ?
/UW/) in Cuttino, (o ? /OW/) in Pesola, and (i ?
/IY/) in Zavadil and Vivona. Interestingly, the name
Bencivenga is categorized as Spanish but appears
with the letter-to-phoneme mapping (c ? /CH/),
which corresponds to Italian as the language of ori-
gin. We found other examples of the (c ? /CH/)
mappings, indicating that Italian-origin names have
been folded into Spanish data. This is not surprising
since Spanish and Italian names have high confusion
with each other. Effectively, our word origin model
produced a noisy Spanish G2P training set, but the
re-weighted G2P system is robust to these errors.
We see examples in the Slavic set where the gold
standard dictionary pronunciation is partially but not
completely Americanized. In Jaworowski, we have
the mappings (j? /Y/) and (w? /F/), both of which
are derived from the original Polish pronunciation.
But for the same name, we also have (w ? /W/)
rather than (w? /V/), although the latter is truer to
the original Polish. This illustrates one of the goals
of our project, which is is to capture these patterns
of Americanization as they occur in the data.
5 Conclusion
We apply word origin modeling to grapheme-
to-phoneme conversion, interpolating between
language-independent and language-specific proba-
bilistic grapheme-to-phoneme models. We find that
our system outperforms the baseline in predicting
Americanized surname pronunciations and captures
several letter-to-phoneme features that are specific
to the language of origin.
Our method operates as a wrapper around G2P
output without modifying the underlying algorithm,
and therefore can be applied to any state-of-the-art
G2P system that outputs posterior probabilities of
phoneme sequences for a word.
Future work will consider unsupervised or semi-
supervised approaches to word origin recognition
for this task, and methods to tune the smoothing
weights ? at the language rather than the global
level.
370
References
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proceed-
ings of NAACL.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication.
Dipankar Bose and Sudeshna Sarkar. 2009. Learning
multi character alignment rules and classification of
training data for transliteration. In Proceedings of the
ACL Named Entities Workshop.
Stanley F. Chen and Beno??t Maison. 2003. Using place
name data to train language identification models. In
Proceedings of Eurospeech.
Nick Cremelie and Louis ten Bosch. 2001. Improv-
ing the recognition of foreign names and non-native
speech by combining multiple grapheme-to-phoneme
converters. In Proceedings of ITRW on Adaptation
Methods for Speech Recognition.
Masato Hagiwara and Satoshi Sekine. 2011. Latent class
transliteration based on source language origin. In
Proceedings of ACL.
Patrick Hanks. 2003. Dictionary of American family
names. New York : Oxford University Press.
Mitesh M. Khapra and Pushpak Bhattacharyya. 2009.
Improving transliteration accuracy using word-origin
detection and lexicon lookup. In Proceedings of the
ACL Named Entities Workshop.
Ariadna Font Llitjo?s. 2001. Improving pronunciation
accuracy of proper names with language origin classes.
Master?s thesis, Carnegie Mellon University.
Beno??t Maison, Stanley F. Chen, and Paul S. Cohen.
2003. Pronunciation modeling for names of foreign
origin. In Proceedings of ASRU.
Sonjia Waxmonsky. 2011. Natural language process-
ing for named entities with word-internal information.
Ph.D. thesis, University of Chicago.
371
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 77?82,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Rhyme Schemes
Sravana Reddy
Department of Computer Science
The University of Chicago
Chicago, IL 60637
sravana@cs.uchicago.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
This paper describes an unsupervised,
language-independent model for finding
rhyme schemes in poetry, using no prior
knowledge about rhyme or pronunciation.
1 Introduction
Rhyming stanzas of poetry are characterized by
rhyme schemes, patterns that specify how the lines
in the stanza rhyme with one another. The question
we raise in this paper is: can we infer the rhyme
scheme of a stanza given no information about pro-
nunciations or rhyming relations among words?
Background A rhyme scheme is represented as a
string corresponding to the sequence of lines that
comprise the stanza, in which rhyming lines are de-
noted by the same letter. For example, the limerick?s
rhyme scheme is aabba, indicating that the 1st, 2nd,
and 5th lines rhyme, as do the the 3rd and 4th.
Motivation Automatic rhyme scheme annotation
would benefit several research areas, including:
? Machine Translation of Poetry There has been
a growing interest in translation under con-
straints of rhyme and meter, which requires
training on a large amount of annotated poetry
data in various languages.
? ?Culturomics? The field of digital humanities
is growing, with a focus on statistics to track
cultural and literary trends (partially spurred
by projects like the Google Books Ngrams1).
1http://ngrams.googlelabs.com/
Rhyming corpora could be extremely useful for
large-scale statistical analyses of poetic texts.
? Historical Linguistics/Study of Dialects
Rhymes of a word in poetry of a given time
period or dialect region provide clues about its
pronunciation in that time or dialect, a fact that
is often taken advantage of by linguists (Wyld,
1923). One could automate this task given
enough annotated data.
An obvious approach to finding rhyme schemes
is to use word pronunciations and a definition of
rhyme, in which case the problem is fairly easy.
However, we favor an unsupervised solution that uti-
lizes no external knowledge for several reasons.
? Pronunciation dictionaries are simply not avail-
able for many languages. When dictionaries
are available, they do not include all possible
words, or account for different dialects.
? The definition of rhyme varies across poetic
traditions and languages, and may include
slant rhymes like gate/mat, ?sight rhymes? like
word/sword, assonance/consonance like shore/
alone, leaves/lance, etc.
? Pronunciations and spelling conventions
change over time. Words that rhymed histori-
cally may not anymore, like prove and love ?
or proued and beloued.
2 Related Work
There have been a number of recent papers on the
automated annotation, analysis, or translation of po-
77
etry. Greene et al (2010) use a finite state trans-
ducer to infer the syllable-stress assignments in lines
of poetry under metrical constraints. Genzel et al
(2010) incorporate constraints on meter and rhyme
(where the stress and rhyming information is derived
from a pronunciation dictionary) into a machine
translation system. Jiang and Zhou (2008) develop a
system to generate the second line of a Chinese cou-
plet given the first. A few researchers have also ex-
plored the problem of poetry generation under some
constraints (Manurung et al, 2000; Netzer et al,
2009; Ramakrishnan et al, 2009). There has also
been some work on computational approaches to
characterizing rhymes (Byrd and Chodorow, 1985)
and global properties of the rhyme network (Son-
deregger, 2011) in English. To the best of our knowl-
edge, there has been no language-independent com-
putational work on finding rhyme schemes.
3 Finding Stanza Rhyme Schemes
A collection of rhyming poetry inevitably contains
repetition of rhyming pairs. For example, the word
trees will often rhyme with breeze across different
stanzas, even those with different rhyme schemes
and written by different authors. This is partly due
to sparsity of rhymes ? many words that have no
rhymes at all, and many others have only a handful,
forcing poets to reuse rhyming pairs.
In this section, we describe an unsupervised al-
gorithm to infer rhyme schemes that harnesses this
repetition, based on a model of stanza generation.
3.1 Generative Model of a Stanza
1. Pick a rhyme scheme r of length n with proba-
bility P (r).
2. For each i ? [1, n], pick a word sequence,
choosing the last2 word xi as follows:
(a) If, according to r, the ith line does not
rhyme with any previous line in the stanza, pick
a word xi from a vocabulary of line-end words
with probability P (xi).
(b) If the ith line rhymes with some previous
line(s) j according to r, choose a word xi that
2A rhyme may span more than one word in a line ? for ex-
ample, laureate... / Tory at... / are ye at (Byron, 1824), but this
is uncommon. An extension of our model could include a latent
variable that selects the entire rhyming portion of a line.
rhymes with the last words of all such lines
with probability
?
j<i:ri=rj
P (xi|xj).
The probability of a stanza x of length n is given
by Eq. 1. Ii,r is the indicator variable for whether
line i rhymes with at least one previous line under r.
P (x) =
?
r?R
P (r)P (x|r) =
?
r?R
P (r)
n?
i=1
(1? Ii,r)P (xi) + Ii,r
?
j<i:ri=rj
P (xi|xj) (1)
3.2 Learning
We denote our data by X , a set of stanzas. Each
stanza x is represented as a sequence of its line-end
words, xi, . . . xlen(x). We are also given a large set
R of all possible rhyme schemes.3
If each stanza in the data is generated indepen-
dently (an assumption we relax in ?4), the log-
likelihood of the data is
?
x?X logP (x). We would
like to maximize this over all possible rhyme scheme
assignments, under the latent variables ?, which rep-
resents pairwise rhyme strength, and ?, the distribu-
tion of rhyme schemes. ?v,w is defined for all words
v and w as a non-negative real value indicating how
strongly the words v and w rhyme, and ?r is P (r).
The expectation maximization (EM) learning al-
gorithm for this formulation is described below. The
intuition behind the algorithm is this: after one iter-
ation, ?v,w = 0 for all v and w that never occur to-
gether in a stanza. If v and w co-occur in more than
one stanza, ?v,w has a high pseudo-count, reflecting
the fact that they are likely to be rhymes.
Initialize: ? and ? uniformly (giving ? the same
positive value for all word pairs).
Expectation Step: Compute P (r|x) =
P (x|r)?r/
?
q?R P (x|q)?q, where
P (x|r) =
n?
i=1
(1? Ii,r)P (xi) +
Ii,r
?
j<i:ri=rj
?xi,xj/
?
w
?w,xi (2)
3While the number of rhyme schemes of length n is tech-
nically the number of partitions of an n- element set (the Bell
number), only a subset of these are typically used.
78
P (xi) is simply the relative frequency of the
word xi in the data.
Maximization Step: Update ? and ?:
?v,w =
?
r,x:v rhymes with w
P (r|x) (3)
?r =
?
x?X
P (r|x)/
?
q?R,x?X
P (q|x) (4)
After Convergence: Label each stanza x with the
best rhyme scheme, argmaxr?R P (r|x).
3.3 Data
We test the algorithm on rhyming poetry in En-
glish and French. The English data is an edited ver-
sion of the public-domain portion of the corpus used
by Sonderegger (2011), and consists of just under
12000 stanzas spanning a range of poets and dates
from the 15th to 20th centuries. The French data
is from the ARTFL project (Morrissey, 2011), and
contains about 3000 stanzas. All poems in the data
are manually annotated with rhyme schemes.
The set R is taken to be all the rhyme schemes
from the gold standard annotations of both corpora,
numbering 462 schemes in total, with an average of
6.5 schemes per stanza length. There are 27.12 can-
didate rhyme schemes on an average for each En-
glish stanza, and 33.81 for each French stanza.
3.4 Results
We measure the accuracy of the discovered rhyme
schemes relative to the gold standard. We also eval-
uate for each word token xi, the set of words in
{xi+1, xi+2, . . .} that are found to rhyme with xi by
measuring precision and recall. This is to account
for partial correctness ? if abcb is found instead of
abab, for example, we would like to credit the algo-
rithm for knowing that the 2nd and 4th lines rhyme.
Table 1 shows the results of the algorithm for the
entire corpus in each language, as well as for a few
sub-corpora from different time periods.
3.5 Orthographic Similarity Bias
So far, we have relied on the repetition of rhymes,
and have made no assumptions about word pronun-
ciations. Therefore, the algorithm?s performance
is strongly correlated4 with the predictability of
rhyming words. For writing systems where the
written form of a word approximates its pronunci-
ation, we have some additional information about
rhyming: for example, English words ending with
similar characters are most probably rhymes. We
do not want to assume too much in the interest of
language-independence ? following from our earlier
point in ?1 about the nebulous definition of rhyme
? but it is safe to say that rhyming words involve
some orthographic similarity (though this does not
hold for writing systems like Chinese). We therefore
initialize ? at the start of EM with a simple similarity
measure: (Eq. 5). The addition of  = 0.001 ensures
that words with no letters in common, like new and
you, are not eliminated as rhymes.
?v,w =
# letters common to v & w
min(len(v), len(w))
+  (5)
This simple modification produces results that
outperform the na??ve baselines for most of the data
by a considerable margin, as detailed in Table 2.
3.6 Using Pronunciation, Rhyming Definition
How does our algorithm compare to a standard sys-
tem where rhyme schemes are determined by pre-
defined rules of rhyming and dictionary pronunci-
ations? We use the accepted definition of rhyme
in English: two words rhyme if their final stressed
vowels and all following phonemes are identical.
For every pair of English words v, w, we let ?v,w =
1 +  if the CELEX (Baayen et al, 1995) pronun-
ciations of v and w rhyme, and ?v,w = 0 +  if not
(with  = 0.001). If either v or w is not present
in CELEX, we set ?v,w to a random value in [0, 1].
We then find the best rhyme scheme for each stanza,
using Eq. 2 with uniformly initialized ?.
Figure 1 shows that the accuracy of this system
is generally much lower than that of our model for
the sub-corpora from before 1750. Performance is
comparable for the 1750-1850 data, after which we
get better accuracies using the rhyming definition
than with our model. This is clearly a reflection of
language change; older poetry differs more signifi-
cantly in pronunciation and lexical usage from con-
4For the five English sub-corpora,R2 = 0.946 for the nega-
tive correlation of accuracy with entropy of rhyming word pairs.
79
Table 1: Rhyme scheme accuracy and F-Score (computed from average precision and recall over all lines) using our algorithm
for independent stanzas, with uniform initialization of ?. Rows labeled ?All? refer to training and evaluation on all the data in the
language. Other rows refer to training and evaluating on a particular sub-corpus only. Bold indicates that we outperform the na??ve
baseline, where most common scheme of the appropriate length from the gold standard of the entire corpus is assigned to every
stanza, and italics that we outperform the ?less na??ve? baseline, where we assign the most common scheme of the appropriate length
from the gold standard of the given sub-corpus.
Sub-corpus Sub-corpus overview Accuracy (%) F-Score
(time- # of Total # # of line- EM Na??ve Less na??ve EM Na??ve Less
period) stanzas of lines end words induction baseline baseline induction baseline na??ve
En
All 11613 93030 13807 62.15 56.76 60.24 0.79 0.74 0.77
1450-1550 197 1250 782 17.77 53.30 97.46 0.41 0.73 0.98
1550-1650 3786 35485 7826 67.17 62.28 74.72 0.82 0.78 0.85
1650-1750 2198 20110 4447 87.58 58.42 82.98 0.94 0.68 0.91
1750-1850 2555 20598 5188 31.00 69.16 74.52 0.65 0.83 0.87
1850-1950 2877 15587 4382 50.92 37.43 49.70 0.81 0.55 0.68
Fr
All 2814 26543 10781 40.29 39.66 64.46 0.58 0.57 0.80
1450-1550 1478 14126 7122 28.21 58.66 77.67 0.59 0.83 0.89
1550-1650 1336 12417 5724 52.84 18.64 61.23 0.70 0.28 0.75
temporary dictionaries, and therefore, benefits more
from a model that assumes no pronunciation knowl-
edge. (While we may get better results on older
data using dictionaries that are historically accurate,
these are not easily available, and require a great
deal of effort and linguistic knowledge to create.)
Initializing ? as specified above and then running
EM produces some improvement compared to or-
thographic similarity (Table 2).
4 Accounting for Stanza Dependencies
So far, we have treated stanzas as being indepen-
dent of each other. In reality, stanzas in a poem are
usually generated using the same or similar rhyme
schemes. Furthermore, some rhyme schemes span
multiple stanzas ? for example, the Italian form terza
rima has the scheme aba bcb cdc... (the 1st and 3rd
lines rhyme with the 2nd line of the previous stanza).
4.1 Generative Model
We model stanza generation within a poem as a
Markov process, where each stanza is conditioned
on the previous one. To generate a poem y consist-
ing of m stanzas, for each k ? [1,m], generate a
stanza xk of length nk as described below:
1. If k = 1, pick a rhyme scheme rk of length nk
with probability P (rk), and generate the stanza
as in the previous section.
Figure 1: Comparison of EM with a definition-based system
0?
0.2?
0.4?
0.6?
0.8?
1?
1.2?
1.4?
1.6?
1450-1550 1550-1650 1650-1750 1750-1850 1850-1950 
Ra
tio
 of
 rh
ym
ing
 ru
les
 to
 
EM
 pe
rfo
rm
an
ce
 
Accuracy 
F-Score 
(a) Accuracy and F-Score ratios of the rhyming-definition-
based system over that of our model with orthographic sim-
ilarity. The former is more accurate than EM for post-1850
data (ratio > 1), but is outperformed by our model for older
poetry (ratio< 1), largely due to pronunciation changes like
the Great Vowel Shift that alter rhyming relations.
Found by EM Found by definitions
1450-1550 left/craft, shone/done edify/lie, adieu/hue
1550-1650 appeareth/weareth, obtain/vain, amend/
speaking/breaking, depend, breed/heed,
proue/moue, doe/two prefers/hers
1650-1750 most/cost, presage/ see/family, blade/
rage, join?d/mind shade, noted/quoted
1750-1850 desponds/wounds, gore/shore, ice/vice,
o?er/shore, it/basket head/tread, too/blew
1850-1950 of/love, lover/ old/enfold, within/
half-over, again/rain win, be/immortality
(b) Some examples of rhymes in English found by EM but not
the definition-based system (due to divergence from the contem-
porary dictionary or rhyming definition), and vice-versa (due to
inadequate repetition).
80
Table 2: Performance of EM with ? initialized by orthographic similarity (?3.5), pronunciation-based rhyming definitions (?3.6),
and the HMM for stanza dependencies (?4). Bold and italics indicate that we outperform the na??ve baselines shown in Table 1.
Sub-corpus Accuracy (%) F-Score
(time- HMM Rhyming Orthographic Uniform HMM Rhyming Ortho. Uniform
period) stanzas definition init. initialization initialization stanzas defn. init. init. init.
En
All 72.48 64.18 63.08 62.15 0.88 0.84 0.83 0.79
1450-1550 74.31 75.63 69.04 17.77 0.86 0.86 0.82 0.41
1550-1650 79.17 69.76 71.98 67.17 0.90 0.86 0.88 0.82
1650-1750 91.23 91.95 89.54 87.58 0.97 0.97 0.96 0.94
1750-1850 49.11 42.74 33.62 31.00 0.82 0.77 0.70 0.65
1850-1950 58.95 57.18 54.05 50.92 0.90 0.89 0.84 0.81
Fr
All 56.47 - 48.90 40.29 0.81 - 0.75 0.58
1450-1550 61.28 - 35.25 28.21 0.86 - 0.71 0.59
1550-1650 67.96 - 63.40 52.84 0.79 - 0.77 0.70
2. If k > 1, pick a scheme rk of length nk with
probability P (rk|rk?1). If no rhymes in rk
are shared with the previous stanza?s rhyme
scheme, rk?1, generate the stanza as before.
If rk shares rhymes with rk?1, generate the
stanza as a continuation of xk?1. For exam-
ple, if xk?1 = [dreams, lay, streams], and rk?1
and rk = aba and bcb, the stanza xk should be
generated so that xk1 and x
k
3 rhyme with lay.
4.2 Learning
This model for a poem can be formalized as an au-
toregressive HMM, an hidden Markov model where
each observation is conditioned on the previous ob-
servation as well as the latent state. An observation
at a time step k is the stanza xk, and the latent state at
that time step is the rhyme scheme rk. This model is
parametrized by ? and ?, where ?r,q = P (r|q) for all
schemes r and q. ? is initialized with orthographic
similarity. The learning algorithm follows from EM
for HMMs and our earlier algorithm.
Expectation Step: Estimate P (r|x) for each
stanza in the poem using the forward-backward
algorithm. The ?emission probability? P (x|r)
for the first stanza is same as in ?3, and for
subsequent stanzas xk, k > 1 is given by:
P (xk|xk?1, rk) =
nk?
i=1
(1? Ii,rk)P (x
k
i ) +
Ii,rk
?
j<i:rki =r
k
j
P (xki |x
k
j )
?
j:rki =r
k?1
j
P (xki |x
k?1
j ) (6)
Maximization Step: Update ? and ? analogously
to HMM transition and emission probabilities.
4.3 Results
As Table 2 shows, there is considerable improve-
ment over models that assume independent stanzas.
The most gains are found in French, which contains
many instances of ?linked? stanzas like the terza
rima, as well as English data containing long poems
made of several stanzas with the same scheme.
5 Future Work
Some possible extensions of our work include au-
tomatically generating the set of possible rhyme
schemes R, and incorporating partial supervision
into our algorithm as well as better ways of using
and adapting pronunciation information when avail-
able. We would also like to test our method on a
range of languages and texts.
To return to the motivations, one could use
the discovered annotations for machine translation
of poetry, or to computationally reconstruct pro-
nunciations, which is useful for historical linguis-
tics as well as other applications involving out-of-
vocabulary words.
Acknowledgments
We would like to thank Morgan Sonderegger for
providing most of the annotated English data in the
rhyming corpus and for helpful discussion, and the
anonymous reviewers for their suggestions.
81
References
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (CD-ROM). Linguistic
Data Consortium.
Roy J. Byrd and Martin S. Chodorow. 1985. Using an
online dictionary to find rhyming words and pronunci-
ations for unknown words. In Proceedings of ACL.
Lord Byron. 1824. Don Juan.
Dmitriy Genzel, Jakob Uszkoreit, and Franz Och. 2010.
?Poetic? statistical machine translation: Rhyme and
meter. In Proceedings of EMNLP.
Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010.
Automatic analysis of rhythmic poetry with applica-
tions to generation and translation. In Proceedings of
EMNLP.
Long Jiang and Ming Zhou. 2008. Generating Chinese
couplets using a statistical MT approach. In Proceed-
ings of COLING.
Hisar Maruli Manurung, Graeme Ritchie, and Henry
Thompson. 2000. Towards a computational model of
poetry generation. In Proceedings of AISB Symposium
on Creative and Cultural Aspects and Applications of
AI and Cognitive Science.
Robert Morrissey. 2011. ARTFL : American research
on the treasury of the French language. http://artfl-
project.uchicago.edu/content/artfl-frantext.
Yael Netzer, David Gabay, Yoav Goldberg, and Michael
Elhadad. 2009. Gaiku : Generating Haiku with word
associations norms. In Proceedings of the NAACL
workshop on Computational Approaches to Linguistic
Creativity.
Ananth Ramakrishnan, Sankar Kuppan, and
Sobha Lalitha Devi. 2009. Automatic genera-
tion of Tamil lyrics for melodies. In Proceedings of
the NAACL workshop on Computational Approaches
to Linguistic Creativity.
Morgan Sonderegger. 2011. Applications of graph the-
ory to an English rhyming corpus. Computer Speech
and Language, 25:655?678.
Henry Wyld. 1923. Studies in English rhymes from Sur-
rey to Pope. J Murray, London.
82
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 80?84,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Decoding Running Key Ciphers
Sravana Reddy?
Department of Computer Science
The University of Chicago
1100 E. 58th Street
Chicago, IL 60637, USA
sravana@cs.uchicago.edu
Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292, USA
knight@isi.edu
Abstract
There has been recent interest in the problem
of decoding letter substitution ciphers using
techniques inspired by natural language pro-
cessing. We consider a different type of classi-
cal encoding scheme known as the running key
cipher, and propose a search solution using
Gibbs sampling with a word language model.
We evaluate our method on synthetic cipher-
texts of different lengths, and find that it out-
performs previous work that employs Viterbi
decoding with character-based models.
1 Introduction
The running key cipher is an encoding scheme that
uses a secret keyR that is typically a string of words,
usually taken from a book or other text that is agreed
upon by the sender and receiver. When sending a
plaintext message P , the sender truncates R to the
length of the plaintext. The scheme also relies on
a substitution function f , which is usually publicly
known, that maps a plaintext letter p and key letter
r to a unique ciphertext letter c. The most common
choice for f is the tabula recta, where c = (p + r)
mod 26 for letters in the English alphabet, with A
= 0, B = 1, and so on.
To encode a plaintext with a running key, the
spaces in the plaintext and the key are removed, and
for every 0 ? i < |P |, the ciphertext letter at posi-
tion i is computed to be Ci ? f(Pi, Ri). Figure 1
shows an example encoding using the tabula recta.
For a given ciphertext and known f , the plaintext
uniquely determines the running key and vice versa.
?Research conducted while the author was visiting ISI.
Since we know that the plaintext and running key
are both drawn from natural language, our objective
function for the solution plaintext under some lan-
guage model is:
P? = argmax
P
log Pr(P ) Pr(RP,C) (1)
where the running key RP,C is the key that corre-
sponds to plaintext P and ciphertext C.
Note that if RP,C is a perfectly random sequence
of letters, this scheme is effectively a ?one-time pad?,
which is provably unbreakable (Shannon, 1949).
The knowledge that both the plaintext and the key
are natural language strings is important in breaking
a running key cipher.
The letter-frequency distribution of running key
ciphertexts is notably flatter than than the plaintext
distribution, unlike substitution ciphers where the
frequency profile remains unchanged, modulo letter
substitutions. However, the ciphertext letter distri-
bution is not uniform; there are peaks corresponding
to letters (like I) that are formed by high-frequency
plaintext/key pairs (like E and E).
2 Related Work
2.1 Running Key Ciphers
Bauer and Tate (2002) use letter n-grams (without
smoothing) up to order 6 to find the most probable
plaintext/key character pair at each position in the ci-
phertext. They test their method on 1000-character
ciphertexts produced from plaintexts and keys ex-
tracted from Project Gutenberg. Their accuracies
range from 28.9% to 33.5%, where accuracy is mea-
sured as the percentage of correctly decoded char-
80
Figure 1: Example of a running key cipher. Note that key is truncated to the length of the plaintext.
Plaintext ? linguistics is fun, Running Key ? colorless green ideas, tabula recta substitution where Ci ? (Pi +Ri) mod 26
Plaintext: L I N G U I S T I C S I S F U N
Running Key: C O L O R L E S S G R E E N I D
Ciphertext: N W Y U L T W L A I J M W S C Q
acters. Such figures are too low to produce read-
able plaintexts, especially if the decoded regions are
not contiguous. Griffing (2006) uses Viterbi decod-
ing and letter 6-grams to improve on the above re-
sult, achieving a median 87% accuracy over several
1000-character ciphertexts. A key shortcoming of
this work is that it requires searching through about
265 states at each position in the ciphertext.
2.2 Letter Substitution Ciphers
Previous work in decipherment of classical ciphers
has mainly focused on letter substitution. These ci-
phers use a substitution table as the secret key. The
ciphertext is generated by substituting each letter of
the plaintext according to the substitution table. The
table may be homophonic; that is, a single plaintext
letter could map to more than one possible cipher-
text letter. Just as in running key ciphers, spaces in
the plaintext are usually removed before encoding.
Proposed decipherment solutions for letter substi-
tution ciphers include techniques that use expecta-
tion maximization (Ravi and Knight, 2008), genetic
algorithms (Oranchak, 2008), integer programming
(Ravi and Knight, 2009), A* decoding (Corlett and
Penn, 2010), and Bayesian learning with Dirichlet
processes (Ravi and Knight, 2011).
2.3 Vigene`re Ciphers
A scheme similar to the running key cipher is the Vi-
gene`re cipher, also known as the periodic key cipher.
Instead of a single long string spanning the length of
the plaintext, the key is a short string ? usually but
not always a single word or phrase ? repeated to the
length of the plaintext. Figure 2 shows an example
Vigene`re cipher encoding. This cipher is less secure
than the running key, since the short length of the
key vastly reduces the size of the search space, and
the periodic repetition of the key leaks information.
Recent work on decoding periodic key ciphers
perform Viterbi search on the key using letter n-
gram models (Olsen et al, 2011), with the assump-
tion that the length of the key is known. If unknown,
the key length can be inferred using the Kasiski Test
(Kasiski, 1863) which takes advantage of repeated
plaintext/key character pairs.
3 Solution with Gibbs Sampling
In this paper, we describe a search algorithm that
uses Gibbs Sampling to break a running key cipher.
3.1 Choice of Language Model
The main advantage of a sampling-based approach
over Viterbi decoding is that it allows us to seam-
lessly use word-based language models. Lower or-
der letter n-grams may fail to decipher most cipher-
texts even with perfect search, since an incorrect
plaintext and key could have higher likelihood un-
der a weak language model than the actual message.
3.2 Blocked Sampling
One possible approach is to sample a plaintext letter
at each position in the ciphertext. The limitation of
such a sampler for the running key problem is that
is extremely slow to mix, especially for longer ci-
phertexts: we found that in practice, it does not usu-
ally converge to the optimal solution in a reasonable
number of iterations even with simulated annealing.
We therefore propose a blocked sampling algorithm
that samples words rather than letters in the plain-
text, as follows:
1. Initialize randomly P := p1p2 . . . p|C|, fix R as
the key that corresponds to P,C
2. Repeat for some number of iterations
(a) Sample spaces (word boundaries) in P ac-
cording to Pr(P )
(b) Sample spaces in R according to Pr(R)
(c) Sample each word in P according to
Pr(P ) Pr(R), updating R along with P
(d) Sample each word in R according to
Pr(P ) Pr(R), updating P along with R
81
Figure 2: Example of a Vigene`re cipher cipher, with a 5-letter periodic key, repeated to the length of the plaintext.
Plaintext ? linguistics is fun, Periodic Key ? green, tabula recta substitution.
Plaintext: L I N G U I S T I C S I S F U N
Running Key: G R E E N G R E E N G R E E N G
Ciphertext: R Z R K H O J X M P Y Z W J H T
3. Remove spaces and return P,R
Note that every time a word in P is sampled, it
induces a change in R that may not be a word or a
sequence of words, and vice versa. Sampling word
boundaries will also produce hypotheses contain-
ing non-words. For this reason, we use a word tri-
gram model linearly interpolated with letter trigrams
(including the space character).1 The interpolation
mainly serves to smooth the search space, with the
added benefit of accounting for out-of-vocabulary,
misspelled, or truncated words in the actual plaintext
or key. Table 1 shows an example of one sampling
iteration on the ciphertext shown in Figure 1.
Table 1: First sampling iteration on the ciphertext
NWYULTWLAIJMWSCQ
Generate P,R P : WERGATERYBVIEDOW
with letter trigrams R: RSHOLASUCHOESPOU
Sample spaces in P P : WERGAT ER YB VIEDOW
Sample spaces in R R: RS HOLASUCHOES POU
Sample words in P P : ADJUST AN MY WILLOW
R: NT PATAWYOKNEL HOU
Sample words in R P : NEWNXI ST HE SYLACT
R: AS CHOLESTEROL SAX
4 Experiments
4.1 Data
We randomly select passages from the Project
Gutenberg and Wall Street Journal Corpus extracts
that are included in the NLTK toolkit (Bird et al,
2009). The passages are used as plaintext and key
pairs, and combined to generate synthetic ciphertext
data. Unlike previous works which used constant-
length ciphertexts, we study the effect of message
length on decipherment by varying the ciphertext
length (10, 100, and 1000 characters).
Our language model is an interpolation of word
trigrams and letter trigrams trained on the Brown
1Pr(P ) = ?Pr(P |word LM) + (1 ? ?) Pr(P |letter LM),
and similarly for Pr(R).
Corpus (Nelson and Kucera, 1979), with Kneser-
Ney smoothing. We fixed the word language model
interpolation weight to ? = 0.7.
4.2 Baseline and Evaluation
For comparison with the previous work, we re-
implement Viterbi decoding over letter 6-grams
(Griffing, 2006) trained on the Brown Corpus. In
addition to decipherment accuracy, we compare the
running time in seconds of the two algorithms.
Both decipherment programs were implemented in
Python and run on the same machines. The Gibbs
Sampler was run for 10000 iterations.
As in the Griffing (2006) paper, since the plaintext
and running key are interchangeable, we measure
the accuracy of a hypothesized solution against the
reference as the max of the accuracy between the hy-
pothesized plaintext and the reference plaintext, and
the hypothesized plaintext and the reference key.
4.3 Results
Table 2 shows the average decipherment accuracy of
our algorithm and the baseline on each dataset. Also
shown is the number of times that the Gibbs Sam-
pling search failed ? that is, when the algorithm did
not hypothesize a solution that had a probability at
least as high as the reference within 10000 iterations.
It is clear that the Gibbs Sampler is orders of mag-
nitude faster than Viterbi decoding. Performance
on the short (length 10) ciphertexts is poor under
both algorithms. This is expected, since the degree
of message uncertainty, or message equivocation as
defined by Shannon, is high for short ciphertexts:
there are several possible plaintexts and keys be-
sides the original that are likely under an English
language model. Consider the ciphertext WAEEXF-
PROV which was generated by the plaintext seg-
ment ON A REFEREN and key INENTAL AKI.
The algorithm hypothesizes that the plaintext is THE
STRAND S and key DTAME OPELD, which both
receive high language model probability.
82
Table 2: Decipherment accuracy (proportion of correctly deciphered characters). Plaintext and key sources for the
ciphertext test data were extracted by starting at random points in the corpora, and selecting the following n characters.
Length of Plaintext and key # Cipher- Average Accuracy Avg. running time (sec) # Failed Gibbs
ciphertext source texts Viterbi Gibbs Viterbi Gibbs searches
10
Project Gutenberg 100 14% 17% 1005 47 5
Wall Street Journal 100 10% 26% 986 38 2
100
Project Gutenberg 100 27% 42% 10212 236 19
Wall Street Journal 100 22% 58% 10433 217 12
1000
Project Gutenberg 100 63% 88% 112489 964 32
Wall Street Journal 100 60% 93% 117303 1025 25
Table 3: Substitution function parameterized by the keyword, CIPHER. f(p, r) is the entry in the row corresponding to p and the
column corresponding to r.
A B C D E F G H I J K L M N O P Q R S T U V W X Y Z
A C I P H E R A B D F G J K L M N O Q S T U V W X Y Z
B I P H E R A B D F G J K L M N O Q S T U V W X Y Z C
C P H E R A B D F G J K L M N O Q S T U V W X Y Z C I
. . .
However, on the long ciphertexts, our algorithm
gets close to perfect decipherment, surpassing the
Viterbi algorithm by a large margin.2 Accuracies on
the Wall Street Journal ciphertexts are higher than on
the Gutenberg ciphertexts for our algorithm, which
may be because the latter is more divergent from the
Brown Corpus language model.
5 Future Work
5.1 Unknown substitution functions
Some running key ciphers also use a secret substi-
tution function f rather than the tabula recta or an-
other known function. In typical cases, these func-
tions are not arbitrary, but are parameterized by a se-
cret keyword that mutates the tabula recta table. For
example, the function with the keyword CIPHER
would be the substitution table shown in Table 3.
Decoding a running key ciphertext under a latent
substitution function is an open line of research. One
possibility is to extend our approach by sampling the
keyword or function in addition to the plaintext.
5.2 Exact search
Since some the errors in Gibbs Sampling decipher-
ment are due to search failures, a natural exten-
sion of this work would be to adapt Viterbi search
2The accuracies that we found for Viterbi decoding are
lower than those reported by Griffing (2006), which might be
because they use an in-domain language model.
or other exact decoding algorithms like A* to use
word-level language models. A naive implementa-
tion of Viterbi word-based decoding results in com-
putationally inefficient search spaces for large vo-
cabularies, so more sophisticated methods or heuris-
tics will be required.
5.3 Analysis of Running Key Decipherment
While there has been theoretical and empirical anal-
ysis of the security of letter substitution ciphers
of various lengths under different language models
(Shannon, 1949; Ravi and Knight, 2008), there has
been no similar exposition of running key ciphers,
which we reserve for future work.
6 Conclusion
We propose a decipherment algorithm for running
key ciphers that uses Blocked Gibbs Sampling and
word-based language models, which shows signifi-
cant speed and accuracy improvements over previ-
ous research into this problem.
Acknowledgments
We would like to thank Sujith Ravi for initial exper-
iments using Gibbs sampling, and the anonymous
reviewers. This research was supported in part by
NSF grant 0904684.
83
References
Craig Bauer and Christian Tate. 2002. A statistical attack
on the running key cipher. Cryptologia, 26(4).
Steven Bird, Edward Loper, and Ewan Klein. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia Inc.
Eric Corlett and Gerald Penn. 2010. An exact A* method
of deciphering letter-substitution ciphers. In Proceed-
ings of ACL.
Alexander Griffing. 2006. Solving the running key ci-
pher with the Viterbi algorithm. Cryptologia, 30(4).
Friedrich Kasiski. 1863. Die Geheimschriften und die
Dechiffrir-Kunst. E. S. Mittler und Sohn.
Francis Nelson and Henry Kucera. 1979. The Brown
Corpus: A Standard Corpus of Present-Day Edited
American English. Brown University.
Peder Olsen, John Hershey, Steven Rennie, and Vaib-
hava Goel. 2011. A speech recognition solution to
an ancient cryptography problem. Technical Report
RC25109 (W1102-005), IBM Research.
David Oranchak. 2008. Evolutionary algorithm for de-
cryption of monoalphabetic homophonic substitution
ciphers encoded as constraint satisfaction problems. In
Proceedings of the Conference on Genetic and Evolu-
tionary Computation.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of EMNLP.
Sujith Ravi and Kevin Knight. 2009. Attacking letter
substitution ciphers with integer programming. Cryp-
tologia, 33(4).
Sujith Ravi and Kevin Knight. 2011. Bayesian inference
for Zodiac and other homophonic ciphers. In Proceed-
ings of ACL.
Claude Shannon. 1949. Communication theory of se-
crecy systems. Bell System Technical Journal, 28(4).
84
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 78?86,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
What We Know About The Voynich Manuscript
Sravana Reddy?
Department of Computer Science
The University of Chicago
Chicago, IL 60637
sravana@cs.uchicago.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
The Voynich Manuscript is an undeciphered
document from medieval Europe. We present
current knowledge about the manuscript?s text
through a series of questions about its linguis-
tic properties.
1 Introduction
The Voynich manuscript, also referred to as the
VMS, is an illustrated medieval folio written in an
undeciphered script.
There are several reasons why the study of the
manuscript is of interest to the natural language pro-
cessing community, besides its appeal as a long-
enduring unsolved mystery. Since even the ba-
sic structure of the text is unknown, it provides a
perfect opportunity for the application of unsuper-
vised learning algorithms. Furthermore, while the
manuscript has been examined by various scholars,
it has much to benefit from attention by a commu-
nity with the right tools and knowledge of linguis-
tics, text analysis, and machine learning.
This paper presents a review of what is currently
known about the VMS, as well as some original ob-
servations. Although the manuscript raises several
questions about its origin, authorship, the illustra-
tions, etc., we focus on the text through questions
about its properties. These range from the level of
the letter (for example, are there vowels and conso-
nants?) to the page (do pages have topics?) to the
document as a whole (are the pages in order?).
? This work was completed while the author was visiting
the Information Sciences Institute.
2 Background
2.1 History
From the illustrations ? hairstyles and features of the
human figures ? as well as the shapes of the glyphs,
the manuscript is posited to have been created in Eu-
rope. Carbon-dating at the University of Arizona
has found that the vellum was created in the 15th
century, and the McCrone Research Institute has as-
serted that the ink was added shortly afterwards1.
The exact history of the VMS is not established.
According to Zandbergen (2010), the earliest owner
that it can be traced to is Jacobus de Tepenec in
Prague in the early 1600s. It is speculated that it was
given to him by Emperor Rudolf II, but it is unclear
how and from where the manuscript entered Prague.
The VMS appears to have circulated in Prague for
some time, before being sent to Athanasius Kircher
in Italy in 1665. It remained in Italy until 1912,
when it was sold to Wilfrid Voynich, who brought
it to America. It was then sold to the bookdealer
Kraus, who later donated it to the Yale University
library2, where it is currently housed.
2.2 Overview
The manuscript is divided into quires ? sections
made out of folded parchment, each of which con-
sists of folios, with writing on both sides of each fo-
lio (Reeds, 2002). Including blank pages and pages
with no text, there are 240 pages, although it is be-
lieved that some are missing (Pelling, 2006). 225
1These results are as yet unpublished. A paper about the
carbon-dating experiments is forthcoming in 2011.
2High-resolution scans are available at
http://beinecke.library.yale.edu/digitallibrary/voynich.html
78
pages include text, and most are illustrated. The text
was probably added after the illustrations, and shows
no evidence of scratching or correction.
The text is written left to right in paragraphs that
are left-aligned, justified, and divided by whitespace
into words. Paragraphs do not span multiple pages.
A few glyphs are ambiguous, since they can
be interpreted as a distinct character, or a ligature
of two or more other characters. Different tran-
scriptions of the manuscript have been created,
depending on various interpretations of the glyphs.
We use a machine-readable transcription based on
the alphabet proposed by Currier (1976), edited
by D?Imperio (1980) and others, made avail-
able by the members of the Voynich Manuscript
Mailing List (Gillogly and Reeds, 2005) at
http://www.voynich.net/reeds/gillogly/voynich.now.
The Currier transcription maps the characters to the
ASCII symbols A-Z, 0-9, and *. Under this tran-
scription, the VMS is comprised of 225 pages, 8114
word types, and 37919 word tokens. Figure 1 shows
a sample VMS page and its Currier transcription.
2.3 Manuscript sections
Based on the illustrations, the manuscript has tradi-
tionally been divided into six sections: (1) herbal,
containing drawings of plants; (2) Astronomical,
containing zodiac-like illustrations; (3) Biological,
mainly containing drawings of female human fig-
ures; (4) Cosmological, consisting of circular illus-
trations; (5) Pharmaceutical, containing drawing of
small containers and parts of plants, and (6) Stars
(sometimes referred to as Recipes), containing very
dense text with drawings of stars in the margins.
Currier (1976) observed from letter and substring
frequencies that the text is comprised of two distinct
?languages?, A and B. Interestingly, the Biological
and Stars sections are mainly written in the B lan-
guage, and the rest mainly in A.
Using a two-state bigram HMM over the entire
text, we find that the two word classes induced by
EM more or less correspond to the same division
? words in pages classified as being in the A lan-
guage tend to be tagged as one class, and words in
B language pages as the other, indicating that the
manuscript does indeed contain two different vocab-
ularies (which may be related languages, dialects, or
simply different textual domains). In Figure 2, we
Figure 1: Page f81v (from the Biological section).
(a) Scan of page
BAR ZC9 FCC89 ZCFAE 8AE 8AR OE BSC89 ZCF 8AN
OVAE ZCF9 4OFC89 OFAM FAT OFAE 2AR OE FAN
OEFAN AE OE ROE 8E 2AM 8AM OEFCC89 OFC89 89FAN
ZCF S89 8AEAE OE89 4OFAM OFAN SCCF9 89 OE FAM
8AN 89 8AM SX9 OFAM 8AM OPAN SX9 OFCC89 4OF9
FAR 8AM OFAR 4OFAN OFAM OE SC89 SCOE EF9 E2
AM OFAN 8AE89 OEOR OE ZCXAE 8AM 4OFCC8AE 8AM
SX9 2SC89 4OE 9FOE OR ZC89 ZCC89 4OE FCC89 8AM
8FAN WC89 OE89 9AR OESC9 FAM OFCC9 8AM OEOR
SCX9 8AII89
BOEZ9 OZ9PCC8 4OB OFCC89 OPC89 OFZC89 4OP9
8ATAJ OZC9 4OFCC9 OFCC9 OF9 9FCC9 4OF9 OF9EF9
OES9 F9 8ZOE98 4OE OE S89 ZC89 4OFC89 9PC89
SCPC89 EFC8C9 9PC89 9FCC2C9 8SC8 9PC89 9PC89
8AR 9FC8A IB*9 4OP9 9FC89 OFAE 8ZC89 9FCC89
C2CCF9 8AM OFC89 4OFCC8 4OFC89 ESBS89 4OFAE
SC89 OE ZCC9 2AEZQ89 4OVSC89 R SC89 EPAR9
EOR ZC89 4OCC89 OE S9 RZ89 EZC89 8AR S89
BS89 2ZFS89 SC89 OE ZC89 4OESC89 4OFAN ZX9 8E
RAE 4OFS89 SC9 OE SCF9 OE ZC89 4OFC89 4OFC89
SX9 4OF9 2OEFCC9 OE ZC89 4OFAR ZCX9 8C2C89
4OFAR 4OFAE 8OE S9 4OQC9 SCFAE SO89 4OFC89
EZCP9 4OE89 EPC89 4OPAN EZO 4OFC9 EZC89 EZC89
SC89 4OEF9 ESC8AE 4OE OPAR 4OFAE 4OE OM SCC9
8AE EO*C89 ZC89 2AE SPC89PAR ZOE 4CFS9 9FAM
OEFAN ZC89 4OF9 8SC89 ROE OE Q89 9PC9 OFSC89
4OFAE OFCC9 4OE SCC89 2AE PCOE 8S89 E9 OZC89
4OPC89 ZOE SC89 9ZSC9 OE SC9 4OE SC89 PS8 OF9
OE SCSOE PAR OM OFC89 8AE ZC9 OEFCOE OEFCC89
OFCOE 8ZCOE O3 OEFCC89 PC89 SCF9 ZXC89 SAE
OPON OEFOE
(b) Transcription in the Currier alphabet. Paragraph (but not
line) breaks are indicated.
79
illustrate the division of the manuscript pages into
the six sections, and show the proportion of words
in each page that are classified as the B language.
For coherence, all our experimental results in the
rest of this paper are on the B language (which we
denote by VMS B) ? specifically, the Biological and
Stars sections ? unless otherwise specified. These
sections together contain 43 pages, with 3920 word
types, 17597 word tokens, and 35 characters. We
compare the VMS?s statistical properties with three
natural language texts of similar size: the first 28551
words from the English Wall Street Journal Corpus,
19327 words from the Arabic Quran (in Buckwalter
transcription), and 18791 words from the Chinese
Sinica Treebank.
3 The Letter
3.1 Are vowels and consonants represented?
If a script is alphabetic, i.e., it uses approximately
one character per phoneme, vowel and consonant
characters can be separated in a fully unsupervised
way. Guy (1991) applies the vowel-consonant sep-
aration algorithm of (Sukhotin, 1962) on two pages
of the Biological section, and finds that four charac-
ters (O, A, C, G) separate out as vowels. However,
the separation is not very strong, and several words
do not contain these characters.
Another method is to use a two-state bigram
HMM (Knight et al, 2006; Goldsmith and Xanthos,
2009) over letters, and induce two clusters of letters
with EM. In alphabetic languages like English, the
clusters correspond almost perfectly to vowels and
consonants. We find that a curious phenomenon oc-
curs with the VMS ? the last character of every word
is generated by one of the HMM states, and all other
characters by another; i.e., the word grammar is a?b.
There are a few possible interpretations of this. It
is possible that the vowels from every word are re-
moved and placed at the end of the word, but this
means that even long words have only one vowel,
which is unlikely. Further, the number of vowel
types would be nearly half the alphabet size. If the
script is a syllabary or a logograph, a similar clus-
tering will surface, but given that there are only 35
characters, it is unlikely that each of them represents
a syllable or word. A more likely explanation is that
the script is an abjad, like the scripts of Semitic lan-
guages, where all or most vowels are omitted. In-
deed, we find that a 2-state HMM on Arabic without
diacritics and English without vowels learns a simi-
lar grammar, a?b+.
3.2 Do letters have cases?
Some characters (F, B, P, V) that appear mainly at
paragraphs beginnings are referred to ?gallows? ?
glyphs that are taller and more ornate than others.
Among the glyphs, these least resemble Latin, lead-
ing to the belief that they are null symbols, which
Morningstar (2001) refutes.
Another hypothesis is that gallows are upper-
case versions of other characters. We define
BESTSUB(c) to be the character x that produces the
highest decrease in unigram word entropy when x
is substituted for all instances of c. For English up-
percase characters c, BESTSUB(c) is the lowercase
version. However, BESTSUB of the VMS gallows
is one of the other gallows! This demonstrates that
they are not uppercase versions of other letters, and
also that they are contextually similar to one another.
3.3 Is there punctuation?
We define punctuation as symbols that occur only at
word edges, whose removal from the word results in
an existing word. There are two characters that are
only found at the ends of words (Currier K and L), but
most of the words produced by removing K and L are
not in the vocabulary. Therefore, there is most likely
no punctuation, at least in the traditional sense.
4 The Word
4.1 What are the word frequency and length
distributions?
The word frequency distribution follows Zipf?s law,
which is a necessary (though not sufficient) test of
linguistic plausibility. We also find that the unigram
word entropy is comparable to the baseline texts (Ta-
ble 1).
Table 1: Unigram word entropy in bits.
VMS B English Arabic Chinese
9.666 10.07 9.645 10.31
Several works have noted the narrow binomial
distribution of word lengths, and contrasted it with
80
Figure 2: VMS sections, and percentage of word tokens in each page that are tagged as language B by the HMM.
the wide asymmetric distribution of English, Latin,
and other European languages. This contributed to
speculation that the VMS is not a natural language,
but a code or generated by some other stochastic
process. However, Stolfi (2005) show that Pinyin
Chinese, Tibetan, and Vietnamese word lengths fol-
low a binomial distribution, and we found (Figure 3)
that certain scripts that do not contain vowels, like
Buckwalter Arabic and devoweled English, have a
binomial distribution as well.3 The similarity with
devoweled scripts, especially Arabic, reinforces the
hypothesis that the VMS script may be an abjad.
Figure 3: Word length distributions (word types).
Landini (2001) found that the VMS follows Zipf?s
law of word lengths: there is an inverse relationship
between the frequency and length of a word.
3This is an example of why comparison with a range of
languages is required before making conclusions about the
language-like nature of a text.
4.2 How predictable are letters within a word?
Bennett (1976) notes that the second-order entropy
of VMS letters is lower than most European lan-
guages. Stolfi (2005) computes the entropy of each
character given the left and right contexts and finds
that it is low for most of the VMS text, particularly
the Biological section, compared to texts in other
languages. He also ascertains that spaces between
words have extremely low entropy.
We measure the predictability of letters, and com-
pare it to English, Arabic, and Pinyin Chinese. Pre-
dictability is measured by finding the probabilities
over a training set of word types, guessing the most
likely letter (the one with the highest probability) at
each position in a word in the held-out test set, and
counting the proportion of times a guess is correct.
Table 2 shows the predictability of letters as uni-
grams, and given the preceding letter in a word (bi-
grams). VMS letters are more predictable than other
languages, with the predictability increasing sharply
given the preceding contexts, similarly to Pinyin.
Table 2: Predictability of letters, averaged over 10-fold
cross-validation runs.
VMS B English Arabic Pinyin
Bigram 40.02% 22.62% 24.78% 38.92%
Unigram 14.65% 11.09% 13.29% 11.20%
Zandbergen (2010) computes the entropies of
characters at different positions in words in the Stars
section, and finds that the 1st and 2nd characters of a
word are more predictable than in Latin or Vulgate,
but the 3rd and 4th characters are less predictable.
81
It has also been observed that word-final char-
acters have much lower entropy compared to most
other languages ? some characters appear almost ex-
clusively at the ends of words.
4.3 Is there morphological structure?
The above observations suggest that words are made
up of morpheme-like chunks. Several hypotheses
about VMS word structure have been proposed. Tilt-
man (1967) proposed a template consisting of roots
and suffixes. Stolfi (2005) breaks down the morphol-
ogy into ?prefix-midfix-suffix?, where the letters in
the midfixes are more or less disjoint from the let-
ters in the suffixes and prefixes. Stolfi later modified
this to a ?core-mantel-crust? model, where words are
composed of three nested layers.
To determine whether VMS words have affixal
morphology, we run an unsupervised morphologi-
cal segmentation algorithm, Linguistica (Goldsmith,
2001), on the VMS text. The MDL-based algo-
rithm segments words into prefix+stem+suffix, and
extracts ?signatures?, sets of affixes that attach to the
same set of stems. Table 3 lists a few sample signa-
tures, showing that stems in the same signature tend
to have some structural similarities.
Table 3: Some morphological signatures.
Affixes Stems
OE+, A3 AD AE AE9 AEOR AJ AM AN AR AT
OP+, E O O2 OE OJ OM ON OR
null+ SAJ SAR SCC9 SCCO SCO2 SO
OE+ BSC28 BSC9 CCC8 COC8CR FAEOE
FAK FAU FC8 FC8AM FCC FCC2 FCC9R
FCCAE FCCC2 FCCCAR9 FCO9 FCS9
FCZAR FCZC9 OEAR9 OESC9 OF9 OR8
SC29 SC89O SC8R SCX9 SQ9
+89, 4OFCS 4OFCZ 4OFZ 4OPZ 8AES 8AEZ
+9, 9FS 9PS EFCS FCS PS PZ
+ C89 OEFS OF OFAES OFCS OFS OFZ
5 Syntax
5.1 Is there word order?
One of the most puzzling features of the VMS is its
weak word order. Notably, the text has very few re-
peated word bigrams or trigrams, which is surpris-
ing given that the unigram word entropy is com-
parable to other languages. Furthermore, there are
sequences of two or more repeated words, or rep-
etitions of very similar words. For example, the
first page of the Biological section contains the line
4OFCC89 4OFCC89 4OFC89 4OFC89 4OFCC89 E89.
We compute the predictability of a word given the
previous word (Table 4). Bigram contexts only pro-
vide marginal improvement in predictability for the
VMS, compared to the other texts. For comparison
with a language that has ?weak word order?, we also
compute the same numbers for the first 22766 word
tokens of the Hungarian Bible, and find that the em-
pirical word order is not that weak after all.
Table 4: Predictability of words (over 10-fold cross-
validation) with bigram contexts, compared to unigrams.
Unigram Bigram Improvement
VMS B 2.30% 2.50% 8.85%
English 4.72% 11.9% 151%
Arabic 3.81% 14.2% 252%
Chinese 16.5% 19.8% 19.7%
Hungarian 5.84% 13.0% 123%
5.2 Are there latent word classes?
While there are very few repeated word bigrams,
perhaps there are latent classes of words that gov-
ern word order. We induce ten word classes using a
bigram HMM trained with EM (Figure 4). As with
the stems in the morphological signatures, the words
in each class show some regularities ? although it
is hard to quantify the similarities ? suggesting that
these latent classes are meaningful.
Currier (1976) found that some word-initial char-
acters are affected by the word-final characters of
the immediately preceding word. He concludes that
the ?words? being syllables or digits would explain
this phenomenon, although that is unlikely given the
rarity of repeated sequences.
We redo the predictability experiments of the pre-
vious section, using the last m letters of the previous
word to predict the first n letters of the current word.
When n > 2, improvement in predictability remains
low. However, when n is 1 or 2, there is a noticeable
improvement when using the last few characters of
the previous word as contexts (Table 5).
5.3 Are there long-distance word correlations?
Weak bigram word order can arise if the text is
scrambled or is generated by a unigram process. Al-
ternately, the text might have been created by inter-
82
Figure 4: Some of the induced latent classes.
(a) (b) (c)
Table 5: Relative improvement in predictability of first
n word-characters using last m characters of previous
word, over using no contextual information.
VMS B English Arabic
Whole words 8.85% 151% 252%
m = 1 31.8% 31.1% 26.8%
n = 1 m = 2 30.7% 45.8% 61.5%
m = 3 29.9% 60.3% 92.4%
m = 1 16.0% 42.8% 0.0736%
n = 2 m = 2 12.4% 67.5% 14.1%
m = 3 10.9% 94.6% 33.2%
leaving the words of two or more texts, in which case
there will be long-distance correlations.
Schinner (2007) shows that the probability of sim-
ilar words repeating in the text at a given distance
from each other follows a geometric distribution.
Figure 5 illustrates the ?collocationness? at dis-
tance d, measured as the average pointwise mutual
information over all pairs of words w1, w2 that occur
more than once at distance d apart. VMS words do
not show significant long-distance correlations.
6 The Page
6.1 Do pages have topics?
That is, do certain words ?burst? with a high fre-
quency within a page, or are words randomly dis-
tributed across the manuscript? Figure 6 shows a vi-
sualization of the TF-IDF values of words in a VMS
B page, where the ?documents? are pages, indicating
the relevance of each word to the page. Also shown
is the same page in a version of the document created
by scrambling the words of the original manuscript,
and repaginating to the same page lengths. This sim-
ulates a document where words are generated inde-
pendent of the page, i.e., the pages have no topics.
Figure 5: Long-range collocationness. Arabic shows
stronger levels of long-distance correlation compared to
English and Chinese. VMS B shows almost no correla-
tions for distance d > 1.
To quantify the degree to which a page contains
topics, we measure the entropy of words within the
page, and denote the overall ?topicality? T of a doc-
ument as the average entropy over all the pages. As
a control, we compute the topicality Trand of the
scrambled version of the document. 1 ? T/Trand
indicates the extent to which the pages of the docu-
ment contain topics. Table 6 shows that by this mea-
sure, the VMS?s strength of page topics is less than
the English texts, but more than the Quran4, signify-
ing that the pages probably do have topics, but are
not independent of one another.
6.2 Is the text prose?
Visually, the text looks like prose written in para-
graphs. However, Currier (1976) stated that ?the line
4We demarcate a ?page? to be approximately 25 verses for
the Quran, a chapter for the Genesis, and an article for the WSJ.
83
Figure 6: TF-IDF visualization of page f108v in the Stars section.
(a) Original document, showing bursts (b) Scrambled version ? flatter distribution
Table 6: Strength of page topics in VMS and other texts,
cropped to be of comparable length to the VMS.
VMS English English Arabic
B WSJ Genesis Quran
T 7.5 6.3 6.6 7.7
Trand 7.7 6.5 7.1 7.9
1? T/Trand 0.033 0.037 0.069 0.025
is a functional entity? ? that is, there are patterns to
lines on the page that are uncharacteristic of prose.
In particular, certain characters or sequences appear
almost exclusively at the beginnings or ends of lines.
Figure 7 shows the distribution of characters at
line-edges, relative to their occurrences at word
beginnings or endings,confirming Currier?s obser-
vation. It is particularly interesting that lower-
frequency characters occur more at line-ends, and
higher-frequency ones at the beginnings of lines.
Schinner (2007) found that characters show long-
range correlations at distances over 72 characters,
which is a little over the average line length.
7 The Document
7.1 Are the pages in order?
We measure the similarity between two pages as the
cosine similarity over bags of words, and count the
proportion of pages Pi where the page Pi?1 or Pi+1
is the most similar page to Pi. We denote this mea-
sure by ADJPAGESIM. If ADJPAGESIM is high, it
indicates that (1) the pages are not independent of
each other and (2) the pages are in order.
Table 7 shows ADJPAGESIM for the VMS and
other texts. As expected, ADJPAGESIM is close to
zero for the VMS with pages scrambled, as well as
the WSJ, where each page is an independent article,
and is highest for the VMS, particularly the B pages.
Table 7: ADJPAGESIM for VMS and other texts.
VMS B 38.8%
VMS All 15.6%
VMS B pages scrambled 0%
VMS All pages scrambled 0.444%
WSJ 1.34%
English Genesis 25.0%
Arabic Quran 27.5%
This is a convincing argument for the pages be-
ing mostly in order. However, the non-contiguity
of the herbal and pharmaceutical sections and the
interleaving of the A and B languages indicates
that larger chunks of pages were probably re-
ordered. In addition, details involving illustrations
and ink-transfer across pages point to a few local re-
orderings (Pelling, 2006).
7.2 How many authors were involved?
Currier (1976) observed that the distinction between
the A and B languages corresponds to two different
types of handwriting, implying at least two authors.
He claimed that based on finer handwriting analysis,
there may have been as many as eight scribes.
8 Latin, Cipher, or Hoax?
Claims of decipherment of the VMS script have
been surfacing for several years, none of which are
convincing. Newbold (1928) believed that micro-
scopic irregularities of glyph edges correspond to
anagrammed Latin. Feely in 1943 proposed that the
script is a code for abbreviated Latin (D?Imperio,
1980). Sherwood (2008) believes that the words
are coded anagrams of Italian. Others have hypoth-
84
Figure 7: Proportion of word-edge characters at line-edges for lines that span the width of the page. Characters are in
ascending order of their total frequencies.
(a) Original document, showing biased distribution. (b) Flat distribution when words within lines are scrambled.
esized that the script is an encoding of Ukrainian
(Stojko, 1978), English (Strong, 1945; Brumbaugh,
1976), or a Flemish Creole (Levitov, 1987). The
word length distribution and other properties have
invoked decodings into East Asian languages like
Manchu (Banasik, 2004). These theories tend to rely
on arbitrary anagramming and substitutions, and are
not falsifiable or well-defined.
The mysterious properties of the text and its resis-
tance to decoding have led some to conclude that it
is a hoax ? a nonsensical string made to look vaguely
language-like. Rugg (2004) claims that words might
have been generated using a ?Cardan Grille? ? a
way to deterministically generate words from a ta-
ble of morphemes. However, it seems that the Grille
emulates a restricted finite state grammar of words
over prefixes, midfixes, and suffixes. Such a gram-
mar underlies many affixal languages, including En-
glish. Martin (2008) proposes a method of generat-
ing VMS text from anagrams of number sequences.
Like the previous paper, it only shows that this
method can create VMS-like words ? not that it is
the most plausible way of generating the manuscript.
It is also likely that the proposed scheme can be used
to generate any natural language text.
Schinner (2007) votes for the hoax hypothesis
based on his observations about characters showing
long-range correlations, and the geometric distribu-
tion of the probability of similar words repeating at
a fixed distance. These observations only confirm
that the VMS has some properties unlike natural lan-
guage, but not that it is necessarily a hoax.
9 Conclusion
We have detailed various known properties of the
Voynich manuscript text. Some features ? the lack
of repeated bigrams and the distributions of letters at
line-edges ? are linguistically aberrant, which others
? the word length and frequency distributions, the
apparent presence of morphology, and most notably,
the presence of page-level topics ? conform to natu-
ral language-like text.
It is our hope that this paper will motivate re-
search into understanding the manuscript by schol-
ars in computational linguistics. The questions pre-
sented here are obviously not exhaustive; a deeper
examination of the statistical features of the text in
comparison to a number of scripts and languages is
needed before any definite conclusions can be made.
Such studies may also inspire a quantitative interest
in linguistic and textual typologies, and be applica-
ble to the decipherment of other historical scripts.
Acknowledgments
We would like to thank the anonymous reviewers
and our colleagues at ISI and Chicago for their help-
ful suggestions. This work was supported in part by
NSF Grant 0904684.
85
References
Zbigniew Banasik. 2004.
http://www.ic.unicamp.br/ stolfi/voynich/04-05-
20-manchu-theo/alphabet.html.
William Ralph Bennett. 1976. Scientific and engineering
problem solving with a computer. Prentice-Hall.
Robert Brumbaugh. 1976. The Voynich ?Roger Bacon?
cipher manuscript: deciphered maps of stars. Journal
of the Warburg and Courtauld Institutes.
Prescott Currier. 1976. New research on the
Voynich Manuscript: Proceedings of a semi-
nar. Unpublished communication, available from
http://www.voynich.nu/extra/curr pdfs.html.
Mary D?Imperio. 1980. The Voynich Manuscript: An
Elegant Enigma. Aegean Park Press.
Jim Gillogly and Jim Reeds. 2005. Voynich Manuscript
mailing list. http://voynich.net/.
John Goldsmith and Aris Xanthos. 2009. Learning
phonological categories. Language, 85:4?38.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics.
Jacques Guy. 1991. Statistical properties of two folios of
the Voynich Manuscript. Cryptologia.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of COLING.
Gabriel Landini. 2001. Evidence of linguistic struc-
ture in the Voynich Manuscript using spectral analysis.
Cryptologia.
Leo Levitov. 1987. Solution of the Voynich Manuscript:
A Liturgical Manual for the Endura Rite of the Cathari
Heresy, the Cult of Isis. Aegean Park Press.
Claude Martin. 2008. Voynich, the game is over.
http://www.voynich.info/.
Jason Morningstar. 2001. Gallows variants as null char-
acters in the Voynich Manuscript. Master?s thesis,
University of North Carolina.
William Newbold. 1928. The Cipher of Roger Bacon.
University of Pennsylvania Press.
Nicholas John Pelling. 2006. The Curse of the Voyn-
ich: The Secret History of the World?s Most Mysteri-
ous Manuscript. Compelling Press.
Jim Reeds. 2002. Voynich Manuscript.
http://www.ic.unicamp.br/ stolfi/voynich/mirror/reeds.
Gordon Rugg. 2004. The mystery of the Voynich
Manuscript. Scientific American Magazine.
Andreas Schinner. 2007. The Voynich Manuscript: Evi-
dence of the hoax hypothesis. Cryptologia.
Edith Sherwood. 2008. The
Voynich Manuscript decoded?
http://www.edithsherwood.com/voynich decoded/.
John Stojko. 1978. Letters to God?s Eye: The Voynich
Manuscript for the first time deciphered and translated
into English. Vantage Press.
Jorge Stolfi. 2005. Voynich Manuscript stuff.
http://www.dcc.unicamp.br/ stolfi/voynich/.
Leonell Strong. 1945. Anthony Ashkam, the author of
the Voynich Manuscript. Science.
Boris Sukhotin. 1962. Eksperimental?noe vydelenie
klassov bukv s pomoscju evm. Problemy strukturnoj
lingvistiki.
John Tiltman. 1967. The Voynich Manuscript, the most
mysterious manuscript in the world. NSA Technical
Journal.
Rene? Zandbergen. 2010. Voynich MS.
http://www.voynich.nu/index.html.
86
